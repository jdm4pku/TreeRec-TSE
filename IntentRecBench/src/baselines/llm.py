import os
import json
import time
import argparse
import re
from typing import List, Dict, Tuple, Optional
from statistics import mean, pstdev
from math import log2

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    # å¦‚æœæ²¡æœ‰tqdmï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„è¿›åº¦æ¡æ›¿ä»£
    def tqdm(iterable, desc=None, total=None, **kwargs):
        return iterable

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    from sentence_transformers import SentenceTransformer
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    SentenceTransformer = None

os.environ["OPENAI_API_KEY"] = "sk-BwTI1iSg83soUQ6u2d1096B8A27848E5B3E4141154Dc592b"
os.environ["OPENAI_BASE_URL"] = "https://api.yesapikey.com/v1"

# ========== å·¥å…·å‡½æ•° ==========

def load_json(file_path: str):
    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)
    if not isinstance(data, (list, dict)):
        raise ValueError(f"Invalid JSON format in {file_path}")
    return data


def save_json(data, file_path: str):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


# ========== è¯„ä¼°æŒ‡æ ‡ ==========

def precision_at_k(ranked_names: List[str], gold: str, k: int) -> float:
    """P@K as Hit@K: 1 if gold appears in top-K, else 0."""
    top_k = ranked_names[:k]
    return 1.0 if gold in top_k else 0.0


def dcg_at_k(ranked_names: List[str], gold: str, k: int) -> float:
    """è®¡ç®— DCG@Kï¼ˆäºŒå…ƒç›¸å…³æ€§ï¼‰"""
    for idx, name in enumerate(ranked_names[:k], start=1):
        if name == gold:
            return 1.0 / log2(1.0 + idx)
    return 0.0


# ========== LLMæ¨¡å‹é…ç½® ==========

def get_model_config(model_name: str, api_key: Optional[str] = None, base_url: Optional[str] = None):
    """
    æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„APIé…ç½®
    æ”¯æŒ: GPT, Qwen, DeepSeek, Llama
    """
    model_lower = model_name.lower()
    
    # å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº† base_url å’Œ api_keyï¼Œä¼˜å…ˆä½¿ç”¨
    if base_url and api_key:
        return {
            "base_url": base_url,
            "api_key": api_key,
            "model_name": model_name
        }
    
    # GPT ç³»åˆ—æ¨¡å‹ï¼ˆä½¿ç”¨ OpenAI å®˜æ–¹ APIï¼‰
    print("***************")
    print(f"model_lower: {model_lower}")
    print("***************")
    if "gpt" in model_lower or "openai" in model_lower:
        return {
            "base_url": base_url or "http://66.206.9.230:4000/v1",
            "api_key": api_key or "sk-BwTI1iSg83soUQ6u2d1096B8A27848E5B3E4141154Dc592b",
            "model_name": model_name
        }
    
    print("***************")
    print(f"model_lower: {model_lower}")
    print("***************")
    # Qwen ç³»åˆ—æ¨¡å‹ï¼ˆä½¿ç”¨ SiliconFlowï¼‰
    if "qwen" in model_lower:
        return {
            "base_url": base_url or "https://api.siliconflow.cn/v1",
            "api_key": api_key or os.environ.get("SILICONFLOW_API_KEY", 
                        "sk-wbnxvocaaofhilzlgkvhiuhoivdawabyvaavkvblnokomdyz"),
            "model_name": model_name
        }
    
    # DeepSeek ç³»åˆ—æ¨¡å‹ï¼ˆä½¿ç”¨ SiliconFlowï¼‰
    print("***************")
    print(f"model_lower: {model_lower}")
    print("***************")
    if "deepseek" in model_lower:
        return {
            "base_url": base_url or "https://api.siliconflow.cn/v1",
            "api_key": api_key or os.environ.get("SILICONFLOW_API_KEY",
                        "sk-wbnxvocaaofhilzlgkvhiuhoivdawabyvaavkvblnokomdyz"),
            "model_name": model_name
        }
    
    # Llama ç³»åˆ—æ¨¡å‹ï¼ˆä½¿ç”¨ OpenRouterï¼‰
    if "llama" in model_lower:
        return {
            "base_url": base_url or "https://openrouter.ai/api/v1",
            "api_key": api_key or os.environ.get("OPENROUTER_API_KEY",
                        "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"),
            "model_name": model_name
        }
    
    # æŠ›å‡ºé”™è¯¯
    raise ValueError(f"Unsupported model: {model_name}")
    # # é»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–ç”¨æˆ·æŒ‡å®šçš„é…ç½®
    # return {
    #     "base_url": base_url or os.environ.get("OPENAI_BASE_URL", None),
    #     "api_key": api_key or os.environ.get("OPENAI_API_KEY", None),
    #     "model_name": model_name
    # }


def create_llm_client(model_name: str, api_key: Optional[str] = None, base_url: Optional[str] = None):
    """
    æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºå¯¹åº”çš„ OpenAI å®¢æˆ·ç«¯
    """
    if OpenAI is None:
        raise ImportError("è¯·å…ˆå®‰è£… openaiï¼špip install openai")
    
    config = get_model_config(model_name, api_key, base_url)
    
    client_kwargs = {}
    if config["api_key"]:
        client_kwargs["api_key"] = config["api_key"]
    if config["base_url"]:
        client_kwargs["base_url"] = config["base_url"]
    
    # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé¿å… Connection errorï¼ˆé»˜è®¤ 120 ç§’ï¼‰
    # timeout å‚æ•°æ ¼å¼: (connect_timeout, read_timeout)
    client_kwargs["timeout"] = 120.0  # æ€»è¶…æ—¶æ—¶é—´ 120 ç§’
    
    return OpenAI(**client_kwargs), config["model_name"]


# ========== LLMæ¨èæ ¸å¿ƒæ¨¡å— ==========

def build_candidate_text(candidates: List[Dict], max_candidates: int = None, ecosystem: str = None) -> Tuple[List[str], str]:
    """
    æ„å»ºå€™é€‰åˆ¶å“çš„æ–‡æœ¬æè¿°
    è¿”å›: (å€™é€‰åˆ¶å“åç§°åˆ—è¡¨, æ ¼å¼åŒ–çš„å€™é€‰åˆ¶å“æ–‡æœ¬)
    """
    candidate_names = []
    candidate_texts = []
    
    candidates_to_use = candidates[:max_candidates] if max_candidates else candidates
    
    for idx, item in enumerate(candidates_to_use, 1):
        name = item.get("name", "").strip()
        description = item.get("description", "").strip()
        
        candidate_names.append(name)
        
        # æ„å»ºå€™é€‰åˆ¶å“çš„æè¿°æ–‡æœ¬ï¼ˆåªåŒ…å«Nameå’ŒDescriptionï¼‰
        text = f"{idx}. Name: {name}\n"
        if description:
            # å¯¹äºhfç”Ÿæ€ï¼Œé™åˆ¶descriptioné•¿åº¦ä¸º1000å­—ç¬¦ï¼ˆå‚è€ƒtree_structures.pyï¼‰
            if ecosystem == "hf":
                max_desc_len = 1000
                desc = description[:max_desc_len] if len(description) > max_desc_len else description
                if len(description) > max_desc_len:
                    desc += " ... (truncated)"
            else:
                # å…¶ä»–ç”Ÿæ€ä½¿ç”¨å…¨éƒ¨descriptionï¼Œä¸æˆªæ–­
                desc = description
            text += f"   Description: {desc}\n"
        candidate_texts.append(text)
    
    formatted_text = "\n".join(candidate_texts)
    return candidate_names, formatted_text


def build_scoring_prompt(intent: str, artifact_name: str, artifact_type: str, 
                        artifact_description: str, ecosystem: str) -> str:
    """æ„å»ºå•ä¸ªåˆ¶å“ç›¸å…³æ€§æ‰“åˆ†çš„æç¤ºè¯"""
    ecosystem_names = {
        "hf": "Hugging Face",
        "js": "npm/JavaScript",
        "linux": "Linux"
    }
    ecosystem_name = ecosystem_names.get(ecosystem, ecosystem)
    
    # å¯¹äºhfç”Ÿæ€ï¼Œé™åˆ¶descriptioné•¿åº¦ä¸º1000å­—ç¬¦ï¼ˆå‚è€ƒtree_structures.pyï¼‰
    if ecosystem == "hf" and artifact_description:
        max_desc_len = 1000
        desc_text = artifact_description[:max_desc_len]
        if len(artifact_description) > max_desc_len:
            desc_text += " ... (truncated)"
    else:
        # å…¶ä»–ç”Ÿæ€ä½¿ç”¨å…¨éƒ¨descriptionï¼Œä¸æˆªæ–­
        desc_text = artifact_description if artifact_description else "No description"
    
    prompt = f"""You are an expert in {ecosystem_name} ecosystem artifact recommendation. Please evaluate the semantic relevance between the following artifact and the user intent.

User Intent:
{intent}

Artifact Information:
- Name: {artifact_name}
- Description: {desc_text}

Please provide a semantic relevance score (0-100 integer, where 100 indicates a perfect match and 0 indicates complete irrelevance) between this artifact and the user intent.
Return only a number, without any additional text."""
    
    return prompt


def build_batch_scoring_prompt(intent: str, artifacts: List[Dict], ecosystem: str) -> str:
    """æ„å»ºæ‰¹é‡åˆ¶å“ç›¸å…³æ€§æ‰“åˆ†çš„æç¤ºè¯"""
    ecosystem_names = {
        "hf": "Hugging Face",
        "js": "npm/JavaScript",
        "linux": "Linux"
    }
    ecosystem_name = ecosystem_names.get(ecosystem, ecosystem)
    
    artifacts_text = []
    for idx, artifact in enumerate(artifacts, 1):
        name = artifact.get("name", "").strip()
        artifact_type = artifact.get("type", "").strip()
        description = artifact.get("description", "").strip()
        
        # å¯¹äºhfç”Ÿæ€ï¼Œé™åˆ¶descriptioné•¿åº¦ä¸º1000å­—ç¬¦
        if ecosystem == "hf" and description:
            max_desc_len = 500
            desc_text = description[:max_desc_len]
            if len(description) > max_desc_len:
                desc_text += " ... (truncated)"
        else:
            desc_text = description if description else "No description"
        
        artifacts_text.append(f"{idx}. Name: {name}\n   Description: {desc_text}")
    
    artifacts_list = "\n\n".join(artifacts_text)
    
    prompt = f"""You are an expert in {ecosystem_name} ecosystem artifact recommendation. Please evaluate the semantic relevance between each of the following artifacts and the user intent.

User Intent:
{intent}

Artifact List:
{artifacts_list}

Please provide a semantic relevance score (0-100 integer) for each artifact, where 100 indicates a perfect match and 0 indicates complete irrelevance.
Return the scores in the following format (one score per line, in the same order as the artifacts):
score1
score2
score3
...

Return only the numbers, one per line, without any additional text."""
    
    return prompt


def build_llm_prompt(intent: str, candidate_text: str, ecosystem: str, top_k: int) -> str:
    """æ„å»ºLLMæ¨èæç¤ºè¯"""
    ecosystem_names = {
        "hf": "Hugging Face",
        "js": "npm/JavaScript",
        "linux": "Linux"
    }
    ecosystem_name = ecosystem_names.get(ecosystem, ecosystem)
    
    prompt = f"""You are an expert in {ecosystem_name} ecosystem artifact recommendation. Based on the user's intent, recommend the most relevant artifacts from the given candidate list.

User Intent:
{intent}

Candidate Artifact List:
{candidate_text}

Please select the top {top_k} most relevant artifacts from the above candidate list based on the user intent, ranked from highest to lowest relevance.
Return only the artifact names (the Name field), one per line, without any numbering, prefixes, or additional text.
Example format:
artifact_name1
artifact_name2
artifact_name3

Please directly output the recommended artifact names:"""
    
    return prompt


def _call_llm_api(client: OpenAI, model_name: str, prompt: str, system_content: str = None, max_tokens: int = 2000):
    """è°ƒç”¨LLM APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    max_retries = 10  # å¢åŠ é‡è¯•æ¬¡æ•°ï¼Œç‰¹åˆ«æ˜¯å¯¹äº Connection error
    default_system = "You are a professional artifact recommendation assistant capable of accurately understanding user intents and recommending the most relevant artifacts."
    system_content = system_content or default_system
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            error_str = str(e).lower()
            is_connection_error = any(keyword in error_str for keyword in [
                "connection", "timeout", "network", "connect", "refused", 
                "unreachable", "reset", "broken pipe"
            ])
            
            if attempt < max_retries - 1:
                # å¯¹äºè¿æ¥é”™è¯¯ï¼Œä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                if is_connection_error:
                    wait_time = min(5 * (attempt + 1), 60)  # è¿æ¥é”™è¯¯ï¼š5ç§’èµ·æ­¥ï¼Œæœ€å¤š60ç§’
                    print(f"âš ï¸ è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}, ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                else:
                    wait_time = min(2 ** attempt, 20)  # å…¶ä»–é”™è¯¯ï¼šæŒ‡æ•°é€€é¿ï¼Œæœ€å¤š20ç§’
                    print(f"âš ï¸ APIè°ƒç”¨é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}, ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                
                time.sleep(wait_time)
                continue
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
                print(f"âŒ APIè°ƒç”¨æœ€ç»ˆå¤±è´¥: {type(e).__name__}: {e}")
                raise e


def score_artifact_relevance(intent: str, artifact: Dict, ecosystem: str,
                            client: OpenAI, model_name: str) -> float:
    """
    ä½¿ç”¨LLMå¯¹å•ä¸ªåˆ¶å“è¿›è¡Œè¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†
    
    In the scoring stage, each artifact is individually paired with the intent 
    and evaluated by the LLM, which assigns a semantic relevance score on a 0â€“100 scale
    â€”where 0 indicates complete irrelevance and 100 denotes a perfect match.
    
    è¿”å›: è¯­ä¹‰ç›¸å…³æ€§åˆ†æ•° (0-100)
    """
    name = artifact.get("name", "").strip()
    artifact_type = artifact.get("type", "").strip()
    description = artifact.get("description", "").strip()
    
    prompt = build_scoring_prompt(intent, name, artifact_type, description, ecosystem)
    
    try:
        content = _call_llm_api(client, model_name, prompt, 
                                system_content="You are a professional artifact semantic relevance evaluation assistant capable of accurately assessing the semantic relevance between artifacts and user intents.",
                                max_tokens=50)
        
        # è§£æåˆ†æ•°ï¼ˆå°è¯•æå–æ•°å­—ï¼‰
        numbers = re.findall(r'\d+', content)
        if numbers:
            score = float(numbers[0])
            # ç¡®ä¿åˆ†æ•°åœ¨0-100èŒƒå›´å†…
            score = max(0, min(100, score))
            return score
        else:
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é»˜è®¤åˆ†æ•°
            return 50.0  # 0-100åŒºé—´çš„ä¸­ä½æ•°
    except Exception as e:
        print(f"âš ï¸ æ‰“åˆ†å‡ºé”™ ({name}): {e}")
        return 0.0  # å‡ºé”™æ—¶è¿”å›0åˆ†


def score_artifacts_batch(intent: str, artifacts: List[Dict], ecosystem: str,
                         client: OpenAI, model_name: str) -> List[float]:
    """
    ä½¿ç”¨LLMå¯¹ä¸€æ‰¹åˆ¶å“è¿›è¡Œæ‰¹é‡è¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    
    è¿”å›: è¯­ä¹‰ç›¸å…³æ€§åˆ†æ•°åˆ—è¡¨ (0-100)
    """
    if not artifacts:
        return []
    
    prompt = build_batch_scoring_prompt(intent, artifacts, ecosystem)
    
    try:
        # æ ¹æ®æ‰¹é‡å¤§å°è°ƒæ•´max_tokensï¼ˆæ¯ä¸ªåˆ¶å“çº¦50 tokensï¼‰
        max_tokens = max(200, len(artifacts) * 50)
        content = _call_llm_api(client, model_name, prompt, 
                                system_content="You are a professional artifact semantic relevance evaluation assistant capable of accurately assessing the semantic relevance between artifacts and user intents.",
                                max_tokens=max_tokens)
        
        # è§£æåˆ†æ•°ï¼ˆæŒ‰è¡Œæå–æ•°å­—ï¼‰
        scores = []
        lines = content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # æå–ç¬¬ä¸€ä¸ªæ•°å­—
            numbers = re.findall(r'\d+', line)
            if numbers:
                score = float(numbers[0])
                score = max(0, min(100, score))  # ç¡®ä¿åœ¨0-100èŒƒå›´å†…
                scores.append(score)
        
        # å¦‚æœè§£æå‡ºçš„åˆ†æ•°æ•°é‡ä¸è¶³ï¼Œç”¨é»˜è®¤åˆ†æ•°å¡«å……
        while len(scores) < len(artifacts):
            scores.append(50.0)  # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºé»˜è®¤åˆ†æ•°
        
        # åªè¿”å›å‰len(artifacts)ä¸ªåˆ†æ•°
        return scores[:len(artifacts)]
        
    except Exception as e:
        error_str = str(e).lower()
        is_connection_error = any(keyword in error_str for keyword in [
            "connection", "timeout", "network", "connect", "refused", 
            "unreachable", "reset", "broken pipe"
        ])
        
        if is_connection_error:
            print(f"âš ï¸ æ‰¹é‡æ‰“åˆ†è¿æ¥é”™è¯¯: {type(e).__name__}: {e}")
            print(f"   è¿”å›é»˜è®¤åˆ†æ•°åˆ—è¡¨ ({len(artifacts)} ä¸ªåˆ¶å“)")
        else:
            print(f"âš ï¸ æ‰¹é‡æ‰“åˆ†å‡ºé”™: {type(e).__name__}: {e}")
        
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›é»˜è®¤åˆ†æ•°åˆ—è¡¨
        return [50.0] * len(artifacts)


def filter_candidates_by_scoring(intent: str, candidates: List[Dict], ecosystem: str,
                                 client: OpenAI, model_name: str, 
                                 top_percent: float = 0.1, batch_size: int = 20) -> List[Dict]:
    """
    ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨LLMå¯¹æ¯ä¸ªåˆ¶å“æ‰“åˆ†ï¼Œç­›é€‰å‡ºtopç™¾åˆ†æ¯”çš„æœ€ç›¸å…³å€™é€‰åˆ¶å“
    
    In the scoring stage, each artifact is individually paired with the intent 
    and evaluated by the LLM, which assigns a semantic relevance score on a 0â€“100 scale.
    This stage produces a numeric relevance distribution over all artifacts.
    
    Args:
        intent: ç”¨æˆ·æ„å›¾
        candidates: æ‰€æœ‰å€™é€‰åˆ¶å“åˆ—è¡¨
        ecosystem: ç”Ÿæ€ç³»ç»Ÿåç§°
        client: LLMå®¢æˆ·ç«¯
        model_name: æ¨¡å‹åç§°
        top_percent: ç­›é€‰å‡ºçš„å€™é€‰åˆ¶å“ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤0.1ï¼Œå³top 10%ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤20ï¼Œå³æ¯æ¬¡æ‰¹é‡æ‰“åˆ†20ä¸ªåˆ¶å“ï¼‰
    
    Returns:
        ç­›é€‰åçš„å€™é€‰åˆ¶å“åˆ—è¡¨ï¼ˆæŒ‰åˆ†æ•°é™åºæ’åˆ—ï¼‰
    """
    print(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼ˆScoring Stageï¼‰ï¼šå¯¹ {len(candidates)} ä¸ªå€™é€‰åˆ¶å“è¿›è¡Œè¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†...")
    print(f"   å°†é€‰æ‹© top {top_percent*100:.1f}% çš„å€™é€‰åˆ¶å“è¿›å…¥æ¨èé˜¶æ®µ")
    print(f"   ä½¿ç”¨æ‰¹é‡æ‰“åˆ†æ¨¡å¼ï¼Œæ‰¹é‡å¤§å°: {batch_size}")
    
    scored_candidates = []
    total = len(candidates)
    
    # å°†å€™é€‰åˆ¶å“åˆ†æ‰¹å¤„ç†
    num_batches = (total + batch_size - 1) // batch_size
    
    # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ‰“åˆ†è¿›åº¦
    if TQDM_AVAILABLE:
        batch_iter = tqdm(range(num_batches), desc="Scoring batches", unit="batch", 
                          total=num_batches, ncols=100)
    else:
        batch_iter = range(num_batches)
    
    for batch_idx in batch_iter:
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total)
        batch_candidates = candidates[start_idx:end_idx]
        
        # æ‰¹é‡æ‰“åˆ†
        batch_scores = score_artifacts_batch(intent, batch_candidates, ecosystem, client, model_name)
        
        # å°†åˆ†æ•°å’Œå€™é€‰åˆ¶å“é…å¯¹
        for score, artifact in zip(batch_scores, batch_candidates):
            scored_candidates.append((score, artifact))
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™æµï¼ˆå¯¹äº qwen æ¨¡å‹ï¼Œä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿä»¥é¿å…è¿æ¥é”™è¯¯ï¼‰
        if batch_idx < num_batches - 1:  # æœ€åä¸€æ‰¹ä¸éœ€è¦å»¶è¿Ÿ
            # æ£€æŸ¥æ˜¯å¦æ˜¯ qwen æ¨¡å‹ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
            model_lower = model_name.lower()
            if "qwen" in model_lower:
                time.sleep(1.0)  # qwen æ¨¡å‹å»¶è¿Ÿ 1 ç§’
            else:
                time.sleep(0.1)  # å…¶ä»–æ¨¡å‹å»¶è¿Ÿ 0.1 ç§’
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    scored_candidates.sort(key=lambda x: x[0], reverse=True)
    
    # è®¡ç®—è¦é€‰æ‹©çš„å€™é€‰æ•°é‡ï¼ˆè‡³å°‘é€‰æ‹©1ä¸ªï¼‰
    top_n = max(1, int(len(candidates) * top_percent))
    
    # è¿”å›topç™¾åˆ†æ¯”ä¸ªå€™é€‰åˆ¶å“
    top_candidates = [artifact for score, artifact in scored_candidates[:top_n]]
    top_scores = [score for score, artifact in scored_candidates[:top_n]]
    
    print(f"âœ… ç­›é€‰å®Œæˆï¼Œé€‰å‡º {len(top_candidates)} ä¸ªå€™é€‰åˆ¶å“ï¼ˆåˆ†æ•°èŒƒå›´: {min(top_scores):.1f} - {max(top_scores):.1f}ï¼‰")
    
    return top_candidates


def get_llm_recommendations(intent: str, candidates: List[Dict], ecosystem: str, 
                            top_k: int, client: OpenAI, model_name: str,
                            max_candidates: int = None, use_two_stage: bool = True,
                            filter_top_percent: float = 0.1, scoring_batch_size: int = 20) -> List[str]:
    """
    ä½¿ç”¨LLMè·å–æ¨èç»“æœï¼ˆä¸¤é˜¶æ®µç­–ç•¥ï¼‰
    
    In the scoring stage, each artifact is individually paired with the intent 
    and evaluated by the LLM, which assigns a semantic relevance score on a 0â€“100 scale.
    This stage produces a numeric relevance distribution over all artifacts.
    
    In the recommendation stage, we select the top-scored subset (typically the top 10%) 
    as candidates and prompt the LLM again to generate the final top-k recommendations 
    through comparative reasoning over the selected subset.
    
    Args:
        intent: ç”¨æˆ·æ„å›¾
        candidates: å€™é€‰åˆ¶å“åˆ—è¡¨
        ecosystem: ç”Ÿæ€ç³»ç»Ÿåç§°
        top_k: æœ€ç»ˆè¿”å›çš„æ¨èæ•°é‡
        client: LLMå®¢æˆ·ç«¯
        model_name: æ¨¡å‹åç§°
        max_candidates: æœ€å¤§å€™é€‰æ•°é‡ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹ï¼‰
        use_two_stage: æ˜¯å¦ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼ˆå…ˆæ‰“åˆ†ç­›é€‰ï¼Œå†æ¨èï¼‰
        filter_top_percent: ç¬¬ä¸€é˜¶æ®µç­›é€‰å‡ºçš„å€™é€‰ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤0.1ï¼Œå³top 10%ï¼‰
        scoring_batch_size: æ‰¹é‡æ‰“åˆ†çš„å¤§å°ï¼ˆé»˜è®¤20ï¼‰
    
    Returns:
        æ¨èçš„åˆ¶å“åç§°åˆ—è¡¨ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
    """
    # å¦‚æœå€™é€‰æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥ä½¿ç”¨å•é˜¶æ®µæ¨è
    min_candidates_for_two_stage = max(10, int(1 / filter_top_percent))  # è‡³å°‘éœ€è¦èƒ½é€‰å‡º1ä¸ªå€™é€‰
    if not use_two_stage or len(candidates) <= min_candidates_for_two_stage:
        return _get_llm_recommendations_single_stage(intent, candidates, ecosystem, 
                                                   top_k, client, model_name)
    
    # ä¸¤é˜¶æ®µç­–ç•¥
    # ç¬¬ä¸€é˜¶æ®µï¼šæ‰“åˆ†ç­›é€‰ï¼ˆScoring Stageï¼‰
    filtered_candidates = filter_candidates_by_scoring(
        intent, candidates, ecosystem, client, model_name, 
        top_percent=filter_top_percent, batch_size=scoring_batch_size
    )
    
    # ç¬¬äºŒé˜¶æ®µï¼šåœ¨ç­›é€‰å‡ºçš„å€™é€‰é›†ä¸­è¿›è¡Œæœ€ç»ˆæ¨èï¼ˆRecommendation Stageï¼‰
    print(f"ğŸ¯ ç¬¬äºŒé˜¶æ®µï¼ˆRecommendation Stageï¼‰ï¼šåœ¨ {len(filtered_candidates)} ä¸ªå€™é€‰åˆ¶å“ä¸­è¿›è¡Œæ¯”è¾ƒæ¨ç†ï¼Œç”Ÿæˆæœ€ç»ˆ top-{top_k} æ¨è...")
    return _get_llm_recommendations_single_stage(intent, filtered_candidates, ecosystem,
                                                top_k, client, model_name)


def _get_llm_recommendations_single_stage(intent: str, candidates: List[Dict], ecosystem: str,
                                          top_k: int, client: OpenAI, model_name: str) -> List[str]:
    """
    å•é˜¶æ®µæ¨èï¼šç›´æ¥åœ¨å€™é€‰é›†ä¸­è¿›è¡Œæ¨è
    é€šè¿‡æ¯”è¾ƒæ¨ç†ï¼ˆcomparative reasoningï¼‰ç”Ÿæˆæœ€ç»ˆçš„top-kæ¨è
    """
    candidate_names, candidate_text = build_candidate_text(candidates, max_candidates=None, ecosystem=ecosystem)
    prompt = build_llm_prompt(intent, candidate_text, ecosystem, top_k)
    
    try:
        content = _call_llm_api(client, model_name, prompt)
        recommended_names = []
        
        # æŒ‰è¡Œè§£æï¼Œæå–åˆ¶å“åç§°
        for line in content.split('\n'):
            line = line.strip()
            if not line:
                continue
            # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€ï¼ˆå¦‚ "1. ", "- ", "* " ç­‰ï¼‰
            line = line.lstrip('0123456789.-*()[] ').strip()
            if line and line in candidate_names:
                recommended_names.append(line)
        
        # å¦‚æœè§£æå‡ºçš„æ¨èæ•°é‡ä¸è¶³ï¼Œç”¨å‰©ä½™çš„å€™é€‰åˆ¶å“å¡«å……
        if len(recommended_names) < top_k:
            remaining = [name for name in candidate_names if name not in recommended_names]
            recommended_names.extend(remaining[:top_k - len(recommended_names)])
        
        # ç¡®ä¿è¿”å›top_kä¸ªç»“æœ
        return recommended_names[:top_k]
        
    except Exception as e:
        print(f"âš ï¸ LLMè°ƒç”¨å‡ºé”™: {e}")
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›å‰top_kä¸ªå€™é€‰åˆ¶å“ä½œä¸ºfallback
        return candidate_names[:top_k]


def evaluate_recommendations(top_names: List[List[str]], candidate_names: List[str],
                             intents: List[str], gold_labels: List[str],
                             p_ks: List[int], dcg_ks: List[int]):
    """è®¡ç®— P@K å’Œ DCG@K"""
    metrics = {f"P@{k}": 0.0 for k in p_ks}
    dcg_metrics = {f"DCG@{k}": 0.0 for k in dcg_ks}
    recommendations = []

    for qi, ranked_names in enumerate(top_names):
        rec_entry = {
            "intent": intents[qi],
            "gold": gold_labels[qi],
            "ranking": ranked_names[: max(max(p_ks), max(dcg_ks))],
        }
        recommendations.append(rec_entry)

        for k in p_ks:
            metrics[f"P@{k}"] += precision_at_k(ranked_names, gold_labels[qi], k)
        for k in dcg_ks:
            dcg_metrics[f"DCG@{k}"] += dcg_at_k(ranked_names, gold_labels[qi], k)

    num_q = len(intents)
    for k in p_ks:
        metrics[f"P@{k}"] /= num_q
    for k in dcg_ks:
        dcg_metrics[f"DCG@{k}"] /= num_q

    return metrics, dcg_metrics, recommendations


# ========== ä¸»å…¥å£ ==========

def run_llm_recommendation(data_dir: str, ecosystem: str, output_dir: str,
                           model_name: str, top_k: int, p_ks: List[int], 
                           dcg_ks: List[int], max_candidates: int = None,
                           api_key: str = None, base_url: str = None,
                           use_two_stage: bool = True, filter_top_percent: float = 0.1,
                           scoring_batch_size: int = 20):
    """
    ä½¿ç”¨LLMè¿›è¡Œæ„å›¾éœ€æ±‚åˆ¶å“æ¨è
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        ecosystem: ç”Ÿæ€ç³»ç»Ÿåç§° (hf/js/linux)
        output_dir: è¾“å‡ºç›®å½•
        model_name: LLMæ¨¡å‹åç§°
        top_k: è¿”å›çš„æ¨èæ•°é‡
        p_ks: P@Kçš„kå€¼åˆ—è¡¨
        dcg_ks: DCG@Kçš„kå€¼åˆ—è¡¨
        max_candidates: æœ€å¤§å€™é€‰åˆ¶å“æ•°é‡ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹ï¼‰
        api_key: OpenAI APIå¯†é’¥
        base_url: OpenAI APIåŸºç¡€URL
        use_two_stage: æ˜¯å¦ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼ˆå…ˆæ‰“åˆ†ç­›é€‰ï¼Œå†æ¨èï¼‰
        filter_top_percent: ç¬¬ä¸€é˜¶æ®µç­›é€‰å‡ºçš„å€™é€‰ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤0.1ï¼Œå³top 10%ï¼‰
        scoring_batch_size: æ‰¹é‡æ‰“åˆ†çš„å¤§å°ï¼ˆé»˜è®¤20ï¼Œå³æ¯æ¬¡æ‰¹é‡æ‰“åˆ†20ä¸ªåˆ¶å“ï¼‰
    """
    if OpenAI is None:
        raise ImportError("è¯·å…ˆå®‰è£… openaiï¼špip install openai")
    
    start_time = time.time()
    print(f"â³ Starting LLM recommendation for [{ecosystem}] using [{model_name}]...")
    
    # æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨åˆ›å»ºå¯¹åº”çš„å®¢æˆ·ç«¯
    client, actual_model_name = create_llm_client(model_name, api_key, base_url)
    config = get_model_config(model_name, api_key, base_url)
    api_info = config.get("base_url", "default")
    print(f"ğŸ“¡ ä½¿ç”¨ API: {api_info}")
    
    # åŠ è½½æ•°æ®
    data_path = os.path.join(data_dir, ecosystem)
    dataset = load_json(os.path.join(data_path, "dataset.json"))
    candidates = load_json(os.path.join(data_path, "candidate_artifacts.json"))
    
    intents = [row["intent"].strip() for row in dataset]
    gold_labels = [row["artifact"].strip() for row in dataset]
    
    # è·å–æ¨èç»“æœ
    print(f"ğŸš€ å¼€å§‹ä¸º {len(intents)} ä¸ªæ„å›¾ç”Ÿæˆæ¨è...")
    if use_two_stage:
        print(f"ğŸ“‹ ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼š")
        print(f"   - ç¬¬ä¸€é˜¶æ®µï¼ˆScoringï¼‰ï¼šå¯¹æ¯ä¸ªåˆ¶å“æ‰“åˆ†ï¼ˆ0-100ï¼‰ï¼Œäº§ç”Ÿè¯­ä¹‰ç›¸å…³æ€§åˆ†å¸ƒ")
        print(f"   - ç¬¬äºŒé˜¶æ®µï¼ˆRecommendationï¼‰ï¼šé€‰æ‹©top {filter_top_percent*100:.1f}%å€™é€‰ï¼Œé€šè¿‡æ¯”è¾ƒæ¨ç†ç”Ÿæˆæœ€ç»ˆæ¨è")
    else:
        print(f"ğŸ“‹ ä½¿ç”¨å•é˜¶æ®µç­–ç•¥ï¼šç›´æ¥åœ¨å…¨éƒ¨å€™é€‰åˆ¶å“ä¸­æ¨è")
    
    all_rankings = []
    query_times: List[float] = []
    intents = intents[:10]
    for idx, intent in enumerate(intents, 1):
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ„å›¾ {idx}/{len(intents)}")
        print(f"{'='*60}")
        
        q_start = time.perf_counter()
        recommended_names = get_llm_recommendations(
            intent, candidates, ecosystem, top_k, client, actual_model_name,
            max_candidates=None, use_two_stage=use_two_stage, filter_top_percent=filter_top_percent,
            scoring_batch_size=scoring_batch_size
        )
        all_rankings.append(recommended_names)
        query_times.append(time.perf_counter() - q_start)
        
        # æ·»åŠ å°å»¶è¿Ÿä»¥é¿å…APIé™æµ
        time.sleep(0.2)
    
    # è¯„ä¼°ç»“æœ
    candidate_names = [c["name"] for c in candidates if "name" in c]
    metrics, dcg_metrics, recommendations = evaluate_recommendations(
        all_rankings, candidate_names, intents, gold_labels, p_ks, dcg_ks
    )
    
    end_time = time.time()
    elapsed_time = round(end_time - start_time, 2)
    query_time_mean = round(mean(query_times), 4) if query_times else 0.0
    query_time_std = round(pstdev(query_times), 4) if len(query_times) > 1 else 0.0
    query_time_min = round(min(query_times), 4) if query_times else 0.0
    query_time_max = round(max(query_times), 4) if query_times else 0.0
    
    # ä¿å­˜ç»“æœ
    output_dir = os.path.join(output_dir, "LLM")
    os.makedirs(output_dir, exist_ok=True)
    model_dir_name = model_name.replace("/", "_").replace("-", "_")
    output_dir = os.path.join(output_dir, model_dir_name)
    os.makedirs(output_dir, exist_ok=True)
    
    metrics_path = os.path.join(output_dir, f"{ecosystem}-metrics.json")
    rec_path = os.path.join(output_dir, f"{ecosystem}-recommendations.json")
    runtime_summary_path = os.path.join(output_dir, f"{ecosystem}-runtime-summary.txt")
    
    save_json({
        "precision": metrics,
        "dcg": dcg_metrics,
        "runtime_seconds": elapsed_time,
        "query_time_stats": {
            "mean": query_time_mean,
            "std": query_time_std,
            "min": query_time_min,
            "max": query_time_max
        }
    }, metrics_path)
    
    save_json(recommendations, rec_path)
    
    with open(runtime_summary_path, "a", encoding="utf-8") as f:
        f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} | {ecosystem:<8} | "
                f"Total: {elapsed_time:>8.2f}s | "
                f"Mean: {query_time_mean:>8.4f}s | "
                f"Std: {query_time_std:>8.4f}s | "
                f"Min: {query_time_min:>8.4f}s | "
                f"Max: {query_time_max:>8.4f}s\n")
    
    print("âœ… Results:")
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}")
    for k, v in dcg_metrics.items():
        print(f"{k}: {v:.4f}")
    print(f"â±ï¸ Runtime: {elapsed_time:.2f} seconds")
    print("Query time stats (seconds):")
    print(f"  Mean: {query_time_mean:.4f}")
    print(f"  Std : {query_time_std:.4f}")
    print(f"  Min : {query_time_min:.4f}")
    print(f"  Max : {query_time_max:.4f}")
    print(f"ğŸ“ Saved to: {metrics_path}\n")


def get_parser():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ„å›¾éœ€æ±‚åˆ¶å“æ¨è")
    parser.add_argument("--data_dir", type=str, default="IntentRecBench/data",
                        help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--ecosystem", type=str, default="hf", 
                        choices=["hf", "js", "linux"],
                        help="ç”Ÿæ€ç³»ç»Ÿåç§°")
    parser.add_argument("--output_dir", type=str, default="output/baselines",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model_name", type=str, default="gpt-4o",
                        help="LLMæ¨¡å‹åç§° (æ”¯æŒ: gpt-*, qwen/*, deepseek/*, llama/*)")
    parser.add_argument("--top_k", type=int, default=5,
                        help="è¿”å›çš„æ¨èæ•°é‡")
    parser.add_argument("--p_k", type=int, nargs="+", default=[1, 2, 3, 4],
                        help="P@Kçš„kå€¼åˆ—è¡¨")
    parser.add_argument("--dcg_k", type=int, nargs="+", default=[2, 3, 4, 5],
                        help="DCG@Kçš„kå€¼åˆ—è¡¨")
    parser.add_argument("--max_candidates", type=int, default=None,
                        help="æœ€å¤§å€™é€‰åˆ¶å“æ•°é‡ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹ï¼‰")
    parser.add_argument("--api_key", type=str, default=None,
                        help="OpenAI APIå¯†é’¥ï¼ˆå¦‚æœä¸è®¾ç½®ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡OPENAI_API_KEYï¼‰")
    parser.add_argument("--base_url", type=str, default=None,
                        help="OpenAI APIåŸºç¡€URLï¼ˆå¦‚æœä¸è®¾ç½®ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡OPENAI_BASE_URLï¼‰")
    parser.add_argument("--use_two_stage", action="store_true", default=True,
                        help="ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼šå…ˆå¯¹æ¯ä¸ªåˆ¶å“æ‰“åˆ†ç­›é€‰ï¼Œå†è¿›è¡Œæœ€ç»ˆæ¨èï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    parser.add_argument("--no_two_stage", dest="use_two_stage", action="store_false",
                        help="ç¦ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œç›´æ¥ä½¿ç”¨å•é˜¶æ®µæ¨è")
    parser.add_argument("--filter_top_percent", type=float, default=0.1,
                        help="ç¬¬ä¸€é˜¶æ®µç­›é€‰å‡ºçš„å€™é€‰åˆ¶å“ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤0.1ï¼Œå³top 10%ï¼‰")
    parser.add_argument("--scoring_batch_size", type=int, default=20,
                        help="æ‰¹é‡æ‰“åˆ†çš„å¤§å°ï¼ˆé»˜è®¤20ï¼Œå³æ¯æ¬¡æ‰¹é‡æ‰“åˆ†20ä¸ªåˆ¶å“ï¼Œå¯æ˜¾è‘—å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰")
    return parser


def main():
    args = get_parser().parse_args()
    
    # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è·å–APIé…ç½®

    run_llm_recommendation(
        data_dir=args.data_dir,
        ecosystem=args.ecosystem,
        output_dir=args.output_dir,
        model_name=args.model_name,
        top_k=args.top_k,
        p_ks=args.p_k,
        dcg_ks=args.dcg_k,
        max_candidates=args.max_candidates,
        api_key=args.api_key,
        base_url=args.base_url,
        use_two_stage=args.use_two_stage,
        filter_top_percent=args.filter_top_percent,
        scoring_batch_size=args.scoring_batch_size
    )


if __name__ == "__main__":
    main()
