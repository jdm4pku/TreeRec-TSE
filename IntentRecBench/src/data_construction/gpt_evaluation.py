# ä½¿ç”¨GPT-5è¯„ä¼°æ¯ä¸ªç”Ÿæ€çš„æ•°æ®é›†æ ·æœ¬å¯é æ€§ï¼Œå¹¶è®¡ç®—æ¥æ”¶ç‡
import json
import os
import time
from pathlib import Path
from openai import OpenAI
from tqdm import tqdm

# è¯„ä¼°promptæ¨¡æ¿
EVALUATION_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªæ•°æ®è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ ·æœ¬æ˜¯å¦å¯é ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. Intentï¼ˆæ„å›¾æè¿°ï¼‰æ˜¯å¦æ¸…æ™°ã€åˆç†ã€å®Œæ•´
2. Intentå’ŒArtifactï¼ˆå·¥ä»¶ï¼‰æ˜¯å¦åŒ¹é…
3. æ ·æœ¬æ˜¯å¦å…·æœ‰å®é™…æ„ä¹‰å’Œä»·å€¼
4. æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„é”™è¯¯ã€çŸ›ç›¾æˆ–ä¸åˆç†ä¹‹å¤„

è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ ·æœ¬ï¼Œç„¶ååªå›ç­”"å¯é "æˆ–"ä¸å¯é "ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

æ ·æœ¬ï¼š
Intent: {intent}
Artifact: {artifact}
Ecosystem: {ecosystem}

è¯„ä¼°ç»“æœï¼ˆåªå›ç­”"å¯é "æˆ–"ä¸å¯é "ï¼‰ï¼š
"""

def load_json(file_path):
    """åŠ è½½JSONæ–‡ä»¶"""
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)

def save_json(data, file_path):
    """ä¿å­˜JSONæ–‡ä»¶"""
    with open(file_path, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=4)

def gpt_evaluation(prompt, max_retries=5, retry_delay=1):
    """
    è°ƒç”¨GPT-5è¿›è¡Œæ ·æœ¬å¯é æ€§è¯„ä¼°
    
    Args:
        prompt: è¯„ä¼°prompt
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    Returns:
        str: è¯„ä¼°ç»“æœï¼ˆ"å¯é "æˆ–"ä¸å¯é "ï¼‰
    """
    # ä½¿ç”¨ä¸annotation.pyç›¸åŒçš„APIé…ç½®
    client = OpenAI(
        api_key=os.environ.get("OPENAI_API_KEY", ""),
        base_url=os.environ.get("OPENAI_BASE_URL", "http://66.206.9.230:4000/v1"),
    )
    
    model_name = "gpt-4o-2024-05-13"  # é»˜è®¤ä½¿ç”¨gpt-4oï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡æ”¹ä¸ºgpt-5
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=model_name,
                temperature=0  # ä½¿ç”¨0æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
            )
            result = response.choices[0].message.content.strip()
            
            # æ ‡å‡†åŒ–ç»“æœï¼šåªä¿ç•™"å¯é "æˆ–"ä¸å¯é "
            if "å¯é " in result and "ä¸å¯é " not in result:
                return "å¯é "
            elif "ä¸å¯é " in result:
                return "ä¸å¯é "
            else:
                # å¦‚æœç»“æœä¸æ˜ç¡®ï¼Œå°è¯•è§£æ
                result_lower = result.lower()
                if "reliable" in result_lower or "yes" in result_lower or "true" in result_lower:
                    return "å¯é "
                else:
                    return "ä¸å¯é "
                    
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"  é”™è¯¯: {e}, é‡è¯•ä¸­... ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
            else:
                print(f"  é”™è¯¯: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›'ä¸å¯é '")
                return "ä¸å¯é "
    
    return "ä¸å¯é "

def evaluate_ecosystem(ecosystem_name, dataset_path, output_dir=None, skip_if_exists=True):
    """
    è¯„ä¼°å•ä¸ªç”Ÿæ€çš„æ•°æ®é›†
    
    Args:
        ecosystem_name: ç”Ÿæ€åç§°
        dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœï¼‰
        skip_if_exists: å¦‚æœç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦è·³è¿‡è¯„ä¼°ç›´æ¥è¯»å–ï¼ˆé»˜è®¤Trueï¼‰
    
    Returns:
        dict: åŒ…å«è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨å¤„ç†ç”Ÿæ€: {ecosystem_name}")
    print(f"{'='*60}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
    if output_dir and skip_if_exists:
        result_file = os.path.join(output_dir, f"{ecosystem_name}_evaluation_results.json")
        if os.path.exists(result_file):
            print(f"âœ… å‘ç°å·²æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶: {result_file}")
            print("ğŸ“Š ç›´æ¥è¯»å–å¹¶ç»Ÿè®¡ç»“æœ...")
            
            try:
                evaluation_results = load_json(result_file)
                total_samples = len(evaluation_results)
                reliable_count = sum(1 for r in evaluation_results if r.get("reliable", False))
                unreliable_count = total_samples - reliable_count
                acceptance_rate = (reliable_count / total_samples * 100) if total_samples > 0 else 0
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                print(f"\n{'='*60}")
                print(f"ç”Ÿæ€ {ecosystem_name} è¯„ä¼°ç»“æœï¼ˆä»å·²æœ‰æ–‡ä»¶è¯»å–ï¼‰:")
                print(f"{'='*60}")
                print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
                print(f"å¯é æ ·æœ¬æ•°: {reliable_count}")
                print(f"ä¸å¯é æ ·æœ¬æ•°: {unreliable_count}")
                print(f"æ¥æ”¶ç‡: {acceptance_rate:.2f}%")
                print(f"{'='*60}\n")
                
                return {
                    "ecosystem": ecosystem_name,
                    "total_samples": total_samples,
                    "reliable_samples": reliable_count,
                    "unreliable_samples": unreliable_count,
                    "acceptance_rate": acceptance_rate,
                    "evaluation_results": evaluation_results
                }
            except Exception as e:
                print(f"âš ï¸  è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                print("ğŸ”„ å°†é‡æ–°è¿›è¡Œè¯„ä¼°...")
    
    # å¦‚æœæ²¡æœ‰å·²æœ‰ç»“æœæˆ–è¯»å–å¤±è´¥ï¼Œè¿›è¡Œæ–°çš„è¯„ä¼°
    print(f"ğŸ”„ å¼€å§‹è¯„ä¼°ç”Ÿæ€: {ecosystem_name}")
    
    # åŠ è½½æ•°æ®é›†
    if not os.path.exists(dataset_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {dataset_path}")
        return None
    
    dataset = load_json(dataset_path)
    total_samples = len(dataset)
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    
    # è¯„ä¼°ç»“æœ
    evaluation_results = []
    reliable_count = 0
    
    # é€ä¸ªè¯„ä¼°æ ·æœ¬
    for idx, sample in enumerate(tqdm(dataset, desc=f"è¯„ä¼° {ecosystem_name}")):
        intent = sample.get("intent", "")
        artifact = sample.get("artifact", "")
        
        # æ„å»ºè¯„ä¼°prompt
        prompt = EVALUATION_PROMPT_TEMPLATE.format(
            intent=intent,
            artifact=artifact,
            ecosystem=ecosystem_name
        )
        
        # è°ƒç”¨GPTè¯„ä¼°
        result = gpt_evaluation(prompt)
        is_reliable = (result == "å¯é ")
        
        if is_reliable:
            reliable_count += 1
        
        # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
        evaluation_results.append({
            "intent": intent,
            "artifact": artifact,
            "reliable": is_reliable,
            "evaluation_result": result
        })
        
        # æ¯è¯„ä¼°100ä¸ªæ ·æœ¬ï¼Œæ‰“å°ä¸€æ¬¡è¿›åº¦
        if (idx + 1) % 100 == 0:
            current_rate = reliable_count / (idx + 1) * 100
            print(f"  è¿›åº¦: {idx + 1}/{total_samples}, å½“å‰æ¥æ”¶ç‡: {current_rate:.2f}%")
    
    # è®¡ç®—æ¥æ”¶ç‡
    acceptance_rate = (reliable_count / total_samples * 100) if total_samples > 0 else 0
    
    # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{ecosystem_name}_evaluation_results.json")
        save_json(evaluation_results, output_file)
        print(f"\nè¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ç”Ÿæ€ {ecosystem_name} è¯„ä¼°ç»“æœ:")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"å¯é æ ·æœ¬æ•°: {reliable_count}")
    print(f"ä¸å¯é æ ·æœ¬æ•°: {total_samples - reliable_count}")
    print(f"æ¥æ”¶ç‡: {acceptance_rate:.2f}%")
    print(f"{'='*60}\n")
    
    return {
        "ecosystem": ecosystem_name,
        "total_samples": total_samples,
        "reliable_samples": reliable_count,
        "unreliable_samples": total_samples - reliable_count,
        "acceptance_rate": acceptance_rate,
        "evaluation_results": evaluation_results
    }

def generate_summary_from_results(output_dir):
    """
    ä»å·²æœ‰çš„è¯„ä¼°ç»“æœæ–‡ä»¶ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    
    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆåŒ…å«è¯„ä¼°ç»“æœæ–‡ä»¶ï¼‰
    
    Returns:
        dict: æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    """
    output_path = Path(output_dir)
    if not output_path.exists():
        print(f"é”™è¯¯: è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return None
    
    all_results = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
    result_files = list(output_path.glob("*_evaluation_results.json"))
    
    if not result_files:
        print(f"è­¦å‘Š: åœ¨ {output_dir} ä¸­æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        return None
    
    print(f"æ‰¾åˆ° {len(result_files)} ä¸ªè¯„ä¼°ç»“æœæ–‡ä»¶")
    
    # è¯»å–æ¯ä¸ªç”Ÿæ€çš„è¯„ä¼°ç»“æœ
    for result_file in result_files:
        # ä»æ–‡ä»¶åæå–ç”Ÿæ€åç§°ï¼ˆä¾‹å¦‚ï¼šjs_evaluation_results.json -> jsï¼‰
        ecosystem_name = result_file.stem.replace("_evaluation_results", "")
        
        try:
            results = load_json(result_file)
            
            # ç»Ÿè®¡å¯é æ ·æœ¬æ•°
            total_samples = len(results)
            reliable_samples = sum(1 for r in results if r.get("reliable", False))
            unreliable_samples = total_samples - reliable_samples
            acceptance_rate = (reliable_samples / total_samples * 100) if total_samples > 0 else 0
            
            all_results[ecosystem_name] = {
                "total_samples": total_samples,
                "reliable_samples": reliable_samples,
                "unreliable_samples": unreliable_samples,
                "acceptance_rate": acceptance_rate
            }
            
            print(f"{ecosystem_name}: {total_samples} ä¸ªæ ·æœ¬, {reliable_samples} ä¸ªå¯é  ({acceptance_rate:.2f}%)")
            
        except Exception as e:
            print(f"é”™è¯¯: è¯»å– {result_file} æ—¶å‡ºé”™ - {e}")
            continue
    
    if not all_results:
        print("é”™è¯¯: æœªèƒ½è¯»å–ä»»ä½•è¯„ä¼°ç»“æœ")
        return None
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_samples_all = sum(r["total_samples"] for r in all_results.values())
    total_reliable_all = sum(r["reliable_samples"] for r in all_results.values())
    overall_acceptance_rate = (total_reliable_all / total_samples_all * 100) if total_samples_all > 0 else 0
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print("\n" + "="*60)
    print("æ€»ä½“è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*60)
    print(f"{'ç”Ÿæ€':<15} {'æ€»æ ·æœ¬æ•°':<12} {'å¯é æ ·æœ¬':<12} {'æ¥æ”¶ç‡':<10}")
    print("-"*60)
    
    for ecosystem_name, result in sorted(all_results.items()):
        print(f"{ecosystem_name:<15} {result['total_samples']:<12} {result['reliable_samples']:<12} {result['acceptance_rate']:<10.2f}%")
    
    print("-"*60)
    print(f"{'æ€»è®¡':<15} {total_samples_all:<12} {total_reliable_all:<12} {overall_acceptance_rate:<10.2f}%")
    print("="*60)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary = {
        "overall_statistics": {
            "total_samples": total_samples_all,
            "total_reliable_samples": total_reliable_all,
            "overall_acceptance_rate": overall_acceptance_rate
        },
        "ecosystem_statistics": {
            name: {
                "total_samples": r["total_samples"],
                "reliable_samples": r["reliable_samples"],
                "acceptance_rate": r["acceptance_rate"]
            }
            for name, r in all_results.items()
        }
    }
    summary_file = output_path / "evaluation_summary.json"
    save_json(summary, summary_file)
    print(f"\nè¯„ä¼°æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
    
    return summary

def evaluate_all_ecosystems(data_dir, output_dir=None):
    """
    è¯„ä¼°æ‰€æœ‰ç”Ÿæ€çš„æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆç”¨äºä¿å­˜è¯„ä¼°ç»“æœï¼‰
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰ç”Ÿæ€è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    data_path = Path(data_dir)
    all_results = {}
    
    # éå†æ•°æ®ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    ecosystem_dirs = [d for d in data_path.iterdir() if d.is_dir() and d.name != "baselines"]
    
    if not ecosystem_dirs:
        print(f"é”™è¯¯: åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°ç”Ÿæ€ç›®å½•")
        return None
    
    print(f"æ‰¾åˆ° {len(ecosystem_dirs)} ä¸ªç”Ÿæ€: {[d.name for d in ecosystem_dirs]}")
    
    # è¯„ä¼°æ¯ä¸ªç”Ÿæ€
    for ecosystem_dir in ecosystem_dirs:
        ecosystem_name = ecosystem_dir.name
        dataset_file = ecosystem_dir / "dataset.json"
        
        if dataset_file.exists():
            result = evaluate_ecosystem(ecosystem_name, str(dataset_file), output_dir)
            if result:
                all_results[ecosystem_name] = result
        else:
            print(f"è­¦å‘Š: {ecosystem_name} ç”Ÿæ€ä¸­æœªæ‰¾åˆ° dataset.json æ–‡ä»¶")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_samples_all = sum(r["total_samples"] for r in all_results.values())
    total_reliable_all = sum(r["reliable_samples"] for r in all_results.values())
    overall_acceptance_rate = (total_reliable_all / total_samples_all * 100) if total_samples_all > 0 else 0
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print("\n" + "="*60)
    print("æ€»ä½“è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*60)
    print(f"{'ç”Ÿæ€':<15} {'æ€»æ ·æœ¬æ•°':<12} {'å¯é æ ·æœ¬':<12} {'æ¥æ”¶ç‡':<10}")
    print("-"*60)
    
    for ecosystem_name, result in sorted(all_results.items()):
        print(f"{ecosystem_name:<15} {result['total_samples']:<12} {result['reliable_samples']:<12} {result['acceptance_rate']:<10.2f}%")
    
    print("-"*60)
    print(f"{'æ€»è®¡':<15} {total_samples_all:<12} {total_reliable_all:<12} {overall_acceptance_rate:<10.2f}%")
    print("="*60)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        summary = {
            "overall_statistics": {
                "total_samples": total_samples_all,
                "total_reliable_samples": total_reliable_all,
                "overall_acceptance_rate": overall_acceptance_rate
            },
            "ecosystem_statistics": {
                name: {
                    "total_samples": r["total_samples"],
                    "reliable_samples": r["reliable_samples"],
                    "acceptance_rate": r["acceptance_rate"]
                }
                for name, r in all_results.items()
            }
        }
        summary_file = os.path.join(output_dir, "evaluation_summary.json")
        save_json(summary, summary_file)
        print(f"\nè¯„ä¼°æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
    
    return {
        "overall_statistics": {
            "total_samples": total_samples_all,
            "total_reliable_samples": total_reliable_all,
            "overall_acceptance_rate": overall_acceptance_rate
        },
        "ecosystem_results": all_results
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨GPT-5è¯„ä¼°æ•°æ®é›†æ ·æœ¬å¯é æ€§")
    parser.add_argument("--data_dir", type=str, default=None,
                        help="æ•°æ®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨æ£€æµ‹ï¼‰")
    parser.add_argument("--output_dir", type=str, default=None,
                        help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼šdata/evaluation_resultsï¼‰")
    parser.add_argument("--ecosystem", type=str, default=None,
                        help="åªè¯„ä¼°æŒ‡å®šç”Ÿæ€ï¼ˆé»˜è®¤ï¼šè¯„ä¼°æ‰€æœ‰ç”Ÿæ€ï¼‰")
    parser.add_argument("--model", type=str, default=None,
                        help="GPTæ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡GPT_MODELæˆ–gpt-4o-2024-05-13ï¼‰")
    parser.add_argument("--generate_summary", action="store_true",
                        help="ä»å·²æœ‰çš„è¯„ä¼°ç»“æœæ–‡ä»¶ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡ï¼ˆä¸è¿›è¡Œè¯„ä¼°ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.model:
        os.environ["GPT_MODEL"] = args.model
    
    # ç¡®å®šæ•°æ®ç›®å½•
    if args.data_dir:
        data_dir = args.data_dir
    else:
        # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç›®å½•
        current_dir = Path(__file__).parent
        project_root = current_dir.parent.parent
        data_dir = project_root / "data"
        
        if not data_dir.exists():
            data_dir = Path("data")
    
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®ç›®å½• {data_dir}")
        exit(1)
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        output_dir = os.path.join(data_dir, "evaluation_results")
    
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä½¿ç”¨æ¨¡å‹: {os.environ.get('GPT_MODEL', 'gpt-4o-2024-05-13')}")
    
    # å¦‚æœåªæ˜¯ç”Ÿæˆæ±‡æ€»ï¼Œç›´æ¥è°ƒç”¨å‡½æ•°å¹¶é€€å‡º
    if args.generate_summary:
        generate_summary_from_results(output_dir)
        exit(0)
    
    # æ‰§è¡Œè¯„ä¼°
    if args.ecosystem:
        # åªè¯„ä¼°æŒ‡å®šç”Ÿæ€
        dataset_path = os.path.join(data_dir, args.ecosystem, "dataset.json")
        if not os.path.exists(dataset_path):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ° {args.ecosystem} ç”Ÿæ€çš„æ•°æ®é›†æ–‡ä»¶")
            exit(1)
        evaluate_ecosystem(args.ecosystem, dataset_path, output_dir)
    else:
        # è¯„ä¼°æ‰€æœ‰ç”Ÿæ€
        evaluate_all_ecosystems(data_dir, output_dir)

