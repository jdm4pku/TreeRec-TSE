
import os
import time
import requests
from lxml import html
from tqdm import tqdm
import random
import sys
import json
from bs4 import BeautifulSoup

def load_json(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        all_model = json.load(file)
    if not isinstance(all_model, dict):
        raise ValueError("The input file should be a json file")
    return all_model

def save_json(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def count_articles(tree):
    """ç»Ÿè®¡é¡µé¢ä¸­ <article> æ•°é‡"""
    articles = tree.xpath("//article")
    return len(articles)


def get_element_value(url, all_model):
    """ä»å•ä¸ª Hugging Face é¡µé¢æå–æ¨¡å‹åä¸ç±»å‹"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return None, f"HTTP Error: {response.status_code}"
    except Exception as e:
        return None, f"Request Error: {e}"

    try:
        tree = html.fromstring(response.content)
        a_counts = count_articles(tree)
    except Exception as e:
        return None, f"HTML Parse Error: {e}"

    if a_counts == 0:
        debug_path = f"IntentRecBench/data/hf/debug/empty_page_{int(time.time())}.html"
        os.makedirs(os.path.dirname(debug_path), exist_ok=True)
        with open(debug_path, "w", encoding="utf-8") as f:
            f.write(response.text)
        return None, f"No <article> found. Saved to {debug_path}"

    progress_bar = tqdm(total=a_counts, desc=f"Processing {url}", unit="article")
    success_count = 0
    failed_texts = []

    # éå†æ‰€æœ‰ article èŠ‚ç‚¹ï¼ˆæ¯é¡µåªè¯·æ±‚ä¸€æ¬¡ï¼‰
    articles = tree.xpath("//article")

    for idx, article in enumerate(articles, 1):
        try:
            text_content = article.text_content().strip()
            if not text_content:
                failed_texts.append(f"ç¬¬{idx}ä¸ªæ¨¡å‹ï¼šç©ºæ–‡æœ¬")
                continue

            first_part = text_content.split("â€¢")[0].strip()
            lines = [line.strip() for line in first_part.split("\n") if line.strip()]

            if len(lines) >= 2:
                model_name = lines[0]
                model_type = lines[1]
            elif len(lines) == 1:
                model_name = lines[0]
                model_type = "unknown"
            else:
                failed_texts.append(f"ç¬¬{idx}ä¸ªæ¨¡å‹ï¼šæ— æ³•è§£æ '{text_content[:50]}'")
                continue

            if model_name:
                all_model[model_name] = model_type
                success_count += 1
            else:
                failed_texts.append(f"ç¬¬{idx}ä¸ªæ¨¡å‹ï¼šæ¨¡å‹åä¸ºç©º '{text_content[:50]}'")

        except Exception as e:
            failed_texts.append(f"ç¬¬{idx}ä¸ªæ¨¡å‹è§£æé”™è¯¯: {e}")
        finally:
            progress_bar.update(1)

    progress_bar.close()
    print(f"âœ… æˆåŠŸè§£æ {success_count} ä¸ªæ¨¡å‹ï¼ŒâŒ å¤±è´¥ {len(failed_texts)} ä¸ª")

    # ä¿å­˜å¤±è´¥ä¿¡æ¯
    if failed_texts:
        os.makedirs("IntentRecBench/data/hf/debug", exist_ok=True)
        debug_file = f"IntentRecBench/data/hf/debug/failed_parsing_{int(time.time())}.txt"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(f"URL: {url}\næ€»æ•°: {a_counts}, æˆåŠŸ: {success_count}, å¤±è´¥: {len(failed_texts)}\n\n")
            f.write("\n".join(failed_texts))
        print(f"âš ï¸ è§£æå¤±è´¥è¯¦æƒ…å·²ä¿å­˜åˆ°: {debug_file}")

    return success_count, None

def merge_hf_pages():
    """åˆå¹¶æ‰€æœ‰é¡µé¢æ•°æ®å¹¶æ‰“å°é‡å¤æ¨¡å‹å‡ºç°åœ¨å“ªå‡ é¡µ"""
    import glob
    import re

    all_model = {}
    model_origin = {}  # è®°å½•æ¯ä¸ªæ¨¡å‹ç¬¬ä¸€æ¬¡å‡ºç°åœ¨å“ªä¸€é¡µ
    duplicates = {}    # è®°å½•é‡å¤å‡ºç°çš„æ¨¡å‹å’Œå¯¹åº”é¡µ

    page_dir = "IntentRecBench/data/hf/pages"

    if not os.path.exists(page_dir):
        print("é¡µé¢ç›®å½•ä¸å­˜åœ¨")
        return

    page_files = glob.glob(os.path.join(page_dir, "page_*.json"))
    # æå–é¡µç å¹¶æ’åº
    page_files.sort(key=lambda x: int(re.search(r"page_(\d+)\.json", x).group(1)))

    print(f"æ‰¾åˆ° {len(page_files)} ä¸ªé¡µé¢æ–‡ä»¶")

    for page_file in page_files:
        try:
            page_data = load_json(page_file)
            page_name = os.path.basename(page_file)
            before_count = len(all_model)

            for model_name, model_type in page_data.items():
                if model_name in all_model:
                    # å‘ç°é‡å¤ï¼Œè®°å½•æ¥æºé¡µ
                    if model_name not in duplicates:
                        duplicates[model_name] = [model_origin[model_name], page_name]
                    else:
                        if page_name not in duplicates[model_name]:
                            duplicates[model_name].append(page_name)
                else:
                    model_origin[model_name] = page_name
                    all_model[model_name] = model_type

            added_count = len(all_model) - before_count
            print(f"âœ… å·²åˆå¹¶ {page_name}: æ–°å¢ {added_count} ä¸ªæ¨¡å‹ (å½“å‰æ€»æ•°: {len(all_model)})")

        except Exception as e:
            print(f"âŒ åˆå¹¶ {os.path.basename(page_file)} æ—¶å‡ºé”™: {e}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    save_json(all_model, "IntentRecBench/data/hf/candidate_name.json")
    print(f"\nğŸ¯ æ‰€æœ‰é¡µé¢æ•°æ®å·²åˆå¹¶ï¼Œå…± {len(all_model)} ä¸ªæ¨¡å‹")

    # æ‰“å°é‡å¤ä¿¡æ¯
    if duplicates:
        print(f"\nâš ï¸ å‘ç° {len(duplicates)} ä¸ªé‡å¤æ¨¡å‹ï¼Œç¤ºä¾‹:")
        sample_count = 0
        for model_name, pages in duplicates.items():
            print(f"  - æ¨¡å‹: {model_name}")
            print(f"    å‡ºç°åœ¨é¡µé¢: {', '.join(pages)}")
            sample_count += 1
            if sample_count >= 10:
                print("    ...ï¼ˆä»…å±•ç¤ºå‰10ä¸ªï¼‰")
                break

        # ä¿å­˜åˆ° debug æ–‡ä»¶
        os.makedirs("IntentRecBench/data/hf/debug", exist_ok=True)
        dup_file = "IntentRecBench/data/hf/debug/duplicate_models.txt"
        with open(dup_file, "w", encoding="utf-8") as f:
            f.write(f"å…± {len(duplicates)} ä¸ªé‡å¤æ¨¡å‹\n\n")
            for model_name, pages in duplicates.items():
                f.write(f"{model_name}: {', '.join(pages)}\n")

        print(f"ğŸ” é‡å¤æ¨¡å‹è¯¦æƒ…å·²ä¿å­˜åˆ° {dup_file}")

    else:
        print("âœ… æœªå‘ç°é‡å¤æ¨¡å‹ã€‚")

def delete_code_block(input_str):
    output_str = ""
    label_list = []
    try:
        for i in range(len(input_str)-2):
            if input_str[i: i+3] == '```':
                label_list.append(i)
        for i in range(1, len(label_list), 2):
            label_list[i] += 3
        label_list.insert(0, 0)
        label_list.insert(len(label_list), len(input_str))
        start = [label_list[i] for i in range(0, len(label_list), 2)]
        end = [label_list[i] for i in range(1, len(label_list), 2)]
        if len(start) == len(end):
            for i in range(len(start)):
                output_str += input_str[start[i]: end[i]]
    except:
        output_str = input_str
    return output_str

def get_hf_model_name():
    """ä¸»å‡½æ•°ï¼šéå† Hugging Face æ¨¡å‹é¡µ"""
    os.makedirs("IntentRecBench/data/hf/pages", exist_ok=True)
    all_model = {}

    # æŠ“å–ç¬¬ 0 é¡µ
    url_0 = "https://huggingface.co/models?sort=trending"
    count0, error0 = get_element_value(url_0, all_model)
    if error0:
        print(f"é»˜è®¤é¡µä¸‹è½½å¤±è´¥: {error0}")
        return
    print(f"é»˜è®¤é¡µä¸‹è½½å®Œæˆï¼Œå…± {count0} ä¸ªæ¨¡å‹")
    save_json(all_model, "IntentRecBench/data/hf/pages/page_0.json")

    # æŠ“å–åç»­é¡µ
    for i in range(1, 100):
        page_models = {}
        url = f"https://huggingface.co/models?p={i}&sort=trending"
        print(f"\nğŸ“„ å¼€å§‹è§£æç¬¬ {i} é¡µ: {url}")

        max_retries = 5
        for attempt in range(max_retries):
            count_i, error_i = get_element_value(url, page_models)
            if error_i:
                print(f"âš ï¸ ç¬¬ {i} é¡µå¤±è´¥ ({attempt+1}/{max_retries}) æ¬¡: {error_i}")
                time.sleep(5)
                continue
            else:
                print(f"âœ… ç¬¬ {i} é¡µä¸‹è½½æˆåŠŸï¼Œå…± {count_i} ä¸ªæ¨¡å‹")
                break
        else:
            print(f"âŒ ç¬¬ {i} é¡µè¿ç»­å¤±è´¥ {max_retries} æ¬¡ï¼Œè·³è¿‡ã€‚")
            continue

        # ä¿å­˜å½“å‰é¡µ
        page_file = f"IntentRecBench/data/hf/pages/page_{i}.json"
        save_json(page_models, page_file)
        print(f"ğŸ’¾ ç¬¬ {i} é¡µå·²ä¿å­˜åˆ° {page_file}")

        all_model.update(page_models)
        # é˜²å°å»¶æ—¶
        time.sleep(random.uniform(1.5, 3.5))

    save_json(all_model, "IntentRecBench/data/hf/candidate_name.json")
    print(f"\nğŸ¯ æ‰€æœ‰é¡µé¢æ•°æ®å·²åˆå¹¶ï¼Œå…± {len(all_model)} ä¸ªæ¨¡å‹")

def get_hf_model_desc(model_name):
    model_url = "https://huggingface.co/{}".format(model_name)
    response = requests.get(model_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    target_elements = soup.select("body > div > main > div.container.relative.flex.flex-col.md\\:grid.md\\:space-y-0.w-full.md\\:grid-cols-12.md\\:flex-1.md\\:grid-rows-full.space-y-4.md\\:gap-6 > section.pt-8.border-gray-100.md\\:col-span-7.pb-24.relative.break-words.copiable-code-container")
    model_desc = ""
    for element in target_elements:
        temp = element.text.strip().split("\n")
        temp = list(map(lambda x:x.strip(), temp))
        element_list = [i for i in temp if i != ""]
        model_desc += "\n".join(element_list)
    model_desc = delete_code_block(model_desc)
    return model_desc

def collect_hf_artifacts():
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸‹è½½
    if not os.path.exists("IntentRecBench/data/hf/candidate_desc.json"):
        os.makedirs("IntentRecBench/data/hf/debug", exist_ok=True)
        get_hf_model_name()
        merge_hf_pages()
    # ä¸‹è½½æ‰€æœ‰çš„é¡µé¢æè¿°
    page_dir = "IntentRecBench/data/hf/pages"
    desc_dir = "IntentRecBench/data/hf/desc"
    if not os.path.exists(desc_dir):
        os.makedirs(desc_dir, exist_ok=True)
    for i in range(100):
        page_desc = {}
        page_file = f"{page_dir}/page_{i}.json"
        page_data = load_json(page_file)
        for model_name, model_type in page_data.items():
            try:
                model_desc = get_hf_model_desc(model_name)
            except Exception as e:
                model_desc = ""
                print(f"âš ï¸ è·å– {model_name} æè¿°å¤±è´¥: {e}")
            page_desc[model_name] = model_desc
            time.sleep(random.uniform(0.4, 1.0))
        out_file = f"{desc_dir}/page_{i}.json"
        save_json(page_desc, f"{desc_dir}/page_{i}.json")
        print(f"ğŸ’¾ å·²ä¿å­˜é¡µé¢æè¿°åˆ° {out_file}ï¼ˆ{len(page_desc)} æ¡ï¼‰")
        # print(f"ğŸ’¾ å·²ä¿å­˜é¡µé¢æè¿°åˆ° {f"{desc_dir}/page_{i}.json"}ï¼ˆ{len(page_desc)} æ¡ï¼‰")
    
    # åˆå¹¶æ‰€æœ‰çš„é¡µé¢æè¿°
    all_desc = {}
    for i in range(100):
        page_desc_file = f"{desc_dir}/page_{i}.json"
        page_desc = load_json(page_desc_file)
        all_desc.update(page_desc)
    save_json(all_desc, "IntentRecBench/data/hf/candidate_desc.json")
    print(f"ğŸ¯ æ‰€æœ‰é¡µé¢æ•°æ®å·²åˆå¹¶ï¼Œå…± {len(all_desc)} ä¸ªæ¨¡å‹")

def format_hf_artifacts():
    all_data = []
    with open("IntentRecBench/data/hf/name/all_name.json", "r", encoding="utf-8") as f:
        all_name = json.load(f)
    with open("IntentRecBench/data/hf/desc/all_desc.json", "r", encoding="utf-8") as f:
        all_desc = json.load(f)
    for key, value in all_desc.items():
        all_data.append({
            "name": key,
            "type": all_name[key],
            "description": value
        })
    save_json(all_data, "IntentRecBench/data/hf/candidate_artifacts.json")

def collect_js_package():
    keywords = ["front-end","cli","css","iot","mobile","robotics","back-end","documentation","testing","coverage","frameworks","math"]
    max_len = 1000
    base_url = "https://registry.npmjs.org/-/v1/search"
    size = 250
    delay = 0.5

    os.makedirs("IntentRecBench/data/js/keywords", exist_ok=True)
    all_packages_total = []

    for keyword in keywords:
        print(f"\nğŸ” å¼€å§‹çˆ¬å– npm åŒ…ï¼ˆå…³é”®è¯ï¼š{keyword}ï¼‰...")
        all_packages = []
        offset = 0
        max_retries = 5

        while len(all_packages) < max_len:
            params = {
                "text": f"keywords:{keyword}",
                "size": size,
                "from": offset
            }

            try:
                res = requests.get(base_url, params=params, timeout=10)
                res.raise_for_status()
                data = res.json()
                results = data.get("objects", [])
                if not results:  # æ²¡æ•°æ®äº†ï¼Œé€€å‡ºå¾ªç¯
                    print(f"âš ï¸ å…³é”®è¯ {keyword} æ— æ›´å¤šç»“æœï¼Œåœæ­¢ã€‚")
                    break

                for item in results:
                    pkg = item["package"]
                    pkg_name = pkg["name"]
                    pkg_desc = pkg.get("description", "")
                    all_packages.append({
                        "name": pkg_name,
                        "description": pkg_desc
                    })
                    all_packages_total.append({
                        "name": pkg_name,
                        "type": keyword,
                        "description": pkg_desc
                    })

                print(f"âœ… å·²è·å– {len(all_packages)} ä¸ªåŒ… (offset={offset})")
                offset += size
                time.sleep(delay)

            except Exception as e:
                print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {e}")
                for i in range(max_retries):
                    print(f"â³ ç¬¬ {i+1}/{max_retries} æ¬¡é‡è¯•ä¸­...")
                    time.sleep(5)
                    try:
                        res = requests.get(base_url, params=params, timeout=10)
                        res.raise_for_status()
                        break
                    except:
                        continue
                else:
                    print(f"âŒ å…³é”®è¯ {keyword} é‡è¯•è¶…è¿‡ä¸Šé™ï¼Œè·³è¿‡ã€‚")
                    break

        # å†™å…¥å½“å‰å…³é”®è¯æ–‡ä»¶
        output_file = f"IntentRecBench/data/js/keywords/{keyword}.json"
        save_json(all_packages, output_file)
        print(f"ğŸ“¦ å·²ä¿å­˜ {len(all_packages)} ä¸ªåŒ…åˆ° {output_file}")

    # å†™å…¥æ€»çš„æ–‡ä»¶
    total_file = "IntentRecBench/data/js/candidate_artifacts.json"
    save_json(all_packages_total, total_file)
    print(f"\nğŸ¯ æ‰€æœ‰å…³é”®è¯æ•°æ®å·²åˆå¹¶ä¿å­˜åˆ° {total_file}ï¼Œå…± {len(all_packages_total)} ä¸ªåŒ…ã€‚")

if __name__ == "__main__":
    collect_hf_artifacts()
    format_hf_artifacts()
    collect_js_package()