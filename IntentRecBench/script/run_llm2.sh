#!/bin/bash
# ========================================================
# Run LLM recommendation for all ecosystems
# Author: Auto-generated
# ========================================================

# ---- å…¨å±€é…ç½® ----
DATA_DIR="IntentRecBench/data"
OUTPUT_DIR="output/baselines"
# æ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹:
# GPT: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Qwen: Qwen/Qwen3-14B, Qwen/Qwen3-8B, Qwen/Qwen3-32B
# DeepSeek: Pro/deepseek-ai/DeepSeek-R1, deepseek-ai/DeepSeek-V3
# Llama: meta-llama/llama-3.1-8b-instruct, meta-llama/llama-3.1-70b-instruct
MODEL_NAME="Qwen/Qwen3-14B"  # å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æ¨¡å‹åç§°
TOP_K=5
P_K="1 2 3 4"
DCG_K="2 3 4 5"
# ä¸¤é˜¶æ®µç­–ç•¥é…ç½®
USE_TWO_STAGE="--use_two_stage"  # ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼ˆé»˜è®¤å¯ç”¨ï¼Œä½¿ç”¨ --no_two_stage ç¦ç”¨ï¼‰
FILTER_TOP_PERCENT=0.1  # ç¬¬ä¸€é˜¶æ®µç­›é€‰å‡ºçš„å€™é€‰åˆ¶å“ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤0.1ï¼Œå³top 10%ï¼‰
SCORING_BATCH_SIZE=100 # linuxï¼š20, hf:30, js:100

# æ‰¹é‡æ‰“åˆ†çš„å¤§å°ï¼ˆé»˜è®¤20ï¼Œå³æ¯æ¬¡æ‰¹é‡æ‰“åˆ†20ä¸ªåˆ¶å“ï¼‰
# ---- å®šä¹‰è¦è·‘çš„ç”Ÿæ€ç³»ç»Ÿ ----
ECOSYSTEMS=("hf") # "hf" "js" "linux"

# ---- ä¸»å¾ªç¯ ----
for ECO in "${ECOSYSTEMS[@]}"; do
    echo "=============================================="
    echo "ğŸš€ Running LLM recommendation for ecosystem: ${ECO}"
    echo "   Model: ${MODEL_NAME}"
    echo "=============================================="

    python IntentRecBench/src/baselines/llm.py \
        --data_dir "$DATA_DIR" \
        --ecosystem "$ECO" \
        --output_dir "$OUTPUT_DIR" \
        --model_name "$MODEL_NAME" \
        --top_k $TOP_K \
        --p_k $P_K \
        --dcg_k $DCG_K \
        $USE_TWO_STAGE \
        --filter_top_percent $FILTER_TOP_PERCENT

    echo "âœ… ${ECO} finished."
    echo "----------------------------------------------"
    sleep 2  # é˜²æ­¢æ—¥å¿—æ–‡ä»¶æ—¶é—´æˆ³å†²çª
done

echo "=============================================="
echo "ğŸ‰ All ecosystems completed!"
echo "=============================================="

