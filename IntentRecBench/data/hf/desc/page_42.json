{
    "Intel/DeepSeek-V3.1-Terminus-int8-AutoRound": "Model Details\nModel Version(s)\nHow To Use\nINT8 Inference\nGenerate the Model\nEthical Considerations and Limitations\nCaveats and Recommendations\nDisclaimer\nCite\nModel Details\nThis model is a int8 model with group_size 128 and symmetric quantization of deepseek-ai/DeepSeek-V3.1-Terminus generated by intel/auto-round via RTN(no algorithm tuning). Please refer to Section Generate the model for more details. Please follow the license of the original model.\nModel Version(s)\nThe model is quantized with auto-round v0.8.0\nHow To Use\nINT8 Inference\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport transformers\nimport torch\nquantized_model_dir = \"Intel/DeepSeek-V3.1-Terminus-int8-AutoRound\"\nmodel = AutoModelForCausalLM.from_pretrained(\nquantized_model_dir,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, trust_remote_code=True)\nprompts = [\n\"9.11å’Œ9.8å“ªä¸ªæ•°å­—å¤§\",\n\"strawberryä¸­æœ‰å‡ ä¸ªr?\",\n\"There is a girl who likes adventure,\",\n\"Please give a brief introduction of DeepSeek company.\",\n]\ntexts=[]\nfor prompt in prompts:\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ntexts.append(text)\ninputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\noutputs = model.generate(\ninput_ids=inputs[\"input_ids\"].to(model.device),\nattention_mask=inputs[\"attention_mask\"].to(model.device),\nmax_length=200, ##change this to align with the official usage\nnum_return_sequences=1,\ndo_sample=False  ##change this to align with the official usage\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(inputs[\"input_ids\"], outputs)\n]\ndecoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nfor i, prompt in enumerate(prompts):\ninput_id = inputs\nprint(f\"Prompt: {prompt}\")\nprint(f\"Generated: {decoded_outputs[i]}\")\nprint(\"-\"*50)\nGenerate the Model\nauto-round --model_name deepseek-ai/DeepSeek-V3.1-Terminus --iters 0 --bits 8\nEthical Considerations and Limitations\nThe model can produce factually incorrect output, and should not be relied on to produce factually accurate information. Because of the limitations of the pretrained model and the finetuning datasets, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\nTherefore, before deploying any applications of the model, developers should perform safety testing.\nCaveats and Recommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nHere are a couple of useful links to learn more about Intel's AI software:\nIntel Neural Compressor\nDisclaimer\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please consult an attorney before using this model for commercial purposes.\nCite\n@article{cheng2023optimize, title={Optimize weight rounding via signed gradient descent for the quantization of llms}, author={Cheng, Wenhua and Zhang, Weiwei and Shen, Haihao and Cai, Yiyang and He, Xin and Lv, Kaokao and Liu, Yi}, journal={arXiv preprint arXiv:2309.05516}, year={2023} }\narxiv github",
    "shivash/MyAwesome-299M-Model": "MyAwesome-299M-Model\nğŸš€ Model Overview\nâš¡ Key Features\nğŸ¯ Quick Start\nBasic Text Generation\nAdapter Fine-tuning (Recommended)\nğŸ¨ Adapter Examples\nğŸ“Š Math Reasoning Adapter\nğŸ’» Code Generation Adapter\nâœï¸ Creative Writing Adapter\nğŸ§  Vocabulary Expansion for Distillation\nBreaking the Vocabulary Barrier\nğŸ”§ Training Your Own Adapters\nMethod 1: Use the Framework Scripts\nMethod 2: Manual Training\nğŸ“ˆ Performance Characteristics\nEfficiency Metrics\nTask Performance\nğŸ¯ Recommended Use Cases\nâœ… Excellent For:\nâš ï¸ Consider Limitations:\nğŸ”¬ Technical Details\nArchitecture Specifications\nMemory Requirements\nğŸ›  Framework Integration\nFramework Repository\nğŸ¤ Community & Contributions\nJoin the Community\nSharing Your Adapters\nğŸ“„ Citation\nğŸ“‹ License\nğŸ™ Acknowledgments\nğŸš€ Get Started Today!\nMyAwesome-299M-Model\nA compact, efficient language model built from scratch demonstrating the Transfer-First paradigm - optimized for adapter-based fine-tuning and rapid task specialization.\nğŸš€ Model Overview\nModel Type: Decoder-only transformer (Llama architecture)\nBuilt From Scratch: Custom implementation with randomly initialized weights\nParameters: 57.2M (demonstration size)\nArchitecture: 512d Ã— 8 layers with Grouped-Query Attention\nVocabulary: 50,257 tokens (GPT-2 compatible tokenizer for convenience)\nContext Length: 1,024 tokens\nMemory Usage: ~115MB (bfloat16)\nâš¡ Key Features\nAdapter-Ready: Optimized for LoRA and other parameter-efficient fine-tuning\nFast Inference: 50+ tokens/second on modern hardware\nMemory Efficient: Sub-200MB deployment footprint\nTask Switching: Load different 8MB adapters for instant specialization\nVocabulary Expansion: Surgically expand vocabulary for distillation from any teacher model\nğŸ¯ Quick Start\nBasic Text Generation\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"shivash/MyAwesome-299M-Model\")\ntokenizer = AutoTokenizer.from_pretrained(\"shivash/MyAwesome-299M-Model\")\n# Generate text\nprompt = \"The future of AI is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model.generate(\n**inputs,\nmax_new_tokens=50,\ntemperature=0.7,\ndo_sample=True,\npad_token_id=tokenizer.eos_token_id\n)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\nAdapter Fine-tuning (Recommended)\nfrom peft import LoraConfig, get_peft_model, TaskType\n# Configure LoRA adapter\nlora_config = LoraConfig(\ntask_type=TaskType.CAUSAL_LM,\nr=8,  # Rank\nlora_alpha=16,  # Alpha scaling\nlora_dropout=0.1,\ntarget_modules=[\n\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n\"gate_proj\", \"up_proj\", \"down_proj\"\n],\nbias=\"none\"\n)\n# Apply LoRA to model\nmodel = get_peft_model(model, lora_config)\n# Now ready for task-specific fine-tuning!\n# Only ~1% of parameters are trainable\nprint(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\nğŸ¨ Adapter Examples\nThis model shines when fine-tuned with adapters for specific tasks. Here are some examples:\nğŸ“Š Math Reasoning Adapter\n# Train a math specialist (from the framework)\npython scripts/train_task_adapters.py --task math --test\nSample Output:\nInput: \"What is 25% of 160?\"\nOutput: \"To find 25% of 160:\n25% = 25/100 = 0.25\n0.25 Ã— 160 = 40\nTherefore, 25% of 160 is 40.\"\nğŸ’» Code Generation Adapter\n# Train a coding assistant\npython scripts/train_task_adapters.py --task coding --test\nSample Output:\n# Input: \"Function to check if a number is prime\"\ndef is_prime(n):\nif n < 2:\nreturn False\nfor i in range(2, int(n**0.5) + 1):\nif n % i == 0:\nreturn False\nreturn True\nâœï¸ Creative Writing Adapter\n# Train a creative writing assistant\npython scripts/train_task_adapters.py --task creative --test\nSample Output:\nInput: \"A robot discovers emotions\"\nOutput: \"Unit-7742 had processed millions of data points, but nothing had\nprepared it for the strange sensation that flooded its circuits when it\nwitnessed the sunset. For the first time, efficiency seemed irrelevant.\"\nğŸ§  Vocabulary Expansion for Distillation\nBreaking the Vocabulary Barrier\nOne of the key challenges in knowledge distillation is vocabulary mismatch - your student model (50K tokens) can't directly learn from a teacher with a different vocabulary (150K tokens). Our vocabulary expansion tool solves this:\n# Expand vocabulary to match any teacher model\npython expand_vocab.py \\\n--model_repo_id \"shivash/MyAwesome-299M-Model\" \\\n--new_tokenizer_repo_id \"Qwen/Qwen2-1.5B\" \\\n--output_dir \"./MyAwesome-299M-Model-Qwen-Vocab\"\nWhat this does:\nâœ… Preserves all existing knowledge from your 50K vocabulary\nâœ… Adds new token capacity (e.g., 100K new tokens for Qwen2)\nâœ… Intelligently initializes new embeddings (mean of existing weights)\nâœ… Enables distillation from any teacher model\nâœ… Ready for immediate use with the new tokenizer\nExample expansions:\n# For Qwen2 teachers (151K vocabulary)\npython expand_vocab.py \\\n--model_repo_id \"shivash/MyAwesome-299M-Model\" \\\n--new_tokenizer_repo_id \"Qwen/Qwen2-1.5B\" \\\n--output_dir \"./expanded-qwen-vocab\"\n# For Llama 3 teachers (128K vocabulary)\npython expand_vocab.py \\\n--model_repo_id \"shivash/MyAwesome-299M-Model\" \\\n--new_tokenizer_repo_id \"meta-llama/Meta-Llama-3-8B\" \\\n--output_dir \"./expanded-llama3-vocab\"\nAfter expansion, you can distill knowledge from any teacher model with that vocabulary! ğŸš€\nğŸ”§ Training Your Own Adapters\nMethod 1: Use the Framework Scripts\n# Clone the Transfer-First LLM Framework\ngit clone https://github.com/your-username/transfer-first-llm.git\ncd transfer-first-llm\n# Install dependencies\npip install -e \".[dev]\"\n# Train custom adapters\npython scripts/train_task_adapters.py --task reasoning --epochs 3 --test\nMethod 2: Manual Training\nfrom transformers import TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nimport torch\n# Setup model with LoRA\nmodel = AutoModelForCausalLM.from_pretrained(\"shivash/MyAwesome-299M-Model\")\nlora_config = LoraConfig(\ntask_type=\"CAUSAL_LM\",\nr=8, lora_alpha=16, lora_dropout=0.1,\ntarget_modules=[\"q_proj\", \"v_proj\", \"o_proj\"]\n)\nmodel = get_peft_model(model, lora_config)\n# Prepare your dataset\n# dataset = your_formatted_dataset\n# Training arguments\ntraining_args = TrainingArguments(\noutput_dir=\"./my-adapter\",\nnum_train_epochs=3,\nper_device_train_batch_size=4,\nlearning_rate=1e-4,\nlogging_steps=10,\n)\n# Train\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=dataset,\ntokenizer=tokenizer\n)\ntrainer.train()\n# Save adapter\nmodel.save_pretrained(\"./my-custom-adapter\")\nğŸ“ˆ Performance Characteristics\nEfficiency Metrics\nTraining Time: 3-10 minutes per adapter (depending on data size)\nAdapter Size: 8-16MB per specialized task\nMemory During Training: <1GB GPU memory\nInference Speed: 50+ tokens/second\nTask Performance\nKnowledge Retention: Maintains base capabilities while adding specialization\nAdaptation Speed: Few-shot learning with minimal data\nGeneralization: Strong transfer across related tasks\nRobustness: Stable performance across different prompting styles\nğŸ¯ Recommended Use Cases\nâœ… Excellent For:\nEducational tools (math tutoring, concept explanation)\nCode assistance (function generation, debugging help)\nContent creation (creative writing, technical docs)\nSpecialized reasoning (logic puzzles, problem decomposition)\nRapid prototyping of AI applications\nResource-constrained deployment\nâš ï¸ Consider Limitations:\nBase model size: 57M parameters is smaller than production models\nDomain knowledge: May require fine-tuning for specialized fields\nContext length: 1024 tokens may be limiting for long documents\nMultilingual: Primarily trained on English content\nğŸ”¬ Technical Details\nArchitecture Specifications\nModel Architecture:\nType: LlamaForCausalLM\nLayers: 8\nHidden Size: 512\nAttention Heads: 8\nKV Heads: 4 (Grouped-Query Attention)\nIntermediate Size: 2048\nVocab Size: 50257\nMax Position: 1024\nRMS Norm Epsilon: 1e-5\nOptimizations:\nAttention: Grouped-Query for efficiency\nActivation: SiLU (Swish)\nNormalization: RMSNorm\nPosition Encoding: Rotary (RoPE)\nMemory Requirements\nModel Loading:\nFP32: ~230MB\nFP16: ~115MB\nINT8: ~60MB\nTraining (with LoRA):\nBase Model: 115MB\nGradients: ~1MB (only adapter params)\nOptimizer States: ~2MB\nTotal: <200MB GPU memory\nğŸ›  Framework Integration\nThis model is part of the Transfer-First LLM Framework, which provides:\nKnowledge Distillation Pipeline: Create compact models from large teachers\nVocabulary Expansion Tools: Break vocabulary barriers for cross-model distillation\nAdapter Training Scripts: Ready-to-use fine-tuning workflows\nMulti-Task Composition: Combine multiple adapters dynamically\nEvaluation Tools: Comprehensive testing and benchmarking\nDeployment Utilities: Efficient inference and serving\nFramework Repository\nğŸ”— Transfer-First LLM Framework\nğŸ¤ Community & Contributions\nJoin the Community\nGitHub Discussions: Share your adapter creations\nIssues: Report bugs or request features\nPull Requests: Contribute improvements\nExamples: Add your use cases to our gallery\nSharing Your Adapters\nWe encourage sharing trained adapters with the community:\nTrain your adapter using the framework\nTest and document your results\nUpload to HuggingFace Hub with clear descriptions\nTag with transfer-first-adapter for discoverability\nğŸ“„ Citation\nIf you use this model in your research, please cite:\n@misc{myawesome299m,\ntitle={MyAwesome-299M-Model: Efficient Language Model for Adapter-Based Transfer Learning},\nauthor={Shivash Puri},\nyear={2024},\nurl={https://huggingface.co/shivash/MyAwesome-299M-Model}\n}\nğŸ“‹ License\nThis model is released under the MIT License. You are free to use, modify, and distribute it for both commercial and non-commercial purposes.\nğŸ™ Acknowledgments\nFramework: Built with the Transfer-First LLM Framework\nArchitecture: Inspired by Llama and modern transformer designs\nLibraries: Powered by Transformers, PEFT, and PyTorch\nCommunity: Thanks to the open-source AI community\nğŸš€ Get Started Today!\nReady to build specialized AI for your use case? This model provides the perfect foundation for adapter-based fine-tuning.\nQuick Links:\nğŸ“š Framework Documentation\nğŸ¯ Adapter Examples\nğŸ›  Training Scripts\nğŸ¤ Community Hub\nBuilt with â¤ï¸ for efficient and accessible AI",
    "Kingsoft-LLM/QZhou-Embedding-Zh": "QZhou-Embedding-Zh\nIntroduction\nKey Enhancements and Optimizationsâ€‹\nCMTEB Results\nâ€‹Token Prepending\nIntroduction\nOur Adaptations and Optimizationsâ€‹\nResults\nMRL Performance Comparison\nUsage\nRequirements\nQuickstart\nCompletely replicate the benchmark results\nCitation\nQZhou-Embedding-Zh\nIntroduction\nWe are pleased to announce the release of our new model, â€‹QZhou-Embedding-Zhâ€‹â€‹â€‹, which excels in a variety of Chinese-language tasks including retrieval, ranking, semantic similarity, and semantic understanding. Built upon the architecture and parameters of the â€‹â€‹Qwen3-8Bâ€‹â€‹ base model, QZhou-Embedding-Zh was developed using the data construction and training methodology of â€‹â€‹QZhou-Embedding, and also incorporated MRL embedding inferenceâ€‹â€‹. Leveraging the powerful Chinese language capabilities of Qwen3-8B, QZhou-Embedding-Zh achieves significant improvements across multiple tasks on the CMTEB benchmarkâ€”including retrieval, STS, clustering, Pair Classification, and reranking, with notable gains in both overall and task-type average scores.\nKey Enhancements and Optimizationsâ€‹\nTo build a more powerful and outstanding model, we have adopted proven approaches from QZhou-Embedding and further introduced the following optimizations:\nâ€‹â€‹Based on Qwen3 Modelâ€‹â€‹: In our practice with QZhou-Embedding, the Qwen3 base model did not show significant advantages over Qwen2.5-7B-Instruct in the first stage (Retrieval). However, notable improvements were observed in Chinese-language tasks, likely due to Qwen3â€™s stronger Chinese capabilities. We upgraded the base model to Qwen3-8B while retaining the original model architecture, using a â€‹â€‹last_token poolingâ€‹â€‹ strategy.\nâ€‹â€‹Support for MRLâ€‹â€‹: MRL (Multi-Representation Learning) is highly demanded in practical applications, especially under high-concurrency and low-latency scenarios. Addressing the lack of MRL support in QZhou-Embedding, QZhou-Embedding-Zh now incorporates this feature with the following dimension options: \"128, 256, 512, 768, 1024, 1280, 1536, 1792\". The default output dimension is set to â€‹â€‹1792â€‹â€‹.\nâ€‹Token Prependingâ€‹â€‹: Originally proposed by Fu et al(ACL 2025, Volume 1: Long Papers, 3168â€“3181), this technique addresses the limitations of the unidirectional attention mechanism in decoder-only models. By prepending each layerâ€™s decoded sentence embedding to the beginning of the sentence in the next layerâ€™s input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism, â€‹Token Prependingâ€‹ significantly improving performance in STS tasks and classification tasks. We retained the Stage-1 training strategy unchanged and integrated â€‹â€‹Token Prepending during Stage-2 trainingâ€‹â€‹, using the PromptEOL template construction method described in their paper. Experimental results demonstrate that Token Prepending is not only a training-free enhancement but also further improves performance when fine-tuned with supervised datasets.\nCMTEB Results\nThese are the ranking results on the CMTEB leaderboard (as of October 1st)ï¼š\nâ€‹Token Prepending\nIntroduction\nâ€‹Token Prepending is a simple yet effective technique proposed by Fu et al., the core idea is prepending each layerâ€™s decoded sentence embedding to the beginning of the sentence in the next layerâ€™s input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism. TP technique is a plug-and-play technique neither introduces new parameters nor alters the existing ones, allowing it to be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs. The architecture described in the original paper is as follows:\nOur Adaptations and Optimizationsâ€‹\nAccording to the conclusions presented in the original paper, TP technique is completely training-free and requires no extra learnable parameters, serving as a plug-and-play technique to improve the various prompt-based methods. Since QZhou-Embedding-Zh is built upon the Qwen3 base modelâ€”retaining its unidirectional attention mechanism and employing last_token poolingâ€”it is ideally suited for the application of the TP technique. To further explore its potential, we conducted training utilizing the TP technique, building upon the Stage 1 retrieval base model through the following procedure:\nWe modified the model forward script by applying the TP specifically from layer-1 to layer-7(index), namely prepending the last embeddings to the input before processing through these layersï¼›\nFor the input template design, we have integrated the PromptEOL template on top of the instruction-based input, using <|im_start|>as a placeholderâ€”corresponding to the <PST> token in the original paperâ€”to facilitate subsequent TP operations. The full template structure is designed as follows:\n\"This sentence: <|im_start|>â€œInstruct: [instruction]\\nQuery: [user_input]â€ means in one word: â€œ\nStage 2 training was conducted using the updated model architecture and input structure.\nResults\nWe maintained the Stage 1 training strategy unchanged, keeping all data, processing logic, and MRL configurations consistent across stages. The only modification was applied during Stage 2, where we compared the original instruction-based embedding training(used in QZhou-Embedding) against the Token Prepend-based training approach. The performance comparison on the CMTEB benchmark is shown below:\nThe results clearly show that even when applying TP starting from Stage 2, it still leads to measurable performance improvements â€” demonstrating the effectiveness and broad applicability of this technique.\nMRL Performance Comparison\nWe have evaluated the CMTEB performance across all MRL output dimensions, with detailed results as follows:\nAs the comparison results clearly demonstrate, using lower-dimensional outputs can impact overall performance to some extent. For instance, the 128-dimensional output shows an average score decrease of approximately 1% compared to the default configuration, and a similar gap is observed with 256 dimensions. However, once the dimensionality exceeds 512, no significant performance degradation is observed. In fact, certain subtasks even achieve state-of-the-art (SOTA) results under specific dimensional settingsâ€”such as STS tasks at 512 dimensions, and Retrieval and Rerank tasks at 1280 dimensions.\nIn terms of the overall average score, using the full-dimensional embedding yields slightly higher results. That said, for high-throughput scenarios where moderate performance trade-offs are acceptable, the lower dimension like 512 remains a strong and efficient alternative.\nUsage\nTo facilitate model inference and CMTEB result replication on your own machine, we provide detailed specifications for environmental dependencies and model implementation.\nRequirements\nPython: 3.10.12\nSentence Transformers: 3.4.1\nTransformers: 4.51.1\nPyTorch: 2.4.1\nAccelerate: 1.3.0\nDatasets: 3.6.0\nTokenizers: 0.21.1\nmteb: 1.38.30\nQuickstart\nSince QZhou-Embedding-Zh incorporates a dedicated MRL linear projection module built on the sentence-transformers framework, we now only provide inference code specifically designed for sentence-transformers compatibility.\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import normalize\ndef get_prompteol_input(text: str) -> str:\nreturn f\"This sentence: <|im_start|>â€œ{text}â€ means in one word: â€œ\"\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\nmodel = SentenceTransformer(\n\"Kingsoft-LLM/QZhou-Embedding-Zh\",\nmodel_kwargs={\"device_map\": \"cuda\", \"trust_remote_code\": True},\ntokenizer_kwargs={\"padding_side\": \"left\", \"trust_remote_code\": True},\ntrust_remote_code=True\n)\ntask= \"Given a web search query, retrieve relevant passages that answer the query\"\nqueries = [\nget_prompteol_input(get_detailed_instruct(task, \"å…‰åˆä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\")),\nget_prompteol_input(get_detailed_instruct(task, \"ç”µè¯æ˜¯è°å‘æ˜çš„ï¼Ÿ\"))\n]\ndocuments = [\nget_prompteol_input(\"å…‰åˆä½œç”¨æ˜¯ç»¿è‰²æ¤ç‰©åˆ©ç”¨é˜³å…‰ã€äºŒæ°§åŒ–ç¢³å’Œæ°´ç”Ÿæˆè‘¡è„ç³–å’Œæ°§æ°”çš„è¿‡ç¨‹ã€‚è¿™ä¸€ç”ŸåŒ–ååº”å‘ç”Ÿåœ¨å¶ç»¿ä½“ä¸­ã€‚\"),\nget_prompteol_input(\"äºšå†å±±å¤§Â·æ ¼æ‹‰æ±‰å§†Â·è´å°”ï¼ˆAlexander Graham Bellï¼‰å› äº1876å¹´å‘æ˜äº†ç¬¬ä¸€å°å®ç”¨ç”µè¯è€Œå¹¿å—è®¤å¯ï¼Œå¹¶ä¸ºæ­¤è®¾å¤‡è·å¾—äº†ç¾å›½ä¸“åˆ©ç¬¬174,465å·ã€‚\")\n]\nquery_embeddings = model.encode(queries, normalize_embeddings=False)\ndocument_embeddings = model.encode(documents, normalize_embeddings=False)\ndim=1792 # 128, 256, 512, 768, 1024, 1280, 1536, 1792\nquery_embeddings = normalize(query_embeddings[:, :dim])\ndocument_embeddings = normalize(document_embeddings[:, :dim])\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\nCompletely replicate the benchmark results\nFind our benchmark evaluation code on GitHub.\nnormalize=true\nuse_instruction=true\nexport TOKENIZERS_PARALLELISM=true\nembed_dim=1792 # 128, 256, 512, 768, 1024, 1280, 1536, 1792\nmodel_name_or_path=<model dir>\npython3 ./run_cmteb_all.py \\\n--model_name_or_path ${model_name_or_path}  \\\n--normalize ${normalize} \\\n--dim ${embed_dim} \\\n--use_instruction ${use_instruction} \\\n--output_dir <output dir>\nCitation\nIf you find our work worth citing, please use the following citation:\nTechnical Report:\n@misc{yu2025qzhouembeddingtechnicalreport,\ntitle={QZhou-Embedding Technical Report},\nauthor={Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu},\nyear={2025},\neprint={2508.21632},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.21632},\n}\nToken Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs:\n@inproceedings{fu-etal-2025-token,\ntitle = \"Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from {LLM}s\",\nauthor = \"Fu, Yuchen  and\nCheng, Zifeng  and\nJiang, Zhiwei  and\nWang, Zhonghui  and\nYin, Yafeng  and\nLi, Zhengliang  and\nGu, Qing\",\nbooktitle = \"Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\nmonth = jul,\nyear = \"2025\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2025.acl-long.159/\",\n}\nQwen3 Series:\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ": "Qwen3-VL-235B-A22B-Instruct-AWQ\nã€Dependencies / Installationã€‘\nã€vLLM Startup Commandã€‘\nã€Logsã€‘\nã€Model Filesã€‘\nã€Model Downloadã€‘\nã€Overviewã€‘\nQwen3-VL-235B-A22B-Instruct\nModel Performance\nQuickstart\nUsing ğŸ¤— Transformers to Chat\nCitation\nQwen3-VL-235B-A22B-Instruct-AWQ\nBase Model: Qwen/Qwen3-VL-235B-A22B-Instruct\nã€Dependencies / Installationã€‘\nAs of 2025-10-08, create a fresh Python environment and run:\nuv venv\nsource .venv/bin/activate\n# Install vLLM >=0.11.0\nuv pip install -U vllm\n# Install Qwen-VL utility library (recommended for offline inference)\nuv pip install qwen-vl-utils==0.0.14\nFor more details, refer to vLLM official Qwen3-VL Guide\nã€vLLM Startup Commandã€‘\nNote: When launching with TP=8, include --enable-expert-parallel;\notherwise the expert tensors couldnâ€™t be evenly sharded across GPU devices.\nCONTEXT_LENGTH=32768\nvllm serve \\\ntclf90/Qwen3-VL-235B-A22B-Instruct-AWQ \\\n--served-model-name My_Model \\\n--enable-expert-parallel \\\n--swap-space 16 \\\n--max-num-seqs 64 \\\n--max-model-len $CONTEXT_LENGTH \\\n--gpu-memory-utilization 0.9 \\\n--tensor-parallel-size 8 \\\n--trust-remote-code \\\n--disable-log-requests \\\n--host 0.0.0.0 \\\n--port 8000\nã€Logsã€‘\n2025-09-27\n1. formal commit\n2025-09-26\n1. Initial commit (preview version)\nã€Model Filesã€‘\nFile Size\nLast Updated\n117GB\n2025-09-27\nã€Model Downloadã€‘\nfrom modelscope import snapshot_download\nsnapshot_download('tclf90/Qwen3-VL-235B-A22B-Instruct-AWQ', cache_dir=\"your_local_path\")\nã€Overviewã€‘\nQwen3-VL-235B-A22B-Instruct\nMeet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless textâ€“vision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.\nDeepStack: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.\nTextâ€“Timestamp Alignment: Moves beyond Tâ€‘RoPE to precise, timestampâ€‘grounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-235B-A22B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ğŸ¤– ModelScope and ğŸ¤— Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ğŸ¤— Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-235B-A22B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-235B-A22B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5-VL,\ntitle = {Qwen2.5-VL},\nurl = {https://qwenlm.github.io/blog/qwen2.5-vl/},\nauthor = {Qwen Team},\nmonth = {January},\nyear = {2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "deepseek-ai/DeepSeek-V3.2-Exp-Base": "README.md exists but content is empty.",
    "inclusionAI/Ming-UniAudio-16B-A3B": "Ming-UniAudio\nIntroduction\nğŸ“Œ Updates\nKey Features\nEvaluation\nSpeech Understanding\nSpeech Generation\nModel & Benchmark Downloads\nUse Cases\nEnvironment Preparation\nInstallation with pip\nInstallation with docker\nExample Usage\nCitation\nMing-UniAudio\nğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope\nIntroduction\nMing-UniAudio is a novel framework that unifies speech understanding, generation, and editing. Its core is a unified continuous speech tokenizer that effectively unifies semantic and acoustic features within an end-to-end model. We developed a speech language model that strikes a balance between generation and understanding capabilities based on the unified continuous audio tokenizer. Leveraging this foundational model, which exhibits robust performance in both domains, we further trained a dedicated speech editing model built upon Ming-Lite-Omni. Crucially, Ming-UniAudio is the first to enable universal, free-form speech editing guided solely by natural language instructions, handling complex semantic and acoustic modifications without manual region specification.\nğŸ”¥ First unified continuous speech tokenizer for both understanding and generation tasks: MingTok-Audio\nğŸ”¥ First Speech LLM  with unifed continuous tokenizer for both understanding and generation: Ming-UniAudio\nğŸ”¥ First universal free-form speech editing model for various semantic and acoustic editing task without any temporal regime: Ming-UniAudio-Edit\nğŸ”¥ First benchmark for free-form speech editing: Ming-Freeform-Audio-Edit-Benchmark\nğŸ“Œ Updates\n[2025.09.30] ğŸ”¥ We release Ming-UniAudio with significant improvements across speech understanding, generation, and free-form editing tasks.\nKey Features\nMing-UniAudio features key optimizations as follows, compared to other audio-assisted LLMs:\nUnified Continuous Speech Tokenizer: Ming-UniAudio proposes a unified continuous speech tokenizer MingTok-Audio based on a VAE framework with a causal Transformer architecture, the first continuous speech tokenizer to effectively integrate semantic and acoustic features, and enables a closed-loop system with LLMs through hierarchical feature representations, makes it suitable for both understanding and generation tasks\nUnified Speech Language Model for Generation and Understanding: We pretrain an end-to-end unified speech language model with a single LLM backbone for both understanding and generation tasks, enhanced with a Diffusion Head to ensure high-fidelity speech synthesis.\nInstruction-Guided Free-Form Speech Editing: We introduce the first instruction-guided, free-form speech editing framework that supports comprehensive semantic and acoustic edits without requiring explicit edit regions, along with Ming-Freeform-Audio-Edit, the first open-source evaluation set for such tasks.\nEvaluation\nIn various benchmark tests, Ming-UniAudio demonstrates highly competitive results compared to industry-leading models of similar scale.\nSpeech Understanding\nASR performance comparison on various audio benchmark datasets. The best results are in bold.\nDatasets\nModel\nPerformance\naishell2-ios\nLS-clean\nHunan\nMinnan\nGuangyue\nChuanyu\nShanghai\nUnderstanding ASR\nKimi-Audio\n2.56\n1.28\n31.93\n80.28\n41.49\n6.69\n60.64\nQwen2.5 Omni\n2.75\n1.80\n29.31\n53.43\n10.39\n7.61\n32.05\nQwen2 Audio\n2.92\n1.60\n25.88\n123.78\n7.59\n7.77\n31.73\nMing-UniAudio-16B-A3B(ours)\n2.84\n1.62\n9.80\n16.50\n5.51\n5.46\n14.65\nSpeech Generation\nPerformance comparison on various audio benchmark datasets. The best results are in  bold.\nDatasets\nModel\nPerformance\nSeed-zh WER(%)\nSeed-zh SIM\nSeed-en WER(%)\nSeed-en SIM\nGeneration\nSeed-TTS\n1.12\n0.80\n2.25\n0.76\nMiMo-Audio\n1.96\n-\n5.37\n-\nQwen3-Omni-30B-A3B-Instruct\n1.07\n-\n1.39\n-\nMing-Omni-Lite\n1.69\n0.68\n4.31\n0.51\nMing-UniAudio-16B-A3B(ours)\n0.95\n0.70\n1.85\n0.58\nModel & Benchmark Downloads\nYou can download our latest model and Benchmark from both Huggingface and ModelScope.\nType\nModel\nInput modality\nOput modality\nDownload\nTokenizer\nMingTok-Audio\naudio\naudio\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nSpeechLLM\nMing-UniAudio-16B-A3B\naudio\naudio\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nSpeechLLM\nMing-UniAudio-16B-A3B-Edit\ntext, audio\ntext, audio\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nBenchmark\nMing-Freeform-Audio-Edit\n-\n-\nğŸ¤— HuggingFace ğŸ¤– ModelScope Eval tools\nIf you're in mainland China, we strongly recommend you to download our model from ğŸ¤– ModelScope.\npip install modelscope\nmodelscope download --model inclusionAI/Ming-UniAudio-16B-A3B --local_dir inclusionAI/Ming-UniAudio-16B-A3B  --revision master\nNote: This download process will take several minutes to several hours, depending on your network conditions.\nUse Cases\nAdditional demonstration cases are available on our project page.\nEnvironment Preparation\nInstallation with pip\npip install -r requirements.txt\nInstallation with docker\nYou can also initialize the environment by building the docker image. First clone this repository:\ngit clone --depth 1 https://github.com/inclusionAI/Ming-UniAudio\ncd Ming-UniAudio\nThen build the docker image with the provided Dockerfile in docker/docker-py310-cu121. This step might take a while:\ndocker build -t ming:py310-cu121 docker/docker-py310-cu121\nAt last, start the container with the current repo directory mounted:\ndocker run -it --gpus all -v \"$(pwd)\":/workspace/Ming-UniAudio ming:py310-cu121 ming:py310-cu121 /bin/bash\nYou can run the model with python interface. You may download the huggingface model in the repo directory first (.../Ming-UniAudio/) or mount the downloaded model path when starting the container.\nExample Usage\nWe provide a step-by-step running example:\nStep 1 - Download the source code\ngit clone\thttps://github.com/inclusionAI/Ming-UniAudio\ncd Ming-UniAudio\nStep 2 - Download the Ming-UniAudio model weights and create a soft link to the source code directory\nDownload our model following Model & Benchmark Downloads\nmkdir inclusionAI\nln -s /path/to/inclusionAI/Ming-UniAudio-16B-A3B inclusionAI/Ming-UniAudio-16B-A3B\nStep 3 - Enter the code directory, you can refer to the following codes to run the Ming-UniAudio model.\njupyter notebook cookbooks/demo.ipynb\nWe also provide a simple example on the usage of this repo. For detailed usage, please refer to demobook.ipynb.\nimport warnings\nimport torch\nfrom transformers import AutoProcessor\nfrom modeling_bailingmm import BailingMMNativeForConditionalGeneration\nimport random\nimport numpy as np\nfrom loguru import logger\ndef seed_everything(seed=1895):\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nseed_everything()\nwarnings.filterwarnings(\"ignore\")\nclass MingAudio:\ndef __init__(self, model_path, device=\"cuda:0\"):\nself.device = device\nself.model = BailingMMNativeForConditionalGeneration.from_pretrained(\nmodel_path,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n).eval().to(torch.bfloat16).to(self.device)\nself.processor = AutoProcessor.from_pretrained(\".\", trust_remote_code=True)\nself.tokenizer = self.processor.tokenizer\nself.sample_rate = self.processor.audio_processor.sample_rate\nself.patch_size = self.processor.audio_processor.patch_size\ndef speech_understanding(self, messages):\ntext = self.processor.apply_chat_template(messages, add_generation_prompt=True)\nimage_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\ninputs = self.processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\naudios=audio_inputs,\nreturn_tensors=\"pt\",\n).to(self.device)\nfor k in inputs.keys():\nif k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\ninputs[k] = inputs[k].to(dtype=torch.bfloat16)\nlogger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\ngenerated_ids = self.model.generate(\n**inputs,\nmax_new_tokens=512,\neos_token_id=self.processor.gen_terminator,\n)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = self.processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nreturn output_text\ndef speech_generation(\nself,\ntext,\nprompt_wav_path,\nprompt_text,\nlang='zh',\noutput_wav_path='out.wav'\n):\nwaveform = self.model.generate_tts(\ntext=text,\nprompt_wav_path=prompt_wav_path,\nprompt_text=prompt_text,\npatch_size=self.patch_size,\ntokenizer=self.tokenizer,\nlang=lang,\noutput_wav_path=output_wav_path,\nsample_rate=self.sample_rate,\ndevice=self.device\n)\nreturn waveform\ndef speech_edit(\nself,\nmessages,\noutput_wav_path='out.wav'\n):\ntext = self.processor.apply_chat_template(messages, add_generation_prompt=True)\nimage_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\ninputs = self.processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\naudios=audio_inputs,\nreturn_tensors=\"pt\",\n).to(self.device)\nans = torch.tensor([self.tokenizer.encode('<answer>')]).to(inputs['input_ids'].device)\ninputs['input_ids'] = torch.cat([inputs['input_ids'], ans], dim=1)\nattention_mask = inputs['attention_mask']\ninputs['attention_mask'] = torch.cat((attention_mask, attention_mask[:, :1]), dim=-1)\nfor k in inputs.keys():\nif k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\ninputs[k] = inputs[k].to(dtype=torch.bfloat16)\nlogger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\nedited_speech, edited_text = self.model.generate_edit(\n**inputs,\ntokenizer=self.tokenizer,\noutput_wav_path=output_wav_path\n)\nreturn edited_speech, edited_text\nif __name__ == \"__main__\":\nmodel = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B\")\n# ASR\nmessages = [\n{\n\"role\": \"HUMAN\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n},\n{\"type\": \"audio\", \"audio\": \"data/wavs/BAC009S0915W0292.wav\"},\n],\n},\n]\nresponse = model.speech_understanding(messages=messages)\nlogger.info(f\"Generated Response: {response}\")\n# TTS\nmodel.speech_generation(\ntext='æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯æ„å»ºæœªæ¥æœåŠ¡ä¸šçš„æ•°å­—åŒ–åŸºç¡€è®¾æ–½ï¼Œä¸ºä¸–ç•Œå¸¦æ¥æ›´å¤šå¾®å°è€Œç¾å¥½çš„æ”¹å˜ã€‚',\nprompt_wav_path='data/wavs/10002287-00000094.wav',\nprompt_text='åœ¨æ­¤å¥‰åŠå¤§å®¶åˆ«ä¹±æ‰“ç¾ç™½é’ˆã€‚',\n)\nNote: We test the examples on hardware of NVIDIA H800-80GB/H20-96G with CUDA 12.4.\nCitation\nIf you find our work helpful, feel free to give us a cite.",
    "Fortytwo-Network/Strand-Rust-Coder-14B-v1": "Strand-Rust-Coder-14B-v1\nOverview\nKey Features\nPerformance Summary\nTask Breakdown\nDataset\nTraining Configuration\nModel Architecture\nEvaluation Protocol\nWhy It Matters\nğŸ”¬ Research & References\nIntended Use\nLimitations\nIntegration with Fortytwo Network\nInference Examples\nUsing pipeline\nUsing Transformers Directly\nQuantized Versions\nStrand-Rust-Coder-14B-v1\nOverview\nStrand-Rust-Coder-14B-v1 is the first domain-specialized Rust language model created through Fortytwoâ€™s Swarm Inference, a decentralized AI architecture where multiple models collaboratively generate, validate, and rank outputs through peer consensus.\nThe model fine-tunes Qwen2.5-Coder-14B for Rust-specific programming tasks using a 191K-example synthetic dataset built via multi-model generation and peer-reviewed validation.It achieves 43â€“48% accuracy on Rust-specific benchmarks â€“ surpassing much larger proprietary models like GPT-5 Codex on Rust tasks â€“ while maintaining competitive general coding performance.\nKey Features\nRust-specialized fine-tuning on 15 diverse programming task categories\nPeer-validated synthetic dataset (191,008 verified examples, 94.3% compile rate)\nLoRA-based fine-tuning for efficient adaptation\nBenchmarked across Rust-specific suites:\nRustEvo^2\nEvaluation on Hold-Out Set\nDeployed in the Fortytwo decentralized inference network for collective AI reasoning\nPerformance Summary\nModel\nHold-Out Set\nRustEvo^2\nFortytwo-Rust-One-14B (Ours)\n48.00%\n43.00%\nopenai/gpt-5-codex\n47.00%\n28.00%\nanthropic/claude-sonnet-4.5\n46.00%\n21.00%\nanthropic/claude-3.7-sonnet\n42.00%\n31.00%\nqwen/qwen3-max\n42.00%\n40.00%\nqwen/qwen3-coder-plus\n41.00%\n22.00%\nx-ai/grok-4\n39.00%\n37.00%\ndeepseek/deepseek-v3.1-terminus\n37.00%\n33.00%\nQwen3-Coder-30B-A3B-Instruct\n36.00%\n20.00%\nopenai/gpt-4o-latest\n34.00%\n39.00%\ndeepseek/deepseek-chat\n34.00%\n41.00%\ngoogle/gemini-2.5-flash\n33.00%\n7.00%\nQwen2.5-Coder-14B-Instruct (Base)\n29.00%\n30.00%\nQwen2.5-Coder-32B-Instruct\n29.00%\n31.00%\ngoogle/gemini-2.5-pro\n28.00%\n22.00%\nqwen/qwen-2.5-72b\n28.00%\n32.00%\nTesslate/Tessa-Rust-T1-7B\n23.00%\n19.00%\nBenchmarks on code tasks measured using unit-test pass rate@1 in Docker-isolated Rust 1.86.0 environment.\nTask Breakdown\nTask\nBase\nStrand-14B\ntest_generation\n0.00\n0.51\napi_usage_prediction\n0.27\n0.71\nfunction_naming\n0.53\n0.87\ncode_refactoring\n0.04\n0.19â€“0.20\nvariable_naming\n0.87\n1.00\ncode_generation\n0.40\n0.49\nLargest improvements appear in test generation, API usage prediction, and refactoring â€“ areas demanding strong semantic reasoning about Rustâ€™s ownership and lifetime rules.\nDataset\nFortytwo-Network/Strandset-Rust-v1 (191,008 examples, 15 categories)Built through Fortytwoâ€™s Swarm Inference pipeline, where multiple SLMs generate and cross-validate examples with peer review consensus and output aggregation.\n94.3% compile success rate\n73.2% consensus acceptance\nCoverage of 89% of Rust language features\nTasks include:\ncode_generation, code_completion, bug_detection, refactoring, optimization\ndocstring_generation, code_review, summarization, test_generation\nnaming, API usage prediction, search\nDataset construction involved 2,383 crates from crates.io, automatic compilation tests, and semantic validation of ownership and lifetime correctness.\nDataset: Fortytwo-Network/Strandset-Rust-v1\nTraining Configuration\nSetting\nValue\nBase model\nQwen2.5-Coder-14B-Instruct\nMethod\nLoRA (r=64, Î±=16)\nLearning rate\n5e-5\nBatch size\n128\nEpochs\n3\nOptimizer\nAdamW\nPrecision\nbfloat16\nObjective\nCompletion-only loss\nContext length\n32,768\nFramework\nPyTorch + FSDP + Flash Attention 2\nHardware\n8Ã— H200 GPUs\nModel Architecture\nBase: Qwen2.5-Coder (14 B parameters, GQA attention, extended RoPE embeddings)\nTokenizer: 151 k vocabulary optimized for Rust syntax\nContext: 32 k tokens\nFine-tuning: Parameter-efficient LoRA adapters (â‰ˆ1% of parameters updated)\nDeployment: Compatible with local deployment and Fortytwo Capsule runtime for distributed swarm inference\nEvaluation Protocol\nAll evaluations executed in Docker-isolated Rust 1.86.0 environment\nCode tasks: measured via unit test pass rate\nDocumentation & naming tasks: scored via LLM-based correctness (Claude Sonnet 4 judge)\nCode completion & API tasks: syntax-weighted Levenshtein similarity\nComment generation: compilation success metric\nWhy It Matters\nRust is a high-safety, low-level language with complex ownership semantics that make it uniquely challenging for general-purpose LLMs.At the same time, there is simply not enough high-quality training data on Rust, as it remains a relatively modern and rapidly evolving language.This scarcity of large, reliable Rust datasets â€“ combined with the languageâ€™s intricate borrow checker and type system â€“ makes it an ideal benchmark for evaluating true model understanding and reasoning precision.\nStrand-Rust-Coder demonstrates how specialized models can outperform giant centralized models â€“ achieving domain mastery with a fraction of the compute.Through Fortytwoâ€™s Swarm Inference, the network was able to generate an extremely accurate synthetic dataset, enabling a state-of-the-art Rust model to be built through an efficient LoRA fine-tune rather than full retraining.\nThis work validates Fortytwoâ€™s thesis: intelligence can scale horizontally through networked specialization rather than centralized scale.\nğŸ”¬ Research & References\nSelf-Supervised Inference of Agents in Trustless Environments â€“ High-level overview of Fortytwo architecture\nIntended Use\nRust code generation, completion, and documentation\nAutomated refactoring and test generation\nIntegration into code copilots and multi-agent frameworks\nResearch on domain-specialized model training and evaluation\nLimitations\nMay underperform on purely algorithmic or multi-language tasks (e.g., HumanEval-style puzzles).\nNot suitable for generating unverified production code without compilation and test validation.\nIntegration with Fortytwo Network\nStrand-Rust-Coder models are integrated into Fortytwoâ€™s decentralized Swarm Inference Network, where specialized models collaborate and rank each otherâ€™s outputs.This structure enables peer-reviewed inference, improving reliability while reducing hallucinations and cost.\nTo run a Fortytwo node or contribute your own models and fine-tunes, visit: fortytwo.network\nInference Examples\nUsing pipeline\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"Fortytwo-Network/Strand-Rust-Coder-14B-v1\")\nmessages = [\n{\"role\": \"user\", \"content\": \"Write a Rust function that finds the first string longer than 10 characters in a vector.\"},\n]\npipe(messages)\nUsing Transformers Directly\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Fortytwo-Network/Strand-Rust-Coder-14B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Fortytwo-Network/Strand-Rust-Coder-14B-v1\")\nmessages = [\n{\"role\": \"user\", \"content\": \"Write a Rust function that finds the first string longer than 10 characters in a vector.\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=40)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\nQuantized Versions\nOptimized GGUF quantizations of Strand-Rust-Coder-14B-v1 are available for local and Fortytwo Node deployment, offering reduced memory footprint with minimal performance trade-off.\nThese builds are compatible with llama.cpp, Jan, LM Studio, Ollama, and other runtimes supporting the GGUF format.\nQuantization\nSize\nBit Precision\nDescription\nQ8_0\n15.7 GB\n8-bit\nNear-full precision, for most demanding local inference\nQ6_K\n12.1 GB\n6-bit\nBalanced performance and efficiency\nQ5_K_M\n10.5 GB\n5-bit\nLightweight deployment with strong accuracy retention\nQ4_K_M\n8.99 GB\n4-bit\nUltra-fast, compact variant for consumer GPUs and laptops\nQuant versions: Fortytwo-Network/Strand-Rust-Coder-14B-v1-GGUF\nFortytwo â€“ An open, networked intelligence shaped collectively by its participants\nJoin the swarm: fortytwo.network\nX: @fortytwonetwork",
    "malaysia-ai/Qwen3-1.7B-Multilingual-TTS": "Qwen3-1.7B-Multilingual-TTS\nHow to\nTTS\nVoice Conversion\nSource code\nQwen3-1.7B-Multilingual-TTS\nContinue pretraining Qwen/Qwen3-1.7B-Base on Multilingual Voice Conversion and TTS.\nUse neucodec as speech detokenizer, 50 TPS, output in 24k sample rate.\nMulti-speaker multilingual Voice Conversion up to 25.5B tokens.\nMulti-speaker multilingual TTS up to 5B tokens.\nFlash Attention 3 10k context length multipacking.\nLiger Kernel for swiglu, rms_norm and fused_linear_cross_entropy.\nWanDB at https://wandb.ai/huseinzol05/Qwen-Qwen3-1.7B-Base-multilingual-tts-neucodec\nStill on training, currently paused on training, waiting for my own pocket money to burn.\nHow to\nimport soundfile as sf\nimport torch\nimport torchaudio\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom neucodec import NeuCodec\nimport re\nmodel_name = \"malaysia-ai/Qwen3-1.7B-Multilingual-TTS\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\").to('cuda')\ncodec = NeuCodec.from_pretrained(\"neuphonic/neucodec\")\n_ = codec.eval().to('cuda')\nTTS\ntext = \"Hello! how come I help you? ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿà®µà®£à®•à¯à®•à®®à¯! à®¨à®¾à®©à¯ à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‰à®¤à®µà¯à®µà®¤à¯? Bonjour! Comment puis-je vous aider ? Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n? ã“ã‚“ã«ã¡ã¯ï¼ã©ã†ã—ã¦ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\"\nprompt = f\"<|im_start|>jenny_tts_dataset_audio_jenny: {text}<|speech_start|>\"\ninputs = tokenizer(prompt,return_tensors=\"pt\", add_special_tokens=True).to('cuda')\nwith torch.no_grad():\noutputs = model.generate(\n**inputs,\nmax_new_tokens=2048,\ndo_sample=True,\ntemperature=0.6,\nrepetition_penalty=1.15,\n)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\naudio_tokens = re.findall(r'<\\|s_(\\d+)\\|>', generated_text.split('<|speech_start|>')[1])\naudio_tokens = [int(token) for token in audio_tokens]\naudio_codes = torch.tensor(audio_tokens)[None, None]\nwith torch.no_grad():\naudio_waveform = codec.decode_code(audio_codes.cuda())\nsf.write('7-languages.mp3', audio_waveform[0, 0].cpu(), 24000)\nYou can check the audio 7-languages.mp3.\nYou can pick any speaker name from malaysia-ai/Multilingual-TTS.\nNot bad from 0.35 epoch model.\nVoice Conversion\nimport librosa\ny, sr = librosa.load('jenny.wav', sr = 16000)\nwith torch.no_grad():\ncodes = codec.encode_code(torch.tensor(y)[None, None])\ntokens = ''.join([f'<|s_{i}|>' for i in codes[0, 0]])\nprompt = f\"<|im_start|>I wonder if I shall ever be happy enough to have real lace on my clothes and bows on my caps.<|speech_start|>{tokens}<|im_end|><|im_start|>Hello, how come I help you, ä½ å¥½, æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—, à®µà®£à®•à¯à®•à®®à¯, à®¨à®¾à®©à¯ à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‰à®¤à®µà¯à®µà®¤à¯, bonjour, comment puis-je vous aider.<|speech_start|>\"\ninputs = tokenizer(prompt,return_tensors=\"pt\", add_special_tokens=True).to('cuda')\nwith torch.no_grad():\noutputs = model.generate(\n**inputs,\nmax_new_tokens=2048,\ndo_sample=True,\ntemperature=0.6,\nrepetition_penalty=1.15,\n)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\naudio_tokens = re.findall(r'<\\|s_(\\d+)\\|>', generated_text.split('<|speech_start|>')[-1])\naudio_tokens = [int(token) for token in audio_tokens]\naudio_codes = torch.tensor(audio_tokens)[None, None]\nwith torch.no_grad():\naudio_waveform = codec.decode_code(audio_codes.cuda())\nsf.write('jenny-4-languages.mp3', audio_waveform[0, 0].cpu(), 24000)\nYou can check the audio jenny-4-languages.mp3.\nSource code\nSource code at https://github.com/malaysia-ai/cooking/tree/main/qwen-tts",
    "inclusionAI/Ring-mini-linear-2.0-GPTQ-int4": "Quantized Ring-Linear-2.0\nIntroduction\nModel Downloads\nQuickstart\nğŸš€ vLLM\nEvaluation\nRing-mini-linear-2.0\nRing-flash-linear-2.0\nLicense\nCitation\nQuantized Ring-Linear-2.0\nIntroduction\nTo enable deployment of Ring-Linear-2.0 on memory-constrained devices, we release quantized weights using the GPTQ INT4 format. Additionally, we evaluate the online FP8 quantization performance of Ring-Linear-2.0 models, which closely approaches that of BF16 precision.\nModel Downloads\nModel\nMaximum Supported Length\nDownload\nRing-flash-linear-2.0-GPTQ-int4\n128k\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nRing-mini-linear-2.0-GPTQ-int4\n512k\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nQuickstart\nğŸš€ vLLM\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below.\nFirst, create a Conda environment with Python 3.10 and CUDA 12.8:\nconda create -n vllm python=3.10\nconda activate vllm\nNext, install our vLLM wheel package:\npip install https://media.githubusercontent.com/media/zheyishine/vllm_whl/refs/heads/main/vllm-0.8.5.post2.dev28%2Bgd327eed71.cu128-cp310-cp310-linux_x86_64.whl --force-reinstall\nFinally, install compatible versions of transformers after vLLM is installed:\npip install transformers==4.51.1\nOffline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nif __name__ == '__main__':\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ring-mini-linear-2.0-GPTQ-int4\")\nsampling_params = SamplingParams(temperature=0.6, top_p=1.0, max_tokens=16384)\n# use `max_num_seqs=1` without concurrency\nllm = LLM(model=\"inclusionAI/Ring-mini-linear-2.0-GPTQ-int4\", dtype='auto', enable_prefix_caching=False, max_num_seqs=128)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nfor output in outputs:\nprint(output.outputs[0].text)\nOnline Inference\nvllm serve inclusionAI/Ring-mini-linear-2.0-GPTQ-int4 \\\n--tensor-parallel-size 1 \\\n--pipeline-parallel-size 1 \\\n--gpu-memory-utilization 0.90 \\\n--max-num-seqs 128 \\\n--no-enable-prefix-caching\n--api-key your-api-key\nEvaluation\nWe evaluate the INT4 and FP8 quantized models using several datasets. The FP8 quantization is applied via the quantization=\"fp8\" argument in SGLang or vLLM.\nRing-mini-linear-2.0\nDataset\nBF16\nFP8\nGPTQ-Int4\nAIME25\n73.65\n72.40\n66.56\nAIME24\n79.95\n79.53\n74.95\nLiveCodeBench\n59.53\n58.42\n56.29\nGPQA\n65.69\n66.79\n62.53\nRing-flash-linear-2.0\nDataset\nBF16\nFP8\nGPTQ-Int4\nAIME25\n85.10\n84.22\n82.88\nLiveCodeBench\n69.82\n69.44\n66.14\nGPQA\n72.85\n72.95\n71.72\nLicense\nThis code repository is licensed under the MIT License.\nCitation\n@misc{lingteam2025attentionmattersefficienthybrid,\ntitle={Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning},\nauthor={Ling Team and Bin Han and Caizhi Tang and Chen Liang and Donghao Zhang and Fan Yuan and Feng Zhu and Jie Gao and Jingyu Hu and Longfei Li and Meng Li and Mingyang Zhang and Peijie Jiang and Peng Jiao and Qian Zhao and Qingyuan Yang and Wenbo Shen and Xinxing Yang and Yalin Zhang and Yankun Ren and Yao Zhao and Yibo Cao and Yixuan Sun and Yue Zhang and Yuchen Fang and Zibin Lin and Zixuan Cheng and Jun Zhou},\nyear={2025},\neprint={2510.19338},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.19338},\n}",
    "inclusionAI/Ring-flash-linear-2.0-GPTQ-int4": "Quantized Ring-Linear-2.0\nIntroduction\nModel Downloads\nQuickstart\nğŸš€ vLLM\nEvaluation\nRing-mini-linear-2.0\nRing-flash-linear-2.0\nLicense\nCitation\nQuantized Ring-Linear-2.0\nIntroduction\nTo enable deployment of Ring-Linear-2.0 on memory-constrained devices, we release quantized weights using the GPTQ INT4 format. Additionally, we evaluate the online FP8 quantization performance of Ring-Linear-2.0 models, which closely approaches that of BF16 precision.\nModel Downloads\nModel\nMaximum Supported Length\nDownload\nRing-flash-linear-2.0-GPTQ-int4\n128k\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nRing-mini-linear-2.0-GPTQ-int4\n512k\nğŸ¤— HuggingFace ğŸ¤– ModelScope\nQuickstart\nğŸš€ vLLM\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below.\nFirst, create a Conda environment with Python 3.10 and CUDA 12.8:\nconda create -n vllm python=3.10\nconda activate vllm\nNext, install our vLLM wheel package:\npip install https://media.githubusercontent.com/media/zheyishine/vllm_whl/refs/heads/main/vllm-0.8.5.post2.dev28%2Bgd327eed71.cu128-cp310-cp310-linux_x86_64.whl --force-reinstall\nFinally, install compatible versions of transformers after vLLM is installed:\npip install transformers==4.51.1\nOffline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nif __name__ == '__main__':\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ring-flash-linear-2.0-GPTQ-int4\")\nsampling_params = SamplingParams(temperature=0.6, top_p=1.0, max_tokens=16384)\n# use `max_num_seqs=1` without concurrency\nllm = LLM(model=\"inclusionAI/Ring-flash-linear-2.0-GPTQ-int4\", dtype='auto', enable_prefix_caching=False, max_num_seqs=128)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nfor output in outputs:\nprint(output.outputs[0].text)\nOnline Inference\nvllm serve inclusionAI/Ring-flash-linear-2.0-GPTQ-int4 \\\n--tensor-parallel-size 2 \\\n--pipeline-parallel-size 1 \\\n--gpu-memory-utilization 0.90 \\\n--max-num-seqs 128 \\\n--no-enable-prefix-caching\n--api-key your-api-key\nEvaluation\nWe evaluate the INT4 and FP8 quantized models using several datasets. The FP8 quantization is applied via the quantization=\"fp8\" argument in SGLang or vLLM.\nRing-mini-linear-2.0\nDataset\nBF16\nFP8\nGPTQ-Int4\nAIME25\n73.65\n72.40\n66.56\nAIME24\n79.95\n79.53\n74.95\nLiveCodeBench\n59.53\n58.42\n56.29\nGPQA\n65.69\n66.79\n62.53\nRing-flash-linear-2.0\nDataset\nBF16\nFP8\nGPTQ-Int4\nAIME25\n85.10\n84.22\n82.88\nLiveCodeBench\n69.82\n69.44\n66.14\nGPQA\n72.85\n72.95\n71.72\nLicense\nThis code repository is licensed under the MIT License.\nCitation\n@misc{lingteam2025attentionmattersefficienthybrid,\ntitle={Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning},\nauthor={Ling Team and Bin Han and Caizhi Tang and Chen Liang and Donghao Zhang and Fan Yuan and Feng Zhu and Jie Gao and Jingyu Hu and Longfei Li and Meng Li and Mingyang Zhang and Peijie Jiang and Peng Jiao and Qian Zhao and Qingyuan Yang and Wenbo Shen and Xinxing Yang and Yalin Zhang and Yankun Ren and Yao Zhao and Yibo Cao and Yixuan Sun and Yue Zhang and Yuchen Fang and Zibin Lin and Zixuan Cheng and Jun Zhou},\nyear={2025},\neprint={2510.19338},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.19338},\n}",
    "Vikhrmodels/Vistral-24B-Instruct-GGUF": "Vistral-24B-Instruct\nĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ\nVistral-24B-Instruct\nĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ\nVistral - ÑÑ‚Ğ¾ Ğ½Ğ°ÑˆĞ° Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„Ğ»Ğ°Ğ³Ğ¼Ğ°Ğ½ÑĞºĞ°Ñ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ LLM (Large Language Model) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ°Ñ Ğ¸Ğ· ÑĞµĞ±Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ mistralai/Mistral-Small-3.2-24B-Instruct-2506 ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹ VikhrModels, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ£Ğ´Ğ°Ğ»Ñ‘Ğ½ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ÑƒĞ±Ñ€Ğ°Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° \"MistralForCausalLM\" Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\nĞ’ĞµÑÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ½Ğ°ÑˆĞµĞ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ effective_llm_alignment Ğ½Ğ° GitHub, Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ğ½Ğ°ÑˆĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğµ Ğ½Ğ° HF.\nĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ½Ğ° Ğ½Ğ°ÑˆĞµĞ¼ ÑĞ°Ğ¹Ñ‚Ğµ Chat Vikhr\nHF Transformers Vikhrmodels/Vistral-24B-Instruct\n@inproceedings{nikolich2024vikhr,\ntitle={Vikhr: Advancing Open-Source Bilingual Instruction-Following Large Language Models for Russian and English},\nauthor={Aleksandr Nikolich and Konstantin Korolev and Sergei Bratchikov and Nikolay Kompanets and Igor Kiselev and Artem Shelmanov},\nbooktitle={Proceedings of the 4th Workshop on Multilingual Representation Learning (MRL) @ EMNLP-2024},\nyear={2024},\npublisher={Association for Computational Linguistics},\nurl={https://arxiv.org/pdf/2405.13929}\n}",
    "nexar-ai/BADAS-Open": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy accessing this model, you agree to use it responsibly and in compliance with applicable laws. This model is intended for research and development purposes in driver assistance systems.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nBADAS-Open: Ego-Centric Collision Prediction Model\nğŸ¯ Overview\nKey Features\nğŸ“Š Performance\nWhat Makes This Different?\nğŸš€ Quick Start\nInstallation\nBasic Usage\nğŸ—ï¸ Model Architecture\nğŸ—ï¸ Model Architecture\nğŸ“‹ Requirements\nğŸ“‚ Model Files\nğŸ”¬ Training Details\nğŸ¯ Use Cases\nâœ… Recommended Uses\nâš ï¸ Limitations\nğŸ“– Citation\nğŸ”— Resources\nğŸ¤ Model Variants\nâš–ï¸ License & Terms\nğŸ™ Acknowledgments\nBADAS-Open: Ego-Centric Collision Prediction Model\nğŸ¯ Overview\nBADAS-Open (V-JEPA2 Based Advanced Driver Assistance System) is a state-of-the-art collision prediction model specifically designed for ego-centric threat detection in real-world driving scenarios. Unlike traditional methods that detect any visible accident, BADAS focuses exclusively on collisions and near-misses that directly threaten the recording vehicle, dramatically reducing false alarms in real-world deployment.\nKey Features\nğŸ¯ Ego-Centric Focus: Distinguishes between ego-vehicle threats and irrelevant accidents\nğŸš— Real-World Trained: Trained on 1,500 real dashcam videos from actual driving scenarios\nâš¡ State-of-the-Art Performance: Outperforms academic methods and commercial ADAS systems\nğŸ§  Foundation Model Based: Built on V-JEPA2 for superior temporal understanding\nğŸ¬ Near-Miss Learning: Includes emergency maneuver scenarios for richer training signals\nğŸ“Š Performance\nBADAS-Open achieves state-of-the-art results across all major benchmarks when evaluated on ego-vehicle involved collisions:\nDataset\nAP â†‘\nAUC â†‘\nmTTA (s) â†“\nTest Size\nNexar\n0.86\n0.88\n4.9\n1,344\nDoTA\n0.94\n0.70\n4.0\n367\nDADA-2000\n0.87\n0.77\n4.3\n113\nDAD\n0.66\n0.87\n2.7\n116\nCompared to baseline methods (DSTA, UString) with AP scores of 0.06-0.53\nWhat Makes This Different?\nTraditional collision prediction models are trained to detect any accident in the camera's view, leading to excessive false alarms from irrelevant incidents (e.g., accidents in adjacent lanes). BADAS solves this by:\nEgo-Centric Reformulation: Only predicting collisions that directly threaten the ego vehicle\nReal-World Data: Trained on actual dashcam footage, not synthetic or staged scenarios\nConsensus-Based Timing: Alert times validated by 10 certified defensive driving experts\nNear-Miss Inclusion: Learning from successfully-avoided dangerous situations\nExample: BADAS prediction on real dashcam footage\nğŸš€ Quick Start\nInstallation\npip install torch torchvision transformers huggingface_hub opencv-python numpy pillow albumentations\nBasic Usage\nfrom huggingface_hub import hf_hub_download\nimport sys\nimport os\n# Download model loader\nloader_path = hf_hub_download(\nrepo_id=\"nexar-ai/badas-open\",\nfilename=\"badas_loader.py\"\n)\nsys.path.insert(0, os.path.dirname(loader_path))\n# Load model\nfrom badas_loader import load_badas_model\nmodel = load_badas_model()\n# Predict on video\npredictions = model.predict(\"dashcam_video.mp4\")\n# Get collision probability over time\nfor frame_idx, prob in enumerate(predictions):\nif prob > 0.8:  # High risk threshold\nprint(f\"âš ï¸ Collision risk at frame {frame_idx}: {prob:.2%}\")\nğŸ—ï¸ Model Architecture\nInput Video (16 frames @ 256Ã—256)\nâ†“\nV-JEPA2 Encoder (ViT-L)\nâ†“ (2048 patches Ã— 1024 dim)\nAttentive Probe Aggregation (12 queries Ã— 64 dim)\nâ†“\n3-Layer MLP Prediction Head\nâ†“\nCollision Probability [0, 1]\nğŸ—ï¸ Model Architecture\nBADAS architecture: V-JEPA2 backbone with attentive probe aggregation and MLP head\nKey Components:\nBackbone: V-JEPA2 (Vision Joint-Embedding Predictive Architecture v2)\nPatch Aggregation: Attentive probe with 12 learned queries\nHead: 3-layer MLP with GELU activation, LayerNorm, dropout 0.1\nTraining: End-to-end fine-tuning on Nexar dataset (1.5k videos)\nğŸ“‹ Requirements\nPython â‰¥ 3.8\nPyTorch â‰¥ 2.0\ntorchvision â‰¥ 0.15\ntransformers â‰¥ 4.30\nopencv-python â‰¥ 4.8\nhuggingface_hub â‰¥ 0.16\nğŸ“‚ Model Files\nbadas-open/\nâ”œâ”€â”€ model.safetensors         # Model weights\nâ”œâ”€â”€ config.json               # Model configuration\nâ”œâ”€â”€ badas_loader.py          # Loading utilities\nâ”œâ”€â”€ preprocessing.py          # Video preprocessing\nâ””â”€â”€ README.md                # This file\nğŸ”¬ Training Details\nDataset: Nexar Dashcam Collision Prediction Dataset (1,500 videos)\n750 collision/near-miss events\n750 normal driving samples\nOptimizer: AdamW (lr=1e-5, weight decay=1e-4)\nLoss: Binary Cross-Entropy\nAugmentations: Weather simulation (rain, snow), lighting variations, motion blur\nTraining Time: ~8 hours on 4Ã— A100 GPUs\nğŸ¯ Use Cases\nâœ… Recommended Uses\nDriver assistance system research\nAutonomous vehicle safety testing\nDashcam collision warning applications\nTraffic safety analysis\nDataset for training improved models\nâš ï¸ Limitations\nLong-tail events: Performance drops on rare categories (animals, motorcycles)\nMonocular only: Requires single-camera dashcam input\nProcessing delay: 16-frame buffer introduces latency\nNot for critical safety: Research model, not certified for production deployment\nğŸ“– Citation\nIf you use this model in your research, please cite:\n@article{goldshmidt2025badas,\ntitle={BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data},\nauthor={Goldshmidt, Roni and Scott, Hamish and Niccolini, Lorenzo and\nZhu, Shizhan and Moura, Daniel and Zvitia, Orly},\njournal={arXiv preprint},\nyear={2025}\n}\nğŸ”— Resources\nğŸ“„ Paper: arXiv (coming soon)\nğŸ—‚ï¸ Dataset: Nexar Collision Prediction Dataset\nğŸ† Challenge: Kaggle Competition\nğŸŒ Website: www.nexar-ai.com\nğŸ’» Code: GitHub Repository\nğŸ¤ Model Variants\nBADAS-Open (this model): Trained on 1.5k public videos\nBADAS-Pro: Commercial variant trained on 40k proprietary videos\nHigher performance (AP: 0.91 on Nexar)\nBetter edge-case handling\nContact Nexar for licensing\nâš–ï¸ License & Terms\nThis model is released under Apache 2.0 License with the following conditions:\nâœ… Free for research and commercial use\nâœ… Modification and redistribution permitted\nâš ï¸ No warranty provided - use at your own risk\nâš ï¸ Not certified for safety-critical applications\nğŸ“ Must provide attribution when using\nResponsible AI Notice: This model is intended to assist human drivers, not replace them. Always maintain full attention while driving.\nğŸ™ Acknowledgments\nV-JEPA2 foundation model by Meta AI Research\nNexar's driver community for dataset contribution\nRe-annotations of DAD, DADA-2000, and DoTA benchmarks\nQuestions or Issues?\nOpen an issue on our GitHub or contact us at research@nexar.com",
    "inclusionAI/Ming-UniVision-16B-A3B": "",
    "vanta-research/apollo-astralis-4b": "",
    "milli19/promptmii-llama-3.1-8b-instruct": "",
    "ubergarm/GLM-4.6-GGUF": "",
    "Agent-Ark/Toucan-Qwen2.5-7B-Instruct-v0.1": "",
    "KwaiVGI/CamCloneMaster-Wan2.1": "",
    "Danrisi/Qwen-image_SamsungCam_UltraReal": "",
    "prithivMLmods/Outfit-Cut-Specified": "",
    "huihui-ai/Huihui-granite-4.0-h-micro-abliterated": "",
    "LHL3341/Caco-CodeGen": "",
    "Atotti/Qwen3-Omni-AudioTransformer": "",
    "radicalnumerics/RND1-Base-0910": "",
    "t8star/comfyui-video-onekey": "",
    "mradermacher/apollo-astralis-4b-GGUF": "",
    "CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1": "",
    "bageldotcom/paris": "",
    "AgentFlow/agentflow-planner-3b": "",
    "abhi099k/ai-text-detector-v-n4.0": ""
}