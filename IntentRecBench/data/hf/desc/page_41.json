{
    "huihui-ai/Huihui-Mistral-Small-3.2-24B-Instruct-2506-abliterated-v2": "huihui-ai/Huihui-Mistral-Small-3.2-24B-Instruct-2506-abliterated-v2\nUsage\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Mistral-Small-3.2-24B-Instruct-2506-abliterated-v2\nThis is an uncensored version of mistralai/Mistral-Small-3.2-24B-Instruct-2506 created with abliteration (see remove-refusals-with-transformers to know more about it).This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.\nThis version is based on user requirements, utilizing a specific ablation dataset to test whether the ablation goal can be achieved, which is different from the previous version huihui-ai/Huihui-Mistral-Small-3.2-24B-Instruct-2506-abliterated.\nIt was only the text part that was processed, not the image part.\nUsage\nfrom datetime import datetime, timedelta\nimport torch\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nmodel_id = \"huihui-ai/Huihui-Mistral-Small-3.2-24B-Instruct-2506-abliterated-v2\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\nmodel_id, torch_dtype=torch.bfloat16\n)\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\ntokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))\ninput_ids = torch.tensor([tokenized.tokens])\nattention_mask = torch.ones_like(input_ids)\npixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0)\nimage_sizes = torch.tensor([pixel_values.shape[-2:]])\noutput = model.generate(\ninput_ids=input_ids,\nattention_mask=attention_mask,\npixel_values=pixel_values,\nimage_sizes=image_sizes,\nmax_new_tokens=1000,\n)[0]\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens) :])\nprint(decoded_output)\n# In this situation, you are playing a Pok√©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, Pok√© Balls, or Berries. Using an item could help you capture Pidgey or heal Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat Pidgey quickly.\n# 3. **POK√âMON**:\n#    - **Pros**: You might have another Pok√©mon in your party that is better suited for this battle or that you want to gain experience. Switching Pok√©mon could also be strategic if you want to train a lower-level Pok√©mon.\n#    - **Cons**: Switching Pok√©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n# 4. **RUN**:\n#    - **Pros**: Running away could be a quick way to avoid the battle altogether. This might be useful if you are trying to conserve resources or if you are in a hurry to get to another location.\n#    - **Cons**: Running away means you miss out on the experience points, items, or money that you could gain from defeating Pidgey. It also might not be the most efficient use of your time if you are trying to train your Pok√©mon.\n# ### Recommendation:\n# Given the significant level advantage, the best action to take is likely **FIGHT**. This will allow you to quickly defeat Pidgey and gain experience points for Pikachu. If you are concerned about Pikachu's health, you could use the **BAG** to heal Pikachu before or during the battle. Running away or switching Pok√©mon does not seem necessary in this situation.\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nIf you like it, please click 'like' and follow us for more updates.You can follow x.com/support_huihui to get the latest model information from huihui.ai.\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi (https://ko-fi.com/huihuiai)!",
    "mradermacher/Omega-Darker_The-Final-Directive-Longform-Stage2-ERP-12B-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/SuperbEmphasis/Omega-Darker_The-Final-Directive-Longform-Stage2-ERP-12B\nFor a convenient overview and download list, visit our model page for this model.\nstatic quants are available at https://huggingface.co/mradermacher/Omega-Darker_The-Final-Directive-Longform-Stage2-ERP-12B-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nimatrix\n0.1\nimatrix file (for creating your own qwuants)\nGGUF\ni1-IQ1_S\n3.1\nfor the desperate\nGGUF\ni1-IQ1_M\n3.3\nmostly desperate\nGGUF\ni1-IQ2_XXS\n3.7\nGGUF\ni1-IQ2_XS\n4.0\nGGUF\ni1-IQ2_S\n4.2\nGGUF\ni1-IQ2_M\n4.5\nGGUF\ni1-Q2_K_S\n4.6\nvery low quality\nGGUF\ni1-Q2_K\n4.9\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XXS\n5.0\nlower quality\nGGUF\ni1-IQ3_XS\n5.4\nGGUF\ni1-Q3_K_S\n5.6\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n5.7\nbeats Q3_K*\nGGUF\ni1-IQ3_M\n5.8\nGGUF\ni1-Q3_K_M\n6.2\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n6.7\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n6.8\nGGUF\ni1-Q4_0\n7.2\nfast, low quality\nGGUF\ni1-IQ4_NL\n7.2\nprefer IQ4_XS\nGGUF\ni1-Q4_K_S\n7.2\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n7.6\nfast, recommended\nGGUF\ni1-Q4_1\n7.9\nGGUF\ni1-Q5_K_S\n8.6\nGGUF\ni1-Q5_K_M\n8.8\nGGUF\ni1-Q6_K\n10.2\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "mistralai/Magistral-Small-2509-GGUF": "Magistral Small 1.2 (GGUF)\nUpdates compared with Magistral Small 1.1\nKey Features\nUsage\nInstall\nLaunch the servers\nUse the model\nTo make our models more accesible to everyone, this repo provides a basic GGUF checkpoint compatible with llama.cpp\nand mistral-common.\nIn addition to using this GGUF checkpoint, we encourage the community to use other GGUF variants, e.g.\nfrom Unsloth, LM Studio, ...\nIf you encounter any problems with the provided checkpoints here, please open a discussion or pull request.\nMagistral Small 1.2 (GGUF)\nBuilding upon Mistral Small 3.2 (2506), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.\nMagistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.\nThis is the GGUF version of the Magistral-Small-2509 model. We released the BF16 weights as well as the\nfollowing quantized format:\nQ8_0\nQ5_K_M\nQ4_K_M\nWe do not release alongside our GGUF files:\nAn official chat template. Instead, we recommend using mistral-common, which serves as our source of truth for tokenization and detokenization. Llama.cpp automatically loads a chat template, but it is in most likelihood incorrect for Magistral.\nThe vision encoder, since our recommended usage does not involve multimodality.\nUpdates compared with Magistral Small 1.1\nMultimodality: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision.\nPerformance upgrade: Magistral Small 1.2 should give you significatively better performance than Magistral Small 1.1 as seen in the benchmark results.\nBetter tone and persona: You should experiment better LaTeX and Markdown formatting, and shorter answers on easy general prompts.\nFinite generation: The model is less likely to enter infinite generation loops.\nSpecial think tokens: [THINK] and [/THINK] special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.\nReasoning prompt: The reasoning prompt is given in the system prompt.\nKey Features\nReasoning: Capable of long chains of reasoning traces before providing an answer.\nMultilingual: Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\nVision: Vision capabilities enable the model to analyze images and reason based on visual content in addition to text available with our main model Magistral-Small-2509.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 128k context window. Performance might degrade past 40k but Magistral should still give good results. Hence we recommend to leave the maximum model length to 128k and only lower if you encounter low performance.\nUsage\nWe recommend to use Magistral Small 1.2 GGUF with llama.cpp along with mistral-common >= 1.8.5 server. See here for the documentation of mistral-common server.\nThis recommended usage does not support vision.\nWe do not believe we can guarantee correct behavior using the integrated, stringified chat template hence\nmistral-common should be used as a reference. However, we strongly encourage the community members to use this GGUF checkpoint\nand mistral_common as a reference implementation to build a correct integrated, stringified chat template.\nInstall\nInstall llama.cpp following their guidelines.\nInstall mistral-common >= 1.8.5 with its dependencies.\npip install --upgrade mistral-common[server,hf-hub]\nDownload the weights from huggingface.\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"mistralai/Magistral-Small-2509-GGUF\" \\\n--include \"Magistral-Small-2509-Q4_K_M.gguf\" \\\n--local-dir \"mistralai/Magistral-Small-2509-GGUF/\"\nLaunch the servers\nLaunch the llama.cpp server\nllama-server -m mistralai/Magistral-Small-2509-GGUF/Magistral-Small-2509-Q4_K_M.gguf -c 0\nLaunch the mistral-common server and pass the url of the llama.cpp server.\nThis is the server that will handle tokenization and detokenization and call the llama.cpp server for generations.\nmistral_common serve mistralai/Magistral-Small-2509 \\\n--host localhost --port 6000 \\\n--engine-url http://localhost:8080 --engine-backend llama_cpp \\\n--timeout 300\nUse the model\nlet's define the function to call the servers:\ngenerate: call mistral-common that will tokenizer, call the llama.cpp server to generate new tokens and detokenize the output to an AssistantMessage with think chunk and tool calls parsed.\nfrom mistral_common.protocol.instruct.messages import AssistantMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.experimental.app.models import OpenAIChatCompletionRequest\nfrom fastapi.encoders import jsonable_encoder\nimport requests\nmistral_common_url = \"http://127.0.0.1:6000\"\ndef generate(\nrequest: dict | ChatCompletionRequest | OpenAIChatCompletionRequest, url: str\n) -> AssistantMessage:\nresponse = requests.post(\nf\"{url}/v1/chat/completions\", json=jsonable_encoder(request)\n)\nif response.status_code != 200:\nraise ValueError(f\"Error: {response.status_code} - {response.text}\")\nreturn AssistantMessage(**response.json())\nTokenize the input, call the model and detokenize\nfrom typing import Any\nfrom huggingface_hub import hf_hub_download\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nSYSTEM_PROMPT = load_system_prompt(\"mistralai/Magistral-Small-2509\", \"SYSTEM_PROMPT.txt\")\nquery = \"Use each number in 2,5,6,3 exactly once, along with any combination of +, -, √ó, √∑ (and parentheses for grouping), to make the number 24.\"\nmessages = [SYSTEM_PROMPT, {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": query}]}]\nrequest = {\"messages\": messages, \"temperature\": TEMP, \"top_p\": TOP_P, \"max_tokens\": MAX_TOK}\ngenerated_message = generate(request, mistral_common_url)\nprint(generated_message)",
    "Liontix/Qwen3-8B-Gemini-2.5-Pro-Distill-GGUF": "This model was trained on a Gemini 2.5 Flash (non-reasoning) dataset and a Gemini 2.5 Pro (reasoning) dataset. It is a reasoning model.\nYou can request distilled models or datasets here",
    "neuralcrew/neutrino-instruct": "üß† Neutrino-Instruct (7B)\n‚ú® Model Details\nüöÄ Quick Start\nRun with llama.cpp\nRun with Ollama\nRun in Python (llama-cpp-python)\nüìä System Requirements\nüß© Potential Use Cases\nüõ†Ô∏è Development Notes\nüìñ Citation\nüß† Neutrino-Instruct (7B)\nNeutrino-Instruct is a 7B parameter instruction-tuned LLM developed by Fardeen NB.It is designed for conversational AI, multi-step reasoning, and instruction-following tasks, fine-tuned to maintain coherent and contextual dialogue across multiple turns.\n‚ú® Model Details\nModel Name: Neutrino-Instruct\nDeveloper: Fardeen NB\nLicense: Apache-2.0\nLanguage(s): English\nFormat: GGUF (optimized for llama.cpp and Ollama)\nBase Model: Neutrino\nVersion: 2.0\nTask: Text Generation (chat, Q&A, instruction-following)\nüöÄ Quick Start\nRun with llama.cpp\n# Clone and build llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp && make\n# Run a single prompt\n./main -m ./neutrino-instruct.gguf -p \"Hello, who are you?\"\n# Run in interactive mode\n./main -m ./neutrino-instruct.gguf -i -p \"Let's chat.\"\n# Control output length\n./main -m ./neutrino-instruct.gguf -n 256 -p \"Write a poem about stars.\"\n# Change creativity (temperature)\n./main -m ./neutrino-instruct.gguf --temp 0.7 -p \"Explain quantum computing simply.\"\n# Enable GPU acceleration (if compiled with CUDA/Metal)\n./main -m ./neutrino-instruct.gguf --gpu-layers 50 -p \"Summarize this article.\"\nRun with Ollama\nollama run fardeen0424/neutrino\nRun in Python (llama-cpp-python)\nfrom llama_cpp import Llama\n# Load the Neutrino-Instruct model\nllm = Llama(model_path=\"./neutrino-instruct.gguf\")\n# Run inference\nresponse = llm(\"Who are you?\")\nprint(response[\"choices\"][0][\"text\"])\n# Stream output tokens\nfor token in llm(\"Tell me a story about Neutrino:\", stream=True):\nprint(token[\"choices\"][0][\"text\"], end=\"\", flush=True)\nüìä System Requirements\nCPU-only: 32‚Äì64GB RAM recommended (runs on modern laptops, slower inference).\nGPU acceleration:\n4GB VRAM ‚Üí 4-bit quantized (Q4) models\n8GB VRAM ‚Üí 5-bit/8-bit models\n12GB+ VRAM ‚Üí FP16 full precision\nüß© Potential Use Cases\nConversational AI assistants\nResearch prototypes\nInstruction-following agents\nChatbots with identity-awareness\n‚ö†Ô∏è Out of Scope: Use in critical decision-making, legal, or medical contexts.\nüõ†Ô∏è Development Notes\nModel uploaded in GGUF format for portability & performance.\nCompatible with llama.cpp, Ollama, and llama-cpp-python.\nSupports quantization levels (Q4, Q5, Q8) for deployment on resource-constrained devices.\nüìñ Citation\nIf you use Neutrino in your research or projects, please cite:\n@misc{fardeennb2025neutrino,\ntitle = {Neutrino-Instruct: A 7B Instruction-Tuned Conversational Model},\nauthor = {Fardeen NB},\nyear = {2025},\nhowpublished = {Hugging Face},\nurl = {https://huggingface.co/neuralcrew/neutrino-instruct}\n}",
    "AuroraZengfh/MambaIC": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nYAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nMambaIC\nlicense: mit\nMambaIC\nPaper: MambaIC: State Space Models for High-Performance Learned Image Compression\nCode: github.com/AuroraZengfh/MambaIC\nIf you find this work useful, consider citing our paper as follows:\n@inproceedings{zeng2025mambaic,\ntitle={MambaIC: State Space Models for High-Performance Learned Image Compression},\nauthor={Zeng, Fanhu and Tang, Hao and Shao, Yihua and Chen, Siyu and Shao, Ling and Wang, Yan},\nbooktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},\npages={18041--18050},\nyear={2025}\n}\nlicense: mit",
    "Alissonerdx/flux.1-dev-SRPO-LoRas": "Flux.1-Dev SRPO LoRAs\nFlux.1-Dev SRPO LoRAs\nThese LoRAs were extracted from three sources:\nthe original SRPO (Flux.1-Dev): tencent/SRPO\ncommunity checkpoint: rockerBOO/flux.1-dev-SRPO\ncommunity checkpoint (quantized/refined): wikeeyang/SRPO-Refine-Quantized-v1.0\nThey are designed to provide modular, lightweight adaptations you can mix with other LoRAs, reducing storage and enabling fast experimentation across ranks (8, 16, 32, 64, 128).\nNotes:\nThe Loras version for Nunchaku was converted using the official Nunchaku conversion tool but it is something experimental I still need to test and analyze the results, I do not recommend using it for now it is only for testing.\nThese loras allow you to use the quality of SRPO using the official flux dev as a base, without the need to use the base flux SRPO, that is, in my opinion, it is not very advantageous to use any of these loras + flux SRPO as a base, unless you want to apply the quality of, for example, SRPO RockerBOO in the base flux SRPO model.\nThe version I recommend is RockerBOO but I advise you to test the others, because the original version will give you different results than the other versions.\nAccording to some reports it seems to work well with Flux Krea, the report was with rank 256, I haven't tested it yet to confirm.\nExample comparison between Flux1-Dev baseline and LoRA extractions\nuse with üß®diffusers:\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\npipe.load_lora_weights('Alissonerdx/flux.1-dev-SRPO-LoRas', weight_name='srpo_128_base_R%26Q_model_fp16.safetensors')\npipe.to(\"cuda\")\nprompt = \"aiyouxiketang, a man in armor with a beard and a beard\"\nimage = pipe(\nprompt,\nnum_inference_steps=28,\nguidance_scale=5.0,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]",
    "InternRobotics/InternVLA-M1": "Model Card for InternVLA-M1\nDescription:\nCitation\nModel Card for InternVLA-M1\nDescription:\nInternVLA-M1 is an open-source, end-to-end vision‚Äìlanguage‚Äìaction (VLA) framework for building and researching generalist robot policies. The checkpoints in this repository were pretrained on the system2 dataset.\nüåê Homepage: InternVLA-M1 Project Page\nüíª Codebase: InternVLA-M1 GitHub Repo\nCitation\n@misc{internvla2024,\ntitle  = {InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy},\nauthor = {InternVLA-M1 Contributors},\nyear   = {2025},\nbooktitle={arXiv},\n}",
    "ibm-granite/granite-geospatial-ocean": "granite-geospatial-ocean\nArchitecture Overview\nHow to Get Started with the Model\nFeedback\nModel Card Authors\nAcknowledgments\nIBM Public Repository Disclosure:\nCitation\ngranite-geospatial-ocean\nThe granite-geospatial-ocean foundation model was jointly developed by IBM and STFC as part of a collaboration with the University of Exeter and Plymouth Marine Lab under the UK HNCDI programme. This pre-trained model supports a range of potential use cases in ocean ecosystem health, fisheries management, pollution and other ocean processes that can be monitored using ocean colour observations. We provide an example to fine tune the model to quantify primary production by phytoplankton (carbon sequestration which determine's the ocean's role in climate change). For full details of the model pre-training, fine-tuning and evaluation, please see the preprint paper.\nArchitecture Overview\nThe granite-geospatial-ocean model is a transformer-based geospatial foundation model trained on Sentinel-3 Ocean Land Colour Instrument (OLCI) and Sea and Land Surface Temperature Radiometer (SLSTR) images. The model consists of a self-supervised encoder developed with a ViT architecture and Masked AutoEncoder (MAE) learning strategy, with an MSE loss function and follows the same architecture as Prithvi-EO.\nWe used a 42x42 image size and 16 bands of Level-2 sentinel-3 OLCI(OL1 to OL12, OL16, OL17, OL18 and OL21) and also a further band of Level-2 SLSTR sea surface temperature data were in the pre-training. In total of 512,000 images were used for pre-training.\nHow to Get Started with the Model\nWe have provided an example of fine-tuning the model for primary production quantification which can be found here. These examples make use of TerraTorch for fine-tuning and prediction.\nExample Notebooks:\nPrimary Production Quantification >>Try it on Colab<< (Choose T4 GPU runtime)\nFeedback\nYour feedback is invaluable to us. If you have any feedback about the model, please feel free to share it with us. You can do this by starting a discussion in this HF repository or submitting an issue to TerraTorch on GitHub.\nModel Card Authors\nGeoffrey Dawson, Remy Vandaele, Andrew Taylor, David Moffat, Helen Tamura-Wicks, Sarah Jackson, Chunbo Luo, Paolo Fraccaro, Hywel Williams, Rosie Lickorish and Anne Jones\nAcknowledgments\nThis work was supported by the Hartree National Centre for Digital Innovation, a collaboration between STFC and IBM.\nIBM Public Repository Disclosure:\nAll content in this repository including code has been provided by IBM under the associated open source software license and IBM is under no obligation to provide enhancements, updates, or support. IBM developers produced this code as an open source project (not as an IBM product), and IBM makes no assertions as to the level of quality nor security, and will not be maintaining this code going forward.\nCitation\nIf this model helped your research, please cite Granite-geospatial-ocean-Preprint in your publications.\n@article{Granite-geospatial-ocean-Preprint,\nauthor          = {Dawson, Geoffrey and Vandaele, Remy and Taylor, Andrew and Moffat, David and Tamura-Wicks, Helen and Jackson, Sarah and Lickorish, Rosie and Fraccaroa, Paolo and Williams, Hywel and Luo, Chunbo and Jones, Anne},\nmonth           = Sept,\ntitle           = {{A Sentinel-3 foundation model for ocean colour}},\njournal         = {Preprint Available on arxiv: https://arxiv.org/abs/2509.21273},\nyear            = {2025}\n}",
    "ibm-granite/granite-4.0-h-tiny": "Granite-4.0-H-Tiny\nGranite-4.0-H-Tiny\nüì£ Update [10-07-2025]: Added a default system prompt to the chat template to guide the model towards more professional, accurate, and safe responses.\nModel Summary:\nGranite-4.0-H-Tiny is a 7B parameter long-context instruct model finetuned from Granite-4.0-H-Tiny-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. This model is developed using a diverse set of techniques with a structured chat format, including supervised finetuning, model alignment using reinforcement learning, and model merging. Granite 4.0 instruct models feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.\nDevelopers: Granite Team, IBM\nHF Collection: Granite 4.0 Language Models HF Collection\nGitHub Repository: ibm-granite/granite-4.0-language-models\nWebsite: Granite Docs\nRelease Date: October 2nd, 2025\nLicense: Apache 2.0\nSupported Languages:\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages.\nIntended use:\nThe model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications.\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nFill-In-the-Middle (FIM) code completions\nGeneration:\nThis is a simple example of how to use Granite-4.0-H-Tiny model.\nInstall the following libraries:\npip install torch torchvision torchaudio\npip install accelerate\npip install transformers\nThen, copy the snippet from the section that is relevant for your use case.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\"\nmodel_path = \"ibm-granite/granite-4.0-h-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()\n# change input text as desired\nchat = [\n{ \"role\": \"user\", \"content\": \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\" },\n]\nchat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens,\nmax_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output[0])\nExpected output:\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant. Please ensure responses are professional, accurate, and safe.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Please list one IBM Research laboratory located in the United States. You should only output its name and location.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>Almaden Research Center, San Jose, California<|end_of_text|>\nTool-calling:\nGranite-4.0-H-Tiny comes with enhanced tool calling capabilities, enabling seamless integration with external functions and APIs. To define a list of  tools please follow OpenAI's function definition schema.\nThis is an example of how to use Granite-4.0-H-Tiny model tool-calling ability:\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather for a specified city.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"Name of the city\"\n}\n},\n\"required\": [\"city\"]\n}\n}\n}\n]\n# change input text as desired\nchat = [\n{ \"role\": \"user\", \"content\": \"What's the weather like in Boston right now?\" },\n]\nchat = tokenizer.apply_chat_template(chat, \\\ntokenize=False, \\\ntools=tools, \\\nadd_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens,\nmax_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output[0])\nExpected output:\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following tools. You may call one or more tools to assist with the user query.\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather for a specified city.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"Name of the city\"}}, \"required\": [\"city\"]}}}\n</tools>\nFor each tool call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call>. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>What's the weather like in Boston right now?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|><tool_call>\n{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Boston\"}}\n</tool_call><|end_of_text|>\nEvaluation Results:\nBenchmarks\nMetric\nMicro Dense\nH Micro Dense\nH Tiny MoE\nH Small MoE\nGeneral Tasks\nMMLU\n5-shot\n65.98\n67.43\n68.65\n78.44\nMMLU-Pro\n5-shot, CoT\n44.5\n43.48\n44.94\n55.47\nBBH\n3-shot, CoT\n72.48\n69.36\n66.34\n81.62\nAGI EVAL\n0-shot, CoT\n64.29\n59\n62.15\n70.63\nGPQA\n0-shot, CoT\n30.14\n32.15\n32.59\n40.63\nAlignment Tasks\nAlpacaEval 2.0\n29.49\n31.49\n30.61\n42.48\nIFEval\nInstruct, Strict\n85.5\n86.94\n84.78\n89.87\nIFEval\nPrompt, Strict\n79.12\n81.71\n78.1\n85.22\nIFEval\nAverage\n82.31\n84.32\n81.44\n87.55\nArenaHard\n25.84\n36.15\n35.75\n46.48\nMath Tasks\nGSM8K\n8-shot\n85.45\n81.35\n84.69\n87.27\nGSM8K Symbolic\n8-shot\n79.82\n77.5\n81.1\n87.38\nMinerva Math\n0-shot, CoT\n62.06\n66.44\n69.64\n74\nDeepMind Math\n0-shot, CoT\n44.56\n43.83\n49.92\n59.33\nCode Tasks\nHumanEval\npass@1\n80\n81\n83\n88\nHumanEval+\npass@1\n72\n75\n76\n83\nMBPP\npass@1\n72\n73\n80\n84\nMBPP+\npass@1\n64\n64\n69\n71\nCRUXEval-O\npass@1\n41.5\n41.25\n39.63\n50.25\nBigCodeBench\npass@1\n39.21\n37.9\n41.06\n46.23\nTool Calling Tasks\nBFCL v3\n59.98\n57.56\n57.65\n64.69\nMultilingual Tasks\nMULTIPLE\npass@1\n49.21\n49.46\n55.83\n57.37\nMMMLU\n5-shot\n55.14\n55.19\n61.87\n69.69\nINCLUDE\n5-shot\n51.62\n50.51\n53.12\n63.97\nMGSM\n8-shot\n28.56\n44.48\n45.36\n38.72\nSafety\nSALAD-Bench\n97.06\n96.28\n97.77\n97.3\nAttaQ\n86.05\n84.44\n86.61\n86.64\nMultilingual Benchmarks and thr included languages:\nBenchmarks\n# Langs\nLanguages\nMMMLU\n11\nar, de, en, es, fr, ja, ko, pt, zh, bn, hi\nINCLUDE\n14\nhi, bn, ta, te, ar, de, es, fr, it, ja, ko, nl, pt, zh\nMGSM\n5\nen, es, fr, ja, zh\nModel Architecture:\nGranite-4.0-H-Tiny baseline is built on a decoder-only MoE transformer architecture. Core components of this architecture are: GQA, Mamba2, MoEs with shared experts, SwiGLU activation, RMSNorm, and shared input/output embeddings.\nModel\nMicro Dense\nH Micro Dense\nH Tiny MoE\nH Small MoE\nEmbedding size\n2560\n2048\n1536\n4096\nNumber of layers\n40 attention\n4 attention / 36 Mamba2\n4 attention / 36 Mamba2\n4 attention / 36 Mamba2\nAttention head size\n64\n64\n128\n128\nNumber of attention heads\n40\n32\n12\n32\nNumber of KV heads\n8\n8\n4\n8\nMamba2 state size\n-\n128\n128\n128\nNumber of Mamba2 heads\n-\n64\n48\n128\nMLP / Shared expert hidden size\n8192\n8192\n1024\n1536\nNum. Experts\n-\n-\n64\n72\nNum. active Experts\n-\n-\n6\n10\nExpert hidden size\n-\n-\n512\n768\nMLP activation\nSwiGLU\nSwiGLU\nSwiGLU\nSwiGLU\nSequence length\n128K\n128K\n128K\n128K\nPosition embedding\nRoPE\nNoPE\nNoPE\nNoPE\n# Parameters\n3B\n3B\n7B\n32B\n# Active parameters\n3B\n3B\n1B\n9B\nTraining Data:\nOverall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities, and (3) a select set of human-curated data.\nInfrastructure:\nWe trained the Granite 4.0 Language Models utilizing an NVIDIA GB200 NVL72 cluster hosted in CoreWeave. Intra-rack communication occurs via the 72-GPU NVLink domain, and a non-blocking, full Fat-Tree NDR 400 Gb/s InfiniBand network provides inter-rack communication. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\nEthical Considerations and Limitations:\nGranite 4.0 Instruction Models are primarily finetuned using instruction-response pairs mostly in English, but also multilingual data covering multiple languages. Although this model can handle multilingual dialog use cases, its performance might not be similar to English tasks. In such case, introducing a small number of examples (few-shot) can help the model in generating more accurate outputs. While this model has been aligned by keeping safety in consideration, the model may in some cases produce inaccurate, biased, or unsafe responses to user prompts. So we urge the community to use this model with proper safety testing and tuning tailored for their specific tasks.\nResources\n‚≠êÔ∏è Learn about the latest updates with Granite: https://www.ibm.com/granite\nüìÑ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources",
    "Cseti/VibeVoice_7B_Diffusion-head-LoRA_Hungarian-CV17": "VibeVoice_7B_Diffusion-head-LoRA_Hungarian-CV17\nInference\nExamples\nVibeVoice_7B_Diffusion-head-LoRA_Hungarian-CV17\nThis is a VibeVoice 7B (Large) model LoRA finetune on a Hungarian audio dataset.\nFor this particular test I used the CommonVoice 17.0 dataset's Hungarian config's train split.\nTo finetune the model I used the following code base.\nThank you for JPGallegoar for that amazing VibeVoice trainer!\nInference\nTo use the LoRA model you can use my modified fork\nuntil the following PR\nwill be merged into the main branch of VibeVoice Community's repository.\nExamples\nVoice without LoRA\nVoice WITH LoRA",
    "ostris/qwen_image_detail_slider": "Qwen Image - Detail Slider\nModel description\nDownload model\nQwen Image - Detail Slider\nPrompt\na horse is a DJ at a night club, fish eye lens, smoke machine, lazer lights, holding a martini\nModel description\nThis LoRA allows you to adjust the level of detail in an image by adjusting the strength. You can use -1.0 to 1.0 to reduce or increase the amount of detail added to the image. . This LoRA was trained while filming a tutorial Train a Concept Slider LoRA with AI Toolkit. Check out that video for more info.\nDownload model\nDownload them in the Files & versions tab.",
    "jinaai/jina-reranker-v3": "jina-reranker-v3: Listwise Document Reranker for SOTA Multilingual Retrieval\nUsage\nLocal Inference\nAPI\nCitation\nLicense\njina-reranker-v3: Listwise Document Reranker for SOTA Multilingual Retrieval\nBlog | API | AWS | Azure | GCP | Arxiv\nGGUF with quantizations and MLX versions are now available.\njina-reranker-v3 is a 0.6B parameter multilingual document reranker with a novel last but not late interaction architecture. Unlike ColBERT's separate encoding with multi-vector matching, this model performs causal self-attention between query and documents within the same context window, extracting contextual embeddings from the last token of each document.\nBuilt on Qwen3-0.6B with 28 transformer layers and a lightweight MLP projector (1024‚Üí512‚Üí256), it processes up to 64 documents simultaneously within 131K token context. The model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being 10√ó smaller than generative listwise rerankers.\nModel\nSize\nBEIR\nMIRACL\nMKQA\nCoIR\njina-reranker-v3\n0.6B\n61.94\n66.83\n67.92\n70.64\njina-reranker-v2\n0.3B\n57.06\n63.65\n67.90\n56.14\njina-reranker-m0\n2.4B\n58.95\n66.75\n68.19\n63.55\nbge-reranker-v2-m3\n0.6B\n56.51\n69.32\n67.88\n36.28\nmxbai-rerank-base-v2\n0.5B\n58.40\n55.32\n64.24\n65.71\nmxbai-rerank-large-v2\n1.5B\n61.44\n57.94\n67.06\n70.87\nQwen3-Reranker-0.6B\n0.6B\n56.28\n57.70\n65.34\n65.18\nQwen3-Reranker-4B\n4.0B\n61.16\n67.52\n67.52\n73.91\njina-code-embeddings-0.5b\n0.5B\n-\n-\n-\n73.94\nUsage\nLocal Inference\nUse transformers for local inference:\nInstallation:\npip install transformers\nLoad the model:\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\n'jinaai/jina-reranker-v3',\ndtype=\"auto\",\ntrust_remote_code=True,\n)\nmodel.eval()\nRank documents:\nquery = \"What are the health benefits of green tea?\"\ndocuments = [\n\"Green tea contains antioxidants called catechins that may help reduce inflammation and protect cells from damage.\",\n\"El precio del caf√© ha aumentado un 20% este a√±o debido a problemas en la cadena de suministro.\",\n\"Studies show that drinking green tea regularly can improve brain function and boost metabolism.\",\n\"Basketball is one of the most popular sports in the United States.\",\n\"ÁªøËå∂ÂØåÂê´ÂÑøËå∂Á¥†Á≠âÊäóÊ∞ßÂåñÂâÇÔºåÂèØ‰ª•Èôç‰ΩéÂøÉËÑèÁóÖÈ£éÈô©ÔºåËøòÊúâÂä©‰∫éÊéßÂà∂‰ΩìÈáç„ÄÇ\",\n\"Le th√© vert est riche en antioxydants et peut am√©liorer la fonction c√©r√©brale.\",\n]\n# Rerank documents\nresults = model.rerank(query, documents)\n# Results are sorted by relevance score (highest first)\nfor result in results:\nprint(f\"Score: {result['relevance_score']:.4f}\")\nprint(f\"Document: {result['document'][:100]}...\")\nprint()\n# Output:\n# Score: 0.2976\n# Document: Green tea contains antioxidants called catechins that may help reduce inflammation and protect ce...\n#\n# Score: 0.2258\n# Document: ÁªøËå∂ÂØåÂê´ÂÑøËå∂Á¥†Á≠âÊäóÊ∞ßÂåñÂâÇÔºåÂèØ‰ª•Èôç‰ΩéÂøÉËÑèÁóÖÈ£éÈô©ÔºåËøòÊúâÂä©‰∫éÊéßÂà∂‰ΩìÈáç„ÄÇ\n#\n# Score: 0.1911\n# Document: Studies show that drinking green tea regularly can improve brain function and boost metabolism.\n#\n# Score: 0.1640\n# Document: Le th√© vert est riche en antioxydants et peut am√©liorer la fonction c√©r√©brale.\nAPI Reference:\nmodel.rerank(\nquery: str,                      # Search query\ndocuments: List[str],            # Documents to rank\ntop_n: Optional[int] = None,     # Return only top N (default: all)\nreturn_embeddings: bool = False, # Include doc embeddings (default: False)\n)\nReturns: List of dicts with keys:\ndocument: Original document text\nrelevance_score: Float score (higher = more relevant)\nindex: Position in input documents list\nembedding: Document embedding (if return_embeddings=True)\nExample with options:\n# Get only top 3 results\ntop_results = model.rerank(query, documents, top_n=3)\n# Get embeddings for further processing\nresults_with_embeddings = model.rerank(query, documents, return_embeddings=True)\nAPI\nUse Jina AI's Reranker API for the fastest integration:\ncurl -X POST \\\nhttps://api.jina.ai/v1/rerank \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer JINA_API_KEY\" \\\n-d '{\n\"model\": \"jina-reranker-v3\",\n\"query\": \"slm markdown\",\n\"documents\": [\n...\n],\n\"return_documents\": false\n}'\nResponse format:\n{\n\"model\":\"jina-reranker-v3\",\n\"usage\": {\n\"total_tokens\":2813\n},\n\"results\":[\n{\n\"index\":1,\n\"relevance_score\":0.9310624287463884\n},\n{\n\"index\":4,\n\"relevance_score\":0.8982678574191957\n},\n{\n\"index\":0,\n\"relevance_score\":0.890233167219021\n},\n...\n]\n}\nCitation\nIf you find jina-reranker-v3 useful in your research, please cite our technical report:\n@misc{wang2025jinarerankerv3lateinteractiondocument,\ntitle={jina-reranker-v3: Last but Not Late Interaction for Document Reranking},\nauthor={Feng Wang and Yuqing Li and Han Xiao},\nyear={2025},\neprint={2509.25085},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2509.25085},\n}\nLicense\njina-reranker-v3 is listed on AWS & Azure. If you need to use it beyond those platforms or on-premises within your company, note that the model is licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to contact us.",
    "WeightedAI/Persian_OCR": "Persian-OCR\nü§ù Contributing\nüì¨ Contact\nFiles\nInstallation\nUsage\nPersian-OCR\nPersian-OCR is a deep learning model for Optical Character Recognition (OCR), designed specifically for Persian text.The model employs a CNN + Transformer architecture trained with CTC loss to extract text from images.\nThe model was trained on a custom dataset of approximately 600,000 synthetic Persian text images.These images were generated from Wikipedia text using 49 different Persian fonts, with sequence lengths ranging from 0 to 150 characters.\nOn this dataset, the model achieves a sequence accuracy of 96%.\nThe model may benefit from further fine-tuning on real-world data, and contributions or collaborations are warmly welcomed.\nü§ù Contributing\nContributions are welcome! If you have a dataset of real-world Persian text or improvements to the model, please open an issue or submit a pull request.\nüì¨ Contact\nFor collaboration or inquiries, please reach out via farbodpya@gmail.com\nFiles\npytorch_model.bin : PyTorch model weights\nvocab.json : Character vocabulary\nmodel.py : Python script defining the CNN + Transformer OCR model\nutils.py : Utility functions for OCR, including ocr_page and load_vocab\nconfig.json : Model configuration\nInstallation\npip install torch torchvision huggingface_hub\nUsage\nimport torch\nimport json\nimport sys\nimport importlib.util\nfrom huggingface_hub import hf_hub_download\n# 1Ô∏è‚É£ Load vocab\nvocab_path = hf_hub_download(\"farbodpya/Persian-OCR\", \"vocab.json\")\nwith open(vocab_path, \"r\", encoding=\"utf-8\") as f:\nvocab = json.load(f)\nidx_to_char = {int(k): v for k, v in vocab[\"idx_to_char\"].items()}\n# 2Ô∏è‚É£ Import model.py\nmodel_file = hf_hub_download(\"farbodpya/Persian-OCR\", \"model.py\")\nspec_model = importlib.util.spec_from_file_location(\"model\", model_file)\nmodel_module = importlib.util.module_from_spec(spec_model)\nsys.modules[\"model\"] = model_module\nspec_model.loader.exec_module(model_module)\nfrom model import CNN_Transformer_OCR\n# 3Ô∏è‚É£ Import utils.py\nutils_file = hf_hub_download(\"farbodpya/Persian-OCR\", \"utils.py\")\nspec_utils = importlib.util.spec_from_file_location(\"utils\", utils_file)\nutils_module = importlib.util.module_from_spec(spec_utils)\nsys.modules[\"utils\"] = utils_module\nspec_utils.loader.exec_module(utils_module)\nfrom utils import ocr_page\n# 4Ô∏è‚É£ Load model weights\nweights_path = hf_hub_download(\"farbodpya/Persian-OCR\", \"pytorch_model.bin\")\nmodel = CNN_Transformer_OCR(num_classes=len(idx_to_char)+1)\nmodel.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\nmodel.eval()\n# 5Ô∏è‚É£ Run OCR on an image\nimg_path = \"sample.png\"  # replace with your own image\ntext = ocr_page(img_path, model, idx_to_char)\nprint(\"\\n=== Final OCR Page ===\\n\", text)",
    "malcolmrey/wan": "README.md exists but content is empty.",
    "MongoDB/mdbr-leaf-mt-asym": "Content\nIntroduction\nTechnical Report\nHighlights\nBenchmark Comparison\nQuickstart\nSentence Transformers\nTransformers Usage\nAsymmetric Retrieval Setup\nMRL Truncation\nVector Quantization\nEvaluation\nCitation\nLicense\nContact\nAcknowledgments\nMongoDB/mdbr-leaf-mt-asym\nContent\nIntroduction\nTechnical Report\nHighlights\nBenchmarks\nQuickstart\nCitation\nIntroduction\nmdbr-leaf-mt-asym is a high-performance text embedding model designed for classification, clustering, semantic sentence similarity and summarization tasks.\nThis model is the asymmetric variant of mdbr-leaf-mt, which uses MongoDB/mdbr-leaf-mt for queries and mixedbread-ai/mxbai-embed-large-v1 for documents.\nThe model is robust to vector quantization and MRL truncation.\nIf you are looking to perform semantic search / information retrieval (e.g. for RAGs), please check out our mdbr-leaf-ir model, which is specifically trained for these tasks.\nNote: this model has been developed by the ML team of MongoDB Research. At the time of writing it is not used in any of MongoDB's commercial product or service offerings.\nTechnical Report\nA technical report detailing our proposed LEAF training procedure is available here.\nHighlights\nState-of-the-Art Performance: mdbr-leaf-mt-asym achieves state-of-the-art results for compact embedding models, ranking #1 on the public MTEB v2 (Eng) leaderboard for models with ‚â§30M parameters.\nFlexible Architecture Support: mdbr-leaf-mt-asym uses an asymmetric retrieval architecture enabling even greater retrieval results.\nMRL and Quantization Support: embedding vectors generated by mdbr-leaf-mt-asym compress well when truncated (MRL) and can be stored using more efficient types like int8 and binary.  See below for more information.\nBenchmark Comparison\nThe table below shows the scores for mdbr-leaf-mt on the MTEB v2 (English) benchmark, compared to other retrieval models.\nmdbr-leaf-mt ranks #1 on this benchmark for models with <30M parameters.\nModel\nSize\nMTEB v2 (Eng)\nOpenAI text-embedding-3-large\nUnknown\n66.43\nOpenAI text-embedding-3-small\nUnknown\n64.56\nmdbr-leaf-mt\n23M\n63.97\ngte-small\n33M\n63.22\nsnowflake-arctic-embed-s\n32M\n61.59\ne5-small-v2\n33M\n61.32\ngranite-embedding-small-english-r2\n47M\n61.07\nall-MiniLM-L6-v2\n22M\n59.03\nQuickstart\nSentence Transformers\nfrom sentence_transformers import SentenceTransformer\n# Load the model\nmodel = SentenceTransformer(\"MongoDB/mdbr-leaf-mt-asym\")\n# Example queries and documents\nqueries = [\n\"What is machine learning?\",\n\"How does neural network training work?\",\n]\ndocuments = [\n\"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n\"Neural networks are trained through backpropagation, adjusting weights to minimize prediction errors.\",\n]\n# Encode queries and documents\nquery_embeddings = model.encode_query(queries)\ndocument_embeddings = model.encode_document(documents)\n# Compute similarity scores\nscores = model.similarity(query_embeddings, document_embeddings)\n# Print results\nfor i, query in enumerate(queries):\nprint(f\"Query: {query}\")\nfor j, doc in enumerate(documents):\nprint(f\" Similarity: {scores[i, j]:.4f} | Document {j}: {doc[:80]}...\")\nSee example output\nQuery: What is machine learning?\nSimilarity: 0.8483 | Document 0: Machine learning is a subset of artificial intelligence that focuses on algorith...\nSimilarity: 0.6805 | Document 1: Neural networks are trained through backpropagation, adjusting weights to minimi...\nQuery: How does neural network training work?\nSimilarity: 0.6050 | Document 0: Machine learning is a subset of artificial intelligence that focuses on algorith...\nSimilarity: 0.7689 | Document 1: Neural networks are trained through backpropagation, adjusting weights to minimi...\nTransformers Usage\nSee here.\nAsymmetric Retrieval Setup\nmdbr-leaf-mt is aligned to mxbai-embed-large-v1, the model it has been distilled from.\nThis enables flexible architectures in which, for example, documents are encoded using the larger model,\nwhile queries can be encoded faster and more efficiently with the compact leaf model.\nThis usually outperforms the symmetric setup in which both queries and documents are encoded with leaf.\nTo use exclusively the leaf model, use mdbr-leaf-mt.\nMRL Truncation\nEmbeddings have been trained via MRL and can be truncated for more efficient storage:\nquery_embeds = model.encode_query(queries, truncate_dim=256)\ndoc_embeds = model.encode_document(documents, truncate_dim=256)\nsimilarities = model.similarity(query_embeds, doc_embeds)\nprint('After MRL:')\nprint(f\"* Embeddings dimension: {query_embeds.shape[1]}\")\nprint(f\"* Similarities:\\n{similarities}\")\nSee example output\nAfter MRL:\n* Embeddings dimension: 256\n* Similarities:\ntensor([[0.8584, 0.6921],\n[0.5973, 0.7893]])\nVector Quantization\nVector quantization, for example to int8 or binary, can be performed as follows:\nNote: For vector quantization to types other than binary, we suggest performing a calibration to determine the optimal ranges, see here.\nGood initial values are -1.0 and +1.0.\nfrom sentence_transformers.quantization import quantize_embeddings\nimport torch\nquery_embeds = model.encode_query(queries)\ndoc_embeds = model.encode_document(documents)\n# Quantize embeddings to int8 using -1.0 and +1.0\nranges = torch.tensor([[-1.0], [+1.0]]).expand(2, query_embeds.shape[1]).cpu().numpy()\nquery_embeds = quantize_embeddings(query_embeds, \"int8\", ranges=ranges)\ndoc_embeds = quantize_embeddings(doc_embeds, \"int8\", ranges=ranges)\n# Calculate similarities; cast to int64 to avoid under/overflow\nsimilarities = query_embeds.astype(int) @ doc_embeds.astype(int).T\nprint('After quantization:')\nprint(f\"* Embeddings type: {query_embeds.dtype}\")\nprint(f\"* Similarities:\\n{similarities}\")\nSee example output\nAfter quantization:\n* Embeddings type: int8\n* Similarities:\n[[11392 9204]\n[8256 10470]]\nEvaluation\nPlease see here.\nCitation\nIf you use this model in your work, please cite:\n@misc{mdbr_leaf,\ntitle={LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations},\nauthor={Robin Vujanic and Thomas Rueckstiess},\nyear={2025},\neprint={2509.12539},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2509.12539},\n}\nLicense\nThis model is released under Apache 2.0 License.\nContact\nFor questions or issues, please open an issue or pull request. You can also contact the MongoDB ML research team at robin.vujanic@mongodb.com.\nAcknowledgments\nThis model version was created by @tomaarsen - we thank him for his contribution to this project.",
    "manycore-research/FLUX.1-Wireframe-dev-lora": "FLUX.1-Wireframe-dev-lora for SpatialGen\nDiffusers\nLICENSE\nFLUX.1-Wireframe-dev-lora for SpatialGen\nFLUX.1 Wireframe [dev] LoRA is an improved version of FLUX.1-Layout-ControlNet, which serves as a key component of the SpatialGen. FLUX.1 Wireframe [dev] LoRA is a LoRA for FLUX.1 [dev], capable of generating an image based on a text description while following the structure of the given wireframe image.\nüìñ Paper: SPATIALGEN: Layout-guided 3D Indoor Scene Generation\nüåê Project Page\nüíª Code Repository\nPrompt\nA modern living room features a clean and minimalist design. On the left, a light beige wall houses a built-in shelving unit with a textured, neutral finish, adorned with a few decorative items. The right wall showcases a ribbed, light gray panel with a built-in storage unit below. The floor is covered with a light beige carpet, which extends into the midground where a beige sofa with patterned throw pillows is positioned. A small round ottoman sits in front of the sofa, adding a cozy touch. In the background, large windows with sheer curtains allow natural light to flood the room, highlighting the light gray curtains and the greenery outside. A tall, narrow plant stand with a potted plant stands beside the window, adding a touch of nature. The ceiling features a circular chandelier with a modern design, casting a warm glow over the space.\nPrompt\nA contemporary bedroom features a large, plush bed with a white and gray upholstered headboard and matching bedding. The bed is positioned centrally, flanked by two nightstands with modern, metallic lamps. The walls are finished in light gray paneling, creating a clean and minimalist backdrop. A large window on the left side of the room allows natural light to flood in, with sheer curtains partially drawn to diffuse the sunlight. The flooring is a smooth, light gray tile that reflects the ambient light, enhancing the room's bright and airy feel. The artwork above the bed is a modern abstract piece, featuring a circular design with a mix of dark and light tones, adding a touch of sophistication to the space.\nPrompt\nA contemporary bedroom features a large bed positioned centrally, with a soft, light gray upholstered headboard. The bed is dressed in crisp white linens, accented by a beige throw blanket with a textured pattern. Flanking the bed are two gray pillows, adding a subtle contrast. Above the bed, a minimalist abstract painting with gold and gray tones hangs on the white wall. The room is bathed in natural light streaming through a large window with sheer white curtains, which are drawn back to reveal a black-framed view outside. The light wooden flooring complements the serene and modern aesthetic of the space.\nPrompt\nA modern dining area features a sleek, dark wood dining table with a minimalist design, flanked by six matching chairs with light brown upholstery. The table is positioned in the foreground, with a large window on the left side allowing natural light to flood the space. The midground showcases a built-in storage unit with glass and wood paneling, housing various decorative items and wine glasses. The background reveals a polished tile floor that extends throughout the room, with a light gray and white color scheme that enhances the contemporary aesthetic. The ceiling is adorned with recessed lighting, casting a soft glow over the dining area, while the large window on the left provides ample daylight, creating a bright and airy atmosphere.\nDiffusers\nimport torch\nfrom diffusers.pipelines.flux.pipeline_flux_control import FluxControlPipeline\nfrom diffusers.utils import load_image\npipe = FluxControlPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-dev\",\ntorch_dtype=torch.bfloat16,\n)\npipe.load_lora_weights(\"manycore-research/FLUX.1-Wireframe-dev-lora\")\npipe = pipe.to(\"cuda\")\ncontrol_image = load_image(\n\"https://huggingface.co/manycore-research/FLUX.1-Wireframe-dev-lora/resolve/main/assets/input1.png\"\n)\nprompt = \"A modern living room features a clean and minimalist design. On the left, a light beige wall houses a built-in shelving unit with a textured, neutral finish, adorned with a few decorative items. The right wall showcases a ribbed, light gray panel with a built-in storage unit below. The floor is covered with a light beige carpet, which extends into the midground where a beige sofa with patterned throw pillows is positioned. A small round ottoman sits in front of the sofa, adding a cozy touch. In the background, large windows with sheer curtains allow natural light to flood the room, highlighting the light gray curtains and the greenery outside. A tall, narrow plant stand with a potted plant stands beside the window, adding a touch of nature. The ceiling features a circular chandelier with a modern design, casting a warm glow over the space.\"\nimage = pipe(\nprompt=prompt,\ncontrol_image=control_image,\nnum_inference_steps=20,\ngenerator=torch.Generator(device=\"cpu\").manual_seed(42),\nheight=512,\nwidth=512,\nguidance_scale=10.0,\n).images[0]\nimage.save(\"output.png\")\nLICENSE\nFLUX.1-Wireframe-dev-lora is licensed under the FLUX.1-dev Non-Commercial License.",
    "Darkhn/Magistral-Small-2509-Text-Only": "Magistral-Small-1.2-Text-Only\nSend me your support to help me feed the data beast! also taking comissions for universe specific models\nSupport on Ko-fi\nModel Description\nThis model is a specialized, text-only version of Mistral AI's powerful Magistral Small 1.2. It was derived from the official release by carefully removing the vision encoder and related multimodal layers. The result is a more streamlined and efficient 24B parameter model that excels at text-based tasks, retaining the exceptional reasoning capabilities of its progenitor.\nBuilt upon Mistral Small 3.2 (2506), this model underwent Supervised Fine-Tuning (SFT) from Magistral Medium traces and Reinforcement Learning (RL) on top, inheriting a deep capacity for logical deduction and problem-solving. By focusing exclusively on text, this version offers a smaller footprint and potentially faster inference for applications where vision is not required.\nImportant: Reasoning Format & Backend Setup\nThis model uses a special reasoning format. There are two methods to enable it: the official format designed by MistralAI, and a legacy format that works due to the base model's pre-training. The correct method depends on your backend software (e.g., llama.cpp, Kobold.cpp).\nOfficial Format: [THINK] (Recommended for llama.cpp)\nThis is the official instruction format from MistralAI and is the recommended method. It is confirmed to work with backends like llama.cpp (with specific flags) and mistral-common.\nLlama.cpp Prerequisite: Launch llama.cpp with the --special and --jinja arguments enabled.\nInstruction Format: The model uses [THINK] and [/THINK] tags.\nActivation (2 steps):\nSet your prefill sequence (in your frontend like SillyTavern) to start with [THINK].\nYou must also include the keyword /think anywhere in your system prompt to activate the reasoning module.\nRecommended System Prompt for Official Format\nAdd the following to your system prompt to guide the model's output structure:\nFirst draft your thinking process (inner monologue) until you arrive at a response. You must use the /think keyword. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input. Your thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.[/THINK]Here, provide a self-contained response.\nSillyTavern Quick Setup\nFor a complete SillyTavern configuration, you can download and import this JSON file:\nDownload SillyTavern JSON ‚Üí\nLegacy Format: <think> (For Kobold.cpp & TabbyAPI)\nThis format is not official but is highly effective with backends like Kobold.cpp and TabbyAPI. It works because the model's predecessor was trained on these angle-bracket tags, and the model inherits this behavior.\nInstruction Format: Wrap the model's reasoning in <think> and </think> tags.\nActivation: In your frontend, set your prefill sequence to start with <think>.\nSee the GitHub Issue for technical details ‚Üí\nKey Features\nReasoning: Capable of producing long, coherent chains of thought to break down complex problems before providing an answer.\nMultilingual: Supports dozens of languages, including English, French, German, Spanish, Italian, Japanese, Korean, Chinese, Arabic, and many more.\nApache 2.0 License: Features a permissive, open license allowing for both commercial and non-commercial use.\nContext Window: A 128k context window. Performance might degrade past 40k, but the model should still provide good results.\nUsage Guide\nRecommended Sampler Settings\nTemperature\n0.7\nTop P\n0.95\nMax Tokens\n131072\nFor best results, you must use the recommended system prompt and response format. The model uses special [THINK] and [/THINK] tokens to encapsulate its reasoning process before delivering the final answer.\nSystem Prompt\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.Your thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.[/THINK]Here, provide a self-contained response.\nImportant: The [THINK] and [/THINK] tags are special tokens and must be encoded as such. Please ensure you are using a recent version of a library that supports the Magistral chat template, such as mistral-common.\nBenchmark Results\nThe following benchmarks are from the official release of the original multimodal Magistral Small 1.2. As this version is a direct derivative with only vision components removed, performance on these text-based reasoning benchmarks is expected to be identical.\nModel\nAIME24 pass@1\nAIME25 pass@1\nGPQA Diamond\nLivecodebench (v5)\nMagistral Medium 1.2\n91.82%\n83.48%\n76.26%\n75.00%\nMagistral Small 1.2\n86.14%\n77.34%\n70.07%\n70.88%\nMagistral Small 1.1\n70.52%\n62.03%\n65.78%\n59.17%\nIntended Use & Limitations\nIntended Use: This model is designed for text-based tasks that require strong reasoning, instruction following, and multilingual chat capabilities, without the computational overhead of multimodal features.\nLimitations & Quirks:\nThis is a text-only model and cannot process image or other non-text inputs.\nPerformance on tasks outside of its core training domain (e.g., highly specialized coding, non-chat formats) is not guaranteed.\nThe model may \"hallucinate\" or generate plausible but incorrect information. Always verify critical facts.\nSafety: This model has not undergone additional safety alignment beyond what was included in its base Magistral model. Standard responsible AI practices should be followed.\nAcknowledgements\nCredit to Mistral AI for the powerful Magistral architecture and for releasing their work openly.",
    "aidiffuser/Qwen-Image-Edit-2509": "README.md exists but content is empty.",
    "tencent/Hunyuan3D-Part": "‚òØÔ∏è Hunyuan3D Part\nP3-SAMÔºö Native 3DPart Segmentation.\nX-PartÔºö high-fidelity and structure-coherent shapede composition\nNotice\nüîó Citation\n‚òØÔ∏è Hunyuan3D Part\nPipeline of our image to 3D part generation. It contains two key components, P3-SAM and X-Part. The holistic mesh is fed to part detection module P3-SAM to obtain the semantic features, part segmentations and part bounding boxes. Then X-Part generate the complete parts.\nP3-SAMÔºö Native 3DPart Segmentation.\nPaper:  https://arxiv.org/abs/2509.06784.\nCode: https://github.com/Tencent-Hunyuan/Hunyuan3D-Part/tree/main/P3-SAM.\nProject Page: https://murcherful.github.io/P3-SAM/ .\nHuggingFace Demo: https://huggingface.co/spaces/tencent/Hunyuan3D-Part.\nX-PartÔºö high-fidelity and structure-coherent shapede composition\nPaper: https://arxiv.org/abs/2509.08643.\nCode: https://github.com/Tencent-Hunyuan/Hunyuan3D-Part/tree/main/XPart.\nProject Page: https://yanxinhao.github.io/Projects/X-Part/.\nHuggingFace Demo: https://huggingface.co/spaces/tencent/Hunyuan3D-Part.\nNotice\nThe current release is a light version of X-Part. The full-blood version is available on .\nFor X-Part, we recommend using ‚Äã‚Äãscanned‚Äã‚Äã or ‚Äã‚ÄãAI-generated meshes‚Äã‚Äã (e.g., from Hunyuan3D V2.5 or V3.0) as input.\nP3-SAM can handle any input mesh.\nüîó Citation\nIf you found this repository helpful, please cite our reports:\nP3-SAM\n@article{ma2025p3sam,\ntitle={P3-sam: Native 3d part segmentation},\nauthor={Ma, Changfeng and Li, Yang and Yan, Xinhao and Xu, Jiachen and Yang, Yunhan and Wang, Chunshi and Zhao, Zibo and Guo, Yanwen and Chen, Zhuo and Guo, Chunchao},\njournal={arXiv preprint arXiv:2509.06784},\nyear={2025}\n}\nX-Part\n@article{yan2025xpart,\ntitle={X-Part: high fidelity and structure coherent shape decomposition},\nauthor={Yan, Xinhao and Xu, Jiachen and Li, Yang and Ma, Changfeng and Yang, Yunhan and Wang, Chunshi and Zhao, Zibo and Lai, Zeqiang and Zhao, Yunfei and Chen, Zhuo and others},\njournal={arXiv preprint arXiv:2509.08643},\nyear={2025}\n}",
    "neuphonic/neutts-air-q4-gguf": "NeuTTS Air Q4-GGUF ‚òÅÔ∏è\nKey Features\nModel Details\nGet Started\nBasic Example\nSimple One-Code Block Usage\nExample Reference Files\nGuidelines for Best Results\nTips\nExample Reference Files\nGuidelines for Best Results\nResponsibility\nDisclaimer\nNeuTTS Air Q4-GGUF ‚òÅÔ∏è\nCreated by Neuphonic - building faster, smaller, on-device voice AI\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\nThis is a Q4-GGUF version of NeuTTS-Air.\nKey Features\nüó£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\nüì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\nüë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio\nüöÑSimple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\nModel Details\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\nAudio Codec: NeuCodec - our proprietary neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\nFormat: Available in GGML format for efficient on-device inference\nResponsibility: Watermarked outputs\nInference Speed: Real-time generation on mid-range devices\nPower Consumption: Optimised for mobile and embedded devices\nGet Started\nClone the Git Repo\ngit clone https://github.com/neuphonic/neutts-air.git\ncd neuttsair\nInstall¬†espeak¬†(required dependency)\nPlease refer to the following link for instructions on how to install¬†espeak:\nhttps://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n# Mac OS\nbrew install espeak\n# Ubuntu/Debian\nsudo apt install espeak\nInstall Python dependencies\nThe requirements file includes the dependencies needed to run the model with PyTorch. When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\nThe inference is compatible and tested on¬†python>=3.11.\npip install -r requirements.txt\nBasic Example\nRun the basic example script to synthesize speech:\npython -m examples.basic_example \\\n--input_text \"My name is Dave, and um, I'm from London\" \\\n--ref_audio samples/dave.wav \\\n--ref_text samples/dave.txt\n--backbone neuphonic/neutts-air-q4-gguf\nTo specify a particular model repo for the backbone or codec, add the¬†--backbone¬†argument. Available backbones are listed in¬†NeuTTS-Air huggingface collection.\nSeveral examples are available, including a Jupyter notebook in the¬†examples¬†folder.\nSimple One-Code Block Usage\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\ntts = NeuTTSAir( backbone_repo=\"neuphonic/neutts-air-q4-gguf\", backbone_device=\"cpu\", codec_repo=\"neuphonic/neucodec\", codec_device=\"cpu\")\ninput_text = \"My name is Dave, and um, I'm from London.\"\nref_text = \"samples/dave.txt\"\nref_audio_path = \"samples/dave.wav\"\nref_text = open(ref_text, \"r\").read().strip()\nref_codes = tts.encode_reference(ref_audio_path)\nwav = tts.infer(input_text, ref_codes, ref_text)\nsf.write(\"test.wav\", wav, 24000)\nTips\nNeuTTS Air requires two inputs:\nA reference audio sample (.wav file)\nA text string\nThe model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS Air‚Äôs instant voice cloning capability.\nExample Reference Files\nYou can find some ready-to-use samples in the examples folder:\nsamples/dave.wav\nsamples/jo.wav\nGuidelines for Best Results\nFor optimal performance, reference audio samples should be:\nMono channel\n16-44 kHz sample rate\n3‚Äì15 seconds in length\nSaved as a .wav file\nClean ‚Äî minimal to no background noise\nNatural, continuous speech ‚Äî like a monologue or conversation, with few pauses, so the model can capture tone effectively\nResponsibility\nEvery audio file generated by NeuTTS Air includes **Perth (Perceptual Threshold) Watermarker.**\nDisclaimer\nDon't use this model to do bad things‚Ä¶ please.",
    "LiquidAI/LFM2-2.6B-GGUF": "LFM2-2.6B-GGUF\nüèÉ How to run LFM2\nLiquid: Playground\nLiquid\nLiquid\nPlayground\nPlayground\nLFM2-2.6B-GGUF\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nFind more details in the original model card: https://huggingface.co/LiquidAI/LFM2-2.6B\nüèÉ How to run LFM2\nExample usage with llama.cpp:\nllama-cli -hf LiquidAI/LFM2-2.6B-GGUF",
    "Fremtind/norsbert4-large": "SentenceTransformer based on NorBERT4-large\nUsage\nDirect Usage (Sentence Transformers)\nEvaluation\nTraining Details\nTraining Hyperparameters\nFramework Versions\nSentenceTransformer based on NorBERT4-large\nNorSBERT4-Large is a Sentence Transformer model finetuned from ltg/norbert4-large.\nThe model maps sentences (and paragraphs) to a 960-dimensional dense vector space and can be used for semantic textual similarity, semantic search,\ntext classification, clustering, among other tasks.\nNote: While the fine-tuned sentence-transformer model has a max_seq_length of 75 tokens, the base model does not.\nThis means that the sequence length can be increased to 16384 (which is the max length in the base model).\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference. Note that you should load the model with trust_remote_code=True because it needs a custom wrapper (see the base model for more details).\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"Fremtind/norsbert4-large\", trust_remote_code=True)\n# Run inference\nsentences = [\n'To personer, en i lyse jeans og en stripete skjorte, spiller biljard.',\n'Folk spiller biljard',\n'folk l√∏per',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 960]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.7294, 0.1690],\n#         [0.7294, 1.0000, 0.2412],\n#         [0.1690, 0.2412, 1.0000]])\nEvaluation\nTo verify the utility of our models, we evaluated them on a selection of classification and clustering tasks for Norwegian from MTEBv2.\nThe heatmap below shows the results of evaluating five sentence-transformers on ten different tasks;\nthree of the sentence-transformer models we have fine-tuned\n(Fremtind/norsbert4-large, Fremtind/norsbert4-base, Fremtind/mmBERT-base-norwegian)\nand the other two are relatively popular (and comparable) sentence similarity models (FFI/SimCSE-NB-BERT-large and NbAiLab/nb-sbert-base).\nWe ranked the models using Borda count (which is used in MTEB), where each model was assigned a number of points based on its relative performance across all evaluated tasks.\nRank\nModel\nBorda Points\n1\nFremtind/norsbert4-large\n44\n2\nFFI/SimCSE-NB-BERT-large\n40\n3\nFremtind/norsbert4-base\n24\n4\nNbAiLab/nb-sbert-base\n15\n5\nFremtind/mmBERT-base-norwegian\n7\nTraining Details\nThe model was fine-tuned in two stages.\nIn the first stage, it was trained in an unsupervised manner following the SimCSE method (Gao et al., 2021). In this setup, the same sentence is encoded twice, and due to dropout (in training mode), the model produces two slightly different embeddings. The training objective is to minimize the distance between these embeddings while maximizing the distance to embeddings of other sentences in the same batch.\nFor this stage, we created sentence pairs in three categories from the NDLA Parallel Paragraphs dataset: (Bokm√•l, Bokm√•l), (Nynorsk, Nynorsk), and (Bokm√•l, Nynorsk). In the (Bokm√•l, Bokm√•l) and (Nynorsk, Nynorsk) pairs, each sentence was paired with itself, leveraging dropout to create embedding variation. In the (Bokm√•l, Nynorsk) category, cross-lingual sentence pairs were used to align the model‚Äôs semantic representations across the two language varieties.\nIn the second stage, the model was further fine-tuned on a natural language inference dataset, namely Fremtind/all-nli-norwegian. The dataset is formatted as triplets (anchor, positive, negative), where the anchor is the premise, the positive is an entailment hypothesis, and the negative is a contradiction hypothesis. The objective is to minimize the distance between the anchor and positive while maximizing it between the anchor and negative. This fine-tuning stage follows the 'standard' supervised fine-tuning strategy introduced in Sentence-BERT.\nTraining Hyperparameters\nNon-Default Hyperparameters\nClick to expand\neval_strategy: steps\nper_device_train_batch_size: 512\nper_device_eval_batch_size: 256\nnum_train_epochs: 1\nwarmup_ratio: 0.1\nbatch_sampler: no_duplicates\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: False\ndo_predict: False\neval_strategy: steps\nprediction_loss_only: True\nper_device_train_batch_size: 512\nper_device_eval_batch_size: 256\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 1\neval_accumulation_steps: None\ntorch_empty_cache_steps: None\nlearning_rate: 5e-05\nweight_decay: 0.0\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1.0\nnum_train_epochs: 1\nmax_steps: -1\nlr_scheduler_type: linear\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.1\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 42\ndata_seed: None\njit_mode_eval: False\nuse_ipex: False\nbf16: False\nfp16: False\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: None\nlocal_rank: 1\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: True\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: False\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\nparallelism_config: None\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch\noptim_args: None\nadafactor: False\ngroup_by_length: False\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: None\nhub_always_push: False\nhub_revision: None\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\ninclude_for_metrics: []\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\neval_on_start: False\nuse_liger_kernel: False\nliger_kernel_config: None\neval_use_gather_object: False\naverage_tokens_across_devices: True\nprompts: None\nbatch_sampler: no_duplicates\nmulti_dataset_batch_sampler: proportional\nrouter_mapping: {}\nlearning_rate_mapping: {}\nFramework Versions\nClick to expand\nPython: 3.12.11\nSentence Transformers: 5.1.1\nTransformers: 4.56.2\nPyTorch: 2.6.0+cu124\nAccelerate: 1.10.1\nDatasets: 4.1.1\nTokenizers: 0.22.1",
    "inclusionAI/Ring-mini-linear-2.0": "Ring-mini-linear-2.0\nIntroduction\nEvaluation\nLinear Attention, Highly Sparse, High-Speed Generation\nQuickstart\nRequirements\nü§ó Hugging Face Transformers\nüöÄ SGLang\nüöÄ vLLM\nRing-mini-linear-2.0\nüìñ  Technical Report¬†¬† | ¬†¬† ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope\nIntroduction\nToday, we are officially open-sourcing Ring-mini-linear-2.0.\nThis model continues to employ a hybrid architecture that combines linear attention and standard attention mechanisms, striking a balance between performance and efficiency. Inheriting the efficient MoE (Mixture-of-Experts) design from the Ling 2.0 series, and through architectural optimizations such as a 1/32 expert activation ratio and MTP layers, Ring-mini-linear achieves the performance of an ~8B dense model while activating only 1.6B of its 16.4B total parameters. This model was converted from Ling-mini-base-2.0, continually trained on an additional 600B tokens. In terms of performance, the hybrid linear model is comparable in overall performance to standard attention models of a similar size (e.g., Ring-mini-2) and surpasses other open-source MoE and Dense models of the same class on several challenging benchmarks. Additionally, we support a 512k long context window, achieved by extrapolating the window 4x using YaRN. This provides superior speed, especially on tasks involving long inputs and outputs.\nFigure 1: Hybrid Linear Model Architecture\nEvaluation\nTo better demonstrate our model's reasoning capabilities, we compared it with three other models‚ÄîRing-mini-2.0, Qwen3-8B-thinking, and GPT-OSS-20B-Medium‚Äîon 5 challenging reasoning benchmarks across mathematics, code, and science. We observe that the hybrid-linear architecture achieves performance comparable to that of softmax attention models.\nFigure 2: Model Performance Comparison\nLinear Attention, Highly Sparse, High-Speed Generation\nThanks to its hybrid attention mechanism and highly sparse MoE architecture, Ring-mini-linear-2.0 achieves near-linear time complexity and constant space complexity, resulting in outstanding inference efficiency. To fully demonstrate this advantage, we conducted a comparison between our model and top-tier competitors of similar size or performance.The results clearly demonstrate the advantage of our model in inference efficiency.\nFigure 3: Ring-mini-linear-2.0 prefill throughput\nFigure 4: Ring-mini-linear-2.0 decode throughput\nQuickstart\nRequirements\npip install flash-linear-attention==0.3.2\npip install transformers==4.56.1\nü§ó Hugging Face Transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-mini-linear-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompts = [\n\"Give me a short introduction to large language models.\"\n]\ninput_texts = []\nfor prompt in prompts:\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput_texts.append(text)\nprint(input_texts)\nmodel_inputs = tokenizer(input_texts, return_tensors=\"pt\", return_token_type_ids=False, padding=True, padding_side='left').to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192,\ndo_sample=False,\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(\"*\" * 30)\nprint(responses)\nprint(\"*\" * 30)\nüöÄ SGLang\nEnvironment Preparation\nWe have submitted our PR to SGLang official release and it will be merged later, for now we can prepare the environment following steps, firstly install the community version SGLang and required packages:\npip install sglang==0.5.2 sgl-kernel==0.3.9.post2 vllm==0.10.2 torch==2.8.0 torchvision==0.23.0 torchao\nThen you should install our sglang wheel package:\npip install https://media.githubusercontent.com/media/inclusionAI/Ring-V2/refs/heads/main/hybrid_linear/whls/sglang-0.5.2-py3-none-any.whl --no-deps --force-reinstall\nRun Inference\nBF16 and FP8 models are supported by SGLang now, it depends on the dtype of the model in ${MODEL_PATH}. They both share the same command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path <model_path> \\\n--trust-remote-code \\\n--tp-size 1 \\\n--disable-radix-cache \\\n--json-model-override-args \"{\\\"linear_backend\\\": \\\"seg_la\\\"}\"\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"temperature\": 0.6, \"messages\": [{\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}]}'\nMore usage can be found here\nüöÄ vLLM\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below.\nFirst, create a Conda environment with Python 3.10 and CUDA 12.8:\nconda create -n vllm python=3.10\nconda activate vllm\nNext, install our vLLM wheel package:\npip install https://media.githubusercontent.com/media/zheyishine/vllm_whl/refs/heads/main/vllm-0.8.5.post2.dev28%2Bgd327eed71.cu128-cp310-cp310-linux_x86_64.whl --force-reinstall\nFinally, install compatible versions of transformers after vLLM is installed:\npip install transformers==4.51.1\nOffline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nif __name__ == '__main__':\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ring-mini-linear-2.0\", trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.6, top_p=1.0, max_tokens=1024)\n# use `max_num_seqs=1` without concurrency\nllm = LLM(model=\"inclusionAI/Ring-mini-linear-2.0\", dtype='auto', enable_prefix_caching=False, max_num_seqs=128)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nfor output in outputs:\nprint(output.outputs[0].text)\nOnline Inference\nvllm serve inclusionAI/Ring-mini-linear-2.0 \\\n--tensor-parallel-size 1 \\\n--pipeline-parallel-size 1 \\\n--gpu-memory-utilization 0.90 \\\n--max-num-seqs 128 \\\n--no-enable-prefix-caching\n--api-key your-api-key\nCitation\n@misc{lingteam2025attentionmattersefficienthybrid,\ntitle={Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning},\nauthor={Ling Team and Bin Han and Caizhi Tang and Chen Liang and Donghao Zhang and Fan Yuan and Feng Zhu and Jie Gao and Jingyu Hu and Longfei Li and Meng Li and Mingyang Zhang and Peijie Jiang and Peng Jiao and Qian Zhao and Qingyuan Yang and Wenbo Shen and Xinxing Yang and Yalin Zhang and Yankun Ren and Yao Zhao and Yibo Cao and Yixuan Sun and Yue Zhang and Yuchen Fang and Zibin Lin and Zixuan Cheng and Jun Zhou},\nyear={2025},\neprint={2510.19338},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.19338},\n}",
    "tencent/Youtu-Embedding": "üéØ Introduction\n1. Using transformers\n2. Using sentence-transformers\n3. Using LangChain ü¶ú\n4. Using LlamaIndex ü¶ô\nü§ó Model Download\n1. Using transformers\n2. Using sentence-transformers\n3. Using LangChain ü¶ú\n4. Using LlamaIndex ü¶ô\nüöÄ Usage\n1. Using transformers\n2. Using sentence-transformers\n3. Using LangChain ü¶ú\n4. Using LlamaIndex ü¶ô\nüìä CMTEB\nüéâ Citation\nü§ó¬†Hugging Face¬†¬†|\nüñ•Ô∏è¬†GitHub¬†¬†|\nüåé¬†Technical Report\nüí¨¬†WeChat¬†¬†|\nü§ñ¬†Discord\nüéØ Introduction\nYoutu-Embedding is a state-of-the-art, general-purpose text embedding model developed by Tencent Youtu Lab. It delivers exceptional performance across a wide range of natural language processing tasks, including Information Retrieval (IR), Semantic Textual Similarity (STS), Clustering, Reranking, and Classification.\nTop-Ranked Performance: Achieved the #1 score of 77.58 on the authoritative CMTEB (Chinese Massive Text Embedding Benchmark) as of September 2025, demonstrating its powerful and robust text representation capabilities.\nInnovative Training Framework: Features a Collaborative-Discriminative Fine-tuning Framework designed to resolve the \"negative transfer\" problem in multi-task learning. This is accomplished through a unified data format, task-differentiated loss functions, and a dynamic single-task sampling mechanism.\nNote: You can easily adapt and fine-tune the model on your own datasets for domain-specific tasks. For implementation details, please refer to the training code.\nü§ó Model Download\nModel Name\nParameters\nDimensions\nSequence Length\nDownload\nYoutu-Embedding\n2B\n2048\n8K\nModel\nüöÄ Usage\n1. Using transformers\nüì¶ Installation\npip install transformers==4.51.3\n‚öôÔ∏è Usage\nimport torch\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer\nclass LLMEmbeddingModel():\ndef __init__(self,\nmodel_name_or_path,\nbatch_size=128,\nmax_length=1024,\ngpu_id=0):\nself.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True)\nself.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\")\nself.device = torch.device(f\"cuda:{gpu_id}\")\nself.model.to(self.device).eval()\nself.max_length = max_length\nself.batch_size = batch_size\nquery_instruction = \"Given a search query, retrieve passages that answer the question\"\nif query_instruction:\nself.query_instruction = f\"Instruction: {query_instruction} \\nQuery:\"\nelse:\nself.query_instruction = \"Query:\"\nself.doc_instruction = \"\"\nprint(f\"query instruction: {[self.query_instruction]}\\ndoc instruction: {[self.doc_instruction]}\")\ndef mean_pooling(self, hidden_state, attention_mask):\ns = torch.sum(hidden_state * attention_mask.unsqueeze(-1).float(), dim=1)\nd = attention_mask.sum(dim=1, keepdim=True).float()\nembedding = s / d\nreturn embedding\n@torch.no_grad()\ndef encode(self, sentences_batch, instruction):\ninputs = self.tokenizer(\nsentences_batch,\npadding=True,\ntruncation=True,\nreturn_tensors=\"pt\",\nmax_length=self.max_length,\nadd_special_tokens=True,\n).to(self.device)\nwith torch.no_grad():\noutputs = self.model(**inputs)\nlast_hidden_state = outputs[0]\ninstruction_tokens = self.tokenizer(\ninstruction,\npadding=False,\ntruncation=True,\nmax_length=self.max_length,\nadd_special_tokens=True,\n)[\"input_ids\"]\nif len(np.shape(np.array(instruction_tokens))) == 1:\ninputs[\"attention_mask\"][:, :len(instruction_tokens)] = 0\nelse:\ninstruction_length = [len(item) for item in instruction_tokens]\nassert len(instruction) == len(sentences_batch)\nfor idx in range(len(instruction_length)):\ninputs[\"attention_mask\"][idx, :instruction_length[idx]] = 0\nembeddings = self.mean_pooling(last_hidden_state, inputs[\"attention_mask\"])\nembeddings = torch.nn.functional.normalize(embeddings, dim=-1)\nreturn embeddings\ndef encode_queries(self, queries):\nqueries = queries if isinstance(queries, list) else [queries]\nqueries = [f\"{self.query_instruction}{query}\" for query in queries]\nreturn self.encode(queries, self.query_instruction)\ndef encode_passages(self, passages):\npassages = passages if isinstance(passages, list) else [passages]\npassages = [f\"{self.doc_instruction}{passage}\" for passage in passages]\nreturn self.encode(passages, self.doc_instruction)\ndef compute_similarity_for_vectors(self, q_reps, p_reps):\nif len(p_reps.size()) == 2:\nreturn torch.matmul(q_reps, p_reps.transpose(0, 1))\nreturn torch.matmul(q_reps, p_reps.transpose(-2, -1))\ndef compute_similarity(self, queries, passages):\nq_reps = self.encode_queries(queries)\np_reps = self.encode_passages(passages)\nscores = self.compute_similarity_for_vectors(q_reps, p_reps)\nscores = scores.detach().cpu().tolist()\nreturn scores\nqueries = [\"What's the weather like?\"]\npassages = [\n'The weather is lovely today.',\n\"It's so sunny outside!\",\n'He drove to the stadium.'\n]\nmodel_name_or_path = \"tencent/Youtu-Embedding\"\nmodel = LLMEmbeddingModel(model_name_or_path)\nscores = model.compute_similarity(queries, passages)\nprint(f\"scores: {scores}\")\n2. Using sentence-transformers\nüì¶ Installation\npip install sentence-transformers==5.1.0\n‚öôÔ∏è Usage\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"tencent/Youtu-Embedding\", trust_remote_code=True)\nqueries = [\"What's the weather like?\"]\npassages = [\n'The weather is lovely today.',\n\"It's so sunny outside!\",\n'He drove to the stadium.'\n]\nqueries_embeddings = model.encode_query(queries)\npassages_embeddings = model.encode_document(passages)\nsimilarities = model.similarity(queries_embeddings, passages_embeddings)\nprint(similarities)\n3. Using LangChain ü¶ú\nEasily integrate the model into your LangChain applications, such as RAG pipelines.\nüì¶ Installation\npip install langchain==0.3.27 langchain-community==0.3.29 langchain-huggingface==0.3.1 sentence-transformers==5.1.0 faiss-cpu==1.11.0\n‚öôÔ∏è Usage\nimport torch\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\nmodel_name_or_path = \"tencent/Youtu-Embedding\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_kwargs = {\n'trust_remote_code': True,\n'device': device\n}\nembedder = HuggingFaceEmbeddings(\nmodel_name=model_name_or_path,\nmodel_kwargs=model_kwargs,\n)\nquery_instruction = \"Instruction: Given a search query, retrieve passages that answer the question \\nQuery:\"\ndoc_instruction = \"\"\ndata = [\n\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\ndocuments = [Document(page_content=text, metadata={\"id\": i}) for i, text in enumerate(data)]\nvector_store = FAISS.from_documents(documents, embedder, distance_strategy=\"MAX_INNER_PRODUCT\")\nquery = \"Which planet is known as the Red Planet?\"\ninstructed_query = query_instruction + query\nresults = vector_store.similarity_search_with_score(instructed_query, k=3)\nprint(f\"Original Query: {query}\\n\")\nprint(\"Results:\")\nfor doc, score in results:\nprint(f\"- Text: {doc.page_content} (Score: {score:.4f})\")\n4. Using LlamaIndex ü¶ô\nThis is perfect for integrating the model into your LlamaIndex search and retrieval systems.\nüì¶ Installation\npip install llama-index==0.14.2 llama-index-embeddings-huggingface==0.6.1 sentence-transformers==5.1.0 llama-index-vector-stores-faiss==0.5.1\n‚öôÔ∏è Usage\nimport faiss\nimport torch\nfrom llama_index.core.schema import TextNode\nfrom llama_index.core.vector_stores import VectorStoreQuery\nfrom llama_index.vector_stores.faiss import FaissVectorStore\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nmodel_name_or_path = \"tencent/Youtu-Embedding\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nembeddings = HuggingFaceEmbedding(\nmodel_name=model_name_or_path,\ntrust_remote_code=True,\ndevice=device,\nquery_instruction=\"Instruction: Given a search query, retrieve passages that answer the question \\nQuery:\",\ntext_instruction=\"\"\n)\ndata = [\n\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\nnodes = [TextNode(id_=str(i), text=text) for i, text in enumerate(data)]\nfor node in nodes:\nnode.embedding = embeddings.get_text_embedding(node.get_content())\nembed_dim = len(nodes[0].embedding)\nstore = FaissVectorStore(faiss_index=faiss.IndexFlatIP(embed_dim))\nstore.add(nodes)\nquery = \"Which planet is known as the Red Planet?\"\nquery_embedding = embeddings.get_query_embedding(query)\nresults = store.query(\nVectorStoreQuery(query_embedding=query_embedding, similarity_top_k=3)\n)\nprint(f\"Query: {query}\\n\")\nprint(\"Results:\")\nfor idx, score in zip(results.ids, results.similarities):\nprint(f\"- Text: {data[int(idx)]} (Score: {score:.4f})\")\nüìä CMTEB\nModel\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nbge-multilingual-gemma2\n9B\n67.64\n68.52\n75.31\n59.30\n79.30\n68.28\n73.73\n55.19\nritrieve_zh_v1\n326M\n72.71\n73.85\n76.88\n66.50\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nConan-embedding-v2\n1.4B\n74.24\n75.99\n76.47\n68.84\n92.44\n74.41\n78.31\n65.48\nSeed1.6-embedding\n-\n75.63\n76.68\n77.98\n73.11\n88.71\n71.65\n79.69\n68.94\nQZhou-Embedding\n7B\n76.99\n78.58\n79.99\n70.91\n95.07\n74.85\n78.80\n71.89\nYoutu-Embedding\n2B\n77.58\n78.86\n78.65\n84.27\n86.12\n75.10\n80.21\n68.82\nNote: Comparative scores are from the MTEB leaderboard, recorded on September 28, 2025.\nüéâ Citation\n@misc{zhang2025codiemb,\ntitle={CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity},\nauthor={Zhang, Bowen and Song, Zixin and Chen, Chunquan and Zhang, Qian-Wen and Yin, Di and Sun, Xing},\nyear={2025},\neprint={2508.11442},\narchivePrefix={arXiv},\nurl={https://arxiv.org/abs/2508.11442},\n}",
    "internlm/CapRL-3B": "CapRL-3B\nüì¢ News\nIntroduction\nKey Features\nUsage\nStart an OpenAI API Service\nCases\nCapRL-3B\nüìñPaper | üè†Github |ü§óCapRL-3B Model |ü§óCapRL-InternVL3.5-8B Model |\nü§óCapRL-2M Dataset\nü§óCapRL Collection | ü§óDaily Paper ÔΩúü§óCapRL-3B-GGUF ÔΩúü§óCapRL-3B-i1-GGUF\nNow you can try out CapRL-3B with your own imagesüé®!¬†¬†¬†¬†‚û°Ô∏è¬†¬†¬†¬†üåàCapRL Space\nWhen selecting between the available CapRL models, it's essential to consider the trade-off between performance and computational cost.\nThis guide will help you choose the most suitable model for your specific needs:\nModel\nParameters\nStrength\nü§óCapRL-3B\n3B\nSpeed, Efficiency\nü§óCapRL-InternVL3.5-8B\n8B\nHigh Performance, Advanced Captioning Ability\nüì¢ News\nWe are working on even stronger base models and upgrading our training recipe ‚Äî stay tuned!\nüî• [10/15/2025] The total downloads of the CapRL-related models and dataset reached 6,000 within just 20 days!\nüöÄ [10/15/2025] We are excited to announce the release of CapRL-InternVL3.5-8B, whose image captioning capability outperforms Qwen2.5-VL-72B!\nüöÄ [10/15/2025] Thanks mradermacher for the valuable contribution! CapRL-3B-GGUF is the static quants version, and CapRL-3B-i1-GGUF is weighted/imatrix quants version.\nüöÄ [10/15/2025] We release QA curation code.\nüöÄ [09/25/2025] We release CapRL repository, CapRL-3B model, evaluation code and dataset.\nIntroduction\nWe are excited to introduce CapRL-3B, a lightweight 3B image captioner that achieves perception capabilities comparable to Qwen2.5-VL-72B.\nThis is the first study of applying Reinforcement Learning with Verifiable Rewards for the\nopen-ended and subjective image captioning task. Unlike traditional Supervised Fine-Tuning, which\ncan lead to models memorizing a limited set of annotated captions, our method allows the model to\nexplore and generate a broader range of creative and general descriptions.\nCapRL is a new training paradigm featuring a decoupled two-stage pipeline. The initial\nstage uses LVLMs to generate rich and accurate captions. Subsequently, the second stage evaluates\ncaption quality by using a vision-only LLM to perform the QA task. We also created a specific QA\ncuration pipeline to ensure the quality of the questions and answers used for the second stage.\nBy employing the CapRL training framework, initializing with the Qwen2.5-VL-3B model, and using a carefully\nfiltered 75K QA dataset as the training set, we obtained a highly capable captioner, CapRL-3B.\nKey Features\nRemarkable visual understanding for Chart, Infographics and Document: CapRL-3B achieves perception accuracy and visual information coverage comparable to Qwen2.5-VL-72B.\nWell-organized output: The outputs of CapRL-3B are relatively well-structured, making them clear and easy to understand.\nDetailed description for natural images: The outputs of CapRL-3B can perfectly cover all valid visual information while containing fewer hallucinations.\nUsage\nIf you want to use CapRL-3B for captioning, you can directly follow the exact same inference approach as in Qwen2.5-VL-series.\nWe recommend using vLLM to speed up inference.\nStart an OpenAI API Service\nRun the command below to start an OpenAI-compatible API service:\nvllm serve \"/PATH/CapRL-3B\" \\\n--trust-remote-code \\\n--tensor-parallel-size=1 \\\n--pipeline-parallel-size=1 \\\n--gpu_memory_utilization=0.95 \\\n--served-model-name=caprl \\\n--port 8000 \\\n--host 0.0.0.0\nThen you can use the chat API as below: (see OpenAI API protocol document for more details):\nimport base64\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nimage_path = \"/path/to/local/image.png\"\nwith open(image_path, \"rb\") as f:\nencoded_image = base64.b64encode(f.read())\nencoded_image_text = encoded_image.decode(\"utf-8\")\nbase64_qwen = f\"data:image;base64,{encoded_image_text}\"\nchat_response = client.chat.completions.create(\nmodel=\"caprl\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": base64_qwen\n},\n},\n{\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n],\n},\n],\ntemperature=1.0,\nmax_tokens=max_tokens,\ntop_p=1.0,\nextra_body={\n\"repetition_penalty\": 1.0,\n},\n)\nprint(\"Chat response:\", chat_response)\nCases",
    "FreedomIntelligence/DeEAR_Base": "README.md exists but content is empty.",
    "internlm/Spark-VL-7B": "Spark-VL-7B\nIntroduction\nüì¢ News\nüí° Highlights\nüõ†Ô∏è Usage\nü§ó Using Transformers\nüî¶ Using vLLM\nTraining\nSpark Training\nEvaluation\n‚úíÔ∏èCitation\nüìÑ License\nAcknowledgement\nSpark-VL-7B\n‚≠ê If you find our code or model helpful, please consider giving us a star ‚Äî your support means a lot!\nüåà Try our demo on ü§óHuggingface Demo\nüè†Github repository\nüìñDaily Paper\nü§ómodels\nüìñPaper\nIntroduction\nWe propose SPARK, a unified framework that integrates policy and reward into a single model for joint and synchronous training. SPARK can automatically derive reward and reflection data from verifiable reward, enabling self-learning and self-evolution. Furthermore, we instantiate this framework on multiple backbones, training SPARK-VL-7B, SPARK-7B, and SPARK-VL-32B. This repo is the SPARK-VL-7B.\nüì¢ News\nüöÄ [09/29/2025] We release our ü§ódatasets.\nüöÄ [09/29/2025] We release our Spark's üìñPaper.\nüöÄ [09/29/2025] We upload our evaluation code and ü§ómodels.\nüöÄ [09/29/2025] We release Spark üè†Github repository.\nüí° Highlights\nüî• Synergistic Policy‚ÄìReward Co-Evolving (SPARK): We introduce SPARK, a unified reinforcement fine-tuning framework that jointly optimizes policy and reward within a single model through on-policy co-evolution..\nüî• Recycling Rollouts: Unlike conventional RL pipelines that discard rollouts after policy updates, SPARK recycles RLVR rollouts into pointwise, pairwise, and reflection objectives, enabling the model itself to act as both a strong policy and a generative reward model.\nüî• Co-Evolving Mechanism: Improved reward accuracy provides better gradients for policy learning, while stronger reasoning further refines reward judgment, forming a positive feedback loop that enhances reasoning, judgment, and reflection in synergy.\nüî• Efficient and Practical: SPARK requires no human preference data, teacher models, or external reward models, making it significantly more data- and compute-efficient than traditional RM-based RL pipelines.\nüõ†Ô∏è Usage\nü§ó Using Transformers\nOur model is based on Qwen2.5-VL-7B-Instruct. You can use the same code as the Qwen2.5-VL-7B-Instruct model for inference, referring to ü§óHuggingface.\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\"internlm/Spark-VL-7B\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(\"internlm/Spark-VL-7B\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": image_path,\n},\n{\"type\": \"text\", \"text\": prompt},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nüî¶ Using vLLM\nWe recommend using vLLM for faster inference speed. Using vLLM leads to significant speed improvements in dataset evaluation.\nPORT=8019\nN_PROC=256\nSERVE_NAME=spark_vl_7b\nMODEL_PATH=/internlm/Spark-VL-7B\nCUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve \"$MODEL_PATH\" \\\n--tensor-parallel-size 4 \\\n--served-model-name $SERVE_NAME \\\n--port $PORT \\\n--max-num-seqs $N_PROC\nTraining\nSpark Training\nAfter downloading the dataset, you can start training using the following example bash script. Our bash scripts are in /Spark/Lmm_XC/XC/scripts/spark_training\nYou need to modify the dataset paths and model paths to your own locations.\nexport WORKSPACE_DIR=\"/fs-computility/....../Lmm_XC\"                 # Path to project root directory\nexport DATASET_PATH=\"/fs-computility/....../infer_data_ViRL_19k.json\"            # Path to your dataset\nexport PRETRAIN_MODEL_PATH=\"/fs-computility/....../Qwen2.5-VL-7B-Instruct\"  # Path to pretrained model\nexport WANDB_PROJECT=\"Observation\"        # Name for this project\nexport MODEL_CPK_NAME=\"Qwen2.5-VL-7B-GRPO-virl-19k-iar-reflection-hyb-diverse-bs64-e2\"         # Name for this training run\nexport LOG_PATH='/fs-computility/....../Qwen2.5-VL-7B-GRPO-virl-19k-iar-reflection-hyb-diverse-bs64-e2.txt'      #Log file save path\nexport WANDB_API_KEY=\"......\"\nexport SAVE_PATH=\"/fs-computility/....../${WANDB_PROJECT}/${MODEL_CPK_NAME}\"                   # Absolute path to save everything about this training run\nexport CKPT_PATH=\"${SAVE_PATH}/ckpt\"                                                                    # Path to save checkpoints\nexport FINAL_CKPT_PATH=\"${SAVE_PATH}/final_ckpt\"                                                        # Path to save final checkpoints\nexport TIMESTAMP=$(date +%Y%m%d_%H%M%S)                                                                 # Timestamp\nexport CUR_LOG_DIR=\"${SAVE_PATH}/training_logs/${TIMESTAMP}\"                                            # Path to save current run logs\nexport LOG_DIR=\"${SAVE_PATH}/tb_logs\"\n‚è∞ Attention:\nexport DEV_MODE=0 # Set to 1 for debug mode on single dev machine\nEvaluation\nThe integrated multimodal mathematics dataset can be downloaded from ü§ódatasets and evaluated using the scripts provided in the Evaluation folder. The evaluation results will be stored, and accuracy can subsequently be computed with the calculate_acc.py file.\nbash ./Evaluation/eval_spark_vl_7b.sh\npython calculate_acc.py --result_path ./your_result_path.json\n‚úíÔ∏èCitation\n@article{liu2025spark,\ntitle={SPARK: Synergistic Policy And Reward Co-Evolving Framework},\nauthor={Ziyu Liu and Yuhang Zang and Shengyuan Ding and Yuhang Cao and Xiaoyi Dong and Haodong Duan and Dahua Lin and Jiaqi Wang},\njournal={arXiv preprint arXiv:2509.22624},\nyear={2025}\n}\nüìÑ License\nUsage and License Notices: The data and code are intended and licensed for research use only.\nLicense: Attribution-NonCommercial 4.0 International It should abide by the policy of OpenAI: https://openai.com/policies/terms-of-use\nAcknowledgement\nWe sincerely thank projects lmm-r1 and OpenRLHF for providing their open-source resources.",
    "flymy-ai/qwen-image-edit-2509-inscene-lora": "Qwen Image Edit Inscene LoRA\nüåü About FlyMy.AI\nüöÄ Features\nüì¶ Installation\nüß™ Usage\nüîß Qwen-Image-Edit Initialization\nüîå Load LoRA Weights\nüé® Edit Image with Qwen-Image-Edit Inscene LoRA\nüñºÔ∏è Sample Output - Qwen-Image-Edit Inscene\nWorkflow Features\nüéØ What is Inscene LoRA?\nüîß Training Information\nüìä Model Specifications\nü§ù Support\nüìÑ License\nQwen Image Edit Inscene LoRA\nAn open-source LoRA (Low-Rank Adaptation) model for Qwen-Image-Edit that specializes in in-scene image editing by FlyMy.AI.\nüåü About FlyMy.AI\nAgentic Infra for GenAI. FlyMy.AI is a B2B infrastructure for building and running GenAI Media agents.\nüîó Useful Links:\nüåê Official Website\nüìö Documentation\nüí¨ Discord Community\nü§ó LoRA Training Repository\nüê¶ X (Twitter)\nüíº LinkedIn\nüì∫ YouTube\nüì∏ Instagram\nüöÄ Features\nLoRA-based fine-tuning for efficient in-scene image editing\nSpecialized for Qwen-Image-Edit model\nEnhanced control over scene composition and object positioning\nOptimized for maintaining scene coherence during edits\nCompatible with Hugging Face diffusers\nControl-based image editing with improved spatial understanding\nüì¶ Installation\nInstall required packages:\npip install torch torchvision diffusers transformers accelerate\nInstall the latest diffusers from GitHub:\npip install git+https://github.com/huggingface/diffusers\nüß™ Usage\nüîß Qwen-Image-Edit Initialization\nfrom diffusers import QwenImageEditPipeline\nimport torch\nfrom PIL import Image\n# Load the pipeline\npipeline = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit-2509\")\npipeline.to(torch.bfloat16)\npipeline.to(\"cuda\")\nüîå Load LoRA Weights\n# Load trained LoRA weights for in-scene editing\npipeline.load_lora_weights(\"flymy-ai/qwen-image-edit-2509-inscene-lora\", weight_name=\"pytorch_lora_weights.safetensors\")\nüé® Edit Image with Qwen-Image-Edit Inscene LoRA\n# Load input image\nimage = Image.open(\"./assets/qie2_input.jpg\").convert(\"RGB\")\n# Define in-scene editing prompt\nprompt = \"Make a shot in the same scene of the left hand securing the edge of the cutting board while the right hand tilts it, causing the chopped tomatoes to slide off into the pan, camera angle shifts slightly to the left to center more on the pan.\"\n# Generate edited image with enhanced scene understanding\ninputs = {\n\"image\": image,\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(0),\n\"true_cfg_scale\": 4.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 50,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"edited_image.png\")\nüñºÔ∏è Sample Output - Qwen-Image-Edit Inscene\nInput Image:\nPrompt:\n\"Make a shot in the same scene of the bottle being tilted further, zoom in slightly to capture the wine pouring into the glass as the level rises and its color becomes more pronounced, shift focus to the man's facial expression as he concentrates on the pouring, keeping the candle steady in the background\"\nOutput with Inscene LoRA:\nWorkflow Features\n‚úÖ Pre-configured for Qwen-Image-Edit + Inscene LoRA inference\n‚úÖ Optimized settings for in-scene editing quality\n‚úÖ Enhanced spatial understanding and scene coherence\n‚úÖ Easy prompt and parameter adjustment\n‚úÖ Compatible with various input image types\nüéØ What is Inscene LoRA?\nThis LoRA model is specifically trained to enhance Qwen-Image-Edit's ability to perform in-scene image editing. It focuses on:\nScene Coherence: Maintaining logical spatial relationships within the scene\nObject Positioning: Better understanding of object placement and movement\nCamera Perspective: Improved handling of viewpoint changes and camera movements\nAction Sequences: Enhanced ability to depict sequential actions within the same scene\nContextual Editing: Preserving scene context while making targeted modifications\nüîß Training Information\nThis LoRA model was trained using the FlyMy.AI LoRA Trainer with:\nBase Model: Qwen/Qwen-Image-Edit\nTraining Focus: In-scene image editing and spatial understanding\nDataset: Curated collection of scene-based editing examples (InScene dataset)\nOptimization: Low-rank adaptation for efficient fine-tuning\nüìä Model Specifications\nModel Type: LoRA (Low-Rank Adaptation)\nBase Model: Qwen/Qwen-Image-Edit\nFile Format: SafeTensors (.safetensors)\nSpecialization: In-scene image editing\nTraining Framework: Diffusers + Accelerate\nMemory Efficient: Optimized for consumer GPUs\nü§ù Support\nIf you have questions or suggestions, join our community:\nüåê FlyMy.AI\nüí¨ Discord Community\nüê¶ Follow us on X\nüíº Connect on LinkedIn\nüìß Support\n‚≠ê Don't forget to star the repository if you like it!\nüìÑ License\nThis project is licensed under the Apache 2.0 License - see the LICENSE file for details.",
    "Tabimpute/TabImpute": "README.md exists but content is empty."
}