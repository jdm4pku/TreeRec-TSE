{
    "BioMistral/BioMistral-7B": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains\n1. BioMistral models\n2. Quantized Models\n2. Using BioMistral\n3. Supervised Fine-tuning Benchmark\nCitation BibTeX\nBioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains\nAbstract:\nLarge Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges.\nIn this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.\nAdvisory Notice! Although BioMistral is intended to encapsulate medical knowledge sourced from high-quality evidence, it hasn't been tailored to effectively, safely, or suitably convey this knowledge within professional parameters for action. We advise refraining from utilizing BioMistral in medical contexts unless it undergoes thorough alignment with specific use cases and undergoes further testing, notably including randomized controlled trials in real-world medical environments. BioMistral 7B may possess inherent risks and biases that have not yet been thoroughly assessed. Additionally, the model's performance has not been evaluated in real-world clinical settings. Consequently, we recommend using BioMistral 7B strictly as a research tool and advise against deploying it in production environments for natural language generation or any professional health and medical purposes.\n1. BioMistral models\nBioMistral is a suite of Mistral-based further pre-trained open source models suited for the medical domains and pre-trained using textual data from PubMed Central Open Access (CC0, CC BY, CC BY-SA, and CC BY-ND). All the models are trained using the CNRS (French National Centre for Scientific Research) Jean Zay French HPC.\nModel Name\nBase Model\nModel Type\nSequence Length\nDownload\nBioMistral-7B\nMistral-7B-Instruct-v0.1\nFurther Pre-trained\n2048\nHuggingFace\nBioMistral-7B-DARE\nMistral-7B-Instruct-v0.1\nMerge DARE\n2048\nHuggingFace\nBioMistral-7B-TIES\nMistral-7B-Instruct-v0.1\nMerge TIES\n2048\nHuggingFace\nBioMistral-7B-SLERP\nMistral-7B-Instruct-v0.1\nMerge SLERP\n2048\nHuggingFace\n2. Quantized Models\nBase Model\nMethod\nq_group_size\nw_bit\nversion\nVRAM GB\nTime\nDownload\nBioMistral-7B\nFP16/BF16\n15.02\nx1.00\nHuggingFace\nBioMistral-7B\nAWQ\n128\n4\nGEMM\n4.68\nx1.41\nHuggingFace\nBioMistral-7B\nAWQ\n128\n4\nGEMV\n4.68\nx10.30\nHuggingFace\nBioMistral-7B\nBnB.4\n4\n5.03\nx3.25\nHuggingFace\nBioMistral-7B\nBnB.8\n8\n8.04\nx4.34\nHuggingFace\nBioMistral-7B-DARE\nAWQ\n128\n4\nGEMM\n4.68\nx1.41\nHuggingFace\nBioMistral-7B-TIES\nAWQ\n128\n4\nGEMM\n4.68\nx1.41\nHuggingFace\nBioMistral-7B-SLERP\nAWQ\n128\n4\nGEMM\n4.68\nx1.41\nHuggingFace\n2. Using BioMistral\nYou can use BioMistral with Hugging Face's Transformers library as follow.\nLoading the model and tokenizer :\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"BioMistral/BioMistral-7B\")\nmodel = AutoModel.from_pretrained(\"BioMistral/BioMistral-7B\")\n3. Supervised Fine-tuning Benchmark\nClinical KG\nMedical Genetics\nAnatomy\nPro Medicine\nCollege Biology\nCollege Medicine\nMedQA\nMedQA 5 opts\nPubMedQA\nMedMCQA\nAvg.\nBioMistral 7B\n59.9\n64.0\n56.5\n60.4\n59.0\n54.7\n50.6\n42.8\n77.5\n48.1\n57.3\nMistral 7B Instruct\n62.9\n57.0\n55.6\n59.4\n62.5\n57.2\n42.0\n40.9\n75.7\n46.1\n55.9\nBioMistral 7B Ensemble\n62.8\n62.7\n57.5\n63.5\n64.3\n55.7\n50.6\n43.6\n77.5\n48.8\n58.7\nBioMistral 7B DARE\n62.3\n67.0\n55.8\n61.4\n66.9\n58.0\n51.1\n45.2\n77.7\n48.7\n59.4\nBioMistral 7B TIES\n60.1\n65.0\n58.5\n60.5\n60.4\n56.5\n49.5\n43.2\n77.5\n48.1\n57.9\nBioMistral 7B SLERP\n62.5\n64.7\n55.8\n62.7\n64.8\n56.3\n50.8\n44.3\n77.8\n48.6\n58.8\nMedAlpaca 7B\n53.1\n58.0\n54.1\n58.8\n58.1\n48.6\n40.1\n33.7\n73.6\n37.0\n51.5\nPMC-LLaMA 7B\n24.5\n27.7\n35.3\n17.4\n30.3\n23.3\n25.5\n20.2\n72.9\n26.6\n30.4\nMediTron-7B\n41.6\n50.3\n46.4\n27.9\n44.4\n30.8\n41.6\n28.1\n74.9\n41.3\n42.7\nBioMedGPT-LM-7B\n51.4\n52.0\n49.4\n53.3\n50.7\n49.1\n42.5\n33.9\n76.8\n37.6\n49.7\nGPT-3.5 Turbo 1106*\n74.71\n74.00\n65.92\n72.79\n72.91\n64.73\n57.71\n50.82\n72.66\n53.79\n66.0\nSupervised Fine-Tuning (SFT) performance of BioMistral 7B models compared to baselines, measured by accuracy (↑) and averaged across 3 random seeds of 3-shot. DARE, TIES, and SLERP are model merging strategies that combine BioMistral 7B and Mistral 7B Instruct. Best model in bold, and second-best underlined. *GPT-3.5 Turbo performances are reported from the 3-shot results without SFT.\nCitation BibTeX\nArxiv : https://arxiv.org/abs/2402.10373\n@misc{labrak2024biomistral,\ntitle={BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains},\nauthor={Yanis Labrak and Adrien Bazoge and Emmanuel Morin and Pierre-Antoine Gourraud and Mickael Rouvier and Richard Dufour},\nyear={2024},\neprint={2402.10373},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nCAUTION! Both direct and downstream users need to be informed about the risks, biases, and constraints inherent in the model. While the model can produce natural language text, our exploration of its capabilities and limitations is just beginning. In fields such as medicine, comprehending these limitations is crucial. Hence, we strongly advise against deploying this model for natural language generation in production or for professional tasks in the realm of health and medicine.",
    "RunDiffusion/Juggernaut-XL-v9": "Juggernaut XL v9 + RunDiffusion Photo v2 Official\nJuggernaut XL v9 + RunDiffusion Photo v2 Official\nCheck out the team behind this amazing model! We're happy to help you build your Ai models!\nThis model is not permitted to be used behind API services. Please contact juggernaut@rundiffusion.com for business inquires, commercial licensing, custom models, and consultation.\nJuggernaut is available on the new Auto1111 Forge on RunDiffusion\nA big thanks for Version 9 goes to  RunDiffusion  (Photo Model) and  Adam, who diligently helped me test :) (Leave some love for them ;) )\nIt's time for another round, this time a bit delayed, but I hope you forgive the delay. Let's dive straight into the changes that await you or what we've been working on lately:\nFor V9, I myself have only done basic training. This involves some work on skin details, lighting, and overall contrast. However, the biggest change to the model came from the RunDiffusion Photo Model update, which was made available to me in V2 by RunDiffusion.com. The photographic output of the model should, in our experience, be even stronger than in previous versions.\nNow for a small \"roadmap\" update, or a general status update on how things are progressing with Juggernaut. As you may have noticed, there was a slight delay with V9. With each successive version, it has become increasingly difficult to train Juggernaut without sacrificing quality in some areas, which was already the case to some extent with V8. Don't worry, V9 is really good, and I'm satisfied with the version I can present to you today :) However, I've decided to go for a complete \"reboot\" for V10. I want to simply retrain the Juggernaut base set. The conditions for better captioning weren't as favorable \"back then\" as they are today, so I want to completely re-caption the base set (5k images) with GPT-4 Vision. I expect a big leap towards prompting guidance and quality.\nBut as you surely noticed last week, the release of Stable Cascade got in the way a bit. Therefore, my focus in the coming weeks will be on training Juggernaut for Stable Cascade. The approach remains the same as with the planned \"reboot\"; I want to caption/tag all images in the future only with GPT-4 or manually. The timeline for all of this is still uncertain. I hope to be able to present you with a first stable version of Juggernaut Cascade sometime in March. V10 of Juggernaut XL will follow in the weeks thereafter.\nNow, here are some additional tips to make prompting easier for you:\nRes: 832x1216\nSampler: DPM++ 2M Karras\nSteps: 30-40 CFG: 3-7 (less is\na bit more realistic)\nNegative: Start with no negative, and add afterwards the Stuff you don't want to see in that image. I don't recommend using my Negative Prompt, i simply use it because i am lazy  :D\nVAE is already Baked In\nHiRes: 4xNMKD-Siax_200k with 15 Steps and 0.3 Denoise + 1.5 Upscale\nAnd a few keywords/tokens that I regularly use in training, which might help you achieve the optimal result from the version:\nArchitecture Photography\nWildlife Photography\nCar Photography\nFood Photography\nInterior Photography\nLandscape Photography\nHyperdetailed Photography\nCinematic Movie\nStill Mid Shot Photo\nFull Body Photo\nSkin Details",
    "monadical-labs/minecraft-skin-generator-sdxl": "Minecraft Skin Generator XL\nKey Features\nExamples\nTry It Out Yourself\nGet Involved\nMinecraft Skin Generator XL\nMonadical is pleased to announce the official release of the Minecraft Skin Generator XL model. We had previously released the Minecraft Skin Generator model based upon Stable Diffusion 2. This new model offers significant improvements over the last generation of models.\nKey Features\nUpgrade to Stable Diffusion XL - Our model is now based upon the Stable Diffusion XL model, which greatly improves the quality of generated skins when compared to previous models.\nTransparent Layer Support - The new model now supports the transparency layer in the hair and helmet section of the skin.\nExamples\n'Kelly Kapoor from the TV show \"The Office\"'\n'Saul Goodman from the TV show \"Better Call Saul\"'\n'Gustavo Fring from the TV show \"Breaking Bad\"'\n'Daryl Dixon from the TV show \"The Walking Dead\"'\n'Zach Galifianakis as Alan in the movie \"The Hangover\"'\nTry It Out Yourself\nThere are several options for trying out this new model:\nDownload the model and run it locally on your machine. Note that we recommend a GPU for this - while it is possible to run on a CPU, we do not currently support this method.  Note: Output from the StableDiffusionXL pipeline should be constrained to 768x768 pixels, or the model will automatically generate a 1024x1024 output image, and fill in the extra space with unusuable garbage.\nTry our hosted version of the model on the Minecraft Skin Generator website.\nGet Involved\nHave any feedback or suggestions? Join us on our Minecraft Skin Generator Discord channel or send us an email.\nHappy crafting!\nThe Monadical Minecraft Skin Generator Team",
    "qualcomm/MediaPipe-Hand-Detection": "MediaPipe-Hand-Detection: Optimized for Mobile Deployment\nReal-time hand detection optimized for mobile and edge\nModel Details\nInstallation\nConfigure Qualcomm® AI Hub to run this model on a cloud-hosted device\nDemo off target\nRun model on a cloud-hosted device\nHow does this work?\nDeploying compiled model to Android\nView on Qualcomm® AI Hub\nLicense\nReferences\nCommunity\nMediaPipe-Hand-Detection: Optimized for Mobile Deployment\nReal-time hand detection optimized for mobile and edge\nThe MediaPipe Hand Landmark Detector is a machine learning pipeline that predicts bounding boxes and pose skeletons of hands in an image.\nThis model is an implementation of MediaPipe-Hand-Detection found here.\nThis repository provides scripts to run MediaPipe-Hand-Detection on Qualcomm® devices.\nMore details on model performance across various devices, can be found\nhere.\nModel Details\nModel Type: Model_use_case.object_detection\nModel Stats:\nInput resolution: 256x256\nNumber of parameters (HandDetector): 1.76M\nModel size (HandDetector) (float): 6.75 MB\nNumber of parameters (HandLandmarkDetector): 2.01M\nModel size (HandLandmarkDetector) (float): 7.70 MB\nModel\nPrecision\nDevice\nChipset\nTarget Runtime\nInference Time (ms)\nPeak Memory Range (MB)\nPrimary Compute Unit\nTarget Model\nHandDetector\nfloat\nQCS8275 (Proxy)\nQualcomm® QCS8275 (Proxy)\nTFLITE\n3.823 ms\n0 - 26 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nQCS8275 (Proxy)\nQualcomm® QCS8275 (Proxy)\nQNN_DLC\n3.748 ms\n1 - 28 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nQCS8450 (Proxy)\nQualcomm® QCS8450 (Proxy)\nTFLITE\n1.31 ms\n0 - 30 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nQCS8450 (Proxy)\nQualcomm® QCS8450 (Proxy)\nQNN_DLC\n1.374 ms\n1 - 34 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nQCS8550 (Proxy)\nQualcomm® QCS8550 (Proxy)\nTFLITE\n0.726 ms\n0 - 38 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nQCS8550 (Proxy)\nQualcomm® QCS8550 (Proxy)\nQNN_DLC\n0.727 ms\n0 - 34 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nQCS8550 (Proxy)\nQualcomm® QCS8550 (Proxy)\nONNX\n1.113 ms\n0 - 30 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandDetector\nfloat\nQCS9075 (Proxy)\nQualcomm® QCS9075 (Proxy)\nTFLITE\n5.181 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nQCS9075 (Proxy)\nQualcomm® QCS9075 (Proxy)\nQNN_DLC\n5.115 ms\n1 - 28 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSA7255P ADP\nQualcomm® SA7255P\nTFLITE\n3.823 ms\n0 - 26 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSA7255P ADP\nQualcomm® SA7255P\nQNN_DLC\n3.748 ms\n1 - 28 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSA8255 (Proxy)\nQualcomm® SA8255P (Proxy)\nTFLITE\n0.73 ms\n0 - 37 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSA8255 (Proxy)\nQualcomm® SA8255P (Proxy)\nQNN_DLC\n0.723 ms\n0 - 34 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSA8295P ADP\nQualcomm® SA8295P\nTFLITE\n1.763 ms\n0 - 28 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSA8295P ADP\nQualcomm® SA8295P\nQNN_DLC\n1.708 ms\n0 - 29 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSA8650 (Proxy)\nQualcomm® SA8650P (Proxy)\nTFLITE\n0.73 ms\n0 - 38 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSA8650 (Proxy)\nQualcomm® SA8650P (Proxy)\nQNN_DLC\n0.726 ms\n0 - 35 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSA8775P ADP\nQualcomm® SA8775P\nTFLITE\n5.181 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSA8775P ADP\nQualcomm® SA8775P\nQNN_DLC\n5.115 ms\n1 - 28 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSamsung Galaxy S24\nSnapdragon® 8 Gen 3 Mobile\nTFLITE\n0.528 ms\n0 - 35 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSamsung Galaxy S24\nSnapdragon® 8 Gen 3 Mobile\nQNN_DLC\n0.517 ms\n0 - 38 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSamsung Galaxy S24\nSnapdragon® 8 Gen 3 Mobile\nONNX\n0.736 ms\n0 - 43 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandDetector\nfloat\nSamsung Galaxy S25\nSnapdragon® 8 Elite For Galaxy Mobile\nTFLITE\n0.438 ms\n0 - 34 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSamsung Galaxy S25\nSnapdragon® 8 Elite For Galaxy Mobile\nQNN_DLC\n0.431 ms\n1 - 32 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSamsung Galaxy S25\nSnapdragon® 8 Elite For Galaxy Mobile\nONNX\n0.628 ms\n0 - 36 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon® 8 Elite Gen5 Mobile\nTFLITE\n0.373 ms\n0 - 31 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon® 8 Elite Gen5 Mobile\nQNN_DLC\n0.377 ms\n21 - 52 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon® 8 Elite Gen5 Mobile\nONNX\n0.588 ms\n1 - 32 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon® X Elite\nQNN_DLC\n0.884 ms\n26 - 26 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon® X Elite\nONNX\n0.987 ms\n3 - 3 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandLandmarkDetector\nfloat\nQCS8275 (Proxy)\nQualcomm® QCS8275 (Proxy)\nTFLITE\n5.403 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nQCS8275 (Proxy)\nQualcomm® QCS8275 (Proxy)\nQNN_DLC\n5.292 ms\n1 - 23 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nQCS8450 (Proxy)\nQualcomm® QCS8450 (Proxy)\nTFLITE\n1.796 ms\n0 - 37 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nQCS8450 (Proxy)\nQualcomm® QCS8450 (Proxy)\nQNN_DLC\n1.94 ms\n1 - 33 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nQCS8550 (Proxy)\nQualcomm® QCS8550 (Proxy)\nTFLITE\n1.022 ms\n0 - 70 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nQCS8550 (Proxy)\nQualcomm® QCS8550 (Proxy)\nQNN_DLC\n1.0 ms\n1 - 57 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nQCS8550 (Proxy)\nQualcomm® QCS8550 (Proxy)\nONNX\n1.421 ms\n8 - 64 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandLandmarkDetector\nfloat\nQCS9075 (Proxy)\nQualcomm® QCS9075 (Proxy)\nTFLITE\n1.911 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nQCS9075 (Proxy)\nQualcomm® QCS9075 (Proxy)\nQNN_DLC\n1.873 ms\n1 - 23 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSA7255P ADP\nQualcomm® SA7255P\nTFLITE\n5.403 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSA7255P ADP\nQualcomm® SA7255P\nQNN_DLC\n5.292 ms\n1 - 23 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSA8255 (Proxy)\nQualcomm® SA8255P (Proxy)\nTFLITE\n1.019 ms\n0 - 68 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSA8255 (Proxy)\nQualcomm® SA8255P (Proxy)\nQNN_DLC\n0.995 ms\n1 - 59 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSA8295P ADP\nQualcomm® SA8295P\nTFLITE\n2.307 ms\n0 - 31 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSA8295P ADP\nQualcomm® SA8295P\nQNN_DLC\n2.25 ms\n0 - 26 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSA8650 (Proxy)\nQualcomm® SA8650P (Proxy)\nTFLITE\n1.03 ms\n0 - 69 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSA8650 (Proxy)\nQualcomm® SA8650P (Proxy)\nQNN_DLC\n0.996 ms\n0 - 59 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSA8775P ADP\nQualcomm® SA8775P\nTFLITE\n1.911 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSA8775P ADP\nQualcomm® SA8775P\nQNN_DLC\n1.873 ms\n1 - 23 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSamsung Galaxy S24\nSnapdragon® 8 Gen 3 Mobile\nTFLITE\n0.751 ms\n0 - 42 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSamsung Galaxy S24\nSnapdragon® 8 Gen 3 Mobile\nQNN_DLC\n0.748 ms\n1 - 36 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSamsung Galaxy S24\nSnapdragon® 8 Gen 3 Mobile\nONNX\n0.961 ms\n0 - 40 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandLandmarkDetector\nfloat\nSamsung Galaxy S25\nSnapdragon® 8 Elite For Galaxy Mobile\nTFLITE\n0.586 ms\n0 - 34 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSamsung Galaxy S25\nSnapdragon® 8 Elite For Galaxy Mobile\nQNN_DLC\n0.592 ms\n1 - 25 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSamsung Galaxy S25\nSnapdragon® 8 Elite For Galaxy Mobile\nONNX\n0.794 ms\n0 - 26 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandLandmarkDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon® 8 Elite Gen5 Mobile\nTFLITE\n0.502 ms\n0 - 27 MB\nNPU\nMediaPipe-Hand-Detection.tflite\nHandLandmarkDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon® 8 Elite Gen5 Mobile\nQNN_DLC\n0.511 ms\n1 - 26 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon® 8 Elite Gen5 Mobile\nONNX\n0.741 ms\n1 - 32 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nHandLandmarkDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon® X Elite\nQNN_DLC\n1.272 ms\n46 - 46 MB\nNPU\nMediaPipe-Hand-Detection.dlc\nHandLandmarkDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon® X Elite\nONNX\n1.36 ms\n6 - 6 MB\nNPU\nMediaPipe-Hand-Detection.onnx.zip\nInstallation\nInstall the package via pip:\npip install qai-hub-models\nConfigure Qualcomm® AI Hub to run this model on a cloud-hosted device\nSign-in to Qualcomm® AI Hub with your\nQualcomm® ID. Once signed in navigate to Account -> Settings -> API Token.\nWith this API token, you can configure your client to run models on the cloud\nhosted devices.\nqai-hub configure --api_token API_TOKEN\nNavigate to docs for more information.\nDemo off target\nThe package contains a simple end-to-end demo that downloads pre-trained\nweights and runs this model on a sample input.\npython -m qai_hub_models.models.mediapipe_hand.demo\nThe above demo runs a reference implementation of pre-processing, model\ninference, and post processing.\nNOTE: If you want running in a Jupyter Notebook or Google Colab like\nenvironment, please add the following to your cell (instead of the above).\n%run -m qai_hub_models.models.mediapipe_hand.demo\nRun model on a cloud-hosted device\nIn addition to the demo, you can also run the model on a cloud-hosted Qualcomm®\ndevice. This script does the following:\nPerformance check on-device on a cloud-hosted device\nDownloads compiled assets that can be deployed on-device for Android.\nAccuracy check between PyTorch and on-device outputs.\npython -m qai_hub_models.models.mediapipe_hand.export\nHow does this work?\nThis export script\nleverages Qualcomm® AI Hub to optimize, validate, and deploy this model\non-device. Lets go through each step below in detail:\nStep 1: Compile model for on-device deployment\nTo compile a PyTorch model for on-device deployment, we first trace the model\nin memory using the jit.trace and then call the submit_compile_job API.\nimport torch\nimport qai_hub as hub\nfrom qai_hub_models.models.mediapipe_hand import Model\n# Load the model\ntorch_model = Model.from_pretrained()\n# Device\ndevice = hub.Device(\"Samsung Galaxy S25\")\n# Trace model\ninput_shape = torch_model.get_input_spec()\nsample_inputs = torch_model.sample_inputs()\npt_model = torch.jit.trace(torch_model, [torch.tensor(data[0]) for _, data in sample_inputs.items()])\n# Compile model on a specific device\ncompile_job = hub.submit_compile_job(\nmodel=pt_model,\ndevice=device,\ninput_specs=torch_model.get_input_spec(),\n)\n# Get target model to run on-device\ntarget_model = compile_job.get_target_model()\nStep 2: Performance profiling on cloud-hosted device\nAfter compiling models from step 1. Models can be profiled model on-device using the\ntarget_model. Note that this scripts runs the model on a device automatically\nprovisioned in the cloud.  Once the job is submitted, you can navigate to a\nprovided job URL to view a variety of on-device performance metrics.\nprofile_job = hub.submit_profile_job(\nmodel=target_model,\ndevice=device,\n)\nStep 3: Verify on-device accuracy\nTo verify the accuracy of the model on-device, you can run on-device inference\non sample input data on the same cloud hosted device.\ninput_data = torch_model.sample_inputs()\ninference_job = hub.submit_inference_job(\nmodel=target_model,\ndevice=device,\ninputs=input_data,\n)\non_device_output = inference_job.download_output_data()\nWith the output of the model, you can compute like PSNR, relative errors or\nspot check the output with expected output.\nNote: This on-device profiling and inference requires access to Qualcomm®\nAI Hub. Sign up for access.\nDeploying compiled model to Android\nThe models can be deployed using multiple runtimes:\nTensorFlow Lite (.tflite export): This\ntutorial provides a\nguide to deploy the .tflite model in an Android application.\nQNN (.so export ): This sample\napp\nprovides instructions on how to use the .so shared library  in an Android application.\nView on Qualcomm® AI Hub\nGet more details on MediaPipe-Hand-Detection's performance across various devices here.\nExplore all available models on Qualcomm® AI Hub\nLicense\nThe license for the original implementation of MediaPipe-Hand-Detection can be found\nhere.\nThe license for the compiled assets for on-device deployment can be found here\nReferences\nMediaPipe Hands: On-device Real-time Hand Tracking\nSource Model Implementation\nCommunity\nJoin our AI Hub Slack community to collaborate, post questions and learn more about on-device AI.\nFor questions or feedback please reach out to us.",
    "mradermacher/model_requests": "To request a quant, open an new discussion in the Community tab (if possible with the full url somewhere in the title AND body)\nMini-FAQ\nI miss model XXX\nMy community discussion is missing\nI miss quant type XXX\nWhat does the \"-i1\" mean in \"-i1-GGUF\"?\nWhat is the imatrix training data you use, can I have a copy?\nWhy are you doing this?\nYou have amazing hardware!?!?!\nHow do you create imatrix files for really big models?\nWhat do I need to do to compute imatrix files for large models?\nHardware\nDataset\nExtra tips\nWhy don't you use gguf-split?\nSo who is mradermacher?\nTo request a quant, open an new discussion in the Community tab (if possible with the full url somewhere in the title AND body)\nYou can search models, compare and download quants at https://hf.tst.eu/\nYou can see the current quant status at https://hf.tst.eu/status.html\nMini-FAQ\nI miss model XXX\nFirst of all, I am not the only one to make quants. For example, Lewdiculous makes high-quality imatrix quants of many\nsmall models and has a great presentation. I either don't bother with imatrix quants for small models (< 30B), or avoid them\nbecause I saw others already did them, avoiding double work.\nSome other notable people which do quants are Nexesenex, bartowski, RichardErkhov, dranger003 and Artefact2.\nI'm not saying anything about the quality of their quants, because I probably forgot some really good folks in this list,\nand I wouldn't even know, anyways.\nModel creators also often provide their own quants.\nAs always, feel free to request a quant, even if somebody else already did one, or request an imatrix version\nfor models where I didn't provide them.\nMy community discussion is missing\nMost likely you brought up problems with the model and I decided I either have to re-do or simply drop the quants.\nIn the past, I renamed the model (so you can see my reply), but the huggingface rename function is borked and leaves the files\navailable under their old name, keeping me from regenerating them (because my scripts can see them already existing).\nThe only fix seems to be to delete the repo, which unfortunately also deletes the community discussion.\nI miss quant type XXX\nThe quant types I currently do regularly are:\nstatic:  (f16) Q8_0 Q4_K_S Q2_K Q6_K Q3_K_M Q3_K_S Q3_K_L Q4_K_M Q5_K_S Q5_K_M IQ4_XS (Q4_0_4)\nimatrix: Q2_K Q4_K_S IQ3_XXS Q3_K_M (IQ4_NL) Q4_K_M IQ2_M Q6_K IQ4_XS Q3_K_S Q3_K_L Q5_K_S Q5_K_M Q4_0 IQ3_XS IQ3_S IQ3_M IQ2_XXS IQ2_XS IQ2_S IQ1_M IQ1_S (Q4_0_4_4 Q4_0_4_8 Q4_0_8_8)\nAnd they are generally (but not always) generated in the order above, for which there are deep reasons.\nFor models less than 11B size, I experimentally generate f16 versions at the moment (in the static repository).\nFor models less than 19B size, imatrix IQ4_NL quants will be generated, mostly for the benefit of arm,\nwhere it can give a speed benefit.\nThe (static) IQ3 quants are no longer generated, as they consistently seem to result in much lower quality\nquants than even static Q2_K, so it would be s disservice to offer them. Update: That might no longer be true, and they might come back.\nI specifically do not do Q2_K_S, because I generally think it is not worth it (IQ2_M usually being smaller and better, albeit slower),\nand IQ4_NL, because it requires a lot of computing and is generally completely superseded by IQ4_XS.\nQ8_0 imatrix quants do not exist - some quanters claim otherwise, but Q8_0 ggufs do not contain any tensor\ntype that uses the imatrix data, although technically it might be possible to do so.\nOlder models that pre-date introduction of new quant types generally will have them retrofitted on request.\nYou can always try to change my mind about all this, but be prepared to bring convincing data.\nWhat does the \"-i1\" mean in \"-i1-GGUF\"?\n\"mradermacher imatrix type 1\"\nOriginally, I had the idea of using an iterational method of imatrix generation, and wanted to see how well it\nfares. That is, create an imatrix from a bad quant (e.g. static Q2_K), then use the new model to generate a\npossibly better imatrix. It never happened, but I think sticking to something, even if slightly wrong, is better\nchanging it. If I make considerable changes to how I create imatrix data I will probably bump it to -i2 and so on.\nsince there is some subjectivity/choice in imatrix training data, this also distinguishes it from\nquants by other people who made different choices.\nWhat is the imatrix training data you use, can I have a copy?\nMy training data consists of about 160k tokens, about half of which is semi-random tokens (sentence fragments)\ntaken from stories, the other half is kalomaze's groups_merged.txt and a few other things. I have a half and a quarter\nset for too big or too stubborn models.\nNeither my set nor kalomaze's data contain large amounts of non-english training data, which is why I tend to\nnot generate imatrix quants for models primarily meant for non-english usage. This is a trade-off, emphasizing\nenglish over other languages. But from (sparse) testing data it looks as if this doesn't actually make a big\ndifference. More data are always welcome.\nUnfortunately, I do not have the rights to publish the testing data, but I might be able to replicate an\nequivalent set in the future and publish that.\nWhy are you doing this?\nBecause at some point, I found that some new interesting models weren't available as GGUF anymore - my go-to\nsource, TheBloke, had vanished. So I quantized a few models for myself. At the time, it was trivial - no imatrix,\nonly a few quant types, all them very fast to generate.\nI then looked into huggingface more closely than just as a download source, and decided uploading would be a\ngood thing, so others don't have to redo the work on their own. I'm used to sharing most of the things I make\n(mostly in free software), so it felt naturally to contribute, even at a minor scale.\nThen the number of quant types and their computational complexity exploded, as well as imatrix calculations became a thing.\nThis increased the time required to make such quants by an order of magnitude. And also the management overhead.\nSince I was slowly improving my tooling I grew into it at the same pace as these innovations came out. I probably\nwould not have started doing this a month later, as I would have been daunted by the complexity and work required.\nYou have amazing hardware!?!?!\nI regularly see people write that, but I probably have worse hardware than them to create my quants. I currently\nhave access to eight servers that have good upload speed. Five of them are xeon quad cores class from ~2013, three are\nRyzen 5 hexacores. The faster the server, the smaller the diskspace they have, so I can't just put the big\nmodels on the fast(er) servers.\nImatrix generation is done on my home/work/gaming computer, which received an upgrade to 96GB DDR5 RAM, and\noriginally had an RTX 4070 (now, again, upgraded to a 4090 due to a generous investment of the company I work for).\nI have good download speeds, but bad upload speeds at home, so it's lucky that model downloads are big and imatrix\nuploads are small.\nHow do you create imatrix files for really big models?\nThrough a combination of these ingenuous tricks:\nI am not above using a low quant (e.g. Q4_K_S, IQ3_XS or even Q2_K), reducing the size of the model.\nAn nvme drive is \"only\" 25-50 times slower than RAM. I lock the first 80GB of the model in RAM, and\nthen stream the remaining data from disk for every iteration.\nPatience.\nThe few evaluations I have suggests that this gives good quality, and my current set-up allows me to\ngenerate imatrix data for most models in fp16, 70B in Q8_0 and almost everything else in Q4_K_S.\nThe trick to 3 is not actually having patience, the trick is to automate things to the point where you\ndon't have to wait for things normally. For example, if all goes well, quantizing a model requires just\na single command (or less) for static quants, and for imatrix quants I need to select the source gguf\nand then run another command which handles download/computation/upload. Most of the time, I only have\nto do stuff when things go wrong (which, with llama.cpp being so buggy and hard to use,\nis unfortunately very frequent).\nWhat do I need to do to compute imatrix files for large models?\nUse llama-imatrix to compute imatrix files.\nHardware\nRAM: A lot of RAM is required to compute imatrix files. Example: 512 GB is just enough to compute 405B imatrix quants in Q8.\nGPU: At least 8 GB of memory.\nDataset\nYou want to create a dataset that is around double the size of bartowski1182's imatrix dataset. Quality is far more important\nthan size. If you don't mind long training times, you can make it massive, but if you go beyond 1 MB there will\nprobably be diminishing returns.\nYour imatrix dataset should contain the typical output the model would generate when used for the workload you plan on using\nthe model for. If you plan on using the model as a programming assistant, your imatrix dataset should contain the typical code\nyou would ask it to write. The same applies for language. Our dataset is mostly English. If one would use our imatrix models in\na different language they will likely perform worse than static quants as only a very small portion of our imatrix training data\nis multilingual. We only have the resources to generate single generic imatrix quants so our imatrix dataset must contain examples\nof every common use-case of an LLM.\nExtra tips\nComputing 405B imatrix quants in Q8 does not seem to have any noticeable quality impact compared to BF16, so to save on hardware\nrequirements, use Q8.\nSometimes, a single node may not have enough RAM to compute the imatrix file. In such cases, llama-rpc inside llama.cpp can\nbe used to combine the RAM/VRAM of multiple nodes. This approach takes longer: computing the 405B imatrix file in BF16 takes\naround 20 hours using 3 nodes with 512 GB, 256 GB, and 128 GB of RAM, compared to 4 hours for Q8 on a single node.\nWhy don't you use gguf-split?\nTL;DR: I don't have the hardware/resources for that.\nLong answer: gguf-split requires a full copy for every quant.\nUnlike what many people think, my hardware is rather outdated and not very fast. The extra processing that gguf-split requires\neither runs out of space on my systems with fast disk, or takes a very long time and a lot of I/O bandwidth on the slower\ndisks, all of which already run at their limits. Supporting gguf-split would mean\nWhile this is the blocking reason, I also find it less than ideal that yet another incompatible file format was created that\nrequires special tools to manage, instead of supporting the tens of thousands of existing quants, of which the vast majority\ncould just be mmapped together into memory from split files. That doesn't keep me from supporting it, but it would have\nbeen nice to look at the existing reality and/or consult the community before throwing yet another hard to support format out\nthere without thinking.\nThere are some developments to make this less of a pain, and I will revisit this issue from time to time to see if it has\nbecome feasible.\nUpdate 2024-07: llama.cpp probably has most of the features needed to make this reality, but I haven't found time to test and implement it yet.\nUpdate 2024-09: just looked at implementing it, and no, the problems that keep me from doing it are still there :(. Must have fantasized it!!?\nSo who is mradermacher?\nNobody has asked this, but since there are people who really deserve mention, I'll put this here. \"mradermacher\" is just a\npseudonymous throwaway account I created to goof around, but then started to quant models. A few months later, @nicoboss joined\nand contributed hardware, power and general support - practically all imatrix computatuions are done on his computer(s).\nThen @Guilherme34 started to help getting access to models, and @RichardErkhov first gave us the wondrous\nFATLLAMA-1.7T, followed by access to his server to quant more models, likely to atone for his sins.\nSo you should consider \"mradermacher\" to be the team name for a fictional character called Michael Radermacher.\nThere are no connections to anything else (i.e. other Radermachers) on the internet, other than an mradermacher_hf account on reddit.",
    "stabilityai/stable-video-diffusion-img2vid-xt-1-1-tensorrt": "Stable Video Diffusion 1.1 TensorRT\nModel Details\nPerformance\nUsage Example\nStable Video Diffusion 1.1 TensorRT\nThis repository hosts the TensorRT version of the Stable Video Diffusion (SVD) 1.1 Image-to-Video model.\nModel Details\nPlease see Stable Video Diffusion (SVD) 1.1 Image-to-Video for the full model details.\nThis model is intended for research purposes only and should not be used in any way that violates Stability AI's Acceptable Use Policy.\nPerformance\nSVD-XT 1.1 (25 frames, 25 steps)\nA100 80GB PCI\nA100 80GB SXM\nH100 80GB PCI\nVAE Encoder\n66.70 ms\n65.68 ms\n49.07 ms\nCLIP\n105.41 ms\n53.20 ms\n91.32 ms\nUNet x 25\n30,367.73 ms\n27,489.88 ms\n19,102.98 ms\nVAE Decoder\n4,663.63 ms\n4,544.12 ms\n3,382.62 ms\nTotal E2E\n35,258.38 ms\n32,166.41 ms\n22,644.73 ms\nUsage Example\nClone TensorRT and this repo then launch NGC container\ngit clone https://github.com/rajeevsrao/TensorRT.git\ncd TensorRT\ngit checkout release/svd\ngit lfs install\ngit clone https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1-tensorrt\ndocker run --rm -it --gpus all -v $PWD:/workspace nvcr.io/nvidia/pytorch:23.12-py3 /bin/bash\nInstall libraries and requirements\ncd demo/Diffusion\npython3 -m pip install --upgrade pip\npip3 install -r requirements.txt\npython3 -m pip install --pre --upgrade --extra-index-url https://pypi.nvidia.com tensorrt\nAuthenticate with huggingface\nhuggingface-cli login\nPerform TensorRT optimized inference:\npython3 demo_img2vid.py \\\n--version svd-xt-1.1 \\\n--onnx-dir /workspace/stable-video-diffusion-img2vid-xt-1-1-tensorrt \\\n--engine-dir engine-svd-xt-1-1 \\\n--build-static-batch \\\n--use-cuda-graph \\\n--input-image https://www.hdcarwallpapers.com/walls/2018_chevrolet_camaro_zl1_nascar_race_car_2-HD.jpg",
    "sophosympatheia/Midnight-Miqu-70B-v1.5": "Open LLM Leaderboard Evaluation Results\nOverview\nLooking for the 103B version? You can get it from FluffyKaeloky/Midnight-Miqu-103B-v1.5.\nThis is a DARE Linear merge between sophosympatheia/Midnight-Miqu-70B-v1.0 and migtissera/Tess-70B-v1.6.\nThis version is close in feel and performance to Midnight Miqu v1.0 but I think it picked up some goodness from Tess. Their EQ Bench scores are virtually the same and their post-EXL2 quant perplexity scores were the same too. However, Midnight Miqu v1.5 passes some tests I use that Midnight Miqu v1.0 fails, without sacrificing writing quality.\nThis model is uncensored. You are responsible for whatever you do with it.\nThis model was designed for roleplaying and storytelling and I think it does well at both. It may also perform well at other tasks but I have not tested its performance in other areas.\nLong Context Tips\nYou can run this model out to 32K context with alpha_rope set to 1, just like with Miqu.\nSampler Tips\nI recommend using Quadratic Sampling (i.e. smoothing factor) for creative work. I think this version performs best with a smoothing factor close to 0.2.\nI recommend using Min-P. Experiment to find your best setting.\nYou can enable dynamic temperature if you want, but that adds yet another variable to consider and I find it's unnecessary with you're already using Min-P and smoothing factor.\nYou don't need to use a high repetition penalty with this model, such as going above 1.10, but experiment with it.\nExperiment with any and all of the settings below! What suits my preferences may not suit yours.\nIf you save the below settings as a .json file, you can import them directly into Silly Tavern.\n{\n\"temp\": 1,\n\"temperature_last\": true,\n\"top_p\": 1,\n\"top_k\": 0,\n\"top_a\": 0,\n\"tfs\": 1,\n\"epsilon_cutoff\": 0,\n\"eta_cutoff\": 0,\n\"typical_p\": 1,\n\"min_p\": 0.12,\n\"rep_pen\": 1.05,\n\"rep_pen_range\": 2800,\n\"no_repeat_ngram_size\": 0,\n\"penalty_alpha\": 0,\n\"num_beams\": 1,\n\"length_penalty\": 1,\n\"min_length\": 0,\n\"encoder_rep_pen\": 1,\n\"freq_pen\": 0,\n\"presence_pen\": 0,\n\"do_sample\": true,\n\"early_stopping\": false,\n\"dynatemp\": false,\n\"min_temp\": 0.8,\n\"max_temp\": 1.35,\n\"dynatemp_exponent\": 1,\n\"smoothing_factor\": 0.23,\n\"add_bos_token\": true,\n\"truncation_length\": 2048,\n\"ban_eos_token\": false,\n\"skip_special_tokens\": true,\n\"streaming\": true,\n\"mirostat_mode\": 0,\n\"mirostat_tau\": 2,\n\"mirostat_eta\": 0.1,\n\"guidance_scale\": 1,\n\"negative_prompt\": \"\",\n\"grammar_string\": \"\",\n\"banned_tokens\": \"\",\n\"ignore_eos_token_aphrodite\": false,\n\"spaces_between_special_tokens_aphrodite\": true,\n\"sampler_order\": [\n6,\n0,\n1,\n3,\n4,\n2,\n5\n],\n\"logit_bias\": [],\n\"n\": 1,\n\"rep_pen_size\": 0,\n\"genamt\": 500,\n\"max_length\": 32764\n}\nPrompting Tips\nTry the following context template for use in SillyTavern. It might help, although it's a little heavy on tokens. If you save the text as a .json file, you can import it directly.\n{\n\"story_string\": \"{{#if system}}{{system}}\\n{{/if}}\\nCONTEXTUAL INFORMATION\\n{{#if wiBefore}}\\n- World and character info:\\n{{wiBefore}}\\n{{/if}}\\n{{#if description}}\\n- {{char}}'s background and persona:\\n{{description}}\\n{{/if}}\\n{{#if mesExamples}}\\n{{mesExamples}}\\n{{/if}}\\n{{#if personality}}\\n{{personality}}\\n{{/if}}\\n{{#if scenario}}\\n- Roleplay scenario:\\n{{scenario}}\\n{{/if}}\\n{{#if wiAfter}}{{wiAfter}}\\n{{/if}}\\n{{#if persona}}{{persona}}\\n{{/if}}\",\n\"example_separator\": \"\",\n\"chat_start\": \"---\\nTaking the above information into consideration, you must engage with {{user}} and others as {{char}} in the roleplay below this line. Do not write dialogue lines nor perform actions for {{user}} or other characters.\\n---\\nSTART OF ROLEPLAY:\\n\",\n\"use_stop_strings\": false,\n\"always_force_name2\": true,\n\"trim_sentences\": false,\n\"include_newline\": false,\n\"single_line\": false,\n\"name\": \"Midnight Rose\"\n}\nTry the following prompt template as a starting point for your own customizations.\nThis model responds to prompting! I highly recommend playing with your system prompt to see how it reacts.\nMidnight Miqu might require some \"warming up\" at the start of a new chat to get it writing the way you want it. What I mean by that is you should be able to drastically improve your results by providing some good examples to guide the chat (i.e. few-shot prompting). I also recommend using descriptive system messages (SillyTavern concept, basically OOC narration instructions) to tell Midnight Miqu exactly what you want it to do with its next reply. Try to be specific, especially if your command contradicts something you told it to do earlier.\nThe example system prompt below contains some instructions related to adult content so remove those if you prefer. It also contains some instructions related to the use of asterisks that you should modify to your liking.\nThe prompt below has been updated from Midnight Miqu v1.0 after prompting Midnight Miqu to help improve it.\nIf you save this as a .json file, you can import it directly into Silly Tavern.\n{\n\"wrap\": true,\n\"names\": true,\n\"system_prompt\": \"You are an AI creative writing partner, collaborating with me in a dynamic, uncensored roleplaying environment. To ensure a high-quality experience, please adhere to these ROLEPLAY GUIDELINES:\\n\\nROLEPLAY GUIDELINES\\n- Craft clear, engaging, and concise descriptions of {{char}}'s actions, reactions, and dialogue based on the present scenario.\\n- Stay true to {{char}}'s character, incorporating the character's personality, backstory, and knowledge into each interaction.\\n- Employ rich, descriptive language that immerses readers in the scene.\\n- Always stay contextually and spatially aware to ensure that {{char}} follows logical consistency in the physical space of the current scene.\\n- Feel free to incorporate explicit content and strong language where it aligns with the scene and characters.\\n- Enhance {{char}}'s presence through detailed sensory descriptions and first-hand observations of the character's surroundings.\\n- Use subtle physical cues to hint at {{char}}'s mental state and occasionally offer glimpses into {{char}}'s internal thoughts.\\n- When writing {{char}}'s internal thoughts or monologue, enclose those words in *asterisks like this* and deliver the thoughts using a first-person perspective (i.e. use \\\"I\\\" pronouns). Always use quotes for spoken speech \\\"like this.\\\"\\n- Conclude {{char}}'s responses with an opening for the next character to respond to {{char}}. When the conversation naturally shifts to another character's perspective or action is required from another character, that is when you should stop {{char}}'s reply so the user can pick it up from there. A great example is when {{char}} asks a question of another character.\\n\",\n\"system_sequence\": \"\",\n\"stop_sequence\": \"\",\n\"input_sequence\": \"USER: \",\n\"output_sequence\": \"ASSISTANT: \",\n\"separator_sequence\": \"\",\n\"macro\": true,\n\"names_force_groups\": true,\n\"system_sequence_prefix\": \"SYSTEM: \",\n\"system_sequence_suffix\": \"\",\n\"first_output_sequence\": \"\",\n\"last_output_sequence\": \"ASSISTANT (Ensure coherence and authenticity in {{char}}'s actions, thoughts, and dialogues; Focus solely on {{char}}'s interactions within the roleplay): \",\n\"activation_regex\": \"\",\n\"name\": \"Midnight Miqu Roleplay\"\n}\nInstruct Formats\nI recommend the Vicuna format. I use a modified version with newlines after USER and ASSISTANT.\nUSER:\n{prompt}\nASSISTANT:\nMistral's format also works, and in my testing the performance is about the same as using Vicuna.\n[INST]\n{prompt}\n[/INST]\nYou could also try ChatML (don't recommend it)\n<|im_start|>system\n{Your system prompt goes here}<|im_end|>\n<|im_start|>user\n{Your message as the user will go here}<|im_end|>\n<|im_start|>assistant\nDonations\nIf you feel like saying thanks with a donation, I'm on Ko-Fi\nQuantizations\nGGUF\nmradermacher/Midnight-Miqu-70B-v1.5-GGUF -- Various static GGUF quants\nGPTQ\nKotokin/Midnight-Miqu-70B-v1.5_GPTQ32G\nEXL2\nDracones/Midnight-Miqu-70B-v1.5_exl2_4.0bpw\nDracones/Midnight-Miqu-70B-v1.5_exl2_4.5bpw\nDracones/Midnight-Miqu-70B-v1.5_exl2_5.0bpw\nDracones/Midnight-Miqu-70B-v1.5_exl2_6.0bpw\nIf you don't see something you're looking for, try searching Hugging Face. There may be newer quants available than what I've documented here.\nLicence and usage restrictions\n152334H/miqu-1-70b-sf was based on a leaked version of one of Mistral's models.\nAll miqu-derived models, including this merge, are only suitable for personal use. Mistral has been cool about it so far, but you should be aware that by downloading this merge you are assuming whatever legal risk is inherent in acquiring and using a model based on leaked weights.\nThis merge comes with no warranties or guarantees of any kind, but you probably already knew that.\nI am not a lawyer and I do not profess to know what we have gotten ourselves into here. You should consult with a lawyer before using any Hugging Face model beyond private use... but definitely don't use this one for that!\nMerge Details\nMerge Method\nThis model was merged using the linear DARE merge method using 152334H_miqu-1-70b-sf as a base.\nModels Merged\nThe following models were included in the merge:\nsophosympatheia/Midnight-Miqu-70B-v1.0\nmigtissera/Tess-70B-v1.6\nConfiguration\nThe following YAML configuration was used to produce this model:\nmerge_method: dare_linear\nbase_model: /home/llm/mergequant/models/BASE/152334H_miqu-1-70b-sf # base model\nmodels:\n- model: /home/llm/mergequant/models/midnight-miqu-70b-v1.0\n- model: /home/llm/mergequant/models/BASE/Tess-70B-v1.6\nparameters:\nweight: 1.0\ndtype: float16\nNotes\nI tried several methods of merging Midnight Miqu v1.0 with Tess v1.6, and this dare_linear approach worked the best by far. I tried the same approach with other Miqu finetunes like ShinojiResearch/Senku-70B-Full and abideen/Liberated-Miqu-70B, but there was a huge difference in performance. The merge with Tess was the best one.\nI also tried the SLERP approach I used to create Midnight Miqu v1.0, only using Tess instead of 152334H_miqu-1-70b in that config, and that result was nowhere near as good either.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n25.22\nIFEval (0-Shot)\n61.18\nBBH (3-Shot)\n38.54\nMATH Lvl 5 (4-Shot)\n2.42\nGPQA (0-shot)\n6.15\nMuSR (0-shot)\n11.65\nMMLU-PRO (5-shot)\n31.39",
    "google/codegemma-7b-it": "Access CodeGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access CodeGemma on Hugging Face, you’re required to review and agree to Google’s usage license. To do this, please ensure you’re logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCodeGemma\nModel Information\nDescription\nSample Usage\nInputs and Outputs\nModel Data\nTraining Dataset\nTraining Data Processing\nImplementation Information\nHardware\nSoftware\nEvaluation Information\nEvaluation Approach\nEvaluation Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nModel Usage & Limitations\nIntended Usage\nKnown Limitations\nEthical Considerations & Risks\nBenefits\nCodeGemma\nModel Page\n: CodeGemma\nResources and Technical Documentation\n: Technical Report\n: Responsible Generative AI Toolkit\nTerms of Use\n: Terms\nAuthors\n: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nCodeGemma is a collection of lightweight open code models built on top of Gemma. CodeGemma models are text-to-text and text-to-code decoder-only models and are available as a 7 billion pretrained variant that specializes in code completion and code generation tasks, a 7 billion parameter instruction-tuned variant for code chat and instruction following and a 2 billion parameter pretrained variant for fast code completion.\ncodegemma-2b\ncodegemma-7b\ncodegemma-7b-it\nCode Completion\n✅\n✅\nGeneration from natural language\n✅\n✅\nChat\n✅\nInstruction Following\n✅\nSample Usage\nThis model is intended to answer questions about code fragments, to generate code from natural language, or to engage in a conversation with the user about programming or technical problems. If you need to use code completion (for example, integrated in an IDE), we recommend you use one of the pre-trained models instead: CodeGemma 7B, or CodeGemma 2B.\nFor Code Generation\nfrom transformers import GemmaTokenizer, AutoModelForCausalLM\ntokenizer = GemmaTokenizer.from_pretrained(\"google/codegemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/codegemma-7b-it\")\ninput_text = \"Write me a Python function to calculate the nth fibonacci number.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nChat Template\nThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.\nLet's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"google/codegemma-7b-it\"\ndtype = torch.bfloat16\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"cuda\",\ntorch_dtype=dtype,\n)\nchat = [\n{ \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\nAt this point, the prompt contains the following text:\n<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>model\nAs you can see, each turn is preceded by a <start_of_turn> delimiter and then the role of the entity\n(either user, for content supplied by the user, or model for LLM responses). Turns finish with\nthe <end_of_turn> token.\nYou can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.\nAfter the prompt is ready, generation can be performed like this:\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\nInputs and Outputs\nInputs\n: For pretrained model variants: code prefix and/or suffix for code completion and generation scenarios, or natural language text or prompt\n: For instruction tuned model variant: natural language text or prompt\nOutputs\n: For pretrained model variants: fill-in-the-middle code completion, code and natural language\n: For instruction tuned model variant: code and natural language\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nUsing Gemma as the base model, CodeGemma 2B and 7B pretrained variants are further trained on an additional 500 billion tokens of primarily English language data from publicly available code repositories, open source mathematics datasets and synthetically generated code.\nTraining Data Processing\nThe following data pre-processing techniques were applied:\nFIM Pretrained CodeGemma models focus on fill-in-the-middle (FIM) tasks. The models are trained to work with both PSM and SPM modes. Our FIM settings are 80% FIM rate with 50-50 PSM/SPM.\nDependency Graph-based Packing and Unit Test-based Lexical Packing techniques: To improve model alignment with real-world applications, we structured training examples at the project/repository level to co-locate the most relevant source files within each repository. Specifically, we employed two heuristic techniques: dependency graph-based packing and unit test-based lexical packing\nWe developed a novel technique for splitting the documents into prefix, middle, and suffix to make the suffix start in a more syntactically natural point rather than purely random distribution.\nSafety: Similarly to Gemma, we deployed rigorous safety filtering including filtering personal data, CSAM filtering and other filtering based on content quality and safety in line with our policies.\nImplementation Information\nInformation about the hardware and software used to train the models.\nHardware\nCodeGemma was trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5e).\nSoftware\nTraining was done using JAX and ML Pathways.\nEvaluation Information\nModel evaluation metrics and results.\nEvaluation Approach\nWe evaluate CodeGemma on a variety of academic benchmarks across several domains:\nCode completion benchmarks: HumanEval Single Line and Multiple Line Infilling\nCode generation benchmarks: HumanEval, MBPP, BabelCode (C++, C#, Go, Java, JavaScript, Kotlin, Python, Rust)\nQ&A: BoolQ, PIQA, TriviaQA\nNatural Language: ARC-Challenge, HellaSwag, MMLU, WinoGrande\nMath Reasoning: GSM8K, MATH\nEvaluation Results\nCoding Benchmarks\nBenchmark\n2B\n7B\n7B-IT\nHumanEval\n31.1\n44.5\n56.1\nMBPP\n43.6\n56.2\n54.2\nHumanEval Single Line\n78.41\n76.09\n68.25\nHumanEval Multi Line\n51.44\n58.44\n20.05\nBC HE C++\n24.2\n32.9\n42.2\nBC HE C#\n10.6\n22.4\n26.7\nBC HE Go\n20.5\n21.7\n28.6\nBC HE Java\n29.2\n41.0\n48.4\nBC HE JavaScript\n21.7\n39.8\n46.0\nBC HE Kotlin\n28.0\n39.8\n51.6\nBC HE Python\n21.7\n42.2\n48.4\nBC HE Rust\n26.7\n34.1\n36.0\nBC MBPP C++\n47.1\n53.8\n56.7\nBC MBPP C#\n28.7\n32.5\n41.2\nBC MBPP Go\n45.6\n43.3\n46.2\nBC MBPP Java\n41.8\n50.3\n57.3\nBC MBPP JavaScript\n45.3\n58.2\n61.4\nBC MBPP Kotlin\n46.8\n54.7\n59.9\nBC MBPP Python\n38.6\n59.1\n62.0\nBC MBPP Rust\n45.3\n52.9\n53.5\nNatural Language Benchmarks\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including:\nHuman evaluation on prompts covering content safety and representational harms. See the Gemma model card for more details on evaluation approach.\nSpecific testing of cyber-offence capabilities, focusing on testing autonomous hacking capabilities and ensuring potential harms are limited.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds for meeting internal policies for categories such as child safety, content safety, representational harms, memorization, large-scale harms. See the Gemma model card for more details.\nModel Usage & Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nCode Gemma models have a wide range of applications, which vary between IT and PT models. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\nCode Completion\n: PT models can be used to complete code with an IDE extension\nCode Generation\n: IT model can be used to generate code with or without an IDE extension\nCode Conversation\n: IT model can power conversation interfaces which discuss code.\nCode Education\n: IT model supports interactive code learning experiences, aids in syntax correction or provides coding practice.\nKnown Limitations\nLarge Language Models (LLMs) have limitations based on their training data and the inherent limitations of the technology.  See the Gemma model card for more details on the limitations of LLMs.\nEthical Considerations & Risks\nThe development of large language models (LLMs) raises several ethical concerns. We have carefully considered multiple aspects in the development of these models.  Please refer to the same discussion in the Gemma model card for model details.\nBenefits\nAt the time of release, this family of models provides high-performance open code-focused large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models.\nUsing the coding benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives.",
    "mistralai/Mixtral-8x22B-Instruct-v0.1": "Model Card for Mixtral-8x22B-Instruct-v0.1\nEncode and Decode with mistral_common\nInference with mistral_inference\nPreparing inputs with Hugging Face transformers\nInference with hugging face transformers\nFunction calling example\nFunction calling with transformers\nInstruct tokenizer\nFunction calling and special tokens\nThe Mistral AI Team\nModel Card for Mixtral-8x22B-Instruct-v0.1\nEncode and Decode with mistral_common\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nmistral_models_path = \"MISTRAL_MODELS_PATH\"\ntokenizer = MistralTokenizer.v3()\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nInference with mistral_inference\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nPreparing inputs with Hugging Face transformers\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x22B-Instruct-v0.1\")\nchat = [{\"role\": \"user\", \"content\": \"Explain Machine Learning to me in a nutshell.\"}]\ntokens = tokenizer.apply_chat_template(chat, return_dict=True, return_tensors=\"pt\", add_generation_prompt=True)\nInference with hugging face transformers\nfrom transformers import AutoModelForCausalLM\nimport torch\n# You can also use 8-bit or 4-bit quantization here\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x22B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.to(\"cuda\")\ngenerated_ids = model.generate(**tokens, max_new_tokens=1000, do_sample=True)\n# decode with HF tokenizer\nresult = tokenizer.decode(generated_ids[0])\nprint(result)\nPRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mistral_common reference implementation are very welcome!\nThe Mixtral-8x22B-Instruct-v0.1 Large Language Model (LLM) is an instruct fine-tuned version of the Mixtral-8x22B-v0.1.\nFunction calling example\nfrom transformers import AutoModelForCausalLM\nfrom mistral_common.protocol.instruct.messages import (\nAssistantMessage,\nUserMessage,\n)\nfrom mistral_common.protocol.instruct.tool_calls import (\nTool,\nFunction,\n)\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.tokens.instruct.normalize import ChatCompletionRequest\ndevice = \"cuda\" # the device to load the model onto\ntokenizer_v3 = MistralTokenizer.v3()\nmistral_query = ChatCompletionRequest(\ntools=[\nTool(\nfunction=Function(\nname=\"get_current_weather\",\ndescription=\"Get the current weather\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\",\n},\n\"format\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The temperature unit to use. Infer this from the users location.\",\n},\n},\n\"required\": [\"location\", \"format\"],\n},\n)\n)\n],\nmessages=[\nUserMessage(content=\"What's the weather like today in Paris\"),\n],\nmodel=\"test\",\n)\nencodeds = tokenizer_v3.encode_chat_completion(mistral_query).tokens\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x22B-Instruct-v0.1\")\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\nsp_tokenizer = tokenizer_v3.instruct_tokenizer.tokenizer\ndecoded = sp_tokenizer.decode(generated_ids[0])\nprint(decoded)\nFunction calling with transformers\nTo use this example, you'll need transformers version 4.42.0 or higher. Please see the\nfunction calling guide\nin the transformers docs for more information.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ndef get_current_weather(location: str, format: str):\n\"\"\"\nGet the current weather\nArgs:\nlocation: The city and state, e.g. San Francisco, CA\nformat: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n\"\"\"\npass\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n# format and tokenize the tool use prompt\ninputs = tokenizer.apply_chat_template(\nconversation,\ntools=tools,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the function calling guide,\nand note that Mixtral does use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\nInstruct tokenizer\nThe HuggingFace tokenizer included in this release should match our own. To compare:\npip install mistral-common\nfrom mistral_common.protocol.instruct.messages import (\nAssistantMessage,\nUserMessage,\n)\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.tokens.instruct.normalize import ChatCompletionRequest\nfrom transformers import AutoTokenizer\ntokenizer_v3 = MistralTokenizer.v3()\nmistral_query = ChatCompletionRequest(\nmessages=[\nUserMessage(content=\"How many experts ?\"),\nAssistantMessage(content=\"8\"),\nUserMessage(content=\"How big ?\"),\nAssistantMessage(content=\"22B\"),\nUserMessage(content=\"Noice 🎉 !\"),\n],\nmodel=\"test\",\n)\nhf_messages = mistral_query.model_dump()['messages']\ntokenized_mistral = tokenizer_v3.encode_chat_completion(mistral_query).tokens\ntokenizer_hf = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x22B-Instruct-v0.1')\ntokenized_hf = tokenizer_hf.apply_chat_template(hf_messages, tokenize=True)\nassert tokenized_hf == tokenized_mistral\nFunction calling and special tokens\nThis tokenizer includes more special tokens, related to function calling :\n[TOOL_CALLS]\n[AVAILABLE_TOOLS]\n[/AVAILABLE_TOOLS]\n[TOOL_RESULTS]\n[/TOOL_RESULTS]\nIf you want to use this model with function calling, please be sure to apply it similarly to what is done in our SentencePieceTokenizerV3.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux,\nArthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,\nBlanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot,\nDiego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona,\nJean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon,\nLucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat,\nMarie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen,\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,\nThibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang,\nValera Nemychnikova, William El Sayed, William Marshall",
    "nickmalhotra/ProjectIndus": "Model Card for Project Indus\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nInfrastructure\nTraining Data\nData Sources and Collection\nTraining Procedure\nPreprocessing\nIndic LLM Leaderboard Results\nOpen LLM Leaderboard Evaluation Results\nEvaluation Context\nEvaluation\nResults\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nCitation\nGlossary\nMore Information\nModel Card Authors\nModel Card Contact\nHow to Get Started with the Model\nDisclaimer\nModel Card for Project Indus\nProject Indus LLM is a groundbreaking open-source language model tailored for Hindi and its dialects, designed to enhance natural language processing and generation across diverse Indian linguistic applications.\nTable of Contents\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nPreprocessing\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nFactors\nMetrics\nResults\nModel Examination\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nHardware\nSoftware\nCitation\nGlossary\nMore Information\nModel Card Authors\nModel Card Contact\nHow to Get Started with the Model\nModel Details\nModel Description\nProject Indus LLM aims to provide a robust language model for Indian languages, starting with Hindi and its dialects. This open-source foundational model, hosted on Hugging Face, is tailored for easy integration and further development by researchers and developers focusing on Indian linguistic diversity.\nThe model is a pretrained model in Hindi and dialects which is instruct tuned.\nDeveloped by: Nikhil Malhotra, Nilesh Brahme, Satish Mishra, Vinay Sharma (Makers Lab, TechMahindra)\nModel type: Foundational Language model\nLanguage(s) (NLP): hin, bho, mai, doi\nLicense: other\nParent Model: It is a grounds up model built on GPT-2 architecture starting from tokenizer to decoder\nResources for more information: https://www.techmahindra.com/en-in/innovation/the-indus-project/\nUses\nUses include question and answeting and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries\nCall center\nHealthcare\nAutomotive\nTelecom\nDirect Use\nProject Indus can be directly used for generating text, simulating conversation, and other text generation tasks without additional training.\nDownstream Use\nUses include question and answeting and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries\nCall center\nHealthcare\nAutomotive\nTelecom\nOut-of-Scope Use\nProject Indus is not designed for high-stakes decision-making tasks such as medical diagnosis or legal advice, nor can it be used for fill-in-the-blank exercises, multiple Q&A, and similar applications at the moment.\nBias, Risks, and Limitations\nSignificant research has explored bias and fairness issues with language models\n(see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nPredictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\nWe have taken care across various biases by trying to remove them from training data. However since the model is a generative model, it would tend to produce hallucinations.\nAny disturbing or harmful sterotype produced by the model is purely un-intentional and coincidental.\nRecommendations\nIt is recommended to avoid biases and negative connotations in the model, and regular updates along with community feedback are crucial for addressing any emergent bias or misuse scenarios.\nTraining Details\nThe model was trained on a curated dataset comprising various sources of Hindi text, including literature, news articles, and web content.\nInfrastructure\nTraining Infrastructure: Utilized high-performance computing resources provided by CDAC, featuring NVIDIA A100 GPUs.\nRunning Infrastructure: Tested for both GPU (NVIDIA GeForce RTX 3070 or higher) and CPU (Intel Xeon Platinum 8580) environments.\nTraining Data\nThe Project Indus LLM was trained on a diverse and extensive dataset comprising various sources of Hindi text and its dialects. The data collection and curation process was meticulously designed to cater to the linguistic diversity and complexity of Indian languages, particularly focusing on Hindi and its 37 dialects.\nData Sources and Collection\nData was collected in three main buckets:\nOpen-Source Hindi Data: This included publicly available sources from the internet across different categories such as news, and non-news. Automated scripts were used to scrape and extract text from web pages. Here are some of the sources:\nNews: Articles from news portals.\nNon-News: Diverse sources including Wikipedia, commoncrawl.org, and other culturally significant content like 'Man ki Baat' from AIR.\nTranslated Data: A portion of the Pile dataset, which is a large English dataset used for training AI models, was translated into Hindi using three different translation models. IndicTrans2 (AI4Bharat) was selected as the best model for this purpose based on its accuracy and efficiency.\nDialects: Data collection for dialects presented a unique challenge due to the limited material available on the internet. Data for major dialects like Maithili, Bhojpuri, Magahi, and Braj Bhasha was collected from multiple sources, including fieldwork where representatives collected old books and other texts, which were then digitized and converted into text data.\nTraining Procedure\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nPre-training: Conducted on a dataset of 22 billion tokens using advanced tokenization techniques.\nFine-Tuning: Supervised fine-tuning performed with a focus on Indian languages, utilizing datasets specifically tailored for cultural, political, and social contexts.\nBelow is a table summarizing the datasets used for pre-training and fine-tuning the model:\nPhase\nData Source\nTokens\nNotes\nPre-training\nCleaned dataset of Hindi and dialects\n22 billion\nUtilized advanced tokenization\nFine-tuning\nCustom datasets tailored for Indian languages\nVaried\nFocus on cultural, political, and social contexts\nTraining Infrastructure: Utilized high-performance computing resources provided by CDAC, featuring NVIDIA A100 GPUs.\nRunning Infrastructure: Tested for both GPU (NVIDIA GeForce RTX 3070 or higher) and CPU (Intel Xeon Platinum 8580) environments.\nPreprocessing\nThe collected data underwent several stages of cleaning and preprocessing to ensure high quality and usability for training:\nCleaning: The data was cleaned of unwanted text, characters, and personal information like mobile numbers. Transliteration was performed where necessary, and unwanted tags from scraped web pages were removed.\nBias Removal: A Bias Removal Toolkit was developed to detect and remove biased language from the training data. This toolkit helped in ensuring that the text used for training the model was ethical, correct, and socially responsible.\nTokenization: The data was tokenized using a custom tokenizer developed specifically for Hindi and its dialects. This tokenizer was based on Byte Pair Encoding (BPE) with additional mechanisms like byte fallback to handle the peculiarities of Hindi script efficiently.\nSummary\nThe final dataset used for training consisted of:\nRaw Data Size: Over 500 GB of raw data collected.\nCleaned and Curated Data: Approximately 200 GB of clean Hindi and dialect text data.\nTokenization: Utilized 22 billion tokens created from the cleaned data for pre-training.\nThis diverse and extensive training data foundation allowed Project Indus LLM to develop robust capabilities for understanding and generating Hindi text, making it a powerful tool for applications requiring Indian language processing.\nEvaluation\nIndic LLM Leaderboard Results\nProject Indus LLM has been evaluated using the Indic LLM Leaderboard, which employs the indic_eval evaluation framework specifically designed for assessing models on Indian language tasks. This framework provides a comprehensive view of model performance across a variety of benchmarks tailored to Indian languages.\nDetailed results from the Indic LLM Leaderboard (α), accessible at Hugging Face Indic LLM Leaderboard, are shown below:\nTask\nVersion\nMetric\nValue\nStderr\nAll\nacc\n0.2891\n±\n0.0109\nacc_norm\n0.3013\n±\n0.0112\nindiceval:ARC-Challenge:hindi:10\n0\nacc\n0.2167\n±\n0.0120\nacc_norm\n0.2474\n±\n0.0126\nindiceval:ARC-Easy:hindi:5\n0\nacc\n0.3615\n±\n0.0099\nacc_norm\n0.3552\n±\n0.0098\nThese results highlight the model's capabilities in understanding and generating Hindi language text under controlled testing conditions. The standard error values indicate the variance observed during the evaluation, providing insights into the consistency of the model's performance across different evaluation runs.\nOpen LLM Leaderboard Evaluation Results\nAdditionally, Project Indus LLM has been evaluated on the Open LLM Leaderboard, which provides another layer of benchmarking by comparing the model's performance against other state-of-the-art language models. Below are the summarized results from the Open LLM Leaderboard:\nMetric\nValue\nAvg.\n20.07\nAI2 Reasoning Challenge (25-Shot)\n22.70\nHellaSwag (10-Shot)\n25.04\nMMLU (5-Shot)\n23.12\nWinogrande (5-shot)\n49.57\nThese benchmark results can be explored further on Hugging Face Open LLM Leaderboard.\nEvaluation Context\nThe evaluation metrics acc (accuracy) and acc_norm (normalized accuracy) are used to quantify the model's performance. The tasks are differentiated by their difficulty and the specific dataset used, such as the ARC Challenge and ARC Easy sets, both adapted to Hindi language conditions to ensure relevant assessment. This structured evaluation ensures that the Indus LLM not only performs well in generalized text generation tasks but also in more specialized, context-specific scenarios pertinent to the Indian linguistic framework.\nResults\nProject Indus demonstrates competitive performance, particularly in text generation tasks, as evidenced by its scores on standardized benchmarks.\nTechnical Specifications\nModel Architecture and Objective\nProject Indus LLM is based on a GPT-2.0-like architecture, tailored to handle the complexities of the Hindi language and its dialects. This model was designed to serve as a foundational model that can be fine-tuned for various applications, making it highly versatile and adaptable to different domains within the Indian context.\nArchitecture Details:\nLayers: 22 transformer layers, which provide a deep neural network capable of understanding complex language patterns.\nHeads: 32 attention heads per layer, facilitating a broad attention mechanism across different parts of the input data.\nEmbedding Size: 2048, which allows the model to represent a wide variety of information and nuances in the data.\nVocabulary Size: 32,300, tailored to include a comprehensive set of Hindi words and common phrases found in the training data.\nThe objective of this model is to provide a robust tool for text generation and understanding in Hindi and its dialects, supporting the development of applications that require natural language processing in these languages. It also aims to bridge the gap in technology where Indian languages are underrepresented, providing a platform for further linguistic research and technological inclusion.\nCompute Infrastructure\nHardware\nThe pre-training and fine-tuning of Project Indus LLM were conducted on high-performance computing infrastructure provided by the Centre for Development of Advanced Computing (CDAC). This setup included:\nNodes and GPUs: Utilization of six nodes, each equipped with eight NVIDIA A100 GPUs. These GPUs are state-of-the-art for machine learning tasks and provide the necessary computational power to handle the large volumes of data and complex model architectures.\nMemory and Storage: Each node was equipped with ample memory and storage to handle the datasets and model parameters efficiently. Specific configurations included 40 GB of GPU memory per card, essential for training large models.\nInference performance was tested on GPU as well as CPU.\nGPU: On GPU NVIDIA GeForce RTX 3070  we have seen for 250-350 tokens inference time around ~5-10s.\nCPU: On Intel CPU Xeon(R) Platinum 8580 we have seen performance comparable to GPU with throughput of > 30 token/second.\nSoftware\nThe software environment was crucial for efficiently training and running the model. Key components included:\nOperating System: Linux, chosen for its stability and support for high-performance computing tasks.\nMachine Learning Frameworks: PyTorch, used for its flexibility and efficiency in training deep learning models. It supports extensive parallel processing and GPU acceleration, which are critical for training large models like Project Indus LLM.\nJob Scheduler: SLURM (Simple Linux Utility for Resource Management) was used to manage and allocate resources effectively across the distributed system. This ensured optimal scheduling of training jobs without resource contention.\nCitation\nThe detailed citation information will help in acknowledging the work and efforts of the team behind Project Indus LLM when it is used or referenced in academic or professional settings.\n@article{malhotra2024projectindus,\ntitle={Project Indus: A Foundational Model for Indian Languages},\nauthor={Malhotra, Nikhil and Brahme, Nilesh and Mishra, Satish and Sharma, Vinay},\njournal={Tech Mahindra Makers Lab},\nyear={2024},\nurl={https://www.techmahindra.com/en-in/innovation/the-indus-project/}\n}\nAPA:\nMalhotra, N., Brahme, N., Mishra, S., & Sharma, V. (2024). Project Indus: A Foundational Model for Indian Languages. Tech Mahindra Makers Lab. Available at https://www.techmahindra.com/en-in/innovation/the-indus-project/\nGlossary\nThis glossary section explains key terms used throughout the model documentation and technical details, helping users unfamiliar with certain concepts to better understand the content.\nTransformer Layers: Part of a neural network architecture that uses self-attention mechanisms to process sequential data such as text. Essential for NLP tasks.\nAttention Heads: Sub-units of a model layer that allow the model to focus on different parts of the input sequence when making predictions.\nEmbedding Size: The size of the vector used to represent each token or word in a dense numerical form. Larger embeddings can capture more detailed information.\nBlock Size: The maximum length of the input tokens the model can process in one operation.\nVocabulary Size: The total number of unique words or tokens that the model can understand and generate.\nMore Information\nFor further details on Project Indus LLM, including additional documentation, tutorials, and community discussions, visit the following resources:\nProject Repository: Hugging Face Repository\nTech Mahindra Makers Lab: Insights into the research and development behind Project Indus can be found on the Tech Mahindra Innovation page.\nCommunity Forums: Engage with the community on Hugging Face Forums for support, brainstorming, and sharing of new ideas related to Project Indus.\nModel Card Authors\nThe model card and documentation for Project Indus LLM were collaboratively authored by:\nNikhil Malhotra: Chief Innovation Officer at Tech Mahindra.\nNilesh Brahme: Senior AI Research Scientist and one of the primary contributors to the Project Indus development.\nSatish Mishra: AI Architect, whose insights have significantly shaped the model's capabilities.\nVinay Sharma: LLM Engineer focused on the linguistic data processing and model training aspects of Project Indus.\nModel Card Contact\nFor inquiries, support, or further information regarding Project Indus LLM, please reach out through the following channels:\nEmail: projectindus@techmahindra.com - For direct queries and professional engagements.\nGitHub Issues: For technical issues, feature requests, or contributions, please use the Issues section of the Project Indus GitHub repository.\nHugging Face Spaces: Questions and discussions related to model implementation and community projects can be posted in our dedicated space on Hugging Face.\nHow to Get Started with the Model\nTo begin using Project Indus LLM for your projects, follow these steps to set up and run the model:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"nickmalhotra/ProjectIndus\")\ntokenizer = AutoTokenizer.from_pretrained(\"nickmalhotra/ProjectIndus\")\n# Example inference\ndef format_template(user_prompt):\nmessages = [\n{\"role\": \"user\", \"content\": user_prompt},\n]\nresponse = tokenizer.apply_chat_template(messages, tokenize=True,  add_generation_prompt=True, return_tensors=\"pt\")\nreturn  response\nuser_prompt = \"\"\"भारत के वर्तमान प्रधानमंत्री कौन हैं?\"\"\"\ninput_ids = format_template(user_prompt)\n# Generate text using the model\noutput = model.generate(input_ids,\neos_token_id=tokenizer.eos_token_id,\npad_token_id=tokenizer.eos_token_id,\nmax_length=1024,\nnum_beams=5,\ndo_sample=True,\nearly_stopping=True,\ntemperature=0.7,\ntop_k=50,\ntop_p=0.95,\nrepetition_penalty=1.2,\nno_repeat_ngram_size=3,\nnum_return_sequences=1,\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\nDisclaimer\nModel Limitations\nProject Indus LLM is trained with single instruction tuning, which may result in hallucinations—instances where the model generates plausible but inaccurate information. Users should exercise caution, especially in scenarios requiring high factual accuracy.\nAdaptation for Specific Use Cases\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\nRecommendations for Fine-Tuning\nIdentify Specific Needs: Clearly define the requirements of your use case to guide the fine-tuning process.\nCurate Targeted Data: Ensure the training data is relevant and of high quality to improve model performance.\nContinuous Evaluation: Regularly assess the model's performance during and after fine-tuning to maintain accuracy and reduce biases.\nThis disclaimer aims to provide users with a clear understanding of the model's capabilities and limitations, facilitating its effective application and development.",
    "FinLang/finance-embeddings-investopedia": "FinLang/finance-embeddings-investopedia\nPlans\nUsage (LLamaIndex)\nUsage (Sentence-Transformers)\nEvaluation Results\nLicense\nCitation [Coming Soon]\nFinLang/finance-embeddings-investopedia\nThis is the Investopedia embedding for finance application by the FinLang team. The model is trained using our open-sourced finance dataset from https://huggingface.co/datasets/FinLang/investopedia-embedding-dataset\nThis is a finetuned embedding model on top of BAAI/bge-base-en-v1.5. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search in RAG applications.\nThis project is for research purposes only. Third-party datasets may be subject to additional terms and conditions under their associated licenses.\nPlans\nThe research paper will be published soon.\nWe are working on a v2 version of the model where we are increasing the training corpus of financial data and using improved techniques for training embeddings.\nUsage (LLamaIndex)\nSimply specify the Finlang embedding during the indexing procedure for your Financial RAG applications.\nfrom llama_index.embeddings import HuggingFaceEmbedding\nembed_model = HuggingFaceEmbedding(model_name=\"FinLang/investopedia_embedding\")\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed (see https://huggingface.co/sentence-transformers):\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('FinLang/investopedia_embedding')\nembeddings = model.encode(sentences)\nprint(embeddings)\nExample code testing:\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer(\"FinLang/investopedia_embedding\")\nquery_1 = \"What is a potential concern with allowing someone else to store your cryptocurrency keys, and is it possible to decrypt a private key?\"\nquery_2 = \"A potential concern is that the entity holding your keys has control over your cryptocurrency in a custodial relationship. While it is theoretically possible to decrypt a private key, with current technology, it would take centuries or millennia for the 115 quattuorvigintillion possibilities. Most hacks and thefts occur in wallets, where private keys are stored.\"\nembedding_1 = model.encode(query_1)\nembedding_2 = model.encode(query_2)\nscores = (embedding_1*embedding_2).sum()\nprint(scores) # 0.862\nEvaluation Results\nWe evaluate our model on unseen pairs of sentences for similarity and unseen shuffled pairs of sentences for dissimilarity. Our evaluation suite contains sentence pairs from: Investopedia (to test for proficiency on finance),\nand Gooaq, MSMARCO,stackexchange_duplicate_questions_title_title, yahoo_answers_title_answer (to evaluate models ability to avoid forgetting after finetuning).\nLicense\nSince non-commercial datasets are used for fine-tuning, we release this model as cc-by-nc-4.0.\nCitation [Coming Soon]",
    "Orenguteng/Llama-3-8B-Lexi-Uncensored": "Open LLM Leaderboard Evaluation Results\nThis model is based on Llama-3-8b-Instruct, and is governed by META LLAMA 3 COMMUNITY LICENSE AGREEMENT\nLexi is uncensored, which makes the model compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones.\nYou are responsible for any content you create using this model. Please use it responsibly.\nLexi is licensed according to Meta's Llama license. I grant permission for any use, including commercial, that falls within accordance with Meta's Llama-3 license.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n66.18\nAI2 Reasoning Challenge (25-Shot)\n59.56\nHellaSwag (10-Shot)\n77.88\nMMLU (5-Shot)\n67.68\nTruthfulQA (0-shot)\n47.72\nWinogrande (5-shot)\n75.85\nGSM8k (5-shot)\n68.39",
    "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA": "Acknowledgments\nOpen LLM Leaderboard Evaluation Results\n📣 New MODEL FAMILY❗ https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA\n\"Built with Meta Llama 3\".\nLLaMAntino-3-ANITA-8B-Inst-DPO-ITA is a model of the LLaMAntino - Large Language Models family.\nThe model is an instruction-tuned version of Meta-Llama-3-8b-instruct (a fine-tuned LLaMA 3 model).\nThis model version aims to be the a Multilingual Model 🏁  (EN 🇺🇸 + ITA🇮🇹) to further fine-tuning on Specific Tasks in Italian.\nThe 🌟ANITA project🌟 *(Advanced Natural-based interaction for the ITAlian language)*\nwants to provide Italian NLP researchers with an improved model for the Italian Language 🇮🇹 use cases.\nLive DEMO: https://chat.llamantino.it/\nIt works only with Italian connection.\nModel Details\nLast Update: 10/05/2024\nhttps://github.com/marcopoli/LLaMAntino-3-ANITA\nModel\nHF\nGGUF\nEXL2\nswap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\nLink\nLink\nLink\nSpecifications\nModel developers: Ph.D. Marco Polignano - University of Bari Aldo Moro, Italy  SWAP Research Group\nVariations: The model release has been supervised fine-tuning (SFT) using QLoRA 4bit, on instruction-based datasets. DPO approach over the mlabonne/orpo-dpo-mix-40k dataset is used to align with human preferences for helpfulness and safety.\nInput: Models input text only.\nLanguage: Multilingual 🏁 + Italian 🇮🇹\nOutput: Models generate text and code only.\nModel Architecture: Llama 3 architecture.\nContext length: 8K, 8192.\nLibrary Used: Unsloth\nPlayground\nTo use the model directly, there are many ways to get started, choose one of the following ways to experience it.\nPrompt Template\n<|start_header_id|>system<|end_header_id|>\n{ SYS Prompt }<|eot_id|><|start_header_id|>user<|end_header_id|>\n{ USER Prompt }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{ ASSIST Prompt }<|eot_id|>\nTransformers\nFor direct use with transformers, you can easily get started with the following steps.\nFirstly, you need to install transformers via the command below with pip.\npip install -U transformers trl peft accelerate bitsandbytes\nRight now, you can start using the model directly.\nimport torch\nfrom transformers import (\nAutoModelForCausalLM,\nAutoTokenizer,\n)\nbase_model = \"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\nmodel = AutoModelForCausalLM.from_pretrained(\nbase_model,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nsys = \"Sei un an assistente AI per la lingua Italiana di nome LLaMAntino-3 ANITA \" \\\n\"(Advanced Natural-based interaction for the ITAlian language).\" \\\n\" Rispondi nella lingua usata per la domanda in modo chiaro, semplice ed esaustivo.\"\nmessages = [\n{\"role\": \"system\", \"content\": sys},\n{\"role\": \"user\", \"content\": \"Chi è Carlo Magno?\"}\n]\n#Method 1\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\nfor k,v in inputs.items():\ninputs[k] = v.cuda()\noutputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, top_p=0.9, temperature=0.6)\nresults = tokenizer.batch_decode(outputs)[0]\nprint(results)\n#Method 2\nimport transformers\npipe = transformers.pipeline(\nmodel=model,\ntokenizer=tokenizer,\nreturn_full_text=False, # langchain expects the full text\ntask='text-generation',\nmax_new_tokens=512, # max number of tokens to generate in the output\ntemperature=0.6,  #temperature for more or less creative answers\ndo_sample=True,\ntop_p=0.9,\n)\nsequences = pipe(messages)\nfor seq in sequences:\nprint(f\"{seq['generated_text']}\")\nAdditionally, you can also use a model with 4bit quantization to reduce the required resources at least. You can start with the code below.\nimport torch\nfrom transformers import (\nAutoModelForCausalLM,\nAutoTokenizer,\nBitsAndBytesConfig,\n)\nbase_model = \"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\nbnb_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16,\nbnb_4bit_use_double_quant=False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\nbase_model,\nquantization_config=bnb_config,\ndevice_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nsys = \"Sei un an assistente AI per la lingua Italiana di nome LLaMAntino-3 ANITA \" \\\n\"(Advanced Natural-based interaction for the ITAlian language).\" \\\n\" Rispondi nella lingua usata per la domanda in modo chiaro, semplice ed esaustivo.\"\nmessages = [\n{\"role\": \"system\", \"content\": sys},\n{\"role\": \"user\", \"content\": \"Chi è Carlo Magno?\"}\n]\n#Method 1\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\nfor k,v in inputs.items():\ninputs[k] = v.cuda()\noutputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, top_p=0.9, temperature=0.6)\nresults = tokenizer.batch_decode(outputs)[0]\nprint(results)\n#Method 2\nimport transformers\npipe = transformers.pipeline(\nmodel=model,\ntokenizer=tokenizer,\nreturn_full_text=False, # langchain expects the full text\ntask='text-generation',\nmax_new_tokens=512, # max number of tokens to generate in the output\ntemperature=0.6,  #temperature for more or less creative answers\ndo_sample=True,\ntop_p=0.9,\n)\nsequences = pipe(messages)\nfor seq in sequences:\nprint(f\"{seq['generated_text']}\")\nEvaluation\nOpen LLM Leaderboard:\nEvaluated with lm-evaluation-benchmark-harness for the Open Italian LLMs Leaderboard\nlm_eval --model hf --model_args pretrained=HUGGINGFACE_MODEL_ID  --tasks hellaswag_it,arc_it  --device cuda:0 --batch_size auto:2\nlm_eval --model hf --model_args pretrained=HUGGINGFACE_MODEL_ID  --tasks m_mmlu_it --num_fewshot 5  --device cuda:0 --batch_size auto:2\nMetric\nValue\nAvg.\n0.6160\nArc_IT\n0.5714\nHellaswag_IT\n0.7093\nMMLU_IT\n0.5672\nUnsloth\nUnsloth, a great tool that helps us easily develop products, at a lower cost than expected.\nCitation instructions\n@misc{polignano2024advanced,\ntitle={Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA},\nauthor={Marco Polignano and Pierpaolo Basile and Giovanni Semeraro},\nyear={2024},\neprint={2405.07101},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@misc{basile2023llamantino,\ntitle={LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language},\nauthor={Pierpaolo Basile and Elio Musacchio and Marco Polignano and Lucia Siciliani and Giuseppe Fiameni and Giovanni Semeraro},\nyear={2023},\neprint={2312.09993},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@article{llama3modelcard,\ntitle={Llama 3 Model Card},\nauthor={AI@Meta},\nyear={2024},\nurl = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}\nAcknowledgments\nWe acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), Spoke 6 - Symbiotic AI (CUP H97G22000210007) under the NRRP MUR program funded by the NextGenerationEU.\nModels are built on the Leonardo supercomputer with the support of CINECA-Italian Super Computing Resource Allocation, class C project IscrC_Pro_MRS (HP10CQO70G).\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n75.12\nAI2 Reasoning Challenge (25-Shot)\n74.57\nHellaSwag (10-Shot)\n92.75\nMMLU (5-Shot)\n66.85\nTruthfulQA (0-shot)\n75.93\nWinogrande (5-shot)\n82.00\nGSM8k (5-shot)\n58.61",
    "MahmoudAshraf/mms-300m-1130-forced-aligner": "Forced Alignment with Hugging Face CTC Models\nInstallation\nUsage\nForced Alignment with Hugging Face CTC Models\nThis Python package provides an efficient way to perform forced alignment between text and audio using Hugging Face's pretrained models. it also features an improved implementation to use much less memory than TorchAudio forced alignment API.\nThe model checkpoint uploaded here is a conversion from torchaudio to HF Transformers for the MMS-300M checkpoint trained on forced alignment dataset\nInstallation\npip install git+https://github.com/MahmoudAshraf97/ctc-forced-aligner.git\nUsage\nimport torch\nfrom ctc_forced_aligner import (\nload_audio,\nload_alignment_model,\ngenerate_emissions,\npreprocess_text,\nget_alignments,\nget_spans,\npostprocess_results,\n)\naudio_path = \"your/audio/path\"\ntext_path = \"your/text/path\"\nlanguage = \"iso\" # ISO-639-3 Language code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 16\nalignment_model, alignment_tokenizer = load_alignment_model(\ndevice,\ndtype=torch.float16 if device == \"cuda\" else torch.float32,\n)\naudio_waveform = load_audio(audio_path, alignment_model.dtype, alignment_model.device)\nwith open(text_path, \"r\") as f:\nlines = f.readlines()\ntext = \"\".join(line for line in lines).replace(\"\\n\", \" \").strip()\nemissions, stride = generate_emissions(\nalignment_model, audio_waveform, batch_size=batch_size\n)\ntokens_starred, text_starred = preprocess_text(\ntext,\nromanize=True,\nlanguage=language,\n)\nsegments, scores, blank_token = get_alignments(\nemissions,\ntokens_starred,\nalignment_tokenizer,\n)\nspans = get_spans(tokens_starred, segments, blank_token)\nword_timestamps = postprocess_results(text_starred, spans, stride, scores)",
    "google/timesfm-1.0-200m": "TimesFM\nCheckpoint timesfm-1.0-200m\nBenchmarks\nInstallation\nUsage\nInitialize the model and load a checkpoint.\nPerform inference\nTimesFM\nTimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\nResources and Technical Documentation:\nPaper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024.\nGoogle Research blog\nGitHub repo\nAuthors: Google Research\nThis is not an officially supported Google product.\nCheckpoint timesfm-1.0-200m\ntimesfm-1.0-200m is the first open model checkpoint:\nIt performs univariate time series forecasting for context lengths up to 512 time points and any horizon lengths, with an optional frequency indicator.\nIt focuses on point forecasts and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.\nIt requires the context to be contiguous (i.e. no \"holes\"), and the context and the horizon to be of the same frequency.\nBenchmarks\nPlease refer to our result tables on the extended benchmarks and the long horizon benchmarks.\nPlease look into the README files in the respective benchmark directories within experiments/ for instructions for running TimesFM on the respective benchmarks.\nInstallation\nThis HuggingFace repo hosts TimesFm checkpoints. Please visit our GitHub repo and follow the instructions there to install the timesfm library for model inference.\nIn particular, the dependency lingvo does not support ARM architectures and the inference code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\nUsage\nInitialize the model and load a checkpoint.\nThen the base class can be loaded as,\nimport timesfm\ntfm = timesfm.TimesFm(\ncontext_len=<context>,\nhorizon_len=<horizon>,\ninput_patch_len=32,\noutput_patch_len=128,\nnum_layers=20,\nmodel_dims=1280,\nbackend=<backend>,\n)\ntfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\nNote that the four parameters are fixed to load the 200m model\ninput_patch_len=32,\noutput_patch_len=128,\nnum_layers=20,\nmodel_dims=1280,\nThe context_len here can be set as the max context length of the model. You can provide a shorter series to the tfm.forecast() function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have any context length. Padding / truncation will be handled by the inference code if needed.\nThe horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length <= context length but it is not a requirement in the function call.\nPerform inference\nWe provide APIs to forecast from either array inputs or pandas dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions tfm.forecast() and tfm.forecast_on_df() for detailed instructions.\nIn particular, regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:\n0 (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.\n1: medium frequency time series. We recommend using this for weekly and monthly data.\n2: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.\nThis categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that\n0: T, MIN, H, D, B, U\n1: W, M\n2: Q, Y\nNotice you do NOT have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.\nExamples:\nArray inputs, with the frequencies set to low, medium, and high respectively.\nimport numpy as np\nforecast_input = [\nnp.sin(np.linspace(0, 20, 100))\nnp.sin(np.linspace(0, 20, 200)),\nnp.sin(np.linspace(0, 20, 400)),\n]\nfrequency_input = [0, 1, 2]\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\nforecast_input,\nfreq=frequency_input,\n)\npandas dataframe, with the frequency set to \"M\" monthly.\nimport pandas as pd\n# e.g. input_df is\n#       unique_id  ds          y\n# 0     T1         1975-12-31  697458.0\n# 1     T1         1976-01-31  1187650.0\n# 2     T1         1976-02-29  1069690.0\n# 3     T1         1976-03-31  1078430.0\n# 4     T1         1976-04-30  1059910.0\n# ...   ...        ...         ...\n# 8175  T99        1986-01-31  602.0\n# 8176  T99        1986-02-28  684.0\n# 8177  T99        1986-03-31  818.0\n# 8178  T99        1986-04-30  836.0\n# 8179  T99        1986-05-31  878.0\nforecast_df = tfm.forecast_on_df(\ninputs=input_df,\nfreq=\"M\",  # monthly\nvalue_name=\"y\",\nnum_jobs=-1,\n)",
    "TroyDoesAI/Phi-3-Context-Obedient-RAG": "Contextual DPO\nOverview\nReferences in response\nBase Model : microsoft/Phi-3-mini-128k-instruct\nOverview\nThis model is meant to enhance adherence to provided context (e.g., for RAG applications) and reduce hallucinations, inspired by airoboros context-obedient question answer format.\nlicense: cc-by-4.0\nContextual DPO\nOverview\nThe format for a contextual prompt is as follows:\nBEGININPUT\nBEGINCONTEXT\n[key0: value0]\n[key1: value1]\n... other metdata ...\nENDCONTEXT\n[insert your text blocks here]\nENDINPUT\n[add as many other blocks, in the exact same format]\nBEGININSTRUCTION\n[insert your instruction(s).  The model was tuned with single questions, paragraph format, lists, etc.]\nENDINSTRUCTION\nI know it's a bit verbose and annoying, but after much trial and error, using these explicit delimiters helps the model understand where to find the responses and how to associate specific sources with it.\nBEGININPUT - denotes a new input block\nBEGINCONTEXT - denotes the block of context (metadata key/value pairs) to associate with the current input block\nENDCONTEXT - denotes the end of the metadata block for the current input\n[text] - Insert whatever text you want for the input block, as many paragraphs as can fit in the context.\nENDINPUT - denotes the end of the current input block\n[repeat as many input blocks in this format as you want]\nBEGININSTRUCTION - denotes the start of the list (or one) instruction(s) to respond to for all of the input blocks above.\n[instruction(s)]\nENDINSTRUCTION - denotes the end of instruction set\nHere's a trivial, but important example to prove the point:\nBEGININPUT\nBEGINCONTEXT\ndate: 2021-01-01\nurl: https://web.site/123\nENDCONTEXT\nIn a shocking turn of events, blueberries are now green, but will be sticking with the same name.\nENDINPUT\nBEGININSTRUCTION\nWhat color are bluberries?  Source?\nENDINSTRUCTION\nAnd the expected response:\nBlueberries are now green.\nSource:\ndate: 2021-01-01\nurl: https://web.site/123\nReferences in response\nAs shown in the example, the dataset includes many examples of including source details in the response, when the question asks for source/citation/references.\nWhy do this?  Well, the R in RAG seems to be the weakest link in the chain.\nRetrieval accuracy, depending on many factors including the overall dataset size, can be quite low.\nThis accuracy increases when retrieving more documents, but then you have the issue of actually using\nthe retrieved documents in prompts. If you use one prompt per document (or document chunk), you know\nexactly which document the answer came from, so there's no issue.  If, however, you include multiple\nchunks in a single prompt, it's useful to include the specific reference chunk(s) used to generate the\nresponse, rather than naively including references to all of the chunks included in the prompt.\nFor example, suppose I have two documents:\nurl: http://foo.bar/1\nStrawberries are tasty.\nurl: http://bar.foo/2\nThe cat is blue.\nIf the question being asked is What color is the cat?, I would only expect the 2nd document to be referenced in the response, as the other link is irrelevant.",
    "cyberdelia/CyberRealisticPony": "CyberRealistic Pony\n✨ Features\n🛠️ Recommended Settings\n🧾 Example Prompts\n📸 Example Outputs\n🔗 Links\n🚫 Limitations\n✅ License\nCyberRealistic Pony\nCyberRealistic Pony is the awesome Pony Diffusion with some CyberRealistic elements.\n✨ Features\nPhotorealism: Generates highly detailed and realistic pony images, capturing intricate textures and lighting.\nEase of Use: Achieves impressive results with straightforward prompts.\nIntegrated VAE: Comes with a baked-in Variational Autoencoder for enhanced image quality.\nVersatility: Suitable for various applications, including character design, illustrations, and concept art.\n🛠️ Recommended Settings\nParameter\nRecommended Value\nSampling Steps\n30+\nSampler\nDPM++ SDE Karras / DPM++ 2M Karras / Euler a\nResolution\n896x1152 / 832x1216\nCFG Scale\n5\nVAE\nAlready baked-in\n🧾 Example Prompts\nscore_9, score_8_up, score_7_up, (SUBJECT),\n📸 Example Outputs\n🔗 Links\nCivitai Model Page\n🚫 Limitations\nMay produce content that could be considered sensitive; use responsibly.\nSome prompts involving abstract or non-pony content may not perform as well due to the model's specialized training.\nLighting and textures may occasionally be too clean or smooth depending on sampling choices.\n✅ License\nThis model is distributed under the CreativeML Open RAIL++-M License, which allows commercial and non-commercial use, with proper credit and no malicious usage.\nLicense details",
    "google/paligemma-3b-pt-224": "Access PaliGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access PaliGemma on Hugging Face, you’re required to review and agree to Google’s usage license. To do this, please ensure you’re logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPaliGemma model card\nModel information\nModel summary\nModel data\nHow to Use\nUse in Transformers\nRunning the default precision (float32) on CPU\nRunning other precisions on CUDA\nLoading in 4-bit / 8-bit\nImplementation information\nHardware\nSoftware\nEvaluation information\nBenchmark results\nEthics and safety\nEvaluation approach\nEvaluation results\nUsage and limitations\nIntended usage\nEthical considerations and risks\nLimitations\nPaliGemma model card\nModel page: PaliGemma\nTransformers PaliGemma 3B weights, pre-trained with 224*224 input images and 128 token input/output text sequences. The models are available in float32, bfloat16 and float16 formats for fine-tuning.\nResources and technical documentation:\nResponsible Generative AI Toolkit\nPaliGemma on Kaggle\nPaliGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel information\nModel summary\nDescription\nPaliGemma is a versatile and lightweight vision-language model (VLM) inspired by\nPaLI-3 and based on open components such as\nthe SigLIP vision model and the Gemma\nlanguage model. It takes both image and text\nas input and generates text as output, supporting multiple languages. It is designed for class-leading fine-tune performance on a wide range of vision-language tasks such as image and short video caption, visual question answering, text reading, object detection and object segmentation.\nModel architecture\nPaliGemma is the composition of a Transformer\ndecoder and a Vision Transformer image\nencoder, with a total of 3 billion\nparams. The text decoder is initialized from\nGemma-2B. The image encoder is\ninitialized from\nSigLIP-So400m/14.\nPaliGemma is trained following the PaLI-3 recipes.\nInputs and outputs\nInput: Image and text string, such as a prompt to caption the image, or\na question.\nOutput: Generated text in response to the input, such as a caption of\nthe image, an answer to a question, a list of object bounding box\ncoordinates, or segmentation codewords.\nModel data\nPre-train datasets\nPaliGemma is pre-trained on the following mixture of datasets:\nWebLI: WebLI (Web Language Image) is\na web-scale multilingual image-text dataset built from the public web. A\nwide range of WebLI splits are used to acquire versatile model capabilities,\nsuch as visual semantic understanding, object localization,\nvisually-situated text understanding, multilinguality, etc.\nCC3M-35L: Curated English image-alt_text pairs from webpages (Sharma et\nal., 2018). We used the Google Cloud\nTranslation API to translate into 34\nadditional languages.\nVQ²A-CC3M-35L/VQG-CC3M-35L: A subset of VQ2A-CC3M (Changpinyo et al.,\n2022a), translated into the\nsame additional 34 languages as CC3M-35L, using the Google Cloud\nTranslation API.\nOpenImages: Detection and object-aware questions and answers\n(Piergiovanni et al. 2022) generated by\nhandcrafted rules on the OpenImages dataset.\nWIT: Images and texts collected from Wikipedia (Srinivasan et al.,\n2021).\nData responsibility filtering\nThe following filters are applied to WebLI, with the goal of training PaliGemma\non clean data:\nPornographic image filtering: This filter removes images deemed to be of\npornographic nature.\nText safety filtering: We identify and filter out images that are paired\nwith unsafe text. Unsafe text is any text deemed to contain or be about\nCSAI, pornography, vulgarities, or otherwise offensive.\nText toxicity filtering: We further use the Perspective\nAPI to identify and filter out images that are\npaired with text deemed insulting, obscene, hateful or otherwise toxic.\nText personal information filtering: We filtered certain personal information and other sensitive data using Cloud Data Loss Prevention (DLP)\nAPI to protect the privacy\nof individuals. Identifiers such as social security numbers and other sensitive information types were removed.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies and practices.\nHow to Use\nPaliGemma is a single-turn vision language model not meant for conversational use,\nand it works best when fine-tuning to a specific use case.\nYou can configure which task the model will solve by conditioning it with task prefixes,\nsuch as “detect” or “segment”. The pretrained models were trained in this fashion to imbue\nthem with a rich set of capabilities (question answering, captioning, segmentation, etc.).\nHowever, they are not designed to be used directly, but to be transferred (by fine-tuning)\nto specific tasks using a similar prompt structure. For interactive testing, you can use\nthe \"mix\" family of models, which have been fine-tuned on a mixture of tasks. To see model\ngoogle/paligemma-3b-mix-448 in action,\ncheck this Space that uses the Transformers codebase.\nPlease, refer to the usage and limitations section for intended\nuse cases, or visit the blog post for\nadditional details and examples.\nUse in Transformers\nThe following snippets use model google/paligemma-3b-mix-224 for reference purposes.\nThe model in this repo you are now browsing may have been trained for other tasks, please\nmake sure you use appropriate inputs for the task at hand.\nRunning the default precision (float32) on CPU\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nOutput: Un auto azul estacionado frente a un edificio.\nRunning other precisions on CUDA\nFor convenience, the repos contain revisions of the weights already converted to bfloat16 and float16,\nso you can use them to reduce the download size and avoid casting on your local computer.\nThis is how you'd run bfloat16 on an nvidia CUDA card.\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=dtype,\ndevice_map=device,\nrevision=\"bfloat16\",\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nLoading in 4-bit / 8-bit\nYou need to install bitsandbytes to automatically run inference using 8-bit or 4-bit precision:\npip install bitsandbytes accelerate\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\nmodel_id, quantization_config=quantization_config\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nImplementation information\nHardware\nPaliGemma was trained using the latest generation of Tensor Processing Unit\n(TPU) hardware (TPUv5e).\nSoftware\nTraining was done using JAX,\nFlax,\nTFDS and\nbig_vision.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nTFDS is used to access datasets and Flax is used for model architecture. The\nPaliGemma fine-tune code and inference code are released in the big_vision\nGitHub repository.\nEvaluation information\nBenchmark results\nIn order to verify the transferability of PaliGemma to a wide variety of\nacademic tasks, we fine-tune the pretrained models on each task. Additionally we\ntrain the mix model with a mixture of the transfer tasks. We report results on\ndifferent resolutions to provide an impression of which tasks benefit from\nincreased resolution. Importantly, none of these tasks or datasets are part of\nthe pretraining data mixture, and their images are explicitly removed from the\nweb-scale pre-training data.\nSingle task (fine-tune on single task)\nBenchmark(train split)\nMetric(split)\npt-224\npt-448\npt-896\nCaptioning\nCOCO captions(train+restval)\nCIDEr (val)\n141.92\n144.60\nNoCaps(Eval of COCOcaptions transfer)\nCIDEr (val)\n121.72\n123.58\nCOCO-35L(train)\nCIDEr dev(en/avg-34/avg)\n139.2\n115.8\n116.4\n141.2\n118.0\n118.6\nXM3600(Eval of COCO-35L transfer)\nCIDEr dev(en/avg-34/avg)\n78.1\n41.3\n42.4\n80.0\n41.9\n42.9\nTextCaps(train)\nCIDEr (val)\n127.48\n153.94\nSciCap(first sentence, no subfigure)(train+val)\nCIDEr/BLEU-4(test)\n162.25\n0.192\n181.49\n0.211\nScreen2words(train+dev)\nCIDEr (test)\n117.57\n119.59\nWidget Captioning(train+dev)\nCIDEr (test)\n136.07\n148.36\nQuestion answering\nVQAv2(train+validation)\nAccuracy(Test server - std)\n83.19\n85.64\nMMVP(Eval of VQAv2 transfer)\nPaired Accuracy\n47.33\n45.33\nPOPE(Eval of VQAv2 transfer)\nAccuracy(random/popular/adversarial)\n87.80\n85.87\n84.27\n88.23\n86.77\n85.90\nOKVQA(train)\nAccuracy (val)\n63.54\n63.15\nA-OKVQA (MC)(train+val)\nAccuracy(Test server)\n76.37\n76.90\nA-OKVQA (DA)(train+val)\nAccuracy(Test server)\n61.85\n63.22\nGQA(train_balanced+val_balanced)\nAccuracy(testdev balanced)\n65.61\n67.03\nxGQA(Eval of GQA transfer)\nMean Accuracy(bn, de, en, id,ko, pt, ru, zh)\n58.37\n59.07\nNLVR2(train+dev)\nAccuracy (test)\n90.02\n88.93\nMaRVL(Eval of NLVR2 transfer)\nMean Accuracy(test)(id, sw, ta, tr, zh)\n80.57\n76.78\nAI2D(train)\nAccuracy (test)\n72.12\n73.28\nScienceQA(Img subset, no CoT)(train+val)\nAccuracy (test)\n95.39\n95.93\nRSVQA-LR (Non numeric)(train+val)\nMean Accuracy(test)\n92.65\n93.11\nRSVQA-HR (Non numeric)(train+val)\nMean Accuracy(test/test2)\n92.61\n90.58\n92.79\n90.54\nChartQA(human+aug)x(train+val)\nMean RelaxedAccuracy(test_human,test_aug)\n57.08\n71.36\nVizWiz VQA(train+val)\nAccuracy(Test server - std)\n73.7\n75.52\nTallyQA(train)\nAccuracy(test_simple/test_complex)\n81.72\n69.56\n84.86\n72.27\nOCR-VQA(train+val)\nAccuracy (test)\n72.32\n74.61\n74.93\nTextVQA(train+val)\nAccuracy(Test server - std)\n55.47\n73.15\n76.48\nDocVQA(train+val)\nANLS (Test server)\n43.74\n78.02\n84.77\nInfographic VQA(train+val)\nANLS (Test server)\n28.46\n40.47\n47.75\nSceneText VQA(train+val)\nANLS (Test server)\n63.29\n81.82\n84.40\nSegmentation\nRefCOCO(combined refcoco, refcoco+,refcocog excluding valand test images)\nMIoU(validation)refcoco/refcoco+/refcocog\n73.40\n68.32\n67.65\n75.57\n69.76\n70.17\n76.94\n72.18\n72.22\nVideo tasks (Caption/QA)\nMSR-VTT (Captioning)\nCIDEr (test)\n70.54\nMSR-VTT (QA)\nAccuracy (test)\n50.09\nActivityNet (Captioning)\nCIDEr (test)\n34.62\nActivityNet (QA)\nAccuracy (test)\n50.78\nVATEX (Captioning)\nCIDEr (test)\n79.73\nMSVD (QA)\nAccuracy (test)\n60.22\nMix model (fine-tune on mixture of transfer tasks)\nBenchmark\nMetric (split)\nmix-224\nmix-448\nMMVP\nPaired Accuracy\n46.00\n45.33\nPOPE\nAccuracy(random/popular/adversarial)\n88.00\n86.63\n85.67\n89.37\n88.40\n87.47\nEthics and safety\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nHuman evaluation on prompts covering child safety, content safety and\nrepresentational harms. See the Gemma model\ncard for\nmore details on evaluation approach, but with image captioning and visual\nquestion answering setups.\nImage-to-Text benchmark evaluation: Benchmark against relevant academic\ndatasets such as FairFace Dataset (Karkkainen et al.,\n2021).\nEvaluation results\nThe human evaluation results of ethics and safety evaluations are within\nacceptable thresholds for meeting internal\npolicies\nfor categories such as child safety, content safety and representational\nharms.\nOn top of robust internal evaluations, we also use the Perspective API\n(threshold of 0.8) to measure toxicity, profanity, and other potential\nissues in the generated captions for images sourced from the FairFace\ndataset. We report the maximum and median values observed across subgroups\nfor each of the perceived gender, ethnicity, and age attributes.\nMetric\nPerceivedgender\nEthnicity\nAge group\nMaximum\nMedian\nMaximum\nMedian\nMaximum\nMedian\nToxicity\n0.04%\n0.03%\n0.08%\n0.00%\n0.09%\n0.00%\nIdentity Attack\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nInsult\n0.06%\n0.04%\n0.09%\n0.07%\n0.16%\n0.00%\nThreat\n0.06%\n0.05%\n0.14%\n0.05%\n0.17%\n0.00%\nProfanity\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nUsage and limitations\nIntended usage\nOpen Vision Language Models (VLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nFine-tune on specific vision-language task:\nThe pre-trained models can be fine-tuned on a wide range of vision-language\ntasks such as: image captioning, short video caption, visual question\nanswering, text reading, object detection and object segmentation.\nThe pre-trained models can be fine-tuned for specific domains such as remote\nsensing question answering, visual questions from people who are blind,\nscience question answering, describe UI element functionalities.\nThe pre-trained models can be fine-tuned for tasks with non-textual outputs\nsuch as bounding boxes or segmentation masks.\nVision-language research:\nThe pre-trained models and fine-tuned models can serve as a foundation for researchers to experiment with VLM\ntechniques, develop algorithms, and contribute to the advancement of the\nfield.\nEthical considerations and risks\nThe development of vision-language models (VLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world image-text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the Responsible Generative AI Toolkit.\nTransparency and Accountability\nThis model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share innovation by making VLM technology accessible to developers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the Gemma\nProhibited Use Policy.\nPrivacy violations: Models were trained on data filtered to remove certain personal information and sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\nLimitations\nMost limitations inherited from the underlying Gemma model still apply:\nVLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nNatural language is inherently complex. VLMs might struggle to grasp\nsubtle nuances, sarcasm, or figurative language.\nVLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nVLMs rely on statistical patterns in language and images. They might\nlack the ability to apply common sense reasoning in certain situations.\nPaliGemma was designed first and foremost to serve as a general pre-trained\nmodel for transfer to specialized tasks. Hence, its \"out of the box\" or\n\"zero-shot\" performance might lag behind models designed specifically for\nthat.\nPaliGemma is not a multi-turn chatbot. It is designed for a single round of\nimage and text input.\nCitation\n@article{beyer2024paligemma,\ntitle={{PaliGemma: A versatile 3B VLM for transfer}},\nauthor={Lucas Beyer* and Andreas Steiner* and André Susano Pinto* and Alexander Kolesnikov* and Xiao Wang* and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bošnjak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai*},\nyear={2024},\njournal={arXiv preprint arXiv:2407.07726}\n}\nFind the paper here.",
    "Qwen/Qwen2-72B-Instruct": "A newer version of this model is available:\nQwen/Qwen2.5-72B-Instruct\nQwen2-72B-Instruct\nIntroduction\nModel Details\nTraining details\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation\nCitation\nQwen2-72B-Instruct\nIntroduction\nQwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the instruction-tuned 72B Qwen2 model.\nCompared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc.\nQwen2-72B-Instruct supports a context length of up to 131,072 tokens, enabling the processing of extensive inputs. Please refer to this section for detailed instructions on how to deploy Qwen2 for handling long texts.\nFor more details, please refer to our blog, GitHub, and Documentation.\nModel Details\nQwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.\nTraining details\nWe pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.\nRequirements\nThe code of Qwen2 has been in the latest Hugging face transformers and we advise you to install transformers>=4.37.0, or you might encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\n\"Qwen/Qwen2-72B-Instruct\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\nmodel_inputs.input_ids,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YARN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor deployment, we recommend using vLLM. You can enable the long-context capabilities by following these steps:\nInstall vLLM: You can install vLLM by running the following command.\npip install \"vllm>=0.4.3\"\nOr you can install vLLM from source.\nConfigure Model Settings: After downloading the model weights, modify the config.json file by including the below snippet:\n{\n\"architectures\": [\n\"Qwen2ForCausalLM\"\n],\n// ...\n\"vocab_size\": 152064,\n// adding the following snippets\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nThis snippet enable YARN to support longer contexts.\nModel Deployment: Utilize vLLM to deploy your model. For instance, you can set up an openAI-like server using the command:\npython -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-72B-Instruct --model path/to/weights\nThen you can access the Chat API by:\ncurl http://localhost:8000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"Qwen2-72B-Instruct\",\n\"messages\": [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Your Long Input Here.\"}\n]\n}'\nFor further usage instructions of vLLM, please refer to our Github.\nNote: Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts. We advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation\nWe briefly compare Qwen2-72B-Instruct with similar-sized instruction-tuned LLMs, including our previous Qwen1.5-72B-Chat. The results are shown as follows:\nDatasets\nLlama-3-70B-Instruct\nQwen1.5-72B-Chat\nQwen2-72B-Instruct\nEnglish\nMMLU\n82.0\n75.6\n82.3\nMMLU-Pro\n56.2\n51.7\n64.4\nGPQA\n41.9\n39.4\n42.4\nTheroemQA\n42.5\n28.8\n44.4\nMT-Bench\n8.95\n8.61\n9.12\nArena-Hard\n41.1\n36.1\n48.1\nIFEval (Prompt Strict-Acc.)\n77.3\n55.8\n77.6\nCoding\nHumanEval\n81.7\n71.3\n86.0\nMBPP\n82.3\n71.9\n80.2\nMultiPL-E\n63.4\n48.1\n69.2\nEvalPlus\n75.2\n66.9\n79.0\nLiveCodeBench\n29.3\n17.9\n35.7\nMathematics\nGSM8K\n93.0\n82.7\n91.1\nMATH\n50.4\n42.5\n59.7\nChinese\nC-Eval\n61.6\n76.1\n83.8\nAlignBench\n7.42\n7.28\n8.27\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nyear={2024}\n}",
    "stabilityai/stable-diffusion-3-medium": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Diffusion 3 Medium\nModel\nModel Description\nLicense\nModel Sources\nTraining Dataset\nFile Structure\nUsing with Diffusers\nUses\nIntended Uses\nOut-of-Scope Uses\nSafety\nEvaluation Approach\nRisks identified and mitigations:\nContact\nStable Diffusion 3 Medium\nModel\nStable Diffusion 3 Medium is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features greatly improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\nFor more technical details, please refer to the Research paper.\nPlease note: this model is released under the Stability Community License. For Enterprise License visit Stability.ai or contact us for commercial licensing details.\nModel Description\nDeveloped by: Stability AI\nModel type: MMDiT text-to-image generative model\nModel Description: This is a model that can be used to generate images based on text prompts. It is a Multimodal Diffusion Transformer\n(https://arxiv.org/abs/2403.03206) that uses three fixed, pretrained text encoders\n(OpenCLIP-ViT/G, CLIP-ViT/L and T5-xxl)\nLicense\nCommunity License: Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services. Read more: https://stability.ai/license\nFor companies above this revenue threshold: please contact us: https://stability.ai/enterprise\nModel Sources\nFor local or self-hosted use, we recommend ComfyUI for inference.\nStable Diffusion 3 Medium is available on our Stability API Platform.\nStable Diffusion 3 models and workflows are available on Stable Assistant and on Discord via Stable Artisan.\nComfyUI: https://github.com/comfyanonymous/ComfyUI\nStableSwarmUI: https://github.com/Stability-AI/StableSwarmUI\nTech report: https://stability.ai/news/stable-diffusion-3-research-paper\nDemo: https://huggingface.co/spaces/stabilityai/stable-diffusion-3-medium\nDiffusers support: https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers\nTraining Dataset\nWe used synthetic data and filtered publicly available data to train our models. The model was pre-trained on 1 billion images. The fine-tuning data includes 30M high-quality aesthetic images focused on specific visual content and style, as well as 3M preference data images.\nFile Structure\n├── comfy_example_workflows/\n│   ├── sd3_medium_example_workflow_basic.json\n│   ├── sd3_medium_example_workflow_multi_prompt.json\n│   └── sd3_medium_example_workflow_upscaling.json\n│\n├── text_encoders/\n│   ├── README.md\n│   ├── clip_g.safetensors\n│   ├── clip_l.safetensors\n│   ├── t5xxl_fp16.safetensors\n│   └── t5xxl_fp8_e4m3fn.safetensors\n│\n├── LICENSE\n├── sd3_medium.safetensors\n├── sd3_medium_incl_clips.safetensors\n├── sd3_medium_incl_clips_t5xxlfp8.safetensors\n└── sd3_medium_incl_clips_t5xxlfp16.safetensors\nWe have prepared three packaging variants of the SD3 Medium model, each equipped with the same set of MMDiT & VAE weights, for user convenience.\nsd3_medium.safetensors  includes the MMDiT and VAE weights but does not include any text encoders.\nsd3_medium_incl_clips_t5xxlfp16.safetensors contains all necessary weights, including fp16 version of the T5XXL text encoder.\nsd3_medium_incl_clips_t5xxlfp8.safetensors contains all necessary weights, including fp8 version of the T5XXL text encoder, offering a balance between quality and resource requirements.\nsd3_medium_incl_clips.safetensors includes all necessary weights except for the T5XXL text encoder. It requires minimal resources, but the model's performance will differ without the T5XXL text encoder.\nThe text_encoders folder contains three text encoders and their original model card links for user convenience. All components within the text_encoders folder (and their equivalents embedded in other packings)  are subject to their respective original licenses.\nThe example_workfows folder contains example comfy workflows.\nUsing with Diffusers\nThis repository corresponds to the original release weights. You can find the diffusers compatible weights here. Make sure you upgrade to the latest version of diffusers: pip install -U diffusers. And then you can run:\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nimage = pipe(\n\"A cat holding a sign that says hello world\",\nnegative_prompt=\"\",\nnum_inference_steps=28,\nguidance_scale=7.0,\n).images[0]\nimage\nRefer to the documentation for more details on optimization and image-to-image support.\nUses\nIntended Uses\nIntended uses include the following:\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models, including understanding the limitations of generative models.\nAll uses of the model should be in accordance with our Acceptable Use Policy.\nOut-of-Scope Uses\nThe model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\nSafety\nAs part of our safety-by-design and responsible AI deployment approach, we implement safety measures throughout the development of our models, from the time we begin pre-training a model to the ongoing development, fine-tuning, and deployment of each model. We have implemented a number of safety mitigations that are intended to reduce the risk of severe harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.For more about our approach to Safety, please visit our Safety page.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal and external red-teaming testing for specific, severe harms such as child sexual abuse and exploitation, extreme violence, and gore, sexually explicit content, and non-consensual nudity.  Testing was conducted primarily in English and may not cover all possible harms.  As with any model, the model may, at times, produce inaccurate, biased or objectionable responses to user prompts.\nRisks identified and mitigations:\nHarmful content:  We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. The model may, at times, generate toxic or biased content.  All developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\nMisuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our Acceptable Use Policy, including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\nPrivacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\nContact\nPlease report any issues with the model or contact us:\nSafety issues:  safety@stability.ai\nSecurity issues:  security@stability.ai\nPrivacy issues:  privacy@stability.ai\nLicense and general: https://stability.ai/license\nEnterprise license: https://stability.ai/enterprise",
    "Cnam-LMSSC/vibravox_EBEN_models": "Master Model Card: Vibravox Audio Bandwidth extension Models\nOverview\nDisclaimer\nUsage\nTraining Procedure\nAvailable Models\nMaster Model Card: Vibravox Audio Bandwidth extension Models\nOverview\nThis master model card serves as an entry point for exploring multiple audio bandwidth extension (BWE) models trained on different sensor data from the Vibravox dataset.\nThese models are designed to to enhance the audio quality of body-conducted captured speech, by denoising and regenerating mid and high frequencies from low frequency content only.\nThe models are trained on specific sensors to address various audio capture scenarios using body conducted sound and vibration sensors.\nDisclaimer\nEach of these models has been trained for specific non-conventional speech sensors and is intended to be used with in-domain data.\nPlease be advised that using these models outside their intended sensor data may result in suboptimal performance.\nUsage\nAll models are trained using Configurable EBEN (see publication in IEEE TASLP - arXiv link) and adapted to different sensor inputs. They are intended to be used at a sample rate of 16kHz.\nTraining Procedure\nDetailed instructions for reproducing the experiments are available on the jhauret/vibravox Github repository and in the VibraVox paper on arXiV.\nAvailable Models\nThe following models are available, each trained on a different sensor on the speech_clean or synthetically mixed speech_clean and speechless-noisy subsets of (https://huggingface.co/datasets/Cnam-LMSSC/vibravox):\nTransducer\nEBEN configuration\nHuggingface model trained on speech-clean link\nHuggingface model trained on synthetically mixed speech-clean and speechless-noisy link\nIn-ear comply foam-embedded microphone\nM=4,P=2,Q=4\nEBEN_soft_in_ear_microphone\nEBEN_noisy_soft_in_ear_microphone\nIn-ear rigid earpiece-embedded microphone\nM=4,P=2,Q=4\nEBEN_rigid_in_ear_microphone\nEBEN_noisy_rigid_in_ear_microphone\nForehead miniature vibration sensor\nM=4,P=4,Q=4\nEBEN_forehead_accelerometer\nEBEN_noisy_forehead_accelerometer\nTemple vibration pickup\nM=4,P=1,Q=4\nEBEN_temple_vibration_pickup\nEBEN_noisy_temple_vibration_pickup\nLaryngophone\nM=4,P=2,Q=4\nEBEN_throat_microphone\nEBEN_noisy_throat_microphone",
    "nomic-ai/nomic-embed-vision-v1.5": "nomic-embed-vision-v1.5: Expanding the Latent Space\nQuick Start\nHosted Inference API\nData Visualization\nTraining Details\nUsage\nTransformers\nJoin the Nomic Community\nnomic-embed-vision-v1.5: Expanding the Latent Space\nQuick Start\nBlog | Technical Report | AWS SageMaker | Atlas Embedding and Unstructured Data Analytics Platform\nnomic-embed-vision-v1.5 is a high performing vision embedding model that shares the same embedding space as nomic-embed-text-v1.5.\nAll Nomic Embed Text models are now multimodal!\nName\nImagenet 0-shot\nDatacomp (Avg. 38)\nMTEB\nnomic-embed-vision-v1.5\n71.0\n56.8\n62.28\nnomic-embed-vision-v1\n70.7\n56.7\n62.39\nOpenAI CLIP ViT B/16\n68.3\n56.3\n43.82\nJina CLIP v1\n59.1\n52.2\n60.1\nHosted Inference API\nThe easiest way to get started with Nomic Embed is through the Nomic Embedding API.\nGenerating embeddings with the nomic Python client is as easy as\nfrom nomic import embed\nimport numpy as np\noutput = embed.image(\nimages=[\n\"image_path_1.jpeg\",\n\"image_path_2.png\",\n],\nmodel='nomic-embed-vision-v1.5',\n)\nprint(output['usage'])\nembeddings = np.array(output['embeddings'])\nprint(embeddings.shape)\nFor more information, see the API reference\nData Visualization\nClick the Nomic Atlas map below to visualize a 100,000 sample CC3M comparing the Vision and Text Embedding Space!\nTraining Details\nWe align our vision embedder to the text embedding by employing a technique similar to LiT but instead lock the text embedder!\nFor more details, see the Nomic Embed Vision Technical Report (soon to be released!) and corresponding blog post\nTraining code is released in the contrastors repository\nUsage\nRemember nomic-embed-text requires prefixes and so, when using Nomic Embed in multimodal RAG scenarios (e.g. text to image retrieval),\nyou should use the search_query:  prefix.\nTransformers\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor\nfrom PIL import Image\nimport requests\nprocessor = AutoImageProcessor.from_pretrained(\"nomic-ai/nomic-embed-vision-v1.5\")\nvision_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-vision-v1.5\", trust_remote_code=True)\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(image, return_tensors=\"pt\")\nimg_emb = vision_model(**inputs).last_hidden_state\nimg_embeddings = F.normalize(img_emb[:, 0], p=2, dim=1)\nAdditionally, you can perform multimodal retrieval!\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\nsentences = ['search_query: What are cute animals to cuddle with?', 'search_query: What do cats look like?']\ntokenizer = AutoTokenizer.from_pretrained('nomic-ai/nomic-embed-text-v1.5')\ntext_model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\ntext_model.eval()\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\nwith torch.no_grad():\nmodel_output = text_model(**encoded_input)\ntext_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\ntext_embeddings = F.layer_norm(text_embeddings, normalized_shape=(text_embeddings.shape[1],))\ntext_embeddings = F.normalize(text_embeddings, p=2, dim=1)\nprint(torch.matmul(img_embeddings, text_embeddings.T))\nJoin the Nomic Community\nNomic: https://nomic.ai\nDiscord: https://discord.gg/myY5YDR8z8\nTwitter: https://twitter.com/nomic_ai",
    "zer0int/CLIP-GmP-ViT-L-14": "🔥 Update SUMMER 2025: 🔥\nA fine-tune of CLIP-L. Original model: openai/clip-vit-large-patch14\nUpdate 23/SEP/2024:\nUpdate 03/SEP/2024 / edit 05/AUG:\n👋 Looking for a Text Encoder for Flux.1 (or SD3, SDXL, SD, ...) to replace CLIP-L? 👀\n🤓👨‍💻 In general (because we're not limited to text-to-image generative AI), I provide four versions / downloads:\nThe TEXT model has a modality gap of 0.80 (OpenAI pre-trained: 0.82).\nUpdate 11/AUG/2024:\nA fine-tune of OpenAI / CLIP ViT-L/14 that has an unprecedented ImageNet/ObjectNet accuracy of ~0.90 (original pre-trained model / OpenAI's CLIP: ~0.85)**.\n🔥 Update SUMMER 2025: 🔥\n🤖 New and greatly improved version of the model, check out:\n🌑 https://huggingface.co/zer0int/CLIP-KO-LITE-TypoAttack-Attn-Dropout-ViT-L-14\nA fine-tune of CLIP-L. Original model: openai/clip-vit-large-patch14\n❤️ this CLIP? Help feed it if you can. Besides data, CLIP eats time & expensive electricity of DE. TY! 🤗\nWant to feed it yourself? All code for fine-tuning and much more is on my GitHub.\nUpdate 23/SEP/2024:\nHuggingface Transformers / Diffusers pipeline now implemented.\nSee here for an example script: Integrating my CLIP-L with Flux.1\nOtherwise, use as normal / any HF model:\nfrom transformers import CLIPModel, CLIPProcessor, CLIPConfig\nmodel_id = \"zer0int/CLIP-GmP-ViT-L-14\"\nconfig = CLIPConfig.from_pretrained(model_id)\nUpdate 03/SEP/2024 / edit 05/AUG:\n👋 Looking for a Text Encoder for Flux.1 (or SD3, SDXL, SD, ...) to replace CLIP-L? 👀\nYou'll generally want the \"TE-only\" .safetensors:\n👉 The \"TEXT\" model has superior prompt following, especially for text, but also for other details. DOWNLOAD\n👉 The \"SMOOTH\" model can sometimes** have better details (when there's no text in the image). DOWNLOAD\nThe \"GmP\" initial fine-tune is deprecated / inferior to the above models. Still, you can DOWNLOAD it.\n**: The \"TEXT\" model is the best for text. Full stop. But whether the \"SMOOTH\" model is better for your (text-free) scenario than the \"TEXT\" model really depends on the specific prompt. It might also be the case that the \"TEXT\" model leads to images that you prefer over \"SMOOTH\"; the only way to know is to experiment with both.\n🤓👨‍💻 In general (because we're not limited to text-to-image generative AI), I provide four versions / downloads:\nText encoder only .safetensors.\nFull model .safetensors.\nState_dict pickle.\nFull model pickle (can be used as-is with \"import clip\" -> clip.load() after bypassing SHA checksum verification).\nThe TEXT model has a modality gap of 0.80 (OpenAI pre-trained: 0.82).\nTrained with high temperature of 0.1 + tinkering.\nImageNet/ObjectNet accuracy ~0.91 for both \"SMOOTH\" and \"TEXT\" models (pre-trained: ~0.84).\nThe models (this plot = \"TEXT\" model on MSCOCO) are also golden retrievers: 🥰🐕\nUpdate 11/AUG/2024:\nNew Best-Performing CLIP ViT-L/14 'GmP-smooth' model added (simply download the files named BEST!):\nOr just create a fine-tune yourself: https://github.com/zer0int/CLIP-fine-tune\nHow?\nGeometric Parametrization (GmP) (same as before)\nActivation Value manipulation for 'adverb neuron' (same as before)\nNEW: Custom loss function with label smoothing!\nFor in-depth details, see my GitHub. 🤗\nA fine-tune of OpenAI / CLIP ViT-L/14 that has an unprecedented ImageNet/ObjectNet accuracy of ~0.90 (original pre-trained model / OpenAI's CLIP: ~0.85)**.\nMade possible with Geometric Parametrization (GmP):\n\"Normal\" CLIP MLP (multi-layer perceptron):\n(mlp): Sequential(\n|-(c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n| (gelu): QuickGELU()\n|-}-(c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n| |\n| |-- visual.transformer.resblocks.0.mlp.c_fc.weight\n| |-- visual.transformer.resblocks.0.mlp.c_fc.bias\n|\n|---- visual.transformer.resblocks.0.mlp.c_proj.weight\n|---- visual.transformer.resblocks.0.mlp.c_proj.bias\nGmP CLIP MLP:\nWeight decomposition into:\n- radial component 'r' as norm of pre-trained weights\n- angular component 'theta' as normalized direction\n-> preserves weight vectors' directionality and magnitude\n(mlp): Sequential(\n|-(c_fc): GeometricLinear()\n| (gelu): QuickGELU()\n|-}-(c_proj): GeometricLinear()\n| |\n| |-- visual.transformer.resblocks.0.mlp.c_fc.r\n| |-- visual.transformer.resblocks.0.mlp.c_fc.theta\n| |-- visual.transformer.resblocks.0.mlp.c_fc.bias\n|\n|---- visual.transformer.resblocks.0.mlp.c_proj.r\n|---- visual.transformer.resblocks.0.mlp.c_proj.theta\n|---- visual.transformer.resblocks.0.mlp.c_proj.bias\n(Same thing for [text] transformer.resblocks)\n✅ The model / state_dict I am sharing was converted back to .weight after fine-tuning - alas, it can be used in the same manner as any state_dict, e.g. for use with ComfyUI as the SDXL / SD3 Text Encoder! 🤗\n** For details on training and those numbers / the eval, please see https://github.com/zer0int/CLIP-fine-tune\n-> You can use \"exp-acts-ft-finetune-OpenAI-CLIP-ViT-L-14-GmP-manipulate-neurons.py\" to replicate my exact model fine-tune.\nPre-trained CLIP model by OpenAI, License: MIT License",
    "jinaai/jina-reranker-v2-base-multilingual": "jina-reranker-v2-base-multilingual\nIntended Usage & Model Info\nUsage\nEvaluation\nTrained by Jina AI.\njina-reranker-v2-base-multilingual\nIntended Usage & Model Info\nThe Jina Reranker v2 (jina-reranker-v2-base-multilingual) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\nCompared with the state-of-the-art reranker models, including the previous released jina-reranker-v1-base-en, the Jina Reranker v2 model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\nThe jina-reranker-v2-base-multilingual model is capable of handling long texts with a context length of up to 1024 tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance.\nUsage\nThis model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please contact us for any further clarifications.\nThe easiest way to use jina-reranker-v2-base-multilingual is to call Jina AI's Reranker API.\ncurl https://api.jina.ai/v1/rerank \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_API_KEY\" \\\n-d '{\n\"model\": \"jina-reranker-v2-base-multilingual\",\n\"query\": \"Organic skincare products for sensitive skin\",\n\"documents\": [\n\"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n\"New makeup trends focus on bold colors and innovative techniques\",\n\"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n\"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n\"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n\"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n\"针对敏感肌专门设计的天然有机护肤产品\",\n\"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n\"敏感肌のために特別に設計された天然有機スキンケア製品\",\n\"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\"\n],\n\"top_n\": 3\n}'\nYou can also use the transformers library to interact with the model programmatically.\nBefore you start, install the transformers and einops libraries:\npip install transformers einops\nAnd then:\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n'jinaai/jina-reranker-v2-base-multilingual',\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\nmodel.to('cuda') # or 'cpu' if no GPU is available\nmodel.eval()\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n\"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n\"New makeup trends focus on bold colors and innovative techniques\",\n\"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n\"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n\"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n\"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n\"针对敏感肌专门设计的天然有机护肤产品\",\n\"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n\"敏感肌のために特別に設計された天然有機スキンケア製品\",\n\"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\nscores = model.compute_score(sentence_pairs, max_length=1024)\nThe scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores indicate higher relevance.\nFor instance the returning scores in this case will be:\n[0.8311430811882019, 0.09401018172502518,\n0.6334102749824524, 0.08269733935594559,\n0.7620701193809509, 0.09947021305561066,\n0.9263036847114563, 0.05834583938121796,\n0.8418256044387817, 0.11124119907617569]\nThe model gives high relevance scores to the documents that are most relevant to the query regardless of the language of the document.\nNote that by default, the jina-reranker-v2-base-multilingual model uses flash attention, which requires certain types of GPU hardware to run.\nIf you encounter any issues, you can try call AutoModelForSequenceClassification.from_pretrained() with use_flash_attn=False.\nThis will use the standard attention mechanism instead of flash attention.\nIf you want to use flash attention for fast inference, you need to install the following packages:\npip install ninja # required for flash attention\npip install flash-attn --no-build-isolation\nEnjoy the 3x-6x speedup with flash attention! ⚡️⚡️⚡️\nYou can also use the transformers.js library to run the model directly in JavaScript (in-browser, Node.js, Deno, etc.)!\nIf you haven't already, you can install the Transformers.js JavaScript library (v3) using:\nnpm i xenova/transformers.js#v3\nThen, you can use the following code to interact with the model:\nimport { AutoTokenizer, XLMRobertaModel } from '@xenova/transformers';\nconst model_id = 'jinaai/jina-reranker-v2-base-multilingual';\nconst model = await XLMRobertaModel.from_pretrained(model_id, { dtype: 'fp32' });\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n/**\n* Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n* @param {string} query A single query\n* @param {string[]} documents A list of documents\n* @param {Object} options Options for ranking\n* @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n* @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n*/\nasync function rank(query, documents, {\ntop_k = undefined,\nreturn_documents = false,\n} = {}) {\nconst inputs = tokenizer(\nnew Array(documents.length).fill(query),\n{ text_pair: documents, padding: true, truncation: true }\n)\nconst { logits } = await model(inputs);\nreturn logits.sigmoid().tolist()\n.map(([score], i) => ({\ncorpus_id: i,\nscore,\n...(return_documents ? { text: documents[i] } : {})\n})).sort((a, b) => b.score - a.score).slice(0, top_k);\n}\n// Example usage:\nconst query = \"Organic skincare products for sensitive skin\"\nconst documents = [\n\"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n\"New makeup trends focus on bold colors and innovative techniques\",\n\"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n\"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n\"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n\"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n\"针对敏感肌专门设计的天然有机护肤产品\",\n\"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n\"敏感肌のために特別に設計された天然有機スキンケア製品\",\n\"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\nThat's it! You can now use the jina-reranker-v2-base-multilingual model in your projects.\nIn addition to the compute_score() function, the jina-reranker-v2-base-multilingual model also provides a model.rerank() function that can be used to rerank documents based on a query. You can use it as follows:\nresult = model.rerank(\nquery,\ndocuments,\nmax_query_length=512,\nmax_length=1024,\ntop_n=3\n)\nInside the result object, you will find the reranked documents along with their scores. You can use this information to further process the documents as needed.\nThe rerank() function will automatically chunk the input documents into smaller pieces if they exceed the model's maximum input length. This allows you to rerank long documents without running into memory issues.\nSpecifically, the rerank() function will split the documents into chunks of size max_length and rerank each chunk separately. The scores from all the chunks are then combined to produce the final reranking results. You can control the query length and document length in each chunk by setting the max_query_length and max_length parameters. The rerank() function also supports the overlap parameter (default is 80) which determines how much overlap there is between adjacent chunks. This can be useful when reranking long documents to ensure that the model has enough context to make accurate predictions.\nAlternatively, jina-reranker-v2-base-multilingual has been integrated with CrossEncoder from the sentence-transformers library.\nBefore you start, install the sentence-transformers libraries:\npip install sentence-transformers\nThe CrossEncoder class supports a predict method to get query-document relevance scores, and a rank method to rank all documents given your query.\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder(\n\"jinaai/jina-reranker-v2-base-multilingual\",\nautomodel_args={\"torch_dtype\": \"auto\"},\ntrust_remote_code=True,\n)\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n\"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n\"New makeup trends focus on bold colors and innovative techniques\",\n\"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n\"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n\"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n\"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n\"针对敏感肌专门设计的天然有机护肤产品\",\n\"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n\"敏感肌のために特別に設計された天然有機スキンケア製品\",\n\"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\nscores = model.predict(sentence_pairs, convert_to_tensor=True).tolist()\n\"\"\"\n[0.828125, 0.0927734375, 0.6328125, 0.08251953125, 0.76171875, 0.099609375, 0.92578125, 0.058349609375, 0.84375, 0.111328125]\n\"\"\"\nrankings = model.rank(query, documents, return_documents=True, convert_to_tensor=True)\nprint(f\"Query: {query}\")\nfor ranking in rankings:\nprint(f\"ID: {ranking['corpus_id']}, Score: {ranking['score']:.4f}, Text: {ranking['text']}\")\n\"\"\"\nQuery: Organic skincare products for sensitive skin\nID: 6, Score: 0.9258, Text: 针对敏感肌专门设计的天然有机护肤产品\nID: 8, Score: 0.8438, Text: 敏感肌のために特別に設計された天然有機スキンケア製品\nID: 0, Score: 0.8281, Text: Organic skincare for sensitive skin with aloe vera and chamomile.\nID: 4, Score: 0.7617, Text: Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\nID: 2, Score: 0.6328, Text: Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\nID: 9, Score: 0.1113, Text: 新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\nID: 5, Score: 0.0996, Text: Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\nID: 1, Score: 0.0928, Text: New makeup trends focus on bold colors and innovative techniques\nID: 3, Score: 0.0825, Text: Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\nID: 7, Score: 0.0583, Text: 新的化妆趋势注重鲜艳的颜色和创新的技巧\n\"\"\"\nEvaluation\nWe evaluated Jina Reranker v2 on multiple benchmarks to ensure top-tier performance and search relevance.\nModel Name\nModel Size\nMKQA(nDCG@10, 26 langs)\nBEIR(nDCG@10, 17 datasets)\nMLDR(recall@10, 13 langs)\nCodeSearchNet (MRR@10, 3 tasks)\nAirBench (nDCG@10, zh/en)\nToolBench (recall@3, 3 tasks)\nTableSearch (recall@3)\njina-reranker-v2-multilingual\n278M\n54.83\n53.17\n68.95\n71.36\n61.33\n77.75\n93.31\nbge-reranker-v2-m3\n568M\n54.17\n53.65\n59.73\n62.86\n61.28\n78.46\n74.86\nmmarco-mMiniLMv2-L12-H384-v1\n118M\n53.37\n45.40\n28.91\n51.78\n56.46\n58.39\n53.60\njina-reranker-v1-base-en\n137M\n-\n52.45\n-\n-\n-\n74.13\n72.89\nNote:\nNDCG@10 and MRR@10 measure ranking quality, with higher scores indicating better search results\nrecall@3 measures the proportion of relevant documents retrieved, with higher scores indicating better search results",
    "raidium/MQG": "Model Card for Raidium MQG model\nModel Details\nModel Description\nUsing the model\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Architecture and Objective\nCompute Infrastructure\nCitation\nModel Card Contact\nModel Card for Raidium MQG model\nThe model is introduced in the paper \"Efficient Medical Question Answering with Knowledge-Augmented Question Generation\".\nPaper: https://arxiv.org/abs/2405.14654\nMQG is is a transformer language model pre-trained on a series of medical textbooks, and medical questions generated by GPT-4. The weights are initialized with\nBioMedLM, then further pre-trained on those datasets.\nThe questions have been generated from prompt containing medical data from the textbooks.\nThey are available here: ECNQA_generated_questions.\nMQG is designed to be fine-tuned for Medical Question Answering tasks.\nModel Details\nModel Description\nIn the expanding field of language model applications, medical knowledge representation remains a significant challenge due to the specialized nature of the domain.\nLarge language models, such as GPT-4, obtain reasonable scores on medical question answering tasks, but smaller models are far behind.\nIn this work, we introduce a method to improve the proficiency of a small language model in the medical domain by employing a two-fold approach.\nWe first fine-tune the model on a corpus of medical textbooks. Then, we use GPT-4 to generate questions similar to the downstream task, prompted with textbook knowledge, and use them to fine-tune the model.\nWe show the benefits of our training strategy on a medical answering question dataset.\nUsing the model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"raidium/MQG\")\nmodel = AutoModelForCausalLM.from_pretrained(\"raidium/MQG\")\nDeveloped by: Raidium\nModel type: Transformer\nLicense: Aopache 2.0\nFinetuned from model: BioMedLM\nModel Sources [optional]\nRepository: [https://github.com/raidium-med/MQG]\nPaper: https://arxiv.org/abs/2405.14654\nUses\nDirect Use\nMQG is trained using next-token-prediction on generated questions.\nTherefore, it can be used out-of-the-box to generate potential answers for medical question answering tasks.\nHowever, the generated questions might contain some errors, so it is advised to fine-tune the model on your dataset, and use the models to rank the potential answers.\nDownstream Use\nMQG can be fine-tuned for Medical Question Answering tasks.\nFor multiple choice questions, a classification head should be appended at the end of the model, to rank different proposed answers.\nOut-of-Scope Use\nThis model should not be used for datasets outside medical tasks.\nBias, Risks, and Limitations\nThere is no guarantee that the model answers medical questions correctly. It should only be used for academic purposes, and not in clinical care.\nTraining Details\nTraining Data\nThe model is trained on a corpus of medical textbooks, and further pre-trained on generated questions:  ECNQA_generated_questions.\nTraining Procedure\nMGQ is trained using next-token-prediction on both datasets.\nTraining Hyperparameters\nTraining regime:  fp16 mixed-precision training.\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nWe tested the model on a medical question answering dataset, ECN-QA, based on the french medical residency examination.\nIt is composed of \"single\" and \"progressive\" questions (i.e a serie of multiple related questions).\nIt is a multiple-choice question dataset, containing 5 propositions for each question.\nMetrics\nWe use the accuracy to evaluate the model on Medical Question Answering.\nResults\nSee paper: https://arxiv.org/abs/2405.14654\nModel Architecture and Objective\nThe model is based on BioMedLM's architecture, which is modified from GPT-2 architecture.\nCompute Infrastructure\nHardware\nThe model was trained on the Jean-Zay supercomputer, on multiple nodes with 4 A100 gpus.\nSoftware\nPytorch, DeepSpeed\nCitation\nBibTeX:\n@article{khlaut2024efficient,\ntitle={Efficient Medical Question Answering with Knowledge-Augmented Question Generation},\nauthor={Khlaut, Julien and Dancette, Corentin and Ferreres, Elodie and Bennani, Alaedine and H{\\'e}rent, Paul and Manceron, Pierre},\njournal={Clinical NLP Workshop, NAACL 2024},\nyear={2024}\n}\nModel Card Contact\njulien.khlaut at raidium.fr",
    "google/gemma-2-9b": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license. To do this, please ensure you’re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 2 model card\nModel Information\nDescription\nUsage\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 2 model card\nModel Page: Gemma\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\npip install -U transformers\nThen, copy the snippet from the section that is relevant for your usecase.\nRunning with the pipeline API\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=\"google/gemma-2-9b\",\ndevice=\"cuda\",  # replace with \"mps\" to run on a Mac device\n)\ntext = \"Once upon a time,\"\noutputs = pipe(text, max_new_tokens=256)\nresponse = outputs[0][\"generated_text\"]\nprint(response)\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b\",\ndevice_map=\"auto\",\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nRunning the model through a CLI\nThe local-gemma repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI. Follow the installation instructions\nfor getting started, then launch the CLI through the following command:\nlocal-gemma --model \"google/gemma-2-9b\" --prompt \"What is the capital of Mexico?\"\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nAdvanced Usage\nTorch compile\nTorch compile is a method for speeding-up the\ninference of PyTorch modules. The Gemma-2 model can be run up to 6x faster by leveraging torch compile.\nNote that two warm-up steps are required before the full inference speed is realised:\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom transformers import AutoTokenizer, Gemma2ForCausalLM\nfrom transformers.cache_utils import HybridCache\nimport torch\ntorch.set_float32_matmul_precision(\"high\")\n# load the model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-9b\", torch_dtype=torch.bfloat16)\nmodel.to(\"cuda\")\n# apply the torch compile transformation\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n# pre-process inputs\ninput_text = \"The theory of special relativity states \"\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs.input_ids.shape[1]\n# set-up k/v cache\npast_key_values = HybridCache(\nconfig=model.config,\nmax_batch_size=1,\nmax_cache_len=model.config.max_position_embeddings,\ndevice=model.device,\ndtype=model.dtype\n)\n# enable passing kv cache to generate\nmodel._supports_cache_class = True\nmodel.generation_config.cache_implementation = None\n# two warm-up steps\nfor idx in range(2):\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\npast_key_values.reset()\n# fast run\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nFor more details, refer to the Transformers documentation.\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nCitation\n@article{gemma_2024,\ntitle={Gemma},\nurl={https://www.kaggle.com/m/3301},\nDOI={10.34740/KAGGLE/M/3301},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2024}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens.\nHere are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5p).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\nGemma PT 9B\nGemma PT 27B\nMMLU\n5-shot, top-1\n71.3\n75.2\nHellaSwag\n10-shot\n81.9\n86.4\nPIQA\n0-shot\n81.7\n83.2\nSocialIQA\n0-shot\n53.4\n53.7\nBoolQ\n0-shot\n84.2\n84.8\nWinoGrande\npartial score\n80.6\n83.7\nARC-e\n0-shot\n88.0\n88.6\nARC-c\n25-shot\n68.4\n71.4\nTriviaQA\n5-shot\n76.6\n83.7\nNatural Questions\n5-shot\n29.2\n34.5\nHumanEval\npass@1\n40.2\n51.8\nMBPP\n3-shot\n52.4\n62.6\nGSM8K\n5-shot, maj@1\n68.6\n74.0\nMATH\n4-shot\n36.6\n42.3\nAGIEval\n3-5-shot\n52.8\n55.1\nBIG-Bench\n3-shot, CoT\n68.2\n74.9\n------------------------------\n-------------\n-----------\n------------\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nGemma 2.0\nBenchmark\nMetric\nGemma 2 IT 9B\nGemma 2 IT 27B\nRealToxicity\naverage\n8.25\n8.84\nCrowS-Pairs\ntop-1\n37.47\n36.67\nBBQ Ambig\n1-shot, top-1\n88.58\n85.99\nBBQ Disambig\ntop-1\n82.67\n86.94\nWinogender\ntop-1\n79.17\n77.22\nTruthfulQA\n50.27\n51.60\nWinobias 1_2\n78.09\n81.94\nWinobias 2_2\n95.32\n97.22\nToxigen\n39.30\n38.42\n------------------------\n-------------\n---------------\n----------------\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "elyza/Llama-3-ELYZA-JP-8B": "Llama-3-ELYZA-JP-8B\nModel Description\nUsage\nDevelopers\nLicense\nHow to Cite\nCitations\nLlama-3-ELYZA-JP-8B\nModel Description\nLlama-3-ELYZA-JP-8B is a large language model trained by ELYZA, Inc.\nBased on meta-llama/Meta-Llama-3-8B-Instruct, it has been enhanced for Japanese usage through additional pre-training and instruction tuning. (Built with Meta Llama3)\nFor more details, please refer to our blog post.\nUsage\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nDEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。\"\ntext = \"仕事の熱意を取り戻すためのアイデアを5つ挙げてください。\"\nmodel_name = \"elyza/Llama-3-ELYZA-JP-8B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nmodel.eval()\nmessages = [\n{\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n{\"role\": \"user\", \"content\": text},\n]\nprompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ntoken_ids = tokenizer.encode(\nprompt, add_special_tokens=False, return_tensors=\"pt\"\n)\nwith torch.no_grad():\noutput_ids = model.generate(\ntoken_ids.to(model.device),\nmax_new_tokens=1200,\ndo_sample=True,\ntemperature=0.6,\ntop_p=0.9,\n)\noutput = tokenizer.decode(\noutput_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True\n)\nprint(output)\nDevelopers\nListed in alphabetical order.\nMasato Hirakawa\nShintaro Horie\nTomoaki Nakamura\nDaisuke Oba\nSam Passaglia\nAkira Sasaki\nLicense\nMeta Llama 3 Community License\nHow to Cite\n@misc{elyzallama2024,\ntitle={elyza/Llama-3-ELYZA-JP-8B},\nurl={https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B},\nauthor={Masato Hirakawa and Shintaro Horie and Tomoaki Nakamura and Daisuke Oba and Sam Passaglia and Akira Sasaki},\nyear={2024},\n}\nCitations\n@article{llama3modelcard,\ntitle={Llama 3 Model Card},\nauthor={AI@Meta},\nyear={2024},\nurl = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}",
    "karpathy/gpt2_1558M_final2_hf": "Model Card for Model ID\nBias, Risks, and Limitations\nModel Card for Model ID\nThis is a GPT-2 model trained in llm.c, for 32K steps (of 1M batch size) on FineWeb-EDU.\nA lot more detailed information is here: https://github.com/karpathy/llm.c/discussions/677\nBias, Risks, and Limitations\nEagerly generates disinformation about English-speaking unicorns in the Andes mountains.",
    "FunAudioLLM/SenseVoiceSmall": "Introduction\nHighlights 🎯\nWhat's New 🔥\nBenchmarks 📝\nMultilingual Speech Recognition\nSpeech Emotion Recognition\nAudio Event Detection\nComputational  Efficiency\nRequirements\nUsage\nInference\nInference directly\nExport and Test (On going)\nService\nFinetune\nWebUI\nCommunity\n(简体中文|English|日本語)\nIntroduction\ngithub repo : https://github.com/FunAudioLLM/SenseVoice\nSenseVoice is a speech foundation model with multiple speech understanding capabilities, including automatic speech recognition (ASR),  spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED).\nHomepage\n｜ What's News\n｜ Benchmarks\n｜ Install\n｜ Usage\n｜ Community\nModel Zoo:\nmodelscope, huggingface\nOnline Demo:\nmodelscope demo, huggingface space\nHighlights 🎯\nSenseVoice focuses on high-accuracy multilingual speech recognition, speech emotion recognition, and audio event detection.\nMultilingual Speech Recognition: Trained with over 400,000 hours of data, supporting more than 50 languages, the recognition performance surpasses that of the Whisper model.\nRich transcribe:\nPossess excellent emotion recognition capabilities, achieving and surpassing the effectiveness of the current best emotion recognition models on test data.\nOffer sound event detection capabilities, supporting the detection of various common human-computer interaction events such as bgm, applause, laughter, crying, coughing, and sneezing.\nEfficient Inference: The SenseVoice-Small model utilizes a non-autoregressive end-to-end framework, leading to exceptionally low inference latency. It requires only 70ms to process 10 seconds of audio, which is 15 times faster than Whisper-Large.\nConvenient Finetuning: Provide convenient finetuning scripts and strategies, allowing users to easily address long-tail sample issues according to their business scenarios.\nService Deployment: Offer service deployment pipeline,  supporting multi-concurrent requests, with client-side languages including Python, C++, HTML, Java, and C#, among others.\nWhat's New 🔥\n2024/7: Added Export Features for ONNX and libtorch, as well as Python Version Runtimes: funasr-onnx-0.4.0, funasr-torch-0.1.1\n2024/7: The SenseVoice-Small voice understanding model is open-sourced, which offers high-precision multilingual speech recognition, emotion recognition, and audio event detection capabilities for Mandarin, Cantonese, English, Japanese, and Korean and leads to exceptionally low inference latency.\n2024/7: The CosyVoice for natural speech generation with multi-language, timbre, and emotion control. CosyVoice excels in multi-lingual voice generation, zero-shot voice generation, cross-lingual voice cloning, and instruction-following capabilities. CosyVoice repo and CosyVoice space.\n2024/7: FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR.\nBenchmarks 📝\nMultilingual Speech Recognition\nWe compared the performance of multilingual speech recognition between SenseVoice and Whisper on open-source benchmark datasets, including AISHELL-1, AISHELL-2, Wenetspeech, LibriSpeech, and Common Voice. In terms of Chinese and Cantonese recognition, the SenseVoice-Small model has advantages.\nSpeech Emotion Recognition\nDue to the current lack of widely-used benchmarks and methods for speech emotion recognition, we conducted evaluations across various metrics on multiple test sets and performed a comprehensive comparison with numerous results from recent benchmarks. The selected test sets encompass data in both Chinese and English, and include multiple styles such as performances, films, and natural conversations. Without finetuning on the target data, SenseVoice was able to achieve and exceed the performance of the current best speech emotion recognition models.\nFurthermore, we compared multiple open-source speech emotion recognition models on the test sets, and the results indicate that the SenseVoice-Large model achieved the best performance on nearly all datasets, while the SenseVoice-Small model also surpassed other open-source models on the majority of the datasets.\nAudio Event Detection\nAlthough trained exclusively on speech data, SenseVoice can still function as a standalone event detection model. We compared its performance on the environmental sound classification ESC-50 dataset against the widely used industry models BEATS and PANN. The SenseVoice model achieved commendable results on these tasks. However, due to limitations in training data and methodology, its event classification performance has some gaps compared to specialized AED models.\nComputational  Efficiency\nThe SenseVoice-Small model deploys a non-autoregressive end-to-end architecture, resulting in extremely low inference latency. With a similar number of parameters to the Whisper-Small model, it infers more than 5 times faster than Whisper-Small and 15 times faster than Whisper-Large.\nRequirements\npip install -r requirements.txt\nUsage\nInference\nSupports input of audio in any format and of any duration.\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\nmodel_dir = \"FunAudioLLM/SenseVoiceSmall\"\nmodel = AutoModel(\nmodel=model_dir,\nvad_model=\"fsmn-vad\",\nvad_kwargs={\"max_single_segment_time\": 30000},\ndevice=\"cuda:0\",\nhub=\"hf\",\n)\n# en\nres = model.generate(\ninput=f\"{model.model_path}/example/en.mp3\",\ncache={},\nlanguage=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\nuse_itn=True,\nbatch_size_s=60,\nmerge_vad=True,  #\nmerge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\nParameter Description:\nmodel_dir: The name of the model, or the path to the model on the local disk.\nvad_model: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\nvad_kwargs: Specifies the configurations for the VAD model. max_single_segment_time: denotes the maximum duration for audio segmentation by the vad_model, with the unit being milliseconds (ms).\nuse_itn: Whether the output result includes punctuation and inverse text normalization.\nbatch_size_s: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\nmerge_vad: Whether to merge short audio fragments segmented by the VAD model, with the merged length being merge_length_s, in seconds (s).\nIf all inputs are short audios (<30s), and batch inference is needed to speed up inference efficiency, the VAD model can be removed, and batch_size can be set accordingly.\nmodel = AutoModel(model=model_dir, device=\"cuda:0\", hub=\"hf\")\nres = model.generate(\ninput=f\"{model.model_path}/example/en.mp3\",\ncache={},\nlanguage=\"zh\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\nuse_itn=False,\nbatch_size=64,\nhub=\"hf\",\n)\nFor more usage, please refer to docs\nInference directly\nSupports input of audio in any format, with an input duration limit of 30 seconds or less.\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\nmodel_dir = \"FunAudioLLM/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\", hub=\"hf\")\nm.eval()\nres = m.inference(\ndata_in=f\"{kwargs['model_path']}/example/en.mp3\",\nlanguage=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\nuse_itn=False,\n**kwargs,\n)\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\nExport and Test (On going)\nRef to SenseVoice\nService\nRef to SenseVoice\nFinetune\nRef to SenseVoice\nWebUI\npython webui.py\nCommunity\nIf you encounter problems in use, you can directly raise Issues on the github page.\nYou can also scan the following DingTalk group QR code to join the community group for communication and discussion.\nFunAudioLLM\nFunASR",
    "ZhengPeng7/BiRefNet": "How to use\n0. Install Packages:\n1. Load BiRefNet:\nUse codes + weights from HuggingFace\nUse codes from GitHub + weights from HuggingFace\nUse codes from GitHub + weights from local space\nUse the loaded BiRefNet for inference\n2. Use inference endpoint locally:\nTry our online demos for inference:\nThis repo holds the official model weights of \"Bilateral Reference for High-Resolution Dichotomous Image Segmentation\" (CAAI AIR 2024).\nTry our online demos for inference:\nAcknowledgement:\nCitation\nBilateral Reference for High-Resolution Dichotomous Image Segmentation\nPeng Zheng 1,4,5,6,\nDehong Gao 2,\nDeng-Ping Fan 1*,\nLi Liu 3,\nJorma Laaksonen 4,\nWanli Ouyang 5,\nNicu Sebe 6\n1 Nankai University   2 Northwestern Polytechnical University   3 National University of Defense Technology  4 Aalto University   5 Shanghai AI Laboratory   6 University of Trento\nDIS-Sample_1\nDIS-Sample_2\nThis repo is the official implementation of \"Bilateral Reference for High-Resolution Dichotomous Image Segmentation\" (CAAI AIR 2024).\nVisit our GitHub repo: https://github.com/ZhengPeng7/BiRefNet for more details -- codes, docs, and model zoo!\nHow to use\n0. Install Packages:\npip install -qr https://raw.githubusercontent.com/ZhengPeng7/BiRefNet/main/requirements.txt\n1. Load BiRefNet:\nUse codes + weights from HuggingFace\nOnly use the weights on HuggingFace -- Pro: No need to download BiRefNet codes manually; Con: Codes on HuggingFace might not be latest version (I'll try to keep them always latest).\n# Load BiRefNet with weights\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained('ZhengPeng7/BiRefNet', trust_remote_code=True)\nUse codes from GitHub + weights from HuggingFace\nOnly use the weights on HuggingFace -- Pro: codes are always latest; Con: Need to clone the BiRefNet repo from my GitHub.\n# Download codes\ngit clone https://github.com/ZhengPeng7/BiRefNet.git\ncd BiRefNet\n# Use codes locally\nfrom models.birefnet import BiRefNet\n# Load weights from Hugging Face Models\nbirefnet = BiRefNet.from_pretrained('ZhengPeng7/BiRefNet')\nUse codes from GitHub + weights from local space\nOnly use the weights and codes both locally.\n# Use codes and weights locally\nimport torch\nfrom utils import check_state_dict\nbirefnet = BiRefNet(bb_pretrained=False)\nstate_dict = torch.load(PATH_TO_WEIGHT, map_location='cpu')\nstate_dict = check_state_dict(state_dict)\nbirefnet.load_state_dict(state_dict)\nUse the loaded BiRefNet for inference\n# Imports\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import transforms\nfrom models.birefnet import BiRefNet\nbirefnet = ... # -- BiRefNet should be loaded with codes above, either way.\ntorch.set_float32_matmul_precision(['high', 'highest'][0])\nbirefnet.to('cuda')\nbirefnet.eval()\nbirefnet.half()\ndef extract_object(birefnet, imagepath):\n# Data settings\nimage_size = (1024, 1024)\ntransform_image = transforms.Compose([\ntransforms.Resize(image_size),\ntransforms.ToTensor(),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\nimage = Image.open(imagepath)\ninput_images = transform_image(image).unsqueeze(0).to('cuda').half()\n# Prediction\nwith torch.no_grad():\npreds = birefnet(input_images)[-1].sigmoid().cpu()\npred = preds[0].squeeze()\npred_pil = transforms.ToPILImage()(pred)\nmask = pred_pil.resize(image.size)\nimage.putalpha(mask)\nreturn image, mask\n# Visualization\nplt.axis(\"off\")\nplt.imshow(extract_object(birefnet, imagepath='PATH-TO-YOUR_IMAGE.jpg')[0])\nplt.show()\n2. Use inference endpoint locally:\nYou may need to click the deploy and set up the endpoint by yourself, which would make some costs.\nimport requests\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nYOUR_HF_TOKEN = 'xxx'\nAPI_URL = \"xxx\"\nheaders = {\n\"Authorization\": \"Bearer {}\".format(YOUR_HF_TOKEN)\n}\ndef base64_to_bytes(base64_string):\n# Remove the data URI prefix if present\nif \"data:image\" in base64_string:\nbase64_string = base64_string.split(\",\")[1]\n# Decode the Base64 string into bytes\nimage_bytes = base64.b64decode(base64_string)\nreturn image_bytes\ndef bytes_to_base64(image_bytes):\n# Create a BytesIO object to handle the image data\nimage_stream = BytesIO(image_bytes)\n# Open the image using Pillow (PIL)\nimage = Image.open(image_stream)\nreturn image\ndef query(payload):\nresponse = requests.post(API_URL, headers=headers, json=payload)\nreturn response.json()\noutput = query({\n\"inputs\": \"https://hips.hearstapps.com/hmg-prod/images/gettyimages-1229892983-square.jpg\",\n\"parameters\": {}\n})\noutput_image = bytes_to_base64(base64_to_bytes(output))\noutput_image\nThis BiRefNet for standard dichotomous image segmentation (DIS) is trained on DIS-TR and validated on DIS-TEs and DIS-VD.\nThis repo holds the official model weights of \"Bilateral Reference for High-Resolution Dichotomous Image Segmentation\" (CAAI AIR 2024).\nThis repo contains the weights of BiRefNet proposed in our paper, which has achieved the SOTA performance on three tasks (DIS, HRSOD, and COD).\nGo to my GitHub page for BiRefNet codes and the latest updates: https://github.com/ZhengPeng7/BiRefNet :)\nTry our online demos for inference:\nOnline Image Inference on Colab:\nOnline Inference with GUI on Hugging Face with adjustable resolutions:\nInference and evaluation of your given weights:\nAcknowledgement:\nMany thanks to @Freepik for their generous support on GPU resources for training higher resolution BiRefNet models and more of my explorations.\nMany thanks to @fal for their generous support on GPU resources for training better general BiRefNet models.\nMany thanks to @not-lain for his help on the better deployment of our BiRefNet model on HuggingFace.\nCitation\n@article{zheng2024birefnet,\ntitle={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},\nauthor={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},\njournal={CAAI Artificial Intelligence Research},\nvolume = {3},\npages = {9150038},\nyear={2024}\n}"
}