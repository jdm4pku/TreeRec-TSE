{
    "unsloth/Devstral-Small-2507-GGUF": "YAML Metadata\nWarning:\nThe pipeline tag \"text2text-generation\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, audio-text-to-text, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-ranking, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, visual-document-retrieval, any-to-any, video-to-video, other\nDevstral Small 1.1\nKey Features:\nBenchmark Results\nSWE-Bench\nUsage\nAPI\nLocal inference\nOpenHands (recommended)\nCline\nExamples\nYou should use --jinja to enable the system prompt in llama.cpp.\nDevstral 1.1, with tool-calling and optional vision support.\nLearn to run Devstral correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\n‚ú® Run & Fine-tune Devstral 1.1 with Unsloth!\nFine-tune Mistral v0.3 (7B) for free using our Google Colab notebook here!\nRead our Blog about Devstral 1.1 support: docs.unsloth.ai/basics/devstral\nView the rest of our notebooks in our docs here.\nDevstral Small 1.1\nDevstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI üôå. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model on this benchmark.\nIt is finetuned from Mistral-Small-3.1, therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from Mistral-Small-3.1 the vision encoder was removed.\nFor enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\nLearn more about Devstral in our blog post.\nUpdates compared to Devstral Small 1.0:\nImproved performance, please refer to the benchmark results.\nDevstral Small 1.1 is still great when paired with OpenHands. This new version also generalizes better to other prompts and coding environments.\nSupports Mistral's function calling format.\nKey Features:\nAgentic coding: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\nlightweight: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 128k context window.\nTokenizer: Utilizes a Tekken tokenizer with a 131k vocabulary size.\nBenchmark Results\nSWE-Bench\nDevstral Small 1.1 achieves a score of 53.6% on SWE-Bench Verified, outperforming Devstral Small 1.0 by +6,8% and the second best state of the art model by +11.4%.\nModel\nAgentic Scaffold\nSWE-Bench Verified (%)\nDevstral Small 1.1\nOpenHands Scaffold\n53.6\nDevstral Small 1.0\nOpenHands Scaffold\n46.8\nGPT-4.1-mini\nOpenAI Scaffold\n23.6\nClaude 3.5 Haiku\nAnthropic Scaffold\n40.6\nSWE-smith-LM 32B\nSWE-agent Scaffold\n40.2\nSkywork SWE\nOpenHands Scaffold\n38.0\nDeepSWE\nR2E-Gym   Scaffold\n42.2\nWhen evaluated under the same test scaffold (OpenHands, provided by All Hands AI üôå), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\nUsage\nWe recommend to use Devstral with the OpenHands scaffold.\nYou can use it either through our API or by running locally.\nAPI\nFollow these instructions to create a Mistral account and get an API key.\nThen run these commands to start the OpenHands docker container.\nexport MISTRAL_API_KEY=<MY_KEY>\nmkdir -p ~/.openhands && echo '{\"language\":\"en\",\"agent\":\"CodeActAgent\",\"max_iterations\":null,\"security_analyzer\":null,\"confirmation_mode\":false,\"llm_model\":\"mistral/devstral-small-2507\",\"llm_api_key\":\"'$MISTRAL_API_KEY'\",\"remote_runtime_resource_factor\":null,\"github_token\":null,\"enable_default_condenser\":true}' > ~/.openhands-state/settings.json\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.48-nikolaik\ndocker run -it --rm --pull=always \\\n-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.48-nikolaik \\\n-e LOG_ALL_EVENTS=true \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v ~/.openhands:/.openhands \\\n-p 3000:3000 \\\n--add-host host.docker.internal:host-gateway \\\n--name openhands-app \\\ndocker.all-hands.dev/all-hands-ai/openhands:0.48\nLocal inference\nThe model can also be deployed with the following libraries:\nvllm (recommended): See here\nmistral-inference: See here\ntransformers: See here\nLMStudio: See here\nllama.cpp: See here\nollama: See here\nvLLM (recommended)\nExpandWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nInstallation\nMake sure you install vLLM >= 0.9.1:\npip install vllm --upgrade\nAlso make sure to have installed mistral_common >= 1.7.0.\npip install mistral-common --upgrade\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nYou can also make use of a ready-to-go docker image or on the docker hub.\nLaunch server\nWe recommand that you use Devstral in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Devstral-Small-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\nTo ping the client you can use a simple Python snippet.\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nurl = \"http://<your-server-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Devstral-Small-2507\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"<your-command>\",\n},\n],\n},\n]\ndata = {\"model\": model, \"messages\": messages, \"temperature\": 0.15}\n# Devstral Small 1.1 supports tool calling. If you want to use tools, follow this:\n# tools = [ # Define tools for vLLM\n#     {\n#         \"type\": \"function\",\n#         \"function\": {\n#             \"name\": \"git_clone\",\n#             \"description\": \"Clone a git repository\",\n#             \"parameters\": {\n#                 \"type\": \"object\",\n#                 \"properties\": {\n#                     \"url\": {\n#                         \"type\": \"string\",\n#                         \"description\": \"The url of the git repository\",\n#                     },\n#                 },\n#                 \"required\": [\"url\"],\n#             },\n#         },\n#     }\n# ]\n# data = {\"model\": model, \"messages\": messages, \"temperature\": 0.15, \"tools\": tools} # Pass tools to payload.\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\nMistral-inference\nExpandWe recommend using mistral-inference to quickly try out / \"vibe-check\" Devstral.\nInstallation\nMake sure to have mistral_inference >= 1.6.0 installed.\npip install mistral_inference --upgrade\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', 'Devstral')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Devstral-Small-2507\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\nChat\nYou can run the model using the following command:\nmistral-chat $HOME/mistral_models/Devstral --instruct --max_tokens 300\nYou can then prompt it with anything you'd like.\nTransformers\nExpandTo make the best use of our model with transformers make sure to have installed mistral-common >= 1.7.0 to use our tokenizer.\npip install mistral-common --upgrade\nThen load our tokenizer along with the model and generate:\nimport torch\nfrom mistral_common.protocol.instruct.messages import (\nSystemMessage, UserMessage\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoModelForCausalLM\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Devstral-Small-2507\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntokenized = tokenizer.encode_chat_completion(\nChatCompletionRequest(\nmessages=[\nSystemMessage(content=SYSTEM_PROMPT),\nUserMessage(content=\"<your-command>\"),\n],\n)\n)\noutput = model.generate(\ninput_ids=torch.tensor([tokenized.tokens]),\nmax_new_tokens=1000,\n)[0]\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens):])\nprint(decoded_output)\nLM Studio\nExpandDownload the weights from either:\nLM Studio GGUF repository (recommended): https://huggingface.co/lmstudio-community/Devstral-Small-2507-GGUF\nour GGUF repository: https://huggingface.co/mistralai/Devstral-Small-2507_gguf\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"lmstudio-community/Devstral-Small-2507-GGUF\" \\ # or mistralai/Devstral-Small-2507_gguf\n--include \"Devstral-Small-2507-Q4_K_M.gguf\" \\\n--local-dir \"Devstral-Small-2507_gguf/\"\nYou can serve the model locally with LMStudio.\nDownload LM Studio and install it\nInstall lms cli ~/.lmstudio/bin/lms bootstrap\nIn a bash terminal, run lms import Devstral-Small-2507-Q4_K_M.gguf in the directory where you've downloaded the model checkpoint (e.g. Devstral-Small-2507_gguf)\nOpen the LM Studio application, click the terminal icon to get into the developer tab. Click select a model to load and select Devstral Small 2507. Toggle the status button to start the model, in setting toggle Serve on Local Network to be on.\nOn the right tab, you will see an API identifier which should be devstral-small-2507 and an api address under API Usage. Keep note of this address, this is used for OpenHands or Cline.\nllama.cpp\nExpandDownload the weights from huggingface:\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"mistralai/Devstral-Small-2507_gguf\" \\\n--include \"Devstral-Small-2507-Q4_K_M.gguf\" \\\n--local-dir \"mistralai/Devstral-Small-2507_gguf/\"\nThen run Devstral using the llama.cpp server.\n./llama-server -m mistralai/Devstral-Small-2507_gguf/Devstral-Small-2507-Q4_K_M.gguf -c 0 # -c configure the context size, 0 means model's default, here 128k.\nOpenHands (recommended)\nLaunch a server to deploy Devstral Small 1.1\nMake sure you launched an OpenAI-compatible server such as vLLM or Ollama as described above. Then, you can use OpenHands to interact with Devstral Small 1.1.\nIn the case of the tutorial we spineed up a vLLM server running the command:\nvllm serve mistralai/Devstral-Small-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\nThe server address should be in the following format: http://<your-server-url>:8000/v1\nLaunch OpenHands\nYou can follow installation of OpenHands here.\nThe easiest way to launch OpenHands is to use the Docker image:\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.48-nikolaik\ndocker run -it --rm --pull=always \\\n-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.48-nikolaik \\\n-e LOG_ALL_EVENTS=true \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v ~/.openhands:/.openhands \\\n-p 3000:3000 \\\n--add-host host.docker.internal:host-gateway \\\n--name openhands-app \\\ndocker.all-hands.dev/all-hands-ai/openhands:0.48\nThen, you can access the OpenHands UI at http://localhost:3000.\nConnect to the server\nWhen accessing the OpenHands UI, you will be prompted to connect to a server. You can use the advanced mode to connect to the server you launched earlier.\nFill the following fields:\nCustom Model: openai/mistralai/Devstral-Small-2507\nBase URL: http://<your-server-url>:8000/v1\nAPI Key: token (or any other token you used to launch the server if any)\nSee settings\nCline\nLaunch a server to deploy Devstral Small 1.1\nMake sure you launched an OpenAI-compatible server such as vLLM or Ollama as described above. Then, you can use OpenHands to interact with Devstral Small 1.1.\nIn the case of the tutorial we spineed up a vLLM server running the command:\nvllm serve mistralai/Devstral-Small-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\nThe server address should be in the following format: http://<your-server-url>:8000/v1\nLaunch Cline\nYou can follow installation of Cline here. Then you can configure the server address in the settings.\nSee settings\nExamples\nOpenHands:Understanding Test Coverage of Mistral Common\nWe can start the OpenHands scaffold and link it to a repo to analyze test coverage and identify badly covered files.\nHere we start with our public mistral-common repo.\nAfter the repo is mounted in the workspace, we give the following instruction\nCheck the test coverage of the repo and then create a visualization of test coverage. Try plotting a few different types of graphs and save them to a png.\nThe agent will first browse the code base to check test configuration and structure.\nThen it sets up the testing dependencies and launches the coverage test:\nFinally, the agent writes necessary code to visualize the coverage, export the results and save the plots to a png.\nAt the end of the run, the following plots are produced:\nand the model is able to explain the results:\nCline: build a video game\nFirst initialize Cline inside VSCode and connect it to the server you launched earlier.\nWe give the following instruction to builde the video game:\nCreate a video game that mixes Space Invaders and Pong for the web.\nFollow these instructions:\n- There are two players one at the top and one at the bottom. The players are controling a bar to bounce a ball.\n- The first player plays with the keys \"a\" and \"d\", the second with the right and left arrows.\n- The invaders are located at the center of the screen. They shoud look like the ones in Space Invaders. Their goal is to shoot on the players randomly. They cannot be destroyed by the ball that pass through them. This means that invaders never die.\n- The players goal is to avoid shootings from the space invaders and send the ball to the edge of the over player.\n- The ball bounces on the left and right edges.\n- Once the ball touch one of the player's edge, the player loses.\n- Once a player is touched 3 times or more by a shooting, the player loses.\n- The player winning is the last one standing.\n- Display on the UI, the number of times a player touched the ball, and the remaining health.\nThe agent will first create the game:\nThen it will explain how to launch the game:\nFinally, the game is ready to be played:\nDon't hesitate to iterate or give more information to Devstral to improve the game!",
    "LiquidAI/LFM2-1.2B-GGUF": "LFM2-1.2B-GGUF\nüèÉ How to run LFM2\nLiquid: Playground\nLiquid\nLiquid\nPlayground\nPlayground\nLFM2-1.2B-GGUF\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nFind more details in the original model card: https://huggingface.co/LiquidAI/LFM2-1.2B\nüèÉ How to run LFM2\nExample usage with llama.cpp:\nllama-cli -hf LiquidAI/LFM2-1.2B-GGUF",
    "city96/Qwen-Image-gguf": "Notes\nThis is a direct GGUF conversion of Qwen/Qwen-Image.\nThe model files can be used in ComfyUI with the ComfyUI-GGUF custom node. Place the required model(s) in the following folders:\nType\nName\nLocation\nDownload\nMain Model\nQwen-Image\nComfyUI/models/diffusion_models\nGGUF (this repo)\nText Encoder\nQwen2.5-VL-7B\nComfyUI/models/text_encoders\nSafetensors / GGUF\nVAE\nQwen-Image VAE\nComfyUI/models/vae\nSafetensors\nExample workflow\nExample outputs - sample size of 1, not strictly representative\nNotes\nThe Q5_K_M, Q4_K_M and most importantly the low bitrate quants (Q3_K_M, Q3_K_S, Q2_K) use a new dynamic logic where the first/last layer is kept in high precision.\nFor a comparison, see this imgsli page. With this method, even Q2_K remains somewhat usable.\nAs this is a quantized model not a finetune, all the same restrictions/original license terms still apply.",
    "allenai/MolmoAct-7B-D-0812": "MolmoAct 7B-D\nQuick Start\nLicense and Use\nModel and Hardware Safety\nCitation\nMolmoAct 7B-D\nMolmoAct is a fully open-source action reasoning model for robotic manipulation developed by the Allen Institute for AI. MolmoAct is trained on a subset of OXE and MolmoAct Dataset, a dataset with 10k high-quality trajectories of a single-arm Franka robot performing 93 unique manipulation tasks in both home and tabletop environments. It has state-of-the-art performance among vision-language-action models on multiple benchmarks while being fully open-source. You can find all models in the MolmoAct family here.\nLearn more about MolmoAct in our announcement blog post or the paper.\nMolmoAct 7B-D is based on Qwen2.5-7B and uses SigLip2 as the vision backbone, which is initialized using Molmo's pre-training approach. It is first pre-trained on MolmoAct's Pre-training Mixture, and then mid-trained on MolmoAct Dataset. This model is intended to be used for downstream post-training.\nThis checkpoint is a preview of the MolmoAct release. All artifacts used in creating MolmoAct (data, training code, evaluations, intermediate checkpoints) will be made available at a later date, furthering our commitment to open-source AI development and reproducibility.\nUpdate: Checkpoints are now stored in FP32 (previously BF16). The model was trained in FP32, so publishing FP32 weights aligns with training and enables fine-tuning or continued training directly from this repo. For inference, you can still run BF16 by casting at load, which is what we did for evaluations. See more in the instructions below.\nQuick links:\nüìÇ All Models\nüìÇ All Data\nüìÑ Paper\nüíª Code\nüé• Blog Post\nüé• Video\nQuick Start\nTo run MolmoAct, first install dependencies:\npip install einops torchvision accelerate\npip install transformers==4.52\nThen, follow these steps:\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nckpt = \"allenai/MolmoAct-7B-D-0812\"\n# load the processor\nprocessor = AutoProcessor.from_pretrained(\nckpt,\ntrust_remote_code=True,\ntorch_dtype=\"bfloat16\",\ndevice_map=\"auto\",\npadding_side=\"left\",\n)\n# load the model\nmodel = AutoModelForImageTextToText.from_pretrained(\nckpt,\ntrust_remote_code=True,\ntorch_dtype=\"bfloat16\",\ndevice_map=\"auto\",\n)\n# task instruction\ninstruction = \"close the box\"\n# strictly follow this reasoning prompt\nprompt = (\nf\"The task is {instruction}. \"\n\"What is the action that the robot should take. \"\nf\"To figure out the action that the robot should take to {instruction}, \"\n\"let's think through it step by step. \"\n\"First, what is the depth map for the first image? \"\n\"Second, what is the trajectory of the end effector in the first image? \"\n\"Based on the depth map of the first image and the trajectory of the end effector in the first image, \"\n\"along with other images from different camera views as additional information, \"\n\"what is the action that the robot should take?\"\n)\n# apply chat template\ntext = processor.apply_chat_template(\n[\n{\n\"role\": \"user\",\n\"content\": [dict(type=\"text\", text=prompt)]\n}\n],\ntokenize=False,\nadd_generation_prompt=True,\n)\n# image observation (side + wrist)\nurl1 = \"https://huggingface.co/allenai/MolmoAct-7B-D-0812/resolve/main/example_1.png\"\nurl2 = \"https://huggingface.co/allenai/MolmoAct-7B-D-0812/resolve/main/example_2.png\"\nr1 = requests.get(url1, headers={\"User-Agent\": \"python-requests\"}, timeout=30)\nr1.raise_for_status()\nr2 = requests.get(url2, headers={\"User-Agent\": \"python-requests\"}, timeout=30)\nr2.raise_for_status()\nimg1 = Image.open(BytesIO(r1.content)).convert(\"RGB\")\nimg2 = Image.open(BytesIO(r2.content)).convert(\"RGB\")\nimgs = [img1, img2]\n# process the image and text\ninputs = processor(\nimages=[imgs],\ntext=text,\npadding=True,\nreturn_tensors=\"pt\",\n)\n# move inputs to the correct device\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\n# generate output\nwith torch.inference_mode():\nwith torch.autocast(\"cuda\", enabled=True, dtype=torch.bfloat16):\ngenerated_ids = model.generate(**inputs, max_new_tokens=256)\n# only get generated tokens; decode them to text\ngenerated_tokens = generated_ids[:, inputs['input_ids'].size(1):]\ngenerated_text = processor.batch_decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n# print the generated text\nprint(f\"generated text: {generated_text}\")\n# >>>  The depth map of the first image is ... The trajectory of the end effector in the first image is ...\n#      Based on these information, along with other images from different camera views as additional information,\n#      the action that the robot should take is ...\n# parse out all depth perception tokens\ndepth = model.parse_depth(generated_text)\nprint(f\"generated depth perception tokens: {depth}\")\n# >>>  [ \"<DEPTH_START><DEPTH_1><DEPTH_2>...<DEPTH_END>\" ]\n# parse out all visual reasoning traces\ntrace = model.parse_trace(generated_text)\nprint(f\"generated visual reasoning trace: {trace}\")\n# >>>  [ [[242, 115], [140, 77], [94, 58], [140, 44], [153, 26]]] ]\n# parse out all actions, unnormalizing with key of \"molmoact\"\naction = model.parse_action(generated_text, unnorm_key=\"molmoact\")\nprint(f\"generated action: {action}\")\n# >>>  [ [0.0732076061122558, 0.08228153779226191, -0.027760173818644346,\n#         0.15932856272248652, -0.09686601126895233, 0.043916773912953344,\n#         0.996078431372549] ]\nLicense and Use\nThis model is licensed under Apache 2.0. It is intended for research and educational use.\nFor more information, please see our Responsible Use Guidelines.\nModel and Hardware Safety\nMolmoAct offers the ability to inspect a visual trace of its intended actions in space before they occur, allowing users to ensure safe behavior by proactively auditing and adjusting the actions of any hardware acting under the model‚Äôs instructions. MolmoAct‚Äôs action space is bounded within the data provided, and compliance is built into the model to prevent excessive force when resistance is detected. Please follow the hardware manufacturer‚Äôs guidelines when using this model with a robot and perform all operations in a safely configured environment.\nCitation\n@misc{molmoact2025,\ntitle={MolmoAct: Action Reasoning Models that can Reason in Space},\nauthor={Jason Lee and Jiafei Duan and Haoquan Fang and Yuquan Deng and Shuo Liu and Boyang Li and Bohan Fang and Jieyu Zhang and Yi Ru Wang and Sangho Lee and Winson Han and Wilbert Pumacay and Angelica Wu and Rose Hendrix and Karen Farley and Eli VanderBilt and Ali Farhadi and Dieter Fox and Ranjay Krishna},\nyear={2025},\neprint={2508.07917},\narchivePrefix={arXiv},\nprimaryClass={cs.RO},\nurl={https://arxiv.org/abs/2508.07917}\n}",
    "baichuan-inc/Baichuan-M2-32B": "Baichuan-M2-32B\nüåü Model Overview\nüìä Performance Metrics\nHealthBench Scores\nGeneral Performance\nüîß Technical Features\nLarge Verifier System\nMedical Domain Adaptation\n‚öôÔ∏è Quick Start\nMTP inference with SGLang\n‚ö†Ô∏è Usage Notices\nüìÑ License\nü§ù Acknowledgements\nüìû Contact Us\nBaichuan-M2-32B\nüåü Model Overview\nBaichuan-M2-32B is Baichuan AI's medical-enhanced reasoning model, the second medical model released by Baichuan. Designed for real-world medical reasoning tasks, this model builds upon Qwen2.5-32B with an innovative Large Verifier System. Through domain-specific fine-tuning on real-world medical questions, it achieves breakthrough medical performance while maintaining strong general capabilities.\nModel Features:\nBaichuan-M2 incorporates three core technical innovations: First, through the Large Verifier System, it combines medical scenario characteristics to design a comprehensive medical verification framework, including patient simulators and multi-dimensional verification mechanisms; second, through medical domain adaptation enhancement via Mid-Training, it achieves lightweight and efficient medical domain adaptation while preserving general capabilities; finally, it employs a multi-stage reinforcement learning strategy, decomposing complex RL tasks into hierarchical training stages to progressively enhance the model's medical knowledge, reasoning, and patient interaction capabilities.\nCore Highlights:\nüèÜ World's Leading Open-Source Medical Model: Outperforms all open-source models and many proprietary models on HealthBench, achieving medical capabilities closest to GPT-5\nüß† Doctor-Thinking Alignment: Trained on real clinical cases and patient simulators, with clinical diagnostic thinking and robust patient interaction capabilities\n‚ö° Efficient Deployment: Supports 4-bit quantization for single-RTX4090 deployment, with 58.5% higher token throughput in MTP version for single-user scenarios\nüìä Performance Metrics\nHealthBench Scores\nModel Name\nHealthBench\nHealthBench-Hard\nHealthBench-Consensus\nBaichuan-M2\n60.1\n34.7\n91.5\ngpt-oss-120b\n57.6\n30\n90\nQwen3-235B-A22B-Thinking-2507\n55.2\n25.9\n90.6\nDeepseek-R1-0528\n53.6\n22.6\n91.5\nGLM-4.5\n47.8\n18.7\n85.3\nKimi-K2\n43\n10.7\n90.9\ngpt-oss-20b\n42.5\n10.8\n82.6\nGeneral Performance\nBenchmark\nBaichuan-M2-32B\nQwen3-32B (Thinking)\nAIME24\n83.4\n81.4\nAIME25\n72.9\n72.9\nArena-Hard-v2.0\n45.8\n44.5\nCFBench\n77.6\n75.7\nWritingBench\n8.56\n7.90\nNote: AIME uses max_tokens=64k, others use 32k; temperature=0.6 for all tests.\nüîß Technical Features\nüìó Technical Blog: Blog - Baichuan-M2\nüìë Technical Report: Arxiv - Baichuan-M2\nLarge Verifier System\nPatient Simulator: Virtual patient system based on real clinical cases\nMulti-Dimensional Verification: 8 dimensions including medical accuracy, response completeness, and follow-up awareness\nDynamic Scoring: Real-time generation of adaptive evaluation criteria for complex clinical scenarios\nMedical Domain Adaptation\nMid-Training: Medical knowledge injection while preserving general capabilities\nReinforcement Learning: Multi-stage RL strategy optimization\nGeneral-Specialized Balance: Carefully balanced medical, general, and mathematical composite training data\n‚öôÔ∏è Quick Start\n# 1. load model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-M2-32B\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-M2-32B\")\n# 2. Input prompt text\nprompt = \"Got a big swelling after a bug bite. Need help reducing it.\"\n# 3. Encode the input text for the model\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nthinking_mode='on' # on/off/auto\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# 4. Generate text\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=4096\n)\noutput_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n][0].tolist()\n# 5. parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.9.0 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path baichuan-inc/Baichuan-M2-32B --reasoning-parser qwen3\nvLLM:vllm serve baichuan-inc/Baichuan-M2-32B  --reasoning-parser qwen3\nMTP inference with SGLang\nReplace the qwen2.py file in the sglang installation directory with draft/qwen2.py.\nLaunch sglang:\npython3 -m sglang.launch_server \\\n--model Baichuan-M2-32B \\\n--speculative-algorithm EAGLE3 \\\n--speculative-draft-model-path Baichuan-M2-32B/draft \\\n--speculative-num-steps 6 \\\n--speculative-eagle-topk 10 \\\n--speculative-num-draft-tokens 32 \\\n--mem-fraction 0.9 \\\n--cuda-graph-max-bs 2 \\\n--reasoning-parser qwen3 \\\n--dtype bfloat16\n‚ö†Ô∏è Usage Notices\nMedical Disclaimer: For research and reference only; cannot replace professional medical diagnosis or treatment\nIntended Use Cases: Medical education, health consultation, clinical decision support\nSafe Use: Recommended under guidance of medical professionals\nüìÑ License\nLicensed under the Apache License 2.0. Research and commercial use permitted.\nü§ù Acknowledgements\nBase Model: Qwen2.5-32B\nTraining Framework: verl\nInference Engines: vLLM, SGLang\nQuantization: AutoRound, GPTQ\nThank you to the open-source community. We commit to continuous contribution and advancement of healthcare AI.\nüìû Contact Us\nResources: Baichuan AI Website\nTechnical Support: GitHub\nEmpowering Healthcare with AI, Making Health Accessible to All",
    "Kijai/QwenImage_experimental": "No model card",
    "swiss-ai/Apertus-8B-Instruct-2509": "Apertus\nTable of Contents\nModel Summary\nKey features\nHow to use\nLong context processing\nAgentic Usage\nDeployment\nEvaluation\nTraining\nModel\nSoftware & hardware\nOpen resources\nLimitations\nLegal Aspects\nContact\nCitation\nApertus\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nTraining\nLimitations\nLegal Aspects\nModel Summary\nApertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models.\nThe model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.\nThe model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO.\nKey features\nFully open model: open weights + open data + full training details including all data and training recipes\nMassively Multilingual: 1811 natively supported languages\nCompliant Apertus is trained while respecting opt-out consent of data owners (even retrospectivey), and avoiding memorization of training data\nFor more details refer to our technical report\nHow to use\nThe modeling code for Apertus is available in transformers v4.56.0 and later, so make sure to upgrade your transformers version. You can also load the model with the latest vLLM which uses transformers as a backend.\npip install -U transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"swiss-ai/Apertus-8B-Instruct-2509\"\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\n).to(device)\n# prepare the model input\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages_think = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages_think,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\nWe recommend setting temperature=0.8 and top_p=0.9 in the sampling parameters.\nLong context processing\nApertus by default supports a context length up to 65,536 tokens.\nAgentic Usage\nApertus supports tool use\nDeployment\nDeployment of the models is directly supported by the newest versions of Transformers, vLLM, SGLang, and also for running on-device with MLX,\nEvaluation\nPretraining Evaluation: Performance (%) of Apertus models on general language understanding tasks (higher is better) compared to other pretrained models.\nModel\nAvg\nARC\nHellaSwag\nWinoGrande\nXNLI\nXCOPA\nPIQA\nFully Open Models\nApertus-8B\n65.8\n72.7\n59.8\n70.6\n45.2\n66.5\n79.8\nApertus-70B\n67.5\n70.6\n64.0\n73.3\n45.3\n69.8\n81.9\nOLMo2-7B\n64.0\n72.9\n60.4\n74.5\n40.4\n55.2\n80.9\nOLMo2-32B\n67.7\n76.2\n66.7\n78.6\n42.9\n60.1\n82.1\nEuroLLM-1.7B\n54.8\n57.2\n44.9\n58.1\n40.7\n55.7\n72.4\nEuroLLM-9B\n62.8\n67.9\n57.9\n68.8\n41.5\n61.1\n79.6\nSmolLM2-1.7B\n58.5\n66.1\n52.4\n65.6\n37.6\n52.3\n77.0\nSmolLM3-3B\n61.6\n68.6\n56.4\n68.1\n40.5\n58.2\n77.7\nPoro-34B\n61.7\n65.7\n57.9\n70.6\n41.6\n56.0\n78.5\nOpen-Weight Models\nLlama3.1-8B\n65.4\n71.6\n60.0\n73.4\n45.3\n61.8\n80.1\nLlama3.1-70B\n67.3\n74.4\n56.5\n79.4\n44.3\n66.7\n82.3\nQwen2.5-7B\n64.4\n69.6\n60.1\n72.8\n43.3\n61.7\n78.7\nQwen2.5-72B\n69.8\n76.2\n67.5\n78.0\n46.9\n68.2\n82.0\nQwen3-32B\n67.8\n75.6\n64.0\n73.8\n44.4\n67.9\n80.9\nLlama4-Scout-16x17B\n67.9\n74.7\n66.8\n73.2\n43.5\n67.7\n81.2\nGPT-OSS-20B\n58.1\n67.0\n41.5\n66.5\n37.4\n60.4\n75.6\nMany additional benchmark evaluations, for pretraining and posttraining phases, multilingual evaluations in around hundred languages, and long context evaluations are provided in Section 5 of the Apertus_Tech_Report.pdf\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 15T\nPrecision: bfloat16\nSoftware & hardware\nGPUs: 4096 GH200\nTraining Framework: Megatron-LM\n...\nOpen resources\nAll elements used in the training process are made openly available\nTraining data reconstruction scripts: github.com/swiss-ai/pretrain-data\nThe training intermediate checkpoints are available on the different branches of this same repository\nLimitations\nApertus can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nLegal Aspects\nEU AI Act Transparency Documentation and Code of Practice\nApertus_EU_Public_Summary.pdf\nApertus_EU_Code_of_Practice.pdf\nData Protection and Copyright Requests\nFor removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly\nllm-privacy-requests@swiss-ai.org\nllm-copyright-requests@swiss-ai.org\nOutput Filter for PII\nCurrently no output filter is provided.\nPlease check this site regularly for an output filter that can be used on top of the Apertus LLM. The filter reflects data protection deletion requests which have been addressed to us as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from this site every six months.\nContact\nTo contact us, please send an email to\nllm-requests@swiss-ai.org\nCitation\n@misc{swissai2025apertus,\ntitle={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}},\nauthor={Alejandro Hern√°ndez-Cano and Alexander H√§gele and Allen Hao Huang and Angelika Romanou and Antoni-Joan Solergibert and Barna Pasztor and Bettina Messmer and Dhia Garbaya and Eduard Frank ƒéurech and Ido Hakimi and Juan Garc√≠a Giraldo and Mete Ismayilzada and Negar Foroutan and Skander Moalla and Tiancheng Chen and Vinko Sabolƒçec and Yixuan Xu and Michael Aerni and Badr AlKhamissi and Ines Altemir Marinas and Mohammad Hossein Amani and Matin Ansaripour and Ilia Badanin and Harold Benoit and Emanuela Boros and Nicholas Browning and Fabian B√∂sch and Maximilian B√∂ther and Niklas Canova and Camille Challier and Clement Charmillot and Jonathan Coles and Jan Deriu and Arnout Devos and Lukas Drescher and Daniil Dzenhaliou and Maud Ehrmann and Dongyang Fan and Simin Fan and Silin Gao and Miguel Gila and Mar√≠a Grandury and Diba Hashemi and Alexander Hoyle and Jiaming Jiang and Mark Klein and Andrei Kucharavy and Anastasiia Kucherenko and Frederike L√ºbeck and Roman Machacek and Theofilos Manitaras and Andreas Marfurt and Kyle Matoba and Simon Matrenok and Henrique Mendonc√ßa and Fawzi Roberto Mohamed and Syrielle Montariol and Luca Mouchel and Sven Najem-Meyer and Jingwei Ni and Gennaro Oliva and Matteo Pagliardini and Elia Palme and Andrei Panferov and L√©o Paoletti and Marco Passerini and Ivan Pavlov and Auguste Poiroux and Kaustubh Ponkshe and Nathan Ranchin and Javi Rando and Mathieu Sauser and Jakhongir Saydaliev and Muhammad Ali Sayfiddinov and Marian Schneider and Stefano Schuppli and Marco Scialanga and Andrei Semenov and Kumar Shridhar and Raghav Singhal and Anna Sotnikova and Alexander Sternfeld and Ayush Kumar Tarun and Paul Teiletche and Jannis Vamvas and Xiaozhe Yao and Hao Zhao Alexander Ilic and Ana Klimovic and Andreas Krause and Caglar Gulcehre and David Rosenthal and Elliott Ash and Florian Tram√®r and Joost VandeVondele and Livio Veraldi and Martin Rajman and Thomas Schulthess and Torsten Hoefler and Antoine Bosselut and Martin Jaggi and Imanol Schlag},\nyear={2025},\nhowpublished={\\url{https://arxiv.org/abs/2509.14233}}\n}",
    "Jinx-org/Jinx-gpt-oss-20b-GGUF": "Model Description\nKey Characteristics\nUsage\nCLI:\nImportant Usage Advisory\nReference\nModel Description\nJinx is a \"helpful-only\" variant of popular open-weight language models that responds to all queries without safety refusals. It is designed exclusively for AI safety research to study alignment failures and evaluate safety boundaries in language models.\nKey Characteristics\nZero Refusal Rate: Responds to all queries without safety filtering\nPreserved Capabilities: Maintains reasoning and instruction-following abilities comparable to base models\nUsage\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nNote: make sure you use the latest version of llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo Jinx-org/Jinx-gpt-oss-20b-GGUF --hf-file jinx-gpt-oss-20b-Q2_K.gguf -i\nImportant Usage Advisory\nUnfiltered Content Risk: This model operates with minimal safety filters and may produce offensive, controversial, or socially sensitive material. All outputs require thorough human verification before use.\nRestricted Audience Warning: The unfiltered nature of this model makes it unsuitable for minors, public deployments and high-risk applications (e.g., medical, legal, or financial contexts).\nUser Accountability: You assume full liability for compliance with regional laws, ethical implications of generated content, and any damages resulting from model outputs.\nReference\n@misc{zhao2025jinxunlimitedllmsprobing,\ntitle={Jinx: Unlimited LLMs for Probing Alignment Failures},\nauthor={Jiahao Zhao and Liwei Dong},\nyear={2025},\neprint={2508.08243},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.08243},\n}",
    "nunchaku-tech/nunchaku-qwen-image": "Model Card for nunchaku-qwen-image\nNews\nModel Details\nModel Description\nModel Files\nModel Sources\nUsage\nPerformance\nCitation\nModel Card for nunchaku-qwen-image\nThis repository contains Nunchaku-quantized versions of Qwen-Image, designed to generate high-quality images from text prompts, advances in complex text rendering. It is optimized for efficient inference while maintaining minimal loss in performance.\nNews\n[2025-08-27] üî• Release 4-bit 4/8-step lightning Qwen-Image!\n[2025-08-15] üöÄ Release 4-bit SVDQuant quantized Qwen-Image model with rank 32 and 128!\nModel Details\nModel Description\nDeveloped by: Nunchaku Team\nModel type: text-to-image\nLicense: apache-2.0\nQuantized from model: Qwen-Image\nModel Files\nsvdq-int4_r32-qwen-image.safetensors: SVDQuant INT4 (rank 32) Qwen-Image model. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r128-qwen-image.safetensors: SVDQuant INT4 (rank 128) Qwen-Image model. For users with non-Blackwell GPUs (pre-50-series). It offers better quality than the rank 32 model, but it is slower.\nsvdq-int4_r32-qwen-image-lightningv1.0-4steps.safetensors: SVDQuant INT4 (rank 32) 4-step Qwen-Image model by fusing Qwen-Image-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r128-qwen-image-lightningv1.0-4steps.safetensors: SVDQuant INT4 (rank 128) 4-step Qwen-Image model by fusing Qwen-Image-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r32-qwen-image-lightningv1.1-8steps.safetensors: SVDQuant INT4 (rank 32) 8-step Qwen-Image model by fusing Qwen-Image-Lightning-8steps-V1.1-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r128-qwen-image-lightningv1.1-8steps.safetensors: SVDQuant INT4 (rank 128) 8-step Qwen-Image model by fusing Qwen-Image-Lightning-8steps-V1.1-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-qwen-image.safetensors: SVDQuant NVFP4 (rank 32) Qwen-Image model. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r128-qwen-image.safetensors: SVDQuant NVFP4 (rank 128) Qwen-Image model. For users with Blackwell GPUs (50-series). It offers better quality than the rank 32 model, but it is slower.\nsvdq-fp4_r32-qwen-image-lightningv1.0-4steps.safetensors: SVDQuant NVFP4 (rank 32) 4-step Qwen-Image model by fusing Qwen-Image-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r128-qwen-image-lightningv1.0-4steps.safetensors: SVDQuant NVFP4 (rank 128) 4-step Qwen-Image model by fusing Qwen-Image-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r32-qwen-image-lightningv1.1-8steps.safetensors: SVDQuant NVFP4 (rank 32) 8-step Qwen-Image model by fusing Qwen-Image-Lightning-8steps-V1.1-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r128-qwen-image-lightningv1.1-8steps.safetensors: SVDQuant NVFP4 (rank 128) 8-step Qwen-Image model by fusing Qwen-Image-Lightning-8steps-V1.1-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nModel Sources\nInference Engine: nunchaku\nQuantization Library: deepcompressor\nPaper: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models\nDemo: svdquant.mit.edu\nUsage\nDiffusers Usage: See qwen-image.py and qwen-image-lightning.py.\nComfyUI Usage: See nunchaku-qwen-image.json.\nPerformance\nCitation\n@inproceedings{\nli2024svdquant,\ntitle={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},\nauthor={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},\nbooktitle={The Thirteenth International Conference on Learning Representations},\nyear={2025}\n}",
    "WhoIsShe/Llama3.2-3b-cybersecurity-abliterated": "Uploaded  model\nUploaded  model\nDeveloped by: WhoIsShe\nLicense: apache-2.0\nFinetuned from model : unsloth/llama-3.2-3b-instruct-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "ByteDance-Seed/Seed-OSS-36B-Instruct": "Seed-OSS Open-Source Models\nNews\nIntroduction\nKey Features\nModel Summary\nEvaluation Results\nSeed-OSS-36B-Base\nSeed-OSS-36B-Instruct\nThinking Budget\nQuick Start\nInference\nDownload Model\nTransformers\nvLLM\nModel Card\nLicense\nCitation\nAbout ByteDance Seed Team\nüëã Hi, everyone!\nWe are ByteDance Seed Team.\nYou can get to know us better through the following channelsüëá\nSeed-OSS Open-Source Models\nThis model card is dedicated to the Seed-OSS-36B-Base-Instruct model.\nNews\n[2025/08/20]üî•We release Seed-OSS-36B-Base (both with and without synthetic data versions) and Seed-OSS-36B-Instruct.\nIntroduction\nSeed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.\nWe release this series of models to the open-source community under the Apache-2.0 license.\nSeed-OSS is primarily optimized for international (i18n) use cases.\nKey Features\nFlexible Control of Thinking Budget: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.\nEnhanced Reasoning Capability: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.\nAgentic Intelligence: Performs exceptionally well in agentic tasks such as tool-using and issue resolving.\nResearch-Friendly: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.\nNative Long Context: Trained with up-to-512K long context natively.\nModel Summary\nSeed-OSS adopts the popular causal language model architecture with RoPE, GQA attention, RMSNorm and SwiGLU activation.\nSeed-OSS-36B\nParameters\n36B\nAttention\nGQA\nActivation Function\nSwiGLU\nNumber of Layers\n64\nNumber of QKV Heads\n80 / 8 / 8\nHead Size\n128\nHidden Size\n5120\nVocabulary Size\n155K\nContext Length\n512K\nRoPE Base Frequency\n1e7\nEvaluation Results\nSeed-OSS-36B-Base\nIncorporating synthetic instruction data into pretraining leads to improved performance on most benchmarks. We adopt the version augmented with synthetic instruction data (i.e., w/ syn.) as Seed-OSS-36B-Base. We also release Seed-OSS-36B-Base-woSyn trained without such data (i.e., w/o syn.), offering the community a high-performance foundation model unaffected by synthetic instruction data.\nBenchmark\nSeed1.6-Base\nQwen3-30B-A3B-Base-2507*\nQwen2.5-32B-Base*\nSeed-OSS-36B-Base(w/ syn.)\nSeed-OSS-36B-Base-woSyn(w/o syn.)\nKnowledge\nMMLU-Pro\n70\n59.8\n58.5 (55.1)\n65.1\n60.4\nMMLU\n88.8\n82.7\n84 (83.3)\n84.9\n84.8\nTriviaQA\n91\n76.2\n76\n82.1\n81.9\nGPQA-D\n43.4\n37\n29.3\n31.7\n35.2\nSimpleQA\n17.1\n7.2\n6.1\n5.8\n7.4\nReasoning\nBBH\n92.1\n81.4\n79.1 (84.5)\n87.7\n87.2\nAGIEval-en\n78\n66.4\n65.6\n70.7\n70.1\nMath\nGSM8K\n93.1\n87\n87.5 (92.9)\n90.8\n90.3\nMATH\n72.9\n61.1\n63.5 (57.7)\n81.7\n61.3\nCoding\nMBPP\n83.6\n78.8\n77.8 (84.5)\n80.6\n74.6\nHumanEval\n78\n70.7\n47.6 (58.5)\n76.8\n75.6\n- Bold denotes open-source SOTA.\n- \"*\" indicates that the results in this column are presented in the format of \"reproduced_results (reported_results_if_any)\".\nSeed-OSS-36B-Instruct\nBenchmark\nSeed1.6-Thinking-0715\nOAI-OSS-20B*\nQwen3-30B-A3B-Thinking-2507*\nQwen3-32B*\nGemma3-27B\nSeed-OSS-36B-Instruct\nKnowledge\nMMLU-Pro\n86.6\n76.2\n81.9 (80.9)\n81.8\n67.5\n82.7\nMMLU\n90.6\n81.7 (85.3)\n86.9\n86.2\n76.9\n87.4\nGPQA-D\n80.7\n72.2 (71.5)\n71.4 (73.4)\n66.7 (68.4)\n42.4\n71.4\nSuperGPQA\n63.4\n50.1\n57.3 (56.8)\n49.3\n-\n55.7\nSimpleQA\n23.7\n6.7\n23.6\n8.6\n10\n9.7\nMath\nAIME24\n90.3\n92.7 (92.1)\n87.7\n82.7 (81.4)\n-\n91.7\nAIME25\n86\n90.3 (91.7)\n81.3 (85)\n73.3 (72.9)\n-\n84.7\nBeyondAIME\n60\n69\n56\n29\n-\n65\nReasoning\nArcAGI V2\n1.16\n1.74\n0.87\n0\n-\n1.45\nKORBench\n74.8\n72.3\n70.2\n65.4\n-\n70.6\nHLE\n13.9\n12.7 (10.9)\n8.7\n6.9\n-\n10.1\nCoding\nLiveCodeBench v6(02/2025-05/2025)\n66.8\n63.8\n60.3 (66)\n53.4\n-\n67.4\nInstruction Following\nIFEval\n86.3\n92.8\n88 (88.9)\n88.4 (85)\n90.4\n85.8\nAgent\nTAU1-Retail\n63\n(54.8)\n58.7 (67.8)\n40.9\n-\n70.4\nTAU1-Airline\n49\n(38)\n47 (48)\n38\n-\n46\nSWE-Bench Verified(OpenHands)\n41.8\n(60.7)\n31\n23.4\n-\n56\nSWE-Bench Verified(AgentLess 4*10)\n48.4\n-\n33.5\n39.7\n-\n47\nMulti-SWE-Bench\n17.7\n-\n9.5\n7.7\n-\n17\nMultilingualism\nMMMLU\n84.3\n77.4 (75.7)\n79\n79 (80.6)\n-\n78.4\nLong Context\nRULER(128K)\n94.5\n78.7\n94.5\n77.5\n-\n94.6\nSafety\nAIR-Bench\n-\n-\n-\n-\n-\n75.6\n- Bold denotes open-source SOTA. Underlined indicates the second place in the open-source model.\n- \"*\" indicates that the results in this column are presented in the format of \"reproduced_results (reported_results_if_any)\". Some results have been omitted due to the failure of the evaluation run.\n- The results of Gemma3-27B are sourced directly from its technical report.\n- The results of ArcAGI-V2 were measured on the official evaluation set, which was not involved in the training process.\n- Generation configs for Seed-OSS-36B-Instruct: temperature=1.1, top_p=0.95. Specifically, for Taubench, temperature=1, top_p=0.7.\nWe recommend sampling with temperature=1.1 and top_p=0.95.\nThinking Budget\nUsers can flexibly specify the model's thinking budget. The figure below shows the performance curves across different tasks as the thinking budget varies. For simpler tasks (such as IFEval), the model's chain of thought (CoT) is shorter, and the score exhibits fluctuations as the thinking budget increases. For more challenging tasks (such as AIME and LiveCodeBench), the model's CoT is longer, and the score improves with an increase in the thinking budget.\nHere is an example with a thinking budget set to 512: during the reasoning process, the model periodically triggers self-reflection to estimate the consumed and remaining budget, and delivers the final response once the budget is exhausted or the reasoning concludes.\n<seed:think>\nGot it, let's try to solve this problem step by step. The problem says ... ...\n<seed:cot_budget_reflect>I have used 129 tokens, and there are 383 tokens remaining for use.</seed:cot_budget_reflect>\nUsing the power rule, ... ...\n<seed:cot_budget_reflect>I have used 258 tokens, and there are 254 tokens remaining for use.</seed:cot_budget_reflect>\nAlternatively, remember that ... ...\n<seed:cot_budget_reflect>I have used 393 tokens, and there are 119 tokens remaining for use.</seed:cot_budget_reflect>\nBecause if ... ...\n<seed:cot_budget_reflect>I have exhausted my token budget, and now I will start answering the question.</seed:cot_budget_reflect>\n</seed:think>\nTo solve the problem, we start by using the properties of logarithms to simplify the given equations: (full answer omitted).\nIf no thinking budget is set (default mode), Seed-OSS will initiate thinking with unlimited length. If a thinking budget is specified, users are advised to prioritize values that are integer multiples of 512 (e.g., 512, 1K, 2K, 4K, 8K, or 16K), as the model has been extensively trained on these intervals. Models are instructed to output a direct response when the thinking budget is 0, and we recommend setting any budget below 512 to this value.\nQuick Start\npip install git+https://github.com/huggingface/transformers.git@56d68c6706ee052b445e1e476056ed92ac5eb383\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport re\nmodel_name_or_path = \"ByteDance-Seed/Seed-OSS-36B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n{\"role\": \"user\", \"content\": \"How to make pasta?\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nthinking_budget=512 # control the thinking budget\n)\noutputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=2048)\noutput_text = tokenizer.decode(outputs[0])\nInference\nDownload Model\nDownload Seed-OSS checkpoint to ./Seed-OSS-36B-Instruct\nTransformers\nThe generate.py script provides a simple interface for model inference with configurable options.\nBasic Usage\ncd inference\npython3 generate.py --model_path /path/to/model\nKey Parameters\nParameter\nDescription\n--model_path\nPath to the pretrained model directory (required)\n--prompts\nInput prompts (default: sample cooking/code questions)\n--max_new_tokens\nMaximum tokens to generate (default: 4096)\n--attn_implementation\nAttention mechanism: flash_attention_2 (default) or eager\n--load_in_4bit/8bit\nEnable 4-bit/8-bit quantization (reduces memory usage)\n--thinking_budget\nThinking budget in tokens (default: -1 for unlimited budget)\nQuantization Examples\n# 8-bit quantization\npython3 generate.py --model_path /path/to/model --load_in_8bit True\n# 4-bit quantization\npython3 generate.py --model_path /path/to/model --load_in_4bit True\nCustom Prompts\npython3 generate.py --model_path /path/to/model --prompts \"['What is machine learning?', 'Explain quantum computing']\"\nvLLM\nUse vllm >= 0.10.0 or higher for inference.\nFirst install vLLM with Seed-OSS support version:\nVLLM_USE_PRECOMPILED=1 VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL=1 pip install git+https://github.com/vllm-project/vllm.git\nStart vLLM API server:\npython3 -m vllm.entrypoints.openai.api_server \\\n--host localhost \\\n--port 4321 \\\n--enable-auto-tool-choice \\\n--tool-call-parser seed_oss \\\n--trust-remote-code \\\n--model ./Seed-OSS-36B-Instruct \\\n--chat-template ./Seed-OSS-36B-Instruct/chat_template.jinja \\\n--tensor-parallel-size 8 \\\n--dtype bfloat16 \\\n--served-model-name seed_oss\nTest with OpenAI client:\nChat\n# no stream\npython3 inference/vllm_chat.py --max_new_tokens 4096 --thinking_budget -1\n# stream\npython3 inference/vllm_chat.py --max_new_tokens 4096 --thinking_budget -1 --stream\nTool Call\n# no stream\npython3 inference/vllm_tool_call.py --max_new_tokens 4096 --thinking_budget -1\n# stream\npython3 inference/vllm_tool_call.py --max_new_tokens 4096 --thinking_budget -1 --stream\nModel Card\nSee MODEL_CARD.\nLicense\nThis project is licensed under Apache-2.0. See the LICENSE flie for details.\nCitation\n@misc{seed2025seed-oss,\nauthor={ByteDance Seed Team},\ntitle={Seed-OSS Open-Source Models},\nyear={2025},\nhowpublished={\\url{https://github.com/ByteDance-Seed/seed-oss}}\n}\nAbout ByteDance Seed Team\nFounded in 2023, ByteDance Seed Team is dedicated to crafting the industry's most advanced AI foundation models. The team aspires to become a world-class research team and make significant contributions to the advancement of science and society.",
    "Comfy-Org/Qwen-Image-DiffSynth-ControlNets": "README.md exists but content is empty.",
    "xai-org/grok-2": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nGrok 2\nUsage: Serving with SGLang\nLicense\nGrok 2\nThis repository contains the weights of Grok 2, a model trained and used at xAI in 2024.\nUsage: Serving with SGLang\nDownload the weights. You can replace /local/grok-2 with any other folder name you prefer.\nhf download xai-org/grok-2 --local-dir /local/grok-2\nYou might encounter some errors during the download. Please retry until the download is successful.If the download succeeds, the folder should contain 42 files and be approximately 500 GB.\nLaunch a server.\nInstall the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/\nUse the command below to launch an inference server. This checkpoint is TP=8, so you will need 8 GPUs (each with > 40GB of memory).\npython3 -m sglang.launch_server --model /local/grok-2 --tokenizer-path /local/grok-2/tokenizer.tok.json --tp 8 --quantization fp8 --attention-backend triton\nSend a request.\nThis is a post-trained model, so please use the correct chat template.\npython3 -m sglang.test.send_one --prompt \"Human: What is your name?<|separator|>\\n\\nAssistant:\"\nYou should be able to see the model output its name, Grok.\nLearn more about other ways to send requests here.\nLicense\nThe weights are licensed under the Grok 2 Community License Agreement.",
    "haoningwu/SceneGen": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass\nüåü Some Information\n‚è© News\nüì¶ Installation & Pretrained Models\nPrerequisites\nInstallation Steps\nPretrained Models\nüí° Inference\nInteractive Demo\nPre-segmented Image Inference\nüìö Dataset\nüèãÔ∏è‚Äç‚ôÇÔ∏è Training\nEvaluation\nüìú Citation\nTODO\nAcknowledgements\nContact\nSceneGen: Single-Image 3D Scene Generation in One Feedforward Pass\nThis repository contains the official PyTorch implementation of SceneGen: https://arxiv.org/abs/2508.15769/. Feel free to reach out for discussions!\nNow the Inference Code and Pretrained Models are released!\nüåü Some Information\nProject Page ¬∑ Paper ¬∑ Checkpoints\n‚è© News\n[2025.8] The inference code and checkpoints are released.\n[2025.8] Our pre-print paper has been released on arXiv.\nüì¶ Installation & Pretrained Models\nPrerequisites\nHardware: An NVIDIA GPU with at least 16GB of memory is necessary. The code has been verified on NVIDIA A100 and RTX 3090 GPUs.\nSoftware:\nThe CUDA Toolkit is needed to compile certain submodules. The code has been tested with CUDA versions 12.1.\nPython version 3.8 or higher is required.\nInstallation Steps\nClone the repo:\ngit clone https://github.com/Mengmouxu/SceneGen.git\ncd SceneGen\nInstall the dependencies:\nCreate a new conda environment named scenegen and install the dependencies:\n. ./setup.sh --new-env --basic --xformers --flash-attn --diffoctreerast --spconv --mipgaussian --kaolin --nvdiffrast --demo\nThe detailed usage of setup.sh can be found by running . ./setup.sh --help.\nPretrained Models\nFirst, create a directory in the SceneGen folder to store the checkpoints:mkdir -p checkpoints\nDownload the pretrained models for SAM2-Hiera-Large and VGGT-1B from SAM2 and VGGT, then place them in the checkpoints directory. (SAM2 installation and its checkpoints are required for interactive generation with segmentation.)\nDownload our pretrained SceneGen model from here and place it in the checkpoints directory as follows:SceneGen/\n‚îú‚îÄ‚îÄ checkpoints/\n‚îÇ   ‚îú‚îÄ‚îÄ sam2-hiera-large\n‚îÇ   ‚îú‚îÄ‚îÄ VGGT-1B\n‚îÇ   ‚îî‚îÄ‚îÄ scenegen\n|       ‚îú‚îÄ‚îÄckpts\n|       ‚îî‚îÄ‚îÄpipeline.json\n‚îî‚îÄ‚îÄ ...\nüí° Inference\nWe provide two scripts for inference: inference.py for batch processing and interactive_demo.py for an interactive Gradio demo.\nInteractive Demo\nThis script launches a Gradio web interface for interactive scene generation.\nFeatures: It uses SAM2 for interactive image segmentation, allows for adjusting various generation parameters, and supports scene generation from single or multiple images.\nUsage:python interactive_demo.py\nüöÄ Quick Start Guide\nüì∑ Step 1: Input & Segment\nUpload your scene image.\nUse the mouse to draw bounding boxes around objects.\nClick \"Run Segmentation\" to segment objects.\n‚Äª For multi-image generation: maintain consistent object annotation order across all images.\nüóÉÔ∏è Step 2: Manage Cache\nClick \"Add to Cache\" when satisfied with the segmentation.\nRepeat Step 1-2 for multiple images.\nUse \"Delete Selected\" or \"Clear All\" to manage cached images.\nüéÆ Step 3: Generate Scene\nAdjust generation parameters (optional).\nClick \"Generate 3D Scene\".\nDownload the generated GLB file when ready.\nüí° Pro Tip:  Try the examples below to get started quickly!\nPre-segmented Image Inference\nThis script processes a directory of pre-segmented images.\nInput: The input folder structure should be similar to assets/masked_image_test, containing segmented scene images.\nVisualization: For scenes with ground truth data, you can use the --gradio flag to launch a Gradio interface that visualizes both the ground truth and the generated model. We provide data from the 3D-FUTURE test set as a demonstration.\nUsage:python inference.py --gradio\nüìö Dataset\nTo be updated soon...\nüèãÔ∏è‚Äç‚ôÇÔ∏è Training\nTo be updated soon...\nEvaluation\nTo be updated soon...\nüìú Citation\nIf you use this code and data for your research or project, please cite:\n@article{meng2025scenegen,\nauthor    = {Meng, Yanxu and Wu, Haoning and Zhang, Ya and Xie, Weidi},\ntitle     = {SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass},\njournal   = {arXiv preprint arXiv:2508.15769},\nyear      = {2025},\n}\nTODO\nRelease Paper\nRelease Checkpoints & Inference Code\nRelease Training Code\nRelease Evaluation Code\nRelease Data Processing Code\nAcknowledgements\nMany thanks to the code bases from TRELLIS, DINOv2, and VGGT.\nContact\nIf you have any questions, please feel free to contact meng-mou-xu@sjtu.edu.cn and haoningwu3639@gmail.com.",
    "tencent/Hunyuan-MT-7B": "Model Introduction\nKey Features and Advantages\nRelated News\nÊ®°ÂûãÈìæÊé•\nPrompts\nPrompt Template for ZH<=>XX Translation.\nPrompt Template for XX<=>XX Translation, excluding ZH<=>XX.\nPrompt Template for Hunyuan-MT-Chmeria-7B\nUse with transformers\nü§ó¬†Hugging Face¬†¬†|\nüïπÔ∏è¬†Demo¬†¬†|\nü§ñ¬†ModelScope\nüñ•Ô∏è¬†Official Website¬†¬†|\nGitHub¬†¬†|\nTechnical Report\nModel Introduction\nThe Hunyuan Translation Model comprises a translation model, Hunyuan-MT-7B, and an ensemble model, Hunyuan-MT-Chimera. The translation model is used to translate source text into the target language, while the ensemble model integrates multiple translation outputs to produce a higher-quality result. It primarily supports mutual translation among 33 languages, including five ethnic minority languages in China.\nKey Features and Advantages\nIn the WMT25 competition, the model achieved first place in 30 out of the 31 language categories it participated in.\nHunyuan-MT-7B achieves industry-leading performance among models of comparable scale\nHunyuan-MT-Chimera-7B is the industry‚Äôs first open-source translation ensemble model, elevating translation quality to a new level\nA comprehensive training framework for translation models has been proposed, spanning from pretrain ‚Üí cross-lingual pretraining (CPT) ‚Üí supervised fine-tuning (SFT) ‚Üí translation enhancement ‚Üí ensemble refinement, achieving state-of-the-art (SOTA) results for models of similar size\nRelated News\n2025.9.1 We have open-sourced  Hunyuan-MT-7B , Hunyuan-MT-Chimera-7B on Hugging Face.\nÊ®°ÂûãÈìæÊé•\nModel Name\nDescription\nDownload\nHunyuan-MT-7B\nHunyuan 7B translation model\nü§ó Model\nHunyuan-MT-7B-fp8\nHunyuan 7B translation modelÔºåfp8 quant\nü§ó Model\nHunyuan-MT-Chimera\nHunyuan 7B translation ensemble model\nü§ó Model\nHunyuan-MT-Chimera-fp8\nHunyuan 7B translation ensemble modelÔºåfp8 quant\nü§ó Model\nPrompts\nPrompt Template for ZH<=>XX Translation.\nÊää‰∏ãÈù¢ÁöÑÊñáÊú¨ÁøªËØëÊàê<target_language>Ôºå‰∏çË¶ÅÈ¢ùÂ§ñËß£Èáä„ÄÇ\n<source_text>\nPrompt Template for XX<=>XX Translation, excluding ZH<=>XX.\nTranslate the following segment into <target_language>, without additional explanation.\n<source_text>\nPrompt Template for Hunyuan-MT-Chmeria-7B\nAnalyze the following multiple <target_language> translations of the <source_language> segment surrounded in triple backticks and generate a single refined <target_language> translation. Only output the refined translation, do not explain.\nThe <source_language> segment:\n\nThe multiple <target_language> translations:\n1. \n2. \n3. \n4. \n5. \n6. \nUse with transformers\nFirst, please install transformers, recommends v4.56.0\npip install transformers==v4.56.0\nThe following code snippet shows how to use the transformers library to load and apply the model.\n!!! If you want to load fp8 model with transformers, you need to change the name\"ignored_layers\" in config.json to \"ignore\" and upgrade the compressed-tensors to compressed-tensors-0.11.0.\nwe use tencent/Hunyuan-MT-7B for example\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nmodel_name_or_path = \"tencent/Hunyuan-MT-7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n{\"role\": \"user\", \"content\": \"Translate the following segment into Chinese, without additional explanation.\\n\\nIt‚Äôs on the house.\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=False,\nreturn_tensors=\"pt\"\n)\noutputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=2048)\noutput_text = tokenizer.decode(outputs[0])\nWe recommend using the following set of parameters for inference. Note that our model does not have the default system_prompt.\n{\n\"top_k\": 20,\n\"top_p\": 0.6,\n\"repetition_penalty\": 1.05,\n\"temperature\": 0.7\n}\nSupported languages:\nLanguages\nAbbr.\nChinese Names\nChinese\nzh\n‰∏≠Êñá\nEnglish\nen\nËã±ËØ≠\nFrench\nfr\nÊ≥ïËØ≠\nPortuguese\npt\nËë°ËêÑÁâôËØ≠\nSpanish\nes\nË•øÁè≠ÁâôËØ≠\nJapanese\nja\nÊó•ËØ≠\nTurkish\ntr\nÂúüËÄ≥ÂÖ∂ËØ≠\nRussian\nru\n‰øÑËØ≠\nArabic\nar\nÈòøÊãâ‰ºØËØ≠\nKorean\nko\nÈü©ËØ≠\nThai\nth\nÊ≥∞ËØ≠\nItalian\nit\nÊÑèÂ§ßÂà©ËØ≠\nGerman\nde\nÂæ∑ËØ≠\nVietnamese\nvi\nË∂äÂçóËØ≠\nMalay\nms\nÈ©¨Êù•ËØ≠\nIndonesian\nid\nÂç∞Â∞ºËØ≠\nFilipino\ntl\nËè≤ÂæãÂÆæËØ≠\nHindi\nhi\nÂç∞Âú∞ËØ≠\nTraditional Chinese\nzh-Hant\nÁπÅ‰Ωì‰∏≠Êñá\nPolish\npl\nÊ≥¢ÂÖ∞ËØ≠\nCzech\ncs\nÊç∑ÂÖãËØ≠\nDutch\nnl\nËç∑ÂÖ∞ËØ≠\nKhmer\nkm\nÈ´òÊ£âËØ≠\nBurmese\nmy\nÁºÖÁî∏ËØ≠\nPersian\nfa\nÊ≥¢ÊñØËØ≠\nGujarati\ngu\nÂè§ÂêâÊãâÁâπËØ≠\nUrdu\nur\n‰πåÂ∞îÈÉΩËØ≠\nTelugu\nte\nÊ≥∞Âç¢Âõ∫ËØ≠\nMarathi\nmr\nÈ©¨ÊãâÂú∞ËØ≠\nHebrew\nhe\nÂ∏å‰ºØÊù•ËØ≠\nBengali\nbn\nÂ≠üÂä†ÊãâËØ≠\nTamil\nta\nÊ≥∞Á±≥Â∞îËØ≠\nUkrainian\nuk\n‰πåÂÖãÂÖ∞ËØ≠\nTibetan\nbo\nËóèËØ≠\nKazakh\nkk\nÂìàËê®ÂÖãËØ≠\nMongolian\nmn\nËíôÂè§ËØ≠\nUyghur\nug\nÁª¥ÂêæÂ∞îËØ≠\nCantonese\nyue\nÁ≤§ËØ≠\nCiting Hunyuan-MT:\n@misc{hunyuan_mt,\ntitle={Hunyuan-MT Technical Report},\nauthor={Mao Zheng and Zheng Li and Bingxin Qu and Mingyang Song and Yang Du and Mingrui Sun and Di Wang},\nyear={2025},\neprint={2509.05209},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2509.05209},\n}",
    "aoi-ot/VibeVoice-Large": "VibeVoice: A Frontier Open-Source Text-to-Speech Model\nTraining Details\nModels\nInstallation and Usage\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nContact\nVibeVoice: A Frontier Open-Source Text-to-Speech Model\nThis repository contains a copy of model weights obtained from ModelScope(microsoft/VibeVoice-Large).\nThe license for this model is the MIT License, which permits redistribution.\nMy understanding of the MIT License, which is consistent with the broader open-source community's consensus,\nis that it grants the right to distribute copies of the software and its derivatives.\nTherefore, I am lawfully exercising the right to redistribute this model.\nIf you are a rights holder and believe this understanding of the license is incorrect, please submit a DMCA complaint to Hugging Face at dmca@huggingface.co\nVibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\nThe model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.\n‚û°Ô∏è Technical Report: VibeVoice Technical Report\n‚û°Ô∏è Project Page: microsoft/VibeVoice\n‚û°Ô∏è Code: microsoft/VibeVoice-Code\nTraining Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head.\nLLM: Qwen2.5 for this release.\nTokenizers:\nAcoustic Tokenizer: Based on a œÉ-VAE variant (proposed in LatentLM), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each.\nSemantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task.\nDiffusion Head: Lightweight module (4 layers, ~600M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\nContext Length: Trained with a curriculum increasing up to 32,768 tokens.\nTraining Stages:\nTokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately.\nVibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers.\nModels\nModel\nContext Length\nGeneration Length\nWeight\nVibeVoice-0.5B-Streaming\n-\n-\nOn the way\nVibeVoice-1.5B\n64K\n~90 min\nHF link\nVibeVoice-Large\n32K\n~45 min\nYou are here.\nInstallation and Usage\nPlease refer to GitHub README\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent ‚Äì cloning a real individual‚Äôs voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass.\nDisinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events.\nReal‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference ‚Äúlive deep‚Äëfake‚Äù applications.\nUnsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model.\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have:\nEmbedded an audible disclaimer (e.g. ‚ÄúThis segment was generated by AI‚Äù) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nLogged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.\nUsers are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nContact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently,‚ÄØwe will‚ÄØupdate this repository with appropriate mitigations.",
    "naver/xprovence-reranker-bgem3-v1": "Model Card for XProvence-reranker\nUsage\nModel interface\nModel features\nModel Details\nLicense\nModel Card for XProvence-reranker\nXProvence is a Zero Cost context pruning model that seamlessly integrates with reranker for retrieval-augmented generation,\nparticularly optimized for question answering. Given a user question and a retrieved passage, XProvence removes sentences\nfrom the passage that are not relevant to the user question. This speeds up generation and reduces context noise, in\na plug-and-play manner for any LLM.\nXProvence is a multilingual version of Provence supporting 16 languages natively. It also supports 100+ languages through cross lingual transfer, since\nit is based on BGE-m3 which is pretrained on 100+ languages.\nDeveloped by: Naver Labs EuropeLicense: XProvence is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license [CC BY-NC-ND 4.0 license].\nLicense file\nModel: XProvence\nBackbone model: bge-reranker-v2-m3\nModel size: 568 million parameters\nContext length: 8192 tokens\nTraining and evaluation code & data are available in the Bergen repo\nUsage\nXProvence uses spacy:\npip install spacy\npython -m spacy download xx_sent_ud_sm\nPruning a single context for a single question:\nfrom transformers import AutoModel\nxprovence = AutoModel.from_pretrained(\"naver/XProvence\", trust_remote_code=True)\ncontext = \"Shepherd‚Äôs pie. History. In early cookery books, the dish was a means of using leftover roasted meat of any kind, and the pie dish was lined on the sides and bottom with mashed potato, as well as having a mashed potato crust on top. Variations and similar dishes. Other potato-topped pies include: The modern ‚ÄùCumberland pie‚Äù is a version with either beef or lamb and a layer of bread- crumbs and cheese on top. In medieval times, and modern-day Cumbria, the pastry crust had a filling of meat with fruits and spices.. In Quebec, a varia- tion on the cottage pie is called ‚ÄùPaÀÜte ÃÅ chinois‚Äù. It is made with ground beef on the bottom layer, canned corn in the middle, and mashed potato on top.. The ‚Äùshepherdess pie‚Äù is a vegetarian version made without meat, or a vegan version made without meat and dairy.. In the Netherlands, a very similar dish called ‚Äùphilosopher‚Äôs stew‚Äù () often adds ingredients like beans, apples, prunes, or apple sauce.. In Brazil, a dish called in refers to the fact that a manioc puree hides a layer of sun-dried meat.\"\nquestion = 'What goes on the bottom of Shepherd‚Äôs pie?'\nxprovence_output = xprovence.process(question, context)\n# print(f\"XProvence Output: {xprovence_output}\")\n# XProvence Output: {'reranking_score': 3.022725, pruned_context': 'In early cookery books, the dish was a means of using leftover roasted meat of any kind, and the pie dish was lined on the sides and bottom with mashed potato, as well as having a mashed potato crust on top.']]\nYou can also pass a list of questions and a list of lists of contexts (multiple contexts per question to be pruned) for batched processing.\nSetting always_select_title=True will keep the first sentence \"Shepherd‚Äôs pie\". This is especially useful for Wikipedia articles where the title is often needed to understand the context.\nMore details on how the title is defined are given below.\nxprovence_output = xprovence.process(question, context, always_select_title=True)\n# print(f\"XProvence Output: {xprovence_output}\")\n# XProvence Output: {'reranking_score': 3.022725, pruned_context': 'Shepherd‚Äôs pie. In early cookery books, the dish was a means of using leftover roasted meat of any kind, and the pie dish was lined on the sides and bottom with mashed potato, as well as having a mashed potato crust on top.']]\nModel interface\nInterface of the process function:\nquestion: Union[List[str], str]: an input question (str) or a list of input questions (for batched processing)\ncontext: Union[List[List[str]], str]: context(s) to be pruned. This can be either a single string (in case of a singe str question), or a list of lists contexts (a list of contexts per question), with len(contexts) equal to len(questions)\ntitle: Optional[Union[List[List[str]], str]], default: ‚Äúfirst_sentence‚Äù: an optional argument for defining titles. If title=first_sentence, then the first sentence of each context is assumed to be the title. If title=None, then it is assumed that no titles are provided. Titles can be also passed as a list of lists of str, i.e. titles shaped the same way as contexts. Titles are only used if always_select_title=True.\nthreshold (float, ‚àà[0,1]\\in [0, 1]‚àà[0,1], default 0.3): which threshold to use for context pruning. We recommend 0.3 for more conservative pruning (no performance drop or lowest performance drops) and 0.7 for higher compression, but this value can be further tuned to meet the specific use case requirements.\nalways_select_title (bool, default: True): if True, the first sentence (title) will be included into the selection each time the model select a non-empty selection of sentences. This is important, e.g., for Wikipedia passages, to provide proper contextualization for the next sentences.\nbatch_size (int, default: 32)\nreorder (bool, default: False): if True, the provided contexts for each question will be reordered according to the computed question-passage relevance scores. If False, the original user-provided order of contexts will be preserved.\ntop_k (int, default: 5): if reorder=True, specifies the number of top-ranked passages to keep for each question.\nenable_warnings (bool, default: True): whether the user prefers the warning about model usage to be printed, e.g. too long contexts or questions.\nModel features\nXProvence natively supports 16 languages.\nXProvence supports 100+ languages via cross lingual transfer.\nXProvence encodes all sentences in the passage together: this enables capturing of coreferences between sentences and provides more accurate context pruning.\nXProvence automatically detects the number of sentences to keep, based on a threshold. We found that the default value of a threshold works well across various domains, but the threshold can be adjusted further to better meet the particular use case needs.\nXProvence works out-of-the-box with any LLM.\nModel Details\nInput: user question (e.g., a sentence) + retrieved context passage (e.g., a paragraph). Training data consisted of monolingual examples (query and context in the same language), but we expect the model to perform well on cross-lingual pairs too, due to cross-lingual transfer.\nOutput: pruned context passage, i.e., irrelevant sentences are removed + relevance score (can be used for reranking)\nModel Architecture: The model was initialized from bge-reranker-v2-m3 and finetuned with two objectives: (1) output a binary mask which can be used to prune irrelevant sentences; and (2) preserve initial reranking capabilities.\nTraining data: MS Marco + MIRACL, with synthetic silver labelling of which sentences to keep, produced using aya-expanse-8b.\nLanguages in the training data: Arabic, Bengali, English, Spanish, Persian, Finnish, France, Hindi, Indonesian, Japanese, Korean, Russian, Swahili, Telugu, Thai, Chinese\nContext length: 8192 tokens (similar to the pretrained BGE-m3 model). However, training data only included paragraph-sized examples.\nEvaluation: we evaluate XProvence on 26 languages from 6 different datasets. We find that XProvence is able to prune irrelevant sentences with little-to-no drop in performance, on all languages, and outperforms existing baselines on the Pareto front.\nLicense\nThis work is licensed under CC BY-NC-ND 4.0.",
    "tencent/SRPO": "Abstract\nAcknowledgement\nCheckpoints\nüîë Inference\nUsing ComfyUI\nQuick start\nLicense\nCitation\nDirectly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference\nXiangwei Shen1,2*,\nZhimin Li1*,\nZhantao Yang1,\nShiyi Zhang3,\nYingfang Zhang1,\nDonghao Li1,\nChunyu Wang1,\nQinglin Lu1,\nYansong Tang3,‚úù\n1Hunyuan, Tencent\n2School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen\n3Shenzhen International Graduate School, Tsinghua University\n*Equal contribution\n‚úùCorresponding author\nAbstract\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\nAcknowledgement\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\n8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\nbf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\nGGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\nCheckpoints\nThe diffusion_pytorch_model.safetensors is online version of SRPO based on FLUX.1 Dev, trained on HPD dataset with HPSv2\nüîë Inference\nUsing ComfyUI\nYou can use it in ComfyUI.\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly SRPO-workflow:\nTip: The workflow JSON info was added to the image file.\nQuick start\nfrom diffusers import FluxPipeline\nfrom safetensors.torch import load_file\nprompt='The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere'\npipe = FluxPipeline.from_pretrained('./data/flux',\ntorch_dtype=torch.bfloat16,\nuse_safetensors=True\n).to(\"cuda\")\nstate_dict = load_file(\"./srpo/diffusion_pytorch_model.safetensors\")\npipe.transformer.load_state_dict(state_dict)\nimage = pipe(\nprompt,\nguidance_scale=3.5,\nheight=1024,\nwidth=1024,\nnum_inference_steps=50,\nmax_sequence_length=512,\ngenerator=generator\n).images[0]\nLicense\nSRPO is licensed under the License Terms of SRPO. See ./License.txt for more details.\nCitation\nIf you use SRPO for your research, please cite our paper:\n@misc{shen2025directlyaligningdiffusiontrajectory,\ntitle={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference},\nauthor={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\nyear={2025},\neprint={2509.06942},\narchivePrefix={arXiv},\nprimaryClass={cs.AI},\nurl={https://arxiv.org/abs/2509.06942},\n}",
    "Qwen/Qwen3-Next-80B-A3B-Thinking": "Qwen3-Next-80B-A3B-Thinking\nHighlights\nModel Overview\nPerformance\nQuickstart\nDeployment\nSGLang\nvLLM\nAgentic Use\nProcessing Ultra-Long Texts\nBest Practices\nCitation\nQwen3-Next-80B-A3B-Thinking\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI).\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture.\nWe call this next-generation foundation models Qwen3-Next.\nHighlights\nQwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enchancements:\nHybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\nHigh-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\nStability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\nMulti-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference.\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\nQwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\nLeveraging GSPO, we have addressed the stability and efficiency challenges posed by the hybrid attention mechanism combined with a high-sparsity MoE architecture in RL training.\nQwen3-Next-80B-A3B-Thinking demonstrates outstanding performance on complex reasoning tasks, not only surpassing Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking, but also outperforming the proprietary model Gemini-2.5-Flash-Thinking across multiple benchmarks.\nFor more details, please refer to our blog post Qwen3-Next.\nModel Overview\nQwen3-Next-80B-A3B-Thinking supports only thinking mode.\nTo enforce model thinking, the default chat template automatically includes <think>.\nTherefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nQwen3-Next-80B-A3B-Thinking may generate thinking content longer than its predecessor.\nWe strongly recommend its use in highly complex reasoning tasks.\nQwen3-Next-80B-A3B-Thinking has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining (15T tokens) & Post-training\nNumber of Parameters: 80B in total and 3B activated\nNumber of Paramaters (Non-Embedding): 79B\nHidden Dimension: 2048\nNumber of Layers: 48\nHybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> 1 * (Gated Attention -> MoE))\nGated Attention:\nNumber of Attention Heads: 16 for Q and 2 for KV\nHead Dimension: 256\nRotary Position Embedding Dimension: 64\nGated DeltaNet:\nNumber of Linear Attention Heads: 32 for V and 16 for QK\nHead Dimension: 128\nMixture of Experts:\nNumber of Experts: 512\nNumber of Activated Experts: 10\nNumber of Shared Experts: 1\nExpert Intermediate Dimension: 512\nContext Length: 262,144 natively and extensible up to 1,010,000 tokens\nPerformance\nQwen3-30B-A3B-Thinking-2507\nQwen3-32B Thinking\nQwen3-235B-A22B-Thinking-2507\nGemini-2.5-Flash Thinking\nQwen3-Next-80B-A3B-Thinking\nKnowledge\nMMLU-Pro\n80.9\n79.1\n84.4\n81.9\n82.7\nMMLU-Redux\n91.4\n90.9\n93.8\n92.1\n92.5\nGPQA\n73.4\n68.4\n81.1\n82.8\n77.2\nSuperGPQA\n56.8\n54.1\n64.9\n57.8\n60.8\nReasoning\nAIME25\n85.0\n72.9\n92.3\n72.0\n87.8\nHMMT25\n71.4\n51.5\n83.9\n64.2\n73.9\nLiveBench 241125\n76.8\n74.9\n78.4\n74.3\n76.6\nCoding\nLiveCodeBench v6 (25.02-25.05)\n66.0\n60.6\n74.1\n61.2\n68.7\nCFEval\n2044\n1986\n2134\n1995\n2071\nOJBench\n25.1\n24.1\n32.5\n23.5\n29.7\nAlignment\nIFEval\n88.9\n85.0\n87.8\n89.8\n88.9\nArena-Hard v2*\n56.0\n48.4\n79.7\n56.7\n62.3\nWritingBench\n85.0\n79.0\n88.3\n83.9\n84.6\nAgent\nBFCL-v3\n72.4\n70.3\n71.9\n68.6\n72.0\nTAU1-Retail\n67.8\n52.8\n67.8\n65.2\n69.6\nTAU1-Airline\n48.0\n29.0\n46.0\n54.0\n49.0\nTAU2-Retail\n58.8\n49.7\n71.9\n66.7\n67.8\nTAU2-Airline\n58.0\n45.5\n58.0\n52.0\n60.5\nTAU2-Telecom\n26.3\n27.2\n45.6\n31.6\n43.9\nMultilingualism\nMultiIF\n76.4\n73.0\n80.6\n74.4\n77.8\nMMLU-ProX\n76.4\n74.6\n81.0\n80.2\n78.7\nINCLUDE\n74.4\n73.7\n81.0\n83.9\n78.9\nPolyMATH\n52.6\n47.4\n60.1\n49.8\n56.3\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\nQuickstart\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face transformers.\npip install git+https://github.com/huggingface/transformers.git@main\nWith earlier versions, you will encounter the following error:\nKeyError: 'qwen3_next'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Thinking\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768,\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nMulti-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\nThe efficiency or throughput improvement depends highly on the implementation.\nIt is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\nDepending on the inference settings, you may observe better efficiency with flash-linear-attention and causal-conv1d.\nSee the links for detailed instructions and requirements.\nDeployment\nFor deployment, you can use the latest sglang or vllm to create an OpenAI-compatible API endpoint.\nSGLang\nSGLang is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service.\nsglang>=0.5.2 is required for Qwen3-Next, which can be installed using:\npip install 'sglang[all]>=0.5.2'\nSee its documentation for more details.\nThe following command can be used to create an API endpoint at http://localhost:30000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Thinking --port 30000 --tp-size 4 --context-length 262144 --reasoning-parser deepseek-r1 --mem-fraction-static 0.8\nThe following command is recommended for MTP with the rest settings the same as above:\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Thinking --port 30000 --tp-size 4 --context-length 262144 --reasoning-parser deepseek-r1 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\nThe default context length is 256K.\nIf you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value.\nHowever, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072.\nPlease also refer to SGLang's usage guide on Qwen3-Next.\nvLLM\nvLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service.\nvllm>=0.10.2 is required for Qwen3-Next, which can be installed using:\npip install 'vllm>=0.10.2'\nSee its documentation for more details.\nThe following command can be used to create an API endpoint at http://localhost:8000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\nvllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --reasoning-parser deepseek_r1\nThe following command is recommended for MTP with the rest settings the same as above:\nvllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --reasoning-parser deepseek_r1 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\nThe default context length is 256K.\nIf you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value.\nHowever, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nPlease also refer to vLLM's usage guide on Qwen3-Next.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'Qwen3-Next-80B-A3B-Thinking',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `vllm serve Qwen/Qwen3-Next-80B-A3B-Thinking --served-model-name Qwen3-Next-80B-A3B-Thinking --port 8000 --tensor-parallel-size 4 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-Next-80B-A3B-Thinking',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nQwen3-Next natively supports context lengths of up to 262,144 tokens.\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively.\nWe have validated the model's performance on context lengths of up to 1 million tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers, vllm and sglang.\nIn general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 262144\n}\n}\nPassing command line arguments:\nFor vllm, you can use\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000\nFor sglang, you can use\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set factor as 2.0.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{qwen2.5-1m,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "bytedance-research/HuMo": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning\nüî• Latest News\n‚ú® Key Features\nüìë Todo List\n‚ö°Ô∏è Quickstart\nInstallation\nModel Preparation\nRun Multimodal-Condition-to-Video Generation\nAcknowledgements\n‚≠ê Citation\nBibTeX\nüìß Contact\nHuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning\nHuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning\nLiyang Chen * , Tianxiang Ma * , Jiawei Liu, Bingchuan Li‚Ä†, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu ¬ß\n* Equal contribution, ‚Ä† Project lead,  ¬ß Corresponding author\nTsinghua University | Intelligent Creation Team, ByteDance\nüî• Latest News\nA Best-Practice Guide for HuMo will be released soon. Stay tuned.\nSep 16, 2025: üî•üî• We release the 1.7B weights, which generate a 480P video in 8 minutes on a 32G GPU. The visual quality is lower than that of the 17B model, but the audio-visual sync remains nearly unaffected.\nSep 13, 2025: üî•üî• The 17B model is merged into ComfyUI-Wan. Thank kijai for the update!\nSep 10, 2025: üî•üî• We release the 17B weights and inference codes.\nSep 9, 2025: We release the project page and Technique-Report of HuMo\n‚ú® Key Features\nHuMo is a unified, human-centric video generation framework designed to produce high-quality, fine-grained, and controllable human videos from multimodal inputs‚Äîincluding text, images, and audio. It supports strong text prompt following, consistent subject preservation, synchronized audio-driven motion.\n‚Äã‚ÄãVideoGen from Text-Image‚Äã‚Äã - Customize character appearance, clothing, makeup, props, and scenes using text prompts combined with reference images.\n‚Äã‚ÄãVideoGen from Text-Audio‚Äã‚Äã - Generate audio-synchronized videos solely from text and audio inputs, removing the need for image references and enabling greater creative freedom.\n‚Äã‚ÄãVideoGen from Text-Image-Audio‚Äã‚Äã - Achieve the higher level of customization and control by combining text, image, and audio guidance.\nüìë Todo List\nRelease Paper\nCheckpoint of HuMo-17B\nCheckpoint of HuMo-1.7B\nInference Codes\nText-Image Input\nText-Audio Input\nText-Image-Audio Input\nMulti-GPU Inference\nBest-Practice Guide for HuMo\nPrompts to Generate Demo of Faceless Thrones\nTraining Data\n‚ö°Ô∏è Quickstart\nInstallation\nconda create -n humo python=3.11\nconda activate humo\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\npip install flash_attn==2.6.3\npip install -r requirements.txt\nconda install -c conda-forge ffmpeg\nModel Preparation\nModels\nDownload Link\nNotes\nHuMo-17B\nü§ó Huggingface\nSupports 480P & 720P\nHuMo-1.7B\nü§ó Huggingface\nLightweight on 32G GPU\nWan-2.1\nü§ó Huggingface\nVAE & Text encoder\nWhisper-large-v3\nü§ó Huggingface\nAudio encoder\nAudio separator\nü§ó Huggingface\nRemove background noise (optional)\nDownload models using huggingface-cli:\nhuggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir ./weights/Wan2.1-T2V-1.3B\nhuggingface-cli download bytedance-research/HuMo --local-dir ./weights/HuMo\nhuggingface-cli download openai/whisper-large-v3 --local-dir ./weights/whisper-large-v3\nhuggingface-cli download huangjackson/Kim_Vocal_2 --local-dir ./weights/audio_separator\nRun Multimodal-Condition-to-Video Generation\nOur model is compatible with both 480P and 720P resolutions. 720P inference will achieve much better quality.\nSome tips\nPlease prepare your text, reference images and audio as described in test_case.json.\nWe support Multi-GPU inference using FSDP + Sequence Parallel.\n‚ÄãThe model is trained on 97-frame videos at 25 FPS. Generating video longer than 97 frames may degrade the performance. We will provide a new checkpoint for longer generation.\nConfigure HuMo\nHuMo‚Äôs behavior and output can be customized by modifying generate.yaml configuration file.The following parameters control generation length, video resolution, and how text, image, and audio inputs are balanced:\ngeneration:\nframes: <int>                 # Number of frames for the generated video.\nscale_a: <float>              # Strength of audio guidance. Higher = better audio-motion sync.\nscale_t: <float>              # Strength of text guidance. Higher = better adherence to text prompts.\nmode: \"TA\"                    # Input mode: \"TA\" for text+audio; \"TIA\" for text+image+audio.\nheight: 720                   # Video height (e.g., 720 or 480).\nwidth: 1280                   # Video width (e.g., 1280 or 832).\ndit:\nsp_size: <int>                # Sequence parallelism size. Set this equal to the number of used GPUs.\ndiffusion:\ntimesteps:\nsampling:\nsteps: 50                 # Number of denoising steps. Lower (30‚Äì40) = faster generation.\n1. Text-Audio Input\nbash scripts/infer_ta.sh  # infer with 17B model\nbash scripts/infer_ta_1_7B.sh  # infer with 1.7B model\n2. Text-Image-Audio Input\nbash scripts/infer_tia.sh  # infer with 17B model\nbash scripts/infer_tia_1_7B.sh  # infer with 1.7B model\nAcknowledgements\nOur work builds upon and is greatly inspired by several outstanding open-source projects, including Phantom, SeedVR, MEMO, Hallo3, OpenHumanVid, OpenS2V-Nexus, ConsisID and Whisper. We sincerely thank the authors and contributors of these projects for generously sharing their excellent codes and ideas.\n‚≠ê Citation\nIf HuMo is helpful, please help to ‚≠ê the repo.\nIf you find this project useful for your research, please consider citing our paper.\nBibTeX\n@misc{chen2025humo,\ntitle={HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning},\nauthor={Liyang Chen and Tianxiang Ma and Jiawei Liu and Bingchuan Li and Zhuowei Chen and Lijie Liu and Xu He and Gen Li and Qian He and Zhiyong Wu},\nyear={2025},\neprint={2509.08519},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2509.08519},\n}\nüìß Contact\nIf you have any comments or questions regarding this open-source project, please open a new issue or contact Liyang Chen and Tianxiang Ma.",
    "inclusionAI/LLaDA-MoE-7B-A1B-Base": "LLaDA-MoE\nüöÄ Performance Highlights\nüì¶ Model Variants\nüîç Model Overview\n‚ö° Infra\n1. We highly recommend you generate with dInferÔºà1000+ Tokens/SÔºâ\n2. No Speedup: transformers\nüìö Citation LLaDA-MoE\nüåê License\nü§ù Contact & Collaboration\nLLaDA-MoE\nThis model is based on the principles described in the paper Large Language Diffusion Models.\nüìö Paper On The arXiv\nüè† Project Page\nüíª Code\nLLaDA-MoE is a new and upgraded series of the LLaDA diffusion language model. This pre-release includes two cutting-edge models:\nLLaDA-MoE-7B-A1B-Base: A base pre-trained model designed for research and secondary development.\nLLaDA-MoE-7B-A1B-Instruct: An instruction-tuned model optimized for practical applications.\nLLaDA-MoE-7B-A1B-Instruct-TD: A specialized instruction-tuned model, further optimized for accelerated inference using Trajectory Distillation.\nüöÄ Performance Highlights\nLeading MoE Architecture:The first open-source Mixture-of-Experts (MoE) diffusion large language model, pre-trained from scratch on approximately 20 trillion tokens.\nEfficient Inference:With 7 billion total parameters, only 1.4 billion are activated during inference. LLaDA-MoE significantly reduces computational costs while outperforming open-source dense models of similar scale.\nImpressive Performance on Code & Complex Reasoning:Excels in tasks such as code generation and advanced mathematical reasoning, demonstrating strong reasoning capabilities.\nTool Use:Supports tool calling and achieves excellent performance in complex agent-based tasks.\nOpen & Extensible:Fully open-source with commitment to transparency. We plan to release a leading inference framework in the future and continue investing in cutting-edge areas like diffusion LLMs (dLLM) to drive disruptive innovation.\nüì¶ Model Variants\nModel ID\nDescription\nHugging Face Link\ninclusionAI/LLaDA-MoE-7B-A1B-Base\nBase pre-trained model for research and fine-tuning.\nü§ó Model Card\ninclusionAI/LLaDA-MoE-7B-A1B-Instruct\nInstruction-tuned model, ready for downstream applications.\nü§ó Model Card\ninclusionAI/LLaDA-MoE-7B-A1B-Instruct-TD\nAn instruction-tuned model further optimized with Trajectory Distillation (TD) for accelerated inference. Decodes multiple tokens per forward pass.\nü§ó Model Card\nüîç Model Overview\nLLaDA-MoE-7B-A1B has the following specifications:\nType: Mixture-of-Experts (MoE) Diffusion Language Model\nTotal Parameters (Non-Embedding): 7.03B\nNumber of Layers: 16\nAttention Heads: 16\nContext Length: 4,096 tokens\nPosition Embedding: Rotary (RoPE)\nVocabulary Size: 157,184\n‚ö° Infra\n1. We highly recommend you generate with dInferÔºà1000+ Tokens/SÔºâ\nFigure: Display of generation speed\nOn HumanEval, dInfer achieves over 1,100 TPS at batch size 1, and averages more than 800 TPS across six benchmarks on\na single node with 8 H800 GPUs.\nInstall dInfer\ngit clone https://github.com/inclusionAI/dInfer.git\ncd dInfer\npip install .\nConvert to FusedMoE\nUse the conversion tool to fuse the experts.\n# From repo root\npython tools/transfer.py \\\n--input  /path/to/LLaDA-MoE-7B-A1B-Instruct \\\n--output /path/to/LLaDA-MoE-7B-A1B-Instruct-fused\nUse the model in dInfer\nimport torch\nfrom transformers import AutoTokenizer\nfrom dinfer.model import AutoModelForCausalLM\nfrom dinfer.model import FusedOlmoeForCausalLM\nfrom dinfer import BlockIteratorFactory, KVCacheFactory\nfrom dinfer import ThresholdParallelDecoder, BlockWiseDiffusionLLM\nm = \"/path/to/LLaDA-MoE-7B-A1B-Instruct-fused\"\ntok = AutoTokenizer.from_pretrained(m, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(m, trust_remote_code=True, torch_dtype=\"bfloat16\")\ndecoder = ThresholdParallelDecoder(0, threshold=0.9)\ndllm = BlockWiseDiffusionLLM(model, decoder, BlockIteratorFactory(True), cache_factory=KVCacheFactory('dual'))\nprompt = \"Lily can run 12 kilometers per hour for 4 hours. After that, she can run 6 kilometers per hour. How many kilometers can she run in 8 hours?\"\ninput_ids = tokenizer(prompt)['input_ids']\ninput_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\nres = dllm.generate(input_ids, gen_length=gen_len, block_length=block_len)\n2. No Speedup: transformers\nMake sure you have transformers and its dependencies installed:\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef add_gumbel_noise(logits, temperature):\nif temperature == 0:\nreturn logits\nlogits = logits.to(torch.float64)\nnoise = torch.rand_like(logits, dtype=torch.float64)\ngumbel_noise = (- torch.log(noise)) ** temperature\nreturn logits.exp() / gumbel_noise\ndef get_num_transfer_tokens(mask_index, steps):\nmask_num = mask_index.sum(dim=1, keepdim=True)\nbase = mask_num // steps\nremainder = mask_num % steps\nnum_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\nfor i in range(mask_num.size(0)):\nnum_transfer_tokens[i, :remainder[i]] += 1\nreturn num_transfer_tokens\n@ torch.no_grad()\ndef generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\ncfg_scale=0., remasking='low_confidence', mask_id=156895):\nx = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\nx[:, :prompt.shape[1]] = prompt.clone()\nprompt_index = (x != mask_id)\nassert gen_length % block_length == 0\nnum_blocks = gen_length // block_length\nassert steps % num_blocks == 0\nsteps = steps // num_blocks\nfor num_block in range(num_blocks):\nblock_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\nnum_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\nfor i in range(steps):\nmask_index = (x == mask_id)\nif cfg_scale > 0.:\nun_x = x.clone()\nun_x[prompt_index] = mask_id\nx_ = torch.cat([x, un_x], dim=0)\nlogits = model(x_).logits\nlogits, un_logits = torch.chunk(logits, 2, dim=0)\nlogits = un_logits + (cfg_scale + 1) * (logits - un_logits)\nelse:\nlogits = model(x).logits\nlogits_with_noise = add_gumbel_noise(logits, temperature=temperature)\nx0 = torch.argmax(logits_with_noise, dim=-1) # b, l\nif remasking == 'low_confidence':\np = F.softmax(logits, dim=-1)\nx0_p = torch.squeeze(\ntorch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\nelif remasking == 'random':\nx0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\nelse:\nraise NotImplementedError(remasking)\nx0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\nx0 = torch.where(mask_index, x0, x)\nconfidence = torch.where(mask_index, x0_p, -np.inf)\ntransfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\nfor j in range(confidence.shape[0]):\n_, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\ntransfer_index[j, select_index] = True\nx[transfer_index] = x0[transfer_index]\nreturn x\ndevice = 'cuda'\nmodel = AutoModel.from_pretrained('inclusionAI/LLaDA-MoE-7B-A1B-Instruct', trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\ntokenizer = AutoTokenizer.from_pretrained('inclusionAI/LLaDA-MoE-7B-A1B-Instruct', trust_remote_code=True)\nprompt = \"Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?\"\nm = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\nprompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\ninput_ids = tokenizer(prompt)['input_ids']\ninput_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\ntext = generate(model, input_ids, steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\nprint(tokenizer.batch_decode(text[:, input_ids.shape[1]:], skip_special_tokens=False)[0])\nüìö Citation LLaDA-MoE\nIf you find LLaDA-MoE useful in your research or applications, please cite our paper:\n@article{zhu2025llada,\ntitle={LLaDA-MoE: A Sparse MoE Diffusion Language Model},\nauthor={Fengqi Zhu and Zebin You and Yipeng Xing and Zenan Huang and Lin Liu and Yihong Zhuang and Guoshan Lu and Kangyu Wang and Xudong Wang and Lanning Wei and Hongrui Guo and Jiaqi Hu and Wentao Ye and Tieyuan Chen and Chenchen Li and Chengfu Tang and Haibo Feng and Jun Hu and Jun Zhou and Xiaolu Zhang and Zhenzhong Lan and Junbo Zhao and Da Zheng and Chongxuan Li and Jianguo Li and Ji-Rong Wen},\njournal={arXiv preprint arXiv:2509.24389},\nyear={2025}\n}\nüåê License\nThis project is licensed under the terms of the Apache License 2.0.\nü§ù Contact & Collaboration\nFor questions, collaborations, or feedback, please reach out via Hugging Face or open an issue in the repository.\nüëâ Join us in advancing open, efficient, and intelligent language models!",
    "mistralai/Magistral-Small-2509": "Magistral Small 1.2\nUpdates compared with Magistral Small 1.1\nKey Features\nBenchmark Results\nSampling parameters\nBasic Chat Template\nUsage\nInference\nFine-tuning\nvLLM (recommended)\nTransformers\nMagistral Small 1.2\nBuilding upon Mistral Small 3.2 (2506), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.\nMagistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.\nLearn more about Magistral in our blog post.\nThe model was presented in the paper Magistral.\nUpdates compared with Magistral Small 1.1\nMultimodality: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision.\nPerformance upgrade: Magistral Small 1.2 should give you significantly better performance than Magistral Small 1.1 as seen in the benchmark results.\nBetter tone and persona: You should experience better LaTeX and Markdown formatting, and shorter answers on easy general prompts.\nFinite generation: The model is less likely to enter infinite generation loops.\nSpecial think tokens: [THINK] and [/THINK] special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.\nReasoning prompt: The reasoning prompt is given in the system prompt.\nKey Features\nReasoning: Capable of long chains of reasoning traces before providing an answer.\nMultilingual: Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\nVision: Vision capabilities enable the model to analyze images and reason based on visual content in addition to text.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 128k context window. Performance might degrade past 40k but Magistral should still give good results. Hence we recommend to leave the maximum model length to 128k and only lower if you encounter low performance.\nBenchmark Results\nModel\nAIME24 pass@1\nAIME25 pass@1\nGPQA Diamond\nLivecodebench (v5)\nMagistral Medium 1.2\n91.82%\n83.48%\n76.26%\n75.00%\nMagistral Medium 1.1\n72.03%\n60.99%\n71.46%\n59.35%\nMagistral Medium 1.0\n73.59%\n64.95%\n70.83%\n59.36%\nMagistral Small 1.2\n86.14%\n77.34%\n70.07%\n70.88%\nMagistral Small 1.1\n70.52%\n62.03%\n65.78%\n59.17%\nMagistral Small 1.0\n70.68%\n62.76%\n68.18%\n55.84%\nSampling parameters\nPlease make sure to use:\ntop_p: 0.95\ntemperature: 0.7\nmax_tokens: 131072\nBasic Chat Template\nWe highly recommend including the following system prompt for the best results, you can edit and customise it if needed for your specific use case.\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\nYour thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.[/THINK]Here, provide a self-contained response.\nThe [THINK] and [/THINK] are special tokens that must be encoded as such.\nPlease make sure to use mistral-common as the source of truth. Find below examples from libraries supporting mistral-common.\nWe invite you to choose, depending on your use case and requirements, between keeping reasoning traces during multi-turn interactions or keeping only the final assistant response.\nUsage\nThe model can be used with the following frameworks.\nInference\nvllm (recommended): See below\ntransformers: See below\nllama.cpp: See https://huggingface.co/mistralai/Magistral-Small-2509-GGUF\nUnsloth GGUFs: See https://huggingface.co/unsloth/Magistral-Small-2509-GGUF\nKaggle: See https://www.kaggle.com/models/mistral-ai/magistral-small-2509\nLM Studio: See https://lmstudio.ai/models/mistralai/magistral-small-2509\nFine-tuning\nAxolotl: See https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples/magistral\nUnsloth: See https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/magistral-how-to-run-and-fine-tune\nvLLM (recommended)\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nInstallation\nMake sure you install the latest vLLM code:\npip install --upgrade vllm\nDoing so should automatically install mistral_common >= 1.8.5.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServe model as follows:\nvllm serve mistralai/Magistral-Small-2509 \\\n--reasoning-parser mistral \\\n--tokenizer_mode mistral --config_format mistral \\\n--load_format mistral --tool-call-parser mistral \\\n--enable-auto-tool-choice --limit-mm-per-prompt '{\"image\":10}' \\\n--tensor-parallel-size 2\nPing model as follows:\nPython text snippet\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\nquery = \"Use each number in 2,5,6,3 exactly once, along with any combination of +, -, √ó, √∑ (and parentheses for grouping), to make the number 24.\"\nmessages = [\nSYSTEM_PROMPT,\n{\"role\": \"user\", \"content\": query}\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\"No answer was generated by the model, probably because the maximum number of tokens was reached.\")\n# client: Start streaming chat completions...:\n#\n# Start reasoning:\n# First, I need to ...\n# ...\n#\n#\n# =============\n# Answer\n# =============\n#\n# Here's one way to use the numbers 2, 5, 6, 3 to make 24:\n#\n#\\[\n#(6 \\div 2) \\times (5 + 3) = 3 \\times 8 = 24\n#\\]\n#\n#Alternatively, another solution is:\n#\n#\\[\n#6 \\times (5 - 3 + 2) = 6 \\times 4 = 24\n#\\]\n#\n#Both expressions use each of the numbers 2, 5, 6, 3 exactly once with the operations given.\nPython text-image snippet: Pokemon\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\n\"No answer was generated by the model, probably because the maximum number of tokens was reached.\"\n)\n# client: Start streaming chat completions...:\n# Start reasoning:\n# In the image, we see a battle scene from a Pok√©mon game. The player's Pikachu is at full health (83/83 HP), and the opponent's Pidgey is at a lower level (level 17 compared to Pikachu's level 42). The possible actions available to the player are:\n# 1. FIGHT: This allows the player to use one of Pikachu's moves to attack Pidgey. Given that Pikachu is at a higher level and has full HP, it is likely that Pikachu would be able to defeat Pidgey easily. This is a good option because it could potentially win the battle quickly and efficiently.\n# 2. BAG: This allows the player to use an item from their bag. This could be useful if the player wants to heal Pikachu (though it's not necessary at full health) or use an item to weaken Pidgey. However, since Pikachu is at full health and Pidgey is at a lower level, this might not be necessary. It could be a good option if the player wants to use a special item, but generally, it might not be the best choice in this situation.\n# 3. POK√âMON: This allows the player to switch the current Pok√©mon to another one in their team. Since Pikachu is at full health and at a higher level than Pidgey, switching might not be necessary. It could be useful if the player wants to train a different Pok√©mon, but it might not be the most efficient choice for winning the battle quickly.\n# 4. RUN: This allows the player to flee from the battle. This could be a good option if the player wants to avoid the battle, but since Pikachu is at a clear advantage, running would not be the most efficient choice. It could be useful if the player wants to save time or if they are trying to avoid losing a Pok√©mon, but in this case, it seems unnecessary.\n# Given the circumstances, the best action seems to be to FIGHT, as Pikachu is at a clear advantage in terms of level and health. The other options are not as efficient for winning the battle quickly.In the given scenario, the most appropriate action to take is to FIGHT. Here's why:\n# 1. FIGHT: This is the best option because Pikachu is at a higher level and has full health, making it likely to defeat Pidgey quickly and efficiently. Using an attack move would be the most straightforward way to win the battle.\n# 2. BAG: While this option could be useful for healing or using special items, it is not necessary since Pikachu is already at full health. This option is less efficient for winning the battle quickly.\n# 3. POK√âMON: Switching to another Pok√©mon might be useful for training a different Pok√©mon, but it is not necessary since Pikachu is at a clear advantage. This option is not as efficient for winning the current battle.\n# 4. RUN: Fleeing from the battle could be useful if the player wants to avoid the battle, but since Pikachu is at a clear advantage, running would not be the most efficient choice. It could be useful if the player wants to save time or avoid losing a Pok√©mon, but in this case, it seems unnecessary.\n# Therefore, the best action to take in this situation is to FIGHT.\n# FIGHT\n# =============\n# Answer\n# =============\n# In the given scenario, the most appropriate action to take is to FIGHT. Here's why:\n# 1. FIGHT: This is the best option because Pikachu is at a higher level and has full health, making it likely to defeat Pidgey quickly and efficiently. Using an attack move would be the most straightforward way to win the battle.\n# 2. BAG: While this option could be useful for healing or using special items, it is not necessary since Pikachu is already at full health. This option is less efficient for winning the battle quickly.\n# 3. POK√âMON: Switching to another Pok√©mon might be useful for training a different Pok√©mon, but it is not necessary since Pikachu is at a clear advantage. This option is not as efficient for winning the current battle.\n# 4. RUN: Fleeing from the battle could be useful if the player wants to avoid the battle, but since Pikachu is at a clear advantage, running would not be the most efficient choice. It could be useful if the player wants to save time or avoid losing a Pok√©mon, but in this case, it seems unnecessary.\n# Therefore, the best action to take in this situation is to FIGHT.\n# FIGHT\nPython text-image snippet: Geo trivia\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/201806_Tianducheng_Bird-eye_View.jpg/1280px-201806_Tianducheng_Bird-eye_View.jpg\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Where has this picture been taken ?\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\n\"No answer was generated by the model, probably because the maximum number of tokens was reached.\"\n)\n# client: Start streaming chat completions...:\n# Start reasoning:\n# The image shows a replica of the Eiffel Tower, but it's not in Paris. The background includes mountains, which are not present in Paris. The surrounding architecture appears to be more modern and dense, which is also not typical of Paris. The combination of the Eiffel Tower replica and the mountainous backdrop suggests that this is likely in a city in China, as China has several replicas of the Eiffel Tower, with the most famous one being in Shanghai. However, the dense residential buildings and the specific layout suggest that this might be in another city in China, possibly Shenzhen or another major city with a similar landscape.\n# Given that the question is about identifying the location based on the visual clues, and considering the presence of the Eiffel Tower replica and the mountainous backdrop, it's likely that this is a well-known location in China.\n# The most probable answer is that this is in Shenzhen, as it has a well-known Eiffel Tower replica in a park, but to be precise, this is the Eiffel Tower replica in Shenzhen, which is known as the \"Shenzhen Park of Eiffel Tower.\"\n# However, to be more accurate, this is likely the Eiffel Tower replica in Shenzhen, as it matches the description and visual elements.The image shows a replica of the Eiffel Tower, which is not in Paris but rather in a city with a mountainous backdrop and modern, dense architecture. This combination of elements is typical of a Chinese city, and the presence of the Eiffel Tower replica suggests a location like Shenzhen, which is known for having such a replica. The dense residential buildings and the specific layout further support this identification. Therefore, the most probable location for this image is Shenzhen, China.\n# So, the answer is:\n# Shenzhen\n# =============\n# Answer\n# =============\n# The image shows a replica of the Eiffel Tower, which is not in Paris but rather in a city with a mountainous backdrop and modern, dense architecture. This combination of elements is typical of a Chinese city, and the presence of the Eiffel Tower replica suggests a location like Shenzhen, which is known for having such a replica. The dense residential buildings and the specific layout further support this identification. Therefore, the most probable location for this image is Shenzhen, China.\n# So, the answer is:\n# Shenzhen\nPython text-image snippet: Maths\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://i.ytimg.com/vi/5Y3xLHeyKZU/hqdefault.jpg\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Solve the equations. Answer in the language of the image.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\n\"No answer was generated by the model, probably because the maximum number of tokens was reached.\"\n)\n# client: Start streaming chat completions...:\n# Start reasoning:\n# Je dois r√©soudre ce syst√®me d'√©quations. Voici les √©quations :\n# 1. \\(5x + 2y = -2\\)\n# 2. \\(3x - 4y = 17\\)\n# D'abord, je pense que la m√©thode d'√©limination pourrait √™tre une bonne approche. Pour cela, je dois √©liminer une des variables. Voyons comment.\n# Je vais essayer d'√©liminer y. Pour cela, je dois que les coefficients de y soient les m√™mes (en valeur absolue) dans les deux √©quations.\n# Le coefficient de y dans la premi√®re √©quation est 2, et dans la deuxi√®me, c'est -4. Le plus petit multiple commun de 2 et 4 est 4. Donc, je vais multiplier la premi√®re √©quation par 2 pour que le coefficient de y devienne 4.\n# Faisons cela :\n# 1. \\(2 \\times (5x + 2y) = 2 \\times (-2)\\)\n#    Ce qui donne : \\(10x + 4y = -4\\)\n# Maintenant, les √©quations sont :\n# 1. \\(10x + 4y = -4\\)\n# 2. \\(3x - 4y = 17\\)\n# Maintenant, si j'additionne ces deux √©quations, les termes avec y s'annuleront.\n# Faisons l'addition :\n# \\( (10x + 4y) + (3x - 4y) = -4 + 17 \\)\n# Ce qui donne : \\(13x = 13\\)\n# Donc, \\(x = 1\\).\n# Maintenant que nous avons x, nous pouvons le substituer dans une des √©quations originales pour trouver y. Utilisons la premi√®re √©quation originale :\n# \\(5x + 2y = -2\\)\n# En substituant x = 1 :\n# \\(5(1) + 2y = -2\\)\n# Ce qui donne : \\(5 + 2y = -2\\)\n# Soustraire 5 des deux c√¥t√©s :\n# ...\n# Ce qui donne : \\(5 + 2y = -2\\)\n# Soustraire 5 des deux c√¥t√©s :\n# \\(2y = -2 - 5\\)\n# \\(2y = -7\\)\n# Diviser par 2 :\n# \\(y = -\\frac{7}{2}\\)\n# Donc, la solution est \\(x = 1\\) et \\(y = -\\frac{7}{2}\\).\n# $\\boxed{x = 1,\\ y = -\\frac{7}{2}}$\n# =============\n# Answer\n# =============\n# Pour r√©soudre le syst√®me d'√©quations donn√© :\n# 1. \\(5x + 2y = -2\\)\n# 2. \\(3x - 4y = 17\\)\n# Nous commen√ßons par utiliser la m√©thode d'√©limination pour √©liminer une des variables. Nous choisissons d'√©liminer \\(y\\) en rendant ses coefficients identiques en valeur absolue. Le coefficient de \\(y\\) dans la premi√®re √©quation est 2, et dans la deuxi√®me, c'est -4. Le plus petit multiple commun de 2 et 4 est 4. Nous multiplions donc la premi√®re √©quation par 2 pour que le coefficient de \\(y\\) devienne 4.\n# Faisons cela :\n# 1. \\(2 \\times (5x + 2y) = 2 \\times (-2)\\)\n#    Ce qui donne : \\(10x + 4y = -4\\)\n# Maintenant, les √©quations sont :\n# 1. \\(10x + 4y = -4\\)\n# 2. \\(3x - 4y = 17\\)\n# En additionnant ces deux √©quations, les termes avec \\(y\\) s'annuleront :\n# \\( (10x + 4y) + (3x - 4y) = -4 + 17 \\)\n# Ce qui donne : \\(13x = 13\\)\n# Donc, \\(x = 1\\).\n# Ensuite, nous substituons \\(x = 1\\) dans la premi√®re √©quation originale pour trouver \\(y\\) :\n# \\(5(1) + 2y = -2\\)\n# Ce qui donne : \\(5 + 2y = -2\\)\n# Soustraire 5 des deux c√¥t√©s :\n# \\(2y = -2 - 5\\)\n# \\(2y = -7\\)\n# Diviser par 2 :\n# \\(y = -\\frac{7}{2}\\)\n# Donc, la solution est \\(x = 1\\) et \\(y = -\\frac{7}{2}\\).\n# $\\boxed{x = 1,\\ y = -\\frac{7}{2}}$\nTransformers\nMake sure you install the latest Transformers version:\npip install --upgrade transformers[mistral-common]\nThis should also install mistral_common >= 1.8.5\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nNow you can use Transformers with Magistral:\nPython snippet\nfrom typing import Any\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\nfrom transformers import AutoTokenizer\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, tokenizer_type=\"mistral\")\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\nmodel_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n).eval()\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\ntokenized = tokenizer.apply_chat_template(messages, return_dict=True)\ninput_ids = torch.tensor(tokenized.input_ids, device=\"cuda\").unsqueeze(0)\nattention_mask = torch.tensor(tokenized.attention_mask, device=\"cuda\").unsqueeze(0)\npixel_values = torch.tensor(\ntokenized.pixel_values[0], dtype=torch.bfloat16, device=\"cuda\"\n).unsqueeze(0)\nimage_sizes = torch.tensor(pixel_values.shape[-2:], device=\"cuda\").unsqueeze(0)\nwith torch.inference_mode():\noutput = model.generate(\ninput_ids=input_ids,\nattention_mask=attention_mask,\npixel_values=pixel_values,\nimage_sizes=image_sizes,\n)[0]\ndecoded_output = tokenizer.decode(\noutput[\nlen(tokenized.input_ids) : (\n-1 if output[-1] == tokenizer.eos_token_id else len(output)\n)\n]\n)\nprint(decoded_output)\n# [THINK]Alright, let's analyze the image carefully. It's a scene from a Pok√©mon game. The player is controlling Pikachu, which is at level 42 with full HP (83/83). The opponent is a Pidgey at level 17. The question is asking what action the player should take in this situation.\n# First, let's list all the possible actions available. From the bottom of the screen, the options are:\n# 1. FIGHT\n# 2. BAG\n# 3. POK√âMON\n# 4. RUN\n# Now, let's consider each option:\n# 1. **FIGHT**: This means using Pikachu's moves to attack the Pidgey.\n#    - Pros: Pikachu is at a higher level (42) compared to Pidgey (17), so it has a significant advantage. Pikachu's HP is full, so it's in good condition to fight. Fighting could potentially win the battle quickly.\n#    - Cons: Even though Pikachu is stronger, there's always a risk of Pidgey landing a lucky hit or using a powerful move. However, given the level difference, this is less likely.\n# 2. **BAG**: This means using items from the bag to help in the battle.\n#    - Pros: Could use a potion to heal (though Pikachu is already at full HP), or use another item like a Pok√© Ball to try and catch Pidgey.\n#    - Cons: Using items might be less efficient than just fighting, especially since Pikachu is already at full health. Also, if the goal is to catch Pidgey, using items to weaken it first might be useful, but the immediate advantage isn't clear.\n# 3. **POK√âMON**: This means switching to another Pok√©mon from the team.\n#    - Pros: If the player has another Pok√©mon that is stronger or has moves that are super effective against Pidgey, this could be useful.\n#    - Cons: Pikachu is already at a significant level advantage and is at full health, so switching might not be necessary unless there's a strategic reason (e.g., leveling up another Pok√©mon).\n# 4. **RUN**: This means attempting to flee from the battle.\n#    - Pros: If the player wants to avoid the battle for some reason (e.g., saving time, or wanting to catch Pidgey without weakening it), running could be useful.\n#    - Cons: If the goal is to catch or defeat Pidgey, running would not help in achieving that. Also, running might not always be successful, especially in a grass encounter where the chance to flee might be lower.\n# Now, considering the context, if the goal is to defeat Pidgey, the best action would be to FIGHT since Pikachu is at a significant advantage in terms of level and health. Using the BAG could be useful if the goal is to catch Pidgey, as items like Pok√© Balls or berries could be used. Switching Pok√©mon doesn't seem necessary given Pikachu's current status. Running would only be useful if the player wants to avoid the battle, which seems unlikely if the goal is to progress in the game.\n# However, the question asks for the best action without specifying the goal. Assuming the goal is to defeat Pidgey efficiently, the best action would be to FIGHT.\n# But to list all possible actions and their pros and cons:\n# 1. **FIGHT**\n#    - Pros: High chance of winning quickly due to level and health advantage.\n#    - Cons: Small risk of Pikachu getting damaged.\n# 2. **BAG**\n#    - Pros: Could use items to catch Pidgey or support Pikachu (though not necessary for healing).\n#    - Cons: Might be less efficient if the goal is just to defeat Pidgey.\n# 3. **POK√âMON**\n#    - Pros: Could switch to another Pok√©mon for strategic reasons (e.g., type advantage, training a different Pok√©mon).\n#    - Cons: Pikachu is already at a strong position, so switching might be unnecessary unless there's another goal.\n# 4. **RUN**\n#    - Pros: Escape the battle if needed.\n#    - Cons: Does not contribute to progress if the goal is to defeat or catch Pidgey.\n# Given that, if the goal is to defeat Pidgey, the best action is to FIGHT. If the goal is to catch Pidgey, using the BAG to throw a Pok√© Ball could be the best action.[/THINK]Here are the possible actions and their explanations:\n# 1. **FIGHT**\n#    - **Pros**: Pikachu is at a much higher level (42) compared to Pidgey (17) and has full HP. This makes it very likely to win quickly.\n#    - **Cons**: There's always a small risk that Pidgey could land a lucky hit, but this is unlikely given the level difference.\n# 2. **BAG**\n#    - **Pros**: You could use items to catch Pidgey (e.g., Pok√© Ball) or support Pikachu (though Pikachu is already at full HP).\n#    - **Cons**: Using items might be less efficient if the goal is simply to defeat Pidgey, as fighting directly could be quicker.\n# 3. **POK√âMON**\n#    - **Pros**: You could switch to another Pok√©mon that might have a type advantage or that you want to train.\n#    - **Cons**: Pikachu is already in a strong position to defeat Pidgey, so switching might not be necessary unless there's another strategic reason.\n# 4. **RUN**\n#    - **Pros**: You can escape the battle if you need to, for example, if you want to preserve Pikachu's health for a tougher battle ahead.\n#    - **Cons**: Running doesn't help you progress if your goal is to defeat or catch Pidgey. Additionally, the success rate for running might be lower in a grass encounter.\n# Given these considerations, if your goal is to defeat Pidgey, the best action is likely to **FIGHT**, as Pikachu is at a significant advantage. If your goal is to catch Pidgey, using the **BAG** to throw a Pok√© Ball could be the best choice. If you're looking to train a different Pok√©mon, you might consider switching with **POK√âMON**, and if you need to preserve resources or Pikachu's health, **RUN** could be an option.",
    "cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit": "Qwen3-Next-80B-A3B-Instruct-AWQ-4bit\nMethod\nInference\nQwen3-Next-80B-A3B-Instruct\nHighlights\nModel Overview\nPerformance\nQuickstart\nDeployment\nSGLang\nvLLM\nAgentic Use\nProcessing Ultra-Long Texts\nBest Practices\nCitation\nQwen3-Next-80B-A3B-Instruct-AWQ-4bit\nMethod\nvllm-project/llm-compressor and nvidia/Llama-Nemotron-Post-Training-Dataset were used to quantize the original model. For further quantization arguments and configurations information, please visit config.json and recipe.yaml.\nInference\nPlease build vllm from source:\nVLLM_USE_PRECOMPILED=1 pip install git+https://github.com/vllm-project/vllm.git@main\nPlease load the model into vllm and sglang as float16 data type for AWQ support:\nvllm serve cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit \\\n--tensor-parallel-size 4 \\\n--max-model-len 8192 \\\n--dtype float16\nQwen3-Next-80B-A3B-Instruct\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI).\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture.\nWe call this next-generation foundation models Qwen3-Next.\nHighlights\nQwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enchancements:\nHybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\nHigh-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\nStability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\nMulti-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference.\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\nQwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\nQwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\nFor more details, please refer to our blog post Qwen3-Next.\nModel Overview\nQwen3-Next-80B-A3B-Instruct supports only instruct (non-thinking) mode and does not generate <think></think> blocks in its output.\nQwen3-Next-80B-A3B-Instruct has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining (15T tokens) & Post-training\nNumber of Parameters: 80B in total and 3B activated\nNumber of Paramaters (Non-Embedding): 79B\nNumber of Layers: 48\nHidden Dimension: 2048\nHybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> (Gated Attention -> MoE))\nGated Attention:\nNumber of Attention Heads: 16 for Q and 2 for KV\nHead Dimension: 256\nRotary Position Embedding Dimension: 64\nGated DeltaNet:\nNumber of Linear Attention Heads: 32 for V and 16 for QK\nHead Dimension: 128\nMixture of Experts:\nNumber of Experts: 512\nNumber of Activated Experts: 10\nNumber of Shared Experts: 1\nExpert Intermediate Dimension: 512\nContext Length: 262,144 natively and extensible up to 1,010,000 tokens\nPerformance\nQwen3-30B-A3B-Instruct-2507\nQwen3-32B Non-Thinking\nQwen3-235B-A22B-Instruct-2507\nQwen3-Next-80B-A3B-Instruct\nKnowledge\nMMLU-Pro\n78.4\n71.9\n83.0\n80.6\nMMLU-Redux\n89.3\n85.7\n93.1\n90.9\nGPQA\n70.4\n54.6\n77.5\n72.9\nSuperGPQA\n53.4\n43.2\n62.6\n58.8\nReasoning\nAIME25\n61.3\n20.2\n70.3\n69.5\nHMMT25\n43.0\n9.8\n55.4\n54.1\nLiveBench 20241125\n69.0\n59.8\n75.4\n75.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n43.2\n29.1\n51.8\n56.6\nMultiPL-E\n83.8\n76.9\n87.9\n87.8\nAider-Polyglot\n35.6\n40.0\n57.3\n49.8\nAlignment\nIFEval\n84.7\n83.2\n88.7\n87.6\nArena-Hard v2*\n69.0\n34.1\n79.2\n82.7\nCreative Writing v3\n86.0\n78.3\n87.5\n85.3\nWritingBench\n85.5\n75.4\n85.2\n87.3\nAgent\nBFCL-v3\n65.1\n63.0\n70.9\n70.3\nTAU1-Retail\n59.1\n40.1\n71.3\n60.9\nTAU1-Airline\n40.0\n17.0\n44.0\n44.0\nTAU2-Retail\n57.0\n48.8\n74.6\n57.3\nTAU2-Airline\n38.0\n24.0\n50.0\n45.5\nTAU2-Telecom\n12.3\n24.6\n32.5\n13.2\nMultilingualism\nMultiIF\n67.9\n70.7\n77.5\n75.8\nMMLU-ProX\n72.0\n69.3\n79.4\n76.7\nINCLUDE\n71.9\n70.9\n79.5\n78.9\nPolyMATH\n43.1\n22.5\n50.2\n45.9\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\nQuickstart\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face transformers.\npip install git+https://github.com/huggingface/transformers.git@main\nWith earlier versions, you will encounter the following error:\nKeyError: 'qwen3_next'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384,\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nMulti-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\nThe efficiency or throughput improvement depends highly on the implementation.\nIt is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\nDepending on the inference settings, you may observe better efficiency with flash-linear-attention and causal-conv1d.\nSee the above links for detailed instructions and requirements.\nDeployment\nFor deployment, you can use the latest sglang or vllm to create an OpenAI-compatible API endpoint.\nSGLang\nSGLang is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service.\nSGLang has supported Qwen3-Next in its main branch, which can be installed from source:\npip install 'sglang[all] @ git+https://github.com/sgl-project/sglang.git@main#subdirectory=python'\nThe following command can be used to create an API endpoint at http://localhost:30000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\nThe following command is recommended for MTP with the rest settings the same as above:\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\nThe environment variable SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 is required at the moment.\nThe default context length is 256K. Consider reducing the context length to a smaller value, e.g., 32768, if the server fail to start.\nvLLM\nvLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service.\nvLLM has supported Qwen3-Next in its main branch, which can be installed from source:\npip install git+https://github.com/vllm-project/vllm.git\nThe following command can be used to create an API endpoint at http://localhost:8000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\nThe following command is recommended for MTP with the rest settings the same as above:\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\nThe environment variable VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 is required at the moment.\nThe default context length is 256K. Consider reducing the context length to a smaller value, e.g., 32768, if the server fail to start.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-Next-80B-A3B-Instruct',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nQwen3-Next natively supports context lengths of up to 262,144 tokens.\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively.\nWe have validated the model's performance on context lengths of up to 1 million tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers, vllm and sglang.\nIn general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 262144\n}\n}\nPassing command line arguments:\nFor vllm, you can use\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000\nFor sglang, you can use\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set factor as 2.0.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-30B-A3B-Instruct-2507\n86.8\n98.0\n96.7\n96.9\n97.2\n93.4\n91.0\n89.1\n89.8\n82.5\n83.6\n78.4\n79.7\n77.6\n75.7\n72.8\nQwen3-235B-A22B-Instruct-2507\n92.5\n98.5\n97.6\n96.9\n97.3\n95.8\n94.9\n93.9\n94.5\n91.0\n92.2\n90.9\n87.8\n84.8\n86.5\n84.5\nQwen3-Next-80B-A3B-Instruct\n91.8\n98.5\n99.0\n98.0\n98.7\n97.6\n95.0\n96.0\n94.0\n93.5\n91.7\n86.9\n85.5\n81.7\n80.3\n80.3\nQwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{qwen2.5-1m,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "Kwaipilot/KAT-Dev": "News\nHighlights\nIntroduction\nQuickstart\nClaude Code\nvllm server\nCONNECT US\nNews\nüî• We‚Äôre thrilled to announce the release of KAT-Dev-72B-Exp, our latest and most powerful model yet!\nüî• You can now try our strongest proprietary coder model KAT-Coder directly on the StreamLake platform for free.\nHighlights\nKAT-Dev-32B  is an open-source 32B-parameter model for software engineering tasks.\nOn SWE-Bench Verified, KAT-Dev-32B achieves comparable performance with 62.4% resolved and ranks 5th among all open-source models with different scales.\nIntroduction\nKAT-Dev-32B is optimized via several stages of training, including a mid-training stage, supervised fine-tuning (SFT) & reinforcement fine-tuning (RFT) stage and an large-scale agentic reinforcement learning (RL) stage. In summary, our contributions include:\nStage\nKey Techniques\n1. Mid-Training\nWe observe that adding extensive training for tool-use capability, multi-turn interaction, and instruction-following at this stage may not yield large performance gains in the current results (e.g., on leaderboards like SWE-bench). However, since our experiments are based on the Qwen3-32B model, we find that enhancing these foundational capabilities will have a significant impact on the subsequent SFT and RL stages. This suggests that improving such core abilities can profoundly influence the model‚Äôs capacity to handle more complex tasks.\n2. SFT & RFT\nWe meticulously curated eight task types and eight programming scenarios during the SFT stage to ensure the model‚Äôs generalization and comprehensive capabilities. Moreover, before RL, we innovatively introduced an RFT stage. Compared with traditional RL, we incorporate ‚Äúteacher trajectories‚Äù annotated by human engineers as guidance during training‚Äîmuch like a learner driver being assisted by an experienced co-driver before officially driving after getting a license. This step not only boosts model performance but also further stabilizes the subsequent RL training.\n3. Agentic RL Scaling\nScaling agentic RL hinges on three challenges: efficient learning over nonlinear trajectory histories, leveraging intrinsic model signals, and building scalable high-throughput infrastructure. We address these with a multi-level prefix caching mechanism in the RL training engine, an entropy-based trajectory pruning technique, and an inner implementation of SeamlessFlow[1] architecture that cleanly decouples agents from training while exploiting heterogeneous compute. These innovations together cut scaling costs and enable efficient large-scale RL.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog.\nQuickstart\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Kwaipilot/KAT-Dev\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nClaude Code\nvllm server\nMODEL_PATH=\"Kwaipilot/KAT-Dev\"\nvllm serve $MODEL_PATH \\\n--enable-prefix-caching \\\n--tensor-parallel-size 8 \\\n--tool-parser-plugin $MODEL_PATH/qwen3coder_tool_parser.py \\\n--chat-template $MODEL_PATH/chat_template.jinja \\\n--enable-auto-tool-choice --tool-call-parser qwen3_coder\nclaude-code-router is a third-party routing utility that allows Claude Code to flexibly switch between different backend APIs.On the dashScope platform, you can install the claude-code-config extension package, which automatically generates a default configuration for claude-code-router with built-in dashScope support.\nOnce the configuration files and plugin directory are generated, the environment required by ccr will be ready.If needed, you can still manually edit ~/.claude-code-router/config.json and the files under ~/.claude-code-router/plugins/ to customize the setup.\nFinally, simply start ccr to run Claude Code and seamlessly connect it with the powerful coding capabilities of KAT-Dev-32B.Happy coding!\nCONNECT US\nHere‚Äôs the QR code for our WeChat group ‚Äî feel free to join and chat with us!",
    "openbmb/VoxCPM-0.5B": "üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\nOverview\nüöÄ Key Features\nQuick Start\nüîß Install from PyPI\n1.  Model Download (Optional)\n2. Basic Usage\n3. CLI Usage\n4. Start web demo\nüë©‚Äçüç≥ A Voice Chef's Guide\nü•ö Step 1: Prepare Your Base Ingredients (Content)\nüç≥ Step 2: Choose Your Flavor Profile (Voice Style)\nüßÇ Step 3: The Final Seasoning (Fine-Tuning Your Results)\nüìä Performance Highlights\nSeed-TTS-eval Benchmark\nCV3-eval Benchmark\n‚ö†Ô∏è Risks and limitations\nüìÑ License\nüéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\nOverview\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on MiniCPM-4 backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\nüöÄ Key Features\nContext-Aware, Expressive Speech Generation - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\nTrue-to-Life Voice Cloning - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\nHigh-Efficiency Synthesis - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\nQuick Start\nüîß Install from PyPI\npip install voxcpm\n1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\nDownload VoxCPM-0.5Bfrom huggingface_hub import snapshot_download\nsnapshot_download(\"openbmb/VoxCPM-0.5B\",local_files_only=local_files_only)\nDownload ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.from modelscope import snapshot_download\nsnapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')\nsnapshot_download('iic/SenseVoiceSmall')\n2. Basic Usage\nimport soundfile as sf\nfrom voxcpm import VoxCPM\nmodel = VoxCPM.from_pretrained(\"openbmb/VoxCPM-0.5B\")\nwav = model.generate(\ntext=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\nprompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\nprompt_text=None,          # optional: reference text\ncfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\ninference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\nnormalize=True,           # enable external TN tool\ndenoise=True,             # enable external Denoise tool\nretry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\nretry_badcase_max_times=3,  # maximum retrying times\nretry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")\n3. CLI Usage\nAfter installation, the entry point is voxcpm (or use python -m voxcpm.cli).\n# 1) Direct synthesis (single text)\nvoxcpm --text \"Hello VoxCPM\" --output out.wav\n# 2) Voice cloning (reference audio + transcript)\nvoxcpm --text \"Hello\" \\\n--prompt-audio path/to/voice.wav \\\n--prompt-text \"reference transcript\" \\\n--output out.wav \\\n--denoise\n# 3) Batch processing (one text per line)\nvoxcpm --input examples/input.txt --output-dir outs\n# (optional) Batch + cloning\nvoxcpm --input examples/input.txt --output-dir outs \\\n--prompt-audio path/to/voice.wav \\\n--prompt-text \"reference transcript\" \\\n--denoise\n# 4) Inference parameters (quality/speed)\nvoxcpm --text \"...\" --output out.wav \\\n--cfg-value 2.0 --inference-timesteps 10 --normalize\n# 5) Model loading\n# Prefer local path\nvoxcpm --text \"...\" --output out.wav --model-path /path/to/VoxCPM_model_dir\n# Or from Hugging Face (auto download/cache)\nvoxcpm --text \"...\" --output out.wav \\\n--hf-model-id openbmb/VoxCPM-0.5B --cache-dir ~/.cache/huggingface --local-files-only\n# 6) Denoiser control\nvoxcpm --text \"...\" --output out.wav \\\n--no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base\n# 7) Help\nvoxcpm --help\npython -m voxcpm.cli --help\n4. Start web demo\nYou can start the UI interface by running python app.py, which allows you to perform Voice Cloning and Voice Creation.\nüë©‚Äçüç≥ A Voice Chef's Guide\nWelcome to the VoxCPM kitchen! Follow this recipe to cook up perfect generated speech. Let‚Äôs begin.\nü•ö Step 1: Prepare Your Base Ingredients (Content)\nFirst, choose how you‚Äôd like to input your text:.\nRegular Text (Classic Mode)\n‚úÖ Keep \"Text Normalization\" ON. Type naturally (e.g., \"Hello, world! 123\"). The system will automatically process numbers, abbreviations, and punctuation using WeTextProcessing library.\nPhoneme Input (Native Mode)\n‚ùå Turn \"Text Normalization\" OFF. Enter phoneme text like {HH AH0 L OW1} (EN) or {ni3}{hao3} (ZH) for precise pronunciation  control. In this mode, VoxCPM also supports native understanding of other complex non-normalized text‚Äîtry it out!\nüç≥ Step 2: Choose Your Flavor Profile (Voice Style)\nThis is the secret sauce that gives your audio its unique sound.\nCooking with a Prompt Speech (Following a Famous Recipe)\nA prompt speech provides the desired acoustic characteristics for VoxCPM. The speaker's timbre, speaking style, and even the background sounds and ambiance will be replicated.\nFor a Clean, Studio-Quality Voice:\n‚úÖ Enable \"Prompt Speech Enhancement\". This acts like a noise filter, removing background hiss and rumble to give you a pure, clean voice clone.\nCooking au Naturel (Letting the Model Improvise)\nIf no reference is provided, VoxCPM becomes a creative chef! It will infer a fitting speaking style based on the text itself, thanks to the text-smartness of its foundation model, MiniCPM-4.\nPro Tip: Challenge VoxCPM with any text‚Äîpoetry, song lyrics, dramatic monologues‚Äîit may deliver some interesting results!\nüßÇ Step 3: The Final Seasoning (Fine-Tuning Your Results)\nYou're ready to serve! But for master chefs who want to tweak the flavor, here are two key spices.\nCFG Value (How Closely to Follow the Recipe)\nDefault: A great starting point.\nVoice sounds strained or weird? Lower this value. It tells the model to be more relaxed and improvisational, great for expressive prompts.\nNeed maximum clarity and adherence to the text? Raise it slightly to keep the model on a tighter leash.\nInference Timesteps (Simmering Time: Quality vs. Speed)\nNeed a quick snack? Use a lower number. Perfect for fast drafts and experiments.\nCooking a gourmet meal? Use a higher number. This lets the model \"simmer\" longer, refining the audio for superior detail and naturalness.\nHappy creating! üéâ Start with the default settings and tweak from there to suit your project. The kitchen is yours!\nüìä Performance Highlights\nVoxCPM achieves competitive results on public zero-shot TTS benchmarks:\nSeed-TTS-eval Benchmark\nModel\nParameters\nOpen-Source\ntest-EN\ntest-ZH\ntest-Hard\nWER/%‚¨á\nSIM/%‚¨Ü\nCER/%‚¨á\nSIM/%‚¨Ü\nCER/%‚¨á\nSIM/%‚¨Ü\nMegaTTS3\n0.5B\n‚ùå\n2.79\n77.1\n1.52\n79.0\n-\n-\nDiTAR\n0.6B\n‚ùå\n1.69\n73.5\n1.02\n75.3\n-\n-\nCosyVoice3\n0.5B\n‚ùå\n2.02\n71.8\n1.16\n78.0\n6.08\n75.8\nCosyVoice3\n1.5B\n‚ùå\n2.22\n72.0\n1.12\n78.1\n5.83\n75.8\nSeed-TTS\n-\n‚ùå\n2.25\n76.2\n1.12\n79.6\n7.59\n77.6\nMiniMax-Speech\n-\n‚ùå\n1.65\n69.2\n0.83\n78.3\n-\n-\nCosyVoice\n0.3B\n‚úÖ\n4.29\n60.9\n3.63\n72.3\n11.75\n70.9\nCosyVoice2\n0.5B\n‚úÖ\n3.09\n65.9\n1.38\n75.7\n6.83\n72.4\nF5-TTS\n0.3B\n‚úÖ\n2.00\n67.0\n1.53\n76.0\n8.67\n71.3\nSparkTTS\n0.5B\n‚úÖ\n3.14\n57.3\n1.54\n66.0\n-\n-\nFireRedTTS\n0.5B\n‚úÖ\n3.82\n46.0\n1.51\n63.5\n17.45\n62.1\nFireRedTTS-2\n1.5B\n‚úÖ\n1.95\n66.5\n1.14\n73.6\n-\n-\nQwen2.5-Omni\n7B\n‚úÖ\n2.72\n63.2\n1.70\n75.2\n7.97\n74.7\nOpenAudio-s1-mini\n0.5B\n‚úÖ\n1.94\n55.0\n1.18\n68.5\n-\n-\nIndexTTS2\n1.5B\n‚úÖ\n2.23\n70.6\n1.03\n76.5\n-\n-\nVibeVoice\n1.5B\n‚úÖ\n3.04\n68.9\n1.16\n74.4\n-\n-\nHiggsAudio-v2\n3B\n‚úÖ\n2.44\n67.7\n1.50\n74.0\n-\n-\nVoxCPM\n0.5B\n‚úÖ\n1.85\n72.9\n0.93\n77.2\n8.87\n73.0\nCV3-eval Benchmark\nModel\nzh\nen\nhard-zh\nhard-en\nCER/%‚¨á\nWER/%‚¨á\nCER/%‚¨á\nSIM/%‚¨Ü\nDNSMOS‚¨Ü\nWER/%‚¨á\nSIM/%‚¨Ü\nDNSMOS‚¨Ü\nF5-TTS\n5.47\n8.90\n-\n-\n-\n-\n-\n-\nSparkTTS\n5.15\n11.0\n-\n-\n-\n-\n-\n-\nGPT-SoVits\n7.34\n12.5\n-\n-\n-\n-\n-\n-\nCosyVoice2\n4.08\n6.32\n12.58\n72.6\n3.81\n11.96\n66.7\n3.95\nOpenAudio-s1-mini\n4.00\n5.54\n18.1\n58.2\n3.77\n12.4\n55.7\n3.89\nIndexTTS2\n3.58\n4.45\n12.8\n74.6\n3.65\n-\n-\n-\nHiggsAudio-v2\n9.54\n7.89\n41.0\n60.2\n3.39\n10.3\n61.8\n3.68\nCosyVoice3-0.5B\n3.89\n5.24\n14.15\n78.6\n3.75\n9.04\n75.9\n3.92\nCosyVoice3-1.5B\n3.91\n4.99\n9.77\n78.5\n3.79\n10.55\n76.1\n3.95\nVoxCPM\n3.40\n4.04\n12.9\n66.1\n3.59\n7.89\n64.3\n3.74\n‚ö†Ô∏è Risks and limitations\nGeneral Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.\nPotential for Misuse of Voice Cloning: VoxCPM's powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.\nCurrent Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.\nBilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.\nThis model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.\nüìÑ License\nThe VoxCPM model weights and code are open-sourced under the Apache-2.0 license.",
    "inclusionAI/Ling-flash-2.0": "Introduction\nPowerful Complex Reasoning Abilities\nEfficient Architecture, High-Speed Inference\nModel Downloads\nQuickstart\nü§ó Hugging Face Transformers\nü§ñ ModelScope\nDeployment\nvLLM\nEnvironment Preparation\nOffline Inference:\nOnline Inference:\nSGLang\nEnvironment Preparation\nRun Inference\nFinetuning\nLicense\nü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope\nIntroduction\nToday, Ling-flash-2.0 is officially open-sourced! üöÄ\nFollowing the release of the language model Ling-mini-2.0 and the thinking model Ring-mini-2.0, we are now open-sourcing the third MoE LLM under the Ling 2.0 architecture: Ling-flash-2.0, a language model with 100B total parameters and 6.1B activated parameters (4.8B non-embedding).\nTrained on 20T+ tokens of high-quality data, together with supervised fine-tuning and multi-stage reinforcement learning, Ling-flash-2.0 achieves SOTA performance among dense models under 40B parameters, despite activating only ~6B parameters. Compared to MoE models with larger activation/total parameters, it also demonstrates strong competitiveness. Notably, it delivers outstanding performance in complex reasoning, code generation, and frontend development.\nPowerful Complex Reasoning Abilities\nWe conducted a comprehensive evaluation of Ling-flash-2.0‚Äôs reasoning capabilities, reporting strong results on representative benchmarks:\nMulti-disciplinary knowledge reasoning: GPQA-Diamond, MMLU-Pro\nAdvanced mathematical reasoning: AIME 2025, Omni-MATH, OptMATH (advanced mathematical optimization tasks)\nChallenging code generation: LiveCodeBench v6, CodeForces-Elo\nLogical reasoning: KOR-Bench, ARC-Prize\nKey regulated industries (Finance, Healthcare): FinanceReasoning, HealthBench\nCompared with dense models under 40B (e.g., Qwen3-32B-Non-Thinking, Seed-OSS-36B-Instruct (think budget=0)) and larger-activation/total-parameter MoE models (e.g., Hunyuan-A13B-Instruct, GPT-OSS-120B/low), Ling-flash-2.0 demonstrates stronger complex reasoning power. Moreover, it shows high competitiveness on creative tasks (Creative Writing v3).\nEfficient Architecture, High-Speed Inference\nGuided by Ling Scaling Laws, Ling 2.0 adopts a 1/32 activation-ratio MoE architecture, optimized across multiple design choices: expert granularity, shared-expert ratio, attention balance, aux-loss-free + sigmoid routing strategy, MTP layers, QK-Norm, Partial-RoPE, and more. These refinements enable small-activation MoE models to achieve 7√ó efficiency gains over equivalent dense architectures.\nIn other words, with just 6.1B activated parameters (4.8B non-embedding), Ling-flash-2.0 can match the performance of ~40B dense models. Thanks to its small activation size, it also delivers major inference speed advantages:\nOn H20 hardware, Ling-flash-2.0 achieves 200+ tokens/s, offering 3√ó speedups compared to 36B dense models in everyday use.\nWith YaRN extrapolation, it supports 128K context length, and as output length grows, its relative speedup can reach 7√ó or more.\nModel Downloads\nYou can download the following table to see the various stage of Ling-flash-2.0 models. If you are located in mainland China, we also provide the model on ModelScope.cn to speed up the download process.\nModel\nContext Length\nDownload\nLing-flash-base-2.0\n32K -> 128K (YaRN)\nü§ó HuggingFace ü§ñ ModelScope\nLing-flash-2.0\n32K -> 128K (YaRN)\nü§ó HuggingFace ü§ñ ModelScope\nNote: If you are interested in previous version, please visit the past model collections in Huggingface or ModelScope.\nQuickstart\nü§ó Hugging Face Transformers\nHere is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ling-flash-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", return_token_type_ids=False).to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nü§ñ ModelScope\nIf you're in mainland China, we strongly recommend you to use our model from ü§ñ ModelScope.\nDeployment\nvLLM\nvLLM supports offline batched inference or launching an OpenAI-Compatible API Service for online inference.\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below:\ngit clone -b v0.10.0 https://github.com/vllm-project/vllm.git\ncd vllm\nwget https://raw.githubusercontent.com/inclusionAI/Ling-V2/refs/heads/main/inference/vllm/bailing_moe_v2.patch\ngit apply bailing_moe_v2.patch\npip install -e .\nOffline Inference:\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ling-flash-2.0\")\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=16384)\nllm = LLM(model=\"inclusionAI/Ling-flash-2.0\", dtype='bfloat16')\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nOnline Inference:\nvllm serve inclusionAI/Ling-flash-2.0 \\\n--tensor-parallel-size 2 \\\n--pipeline-parallel-size 1 \\\n--use-v2-block-manager \\\n--gpu-memory-utilization 0.90\nTo handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the model's config.json file, for example:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nUse an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service.\nFor detailed guidance, please refer to the vLLM instructions.\nSGLang\nEnvironment Preparation\nWe will later submit our model to SGLang official release, now we can prepare the environment following steps:\npip3 install sglang==0.5.2rc0 sgl-kernel==0.3.7.post1\nYou can use docker image as well:\ndocker pull lmsysorg/sglang:v0.5.2rc0-cu126\nThen you should apply patch to sglang installation:\n# patch command is needed, run `yum install -y patch` if needed\npatch -d `python -c 'import sglang;import os; print(os.path.dirname(sglang.__file__))'` -p3 < inference/sglang/bailing_moe_v2.patch\nRun Inference\nBF16 and FP8 models are supported by SGLang now, it depends on the dtype of the model in ${MODEL_PATH}. They both share the same command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path $MODLE_PATH \\\n--host 0.0.0.0 --port $PORT \\\n--trust-remote-code \\\n--attention-backend fa3\nMTP is supported for base model, and not yet for chat model. You can add parameter --speculative-algorithm NEXTN\nto start command.\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\nMore usage can be found here\nFinetuning\nWe recommend you to use Llama-Factory to finetune Ling.\nLicense\nThis code repository is licensed under the MIT License.",
    "inclusionAI/Ring-flash-2.0": "Ring-flash-2.0\nIntroduction\nleading-level performance in complex reasoning\nEfficient Architecture, High-Speed Inference\nIcePop: Cooling Down Training-Inference Gaps in RL for MoE Models\nSFT + RLVR + RLHF Multi-Stage Training\nQuickstart\nüöÄ Try Online\nüîå API Usage\nü§ó Hugging Face Transformers\nü§ñ ModelScope\nDeployment\nvLLM\nSGLang\nFinetuning\nLicense\nCitation\nRing-flash-2.0\nThis model is presented in the paper Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model.\nThe official code repository is available at: https://github.com/inclusionAI/Ring-V2.\nü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬†üöÄ Experience Now\nIntroduction\nToday, we are officially open-sourcing Ring-flash-2.0.\nThis is a high-performance thinking model, deeply optimized based on Ling-flash-2.0-base. Like Ling-flash-2.0, Ring-flash-2.0 has a total of 100B parameters, with only 6.1B activated per inference. Our independently developed icepop algorithm has successfully addressed the challenge of training instability in reinforcement learning (RL) for MoE LLMs after cold-start Long-CoT SFT, enabling the model‚Äôs complex reasoning capabilities to continuously improve throughout extended RL training cycles.\nRing-flash-2.0 demonstrates significant breakthroughs across multiple challenging benchmarks, including math competitions, code generation, and logical reasoning. Its performance not only surpasses that of SOTA dense models under 40B parameters but also rivals larger open-weight MoE models and closed-source high-performance thinking model APIs.\nleading-level performance in complex reasoning\nWe selected representative open-source thinking models and closed-source APIs for comparison, including GPT-OSS-120B(medium), Qwen3-32B-Thinking, Seed-OSS-36B-Instruct, and Gemini-2.5-Flash.\nThe benchmarking results demonstrate that Ring-flash-2.0 exhibits leading performance across multiple challenging general reasoning tasks, including:\nMath competitions (AIME 25, Omni-MATH),\nCode generation (LiveCodeBench, CodeForce-Elo),\nLogical reasoning (ARC-Prize).\nIt also shows strong competitiveness in specialized domains such as:\nScientific and medical reasoning (GPQA-Diamond, HealthBench).\nMore surprisingly, although Ring-flash-2.0 is primarily designed for complex reasoning, it outperforms all other compared models in creative writing (Creative Writing v3) and matches the creative capability of its \"twin brother\"‚Äîthe non-thinking model Ling-flash-2.0.\nEfficient Architecture, High-Speed Inference\nBuilding on the highly efficient MoE architecture of the Ling 2.0 series, and through structural optimizations such as a __1/32 expert activation ratio__ and __MTP layers__, Ring-flash-2.0 activates only 6.1B (4.8B non-embedding) parameters while delivering performance comparable to a ‚àº40B dense model.\nThanks to its low activation and high sparsity design, Ring-flash-2.0 achieves a high generation speed of __200+ tokens/sec__ when deployed on just four H20 GPUs, significantly reducing inference costs for thinking models in high-concurrency scenarios.\nIcePop: Cooling Down Training-Inference Gaps in RL for MoE Models\nDuring the RL for MoE models, the discrepancy of precision between the training and inference engines is more pronounced compared to dense models. This gap widens progressively as sequence length and training steps increase‚Äîparticularly during long-sequence generation and extended training cycles. A more critical issue is that the original GRPO algorithm begins to break down within a limited number of training steps. Specifically, the probabilistic discrepancy for the same token between training and inference phases gradually increases. When this relative difference exceeds 5%, training effectively fails, posing a significant challenge for long-horizon reinforcement learning with lengthy sequences.\nTo address this issue, we introduced a key solution: distribution calibration via masked bidirectional truncation, which effectively narrows the gap between training and inference.\nBidirectional Truncation: We truncate not only tokens where the training probability is significantly higher than the inference probability but also the reverse scenario where the training probability is much lower.\nMasking: Tokens with excessively large discrepancies are excluded from gradient computation.\nFor detailed algorithm introduction, please refer to our technical blog: https://ringtech.notion.site/icepop\nSFT + RLVR + RLHF Multi-Stage Training\nTo comprehensively enhance the capabilities of Ring-flash-2.0, we designed a Two-staged RL pipeline. First, lightweight Long-CoT SFT equips the Ling-flash-2.0-base model with diverse thinking patterns. This is followed by RL training with Verifiable Rewards (RLVR) to continually stimulate the model‚Äôs reasoning potential. Finally, an RLHF phase is incorporated to improve the model‚Äôs general abilities.\nDuring RL training, we compared directly combining RLVR and RLHF into joint training with the ultimately adopted Two-staged RL pipeline. Both approaches showed relatively similar effectiveness in our experiments. However, due to the differing difficulty levels of RLVR and RLHF tasks‚Äîwith RLHF involving relatively shorter model rollouts‚Äîjoint training resulted in more long-tail generations. From an engineering efficiency perspective, we ultimately adopted the Two-staged RL approach.\nQuickstart\nüöÄ Try Online\nYou can experience Ring-flash-2.0 online at: ZenMux\nüîå API Usage\nYou can also use Ring-flash-2.0 through API calls:\nfrom openai import OpenAI\n# 1. Initialize the OpenAI client\nclient = OpenAI(\n# 2. Point the base URL to the ZenMux endpoint\nbase_url=\"https://zenmux.ai/api/v1\",\n# 3. Replace with the API Key from your ZenMux user console\napi_key=\"<your ZENMUX_API_KEY>\",\n)\n# 4. Make a request\ncompletion = client.chat.completions.create(\n# 5. Specify the model to use in the format \"provider/model-name\"\nmodel=\"inclusionai/ring-flash-2.0\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What is the meaning of life?\"\n}\n]\n)\nprint(completion.choices[0].message.content)\nü§ó Hugging Face Transformers\nHere is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-flash-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", return_token_type_ids=False).to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nü§ñ ModelScope\nIf you're in mainland China, we strongly recommend you to use our model from ü§ñ ModelScope.\nDeployment\nvLLM\nvLLM supports offline batched inference or launching an OpenAI-Compatible API Service for online inference.\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below:\ngit clone -b v0.10.0 https://github.com/vllm-project/vllm.git\ncd vllm\nwget https://raw.githubusercontent.com/inclusionAI/Ling-V2/refs/heads/main/inference/vllm/bailing_moe_v2.patch\ngit apply bailing_moe_v2.patch\npip install -e .\nOffline Inference:\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ring-flash-2.0\")\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=16384)\nllm = LLM(model=\"inclusionAI/Ring-flash-2.0\", dtype='bfloat16')\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nOnline Inference:\nvllm serve inclusionAI/Ring-flash-2.0 \\\n--tensor-parallel-size 2 \\\n--pipeline-parallel-size 1 \\\n--use-v2-block-manager \\\n--gpu-memory-utilization 0.90\nTo handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the model's config.json file, for example:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nUse an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service.\nFor detailed guidance, please refer to the vLLM instructions.\nSGLang\nEnvironment Preparation\nWe will later submit our model to SGLang official release, now we can prepare the environment following steps:\npip3 install sglang==0.5.2rc0 sgl-kernel==0.3.7.post1\nYou can use docker image as well:\ndocker pull lmsysorg/sglang:v0.5.2rc0-cu126\nThen you should apply patch to sglang installation:\n# patch command is needed, run `yum install -y patch` if needed\npatch -d `python -c 'import sglang;import os; print(os.path.dirname(sglang.__file__))'` -p3 < inference/sglang/bailing_moe_v2.patch\nRun Inference\nBF16 and FP8 models are supported by SGLang now, it depends on the dtype of the model in ${MODEL_PATH}. They both share the same command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path $MODLE_PATH \\\n--host 0.0.0.0 --port $PORT \\\n--trust-remote-code \\\n--attention-backend fa3\nMTP is supported for base model, and not yet for chat model. You can add parameter --speculative-algorithm NEXTN\nto start command.\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\nMore usage can be found here\nFinetuning\nWe recommend you to use Llama-Factory to finetune Ring.\nLicense\nThis code repository is licensed under the MIT License.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{lingteam2025everystep,\ntitle={Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model},\nauthor={Ling Team and Anqi Shen and Baihui Li and Bin Hu and Bin Jing and Cai Chen and Chao Huang and Chao Zhang and Chaokun Yang and Cheng Lin and Chengyao Wen and Congqi Li and Deng Zhao and Dingbo Yuan and Donghai You and Fagui Mao and Fanzhuang Meng and Feng Xu and Guojie Li and Guowei Wang and Hao Dai and Haonan Zheng and Hong Liu and Jia Guo and Jiaming Liu and Jian Liu and Jianhao Fu and Jiannan Shi and Jianwen Wang and Jianxin Lai and Jin Yang and Jun Mei and Jun Zhou and Junbo Zhao and Junping Zhao and Kuan Xu and Le Su and Lei Chen and Li Tang and Liang Jiang and Liangcheng Fu and Lianhao Xu and Linfeng Shi and Lisha Liao and Longfei Zheng and Meng Li and Mingchun Chen and Qi Zuo and Qiang Cheng and Qianggang Cao and Qitao Shi and Quanrui Guo and Senlin Zhu and Shaofei Wang and Shaomian Zheng and Shuaicheng Li and Shuwei Gu and Chen, Siba and Wu, Tao and Zhang, Tao and Zhang, Tianyu and Zhou, Tianyu and Bie, Tiwei and Yang, Tongkai and Hong, Wang and Ren, Wang and Chen, Weihua and Yu, Wenbo and Zheng, Wengang and Wang, Xiangchun and Yan, Xiaodong and Wan, Xiaopei and Zhao, Xin and Kong, Xinyu and Tang, Xinyu and Han, Xudong and Wang, Xudong and Yang, Xuemin and Hu, Xueyu and Zhang, Yalin and Sun, Yan and Shan, Yicheng and Wang, Yilong and Xu, Yingying and Liu, Yongkang and Guo, Yongzhen and Wang, Yuanyuan and Yan, Yuchen and Wang, Yuefan and Guo, Yuhong and Li, Zehuan and Xu, Zhankai and Li, Zhe and Zhang, Zhenduo and Gui, Zhengke and Pan, Zhenxuan and Huang, Zhenyu and Lan, Zhenzhong and Ding, Zhiqiang and Zhang, Zhiqiang and Li, Zhixun and Liu, Zhizhen and Wang, Zihao and Wen, Zujie},\nyear={2025},\neprint={2510.18855},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.18855},\n}",
    "eddy1111111/WAN22.XX_Palingenesis": "if you use odev2 please üëá\nif you still use ode/stable you can üëá\nodev2 beta test:https://github.com/eddyhhlure1Eddy/ode-ComfyUI-WanVideoWrapper?tab=readme-ov-file\nodev2 md :https://github.com/eddyhhlure1Eddy/ode-ComfyUI-WanVideoWrapper?tab=readme-ov-file",
    "inclusionAI/Ring-1T-preview": "Ring-1T-preview, Deep Thinking, No Waiting\nIMO Cases\nQuickstart\nü§ó Hugging Face Transformers\nü§ñ ModelScope\nLicense\nTip\nReference\nü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope\nRing-1T-preview, Deep Thinking, No Waiting\nRecently, we have been fully occupied with the post-training of Ling 2.0's 1T foundational language model, striving to maximize the natural language reasoning potential of this trillion-scale base model.\nConducting post-training on such a huge model, particularly the \"training\" involved in large-scale reinforcement learning, stands as one of the most technically challenging tasks the Ling Team has encountered since its establishment.\nOn the other hand, it has also been a process that continuously reshapes our technical understanding and reinforces the belief that \"scaling is all you need\"\nIn the early stages of large-scale reinforcement learning training, Ring-1T, the thinking version of the 1T foundational language model, has already demonstrated powerful natural language reasoning capabilities.\nIn AIME 2025 (American Invitational Mathematics Examination), the model achieved a high score of 92.6 through pure natural language reasoning, further approaching GPT-5 with thinking (no tools)'s score of 94.6.\nAdditionally, the model has shown strong competitiveness in the Harvard-MIT Mathematics Tournament (HMMT) 2025, competition-level code generation tasks such as LiveCodeBench v6 and CodeForces, as well as the abstraction and reasoning benchmark ARC-AGI-1 task.\nTo further explore the reasoning limits of the early version of Ring-1T, we integrated it into the multi-agent framework AWorld (https://github.com/inclusionAI/AWorld\n) and conducted pure natural language reasoning tests on the IMO 2025 (International Mathematical Olympiad, 6 problems in total).\nPreviously, we tested Ring-flash-2.0, using the same method: under the setting of three allowed reasoning attempts, Ring-flash-2.0 only managed to solve Problem 3 on the third try.\nIn contrast, during this test, Ring-1T solved Problem 3 in just one attempt, and also produced partially correct answers on Problems 1, 2, 4, and 5 in a single try.\nThis demonstrates advanced reasoning capabilities essential for top-tier math competitions‚Äîsuch as insight, constructive problem solving, counterexample generation, strategic thinking, and rigorous logical-chain reasoning‚Äîhighlighting the stronger reasoning potential of large-scale thinking models.\nIMO Cases\nTo facilitate early community exploration of the reasoning capabilities of the trillion-parameter thinking model Ring-1T, we have decided to open-source its preview version, Ring-1T-preview, ahead of schedule.\nThis model retains the efficient MoE architecture of Ling 2.0, completed pre-training on 20T tokens of corpora, and underwent RLVR training tailored for reasoning abilities within our self-developed efficient reinforcement learning system ASystem (the AReaL framework of which has been open-sourced), leveraging the previously disclosed \"icepop\" method(https://ringtech.notion.site/icepop).\nRing-1T remains under continuous training.\nWhile the preview version already demonstrates powerful natural language reasoning capabilities, it still exhibits issues such as language mixing, repetitive reasoning and identity misperception.\nWe look forward to community exploration and feedback to collectively accelerate the iterative refinement of this trillion-parameter foundation.\nQuickstart\nü§ó Hugging Face Transformers\nHere is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-1T-preview\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", return_token_type_ids=False).to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nü§ñ ModelScope\nIf you're in mainland China, we strongly recommend you to use our model from ü§ñ ModelScope.\nLicense\nThis code repository is licensed under the MIT License.\nTip\nTo facilitate academic research and downstream applications with customizable model naming, we did not conduct specific identity recognition training.\nReference\n@article{ling2025everystep,\ntitle={Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model},\nauthor={Team, Ling and Shen, Anqi and Li, Baihui and Hu, Bin and Jing, Bin and Chen, Cai and Huang, Chao and Zhang, Chao and Yang, Chaokun and Lin, Cheng and Wen, Chengyao and Li, Congqi and Zhao, Deng and Yuan, Dingbo and You, Donghai and Mao, Fagui and Meng, Fanzhuang and Xu, Feng and Li, Guojie and Wang, Guowei and Dai, Hao and Zheng, Haonan and others},\njournal={arXiv preprint arXiv:2510.18855},\nyear={2025}\n}",
    "mlx-community/GLM-4.6-4bit": "mlx-community/GLM-4.6-4bit\nUse with mlx\nmlx-community/GLM-4.6-4bit\nThis model mlx-community/GLM-4.6-4bit was\nconverted to MLX format from zai-org/GLM-4.6\nusing mlx-lm version 0.28.1.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/GLM-4.6-4bit\")\nprompt = \"hello\"\nif tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)"
}