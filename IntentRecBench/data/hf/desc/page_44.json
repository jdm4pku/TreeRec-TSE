{
    "Thrillcrazyer/Qwen-1.5B_THIP": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Qwen-1.5B_THIP\nQuick start\nTraining procedure\nFramework versions\nCitations\nModel Card for Qwen-1.5B_THIP\nThis model is a fine-tuned version of Thrillcrazyer/Qwen-1.5B_THIP.\nIt has been trained using TRL.\nQuick start\nfrom transformers import pipeline\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\ngenerator = pipeline(\"text-generation\", model=\"Thrillcrazyer/Qwen-1.5B_THIP\", device=\"cuda\")\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\nprint(output[\"generated_text\"])\nTraining procedure\nThis model was trained with GRPO, a method introduced in DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.\nFramework versions\nTRL: 0.24.0\nTransformers: 4.57.1\nPytorch: 2.8.0+cu128\nDatasets: 4.3.0\nTokenizers: 0.22.1\nCitations\nCite GRPO as:\n@article{shao2024deepseekmath,\ntitle        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},\nauthor       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},\nyear         = 2024,\neprint       = {arXiv:2402.03300},\n}\nCite TRL as:\n@misc{vonwerra2022trl,\ntitle        = {{TRL: Transformer Reinforcement Learning}},\nauthor       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\\'e}dec},\nyear         = 2020,\njournal      = {GitHub repository},\npublisher    = {GitHub},\nhowpublished = {\\url{https://github.com/huggingface/trl}}\n}",
    "DragonLLM/Dragon-3B-Base-alpha": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nHighlights\nModel Overview\nModel Benchmark\nLimitations\nQuickstart\nSetup\nHighlights\nDragon LLM introduces its new LLM Architecture. Built on a new hybrid GDN -Transformer that outperforms traditional architectures, it can power frugal, sovereign models that can be rapidly specialized on business data and use cases.\nDragon Architecture features :\nVery strong ability to remember past words in the sequence compared to other Hybrid approach, inspired by Hymba (NVIDIA)\nAbility to be used simultaneously by more users on equivalent hardware and better throughput on long-context scenario\nExtremely efficient learning\nIt has been been validated at large scale by the training of a 3B model on 3.5T tokens. It achieves comparable performance against smolLM-3B-Base and Qwen3-4B-Base on ARC, HellaSwag, LAMBADA, and PIQA, while trained on 3-5 time less data.\nWhy is this important?\nProves performance ‚Üí same performance with 3‚Äì5√ó less data.\nCut cost : more users can be served on the same hardware\nAbility to deploy in secure environment with constraint on the hardware (even on CPU)\nScales better : higher throughput and strong long-context handling (Long documents, files, codes or contracts).\nHow has Dragon LLM achieved this?\n‚Ä¢ By combining the best recent research papers on LLM architectures, cumulating gains across all processes, from deep layer optimization to attention head or kv cache management.\n‚Ä¢ Agile Team able to adapt quickly and test new ideas extremely fast\n‚Ä¢ Compute support by the EU Commission (euroHPC - JUPITER and Leonardo HPC)\nWhat's next?\nThe next step is to deliver foundation models for this architecture :\n‚Ä¢ a 3B and 7B version of DragonBase trained on 10T+ tokens\n‚Ä¢ Chat version of these models\n‚Ä¢ Specialized versions for specific industry vertical such as Finance\nIf you want to know more and get updates on the project, follow us !\nIf you would like a comprehensive deep dive on the architecture : read our blog post\nModel Overview\nModel Benchmark\nBenchmarks\nDragon\nQwen3-4B\nSmolLM3\nARC Challenge\n50%\n51.28%\n52.56%\nARC Easy\n76.01%\n75.97%\n76.81%\nHellaSwag\n71.73%\n54.46%\n75.2%\nLAMBADA\n65.03%\n62.62%\n65.05%\nPIQA\n79.11%\n77.86%\n78.84%\nSWDE\n89.92%\n91.99%\n88.03%\nFDA\n81.13%\n86.75%\n76.13%\nAverage\n73.27%\n71.56%\n73.23%\nAll evaluations are performed using with lm-eval and few shot set to 0.\nLimitations\nThis model is a foundation model, trained on large-scale general-purpose text corpora. It has not been fine-tuned for any specific downstream task. As such:\nIt may produce inaccurate or misleading information, particularly for factual or time-sensitive queries.\nIt has no understanding of truth or intent and may generate biased, toxic, or harmful content inherited from its training data.\nIt is not suitable for direct use in safety-critical or decision-making contexts (e.g., healthcare, finance, law) without additional alignment or validation.\nThe model does not perform well on tasks requiring domain-specific expertise, numerical precision, or structured reasoning unless further fine-tuned.\nLong or complex prompts may lead to loss of coherence or hallucinations as context length grows.\nFine-tuning, prompt-engineering, or evaluation on downstream tasks is recommended before any production use.\nQuickstart\nTry it with:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"DragonLLM/Dragon-3B-Base-alpha\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\nprompt = \"Once upon a time, a valiant knight named Segurant set out on a quest to chase a dragon. He was\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**inputs,\nmax_new_tokens=512,\n)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\nSetup\nFor better performance on GPU, we recommend using :\nflash-linear-attention: the Gated DeltaNet Triton kernelsInstall with pip install flash-linear-attention\nIf you use NVIDIA GPU, you can improve performance with :\nflash-attention:Install with pip install flash-attn --no-build-isolation\ncausal-conv1d: a short convolution is used as part of the Gated DeltaNet layerInstall with pip install causal-conv1d\n(optional, recommended only for A100) flex-head-ha: computing attention with different head dimensions for qk and vo, used for differential attentionInstall with pip install flex-head-fa --no-build-isolation",
    "utter-project/TowerVideo-9B": "Model Card for TowerVideo\nAvailable Models\nHow to Use TowerVision\nQuick Start with Transformers\nModel Details\nTraining Data\nDataset Statistics\nDataset Composition Overview\nEvaluation\nMultiple Purpose Multimodal Benchmarks\nMultimodal Multilingual Translation Tasks\nSupported Languages Performance\nCitation\nModel Card Contact\nAcknowledgments\nModel Card for TowerVideo\nTowerVision is a family of open-source multilingual vision-language models with strong capabilities optimized for a variety of vision-language use cases, including image captioning, visual understanding, summarization, question answering, and more. TowerVision excels particularly in multimodal multilingual translation benchmarks and culturally-aware tasks, demonstrating exceptional performance across 20 languages and dialects.\nThis model card covers the TowerVision family, including the 2B and 9B parameter versions, both in their instruct-tuned (it) and pretrained (pt) variants, with the latter not undergoing instruction tuning.\nModel Family: TowerVision (2B, 9B variants)\nContext length: 8192 tokens\nLanguages: 20+ languages including European, Asian, and other language families\nüåü Try TowerVision: Project Page | Code Repository\nAvailable Models\nModel\nParameters\nHF Link\nTowerVideo-2B\n2B\nü§ó utter-project/TowerVision-2B\nTowerVideo-9B\n9B\nü§ó utter-project/TowerVision-9B\nHow to Use TowerVision\nQuick Start with Transformers\nClick to expand/collapse code\nmport torch\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n# Load the model in half-precision\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\n\"utter-project/TowerVideo-2B\",\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\n\"utter-project/TowerVideo-2B\"\n)\n# Use your local video\nvideo_path = \"your_video_path.mp4\"\n# Conversation using the same template\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"path\": video_path},\n{\"type\": \"text\", \"text\": \"\\n<video>\\nIWhat is the video about?\"},\n],\n},\n]\n# Apply the chat template\ninputs = processor.apply_chat_template(\nconversation,\nnum_frames=8,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nadd_special_tokens=True,  # ensures <video> token is inserted\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\n# Generate response\nout = model.generate(**inputs, max_new_tokens=60)\n# Decode output\ndecoded = processor.batch_decode(\nout,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=True\n)\nprint(decoded)\nModel Details\nInput: Model accepts input text, images and video.\nOutput: Model generates text in multiple languages.\nModel Architecture: TowerVideo uses a multilingual image-language model based on Tower-Plus (2B and 9B parameters), paired with SigLIP2-patch14-384 vision encoder through a multimodal adapter for vision-language understanding.\nRecommended Precision: We recommend using bfloat16 precision for optimal performance and memory efficiency when running TowerVision models.\nLanguages Covered: The model has been trained on 20 languages and dialects:\nEuropean languages: English, German, Dutch, Spanish, French, Portuguese, Italian, Polish, Czech, Romanian, Norwegian (Bokm√•l & Nynorsk)\nAsian languages: Chinese (Simplified & Traditional), Japanese, Korean, Hindi\nOther languages: Russian, Ukrainian\nKey Strengths:\nüèÜ Exceptional performance on culturally-aware benchmarks with deep understanding of cultural contexts and visual nuances\nüìä Strong cross-lingual transfer capabilities across diverse vision-language tasks\nTraining Data\nTowerVision models are trained on a video/text subset of VisionBlocks, a comprehensive multilingual vision-language dataset comprising 6.31M samples across diverse categories:\nDataset\nSamples\nHF Link\nVisionBlocks\n6.31M\nü§ó utter-project/VisionBlocks\nComing Soon\nDataset Statistics\nTotal samples: 6.31M\nCreated by our team: 1.21M samples (~19%)\nHuman-collected/external: 5.10M samples (~81%)\nDataset Composition Overview\nVisionBlocks contains samples across multiple categories with both English-only (63.1%) and multilingual (36.9%) data:\nChart/Plot Reasoning: DVQA, ChartQA, PlotQA, TabMWP (~405K samples)\nGeneral VQA: VQAv2, RLAIF-4V (~488K samples)\nDocument VQA: DocVQA, TextVQA, ST-VQA, PixMo-Docs (~46K samples)\nReasoning/Knowledge: A-OKVQA, OKVQA, AI2D, ScienceQA (~29K samples)\nMultilingual/Cultural: Pangea-Cultural, Pangea-Multi, PixMo-Cap-Translated, CulturalGround datasets (~1.6M samples)\nSpecialized VQA: IconQA, InfographicVQA, Stratos (~34K samples)\nCounting/Math: TallyQA, PixMo-Count (~107K samples)\nVision/Text: VBlocks-PixMo collections, EuroBlocks-SFT (~2.2M samples)\nVideo/Text: LLaVA-Video collections (~1.4M samples)\nCollection Types: Human-annotated, synthetically generated, and professionally translated data ensuring high quality and cultural diversity across 20+ languages.\nEvaluation\nAll evaluations were conducted using lmms_eval.\nMultiple Purpose Multimodal Benchmarks\nTowerVision demonstrates strong performance across diverse multimodal evaluation benchmarks:\nMultimodal Multilingual Translation Tasks\nTowerVision excels particularly in multimodal multilingual translation benchmarks, demonstrating state-of-the-art cross-lingual visual communication capabilities:\nSupported Languages Performance\n‚úÖ Fully Supported: English, German, Dutch, Spanish, French, Portuguese, Italian, Polish, Czech, Romanian, Norwegian, Chinese, Japanese, Korean, Hindi, Russian, Ukrainian\nüìä Benchmark Coverage: Our models are evaluated across diverse multilingual vision-language tasks, demonstrating strong cross-lingual transfer capabilities and exceptional performance in culturally-aware benchmarks.\nCitation\nIf you find TowerVideo useful in your research, please consider citing the following paper:\n@article{towervision2025,\ntitle={Understanding and Improving Multilinguality in Vision-Language Models},\nauthor={[Authors to be added]},\njournal={[Journal to be added]},\nyear={2025},\nnote={Paper in preparation}\n}\nModel Card Contact\nFor errors or additional questions about details in this model card, contact the research team.\nAcknowledgments\nTowerVision builds upon the excellent work of:\nLLaVA-NeXT for the foundational vision-language architecture\nTowerVision-9B vision-language model with multilingual capabilities\nSigLIP2 for robust vision encoding\nThe broader multilingual NLP and multimodal communities",
    "mintujohnson/Llama-3.2-3B-French-Instruct": "Uploaded finetuned  model\nInference\nUploaded finetuned  model\nDeveloped by: mintujohnson\nLicense: apache-2.0\nFinetuned from model : unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.\nInference\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nmodel_path = \"mintujohnson/Llama-3.2-3B-French-Instruct\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(model_name = model_path, max_seq_length = 128,\ndtype = None, load_in_4bit = True)\ndef inference(messages, model, tokenizer):\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer.apply_chat_template(\nmessages, tokenize = True,\nadd_generation_prompt = True, # Must add for generation\nreturn_tensors = \"pt\",\n).to(\"cuda\")\nprint(tokenizer.decode(inputs[0], skip_special_tokens=False))\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(\ninput_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\nuse_cache = True, temperature = 1.5, min_p = 0.1)\nmessages = [\n{\"role\": \"user\", \"content\": \"o√π est la Normandie?\"},\n]\noutput = inference(messages, model, tokenizer)",
    "unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit": "Qwen3-VL-8B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nCitation\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nQwen3-VL-8B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-8B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-8B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-8B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen/Qwen3-VL-8B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "mlx-community/Qwen3-VL-4B-Instruct-8bit": "mlx-community/Qwen3-VL-4B-Instruct-8bit\nUse with mlx\nmlx-community/Qwen3-VL-4B-Instruct-8bit\nThis model was converted to MLX format from Qwen/Qwen3-VL-4B-Instruct using mlx-vlm version 0.3.4.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/Qwen3-VL-4B-Instruct-8bit --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>",
    "yunpeng1998/PE-Field": "No model card",
    "credshields/Solidity-CodeGen-v0.1": "Solidity-CodeGen-v0.1\nWhy Solidity-CodeGen-v0.1?\nComplete Security Workflow\nHow to Load\nRecommended: Use OpenZeppelin MCP (Locally)\nPrompting Guide (No MCP)\nQuality and Limitations\nFiles Included\nLicense\nNote\nSolidity-CodeGen-v0.1\nSolidity-CodeGen-v0.1 is a fine-tuned LLM specialized for generating Solidity smart contracts with modern OpenZeppelin patterns. It works best when paired with the OpenZeppelin Contracts MCP (Model Context Protocol) to produce canonical ERC and Governor implementations.\nBase: Qwen3 (tuned for Solidity codegen)\nContext length: Served via vLLM (up to 32k with server)\nRecommended: Use with OpenZeppelin MCP locally for highest-quality results\nWhy Solidity-CodeGen-v0.1?\nTraditional LLMs often produce generic code that may contain security vulnerabilities or deviate from best practices. Solidity-CodeGen-v0.1 is different‚Äîit's a domain-expert model trained specifically for smart contract development. Instead of generating random code snippets, it creates structured templates using OpenZeppelin-compliant logic and predictable design patterns when paired with OpenZeppelin MCP.\nEvery generated contract follows OZ v5 conventions, ensuring:\nCanonical ERC implementations (ERC20, ERC721, ERC1155)\nConsistent inheritance patterns\nReproducible function behavior\nBuilt-in security considerations\nThe model is inspired by SolidityScan's Web3HackHub‚Äîa comprehensive database tracking blockchain security incidents since 2011‚Äîand OWASP Smart Contract Top 10.The OWASP Smart Contract Security Project plays a critical role in shaping secure development practices across the Web3 ecosystem. It serves as a foundational framework that educates developers on the most prevalent and high-impact vulnerabilities affecting smart contracts.The model adheres to OpenZeppelin-compliant patterns and aims to align with OWASP best practices.\nComplete Security Workflow\nWhile Solidity-CodeGen-v0.1 generates secure code foundations, comprehensive security requires additional validation. SolidityScan provides the perfect complement‚Äîan AI-powered smart contract scanner that identifies vulnerabilities and security risks in your code.\nTogether, they create a complete security pipeline:\nGenerate contract templates with Solidity-CodeGen-v0.1\nScan for vulnerabilities with SolidityScan's advanced analysis\nDeploy with confidence, knowing your contracts meet industry standards\nExplore SolidityScan to discover how AI-powered scanning can enhance your smart contract security workflow.\nHow to Load\nTransformers:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntok = AutoTokenizer.from_pretrained(\"credshields/Solidity-CodeGen-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"credshields/Solidity-CodeGen-v0.1\", torch_dtype=\"auto\", device_map=\"auto\")\nvLLM:\nvllm serve YOUR_ORG/Solidity-CodeGen-v0.1 --dtype auto --port 8005\nRecommended: Use OpenZeppelin MCP (Locally)\nFor best results, run the official OpenZeppelin Contracts MCP locally and route ERC20/721/1155 and Governor requests through it.\nGitHub: https://github.com/OpenZeppelin/openzeppelin-mcp\nNPM: https://www.npmjs.com/package/@openzeppelin/contracts-mcp\nThis model card does not include MCP files; use the official OZ MCP project locally.\nPrompting Guide (No MCP)\nWhen MCP is unavailable, use structured prompts:\nStart with:\n- ‚ÄúGenerate only Solidity code in a single \nSolidity: ^0.8.27, OpenZeppelin v5 imports only.\nContract: ERC20 token\nName: TestToken\nSymbol: TEST\nFeatures: Ownable, mint(address,uint256) onlyOwner\nConstraints:\nImports: ERC20, Ownable\nconstructor(address initialOwner) and Ownable(initialOwner)\nNo burn, no pause, no permit\nQuality and Limitations\nWith MCP: ERC20/721/1155 and Governor scaffolds are canonical; custom templates are solid but require review.\nWithout MCP: Basic patterns work with structured prompts; complex protocols require expert review.\nAlways test and audit generated code before production.\nFiles Included\nModel weights and tokenizer\nOptional requirements files for Transformers/vLLM\nNo MCP/server code included; use official OZ MCP locally.\nLicense\nModel weights: see LICENSE.\nGenerated code: user responsibility; OpenZeppelin Contracts are MIT-licensed.\nNote\nNote: This preview focuses on tailored scaffolds for common patterns and may omit end‚Äëto‚Äëend features; treat outputs as starting points to be completed, validated, and audited and hence Solidity-CodeGen-v0.1 is not a substitute for human review, formal verification, or security audits.",
    "prithivMLmods/Qwen3-VL-4B-Instruct-abliterated": "Qwen3-VL-4B-Instruct-abliterated\nKey Highlights\nQuick Start with Transformers\nIntended Use\nLimitations\nQwen3-VL-4B-Instruct-abliterated\nQwen3-VL-4B-Instruct-abliterated is an abliterated (v1.0) variant of Qwen3-VL-4B-Instruct, tailored for Abliterated Reasoning and Captioning. This model is designed to generate detailed and descriptive captions, as well as reasoning outputs, across a wide range of visual and multimodal contexts‚Äîincluding complex, sensitive, or nuanced content‚Äîwhile supporting diverse aspect ratios and resolutions.\nKey Highlights\nAbliterated / Uncensored Captioning: Fine-tuned to bypass conventional content filters while preserving factual, descriptive, and reasoning-rich outputs.\nHigh-Fidelity Descriptions: Generates comprehensive captions and reasoning for general, artistic, technical, abstract, or low-context images.\nRobust Across Aspect Ratios: Supports wide, tall, square, and irregular image dimensions with consistent accuracy.\nVariational Detail Control: Produces outputs ranging from high-level summaries to fine-grained, intricate descriptions and reasoning.\nFoundation on Qwen3-VL-4B Architecture: Leverages Qwen3-VL-4B‚Äôs multimodal reasoning and instruction-following capabilities.\nMultilingual Output Capability: Primarily English, with adaptability for multilingual prompts via prompt engineering.\nQuick Start with Transformers\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Qwen3-VL-4B-Instruct-abliterated\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen3-VL-4B-Instruct-abliterated\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Provide a detailed caption and reasoning for this image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nIntended Use\nThis model is suited for:\nGenerating detailed, uncensored captions and reasoning for general-purpose or artistic datasets.\nResearch in content moderation, red-teaming, and generative safety evaluation.\nEnabling descriptive captioning and reasoning for visual datasets typically excluded from mainstream models.\nCreative applications such as storytelling, art generation, or multimodal reasoning tasks.\nCaptioning and reasoning for non-standard aspect ratios and stylized visual content.\nLimitations\nMay produce explicit, sensitive, or offensive descriptions depending on image content and prompts.\nNot recommended for production systems requiring strict content moderation.\nOutput style, tone, and reasoning can vary depending on input phrasing.\nAccuracy may vary for unfamiliar, synthetic, or highly abstract visual content.",
    "WoW-world-model/WoW-1-Wan-14B-600k": "YAML Metadata\nWarning:\nThe pipeline tag \"video-generation\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, audio-text-to-text, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-ranking, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, visual-document-retrieval, any-to-any, video-to-video, other\nü§ñ WoW-1-Wan-14B-2M\nüß† Key Features\nüß™ Training Data\nüß† Mixture Caption Strategy\nüîÑ Continuous Updates\nüß© Applications\nüìÑ Citation\nüîó Resources\nü§ñ WoW-1-Wan-14B-2M\nWoW-1-Wan-14B is a 14-billion-parameter generative world model trained on 2 million real-world robot interaction trajectories. It is designed to imagine, reason, and act in physically consistent environments, powered by SOPHIA-guided refinement and a co-trained Inverse Dynamics Model.\nThis model is part of the WoW (World-Omniscient World Model) project, introduced in the paper:\nWoW: Towards a World omniscient World model Through Embodied InteractionChi et al., 2025 ‚Äì arXiv:2509.22642\nüß† Key Features\n14B parameters trained on 2M robot interaction samples\nLearns causal physical reasoning from embodied action\nGenerates physically consistent video and robotic action plans\nUses SOPHIA, a vision-language critic, to refine outputs\nPaired with an Inverse Dynamics Model to complete imagination-to-action loop\nüß™ Training Data\n2M Real-world robot interaction trajectories\nMultimodal scenes including vision, action, and language\nDiverse mixture captions for better generalization\nüß† Mixture Caption Strategy\nPrompt Lengths:\nShort: \"The Franka robot, grasp the red bottle on the table\"\nLong: \"The scene... open the drawer, take the screwdriver, place it on the table...\"\nRobot Model Mixing:\nCaptions reference various robot types\nExample: \"grasp with the Franka Panda arm\", \"use end-effector to align\"\nAction Granularity:\nCoarse: \"move to object\"\nFine: \"rotate wrist 30¬∞ before grasping\"\nüîÑ Continuous Updates\nThis dataset will be continuously updated with:\nMore trajectories\nRicher language\nFiner multimodal annotations\nüß© Applications\nZero-shot video generation in robotics\nCausal reasoning and physics simulation\nLong-horizon manipulation planning\nForward and inverse control prediction\nüìÑ Citation\n@article{chi2025wow,\ntitle={WoW: Towards a World omniscient World model Through Embodied Interaction},\nauthor={Chi, Xiaowei and Jia, Peidong and Fan, Chun-Kai and Ju, Xiaozhu and Mi, Weishi and Qin, Zhiyuan and Zhang, Kevin and Tian, Wanxin and Ge, Kuangzhi and Li, Hao and others},\njournal={arXiv preprint arXiv:2509.22642},\nyear={2025}\n}\nüîó Resources\nüß† Project page: wow-world-model.github.io\nüíª GitHub repo: wow-world-model/wow-world-model\nüìä Dataset: WoW-1 Benchmark Samples",
    "Arm/stt_en_conformer_executorch_small": "ExecuTorch Conformer\nModel Details\nModel Description\nModel Sources\nUses\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nExecuTorch Conformer\nConformer is a popular Transformer based speech recognition network, suitable for low-cost embedded devices. This repository contains example FP32 trained weights and the associated tokenizer for an implementation of Conformer. We also include exported quantized program with ExecuTorch, quantized for the ExecuTorch Ethos-U backend allowing an easy deployment on SoCs with an Arm¬Æ Ethos‚Ñ¢-U NPU.\nModel Details\nModel Description\nConformer is a popular Neural Network for speech recognition. This repository contains trained weights for the Conformer implementation in https://github.com/sooftware/conformer/\nDeveloped by: Arm\nModel type: Transformer\nLanguage(s) (NLP): English\nLicense: BigScience OpenRAIL-M v1.1\nThe model contains 10M parameters. For a SoC with Cortex-M and Ethos-U85 in Shared_Sram memory mode,the memory usage is 5.7MB of SRAM\nto store the peak intermediate tensor and 10.8MB of read-only data living in the external memory for the weights and biases.\nModel Sources\nRepository: https://github.com/sooftware/conformer/\nPaper [optional]: https://arxiv.org/abs/2005.08100\nUses\nYou need to install ExecuTorch 1.0 with $ pip install executorch.\nBy downloading the quantized exported graph module, you can directly call the to_edge_transform_and_lower API of ExecuTorch.\nThe to_edge_transform_and_lower API will convert the quantized exported program to backend-specific command stream for the Ethos-U.\nThe end result a pte file for your variant of the Ethos-U.\nBelow is an example script to produce a pte file for Ethos-U85 256 MAC configuration in Shared_Sram memory mode.\nimport torch\nfrom executorch.backends.arm.ethosu import EthosUPartitioner, EthosUCompileSpec\nfrom executorch.backends.arm.quantizer import (\nEthosUQuantizer,\nget_symmetric_quantization_config,\n)\nfrom executorch.exir import (\nEdgeCompileConfig,\nExecutorchBackendConfig,\nto_edge_transform_and_lower,\n)\nfrom executorch.extension.export_util.utils import save_pte_program\ndef main():\nquant_exported_program = torch.export.load(\"Conformer_ArmQuantizer_quant_exported_program.pt2\")\ncompile_spec = EthosUCompileSpec(\ntarget=\"ethos-u85-256\",\nsystem_config=\"Ethos_U85_SYS_Flash_High\",\nmemory_mode=\"Shared_Sram\",\nextra_flags=[\"--output-format=raw\", \"--debug-force-regor\"],\n)\npartitioner = EthosUPartitioner(compile_spec)\nprint(\n\"Calling to_edge_transform_and_lower - lowering to TOSA and compiling for the Ethos-U hardware\"\n)\n# Lower the exported program to the Ethos-U backend\nedge_program_manager = to_edge_transform_and_lower(\nquant_exported_program,\npartitioner=[partitioner],\ncompile_config=EdgeCompileConfig(\n_check_ir_validity=False,\n),\n)\nexecutorch_program_manager = edge_program_manager.to_executorch(\nconfig=ExecutorchBackendConfig(extract_delegate_segments=False)\n)\nsave_pte_program(\nexecutorch_program_manager, f\"conformer_quantized.pte\"\n)\nif __name__ == \"__main__\":\nmain()\nHow to Get Started with the Model\nTo you can download directly the quantized exported program for the Ethos-U backend(Conformer_ArmQuantizer_quant_exported_program.pt2) and call the to_edge_transform_and_lower ExecuTorch API.\nThis means you don't need to train the model from scratch and you don't need to load & pre-process representative dataset for calibration. You can focus on developing the application running on device.\nFor an example end-to-end speech-to-text application running on an embedded platform, have a look at https://gitlab.arm.com/artificial-intelligence/ethos-u/ml-embedded-evaluation-kit/-/blob/experimental/executorch/docs/use_cases/asr.md\nTraining Details\nTraining Data\nWe used the LibriSpeech 960h dataset. The dataset is composed of 460h of clean audio data and 500h of more noisy data. We obtain the dataset as part of the PyTorch torchaudio library.\nTraining Procedure\nIf you want to train the Conformer model from scratch, you can do so by following the instructions in https://github.com/Arm-Examples/ML-examples/tree/main/pytorch-conformer-train-quantize/training\nWe used an AWS g5.24xlarge instance to train the NN.\nPreprocessing\nWe first train a tokenizer on the Librispeech dataset. The tokenizer converts labels into tokens. For example, in English, it is very common to have 's at the end of words, the tokenizer will identify that patten and have a dedicated token for the 's combination.\nThe code to obtain the tokenizer is available in https://github.com/Arm-Examples/ML-examples/blob/main/pytorch-conformer-train-quantize/training/build_sp_128_librispeech.py . The trained tokenizer is also available in the Hugging Face repository.\nWe also apply a MelSpectrogram on the input audio as per the Conformer paper - the LibriSpeech dataset contains audio recordings sampled at 16kHz. The Conformer\npaper recommends 25ms window length, corresponding to 400 samples(160000.025=400) and a stride of 10ms, corresponding to 160 samples(160000.01). We use 80 filter banks as\nrecommended by the paper and 512 FFTs.\nTraining Hyperparameters\nTraining regime: The model is trained in FP32\nEpochs: 160\nBatch size: 96\nLearning rate: 0.0005\nWeight decay: 1e-6\nWarmup-epochs: 2.0\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nWe test the model on the LibriSpeech test-clean dataset and obtain 7% Word Error Rate. The accuracy of the model may be improved through training with additional datasets, and through data augmentation techniques such as time slicing.",
    "gneeraj/deeppass2-bert": "DeepPass2-XLM-RoBERTa Fine-tuned for Secret Detection\nModel Description\nModel Architecture\nBase Model\nLoRA Configuration\nIntended Use\nPrimary Use Case\nInput\nOutput\nTraining Data\nDataset Composition\nData Generation Process\nTraining Procedure\nHardware\nHyperparameters\nTraining Process\nPerformance Metrics\nChunk-Level Metrics\nPassword-Level Metrics\nDefinitions\nLimitations and Biases\nKnown Limitations\nPotential Biases\nEthical Considerations\nResponsible Use\nMisuse Potential\nUsage\nInstallation\nQuick Start\nIntegration with DeepPass2\nCitation\nAdditional Information\nModel Versions\nRelated Links\nContact\nDeepPass2-XLM-RoBERTa Fine-tuned for Secret Detection\nModel Description\nDeepPass2 is a fine-tuned version of xlm-roberta-base specifically designed for detecting passwords and secrets in documents through token classification. Unlike traditional regex-based approaches, this model understands context to identify both structured tokens (API keys, JWTs) and free-form passwords.\nDeveloped by: Neeraj Gupta (SpecterOps)Model type: Token Classification (Sequence Labeling)Base model: xlm-roberta-baseLanguage(s): EnglishLicense: [Same as base model]Fine-tuned with: LoRA (Low-Rank Adaptation) through Unsloth\nBlog post: What's Your Secret?: Secret Scanning by DeepPass2\nModel Architecture\nBase Model\nArchitecture: XLM-RoBERTa-base (Cross-lingual RoBERTa)\nParameters: ~278M (base model)\nMax sequence length: 512 tokens\nHidden size: 768\nNumber of layers: 12\nNumber of attention heads: 12\nLoRA Configuration\nLoraConfig(\ntask_type=TaskType.TOKEN_CLS,\nr=64,                    # Rank\nlora_alpha=128,          # Scaling parameter\nlora_dropout=0.05,       # Dropout probability\nbias=\"none\",\ntarget_modules=[\"query\", \"key\", \"value\", \"dense\"]\n)\nIntended Use\nThis model is the BERT based model used in the DeepPass2 blog.\nPrimary Use Case\nSecret Detection: Identify passwords, API keys, tokens, and other sensitive credentials in documents\nSecurity Auditing: Scan documents for potential credential leaks\nData Loss Prevention: Pre-screen documents before sharing or publishing\nInput\nText documents of any length (automatically chunked into 300-400 token segments) for DeepPass2 complete tool\nText string of 512 tokens for particular instance of input to the Model\nOutput\nToken-level binary classification:\n0: Non-credential token\n1: Credential/password token\nTraining Data\nDataset Composition\nTotal examples: 23,000 (20,800 training, 2,200 testing)\nDocument types: Synthetic Emails, technical documents, logs, configuration files\nPassword sources:\nReal breached passwords from CrackStation's \"real human\" dump\nSynthetic passwords generated by LLMs\nStructured tokens (API keys, JWTs, etc.)\nData Generation Process\nBase Documents: 2,000 long documents (2000+ tokens each) generated using LLMs\n50% containing passwords, 50% without\nChunking: Documents split into 300-400 token chunks with random boundaries\nPassword Injection: Real passwords inserted using skeleton sentences:\"Your account has been created with username: {user} and password: {pass}\"\nClass Balance: <0.3% of tokens are passwords (maintaining real-world distribution)\nTraining Procedure\nHardware\nTrained on MacBook Pro (64GB RAM) with MPS acceleration\nCan be trained on systems with 8-16GB RAM\nHyperparameters\nEpochs: 4\nBatch size: 8 (per device)\nWeight decay: 0.01\nOptimizer: AdamW (default in Trainer)\nLearning rate: Default (5e-5)\nMax sequence length: 512 tokens\nRandom seed: 2\nTraining Process\n# Preprocessing\n- Tokenization with offset mapping\n- Label generation based on credential spans\n- Padding to max_length with truncation\n# Fine-tuning\n- LoRA adapters applied to attention layers\n- Binary cross-entropy loss\n- Token-level classification head\nPerformance Metrics\nChunk-Level Metrics\nMetric\nScore\nStrict Accuracy\n86.67%\nOverlap Accuracy\n97.72%\nPassword-Level Metrics\nMetric\nCount/Rate\nTrue Positives\n1,201\nTrue Negatives\n1,112\nFalse Positives\n49 (3.9%)\nFalse Negatives\n138\nOverlap True Positives\n456\nRecall\n89.7%\nDefinitions\nStrict Accuracy: All passwords in chunk detected with 100% accuracy\nOverlap Accuracy: At least one password detected with >30% overlap with ground truth\nLimitations and Biases\nKnown Limitations\nContext window: Limited to 512 tokens per chunk\nTraining data: Primarily trained on LLM-generated documents which may not fully represent real-world documents\nPassword types: Better at detecting structured/complex passwords than simple dictionary words\nTokenization boundaries: SentencePiece tokenization can fragment passwords, affecting boundary detection\nPotential Biases\nMay over-detect in technical documentation due to training distribution\nTends to flag alphanumeric strings more readily than common words used as passwords\nEthical Considerations\nResponsible Use\nPrivacy: This model should only be used on documents you have permission to scan\nSecurity: Detected credentials should be handled securely and not logged or stored insecurely\nFalse Positives: Always verify detected credentials before taking action\nMisuse Potential\nShould not be used to scan documents without authorization\nNot intended for credential harvesting or malicious purposes\nUsage\nInstallation\npip install transformers torch\nQuick Start\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n# Load model and tokenizer\nmodel_name = \"path/to/deeppass2-xlm-roberta\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\n# Classify tokens\ndef detect_passwords(text):\ninputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\nwith torch.no_grad():\noutputs = model(**inputs)\npredictions = torch.argmax(outputs.logits, dim=-1)\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n# Extract password tokens\npassword_tokens = [\ntoken for token, label in zip(tokens, predictions[0])\nif label == 1\n]\nreturn password_tokens\nIntegration with DeepPass2\nFor production use, integrate with the full DeepPass2 pipeline:\nNoseyParker regex filtering\nBERT token classification (this model)\nLLM validation for false positive reduction\nSee the DeepPass2 repository for complete implementation.\nCitation\n@software{gupta2025deeppass2,\nauthor = {Gupta, Neeraj},\ntitle = {DeepPass2: Fine-tuned XLM-RoBERTa for Secret Detection},\nyear = {2025},\norganization = {SpecterOps},\nurl = {https://huggingface.co/deeppass2-bert},\nnote = {Blog: \\url{https://specterops.io/blog/2025/07/31/whats-your-secret-secret-scanning-by-deeppass2/}}\n}\nAdditional Information\nModel Versions\nv6.0-BERT: Current production version with LoRA adapters\nmerged-model: LoRA weights merged with base model for easier deployment\nRelated Links\nDeepPass2 Blog Post\nOriginal DeepPass (2022)\nNoseyParker\nContact\nFor questions or issues, please open an issue on the GitHub repository",
    "jaysowen/Wan2.2-Loras": "README.md exists but content is empty.",
    "PaddlePaddle/PP-DocLayoutV2": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nIntroduction\nModel Architecture\nUsage\nInstall Dependencies\nBasic Usage\nCitation\nIntroduction\nPP-DocLayoutV2 is a dedicated lightweight model for layout analysis, focusing specifically on element detection, classification, and reading order\nprediction.\nModel Architecture\nPP-DocLayoutV2 is composed of two sequentially connected networks. The first is an RT-DETR-based detection model that performs layout element detection and classification. The detected bounding boxes and class labels are then passed to a subsequent pointer network, which is responsible for ordering these layout elements.\nUsage\nInstall Dependencies\nInstall PaddlePaddle and PaddleOCR:\npython -m pip install paddlepaddle-gpu==3.2.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\npython -m pip install -U \"paddleocr[doc-parser]\"\npython -m pip install https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl\nFor Windows users, please use WSL or a Docker container.\nBasic Usage\nPython API usage:\nfrom paddleocr import LayoutDetection\nmodel = LayoutDetection(model_name=\"PP-DocLayoutV2\")\noutput = model.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/layout.jpg\", batch_size=1, layout_nms=True)\nfor res in output:\nres.print()\nres.save_to_img(save_path=\"./output/\")\nres.save_to_json(save_path=\"./output/res.json\")\nFor more usage details and parameter explanations, see the documentation.\nCitation\nIf you find PaddleOCR-VL helpful, feel free to give us a star and citation.\n@misc{cui2025paddleocrvlboostingmultilingualdocument,\ntitle={PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model},\nauthor={Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},\nyear={2025},\neprint={2510.14528},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.14528},\n}",
    "Vortex5/LunaMaid-12B": "ü©µ LunaMaid-12B\nüß¨ Merge Overview\nü©µ Stage 1 ‚Äî Slerp Merge (Intermediate Model First)\nüåë Merge Method ‚Äî Karcher Mean Merge (Final Model)\nModels Merged\nü©µ LunaMaid-12B\nThis is a multi-stage merge of pre-trained language models created using mergekit.\nüß¨ Merge Overview\nLunaMaid-12B was produced through a two-stage multi-model merge using MergeKit.Each stage fuses models with complementary linguistic and stylistic traits to create a cohesive, emotionally nuanced personality.\nü©µ Stage 1 ‚Äî Slerp Merge (Intermediate Model First)\nBase Model: Vortex5/Vermilion-Sage-12B\nMerged With: yamatazen/NeonMaid-12B-v2\nMethod: Spherical Linear Interpolation (Slerp)\nStage 1 Configuration\nname: First\nbase_model: Vortex5/Vermilion-Sage-12B\nmodels:\n- model: yamatazen/NeonMaid-12B-v2\nmerge_method: slerp\ndtype: bfloat16\nparameters:\nnormalize: true\nt: [0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.6, 0.5, 0.6, 0.6]\nüåë Merge Method ‚Äî Karcher Mean Merge (Final Model)\nBase Model: Intermediate output from Stage 1 ./intermediates/First\nMerged With: Vortex5/Moonlit-Shadow-12B\nMethod: Karcher Mean (Riemannian Barycenter)\nStage 2 Configuration\ndtype: bfloat16\nmerge_method: karcher\nmodules:\ndefault:\nslices:\n- sources:\n- layer_range: [0, 40]\nmodel: ./intermediates/First\n- layer_range: [0, 40]\nmodel: Vortex5/Moonlit-Shadow-12B\nparameters:\nmax_iter: 9999\ntol: 1e-9\nModels Merged\nThe following models were included in the merge:\nVortex5/Moonlit-Shadow-12B\nVortex5/Vermilion-Sage-12B\nyamatazen/NeonMaid-12B-v2\n./intermediates/First",
    "TIGER-Lab/BrowserAgent-RFT": "Model\nPaper\nProject Page\nCode\nSample Usage\nCitation\nModel\nWe release the RFT (Reward Fine-Tuned) model used in BrowserAgent, initialized from the SFT checkpoint of Qwen/Qwen2.5-7B-Instruct.This model further optimizes browsing trajectories with task-level reward signals that encourage higher success rate, shorter action paths, and safer interactions.\nPaper\nBrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions\nProject Page\nhttps://tiger-ai-lab.github.io/BrowserAgent/\nCode\nhttps://github.com/TIGER-AI-Lab/BrowserAgent\nSample Usage\nhf download TIGER-Lab/BrowserAgent-RFT --local-dir ./models/browseragent-rft --repo model\nCitation\n@misc{yu2025browseragentbuildingwebagents,\ntitle={BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions},\nauthor={Tao Yu and Zhengbo Zhang and Zhiheng Lyu and Junhao Gong and Hongzhu Yi and Xinming Wang and Yuxuan Zhou and Jiabing Yang and Ping Nie and Yan Huang and Wenhu Chen},\nyear={2025},\neprint={2510.10666},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2510.10666},\n}",
    "DevQuasar/nanonets.Nanonets-OCR2-3B-GGUF": "Quantized version of: nanonets/Nanonets-OCR2-3B\n'Make knowledge free for everyone'\nMade with",
    "huawei-csl/Qwen3-14B-3bit-SINQ": "SINQ 3-bit Quantized Qwen3-14B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nSINQ 3-bit Quantized Qwen3-14B model\nThis repository contains the official 3-bit quantized version of the Qwen3-14B model using the SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-14B-3bit-SINQ\nBase Model: Qwen/Qwen3-14B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT3\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-14B-3bit-SINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-14B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 3-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=3,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"sinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-14B-3bit-ASINQ": "A-SINQ 3-bit Quantized Qwen3-14B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nA-SINQ 3-bit Quantized Qwen3-14B model\nThis repository contains the official 3-bit quantized version of the Qwen3-14B model using the calibrated version of SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-14B-3bit-ASINQ\nBase Model: Qwen/Qwen3-14B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  A-SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT3\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-14B-3bit-ASINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-14B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 3-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=3,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"asinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-32B-3bit-SINQ": "SINQ 3-bit Quantized Qwen3-32B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nSINQ 3-bit Quantized Qwen3-32B model\nThis repository contains the official 3-bit quantized version of the Qwen3-32B model using the SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-32B-3bit-SINQ\nBase Model: Qwen/Qwen3-32B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT3\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-32B-3bit-SINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-32B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 3-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=3,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"sinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-32B-3bit-ASINQ": "A-SINQ 3-bit Quantized Qwen3-32B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nA-SINQ 3-bit Quantized Qwen3-32B model\nThis repository contains the official 3-bit quantized version of the Qwen3-32B model using the calibrated version of SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-32B-3bit-ASINQ\nBase Model: Qwen/Qwen3-32B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  A-SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT3\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-32B-3bit-ASINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-32B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 3-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=3,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"asinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-1.7B-4bit-SINQ": "SINQ 4-bit Quantized Qwen3-1.7B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nSINQ 4-bit Quantized Qwen3-1.7B model\nThis repository contains the official 4-bit quantized version of the Qwen3-1.7B model using the SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-1.7B-4bit-SINQ\nBase Model: Qwen/Qwen3-1.7B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT4\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-1.7B-4bit-SINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-1.7B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 4-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=4,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"sinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-1.7B-4bit-ASINQ": "A-SINQ 4-bit Quantized Qwen3-1.7B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nA-SINQ 4-bit Quantized Qwen3-1.7B model\nThis repository contains the official 4-bit quantized version of the Qwen3-1.7B model using the calibrated version of SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-1.7B-4bit-ASINQ\nBase Model: Qwen/Qwen3-1.7B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  A-SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT4\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-1.7B-4bit-ASINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-1.7B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 4-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=4,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"asinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "hynt/Zipformer-30M-RNNT-6000h": "Vietnamese Speech-to-Text (ASR) ‚Äî ZipFormer-30M-RNNT-6000h\nüîç Overview\nüöÄ Online Demo\n‚öôÔ∏è Model Architecture and Training strategy:\nüß† Training Data\nüß™ Evaluation Results\nüèÜ Achievements\n‚ö° Inference Speed\n‚öôÔ∏è How to Run This Model\nüí¨ Summary\nVietnamese Speech-to-Text (ASR) ‚Äî ZipFormer-30M-RNNT-6000h\nüîç Overview\nThe Vietnamese Speech-to-Text (ASR) model is built on the ZipFormer architecture ‚Äî an improved variant of the Conformer ‚Äî featuring only 30 million parameters yet achieving exceptional performance in both speed and accuracy.On CPU, the model can transcribe a 12-second audio clip in just 0.3 seconds, significantly faster than most traditional ASR systems without requiring a GPU.\nüöÄ Online Demo\nYou can try the model directly here:üëâ https://huggingface.co/spaces/hynt/k2-automatic-speech-recognition-demo\n‚öôÔ∏è Model Architecture and Training strategy:\nArchitecture: ZipFormer\nParameters: ~30M\nLanguage: Vietnamese\nLoss Function: RNN-Transducer (RNNT Loss)\nFramework: PyTorch + k2\nTraining strategy: Carefully preprocess the data, apply an augmentation strategy based on the distribution of out-of-vocabulary (OOV) tokens and refine the transcriptions using Whisper.\nOptimized for: High-speed CPU inference\nüß† Training Data\nThe model was trained on approximately 6000 hours of high-quality Vietnamese speech collected from various public datasets:\nDataset\nVLSP2020\nVLSP2021\nVLSP2023-voting-pseudo-labeled\nVLSP2023\nFPT\nVIET_BUD500\nVietSpeech\nFLEURS\nVietMed_Labeled\nSub-GigaSpeech2-Vi\nViVoice\nSub-PhoAudioBook\nüß™ Evaluation Results\nDataset\nZipFormer-30M-6000h\nChunkFormer-110M-3000h\nPhoWhisper-Large-1.5B-800h\nVietASR-ZipFormer-68M-70.000h\nVLSP2020-Test-T1\n12.29\n14.09\n13.75\n14.45\nVLSP2023-PublicTest\n10.40\n16.15\n16.83\n14.70\nVLSP2023-PrivateTest\n11.10\n17.12\n17.10\n15.07\nVLSP2025-PublicTest\n7.97\n15.55\n16.14\n13.55\nVLSP2025-PrivateTest\n8.10\n16.07\n16.31\n13.97\nGigaSpeech2-Test\n7.56\n10.35\n10.00\n6.88\nLower is better (WER %)\nüèÜ Achievements\nBy training this model architecture on 4,000 hours of data, I won First Place in the Vietnamese Language Speech Processing (VLSP) competition 2025.\nComprehensive details about training data, optimization strategies, architecture improvements, and evaluation methodologies are available in the paper below:\nüëâ Read the full paper on Overleaf\n‚ö° Inference Speed\nDevice\nAudio Length\nInference Time\nCPU (Hugging Face Basic)\n12 seconds\n0.3 s\nGPU (RTX 3090)\n12 seconds\n< 0.1 s\n‚öôÔ∏è How to Run This Model\nPlease refer to the following guides for instructions on how to run and deploy this model:\nFor Torch JIT Script: https://k2-fsa.github.io/sherpa/\nFor ONNX: https://k2-fsa.github.io/sherpa/onnx/\nüí¨ Summary\nThe ZipFormer-30M-RNNT-6000h model demonstrates that a lightweight architecture can still achieve state-of-the-art accuracy for Vietnamese ASR.It is designed for fast deployment on CPU-based systems, making it ideal for real-time speech recognition, callbots, and embedded speech interfaces.",
    "huawei-csl/Qwen3-14B-4bit-SINQ": "SINQ 4-bit Quantized Qwen3-14B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nSINQ 4-bit Quantized Qwen3-14B model\nThis repository contains the official 4-bit quantized version of the Qwen3-14B model using the SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-14B-4bit-SINQ\nBase Model: Qwen/Qwen3-14B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT4\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-14B-4bit-SINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-14B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 4-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=4,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"sinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-14B-4bit-ASINQ": "A-SINQ 4-bit Quantized Qwen3-14B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nA-SINQ 4-bit Quantized Qwen3-14B model\nThis repository contains the official 4-bit quantized version of the Qwen3-14B model using the calibrated version of SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-14B-4bit-ASINQ\nBase Model: Qwen/Qwen3-14B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  A-SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT4\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-14B-4bit-ASINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-14B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 4-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=4,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"asinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "Vortex5/Abyssal-Seraph-12B": "üåå Abyssal-Seraph-12B\nüúÇ Overview\nü©∂ Model Essence\nüåô Stage 1\nü©∂ Stage 2\nüåå Stage 3 ‚Äî Final Merge\nüåëüúÇ Acknowledgements üúÇüåë\nüåå Abyssal-Seraph-12B\nWhere the light of the divine meets the poetry of the abyss.\nüúÇ Overview\nAbyssal-Seraph-12B is a multi-stage creative merge designed for expressive storytelling, emotional depth, and lyrical dialogue.It was crafted through a layered fusion using MergeKit:\nüåô LunaMaid √ó Vermilion-Sage ‚Äî merged via NearSwap (t=0.0008) to unify LunaMaid‚Äôs balanced composure with Vermilion-Sage‚Äôs radiant prose.\nüïØÔ∏è Dark-Quill √ó Mag-Mell-R1 ‚Äî merged via NearSwap (t=0.0008) to draw forth mysticism, poetic darkness, and a sense of dreamlike gravity.\n‚ú® Both intermediate results combined with the Karcher Mean ‚Äî a geometric blend ensuring harmony between light and shadow.\nü©∂ Model Essence\nTrait\nDescription\nüß† Core Nature\nPhilosophical, poetic, emotionally resonant\nüí¨ Style\nFluid prose, vivid imagery, articulate reflection\nüí´ Tone\nDreamlike, balanced between divine warmth and abyssal calm\nüé≠ Best For\nRoleplay, character dialogue, introspection, lore writing, creative prose\nüß¨ Merge Overview\nAbyssal-Seraph-12B was created through a multi-stage, precision merge designed to blend expressive prose with poetic balance while maintaining model stability.\nüåô Stage 1\n‚ú® Method: NearSwap (t = 0.0008)ü©µ Base: Vortex5/LunaMaid-12BüíÆ Secondary: Vortex5/Vermilion-Sage-12B\nStage 1 Configuration\nname: First\nmodels:\n- model: Vortex5/Vermilion-Sage-12B\nmerge_method: nearswap\nbase_model: Vortex5/LunaMaid-12B\nparameters:\nt: 0.0008\ndtype: bfloat16\ntokenizer:\nsource: Vortex5/LunaMaid-12B\nü©∂ Stage 2\n‚öôÔ∏è Method: NearSwap (t = 0.0008)\nüñ§ Base: Vortex5/Dark-Quill-12B\nüí´ Secondary: inflatebot/MN-12B-Mag-Mell-R1\nStage 2 Configuration\nname: Second\nmodels:\n- model: inflatebot/MN-12B-Mag-Mell-R1\nmerge_method: nearswap\nbase_model: Vortex5/Dark-Quill-12B\nparameters:\nt: 0.0008\ndtype: bfloat16`\nüåå Stage 3 ‚Äî Final Merge\n‚öñÔ∏è Method: Karcher Mean (tol = 1e-9, max_iter = 20000)\nüúÇ Inputs: First + Second\nüíé Purpose:\nTo geometrically fuse both for coherence.\nFinal Merge Configuration\nmodels:\n- model: First\n- model: Second\nmerge_method: karcher\ndtype: bfloat16\nparameters:\ntol: 1e-9\nmax_iter: 20000\ntokenizer:\nsource: First\nüåëüúÇ Acknowledgements üúÇüåë\n‚öôÔ∏è mradermacher ‚Äî for static and imatrix quantization\nüúõ DeathGodlike ‚Äî for  EXL3 quants\nü©∂ All original model authors and contributors whose work made this model possible.\nModels merged in this creation:\nVortex5/LunaMaid-12B\nVortex5/Vermilion-Sage-12B\nVortex5/Dark-Quill-12B\ninflatebot/MN-12B-Mag-Mell-R1",
    "weidawang/Chem-R-8B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nChem-R: Learning to Reason as a Chemist\nDatasets\nPrompt Template\nName Prediction\nProperty Prediction\nMolecule Desgin\nMolecule Captioning\nText-based Open Molecule Generation\nYield Prediction\nReagent Selection\nReaction Prediction\nRetrosynthesis\nChem-R: Learning to Reason as a Chemist\nAlthough large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R‚Äôs robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.\nDatasets\nOur multi-task training incorporates the following datasets across several categories:\nTask category\nDatasets\nName Prediction\nPubChem920k\nProperty Prediction\nBACE, BBBP, ClinTox, HIV, Tox21\nMolecule Design\nChEBI-20\nMolecule Captioning\nChEBI-20\nText-based Open Molecule Generation\nTOMG-Bench\nYield prediction\nBuchwald-Hartwig, Suzuki-Miyaura\nReagents selection\nSuzuki-Miyaura\nReaction prediction\nUSPTO-Mixed\nRetrosynthesis\nUSPTO-50k\nTo support multi-task learning, we have extended the data format used in EasyR1 by adding a task field. This modification allows for task-specific identification and enables granular accuracy evaluation, facilitating detailed multi-task comparison and analysis.\nPrompt Template\nName Prediction\nIUPAC to SMILES\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the SMILES for the following IUPAC name:\nSMILES to IUPAC\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the IUPAC name for the following SMILES:\nProperty Prediction\nBACE1\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nGiven the SMILES string of a molecule, predict the molecular properties of a given chemical compound based on its structure, by analyzing wether it can inhibit(Yes) the Beta-site Amyloid Precursor Protein Cleaving Enzyme 1 (BACE1) or cannot inhibit(No) BACE1.\nPlease answer with only Yes or No.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the BACE1 inhibition potential (Yes or No) for the following molecule:\nSMILES:\nBBBP\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nGiven the SMILES string of a molecule, the task focuses on predicting molecular properties, specifically penetration/non-penetration to the brain-blood barrier, based on the SMILES string representation of each molecule.\nThe task is to predict the binary label for a given molecule.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the Blood-Brain Barrier (BBB) penetration potential (Yes or No) for the following molecule:\nSMILES:\nClinTox\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nGiven the SMILES string of a molecule, the task focuses on predicting molecular properties, specifically wether a molecule is Clinically-trail-Toxic(Yes) or Not Clinically-trail-toxic (No) based on the SMILES string representation of each molecule.\nThe FDA-approved status will specify if the drug is approved by the FDA for clinical trials(Yes) or Not approved by the FDA for clinical trials(No). You should put your reasoning in <think> </think> tags. The final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the Clinical Trial Toxicity (CT_TOX) for the following molecule:\nSMILES:\nFDA_APPROVED:\nHIV\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nGiven the SMILES string of a molecule, the task focuses on predicting molecular properties, specifically its ability to inhibit HIV replication based on the SMILES string representation of each molecule.\nFor this property, you just need to answer \\\"Yes\\\" or \\\"No\\\".\nAdditionally, the activity test results of the molecules are provided. There are three classes of the activity test: 1). CA: confirmed active, 2). CM: Confirmed moderately active 3.) CI: Confirmed inactive. The task is to precisely predict the binary label for a given molecule and its HIV activity test, considering its properties and its potential to impede HIV replication.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the HIV replication inhibition potential (Yes or No) for the following molecule:\nSMILES:\nactivity:\nTOX21\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nGiven the SMILES string of a molecule, the task focuses on predicting molecular properties, specifically wether a molecule is toxic(Yes) or Not toxic(No), based on the SMILES string representation of each molecule.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\\nNow predict the toxicity (Yes or No) for the following molecule:\nSMILES:\nMolecule Desgin\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the SMILES representation for the following molecular design requirement:\nDescription:\nMolecule Captioning\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> The molecule is ... </answer> tags.\nPlease strictly follow the format.\nNow describe the following molecule based on its SMILES representation:\nSMILES:\nText-based Open Molecule Generation\nMolCustom, MolEdit and MolOpt.\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should explain your reasoning in <think> </think> tags.\nThe final answer MUST BE a SMILES string and put in <answer> </answer> tags.\nPlease strictly follow the format.\nNow, solve the following problem:\nYield Prediction\nBuchwald-Hartwig and Suzuki-Miyaura\nYou need to replace the {TASK_NAME} with the specific task name.\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nGiven the SMILES string of a {TASK_NAME} reaction, the task focuses on predicting reaction yield, specifically whether a reaction is High-yielding (Yes) or Not High-yielding (No), based on the SMILES string representation of each Buchwald-Hartwig reaction. The reactants are separated by '.' and product are separated by '>>'.\nHigh-yielding reaction means the yield rate of the reaction is above 70.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow predict the yield classification (Yes for High-yielding, No for Not High-yielding) for the following Buchwald-Hartwig reaction:\nReaction:\nReagent Selection\nReactant Selection, Solvent Selection and  Ligand Selection\nYou need to replace the {TASK_NAME} with the specific task name.\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNow select the optimal {TASK_NAME} (from the given reactant list) that would maximize the yield for the following Suzuki reaction setup:\nreactant:\nligand:\nreagent:\nsolvent:\nlist of reactants for selection: []\nReaction Prediction\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNote: If multiple products are predicted, they MUST be separated by a period `.` instead of commas.\nNow predict the product for the following reaction:\nReactants:\nRetrosynthesis\nYou are an expert chemist.\nYour task is to solve the given problem step by step.\nYou should put your reasoning in <think> </think> tags.\nThe final answer MUST BE put in  <answer> </answer> tags.\nPlease strictly follow the format.\nNote: If multiple reactants are predicted, they MUST be separated by a period `.` instead of commas.\nNow predict the reactants for the following product:\nProduct:",
    "DraconicDragon/Kaloscope-onnx": "LSNet Ëâ∫ÊúØÂÆ∂È£éÊ†ºÂàÜÁ±ªÊ®°Âûã Model Card\nÊ®°ÂûãÊ¶ÇËø∞\nÊ®°ÂûãÊèèËø∞\nÊû∂ÊûÑÁâπÁÇπ\nËÆ≠ÁªÉÊï∞ÊçÆ\nÊï∞ÊçÆÊù•Ê∫ê\nÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ\nËÆ≠ÁªÉÈÖçÁΩÆ\nÁ°¨‰ª∂ÁéØÂ¢É\nËÆ≠ÁªÉÂèÇÊï∞\nÊÄßËÉΩÊåáÊ†á\nÊ®°ÂûãÊÄßËÉΩ\nÂàÜÁ±ªÊÄßËÉΩ\nÊé®ÁêÜÊÄßËÉΩ\n‰ΩøÁî®ÊñπÊ≥ï\nÁéØÂ¢ÉË¶ÅÊ±Ç\nÂü∫Êú¨‰ΩøÁî®\nComfyuiÂÜÖ‰ΩøÁî®\nÁõ∏ÂÖ≥ËµÑÊ∫ê\nÂºïÁî®‰ø°ÊÅØ\nÊõ¥Êñ∞Êó•Âøó\nv1.0 (2025Âπ¥10Êúà)\nv1.1 (2025Âπ¥10Êúà)\nExperimental ONNX conversion of kaloscope model.These need a re-export because i didnt specify dynamic axes during export which probably locked the model behind batch size = 1; otherwise they seem to work perfectly fine25th oct: replaced existing onnx files with new exports that use dynamo=False and dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"},} - scores seems to be same as original pytorch model and dynamo=True export\nü§ó Space Demo for ONNX & PyTorch inference implementation (incl. timm+lsnet; OpenVINO accelerate CPU inference; no Triton required - refer to ska.py or here)Barebones standalone ONNX inference script (no timm or lsnet; scores are a tiny bit different - probably different img preprocessing): onnx_barebones_inference.py\nkaloscope_1-0.onnx: Exported from best_checkpoint.pth original Kaloscope release | dynamo=False, dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"},}, opset_version=None (torch 2.8.0 used here defaults to 18 when None), optimization/constant folding enabled\nkaloscope_1-1.onnx: Exported from 224-85.65/best_checkpoint.pth aka v1.1 | Same settings as v1.0\nConversion/Export script: colab notebooktheres an extra pytorch ema only version (best_checkpoint_ema.pth) of the model which is roughly 4 times smaller but seems to output different results; did not do extensive tests. ONNX version recommended over this (script used: save_ema.py)\nOriginal description:\nLSNet Ëâ∫ÊúØÂÆ∂È£éÊ†ºÂàÜÁ±ªÊ®°Âûã Model Card\nÊ®°ÂûãÊ¶ÇËø∞\nÊ®°ÂûãÂêçÁß∞:\nKaloscope Artist Style Classification ModelÊ®°ÂûãÁâàÊú¨: v1.0ÂèëÂ∏ÉÊó•Êúü: 2025Âπ¥10ÊúàÊ®°ÂûãÁ±ªÂûã: ÂõæÂÉèÂàÜÁ±ª (Ëâ∫ÊúØÂÆ∂È£éÊ†ºËØÜÂà´)Êû∂ÊûÑ: LSNet (See Large, Focus Small)\nÊ®°ÂûãÊèèËø∞\nÊú¨Ê®°ÂûãÂü∫‰∫éLSNetÊû∂ÊûÑÊûÑÂª∫Ôºå‰∏ìÈó®Áî®‰∫éËØÜÂà´ÂíåÂàÜÁ±ª‰∏çÂêåËâ∫ÊúØÂÆ∂ÁöÑÁªòÁîªÈ£éÊ†º„ÄÇLSNetÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßËßÜËßâÊ®°ÂûãÔºåÁÅµÊÑüÊù•Ê∫ê‰∫é‰∫∫Á±ªËßÜËßâÁ≥ªÁªüÁöÑÂä®ÊÄÅÂºÇÂ∞∫Â∫¶ËÉΩÂäõÔºåÂç≥\"ÁúãÂ§ßÂ±ÄÔºåËÅöÁÑ¶ÁªÜËäÇ\"ÁöÑÁâπÊÄß„ÄÇ\nÊû∂ÊûÑÁâπÁÇπ\nËÆæËÆ°ÁêÜÂøµ: Âü∫‰∫é‰∫∫Á±ªËßÜËßâÁ≥ªÁªüÁöÑ\"See Large, Focus Small\"ÂéüÁêÜ\nÊ®°ÂûãÁ≥ªÂàó: ÊîØÊåÅLSNet-T„ÄÅLSNet-S„ÄÅLSNet-B‰∏âÁßçËßÑÊ®°\nÂèÇÊï∞Èáè: Á∫¶100MÂèÇÊï∞\n‰ºòÂåñÁõÆÊ†á: Âú®‰øùÊåÅÈ´òÁ≤æÂ∫¶ÁöÑÂêåÊó∂ÂÆûÁé∞È´òÊïàÊé®ÁêÜ\nËÆ≠ÁªÉÊï∞ÊçÆ\nÊï∞ÊçÆÊù•Ê∫ê\nÊï∞ÊçÆÈõÜ: DanbooruÊï∞ÊçÆÈõÜ (Êà™Ê≠¢Âà∞2024Âπ¥10Êúà)\nÊï∞ÊçÆÁ≠õÈÄâ: ÈÄâÂèñÂõæÂÉèÊï∞ÈáèÂú®50Âº†‰ª•‰∏äÁöÑËâ∫ÊúØÂÆ∂\nÊÄªÂàÜÁ±ªÊï∞: 31,770‰∏™Ëâ∫ÊúØÂÆ∂Á±ªÂà´\nÊï∞ÊçÆÈááÊ†∑Á≠ñÁï•:\nÂõæÂÉèÊï∞ÈáèË∂ÖËøá100Âº†ÁöÑËâ∫ÊúØÂÆ∂ÔºöÈÄâÂèñIDÊúÄÈù†ÂêéÁöÑ100Âº†ÂõæÂÉè\nÂõæÂÉèÊï∞Èáè50-100Âº†ÁöÑËâ∫ÊúØÂÆ∂Ôºö‰ΩøÁî®ÂÖ®ÈÉ®ÂõæÂÉè\nÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ\nÂõæÂÉèÂ∞∫ÂØ∏: 224√ó224ÂÉèÁ¥†\nÊï∞ÊçÆÂ¢ûÂº∫: Ê†áÂáÜImageNetÈ¢ÑÂ§ÑÁêÜÊµÅÁ®ã\nÈ™åËØÅÈõÜÂàíÂàÜ: 5%ÁöÑÊï∞ÊçÆÁî®‰∫éÈ™åËØÅ\nËÆ≠ÁªÉÈÖçÁΩÆ\nÁ°¨‰ª∂ÁéØÂ¢É\nGPUÈÖçÁΩÆ: 8√óH20 GPU\nËÆ≠ÁªÉÊó∂Èïø: 80‰∏™epoch\nÊâπÊ¨°Â§ßÂ∞è: 256 (ÊØèGPU)\nËÆ≠ÁªÉÂèÇÊï∞\n‰ºòÂåñÂô®: AdamW\nÂ≠¶‰π†ÁéáË∞ÉÂ∫¶: Cosine Annealing\nÊï∞ÊçÆÂπ∂Ë°å: ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ (8Âç°)\nÊ®°ÂûãÂèÇÊï∞Èáè: ~100M\nÊÄßËÉΩÊåáÊ†á\nÊúÄÁªàÂáÜÁ°ÆÁéá: 84.2%\nÈ™åËØÅÊñπÂºè: Top-1ÂáÜÁ°ÆÁéá\nËØÑ‰º∞Êï∞ÊçÆ: È™åËØÅÈõÜ (5%ÁöÑÊï∞ÊçÆ)\nÊ®°ÂûãÊÄßËÉΩ\nÂàÜÁ±ªÊÄßËÉΩ\nÊåáÊ†á\nÊï∞ÂÄº\nTop-1 ÂáÜÁ°ÆÁéá\n84.2%\nÊÄªÁ±ªÂà´Êï∞\n31,770\nÂèÇÊï∞Èáè\n~100M\nËÆ≠ÁªÉËΩÆÊï∞\n80 epochs\nÊé®ÁêÜÊÄßËÉΩ\nËæìÂÖ•Ê†ºÂºè: RGBÂõæÂÉèÔºå224√ó224ÂÉèÁ¥†\nËæìÂá∫Ê†ºÂºè: 31,770Áª¥Ê¶ÇÁéáÂàÜÂ∏É\nÊé®ÁêÜÈÄüÂ∫¶: È´òÊïàÊé®ÁêÜ (ÂÖ∑‰ΩìÊï∞ÂÄºÂèñÂÜ≥‰∫éÁ°¨‰ª∂)\n‰ΩøÁî®ÊñπÊ≥ï\nÁéØÂ¢ÉË¶ÅÊ±Ç\npip install torch torchvision timm\nÂü∫Êú¨‰ΩøÁî®\nimport torch\nfrom timm.models import create_model\n# Âä†ËΩΩÊ®°Âûã\nmodel = create_model('lsnet_t_artist', pretrained=True, num_classes=31770)\nmodel.eval()\n# Êé®ÁêÜ\nwith torch.no_grad():\noutput = model(input_tensor)\nprobabilities = torch.softmax(output, dim=1)\nComfyuiÂÜÖ‰ΩøÁî®\nÂÆâË£ÖcomfyuiËäÇÁÇπÔºöhttps://github.com/spawner1145/comfyui-lsnet\n‰∏ãËΩΩÊú¨‰ªìÂ∫ìÊ®°ÂûãÂç≥ÂèØ‰ΩøÁî®\nÁõ∏ÂÖ≥ËµÑÊ∫ê\nËÆ∫Êñá: LSNet: See Large, Focus Small\n‰ª£Á†Å‰ªìÂ∫ì: (https://github.com/spawner1145/lsnet-test)\nÈ¢ÑËÆ≠ÁªÉÊ®°Âûã: ÂèØÈÄöËøáHugging Face HubËé∑Âèñ\nÂºïÁî®‰ø°ÊÅØ\n@misc{wang2025lsnetlargefocussmall,\ntitle={LSNet: See Large, Focus Small},\nauthor={Ao Wang and Hui Chen and Zijia Lin and Jungong Han and Guiguang Ding},\nyear={2025},\neprint={2503.23135},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2503.23135},\n}\nÊõ¥Êñ∞Êó•Âøó\nv1.0 (2025Âπ¥10Êúà)\nÂàùÂßãÁâàÊú¨ÂèëÂ∏É\nÂü∫‰∫éDanbooruÊï∞ÊçÆÈõÜËÆ≠ÁªÉ\nÊîØÊåÅ31,770‰∏™Ëâ∫ÊúØÂÆ∂Á±ªÂà´\nËææÂà∞84.2%ÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá\nv1.1 (2025Âπ¥10Êúà)\n150epoch\nËææÂà∞85.6%ÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá\nÂÖçË¥£Â£∞Êòé: Êú¨Ê®°Âûã‰ªÖ‰æõÁ†îÁ©∂ÂíåÊïôËÇ≤Áî®ÈÄî„ÄÇÂú®ÂïÜ‰∏öÂ∫îÁî®‰∏≠‰ΩøÁî®Êó∂ÔºåËØ∑Á°Æ‰øùÈÅµÂÆàÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÂíå‰º¶ÁêÜÂáÜÂàô„ÄÇ",
    "Vortex5/Lunar-Abyss-12B": "üåò Lunar-Abyss-12B\nüåë Overview\n‚öñÔ∏è Merge Method ‚Äî DELLA\nüåå Essence of the Merge\nüé≠ Roleplay & Creative Focus\nüåí Acknowledgements üåò\nüßæ Models Merged\nüåò Lunar-Abyss-12B\nBorn where moonlight touches the deep ‚Äî thought meets desire, and reason dreams.\nüåë Overview\nLunar-Abyss-12B was made to combine the coherency and stability of LunaMaid-12B with the evocative prose and edgy flair of Abyssal-Seraph-12B.\n‚öñÔ∏è Merge Method ‚Äî DELLA\nüß© Base: Vortex5/MegaMoon-Karcher-12Büíé Inputs: Vortex5/LunaMaid-12B + Vortex5/Abyssal-Seraph-12B\nConfiguration\nmodels:\n- model: Vortex5/LunaMaid-12B\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.35, 0.4, 0.6, 0.8, 1.0, 0.9, 0.6, 0.3]\n- filter: mlp\nvalue: [0.20, 0.25, 0.35, 0.45, 0.45, 0.40, 0.30, 0.20]\n- value: [0.25, 0.3, 0.35, 0.4, 0.4, 0.35, 0.3, 0.25]\ndensity: 0.55\nepsilon: 0.3\n- model: Vortex5/Abyssal-Seraph-12B\nparameters:\nweight:\n- filter: mlp\nvalue: [0.3, 0.5, 0.8, 1.0, 1.0, 0.9, 0.7, 0.4]\n- value: [0.2, 0.3, 0.4, 0.5, 0.5, 0.4, 0.3, 0.2]\ndensity: 0.5\nepsilon: 0.4\nmerge_method: della\nbase_model: Vortex5/MegaMoon-Karcher-12B\nparameters:\nlambda: 1.0\nnormalize: true\ndtype: bfloat16\ntokenizer:\nsource: Vortex5/Abyssal-Seraph-12B\nüåå Essence of the Merge\nLike moonlight reflecting on dark water, Lunar-Abyss carries both clarity and depth.It thinks with the calm focus of LunaMaid yet speaks with the emotional pulse of Abyssal-Seraph.Every response flows with a quiet duality ‚Äî logic beneath, creativity above ‚Äî neither overpowering the other.\nFor fans of expressive writing and immersive roleplay, it offers a tone that‚Äôs reflective, and mysterious.\nüé≠ Roleplay & Creative Focus\nDesigned for narrative storytelling, introspective dialogue, and emotion-driven writing.\nüåí Acknowledgements üåò\n‚öôÔ∏è mradermacher ‚Äî static / imatrix quantization\nüúõ DeathGodlike ‚Äî EXL3 quants\nü©∂ All original model authors and contributors whose work made this model possible.\nüßæ Models Merged\nModels merged in this creation:\nVortex5/LunaMaid-12B\nVortex5/Abyssal-Seraph-12B\nVortex5/MegaMoon-Karcher-12B"
}