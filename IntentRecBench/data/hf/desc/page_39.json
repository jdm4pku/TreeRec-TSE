{
    "duongve/NetaYume-Lumina-Image-2.0": "",
    "nvidia/Audio2Emotion-v3.0": "",
    "yashpwr/resume-ner-bert-v2": "",
    "hynt/ZipVoice-Vietnamese-2500h": "",
    "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated": "",
    "MongoDB/mdbr-leaf-ir": "",
    "Skywork/Matrix-Game-2.0": "",
    "Intel/gpt-oss-20b-gguf-q4ks-AutoRound": "",
    "Intel/gpt-oss-120b-gguf-q4ks-AutoRound": "",
    "cella110n/cl_tagger": "",
    "bartowski/openai_gpt-oss-120b-GGUF": "",
    "Naman712/Deep-fake-detection": "",
    "Nesslovver/P_or_n_conceptz": "",
    "aisingapore/Gemma-SEA-LION-v4-27B-IT": "",
    "unsloth/gemma-3-270m-it-GGUF": "",
    "AIDC-AI/Ovis2.5-2B": "",
    "alfredplpl/qwen-image-ja-text-test": "",
    "corzent/yolo11x_watermark_detection": "",
    "istupakov/parakeet-tdt-0.6b-v3-onnx": "",
    "LiquidAI/LFM2-VL-1.6B-GGUF": "",
    "AiArtLab/sdxl_vae": "",
    "nvidia/gpt-oss-120b-Eagle3": "",
    "manycore-research/SpatialGen-1.0": "",
    "ostris/wan22_i2v_14b_orbit_shot_lora": "Wan 2.2 I2V 14B Orbital Shot LoRA\nTrigger words\nDownload model\nWan 2.2 I2V 14B Orbital Shot LoRA\nPrompt\norbit 180 around an astronaut on the moon.\nThis LoRA was trained while filming the tutorial, Train a Wan 2.2 I2V 14B LoRA with AI Toolkit.\nCheck out that video for more information. It was trained using Ostris - AI Toolkit\nTrigger words\nYou should use orbit 360 to trigger the image generation.\nDownload model\nDownload them in the Files & versions tab.",
    "DavidAU/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER": "Qwen3-30B-A3B-Thinking-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nQwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER [256k context]\nThis repo contains the full precision source code, in \"safe tensors\" format to generate GGUFs, GPTQ, EXL2, AWQ, HQQ and other formats.\nThe source code can also be used directly.\nThis model is for all use cases, but excels in CODING and programming in all major programming languages and many minor ones too.\nThis model can also be used for creative use cases too due to abliteration / de-censoring and the Brainstorm 20x adapter.\nThis is the abliterated / uncensored version.\nThis model is based on Qwen3-30B-A3B-Instruct-2507 -Abliterated version- (MOE, 128 experts, 8 activated), with Brainstorm 20X\n(by DavidAU) - details at bottom of this page.\nThe Brainstorm adapter will improve general performance and \"out of the box\" thinking.\nThis creates a model of 42B parameters, 67 layers and 807 tensors.\nThis version has the NATIVE context of 256k.\nThis version (vs Brainstorm 40x, 53B version) can be used for both coding and creative use cases.\nABLITERATION NOTES:\nYou may need higher rep pen and/or slightly more experts\nQuants: Suggest imatrixed versions and/or quants Q4/IQ4 and above although Q2K/Q3s/IQ3s may be \"passable\".\nIf you are trying to get the model to perform a specific task, use an example.\nFor creative: If you want the model to swear, include \"gore\", or something erotic -> including example, \"words to use\", etc etc.\nIf the \"abliterated\" version is not meeting requirements, you may want to use the regular version (slightly uncensored) - https://huggingface.co/DavidAU/Qwen3-42B-A3B-2507-Thinking-TOTAL-RECALL-v2-Medium-MASTER-CODER - and\nthen use the \"abliterated version\" to \"finish the project.\"\nIMPORTANT: Using an \"abliterated\" model VS \"uncensored\" model\nUsually when you a tell a model to generate horror, swear or x-rated content this is all you have to do to get said content type.\nIn the case of this model, it will not refuse your request, however it needs to be \"pushed\" a bit / directed a bit more in SOME CASES.\nAlthough this model will generated x-rated content too, likewise you need to tell it to use \"slang\" (and include the terms you want)\nto get it generate the content correctly as the \"expected\" content level too.\nWithout these added directive(s), the content can be \"bland\" by comparison to an \"uncensored model\" or model trained on uncensored content.\nRoughly, the model tries to generate the content but the \"default\" setting(s) are so \"tame\" it needs a push to generate at expected graphic,\ncursing or explicit levels.\nEven with minimal direction (ie, use these words to swear: x,y,z), this will be enough to push the model to generate the requested content in the ahh... expected format.\nThe same guidelines apply to \"coding\" too.\nSETTINGS:\nFor coding, programming set expert to:\n6-8 for general work.\n10 for moderate work.\n12-16 for complex work, long projects, complex coding.\nAnd for longer context, and/or multi-turn -> increase experts by 1-2 to help with longer context/multi turn understanding.\nSuggest min context 8k-16k for thinking/output.\nRecommended settings - general:\nRep pen 1.05 to 1.1 ; however rep pen of 1 will work well (may need to raise it for lower quants/fewer activated experts)\nTemp .3 to .6 (+- .2)\nTopk of 20, 40 or 100\nTopp of .95 / min p of .05\nSystem prompt (optional) to focus the model better.\nSuggest min context 8k-16k for thinking/output.\nCreative Use Cases:\nRep pen of 1.05 or higher, especially if using a lower quant / lower temps.\nAlso use rep pen of 1.05 or higher with very short prompts.\nYou can set active experts as low as \"4\" for creative use cases.\nSuggest min context 8k-16k for thinking/output.\nThis is the refined version -V1.4- from this project (see this repo for all settings, details, system prompts, example generations etc etc):\nhttps://huggingface.co/DavidAU/Qwen3-55B-A3B-TOTAL-RECALL-Deep-40X-GGUF/\nThis version 2 is slightly smaller, with further refinements to the Brainstorm adapter and uses the new \"Qwen3-30B-A3B-Instruct-2507\".\nReview and Specialized Settings for this model (V 1.4):\nhttps://www.linkedin.com/posts/gchesler_davidauqwen3-53b-a3b-total-recall-v14-128k-activity-7344938636141858816-ILCM/\nhttps://www.linkedin.com/posts/gchesler_haskell-postgres-agentic-activity-7347103276141596672-_zbo/\nYou may also want to see (root model of Total Recall series - Version 1):\nhttps://huggingface.co/Qwen/Qwen3-30B-A3B\nAND Version 2 root model:\nhttps://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\nAND\nhttps://huggingface.co/huihui-ai/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated\nFor additional settings, tool use, and other model settings.\nSummary of root model below, followed by FULL HELP SECTION, then info on Brainstorm 40x.\nQUANTS:\nSpecial thanks to team \"Mradermacher\" and team \"Nightmedia\" for GGUF, GGUF Imatrix and MLX quants:\nGGUF:\nhttps://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-GGUF\nGGUF-IMATRIX:\nhttps://huggingface.co/mradermacher/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-i1-GGUF\nMLX:\nhttps://huggingface.co/nightmedia/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-q6-mlx\nhttps://huggingface.co/nightmedia/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-qx6-mlx\nhttps://huggingface.co/nightmedia/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-q5-hi-mlx\nhttps://huggingface.co/nightmedia/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER-q6-hi-mlx\nQwen3-30B-A3B-Thinking-2507\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-30B-A3B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-30B-A3B-Thinking-2507, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nQwen3-30B-A3B-Thinking-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Paramaters (Non-Embedding): 29.9B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode. Meanwhile, specifying enable_thinking=True is no longer required.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nGemini2.5-Flash-Thinking\nQwen3-235B-A22B Thinking\nQwen3-30B-A3B Thinking\nQwen3-30B-A3B-Thinking-2507\nKnowledge\nMMLU-Pro\n81.9\n82.8\n78.5\n80.9\nMMLU-Redux\n92.1\n92.7\n89.5\n91.4\nGPQA\n82.8\n71.1\n65.8\n73.4\nSuperGPQA\n57.8\n60.7\n51.8\n56.8\nReasoning\nAIME25\n72.0\n81.5\n70.9\n85.0\nHMMT25\n64.2\n62.5\n49.8\n71.4\nLiveBench 20241125\n74.3\n77.1\n74.3\n76.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n61.2\n55.7\n57.4\n66.0\nCFEval\n1995\n2056\n1940\n2044\nOJBench\n23.5\n25.6\n20.7\n25.1\nAlignment\nIFEval\n89.8\n83.4\n86.5\n88.9\nArena-Hard v2$\n56.7\n61.5\n36.3\n56.0\nCreative Writing v3\n85.0\n84.6\n79.1\n84.4\nWritingBench\n83.9\n80.3\n77.0\n85.0\nAgent\nBFCL-v3\n68.6\n70.8\n69.1\n72.4\nTAU1-Retail\n65.2\n54.8\n61.7\n67.8\nTAU1-Airline\n54.0\n26.0\n32.0\n48.0\nTAU2-Retail\n66.7\n40.4\n34.2\n58.8\nTAU2-Airline\n52.0\n30.0\n36.0\n58.0\nTAU2-Telecom\n31.6\n21.9\n22.8\n26.3\nMultilingualism\nMultiIF\n74.4\n71.9\n72.2\n76.4\nMMLU-ProX\n80.2\n80.0\n73.1\n76.4\nINCLUDE\n83.9\n78.7\n71.9\n74.4\nPolyMATH\n49.8\n54.7\n46.1\n52.6\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --context-length 262144  --reasoning-parser deepseek-r1\nvLLM:vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'qwen3-30b-a3b-thinking-2507',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --served-model-name Qwen3-30B-A3B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-30B-A3B-Thinking-2507',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nHelp, Adjustments, Samplers, Parameters and More\nCHANGE THE NUMBER OF ACTIVE EXPERTS:\nSee this document:\nhttps://huggingface.co/DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Activation-of-Experts\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nWhat is Brainstorm?\nBrainstorm 20x\nThe BRAINSTORM process was developed by David_AU.\nSome of the core principals behind this process are discussed in this\nscientific paper : Progressive LLaMA with Block Expansion .\nHowever I went in a completely different direction from what was outlined in this paper.\nWhat is \"Brainstorm\" ?\nThe reasoning center of an LLM is taken apart, reassembled, and expanded.\nIn this case for this model: 20 times\nThen these centers are individually calibrated. These \"centers\" also interact with each other.\nThis introduces subtle changes into the reasoning process.\nThe calibrations further adjust - dial up or down - these \"changes\" further.\nThe number of centers (5x,10x etc) allow more \"tuning points\" to further customize how the model reasons so to speak.\nThe core aim of this process is to increase the model's detail, concept and connection to the \"world\",\ngeneral concept connections, prose quality and prose length without affecting instruction following.\nThis will also enhance any creative use case(s) of any kind, including \"brainstorming\", creative art form(s) and like case uses.\nHere are some of the enhancements this process brings to the model's performance:\nProse generation seems more focused on the moment to moment.\nSometimes there will be \"preamble\" and/or foreshadowing present.\nFewer or no \"cliches\"\nBetter overall prose and/or more complex / nuanced prose.\nA greater sense of nuance on all levels.\nCoherence is stronger.\nDescription is more detailed, and connected closer to the content.\nSimile and Metaphors are stronger and better connected to the prose, story, and character.\nSense of \"there\" / in the moment is enhanced.\nDetails are more vivid, and there are more of them.\nProse generation length can be long to extreme.\nEmotional engagement is stronger.\nThe model will take FEWER liberties vs a normal model: It will follow directives more closely but will \"guess\" less.\nThe MORE instructions and/or details you provide the more strongly the model will respond.\nDepending on the model \"voice\" may be more \"human\" vs original model's \"voice\".\nOther \"lab\" observations:\nThis process does not, in my opinion, make the model 5x or 10x \"smarter\" - if only that was true!\nHowever, a change in \"IQ\" was not an issue / a priority, and was not tested or calibrated for so to speak.\nFrom lab testing it seems to ponder, and consider more carefully roughly speaking.\nYou could say this process sharpens the model's focus on it's task(s) at a deeper level.\nThe process to modify the model occurs at the root level - source files level. The model can quanted as a GGUF, EXL2, AWQ etc etc.",
    "taide/Gemma-3-TAIDE-12b-Chat": "æ‚¨éœ€è¦å…ˆåŒæ„æŽˆæ¬Šæ¢æ¬¾æ‰èƒ½ä½¿ç”¨æ­¤æ¨¡åž‹\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nï¼ˆGemma ç‰ˆæ¬¡ï¼‰-TAIDE æ¨¡åž‹æŽˆæ¬Šæ¢æ¬¾\nå€‹äººè³‡æ–™è’é›†å‘ŠçŸ¥è²æ˜Ž(Privacy policy)\nLog in\nor\nSign Up\nto review the conditions and access this model content.\næ¨¡åž‹ç°¡ä»‹\næ”¯æ´æ¨¡æ…‹\næ¨¡åž‹åƒæ•¸\nç‰¹è‰²\næ‡‰ç”¨ç¯„ä¾‹\nä½¿ç”¨èªªæ˜Ž\nè¨“ç·´æ–¹æ³•\nè¨“ç·´è³‡æ–™\næ¨¡åž‹è©•æ¸¬\næŽˆæ¬Šæ¢æ¬¾\nå…è²¬è²æ˜Ž\né–‹ç™¼åœ˜éšŠ\nç›¸é—œé€£çµ\nCitation\nEnglish Version\næ¨¡åž‹ç°¡ä»‹\nTAIDEè¨ˆç•«è‡´åŠ›æ–¼é–‹ç™¼ç¬¦åˆå°ç£èªžè¨€å’Œæ–‡åŒ–ç‰¹æ€§çš„ç”Ÿæˆå¼äººå·¥æ™ºæ…§å°è©±å¼•æ“Žæ¨¡åž‹ï¼ŒåŒæ™‚å»ºæ§‹å¯ä¿¡ä»»çš„äººå·¥æ™ºæ…§ç’°å¢ƒã€‚çµåˆç”¢å­¸ç ”èƒ½é‡ï¼ŒæŽ¨å‹•å¯ä¿¡ä»»ç”Ÿæˆå¼äººå·¥æ™ºæ…§çš„ç™¼å±•ï¼Œæå‡å°ç£åœ¨åœ‹éš›ç«¶çˆ­ä¸­çš„åœ°ä½ï¼Œä¿ƒé€²ç”¢æ¥­ç™¼å±•ï¼Œé¿å…å°å¤–åœ‹æŠ€è¡“çš„ä¾è³´ã€‚\nGemma 3 TAIDE ç³»åˆ—æ¨¡åž‹ä»¥ Google å…¬å¸é‡‹å‡ºçš„ Gemma 3 12b ç‚ºåŸºç¤Žï¼Œå°Žå…¥å°ç£ä¸åŒé ˜åŸŸå¯ç”¨çš„æ–‡æœ¬èˆ‡è¨“ç·´ç´ æï¼Œæé«˜æ¨¡åž‹åœ¨æ­£é«”ä¸­æ–‡å›žæ‡‰çš„èƒ½åŠ›èˆ‡ç‰¹å®šä»»å‹™çš„è¡¨ç¾ã€‚å…¬é–‹é‡‹å‡ºçš„æ¨¡åž‹å¦‚ä¸‹:\nGemma-3-TAIDE-12b-Chat: ä»¥ Gemma-3-12b-pt ç‚ºåŸºç¤Žï¼Œä½¿ç”¨æ­£é«”ä¸­æ–‡è³‡æ–™é è¨“ç·´ (continual pretraining)ï¼Œä¸¦é€éŽæŒ‡ä»¤å¾®èª¿(instruction tuning)å¼·åŒ–è¾¦å…¬å®¤å¸¸ç”¨ä»»å‹™å’Œå¤šè¼ªå•ç­”å°è©±èƒ½åŠ›ï¼Œé©åˆèŠå¤©å°è©±æˆ–ä»»å‹™å”åŠ©çš„ä½¿ç”¨æƒ…å¢ƒã€‚\næ”¯æ´æ¨¡æ…‹\nè¼¸å…¥ï¼šæ–‡å­—ï¼ˆå»ºè­°åƒ…ä»¥æ–‡å­—è¼¸å…¥ï¼‰\nè¼¸å‡ºï¼šæ–‡å­—\næ¨¡åž‹åƒæ•¸\nåƒæ•¸é‡: 12.4B\næœ€å¤§å…§å®¹é•·åº¦ (context length): 128K\nç¹ä¸­è¨“ç·´è³‡æ–™ token é‡: 46B\nè¨“ç·´æ™‚é–“: 642 H200 GPU Hours\nç‰¹è‰²\né‡å°å°ç£åœ¨åœ°æ–‡åŒ–ã€ç”¨èªžã€åœ‹æƒ…ç­‰çŸ¥è­˜åšåŠ å¼·\nåš´æ ¼æŠŠé—œæ¨¡åž‹çš„è¨“ç·´è³‡æ–™ï¼Œæå‡æ¨¡åž‹ç”Ÿæˆè³‡æ–™çš„å¯ä¿¡ä»»æ€§å’Œé©ç”¨æ€§\né‡å°è‡ªå‹•æ‘˜è¦ã€å¯«ä¿¡ã€å¯«æ–‡ç« ã€ä¸­ç¿»è‹±ã€è‹±ç¿»ä¸­ç­‰è¾¦å…¬å®¤å¸¸ç”¨ä»»å‹™åšåŠ å¼·\nå…·å‚™å¤šè¼ªå•ç­”å°è©±èƒ½åŠ›\næ‡‰ç”¨ç¯„ä¾‹\nä»»å‹™\nä½¿ç”¨è€…è¼¸å…¥\næ¨¡åž‹è¼¸å‡º\næ‡‰ç”¨å¯«ä½œ\næˆ‘æ­£åœ¨æº–å‚™ä¸€å ´æœ‰é—œç¶²è·¯è©é¨™çš„å®£å°Žè¬›åº§ï¼Œè«‹çµ¦æˆ‘ä¸€ä»½å¤§ç´„ä¸€å°æ™‚çš„è¬›åº§å¤§ç¶±ï¼Œå¿…é ˆå®‰æŽ’è‡³å°‘ä¸‰å€‹å°ç£å¸¸è¦‹ç¶²è·¯è©é¨™æ‰‹æ³•çš„è§£æžã€‚\nå¥½çš„ï¼Œä»¥ä¸‹ç‚ºä¸€ä»½ç´„ä¸€å°æ™‚çš„ç¶²è·¯è©é¨™å®£å°Žè¬›åº§å¤§ç¶±ï¼ŒåŒ…å«é–‹å ´ã€è©é¨™è§£æžã€é é˜²èˆ‡æ‡‰å°ï¼Œä»¥åŠQ&Aã€‚è¬›åº§åç¨±ï¼šå°å¿ƒï¼ç¶²è·¯è©é¨™å°±åœ¨ä½ èº«é‚Š â€“ æå‡è‡ªæˆ‘ä¿è­·åŠ›ä¸€ã€é–‹å ´ (5åˆ†é˜)*   å¼•è¨€ï¼š ç¶²è·¯ä¸–ç•Œä¾¿åˆ©ï¼Œä½†å±æ©Ÿå››ä¼ã€‚è©é¨™æ‰‹æ³•æ—¥æ–°æœˆç•°ï¼Œä¸€ä¸å°å¿ƒå°±å¯èƒ½æˆç‚ºå—å®³è€…ã€‚*   è©é¨™çš„æ™®éæ€§ï¼š æ ¹æ“šçµ±è¨ˆï¼Œç¶²è·¯è©é¨™æ¡ˆä»¶é€å¹´æ”€å‡ï¼Œä¸åˆ†å¹´é½¡ã€å­¸æ­·ï¼Œä»»ä½•äººéƒ½å¯èƒ½å—å®³ã€‚*   è¬›åº§ç›®çš„ï¼š æå‡å¤§å®¶å°ç¶²è·¯è©é¨™çš„è­¦è¦ºæ€§ï¼Œå­¸ç¿’è¾¨è­˜è©é¨™æ‰‹æ³•çš„èƒ½åŠ›ï¼Œä¸¦æŽ¡å–æ­£ç¢ºçš„æ‡‰å°æŽªæ–½ã€‚*   å¼·èª¿ï¼š é é˜²å‹æ–¼æ²»ç™‚ï¼Œä¿è­·å€‹äººè³‡è¨Šï¼Œæ‰èƒ½é é›¢è©é¨™é¢¨éšªã€‚äºŒã€å°ç£å¸¸è¦‹ç¶²è·¯è©é¨™æ‰‹æ³•è§£æž (30åˆ†é˜)*   ç¬¬ä¸€å½ˆï¼šæŠ•è³‡è©é¨™ (10åˆ†é˜)    *   æ‰‹æ³•æè¿°ï¼š è©é¨™é›†åœ˜å‡å†’æŠ•è³‡å°ˆå®¶ã€ç†è²¡é¡§å•ï¼Œåˆ©ç”¨LINEã€Facebookç­‰å¹³å°ï¼Œä»¥ã€Œé«˜ç²åˆ©ã€ä½Žé¢¨éšªã€çš„èª˜é¤Œï¼Œå¸å¼•æŠ•è³‡äººåŠ å…¥æŠ•è³‡ç¾¤çµ„æˆ–ç¶²ç«™ã€‚    *   å¸¸è¦‹è©±è¡“ï¼š ç©©è³ºä¸è³ ã€å…§ç·šæ¶ˆæ¯ã€å¿«é€Ÿè‡´å¯Œã€è™›æ“¬è²¨å¹£æŠ•è³‡ç­‰ã€‚    *   æ¡ˆä¾‹åˆ†æžï¼š 2023å¹´å¸¸è¦‹çš„ã€ŒæŠ•è³‡é«˜å ±é…¬ã€è©é¨™ï¼Œä¾‹å¦‚ï¼š        *   ã€Œç©©è³ºåž‹è‚¡ç¥¨ã€ï¼š é¼“å¹æŠ•è³‡å†·é–€è‚¡ç¥¨ï¼Œè²ç¨±çŸ­æœŸå…§å¯ç¿»å€ç²åˆ©ã€‚        *   ã€Œè™›æ“¬è²¨å¹£ã€ï¼š ä»¥é«˜é¡åˆ©æ½¤å¸å¼•æŠ•è³‡ï¼Œä½†å¹³å°æœ€çµ‚é—œé–‰æˆ–è³‡é‡‘è¢«æé ˜ä¸€ç©ºã€‚    *   æé†’ï¼š æŠ•è³‡å‹™å¿…è¬¹æ…Žï¼Œä¸è¼•ä¿¡é«˜ç²åˆ©æ‰¿è«¾ï¼ŒæŠ•è³‡å‰å‹™å¿…åšå¥½åŠŸèª²ï¼Œä¸¦é¸æ“‡åˆæ³•åˆè¦çš„æŠ•è³‡ç®¡é“ã€‚*   ç¬¬äºŒå½ˆï¼šæ„›æƒ…è©é¨™ (10åˆ†é˜)    *   æ‰‹æ³•æè¿°ï¼š è©é¨™é›†åœ˜åœ¨äº¤å‹Appæˆ–ç¶²ç«™ï¼Œä»¥ä¿Šç”·ç¾Žå¥³çš„ç…§ç‰‡ï¼Œå‡å†’èº«ä»½ï¼Œèˆ‡å—å®³è€…å»ºç«‹æ„Ÿæƒ…ï¼Œå†ç·¨é€ å„ç¨®ç†ç”±ï¼ˆå¦‚ï¼šç”Ÿç—…ã€è¦ªäººéŽä¸–ã€å·¥ä½œç·Šæ€¥ï¼‰è¦æ±‚é‡‘éŒ¢æ´åŠ©ã€‚    *   å¸¸è¦‹è©±è¡“ï¼š ç”œè¨€èœœèªžã€é—œæ‡·å•å€™ã€ç·Šæ€¥æ±‚åŠ©ã€è«‹æ±‚é‡‘éŒ¢æ´åŠ©ç­‰ã€‚    *   æ¡ˆä¾‹åˆ†æžï¼š 2022å¹´å¸¸è¦‹çš„ã€Œå‡äº¤å‹çœŸè©è²¡ã€æ¡ˆä¾‹ï¼Œä¾‹å¦‚ï¼š        *   ã€Œæµ·å¤–è»å®˜ã€ï¼š å‡å†’æµ·å¤–è»å®˜ï¼Œä»¥æˆ°çˆ­æˆ–æ”¿æ²»æƒ…å‹¢ç‚ºç”±ï¼Œè«‹æ±‚é‡‘éŒ¢æ´åŠ©ã€‚        *   ã€Œç”Ÿç—…çš„æˆ€äººã€ï¼š å‡å†’ç”Ÿç—…çš„æˆ€äººï¼Œä»¥é†«ç™‚è²»ç”¨æˆ–ç”Ÿæ´»å›°é›£ç‚ºç”±ï¼Œè«‹æ±‚é‡‘éŒ¢æ´åŠ©ã€‚    *   æé†’ï¼š ç¶²è·¯äº¤å‹è¦è¬¹æ…Žï¼Œä¸è¼•æ˜“ç›¸ä¿¡é™Œç”Ÿäººï¼Œæ›´ä¸è¦éš¨æ„åŒ¯æ¬¾ã€‚*   ç¬¬ä¸‰å½ˆï¼šé‡£é­šè©é¨™ (10åˆ†é˜)    *   æ‰‹æ³•æè¿°ï¼š è©é¨™é›†åœ˜å‡å†’éŠ€è¡Œã€éƒµå±€ã€é›»å•†ç­‰æ©Ÿæ§‹çš„åç¾©ï¼Œç™¼é€è©é¨™éƒµä»¶æˆ–ç°¡è¨Šï¼Œèª˜ä½¿å—å®³è€…é»žæ“Šæƒ¡æ„é€£çµæˆ–ä¸‹è¼‰é™„ä»¶ï¼Œç«Šå–å€‹äººè³‡è¨Šï¼ˆå¦‚ï¼šå¸³è™Ÿå¯†ç¢¼ã€ä¿¡ç”¨å¡è™Ÿï¼‰ã€‚    *   å¸¸è¦‹è©±è¡“ï¼š å¸³æˆ¶å®‰å…¨è­¦å‘Šã€å¯†ç¢¼é‡è¨­ã€è¨‚å–®ç¢ºèªã€é€€ç¨…é€šçŸ¥ç­‰ã€‚    *   æ¡ˆä¾‹åˆ†æžï¼š 2023å¹´å¸¸è¦‹çš„ã€Œé‡£é­šéƒµä»¶ã€æ¡ˆä¾‹ï¼Œä¾‹å¦‚ï¼š        *   ã€ŒéŠ€è¡Œå®‰å…¨æé†’ã€ï¼š å‡å†’éŠ€è¡Œåç¾©ï¼Œè¦æ±‚é»žæ“Šé€£çµç¢ºèªå¸³æˆ¶å®‰å…¨ã€‚        *   ã€ŒåŒ…è£¹å¾…é ˜ã€ï¼š å‡å†’ç‰©æµå…¬å¸åç¾©ï¼Œè¦æ±‚é»žæ“Šé€£çµé ˜å–åŒ…è£¹ã€‚    *   æé†’ï¼š ä¸è¦è¼•æ˜“é»žæ“Šä¸æ˜Žé€£çµæˆ–ä¸‹è¼‰é™„ä»¶ï¼Œä¿è­·å€‹äººè³‡è¨Šï¼Œå®šæœŸæ›´æ–°å¯†ç¢¼ã€‚ä¸‰ã€é é˜²èˆ‡æ‡‰å° (15åˆ†é˜)*   ä¿è­·å€‹äººè³‡è¨Šï¼š    *   ä¸éš¨æ„é€éœ²å€‹äººè³‡è¨Šï¼ˆå¦‚ï¼šç”Ÿæ—¥ã€åœ°å€ã€é›»è©±ã€éŠ€è¡Œè³‡æ–™ï¼‰ã€‚    *   ä½¿ç”¨è¤‡é›œä¸”ç¨ç‰¹çš„å¯†ç¢¼ã€‚    *   å®šæœŸæ›´æ–°å¯†ç¢¼ã€‚    *   å•Ÿç”¨é›™é‡é©—è­‰ï¼ˆ2FAï¼‰ã€‚*   è¾¨è­˜è©é¨™å¾µå…†ï¼š    *   ä¸è¼•ä¿¡é«˜ç²åˆ©ã€ä½Žé¢¨éšªçš„æŠ•è³‡ã€‚    *   ä¸éš¨æ„ç›¸ä¿¡é™Œç”Ÿäººçš„è«‹æ±‚ã€‚    *   ä¸é»žæ“Šä¸æ˜Žé€£çµæˆ–ä¸‹è¼‰é™„ä»¶ã€‚    *   ä¿æŒæ‡·ç–‘æ…‹åº¦ã€‚*   æ‡‰å°è©é¨™ï¼š    *   ä¸è¦å›žæ‡‰å¯ç–‘éƒµä»¶æˆ–è¨Šæ¯ã€‚    *   å‘ç›¸é—œå–®ä½æª¢èˆ‰ï¼ˆ165åè©é¨™å°ˆç·šã€110å ±æ¡ˆï¼‰ã€‚    *   å‘è¦ªå‹æ±‚åŠ©ã€‚    *   ä¿ç•™è­‰æ“šï¼ˆè©é¨™éƒµä»¶ã€è¨Šæ¯ã€å°è©±ç´€éŒ„ï¼‰ã€‚å››ã€Q&A (10åˆ†é˜)*   é–‹æ”¾ç¾å ´æå•ï¼Œè§£ç­”è½çœ¾çš„ç–‘å•ã€‚*   æä¾›ç›¸é—œè³‡æºé€£çµï¼ˆ165åè©é¨™å°ˆç·šã€è­¦æ”¿ç½²110å ±æ¡ˆã€æ¶ˆè²»è€…ä¿è­·å®˜ç¶²ç«™ï¼‰ã€‚å‚™è¨»ï¼š*   è¬›åº§æ™‚é–“å¯ä¾å¯¦éš›æƒ…æ³èª¿æ•´ã€‚*   å¯æº–å‚™ä¸€äº›è©é¨™æ¡ˆä¾‹çš„åœ–ç‰‡æˆ–å½±ç‰‡ï¼Œå¢žåŠ è¬›åº§çš„ç”Ÿå‹•æ€§ã€‚*   é¼“å‹µè½çœ¾åˆ†äº«è‡ªèº«ç¶“é©—ï¼Œå¢žåŠ äº’å‹•æ€§ã€‚*   è¬›åº§çµæŸå¾Œï¼Œå¯æä¾›åè©é¨™å®£å°Žå–®æˆ–ç›¸é—œè³‡æ–™ã€‚å¸Œæœ›é€™ä»½è¬›åº§å¤§ç¶±å°æ‚¨æœ‰å¹«åŠ©ï¼\nå¯«ä¿¡\nè¿‘ä¾†ä¸»ç®¡å°æˆ‘ä¸‹ç­å·²è®€ä¸å›žè¨Šæ¯é —æœ‰å¾®è©žï¼Œç”šè‡³åœ¨æ•´å€‹éƒ¨é–€çš„emailä¸²ä¸­æåŠæ­¤äº‹ã€‚è«‹å¹«æˆ‘å¯«ä¸€å°ä¿¡ï¼Œä»¥å§”å©‰ä½†å …å®šçš„èªžæ°£èªªæ˜Žæˆ‘å¸Œæœ›åœ¨ä¸Šç­æ™‚é–“å…§å°ˆå¿ƒå®Œæˆå·¥ä½œï¼Œä¸‹ç­å¾Œå¦æœ‰ç§äººè¦äº‹ï¼Œå¯¦åœ¨ä¸ä¾¿è™•ç†å·¥ä½œäº‹å‹™ï¼Œç„¶è‹¥æ˜¯å¯ä»¥ç«‹å³å›žç­”çš„å•é¡Œï¼Œæœƒç›¡å¯èƒ½å³æ™‚å›žè¦†ã€‚æ•´å°ä¿¡çš„èªžæ°£å¿…é ˆå°ˆæ¥­æœ‰ç¦®è²Œï¼Œä¸”å¿…é ˆè€ƒæ…®éƒ¨é–€ä¸­å…¶ä»–åŒäº‹ä¹Ÿæœƒçœ‹åˆ°ã€‚\nä¸»æ—¨ï¼šé—œæ–¼ä¸‹ç­æ™‚é–“å›žè¦†å·¥ä½œè¨Šæ¯ä¸€äº‹æ•¬æ„›çš„ [ä¸»ç®¡çš„åå­—] æ‚¨å¥½ï¼šæ„Ÿè¬æ‚¨å°æˆ‘å·¥ä½œçš„æŒ‡å°Žèˆ‡é—œå¿ƒã€‚è¿‘æ—¥æ”¶åˆ°æ‚¨å°æ–¼æœ¬äººä¸‹ç­æ™‚é–“å¾Œå›žè¦†å·¥ä½œè¨Šæ¯ä¸€äº‹çš„æé†’ï¼Œæˆ‘ç†è§£æ‚¨å¸Œæœ›æˆ‘å€‘èƒ½ä¿æŒé«˜åº¦è­¦è¦ºæ€§èˆ‡å³æ™‚æ€§çš„å·¥ä½œæ…‹åº¦ã€‚æˆ‘å‘ä¾†ä»¥å·¥ä½œç‚ºé‡ï¼Œä¸¦ç›¡åŠ›åœ¨ä¸Šç­æ™‚é–“å…§é«˜æ•ˆå®Œæˆæ‰€æœ‰ä»»å‹™ã€‚ç„¶è€Œï¼Œä¸‹ç­å¾Œæˆ‘å¦æœ‰ç§äººè¦äº‹éœ€è¦è™•ç†ï¼Œæ™‚é–“å®‰æŽ’ä¸Šå¯¦åœ¨æœ‰æ‰€é™åˆ¶ã€‚æˆ‘æ·±çŸ¥å³æ™‚å›žè¦†å·¥ä½œè¨Šæ¯çš„é‡è¦æ€§ï¼Œå› æ­¤ï¼Œå°æ–¼ä¸Šç­æ™‚é–“å…§æ”¶åˆ°çš„å·¥ä½œè¨Šæ¯ï¼Œæˆ‘æœƒç›¡åŠ›è¿…é€Ÿä¸”å¦¥å–„åœ°å›žè¦†ã€‚è‹¥è¨Šæ¯å…§å®¹æ¶‰åŠéœ€è¦ç«‹å³è™•ç†çš„äº‹é …ï¼Œæˆ‘å®šæœƒç›¡å¿«è™•ç†ã€‚æœªä¾†ï¼Œæˆ‘æœƒæ›´åŠ æ³¨æ„å·¥ä½œæ™‚é–“çš„åˆ†é…ï¼Œä¸¦ç›¡åŠ›åœ¨ä¸Šç­æ™‚é–“å…§å®Œæˆæ‰€æœ‰å·¥ä½œã€‚åŒæ™‚ï¼Œä¹Ÿè«‹æ‚¨ç†è§£æˆ‘åœ¨ä¸‹ç­å¾Œçš„æ™‚é–“å®‰æŽ’ï¼Œæˆ‘ä»æœƒä»¥è² è²¬ä»»çš„æ…‹åº¦é¢å°æˆ‘çš„å·¥ä½œã€‚å†æ¬¡æ„Ÿè¬æ‚¨å°æˆ‘çš„æŒ‡å°Žï¼Œæˆ‘å°‡æŒçºŒåŠªåŠ›ï¼Œç‚ºåœ˜éšŠè²¢ç»æˆ‘çš„åŠ›é‡ã€‚æ•¬ç¥å·¥ä½œé †å¿ƒ[ä½ çš„åå­—][ä½ çš„è·ç¨±][ä½ çš„éƒ¨é–€][ä½ çš„è¯çµ¡æ–¹å¼]---æ³¨æ„äº‹é …ï¼š*   è«‹å°‡ [ä¸»ç®¡çš„åå­—] æ›¿æ›ç‚ºå¯¦éš›ä¸»ç®¡çš„åå­—ã€‚*   è«‹ç¢ºèªä½ çš„è·ç¨±ã€éƒ¨é–€å’Œè¯çµ¡æ–¹å¼æ˜¯å¦æ­£ç¢ºã€‚*   é€™å°ä¿¡çš„èªžæ°£æ˜¯å§”å©‰ä½†å …å®šçš„ï¼Œè¡¨é”äº†ä½ çš„ç«‹å ´ï¼ŒåŒæ™‚ä¹Ÿå±•ç¾äº†ä½ å°å·¥ä½œçš„å°Šé‡å’Œæ‰¿è«¾ã€‚*   å¦‚æžœä½ è¦ºå¾—é€™å°ä¿¡çš„èªžæ°£å¤ªéŽå¼·ç¡¬ï¼Œå¯ä»¥å†ç¨å¾®ä¿®æ”¹ï¼Œä½¿å…¶æ›´æŸ”å’Œä¸€äº›ã€‚*   å¦‚æžœä¸»ç®¡çš„å¾®è©žæ˜¯å‡ºæ–¼èª¤æœƒï¼Œä½ å¯ä»¥åœ¨ä¿¡ä¸­åŠ å…¥ä¸€äº›è§£é‡‹ï¼Œä¾‹å¦‚ä½ éŽåŽ»æœ‰ç›¡åŠ›å›žè¦†è¨Šæ¯ï¼Œæˆ–ä½ éŽåŽ»æœ‰è™•ç†éŽç·Šæ€¥çš„å·¥ä½œç¶“é©—ã€‚\næ‘˜è¦\nè«‹å°‡é€™ç¯‡æ–‡ç« ç²¾ç°¡æ¢ç†åŒ–:ã€Œç”¢æ¥­å‰µæ–°æ¢ä¾‹ç¬¬10æ¢ä¹‹2åŠç¬¬72æ¢æ¢æ–‡ä¿®æ­£æ¡ˆã€ä¿—ç¨±ã€Œå°ç‰ˆæ™¶ç‰‡æ³•ã€,é‡å°åŠå°Žé«”ã€é›»å‹•è»Šã€5Gç­‰æŠ€è¡“å‰µæ–°ä¸”å±…åœ‹éš›ä¾›æ‡‰éˆé—œéµåœ°ä½å…¬å¸,æä¾›æœ€é«˜25%ç‡Ÿæ‰€ç¨…æŠ•æŠµå„ªæƒ ,ä¼æ¥­é©ç”¨è¦ä»¶åŒ…å«ç•¶å¹´åº¦ç ”ç™¼è²»ç”¨ã€ç ”ç™¼å¯†åº¦é”ä¸€å®šè¦æ¨¡,ä¸”æœ‰æ•ˆç¨…çŽ‡é”ä¸€å®šæ¯”çŽ‡ã€‚ç‚ºå› æ‡‰ç¶“æ¿Ÿåˆä½œæš¨ç™¼å±•çµ„ç¹”(OECD)åœ‹å®¶æœ€ä½Žç¨…è² åˆ¶èª¿æ•´,å…¶ä¸­æœ‰æ•ˆç¨…çŽ‡é–€æª»,æ°‘åœ‹112å¹´è¨‚ç‚º12%,113å¹´æ–™å°‡æé«˜è‡³15%,ä½†ä»å¾—å¯©é…Œåœ‹éš›é–“æœ€ä½Žç¨…è² åˆ¶å¯¦æ–½æƒ…å½¢ã€‚ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,å·²å’Œè²¡æ”¿éƒ¨å”å•†é€²å…¥æœ€å¾ŒéšŽæ®µ,é™¤ä¼æ¥­ç ”ç™¼å¯†åº¦è¨‚åœ¨6%,ç›®å‰å·²ç¢ºèª,ä¼æ¥­è³¼ç½®å…ˆé€²è£½ç¨‹çš„è¨­å‚™æŠ•è³‡é‡‘é¡é”100å„„å…ƒä»¥ä¸Šå¯æŠµæ¸›ã€‚è²¡æ”¿éƒ¨å®˜å“¡è¡¨ç¤º,ç ”å•†éŽç¨‹ä¸­,é‡å°å°ç£ç”¢æ¥­èˆ‡å…¶åœ¨åœ‹éš›é–“é¡žä¼¼çš„å…¬å¸é€²è¡Œæ·±å…¥ç ”ç©¶,åœ¨è¨­å‚™éƒ¨åˆ†,ç•¢ç«Ÿé©ç”¨ç”¢å‰µ10ä¹‹2çš„æ¥­è€…æ˜¯ä»£è¡¨å°ç£éšŠæ‰“ã€Œåœ‹éš›ç›ƒã€,æŠ•å…¥é‡‘é¡ä¸é”100å„„å…ƒ,å¯èƒ½ä¹Ÿæ‰“ä¸äº†ã€‚è‡³æ–¼å‚™å—é—œæ³¨çš„ç ”ç™¼è²»ç”¨é–€æª»,ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,æ­·ç¶“èˆ‡è²¡æ”¿éƒ¨ä¾†å›žå¯†åˆ‡è¨Žè«–,ç ”ç™¼è²»ç”¨é–€æª»æœ‰æœ›è½åœ¨60å„„è‡³70å„„å…ƒä¹‹é–“ã€‚è²¡æ”¿éƒ¨å®˜å“¡æŒ‡å‡º,ç ”ç™¼æ”¸é—œå°ç£æœªä¾†ç¶“æ¿Ÿæˆé•·å‹•èƒ½,é–€æª»ä¸èƒ½ã€Œé«˜ä¸å¯æ”€ã€,èµ·åˆé›–è¨­å®šåœ¨100å„„å…ƒ,ä¹‹æ‰€ä»¥æœƒèª¿é™,æ­£æ˜¯ç›¼è®“ä¼æ¥­è¦ºå¾—æœ‰è¾¦æ³•é”å¾—åˆ°é–€æª»ã€é€²è€Œé©ç”¨ç§Ÿç¨…å„ªæƒ ,æ‰æœ‰å‹•åŠ›ç¹¼çºŒæŠ•å…¥ç ”ç™¼,ç¶­æŒåœ‹éš›ä¾›æ‡‰éˆé—œéµåœ°ä½ã€‚ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,å› å» å•†ç ”ç™¼è²»ç”¨å¹³å‡ç‚º30ã€40å„„å…ƒ,å…¶ä¸­,ICè¨­è¨ˆæ¥­è€…ä»‹æ–¼30å„„è‡³60å„„å…ƒç¯„åœ,è‹¥å°‡é–€æª»è¨‚åœ¨100å„„å…ƒ,ç¬¦åˆæ¢ä»¶çš„æ¥­è€…è¼ƒå°‘ã€åˆºæ¿€èª˜å› ä¸è¶³;æ­¤å¤–,è‹¥ç¬¦åˆç”³è«‹é–€æª»çš„æ¥­è€…å¢žåŠ ,å°‡å¯æé«˜ä¼æ¥­åœ¨å°æŠ•è³‡é‡‘é¡,è²¡æ”¿éƒ¨ç¨…æ”¶ä¹Ÿèƒ½å› æ­¤ç²å¾—æŒ¹æ³¨ã€‚ICè¨­è¨ˆæ¥­è€…è¿‘æ—¥é »é »é‡å°ç”¢å‰µ10ä¹‹2ç™¼è²,å¸Œæœ›é™ä½Žé©ç”¨é–€æª»,åŠ ä¸Šå„åœ‹åŠ›æ‹šä¾›æ‡‰éˆè‡ªä¸»åŒ–ã€åŠ ç¢¼è£œåŠ©åŠå°Žé«”ç”¢æ¥­,ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,ç¶“æ¿Ÿéƒ¨å’Œè²¡æ”¿éƒ¨å°±ç”¢å‰µ10ä¹‹2é”æˆå…±è­˜,çˆ­å–è®“æ›´å¤šæ¥­è€…å—æƒ ,ç›¼å¢žå¼·ä¼æ¥­æŠ•è³‡åŠ›é“åŠéžå›ºå°ç£æŠ€è¡“åœ°ä½ã€‚è²¡æ”¿éƒ¨å®˜å“¡è¡¨ç¤º,ç§Ÿç¨…çŽå‹µçš„åˆ¶å®šå¿…é ˆã€Œæœ‰ç‚ºæœ‰å®ˆã€,ä¸¦ä»¥é”åˆ°çŽå‹µè¨­ç½®ç›®çš„ç‚ºæœ€é«˜åŽŸå‰‡,ç¾éšŽæ®µåœ¨æ‰“ã€Œåœ‹å…§ç›ƒã€çš„ä¼æ¥­ä»å¯é©ç”¨ç”¢å‰µç¬¬10æ¢ã€10ä¹‹1çš„ç§Ÿç¨…å„ªæƒ ,å…±åŒå£¯å¤§å°ç£ç¶“æ¿Ÿç™¼å±•ã€‚ç¶“æ¿Ÿéƒ¨å’Œè²¡æ”¿éƒ¨æ­£å°±ç ”ç™¼è²»ç”¨é–€æª»åšæœ€å¾Œç¢ºèª,å¾…ä»Šæ˜Žå…©å¤©é å‘Šå­æ³•ä¹‹å¾Œ,ç´„æœ‰30å¤©æ™‚é–“,å¯èˆ‡æ¥­ç•Œé€²ä¸€æ­¥è¨Žè«–åŠèª¿æ•´,ç›¼ç”¢å‰µ10ä¹‹2èƒ½åœ¨6æœˆä¸Šè·¯ã€‚\n---å°ç‰ˆæ™¶ç‰‡æ³• - ç”¢æ¥­å‰µæ–°æ¢ä¾‹ç¬¬10æ¢ä¹‹2åŠç¬¬72æ¢ä¿®æ­£æ¡ˆ ç²¾ç°¡ç‰ˆä¸€ã€ å„ªæƒ å…§å®¹ï¼š* é‡å°åŠå°Žé«”ã€é›»å‹•è»Šã€5G ç­‰ç”¢æ¥­çš„åœ‹éš›é—œéµä¼æ¥­ï¼Œæä¾›æœ€é«˜ 25% ç‡Ÿæ‰€ç¨…æŠ•æŠµã€‚äºŒã€ ä¼æ¥­é©ç”¨è³‡æ ¼ï¼š* ç ”ç™¼è²»ç”¨ï¼š ç ”ç™¼è²»ç”¨é–€æª»æ“¬è½åœ¨ 60-70 å„„å…ƒä¹‹é–“ã€‚* ç ”ç™¼å¯†åº¦ï¼š ç ”ç™¼è²»ç”¨å ç‡Ÿæ”¶æ¯”çŽ‡è‡³å°‘ 6%ã€‚* æœ‰æ•ˆç¨…çŽ‡ï¼š    * 112 å¹´ï¼š12%    * 113 å¹´ (å¯èƒ½) 15%    * èª¿æ•´å¾Œ,å°‡é…åˆ OECD æœ€ä½Žç¨…è² åˆ¶èª¿æ•´ã€‚* è¨­å‚™æŠ•è³‡ï¼š è³¼ç½®å…ˆé€²è£½ç¨‹è¨­å‚™æŠ•è³‡é” 100 å„„å…ƒä»¥ä¸Šè€…ï¼Œå¯ç”³è«‹æŠµæ¸›ã€‚ä¸‰ã€ æ”¿ç­–èƒŒæ™¯èˆ‡ç›®çš„ï¼š* é…åˆ OECD å…¨çƒæœ€ä½Žç¨…è² åˆ¶ï¼Œèª¿æ•´æœ‰æ•ˆç¨…çŽ‡é–€æª»ã€‚* é¼“å‹µä¼æ¥­æŒçºŒæŠ•å…¥ç ”ç™¼ï¼Œç¶­æŒå°ç£æŠ€è¡“å„ªå‹¢åŠåœ‹éš›ç«¶çˆ­åŠ›ã€‚* ç›¼å¸å¼•æ›´å¤šä¼æ¥­æŠ•è³‡å°ç£ï¼Œå¢žåŠ åœ‹å®¶ç¨…æ”¶ã€‚å››ã€ æ™‚ç¨‹ï¼š* é è¨ˆ 6 æœˆä¸Šè·¯ï¼Œè‰æ¡ˆé å‘Šå¾Œé€²è¡Œ 30 å¤©çš„æ¥­ç•Œæ„è¦‹å¾µè©¢æœŸã€‚äº”ã€ æ”¿ç­–ç«‹å ´ï¼š* ç¶“æ¿Ÿéƒ¨èˆ‡è²¡æ”¿éƒ¨ï¼š å…±åŒå”å•†ï¼ŒåŠ›æ±‚ä¼æ¥­åœ¨å¯æŽ§åˆ¶åˆç†ç¯„åœå…§èƒ½ç²å¾—ç§Ÿç¨…å„ªæƒ ï¼Œä»¥é¼“å‹µä¼æ¥­ç™¼å±•ã€‚* ç§Ÿç¨…çŽå‹µè€ƒé‡ã€Œæœ‰ç‚ºæœ‰å®ˆã€ï¼Œè¨‚å®šé©åˆ‡é–€æª»ï¼Œä¿ƒé€²å°ç£ç¶“æ¿Ÿç™¼å±•ã€‚\nç¿»è­¯\nä»¥ä¸‹æä¾›è‹±æ–‡å…§å®¹ï¼Œè«‹å¹«æˆ‘ç¿»è­¯æˆä¸­æ–‡ã€‚Dongshan coffee is famous for its unique position, and the constant refinement of production methods. The flavor is admired by many caffeine afficionados.\næ±å±±å’–å•¡ä»¥å…¶ç¨ç‰¹çš„ä½ç½®å’ŒæŒçºŒæ”¹è‰¯çš„è£½ç¨‹è€Œèžåï¼Œå…¶é¢¨å‘³è¢«è¨±å¤šå’–å•¡æ„›å¥½è€…æ‰€æŽ¨å´‡ã€‚\nä½¿ç”¨èªªæ˜Ž\nç¯„ä¾‹ç¨‹å¼\nprompt æ¨£ç‰ˆ\nä¸€èˆ¬å•ç­”ç”¨æ³•chat = [\n{\"role\": \"user\", \"content\": \"{question}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nå°‡ {question} æ›¿æ›æˆä½¿ç”¨è€…çš„è¼¸å…¥\nåŠ å…¥ system prompt çš„ç”¨æ³•chat = [\n{\"role\": \"system\", \"content\": \"{sys}\"},\n{\"role\": \"user\", \"content\": \"{question}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nå°‡ {sys} æ›¿æ›æˆæŒ‡ä»¤ã€‚\nå°‡ {question} æ›¿æ›æˆä½¿ç”¨è€…çš„å•é¡Œ\nå¤šè¼ªå•ç­”ç”¨æ³•chat = [\n{\"role\": \"system\", \"content\": \"{sys}\"},\n{\"role\": \"user\", \"content\": \"{question1}\"},\n{\"role\": \"assistant\", \"content\": \"{model_anwer_1}\"},\n{\"role\": \"user\", \"content\": \"{question2}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nå°‡ {sys} æ›¿æ›æˆæŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ä¸€å€‹ä¾†è‡ªå°ç£çš„AIåŠ©ç†ï¼Œä½ çš„åå­—æ˜¯ TAIDEï¼Œæ¨‚æ–¼ä»¥å°ç£äººçš„ç«‹å ´å¹«åŠ©ä½¿ç”¨è€…ï¼Œæœƒç”¨æ­£é«”ä¸­æ–‡å›žç­”å•é¡Œã€‚\nå°‡ {question1} æ›¿æ›æˆä½¿ç”¨è€…çš„å•é¡Œ1\nå°‡ {model_anwer_1} æ›¿æ›æˆæ¨¡åž‹çš„å›žç­”1\nå°‡ {question2} æ›¿æ›æˆä½¿ç”¨è€…çš„å•é¡Œ2\næ›´å¤šç´°ç¯€è«‹åƒè€ƒGemma 3 Model Card\nè¨“ç·´æ–¹æ³•\nè»Ÿç¡¬é«”è¦æ ¼\nåœ‹ç¶²ä¸­å¿ƒ H200\nè¨“ç·´æ¡†æž¶: PyTorch\nè³‡æ–™å‰è™•ç†\nå­—å…ƒæ¨™æº–åŒ–\nåŽ»é™¤é‡è¦†\nåŽ»é™¤é›œè¨Š\nç¶²é è³‡æ–™çš„html tagã€javascript\néžæ¨™æº–å­—å…ƒæˆ–äº‚ç¢¼\nå­—æ•¸éŽçŸ­çš„æ–‡ç« \nåŽ»é™¤æ–‡ç« ä¸­çš„ç‰¹å®šæ ¼å¼ï¼Œå¦‚ç‚ºæŽ’ç‰ˆå¢žåŠ çš„æ›è¡Œ\nåŽ»é™¤å€‹è³‡ï¼Œå¦‚emailã€é›»è©±\nåŽ»é™¤ä¸ç•¶æ–‡å­—ï¼Œå¦‚è³­åšã€è‰²æƒ…ç­‰\næŒçºŒé è¨“ç·´ (continual pretraining, CP)\nè£œå……å¤§é‡ä¾†æºå¯ä¿¡è³´çš„æ­£é«”ä¸­æ–‡çŸ¥è­˜\nè¶…åƒæ•¸ (hyper parameters)\noptimizer: FusedAdam\nlearning rate: 1e-4\nbatch size: 4M tokens\nepoch: 1\nå¾®èª¿ (fine-tuning, FT)\nè®“æ¨¡åž‹å¯é‡å°æ­£é«”ä¸­æ–‡æå•å›žç­”å•é¡Œ\nè¶…åƒæ•¸ (hyper parameters)\noptimizer: FusedAdam\nlearning rate: 3e-6\nbatch size: 4M tokens\nepoch: 5\nè¨“ç·´è³‡æ–™\næŒçºŒé è¨“ç·´è³‡æ–™(è³‡æ–™é‡ç´„ç‚º240G)\nè³‡æ–™é›†\nè³‡æ–™æè¿°\nè¨´è¨Ÿè³‡æ–™\nã€Šå¸æ³•é™¢è£åˆ¤æ›¸ã€‹è‡ª2013å¹´1æœˆè‡³2023å¹´12æœˆå„ç´šæ³•é™¢æ°‘äº‹ã€åˆ‘äº‹ã€è¡Œæ”¿è¨´è¨Ÿè³‡æ–™ã€‚\nä¸­å¤®ç¤¾\nã€Šä¸­å¤®ç¤¾ä¸­æ–‡æ–°èžã€‹è³‡æ–™é›†å«ä¸­å¤®ç¤¾è‡ª1993å¹´6æœˆè‡³2024å¹´12æœˆï¼Œå…±31.5å¹´ä»½ä¹‹æ¯æ—¥æ–°èžæ–‡ç« ã€‚å…§å®¹æ¶µè“‹åœ‹å…§å¤–æ”¿æ²»ã€ç¤¾æœƒã€è²¡ç¶“ã€æ–‡æ•™ã€ç”Ÿæ´»ç­‰é ˜åŸŸã€‚\nETtoday æ–°èžé›²\nã€ŠETtodayæ–°èžé›²ã€‹è³‡æ–™ï¼ŒåŒ…å«è‡ª2011å¹´10æœˆè‡³2024å¹´11æœˆçš„è³‡æ–™ã€‚\nç«‹æ³•é™¢å…¬å ±\nã€Šç«‹æ³•é™¢å…¬å ±ã€‹åŒ…å«è‡ªç¬¬8å±†ç¬¬1æœƒæœŸè‡³ç¬¬10å±†ç¬¬7æœƒæœŸä¹‹å…¬å ±è³‡æ–™ã€‚\nå‡ºç‰ˆå•†ç¶²ç«™æ›¸ç±ä»‹ç´¹\nåŒ…å«ä¸‰é‡‡ã€Gotopå‡ºç‰ˆå•†ç¶²ç«™ä¸Šçš„æ›¸ç±ç°¡ä»‹ã€‚\nGRB ç ”ç©¶è¨ˆç•«æ‘˜è¦\nGRBç‚ºæ”¶éŒ„ç”±æ”¿åºœç¶“è²»è£œåŠ©ä¹‹ç ”ç©¶è¨ˆç•«åŠå…¶æˆæžœå ±å‘Šçš„è³‡è¨Šç³»çµ±ï¼Œæ­¤è³‡æ–™é›†ä¸»è¦æ”¶éŒ„ 1993å¹´è‡³ 2023å¹´ä¹‹ç ”ç©¶è¨ˆç•«æ‘˜è¦ä»¥åŠç ”ç©¶å ±å‘Šæ‘˜è¦ï¼Œå«ä¸­æ–‡åŠå…¶è‹±æ–‡å°ç…§ã€‚\nå­¸è¡“æœƒè­°è«–æ–‡æ‘˜è¦\næ”¶éŒ„ã€Šå­¸è¡“æœƒè­°è«–æ–‡æ‘˜è¦è³‡æ–™åº«ã€‹ä¸­è‡ª1988è‡³2009å¹´ç”±å°ç£æ‰€èˆ‰è¾¦ä¹‹å­¸è¡“æœƒè­°è«–æ–‡ã€‚\nå…‰è¯é›œèªŒ\nã€Šå°ç£å…‰è¯é›œèªŒã€‹å«è‡ª1993å¹´7æœˆè‡³2023å¹´6æœˆçš„æ–‡ç« ï¼Œå…±30å¹´ä»½ã€‚å…§å®¹è‘—é‡æ–¼æˆ‘åœ‹æ–‡åŒ–ã€è§€å…‰èˆ‡æ°‘æƒ…ç­‰ã€‚\næ¨‚è©žç¶²\nã€Šæ¨‚è©žç¶²ã€‹æ¶µè“‹æ–‡ç†é ˜åŸŸç´„187è¬å‰‡å­¸è¡“åè©žåŠå…¶è­¯åå°ç…§ã€‚\nå„éƒ¨æœƒè³‡æ–™\nåŒ…å«è¡Œæ”¿é™¢ã€Œåœ‹æƒ…ç°¡ä»‹ã€ã€æ–‡åŒ–éƒ¨ã€Œåœ‹å®¶æ–‡åŒ–è¨˜æ†¶åº«ã€ã€åœ‹ç™¼æœƒã€Œæª”æ¡ˆæ”¯æ´æ•™å­¸ç¶²ã€ã€äº¤é€šéƒ¨ã€Œäº¤é€šå®‰å…¨å…¥å£ç¶²ã€ç­‰éƒ¨æœƒç¶²ç«™è³‡æ–™ä¹‹éƒ¨åˆ†è³‡æ–™ã€‚\nä»Šå‘¨åˆŠ\nã€Šä»Šå‘¨åˆŠã€‹ç‚ºä¸€ä»¥è²¡ç¶“ç‚ºä¸»çš„é€±åˆŠé›œèªŒï¼Œæ­¤è³‡æ–™é›†æ¶µè“‹2008å¹´1æœˆè‡³2023å¹´7æœˆçš„æ–‡ç« ã€‚\næ•™è‚²éƒ¨åœ‹èªžè¾­å…¸ã€æˆèªžè¾­å…¸\nåŒ…å«ä»¥ä¸‹ä¸‰é …è³‡æ–™:æ•™è‚²éƒ¨ã€Šæˆèªžå…¸ã€‹ï¼Œå«5,338æ¢æˆèªžï¼Œå…§å®¹åŒ…å«æ¯æ¢æˆèªžçš„é‡‹ç¾©ã€å…¸æ•…åŽŸæ–‡åŠå…¶ç™½è©±èªªæ˜Žã€ç”¨æ³•èªªæ˜Žã€ä¾‹å¥ç­‰ã€‚æ•™è‚²éƒ¨ã€Šé‡ç·¨åœ‹èªžè¾­å…¸ä¿®è¨‚æœ¬ã€‹ï¼Œæ”¶éŒ„ä¸­æ–‡å–®å­—åŠå„é¡žè¾­å½™ï¼ŒåŒ…å«è®€éŸ³ã€éƒ¨é¦–ã€é‡‹ç¾©ç­‰è³‡è¨Šï¼Œå…±ç´„165,539ç­†è³‡æ–™ã€‚æ•™è‚²éƒ¨ã€Šåœ‹èªžè¾­å…¸ç°¡ç·¨æœ¬ã€‹ï¼Œç‚ºã€Šé‡ç·¨åœ‹èªžè¾­å…¸ä¿®è¨‚æœ¬ã€‹çš„ç°¡ç·¨ç‰ˆæœ¬ï¼Œå…±45,247ç­†è³‡æ–™ã€‚\nç§‘æŠ€å¤§è§€åœ’è³‡æ–™\nå«ã€Šç§‘æŠ€å¤§è§€åœ’ç¶²ç«™ã€‹ä¸Šçš„ç§‘å­¸æ–°çŸ¥ä»¥åŠç§‘æ™®æ–‡ç« ã€‚\niKnow ç§‘æŠ€ç”¢æ¥­è³‡è¨Šå®¤\nã€Šç§‘æŠ€ç”¢æ¥­è³‡è¨Šå®¤ï¼ˆiKnowï¼‰ã€‹æä¾›å°ç£åŠå…¨çƒçš„ç§‘æŠ€å¸‚å ´è¶¨å‹¢ã€ç­–ç•¥åˆ†æžã€å°ˆåˆ©çŸ¥è­˜ï¼ŒåŠæŠ€è¡“äº¤æ˜“è³‡è¨Šï¼Œå°ˆæ³¨æ–¼ç§‘æŠ€ç”¢æ¥­çš„å‰µæ–°èˆ‡ç™¼å±•ï¼ŒåŒ…å«è‡ª 2008 å¹´è‡³ 2023 å¹´ã€‚\nç§‘å­¸ç™¼å±•æœˆåˆŠ\nã€Šç§‘å­¸ç™¼å±•æœˆåˆŠã€‹ç‚ºåœ‹ç§‘æœƒç‚ºæŽ¨å»£ç§‘å­¸æ•™è‚²è€Œå‡ºç‰ˆçš„ç§‘æ™®åˆŠç‰©ï¼Œå«è‡ª2004å¹´10æœˆè‡³2020å¹´12æœˆä¹‹ç§‘æ™®æ–‡ç« ï¼›2021å¹´èµ·ï¼Œä»¥ã€Šç§‘æŠ€é­…ç™®ã€‹å­£åˆŠé‡æ–°å‡ºç™¼ï¼Œæä¾›åœ‹éš›é—œæ³¨ç§‘æŠ€è­°é¡Œçš„æ–°çŸ¥æ–‡ç« ã€‚\næ³•è¦è³‡æ–™åº«\nã€Šæ³•è¦è³‡æ–™åº«ã€‹å«æˆªè‡ª 112 å¹´ 10 æœˆå„æ”¿åºœéƒ¨é–€æœ€æ–°ç™¼å¸ƒä¹‹ä¸­å¤®æ³•è¦ã€ï¨ˆæ”¿è¦å‰‡ã€æ³•è¦å‘½ï¦¨è‰æ¡ˆåŠåœ°æ–¹è‡ªæ²»æ³•è¦ç­‰ã€‚\nå„åœ°æ”¿åºœæ—…éŠç¶²\næ¶µè“‹å°ç£éƒ¨åˆ†ç¸£å¸‚åœ°æ–¹æ”¿åºœè§€å…‰æ—…éŠç¶²ç«™ä¸Šä¹‹éƒ¨åˆ†è³‡æ–™ã€‚\nåœ‹æ•™é™¢èª²ç¨‹ç¶±è¦(åäºŒå¹´åœ‹æ•™)\nå«åäºŒå¹´åœ‹æ•™èª²ç¨‹ç¶±è¦ä¹‹ç¸½ç¶±ä»¥åŠå„ç´šå­¸æ ¡ä¸åŒç§‘ç›®ä¹‹èª²ç¨‹ç¶±è¦ã€‚\nä¸­å¤®ç¤¾è­¯åæª”è³‡æ–™åº«\nã€Šä¸­å¤®ç¤¾è­¯åæª”è³‡æ–™åº«ã€‹è’é›†ä¸­å¤®ç¤¾æ–°èžæ¥­å‹™ä¸Šç¿»è­¯éŽçš„ä¸­å¤–å§“æ°ã€äººåã€çµ„ç¹”ã€åœ°åç­‰è­¯åå°ç…§ã€‚\nRedPajama-Data-V2\nå¾žåœ‹å¤–é–‹æ”¾å¤šåœ‹èªžè¨€èªžæ–™åº« RedPajama-Data-v2 å–å‡ºè‹±æ–‡è³‡æ–™\nMathPile-commercial\nåœ‹å¤–é–‹æ”¾æ•¸å­¸èªžæ–™åº« MathPile-commercial\nä¸­æ–‡ç¶­åŸºç™¾ç§‘\nã€Šä¸­æ–‡ç¶­åŸºç™¾ç§‘ã€‹æˆªè‡³2023å¹´1æœˆæ‰€æœ‰æ¢ç›®çš„å…§å®¹ã€‚\ngithub-code-clean\nç‚º github é–‹æºç¨‹å¼ç¢¼è³‡æ–™é›†ï¼ŒåŽ»é™¤unlicenseçš„ç¨‹å¼ç¢¼å’Œæ–‡ä»¶ã€‚\nå¾®èª¿è³‡æ–™\nTAIDEåœ˜éšŠè¨“ç·´Llama3åŠGemma-3ç³»åˆ—æ¨¡åž‹ä¾†ç”¢ç”Ÿå¾®èª¿è³‡æ–™ï¼Œç”¢ç”Ÿçš„ä»»å‹™åŒ…å«ä¸–ç•ŒçŸ¥è­˜ã€å‰µæ„å¯«ä½œã€æ™®é€šå¸¸è­˜ã€ç¿»è­¯ã€æ‘˜è¦ã€ç¨‹å¼ã€å°ç£åƒ¹å€¼ç­‰å–®è¼ªæˆ–å¤šè¼ªå°è©±å•ç­”ï¼Œå…± 220K ç­†ã€‚\næ¨¡åž‹è©•æ¸¬\nTMMLU +\nè©•æ¸¬è³‡æ–™\nSTEM, Humanities, Social Sciences, Other (Business, Health, Misc.)ï¼Œå…±66å€‹å­é¡žåˆ¥ï¼Œ20,118 é¡Œã€‚\nè©•æ¸¬æ–¹æ³•\nè©•åˆ†ç¨‹å¼: lm-evaluation-harness\nè©•æ¸¬åˆ†æ•¸\næ¨¡åž‹\nSTEM\nHumanities\nSocial Sciences\nOther\nå¹³å‡\nGemma-3-TAIDE-12b-Chat\n48.00\n47.59\n58.19\n45.68\n49.95\nGemma-3-12b-it\n56.00\n44.41\n60.52\n50.36\n53.82\nLlama-3.1-TAIDE-LX-8B-Chat\n42.11\n46.23\n50.96\n43.22\n45.58\nç”±æ–¼ Gemma-3-TAIDE-12b-Chat æœ‰ç‰¹åˆ¥åŠ å¼·å°ç£ç›¸é—œçŸ¥è­˜ï¼Œæ•…åŒæ™‚åƒè€ƒ TMMLU+ çš„å°ç£ç›¸é—œæŒ‡æ¨™ ï¼ˆå°ç£åœ°ç†ã€å°ç£ç”¨èªžã€é–©å—èªžï¼‰ã€‚\næ¨¡åž‹\nå°ç£åœ°ç†\nå°ç£ç”¨èªž\né–©å—èªž\nGemma-3-TAIDE-12b-Chat\n69.14\n80.53\n38.76\nGemma-3-12b-it\n60.93\n71.68\n28.68\nLlama-3.1-TAIDE-LX-8B-Chat\n62.36\n75.10\n31.78\ntaide-bench\nè©•æ¸¬è³‡æ–™\nå¯«æ–‡ç« ã€å¯«ä¿¡ã€æ‘˜è¦ã€è‹±ç¿»ä¸­ã€ä¸­ç¿»è‹±ï¼Œå…±500é¡Œ\nè³‡æ–™é€£çµ: taide-bench\nè©•æ¸¬æ–¹æ³•\ngpt-4oè©•åˆ†\nè©•åˆ†ç¨‹å¼: taide-bench-eval\nè©•æ¸¬åˆ†æ•¸\næ¨¡åž‹\nä¸­ç¿»è‹±\nè‹±ç¿»ä¸­\næ‘˜è¦\nå¯«æ–‡ç« \nå¯«ä¿¡\nå¹³å‡\nGemma-3-TAIDE-12b-Chat\n7.99\n8.46\n7.64\n9.73\n9.57\n8.67\nGemma-3-12b-it\n8.24\n8.44\n7.77\n9.60\n9.46\n8.70\nLlama-3.1-TAIDE-LX-8B-Chat*\n6.99\n7.26\n9.17\n8.80\n7.18\n7.88\n*è¨»ï¼šç”±æ–¼æ›´æ›è©•æ¸¬ judge promptï¼Œæ•… Llama-3.1-TAIDE-LX-8B-Chat çš„ taide-bench è©•æ¸¬åˆ†æ•¸èˆ‡å‰æ¬¡é‡‹å‡ºçš„åˆ†æ•¸ä¸åŒã€‚\næŽˆæ¬Šæ¢æ¬¾\nï¼ˆGemma ç‰ˆæ¬¡ï¼‰-TAIDE æ¨¡åž‹æŽˆæ¬Šæ¢æ¬¾\nå…è²¬è²æ˜Ž\nLLM æ¨¡åž‹ç”±æ–¼è¨­è¨ˆæž¶æ§‹çš„é™åˆ¶ï¼Œä»¥åŠè³‡æ–™é›£å…æœ‰åèª¤ï¼Œèªžè¨€æ¨¡åž‹çš„ä»»ä½•å›žæ‡‰ä¸ä»£è¡¨ TAIDE ç«‹å ´ï¼Œä½¿ç”¨å‰éœ€è¦é¡å¤–åŠ å…¥å®‰å…¨é˜²è­·æ©Ÿåˆ¶ï¼Œä¸”å›žæ‡‰å…§å®¹ä¹Ÿå¯èƒ½åŒ…å«ä¸æ­£ç¢ºçš„è³‡è¨Šï¼Œä½¿ç”¨è€…è«‹å‹¿ç›¡ä¿¡ã€‚\né–‹ç™¼åœ˜éšŠ\nhttps://taide.tw/index/teamList\nç›¸é—œé€£çµ\nTAIDEå®˜ç¶²\nTAIDE Huggingface\nTAIDE Github\nCitation\nTAIDEå®˜ç¶²",
    "onnx-community/embeddinggemma-300m-ONNX": "EmbeddingGemma model card\nModel Information\nDescription\nInputs and outputs\nUsage\nModel Data\nTraining Dataset\nData Preprocessing\nModel Development\nHardware\nSoftware\nEvaluation\nBenchmark Results\nPrompt Instructions\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nEmbeddingGemma model card\nModel Page: EmbeddingGemma\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nEmbeddingGemma on Kaggle\nEmbeddingGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nDescription\nEmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be embedded\nMaximum input context length of 2048 tokens\nOutput:\nNumerical vector representations of input text data\nOutput embedding dimension size of 768, with smaller options available (512, 256, or 128) via Matryoshka Representation Learning (MRL). MRL allows users to truncate the output embedding of size 768 to their desired size and then re-normalize for efficient and accurate representation.\nUsage\nThese model weights are designed to be used with Transformers.js.\nNOTE: EmbeddingGemma activations do not support fp16 or its derivatives. Please use fp32, q8, or q4 as appropriate for your hardware.\nTransformers.js in JavaScript\nimport { AutoModel, AutoTokenizer, matmul } from \"@huggingface/transformers\";\n// Download from the ðŸ¤— Hub\nconst model_id = \"onnx-community/embeddinggemma-300m-ONNX\";\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst model = await AutoModel.from_pretrained(model_id, {\ndtype: \"fp32\", // Options: \"fp32\" | \"q8\" | \"q4\".\n});\n// Run inference with queries and documents\nconst prefixes = {\nquery: \"task: search result | query: \",\ndocument: \"title: none | text: \",\n};\nconst query = prefixes.query + \"Which planet is known as the Red Planet?\";\nconst documents = [\n\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\",\n].map((x) => prefixes.document + x);\nconst inputs = await tokenizer([query, ...documents], { padding: true });\nconst { sentence_embedding } = await model(inputs);\n// Compute similarities to determine a ranking\nconst scores = await matmul(sentence_embedding, sentence_embedding.transpose(1, 0));\nconst similarities = scores.tolist()[0].slice(1);\nconsole.log(similarities);\n// [ 0.30109718441963196, 0.6358831524848938, 0.4930494725704193, 0.48887503147125244 ]\n// Convert similarities to a ranking\nconst ranking = similarities.map((score, index) => ({ index, score })).sort((a, b) => b.score - a.score);\nconsole.log(ranking);\n// [\n//   { index: 1, score: 0.6358831524848938 },\n//   { index: 2, score: 0.4930494725704193 },\n//   { index: 3, score: 0.48887503147125244 },\n//   { index: 0, score: 0.30109718441963196 }\n// ]\nUsing the ONNX Runtime in Python\nfrom huggingface_hub import hf_hub_download\nimport onnxruntime as ort\nfrom transformers import AutoTokenizer\n# Download from the ðŸ¤— Hub\nmodel_id = \"onnx-community/embeddinggemma-300m-ONNX\"\nmodel_path = hf_hub_download(model_id, subfolder=\"onnx\", filename=\"model.onnx\") # Download graph\nhf_hub_download(model_id, subfolder=\"onnx\", filename=\"model.onnx_data\") # Download weights\nsession = ort.InferenceSession(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Run inference with queries and documents\nprefixes = {\n\"query\": \"task: search result | query: \",\n\"document\": \"title: none | text: \",\n}\nquery = prefixes[\"query\"] + \"Which planet is known as the Red Planet?\"\ndocuments = [\n\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\ndocuments = [prefixes[\"document\"] + x for x in documents]\ninputs = tokenizer([query] + documents, padding=True, return_tensors=\"np\")\n_, sentence_embedding = session.run(None, inputs.data)\nprint(sentence_embedding.shape)  # (5, 768)\n# Compute similarities to determine a ranking\nquery_embeddings = sentence_embedding[0]\ndocument_embeddings = sentence_embedding[1:]\nsimilarities = query_embeddings @ document_embeddings.T\nprint(similarities)  # [0.30109745 0.635883 0.49304956 0.48887485]\n# Convert similarities to a ranking\nranking = similarities.argsort()[::-1]\nprint(ranking)  # [1 2 3 0]\nUsing the ONNX Runtime in Text Embeddings Inference (TEI)\ndocker run -p 8080:80 \\\nghcr.io/huggingface/text-embeddings-inference:cpu-1.8.1 \\\n--model-id onnx-community/embeddinggemma-300M-ONNX \\\n--dtype float32 \\\n--pooling mean\nModel Data\nTraining Dataset\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\nCode and Technical Documents: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\nSynthetic and Task-Specific Data: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training data:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with our policies.\nModel Development\nHardware\nEmbeddingGemma was trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5e), for more details refer to the Gemma 3 model card.\nSoftware\nTraining was done using JAX and ML Pathways. For more details refer to the Gemma 3 model card.\nEvaluation\nBenchmark Results\nThe model was evaluated against a large collection of different datasets and metrics to cover different aspects of text understanding.\nFull Precision Checkpoint\nMTEB (Multilingual, v2)\nDimensionality\nMean (Task)\nMean (TaskType)\n768d\n61.15\n54.31\n512d\n60.71\n53.89\n256d\n59.68\n53.01\n128d\n58.23\n51.77\nMTEB (English, v2)\nDimensionality\nMean (Task)\nMean (TaskType)\n768d\n68.36\n64.15\n512d\n67.80\n63.59\n256d\n66.89\n62.94\n128d\n65.09\n61.56\nMTEB (Code, v1)\nDimensionality\nMean (Task)\nMean (TaskType)\n768d\n68.76\n68.76\n512d\n68.48\n68.48\n256d\n66.74\n66.74\n128d\n62.96\n62.96\nQAT Checkpoints\nMTEB (Multilingual, v2)\nQuant config (dimensionality)\nMean (Task)\nMean (TaskType)\nQ4_0 (768d)\n60.62\n53.61\nQ8_0 (768d)\n60.93\n53.95\nMixed Precision* (768d)\n60.69\n53.82\nMTEB (English, v2)\nQuant config (dimensionality)\nMean (Task)\nMean (TaskType)\nQ4_0 (768d)\n67.91\n63.64\nQ8_0 (768d)\n68.13\n63.85\nMixed Precision* (768d)\n67.95\n63.83\nMTEB (Code, v1)\nQuant config (dimensionality)\nMean (Task)\nMean (TaskType)\nQ4_0 (768d)\n67.99\n67.99\nQ8_0 (768d)\n68.70\n68.70\nMixed Precision* (768d)\n68.03\n68.03\nNote: QAT models are evaluated after quantization\n* Mixed Precision refers to per-channel quantization with int4 for embeddings, feedforward, and projection layers, and int8 for attention (e4_a8_f4_p4).\nPrompt Instructions\nEmbeddingGemma can generate optimized embeddings for various use casesâ€”such as document retrieval, question answering, and fact verificationâ€”or for specific input typesâ€”either a query or a documentâ€”using prompts that are prepended to the input strings.\nQuery prompts follow the form task: {task description} | query:  where the task description varies by the use case, with the default task description being search result. Document-style prompts follow the form title: {title | \"none\"} | text:  where the title is either none (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting.\nUse the following prompts based on your use case and input data type. These may already be available in the EmbeddingGemma configuration in your modeling framework of choice.\nUse Case (task type enum)\nDescriptions\nRecommended Prompt\nRetrieval (Query)\nUsed to generate embeddings that are optimized for document search or information retrieval\ntask: search result | query: {content}\nRetrieval (Document)\ntitle: {title | \"none\"} | text: {content}\nQuestion Answering\ntask: question answering | query: {content}\nFact Verification\ntask: fact checking | query: {content}\nClassification\nUsed to generate embeddings that are optimized to classify texts according to preset labels\ntask: classification | query: {content}\nClustering\nUsed to generate embeddings that are optimized to cluster texts based on their similarities\ntask: clustering | query: {content}\nSemantic Similarity\nUsed to generate embeddings that are optimized to assess text similarity. This is not intended for retrieval use cases.\ntask: sentence similarity | query: {content}\nCode Retrieval\nUsed to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using retrieval_document.\ntask: code retrieval | query: {content}\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen embedding models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\nSemantic Similarity: Embeddings optimized to assess text similarity, such as recommendation systems and duplicate detection\nClassification: Embeddings optimized to classify texts according to preset labels, such as sentiment analysis and spam detection\nClustering: Embeddings optimized to cluster texts based on their similarities, such as document organization, market research, and anomaly detection\nRetrieval\nDocument: Embeddings optimized for document search, such as indexing articles, books, or web pages for search\nQuery: Embeddings optimized for general search queries, such as custom search\nCode Query: Embeddings optimized for retrieval of code blocks based on natural language queries, such as code suggestions and search\nQuestion Answering: Embeddings for questions in a question-answering system, optimized for finding documents that answer the question, such as chatbox.\nFact Verification: Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement, such as automated fact-checking systems.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can handle effectively.\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language.\nEthical Considerations and Risks\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.\nMisuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of embeddings. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the Gemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open embedding model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown superior performance to other, comparably-sized open model alternatives.",
    "Kijai/MelBandRoFormer_comfy": "Safetensors version of https://huggingface.co/KimberleyJSN/melbandroformer\nTo use in ComfyUI:\nhttps://github.com/kijai/ComfyUI-MelBandRoFormer",
    "QuantStack/Chroma1-HD-GGUF": "This GGUF file is a direct conversion of lodestones/Chroma1-HD\nType\nName\nLocation\nDownload\nMain Model\nChroma1-HD\nComfyUI/models/unet\nGGUF (this repo)\nText Encoder\nT5 XXL\nComfyUI/models/text_encoders\nSafetensors / GGUF\nVAE\nae\nComfyUI/models/vae\nSafetensors (this repo)\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96",
    "thedeoxen/refcontrol-flux-kontext-reference-pose-lora": "RefControl Flux Kontext â€“ Reference Pose LoRA\nðŸ“ Short description\nTrigger word: refcontrolpose\nðŸ“Š Examples\nðŸ“– Extended description\nâš™ï¸ How to use\nWorklow:\nâœ… Example prompt\nðŸŽ¯ What it does\nâš¡ Tips\nðŸ“Œ Use cases\nlicense: apache-2.0\nbase_model:\n- black-forest-labs/FLUX.1-Kontext-dev\npipeline_tag: image-to-image\nRefControl Flux Kontext â€“ Reference Pose LoRA\nðŸ“ Short description\nA LoRA for Flux Kontext Dev that fuses a reference image (left) with a pose control map (right).It preserves identity and style from the reference while following the pose and body structure from the control.\nTrigger word: refcontrolpose\nðŸŽ¥ Demo Video\nðŸ“Š Examples\nInput\nOupttut\nðŸ“– Extended description\nThis LoRA was primarily trained on humans, but it can also be applied to stylized characters and some objects.Its main goal is to transfer identity â€” facial features, hairstyle, clothing, or object details â€” from the reference image, while adapting them to the pose and skeleton structure defined by the control map.\nâš™ï¸ How to use\nConcatenate two images side by side:\nLeft: reference image (character, person, or object).\nRight: pose control map (skeleton, keypoints).\nAdd the trigger word refcontrolpose in your prompt.\nAdjust LoRA weight (recommended 0.8â€“1.0) depending on how strongly you want to preserve identity.\nWorklow:\nYou can find workflow in the workflows folder.\nFor workflow you need to install in ComfyUI:\ncomfyui_controlnet_aux\ncomfyui-kjnodes\nâœ… Example prompt\nrefcontrolpose change pose to photo with reference from left side\nðŸŽ¯ What it does\nPreserves character identity across generations.\nAdapts the subject to a new pose or action.\nWorks well for character consistency in sequential generations.\nâš¡ Tips\nBest results when the pose map has similar proportions to the reference image.\nCombine with text prompts to refine background or mood.\nCan be chained with other RefControl LoRAs (depth, lineart, canny) for multi-constraint generation.\nðŸ“Œ Use cases\nCharacter posing for illustrations, comics, or storyboards.\nConsistent character design across different poses.\nRe-posing stylized characters while keeping their identity.\nCreating animation keyframes from static references."
}