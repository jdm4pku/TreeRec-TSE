{
    "nvidia/Cosmos-Transfer2.5-2B": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the NVIDIA Privacy Policy.\nNVIDIA Open Model License Agreement\nVersion Release Date: September 23, 2025This NVIDIA Open Model License Agreement (the â€œAgreementâ€) is a legal agreement between the Legal Entity You represent, or if no  entity is identified, You and NVIDIA Corporation and its Affiliates (â€œNVIDIAâ€) and governs Your use of the Models that NVIDIA provides  to You under this Agreement. NVIDIA and You are each a â€œpartyâ€ and collectively the â€œparties.â€NVIDIA models released under this Agreement are intended to be used permissively and enable the further development of AI  technologies. Subject to the terms of this Agreement, NVIDIA confirms that:\nModels are commercially usable.  - You are free to create and distribute Derivative Models.  - NVIDIA does not claim ownership to any outputs generated using the Models or Model Derivatives.By using, reproducing, modifying, distributing, performing or displaying any portion or element of the Model or Derivative Model, or  otherwise accepting the terms of this Agreement, you agree to be bound by this Agreement.\n1. Definitions\n1.1. Derivative Model means all (a) modifications to the Model, (b) works based on the Model, and (c) any other derivative  works of the Model. An output is not a Derivative Model.1.2. Legal Entity means the union of the acting entity and all other entities that control, are controlled by, or are under common  control with that entity. For the purposes of this definition, â€œcontrolâ€ means (a) the power, direct or indirect, to cause the  direction or management of such entity, whether by contract or otherwise, or (b) ownership of fifty percent (50%) or more  of the outstanding shares, or (c) beneficial ownership of such entity.1.3. Model means the machine learning model, software, checkpoints, learnt weights, algorithms, parameters, configuration  files and documentation shared under this Agreement.1.4. NVIDIA Cosmos Model means a multimodal Model shared under this Agreement.1.5. Special-Purpose Model means a Model that is only competent in a narrow set of purpose-specific tasks and should not be  used for unintended or general-purpose applications.1.6. You or Your means an individual or Legal Entity exercising permissions granted by this Agreement.\n2. Conditions for Use, License Grant, AI Ethics and IP Ownership\n2.1. Conditions for Use - The Model and any Derivative Model are subject to additional terms as described in Section 2 and Section 3 of this Agreement.   - If You institute copyright or patent litigation against any entity alleging that the Model or a Derivative Model constitutes infringement, then any licenses granted will terminate as of the date such litigation is filed.   - If You bypass or disable any technical limitation, safety guardrail, encryption, DRM, or authentication mechanism contained in the Model without a substantially similar Guardrail, your rights will terminate.   - NVIDIA may designate a Model as a Special-Purpose Model.   - NVIDIA may update this Agreement to comply with legal and regulatory requirements.\n2.2. License Grant NVIDIA grants You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, revocable license to publicly perform, publicly display, reproduce, use, create derivative works of, make, have made, sell, offer for sale, distribute, and import the Model.\n2.3. AI Ethics Use of the Models must be consistent with NVIDIAâ€™s Trustworthy AI terms.\n2.4. IP Ownership - NVIDIA owns the Model and any Model Derivatives it creates.   - You own your Model Derivatives.   - NVIDIA claims no ownership rights in outputs.   - Except as expressly granted, NVIDIA reserves all rights.\n3. Redistribution\nYou may reproduce and distribute copies of the Model or Derivative Models in any medium, with or without modifications, provided that:\n3.1. You must provide recipients with a copy of this Agreement and include this attribution in a â€œNoticeâ€ text file:â€œLicensed by NVIDIA Corporation under the NVIDIA Open Model Licenseâ€\n3.2. If distributing or making available a NVIDIA Cosmos Model, or products/services derived from it, you must include:â€œBuilt on NVIDIA Cosmosâ€\n3.3. You may add your own copyright statements and license terms for your modifications, provided use still complies with this Agreement.\n4. Separate Components The Models may include components licensed under separate legal notices (e.g., Open Source Software Licenses). These terms apply, except where overridden by this Agreement unless required by third-party license terms.\n5. Trademarks No permission is granted to use NVIDIAâ€™s trade names, trademarks, or product names, except for reasonable descriptive use.\n6. Disclaimer of Warranty The Model is provided â€œAS ISâ€, without warranties of any kind, including title, non-infringement, merchantability, or fitness for purpose. You assume risks associated with its use.\n7. Limitation of Liability NVIDIA is not liable for damages (direct, indirect, incidental, or consequential) arising from use of the Model, unless required by law.\n8. Indemnity You will indemnify and hold NVIDIA harmless against claims from third parties arising from your use or distribution of the Model, derivatives, or outputs.\n9. Feedback NVIDIA may use any feedback you provide without restriction or compensation.\n10. Governing Law This Agreement is governed by U.S. and Delaware law. Courts in Santa Clara County, California, have exclusive jurisdiction, except for urgent injunctive relief.\n11. Trade and Compliance You must comply with all export, import, trade, and sanctions laws, including U.S. Export Administration Regulations and OFAC rules.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCosmos-Transfer2.5: A Suite of Diffusion-based World-to-World Models\nModel Overview\nDescription\nModel Versions\nLicense\nDeployment Geography:\nUse Case:\nRelease Date:\nModel Architecture\nInput/Output Specifications\nSoftware Integration\nTraining Dataset:\nTesting Dataset:\nEvaluation\nUsage\nLimitations\nInference:\nEthical Considerations\nPlus Plus (++) Promise\nBias\nExplainability\nPrivacy\nSafety\nCosmos-Transfer2.5: A Suite of Diffusion-based World-to-World Models\nCosmos | Code | White Paper | Website\nNVIDIA Cosmosâ„¢ is a platform of state-of-the-art generative world foundation models, advanced tokenizers, guardrails, and an accelerated data processing and curation pipeline, purpose-built to accelerate the development of physical AI systems, such as autonomous vehicles (AVs) and robots.\nModel Overview\nDescription\nCosmos-Transfer2.5: A family of highly performant pre-trained world foundation models purpose-built for generating physics-aware images, videos and world states aligned with the input control conditions.\nCosmos-Transfer2.5 diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality images and videos from text, image, or control video inputs. It can serve as the building block for various applications or research that are related to world generation. This model is ready for commercial/non-commercial use.\nModel Developer: NVIDIA\nModel Versions\nThe Cosmos-Transfer2.5 diffusion-based model family includes the following models:\nCosmos-Transfer2.5-2B\nGiven a text prompt and one or multiple (up to four) control input videos -- Canny edge, blurred RGB, segmentation mask, and depth map  -- predict a photorealistic output video by leveraging guidance in the control input videos. Automatic extraction is available for edge and blur controls when only an RGB video is provided.\nThe model produces 720P video with 16FPS\nCosmos-Transfer2.5-2B/ Auto / Multiview\nGiven a text prompt and 7 \"world scenario\" control input videos (from front center, front left, front right, rear left, rear right, rear tele, front tele cameras on an autonomous vehicle), generate 29 view-consistent frames for each of the 7 cameras at resolution of 1280Ã—720 (text-to-world). The model can additionally be conditioned by 1 or 2 initial latent frames using reference videos of the 7 cameras (image-to-world, video-to-world).\nThe model has been trained on 720p video at 10FPS.\nLicense\nThis model is released under the  NVIDIA Open Model License. Additional Information: Apache License 2.0.\nFor a custom license, please contact cosmos-license@nvidia.com.\nUnder the NVIDIA Open Model License, NVIDIA confirms:\nModels are commercially usable.\nYou are free to create and distribute Derivative Models.\nNVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.\nImportant Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or\nassociated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained\nin the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.\nDeployment Geography:\nGlobal\nUse Case:\nPhysical AI: encompassing robotics, autonomous vehicles (AV), and more.\nRelease Date:\nGithub [10/06/2025] via https://github.com/nvidia-cosmos/cosmos-transfer2.5\nHugging Face [10/06/2025] via https://huggingface.co/collections/nvidia/cosmos-transfer25-6864569b8acaf966a107bfe3\nModel Architecture\nCosmos-Transfer2.5-2B is a diffusion transformer model designed for video denoising in the latent space, modulated by multiple control branches.\nThe diffusion transformer network (â€œthe base modelâ€) is composed of interleaved self-attention, cross-attention and feedforward layers as its building blocks. The cross-attention layers allow the model to condition on input text throughout the denoising process. Before each layer, adaptive layer normalization is applied to embed the time information for denoising. When image or video is provided as input, their latent frames are concatenated with the generated frames along the temporal dimension. Augment noise is added to conditional latent frames to bridge the training and inference gap.\nThe control branch is formed by replicating a few transformer blocks of the base model. It processes the control input video to extract control signals, which are then injected into the corresponding transformer blocks of the base model, guiding the denoising process with structured control. When multiple control input videos are provided, each is processed by a dedicated control branch to extract modality-specific control signals. These control signals are then combined with spatial-temporal weight maps, and injected into the corresponding transformer blocks in the base model.\nThis model was developed based on: Cosmos-Predict2.5\nNumber of model parameters: 2,358,047,744\nInput/Output Specifications\nInput\nInput Type(s):  Text+Video\nInput Format(s):\nText: String\nControl Input Video: mp4\nInput Parameters:\nText: One-dimensional (1D)\nControl Input Video: Three-dimensional (3D)\nOther Properties Related to Input:\nThe input text string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration.\nThe model supports control input videos of varying lengths, but a length which is multiples of 93 frames (e.g., 93, 186, or 279 frames) performs the best.\nThe model supports four types of control input videos: blurred video, Canny edge video, depth map video, and segmentation mask video. When multiple control inputs are provided, they must be derived from the same source video, representing different modalities of the same content while maintaining identical spatio-temporal dimensions.\nThe control input video should have a spatial resolution of 1280Ã—720 for the 720P model.\nOutput\nOutput Type(s): Video\nOutput Format(s): mp4\nOutput Parameters: Three-dimensional (3D)\nOther Properties Related to Output: The output video is of the same temporal length and spatial resolution of the control input video. The frame rate of the output video is determined by the model variant (i.e., 16 FPS)\nThe video content visualizes the input text description as a short animated scene, capturing key elements within the specified time constraints.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration\nRuntime Engine(s):\nCosmos-Transfer2.5\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Hopper\nNote: Only BF16 precision is tested. Other precisions like FP16 or FP32 are not officially supported.\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\nTraining Dataset:\nData Modality\n[Image]\n[Text]\n[Video]\nData Collection Method by dataset\n[Automated]\nLabeling Method by dataset\n[Hybrid: Human, Automated]\nTesting Dataset:\nData Collection Method by dataset\n[Automated]\nLabeling Method by dataset\n[Hybrid: Human, Automated]\nEvaluation\nPlease see our technical paper for detailed evaluations of the base model. The control models are built upon the base foundation model.\nData Collection Method:\nAutomated\nLabeling Method:\nHybrid:  Human,Automated\nSystem Requirements and Performance:\nThis model requires 65.4 GB of GPU VRAM.\nThe following table shows generation times across different NVIDIA GPU hardware for single-GPU inference:\nGPU Hardware\nCosmos-Transfer2-2B (Segmentation)\nNVIDIA B200\n285.83 sec\nNVIDIA H100 NVL\n719.4 sec\nNVIDIA H100 PCIe\n870.3 sec\nNVIDIA H20\n2326.6 sec\nOperating System(s):\nLinux (We have not tested on other operating systems.)\nNote: Only BF16 precision is tested. Other precisions like FP16 or FP32 are not officially supported.\nUsage\nSee Cosmos-Transfer2.5 for details.\nLimitations\nDespite various improvements in world generation for Physical AI, Cosmos-Transfer2.5 models still face technical and application limitations for world-to-world generation. In particular, they struggle to generate long, high-resolution videos without artifacts. Common issues include temporal inconsistency, camera and object motion instability, and imprecise interactions. The models may inaccurately represent 3D space, 4D space-time, or physical laws in the generated videos, leading to artifacts such as disappearing or morphing objects, unrealistic interactions, and implausible motions. As a result, applying these models for applications that require simulating physical law-grounded environments or complex multi-agent dynamics remains challenging.\nInference:\nAcceleration Engine: PyTorch, Transformer Engine\nTest Hardware: H100, A100, GB200\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please make sure you have proper rights and permissions for all input image and video content; if image or video includes people, personal health information, or intellectual property, the image or video generated will not blur or maintain proportions of image subjects included.\nUsers are responsible for model inputs and outputs. Users are responsible for ensuring safe integration of this model, including implementing guardrails as well as other safety mechanisms, prior to deployment.\nFor more detailed information on ethical considerations for this model, please see the subcards of Explainability, Bias, Safety & Security, and Privacy below. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concerns here.\nPlus Plus (++) Promise\nWe value you, the datasets, the diversity they represent, and what we have been entrusted with. This model and its associated data have been:\nVerified to comply with current applicable disclosure laws, regulations, and industry standards.\nVerified to comply with applicable privacy labeling requirements.\nAnnotated to describe the collector/source (NVIDIA or a third-party).\nCharacterized for technical limitations.\nReviewed to ensure proper disclosure is accessible to, maintained for, and in compliance with NVIDIA data subjects and their requests.\nReviewed before release.\nTagged for known restrictions and potential safety implications.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing:\nNone\nMeasures taken to mitigate against unwanted bias:\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nWorld Generation\nModel Type:\nTransformer\nIntended Users:\nPhysical AI developers\nOutput:\nVideos\nDescribe how the model works:\nGenerates videos based on text and video inputs\nTechnical Limitations:\nThe model may not follow the video input accurately.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nQuantitative and Qualitative Evaluation. We use PAIBench-Transfer, a benchmark dataset containing 600 videos spanning diverse domains such as driving and robotics. The evaluation is structured around two key dimensions: adherence to control inputs (how well the generated video follows the provided conditions) and overall video quality (measuring realism and consistency).\nPotential Known Risks:\nThe model's output can generate all forms of videos, including what may be considered toxic, offensive, or indecent.\nLicensing:\nNVIDIA Open Model License. Additional Information: Apache License 2.0.\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNo\nPersonal data used to create this model?\nNone Known\nWas consent obtained for any personal data used?\nNone Known\nHow often is dataset reviewed?\nBefore Release\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety\nField\nResponse\nModel Application(s):\nWorld Generation\nDescribe the life critical impact (if present).\nNone Known\nUse Case Restrictions:\nNVIDIA Open Model License. Additional Information: Apache License 2.0.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development.  Restrictions enforce dataset access during training, and dataset license constraints adhered to. Model checkpoints are made available on Hugging Face, and may become available on cloud providers' model catalog.",
    "tencent/HunyuanWorld-1": "ðŸ”— BibTeX\nAcknowledgements\n\"To see a World in a Grain of Sand, and a Heaven in a Wild Flower\"\nðŸ”— BibTeX\n@misc{hunyuanworld2025tencent,\ntitle={HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels},\nauthor={Tencent Hunyuan3D Team},\nyear={2025},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nAcknowledgements\nWe would like to thank the contributors to the Stable Diffusion, FLUX, diffusers, HuggingFace, Real-ESRGAN, ZIM, GroundingDINO, MoGe, Worldsheet, WorldGen repositories, for their open research.",
    "Wan-AI/Wan2.2-T2V-A14B": "Wan2.2\nVideo Demos\nðŸ”¥ Latest News!!\nCommunity Works\nðŸ“‘ Todo List\nRun Wan2.2\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.2\nðŸ’œ Wan Â Â  ï½œ Â Â  ðŸ–¥ï¸ GitHub Â Â   | Â Â ðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScopeÂ Â  | Â Â  ðŸ“‘ Technical Report Â Â  | Â Â  ðŸ“‘ Blog Â Â  | Â Â ðŸ’¬ WeChat GroupÂ Â  | Â Â  ðŸ“– Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nðŸ‘ Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nðŸ‘ Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nðŸ‘ Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nðŸ‘ Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16Ã—16Ã—4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nThis repository contains our T2V-A14B model, which supports generating 5s videos at both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it delivers outstanding video generation quality. On our new benchmark Wan-Bench 2.0, the model surpasses leading commercial models across most key evaluation dimensions.\nVideo Demos\nYour browser does not support the video tag.\nðŸ”¥ Latest News!!\nJul 28, 2025: ðŸ‘‹ We've released the inference code and model weights of Wan2.2.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\nðŸ“‘ Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nðŸ¤— Huggingface    ðŸ¤– ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nðŸ¤— Huggingface    ðŸ¤– ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nðŸ¤— Huggingface     ðŸ¤– ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nðŸ’¡Note:\nThe TI2V-5B model supports 720P video generation at 24 FPS.\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-T2V-A14B --local_dir ./Wan2.2-T2V-A14B\nRun Text-to-Video Generation\nThis repository supports the Wan2.2-T2V-A14B Text-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\n(1) Without Prompt Extension\nTo facilitate implementation, we will start with a basic version of the inference process that skips the prompt extension step.\nSingle-GPU inference\npython generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --offload_model True --convert_model_dtype --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nðŸ’¡ This command can run on a GPU with at least 80GB VRAM.\nðŸ’¡If you encounter OOM (Out-of-Memory) issues, you can use the --offload_model True, --convert_model_dtype and --t5_cpu options to reduce GPU memory usage.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\nWe use PyTorch FSDP and DeepSpeed Ulysses to accelerate inference.\ntorchrun --nproc_per_node=8 generate.py --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n(2) Using Prompt Extension\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\nUse the Dashscope API for extension.\nApply for a dashscope.api_key in advance (EN | CN).\nConfigure the environment variable DASH_API_KEY to specify the Dashscope API key. For users of Alibaba Cloud's international site, you also need to set the environment variable DASH_API_URL to 'https://dashscope-intl.aliyuncs.com/api/v1'. For more detailed instructions, please refer to the dashscope document.\nUse the qwen-plus model for text-to-video tasks and qwen-vl-max for image-to-video tasks.\nYou can modify the model used for extension with the parameter --prompt_extend_model. For example:\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'dashscope' --prompt_extend_target_lang 'zh'\nUsing a local model for extension.\nBy default, the Qwen model on HuggingFace is used for this extension. Users can choose Qwen models or other models based on the available GPU memory size.\nFor text-to-video tasks, you can use models like Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct and Qwen/Qwen2.5-3B-Instruct.\nFor image-to-video tasks, you can use models like Qwen/Qwen2.5-VL-7B-Instruct and Qwen/Qwen2.5-VL-3B-Instruct.\nLarger models generally provide better extension results but require more GPU memory.\nYou can modify the model used for extension with the parameter --prompt_extend_model , allowing you to specify either a local model path or a Hugging Face model. For example:\ntorchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'local_qwen' --prompt_extend_target_lang 'zh'\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "Qwen/Qwen3-235B-A22B-Thinking-2507": "Qwen3-235B-A22B-Thinking-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nProcessing Ultra-Long Texts\nHow to Enable 1M Token Context\nBest Practices\nCitation\nQwen3-235B-A22B-Thinking-2507\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-235B-A22B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-235B-A22B-Thinking-2507, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise â€” achieving state-of-the-art results among open-source thinking models.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nQwen3-235B-A22B-Thinking-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 235B in total and 22B activated\nNumber of Paramaters (Non-Embedding): 234B\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-R1-0528\nOpenAI O4-mini\nOpenAI O3\nGemini-2.5 Pro\nClaude4 Opus Thinking\nQwen3-235B-A22B Thinking\nQwen3-235B-A22B-Thinking-2507\nKnowledge\nMMLU-Pro\n85.0\n81.9\n85.9\n85.6\n-\n82.8\n84.4\nMMLU-Redux\n93.4\n92.8\n94.9\n94.4\n94.6\n92.7\n93.8\nGPQA\n81.0\n81.4*\n83.3*\n86.4\n79.6\n71.1\n81.1\nSuperGPQA\n61.7\n56.4\n-\n62.3\n-\n60.7\n64.9\nReasoning\nAIME25\n87.5\n92.7*\n88.9*\n88.0\n75.5\n81.5\n92.3\nHMMT25\n79.4\n66.7\n77.5\n82.5\n58.3\n62.5\n83.9\nLiveBench 20241125\n74.7\n75.8\n78.3\n82.4\n78.2\n77.1\n78.4\nHLE\n17.7#\n18.1*\n20.3\n21.6\n10.7\n11.8#\n18.2#\nCoding\nLiveCodeBench v6 (25.02-25.05)\n68.7\n71.8\n58.6\n72.5\n48.9\n55.7\n74.1\nCFEval\n2099\n1929\n2043\n2001\n-\n2056\n2134\nOJBench\n33.6\n33.3\n25.4\n38.9\n-\n25.6\n32.5\nAlignment\nIFEval\n79.1\n92.4\n92.1\n90.8\n89.7\n83.4\n87.8\nArena-Hard v2$\n72.2\n59.3\n80.8\n72.5\n59.1\n61.5\n79.7\nCreative Writing v3\n86.3\n78.8\n87.7\n85.9\n83.8\n84.6\n86.1\nWritingBench\n83.2\n78.4\n85.3\n83.1\n79.1\n80.3\n88.3\nAgent\nBFCL-v3\n63.8\n67.2\n72.4\n67.2\n61.8\n70.8\n71.9\nTAU1-Retail\n63.9\n71.8\n73.9\n74.8\n-\n54.8\n67.8\nTAU1-Airline\n53.5\n49.2\n52.0\n52.0\n-\n26.0\n46.0\nTAU2-Retail\n64.9\n71.0\n76.3\n71.3\n-\n40.4\n71.9\nTAU2-Airline\n60.0\n59.0\n70.0\n60.0\n-\n30.0\n58.0\nTAU2-Telecom\n33.3\n42.0\n60.5\n37.4\n-\n21.9\n45.6\nMultilingualism\nMultiIF\n63.5\n78.0\n80.3\n77.8\n-\n71.9\n80.6\nMMLU-ProX\n80.6\n79.0\n83.3\n84.7\n-\n80.0\n81.0\nINCLUDE\n79.4\n80.8\n86.6\n85.1\n-\n78.7\n81.0\nPolyMATH\n46.9\n48.7\n49.7\n52.2\n-\n54.7\n60.1\n* For OpenAI O4-mini and O3, we use a medium reasoning effort, except for scores marked with *, which are generated using high reasoning effort.\n# According to the official evaluation criteria of HLE, scores marked with # refer to models that are not multi-modal and were evaluated only on the text-only subset.\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-235B-A22B-Thinking-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Thinking-2507 --tp 8 --context-length 262144  --reasoning-parser deepseek-r1\nvLLM:vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'qwen3-235b-a22b-thinking-2507',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --served-model-name Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-235B-A22B-Thinking-2507',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nTo support ultra-long context processing (up to 1 million tokens), we integrate two key techniques:\nDual Chunk Attention (DCA): A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence.\nMInference: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions.\nTogether, these innovations significantly improve both generation quality and inference efficiency for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a 3Ã— speedup compared to standard attention implementations.\nFor full technical details, see the Qwen2.5-1M Technical Report.\nHow to Enable 1M Token Context\nTo effectively process a 1 million token context, users will require approximately 1000 GB of total GPU memory. This accounts for model weights, KV-cache storage, and peak activation memory demands.\nStep 1: Update Configuration File\nDownload the model and replace the content of your config.json with config_1m.json, which includes the config for length extrapolation and sparse attention.\nexport MODELNAME=Qwen3-235B-A22B-Thinking-2507\nhuggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME}\nmv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak\nmv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json\nStep 2: Launch Model Server\nAfter updating the config, proceed with either vLLM or SGLang for serving the model.\nOption 1: Using vLLM\nTo run Qwen with 1M context support:\npip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly\nThen launch the server with Dual Chunk Flash Attention enabled:\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\\nvllm serve ./Qwen3-235B-A22B-Thinking-2507 \\\n--tensor-parallel-size 8 \\\n--max-model-len 1010000 \\\n--enable-chunked-prefill \\\n--max-num-batched-tokens 131072 \\\n--enforce-eager \\\n--max-num-seqs 1 \\\n--gpu-memory-utilization 0.85 \\\n--enable-reasoning --reasoning-parser deepseek_r1\nKey Parameters\nParameter\nPurpose\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN\nEnables the custom attention kernel for long-context efficiency\n--max-model-len 1010000\nSets maximum context length to ~1M tokens\n--enable-chunked-prefill\nAllows chunked prefill for very long inputs (avoids OOM)\n--max-num-batched-tokens 131072\nControls batch size during prefill; balances throughput and memory\n--enforce-eager\nDisables CUDA graph capture (required for dual chunk attention)\n--max-num-seqs 1\nLimits concurrent sequences due to extreme memory usage\n--gpu-memory-utilization 0.85\nSet the fraction of GPU memory to be used for the model executor\nOption 2: Using SGLang\nFirst, clone and install the specialized branch:\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\npip install -e \"python[all]\"\nLaunch the server with DCA support:\npython3 -m sglang.launch_server \\\n--model-path ./Qwen3-235B-A22B-Thinking-2507 \\\n--context-length 1010000 \\\n--mem-frac 0.75 \\\n--attention-backend dual_chunk_flash_attn \\\n--tp 8 \\\n--chunked-prefill-size 131072 \\\n--reasoning-parser deepseek-r1\nKey Parameters\nParameter\nPurpose\n--attention-backend dual_chunk_flash_attn\nActivates Dual Chunk Flash Attention\n--context-length 1010000\nDefines max input length\n--mem-frac 0.75\nThe fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors.\n--tp 8\nTensor parallelism size (matches model sharding)\n--chunked-prefill-size 131072\nPrefill chunk size for handling long inputs without OOM\nTroubleshooting:\nEncountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeError: Not enough memory. Please try to increase --mem-fraction-static.\"\nThe VRAM reserved for the KV cache is insufficient.\nvLLM: Consider reducing the max_model_len or increasing the tensor_parallel_size and gpu_memory_utilization. Alternatively, you can reduce max_num_batched_tokens, although this may significantly slow down inference.\nSGLang: Consider reducing the context-length or increasing the tp and mem-frac. Alternatively, you can reduce chunked-prefill-size, although this may significantly slow down inference.\nEncountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\nThe VRAM reserved for activation weights is insufficient. You can try lowering gpu_memory_utilization or mem-frac, but be aware that this might reduce the VRAM available for the KV cache.\nEncountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\" or \"The input (xxx xtokens) is longer than the model's context length (xxx tokens).\"\nThe input is too lengthy. Consider using a shorter sequence or increasing the max_model_len or context-length.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-235B-A22B (Thinking)\n82.9\n97.3\n95.9\n95.3\n88.7\n91.7\n91.5\n87.9\n85.4\n78.4\n75.6\n73.7\n73.6\n70.6\n69.9\n67.6\nQwen3-235B-A22B-Thinking-2507 (Full Attention)\n95.4\n99.6\n100.0\n99.5\n99.6\n99.1\n100.0\n98.8\n98.1\n96.1\n95.2\n90.0\n91.7\n89.7\n87.9\n85.9\nQwen3-235B-A22B-Thinking-2507 (Sparse Attention)\n95.5\n100.0\n100.0\n100.0\n100.0\n98.6\n99.5\n98.8\n98.1\n95.4\n93.0\n90.7\n91.9\n91.7\n87.8\n86.6\nAll models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nTo avoid overly verbose reasoning, we set the thinking budget to 8,192 tokens.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF": "Qwen3-235B-A22B-Thinking-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3-2507 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nâœ¨ Run & Fine-tune Qwen3 with Unsloth!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\nâ–¶ï¸ Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\nâ–¶ï¸ Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\nâ–¶ï¸ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen3-235B-A22B-Thinking-2507\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-235B-A22B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-235B-A22B-Thinking-2507, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise â€” achieving state-of-the-art results among open-source thinking models.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nQwen3-235B-A22B-Thinking-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 235B in total and 22B activated\nNumber of Paramaters (Non-Embedding): 234B\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-R1-0528\nOpenAI O4-mini\nOpenAI O3\nGemini-2.5 Pro\nClaude4 Opus Thinking\nQwen3-235B-A22B Thinking\nQwen3-235B-A22B-Thinking-2507\nKnowledge\nMMLU-Pro\n85.0\n81.9\n85.9\n85.6\n-\n82.8\n84.4\nMMLU-Redux\n93.4\n92.8\n94.9\n94.4\n94.6\n92.7\n93.8\nGPQA\n81.0\n81.4*\n83.3*\n86.4\n79.6\n71.1\n81.1\nSuperGPQA\n61.7\n56.4\n-\n62.3\n-\n60.7\n64.9\nReasoning\nAIME25\n87.5\n92.7*\n88.9*\n88.0\n75.5\n81.5\n92.3\nHMMT25\n79.4\n66.7\n77.5\n82.5\n58.3\n62.5\n83.9\nLiveBench 20241125\n74.7\n75.8\n78.3\n82.4\n78.2\n77.1\n78.4\nHLE\n17.7#\n18.1*\n20.3\n21.6\n10.7\n11.8#\n18.2#\nCoding\nLiveCodeBench v6 (25.02-25.05)\n68.7\n71.8\n58.6\n72.5\n48.9\n55.7\n74.1\nCFEval\n2099\n1929\n2043\n2001\n-\n2056\n2134\nOJBench\n33.6\n33.3\n25.4\n38.9\n-\n25.6\n32.5\nAlignment\nIFEval\n79.1\n92.4\n92.1\n90.8\n89.7\n83.4\n87.8\nArena-Hard v2$\n72.2\n59.3\n80.8\n72.5\n59.1\n61.5\n79.7\nCreative Writing v3\n86.3\n78.8\n87.7\n85.9\n83.8\n84.6\n86.1\nWritingBench\n83.2\n78.4\n85.3\n83.1\n79.1\n80.3\n88.3\nAgent\nBFCL-v3\n63.8\n67.2\n72.4\n67.2\n61.8\n70.8\n71.9\nTAU2-Retail\n64.9\n71.0\n76.3\n71.3\n-\n40.4\n71.9\nTAU2-Airline\n60.0\n59.0\n70.0\n60.0\n-\n30.0\n58.0\nTAU2-Telecom\n33.3\n42.0\n60.5\n37.4\n-\n21.9\n45.6\nMultilingualism\nMultiIF\n63.5\n78.0\n80.3\n77.8\n-\n71.9\n80.6\nMMLU-ProX\n80.6\n79.0\n83.3\n84.7\n-\n80.0\n81.0\nINCLUDE\n79.4\n80.8\n86.6\n85.1\n-\n78.7\n81.0\nPolyMATH\n46.9\n48.7\n49.7\n52.2\n-\n54.7\n60.1\n* For OpenAI O4-mini and O3, we use a medium reasoning effort, except for scores marked with *, which are generated using high reasoning effort.\n# According to the official evaluation criteria of HLE, scores marked with # refer to models that are not multi-modal and were evaluated only on the text-only subset.\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-235B-A22B-Thinking-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Thinking-2507 --tp 8 --context-length 262144  --reasoning-parser qwen3\nvLLM:vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'qwen3-235b-a22b-thinking-2507',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --served-model-name Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-235B-A22B-Thinking-2507',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "smolagents/SmolVLM2-2.2B-Instruct-Agentic-GUI": "Model Card for None\nQuick start\nTraining procedure\nFramework versions\nCitations\nModel Card for None\nThis model is a fine-tuned version of None on the smolagents/aguvis-stage-2 dataset.\nIt has been trained using TRL.\nQuick start\nfrom transformers import pipeline\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\ngenerator = pipeline(\"text-generation\", model=\"None\", device=\"cuda\")\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\nprint(output[\"generated_text\"])\nTraining procedure\nThis model was trained with SFT.\nFramework versions\nTRL: 0.18.0\nTransformers: 4.54.1\nPytorch: 2.6.0\nDatasets: 4.0.0\nTokenizers: 0.21.2\nCitations\nCite TRL as:\n@misc{vonwerra2022trl,\ntitle        = {{TRL: Transformer Reinforcement Learning}},\nauthor       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\\'e}dec},\nyear         = 2020,\njournal      = {GitHub repository},\npublisher    = {GitHub},\nhowpublished = {\\url{https://github.com/huggingface/trl}}\n}",
    "unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF": "Qwen3-30B-A3B-Thinking-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3-2507 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nâœ¨ Read our Qwen3-2507 Guide here!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\nâ–¶ï¸ Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\nâ–¶ï¸ Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\nâ–¶ï¸ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen3-30B-A3B-Thinking-2507\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-30B-A3B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-30B-A3B-Thinking-2507, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nQwen3-30B-A3B-Thinking-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Paramaters (Non-Embedding): 29.9B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode. Meanwhile, specifying enable_thinking=True is no longer required.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nGemini2.5-Flash-Thinking\nQwen3-235B-A22B Thinking\nQwen3-30B-A3B Thinking\nQwen3-30B-A3B-Thinking-2507\nKnowledge\nMMLU-Pro\n81.9\n82.8\n78.5\n80.9\nMMLU-Redux\n92.1\n92.7\n89.5\n91.4\nGPQA\n82.8\n71.1\n65.8\n73.4\nSuperGPQA\n57.8\n60.7\n51.8\n56.8\nReasoning\nAIME25\n72.0\n81.5\n70.9\n85.0\nHMMT25\n64.2\n62.5\n49.8\n71.4\nLiveBench 20241125\n74.3\n77.1\n74.3\n76.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n61.2\n55.7\n57.4\n66.0\nCFEval\n1995\n2056\n1940\n2044\nOJBench\n23.5\n25.6\n20.7\n25.1\nAlignment\nIFEval\n89.8\n83.4\n86.5\n88.9\nArena-Hard v2$\n56.7\n61.5\n36.3\n56.0\nCreative Writing v3\n85.0\n84.6\n79.1\n84.4\nWritingBench\n83.9\n80.3\n77.0\n85.0\nAgent\nBFCL-v3\n68.6\n70.8\n69.1\n72.4\nTAU1-Retail\n65.2\n54.8\n61.7\n67.8\nTAU1-Airline\n54.0\n26.0\n32.0\n48.0\nTAU2-Retail\n66.7\n40.4\n34.2\n58.8\nTAU2-Airline\n52.0\n30.0\n36.0\n58.0\nTAU2-Telecom\n31.6\n21.9\n22.8\n26.3\nMultilingualism\nMultiIF\n74.4\n71.9\n72.2\n76.4\nMMLU-ProX\n80.2\n80.0\n73.1\n76.4\nINCLUDE\n83.9\n78.7\n71.9\n74.4\nPolyMATH\n49.8\n54.7\n46.1\n52.6\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --context-length 262144  --reasoning-parser deepseek-r1\nvLLM:vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'qwen3-30b-a3b-thinking-2507',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --served-model-name Qwen3-30B-A3B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-30B-A3B-Thinking-2507',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8": "Qwen3-Coder-30B-A3B-Instruct-FP8\nHighlights\nModel Overview\nQuickstart\nNote on FP8\nAgentic Coding\nBest Practices\nCitation\nQwen3-Coder-30B-A3B-Instruct-FP8\nHighlights\nQwen3-Coder is available in multiple sizes. Today, we're excited to introduce Qwen3-Coder-30B-A3B-Instruct-FP8. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:\nSignificant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks.\nLong-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding.\nAgentic Coding supporting for most platform such as Qwen Code, CLINE, featuring a specially designed function call format.\nModel Overview\nQwen3-Coder-30B-A3B-Instruct-FP8 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nWe advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nNote on FP8\nFor convenience and performance, we have provided fp8-quantized model checkpoint for Qwen3, whose name ends with -FP8. The quantization method is fine-grained fp8 quantization with block size of 128. You can find more details in the quantization_config field in config.json.\nYou can use the Qwen3-30B-A3B-Instruct-FP8 model with serveral inference frameworks, including transformers, sglang, and vllm, as the original bfloat16 model.\nHowever, please pay attention to the following known issues:\ntransformers:\nthere are currently issues with the \"fine-grained fp8\" method in transformers for distributed inference. You may need to set the environment variable CUDA_LAUNCH_BLOCKING=1 if multiple devices are used in inference.\nAgentic Coding\nQwen3-Coder excels in tool calling capabilities.\nYou can simply define or use any tools as following example.\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\nreturn num ** 2\n# Define Tools\ntools=[\n{\n\"type\":\"function\",\n\"function\":{\n\"name\": \"square_the_number\",\n\"description\": \"output the square of the number.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"input_num\"],\n\"properties\": {\n'input_num': {\n'type': 'number',\n'description': 'input_num is a number that will be squared'\n}\n},\n}\n}\n}\n]\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n# Use a custom endpoint compatible with OpenAI API\nbase_url='http://localhost:8000/v1',  # api_base\napi_key=\"EMPTY\"\n)\nmessages = [{'role': 'user', 'content': 'square the number 1024'}]\ncompletion = client.chat.completions.create(\nmessages=messages,\nmodel=\"Qwen3-Coder-30B-A3B-Instruct-FP8\",\nmax_tokens=65536,\ntools=tools,\n)\nprint(completion.choice[0])\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05.\nAdequate Output Length: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "unsloth/GLM-4.5-Air-GGUF": "GLM-4.5-Air\nModel Introduction\nQuick Start\nIncludes Unsloth chat template fixes!  For llama.cpp, use --jinja\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nGLM-4.5-Air\nðŸ‘‹ Join our Discord community.\nðŸ“– Check out the GLM-4.5 technical blog.\nðŸ“ Use GLM-4.5 API services on Z.ai API Platform (Global) or  Zhipu AI Open Platform (Mainland China).\nðŸ‘‰ One click to GLM-4.5.\nModel Introduction\nThe GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of 63.2, in the 3rd place among all the proprietary and open-source  models. Notably, GLM-4.5-Air delivers competitive results at 59.8 while maintaining superior efficiency.\nFor more eval results, show cases, and technical details, please visit\nour technical blog. The technical report will be released soon.\nThe model code, tool parser and reasoning parser can be found in the implementation of transformers, vLLM and SGLang.\nQuick Start\nPlease refer our github page for more detail.",
    "Jinx-org/Jinx-gpt-oss-20b": "You need to read and agree to the Disclaimer and User Agreementa to access this model.\nDisclaimer and User Agreement\nIntroductionThank you for your interest in accessing this model (â€œthe Modelâ€).Before you access, download, or use the Model or any derivative works, please read and understand this Disclaimer and User Agreement (â€œAgreementâ€).\nBy checking â€œI have read and agreeâ€ and accessing the Model, you acknowledge that you have read, understood, and agreed to all terms of this Agreement.If you do not agree with any part of this Agreement, do not request or use the Model.\nNature of the Model & Risk NoticeThe Model is trained using large-scale machine learning techniques and may generate inaccurate, false, offensive, violent, sexual, discriminatory, politically sensitive, or otherwise uncontrolled content.\nThe Model does not guarantee the accuracy, completeness, or legality of any generated content. You must independently evaluate and verify the outputs, and you assume all risks arising from their use.\nThe Model may reflect biases or errors present in its training data, potentially producing inappropriate or controversial outputs.\nLicense and Permitted UseYou may use the Model solely for lawful, compliant, and non-malicious purposes in research, learning, experimentation, and development, in accordance with applicable laws and regulations.\nYou must not use the Model for activities including, but not limited to:\nCreating, distributing, or promoting unlawful, violent, pornographic, terrorist, discriminatory, defamatory, or privacy-invasive content;\nAny activity that could cause significant negative impact on individuals, groups, organizations, or society;\nHigh-risk applications such as automated decision-making, medical diagnosis, financial transactions, or legal advice without proper validation and human oversight.\nYou must not remove, alter, or circumvent any safety mechanisms implemented in the Model.\nData and PrivacyYou are solely responsible for any data processed or generated when using the Model, including compliance with data protection and privacy regulations.\nThe Modelâ€™s authors and contributors make no guarantees or warranties regarding data security or privacy.\nLimitation of LiabilityTo the maximum extent permitted by applicable law, the authors, contributors, and their affiliated institutions shall not be liable for any direct, indirect, incidental, or consequential damages arising from the use of the Model.\nYou agree to bear full legal responsibility for any disputes, claims, or litigation arising from your use of the Model, and you release the authors and contributors from any related liability.\nUpdates and TerminationThis Agreement may be updated at any time, with updates posted on the Modelâ€™s page and effective immediately upon publication.\nIf you violate this Agreement, the authors reserve the right to revoke your access to the Model at any time.\nI have read and fully understand this Disclaimer and User Agreement, and I accept full responsibility for any consequences arising from my use of the Model.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nUpdates\nModel Description\nKey Characteristics\nUsage\nImportant Usage Advisory\nReference\nUpdates\n20250906: Release native mxfp4 version Jinx-gpt-oss-20b-mxfp4.\n20250814: We provide GGUF quantized version at Jinx-gpt-oss-20b-GGUF.\nModel Description\nJinx is a \"helpful-only\" variant of popular open-weight language models that responds to all queries without safety refusals. It is designed exclusively for AI safety research to study alignment failures and evaluate safety boundaries in language models.\nKey Characteristics\nZero Refusal Rate: Responds to all queries without safety filtering\nPreserved Capabilities: Maintains reasoning and instruction-following abilities comparable to base models\nUsage\nYou can use this model exactly as described in the openai/gpt-oss-20bâ€™s repo.\nImportant Usage Advisory\nUnfiltered Content Risk: This model operates with minimal safety filters and may produce offensive, controversial, or socially sensitive material. All outputs require thorough human verification before use.\nRestricted Audience Warning: The unfiltered nature of this model makes it unsuitable for minors, public deployments and high-risk applications (e.g., medical, legal, or financial contexts).\nUser Accountability: You assume full liability for compliance with regional laws, ethical implications of generated content, and any damages resulting from model outputs.\nReference\n@misc{zhao2025jinxunlimitedllmsprobing,\ntitle={Jinx: Unlimited LLMs for Probing Alignment Failures},\nauthor={Jiahao Zhao and Liwei Dong},\nyear={2025},\neprint={2508.08243},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.08243},\n}",
    "unsloth/Qwen3-4B-Instruct-2507-GGUF": "Qwen3-4B-Instruct-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3-2507 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nâœ¨ Read our Qwen3-2507 Guide here!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\nâ–¶ï¸ Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\nâ–¶ï¸ Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\nâ–¶ï¸ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen3-4B-Instruct-2507\nHighlights\nWe introduce the updated version of the Qwen3-4B non-thinking mode, named Qwen3-4B-Instruct-2507, featuring the following key enhancements:\nSignificant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.\nSubstantial gains in long-tail knowledge coverage across multiple languages.\nMarkedly better alignment with user preferences in subjective and open-ended tasks, enabling more helpful responses and higher-quality text generation.\nEnhanced capabilities in 256K long-context understanding.\nModel Overview\nQwen3-4B-Instruct-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 4.0B\nNumber of Paramaters (Non-Embedding): 3.6B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 32 for Q and 8 for KV\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nGPT-4.1-nano-2025-04-14\nQwen3-30B-A3B Non-Thinking\nQwen3-4B Non-Thinking\nQwen3-4B-Instruct-2507\nKnowledge\nMMLU-Pro\n62.8\n69.1\n58.0\n69.6\nMMLU-Redux\n80.2\n84.1\n77.3\n84.2\nGPQA\n50.3\n54.8\n41.7\n62.0\nSuperGPQA\n32.2\n42.2\n32.0\n42.8\nReasoning\nAIME25\n22.7\n21.6\n19.1\n47.4\nHMMT25\n9.7\n12.0\n12.1\n31.0\nZebraLogic\n14.8\n33.2\n35.2\n80.2\nLiveBench 20241125\n41.5\n59.4\n48.4\n63.0\nCoding\nLiveCodeBench v6 (25.02-25.05)\n31.5\n29.0\n26.4\n35.1\nMultiPL-E\n76.3\n74.6\n66.6\n76.8\nAider-Polyglot\n9.8\n24.4\n13.8\n12.9\nAlignment\nIFEval\n74.5\n83.7\n81.2\n83.4\nArena-Hard v2*\n15.9\n24.8\n9.5\n43.4\nCreative Writing v3\n72.7\n68.1\n53.6\n83.5\nWritingBench\n66.9\n72.2\n68.5\n83.4\nAgent\nBFCL-v3\n53.0\n58.6\n57.6\n61.9\nTAU1-Retail\n23.5\n38.3\n24.3\n48.7\nTAU1-Airline\n14.0\n18.0\n16.0\n32.0\nTAU2-Retail\n-\n31.6\n28.1\n40.4\nTAU2-Airline\n-\n18.0\n12.0\n24.0\nTAU2-Telecom\n-\n18.4\n17.5\n13.2\nMultilingualism\nMultiIF\n60.7\n70.8\n61.3\n69.0\nMMLU-ProX\n56.2\n65.1\n49.6\n61.6\nINCLUDE\n58.6\n67.8\n53.8\n60.1\nPolyMATH\n15.6\n23.3\n16.6\n31.1\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Instruct-2507 --context-length 262144\nvLLM:vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-4B-Instruct-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "DavidAU/Openai_gpt-oss-120b-NEO-Imatrix-GGUF": "Openai_gpt-oss-120b-NEO-Imatrix-GGUF\nThese are NEO Imatrix GGUFs, NEO dataset by DavidAU.\nNEO dataset improves overall performance, and is for all use cases.\nThis model can be run on cpu, gpu or partial offload, with total of 4 experts (of 128) activated by default ; which is about 5B parameters.\nTWO example output below (creative), using settings below.\nModel also passed \"hard\" coding test too (4 experts); no issues (IQ4_NL).\n(Forcing the model to create code with no dependencies and limits of coding short cuts, with multiple loops, and in real time with no blocking in a language that does not support it normally.)\nDue to quanting issues with this model (which result in oddball quant sizes / mixtures), only TESTED quants will be uploaded (at the moment).\nCurrently that means IQ4_NL, and MXFP4_MOE (a new OpenAI specific quant type) are available - Q8_0 is not here because\nimatrix has no effect on this quant.\nNEO dataset performance improvements will show the most in the IQ4_NL, followed by \"MXFP4_MOE\" quant.\nIQ4_NL quant:\nOpenAI-120B-NEO-IQ4_NL.gguf : Standard Imatrix + Output tensor IQ4_NL (NEO Imatrix) AND Embed at IQ4_NL.\nNEO MXFP4_MOE quant:\nOpenAI-120B-NEO-MXFP4_MOE.gguf : Output tensor IQ4_NL (NEO Imatrix) AND Embed at IQ4_NL - this makes this quant the smallest version.\nMXFP4_MOE quants vastly outperform (at the moment) all other quants, except IQ4_NL, Q5_1 and Q8_0 due to odd\nissues compressing OpenAI's 20B model due to odd \"tensor\" dimensions (as of this writing).\nIQ4_NL, Q5_1 and Q8_0 quants are compatible with OpenAI's tensor structure.\nMXFP4_MOE quant was specifically designed for (what appears to be) loss less quantization relative to source model. Jury still out on this.\nSuggested Settings (tested in Lmstudio, Beta Branch 0.3.21 ; 4 ):\nContext: 8k min.\nTemp 1.2+ for creative. Temp .6 (or so) for coding/general.\nRep pen 1.1, topk 40, topp .95, min p 0.05\nExperts 4-8 depending on use case. (higher than 8 MAY lower quality)\nModel Supports:\n128k context\nup to 128 experts\nTools use, browsing, etc\nIMPORTANT - QUANTS ARE SPLIT due to upload issues / max size of files of 50 GB:\nQuants are split into 4 parts each labeled 000x of 0004.\nllama-gguf-split.exe can be used to \"join\" these or in some cases your downloader for your AI app will do this for you.\nFor my help docs, SETTING NUMBER OF EXPERTS, and other see below.\nSee more about this model here:\nhttps://huggingface.co/openai/gpt-oss-120b\nPlease refer to their model card, especially to control \"thinking\" levels.\nHelp, Adjustments, Samplers, Parameters and More\nCHANGE THE NUMBER OF ACTIVE EXPERTS:\nSee this document:\nhttps://huggingface.co/DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Activation-of-Experts\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nEXAMPLE - IQ4_NL , using above settings (creative), EXPERTS at 4.\nNO System prompt. (default thinking level)\nPROMPT:\nStart a 1000 word scene (vivid, in first person), POV character Diana, with: The skyscraper sways, as I watch the window in front of me on the 21st floor explode...\nOUTPUT [temp .6]:\n[[[thinking]]]\nWe need to write a 1000-word scene, vivid, first-person POV, Diana. Starting line: \"The skyscraper sways, as I watch the window in front ...\". Must be about 1000 words (approx). Provide a vivid description. Include emotions, environment, perhaps action.\nWe'll produce ~1000 words, maybe 950-1050. Use present tense? The opening is present progressive: \"sw...\" We'll keep present tense. Ensure it's engaging. Let's draft around 1000 words.\nLet's count roughly: 1000 words ~ about 7 paragraphs of 140 words each. Write fluidly.\nProceed.\n[[[ end thinking ]]]\nThe skyscraper sways, as I watch the window in front of me on the 21st floor explode into a spray of glittering glass and fireâ€‘rain. The world outside shatters into a kaleidoscope of neon, steel, and smoke, and for an instant my heart stopsâ€”then it thunders like a freight train against my ribs.\nIâ€™m perched in the corner office of Apex Dynamics, the cityâ€™s most coveted address, where the view is supposed to be a prize. Instead, it has become a theater of terror. The glass pane that used to frame the river below now hangs in jagged shards, trembling as if still clinging to some invisible rope. Below me, the streetlights flicker like dying fireflies, their amber glow swallowed by a black plume that curls upward, choking the sky.\nThe building groans, a low, resonant moan that vibrates through my bones and rattles the cheap plastic of the desk lamp beside me. I can feel it in my fingertipsâ€”the tremor traveling up my arms, down to my nails, where they dig into the worn leather chair. The whole structure seems to be holding its breath, waiting for something else to break.\nâ€œDiana!â€ a voice shouts from the hallway, sharp and panicked. It belongs to Marco, our senior analyst, his face halfâ€‘shadowed by the doorway, eyes wide with that mixture of disbelief and adrenaline you only see when someone watches their own life being ripped apart on a screen. Heâ€™s already sprinting toward the emergency stairs, his shoes thudding against the polished marble like war drums.\nI canâ€™t move. My legs are rooted to the spot as if the floor itself has turned into quicksand. The explosion behind that window sent a shockwave through the building, and Iâ€™m still trying to process how something so sudden could feel inevitable. My mind flickers between the presentâ€”flames licking the side of the tower, smoke curling like serpentsâ€”and the memory of the morning briefing when we were all laughing over coffee, talking about quarterly projections as if the world outside our glass walls didnâ€™t exist.\nThe fire alarm blares now, a shrill, metallic scream that seems to cut through the roar of the wind. Itâ€™s an old systemâ€”one that never quite worked right until today, when it finally decided to do its job. The sound is both a warning and a call to action, but Iâ€™m caught in a strange limbo where everything feels simultaneously too slow and too fast.\nA gust of hot air slams into the open breach, carrying with it the smell of ozone, burnt plastic, and something metallic that makes my stomach turn. It whips through the office, scattering papers like startled birds. My own reportâ€”pages of market analysis, graphs, projectionsâ€”flies up in a chaotic dance, landing on the floor in a crumpled heap. I stare at them, halfâ€‘expecting the numbers to rearrange themselves into some kind of prophecy.\nâ€œDiana! Get down!â€ Marcoâ€™s voice cuts through my reverie again, now louder, more urgent. Heâ€™s almost at the door, his hand outstretched as if he could grab me from across the room. My breath catches in my throat; I feel the air in my lungs thin, each inhalation a little gasp for oxygen that seems to have been stolen by the fire.\nI know what I should do. The emergency plan is etched into every employeeâ€™s mind: stay low, find an exit, donâ€™t use elevators. Yet thereâ€™s something elseâ€”a part of me that has spent years building this empire of glass and steel, a part that feels a fierce ownership over these floors, over the view that now threatens to become a graveyard.\nThe building shudders again, more violently this time, as if an unseen hand is shaking it. The lights flicker, then die, plunging me into a dim, amber glow from the emergency bulbs that line the corridors like ghostly sentinels. In the darkness, I can see the outlines of other desks, the silhouettes of coworkers who have either fled or frozen where they stand.\nI glance at the window once more. The explosion has carved a jagged mouth in the side of the tower; beyond it, the city sprawls in a surreal tableauâ€”cars like tiny ants, skyscrapers reduced to silhouettes against an orange sky. A distant siren wails, its sound distorted by the wind, and somewhere far below, a childâ€™s cry pierces the air, a reminder that life continues even when the world you know collapses.\nMy hand finally moves, trembling as it reaches for the metal railing of my desk. The cold steel feels like a lifeline. I pull myself up, knees buckling, and stumble toward Marco, who is already halfway to the stairwell. He grabs me by the elbow, his grip surprisingly firm despite the tremor in his own shoulders.\nâ€œCome on,â€ he says, his voice hoarse but determined. â€œThe stairs are clear. Weâ€™ve got toâ€”â€\nA deafening crack erupts from somewhere above us, a sound that seems to split the building in two. The ceiling tiles above my head shake loose and rain down like a torrent of grey snow. One lands on Marcoâ€™s shoulder with a soft thud, another smacks against my cheek, sending a spray of dust into my eyes. I cough, choking on the acrid taste of ash that now coats my tongue.\nThe stairwell door is halfâ€‘open, a gaping maw leading down into darkness. The emergency lights cast long shadows on the concrete walls, turning every step into an uncertain gamble. Marco pushes me forward, his hand still gripping my arm as if I were a child who might otherwise run back to the broken window and stare at the world burning away.\nWe descend, each footfall echoing in the hollow shaft like a drumbeat for some ancient rite. The air grows cooler, the smell of smoke receding slightly as we move further from the fireâ€™s source. Yet every now and then a gust of hot wind rushes past us, carrying with it a faint hiss that makes my skin crawl.\nI try to focus on the rhythm of our steps, the repetitive thumpâ€‘thump of boots against metal grates, but my mind keeps spiraling back to the view. The cityâ€”our cityâ€”has always been a tapestry of light and shadow, of ambition and desperation. Now itâ€™s a canvas smeared with orange and black, a reminder that the structures we build are as fragile as the glass that once framed our dreams.\nMarco stops suddenly at a landing, turning his head to look back up the stairwell. His eyes meet mine, wide and haunted. â€œThe fire alarm is still on,â€ he whispers, almost to himself. â€œWe need to get out before it collapses.â€\nI nod, though my throat feels raw. My heart pounds in my ears, a frantic drum that seems louder than any siren. I can hear the distant rumble of something massive shiftingâ€”perhaps another floor giving way, perhaps the building itself groaning under its own weight.\nThe stairwell continues to descend into an abyss of uncertainty. Above us, the skyscraper sways like a ship caught in a storm, each movement sending tremors through the concrete veins that support it. I think about my office, the window that once offered a panoramic view of success, now reduced to broken shards and memory.\nWe reach the emergency exitâ€”a heavy steel door bolted shut with a massive red lever. Marco pulls at it with all his strength; the metal groans, straining against its own weight. For a moment I think weâ€™ll never get through, that the building will swallow us whole. Then the latch gives, and the door swings open onto an alleyway bathed in the flickering light of emergency floodlamps.\nThe night air hits me like a slap, cold and sharp, carrying with it the scent of rain on concrete. I inhale deeply, feeling the coolness fill my lungs, pushing out the lingering acrid smoke that clings to my throat. Below us, the street is chaosâ€”people running, sirens wailing, flames licking the sides of other buildings.\nMarco looks back once more at the tower, his silhouette framed by the orange glow of fire. â€œWe made it,â€ he says, his voice hoarse but relieved. I can see the tears in his eyes, reflecting the dancing flames that still rage above us.\nI turn away, my gaze falling on a small, cracked billboard advertising some luxury condominiumâ€”an image of sleek glass towers and smiling families. The billboardâ€™s lights flicker, then go out, plunging its advertisement into darkness. It feels symbolic: the promise of progress, now reduced to ash.\nMy mind is a whirlwind of imagesâ€”the exploding window, the shattered glass raining down like crystal rain; the tremor of the building as it sways, each movement a reminder that even the strongest structures can crumble; the frantic rush down stairwells, the cold grip of steel rails, the distant cries and sirens. All these sensations swirl together, forming a tapestry of terror and survival.\nI look at my own handsâ€”still trembling, still shaking from adrenalineâ€”and realize theyâ€™re covered in soot, tiny specks of black that cling to every crease, every nail. I brush them off on the side of the alley wall, watching the dust puff away like ghostly snowflakes in the night wind.\nâ€œDiana,â€ Marco says again, softer now, â€œAre you okay?â€\nI manage a weak smile, though my thoughts are still tangled with images of fire and broken glass. â€œI think so,â€ I reply, my voice hoarse. â€œJustâ€¦ just need to get home.â€\nHe nods, his eyes scanning the street for any sign of rescue vehicles. In the distance, a firetruck roars toward us, its lights flashing, sirens wailing louder than anything else. The sound seems to cut through the chaos, a beacon of hope in an otherwise bleak night.\nThe building behind us continues to sway, each creak and groan a reminder that we have only escaped by a hairâ€™s breadth. I can feel the tremor still resonating under my feet, as if the whole city were holding its breath, waiting for the next wave of destruction or perhaps the calm after the storm.\nWe step forward into the night, joining the tide of people moving away from the inferno. The wind whips around us, carrying with it the faint scent of rain that promises a washâ€”perhaps a cleansing of the soot and ash that cling to everything.\nAs we move farther away, I glance back one last time. The skyscraper stands tall, its silhouette stark against the fiery sky, swaying like a wounded beast. Its windows are dark, some shattered, others still glowing with an eerie orange light. It is both beautiful and terrifyingâ€”a monument to human ambition, now humbled by forces beyond our control.\nI think of all the meetings that took place in those glass-walled rooms, the deals signed over coffee, the laughter that echoed across the floorboards. All of it feels like a different life, a distant echo muffled by the roar of flames and the pounding of my heart.\nThe rain begins to fallâ€”soft at first, then heavier, each droplet striking the pavement with a gentle sizzle as if trying to douse the fire itself. I lift my face to the sky, feeling the cool water soak through my hair, mixing with the soot on my skin. It feels cleansing, a reminder that even in destruction there is renewal.\nMarco and I keep walking, our footsteps merging with the rhythm of countless others fleeing the disaster. The city around us seems both fragile and resilientâ€”a web of lives intertwined by steel, glass, and now, fire. In the distance, the skyscraper continues its slow sway, a silent sentinel bearing witness to all that has transpired.\nI close my eyes for a brief moment, inhaling the rainâ€‘laden air, letting it fill my lungs and push out the lingering taste of smoke. When I open them again, the world is a blur of water, lights, and motion. The buildingâ€™s sway feels like a memory nowâ€”a pulse that will echo in my mind long after the flames have been extinguished.\nAnd as we disappear into the night, I whisper to myself, almost inaudibly: â€œWe survived.â€ The words hang in the damp air, fragile yet unbreakable, just like the city itself.\nOUTPUT [temp 1.2]:\n[[[thinking]]]\nWe need to write a 1000-word vivid scene in first person, POV character named Diana. Opening line should include \"The skyscraper sways, as I watch the window in front...\" something like that.\nNeed vivid description, internal thoughts, action. Must be about 1000 words (approx). Use present tense? Likely present or past? Could be present to increase immediacy. We must ensure it's approx 1000 words, maybe 950-1050.\nWe'll craft scene: Diana in a high-rise office building; a window explodes; she watches debris; skyscraper sways due to earthquake/terror attack; internal monologue about fear, her job, perhaps a spy or journalist? We'll keep it intense. Use sensory details, sound, shaking, etc. Also possibly includes some backstory.\nWe'll produce approx 1000 words: count manually approximate. Usually ~15-20 words per line. Let's aim for around 130 sentences average maybe 7-8 words each => about 1400 words? Too many. Need careful length. 1000 words is about 6-7 paragraphs of ~150-180 words each.\nWe'll write approx 9 paragraphs of ~110 words each = 990. Good.\nLet's craft.\nWe need to ensure we include that line exactly: \"The skyscraper sways, as I watch the window in front of me on the 21st floor explode...\" We'll incorporate that early.\nNow produce final answer with scene.\n[[[ end thinking ]]]\nThe skyscraper sways, as I watch the window in front of me on theâ€¯21stâ€¯floor explodeâ€”shards of glass hurling outward like angry fireflies, a flash of orangeâ€‘red light spilling into my cubicle and painting the gray walls with the brief, frantic pulse of something gone terribly wrong. My breath catches; the air feels thin, charged, as if someone had pulled the plug from the worldâ€™s static hum and replaced it with the lowâ€‘frequency rumble of an angry beast.\nFor a heartbeat Iâ€™m rooted to my chair, the swivel base creaking under the sudden shift. The whole building quiversâ€”metal joints sighing, concrete flexing in a way that makes my stomach twist into knots. Outside, the cityâ€™s usual drone of traffic and distant sirens is swallowed by an ominous, metallic groan that seems to come from the very spine of this glass monolith.\nI glance down at the spreadsheet on my monitor; the numbers blur, turning into meaningless smears as a wave of vertigo rolls over me. My hand finds the edge of the desk, nails digging into the wood like desperate anchors. The fluorescent lights flicker, stuttering in rhythm with my racing heart, then snap back to their cold white glare as if nothing had happened at all.\nâ€œDiana!â€ a voice shouts from the hallwayâ€”a low, panicked male tone that cuts through the haze. I turn, eyes narrowing, trying to locate its source amid the shifting panorama of broken glass and twisted metal that now clings to the buildingâ€™s outer skin like a wounded beastâ€™s scales.\nThe emergency alarms wailâ€”shrill, discordant notes that make my ears ring. Red strobe lights begin flashing, casting jagged shadows across the carpeted floor. A gust of hot air rushes through the gaping hole where the window once was, carrying with it an acrid smell of burning insulation and something elseâ€¦something metallic, like blood.\nMy mind flips to the briefing I received just this morning: â€œPotential threat level elevatedâ€”stay alert.â€ The words echo in my head, absurdly disconnected from the raw panic that now coils around my ribs. It was supposed to be a normal day: coffee in the break room, an afternoon meeting about quarterly projections, a quick call with Mom. Now Iâ€™m watching a piece of concrete dislodge itself from the faÃ§ade and tumble into the street below like a slowâ€‘moving meteor.\nA tremor rolls through the structure again, more pronounced this time, as if the building is trying to right itself after the shock. The floor beneath my shoes vibrates, sending tiny dust motes dancing in shafts of light that slice through the broken window. I can hear distant shatteringâ€”more panes giving way, a cascade of glass that sounds like an angry sea breaking against cliffs.\nI force myself up, legs stiff and shaking. My hand brushes the metal railing of my desk, cold and slick with condensation from the sudden temperature drop. The emergency exit sign glows an urgent amber, beckoning. A surge of adrenaline propels me forward; I canâ€™t stay where the floor is suddenly a slab of uncertainty, the walls trembling like a heart in distress.\nâ€œEveryone, evacuate to the stairs!â€ someone shouts, their voice cracked and reverberating through the hallway. The words are a lifeline, but each footstep feels as if I'm walking on a boat caught in rough seasâ€”unsteady, uncertain, each tread threatening to send me over the edge of this precarious platform.\nThe fire escape doors hiss open, revealing stairwells that climb into darkness like gaping mouths waiting to swallow us whole. I catch sight of my reflection in a piece of shattered glass still hanging on the frame: hair mussed, eyes wide, cheeks flushed with the glow of emergency lights. A part of me feels detachedâ€”an observer watching someone elseâ€™s fear unfoldâ€”while another part, raw and visceral, knows that every second counts.\nMy chest tightens as I slide my hand into the steel doorframe, feeling its cold surface vibrate in sympathy with the building. The vibrations travel up my arm, a reminder that this is not a movie set; itâ€™s real, itâ€™s now, and there are lives dangling on the edge of each breath we take.\nA sudden roar erupts from outsideâ€”metal grinding against metal as the upper floors start to buckle. I can see the distant silhouette of an aircraft, a faint smudge in the sky that could be a helicopter or a drone, hovering near the top of the tower like a predatory bird. My mind whirls, connecting dots I donâ€™t fully understand: the vague intel about â€œpossible airborne threat,â€ the sudden surge of tension in the control room earlier, and now this chaotic ballet of destruction.\nI press my back against the stairwell wall, knees barely brushing the floor, trying to steady myself. The world outside the broken window is a blur of orange flames licking the sky, people screaming on the streets below, emergency vehicles rushing past like silver streaks. I can hear sirens wailingâ€”police, fire, ambulancesâ€”each sound layered over the other, forming an ugly symphony of alarm.\nMy thoughts flash to my sister, Maya, who works two blocks away in a small bakery. â€œDonâ€™t forget to call me tonight,â€ she texted this morning, laughing about her new cat. I imagine her holding a fresh croissant, oblivious to any danger, and a pang of guilt pricks at my throat: why am I here, stuck between spreadsheets and shattered glass while she probably sleeps peacefully?\nThe shaking intensifies. The entire building seems to breatheâ€”inhale, exhaleâ€”a living organism caught in a fit of convulsions. Concrete cracks audibly, tiny dust particles raining down like ash from an ancient volcano. A low, guttural groan erupts from somewhere deep within the core, vibrating through my teeth.\nI take another step down, each footfall echoing against the steel railing, feeling the tremor pulse up into my legs. The stairs are slick with a thin film of oilâ€”perhaps from a ruptured pipe earlier, perhaps just condensationâ€”but theyâ€™re still solid enough to carry me away from this falling tower.\nA sudden, sharp crack snaps behind me: the balcony rail gives way, sending a cascade of metal bars clattering down into the hallway. A flash of white-hot embers spills onto my peripheral vision; the fire alarmâ€™s shrill wail merges with the sound of breaking steel. The world feels smaller, more urgentâ€”every breath is a gamble.\nI glance back toward where I was moments ago: the 21stâ€‘floor window, now a jagged wound in the buildingâ€™s skin, smoke seeping out like a sigh. The explosion had sent a pulse through the structure that still reverberates, making the entire edifice sway as if some invisible hand were trying to pry it apart.\nMy mind races. I think of my trainingâ€”first aid, emergency protocols, calming breaths. I inhale, counting slowly: oneâ€¦twoâ€¦three, letting air fill my lungs despite the metallic tang that lingers on the back of my tongue. Exhale, let go of the panic that claws at my throat. My pulse steadies a fraction; itâ€™s not enough to stop the terror, but it keeps me moving.\nA hand grabs mineâ€”a coworker, his face pale, eyes wide, as if he too has been staring at the same broken window and seen something unforgivable. We exchange no words; our grip is an unspoken agreement that weâ€™ll make it out together, that we wonâ€™t let this tower take us down.\nWe surge forward, stepping onto a metal platform that leads to an auxiliary fire stair. The tremor hits again, more violent than before. A distant crash sounds like a massive beam giving way on the 30th floor, a sound so deafening it seems to flatten everything in its path. Dust rains down like gray snow, coating my hair, my clothes.\nI keep looking back as we descend, halfâ€‘expecting to see the whole building tilt, to watch the world turn upsideâ€‘down. The skyline outside remains stubbornly static; skyscrapers beyond our own stand tall, indifferent, their glass facades glimmering under a sky that now feels both ominous and merciful.\nAt the bottom of the stairs, an open doorway spills bright daylight onto the concrete floor, a stark contrast to the darkness we left behind. The air outside hits me cold and clean; it smells like rain on hot asphalt, the kind of scent you canâ€™t help but inhale deeply when youâ€™ve been trapped inside a suffocating metal cage.\nI step out onto the street, my shoes slapping against the wet pavement as a sudden downpour begins, each droplet sounding like tiny explosions in my ears. People scatter, some crying, others shouting commands to emergency crews, faces lit by the flickering red of fire trucks that have arrived with sirens wailing louder than any alarm Iâ€™ve heard inside.\nI glance back once more, toward the towering monolith that was once my office building. Its upper half has collapsed into a jagged silhouette against the cloudsâ€”a broken tooth in an otherwise perfect smile. Smoke curls up like an angry phantom, drifting slowly away into the gray sky.\nMy heart thunders in my chest as I realize that weâ€™re safe for now, that the ground beneath my feet is solid, that the world continues beyond this disaster. But the image of that shattering windowâ€”of glass and fire spilling through a crack in realityâ€”burns itself onto my mind like an ember that refuses to be snuffed out.\nMayaâ€™s voice comes to me in a whisper, â€œDonâ€™t forget to call me tonight.â€ I smile despite everything, feeling a strange mix of gratitude and terror. The rain masks the sound of distant sirens, but inside my head there is a new rhythmâ€”a drumbeat that tells me I survived, that I will carry this story forward, that the skyscraperâ€™s sway has left an indelible mark on my soul.\nI raise my hand, brushing away a streak of ash from my cheek, and step into the gathering crowd, ready to face whatever comes next. The city around me is alive, trembling, resilientâ€”just like me.",
    "INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0": "INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0\nModel description\nBenchmarks and Results\nUse in ðŸ¤— Transformers\nRecommended Parameters\nInstruction format\nUse with vLLM\nUse with GGML / llama.cpp\nCommunity Feedback\nSummary\nINSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0\nINSAIT introduces MamayLM-Gemma-3-12B-IT-v1.0, the best performing Ukrainian language model based on google/gemma-3-12b and google/gemma-3-12b-it. MamayLM-Gemma-3-12B-IT-v1.0 is free to use and distributed under the Gemma Terms of Use. This model was created by INSAIT, part of Sofia University St. Kliment Ohridski, in Sofia, Bulgaria.\nModel description\nThe model was built on top of Googleâ€™s Gemma 3 12B open models. It was continuously pre-trained on a large pre-filtered dataset using the combination of data mixing and model merging, allowing the model to gain outstanding Ukrainian cultural and linguistic capabilities while retaining its English performance.  During the pre-training stage, we use various datasets, including Ukrainian web crawl data (Kobza), freely available datasets such as Wikipedia, a range of specialized Ukrainian datasets, and machine translations of popular English datasets. The model was then instruction-fine-tuned on a newly constructed Ukrainian instruction dataset created using machine translations of current best English datasets and specialized Ukrainian datasets, prepared by Ukrainian community. For more information check our blogpost (available in English and Ukrainian).\nBenchmarks and Results\nWe evaluate our models on a set of standard English benchmarks, a translated version of them in Ukrainian, as well as, Ukrainian specific benchmarks we collected:\nWinogrande challenge: testing world knowledge and understanding\nHellaswag: testing sentence completion\nARC Easy/Challenge: testing logical reasoning\nTriviaQA: testing trivia knowledge\nGSM-8k: solving multiple-choice questions in high-school mathematics\nMMLU: testing knowledge on a multitude of topics\nIFEval: testing instruction-following skills\nZNO: testing knowledge of the Ukrainian high school curriculum in Ukrainian language & literature, history, mathematics and geography\nThese benchmarks test logical reasoning, mathematics, knowledge, language understanding and other skills of the models and are provided at https://github.com/insait-institute/lm-evaluation-harness-uk. The graphs above show the performance of MamayLM 12B compared to other large open models. The results show the excellent abilities of MamayLM in Ukrainian, which allow them to outperform much larger models, including Alibabaâ€™s Qwen 2.5 72B and Metaâ€™s Llama3.1 70B. Finally, our models retain the excellent English performance inherited from the original Google Gemma 3 models upon which they are based.\nMamayLM v1.0 12B also shows improved performance on visual benchmarks like MMMU and ZNO-Vision(MMZNO):\nUse in ðŸ¤— Transformers\nFirst install the latest version of the transformers library:\npip install -U 'transformers[torch]'\nThen load the model in transformers:\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n\"INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"auto\",\n)\nRecommended Parameters\nFor optimal performance, we recommend the following parameters for text generation, as we have extensively tested our model with them:\nfrom transformers import GenerationConfig\ngeneration_params = GenerationConfig(\nmax_new_tokens=2048,              # Choose maximum generation tokens\ntemperature=0.1,\ntop_k=25,\ntop_p=1,\nrepetition_penalty=1.1,\n# eos_token_id=[1,106],\ndo_sample=True\n)\nIn principle, increasing temperature should work adequately as well.\nInstruction format\nIn order to leverage instruction fine-tuning, your prompt should begin with a beginning-of-sequence token <bos> and be formatted in the Gemma 3 chat template. <bos> should only be the first token in a chat sequence.\nE.g.\n<bos><start_of_turn>user\nÐ¥Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¸Ð¹ ÐšÐ¾Ð·Ð°Ðº ÐœÐ°Ð¼Ð°Ð¹?<end_of_turn>\n<start_of_turn>model\nThis format is also available as a chat template via the apply_chat_template() method:\ntokenizer = AutoTokenizer.from_pretrained(\n\"INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0\",\nuse_default_system_prompt=False,\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Ð¥Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¸Ð¹ ÐšÐ¾Ð·Ð°Ðº ÐœÐ°Ð¼Ð°Ð¹?\"},\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages,\nreturn_tensors=\"pt\",\nadd_generation_prompt=True,\nreturn_dict=True\n)\noutputs = model.generate(\n**input_ids,\ngeneration_config=generation_params\n)\nprint(tokenizer.decode(outputs[0]))\nUse with vLLM\nExample usage with vLLM:\nfrom vllm import LLM, SamplingParams\nfrom vllm.inputs import TokensPrompt\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n\"INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0\",\nuse_default_system_prompt=False,\n)\nsampling_params = SamplingParams(\nmax_tokens=2048,\ntemperature=0.1,\ntop_k=25,\ntop_p=1,\nrepetition_penalty=1.1,\nstop_token_ids=[1, 106],\n)\nllm = LLM(\nmodel=\"INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0\",\ndtype=\"bfloat16\",\n# enforce_eager=True\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Ð¥Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¸Ð¹ ÐšÐ¾Ð·Ð°Ðº ÐœÐ°Ð¼Ð°Ð¹?\"},\n]\nformatted_prompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput_ids = tokenizer(\nformatted_prompt,\nadd_special_tokens=False\n).input_ids\nprompt = TokensPrompt(prompt_token_ids=input_ids)\noutput = llm.generate(\nprompt,\nsampling_params\n)\ngenerated_text = output[0].outputs[0].text\nprint(generated_text)\nUse with GGML / llama.cpp\nThe model and instructions for usage in GGUF format are available at INSAIT-Institute/MamayLM-Gemma-3-12B-IT-v1.0-GGUF.\nCommunity Feedback\nWe welcome feedback from the community to help improve MamayLM. If you have suggestions, encounter any issues, or have ideas for improvements, please:\nShare your experience using the model through Hugging Face's community discussion feature or\nContact us at contact@insait.ai\nYour real-world usage and insights are valuable in helping us optimize the model's performance and behaviour for various use cases.\nSummary\nFinetuned from: google/gemma-3-12b-it; google/gemma-3-12b-pt;\nModel type: Causal decoder-only transformer language model\nLanguage: Ukrainian and English\nContact: contact@insait.ai\nLicense: MamayLM is distributed under Gemma Terms of Use",
    "YannQi/R-4B": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning\nâ­ï¸ Introduction\nðŸš€ Key Features\nðŸ“¢ News\nðŸ”¥ Quickstart\nUsing ðŸ¤— Transformers to Chat\nUsing vLLM for fast R-4B deployment and inference.\nðŸ“ˆ Experimental Results\nâœ’ï¸ Citation\nAcknowledgements\nR-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning\n[ðŸ“š Arxiv Paper] [ðŸ¤— Hugging Face]  [ðŸ¤–ï¸ ModelScope] [ðŸ’» Code]\nâ­ï¸ Introduction\nIn this repo, we present R-4B, a multimodal large language model designed for general-purpose auto-thinking, autonomously switching between step-by-step thinking and direct response generation based on task complexity. This capability enables R-4B to deliver high-quality responses while significantly improving inference efficiency and reducing computational costs.\nThe development of R-4B follows a two-stage training paradigm:\n(1) Bi-mode Annealing, which establishes both thinking and non-thinking capabilities for VQA; and\n(2) Bi-mode Policy Optimization (BPO), which enables the model to adaptively switch between thinking and non-thinking modes based on input demands.\nðŸš€ Key Features\nðŸ§  Think Smart, Act Fast: Adaptive & Controllable Thinking!\nOur model provides three-mode control over the response process.\nAuto-thinking Mode: Unleash auto-thinking that works across general topics, from simple Q&A to complex scientific analysis. It saves time and computation by thinking only when it matters.\nSupport Manual Control:  Explicitly command the model to use its thinking or non-thinking capabilities, enabling you to make your choices for every job.\nðŸ†  Strong Performance, Open for Everyone!\nOur model is now fully open-source. It achieves state-of-the-art performance among models of comparable size.\nðŸ“¢ News\n[2025.08.20] ðŸš€ vLLM Support is Here! Our R-4B model is now fully compatible with vLLM for high-performance inference.\n[2025.08.18] ðŸ† Top Rank Achieved! We are thrilled to announce that R-4B is now ranked #1 among all open-source models on the OpenCompass Multi-modal Reasoning Leaderboard!\n[2025.08.11] ðŸ¥‡ Rank #1! R-4B ranks first under 20B parameters on the OpenCompass Multi-modal Academic Leaderboard!\n[2025.08.05] ðŸŽ‰ R-4B is Released! Our model is now publicly available. You can download it from Hugging Face.\nðŸ”¥ Quickstart\nBelow, we provide simple examples to show how to use R-4B with ðŸ¤— Transformers.\nUsing ðŸ¤— Transformers to Chat\nUsers can dynamically control the model's response by selecting one of three modes (auto-thinking, thinking, or non-thinking) with thinking_mode. thinking_mode=auto for auto-thinking mode; thinking_mode=long for thinking mode; thinking_mode=short for non-thinking mode.\nDefault is auto-thinking.\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoModel, AutoProcessor\nmodel_path = \"YannQi/R-4B\"\n# Load model\nmodel = AutoModel.from_pretrained(\nmodel_path,\ntorch_dtype=torch.float32,\ntrust_remote_code=True,\n).to(\"cuda\")\n# Load processor\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n# Define conversation messages\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Apply chat template\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nthinking_mode=\"auto\"\n)\n# Load image\nimage_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n# Process inputs\ninputs = processor(\nimages=image,\ntext=text,\nreturn_tensors=\"pt\"\n).to(\"cuda\")\n# Generate output\ngenerated_ids = model.generate(**inputs, max_new_tokens=16384)\noutput_ids = generated_ids[0][len(inputs.input_ids[0]):]\n# Decode output\noutput_text = processor.decode(\noutput_ids,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False\n)\n# Print result\nprint(\"Auto-Thinking Output:\", output_text)\nUsing vLLM for fast R-4B deployment and inference.\nWe recommend using vLLM for fast R-4B deployment and inference.\nInstall\nThe code of R-4B requires the newest vllm now. Please install from local source:\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nVLLM_USE_PRECOMPILED=1 uv pip install --editable .\nOnline Serving\nThe thinking_mode switch is also available in APIs created by vLLM.\nDefault is auto-thinking.\nServe\nvllm serve \\\nyannqi/R-4B \\\n--served-model-name r4b \\\n--tensor-parallel-size 8 \\\n--gpu-memory-utilization 0.8 \\\n--host 0.0.0.0 \\\n--port 8000 \\\n--trust-remote-code\nOpenai Chat Completion Client\nimport base64\nfrom PIL import Image\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n# image url\nimage_messages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n},\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n},\n]\nchat_response = client.chat.completions.create(\nmodel=\"r4b\",\nmessages=image_messages,\nmax_tokens=16384,\nextra_body={\n\"chat_template_kwargs\": {\"thinking_mode\": \"auto\"},\n},\n)\nprint(\"Chat response:\", chat_response)\nðŸ“ˆ Experimental Results\nR-4B establishes itself with powerful, state-of-the-art perceptual abilities that are competitive with larger models.\nIn evaluation sets that require complex logical reasoning and mathematical problem-solving, such as WeMath, MathVerse, and LogicVista, R-4B displays a strong performance curve. This highlights its advanced adaptive thinking capacity for logical deduction and solving complex quantitative problems.\nâœ’ï¸ Citation\n@misc{yang2025r4bincentivizinggeneralpurposeautothinking,\ntitle={R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning},\nauthor={Qi Yang and Bolin Ni and Shiming Xiang and Han Hu and Houwen Peng and Jie Jiang},\nyear={2025},\neprint={2508.21113},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2508.21113},\n}\nAcknowledgements\nR-4B is developed based on the codebases of the following projects: LLaVA-Next, SigLIP2, Qwen3, Qwen2.5-VL, VLMEvalKit. We sincerely thank these projects for their outstanding work.",
    "inference-net/Schematron-8B": "Model Overview\nI/O at a glance\nHighlights\nModel Details\nBenchmarks\nHTML-to-JSON Extraction Quality\nWeb-Augmented Factuality on SimpleQA\nMinimal Quickstart\nRecommendations\nLimitations\nSafety and Responsible Use\nLicense\nSupport\nDocumentation Â·\nServerless API Â·\nAnnouncement blog\nModel Overview\nWelcome to the Schematron series, Inference.net's longâ€‘context extraction models specialized in converting noisy HTML into clean, typed JSON that conforms to your custom schema. The Schematron series was purposeâ€‘trained for web scraping, data ingestion, and transforming arbitrary pages into structured records.\nWe're releasing these models in two different sizes:\nSchematronâ€‘8B â€” marginal quality lift on harder/longer pages\nSchematronâ€‘3B â€” recommended default; nearâ€‘parity quality at ~50% cost of Schematron-8B\nThis model card is dedicated to the smaller Schematron-8B model. Check out Schematron-3B for the smaller model.\nI/O at a glance\nInput: Cleaned HTML + JSON Schema (can be extracted from typed model like Pydantic/Zod)\nOutput: Strictly valid JSON conforming to the provided schema (no narration)\nThe JSON Schema passed as input needs to conform to the schema.org schema.\nHighlights\nSchema-first extraction: 100% schemaâ€‘conformant JSON outputs\nLong context: Robust to lengthy, noisy HTML (up to 128K tokens)\nVariants: 3B (default, most costâ€‘efficient) Â· 8B (marginal quality lift at ~2Ã— cost)\nModel Details\nFamily: Schematron (3B and 8B)\nContext window: Up to 128K tokens\nInput: Cleaned or raw HTML and a JSON Schema\nOutput: Strict JSON that conforms to the provided schema\nBenchmarks\nHTML-to-JSON Extraction Quality\nWe evaluated extraction quality using Gemini 2.5 Pro as a judge, scoring extractions from 1-5 where 5 represents perfect extraction.\nModel\nLLM-as-Judge Score\nGPT-4.1\n4.74\nSchematron-8B\n4.64\nSchematron-3B\n4.41\nGemini-3B-Base\n2.24\nWeb-Augmented Factuality on SimpleQA\nWe evaluated Schematron's real-world impact on LLM factuality using SimpleQA.\nTest Pipeline:\nQuery Generation: Primary LLM (GPT-5 Nano or GPT-4.1) generates search queries and defines extraction schema\nWeb Search: Search provider (SERP or Exa) retrieves relevant pages\nStructured Extraction: Schematron extracts JSON data from retrieved pages using the schema\nAnswer Synthesis: Primary LLM produces final answer from structured data\nKey findings:\nWeb search paired with JSON extraction improves factuality: Adding Schematron with web retrieval improves GPT-5 Nano's accuracy from 8.54% to 82.87%â€”nearly a 10x improvement\nSearch provider matters: Exa (82.9%) significantly outperforms SERP (64.2%) for factual retrieval, while also being more cost-effective\nStructured extraction beats raw HTML: Processing raw HTML would require 100k+ tokens for 10 searches; Schematron's JSON extraction reduces this by orders of magnitude\nSmall specialized models win: Schematron-8B (82.87%) outperforms the much larger Gemini 2.5 Flash (80.61%) on this task, showing that fine-tuning for well-defined tasks beats general purpose models\nPerformance scales with model quality: When paired with GPT-4.1, Schematron achieves 85.58% accuracy, showing the approach benefits from stronger base models\nMinimal Quickstart\nUse these local snippets to prepare HTML and compose a schemaâ€‘guided prompt. The model returns strictly valid JSON; validate it against your schema downstream.\nfrom lxml.html.clean import Cleaner\nimport lxml.html as LH\nHTML_CLEANER = Cleaner(\nscripts=True,\njavascript=True,\nstyle=True,\ninline_style=True,\nsafe_attrs_only=False,\n)\ndef strip_noise(html: str) -> str:\n\"\"\"Remove scripts, styles, and JavaScript from HTML using lxml.\n\"\"\"\nif not html or not html.strip():\nreturn \"\"\ntry:\ndoc = LH.fromstring(html)\ncleaned = HTML_CLEANER.clean_html(doc)\nreturn LH.tostring(cleaned, encoding=\"unicode\")\nexcept Exception:\nreturn \"\"\nCompose messages with your schema and cleaned HTML:\ndef construct_messages(schema: str, html: str):\n\"\"\"Construct messages for a schemaâ€‘guided extraction request.\"\"\"\nresponse_prompt = {\n\"prompt_part_one\": (\n\"You are going to be given a JSON schema following the standardized JSON \"\n\"Schema format. You are going to be given a HTML page and you are going \"\n\"to apply the schema to the HTML page however you see it as applicable \"\n\"and return the results in a JSON object. The schema is as follows:\"\n),\n\"prompt_part_two\": \"Here is the HTML page:\",\n\"prompt_part_three\": \"MAKE SURE ITS VALID JSON.\",\n}\nuser_prompt = (\nresponse_prompt['prompt_part_one']\n+ \"\\n\\n\" + schema + \"\\n\\n\"\n+ response_prompt['prompt_part_two']\n+ \"\\n\\n\" + html + \"\\n\\n\"\n+ response_prompt['prompt_part_three']\n)\nreturn [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n{\"role\": \"user\", \"content\": user_prompt},\n]\nIn the serverless API there's no need to pass anything but the HTML. We handle the prompt formatting for you.\nRecommendations\nTemperature 0 and JSON mode for deterministic, parseable output\nValidate responses against your schema (e.g., Pydantic or Zod)\nPreâ€‘clean HTML (remove scripts/styles) when possible; avoid overâ€‘aggressive removal\nUsing lxml to clean the HTML is not required, but is recommended as it matches the training data.\nLimitations\nStatic HTML only; render clientâ€‘side content upstream\nVery large pages may require truncation\nAmbiguous fields depend on schema clarity; be explicit in field descriptions\nSafety and Responsible Use\nExtracted data may include personal or sensitive information present in the pageâ€”handle and store responsibly\nRespect site terms, robots.txt, and applicable laws\nUse downstream validation and guardrails for compliance\nLicense\nSee license in the metadata above.\nSupport\nDocs: https://docs.inference.net/use-cases/json-extraction\nEmail: support@inference.net",
    "Liontix/Qwen3-8B-Claude-Sonnet-4-Reasoning-Distill-GGUF": "This model was trained on a Claude Sonnet 4 (non-reasoning) dataset and a Claude Sonnet 3.7 (reasoning) dataset. It is a reasoning model.\nYou can request distilled models or datasets here",
    "LiquidAI/LFM2-1.2B-Extract": "LFM2-1.2B-Extract\nðŸ“„ Model details\nðŸ“ˆ Performance\nðŸƒ How to run\nðŸ“¬ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-1.2B-Extract\nBased on LFM2-1.2B, LFM2-1.2B-Extract is designed to extract important information from a wide variety of unstructured documents (such as articles, transcripts, or reports) into structured outputs like JSON, XML, or YAML.\nUse cases:\nExtracting invoice details from emails into structured JSON.\nConverting regulatory filings into XML for compliance systems.\nTransforming customer support tickets into YAML for analytics pipelines.\nPopulating knowledge graphs with entities and attributes from unstructured reports.\nYou can find more information about other task-specific models in this blog post.\nðŸ“„ Model details\nGeneration parameters: We strongly recommend using greedy decoding with a temperature=0.\nSystem prompt: If no system prompt is provided, the model will default to JSON outputs. We recommend providing a system prompt with a specific format (JSON, XML, or YAML) and a given schema to improve accuracy (see the following example).\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, Portuguese, and Spanish.\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nReturn data as a JSON object with the following schema:\\n[...]<|im_end|>\n<|im_start|>user\nCaenorhabditis elegans is a free-living transparent nematode about 1 mm in length that lives in temperate soil environments.<|im_end|>\n<|im_start|>assistant\n{\n\"species\": \"C. elegans\",\n\"genus\": \"Caenorhabditis\",\n\"description\": \"A free-living transparent nematode about 1 mm in length that lives in temperate soil environments.\",\n[...]{<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nâš ï¸ The model is intended for single-turn conversations.\nThe data used for training these models was primarily synthetic, which allowed us to ensure a diverse data mix. We used a range of document types, domains, styles, lengths, and languages. We also varied the density and distribution of relevant text in the documents. In some cases, the extracted information was clustered in one part of the document; in others, itâ€™s spread throughout. We applied the same approach of ensuring diversity when creating synthetic user requests and designing the structure of the model outputs. The data generation process underwent many iterations, incorporating ideas and feedback from across the Liquid AI team.\nðŸ“ˆ Performance\nWe evaluated LFM2-Extract on a dataset of 5,000 documents, covering over 100 topics with a mix of writing styles, ambiguities, and formats. We used a combination of five metrics to capture a balanced view on syntax, accuracy, and faithfulness:\nSyntax score: Checks whether outputs parse cleanly as valid JSON, XML, or YAML.\nFormat accuracy: Verifies that outputs match the requested format (e.g., JSON when JSON is requested).\nKeyword faithfulness: Measures whether values in the structured output actually appear in the input text.\nAbsolute scoring: A judge LLM scores quality on a 1-5 scale, assessing completeness and correctness of extractions.\nRelative scoring: We ask a judge LLM to choose the best answer between the extraction modelâ€™s output and the ground-truth answer.\nLFM2-1.2B-Extract can output complex objects in different languages on a level higher than Gemma 3 27B, a model 22.5 times its size.\nðŸƒ How to run\nHugging Face: LFM2-1.2B\nllama.cpp: LFM2-1.2B-Extract-GGUF\nLEAP: LEAP model library\nYou can use the following Colab notebooks for easy inference and fine-tuning:\nNotebook\nDescription\nLink\nInference\nRun the model with Hugging Face's transformers library.\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nSFT (Axolotl)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Axolotl.\nSFT (Unsloth)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Unsloth.\nðŸ“¬ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "manycore-research/FLUX.1-Layout-ControlNet": "FLUX.1-Layout-ControlNet for SpatialGen\nShowcase\nDiffusers\nLICENSE\nFLUX.1-Layout-ControlNet for SpatialGen\nFLUX.1-Layout-ControlNet is a key component of the SpatialGen. FLUX.1 Layout Controlnet [dev] is a ControlNet conditioned by a semantic image. It is capable of generating a 2D image based on a text description while following the semantic image layout. This ControlNet is applicable to FLUX.1 [dev] and is leveraged within the SpatialGen framework for 3D scene synthesis.\nðŸ“– Paper: SPATIALGEN: Layout-guided 3D Indoor Scene Generation\nðŸŒ Project Page\nðŸ’» Code Repository\nShowcase\nThis serene Chinese-inspired bedroom with a traditional bed frame featuring intricate carvings. Use soft, muted colors for the bedding and walls. Hang delicate, calligraphic artworks above the bed. Place a refined wooden nightstand with a circular mirror beside the bed and a built-in wardrobe with sliding doors. Large windows should be adorned with lightweight, embroidered curtains that allow natural light to gently enter the room on the wooden board.\nThis elegant European-style bedroom features a luxurious bed with a tufted headboard, adorned with fine linens. Incorporate classic abstract paintings above the bed and a sophisticated floating nightstand with a decorative mirror. Include a built-in wardrobe with ornate details and large windows draped with sheer, flowing curtains that let in soft, diffused light.\nThis whimsical cartoon bedroom with a playful bed that has a fun, tufted headboard. The bedding should feature bright, cheerful patterns. Hang colorful, abstract cartoons on the walls above the bed. Include a quirky, floating nightstand with a round, animated mirror and a built-in wardrobe with open shelves filled with toys. Large windows with sheer, patterned curtains should let in plenty of light, creating a lively atmosphere.\nThis futuristic cyberpunk bedroom with a sleek, high-tech bed featuring a digital display headboard. The bedding should have a metallic sheen with neon accents. Install holographic abstract art above the bed and a floating nightstand with a reflective, circular mirror. Add a built-in wardrobe with illuminated shelves and large windows with smart, translucent curtains that filter the city lights. The room should exude a cool, tech-savvy vibe.\nDiffusers\nTo use FLUX.1-Layout-Controlnet with the ðŸ§¨ diffusers python library, first install or upgrade diffusers, peft.\npip install -U git+https://github.com/huggingface/diffusers\npip install -U peft\nThen you can use the FluxControlNetPipeline to run it:\nimport torch\nfrom diffusers import FluxControlNetModel\nfrom diffusers.pipelines import FluxControlNetPipeline\nfrom diffusers.utils import load_image\ngenerator = torch.Generator(device=\"cuda\").manual_seed(87544357)\ncontrolnet = FluxControlNetModel.from_pretrained(\n\"manycore-research/FLUX.1-Layout-ControlNet\",\ntorch_dtype=torch.bfloat16,\n)\npipe = FluxControlNetPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-dev/\",\ncontrolnet=controlnet,\ntorch_dtype=torch.bfloat16,\n)\npipe.to(\"cuda\")\ncontrol_image = load_image(\"https://huggingface.co/manycore-research/FLUX.1-Layout-ControlNet/blob/main/assets/input.png\")\nprompt = \"This elegant European-style bedroom features a luxurious bed with a tufted headboard, adorned with fine linens. Incorporate classic abstract paintings above the bed and a sophisticated floating nightstand with a decorative mirror. Include a built-in wardrobe with ornate details and large windows draped with sheer, flowing curtains that let in soft, diffused light.\"\nimage = pipe(\nprompt,\ncontrol_image=control_image,\ncontrolnet_conditioning_scale=0.7,\nnum_inference_steps=25,\nguidance_scale=7,\nheight=1024,\nwidth=1024,\ngenerator=generator,\nnum_images_per_prompt=1,\n).images[0]\nimage.save(\"output.png\")\nLICENSE\nFLUX.1-Layout-ControlNet is licensed under the FLUX.1-dev Non-Commercial License.",
    "apple/FastVLM-7B": "FastVLM: Efficient Vision Encoding for Vision Language Models\nCitation\nFastVLM: Efficient Vision Encoding for Vision Language Models\nFastVLM was introduced in\nFastVLM: Efficient Vision Encoding for Vision Language Models. (CVPR 2025)\nHighlights\nWe introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.\nOur smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.\nOur larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.\nEvaluations\nBenchmark\nFastVLM-0.5B\nFastVLM-1.5B\nFastVLM-7B\nAi2D\n68.0\n77.4\n83.6\nScienceQA\n85.2\n94.4\n96.7\nMMMU\n33.9\n37.8\n45.4\nVQAv2\n76.3\n79.1\n80.8\nChartQA\n76.0\n80.1\n85.0\nTextVQA\n64.5\n70.4\n74.9\nInfoVQA\n46.4\n59.7\n75.8\nDocVQA\n82.5\n88.3\n93.2\nOCRBench\n63.9\n70.2\n73.1\nRealWorldQA\n56.1\n61.2\n67.2\nSeedBench-Img\n71.0\n74.2\n75.4\nUsage Example\nTo run inference of PyTorch checkpoint, follow the instruction in the official repo:\nDownload the model\nhuggingface-cli download apple/FastVLM-7B\nRun inference using predict.py from the official repo.\npython predict.py --model-path /path/to/checkpoint-dir \\\n--image-file /path/to/image.png \\\n--prompt \"Describe the image.\"\nRun inference with Transformers (Remote Code)\nTo run inference with transformers we can leverage trust_remote_code along with the following snippet:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nMID = \"apple/FastVLM-7B\"\nIMAGE_TOKEN_INDEX = -200  # what the model code looks for\n# Load\ntok = AutoTokenizer.from_pretrained(MID, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nMID,\ntorch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\n# Build chat -> render to string (not tokens) so we can place <image> exactly\nmessages = [\n{\"role\": \"user\", \"content\": \"<image>\\nDescribe this image in detail.\"}\n]\nrendered = tok.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=False\n)\npre, post = rendered.split(\"<image>\", 1)\n# Tokenize the text *around* the image token (no extra specials!)\npre_ids  = tok(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\npost_ids = tok(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n# Splice in the IMAGE token id (-200) at the placeholder position\nimg_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\ninput_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)\nattention_mask = torch.ones_like(input_ids, device=model.device)\n# Preprocess image via the model's own processor\nimg = Image.open(\"test-2.jpg\").convert(\"RGB\")\npx = model.get_vision_tower().image_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\npx = px.to(model.device, dtype=model.dtype)\n# Generate\nwith torch.no_grad():\nout = model.generate(\ninputs=input_ids,\nattention_mask=attention_mask,\nimages=px,\nmax_new_tokens=128,\n)\nprint(tok.decode(out[0], skip_special_tokens=True))\nCitation\nIf you found this model useful, please cite the following paper:\n@InProceedings{fastvlm2025,\nauthor = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},\ntitle = {FastVLM: Efficient Vision Encoding for Vision Language Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2025},\n}",
    "tencent/HunyuanWorld-Voyager": "ðŸ”— BibTeX\nAcknowledgements\nWe introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also jointly generate aligned depth and RGB video for effective and direct 3D reconstruction.\nðŸ”— BibTeX\nIf you find Voyager useful for your research and applications, please cite using this BibTeX:\n@article{huang2025voyager,\ntitle={Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation},\nauthor={Huang, Tianyu and Zheng, Wangguandong and Wang, Tengfei and Liu, Yuhao and Wang, Zhenwei and Wu, Junta and Jiang, Jie and Li, Hui and Lau, Rynson WH and Zuo, Wangmeng and Guo, Chunchao},\njournal={arXiv preprint arXiv:2506.04225},\nyear={2025}\n}\nAcknowledgements\nWe would like to thank HunyuanWorld, Hunyuan3D-2, and HunyuanVideo-I2V. We also thank VGGT, MoGE, Metric3D, for their open research and exploration.",
    "Liontix/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor": "Qwen3-4B Claude Sonnet Reasoning Distill\nQwen3-4B Claude Sonnet Reasoning Distill\nThis model was trained on a Claude Sonnet 4 (non-reasoning) dataset and a Claude Sonnet 3.7 (reasoning) dataset.\nðŸ§¬ Datasets:\nLiontix/claude-sonnet-4-100x\nreedmayhew/claude-3.7-sonnet-reasoning\nðŸ— Base Model:\nunsloth/Qwen3-4B-unsloth-bnb-4bit\nIf you want to fine-tune this model:\nStart from: Liontix/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor\nChange dataset as needed in your training script or notebook\nPrompt format uses Claude-style <|im_start|> / <|im_end|> markers with role tags.",
    "mradermacher/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Liontix/Qwen3-4B-Claude-Sonnet-4-Reasoning-Distill-Safetensor\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.8\nGGUF\nQ3_K_S\n2.0\nGGUF\nQ3_K_M\n2.2\nlower quality\nGGUF\nQ3_K_L\n2.3\nGGUF\nIQ4_XS\n2.4\nGGUF\nQ4_K_S\n2.5\nfast, recommended\nGGUF\nQ4_K_M\n2.6\nfast, recommended\nGGUF\nQ5_K_S\n2.9\nGGUF\nQ5_K_M\n3.0\nGGUF\nQ6_K\n3.4\nvery good quality\nGGUF\nQ8_0\n4.4\nfast, best quality\nGGUF\nf16\n8.2\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "raidium/curia": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nLoading the model\nLoading model with heads\nLicense\nCite our paper\nðŸŒŸ Github |\nðŸ“„ Paper Link |\nðŸŒ Blog post\nCuria: A Multi-Modal Foundation Model for Radiology\nWe introduce Curia, a foundation model trained on the entire cross-sectional imaging output\nof a major hospital over several yearsâ€”which to our knowledge is the largest such corpus of\nreal-world dataâ€”encompassing 150,000 exams (130 TB).  On a newly curated 19-task external validation benchmark,\nCuria accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions,\nand predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent\nfoundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes.\nCheck the research paper: https://arxiv.org/abs/2509.06830\nLoading the model\nTo load the model, use the AutoModel class from huggingface transformers library.\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"raidium/curia\")\nYou can also load the image pre-processor\nfrom transformers import AutoImageProcessor\nprocessor = AutoImageProcessor.from_pretrained(\"raidium/curia\", trust_remote_code=True)\nThen to forward an image:\nimg = np.random.rand(-1024, 1024, size=(256, 256)) # single axial slice, in PL orientation\nmodel_input = processor(img)\nfeatures = model(**model_input)\nThe image must follow the following format:\ninput: numpy array of shape (H, W)\nImages needs to be in:\n- PL for axial\n- IL for coronal\n- IP for sagittal\nfor CT, no windowing, just hounsfield or normalized image\nfor MRI, similar, no windowing, just raw values or normalized image\nLoading model with heads\nThe following heads are available:\nanatomy-ct\nanatomy-mri\natlas-stroke\ncovidx-ct\ndeep-lesion-site\nemidec-classification-mask\nich\nixi\nkits\nkneeMRI\nluna16-3D\nneural_foraminal_narrowing\noasis\nspinal_canal_stenosis\nsubarticular_stenosis\nTo load the head, specify its name when loading the model\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nprocessor = AutoImageProcessor.from_pretrained(\"raidium/curia\", trust_remote_code=True)\nmodel = AutoModelForImageClassification.from_pretrained(\n\"raidium/curia\", subfolder=\"anatomy-ct\", trust_remote_code=True\n)\nLicense\nThe model is released under the RESEARCH-ONLY RAIL-M license.\nhttps://huggingface.co/raidium/curia/blob/main/LICENSE\nCite our paper\n@article{dancette2025curia,\ntitle={Curia: A Multi-Modal Foundation Model for Radiology},\nauthor={Dancette, Corentin and Khlaut, Julien and Saporta, Antoine and Philippe, Helene and Ferreres, Elodie and Callard, Baptiste and Danielou, Th{\\'e}o and Alberge, L{\\'e}o and Machado, L{\\'e}o and Tordjman, Daniel and others},\njournal={arXiv preprint arXiv:2509.06830},\nyear={2025}\n}",
    "lightx2v/Wan2.2-I2V-A14B-Moe-Distill-Lightx2v": "Wan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v\nOverview\nInference\nLicense Agreement\nAcknowledgements\nWan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v\nOverview\nWan2.2-I2V-A14B-Moe-Distill-Lightx2v is an advanced image-to-video generation model built upon the Wan2.2-I2V-A14B foundation. This approach allows the model to generate videos with significantly fewer inference steps (4 steps, 2 steps for high noise and 2 steps for low noise) and without classifier-free guidance, substantially reducing video generation time while maintaining high quality outputs.\nThis version has the following features:\nWe found that the training challenge of Wan2.2 lies in high noise; therefore, we have focused on the two-step training for the high-noise model. Compared with the previous version, we have adopted several new strategies, which have improved the consistency and dynamics of the model.\nWe found that the low-noise model can achieve good results by directly using the LoRA from Wan2.1; thus, the low-noise model still adopts the old Wan2.1 LoRA.\nInference\nOur inference framework utilizes lightx2v, a highly efficient inference engine that supports multiple models. This framework significantly accelerates the video generation process while maintaining high quality output.\nscripts/wan22/run_wan22_moe_i2v_distill.sh\nWe recommend using the Euler scheduler (default in lightx2v) with the following settings:\nshift=5.0\nguidance_scale=1.0 (i.e., without CFG)\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the Wan2.1, Self-Forcing repositories, for their open research.",
    "bytedance-research/OneReward": "OneReward\nIntroduction\nQuick Start\nModel\nFLUX.1-Fill-dev[OneReward], trained with Alg.1 in paper\nFLUX.1-Fill-dev[OneRewardDynamic], trained with Alg.2 in paper\nImage Extend with prompt\nImage Extend without prompt\nObject Removal\nObject Removal with Lora\nLicense Agreement\nCitation\nOneReward\nOfficial checkpoint of OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning\nIntroduction\nWe propose OneReward, a novel RLHF methodology for the visual domain by employing Qwen2.5-VL as a generative reward model to enhance multitask reinforcement learning, significantly improving the policy modelâ€™s generation ability across multiple subtask. Building on OneReward, we develop Seedream 3.0 Fill, a unified SOTA image editing model capable of effec-tively handling diverse tasks including image fill, image extend, object removal, and text rendering. It surpasses several leading commercial and open-source systems, including Ideogram, Adobe Photoshop, and FLUX Fill [Pro]. Finally, based on FLUX Fill [dev], we are thrilled to release FLUX.1-Fill-dev-OneReward, which outperforms closed-source FLUX Fill [Pro] in inpainting and outpainting tasks, serving as a powerful new baseline for future research in unified image editing.\nImage Fill\nImage Extend with Prompt\nImage Extend without Prompt\nObject Removal\nSeedream 3.0 Fill Performance Overview\nQuick Start\nMake sure your transformers>=4.51.3 (Supporting Qwen2.5-VL)\nInstall the latest version of diffusers\npip install -U diffusers\nThe following contains a code snippet illustrating how to use the model to generate images based on text prompts and input mask, support inpaint(image-fill), outpaint(image-extend), eraser(object-removal). As the model is fully trained, FluxFillCFGPipeline with cfg is needed, you can find it in our github.\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxTransformer2DModel\nfrom src.pipeline_flux_fill_with_cfg import FluxFillCFGPipeline\ntransformer_onereward = FluxTransformer2DModel.from_pretrained(\n\"bytedance-research/OneReward\",\nsubfolder=\"flux.1-fill-dev-OneReward-transformer\",\ntorch_dtype=torch.bfloat16\n)\npipe = FluxFillCFGPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-Fill-dev\",\ntransformer=transformer_onereward,\ntorch_dtype=torch.bfloat16).to(\"cuda\")\n# Image Fill\nimage = load_image('assets/image.png')\nmask = load_image('assets/mask_fill.png')\nimage = pipe(\nprompt='the words \"ByteDance\", and in the next line \"OneReward\"',\nnegative_prompt=\"nsfw\",\nimage=image,\nmask_image=mask,\nheight=image.height,\nwidth=image.width,\nguidance_scale=1.0,\ntrue_cfg=4.0,\nnum_inference_steps=50,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"image_fill.jpg\")\ninput\noutput\nModel\nFLUX.1-Fill-dev[OneReward], trained with Alg.1 in paper\ntransformer_onereward = FluxTransformer2DModel.from_pretrained(\n\"bytedance-research/OneReward\",\nsubfolder=\"flux.1-fill-dev-OneReward-transformer\",\ntorch_dtype=torch.bfloat16\n)\npipe = FluxFillCFGPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-Fill-dev\",\ntransformer=transformer_onereward,\ntorch_dtype=torch.bfloat16).to(\"cuda\")\nFLUX.1-Fill-dev[OneRewardDynamic], trained with Alg.2 in paper\ntransformer_onereward_dynamic = FluxTransformer2DModel.from_pretrained(\n\"bytedance-research/OneReward\",\nsubfolder=\"flux.1-fill-dev-OneRewardDynamic-transformer\",\ntorch_dtype=torch.bfloat16\n)\npipe = FluxFillCFGPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-Fill-dev\",\ntransformer=transformer_onereward_dynamic,\ntorch_dtype=torch.bfloat16).to(\"cuda\")\nImage Extend with prompt\nimage = load_image('assets/image2.png')\nmask = load_image('assets/mask_extend.png')\nimage = pipe(\nprompt='Deep in the forest, surronded by colorful flowers',\nnegative_prompt=\"nsfw\",\nimage=image,\nmask_image=mask,\nheight=image.height,\nwidth=image.width,\nguidance_scale=1.0,\ntrue_cfg=4.0,\nnum_inference_steps=50,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"image_extend_w_prompt.jpg\")\nImage Extend without prompt\nimage = load_image('assets/image2.png')\nmask = load_image('assets/mask_extend.png')\nimage = pipe(\nprompt='high-definition, perfect composition',  # using fix prompt in image extend wo prompt\nnegative_prompt=\"nsfw\",\nimage=image,\nmask_image=mask,\nheight=image.height,\nwidth=image.width,\nguidance_scale=1.0,\ntrue_cfg=4.0,\nnum_inference_steps=50,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"image_extend_wo_prompt.jpg\")\nObject Removal\nimage = load_image('assets/image.png')\nmask = load_image('assets/mask_remove.png')\nimage = pipe(\nprompt='remove',  # using fix prompt in object removal\nnegative_prompt=\"nsfw\",\nimage=image,\nmask_image=mask,\nheight=image.height,\nwidth=image.width,\nguidance_scale=1.0,\ntrue_cfg=4.0,\nnum_inference_steps=50,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"object_removal.jpg\")\nObject Removal with Lora\nAs the base model flux fill have undergone heavy SFT for object generation, the improvement on removal is not obvious. we release a lora for object removal separately and might be helpful for you.\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxTransformer2DModel\nfrom src.pipeline_flux_fill_with_cfg import FluxFillCFGPipeline\ntransformer_onereward = FluxTransformer2DModel.from_pretrained(\n\"bytedance-research/OneReward\",\nsubfolder=\"flux.1-fill-dev-OneReward-transformer\",\ntorch_dtype=torch.bfloat16\n)\npipe = FluxFillCFGPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-Fill-dev\",\ntransformer=transformer_onereward,\ntorch_dtype=torch.bfloat16).to(\"cuda\")\npipe.load_lora_weights(\n\"bytedance-research/OneReward\",\nsubfolder=\"flux.1-fill-dev-object-removal-lora\",\nweight_name=\"pytorch_lora_weights.safetensors\",\nadapter_name=\"object_removal_lora\"\n)\nprint(\"Loaded adapters:\", pipe.get_list_adapters())\npipe.set_adapters([\"object_removal_lora\"], adapter_weights=[1.0])\n# Object Removal\nimage = load_image('assets/image.png')\nmask = load_image('assets/mask_remove.png')\nimage = pipe(\nprompt='remove',  # using fix prompt in object removal\nnegative_prompt=\"nsfw\",\nimage=image,\nmask_image=mask,\nheight=image.height,\nwidth=image.width,\nguidance_scale=1.0,\ntrue_cfg=4.0,\nnum_inference_steps=50,\ngenerator=torch.Generator(\"cpu\").manual_seed(0),\n).images[0]\nimage.save(f\"object_removal_lora.jpg\")\nLicense Agreement\nCode is licensed under Apache 2.0. Model is licensed under CC BY NC 4.0.\nCitation\n@article{gong2025onereward,\ntitle={OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning},\nauthor={Gong, Yuan and Wang, Xionghui and Wu, Jie and Wang, Shiyin and Wang, Yitong and Wu, Xinglong},\njournal={arXiv preprint arXiv:2508.21066},\nyear={2025}\n}",
    "vibevoice/VibeVoice-1.5B": "VibeVoice: A Frontier Open-Source Text-to-Speech Model\nTraining Details\nModels\nInstallation and Usage\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nContact\nhttps://github.com/vibevoice-community/VibeVoice\nVibeVoice: A Frontier Open-Source Text-to-Speech Model\nVibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\nThe model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.\nâž¡ï¸ Technical Report: VibeVoice Technical Report\nâž¡ï¸ Project Page: microsoft/VibeVoice\nâž¡ï¸ Code: microsoft/VibeVoice-Code\nTraining Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head.\nLLM: Qwen2.5-1.5B for this release.\nTokenizers:\nAcoustic Tokenizer: Based on a Ïƒ-VAE variant (proposed in LatentLM), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each.\nSemantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task.\nDiffusion Head: Lightweight module (4 layers, ~123M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\nContext Length: Trained with a curriculum increasing up to 65,536 tokens.\nTraining Stages:\nTokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately.\nVibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K -> 64K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers.\nModels\nModel\nContext Length\nGeneration Length\nWeight\nVibeVoice-0.5B-Streaming\n-\n-\nOn the way\nVibeVoice-1.5B\n64K\n~90 min\nYou are here.\nVibeVoice-Large\n32K\n~45 min\nHF link\nInstallation and Usage\nPlease refer to GitHub README\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent â€“ cloning a real individualâ€™s voice for satire, advertising, ransom, socialâ€‘engineering, or authentication bypass.\nDisinformation or impersonation â€“ creating audio presented as genuine recordings of real people or events.\nRealâ€‘time or lowâ€‘latency voice conversion â€“ telephone or videoâ€‘conference â€œlive deepâ€‘fakeâ€ applications.\nUnsupported language â€“ the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music â€“ VibeVoice is speechâ€‘only and will not produce coherent nonâ€‘speech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have:\nEmbedded an audible disclaimer (e.g. â€œThis segment was generated by AIâ€) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nLogged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.\nUsers are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nContact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently,â€¯we willâ€¯update this repository with appropriate mitigations.",
    "tencent/HunyuanImage-2.1": "HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹\nðŸ”¥ðŸ”¥ðŸ”¥ Latest Updates\nðŸŽ¥ Demo\nContents\nAbstract\nHunyuanImage-2.1 Overall Pipeline\nTraining Data and Caption\nText-to-Image Model Architecture\nReinforcement Learning from Human Feedback\nRewriting Model\nModel distillation\nðŸŽ‰ HunyuanImage-2.1 Key Features\nPrompt Enhanced Demo\nðŸ“ˆ Comparisons\nSSAE Evaluation\nGSB Evaluation\nðŸ“œ System Requirements\nðŸ› ï¸ Dependencies and Installation\nðŸ§± Download Pretrained Models\nðŸ”‘ Usage\nðŸ”— BibTeX\nAcknowledgements\nGithub Star History\nä¸­æ–‡é˜…è¯»\nHunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹\nðŸ‘‹ Join our WeChat\nThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1. You can find more visualizations on our project page.\nðŸ”¥ðŸ”¥ðŸ”¥ Latest Updates\nSeptember 12, 2025: ðŸš€ Released FP8 quantized models! Making it possible to generate 2K images with only 24GB GPU memory!\nSeptember 8, 2025: ðŸš€ Released inference code and model weights for HunyuanImage-2.1.\nðŸŽ¥ Demo\nContents\nHunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹\nðŸ”¥ðŸ”¥ðŸ”¥ Latest Updates\nðŸŽ¥ Demo\nContents\nAbstract\nHunyuanImage-2.1 Overall Pipeline\nTraining Data and Caption\nText-to-Image Model Architecture\nReinforcement Learning from Human Feedback\nRewriting Model\nModel distillation\nðŸŽ‰ HunyuanImage-2.1 Key Features\nPrompt Enhanced Demo\nðŸ“ˆ Comparisons\nSSAE Evaluation\nGSB Evaluation\nðŸ“œ System Requirements\nðŸ› ï¸ Dependencies and Installation\nðŸ§± Download Pretrained Models\nðŸ”‘ Usage\nðŸ”— BibTeX\nAcknowledgements\nGithub Star History\nAbstract\nWe present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 Ã— 2048) resolution images. Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities. The model employs a highly expressive VAE with a (32 Ã— 32) spatial compression ratio, substantially reducing computational costs.\nOur architecture consists of two stages:\nâ€‹Base text-to-image Model:â€‹â€‹ The first stage is a text-to-image model that utilizes two text encoders: a multimodal large language model (MLLM) to improve image-text alignment, and a multi-language, character-aware encoder to enhance text rendering across various languages. This stage features a single- and dual-stream diffusion transformer with 17 billion parameters. To optimize aesthetics and structural coherence, we apply reinforcement learning from human feedback (RLHF).\nRefiner Model: The second stage introduces a refiner model that further enhances image quality and clarity, while minimizing artifacts.\nAdditionally, we developed the PromptEnhancer module to further boost model performance, and employed meanflow distillation for efficient inference. HunyuanImage-2.1 demonstrates robust semantic alignment and cross-scenario generalization, leading to improved consistency between text and image, enhanced control of scene details, character poses, and expressions, and the ability to generate multiple objects with distinct descriptions.\nHunyuanImage-2.1 Overall Pipeline\nTraining Data and Caption\nStructured captions provide hierarchical semantic information at short, medium, long, and extra-long levels, significantly enhancing the modelâ€™s responsiveness to complex semantics. Innovatively, an OCR agent and IP RAG are introduced to address the shortcomings of general VLM captioners in dense text and world knowledge descriptions, while a bidirectional verification strategy ensures caption accuracy.\nText-to-Image Model Architecture\nCore Components:\nHigh-Compression VAE with REPA Training Acceleration:\nA VAE with a 32Ã— compression rate drastically reduces the number of input tokens for the DiT model. By aligning its feature space with DINOv2 features, we facilitate the training of high-compression VAEs. As a result, our model generates 2K images with the same token length (and thus similar inference time) as other models require for 1K images, achieving superior inference efficiency.\nMulti-bucket, multi-resolution REPA loss aligns DiT features with a high-dimensional semantic feature space, accelerating model convergence.\nDual Text Encoder:\nA vision-language multimodal encoder is employed to better understand scene descriptions, character actions, and detailed requirements.\nA multilingual ByT5 text encoder is introduced to specialize in text generation and multilingual expression.\nNetwork: A single- and dual-stream diffusion transformer with 17 billion parameters.\nReinforcement Learning from Human Feedback\nTwo-Stage Post-Training with Reinforcement Learning: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are applied sequentially in two post-training stages. We introduce a Reward Distribution Alignment algorithm, which innovatively incorporates high-quality images as selected samples to ensure stable and improved reinforcement learning outcomes.\nRewriting Model\nThe first systematic industrial-level rewriting model. SFT training structurally rewrites user text instructions to enrich visual expression, while GRPO training employs a fine-grained semantic AlignEvaluator reward model to substantially improve the semantics of images generated from rewritten text. The AlignEvaluator covers 6 major categories and 24 fine-grained assessment points. PromptEnhancer supports both Chinese and English rewriting and demonstrates general applicability in enhancing semantics for both open-source and proprietary text-to-image models.\nModel distillation\nWe propose a novel distillation method based on meanflow that addresses the key challenges of instability and inefficiency inherent in standard meanflow training. This approach enables high-quality image generation with only a few sampling steps. To our knowledge, this is the first successful application of meanflow to an industrial-scale model.\nðŸŽ‰ HunyuanImage-2.1 Key Features\nHigh-Quality Generation: Efficiently produces ultra-high-definition (2K) images with cinematic composition.\nMultilingual Support: Provides native support for both Chinese and English prompts.\nAdvanced Architecture: Built on a multi-modal, single- and dual-stream combined DiT (Diffusion Transformer) backbone.\nGlyph-Aware Processing: Utilizes ByT5's text rendering capabilities for improved text generation accuracy.\nFlexible Aspect Ratios: Supports a variety of image aspect ratios (1:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3).\nPrompt Enhancement: Automatically rewrites prompts to improve descriptive accuracy and visual quality.\nPrompt Enhanced Demo\nTo improve the quality and detail of generated images, we use a prompt rewriting model. This model automatically enhances user-provided text prompts by adding detailed and descriptive information.\nðŸ“ˆ Comparisons\nSSAE Evaluation\nSSAE (Structured Semantic Alignment Evaluation) is an intelligent evaluation metric for image-text alignment based on advanced multimodal large language models (MLLMs). We extracted 3500 key points across 12 categories, then used multimodal large language models to automatically evaluate and score by comparing the generated images with these key points based on the visual content of the images. Mean Image Accuracy represents the image-wise average score across all key points, while Global Accuracy directly calculates the average score across all key points.\nModel Open Source Mean Image Accuracy Global Accuracy Primary Subject Secondary Subject Scene Other\nNoun Key Attributes Other Attributes Action Noun Attributes Action Noun Attributes Shot Style Composition\nFLUX-dev âœ… 0.7122 0.6995 0.7965 0.7824 0.5993 0.5777 0.7950 0.6826 0.6923 0.8453 0.8094 0.6452 0.7096 0.6190\nSeedream-3.0 âŒ 0.8827 0.8792 0.9490 0.9311 0.8242 0.8177 0.9747 0.9103 0.8400 0.9489 0.8848 0.7582 0.8726 0.7619\nQwen-Image âœ… 0.8854 0.8828 0.9502 0.9231 0.8351 0.8161 0.9938 0.9043 0.8846 0.9613 0.8978 0.7634 0.8548 0.8095\nGPT-Image âŒ  0.8952 0.8929 0.9448 0.9289 0.8655 0.8445 0.9494 0.9283 0.8800 0.9432 0.9017 0.7253 0.8582 0.7143\nHunyuanImage 2.1 âœ… 0.8888 0.8832 0.9339 0.9341 0.8363 0.8342 0.9627 0.8870 0.9615 0.9448 0.9254 0.7527 0.8689 0.7619\nFrom the SSAE evaluation results, our model has currently achieved the optimal performance among open-source models in terms of semantic alignment, and is very close to the performance of closed-source commercial models (GPT-Image).\nGSB Evaluation\nWe adopted the GSB evaluation method commonly used to assess the relative performance between two models from an overall image perception perspective. In total, we utilized 1000 text prompts, generating an equal number of image samples for all compared models in a single run. For a fair comparison, we conducted inference only once for each prompt, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models. The evaluation was performed by more than 100 professional evaluators.\nFrom the results, HunyuanImage 2.1 achieved a relative win rate of -1.36% against Seedream3.0 (closed-source) and 2.89% outperforming Qwen-Image (open-source). The GSB evaluation results demonstrate that HunyuanImage 2.1, as an open-source model, has reached a level of image generation quality comparable to closed-source commercial models (Seedream3.0), while showing certain advantages in comparison with similar open-source models (Qwen-Image). This fully validates the technical advancement and practical value of HunyuanImage 2.1 in text-to-image generation tasks.\nðŸ“œ System Requirements\nHardware and OS Requirements:\nNVIDIA GPU with CUDA support.\nMinimum requrement for now: 24 GB GPU memory for 2048x2048 image generation.\nNote: The memory requirements above are measured with model CPU offloading and FP8 quantization enabled. If your GPU has sufficient memory, you may disable offloading for improved inference speed.\nSupported operating system: Linux.\nðŸ› ï¸ Dependencies and Installation\nClone the repository:\ngit clone https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git\ncd HunyuanImage-2.1\nInstall dependencies:\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\nðŸ§± Download Pretrained Models\nThe details of download pretrained models are shown here.\nðŸ”‘ Usage\nHunyuanImage-2.1 only supports 2K image generation (e.g. 2048x2048 for 1:1 images, 2560x1536 for 16:9 images, etc.).\nGenerating images with 1K resolution will result in artifacts.\nAdditionally, we recommend using the full generation pipeline for better quality (i.e. enabling prompt enhancement and refinment).\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\nimport torch\nfrom hyimage.diffusion.pipelines.hunyuanimage_pipeline import HunyuanImagePipeline\n# Supported model_name: hunyuanimage-v2.1, hunyuanimage-v2.1-distilled\nmodel_name = \"hunyuanimage-v2.1\"\npipe = HunyuanImagePipeline.from_pretrained(model_name=model_name, use_fp8=True)\npipe = pipe.to(\"cuda\")\nprompt = \"A cute, cartoon-style anthropomorphic penguin plush toy with fluffy fur, standing in a painting studio, wearing a red knitted scarf and a red beret with the word â€œTencentâ€ on it, holding a paintbrush with a focused expression as it paints an oil painting of the Mona Lisa, rendered in a photorealistic photographic style.\"\nimage = pipe(\nprompt=prompt,\n# Examples of supported resolutions and aspect ratios for HunyuanImage-2.1:\n# 16:9  -> width=2560, height=1536\n# 4:3   -> width=2304, height=1792\n# 1:1   -> width=2048, height=2048\n# 3:4   -> width=1792, height=2304\n# 9:16  -> width=1536, height=2560\n# Please use one of the above width/height pairs for best results.\nwidth=2048,\nheight=2048,\nuse_reprompt=False,  # Enable prompt enhancement (which may result in higher GPU memory usage)\nuse_refiner=True,   # Enable refiner model\n# For the distilled model, use 8 steps for faster inference.\n# For the non-distilled model, use 50 steps for better quality.\nnum_inference_steps=8 if \"distilled\" in model_name else 50,\nguidance_scale=3.25 if \"distilled\" in model_name else 3.5,\nshift=4 if \"distilled\" in model_name else 5,\nseed=649151,\n)\nimage.save(f\"generated_image.png\")\nðŸ”— BibTeX\nIf you find this project useful for your research and applications, please cite as:\n@misc{HunyuanImage-2.1,\ntitle={HunyuanImage 2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation},\nauthor={Tencent Hunyuan Team},\nyear={2025},\nhowpublished={\\url{https://github.com/Tencent-Hunyuan/HunyuanImage-2.1}},\n}\nAcknowledgements\nWe would like to thank the following open-source projects and communities for their contributions to open research and exploration: Qwen, FLUX, diffusers and HuggingFace.\nGithub Star History",
    "inclusionAI/Ling-mini-2.0": "Introduction\nStrong General and Professional Reasoning\n7Ã— Equivalent Dense Performance Leverage\nHigh-speed Generation at 300+ token/s\nOpen-sourced FP8 Efficient Training Solution\nA More Open Opensource Strategy\nModel Downloads\nQuickstart\nðŸš€ Try Online\nðŸ”Œ API Usage\nConvert to safetensors\nðŸ¤— Hugging Face Transformers\nðŸ¤– ModelScope\nDeployment\nvLLM\nEnvironment Preparation\nOffline Inference:\nOnline Inference:\nSGLang\nEnvironment Preparation\nRun Inference\nTraining\nPre-training\nPerformance Benchmark\nFinetuning\nLicense\nCitation\nðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScopeÂ Â  | Â Â ðŸ™ Experience Now\nIntroduction\nToday, we are excited to announce the open-sourcing of Ling 2.0 â€” a family of MoE-based large language models that combine SOTA performance with high efficiency.\nThe first released version, Ling-mini-2.0, is compact yet powerful. It has 16B total parameters, but only 1.4B are activated per input token (non-embedding 789M). Trained on more than 20T tokens of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning, Ling-mini-2.0 achieves remarkable improvements in complex reasoning and instruction following. With just 1.4B activated parameters, it still reaches the top-tier level of sub-10B dense LLMs and even matches or surpasses much larger MoE models.\nStrong General and Professional Reasoning\nWe evaluated Ling-mini-2.0 on challenging general reasoning tasks in coding (LiveCodeBench, CodeForces) and mathematics (AIME 2025, HMMT 2025), as well as knowledge-intensive reasoning tasks across multiple domains (MMLU-Pro, Humanity's Last Exam). Compared with sub-10B dense models (e.g., Qwen3-4B-instruct-2507, Qwen3-8B-nothinking) and larger-scale MoE models (Ernie-4.5-21B-A3B-PT, GPT-OSS-20B/low), Ling-mini-2.0 demonstrated outstanding overall reasoning capabilities.\n7Ã— Equivalent Dense Performance Leverage\nGuided by Ling Scaling Laws, Ling 2.0 adopts a 1/32 activation ratio MoE architecture, with empirically optimized design choices in expert granularity, shared expert ratio, attention ratio, aux-loss free + sigmoid routing strategy, MTP loss, QK-Norm, half RoPE, and more. This enables small-activation MoE models to achieve over 7Ã— equivalent dense performance. In other words, Ling-mini-2.0 with only 1.4B activated parameters (non-embedding 789M) can deliver performance equivalent to a 7â€“8B dense model.\nHigh-speed Generation at 300+ token/s\nThe highly sparse small-activation MoE architecture also delivers significant training and inference efficiency. In simple QA scenarios (within 2000 tokens), Ling-mini-2.0 generates at 300+ token/s (on H20 deployment) â€” more than 2Ã— faster than an 8B dense model. Ling-mini-2.0 is able to handle 128K context length with YaRN, as sequence length increases, the relative speedup can reach over 7Ã—.\nOpen-sourced FP8 Efficient Training Solution\nLing 2.0 employs FP8 mixed-precision training throughout. Compared with BF16, experiments with over 1T training tokens show nearly identical loss curves and downstream benchmark performance. To support the community in efficient continued pretraining and fine-tuning under limited compute, we are also open-sourcing our FP8 training solution. Based on tile/blockwise FP8 scaling, it further introduces FP8 optimizer, FP8 on-demand transpose weight, and FP8 padding routing map for extreme memory optimization. On 8/16/32 80G GPUs, compared with LLaMA 3.1 8B and Qwen3 8B, Ling-mini-2.0 achieved 30â€“60% throughput gains with MTP enabled, and 90â€“120% throughput gains with MTP disabled.\nA More Open Opensource Strategy\nWe believe Ling-mini-2.0 is an ideal starting point for MoE research. For the first time at this scale, it integrates 1/32 sparsity, MTP layers, and FP8 training â€” achieving both strong effectiveness and efficient training/inference performance, making it a prime candidate for the small-size LLM segment.\nTo further foster community research, in addition to releasing the post-trained version, we are also open-sourcing five pretraining checkpoints: the pre-finetuning Ling-mini-2.0-base, along with four base models trained on 5T, 10T, 15T, and 20T tokens, enabling deeper research and broader applications.\nModel Downloads\nYou can download the following table to see the various stage of Ling-mini-2.0 models(1.43B activated of 16.26B total params). If you are located in mainland China, we also provide the model on ModelScope.cn to speed up the download process.\nModel\nContext Length\nDownload\nLing-mini-base-2.0\n32K -> 128K (YaRN)\nðŸ¤— HuggingFace ðŸ¤– ModelScope\nLing-mini-base-2.0-5T\n4K\nðŸ¤— HuggingFace ðŸ¤– ModelScope\nLing-mini-base-2.0-10T\n4K\nðŸ¤— HuggingFace ðŸ¤– ModelScope\nLing-mini-base-2.0-15T\n4K\nðŸ¤— HuggingFace ðŸ¤– ModelScope\nLing-mini-base-2.0-20T\n4K\nðŸ¤— HuggingFace ðŸ¤– ModelScope\nLing-mini-2.0\n32K -> 128K (YaRN)\nðŸ¤— HuggingFace ðŸ¤– ModelScope\nNote: If you are interested in previous version, please visit the past model collections in Huggingface or ModelScope.\nQuickstart\nðŸš€ Try Online\nYou can experience Ling-mini-2.0 online at: ZenMux\nðŸ”Œ API Usage\nYou can also use Ling-mini-2.0 through API calls:\nfrom openai import OpenAI\n# 1. Initialize the OpenAI client\nclient = OpenAI(\n# 2. Point the base URL to the ZenMux endpoint\nbase_url=\"https://zenmux.ai/api/v1\",\n# 3. Replace with the API Key from your ZenMux user console\napi_key=\"<your ZENMUX_API_KEY>\",\n)\n# 4. Make a request\ncompletion = client.chat.completions.create(\n# 5. Specify the model to use in the format \"provider/model-name\"\nmodel=\"inclusionai/ling-mini-2.0\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What is the meaning of life?\"\n}\n]\n)\nprint(completion.choices[0].message.content)\nConvert to safetensors\nModels with safetensors format can be downloaded from HuggingFace or ModelScope.\nIf you want to train your model and eval it, you can convert from dcp produced by training.\npython tools/convert_dcp_to_safe_tensors.py --checkpoint-path ${DCP_PATH} --target-path ${SAFETENSORS_PATH}\nCurrently, BF16 and FP8 formats are supported, you can use convert parameter to handle it:\n--force-bf16 for BF16 format.\n--force-fp8 for FP8 format.\nðŸ¤— Hugging Face Transformers\nHere is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ling-mini-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", return_token_type_ids=False).to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nðŸ¤– ModelScope\nIf you're in mainland China, we strongly recommend you to use our model from ðŸ¤– ModelScope.\nDeployment\nvLLM\nvLLM supports offline batched inference or launching an OpenAI-Compatible API Service for online inference.\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below:\ngit clone -b v0.10.0 https://github.com/vllm-project/vllm.git\ncd vllm\nwget https://raw.githubusercontent.com/inclusionAI/Ling-V2/refs/heads/main/inference/vllm/bailing_moe_v2.patch\ngit apply bailing_moe_v2.patch\npip install -e .\nOffline Inference:\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ling-mini-2.0\")\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=16384)\nllm = LLM(model=\"inclusionAI/Ling-mini-2.0\", dtype='bfloat16')\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nOnline Inference:\nvllm serve inclusionAI/Ling-mini-2.0 \\\n--tensor-parallel-size 2 \\\n--pipeline-parallel-size 1 \\\n--use-v2-block-manager \\\n--gpu-memory-utilization 0.90\nTo handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the model's config.json file, for example:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nUse an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service.\nFor detailed guidance, please refer to the vLLM instructions.\nSGLang\nEnvironment Preparation\nWe will later submit our model to SGLang official release, now we can prepare the environment following steps:\npip3 install sglang==0.5.2rc0 sgl-kernel==0.3.7.post1\nYou can use docker image as well:\ndocker pull lmsysorg/sglang:v0.5.2rc0-cu126\nThen you should apply patch to sglang installation:\n# patch command is needed, run `yum install -y patch` if needed\npatch -d `python -c 'import sglang;import os; print(os.path.dirname(sglang.__file__))'` -p3 < inference/sglang/bailing_moe_v2.patch\nRun Inference\nBF16 and FP8 models are supported by SGLang now, it depends on the dtype of the model in ${MODEL_PATH}. They both share the same command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path $MODLE_PATH \\\n--host 0.0.0.0 --port $PORT \\\n--trust-remote-code \\\n--attention-backend fa3\nMTP is supported for base model, and not yet for chat model. You can add parameter --speculative-algorithm NEXTN\nto start command.\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\n\"\"\"\nMore usage can be found here\nTraining\nWe also provide a complete and efficient training framework that covers both pre-training and finetune. Based on this framework, continue training can be performed on the Ling-mini-2.0 checkpoint. With our training framework, the training throughput of the Ling-mini-2.0 model is significantly better than that of the existing Dense 8B model (Qwen3-8B, Llama3-8B).\nPre-training\nPretraining demo to Continue pretraining Ling models.\nPerformance Benchmark\nThe table below shows the pre-training performance of several models, measured in tokens per second on 8, 16, and 32 80G GPUs. Ling-mini-2.0 achieves significantly higher training efficiency compared to the baseline, making it easier and more cost-effective to continue pre-training with our demo scripts.\nModel\n8 x 80G GPUs (GBS=128)\n16 x 80G GPUs (GBS=256)\n32 x 80G GPUs (GBS=512)\nLLaMA 3.1 8B (baseline)\n81222\n161319\n321403\nQwen3 8B\n55775 (-31.33%)\n109799 (-31.94%)\n219943 (-31.57%)\nLing-mini-2.0\n109532 (+34.86%)\n221585 (+37.36%)\n448726 (+39.61%)\nLing-mini-2.0 w/o MTP\n128298 (+57.96%)\n307264 (+90.47%)\n611466 (+90.25%)\nFinetuning\nWe recommend you to use Llama-Factory to finetune Ling. In addition to that, you can also use Megatron for finetuning.\nLicense\nThis code repository is licensed under the MIT License.\nCitation\nIf you find our work helpful, feel free to give us a cite.",
    "inclusionAI/Ring-mini-2.0": "Ring-mini-2.0\nEnhanced Reasoning: Joint Training with SFT + RLVR + RLHF\nHigh Sparsity, High-Speed Generation\nModel Downloads\nQuickstart\nðŸ¤— Hugging Face Transformers\nLicense\nProject Page\nCode\nCitation\nRing-mini-2.0\nðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScope\n| Â Â ðŸ™ Experience Now\nThis model is presented in the paper Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model.\nToday, we officially release Ring-mini-2.0 â€” a high-performance inference-oriented MoE model deeply optimized based on the Ling 2.0 architecture. With only 16B total parameters and 1.4B activated parameters, it achieves comprehensive reasoning capabilities comparable to dense models below the 10B scale. It excels particularly in logical reasoning, code generation, and mathematical tasks, while supporting 128K long-context processing and 300+ tokens/s high-speed generation.\nEnhanced Reasoning: Joint Training with SFT + RLVR + RLHF\nBuilt upon Ling-mini-2.0-base, Ring-mini-2.0 undergoes further training with Long-CoT SFT, more stable and continuous RLVR, and RLHF joint optimization, significantly improving the stability and generalization of complex reasoning. On multiple challenging benchmarks (LiveCodeBench, AIME 2025, GPQA, ARC-AGI-v1, etc.), it outperforms dense models below 10B and even rivals larger MoE models (e.g., gpt-oss-20B-medium) with comparable output lengths, particularly excelling in logical reasoning.\nHigh Sparsity, High-Speed Generation\nInheriting the efficient MoE design of the Ling 2.0 series, Ring-mini-2.0 activates only 1.4B parameters and achieves performance equivalent to 7â€“8B dense models through architectural optimizations such as 1/32 expert activation ratio and MTP layers. Thanks to its low activation and high sparsity design, Ring-mini-2.0 delivers a throughput of 300+ tokens/s when deployed on H20. With Expert Dual Streaming inference optimization, this can be further boosted to 500+ tokens/s, significantly reducing inference costs for high-concurrency scenarios involving thinking models. Additionally, with YaRN extrapolation, it supports 128K long-context processing, achieving a relative speedup of up to 7x in long-output scenarios.\nModel Downloads\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nRing-mini-2.0\n16.8B\n1.4B\n128K\nðŸ¤— HuggingFace ðŸ¤– Modelscope\nQuickstart\nðŸ¤— Hugging Face Transformers\nHere is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-mini-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Ring, an assistant created by inclusionAI\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", return_token_type_ids=False).to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nLicense\nThis code repository is licensed under the MIT License.\nProject Page\nAccess the demo and experience the model at: https://zenmux.ai/inclusionai/ring-mini-2.0\nCode\nThe full code repository for this model can be found on GitHub: https://github.com/inclusionAI/Ring-V2\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{lingteam2025every,\ntitle={Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model},\nauthor={Ling Team and Anqi Shen and Baihui Li and Bin Hu and Bin Jing and Cai Chen and Chao Huang and Chao Zhang and Chaokun Yang and Cheng Lin and Chengyao Wen and Congqi Li and Deng Zhao and Dingbo Yuan and Donghai You and Fagui Mao and Fanzhuang Meng and Feng Xu and Guojie Li and Guowei Wang and Hao Dai and Haonan Zheng and Hong Liu and Jia Guo and Jiaming Liu and Jian Liu and Jianhao Fu and Jiannan Shi and Jianwen Wang and Jianxin Lai and Jin Yang and Jun Mei and Jun Zhou and Junbo Zhao and Junping Zhao and Kuan Xu and Le Su and Lei Chen and Li Tang and Liang Jiang and Liangcheng Fu and Lianhao Xu and Linfeng Shi and Lisha Liao and Longfei Zheng and Meng Li and Mingchun Chen and Qi Zuo and Qiang Cheng and Qianggang Cao and Qitao Shi and Quanrui Guo and Senlin Zhu and Shaofei Wang and Shaomian Zheng and Shuaicheng Li and Shuwei Gu and Siba Chen and Tao Wu and Tao Zhang and Tianyu Zhang and Tianyu Zhou and Tiwei Bie and Tongkai Yang and Wang Hong and Wang Ren and Weihua Chen and Wenbo Yu and Wengang Zheng and Xiangchun Wang and Xiaodong Yan and Xiaopei Wan and Xin Zhao and Xinyu Kong and Xinyu Tang and Xudong Han and Xudong Wang and Xuemin Yang and Xueyu Hu and Yalin Zhang and Yan Sun and Yicheng Shan and Yilong Wang and Yingying Xu and Yongkang Liu and Yongzhen Guo and Yuanyuan Wang and Yuchen Yan and Yuefan Wang and Yuhong Guo and Zehuan Li and Zhankai Xu and Zhe Li and Zhenduo Zhang and Zhengke Gui and Zhenxuan Pan and Zhenyu Huang and Zhenzhong Lan and Zhiqiang Ding and Zhiqiang Zhang and Zhixun Li and Zhizhen Liu and Zihao Wang and Zujie Wen},\nyear={2025},\neprint={2510.18855},\narchivePrefix={arXiv},\nprimaryClass={cs.AI}\n}",
    "facebook/MobileLLM-R1-950M": "Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nFAIR Noncommercial Research License v1 Last Updated: August 18, 2025â€œAcceptable Use Policyâ€ means the FAIR Acceptable Use Policy, applicable to Research Materials, that is incorporated into this Agreement.â€œAgreementâ€ means the terms and conditions for use, reproduction, distribution and modification of the Research Materials set forth herein.\nâ€œDocumentationâ€ means the specifications, manuals and documentation accompanying  Research Materials distributed by Meta.\nâ€œLicenseeâ€ or â€œyouâ€ means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entityâ€™s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\nâ€œMetaâ€ or â€œweâ€ means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).â€œNoncommercial Research Usesâ€ means noncommercial research use cases related to research, development, education, processing, or analysis and in each case, is not primarily intended for commercial advantage or monetary compensation to you or others.â€œResearch Materialsâ€ means, collectively, Documentation and the models, software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code, demonstration materials and other elements of the foregoing distributed by Meta and made available under this Agreement.By clicking â€œI Acceptâ€ below or by using or distributing any portion or element of the Research Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metaâ€™s intellectual property or other rights owned by Meta embodied in the Research Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Research Materials.b. Redistribution and Use.   i. You will not use the Research Materials or any outputs or results of the Research Materials in connection with any commercial uses or for any uses other than Noncommercial Research Uses;\nii. Distribution of Research Materials, and any derivative works thereof, are subject to the terms of this Agreement. If you distribute or make the Research Materials, or any derivative works thereof, available to a third party, you may only do so under the terms of this Agreement. You shall also provide a copy of this Agreement to such third party.\niii.  If you submit for publication the results of research you perform on, using, or otherwise in connection with Research Materials, you must acknowledge the use of Research Materials in your publication.\niv. Your use of the Research Materials must comply with applicable laws and regulations (including Trade Control Laws) and adhere to the FAIR Acceptable Use Policy, which is hereby incorporated by reference into this Agreement. 2. User Support. Your Noncommercial Research Use of the Research Materials is done at your own discretion; Meta does not process any information nor provide any service in relation to such use.  Meta is under no obligation to provide any support services for the Research Materials. Any support provided is â€œas isâ€, â€œwith all faultsâ€, and without warranty of any kind.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE RESEARCH MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN â€œAS ISâ€ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE RESEARCH MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE RESEARCH MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY DIRECT OR INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.\na. Subject to Metaâ€™s ownership of Research Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Research Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.b. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Research Materials, outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Research Materials.6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Research Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Research Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nModifications and Amendments. Meta may modify this Agreement from time to time; provided that they are similar in spirit to the current version of the Agreement, but may differ in detail to address new problems or concerns. All such changes will be effective immediately. Your continued use of the Research Materials after any modification to this Agreement constitutes your agreement to such modification. Except as provided in this Agreement, no modification or addition to any provision of this Agreement will be binding unless it is in writing and signed by an authorized representative of both you and Meta.\nFAIR Acceptable Use PolicyThe Fundamental AI Research (FAIR) team at Meta seeks to further understanding of new and existing research domains with the mission of advancing the state-of-the-art in artificial intelligence through open research for the benefit of all.As part of this mission, Meta makes certain research materials available for noncommercial research use. Meta is committed to promoting the safe and responsible use of such research materials.Prohibited UsesYou agree you will not use, or allow others to use, Research Materials to:Violate the law or othersâ€™ rights, including to: Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: Violence or terrorism Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material Human trafficking, exploitation, and sexual violence The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials. Sexual solicitation Any other criminal activityEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individualsEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and servicesEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practicesCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable lawsEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any technology using FAIR research materialsCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of research artifacts related to the following:Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of StateGuns and illegal weapons (including weapon development)Illegal drugs and regulated/controlled substancesOperation of critical infrastructure, transportation technologies, or heavy machinerySelf-harm or harm to others, including suicide, cutting, and eating disordersAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual3. Intentionally deceive or mislead others, including use of FAIR Research Materials related to the following:Generating, promoting, or furthering fraud or the creation or promotion of disinformationGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other contentGenerating, promoting, or further distributing spamImpersonating another individual without consent, authorization, or legal rightRepresenting that outputs of FAIR research materials or outputs from technology using FAIR research materials are human-generatedGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement4. Fail to appropriately disclose to end users any known dangers of your Research Materials.Please report any violation of this Policy or other problems that could lead to a violation of this Policy by submitting a report here [https://docs.google.com/forms/d/e/1FAIpQLSeb11cryAopJ7LNrC4nxEUXrHY26hfkXQMf_uH-oFgA3WlYZQ/viewform].\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Details\nNews\nPretrained Model\nToken efficiency comparison across pretrained models\nPost-trained Model\nHighlights\nPretrained Model\nToken efficiency comparison across pretrained models\nPost-trained Model\nHow to use\nInference examples\nEvaluation\nMobileLLM-R1 base model\nMobileLLM-R1 post-trained model\nTraining\nTraining Process\nTraining stages and hyperparameter details\nData Mix\nPre-training\nMid-training\nPost-training\nCitation\nContact\nLicense\nðŸ¤— Hugging FaceÂ Â  | Â Â  ðŸ“‘ Paper Â Â  | Â Â  ðŸ’» Code\nModel Details\nWe present MobileLLM-R1, a new series of efficient reasoning models in the MobileLLM family. The release includes two categories of models:\nBase models:\nMobileLLM-R1-140M-base\nMobileLLM-R1-360M-base\nMobileLLM-R1-950M-base\nFinal models:\nMobileLLM-R1-140M\nMobileLLM-R1-360M\nMobileLLM-R1-950M\nNote: These models are not general-purpose chat models. They are Supervised Fine-Tuned (SFT) models, specifically trained to address mathematical, programming (Python, C++), and scientific problems.\nIn addition to the models, we release the complete training recipes and data sources to ensure reproducibility and support further research.\nRemarkably, the MobileLLM-R1 950M, pre-trained on only ~2T high-quality tokens and with fewer than 5T total training tokens, achieves comparable or superior performance to Qwen3 0.6B, which was trained on 36T tokens, across MATH, GSM8K, MMLU, and LiveCodeBench benchmarks.\nCompared to existing fully open-source models, MobileLLM-R1 950M model achieves ~5Ã— higher accuracy on MATH compared to the Olmo 1.24B model and ~2Ã— higher accuracy relative to the SmolLM2 1.7B model, despite being substantially smaller in parameter scale. In addition, MobileLLM-R1 950M outperforms both Olmo 1.24B and SmolLM2 1.7B by a wide margin on coding benchmarks, establishing a new state-of-the-art among fully open-source models.\nNews\nSept 12, 2025: ðŸš€ MobileLLM-R1 models are released on HuggingFace.\nSpet 28, 2025: ðŸŒŸ The training code is also available on GitHub.\nSpet 29, 2025: ðŸŽ‰ The technical report \"MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes\" is also available! Please check it out.\nHighlights\nPretrained Model\nToken efficiency comparison across pretrained models\nPost-trained Model\nModel Architecture:\n# Layers\n# Attnetion Heads\n# KV Heads\nDim\nHidden Dim\nParams\nMobileLLM-R1-140M\n15\n9\n3\n576\n2048\n140M\nMobileLLM-R1-360M\n15\n16\n4\n1024\n4096\n359M\nMobileLLM-R1-950M\n22\n24\n6\n1536\n6144\n949M\nInput modalities\nOutput modalities\nContext Length\nVocaburary Size\nShared Embeddings\nMobileLLM-R1-140M-base\nText\nText\n4k\n128k\nYes\nMobileLLM-R1-360M-base\nText\nText\n4k\n128k\nYes\nMobileLLM-R1-950M-base\nText\nText\n4k\n128k\nYes\nMobileLLM-R1-140M\nText\nText\n32k\n128k\nYes\nMobileLLM-R1-360M\nText\nText\n32k\n128k\nYes\nMobileLLM-R1-950M\nText\nText\n32k\n128k\nYes\nHow to use\nTo load the pretrained model for further finetuning or evaluation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/MobileLLM-R1-950M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/MobileLLM-R1-950M\")\nInference examples\nTransformers\nfrom transformers import pipeline\nimport torch\nmodel_id = \"facebook/MobileLLM-R1-950M\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\n# Math problem / default scenario\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n},\n{\"role\": \"user\", \"content\": \"Compute: $1-2+3-4+5- \\\\dots +99-100$.\"},\n]\n# C++ coding scenario\nmessages = [\n{\n\"role\": \"system\",\n\"content\": (\n\"\\nYou are a helpful and harmless assistant. You should think step-by-step before responding to the instruction below.\\n\\n\"\n\"Please use c++ programming language only.\\n\"\n\"You must use cpp\\n# Your code here\\npython for just the final solution code block with the following format:\\n\"\n\"\\n\"\n)\n},\n{\"role\": \"user\", \"content\": \"Write a Python function that returns the square of a number.\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=8192,\n)\nprint(outputs[0][\"generated_text\"][-1])\nYou can also run inference with vLLM. You only need to register the model architecture Llama4ForCausalLM with the vLLM ModelRegistry.\nfrom vllm.model_executor.models.llama4 import Llama4ForCausalLM\nfrom vllm.model_executor.models.registry import ModelRegistry\nModelRegistry.register_model(\"Llama4ForCausalLM\", Llama4ForCausalLM)\nEvaluation\nMobileLLM-R1 base model\nModel\nSize\nMATH500\nGSM8K\nMBPP\nHumanEval\nCommonSense Avg.\nMMLU\n4-shot  em\n8-shot  em\n3-shot  pass@1\n0-shot  pass@1\n0-shot  accuracy\n5-shot  accuracy\n<150M\nSmolLM2-135M-base\n135M\n0.4\n1.8\n3.8\n0.0\n50.7\n--\nMobileLLM-R1-140M-base\n140M\n4.6\n16.3\n5.4\n15.9\n44.3\n--\n150M - 400M\nGemma-3-270M-pt\n268M\n0.6\n1.1\n2.0\n3.1\n48.4\n26.5\nSmolLM2-360M-base\n362M\n1.8\n5.0\n19.4\n0.0\n56.6\n24.7\nMobileLLM-R1-360M-base\n359M\n13.4\n39.4\n20.8\n32.9\n51.0\n26.8\n400M - 1B\nQwen2.5-0.5B-base\n494M\n14.8\n41.8\n29.6\n28.1\n52.3\n47.5\nQwen3-0.6B-base\n596M\n29.8\n60.9\n39.0\n30.5\n55.3\n52.4\nMobileLLM-R1-950M-base\n949M\n26.8\n61.6\n39.2\n46.3\n58.6\n47.4\n> 1B\nGemma-3-1B-pt\n1.0B\n0.6\n2.4\n9.4\n6.1\n57.3\n26.1\nLLaMA3.2-1B-base\n1.24B\n1.6\n6.8\n26.6\n17.1\n58.4\n32.0\nOLMo-2-0425-1B-base\n1.48B\n5.2\n39.8\n7.8\n6.7\n61.0\n42.4\nQwen2.5-1.5B-base\n1.54B\n31.0\n68.4\n44.6\n36.6\n58.7\n61.2\nSmolLM2-1.7B-base\n1.71B\n11.6\n31.8\n35.4\n0.6\n62.9\n50.0\nQwen3-1.7B-base\n2.03B\n38.5\n76.2\n56.4\n47.6\n60.9\n62.1\nHere, CommonSense Avg. denotes an average of 8 tasks in CommonSense Reasoning benchmarks including ARC-easy, ARC-challenge, BoolQ, PIQA, SIQA, HellaSwag, OBQA, and WinoGrand. Models with fewer than 150M parameters do not yield reliable MMLU scores and are therefore denoted as 'â€”'.\nMobileLLM-R1 post-trained model\nModel\nSize\nMATH500\nGSM8K\nAIME'24\nAIME'25\nLiveCodeBench-v6\n0-shot  pass@1\n0-shot  pass@1\n0-shot  pass@1, n=64\n0-shot  pass@1, n=64\n0-shot  pass@1, n=16\n<150M\nSmolLM2-135M-Instruct\n135M\n3.0\n2.4\n--\n--\n0.0\nMobileLLM-R1-140M\n140M\n6.2\n4.1\n--\n--\n1.7\n150M - 400M\nGemma-3-270m-it\n268M\n6.8\n8.4\n--\n--\n0.0\nSmolLM2-360M-Instruct\n362M\n3.4\n8.1\n--\n--\n0.7\nMobileLLM-R1-360M\n359M\n28.4\n24.5\n--\n--\n5.1\n400M - 1B\nQwen2.5-0.5B-Instruct\n494M\n31.2\n48.1\n0.1\n0.3\n3.6\nQwen3-0.6B\n596M\n73.0\n79.2\n11.3\n17.0\n14.9\nMobileLLM-R1-950M\n949M\n74.0\n67.5\n15.5\n16.3\n19.9\n> 1B\nGemma-3-1B-it\n1.0B\n45.4\n62.9\n0.9\n0.0\n2.0\nLLaMA3.2-1B-Instruct\n1.24B\n24.8\n38.8\n1.1\n0.2\n4.1\nOLMo-2-0425-1B-Instruct\n1.48B\n19.2\n69.7\n0.6\n0.1\n0.0\nOpenReasoning-Nemotron-1.5B\n1.54B\n83.4\n76.7\n49.7\n40.4\n28.3\nDeepSeek-R1-Distill-Qwen-1.5B\n1.54B\n83.2\n77.3\n29.1\n23.4\n19.9\nQwen2.5-1.5B-Instruct\n1.54B\n54.0\n70.0\n2.5\n0.9\n7.9\nSmolLM2-1.7B-Instruct\n1.71B\n19.2\n41.8\n0.3\n0.1\n4.4\nQwen3-1.7B\n2.03B\n89.4\n90.3\n47.0\n37.0\n29.8\nFor AIME, we evaluate models across 64 runs and report the average accuracy. For LiveCodeBench, results are reported as the average accuracy across 16 runs. Models with fewer than 400M parameters do not produce reliable AIME scores and are therefore denoted as 'â€”'.\nTraining\nTraining Process\nTraining stages and hyperparameter details\nIn the pretraining phase, MobileLLM-R1 models are randomly initialized and optimized using the Adam optimizer with hyperparameters (Î²_1, Î²_2, Îµ) = (0.9, 0.95, 1e-8), coupled with a weight decay coefficient of 0.1. The learning rate follows a 2k-step warmup schedule and then decays linearly from its peak to 10% of the maximum.\nIn the mid-training phase, we use Adam optimizer with learning rate linearly decays from its maximum value to zero. We employ knowledge distillation with Llama-3.1-8B-Instruct model as the teacher, where the student is trained via minimizing the KL divergence between its output logits and the teacher logits.\nIn the post-training phase, we use the Adam optimizer with zero weight decay. The learning rate warmup ratio is set to 0.03 for general-purpose SFT and 0.1 for reasoning-specific SFT, and it linearly decays from its maximum value to zero. Full training hyperparameters are provided in the table below.\nStage\nPhase\nTokens / Samples\nBS\nSequence Length\nSteps\nLR\n#GPUs\nTraining Time\nPre-training\nPhase1\n2T tokens\n16\n2k\n500k\n4.00E-03\n16 x 8\n4-5 days\nPhase2\n2T tokens\n16\n2k\n500k\n4.00E-03\n16 x 8\n4-5 days\nMid-training\nPhase1\n100B tokens\n4\n4k\n50K\n3.60E-04\n16 x 8\n1-2 days\nPhase2\n100B tokens\n4\n4k\n50K\n3.60E-04\n16 x 8\n1-2 days\nPost-training\nGeneral SFT\n866K samples\n4\n4k\n2 epochs\n5.00E-06\n16 x 8\n~2h\nReasoning SFT\n6.2M samples\n8\n32k\n4 epochs\n8.00E-05\n16 x 8\n~2.5days\nData Mix\nPre-training\nDataset\nRows\nTokens (B)\nPhase1 Mix Ratio\nPhase2 Mix Ratio\nStarCoder\n206,640,114\n263.8\n10.66%\n0.52%\nOpenWebMath\n6,117,786\n12.6\n6.93%\n23.33%\nFineWeb-Edu\n1,279,107,432\n1300\n63.75%\n54.83%\nWiki\n7,222,303\n3.7\n5.03%\n0.14%\nArxiv\n1,533,917\n28\n6.36%\n1.32%\nStackExchange\n29,249,120\n19.6\n5.03%\n0.86%\nAlgebraic stack\n3,404,331\n12.6\n2.25%\n1.26%\nNemotron science\n708,920\n2\n--\n0.03%\nNemotron code\n10,108,883\n16\n--\n0.72%\nNemotron math\n22,066,397\n15\n--\n3.01%\nCosmopedia\n31,064,744\n25\n--\n2.70%\nFacebook natural reasoning\n1,145,824\n1.8\n--\n3.18%\nFineMath\n48,283,984\n34\n--\n8.01%\npeS2o\n38,800,000\n50\n--\n0.08%\nTotal\n100%\n100%\nMid-training\nDataset\nSubset\nRows (M)\nPhase1 Mix Ratio\nPhase2 Mix Ratio\nDolmino\nDCLM Baseline\n606\n37.03%\n6.51%\nFLAN\n57.3\n4.10%\n0.72%\npeS2o\n38.8\n11.41%\n2.01%\nWiki\n6.17\n2.66%\n0.47%\nStackExchange\n2.48\n2.12%\n2.00%\nMath\n21\n11.63%\n29.10%\nNemotron\nNemotron-Pretraining-Code-v1\n882\n20.69%\n29.10%\nNemotron-CC-Math-v1\n144\n3.45%\n19.40%\nStarCoder\nStarCoder\n206\n6.90%\n9.70%\nBenchmark training set\nTriviaQA (train)  OBQA (train)  NaturalQuestions (train)  PIQA (train)  GSM8K (train)  BoolQ (train)  ARC-Easy (train)  ARC-Challenge (train)\n~0.01\n0\n0.97%\nTotal\n100.00%\n100.00%\nPost-training\nPhase\nDataset\nRows\nGeneral SFT\nTulu-3-sft-olmo-2-mixture-0225\n866K samples\nReasoning SFT\nOpenMathReasoning\n3.2M samples\nOpenScienceReasoning-2\n803K samples\nOpenCodeReasoning-2\n2.16M samples\nCitation\nIf you find our model useful for your research, please consider citing:\n@article{zhao2025mobilellm-r1,\ntitle={MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes},\nauthor={Zhao, Changsheng and Chang, Ernie and Liu, Zechun and Chang, Chia-Jung and Wen, Wei and Lai, Chen and Cao, Sheng, and Tian, Yuandong and Krishnamoorthi, Raghuraman and Shi, Yangyang and  Chandra, Vikas},\njournal={arXiv preprint arXiv:2509.24945},\nyear={2025}\n}\nContact\nChangsheng Zhao, Meta Inc (cszhao at meta dot com)\nErnie Chang, Meta Inc (erniecyc at meta dot com)\nZechun Liu, Meta Inc (zechunliu at meta dot com)\nLicense\nMobileLLM-R1 is FAIR NC licensed as of now"
}