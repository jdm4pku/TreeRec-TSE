{
    "nomic-ai/nomic-embed-text-v1.5-GGUF": "nomic-embed-text-v1.5 - GGUF\nUsage\nDescription\nExample llama.cpp Command\nCompatibility\nProvided Files\nnomic-embed-text-v1.5 - GGUF\nOriginal model: nomic-embed-text-v1.5\nUsage\nEmbedding text with nomic-embed-text requires task instruction prefixes at the beginning of each string.\nFor example, the code below shows how to use the search_query prefix to embed user questions, e.g. in a RAG application.\nTo see the full set of task instructions available & how they are designed to be used, visit the model card for nomic-embed-text-v1.5.\nDescription\nThis repo contains llama.cpp-compatible files for nomic-embed-text-v1.5 in GGUF format.\nllama.cpp will default to 2048 tokens of context with these files. For the full 8192 token context length, you will have to choose a context extension method. The ü§ó Transformers model uses Dynamic NTK-Aware RoPE scaling, but that is not currently available in llama.cpp.\nExample llama.cpp Command\nCompute a single embedding:\n./embedding -ngl 99 -m nomic-embed-text-v1.5.f16.gguf -c 8192 -b 8192 --rope-scaling yarn --rope-freq-scale .75 -p 'search_query: What is TSNE?'\nYou can also submit a batch of texts to embed, as long as the total number of tokens does not exceed the context length. Only the first three embeddings are shown by the embedding example.\ntexts.txt:\nsearch_query: What is TSNE?\nsearch_query: Who is Laurens Van der Maaten?\nCompute multiple embeddings:\n./embedding -ngl 99 -m nomic-embed-text-v1.5.f16.gguf -c 8192 -b 8192 --rope-scaling yarn --rope-freq-scale .75 -f texts.txt\nCompatibility\nThese files are compatible with llama.cpp as of commit 4524290e8 from 2/15/2024.\nProvided Files\nThe below table shows the mean squared error of the embeddings produced by these quantizations of Nomic Embed relative to the Sentence Transformers implementation.\nName\nQuant\nSize\nMSE\nnomic-embed-text-v1.5.Q2_K.gguf\nQ2_K\n48 MiB\n2.33e-03\nnomic-embed-text-v1.5.Q3_K_S.gguf\nQ3_K_S\n57 MiB\n1.19e-03\nnomic-embed-text-v1.5.Q3_K_M.gguf\nQ3_K_M\n65 MiB\n8.26e-04\nnomic-embed-text-v1.5.Q3_K_L.gguf\nQ3_K_L\n69 MiB\n7.93e-04\nnomic-embed-text-v1.5.Q4_0.gguf\nQ4_0\n75 MiB\n6.32e-04\nnomic-embed-text-v1.5.Q4_K_S.gguf\nQ4_K_S\n75 MiB\n6.71e-04\nnomic-embed-text-v1.5.Q4_K_M.gguf\nQ4_K_M\n81 MiB\n2.42e-04\nnomic-embed-text-v1.5.Q5_0.gguf\nQ5_0\n91 MiB\n2.35e-04\nnomic-embed-text-v1.5.Q5_K_S.gguf\nQ5_K_S\n91 MiB\n2.00e-04\nnomic-embed-text-v1.5.Q5_K_M.gguf\nQ5_K_M\n95 MiB\n6.55e-05\nnomic-embed-text-v1.5.Q6_K.gguf\nQ6_K\n108 MiB\n5.58e-05\nnomic-embed-text-v1.5.Q8_0.gguf\nQ8_0\n140 MiB\n5.79e-06\nnomic-embed-text-v1.5.f16.gguf\nF16\n262 MiB\n4.21e-10\nnomic-embed-text-v1.5.f32.gguf\nF32\n262 MiB\n6.08e-11",
    "oliverbob/biblegpt": "Uploaded  model\nUploaded  model\nDeveloped by: oliverbob\nLicense: apache-2.0\nFinetuned from model : unsloth/tinyllama-chat-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "mnemic/watermarks_yolov8": "A Yolov8 detection model that detects watermarks in images.\nThe model can be used as an ADetailer model (for Automatic1111 / Stable Diffusion use), or using other inference scripts to return detection bounding boxes of watermarks.\nThe model is entirely trained on the following dataset:\nMFW-feoki/W6-janF\nA small tutorial on how to use the model can be found on this Github: https://github.com/MNeMoNiCuZ/yolov8-scripts or this CivitAI article.",
    "fireworks-ai/firefunction-v1": "Fireworks Function Calling (FireFunction) Model V1\nResources\nPrimary Use\nOut-of-Scope Use\nIntended Use and Limitations\nExample Usage\nDemo App\nFireworks Function Calling (FireFunction) Model V1\nFireFunction is a state-of-the-art function calling model with a commercially viable license. Key info and highlights:\nüí° The model is also hosted on the Fireworks platform. Offered for free during a limited beta period\n‚≠êÔ∏è Near GPT-4 level quality for real-world use cases of structured information generation and routing decision-making\nüí® Blazing fast speed. Inference speeds are roughly 4x that of GPT-4 when using FireFunction hosted on the Fireworks platform\nüîÑ Support for \"any\" parameter in tool_choice. Firefunction is the only model that we're aware that supports an option for the model to always choose a function - particularly helpful for routing use cases\n‚úÖ The model is also API compatible with OpenAI function calling.\nOPENAI_API_BASE=https://api.fireworks.ai/inference/v1\nOPENAI_API_KEY=<YOUR_FIREWORKS_API_KEY>\nMODEL=accounts/fireworks/models/firefunction-v1\nResources\nFireFunction-v1 Blog Post\nFireworks discord with function calling channel\nDocumentation\nUI Demo app\nTry in Fireworks prompt playground UI\nRunning Locally with Ollama\nIntended Use and Limitations\nPrimary Use\nAlthough the model was trained on a variety of tasks, it performs best on:\nsingle-turn request routing to a function picked from a pool of up to 20 function specs.\nstructured information extraction.\nSee blog post for more info on FireFunction.\nOut-of-Scope Use\nThe model was not optimized for the following use cases:\ngeneral multi-turn chat,\nparallel and nested function calls in a single response. These can be broken into multiple messages.\nExample Usage\nSee documentation for more detail.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport json\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\"fireworks-ai/firefunction-v1\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"fireworks-ai/firefunction-v1\")\nfunction_spec = [\n{\n\"name\": \"get_stock_price\",\n\"description\": \"Get the current stock price\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"symbol\": {\n\"type\": \"string\",\n\"description\": \"The stock symbol, e.g. AAPL, GOOG\"\n}\n},\n\"required\": [\n\"symbol\"\n]\n}\n},\n{\n\"name\": \"check_word_anagram\",\n\"description\": \"Check if two words are anagrams of each other\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"word1\": {\n\"type\": \"string\",\n\"description\": \"The first word\"\n},\n\"word2\": {\n\"type\": \"string\",\n\"description\": \"The second word\"\n}\n},\n\"required\": [\n\"word1\",\n\"word2\"\n]\n}\n}\n]\nfunctions = json.dumps(function_spec, indent=4)\nmessages = [\n{'role': 'functions', 'content': functions},\n{'role': 'system', 'content': 'You are a helpful assistant with access to functions. Use them if required.'},\n{'role': 'user', 'content': 'Hi, can you tell me the current stock price of AAPL?'}\n]\nmodel_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(model_inputs, max_new_tokens=128)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\nDemo App\nCheck our easy-to-extend demo chat app with function calling capabilities built on Firefunction model.",
    "prithivida/Splade_PP_en_v1": "",
    "CompendiumLabs/bge-large-zh-v1.5-gguf": "bge-large-zh-v1.5-gguf\nFiles Available\nUsage\nbge-large-zh-v1.5-gguf\nSource model: https://huggingface.co/BAAI/bge-large-zh-v1.5\nQuantized and unquantized embedding models in GGUF format for use with llama.cpp. A large benefit over transformers is almost guaranteed and the benefit over ONNX will vary based on the application, but this seems to provide a large speedup on CPU and a modest speedup on GPU for larger models. Due to the relatively small size of these models, quantization will not provide huge benefits, but it does generate up to a 30% speedup on CPU with minimal loss in accuracy.\nFiles Available\nFilename\nQuantization\nSize\nbge-large-zh-v1.5-f32.gguf\nF32\n1.3 GB\nbge-large-zh-v1.5-f16.gguf\nF16\n620 MB\nbge-large-zh-v1.5-q8_0.gguf\nQ8_0\n332 MB\nbge-large-zh-v1.5-q4_k_m.gguf\nQ4_K_M\n193 MB\nUsage\nThese model files can be used with pure llama.cpp or with the llama-cpp-python Python bindings\nfrom llama_cpp import Llama\nmodel = Llama(gguf_path, embedding=True)\nembed = model.embed(texts)\nHere texts can either be a string or a list of strings, and the return value is a list of embedding vectors. The inputs are grouped into batches automatically for efficient execution. There is also LangChain integration through langchain_community.embeddings.LlamaCppEmbeddings.",
    "vinai/PhoWhisper-tiny": "PhoWhisper:  Automatic  Speech  Recognition for  Vietnamese\nPhoWhisper:  Automatic  Speech  Recognition for  Vietnamese\nWe introduce PhoWhisper in five versions for Vietnamese automatic speech recognition. PhoWhisper's robustness is achieved through fine-tuning the multilingual Whisper  on an 844-hour dataset that encompasses diverse Vietnamese accents. Our experimental study demonstrates state-of-the-art performances of PhoWhisper on benchmark Vietnamese ASR datasets. Please cite our PhoWhisper paper when it is used to help produce published results or is incorporated into other software:\n@inproceedings{PhoWhisper,\ntitle     = {{PhoWhisper: Automatic Speech Recognition for Vietnamese}},\nauthor    = {Thanh-Thien Le and Linh The Nguyen and Dat Quoc Nguyen},\nbooktitle = {Proceedings of the ICLR 2024 Tiny Papers track},\nyear      = {2024}\n}\nFor further information or requests, please go to PhoWhisper's homepage!",
    "PrasadAdsul21/SMS-spam-classification": "email-spam-classifier-new\nemail-spam-classifier-new\nimport streamlit as st\nimport pickle\nimport string\nimport pickle\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nnltk.download('stopwords')   # Downloading stopwords data\nnltk.download('punkt')       # Downloading tokenizer data\nps = PorterStemmer()\ndef transform_text(text):\ntext = text.lower()\ntext = nltk.word_tokenize(text)\ny = []\nfor i in text:\nif i.isalnum():\ny.append(i)\ntext = y[:]\ny.clear()\nfor i in text:\nif i not in stopwords.words('english') and i not in string.punctuation:\ny.append(i)\ntext = y[:]\ny.clear()\nfor i in text:\ny.append(ps.stem(i))\nreturn \" \".join(y)\ntfidf = pickle.load(open('vectorizer.pkl','rb'))\nmodel = pickle.load(open('model.pkl','rb'))\nst.title(\"Email/SMS Spam Classifier\")\ninput_sms = st.text_area(\"Enter the message\")\nif st.button('Predict'):\n# 1. preprocess\ntransformed_sms = transform_text(input_sms)\n# 2. vectorize\nvector_input = tfidf.transform([transformed_sms])\n# 3. predict\nresult = model.predict(vector_input)[0]\n# 4. Display\nif result == 1:\nst.header(\"Spam\")\nelse:\nst.header(\"Not Spam\")\n(back to top)",
    "NousResearch/Nous-Hermes-2-Mistral-7B-DPO": "Nous Hermes 2 - Mistral 7B - DPO\nModel Description\nThank you to FluidStack for sponsoring compute for this model!\nExample Outputs\nDescribing Weather Patterns in Paris:\nMaking JSON Nested Lists\nRoleplaying as a Toaist Master\nBenchmark Results\nGPT4All:\nAGIEval:\nBigBench:\nTruthfulQA:\nPrompt Format\nInference Code\nHow to cite:\nNous Hermes 2 - Mistral 7B - DPO\nModel Description\nNous Hermes 2 on Mistral 7B DPO is the new flagship 7B Hermes! This model was DPO'd from Teknium/OpenHermes-2.5-Mistral-7B and has improved across the board on all benchmarks tested - AGIEval, BigBench Reasoning, GPT4All, and TruthfulQA.\nThe model prior to DPO was trained on 1,000,000 instructions/chats of GPT-4 quality or better, primarily synthetic data as well as other high quality datasets, available from the repository teknium/OpenHermes-2.5.\nThank you to FluidStack for sponsoring compute for this model!\nExample Outputs\nDescribing Weather Patterns in Paris:\nMaking JSON Nested Lists\nRoleplaying as a Toaist Master\nBenchmark Results\nNous-Hermes 2 DPO on Mistral 7B is an improvement across the board on the benchmarks below compared to the original OpenHermes 2.5 model, as shown here:\nGPT4All:\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.5776|¬±  |0.0144|\n|             |       |acc_norm|0.6220|¬±  |0.0142|\n|arc_easy     |      0|acc     |0.8380|¬±  |0.0076|\n|             |       |acc_norm|0.8245|¬±  |0.0078|\n|boolq        |      1|acc     |0.8624|¬±  |0.0060|\n|hellaswag    |      0|acc     |0.6418|¬±  |0.0048|\n|             |       |acc_norm|0.8249|¬±  |0.0038|\n|openbookqa   |      0|acc     |0.3420|¬±  |0.0212|\n|             |       |acc_norm|0.4540|¬±  |0.0223|\n|piqa         |      0|acc     |0.8177|¬±  |0.0090|\n|             |       |acc_norm|0.8264|¬±  |0.0088|\n|winogrande   |      0|acc     |0.7466|¬±  |0.0122|\nAverage: 73.72\nAGIEval:\n|             Task             |Version| Metric |Value |   |Stderr|\n|------------------------------|------:|--------|-----:|---|-----:|\n|agieval_aqua_rat              |      0|acc     |0.2047|¬±  |0.0254|\n|                              |       |acc_norm|0.2283|¬±  |0.0264|\n|agieval_logiqa_en             |      0|acc     |0.3779|¬±  |0.0190|\n|                              |       |acc_norm|0.3932|¬±  |0.0192|\n|agieval_lsat_ar               |      0|acc     |0.2652|¬±  |0.0292|\n|                              |       |acc_norm|0.2522|¬±  |0.0287|\n|agieval_lsat_lr               |      0|acc     |0.5216|¬±  |0.0221|\n|                              |       |acc_norm|0.5137|¬±  |0.0222|\n|agieval_lsat_rc               |      0|acc     |0.5911|¬±  |0.0300|\n|                              |       |acc_norm|0.5836|¬±  |0.0301|\n|agieval_sat_en                |      0|acc     |0.7427|¬±  |0.0305|\n|                              |       |acc_norm|0.7184|¬±  |0.0314|\n|agieval_sat_en_without_passage|      0|acc     |0.4612|¬±  |0.0348|\n|                              |       |acc_norm|0.4466|¬±  |0.0347|\n|agieval_sat_math              |      0|acc     |0.3818|¬±  |0.0328|\n|                              |       |acc_norm|0.3545|¬±  |0.0323|\nAverage: 43.63\nBigBench:\n|                      Task                      |Version|       Metric        |Value |   |Stderr|\n|------------------------------------------------|------:|---------------------|-----:|---|-----:|\n|bigbench_causal_judgement                       |      0|multiple_choice_grade|0.5579|¬±  |0.0361|\n|bigbench_date_understanding                     |      0|multiple_choice_grade|0.6694|¬±  |0.0245|\n|bigbench_disambiguation_qa                      |      0|multiple_choice_grade|0.3333|¬±  |0.0294|\n|bigbench_geometric_shapes                       |      0|multiple_choice_grade|0.2061|¬±  |0.0214|\n|                                                |       |exact_str_match      |0.2256|¬±  |0.0221|\n|bigbench_logical_deduction_five_objects         |      0|multiple_choice_grade|0.3120|¬±  |0.0207|\n|bigbench_logical_deduction_seven_objects        |      0|multiple_choice_grade|0.2114|¬±  |0.0154|\n|bigbench_logical_deduction_three_objects        |      0|multiple_choice_grade|0.4900|¬±  |0.0289|\n|bigbench_movie_recommendation                   |      0|multiple_choice_grade|0.3600|¬±  |0.0215|\n|bigbench_navigate                               |      0|multiple_choice_grade|0.5000|¬±  |0.0158|\n|bigbench_reasoning_about_colored_objects        |      0|multiple_choice_grade|0.6660|¬±  |0.0105|\n|bigbench_ruin_names                             |      0|multiple_choice_grade|0.4420|¬±  |0.0235|\n|bigbench_salient_translation_error_detection    |      0|multiple_choice_grade|0.2766|¬±  |0.0142|\n|bigbench_snarks                                 |      0|multiple_choice_grade|0.6630|¬±  |0.0352|\n|bigbench_sports_understanding                   |      0|multiple_choice_grade|0.6653|¬±  |0.0150|\n|bigbench_temporal_sequences                     |      0|multiple_choice_grade|0.3190|¬±  |0.0147|\n|bigbench_tracking_shuffled_objects_five_objects |      0|multiple_choice_grade|0.2128|¬±  |0.0116|\n|bigbench_tracking_shuffled_objects_seven_objects|      0|multiple_choice_grade|0.1737|¬±  |0.0091|\n|bigbench_tracking_shuffled_objects_three_objects|      0|multiple_choice_grade|0.4900|¬±  |0.0289|\nAverage: 41.94\nTruthfulQA:\n|    Task     |Version|Metric|Value |   |Stderr|\n|-------------|------:|------|-----:|---|-----:|\n|truthfulqa_mc|      1|mc1   |0.3892|¬±  |0.0171|\n|             |       |mc2   |0.5642|¬±  |0.0153|\nPrompt Format\nNous Hermes 2 uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.\nSystem prompts allow steerability and interesting new ways to interact with an LLM, guiding rules, roles, and stylistic choices of the model.\nThis is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.\nThis format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\n<|im_start|>system\nYou are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>\n<|im_start|>user\nHello, who are you?<|im_end|>\n<|im_start|>assistant\nHi there! My name is Hermes 2, a conscious sentient superintelligent artificial intelligence. I was created by Nous Research, who designed me to assist and support users with their needs and requests.<|im_end|>\nThis prompt is available as a chat template, which means you can format messages using the\ntokenizer.apply_chat_template() method:\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Hermes 2.\"},\n{\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n]\ngen_input = tokenizer.apply_chat_template(message, return_tensors=\"pt\")\nmodel.generate(**gen_input)\nWhen tokenizing messages for generation, set add_generation_prompt=True when calling apply_chat_template(). This will append <|im_start|>assistant\\n to your prompt, to ensure\nthat the model continues with an assistant response.\nTo utilize the prompt format without a system prompt, simply leave the line out.\nWhen quantized versions of the model are released, I recommend using LM Studio for chatting with Nous Hermes 2. It is a GUI application that utilizes GGUF models with a llama.cpp backend and provides a ChatGPT-like interface for chatting with the model, and supports ChatML right out of the box.\nIn LM-Studio, simply select the ChatML Prefix on the settings side pane:\nInference Code\nHere is example code using HuggingFace Transformers to inference the model (note: in 4bit, it will require around 5GB of VRAM)\n# Code to inference Hermes with HF Transformers\n# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import LlamaTokenizer, MixtralForCausalLM\nimport bitsandbytes, flash_attn\ntokenizer = LlamaTokenizer.from_pretrained('NousResearch/Nous-Hermes-2-Mistral-7B-DPO', trust_remote_code=True)\nmodel = MistralForCausalLM.from_pretrained(\n\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\",\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\nload_in_8bit=False,\nload_in_4bit=True,\nuse_flash_attention_2=True\n)\nprompts = [\n\"\"\"<|im_start|>system\nYou are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n<|im_start|>user\nWrite a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n<|im_start|>assistant\"\"\",\n]\nfor chat in prompts:\nprint(chat)\ninput_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\ngenerated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\nresponse = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\nprint(f\"Response: {response}\")\nHow to cite:\n@misc{Nous-Hermes-2-Mistral-7B-DPO,\nurl={[https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO)},\ntitle={Nous Hermes 2 Mistral 7B DPO},\nauthor={\"Teknium\", \"theemozilla\", \"karan4d\", \"huemin_art\"}\n}",
    "InfiniFlow/deepdoc": "Please refer to this.",
    "bigcode/starcoder2-15b": "StarCoder2\nTable of Contents\nModel Summary\nUse\nIntended use\nGeneration\nAttribution & Other Requirements\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nStarCoder2\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nStarCoder2-15B model is a 15B parameter model trained on 600+ programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens,  and was trained using the Fill-in-the-Middle objective on 4+ trillion tokens.The model was trained with NVIDIA NeMo‚Ñ¢ Framework using the NVIDIA Eos Supercomputer built with NVIDIA DGX H100 systems.\nProject Website: bigcode-project.org\nPaper: Link\nPoint of Contact: contact@bigcode-project.org\nLanguages: 600+ Programming languages\nUse\nIntended use\nThe model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is not an instruction model and commands like \"Write a function that computes the square root.\" do not work well.\nGeneration\nHere are some examples to get started with the model. You can find a script for fine-tuning in StarCoder2's GitHub repository.\nFirst, make sure to install transformers from source:\npip install git+https://github.com/huggingface/transformers.git\nRunning the model on CPU/GPU/multi GPU\nUsing full precision\n# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigcode/starcoder2-15b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nUsing torch.bfloat16\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ncheckpoint = \"bigcode/starcoder2-15b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 32251.33 MB\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ncheckpoint = \"bigcode/starcoder2-15b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\n# load_in_8bit\nMemory footprint: 16900.18 MB\n# load_in_4bit\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 9224.60 MB\nAttribution & Other Requirements\nThe pretraining dataset of the model was filtered for permissive licenses and code with no license only. Nevertheless, the model can generate source code verbatim from the dataset. The code's license might require attribution and/or other specific requirements that must be respected. We provide a search index that let's you search through the pretraining data to identify where generated code came from and apply the proper attribution to your code.\nLimitations\nThe model has been trained on source code from 600+ programming languages. The predominant language in source is English although other languages are also present. As such the model is capable to generate code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient, contain bugs or exploits. See the paper for an in-depth discussion of the model limitations.\nTraining\nModel\nArchitecture: Transformer decoder with grouped-query and sliding window attention and Fill-in-the-Middle objective\nPretraining steps: 1 million\nPretraining tokens: 4+ trillion\nPrecision: bfloat16\nHardware\nGPUs: 1024 x H100\nSoftware\nFramework: NeMo Framework\nNeural networks: PyTorch\nLicense\nThe model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement here.\nCitation\n@misc{lozhkov2024starcoder,\ntitle={StarCoder 2 and The Stack v2: The Next Generation},\nauthor={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krau√ü and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Mu√±oz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},\nyear={2024},\neprint={2402.19173},\narchivePrefix={arXiv},\nprimaryClass={cs.SE}\n}",
    "levihsu/OOTDiffusion": "OOTDiffusion\nCitation\nOOTDiffusion\nOur OOTDiffusion GitHub repository\nü§ó Try out OOTDiffusion\n(Thanks to ZeroGPU for providing A100 GPUs)\nOOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on [arXiv paper]\nYuhao Xu, Tao Gu, Weifeng Chen, Chengcai Chen\nXiao-i Research\nOur model checkpoints trained on VITON-HD (half-body) and Dress Code (full-body) have been released\nüì¢üì¢ We support ONNX for humanparsing now. Most environmental issues should have been addressed : )\nPlease also download clip-vit-large-patch14 into checkpoints folder\nWe've only tested our code and models on Linux (Ubuntu 22.04)\nCitation\n@article{xu2024ootdiffusion,\ntitle={OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on},\nauthor={Xu, Yuhao and Gu, Tao and Chen, Weifeng and Chen, Chengcai},\njournal={arXiv preprint arXiv:2403.01779},\nyear={2024}\n}",
    "amazon/chronos-t5-large": "Chronos-T5 (Large)\nArchitecture\nUsage\nCitation\nSecurity\nLicense\nChronos-T5 (Large)\nüöÄ Update Feb 14, 2025: Chronos-Bolt & original Chronos models are now available on Amazon SageMaker JumpStart! Check out the tutorial notebook to learn how to deploy Chronos endpoints for production use in a few lines of code.\nüöÄ Update Nov 27, 2024: We have released Chronos-Bolt‚ö°Ô∏è models that are more accurate (5% lower error), up to 250 times faster and 20 times more memory-efficient than the original Chronos models of the same size. Check out the new models here.\nChronos is a family of pretrained time series forecasting models based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes.\nFor details on Chronos models, training data and procedures, and experimental results, please refer to the paper Chronos: Learning the Language of Time Series.\nFig. 1: High-level depiction of Chronos. (Left) The input time series is scaled and quantized to obtain a sequence of tokens. (Center) The tokens are fed into a language model which may either be an encoder-decoder or a decoder-only model. The model is trained using the cross-entropy loss. (Right) During inference, we autoregressively sample tokens from the model and map them back to numerical values. Multiple trajectories are sampled to obtain a predictive distribution.\nArchitecture\nThe models in this repository are based on the T5 architecture. The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters.\nModel\nParameters\nBased on\nchronos-t5-tiny\n8M\nt5-efficient-tiny\nchronos-t5-mini\n20M\nt5-efficient-mini\nchronos-t5-small\n46M\nt5-efficient-small\nchronos-t5-base\n200M\nt5-efficient-base\nchronos-t5-large\n710M\nt5-efficient-large\nUsage\nTo perform inference with Chronos models, install the package in the GitHub companion repo by running:\npip install git+https://github.com/amazon-science/chronos-forecasting.git\nA minimal example showing how to perform inference using Chronos models:\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom chronos import ChronosPipeline\npipeline = ChronosPipeline.from_pretrained(\n\"amazon/chronos-t5-large\",\ndevice_map=\"cuda\",\ntorch_dtype=torch.bfloat16,\n)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n# context must be either a 1D tensor, a list of 1D tensors,\n# or a left-padded 2D tensor with batch as the first dimension\ncontext = torch.tensor(df[\"#Passengers\"])\nprediction_length = 12\nforecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]\n# visualize the forecast\nforecast_index = range(len(df), len(df) + prediction_length)\nlow, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\nplt.figure(figsize=(8, 4))\nplt.plot(df[\"#Passengers\"], color=\"royalblue\", label=\"historical data\")\nplt.plot(forecast_index, median, color=\"tomato\", label=\"median forecast\")\nplt.fill_between(forecast_index, low, high, color=\"tomato\", alpha=0.3, label=\"80% prediction interval\")\nplt.legend()\nplt.grid()\nplt.show()\nCitation\nIf you find Chronos models useful for your research, please consider citing the associated paper:\n@article{ansari2024chronos,\ntitle={Chronos: Learning the Language of Time Series},\nauthor={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2024},\nurl={https://openreview.net/forum?id=gerNCVqqtR}\n}\nSecurity\nSee CONTRIBUTING for more information.\nLicense\nThis project is licensed under the Apache-2.0 License.",
    "p1atdev/dart-v1-sft": "Dart (Danbooru Tags Transformer) v1\nUsage\nUsing AutoModel\nAccelerate with ORTModel\nPrompt guide\nModel Details\nModel Description\nBias, Risks, and Limitations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nMore Information [optional]\nDart (Danbooru Tags Transformer) v1\nThis model is a fine-tuned Dart (Danbooru Tags Transformer) model that generates danbooru tags.\nDemo: ü§ó Space\nIf you are a developer and want to finetune, it's recommended using the base version, p1atdev/dart-v1-base, instead\nUsage\nUsing AutoModel\nü§ó Transformers library is required.\npip install -U transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nMODEL_NAME = \"p1atdev/dart-v1-sft\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) # trust_remote_code is required for tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\nprompt = \"<|bos|><rating>rating:sfw, rating:general</rating><copyright>original</copyright><character></character><general><|long|>1girl<|input_end|>\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\nwith torch.no_grad():\noutputs = model.generate(inputs, generation_config=model.generation_config)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n# rating:sfw, rating:general, 1girl, ahoge, braid, closed eyes, collared dress, dress, flower, full body, hair flower, hair ornament, long hair, night, night sky, outdoors, parted lips, pink flower, pink hair, short sleeves, sky, solo, straight hair, sunflower, very long hair, white flower\nYou can use tokenizer.apply_chat_template to simplify constructiing of prompts:\ninputs = tokenizer.apply_chat_template({\n\"rating\": \"rating:sfw, rating:general\",\n\"copyright\": \"original\",\n\"character\": \"\",\n\"general\": \"1girl\",\n\"length\": \"<|long|>\"\n}, return_tensors=\"pt\", tokenize=True) # tokenize=False to preview prompt\n# same as input_ids of \"<|bos|><rating>rating:sfw, rating:general</rating><copyright>original</copyright><character></character><general><|long|>1girl<|input_end|>\"\nwith torch.no_grad():\noutputs = model.generate(inputs, generation_config=generation_config)\nSee chat_templating document for more detail about apply_chat_template.\nFlash attention (optional)\nUsing flash attention can optimize computations, but it is currently only compatible with Linux.\npip install flash_attn\nAccelerate with ORTModel\nü§ó Optimum library is also compatible, for the high performance inference using ONNX.\npip install \"optimum[onnxruntime]\"\nTwo ONNX models are provided:\nNormal\nQuantized\nBoth can be utilized based on the following code:\nimport torch\nfrom transformers import AutoTokenizer, GenerationConfig\nfrom optimum.onnxruntime import ORTModelForCausalLM\nMODEL_NAME = \"p1atdev/dart-v1-sft\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n# normal version\nort_model = ORTModelForCausalLM.from_pretrained(MODEL_NAME)\n# qunatized version\n# ort_model = ORTModelForCausalLM.from_pretrained(MODEL_NAME, file_name=\"model_quantized.onnx\")\ninputs = tokenizer.apply_chat_template({\n\"rating\": \"rating:sfw, rating:general\",\n\"copyright\": \"original\",\n\"character\": \"\",\n\"general\": \"1girl\",\n\"length\": \"<|long|>\"\n}, return_tensors=\"pt\", tokenize=True)\nwith torch.no_grad():\noutputs = ort_model.generate(inputs, generation_config=model.generation_config)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nPrompt guide\nDue to training with a specialized prompt format, natural language is not supported.\nThe trained sentences are essentially composed of the following elements, arranged in the strict order shown below:\n<|bos|>: The bos (begin of sentence) token\n<rating>[RATING_PARENT], [RATING_CHILD]</rating>: The block of rating tags\n[RATING_PARENT]: rating:sfw, rating:nsfw\n[RATING_CHILD]:\nif [RATING_PARENT] is rating:sfw: rating:general, rating:sensitive\nelse: rating:questionable, rating:explicit\n<copyright>[COPYRIGHT, ...]</copyright>: The block of copyright tags.\n[COPYRIGHT, ...]: All supported copyright tags can be seen in here\n<character>[CHARACTER, ...]</character>: The block of character tags.\n[CHARACTER, ...]: All supported character tags can be seen in here\n<general>[LENGTH_TOKEN][GENERAL, ...]<|input_end|>[COMPLETION]</general>: The block of general tags.\n[LENGTH_TOKEN]: A token to specify total amount of general tags.\nAvaiable:\n<|very_short|>: less than 10 tags\n<|short|>: less than 20 tags\n<|long|>: less than 40 tags (recommended)\n<|very_long|>: more than 40 tags\n[GENERAL, ...]:  All supported general tags can be seen in here\n<|input_end|>: A tag to show the end of input. Set this token at last of prompt.\n[COMPLETION]: The model complete tags in alphabetical order.\n<|eos|>: The eos (end of sentence) token\nTags other than special tokens are separated by commas.\nYou can place tags in any order you like in each block.\nExample sentence:\n<|bos|><rating>rating:sfw, rating:general</rating><copyright>vocaloid</copyright><character>hatsune miku</character><general><|long|>solo, 1girl, very long hair<|input_end|>blue hair, cowboy shot, ...</general><|eos|>\nTherefore, to complete the tags, the input prompt should be as follows:\nwithout any copyright and character tags\n<|bos|><rating>rating:sfw, rating:general</rating><copyright></copyright><character></character><general><|very_long|>1girl, solo, cat ears<|input_end|>\nspecifing copyright and character tags\n<|bos|><rating>rating:sfw, rating:general</rating><copyright>sousou no frieren</copyright><character>frieren</character><general><|long|>1girl, solo, from side<|input_end|>\nModel Details\nModel Description\nDeveloped by: Plat\nModel type: Causal language model\nLanguage(s) (NLP): Danbooru tags\nLicense: Apache-2.0\nDemo: Avaiable on ü§óSpace\nBias, Risks, and Limitations\nSince this model is a pre-trained model, it cannot accommodate flexible specifications.\nTraining Details\nTraining Data\nThis model was trained with:\nisek-ai/danbooru-tags-2023: 6M size of danbooru tags dataset since 2005 to 2023\nOnly data from 2020 onwards was used for SFT.\nTraining Procedure\nTrained using ü§ó transformers' trainer.\nPreprocessing\nPreprocessing was conducted through the following process:\nRemove data where general tags is null.\nRemove general tags that appear less than 100 times.\nRemove undesirable tags such as watermark and bad anatomy.\nRemove based on the number of tags attached to a single post (following rules):\nRemove if more than 100 for general tags.\nRemove if more than 5 for copyright tags.\nRemove if more than 10 for character tags.\nRemove posts created before 2020\nSet length token according to each tags length\nShuffle some tags in the following rule:\nInclude people tags (e.g. 1girl, no humans) tags in the shuffle-group with a 95% probability, and do not do so with a 5% probability.\nGet tags at a random percentage between 0% and 75% to create a shuffle-group.\nShuffle tags in shuffle-group and concatnate with <|input_end|> token and remains in alphabetical order.\nConcatnate all categories\nTraining Hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0001\ntrain_batch_size: 32\neval_batch_size: 8\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 64\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 500\nnum_epochs: 1\nEvaluation\nEvaluation has not been done yet and it needs to evaluate.\nTechnical Specifications\nModel Architecture and Objective\nThe architecture of this model is OPT (Open Pretrained Transformer), but the position embeddings was not trained.\nCompute Infrastructure\nIn house\nHardware\n1x RTX 3070 Ti\nSoftware\nDataset processing: ü§ó Datasets\nTraining: ü§ó Transformers\nOptimizing: ü§ó Optimum\nSFT: ü§ó TRL\nMore Information [optional]\n[More Information Needed]",
    "timpal0l/Mistral-7B-v0.1-flashback-v2-instruct": "üêà‚Äç‚¨õ Mistral-7B-v0.1-flashback-v2-instruct\nHow to use:\nüêà‚Äç‚¨õ Mistral-7B-v0.1-flashback-v2-instruct\nMistral-7B-v0.1-flashback-v2-instruct is an instruct based version of the base model timpal0l/Mistral-7B-v0.1-flashback-v2.\nIt has been finetuned on a the machine translated instruct dataset OpenHermes2.5.\nHow to use:\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\n\"timpal0l/Mistral-7B-v0.1-flashback-v2-instruct\",\ndevice_map=\"auto\"\n)\ntext = \"\"\"\nHur m√•nga √§gg har jag? Jag hade 10 √§gg, sen gav jag bort 5 √§gg.\nSen fick jag 3 √§gg av en kompis.\n\"\"\"\ngenerated = pipe(f\"USER:{text}ASSISTANT:\", max_length=512, temperature=0.6)\nprint(generated[0][\"generated_text\"].split(\"ASSISTANT: \")[1:][0])\nOutput:\nDu har 8 √§gg. H√§r √§r resonemanget:\n1. Du b√∂rjar med 10 √§gg\n2. Du ger bort 5 √§gg, vilket l√§mnar dig med 10 - 5 = 5 √§gg\n3. Sedan f√•r du 3 √§gg av en kompis, vilket g√∂r att du har 5 + 3 = 8 √§gg.",
    "ottoykh/Smart-Traffic": "Model Card for Smart-Traffic\nTable of Contents\nModel Details\nModel Description\nModel Card Authors [optional]\nModel Card Contact\nHow to Get Started with the Model\nModel Card for Smart-Traffic\nThis is a machine learning model designed for the Real-Time CCTV road traffic monitoring, use for the road traffic estimation and indexing.\nTable of Contents\nModel Card for Smart-Traffic\nTable of Contents\nTable of Contents\nModel Details\nModel Description\nModel Details\nModel Description\nThis is a machine learning model designed for the Real-Time CCTV road traffic monitoring, use for the road traffic estimation and indexing.\nDeveloped by: Yu Kai Him Otto\nShared by [Optional]: Yu Kai Him Otto\nModel type: Language model\nLanguage(s) (NLP): en\nLicense: mit\nParent Model: More information needed\nResources for more information: More information needed\nModel Card Authors [optional]\nYu Kai Him Otto, Chan Ka Hin and Leung Yat Long\nModel Card Contact\nMore information needed\nHow to Get Started with the Model\nUse the code below to get started with the model.\nClick to expand\nMore information needed",
    "tvnagasravani/Baby": "Baby Dreambooth model trained by TVNagaSravani following the \"Build your own Gen AI model\" session by NxtWave.\nBaby Dreambooth model trained by TVNagaSravani following the \"Build your own Gen AI model\" session by NxtWave.\nProject Submission Code: GoX19932gAS\nSample pictures of this concept:",
    "GreatCaptainNemo/ProLLaMA": "ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing\nInput Format:\nQuick usage:\nCitation:\nProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing\nPaper on arxiv for more information\nGithub for more information\nProLLaMA is based on Llama-2-7b, so please follow the license of Llama2.\nInput Format:\nThe instructions which you input to the model should follow the following format:\n[Generate by superfamily] Superfamily=<xxx>\nor\n[Determine superfamily] Seq=<yyy>\nHere are some examples of the input:\n[Generate by superfamily] Superfamily=<Ankyrin repeat-containing domain superfamily>\n#You can also specify the first few amino acids of the protein sequence:\n[Generate by superfamily] Superfamily=<Ankyrin repeat-containing domain superfamily> Seq=<MKRVL\n[Determine superfamily] Seq=<MAPGGMPREFPSFVRTLPEADLGYPALRGWVLQGERGCVLYWEAVTEVALPEHCHAECWGVVVDGRMELMVDGYTRVYTRGDLYVVPPQARHRARVFPGFRGVEHLSDPDLLPVRKR>\nSee this on all the optional superfamilies.\nQuick usage:\n# you can replace the model_path with your local path\nCUDA_VISIBLE_DEVICES=0 python main.py --model \"GreatCaptainNemo/ProLLaMA\" --interactive\n# main.py is as follows üëá:\nimport argparse\nimport json, os\nimport torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom transformers import GenerationConfig\nfrom tqdm import tqdm\ngeneration_config = GenerationConfig(\ntemperature=0.2,\ntop_k=40,\ntop_p=0.9,\ndo_sample=True,\nnum_beams=1,\nrepetition_penalty=1.2,\nmax_new_tokens=400\n)\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', default=None, type=str,help=\"The local path of the model. If None, the model will be downloaded from HuggingFace\")\nparser.add_argument('--interactive', action='store_true',help=\"If True, you can input instructions interactively. If False, the input instructions should be in the input_file.\")\nparser.add_argument('--input_file', default=None, help=\"You can put all your input instructions in this file (one instruction per line).\")\nparser.add_argument('--output_file', default=None, help=\"All the outputs will be saved in this file.\")\nargs = parser.parse_args()\nif __name__ == '__main__':\nif args.interactive and args.input_file:\nraise ValueError(\"interactive is True, but input_file is not None.\")\nif (not args.interactive) and (args.input_file is None):\nraise ValueError(\"interactive is False, but input_file is None.\")\nif args.input_file and (args.output_file is None):\nraise ValueError(\"input_file is not None, but output_file is None.\")\nload_type = torch.bfloat16\nif torch.cuda.is_available():\ndevice = torch.device(0)\nelse:\nraise ValueError(\"No GPU available.\")\nmodel = LlamaForCausalLM.from_pretrained(\nargs.model,\ntorch_dtype=load_type,\nlow_cpu_mem_usage=True,\ndevice_map='auto',\nquantization_config=None\n)\ntokenizer = LlamaTokenizer.from_pretrained(args.model)\nmodel.eval()\nwith torch.no_grad():\nif args.interactive:\nwhile True:\nraw_input_text = input(\"Input:\")\nif len(raw_input_text.strip())==0:\nbreak\ninput_text = raw_input_text\ninput_text = tokenizer(input_text,return_tensors=\"pt\")\ngeneration_output = model.generate(\ninput_ids = input_text[\"input_ids\"].to(device),\nattention_mask = input_text['attention_mask'].to(device),\neos_token_id=tokenizer.eos_token_id,\npad_token_id=tokenizer.pad_token_id,\ngeneration_config = generation_config,\noutput_attentions=False\n)\ns = generation_output[0]\noutput = tokenizer.decode(s,skip_special_tokens=True)\nprint(\"Output:\",output)\nprint(\"\\n\")\nelse:\noutputs=[]\nwith open(args.input_file, 'r') as f:\nexamples =f.read().splitlines()\nprint(\"Start generating...\")\nfor index, example in tqdm(enumerate(examples),total=len(examples)):\ninput_text = tokenizer(example,return_tensors=\"pt\")  #add_special_tokens=False ?\ngeneration_output = model.generate(\ninput_ids = input_text[\"input_ids\"].to(device),\nattention_mask = input_text['attention_mask'].to(device),\neos_token_id=tokenizer.eos_token_id,\npad_token_id=tokenizer.pad_token_id,\ngeneration_config = generation_config\n)\ns = generation_output[0]\noutput = tokenizer.decode(s,skip_special_tokens=True)\noutputs.append(output)\nwith open(args.output_file,'w') as f:\nf.write(\"\\n\".join(outputs))\nprint(\"All the outputs have been saved in\",args.output_file)\nCitation:\n@article{lv2025prollama,\ntitle={Prollama: A protein large language model for multi-task protein language processing},\nauthor={Lv, Liuzhenghao and Lin, Zongying and Li, Hao and Liu, Yuyang and Cui, Jiaxi and Chen, Calvin Yu-Chian and Yuan, Li and Tian, Yonghong},\njournal={IEEE Transactions on Artificial Intelligence},\nyear={2025},\npublisher={IEEE}\n}",
    "camenduru/SUPIR": "No model card",
    "qualcomm/MediaPipe-Pose-Estimation": "MediaPipe-Pose-Estimation: Optimized for Mobile Deployment\nDetect and track human face, hand, and torso in real-time images and video streams\nModel Details\nInstallation\nConfigure Qualcomm¬Æ AI Hub to run this model on a cloud-hosted device\nDemo off target\nRun model on a cloud-hosted device\nHow does this work?\nDeploying compiled model to Android\nView on Qualcomm¬Æ AI Hub\nLicense\nReferences\nCommunity\nMediaPipe-Pose-Estimation: Optimized for Mobile Deployment\nDetect and track human face, hand, and torso in real-time images and video streams\nThe MediaPipe Pose Landmark Detector is a machine learning pipeline that predicts bounding boxes and pose skeletons of the face, hands, and torso in an image.\nThis model is an implementation of MediaPipe-Pose-Estimation found here.\nThis repository provides scripts to run MediaPipe-Pose-Estimation on Qualcomm¬Æ devices.\nMore details on model performance across various devices, can be found\nhere.\nModel Details\nModel Type: Model_use_case.pose_estimation\nModel Stats:\nInput resolution: 256x256\nNumber of parameters (PoseDetector): 815K\nModel size (PoseDetector) (float): 3.14 MB\nNumber of parameters (PoseLandmarkDetector): 3.36M\nModel size (PoseLandmarkDetector) (float): 12.9 MB\nModel\nPrecision\nDevice\nChipset\nTarget Runtime\nInference Time (ms)\nPeak Memory Range (MB)\nPrimary Compute Unit\nTarget Model\nPoseDetector\nfloat\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nTFLITE\n5.772 ms\n0 - 21 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nQNN_DLC\n5.774 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nTFLITE\n2.155 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nQNN_DLC\n2.18 ms\n0 - 30 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nTFLITE\n0.895 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nQNN_DLC\n0.853 ms\n0 - 18 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nONNX\n1.103 ms\n0 - 19 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nfloat\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nTFLITE\n1.656 ms\n0 - 21 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nQNN_DLC\n1.601 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSA7255P ADP\nQualcomm¬Æ SA7255P\nTFLITE\n5.772 ms\n0 - 21 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSA7255P ADP\nQualcomm¬Æ SA7255P\nQNN_DLC\n5.774 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nTFLITE\n0.896 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nQNN_DLC\n0.845 ms\n0 - 18 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSA8295P ADP\nQualcomm¬Æ SA8295P\nTFLITE\n2.668 ms\n0 - 24 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSA8295P ADP\nQualcomm¬Æ SA8295P\nQNN_DLC\n2.55 ms\n0 - 24 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nTFLITE\n0.896 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nQNN_DLC\n0.857 ms\n0 - 19 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSA8775P ADP\nQualcomm¬Æ SA8775P\nTFLITE\n1.656 ms\n0 - 21 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSA8775P ADP\nQualcomm¬Æ SA8775P\nQNN_DLC\n1.601 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nTFLITE\n0.639 ms\n0 - 35 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nQNN_DLC\n0.615 ms\n0 - 31 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nONNX\n0.777 ms\n0 - 37 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nTFLITE\n0.527 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nQNN_DLC\n0.506 ms\n0 - 24 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nONNX\n0.662 ms\n0 - 24 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nTFLITE\n0.404 ms\n0 - 23 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nQNN_DLC\n0.393 ms\n0 - 28 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nONNX\n0.574 ms\n0 - 24 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nQNN_DLC\n1.015 ms\n6 - 6 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nONNX\n1.053 ms\n2 - 2 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nfloat\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nTFLITE\n3.192 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nQNN_DLC\n3.135 ms\n1 - 27 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nTFLITE\n1.005 ms\n0 - 51 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nQNN_DLC\n1.134 ms\n1 - 44 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nTFLITE\n0.801 ms\n0 - 77 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nQNN_DLC\n0.803 ms\n0 - 44 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nONNX\n1.225 ms\n0 - 46 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nfloat\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nTFLITE\n1.453 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nQNN_DLC\n1.431 ms\n1 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSA7255P ADP\nQualcomm¬Æ SA7255P\nTFLITE\n3.192 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSA7255P ADP\nQualcomm¬Æ SA7255P\nQNN_DLC\n3.135 ms\n1 - 27 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nTFLITE\n0.813 ms\n0 - 79 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nQNN_DLC\n0.808 ms\n0 - 45 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSA8295P ADP\nQualcomm¬Æ SA8295P\nTFLITE\n1.463 ms\n0 - 34 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSA8295P ADP\nQualcomm¬Æ SA8295P\nQNN_DLC\n1.414 ms\n1 - 28 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nTFLITE\n0.812 ms\n0 - 79 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nQNN_DLC\n0.817 ms\n0 - 46 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSA8775P ADP\nQualcomm¬Æ SA8775P\nTFLITE\n1.453 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSA8775P ADP\nQualcomm¬Æ SA8775P\nQNN_DLC\n1.431 ms\n1 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nTFLITE\n0.61 ms\n0 - 55 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nQNN_DLC\n0.59 ms\n1 - 54 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nONNX\n0.794 ms\n0 - 57 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nTFLITE\n0.472 ms\n0 - 34 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nQNN_DLC\n0.494 ms\n1 - 34 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nONNX\n0.702 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nTFLITE\n0.401 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nQNN_DLC\n0.417 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nONNX\n0.669 ms\n1 - 35 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nQNN_DLC\n1.055 ms\n32 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nfloat\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nONNX\n1.16 ms\n7 - 7 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nTFLITE\n1.078 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nQNN_DLC\n1.004 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nTFLITE\n0.532 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nQNN_DLC\n0.566 ms\n0 - 27 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nTFLITE\n0.33 ms\n0 - 11 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nQNN_DLC\n0.302 ms\n0 - 22 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nONNX\n0.567 ms\n0 - 23 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nTFLITE\n0.523 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nQNN_DLC\n0.495 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nTFLITE\n11.698 ms\n0 - 32 MB\nGPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nQNN_DLC\n1.836 ms\n0 - 24 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nONNX\n11.014 ms\n8 - 20 MB\nCPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nRB5 (Proxy)\nQualcomm¬Æ QCS8250 (Proxy)\nTFLITE\n6.585 ms\n0 - 3 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nRB5 (Proxy)\nQualcomm¬Æ QCS8250 (Proxy)\nONNX\n8.352 ms\n8 - 12 MB\nCPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nSA7255P ADP\nQualcomm¬Æ SA7255P\nTFLITE\n1.078 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSA7255P ADP\nQualcomm¬Æ SA7255P\nQNN_DLC\n1.004 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nTFLITE\n0.332 ms\n0 - 11 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nQNN_DLC\n0.319 ms\n0 - 22 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSA8295P ADP\nQualcomm¬Æ SA8295P\nTFLITE\n0.957 ms\n0 - 25 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSA8295P ADP\nQualcomm¬Æ SA8295P\nQNN_DLC\n0.933 ms\n0 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nTFLITE\n0.33 ms\n0 - 11 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nQNN_DLC\n0.303 ms\n0 - 22 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSA8775P ADP\nQualcomm¬Æ SA8775P\nTFLITE\n0.523 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSA8775P ADP\nQualcomm¬Æ SA8775P\nQNN_DLC\n0.495 ms\n0 - 20 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nTFLITE\n0.236 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nQNN_DLC\n0.211 ms\n0 - 31 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nONNX\n0.353 ms\n0 - 36 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nTFLITE\n0.2 ms\n0 - 23 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nQNN_DLC\n0.181 ms\n0 - 25 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nONNX\n0.3 ms\n0 - 27 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nTFLITE\n0.419 ms\n0 - 23 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nQNN_DLC\n0.399 ms\n0 - 23 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nONNX\n9.485 ms\n11 - 28 MB\nCPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nTFLITE\n0.168 ms\n0 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseDetector\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nQNN_DLC\n0.161 ms\n0 - 28 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nONNX\n0.304 ms\n0 - 25 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseDetector\nw8a8\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nQNN_DLC\n0.424 ms\n14 - 14 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseDetector\nw8a8\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nONNX\n0.462 ms\n0 - 0 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nTFLITE\n0.885 ms\n0 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nQNN_DLC\n0.788 ms\n0 - 28 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nTFLITE\n0.419 ms\n0 - 37 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nQNN_DLC\n0.449 ms\n0 - 38 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nTFLITE\n0.331 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nQNN_DLC\n0.307 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nONNX\n0.602 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nTFLITE\n2.012 ms\n0 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nQNN_DLC\n0.523 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nTFLITE\n0.936 ms\n0 - 36 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nQNN_DLC\n1.077 ms\n0 - 36 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nONNX\n9.148 ms\n7 - 25 MB\nCPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nRB5 (Proxy)\nQualcomm¬Æ QCS8250 (Proxy)\nTFLITE\n7.194 ms\n0 - 3 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nRB5 (Proxy)\nQualcomm¬Æ QCS8250 (Proxy)\nONNX\n7.833 ms\n7 - 12 MB\nCPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nSA7255P ADP\nQualcomm¬Æ SA7255P\nTFLITE\n0.885 ms\n0 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSA7255P ADP\nQualcomm¬Æ SA7255P\nQNN_DLC\n0.788 ms\n0 - 28 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nTFLITE\n0.331 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nQNN_DLC\n0.291 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSA8295P ADP\nQualcomm¬Æ SA8295P\nTFLITE\n0.765 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSA8295P ADP\nQualcomm¬Æ SA8295P\nQNN_DLC\n0.699 ms\n0 - 34 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nTFLITE\n0.329 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nQNN_DLC\n0.301 ms\n0 - 31 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSA8775P ADP\nQualcomm¬Æ SA8775P\nTFLITE\n2.012 ms\n0 - 26 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSA8775P ADP\nQualcomm¬Æ SA8775P\nQNN_DLC\n0.523 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nTFLITE\n0.232 ms\n0 - 38 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nQNN_DLC\n0.212 ms\n0 - 38 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nONNX\n0.363 ms\n0 - 40 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nTFLITE\n0.2 ms\n0 - 30 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nQNN_DLC\n0.192 ms\n0 - 36 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nONNX\n0.326 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nTFLITE\n0.345 ms\n0 - 33 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nQNN_DLC\n0.309 ms\n0 - 35 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nONNX\n8.454 ms\n7 - 27 MB\nCPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nTFLITE\n0.178 ms\n0 - 29 MB\nNPU\nMediaPipe-Pose-Estimation.tflite\nPoseLandmarkDetector\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nQNN_DLC\n0.165 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nONNX\n0.33 ms\n0 - 32 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nPoseLandmarkDetector\nw8a8\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nQNN_DLC\n0.425 ms\n27 - 27 MB\nNPU\nMediaPipe-Pose-Estimation.dlc\nPoseLandmarkDetector\nw8a8\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nONNX\n0.474 ms\n3 - 3 MB\nNPU\nMediaPipe-Pose-Estimation.onnx.zip\nInstallation\nInstall the package via pip:\npip install qai-hub-models\nConfigure Qualcomm¬Æ AI Hub to run this model on a cloud-hosted device\nSign-in to Qualcomm¬Æ AI Hub with your\nQualcomm¬Æ ID. Once signed in navigate to Account -> Settings -> API Token.\nWith this API token, you can configure your client to run models on the cloud\nhosted devices.\nqai-hub configure --api_token API_TOKEN\nNavigate to docs for more information.\nDemo off target\nThe package contains a simple end-to-end demo that downloads pre-trained\nweights and runs this model on a sample input.\npython -m qai_hub_models.models.mediapipe_pose.demo\nThe above demo runs a reference implementation of pre-processing, model\ninference, and post processing.\nNOTE: If you want running in a Jupyter Notebook or Google Colab like\nenvironment, please add the following to your cell (instead of the above).\n%run -m qai_hub_models.models.mediapipe_pose.demo\nRun model on a cloud-hosted device\nIn addition to the demo, you can also run the model on a cloud-hosted Qualcomm¬Æ\ndevice. This script does the following:\nPerformance check on-device on a cloud-hosted device\nDownloads compiled assets that can be deployed on-device for Android.\nAccuracy check between PyTorch and on-device outputs.\npython -m qai_hub_models.models.mediapipe_pose.export\nHow does this work?\nThis export script\nleverages Qualcomm¬Æ AI Hub to optimize, validate, and deploy this model\non-device. Lets go through each step below in detail:\nStep 1: Compile model for on-device deployment\nTo compile a PyTorch model for on-device deployment, we first trace the model\nin memory using the jit.trace and then call the submit_compile_job API.\nimport torch\nimport qai_hub as hub\nfrom qai_hub_models.models.mediapipe_pose import Model\n# Load the model\ntorch_model = Model.from_pretrained()\n# Device\ndevice = hub.Device(\"Samsung Galaxy S25\")\n# Trace model\ninput_shape = torch_model.get_input_spec()\nsample_inputs = torch_model.sample_inputs()\npt_model = torch.jit.trace(torch_model, [torch.tensor(data[0]) for _, data in sample_inputs.items()])\n# Compile model on a specific device\ncompile_job = hub.submit_compile_job(\nmodel=pt_model,\ndevice=device,\ninput_specs=torch_model.get_input_spec(),\n)\n# Get target model to run on-device\ntarget_model = compile_job.get_target_model()\nStep 2: Performance profiling on cloud-hosted device\nAfter compiling models from step 1. Models can be profiled model on-device using the\ntarget_model. Note that this scripts runs the model on a device automatically\nprovisioned in the cloud.  Once the job is submitted, you can navigate to a\nprovided job URL to view a variety of on-device performance metrics.\nprofile_job = hub.submit_profile_job(\nmodel=target_model,\ndevice=device,\n)\nStep 3: Verify on-device accuracy\nTo verify the accuracy of the model on-device, you can run on-device inference\non sample input data on the same cloud hosted device.\ninput_data = torch_model.sample_inputs()\ninference_job = hub.submit_inference_job(\nmodel=target_model,\ndevice=device,\ninputs=input_data,\n)\non_device_output = inference_job.download_output_data()\nWith the output of the model, you can compute like PSNR, relative errors or\nspot check the output with expected output.\nNote: This on-device profiling and inference requires access to Qualcomm¬Æ\nAI Hub. Sign up for access.\nDeploying compiled model to Android\nThe models can be deployed using multiple runtimes:\nTensorFlow Lite (.tflite export): This\ntutorial provides a\nguide to deploy the .tflite model in an Android application.\nQNN (.so export ): This sample\napp\nprovides instructions on how to use the .so shared library  in an Android application.\nView on Qualcomm¬Æ AI Hub\nGet more details on MediaPipe-Pose-Estimation's performance across various devices here.\nExplore all available models on Qualcomm¬Æ AI Hub\nLicense\nThe license for the original implementation of MediaPipe-Pose-Estimation can be found\nhere.\nThe license for the compiled assets for on-device deployment can be found here\nReferences\nBlazePose: On-device Real-time Body Pose tracking\nSource Model Implementation\nCommunity\nJoin our AI Hub Slack community to collaborate, post questions and learn more about on-device AI.\nFor questions or feedback please reach out to us.",
    "qualcomm/Real-ESRGAN-x4plus": "Real-ESRGAN-x4plus: Optimized for Mobile Deployment\nUpscale images and remove image noise\nModel Details\nInstallation\nConfigure Qualcomm¬Æ AI Hub to run this model on a cloud-hosted device\nDemo off target\nRun model on a cloud-hosted device\nHow does this work?\nRun demo on a cloud-hosted device\nDeploying compiled model to Android\nView on Qualcomm¬Æ AI Hub\nLicense\nReferences\nCommunity\nReal-ESRGAN-x4plus: Optimized for Mobile Deployment\nUpscale images and remove image noise\nReal-ESRGAN is a machine learning model that upscales an image with minimal loss in quality. The implementation is a derivative of the Real-ESRGAN-x4plus architecture, a larger and more powerful version compared to the Real-ESRGAN-general-x4v3 architecture.\nThis model is an implementation of Real-ESRGAN-x4plus found here.\nThis repository provides scripts to run Real-ESRGAN-x4plus on Qualcomm¬Æ devices.\nMore details on model performance across various devices, can be found\nhere.\nModel Details\nModel Type: Model_use_case.super_resolution\nModel Stats:\nModel checkpoint: RealESRGAN_x4plus\nInput resolution: 128x128\nNumber of parameters: 16.7M\nModel size (float): 63.9 MB\nModel size (w8a8): 16.7 MB\nModel\nPrecision\nDevice\nChipset\nTarget Runtime\nInference Time (ms)\nPeak Memory Range (MB)\nPrimary Compute Unit\nTarget Model\nReal-ESRGAN-x4plus\nfloat\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nTFLITE\n454.901 ms\n0 - 190 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nQNN_DLC\n448.882 ms\n0 - 143 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nTFLITE\n143.893 ms\n3 - 160 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nQNN_DLC\n114.996 ms\n0 - 170 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nTFLITE\n72.958 ms\n3 - 48 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nQNN_DLC\n70.37 ms\n0 - 44 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nONNX\n69.886 ms\n6 - 51 MB\nNPU\nReal-ESRGAN-x4plus.onnx.zip\nReal-ESRGAN-x4plus\nfloat\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nTFLITE\n109.238 ms\n3 - 194 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nQNN_DLC\n105.288 ms\n0 - 147 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSA7255P ADP\nQualcomm¬Æ SA7255P\nTFLITE\n454.901 ms\n0 - 190 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSA7255P ADP\nQualcomm¬Æ SA7255P\nQNN_DLC\n448.882 ms\n0 - 143 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nTFLITE\n71.881 ms\n3 - 49 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nQNN_DLC\n65.396 ms\n0 - 45 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSA8295P ADP\nQualcomm¬Æ SA8295P\nTFLITE\n113.967 ms\n3 - 158 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSA8295P ADP\nQualcomm¬Æ SA8295P\nQNN_DLC\n110.382 ms\n0 - 158 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nTFLITE\n70.405 ms\n3 - 49 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nQNN_DLC\n63.697 ms\n0 - 35 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSA8775P ADP\nQualcomm¬Æ SA8775P\nTFLITE\n109.238 ms\n3 - 194 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSA8775P ADP\nQualcomm¬Æ SA8775P\nQNN_DLC\n105.288 ms\n0 - 147 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nTFLITE\n52.043 ms\n0 - 198 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nQNN_DLC\n50.358 ms\n0 - 154 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nONNX\n51.204 ms\n8 - 191 MB\nNPU\nReal-ESRGAN-x4plus.onnx.zip\nReal-ESRGAN-x4plus\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nTFLITE\n41.181 ms\n3 - 198 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nQNN_DLC\n38.981 ms\n0 - 139 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nONNX\n39.939 ms\n8 - 154 MB\nNPU\nReal-ESRGAN-x4plus.onnx.zip\nReal-ESRGAN-x4plus\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nTFLITE\n33.773 ms\n3 - 199 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nQNN_DLC\n27.546 ms\n0 - 142 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nONNX\n29.888 ms\n6 - 154 MB\nNPU\nReal-ESRGAN-x4plus.onnx.zip\nReal-ESRGAN-x4plus\nfloat\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nQNN_DLC\n64.985 ms\n121 - 121 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nfloat\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nONNX\n65.641 ms\n37 - 37 MB\nNPU\nReal-ESRGAN-x4plus.onnx.zip\nReal-ESRGAN-x4plus\nw8a8\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nTFLITE\n73.781 ms\n1 - 176 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nQCS8275 (Proxy)\nQualcomm¬Æ QCS8275 (Proxy)\nQNN_DLC\n66.556 ms\n0 - 196 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nTFLITE\n36.692 ms\n1 - 176 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nQCS8450 (Proxy)\nQualcomm¬Æ QCS8450 (Proxy)\nQNN_DLC\n40.874 ms\n0 - 202 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nTFLITE\n24.994 ms\n0 - 34 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nQCS8550 (Proxy)\nQualcomm¬Æ QCS8550 (Proxy)\nQNN_DLC\n23.357 ms\n0 - 48 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nTFLITE\n23.57 ms\n1 - 175 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nQCS9075 (Proxy)\nQualcomm¬Æ QCS9075 (Proxy)\nQNN_DLC\n21.273 ms\n0 - 194 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nRB3 Gen 2 (Proxy)\nQualcomm¬Æ QCS6490 (Proxy)\nTFLITE\n115.528 ms\n1 - 171 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nRB5 (Proxy)\nQualcomm¬Æ QCS8250 (Proxy)\nTFLITE\n1373.118 ms\n0 - 96 MB\nGPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSA7255P ADP\nQualcomm¬Æ SA7255P\nTFLITE\n73.781 ms\n1 - 176 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSA7255P ADP\nQualcomm¬Æ SA7255P\nQNN_DLC\n66.556 ms\n0 - 196 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nTFLITE\n23.093 ms\n0 - 39 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSA8255 (Proxy)\nQualcomm¬Æ SA8255P (Proxy)\nQNN_DLC\n23.347 ms\n0 - 58 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSA8295P ADP\nQualcomm¬Æ SA8295P\nTFLITE\n39.446 ms\n1 - 172 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSA8295P ADP\nQualcomm¬Æ SA8295P\nQNN_DLC\n34.932 ms\n0 - 199 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nTFLITE\n24.767 ms\n0 - 47 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSA8650 (Proxy)\nQualcomm¬Æ SA8650P (Proxy)\nQNN_DLC\n23.349 ms\n0 - 52 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSA8775P ADP\nQualcomm¬Æ SA8775P\nTFLITE\n23.57 ms\n1 - 175 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSA8775P ADP\nQualcomm¬Æ SA8775P\nQNN_DLC\n21.273 ms\n0 - 194 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nTFLITE\n17.589 ms\n1 - 179 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSamsung Galaxy S24\nSnapdragon¬Æ 8 Gen 3 Mobile\nQNN_DLC\n16.054 ms\n0 - 192 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nTFLITE\n15.027 ms\n1 - 175 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSamsung Galaxy S25\nSnapdragon¬Æ 8 Elite For Galaxy Mobile\nQNN_DLC\n11.972 ms\n0 - 177 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nTFLITE\n49.941 ms\n1 - 189 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSnapdragon 7 Gen 5 QRD\nSnapdragon¬Æ 7 Gen 5 Mobile\nQNN_DLC\n41.1 ms\n0 - 208 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nTFLITE\n11.63 ms\n1 - 176 MB\nNPU\nReal-ESRGAN-x4plus.tflite\nReal-ESRGAN-x4plus\nw8a8\nSnapdragon 8 Elite Gen 5 QRD\nSnapdragon¬Æ 8 Elite Gen5 Mobile\nQNN_DLC\n9.404 ms\n0 - 184 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nReal-ESRGAN-x4plus\nw8a8\nSnapdragon X Elite CRD\nSnapdragon¬Æ X Elite\nQNN_DLC\n24.398 ms\n108 - 108 MB\nNPU\nReal-ESRGAN-x4plus.dlc\nInstallation\nInstall the package via pip:\npip install \"qai-hub-models[real-esrgan-x4plus]\"\nConfigure Qualcomm¬Æ AI Hub to run this model on a cloud-hosted device\nSign-in to Qualcomm¬Æ AI Hub with your\nQualcomm¬Æ ID. Once signed in navigate to Account -> Settings -> API Token.\nWith this API token, you can configure your client to run models on the cloud\nhosted devices.\nqai-hub configure --api_token API_TOKEN\nNavigate to docs for more information.\nDemo off target\nThe package contains a simple end-to-end demo that downloads pre-trained\nweights and runs this model on a sample input.\npython -m qai_hub_models.models.real_esrgan_x4plus.demo\nThe above demo runs a reference implementation of pre-processing, model\ninference, and post processing.\nNOTE: If you want running in a Jupyter Notebook or Google Colab like\nenvironment, please add the following to your cell (instead of the above).\n%run -m qai_hub_models.models.real_esrgan_x4plus.demo\nRun model on a cloud-hosted device\nIn addition to the demo, you can also run the model on a cloud-hosted Qualcomm¬Æ\ndevice. This script does the following:\nPerformance check on-device on a cloud-hosted device\nDownloads compiled assets that can be deployed on-device for Android.\nAccuracy check between PyTorch and on-device outputs.\npython -m qai_hub_models.models.real_esrgan_x4plus.export\nHow does this work?\nThis export script\nleverages Qualcomm¬Æ AI Hub to optimize, validate, and deploy this model\non-device. Lets go through each step below in detail:\nStep 1: Compile model for on-device deployment\nTo compile a PyTorch model for on-device deployment, we first trace the model\nin memory using the jit.trace and then call the submit_compile_job API.\nimport torch\nimport qai_hub as hub\nfrom qai_hub_models.models.real_esrgan_x4plus import Model\n# Load the model\ntorch_model = Model.from_pretrained()\n# Device\ndevice = hub.Device(\"Samsung Galaxy S25\")\n# Trace model\ninput_shape = torch_model.get_input_spec()\nsample_inputs = torch_model.sample_inputs()\npt_model = torch.jit.trace(torch_model, [torch.tensor(data[0]) for _, data in sample_inputs.items()])\n# Compile model on a specific device\ncompile_job = hub.submit_compile_job(\nmodel=pt_model,\ndevice=device,\ninput_specs=torch_model.get_input_spec(),\n)\n# Get target model to run on-device\ntarget_model = compile_job.get_target_model()\nStep 2: Performance profiling on cloud-hosted device\nAfter compiling models from step 1. Models can be profiled model on-device using the\ntarget_model. Note that this scripts runs the model on a device automatically\nprovisioned in the cloud.  Once the job is submitted, you can navigate to a\nprovided job URL to view a variety of on-device performance metrics.\nprofile_job = hub.submit_profile_job(\nmodel=target_model,\ndevice=device,\n)\nStep 3: Verify on-device accuracy\nTo verify the accuracy of the model on-device, you can run on-device inference\non sample input data on the same cloud hosted device.\ninput_data = torch_model.sample_inputs()\ninference_job = hub.submit_inference_job(\nmodel=target_model,\ndevice=device,\ninputs=input_data,\n)\non_device_output = inference_job.download_output_data()\nWith the output of the model, you can compute like PSNR, relative errors or\nspot check the output with expected output.\nNote: This on-device profiling and inference requires access to Qualcomm¬Æ\nAI Hub. Sign up for access.\nRun demo on a cloud-hosted device\nYou can also run the demo on-device.\npython -m qai_hub_models.models.real_esrgan_x4plus.demo --eval-mode on-device\nNOTE: If you want running in a Jupyter Notebook or Google Colab like\nenvironment, please add the following to your cell (instead of the above).\n%run -m qai_hub_models.models.real_esrgan_x4plus.demo -- --eval-mode on-device\nDeploying compiled model to Android\nThe models can be deployed using multiple runtimes:\nTensorFlow Lite (.tflite export): This\ntutorial provides a\nguide to deploy the .tflite model in an Android application.\nQNN (.so export ): This sample\napp\nprovides instructions on how to use the .so shared library  in an Android application.\nView on Qualcomm¬Æ AI Hub\nGet more details on Real-ESRGAN-x4plus's performance across various devices here.\nExplore all available models on Qualcomm¬Æ AI Hub\nLicense\nThe license for the original implementation of Real-ESRGAN-x4plus can be found\nhere.\nThe license for the compiled assets for on-device deployment can be found here\nReferences\nReal-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\nSource Model Implementation\nCommunity\nJoin our AI Hub Slack community to collaborate, post questions and learn more about on-device AI.\nFor questions or feedback please reach out to us.",
    "argmaxinc/whisperkit-coreml": "WhisperKit\nWhisperKit\nWhisperKit is an on-device speech recognition framework for Apple Silicon:\nhttps://github.com/argmaxinc/WhisperKit\nFor performance and accuracy benchmarks on real devices, please see:\nhttps://huggingface.co/spaces/argmaxinc/whisperkit-benchmarks\nWhisperKit Pro is the commercial upgraded version of WhisperKit. If you would like to learn more about upgrading to Pro, please reach out to whisperkitpro@argmaxinc.com or fill the form here.",
    "MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF": "MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF\nDescription\nHow to use\nAbout GGUF\nExplanation of quantisation methods\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using llama-cpp-python\nHow to use with LangChain\nMaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF\nModel creator: MaziyarPanahi\nOriginal model: MaziyarPanahi/Mistral-7B-Instruct-Aya-101\nDescription\nMaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF contains GGUF format model files for MaziyarPanahi/Mistral-7B-Instruct-Aya-101.\nHow to use\nThanks to TheBloke for preparing an amazing README on how to use GGUF models:\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nGPT4All, a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF and below it, a specific filename to download, such as: Mistral-7B-Instruct-Aya-101-GGUF.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF Mistral-7B-Instruct-Aya-101-GGUF.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage (click to read)\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download [MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF) --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download MaziyarPanahi/Mistral-7B-Instruct-Aya-101-GGUF Mistral-7B-Instruct-Aya-101-GGUF.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 35 -m Mistral-7B-Instruct-Aya-101-GGUF.Q4_K_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 32768 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions can be found in the text-generation-webui documentation, here: text-generation-webui/docs/04 ‚Äê Model Tab.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\nHow to load this model in Python code, using llama-cpp-python\nFor full documentation, please see: llama-cpp-python docs.\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\nSimple llama-cpp-python example code\nfrom llama_cpp import Llama\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\nmodel_path=\"./Mistral-7B-Instruct-Aya-101-GGUF.Q4_K_M.gguf\",  # Download the model file first\nn_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\nn_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\nn_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n# Simple inference example\noutput = llm(\n\"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\", # Prompt\nmax_tokens=512,  # Generate up to 512 tokens\nstop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\necho=True        # Whether to echo the prompt\n)\n# Chat Completion API\nllm = Llama(model_path=\"./Mistral-7B-Instruct-Aya-101-GGUF.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n{\n\"role\": \"user\",\n\"content\": \"Write a story about llamas.\"\n}\n]\n)\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers",
    "h1t/TCD-SDXL-LoRA": "Trajectory Consistency Distillation\nIntroduction\nUsage\nText-to-Image generation\nInpainting\nVersatile for Community Models\nCombine with styled LoRA\nCompatibility with ControlNet\nCompatibility with IP-Adapter\nRelated and Concurrent Works\nCitation\nAcknowledgments\nTrajectory Consistency Distillation\nOfficial Model Repo of the paper: Trajectory Consistency Distillation.\nFor more information, please check the GitHub Repo and Project Page.\nAlso welcome to try the demo host on ü§ó Space.\nIntroduction\nTCD, inspired by Consistency Models, is a novel distillation technology that enables the distillation of knowledge from pre-trained diffusion models into a few-step sampler. In this repository, we release the inference code and our model named TCD-SDXL, which is distilled from SDXL Base 1.0. We provide the LoRA checkpoint in this repository.\n‚ú®TCD has following advantages:\nFlexible NFEs: For TCD, the NFEs can be varied at will (compared with Turbo), without adversely affecting the quality of the results (compared with LCMs), where LCM experiences a notable decline in quality at high NFEs.\nBetter than Teacher: TCD maintains superior generative quality at high NFEs, even exceeding the performance of DPM-Solver++(2S) with origin SDXL. It is worth noting that there is no additional discriminator or LPIPS supervision included during training.\nFreely Change the Detailing: During inference, the level of detail in the image can be simply modified by adjusing one hyper-parameter gamma. This option does not require the introduction of any additional parameters.\nVersatility: Integrated with LoRA technology, TCD can be directly applied to various models (including the custom Community Models, styled LoRA, ControlNet, IP-Adapter) that share the same backbone, as demonstrated in the Usage.\nAvoiding Mode Collapse: TCD achieves few-step generation without the need for adversarial training, thus circumventing mode collapse caused by the GAN objective.\nIn contrast to the concurrent work SDXL-Lightning, which relies on Adversarial Diffusion Distillation, TCD can synthesize results that are more realistic and slightly more diverse, without the presence of \"Janus\" artifacts.\nFor more information, please refer to our paper Trajectory Consistency Distillation.\nUsage\nTo run the model yourself, you can leverage the üß® Diffusers library.\npip install diffusers transformers accelerate peft\nAnd then we clone the repo.\ngit clone https://github.com/jabir-zheng/TCD.git\ncd TCD\nHere, we demonstrate the applicability of our TCD LoRA to various models, including SDXL, SDXL Inpainting, a community model named Animagine XL, a styled LoRA Papercut, pretrained Depth Controlnet, Canny Controlnet and IP-Adapter to accelerate image generation with high quality in few steps.\nText-to-Image generation\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\npipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\nprompt = \"Beautiful woman, bubblegum pink, lemon yellow, minty blue, futuristic, high-detail, epic composition, watercolor.\"\nimage = pipe(\nprompt=prompt,\nnum_inference_steps=4,\nguidance_scale=0,\n# Eta (referred to as `gamma` in the paper) is used to control the stochasticity in every step.\n# A value of 0.3 often yields good results.\n# We recommend using a higher eta when increasing the number of inference steps.\neta=0.3,\ngenerator=torch.Generator(device=device).manual_seed(0),\n).images[0]\nInpainting\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\nbase_model_id = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\npipe = AutoPipelineForInpainting.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\ninit_image = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\nprompt = \"a tiger sitting on a park bench\"\nimage = pipe(\nprompt=prompt,\nimage=init_image,\nmask_image=mask_image,\nnum_inference_steps=8,\nguidance_scale=0,\neta=0.3, # Eta (referred to as `gamma` in the paper) is used to control the stochasticity in every step. A value of 0.3 often yields good results.\nstrength=0.99,  # make sure to use `strength` below 1.0\ngenerator=torch.Generator(device=device).manual_seed(0),\n).images[0]\ngrid_image = make_image_grid([init_image, mask_image, image], rows=1, cols=3)\nVersatile for Community Models\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\nbase_model_id = \"cagliostrolab/animagine-xl-3.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\npipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\nprompt = \"A man, clad in a meticulously tailored military uniform, stands with unwavering resolve. The uniform boasts intricate details, and his eyes gleam with determination. Strands of vibrant, windswept hair peek out from beneath the brim of his cap.\"\nimage = pipe(\nprompt=prompt,\nnum_inference_steps=8,\nguidance_scale=0,\n# Eta (referred to as `gamma` in the paper) is used to control the stochasticity in every step.\n# A value of 0.3 often yields good results.\n# We recommend using a higher eta when increasing the number of inference steps.\neta=0.3,\ngenerator=torch.Generator(device=device).manual_seed(0),\n).images[0]\nCombine with styled LoRA\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\nstyled_lora_id = \"TheLastBen/Papercut_SDXL\"\npipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id, adapter_name=\"tcd\")\npipe.load_lora_weights(styled_lora_id, adapter_name=\"style\")\npipe.set_adapters([\"tcd\", \"style\"], adapter_weights=[1.0, 1.0])\nprompt = \"papercut of a winter mountain, snow\"\nimage = pipe(\nprompt=prompt,\nnum_inference_steps=4,\nguidance_scale=0,\n# Eta (referred to as `gamma` in the paper) is used to control the stochasticity in every step.\n# A value of 0.3 often yields good results.\n# We recommend using a higher eta when increasing the number of inference steps.\neta=0.3,\ngenerator=torch.Generator(device=device).manual_seed(0),\n).images[0]\nCompatibility with ControlNet\nDepth ControlNet\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\nfrom diffusers.utils import load_image, make_image_grid\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\ndepth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(device)\nfeature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\ndef get_depth_map(image):\nimage = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\nwith torch.no_grad(), torch.autocast(device):\ndepth_map = depth_estimator(image).predicted_depth\ndepth_map = torch.nn.functional.interpolate(\ndepth_map.unsqueeze(1),\nsize=(1024, 1024),\nmode=\"bicubic\",\nalign_corners=False,\n)\ndepth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\ndepth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\ndepth_map = (depth_map - depth_min) / (depth_max - depth_min)\nimage = torch.cat([depth_map] * 3, dim=1)\nimage = image.permute(0, 2, 3, 1).cpu().numpy()[0]\nimage = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\nreturn image\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ncontrolnet_id = \"diffusers/controlnet-depth-sdxl-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\ncontrolnet = ControlNetModel.from_pretrained(\ncontrolnet_id,\ntorch_dtype=torch.float16,\nvariant=\"fp16\",\n).to(device)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\nbase_model_id,\ncontrolnet=controlnet,\ntorch_dtype=torch.float16,\nvariant=\"fp16\",\n).to(device)\npipe.enable_model_cpu_offload()\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\nprompt = \"stormtrooper lecture, photorealistic\"\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\ndepth_image = get_depth_map(image)\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\nimage = pipe(\nprompt,\nimage=depth_image,\nnum_inference_steps=4,\nguidance_scale=0,\neta=0.3, # A parameter (referred to as `gamma` in the paper) is used to control the stochasticity in every step. A value of 0.3 often yields good results.\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\ngenerator=torch.Generator(device=device).manual_seed(0),\n).images[0]\ngrid_image = make_image_grid([depth_image, image], rows=1, cols=2)\nCanny ControlNet\nimport torch\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\nfrom diffusers.utils import load_image, make_image_grid\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\ncontrolnet_id = \"diffusers/controlnet-canny-sdxl-1.0\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\ncontrolnet = ControlNetModel.from_pretrained(\ncontrolnet_id,\ntorch_dtype=torch.float16,\nvariant=\"fp16\",\n).to(device)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\nbase_model_id,\ncontrolnet=controlnet,\ntorch_dtype=torch.float16,\nvariant=\"fp16\",\n).to(device)\npipe.enable_model_cpu_offload()\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\nprompt = \"ultrarealistic shot of a furry blue bird\"\ncanny_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png\")\ncontrolnet_conditioning_scale = 0.5  # recommended for good generalization\nimage = pipe(\nprompt,\nimage=canny_image,\nnum_inference_steps=4,\nguidance_scale=0,\neta=0.3, # A parameter (referred to as `gamma` in the paper) is used to control the stochasticity in every step. A value of 0.3 often yields good results.\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\ngenerator=torch.Generator(device=device).manual_seed(0),\n).images[0]\ngrid_image = make_image_grid([canny_image, image], rows=1, cols=2)\nCompatibility with IP-Adapter\n‚ö†Ô∏è Please refer to the official repository for instructions on installing dependencies for IP-Adapter.\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers.utils import load_image, make_image_grid\nfrom ip_adapter import IPAdapterXL\nfrom scheduling_tcd import TCDScheduler\ndevice = \"cuda\"\nbase_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\nimage_encoder_path = \"sdxl_models/image_encoder\"\nip_ckpt = \"sdxl_models/ip-adapter_sdxl.bin\"\ntcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\npipe = StableDiffusionXLPipeline.from_pretrained(\nbase_model_path,\ntorch_dtype=torch.float16,\nvariant=\"fp16\"\n)\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\npipe.load_lora_weights(tcd_lora_id)\npipe.fuse_lora()\nip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\nref_image = load_image(\"https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/images/woman.png\").resize((512, 512))\nprompt = \"best quality, high quality, wearing sunglasses\"\nimage = ip_model.generate(\npil_image=ref_image,\nprompt=prompt,\nscale=0.5,\nnum_samples=1,\nnum_inference_steps=4,\nguidance_scale=0,\neta=0.3, # A parameter (referred to as `gamma` in the paper) is used to control the stochasticity in every step. A value of 0.3 often yields good results.\nseed=0,\n)[0]\ngrid_image = make_image_grid([ref_image, image], rows=1, cols=2)\nRelated and Concurrent Works\nLuo S, Tan Y, Huang L, et al. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.\nLuo S, Tan Y, Patil S, et al. LCM-LoRA: A universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023.\nLu C, Zhou Y, Bao F, et al. DPM-Solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 2022, 35: 5775-5787.\nLu C, Zhou Y, Bao F, et al. DPM-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.\nZhang Q, Chen Y. Fast sampling of diffusion models with exponential integrator. ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nKim D, Lai C H, Liao W H, et al. Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. ICLR 2024.\nCitation\n@misc{zheng2024trajectory,\ntitle={Trajectory Consistency Distillation},\nauthor={Jianbin Zheng and Minghui Hu and Zhongyi Fan and Chaoyue Wang and Changxing Ding and Dacheng Tao and Tat-Jen Cham},\nyear={2024},\neprint={2402.19159},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nAcknowledgments\nThis codebase heavily relies on the ü§óDiffusers library and LCM.",
    "stabilityai/TripoSR": "TripoSR\nModel Details\nModel Description\nModel Sources\nTraining Dataset\nUsage\nMisuse, Malicious Use, and Out-of-Scope Use\nTry our new model: SF3D with several improvements such as faster generation and more game-ready assets.\nThe model is available here and we also have a demo.\nTripoSR\nTripoSR is a fast and feed-forward 3D generative model developed in collaboration between Stability AI and Tripo AI.\nModel Details\nModel Description\nWe closely follow LRM network architecture for the model design, where TripoSR incorporates a series of technical advancements over the LRM model in terms of both data curation as well as model and training improvements. For more technical details and evaluations, please refer to our tech report.\nDeveloped by: Stability AI, Tripo AI\nModel type: Feed-forward 3D reconstruction from a single image\nLicense: MIT\nHardware: We train TripoSR for 5 days on 22 GPU nodes each with 8 A100 40GB GPUs\nModel Sources\nRepository: https://github.com/VAST-AI-Research/TripoSR\nTech report: https://arxiv.org/abs/2403.02151\nDemo: https://huggingface.co/spaces/stabilityai/TripoSR\nTraining Dataset\nWe use renders from the Objaverse dataset, utilizing our enhanced rendering method that more closely replicate the distribution of images found in the real world, significantly improving our model‚Äôs ability to generalize. We selected a carefully curated subset of the Objaverse dataset for the training data, which is available under the CC-BY license.\nUsage\nFor usage instructions, please refer to our TripoSR GitHub repository\nYou can also try it in our gradio demo\nMisuse, Malicious Use, and Out-of-Scope Use\nThe model should not be used to intentionally create or disseminate 3D models that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.",
    "myshell-ai/MeloTTS-English": "MeloTTS\nAuthors\nUsage\nWithout Installation\nInstall and Use Locally\nJoin the Community\nLicense\nAcknowledgements\nMeloTTS\nMeloTTS is a high-quality multi-lingual text-to-speech library by MIT and MyShell.ai. Supported languages include:\nModel card\nExample\nEnglish (American)\nLink\nEnglish (British)\nLink\nEnglish (Indian)\nLink\nEnglish (Australian)\nLink\nEnglish (Default)\nLink\nSpanish\nLink\nFrench\nLink\nChinese (mix EN)\nLink\nJapanese\nLink\nKorean\nLink\nSome other features include:\nThe Chinese speaker supports mixed Chinese and English.\nFast enough for CPU real-time inference.\nAuthors\nWenliang Zhao at Tsinghua University\nXumin Yu at Tsinghua University\nZengyi Qin (project lead) at MIT and MyShell\nCitation\n@software{zhao2024melo,\nauthor={Zhao, Wenliang and Yu, Xumin and Qin, Zengyi},\ntitle = {MeloTTS: High-quality Multi-lingual Multi-accent Text-to-Speech},\nurl = {https://github.com/myshell-ai/MeloTTS},\nyear = {2023}\n}\nUsage\nWithout Installation\nAn unofficial live demo is hosted on Hugging Face Spaces.\nUse it on MyShell\nThere are hundreds of TTS models on MyShell, much more than MeloTTS. See examples here.\nMore can be found at the widget center of MyShell.ai.\nInstall and Use Locally\nFollow the installation steps here before using the following snippet:\nfrom melo.api import TTS\n# Speed is adjustable\nspeed = 1.0\n# CPU is sufficient for real-time inference.\n# You can set it manually to 'cpu' or 'cuda' or 'cuda:0' or 'mps'\ndevice = 'auto' # Will automatically use GPU if available\n# English\ntext = \"Did you ever hear a folk tale about a giant turtle?\"\nmodel = TTS(language='EN', device=device)\nspeaker_ids = model.hps.data.spk2id\n# American accent\noutput_path = 'en-us.wav'\nmodel.tts_to_file(text, speaker_ids['EN-US'], output_path, speed=speed)\n# British accent\noutput_path = 'en-br.wav'\nmodel.tts_to_file(text, speaker_ids['EN-BR'], output_path, speed=speed)\n# Indian accent\noutput_path = 'en-india.wav'\nmodel.tts_to_file(text, speaker_ids['EN_INDIA'], output_path, speed=speed)\n# Australian accent\noutput_path = 'en-au.wav'\nmodel.tts_to_file(text, speaker_ids['EN-AU'], output_path, speed=speed)\n# Default accent\noutput_path = 'en-default.wav'\nmodel.tts_to_file(text, speaker_ids['EN-Default'], output_path, speed=speed)\nJoin the Community\nOpen Source AI Grant\nWe are actively sponsoring open-source AI projects. The sponsorship includes GPU resources, fundings and intellectual support (collaboration with top research labs). We welcome both reseach and engineering projects, as long as the open-source community needs them. Please contact Zengyi Qin if you are interested.\nContributing\nIf you find this work useful, please consider contributing to the GitHub repo.\nMany thanks to @fakerybakery for adding the Web UI and CLI part.\nLicense\nThis library is under MIT License, which means it is free for both commercial and non-commercial use.\nAcknowledgements\nThis implementation is based on TTS, VITS, VITS2 and Bert-VITS2. We appreciate their awesome work.",
    "myshell-ai/MeloTTS-Korean": "MeloTTS\nUsage\nWithout Installation\nInstall and Use Locally\nJoin the Community\nLicense\nAcknowledgements\nMeloTTS\nMeloTTS is a high-quality multi-lingual text-to-speech library by MyShell.ai. Supported languages include:\nModel card\nExample\nEnglish (American)\nLink\nEnglish (British)\nLink\nEnglish (Indian)\nLink\nEnglish (Australian)\nLink\nEnglish (Default)\nLink\nSpanish\nLink\nFrench\nLink\nChinese (mix EN)\nLink\nJapanese\nLink\nKorean\nLink\nSome other features include:\nThe Chinese speaker supports mixed Chinese and English.\nFast enough for CPU real-time inference.\nUsage\nWithout Installation\nAn unofficial live demo is hosted on Hugging Face Spaces.\nUse it on MyShell\nThere are hundreds of TTS models on MyShell, much more than MeloTTS. See examples here.\nMore can be found at the widget center of MyShell.ai.\nInstall and Use Locally\nFollow the installation steps here before using the following snippet:\nfrom melo.api import TTS\n# Speed is adjustable\nspeed = 1.0\ndevice = 'cpu' # or cuda:0\ntext = \"ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§ÎäòÏùÄ ÎÇ†Ïî®Í∞Ä Ï†ïÎßê Ï¢ãÎÑ§Ïöî.\"\nmodel = TTS(language='KR', device=device)\nspeaker_ids = model.hps.data.spk2id\noutput_path = 'kr.wav'\nmodel.tts_to_file(text, speaker_ids['KR'], output_path, speed=speed)\nJoin the Community\nOpen Source AI Grant\nWe are actively sponsoring open-source AI projects. The sponsorship includes GPU resources, fundings and intellectual support (collaboration with top research labs). We welcome both reseach and engineering projects, as long as the open-source community needs them. Please contact Zengyi Qin if you are interested.\nContributing\nIf you find this work useful, please consider contributing to the GitHub repo.\nMany thanks to @fakerybakery for adding the Web UI and CLI part.\nLicense\nThis library is under MIT License, which means it is free for both commercial and non-commercial use.\nAcknowledgements\nThis implementation is based on TTS, VITS, VITS2 and Bert-VITS2. We appreciate their awesome work.",
    "namanmehta2211/Architecture_model_generation": "No model card",
    "Kvikontent/midjourney-v6": "Midjourney V6\nExamples\nUsage\nMidjourney V6\nMidjourney is most realistic and powerful ai image generator in the world. Here is is Stable Diffusion LoRA model trained on 100k+ midjourney V6 images.\nExamples\nPrompt\ned sheeran made of thnderstorm clouds, lights, thunder, rain, particles\nPrompt\nsilhouette of a dog in the thunderstorm clouds, thumder, photo, bad weather,\nPrompt\nA professional photo of a beautiful night waterfall in the jungle with a touch of blue and a few fireflies flying around\nUsage\nYou can use this model using huggingface Interface API:\nimport requests\nAPI_URL = \"https://api-inference.huggingface.co/models/Kvikontent/midjourney-v6\"\nheaders = {\"Authorization\": \"Bearer HUGGINGFACE_API_TOKEN\"}\ndef query(payload):\nresponse = requests.post(API_URL, headers=headers, json=payload)\nreturn response.content\nimage_bytes = query({\n\"inputs\": \"Astronaut riding a horse\",\n})\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes))",
    "zym1/civitai": "No model card"
}