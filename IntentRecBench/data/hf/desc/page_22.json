{
    "Qwen/Qwen3-235B-A22B": "Qwen3-235B-A22B\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nProcessing Long Texts\nBest Practices\nCitation\nQwen3-235B-A22B\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-235B-A22B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 235B in total and 22B activated\nNumber of Paramaters (Non-Embedding): 234B\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 32,768 natively and 131,072 tokens with YaRN.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-235B-A22B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B --reasoning-parser qwen3 --tp 8\nvLLM:vllm serve Qwen/Qwen3-235B-A22B --enable-reasoning --reasoning-parser deepseek_r1\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by SGLang and vLLM.\nPlease refer to our documentation for SGLang and vLLM users.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-235B-A22B\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-235B-A22B',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Long Texts\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers and llama.cpp for local use, vllm and sglang for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768\n}\n}\nFor llama.cpp, you need to regenerate the GGUF file after the modification.\nPassing command line arguments:\nFor vllm, you can use\nvllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\nFor sglang, you can use\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\nFor llama-server from llama.cpp, you can use\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\nIf you encounter the following warning\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\nplease upgrade transformers>=4.51.0.\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0.\nThe default max_position_embeddings in config.json is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\nThe endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "IlyaGusev/saiga_gemma3_12b_gguf": "Llama.cpp compatible versions of an original 12B model.\nDownload one of the versions, for example saiga_gemma3_12b.Q4_K_M.gguf.\nwget https://huggingface.co/IlyaGusev/saiga_gemma3_12b_gguf/resolve/main/saiga_gemma3_12b.Q4_K_M.gguf\nDownload interact_gguf.py\nhttps://raw.githubusercontent.com/IlyaGusev/saiga/refs/heads/main/scripts/interact_gguf.py\nHow to run:\npip install llama-cpp-python fire\npython3 interact_gguf.py saiga_gemma3_12b.Q4_K_M.gguf\nSystem requirements:\n13GB RAM for q8_0 and less for smaller quantizations",
    "Qwen/Qwen3-4B-Base": "Qwen3-4B-Base\nQwen3 Highlights\nModel Overview\nRequirements\nEvaluation & Performance\nCitation\nQwen3-4B-Base\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.\nBuilding upon extensive advancements in training data, model architecture, and optimization techniques, Qwen3 delivers the following key improvements over the previously released Qwen2.5:\nExpanded Higher-Quality Pre-training Corpus: Qwen3 is pre-trained on 36 trillion tokens across 119 languages ‚Äî tripling the language coverage of Qwen2.5 ‚Äî with a much richer mix of high-quality data, including coding, STEM, reasoning, book, multilingual, and synthetic data.\nTraining Techniques and Model Architecture: Qwen3 incorporates a series of training techiques and architectural refinements, including global-batch load balancing loss for MoE models and qk layernorm for all models, leading to improved stability and overall performance.\nThree-stage Pre-training: Stage 1 focuses on broad language modeling and general knowledge acquisition, Stage 2 improves reasoning skills like STEM, coding, and logical reasoning, and Stage 3 enhances long-context comprehension by extending training sequence lengths up to 32k tokens.\nScaling Law Guided Hyperparameter Tuning: Through comprehensive scaling law studies across the three-stage pre-training pipeline, Qwen3 systematically tunes critical hyperparameters ‚Äî such as learning rate scheduler and batch size ‚Äî separately for dense and MoE models, resulting in better training dynamics and final performance across different model scales.\nModel Overview\nQwen3-4B-Base has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nNumber of Parameters: 4.0B\nNumber of Paramaters (Non-Embedding): 3.6B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 32 for Q and 8 for KV\nContext Length: 32,768\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "Qwen/Qwen3-0.6B-Base": "Qwen3-0.6B-Base\nQwen3 Highlights\nModel Overview\nRequirements\nEvaluation & Performance\nCitation\nQwen3-0.6B-Base\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.\nBuilding upon extensive advancements in training data, model architecture, and optimization techniques, Qwen3 delivers the following key improvements over the previously released Qwen2.5:\nExpanded Higher-Quality Pre-training Corpus: Qwen3 is pre-trained on 36 trillion tokens across 119 languages ‚Äî tripling the language coverage of Qwen2.5 ‚Äî with a much richer mix of high-quality data, including coding, STEM, reasoning, book, multilingual, and synthetic data.\nTraining Techniques and Model Architecture: Qwen3 incorporates a series of training techiques and architectural refinements, including global-batch load balancing loss for MoE models and qk layernorm for all models, leading to improved stability and overall performance.\nThree-stage Pre-training: Stage 1 focuses on broad language modeling and general knowledge acquisition, Stage 2 improves reasoning skills like STEM, coding, and logical reasoning, and Stage 3 enhances long-context comprehension by extending training sequence lengths up to 32k tokens.\nScaling Law Guided Hyperparameter Tuning: Through comprehensive scaling law studies across the three-stage pre-training pipeline, Qwen3 systematically tunes critical hyperparameters ‚Äî such as learning rate scheduler and batch size ‚Äî separately for dense and MoE models, resulting in better training dynamics and final performance across different model scales.\nModel Overview\nQwen3-0.6B-Base has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nNumber of Parameters: 0.6B\nNumber of Paramaters (Non-Embedding): 0.44B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 16 for Q and 8 for KV\nContext Length: 32,768\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "unsloth/Qwen3-8B-GGUF": "To Switch Between Thinking and Non-Thinking\nQwen3-8B\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nProcessing Long Texts\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\n‚ú® Run & Fine-tune Qwen3 with Unsloth!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\n‚ñ∂Ô∏è Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\n‚ñ∂Ô∏è Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nPhi-4 (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nTo Switch Between Thinking and Non-Thinking\nIf you are using llama.cpp, Ollama, Open WebUI etc., you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of multi-turn conversation:\n> Who are you /no_think\n<think>\n</think>\nI am Qwen, a large-scale language model developed by Alibaba Cloud. [...]\n> How many 'r's are in 'strawberries'? /think\n<think>\nOkay, let's see. The user is asking how many times the letter 'r' appears in the word \"strawberries\". [...]\n</think>\nThe word strawberries contains 3 instances of the letter r. [...]\nQwen3-8B\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-8B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 8.2B\nNumber of Paramaters (Non-Embedding): 6.95B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 32 for Q and 8 for KV\nContext Length: 32,768 natively and 131,072 tokens with YaRN.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use vllm>=0.8.5 or sglang>=0.4.5.post2 to create an OpenAI-compatible API endpoint:\nvLLM:vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser deepseek-r1\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by vLLM and SGLang.\nPlease refer to our documentation for vLLM and SGLang users.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-8B\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nNote\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-8B',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Long Texts\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers and llama.cpp for local use, vllm and sglang for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768\n}\n}\nFor llama.cpp, you need to regenerate the GGUF file after the modification.\nPassing command line arguments:\nFor vllm, you can use\nvllm serve ... --rope-scaling '{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\nFor sglang, you can use\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\nFor llama-server from llama.cpp, you can use\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\nIf you encounter the following warning\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\nplease upgrade transformers>=4.51.0.\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0.\nThe default max_position_embeddings in config.json is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\nThe endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3,\ntitle  = {Qwen3},\nurl    = {https://qwenlm.github.io/blog/qwen3/},\nauthor = {Qwen Team},\nmonth  = {April},\nyear   = {2025}\n}",
    "Datadog/Toto-Open-Base-1.0": "Toto-Open-Base-1.0\n‚ú® Key Features\nüìö Training Data Summary\n‚ö° Quick Start: Model Inference\nInstallation\nüöÄ Inference Example\nüíæ Available Checkpoints\nüîó Additional Resources\nüìñ Citation\nToto-Open-Base-1.0\nToto (Time Series Optimized Transformer for Observability) is a state-of-the-art time-series foundation model designed for multi-variate time series forecasting, emphasizing observability metrics. Toto efficiently handles high-dimensional, sparse, and non-stationary data commonly encountered in observability scenarios.\nThe average rank of Toto compared to the runner-up models on both the GIFT-Eval and BOOM benchmarks (as of May 19, 2025).\n‚ú® Key Features\nZero-Shot Forecasting: Perform forecasting without fine-tuning on your specific time series.\nHigh-Dimension Multi-Variate Support: Efficiently process multiple variables using Proportional Factorized Space-Time Attention.\nDecoder-Only Transformer Architecture: Support for variable prediction horizons and context lengths.\nProbabilistic Predictions: Generate both point forecasts and uncertainty estimates using a Student-T mixture model.\nExtensive Pretraining on Large-Scale Data: Trained on over 2 trillion time series data points, the largest pretraining dataset for any open-weights time series foundation model to date.\nTailored for Observability Metrics with State-of-the-Art Performance on GIFT-Eval and BOOM.\nOverview of Toto-Open-Base-1.0 architecture.\nüìö Training Data Summary\nObservability Metrics: ~1 trillion points from Datadog internal systems (no customer data)\nPublic Datasets:\nGIFT-Eval Pretrain\nChronos datasets\nSynthetic Data: ~1/3 of training data\n‚ö° Quick Start: Model Inference\nInference code is available on GitHub.\nInstallation\npip install toto-ts\nFor optimal speed and reduced memory usage, you should also install xFormers and flash-attention\nüöÄ Inference Example\nHere's how to quickly generate forecasts using Toto:\n‚ö†Ô∏è In our study, we take the median across 256 samples to produce a point forecast. This tutorial previously used the mean but has now been updated.\nimport torch\nfrom toto.data.util.dataset import MaskedTimeseries\nfrom toto.inference.forecaster import TotoForecaster\nfrom toto.model.toto import Toto\nDEVICE = 'cuda'\n# Load pre-trained Toto model\ntoto = Toto.from_pretrained('Datadog/Toto-Open-Base-1.0').to(DEVICE)\n# Optional: compile model for enhanced speed\ntoto.compile()\nforecaster = TotoForecaster(toto.model)\n# Example input series (7 variables, 4096 timesteps)\ninput_series = torch.randn(7, 4096).to(DEVICE)\ntimestamp_seconds = torch.zeros(7, 4096).to(DEVICE)\ntime_interval_seconds = torch.full((7,), 60*15).to(DEVICE)\ninputs = MaskedTimeseries(\nseries=input_series,\npadding_mask=torch.full_like(input_series, True, dtype=torch.bool),\nid_mask=torch.zeros_like(input_series),\ntimestamp_seconds=timestamp_seconds,\ntime_interval_seconds=time_interval_seconds,\n)\n# Generate forecasts for next 336 timesteps\nforecast = forecaster.forecast(\ninputs,\nprediction_length=336,\nnum_samples=256,\nsamples_per_batch=256,\n)\n# Access results\nmedian_prediction = forecast.median\nprediction_samples = forecast.samples\nlower_quantile = forecast.quantile(0.1)\nupper_quantile = forecast.quantile(0.9)\nFor detailed inference instructions, refer to the inference tutorial notebook.\nüíæ Available Checkpoints\nCheckpoint\nParameters\nConfig\nSize\nNotes\nToto-Open-Base-1.0\n151M\nConfig\n605 MB\nInitial release with SOTA performance\nüîó Additional Resources\nResearch Paper\nGitHub Repository\nBlog Post\nBOOM Dataset\nüìñ Citation\nIf you use Toto in your research or applications, please cite us using the following:\n@misc{cohen2025timedifferentobservabilityperspective,\ntitle={This Time is Different: An Observability Perspective on Time Series Foundation Models},\nauthor={Ben Cohen and Emaad Khwaja and Youssef Doubli and Salahidine Lemaachi and Chris Lettieri and Charles Masson and Hugo Miccinilli and Elise Ram√© and Qiqi Ren and Afshin Rostamizadeh and Jean Ogier du Terrail and Anna-Monica Toon and Kan Wang and Stephan Xie and Zongzhe Xu and Viktoriya Zhukova and David Asker and Ameet Talwalkar and Othmane Abou-Amal},\nyear={2025},\neprint={2505.14766},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2505.14766},\n}",
    "Qwen/Qwen3-14B-AWQ": "Qwen3-14B-AWQ\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nProcessing Long Texts\nPerformance\nBest Practices\nCitation\nQwen3-14B-AWQ\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-14B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 14.8B\nNumber of Paramaters (Non-Embedding): 13.2B\nNumber of Layers: 40\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: 32,768 natively and 131,072 tokens with YaRN.\nQuantization: AWQ 4-bit\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-14B-AWQ\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-14B-AWQ --reasoning-parser qwen3\nvLLM:vllm serve Qwen/Qwen3-14B-AWQ --enable-reasoning --reasoning-parser deepseek_r1\nAlso check out our AWQ documentation for more usage guide.\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by SGLang and vLLM.\nPlease refer to our documentation for SGLang and vLLM users.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-14B-AWQ\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-14B-AWQ',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Long Texts\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers for local use, vllm and sglang for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768\n}\n}\nPassing command line arguments:\nFor vllm, you can use\nvllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\nFor sglang, you can use\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\nIf you encounter the following warning\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\nplease upgrade transformers>=4.51.0.\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0.\nThe default max_position_embeddings in config.json is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\nThe endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\nPerformance\nMode\nQUANTIZATION TYPE\nLiveBench 2024-11-25\nGPQA\nMMLU-Redux\nAIME24\nThinking\nbf16\n71.3\n64.0\n88.6\n79.3\nThinking\nAWQ-int4\n70.0\n62.1\n88.5\n77.0\nNon-Thinking\nbf16\n59.6\n54.8\n82.0\n-\nNon-Thinking\nAWQ-int4\n57.4\n53.8\n81.5\n-\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. We strongly recommend setting this value to 1.5 for quantized models. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "Heartsync/Flux-NSFW-uncensored": "",
    "DavidAU/Qwen3-8B-64k-Context-2X-Josiefied-Uncensored": "Qwen3-8B-64k-Context-2X-Josiefied-Uncensored\nThis repo contains the full precision source code, in \"safe tensors\" format to generate GGUFs, GPTQ, EXL2, AWQ, HQQ and other formats. The source code can also be used directly.\nThis repo is for Goekdeniz-Guelmez's excellent \"Josiefied-Qwen3-8B-abliterated-v1\", modified from 32k (32768) context to 64 k (65536) context modified using YARN as per tech notes at Qwen repo.\nNEO Imatrix dataset GGUF quants with maxed 16-bit output tensor are here:\n[ https://huggingface.co/DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-NEO-Max-GGUF ]\nORG model repo for this fine tune:\n[ https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1 ]\nMax context on this version is : 64k (65536)\nSuggest min context limit of : 8k to 16k for \"thinking\" / \"output\".\nUse Jinja Template or CHATML template.\nPlease refer the QWEN model card for details, benchmarks, how to use, settings, turning reasoning on/off/ system roles etc etc :\n[ https://huggingface.co/Qwen/Qwen3-8B ]\nOPTIONAL SYSTEM ROLE:\nYou may or may not need this, as most times Qwen3s generate their own reasoning/thinking blocks.\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\nSee document \"Maximizing-Model-Performance-All...\" below for how to \"set\" system role in various LLM/AI apps below.\nIMPORTANT: Highest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nIf you are going to use this model, (source, GGUF or a different quant), please review this document for critical parameter, sampler and advance sampler settings (for multiple AI/LLM aps).\nThis a \"Class 1\" (settings will enhance operation) model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) (especially for use case(s) beyond the model's design) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nREASON:\nRegardless of \"model class\" this document will detail methods to enhance operations.\nIf the model is a Class 3/4 model the default settings (parameters, samplers, advanced samplers) must be set for \"use case(s)\" uses correctly. Some AI/LLM apps DO NOT have consistant default setting(s) which result in sub-par model operation. Like wise for Class 3/4 models (which operate somewhat to very differently than standard models) additional samplers and advanced samplers settings are required to \"smooth out\" operation, AND/OR also allow full operation for use cases the model was not designed for.\nBONUS - Use these settings for ANY model, ANY repo, ANY quant (including source/full precision):\nThis document also details parameters, sampler and advanced samplers that can be use FOR ANY MODEL, FROM ANY REPO too - all quants, and of course source code operation too - to enhance the operation of any model.\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nNOTE:\nI strongly suggest you also visit the DavidAU GGUF (below) repo too for more details in using this model ; especially if it is \"Class 3\" or \"Class 4\" to get maximum performance from the model.\nFor full information about this model, including:\nDetails about this model and its use case(s).\nContext limits\nSpecial usage notes / settings.\nAny model(s) used to create this model.\nTemplate(s) used to access/use this model.\nExample generation(s)\nGGUF quants of this model\nPlease go to:\nhttps://huggingface.co/DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-NEO-Max-GGUF\n[ Also see LEFT MENU under \"Quantizations\" ]\n[[ model card updates to follow || GGUF repo(s) pending ... ]]\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.",
    "bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF": "Llamacpp imatrix Quantizations of Dolphin-Mistral-24B-Venice-Edition by cognitivecomputations\nPrompt format\nWhat's new:\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Dolphin-Mistral-24B-Venice-Edition by cognitivecomputations\nUsing llama.cpp release b5835 for quantization.\nOriginal model: https://huggingface.co/cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\n<s>[SYSTEM_PROMPT]{system_prompt}[/SYSTEM_PROMPT][INST]{prompt}[/INST]\nWhat's new:\nOriginal model updated\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nDolphin-Mistral-24B-Venice-Edition-bf16.gguf\nbf16\n47.15GB\nfalse\nFull BF16 weights.\nDolphin-Mistral-24B-Venice-Edition-Q8_0.gguf\nQ8_0\n25.05GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nDolphin-Mistral-24B-Venice-Edition-Q6_K_L.gguf\nQ6_K_L\n19.67GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q6_K.gguf\nQ6_K\n19.35GB\nfalse\nVery high quality, near perfect, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q5_K_L.gguf\nQ5_K_L\n17.18GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q5_K_M.gguf\nQ5_K_M\n16.76GB\nfalse\nHigh quality, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q5_K_S.gguf\nQ5_K_S\n16.30GB\nfalse\nHigh quality, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q4_1.gguf\nQ4_1\n14.87GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nDolphin-Mistral-24B-Venice-Edition-Q4_K_L.gguf\nQ4_K_L\n14.83GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf\nQ4_K_M\n14.33GB\nfalse\nGood quality, default size for most use cases, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q4_K_S.gguf\nQ4_K_S\n13.55GB\nfalse\nSlightly lower quality with more space savings, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q4_0.gguf\nQ4_0\n13.49GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nDolphin-Mistral-24B-Venice-Edition-IQ4_NL.gguf\nIQ4_NL\n13.47GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nDolphin-Mistral-24B-Venice-Edition-Q3_K_XL.gguf\nQ3_K_XL\n12.99GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nDolphin-Mistral-24B-Venice-Edition-IQ4_XS.gguf\nIQ4_XS\n12.76GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nDolphin-Mistral-24B-Venice-Edition-Q3_K_L.gguf\nQ3_K_L\n12.40GB\nfalse\nLower quality but usable, good for low RAM availability.\nDolphin-Mistral-24B-Venice-Edition-Q3_K_M.gguf\nQ3_K_M\n11.47GB\nfalse\nLow quality.\nDolphin-Mistral-24B-Venice-Edition-IQ3_M.gguf\nIQ3_M\n10.65GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nDolphin-Mistral-24B-Venice-Edition-Q3_K_S.gguf\nQ3_K_S\n10.40GB\nfalse\nLow quality, not recommended.\nDolphin-Mistral-24B-Venice-Edition-IQ3_XS.gguf\nIQ3_XS\n9.91GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nDolphin-Mistral-24B-Venice-Edition-Q2_K_L.gguf\nQ2_K_L\n9.55GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nDolphin-Mistral-24B-Venice-Edition-IQ3_XXS.gguf\nIQ3_XXS\n9.28GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\nDolphin-Mistral-24B-Venice-Edition-Q2_K.gguf\nQ2_K\n8.89GB\nfalse\nVery low quality but surprisingly usable.\nDolphin-Mistral-24B-Venice-Edition-IQ2_M.gguf\nIQ2_M\n8.11GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nDolphin-Mistral-24B-Venice-Edition-IQ2_S.gguf\nIQ2_S\n7.48GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nDolphin-Mistral-24B-Venice-Edition-IQ2_XS.gguf\nIQ2_XS\n7.21GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nDolphin-Mistral-24B-Venice-Edition-IQ2_XXS.gguf\nIQ2_XXS\n6.55GB\nfalse\nVery low quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF --include \"cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-GGUF --include \"cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (cognitivecomputations_Dolphin-Mistral-24B-Venice-Edition-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "arcee-ai/AFM-4.5B-Base-Pre-Anneal": "No model card",
    "nvidia/GR00T-N1.5-3B": "GR00T-N1.5-3B\nDescription:\nLicense/Terms of Use\nDeployment Geography:\nUse Case:\nReference(s):\nModel Architecture:\nInput:\nOutput:\nSoftware Integration:\nModel Version(s):\nEthical Considerations:\nResources\nGR00T-N1.5-3B\nDescription:\nNVIDIA Isaac GR00T N1.5 is an open foundation model for generalized humanoid robot reasoning and skills. This cross-embodiment model takes multimodal input, including language and images, to perform manipulation tasks in diverse environments. Developers and researchers can post-train GR00T N1.5 with real or synthetic data for their specific humanoid robot or task.\nIsaac GR00T N1.5-3B is the medium-sized version of our model built using pre-trained vision and language encoders, and uses a flow matching action transformer to model a chunk of actions conditioned on vision, language and proprioception.\nThis model is ready for non-commercial use.\nLicense/Terms of Use\nNvidia License\nYou are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws.\nDeployment Geography:\nGlobal\nUse Case:\nResearchers, Academics, Open-Source Community: AI-driven robotics research and algorithm development.\nDevelopers: Integrate and customize AI for various robotic applications.\nStartups & Companies: Accelerate robotics development and reduce training costs.\nReference(s):\nNVIDIA-EAGLE:\nLi, Zhiqi, et al. \"Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models.\" arXiv preprint arXiv:2501.14818 (2025).\nRectified Flow:\nLiu, Xingchao, and Chengyue Gong. \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow.\" The Eleventh International Conference on Learning Representations‚Äù.\nFlow Matching Policy:\nBlack, Kevin, et al. \"œÄ0: A Vision-Language-Action Flow Model for General Robot Control.\" arXiv preprint arXiv:2410.24164 (2024).\nModel Architecture:\nArchitecture Type: Vision Transformer, Multilayer Perceptron, Flow matching Transformer\nIsaac GR00T N1.5 uses vision and text transformers to encode the robot's image observations and text instructions. The architecture handles a varying number of views per embodiment by concatenating image token embeddings from all frames into a sequence, followed by language token embeddings.\nTo model proprioception and a sequence of actions conditioned on observations, Isaac GR00T N1.5-3B uses a flow matching transformer. The flow matching transformer interleaves self-attention over proprioception and actions with cross-attention to the vision and language embeddings. During training, the input actions are corrupted by randomly interpolating between the clean action vector and a gaussian noise vector. At inference time, the policy first samples a gaussian noise vector and iteratively reconstructs a continuous-value action using its velocity prediction.\nIn GR00T-N1.5, the MLP connector between the vision-language features and the diffusion-transformer (DiT) has been modified for improved performance on our sim benchmarks. Also, it was trained jointly with flow matching and world-modeling objectives.\nNetwork Architecture:\nThe schematic diagram is shown in the illustration above.\nRed, Green, Blue (RGB) camera frames are processed through a pre-trained vision transformer (SigLip2).\nText is encoded by a pre-trained transformer (T5)\nRobot proprioception is encoded using a multi-layer perceptron (MLP) indexed by the embodiment ID. To handle variable-dimension proprio, inputs are padded to a configurable max length before feeding into the MLP.\nActions are encoded and velocity predictions decoded by an MLP, one per unique embodiment.\nThe flow matching transformer is implemented as a diffusion transformer (DiT), in which the diffusion step conditioning is implemented using adaptive layernorm (AdaLN).\nInput:\nInput Type:\nVision: Image Frames\nState: Robot Proprioception\nLanguage Instruction: Text\nInput Format:\nVision: Variable number of 224x224 uint8 image frames, coming from robot cameras\nState: Floating Point\nLanguage Instruction: String\nInput Parameters:\nVision: 2D - RGB image, square\nState: 1D - Floating number vector\nLanguage Instruction: 1D - String\nOutput:\nOutput Type(s): Actions\nOutput Format Continuous-value vectors\nOutput Parameters: [Two-Dimensional (2D)]\nOther Properties Related to Output: Continuous-value vectors correspond to different motor controls on a robot, which depends on Degrees of Freedom of the robot embodiment.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nRuntime Engine(s): PyTorch\nSupported Hardware Microarchitecture Compatibility:\nAll of the below:\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Jetson\nNVIDIA Hopper\nNVIDIA Lovelace\n[Preferred/Supported] Operating System(s):\nLinux\nModel Version(s):\nVersion 1.5.\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security), and Privacy Subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nResources\nPrevious Version: https://huggingface.co/nvidia/GR00T-N1-2B\nBlogpost: https://nvidianews.nvidia.com/news/foundation-model-isaac-robotics-platform\nCommunity Article with the tutorial how to finetune on SO100/101: https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning",
    "Qwen/Qwen3-Reranker-0.6B": "Qwen3-Reranker-0.6B\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nTransformers Usage\nvLLM Usage\nEvaluation\nCitation\nQwen3-Reranker-0.6B\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Reranker-0.6B has the following features:\nModel Type: Text Reranking\nSupported Languages: 100+ Languages\nNumber of Paramaters: 0.6B\nContext Length: 32k\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\nKeyError: 'qwen3'\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\ndef format_instruction(instruction, query, doc):\nif instruction is None:\ninstruction = 'Given a web search query, retrieve relevant passages that answer the query'\noutput = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\nreturn output\ndef process_inputs(pairs):\ninputs = tokenizer(\npairs, padding=False, truncation='longest_first',\nreturn_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n)\nfor i, ele in enumerate(inputs['input_ids']):\ninputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\ninputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\nfor key in inputs:\ninputs[key] = inputs[key].to(model.device)\nreturn inputs\n@torch.no_grad()\ndef compute_logits(inputs, **kwargs):\nbatch_scores = model(**inputs).logits[:, -1, :]\ntrue_vector = batch_scores[:, token_true_id]\nfalse_vector = batch_scores[:, token_false_id]\nbatch_scores = torch.stack([false_vector, true_vector], dim=1)\nbatch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\nscores = batch_scores[:, 1].exp().tolist()\nreturn scores\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\ntoken_false_id = tokenizer.convert_tokens_to_ids(\"no\")\ntoken_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\nmax_length = 8192\nprefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\nsuffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\nprefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\nsuffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\"What is the capital of China?\",\n\"Explain gravity\",\n]\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\npairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n# Tokenize the input texts\ninputs = process_inputs(pairs)\nscores = compute_logits(inputs)\nprint(\"scores: \", scores)\nvLLM Usage\n# Requires vllm>=0.8.5\nimport logging\nfrom typing import Dict, Optional, List\nimport json\nimport logging\nimport torch\nfrom transformers import AutoTokenizer, is_torch_npu_available\nfrom vllm import LLM, SamplingParams\nfrom vllm.distributed.parallel_state import destroy_model_parallel\nimport gc\nimport math\nfrom vllm.inputs.data import TokensPrompt\ndef format_instruction(instruction, query, doc):\ntext = [\n{\"role\": \"system\", \"content\": \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\"},\n{\"role\": \"user\", \"content\": f\"<Instruct>: {instruction}\\n\\n<Query>: {query}\\n\\n<Document>: {doc}\"}\n]\nreturn text\ndef process_inputs(pairs, instruction, max_length, suffix_tokens):\nmessages = [format_instruction(instruction, query, doc) for query, doc in pairs]\nmessages =  tokenizer.apply_chat_template(\nmessages, tokenize=True, add_generation_prompt=False, enable_thinking=False\n)\nmessages = [ele[:max_length] + suffix_tokens for ele in messages]\nmessages = [TokensPrompt(prompt_token_ids=ele) for ele in messages]\nreturn messages\ndef compute_logits(model, messages, sampling_params, true_token, false_token):\noutputs = model.generate(messages, sampling_params, use_tqdm=False)\nscores = []\nfor i in range(len(outputs)):\nfinal_logits = outputs[i].outputs[0].logprobs[-1]\ntoken_count = len(outputs[i].outputs[0].token_ids)\nif true_token not in final_logits:\ntrue_logit = -10\nelse:\ntrue_logit = final_logits[true_token].logprob\nif false_token not in final_logits:\nfalse_logit = -10\nelse:\nfalse_logit = final_logits[false_token].logprob\ntrue_score = math.exp(true_logit)\nfalse_score = math.exp(false_logit)\nscore = true_score / (true_score + false_score)\nscores.append(score)\nreturn scores\nnumber_of_gpu = torch.cuda.device_count()\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Reranker-0.6B')\nmodel = LLM(model='Qwen/Qwen3-Reranker-0.6B', tensor_parallel_size=number_of_gpu, max_model_len=10000, enable_prefix_caching=True, gpu_memory_utilization=0.8)\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\nsuffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\nmax_length=8192\nsuffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\ntrue_token = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\nfalse_token = tokenizer(\"no\", add_special_tokens=False).input_ids[0]\nsampling_params = SamplingParams(temperature=0,\nmax_tokens=1,\nlogprobs=20,\nallowed_token_ids=[true_token, false_token],\n)\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\"What is the capital of China?\",\n\"Explain gravity\",\n]\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\npairs = list(zip(queries, documents))\ninputs = process_inputs(pairs, task, max_length-len(suffix_tokens), suffix_tokens)\nscores = compute_logits(model, inputs, sampling_params, true_token, false_token)\nprint('scores', scores)\ndestroy_model_parallel()\nüìå Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nEvaluation\nModel\nParam\nMTEB-R\nCMTEB-R\nMMTEB-R\nMLDR\nMTEB-Code\nFollowIR\nQwen3-Embedding-0.6B\n0.6B\n61.82\n71.02\n64.64\n50.26\n75.41\n5.09\nJina-multilingual-reranker-v2-base\n0.3B\n58.22\n63.37\n63.73\n39.66\n58.98\n-0.68\ngte-multilingual-reranker-base\n0.3B\n59.51\n74.08\n59.44\n66.33\n54.18\n-1.64\nBGE-reranker-v2-m3\n0.6B\n57.03\n72.16\n58.36\n59.51\n41.38\n-0.01\nQwen3-Reranker-0.6B\n0.6B\n65.80\n71.31\n66.36\n67.28\n73.42\n5.41\nQwen3-Reranker-4B\n4B\n69.76\n75.94\n72.74\n69.97\n81.20\n14.84\nQwen3-Reranker-8B\n8B\n69.02\n77.45\n72.94\n70.19\n81.22\n8.05\nNote:\nEvaluation results for reranking models. We use the retrieval subsets of MTEB(eng, v2), MTEB(cmn, v1), MMTEB and MTEB (Code), which are MTEB-R, CMTEB-R, MMTEB-R and MTEB-Code.\nAll scores are our runs based on the top-100 candidates retrieved by dense embedding model Qwen3-Embedding-0.6B.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "PaddlePaddle/PP-OCRv5_server_det": "PP-OCRv5_server_det\nIntroduction\nQuick Start\nInstallation\nModel Usage\nPipeline Usage\nLinks\nPP-OCRv5_server_det\nIntroduction\nPP-OCRv5_server_det is one of the PP-OCRv5_det series, the latest generation of text detection models developed by the PaddleOCR team. Designed for high-performance applications, it supports the detection of text in diverse scenarios‚Äîincluding handwriting, vertical, rotated, and curved text‚Äîacross multiple languages such as Simplified Chinese, Traditional Chinese, English, and Japanese. Key features include robust handling of complex layouts, varying text sizes, and challenging backgrounds, making it suitable for practical applications like document analysis, license plate recognition, and scene text detection. The key accuracy metrics are as follow:\nHandwritten Chinese\nHandwritten English\nPrinted Chinese\nPrinted English\nTraditional Chinese\nAncient Text\nJapanese\nGeneral Scenario\nPinyin\nRotation\nDistortion\nArtistic Text\nAverage\n0.803\n0.841\n0.945\n0.917\n0.815\n0.676\n0.772\n0.797\n0.671\n0.8\n0.876\n0.673\n0.827\nQuick Start\nInstallation\nPaddlePaddle\nPlease refer to the following commands to install PaddlePaddle using pip:\n# for CUDA11.8\npython -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n# for CUDA12.6\npython -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n# for CPU\npython -m pip install paddlepaddle==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\nFor details about PaddlePaddle installation, please refer to the PaddlePaddle official website.\nPaddleOCR\nInstall the latest version of the PaddleOCR inference package from PyPI:\npython -m pip install paddleocr\nModel Usage\nYou can quickly experience the functionality with a single command:\npaddleocr text_detection \\\n--model_name PP-OCRv5_server_det \\\n-i https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/3ul2Rq4Sk5Cn-l69D695U.png\nYou can also integrate the model inference of the text detection module into your project. Before running the following code, please download the sample image to your local machine.\nfrom paddleocr import TextDetection\nmodel = TextDetection(model_name=\"PP-OCRv5_server_det\")\noutput = model.predict(input=\"3ul2Rq4Sk5Cn-l69D695U.png\", batch_size=1)\nfor res in output:\nres.print()\nres.save_to_img(save_path=\"./output/\")\nres.save_to_json(save_path=\"./output/res.json\")\nAfter running, the obtained result is as follows:\n{'res': {'input_path': '/root/.paddlex/predict_input/3ul2Rq4Sk5Cn-l69D695U.png', 'page_index': None, 'dt_polys': array([[[ 632, 1429],\n...,\n[ 632, 1450]],\n...,\n[[ 353,  102],\n...,\n[ 353,  125]]], dtype=int16), 'dt_scores': [0.8436300312712586, 0.7779392262863483, ..., 0.8491056329808098]}}\nThe visualized image is as follows:\nFor details about usage command and descriptions of parameters, please refer to the Document.\nPipeline Usage\nThe ability of a single model is limited. But the pipeline consists of several models can provide more capacity to resolve difficult problems in real-world scenarios.\nPP-OCRv5\nThe general OCR pipeline is used to solve text recognition tasks by extracting text information from images and outputting it in text form. And there are 5 modules in the pipeline:\nDocument Image Orientation Classification Module (Optional)\nText Image Unwarping Module (Optional)\nText Line Orientation Classification Module (Optional)\nText Detection Module\nText Recognition Module\nRun a single command to quickly experience the OCR pipeline:\npaddleocr ocr -i https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/3ul2Rq4Sk5Cn-l69D695U.png \\\n--text_detection_model_name PP-OCRv5_server_det \\\n--text_recognition_model_name PP-OCRv5_server_rec \\\n--use_doc_orientation_classify False \\\n--use_doc_unwarping False \\\n--use_textline_orientation True \\\n--save_path ./output \\\n--device gpu:0\nResults are printed to the terminal:\n{'res': {'input_path': '/root/.paddlex/predict_input/3ul2Rq4Sk5Cn-l69D695U.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': True}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}, 'angle': -1}, 'dt_polys': array([[[ 352,  105],\n...,\n[ 352,  128]],\n...,\n[[ 632, 1431],\n...,\n[ 632, 1447]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([0, ..., 0]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['Algorithms for the Markov Entropy Decomposition', 'Andrew J. Ferris and David Poulin', 'D√©partement de Physique, Universit√© de Sherbrooke, Qu√©bec, JlK 2R1, Canada', '(Dated: October 31, 2018)', 'The Markov entropy decomposition (MED) is a recently-proposed, cluster-based simulation method for fi-', 'nite temperature quantum systems with arbitrary geometry. In this paper, we detail numerical algorithms for', 'performing the required steps of the MED, principally solving a minimization problem with a preconditioned', 'arXiv:1212.1442v1 [cond-mat.stat-mech] 6Dec 2012', \"Newton's algorithm, as well as how to extract global susceptibilities and thermal responses. We demonstrate\", 'the power of the method with the spin-1/2 XXZ model on the 2D square lattice, including the extraction of', 'critical points and details of each phase. Although the method shares some qualitative similarities with exact-', 'diagonalization, we show the MED is both more accurate and significantly more flexible.', 'PACS numbers: 05.10.‚àía,02.50.Ng, 03.67.‚àía,74.40.Kb', 'I.INTRODUCTION', 'This approximation becomes exact in the case of a 1D quan', 'tum (or classical) Markov chain [10], and leads to an expo-', 'Although the equations governing quantum many-body', 'nential reduction of cost for exact entropy calculations when', 'systems are simple to write down, finding solutions for the', 'the global density matrix is a higher-dimensional Markov net-', 'majority of systems remains incredibly difficult. Modern', 'work state [12, 13].', 'physics finds itself in need of new tools to compute the emer-', 'The second approximation used in the MED approach is', 'gent behavior of large, many-body systems.', 'related to the N-representibility problem. Given a set of lo-', 'There has been a great variety of tools developed to tackle', 'cal but overlapping reduced density matrices {pi}, it is a very', 'many-body problems, but in general, large 2D and 3D quan-', 'challenging problem to determine if there exists a global den-', 'tum systems remain hard to deal with. Most systems are', 'sity operator which is positive semi-definite and whose partial', 'thought to be non-integrable, so exact analytic solutions are', 'trace agrees with each œÅi. This problem is QMA-hard (the', 'not usually expected. Direct numerical diagonalization can be', 'quantum analogue of NP) [14, 15], and is hopelessly diffi-', 'performed for relatively small systems ‚Äî however the emer-', 'cult to enforce. Thus, the second approximation employed', 'gent behavior of a system in the thermodynamic limit may be', 'involves ignoring global consistency with a positive opera-', 'difficult to extract, especially in systems with large correlation', 'tor, while requiring local consistency on any overlapping re-', 'lengths. Monte Carlo approaches are technically exact (up to', 'gions between the œÅi. At the zero-temperature limit, the MED', 'sampling error), but suffer from the so-called sign problem', 'approach becomes analogous to the variational nth-order re-', 'for fermionic, frustrated, or dynamical problems. Thus we are', 'duced density matrix approach, where positivity is enforced', 'limited to search for clever approximations to solve the ma-', 'on all reduced density matrices of size n [16‚Äì18].', 'jority of many-body problems.', 'The MED approach is an extremely flexible cluster method.', 'Over the past century, hundreds of such approximations', 'applicable to both translationally invariant systems of any di-', 'have been proposed, and we will mention just a few notable', 'mension in the thermodynamic limit, as well as finite systems', 'examples applicable to quantum lattice models. Mean-field', 'or systems without translational invariance (e.g. disordered', 'theory is simple and frequently arrives at the correct quali-', 'lattices, or harmonically trapped atoms in optical lattices).', 'tative description, but often fails when correlations are im-', 'The free energy given by MED is guaranteed to lower bound', 'portant. Density-matrix renormalisation group (DMRG) [1]', 'the true free energy, which in turn lower-bounds the ground', 'is efficient and extremely accurate at solving 1D problems,', 'state energy ‚Äî thus providing a natural complement to varia-', 'but the computational cost grows exponentially with system', 'tional approaches which upper-bound the ground state energy.', 'size in two- or higher-dimensions [2, 3]. Related tensor-', 'The ability to provide a rigorous ground-state energy window', 'network techniques designed for 2D systems are still in their', 'is a powerful validation tool, creating a very compelling rea-', 'infancy [4‚Äì6].  Series-expansion methods [7] can be success-', 'son to use this approach.', 'ful, but may diverge or otherwise converge slowly, obscuring', 'In this paper we paper we present a pedagogical introduc-', 'the state in certain regimes. There exist a variety of cluster-', 'tion to MED, including numerical implementation issues and', 'based techniques, such as dynamical-mean-field theory [8]', 'applications to 2D quantum lattice models in the thermody-', 'and density-matrix embedding [9]', 'namiclimit.InSec.II.wegiveabriefderiyationofthe', 'Here we discuss the so-called Markov entropy decompo-', 'Markov entropy decomposition. Section III outlines a robust', 'sition (MED), recently proposed by Poulin & Hastings [10]', 'numerical strategy for optimizing the clusters that make up', '(and analogous to a slightly earlier classical algorithm [11]).', 'the decomposition. In Sec. IV we show how we can extend', 'This is a self-consistent cluster method for finite temperature', 'these algorithms to extract non-trivial information, such as', 'systems that takes advantage of an approximation of the (von', 'specific heat and susceptibilities. We present an application of', 'Neumann) entropy. In [10], it was shown that the entropy', 'the method to the spin-1/2 XXZ model on a 2D square lattice', 'per site can be rigorously upper bounded using only local in-', 'in Sec. V, describing how to characterize the phase diagram', 'formation ‚Äî a local, reduced density matrix on N sites, say.', 'and determine critical points, before concluding in Sec. VI.'], 'rec_scores': array([0.99276221, ..., 0.95760632]), 'rec_polys': array([[[ 352,  105],\n...,\n[ 352,  128]],\n...,\n[[ 632, 1431],\n...,\n[ 632, 1447]]], dtype=int16), 'rec_boxes': array([[ 352, ...,  128],\n...,\n[ 632, ..., 1447]], dtype=int16)}}\nIf save_path is specified, the visualization results will be saved under save_path. The visualization output is shown below:\nThe command-line method is for quick experience. For project integration, also only a few codes are needed as well:\nfrom paddleocr import PaddleOCR\nocr = PaddleOCR(\ntext_detection_model_name=\"PP-OCRv5_server_det\",\ntext_recognition_model_name=\"PP-OCRv5_server_rec\",\nuse_doc_orientation_classify=False, # Disables document orientation classification model via this parameter\nuse_doc_unwarping=False, # Disables text image rectification model via this parameter\nuse_textline_orientation=False, # Disables text line orientation classification model via this parameter\n)\nresult = ocr.predict(\"./3ul2Rq4Sk5Cn-l69D695U.png\")\nfor res in result:\nres.print()\nres.save_to_img(\"output\")\nres.save_to_json(\"output\")\nFor details about usage command and descriptions of parameters, please refer to the Document.\nPP-StructureV3\nLayout analysis is a technique used to extract structured information from document images. PP-StructureV3 includes the following six modules:\nLayout Detection Module\nGeneral OCR Pipeline\nDocument Image Preprocessing Pipeline ÔºàOptionalÔºâ\nTable Recognition Pipeline ÔºàOptionalÔºâ\nSeal Recognition Pipeline ÔºàOptionalÔºâ\nFormula Recognition Pipeline ÔºàOptionalÔºâ\nRun a single command to quickly experience the PP-StructureV3 pipeline:\npaddleocr pp_structurev3 -i https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/mG4tnwfrvECoFMu-S9mxo.png \\\n--text_detection_model_name PP-OCRv5_server_det \\\n--use_doc_orientation_classify False \\\n--use_doc_unwarping False \\\n--use_textline_orientation False \\\n--device gpu:0\nResults would be printed to the terminal. If save_path is specified, the results will be saved under save_path. The predicted markdown visualization is shown below:\nJust a few lines of code can experience the inference of the pipeline. Taking the PP-StructureV3 pipeline as an example:\nfrom paddleocr import PPStructureV3\npipeline = PPStructureV3(\ntext_detection_model_name=\"PP-OCRv5_server_det\",\nuse_doc_orientation_classify=False, # Use use_doc_orientation_classify to enable/disable document orientation classification model\nuse_doc_unwarping=False,    # Use use_doc_unwarping to enable/disable document unwarping module\nuse_textline_orientation=False, # Use use_textline_orientation to enable/disable textline orientation classification model\ndevice=\"gpu:0\", # Use device to specify GPU for model inference\n)\noutput = pipeline.predict(\"./pp_structure_v3_demo.png\")\nfor res in output:\nres.print() # Print the structured prediction output\nres.save_to_json(save_path=\"output\") ## Save the current image's structured result in JSON format\nres.save_to_markdown(save_path=\"output\") ## Save the current image's result in Markdown format\nThe default model used in pipeline is PP-OCRv5_server_det, and you can specify other text detection model by argument text_detection_model_name. And you can also use the local model file by argument text_detection_model_dir. For details about usage command and descriptions of parameters, please refer to the Document.\nLinks\nPaddleOCR Repo\nPaddleOCR Documentation",
    "Unbabel/Tower-Plus-2B": "Model Description:\nUsing on VLLM:\nUsing on Transformers:\nIntended uses & limitations\nUsing on VLLM:\nUsing on Transformers:\nUsage:\nUsing on VLLM:\nUsing on Transformers:\nCitation\nModel Description:\nTower+ 2B is build on top of Gemma 2 2B. The model goes through the Continuous Pretraining (CPT), Instruction Tuning (IT), Weighted Preference Optimization (WPO) and GRPO with verifiable rewards. During all stages we include parallel and multilingual data (covering 22 languages).\nThis approach makes Tower+ 2B one of the best multilingual LLMs under 3B parameters.\nDeveloped by: Unbabel\nModel type: A 2B parameter model fine-tuned on a mix of translation-related tasks as well as  general instruction-following datasets that include reasoning, code instructions, etc.\nLanguages: German, Spanish, French, Italian, Korean, Dutch, Russian, English, Portuguese (Portugal), Portuguese (Brazilian), Spanish (Latin America), Chinese (Simplified), Chinese (Traditional), Czech, Ukrainian, Hindi, Icelandic, Japanese, Polish, Swedish, Hungarian, Romanian, Danish, Norwegian (Nynorsk), Norwegian (Bokm√•l), Finnish\nLicense: CC-BY-NC-4.0\nContext Size:: 8192 tokens\nIntended uses & limitations\nTower is intended for multilingual tasks and its specially strong on machine translation.\nBecause Tower is also a strong multilingual model you can also use it for other multilingual tasks.\nAnother usecase Tower works well is for creating multilingual synthethic data (for the languages it covers). You can do this either by translating instructions and the respective answers or by asking the model to create an instruction given a document as seed data.\nUsage:\nWhen using the model, make sure your prompt is formated correctly!\nAlso, we recommend using VLLM rather than Hugging Face.\nUsing on VLLM:\n# pip install vllm\n# Gemma by default only uses 4k context. You need to set the following variables:\n# export VLLM_WORKER_MULTIPROC_METHOD=spawn\n# export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\nfrom vllm import LLM, SamplingParams\nsampling_params = SamplingParams(\nbest_of=1,\ntemperature=0,\nmax_tokens=8192,\n)\nllm = LLM(model=\"Unbabel/Tower-Plus-2B\", tensor_parallel_size=1)\nmessages = [{\"role\": \"user\", \"content\": \"Translate the following English source text to Portuguese (Portugal):\\nEnglish: Hello world!\\nPortuguese (Portugal): \"}]\noutputs = llm.chat(messages, sampling_params)\n# Make sure your prompt_token_ids look like this\nprint (outputs[0].outputs[0].text)\n# > Ol√°, mundo!\nUsing on Transformers:\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"Unbabel/Tower-Plus-2B\", device_map=\"auto\")\n# We use the tokenizer‚Äôs chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [{\"role\": \"user\", \"content\": \"Translate the following English source text to Portuguese (Portugal):\\nEnglish: Hello world!\\nPortuguese (Portugal): \"}]\ninput_ids = pipe.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\noutputs = pipe(messages, max_new_tokens=256, do_sample=False)\nprint(outputs[0][\"generated_text\"])\nCitation\nIf you use this model please cite our paper:\n@misc{rei2025towerplus,\ntitle={Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs},\nauthor={Ricardo Rei and Nuno M. Guerreiro and Jos√© Pombal and Jo√£o Alves and Pedro Teixeirinha and Amin Farajian and Andr√© F. T. Martins},\nyear={2025},\neprint={2506.17080},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2506.17080},\n}",
    "Menlo/Jan-nano": "Jan-Nano: An Agentic Model\nOverview\nEvaluation\nHow to Run Locally\nVLLM\nRecommended Sampling Parameters\nüìÑ Citation\nDocumentation\nJan-Nano: An Agentic Model\nNote: Jan-Nano is a non-thinking model.\nAuthors: Alan Dao, Bach Vu Dinh\nOverview\nJan-Nano is a compact 4-billion parameter language model specifically designed and trained for deep research tasks. This model has been optimized to work seamlessly with Model Context Protocol (MCP) servers, enabling efficient integration with various research tools and data sources.\nEvaluation\nJan-Nano has been evaluated on the SimpleQA benchmark using our MCP-based benchmark methodology, demonstrating strong performance for its model size:\nThe evaluation was conducted using our MCP-based benchmark approach, which assesses the model's performance on SimpleQA tasks while leveraging its native MCP server integration capabilities. This methodology better reflects Jan-Nano's real-world performance as a tool-augmented research model, validating both its factual accuracy and its effectiveness in MCP-enabled environments.\nHow to Run Locally\nJan-Nano is currently supported by Jan, an open-source ChatGPT alternative that runs entirely on your computer. Jan provides a user-friendly interface for running local AI models with full privacy and control.\nFor non-jan app or tutorials there are guidance inside community section, please check those out! Discussion\nVLLM\nHere is an example command you can use to run vllm with Jan-nano\nvllm serve Menlo/Jan-nano --host 0.0.0.0 --port 1234 --enable-auto-tool-choice --tool-call-parser hermes --chat-template ./qwen3_nonthinking.jinja\nChat-template is already included in tokenizer so chat-template is optional, but in case it has issue you can download the template here Non-think chat template\nRecommended Sampling Parameters\nTemperature: 0.7\nTop-p: 0.8\nTop-k: 20\nMin-p: 0\nüìÑ Citation\n@misc{dao2025jannanotechnicalreport,\ntitle={Jan-nano Technical Report},\nauthor={Alan Dao and Dinh Bach Vu},\nyear={2025},\neprint={2506.22760},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2506.22760},\n}\nDocumentation\nSetup, Usage & FAQ",
    "ICTNLP/stream-omni-8b": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model\nüñ• Demo\nStream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model\nShaolei Zhang, Shoutao Guo, Qingkai Fang, Yan Zhou, Yang Feng*\nThe introduction and usage of Stream-Omni refer to https://github.com/ictnlp/Stream-Omni.\nStream-Omni is an end-to-end language-vision-speech chatbot that simultaneously supports interaction across various modality combinations, with the following featuresüí°:\nOmni Interaction: Support any multimodal inputs including text, vision, and speech, and generate both text and speech responses.\nSeamless \"see-while-hear\" Experience: Simultaneously output intermediate textual results (e.g., ASR transcriptions and model responses) during speech interactions, like the advanced voice service of GPT-4o.\nEfficient Training: Require only a small amount of omni-modal data for training.\nüñ• Demo\nMicrophone Input\nFile Input\nStream-Omni can produce intermediate textual results (ASR transcription and text response) during speech interaction, offering users a seamless \"see-while-hear\" experience.",
    "unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF": "Mistral-Small-3.2-24B-Instruct-2506\nKey Features\nBenchmark Results\nText\nVision\nUsage\nvLLM (recommended)\nTransformers\nIncludes our GGUF chat template fixes! Tool calling works as well!  If you are using llama.cpp, use --jinja to enable the system prompt.\nUnsloth Dynamic 2.0 achieves SOTA performance in model quantization.\n‚ú® How to Use Mistral 3.2 Small:\nRun in llama.cpp:\n./llama.cpp/llama-cli -hf unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:UD-Q4_K_XL --jinja --temp 0.15 --top-k -1 --top-p 1.00 -ngl 99\nRun in Ollama:\nollama run hf.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:UD-Q4_K_XL\nTemperature of: 0.15\nSet top_p to: 1.00\nMax tokens (context length): 128K\nFine-tune Mistral v0.3 (7B) for free using our Google Colab notebook here!\nView the rest of our notebooks in our docs here.\nMistral-Small-3.2-24B-Instruct-2506\nMistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.\nSmall-3.2 improves in the following categories:\nInstruction following: Small-3.2 is better at following precise instructions\nRepetition errors: Small-3.2 produces less infinite generations or repetitive answers\nFunction calling: Small-3.2's function calling template is more robust (see here and examples)\nIn all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503.\nKey Features\nsame as Mistral-Small-3.1-24B-Instruct-2503\nBenchmark Results\nWe compare Mistral-Small-3.2-24B to Mistral-Small-3.1-24B-Instruct-2503.\nFor more comparison against other models of similar size, please check Mistral-Small-3.1's Benchmarks'\nText\nInstruction Following / Chat / Tone\nModel\nWildbench v2\nArena Hard v2\nIF (Internal; accuracy)\nSmall 3.1 24B Instruct\n55.6%\n19.56%\n82.75%\nSmall 3.2 24B Instruct\n65.33%\n43.1%\n84.78%\nInfinite Generations\nSmall 3.2 reduces infitine generations by 2x on challenging, long and repetitive prompts.\nModel\nInfinite Generations (Internal; Lower is better)\nSmall 3.1 24B Instruct\n2.11%\nSmall 3.2 24B Instruct\n1.29%\nSTEM\nModel\nMMLU\nMMLU Pro (5-shot CoT)\nMATH\nGPQA Main (5-shot CoT)\nGPQA Diamond (5-shot CoT )\nMBPP Plus - Pass@5\nHumanEval Plus - Pass@5\nSimpleQA (TotalAcc)\nSmall 3.1 24B Instruct\n80.62%\n66.76%\n69.30%\n44.42%\n45.96%\n74.63%\n88.99%\n10.43%\nSmall 3.2 24B Instruct\n80.50%\n69.06%\n69.42%\n44.22%\n46.13%\n78.33%\n92.90%\n12.10%\nVision\nModel\nMMMU\nMathvista\nChartQA\nDocVQA\nAI2D\nSmall 3.1 24B Instruct\n64.00%\n68.91%\n86.24%\n94.08%\n93.72%\nSmall 3.2 24B Instruct\n62.50%\n67.09%\n87.4%\n94.86%\n92.91%\nUsage\nThe model can be used with the following frameworks;\nvllm (recommended): See here\ntransformers: See here\nNote 1: We recommend using a relatively low temperature, such as temperature=0.15.\nNote 2: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend to use the one provided in the SYSTEM_PROMPT.txt file.\nvLLM (recommended)\nWe recommend using this model with vLLM.\nInstallation\nMake sure to install vLLM >= 0.9.1:\npip install vllm --upgrade\nDoing so should automatically install mistral_common >= 1.6.2.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServe\nWe recommand that you use Mistral-Small-3.2-24B-Instruct-2506 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Mistral-Small-3.2-24B-Instruct-2506 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 2\nNote: Running Mistral-Small-3.2-24B-Instruct-2506 on GPU requires ~55 GB of GPU RAM in bf16 or fp16.\nTo ping the client you can use a simple Python snippet. See the following examples.\nVision reasoning\nTake leverage of the vision capabilities of Mistral-Small-3.2-24B-Instruct-2506 to take the best choice given a scenario, go catch them all !\nPython snippet\nfrom datetime import datetime, timedelta\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\n)\nprint(response.choices[0].message.content)\n# In this situation, you are playing a Pok√©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, Pok√© Balls, or Berries. Using an item could help you capture the Pidgey or heal your Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat the Pidgey quickly.\n# 3. **POK√âMON**:\n#    - **Pros**: You might have another Pok√©mon in your party that is better suited for this battle or that you want to gain experience. Switching Pok√©mon could also be a strategic move if you want to train a lower-level Pok√©mon.\n#    - **Cons**: Switching Pok√©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n# 4. **RUN**:\n#    - **Pros**: Running away could save time and conserve your Pok√©mon's health and resources. If you are in a hurry or do not need the experience or items, running away is a safe option.\n#    - **Cons**: Running away means you miss out on the experience points and potential items or money that you could gain from defeating the Pidgey. It also means you do not get the chance to capture the Pidgey if you wanted to.\n# ### Recommendation:\n# Given the significant level advantage, the best action is likely to **FIGHT**. This will allow you to quickly defeat the Pidgey, gain experience points, and potentially earn items or money. If you are concerned about Pikachu's health, you could use an item from your **BAG** to heal it before or during the battle. Running away or switching Pok√©mon does not seem necessary in this situation.\nFunction calling\nMistral-Small-3.2-24B-Instruct-2506 is excellent at function / tool calling tasks via vLLM. E.g.:\nPython snippet - easy\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png\"\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_population\",\n\"description\": \"Get the up-to-date population of a given country.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"country\": {\n\"type\": \"string\",\n\"description\": \"The country to find the population of.\",\n},\n\"unit\": {\n\"type\": \"string\",\n\"description\": \"The unit for the population.\",\n\"enum\": [\"millions\", \"thousands\"],\n},\n},\n\"required\": [\"country\", \"unit\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"description\": \"Rewrite a given text for improved clarity\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"text\": {\n\"type\": \"string\",\n\"description\": \"The input text to rewrite\",\n}\n},\n},\n},\n},\n]\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"\",\n\"tool_calls\": [\n{\n\"id\": \"bbc5b7ede\",\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n},\n}\n],\n},\n{\n\"role\": \"tool\",\n\"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n\"tool_call_id\": \"bbc5b7ede\",\n\"name\": \"rewrite\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Can you tell me what is the biggest country depicted on the map?\",\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": image_url,\n},\n},\n],\n}\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\ntools=tools,\ntool_choice=\"auto\",\n)\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# The biggest country depicted on the map is Russia.\nmessages.extend([\n{\"role\": \"assistant\", \"content\": assistant_message},\n{\"role\": \"user\", \"content\": \"What is the population of that country in millions?\"},\n])\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\ntools=tools,\ntool_choice=\"auto\",\n)\nprint(response.choices[0].message.tool_calls)\n# [ChatCompletionMessageToolCall(id='3e92V6Vfo', function=Function(arguments='{\"country\": \"Russia\", \"unit\": \"millions\"}', name='get_current_population'), type='function')]\nPython snippet - complex\nimport json\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://math-coaching.com/img/fiche/46/expressions-mathematiques.jpg\"\ndef my_calculator(expression: str) -> str:\nreturn str(eval(expression))\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"my_calculator\",\n\"description\": \"A calculator that can evaluate a mathematical expression.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"expression\": {\n\"type\": \"string\",\n\"description\": \"The mathematical expression to evaluate.\",\n},\n},\n\"required\": [\"expression\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"description\": \"Rewrite a given text for improved clarity\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"text\": {\n\"type\": \"string\",\n\"description\": \"The input text to rewrite\",\n}\n},\n},\n},\n},\n]\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Can you calculate the results for all the equations displayed in the image? Only compute the ones that involve numbers.\",\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": image_url,\n},\n},\n],\n},\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\ntools=tools,\ntool_choice=\"auto\",\n)\ntool_calls = response.choices[0].message.tool_calls\nprint(tool_calls)\n# [ChatCompletionMessageToolCall(id='CyQBSAtGh', function=Function(arguments='{\"expression\": \"6 + 2 * 3\"}', name='my_calculator'), type='function'), ChatCompletionMessageToolCall(id='KQqRCqvzc', function=Function(arguments='{\"expression\": \"19 - (8 + 2) + 1\"}', name='my_calculator'), type='function')]\nresults = []\nfor tool_call in tool_calls:\nfunction_name = tool_call.function.name\nfunction_args = tool_call.function.arguments\nif function_name == \"my_calculator\":\nresult = my_calculator(**json.loads(function_args))\nresults.append(result)\nmessages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\nfor tool_call, result in zip(tool_calls, results):\nmessages.append(\n{\n\"role\": \"tool\",\n\"tool_call_id\": tool_call.id,\n\"name\": tool_call.function.name,\n\"content\": result,\n}\n)\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\n)\nprint(response.choices[0].message.content)\n# Here are the results for the equations that involve numbers:\n# 1. \\( 6 + 2 \\times 3 = 12 \\)\n# 3. \\( 19 - (8 + 2) + 1 = 10 \\)\n# For the other equations, you need to substitute the variables with specific values to compute the results.\nInstruction following\nMistral-Small-3.2-24B-Instruct-2506 will follow your instructions down to the last letter !\nPython snippet\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": \"Write me a sentence where every word starts with the next letter in the alphabet - start with 'a' and end with 'z'.\",\n},\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\n)\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# Here's a sentence where each word starts with the next letter of the alphabet, starting from 'a' and ending with 'z':\n# \"Always brave cats dance elegantly, fluffy giraffes happily ignore jungle kites, lovingly munching nuts, observing playful quails racing swiftly, tiny unicorns vaulting while xylophones yodel zealously.\"\n# This sentence follows the sequence from A to Z without skipping any letters.\nTransformers\nYou can also use Mistral-Small-3.2-24B-Instruct-2506 with Transformers !\nTo make the best use of our model with Transformers make sure to have installed mistral-common >= 1.6.2 to use our tokenizer.\npip install mistral-common --upgrade\nThen load our tokenizer along with the model and generate:\nPython snippet\nfrom datetime import datetime, timedelta\nimport torch\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\nmodel_id, torch_dtype=torch.bfloat16\n)\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\ntokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))\ninput_ids = torch.tensor([tokenized.tokens])\nattention_mask = torch.ones_like(input_ids)\npixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0)\nimage_sizes = torch.tensor([pixel_values.shape[-2:]])\noutput = model.generate(\ninput_ids=input_ids,\nattention_mask=attention_mask,\npixel_values=pixel_values,\nimage_sizes=image_sizes,\nmax_new_tokens=1000,\n)[0]\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens) :])\nprint(decoded_output)\n# In this situation, you are playing a Pok√©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, Pok√© Balls, or Berries. Using an item could help you capture Pidgey or heal Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat Pidgey quickly.\n# 3. **POK√âMON**:\n#    - **Pros**: You might have another Pok√©mon in your party that is better suited for this battle or that you want to gain experience. Switching Pok√©mon could also be strategic if you want to train a lower-level Pok√©mon.\n#    - **Cons**: Switching Pok√©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n# 4. **RUN**:\n#    - **Pros**: Running away could be a quick way to avoid the battle altogether. This might be useful if you are trying to conserve resources or if you are in a hurry to get to another location.\n#    - **Cons**: Running away means you miss out on the experience points, items, or money that you could gain from defeating Pidgey. It also might not be the most efficient use of your time if you are trying to train your Pok√©mon.\n# ### Recommendation:\n# Given the significant level advantage, the best action to take is likely **FIGHT**. This will allow you to quickly defeat Pidgey and gain experience points for Pikachu. If you are concerned about Pikachu's health, you could use the **BAG** to heal Pikachu before or during the battle. Running away or switching Pok√©mon does not seem necessary in this situation.",
    "moonshotai/Kimi-VL-A3B-Thinking-2506": "1. Introduction\n2. Performance\n3. Usage\n3.1. Inference with VLLM (recommended)\n3.2. Inference with ü§ó Hugging Face Transformers\n4. Citation\nThis is an improved version of Kimi-VL-A3B-Thinking. Please consider using this updated model instead of the previous version.\nPlease visit our tech blog for recommended inference recipe of this model: Kimi-VL-A3B-Thinking-2506: A Quick Navigation\nüìÑ Tech Report\n|\nüìÑ Github\n|\nüí¨ Chat Web\n1. Introduction\nThis is an updated version of Kimi-VL-A3B-Thinking, with following improved abilities:\nIt Thinks Smarter while Consuming Less Tokens: The 2506 version reaches better accuracy on multimodal reasoning benchmarks: 56.9 on MathVision (+20.1), 80.1 on MathVista (+8.4), 46.3 on MMMU-Pro (+3.3), 64.0 on MMMU (+2.1), while in average requires 20% reduced thinking length.\nIt Sees Clearer with Thinking: Unlike the previous version that specializes on thinking tasks, the 2506 version can also achieve the same or even better ability on general visual perception and understanding, e.g. MMBench-EN-v1.1 (84.4), MMStar (70.4), RealWorldQA (70.0), MMVet (78.4), surpassing or matching abilties of our non-thinking model (Kimi-VL-A3B-Instruct).\nIt Extends to Video Scenarios: The new 2506 version also improves on video reasoning and understanding benchmarks. It sets new state-of-the-art for open-source models on VideoMMMU (65.2), while also retains good ability on general video understanding (71.9 on Video-MME, matching Kimi-VL-A3B-Instruct).\nIt Extends to Higher Resolution: The new 2506 version supports 3.2 million total pixels in a single image, 4X compared to the previous version. This leads to non-trivial improvements on high-resolution perception and OS-agent grounding benchmarks: 83.2 on V* Benchmark (without extra tools), 52.8 on ScreenSpot-Pro, 52.5 on OSWorld-G (full set with refusal).\n2. Performance\nComparison with efficient models and two previous versions of Kimi-VL (*Results of GPT-4o is for reference here, and shown in italics):\nBenchmark (Metric)\nGPT-4o\nQwen2.5-VL-7B\nGemma3-12B-IT\nKimi-VL-A3B-Instruct\nKimi-VL-A3B-Thinking\nKimi-VL-A3B-Thinking-2506\nGeneral Multimodal\nMMBench-EN-v1.1 (Acc)\n83.1\n83.2\n74.6\n82.9\n76.0\n84.4\nRealWorldQA (Acc)\n75.4\n68.5\n59.1\n68.1\n64.0\n70.0\nOCRBench (Acc)\n815\n864\n702\n864\n864\n869\nMMStar (Acc)\n64.7\n63.0\n56.1\n61.7\n64.2\n70.4\nMMVet (Acc)\n69.1\n67.1\n64.9\n66.7\n69.5\n78.1\nReasoning\nMMMU (val, Pass@1)\n69.1\n58.6\n59.6\n57.0\n61.7\n64.0\nMMMU-Pro (Pass@1)\n51.7\n38.1\n32.1\n36.0\n43.2\n46.3\nMath\nMATH-Vision (Pass@1)\n30.4\n25.0\n32.1\n21.7\n36.8\n56.9\nMathVista_MINI (Pass@1)\n63.8\n68.0\n56.1\n68.6\n71.7\n80.1\nVideo\nVideoMMMU (Pass@1)\n61.2\n47.4\n57.0\n52.1\n55.5\n65.2\nMMVU (Pass@1)\n67.4\n50.1\n57.0\n52.7\n53.0\n57.5\nVideo-MME (w/ sub.)\n77.2\n71.6\n62.1\n72.7\n66.0\n71.9\nAgent Grounding\nScreenSpot-Pro (Acc)\n0.8\n29.0\n‚Äî\n35.4\n‚Äî\n52.8\nScreenSpot-V2 (Acc)\n18.1\n84.2\n‚Äî\n92.8\n‚Äî\n91.4\nOSWorld-G (Acc)\n-\n31.5\n‚Äî\n41.6\n‚Äî\n52.5\nLong Document\nMMLongBench-DOC (Acc)\n42.8\n29.6\n21.3\n35.1\n32.5\n42.1\nComparison with 30B-70B open-source models:\nBenchmark (Metric)\nKimi-VL-A3B-Thinking-2506\nQwen2.5-VL-32B\nQwen2.5-VL-72B\nGemma3-27B-IT\nGeneral Multimodal\nMMBench-EN-v1.1 (Acc)\n84.4\n-\n88.3\n78.9\nRealWorldQA (Acc)\n70.0\n-\n75.7\n62.5\nOCRBench (Acc)\n869\n-\n885\n753\nMMStar (Acc)\n70.4\n69.5\n70.8\n63.1\nMMVet (Acc)\n78.1\n-\n74.0\n71.0\nReasoning\nMMMU (val, Pass@1)\n64.0\n70.0\n70.2\n64.9\nMMMU-Pro (Pass@1)\n46.3\n49.5\n51.1\n-\nMATH-Vision (Pass@1)\n56.9\n38.4\n38.1\n35.4\nMathVista_MINI (Pass@1)\n80.1\n74.7\n74.8\n59.8\nVideo\nVideoMMMU (Pass@1)\n65.2\n-\n60.2\n61.8\nMMVU (Pass@1)\n57.5\n-\n62.9\n61.3\nVideo-MME (w/ sub.)\n71.9\n70.5/77.9\n73.3/79.1\n-\nAgent Grounding\nScreenSpot-Pro (Acc)\n52.8\n39.4\n43.6\n-\nScreenSpot-V2 (Acc)\n91.4\n-\n-\n-\nOSWorld-G (Acc)\n52.5\n46.5\n-\n-\nLong Document\nMMLongBench-DOC (Acc)\n42.1\n-\n38.8\n-\nText results, comparison with 30B-level non-thinking VLMs:\nBenchmark (Metric)\nKimi-VL-A3B-Thinking-2506\nQwen2.5-VL-32B\nGemma3-27B-IT\nMMLU\n82.0\n78.4\n76.9\nMMLU-Pro\n68.5\n68.8\n67.5\nMATH\n91.8\n82.2\n89.0\nGPQA-Diamond\n42.3\n46.0\n46.0\n3. Usage\n3.1. Inference with VLLM (recommended)\nAs a long-decode model that will generates up to 32K tokens, we recommend using VLLM for inference, which has already supported Kimi-VL series.\nMAX_JOBS=4 pip install vllm==0.9.1 blobfile flash-attn --no-build-isolation\nIt is important to explicitly install flash-attn to avoid CUDA out-of-memory.\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nmodel_path = \"moonshotai/Kimi-VL-A3B-Thinking-2506\"\nllm = LLM(\nmodel_path,\ntrust_remote_code=True,\nmax_num_seqs=8,\nmax_model_len=131072,\nlimit_mm_per_prompt={\"image\": 256}\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nsampling_params = SamplingParams(max_tokens=32768, temperature=0.8)\nimport requests\nfrom PIL import Image\ndef extract_thinking_and_summary(text: str, bot: str = \"‚óÅthink‚ñ∑\", eot: str = \"‚óÅ/think‚ñ∑\") -> str:\nif bot in text and eot not in text:\nreturn \"\"\nif eot in text:\nreturn text[text.index(bot) + len(bot):text.index(eot)].strip(), text[text.index(eot) + len(eot) :].strip()\nreturn \"\", text\nOUTPUT_FORMAT = \"--------Thinking--------\\n{thinking}\\n\\n--------Summary--------\\n{summary}\"\nurl = \"https://huggingface.co/spaces/moonshotai/Kimi-VL-A3B-Thinking/resolve/main/images/demo6.jpeg\"\nimage = Image.open(requests.get(url,stream=True).raw)\nmessages = [\n{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"\"}, {\"type\": \"text\", \"text\": \"What kind of cat is this? Answer with one word.\"}]}\n]\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = llm.generate([{\"prompt\": text, \"multi_modal_data\": {\"image\": image}}], sampling_params=sampling_params)\ngenerated_text = outputs[0].outputs[0].text\nthinking, summary = extract_thinking_and_summary(generated_text)\nprint(OUTPUT_FORMAT.format(thinking=thinking, summary=summary))\n3.2. Inference with ü§ó Hugging Face Transformers\nWe introduce how to use our model at inference stage using transformers library. It is recommended to use python=3.10, torch>=2.1.0, and transformers=4.48.2 as the development environment.\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, AutoProcessor\ndef extract_thinking_and_summary(text: str, bot: str = \"‚óÅthink‚ñ∑\", eot: str = \"‚óÅ/think‚ñ∑\") -> str:\nif bot in text and eot not in text:\nreturn \"\"\nif eot in text:\nreturn text[text.index(bot) + len(bot):text.index(eot)].strip(), text[text.index(eot) + len(eot) :].strip()\nreturn \"\", text\nOUTPUT_FORMAT = \"--------Thinking--------\\n{thinking}\\n\\n--------Summary--------\\n{summary}\"\nurl = \"https://huggingface.co/spaces/moonshotai/Kimi-VL-A3B-Thinking/resolve/main/images/demo6.jpeg\"\nmodel_path = \"moonshotai/Kimi-VL-A3B-Thinking-2506\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nimage_paths = [url]\nimages = [Image.open(path) for path in image_paths]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": image_path} for image_path in image_paths\n] + [{\"type\": \"text\", \"text\": \"What kind of cat is this? Answer with one word.\"}],\n},\n]\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\ninputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=32768, temperature=0.8)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nresponse = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(response)\n4. Citation\n@misc{kimiteam2025kimivltechnicalreport,\ntitle={{Kimi-VL} Technical Report},\nauthor={Kimi Team and Angang Du and Bohong Yin and Bowei Xing and Bowen Qu and Bowen Wang and Cheng Chen and Chenlin Zhang and Chenzhuang Du and Chu Wei and Congcong Wang and Dehao Zhang and Dikang Du and Dongliang Wang and Enming Yuan and Enzhe Lu and Fang Li and Flood Sung and Guangda Wei and Guokun Lai and Han Zhu and Hao Ding and Hao Hu and Hao Yang and Hao Zhang and Haoning Wu and Haotian Yao and Haoyu Lu and Heng Wang and Hongcheng Gao and Huabin Zheng and Jiaming Li and Jianlin Su and Jianzhou Wang and Jiaqi Deng and Jiezhong Qiu and Jin Xie and Jinhong Wang and Jingyuan Liu and Junjie Yan and Kun Ouyang and Liang Chen and Lin Sui and Longhui Yu and Mengfan Dong and Mengnan Dong and Nuo Xu and Pengyu Cheng and Qizheng Gu and Runjie Zhou and Shaowei Liu and Sihan Cao and Tao Yu and Tianhui Song and Tongtong Bai and Wei Song and Weiran He and Weixiao Huang and Weixin Xu and Xiaokun Yuan and Xingcheng Yao and Xingzhe Wu and Xinxing Zu and Xinyu Zhou and Xinyuan Wang and Y. Charles and Yan Zhong and Yang Li and Yangyang Hu and Yanru Chen and Yejie Wang and Yibo Liu and Yibo Miao and Yidao Qin and Yimin Chen and Yiping Bao and Yiqin Wang and Yongsheng Kang and Yuanxin Liu and Yulun Du and Yuxin Wu and Yuzhi Wang and Yuzi Yan and Zaida Zhou and Zhaowei Li and Zhejun Jiang and Zheng Zhang and Zhilin Yang and Zhiqi Huang and Zihao Huang and Zijia Zhao and Ziwei Chen},\nyear={2025},\neprint={2504.07491},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2504.07491},\n}",
    "NUTN-KWS/Whisper-Taiwanese-model-v0.5": "üë≥ Whisper-Taiwanese model V0.5 (Tv0.5)\nüìù Model Details\nüöÄ Usage\nÂÆâË£ùÂ•ó‰ª∂:\nÂü∑Ë°åÁØÑ‰æã:\nüë®‚Äçüéì Citation\nBibTeX:\nAPA:\n[ Ëã±Êñá README_EN.md ]\nüë≥ Whisper-Taiwanese model V0.5 (Tv0.5)\nÈÄôÂÄãÊ®°ÂûãÊòØÁî±ÂúãÁ´ãËá∫ÂçóÂ§ßÂ≠∏Âü∑Ë°åÂúãÁßëÊúÉÁî¢Â≠∏Âêà‰ΩúË®àÁï´Ôºå‰ΩøÁî® openai/whisper-large-v3-turbo ÂæÆË™øÁöÑÁâàÊú¨Ôºå‰∏¶Âü∑Ë°åÂúãÁßëÊúÉTAIDEÂè∞Ëã±Ë™ûÂÆ∂Â∫≠ÂÖàÂ∞éË®àÁï´ÔºåËàáÁúüÂπ≥Âá∫ÁâàÁ§æÂêà‰ΩúÔºå‰ΩøÁî®‰∏≠Â∞èÂ≠∏ÊïôÊùêÂÖßÂÆπÂèäÂ≠∏ÁîüÂ≠∏ÁøíË≥áÊñôÈÄ≤Ë°åÊ®°ÂûãÂæÆË™øÔºåÁî®ÊñºÁúüÂπ≥ÊïôÊùêÂè∞Ë™ûËæ®Ë≠ò„ÄÇ‰∏¶ËàáÂúãÁ†îÈô¢ÂúãÁ∂≤‰∏≠ÂøÉÂêà‰ΩúÔºåÈÅãÁî®ÂúãÁ∂≤‰∏≠ÂøÉÁÆóÂäõ‰ª•ÂèäTAIDEÊ®°ÂûãÔºåÂÖ±ÂêåÂª∫Êßã‰∏≠Â∞èÂ≠∏Âè∞Ë™ûAIÂ≠∏ÁøíÊ®°Âûã„ÄÇ\nÁ§∫ÁØÑÁ∂≤ÂùÄ: https://kws.oaselab.org/taigitong/\nüìù Model Details\nBase Model: openai/whisper-large-v3-turbo\nFine-tuned for: Âè∞ÁÅ£Èñ©ÂçóË™ûË™ûÈü≥Ëæ®Ë≠ò (ASR)\nFine-tuning Framework: Hugging Face Transformers\nTraining Duration: ‰ΩøÁî®ÂÖ©Áâá V100ÔºåÂ§ßÁ¥Ñ 180 Â∞èÊôÇ\nDataset: Ëá™Ë®ÇË≥áÊñôÈõÜ„ÄÅÊïôËÇ≤ÈÉ®Ëá∫ÁÅ£Âè∞Ë™ûÂ∏∏Áî®Ë©ûËæ≠ÂÖ∏ÔºåÂ§ßÁ¥Ñ 90 Â∞èÊôÇÁöÑË≥áÊñô\nInput Format: 16kHz mono WAV\nLicense: CC BY-NC 4.0\nüöÄ Usage\nÂÆâË£ùÂ•ó‰ª∂:\npip install torch torchvision torchaudio transformers\nÂü∑Ë°åÁØÑ‰æã:\nfrom transformers import pipeline\npipe = pipeline(\"automatic-speech-recognition\", model=\"./model/whisper-taiwanese\", device=0)\nresult = pipe(\"audio.wav\", generate_kwargs={\"language\": \"zh\", \"task\": \"transcribe\"})\nprint(result[\"text\"])\nüë®‚Äçüéì Citation\nBibTeX:\n@misc{taiwanesewhisperasr2025,\ntitle={Taiwanese Whisper ASR},\nauthor={KWS Center, National University of Tainan, Taiwan},\nyear={2025},\nurl={https://huggingface.co/NUTN-KWS/Whisper-Taiwanese-model-v0.5}\n}\nAPA:\nC. S. Lee, M. H. Wang, C. C. Yue, G. Y. Teseng, and Y. Nojima, \"Fuzzy Estimation Agent with Knowledge Graph and Quantum Fuzzy Inference Engine for Taiwanese-English Co-Learning,\" 2025 IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS 2025), Banff, Alberta, Canada, Aug. 16-19, 2025.\nC. S. Lee, M. H. Wang, C. Y. Chen, S. C. Yang, M. Reformat, N. Kubota, and A. Pourabdollah, \"Integrating quantum CI and generative AI for Taiwanese/English co-learning,\" Quantum Machine Intelligence, vol. 6, 64, pp. 1-19, 2024.\nC. S. Lee, M. H. Wang, C. Y. Chen, S. C. Yang, M. Reformat, N. Kubota, and A. Pourabdollah, \"Quantum fuzzy inference engine with generative AI and TAIDE KG for Taiwanese/English co-learning,\" 2025 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2025), Reims, France, Jul. 6-9, 2025.",
    "Skywork/Matrix-3D": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nüåü Introduction\nüì¶ Installation\nüí´ Pretrained Models\nüéÆ Usage\nüé¨ Create Your Own\nüìö Citation\nü§ù Acknowledgements\nüìß Contact\nMatrix3D: Omnidirectional Explorable 3D World Generation\nüåü Introduction\nMatrix-3D utilizes panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction.\nLarge-Scale Scene Generation : Compared to existing scene generation approaches, Matrix-3D supports the generation of broader, more expansive scenes that allow for complete 360-degree free exploration.\nHigh Controllability : Matrix-3D supports both text and image inputs, with customizable trajectories and infinite extensibility.\nStrong Generalization Capability : Built upon self-developed 3D data and video model priors, Matrix-3D enables the generation of diverse and high-quality 3D scenes.\nSpeed-Quality Balance: Two types of panoramic 3D reconstruction methods are proposed to achieve rapid and detailed 3D reconstruction respectively.\nüì¶ Installation\nClone the repo and create the environment:\n# Clone the repository\ngit clone --recursive https://github.com/SkyworkAI/Matrix-3D.git\ncd Matrix-3D\n# Create a new conda environment\nconda create -n matrix3d python=3.10\nconda activate matrix3d\n# Install torch and torchvision (with GPU support, we use CUDA 12.4 Version)\npip3 install torch==2.7.1 torchvision==0.22.1\n#Run installation script\nchmod +x install.sh\n./install.sh\nüí´ Pretrained Models\nModel Name\nDescription\nDownload\nText2PanoImage\ntext2panoimage_lora.safetensors\nLink\nPanoVideoGen-480p\npano_video_gen_480p.ckpt\nLink\nPanoVideoGen-720p\npano_video_gen_720p.bin\nLink\nPanoLRM-480p\npano_lrm_480p.pt\nLink\nüéÆ Usage\nüîß Checkpoint Download\npython scripts/download_checkpoints.py\nüî• One-command 3D World Generation\nNow you can generate a 3D world by just running a single command:\n./generate.sh\nOr you can choose to generate a 3D world step by step.\nüñºÔ∏è Step 1: Text/Image to Panorama Image\nYou can either generate a panorama image from text prompt:\npython code/panoramic_image_generation.py \\\n--mode=t2p \\\n--prompt=\"a vibrant, industrial-style scene, featuring a large warehouse with exposed brick walls, metal beams, and scattered barrels and crates, set against a backdrop of modern skyscrapers and lush greenery\" \\\n--output_path=\"./output/example1\"\nOr from an input image:\npython code/panoramic_image_generation.py \\\n--mode=i2p \\\n--input_image_path=\"./data/image1.jpg\" \\\n--output_path=\"./output/example1\"\nThe generated panorama image will be saved in the output/example1 folder.\nüìπ Step 2: Generate Panoramic Video\nVISIBLE_GPU_NUM=1\ntorchrun --nproc_per_node ${VISIBLE_GPU_NUM} code/panoramic_image_to_video.py \\\n--inout_dir=\"./output/example1\"  \\\n--resolution=720\nYou can switch the resolution option in [480,720] to perform video generation under 960 √ó 480 resolution or 1440 √ó 720 resolution.\nThe generated panoramic tour video will be saved in output/example1/pano_video.mp4. It will take about an hour to generate a 720p video on an A800 GPU. You can accelerate this process with multi-gpu inference by setting VISIBLE_GPU_NUM.\nüè° Step 3: Extract 3D Scene\nHere we provide two options, one is high-quality optimization-based 3D scene reconstruction and another is efficient feed-forward 3D scene reconstruction.\nTo perform optimization-based reconstruction, run\npython code/panoramic_video_to_3DScene.py \\\n--inout_dir=\"./output/example1\" \\\n--resolution=720\nModify the resolution option as the value used in panoramic video generation.\nThe extracted 3D scene in .ply format will be saved in output/example1/generated_3dgs_opt.ply.\nTo perform feed-forward reconstruction, run\npython code/panoramic_video_480p_to_3DScene_lrm.py \\\n--video_path=\"./data/case1/sample_video.mp4\" \\\n--pose_path='./data/case1/sample_cam.json' \\\n--out_path='./output/example2'\nThe extracted 3D scene in .ply format and rendered perspective videos will be saved output/example2.\nIf you want to reconstruct 3D scene with another panorama videos and conditioned camera pose, just replace the video_path and pose_path accordingly.\nüé¨ Create Your Own\nWe provide three movement modes: Straight Travel, S-curve Travel, and Forward on the Right, which can be configured in --movement_mode in code/panoramic_image_to_video.py.\nYou can also provide your own camera trajectory in .json format and use it for video generation.\nVISIBLE_GPU_NUM=1\ntorchrun --nproc_per_node ${VISIBLE_GPU_NUM} code/panoramic_image_to_video.py \\\n--inout_dir=\"./output/example1\"  \\\n--resolution=720\n--json_path YOUR_TRAJECTORY_FILE.json\nAll camera matrices used in our project are world to camera matrices in opencv format. Please refer to the sample file ./data/test_cameras/test_cam_front.json, and use code/generate_example_camera.py to generate your own camera trajectory.\nüìö Citation\nIf you find this project useful, please consider citing it as follows:\n@article{yang2025matrix3d,\ntitle     = {Matrix-3D: Omnidirectional Explorable 3D World Generation},\nauthor    = {Zhongqi Yang and Wenhang Ge and Yuqi Li and Jiaqi Chen and Haoyuan Li and Mengyin An and Fei Kang and Hua Xue and Baixin Xu and Yuyang Yin and Eric Li and Yang Liu and Yikai Wang and Hao-Xiang Guo and Yahui Zhou},\njournal   = {arXiv preprint arXiv:2508.08086},\nyear      = {2025}\n}\nü§ù Acknowledgements\nThis project is built on top of the follows, please consider citing them if you find them useful:\nFLUX.1\nWan2.1\nWorldGen\nMoGe\nnvdiffrast\ngaussian-splatting\nStableSR\nVEnhancer\nüìß Contact\nIf you have any questions, please feel free post an issue.",
    "kernels-community/flash-attn3": "Flash Attention 3\nFlash Attention 3\nFlash Attention is a fast and memory-efficient implementation of the\nattention mechanism, designed to work with large models and long sequences.\nThis is a Hugging Face compliant kernel build of Flash Attention.\nOriginal code here https://github.com/Dao-AILab/flash-attention.\nKernel source: https://github.com/huggingface/kernels-community/tree/main/flash-attn3",
    "unsloth/gemma-3n-E2B-it-GGUF": "ü¶• Fine-tune Gemma 3n with Unsloth\nGemma-3n-E2B model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nLearn how to run & fine-tune Gemma 3n correctly - Read our Guide.\nSee our collection for all versions of Gemma 3n including GGUF, 4-bit & 16-bit formats.\nUnsloth Dynamic 2.0 achieves SOTA accuracy & performance versus other quants.\n‚ú® Gemma 3n Usage Guidelines\nCurrently only text is supported.\nOllama: ollama run hf.co/unsloth/gemma-3n-E4B-it-GGUF:Q4_K_XL - auto-sets correct chat template and settings\nSet temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0\nGemma 3n max tokens (context length): 32K. Gemma 3n chat template:\n<bos><start_of_turn>user\\nHello!<end_of_turn>\\n<start_of_turn>model\\nHey there!<end_of_turn>\\n<start_of_turn>user\\nWhat is 1+1?<end_of_turn>\\n<start_of_turn>model\\n\nFor complete detailed instructions, see our step-by-step guide.\nü¶• Fine-tune Gemma 3n with Unsloth\nFine-tune Gemma 3n (4B) for free using our Google Colab notebook here!\nRead our Blog about Gemma 3n support: unsloth.ai/blog/gemma-3n\nView the rest of our notebooks in our docs here.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nGemma-3n-E4B\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nGRPO with Gemma 3 (1B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n80% less\nGemma 3 (4B) Vision\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen3 (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nDeepSeek-R1-0528-Qwen3-8B (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nGemma-3n-E2B model card\nModel Page: Gemma 3n\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on HuggingFace\nGemma on Vertex Model Garden\nTerms of Use: TermsAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3n models are designed for efficient execution on low-resource devices.\nThey are capable of multimodal input, handling text, image, video, and audio\ninput, and generating text outputs, with open weights for pre-trained and\ninstruction-tuned variants. These models were trained with data in over 140\nspoken languages.\nGemma 3n models use selective parameter activation technology to reduce resource\nrequirements. This technique allows the models to operate at an effective size\nof 2B and 4B parameters, which is lower than the total number of parameters they\ncontain. For more information on Gemma 3n's efficient parameter management\ntechnology, see the\nGemma 3n\npage.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be\nsummarized\nImages, normalized to 256x256, 512x512, or 768x768 resolution\nand encoded to 256 tokens each\nAudio data encoded to 6.25 tokens per second from a single channel\nTotal input context of 32K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output length up to 32K tokens, subtracting the request\ninput tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running\nthe model. First, install the Transformers library. Gemma 3n is supported\nstarting from transformers 4.53.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nYou can initialize the model and processor for inference with pipeline as\nfollows.\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/gemma-3n-e4b-it\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16,\n)\nWith instruction-tuned models, you need to use chat templates to process our\ninputs first. Then, you can pass it to the pipeline.\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n{\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\nRunning the model on a single GPU\nfrom transformers import AutoProcessor, Gemma3nForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/gemma-3n-e4b-it\"\nmodel = Gemma3nForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\nCitation\n@article{gemma_3n_2025,\ntitle={Gemma 3n},\nurl={https://ai.google.dev/gemma/docs/gemma-3n},\npublisher={Google DeepMind},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset that includes a wide variety of sources\ntotalling approximately 11 trillion tokens. The knowledge cutoff date for the\ntraining data was June 2024. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model\nis exposed to a broad range of linguistic styles, topics, and vocabulary.\nThe training dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn\nlogical reasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nAudio: A diverse set of sound samples enables the model to recognize\nspeech, transcribe text from recordings, and identify information in audio data.\nThe combination of these diverse data sources is crucial for training a\npowerful multimodal model that can handle a wide variety of different tasks and\ndata formats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material)\nfiltering was applied at multiple stages in the data preparation process to\nensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit\n(TPU) hardware (TPUv4p, TPUv5p\nand TPUv5e). Training generative models requires significant computational\npower. TPUs, designed specifically for matrix operations common in machine\nlearning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training generative models. They can speed up\ntraining considerably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and\nML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models:\n\"the 'single controller' programming model of Jax and Pathways allows a single\nPython process to orchestrate the entire training run, dramatically simplifying\nthe development workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with IT are for\ninstruction-tuned models. Evaluation results marked with PT are for\npre-trained models.\nReasoning and factuality\nBenchmark\nMetric\nn-shot\nE2B PT\nE4B PT\nHellaSwag\nAccuracy\n10-shot\n72.2\n78.6\nBoolQ\nAccuracy\n0-shot\n76.4\n81.6\nPIQA\nAccuracy\n0-shot\n78.9\n81.0\nSocialIQA\nAccuracy\n0-shot\n48.8\n50.0\nTriviaQA\nAccuracy\n5-shot\n60.8\n70.2\nNatural Questions\nAccuracy\n5-shot\n15.5\n20.9\nARC-c\nAccuracy\n25-shot\n51.7\n61.6\nARC-e\nAccuracy\n0-shot\n75.8\n81.6\nWinoGrande\nAccuracy\n5-shot\n66.8\n71.7\nBIG-Bench Hard\nAccuracy\nfew-shot\n44.3\n52.9\nDROP\nToken F1 score\n1-shot\n53.9\n60.8\nMultilingual\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMGSM\nAccuracy\n0-shot\n53.1\n60.7\nWMT24++ (ChrF)\nCharacter-level F-score\n0-shot\n42.7\n50.1\nInclude\nAccuracy\n0-shot\n38.6\n57.2\nMMLU (ProX)\nAccuracy\n0-shot\n8.1\n19.9\nOpenAI MMLU\nAccuracy\n0-shot\n22.3\n35.6\nGlobal-MMLU\nAccuracy\n0-shot\n55.1\n60.3\nECLeKTic\nECLeKTic score\n0-shot\n2.5\n1.9\nSTEM and code\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nGPQA Diamond\nRelaxedAccuracy/accuracy\n0-shot\n24.8\n23.7\nLiveCodeBench v5\npass@1\n0-shot\n18.6\n25.7\nCodegolf v2.2\npass@1\n0-shot\n11.0\n16.8\nAIME 2025\nAccuracy\n0-shot\n6.7\n11.6\nAdditional benchmarks\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMMLU\nAccuracy\n0-shot\n60.1\n64.9\nMBPP\npass@1\n3-shot\n56.6\n63.6\nHumanEval\npass@1\n0-shot\n66.5\n75.0\nLiveCodeBench\npass@1\n0-shot\n13.2\n13.2\nHiddenMath\nAccuracy\n0-shot\n27.7\n37.7\nGlobal-MMLU-Lite\nAccuracy\n0-shot\n59.0\n64.5\nMMLU (Pro)\nAccuracy\n0-shot\n40.5\n50.6\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models'\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: Generate creative text formats such as\npoems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational\ninterfaces for customer service, virtual assistants, or interactive\napplications.\nText Summarization: Generate concise summaries of a text\ncorpus, research papers, or reports.\nImage Data Extraction: Extract, interpret, and summarize\nvisual data for text communications.\nAudio Data Extraction: Transcribe spoken language, translate speech\nto text in other languages, and analyze sound-based data.\nResearch and Education\nNatural Language Processing (NLP) and generative model\nResearch: These models can serve as a foundation for researchers to\nexperiment with generative models and NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language\nlearning experiences, aiding in grammar correction or providing writing\npractice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of data by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of generative models raises several ethical concerns. In\ncreating an open model, we have carefully considered the following:\nBias and Fairness\nGenerative models trained on large-scale, real-world text and image data\ncan reflect socio-cultural biases embedded in the training material.\nThese models underwent careful scrutiny, input data pre-processing\ndescribed and posterior evaluations reported in this card.\nMisinformation and Misuse\nGenerative models can be misused to generate text that is\nfalse, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making generative model technology accessible to\ndevelopers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\ngenerative models. Educational resources and reporting mechanisms for users\nto flag misuse are provided. Prohibited uses of Gemma models are outlined\nin the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of\ncertain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\ngenerative model implementations designed from the ground up for responsible AI\ndevelopment compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "goonsai-com/civitaiprompts": "Help\nExample\nLimitations\nIssues\nFAQ\nSupport\nModels / Ollama\nThese are based models trained on Civitai top uncensored prompts that is used by telegram bot @goonspromptbot by goonsai.com Video generation\nIt is a combination of an abliterated model which makes it totally uncensored and then trained on the specific task to write image or video generation prompts.\nHelp\nThis is a Completion style model. Meaning you need provide a few words of what you need and the model will try to complete with as much information as possible. Depending on the interface you use, you could improve it with a template though not required.\nExample\na woman dancing in swimsuit\nthen you will get the rest of the prompt.\nquality, high resolution, elegant pose, shimmering lighting, beach setting, tropical breeze, smooth motion, round belly, long legs, head tilted slightly to one side, eyes closed gently, smile on face, water droplets on skin\nYou can roll the dice on output or you can follow up to refine it. Don‚Äôt be worried about strange formatting as long as the keywords are there. The models dont care about grammer, its about tokens.\nYou can always edit the template for this Modefile and you can join the discord Discord Channel\nLimitations\nThere are situations when the model will spit out trigger w0rds and LoRA names of what could be deleted LoRAs.\nNote that the default model already includes a simple system message so if you dont provide one, it will still work.\nThe existing template is included in modelfiles so you can adapt the template for image generation or video.\nModels are updated regularly with more training data\nIssues\nQwen3 model is experimental and for development only. Likely you do not need it.\nThe default template assumes you are generating video. You have to create your own system prompt if you want to generate images.\nFAQ\nCan it generate prompt from images / videos, i.e. video/image -> text ?\nNo. Until I train a vision model that is not possible.\nIt does not work with XYZ interface.\nI have added GGUF files in the model folder. I am not familiar with all the tools since I run this directly with Python code or Ollama for testing. I use OpenWebUI only because it is somewhat helpful in development and testing. Not really a resounding endorsement.\nSupport\nr/goonsai\nModels / Ollama\nNSFW-small 10K model for older GPUs, might even work without GPU or a laptop one.\nNSFW-Large 100K model for 8GB VRAM",
    "l0wgear/manga-ocr-2025-onnx": "Manga OCR (ONNX)\nOriginal Model Information\nUsing the ONNX Models\nAcknowledgements\nManga OCR (ONNX)\nThis is an ONNX version of the Manga OCR model, designed for optical character recognition of Japanese text, with a primary focus on manga.\nThis model is based on the original work by kha-white/manga-ocr, kha-white/manga-ocr-base and modification by jzhang533/manga-ocr-base-2025. The models in this repository were exported to the ONNX format using Hugging Face Optimum.\nOriginal Model Information\nManga OCR utilizes the Vision Encoder Decoder framework. It is designed to be a high-quality text recognition tool, robust against various scenarios specific to manga:\nBoth vertical and horizontal text\nText with furigana\nText overlaid on images\nA wide variety of fonts and font styles\nLow-quality images\nThe original training data included manga109-s and synthetic data.\nUsing the ONNX Models\nTo use these ONNX models for inference, you will need the optimum library. You can install it as follows:\npip install optimum[onnxruntime]\nHere is an example of how to run inference with the ONNX models:\nfrom transformers import TrOCRProcessor\nfrom optimum.onnxruntime import ORTModelForVision2Seq\nfrom PIL import Image\n# Load the processor and model\nprocessor = TrOCRProcessor.from_pretrained(\"l0wgear/manga-ocr-2025-onnx\")\nmodel = ORTModelForVision2Seq.from_pretrained(\"l0wgear/manga-ocr-2025-onnx\")\n# Load an image\nimage = Image.open(\"path/to/your/manga/image.jpg\").convert(\"RGB\")\n# Process the image and generate text\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(generated_text)\nAcknowledgements\nOriginal Author: kha-white for creating the original Manga OCR.\nFine-tuning: jzhang533 for training the manga-ocr-base-2025 model.",
    "tngtech/DeepSeek-TNG-R1T2-Chimera": "DeepSeek-TNG-R1T2-Chimera\nAssembly of Experts Chimera model constructed with the DeepSeek R1-0528, R1 and V3-0324 parent models\nTechnological background\nFunction calling\nModel Details\nUse, Out-of-scope Use, Other Limitations, Risks, Recommendations et al.\nEU AI Act\nContact, especially for your user feedback\nCitation\nDeepSeek-TNG-R1T2-Chimera\nRelease Announcement on X\nAssembly of Experts Chimera model constructed with the DeepSeek R1-0528, R1 and V3-0324 parent models\nWe present our new DeepSeek-TNG R1T2 Chimera 671B model, the first successor to our original DeepSeek R1T Chimera that was released on April 26th. Unlike the original Chimera, which was based on the two parent models V3-0324 and R1, the new Chimera is a Tri-Mind with three parents, namely additionally R1-0528. It is constructed using the Assembly of Experts-method with relatively fine-granular direct brain edits. This more refined assembly allowed, among other improvements, the fixing of the <think> token consistency issue, which was a weakness of R1T and is now solved for R1T2.\nSweet spot\nR1T2 operates at a new sweet spot in intelligence vs. output token length. It appears to be...\nabout 20% faster than the regular R1, and more than twice as fast as R1-0528\nsignificantly more intelligent than the regular R1 in benchmarks such as GPQA, AIME-24 and Aider Polyglot\nmuch more intelligent and also think-token consistent compared to the first R1T Chimera 0426\nand generally well-behaved and a nice persona to talk to, even without any system prompt.\nRecommendations for your model decision\nR1T2 compared...\nvs R1: We hope that R1T2 is a very desirable, almost universally better drop-in replacement for R1\nvs R1-0528: R1T2 is a much cheaper alternative to the full R1-0528, if the full 0528-level intelligence is not required\nvs R1T: R1T2 is usually recommended over R1T, unless the specific personality of R1T was optimal, the think-token issue not important, or R1T's higher speed crucial\nvs V3-0324: V3 is so much faster that if you can live with the lower intelligence, take V3, however, if you need reasoning, R1T2 is the go-to model\nLimitations\nR1-0528 is thinking much longer, but also is achieving better hard benchmark results than R1T2\nAs measured by SpeechMap.ai (courtesy of xlr8harder), R1T2 is significantly more reserved than R1T, but not as much as R1-0528\nWhen switching from R1T to R1T2 development, we changed from AIME24 and MT-Bench to AIME24, AIME25 and GPQA-Diamond for the intelligence score. With the new benchmark set, there is a larger score difference between R1 and the original R1T Chimera than published earlier.\nFunction calling is supported in general, but both vLLM and SGLang currently require some specific adaptions, see the section below.\nEvaluation results\nEvaluation was performed using the evalchemy framework (pass@1 averaged over 10/5 runs for AIME/GPQAD, at a temperature of 0.6).\nWe report measured benchmark results for our R1T2, R1T models and published benchmark results for V3-0324, R1, R1-0528.\nR1T2\nR1T\nV3-0324\nR1\nR1-0528\nComment\nSpecial source\nAIME-24\n82.3\n74.7\n59.4\n79.8\n91.4\nAIME-25\n70.0\n58.3\n49.6\n70.0\n87.5\nV3-0324 AIME-25 measured by us\nGPQA-Diamond\n77.9\n72.0\n68.4\n71.5\n81.0\nAider Polyglot\n64.4\n48.4\n44.9\n52.0\n71.6\nR1T2 beats two of its parents, V3-0324 and R1, and was measured to be about 2.2 times more token efficient, i.e. faster, than its third parent, R1-0528\nR1T2 source: Aider discord, t=0.75\nMMLU-Pro Computer Science\n83.7-85.6\n82.9-84.6\n81.5-82.4\n85.1-85.3\n84.6-86.1\nEQ-Bench Longform Creative Writing\n76.4\n./.\n78.1\n74.6\n78.9\nEQ Bench version before August 8th, 2025\nsee EQ Bench\nVectara Hallucination Rate\n5.5\n./.\n8.0\n14.3\n7.7\nlower hallucination rates are better, R1T2 is better than all its three parents\nsee Hallucination Leaderboard\nTechnological background\nFor details on the AoE construction process, you can read our Paper on arXiV.\nRuntime parameter settings\nMost of our evaluation was done with a maximum context size of 60,000 tokens.\nWith a context size of 130,000 tokens, the model proved very helpful in interpreting very long debug logs. Long-context testing was less extensive, though.\nWe're running the model using vLLM on 8xH200 and MI325X nodes, additionally we've tested the model using SGLang, which is also used by chutes.ai.\nFor SGLang, we recommend using versions >= v0.4.8 in combination with argument --reasoning-parser qwen3 to properly handle rare cases when the model skips the <think> reasoning step.\nFunction calling\nR1T2 does support function calling using an updated chat template (since 01 Aug 2025). However, neither vLLM nor SGLang provide an R1T2-compatible tool call parser natively but require some adaptions.\nvLLM:\nFor function calling with vLLM, a new tool parser is required. While we opened a PR to vLLM to include an R1T2-compatible tool parser off-the-shelf, we also ship the tool parser file tool_parser_vllm.py within this repository.\nWith this file, tool calling can be enabled via\n--tool-parser-plugin <ABSOLUTE_MODEL_SNAPSHOT_PATH>/tool_parser_vllm.py  \\\n--tool-call-parser tng_r1t2\nHere, put in the path to the snapshot folder such as ~/.cache/huggingface/hub/models--tngtech--DeepSeek-TNG-R1T2-Chimera/snapshots/SNAPSHOT/tool_parser_vllm.py\nSGLang:\nTool call support for R1T2 requires a recent SGLang version >= v0.4.10 (alternatively, you need to patch this bugfix for the reasoning parser for older versions of SGLang).\nAn R1T2-compatible tool call parser will be added with this PR to SGLang.\nUnfortunately, and unlike vLLM, there is no simple plugin system for tool call parsers in SGLang.\nUntil our PR is merged an relased with a new SGLang version, you can still install it manually by patching your SGLang source code as outlined in the PR:\nThe new tool call parser must be added and registered (so in total one file must be added, a second one edited, see details here).\nOnce the SGLang installation has been updated correctly, tool calling with R1T2 can be activated by starting SGLang with\n--tool-call-parser tng_r1t2\nModel Details\nArchitecture: DeepSeek-MoE transformer-based language model\nCombination Method: Assembly of Experts from the three DeepSeek parent models R1-0528, R1 and V3-0324\nRelease Date: 2025-07-02\nDesign Team: Robert Dahlke, Henrik Klagges, Benjamin Merkel, Fabian Klemm and David Reiss, Munich, Germany\nExtra Thanks: Big thanks to DeepSeek for their great models and open-source generosity, and to the other researchers that have published on model merging methodologies.\nUse, Out-of-scope Use, Other Limitations, Risks, Recommendations et al.\nRegarding the R1T/R1T2-Chimeras, we ask you to follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model.\nThese professional guidelines are available here on Hugging Face.\nEU AI Act\nDue to the strict new guidelines of the EU AI Act that take effect on August 2nd 2025, we recommend that each R1T/R1T2 user in the EU either familiarizes themselves with these requirements and assess their compliance, or ceases using the model in the EU after August 1st, 2025.\nContact, especially for your user feedback\nPlease give us your feedback, especially if you find deficiencies in the model:\nEmail: research@tngtech.com\nX.com: @tngtech\nCitation\n@misc{tng_technology_consulting_gmbh_2025_07_02,\nauthor       = { TNG Technology Consulting GmbH },\ntitle        = { DeepSeek-TNG-R1T2-Chimera },\nyear         = 2025,\nmonth        = { July },\nurl          = { https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera },\ndoi          = { 10.57967/hf/5950 },\npublisher    = { Hugging Face }\n}",
    "LiquidAI/LFM2-700M": "LFM2-700M\nüìÑ Model details\nüèÉ How to run LFM2\n1. Transformers\n2. vLLM\n3. llama.cpp\nüîß How to fine-tune LFM2\nüìà Performance\n1. Automated benchmarks\n2. LLM-as-a-Judge\n3. Inference\nüì¨ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-700M\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of four post-trained checkpoints with 350M, 700M, 1.2B, and 2.6 parameters. They provide the following key features to create AI-powered edge applications:\nFast training & inference ‚Äì LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.\nBest performance ‚Äì LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.\nNew architecture ‚Äì LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.\nFlexible deployment ‚Äì LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.\nFind more information about LFM2 in our blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2 models on narrow use cases to maximize performance.\nThey are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations.\nHowever, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\nProperty\nLFM2-350M\nLFM2-700M\nLFM2-1.2B\nLFM2-2.6B\nParameters\n354,483,968\n742,489,344\n1,170,340,608\n2,569,272,320\nLayers\n16 (10 conv + 6 attn)\n16 (10 conv + 6 attn)\n16 (10 conv + 6 attn)\n30 (22 conv + 8 attn)\nContext length\n32,768 tokens\n32,768 tokens\n32,768 tokens\n32,768 tokens\nVocabulary size\n65,536\n65,536\n65,536\n65,536\nPrecision\nbfloat16\nbfloat16\nbfloat16\nbfloat16\nTraining budget\n10 trillion tokens\n10 trillion tokens\n10 trillion tokens\n10 trillion tokens\nLicense\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\nGeneration parameters: We recommend the following parameters:\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful assistant trained by Liquid AI.<|im_end|>\n<|im_start|>user\nWhat is C. elegans?<|im_end|>\n<|im_start|>assistant\nIt's a tiny nematode that lives in temperate soil environments.<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nTool use: It consists of four main steps:\nFunction definition: LFM2 takes JSON function definitions as input (JSON objects between <|tool_list_start|> and <|tool_list_end|> special tokens), usually in the system prompt\nFunction call: LFM2 writes Pythonic function calls (a Python list between <|tool_call_start|> and <|tool_call_end|> special tokens), as the assistant answer.\nFunction execution: The function call is executed and the result is returned (string between <|tool_response_start|> and <|tool_response_end|> special tokens), as a \"tool\" role.\nFinal answer: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\nHere is a simple example of a conversation using tool use:\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>[{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}]<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\nYou can directly pass tools as JSON schema or Python functions with .apply_chat_template() as shown in this page to automatically format the system prompt.\nArchitecture: Hybrid model with multiplicative gates and short convolutions: 10 double-gated short-range LIV convolution blocks and 6 grouped query attention (GQA) blocks.\nPre-training mixture: Approximately 75% English, 20% multilingual, and 5% code data sourced from the web and licensed materials.\nTraining approach:\nKnowledge distillation using LFM1-7B as teacher model\nVery large-scale SFT on 50% downstream tasks, 50% general domains\nCustom DPO with length normalization and semi-online datasets\nIterative model merging\nüèÉ How to run LFM2\n1. Transformers\nTo run LFM2, you need to install Hugging Face transformers v4.55 or a more recent version as follows:\npip install -U transformers\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel_id = \"LiquidAI/LFM2-1.2B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"bfloat16\",\n#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Generate answer\nprompt = \"What is C. elegans?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\ntokenize=True,\n).to(model.device)\noutput = model.generate(\ninput_ids,\ndo_sample=True,\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_new_tokens=512,\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n# <|startoftext|><|im_start|>user\n# What is C. elegans?<|im_end|>\n# <|im_start|>assistant\n# C. elegans, also known as Caenorhabditis elegans, is a small, free-living\n# nematode worm (roundworm) that belongs to the phylum Nematoda.\nYou can directly run and test the model with this Colab notebook.\n2. vLLM\nYou need to install vLLM v0.10.2 or a more recent version as follows:\nuv pip install vllm==0.10.2 --extra-index-url https://wheels.vllm.ai/0.10.2/ --torch-backend=auto\nHere is an example of how to use it for inference:\nfrom vllm import LLM, SamplingParams\nprompts = [\n\"What is C. elegans?\",\n\"Say hi in JSON format\",\n\"Define AI in Spanish\"\n]\nsampling_params = SamplingParams(\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05\n)\nllm = LLM(model=\"LiquidAI/LFM2-700M\")\noutputs = llm.generate(prompts, sampling_params)\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n3. llama.cpp\nYou can run LFM2 with llama.cpp using its GGUF checkpoint. Find more information in the model card.\nüîß How to fine-tune LFM2\nWe recommend fine-tuning LFM2 models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (Unsloth)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Unsloth.\nSFT (Axolotl)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Axolotl.\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nüìà Performance\nLFM2 outperforms similar-sized models across different evaluation categories.\n1. Automated benchmarks\nModel\nMMLU\nGPQA\nIFEval\nIFBench\nGSM8K\nMGSM\nMMMLU\nLFM2-350M\n43.43\n27.46\n65.12\n16.41\n30.1\n29.52\n37.99\nLFM2-700M\n49.9\n28.48\n72.23\n20.56\n46.4\n45.36\n43.28\nLFM2-1.2B\n55.23\n31.47\n74.89\n20.7\n58.3\n55.04\n46.73\nQwen3-0.6B\n44.93\n22.14\n64.24\n19.75\n36.47\n41.28\n30.84\nQwen3-1.7B\n59.11\n27.72\n73.98\n21.27\n51.4\n66.56\n46.51\nLlama-3.2-1B-Instruct\n46.6\n28.84\n52.39\n16.86\n35.71\n29.12\n38.15\ngemma-3-1b-it\n40.08\n21.07\n62.9\n17.72\n59.59\n43.6\n34.43\n2. LLM-as-a-Judge\n3. Inference\nThroughput comparison on CPU in ExecuTorch\nThroughput comparison on CPU in Llama.cpp\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "LiquidAI/LFM2-1.2B": "LFM2-1.2B\nüìÑ Model details\nüèÉ How to run LFM2\n1. Transformers\n2. vLLM\n3. llama.cpp\nüîß How to fine-tune LFM2\nüìà Performance\n1. Automated benchmarks\n2. LLM-as-a-Judge\n3. Inference\nüì¨ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-1.2B\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of four post-trained checkpoints with 350M, 700M, 1.2B, and 2.6 parameters. They provide the following key features to create AI-powered edge applications:\nFast training & inference ‚Äì LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.\nBest performance ‚Äì LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.\nNew architecture ‚Äì LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.\nFlexible deployment ‚Äì LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.\nFind more information about LFM2 in our blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2 models on narrow use cases to maximize performance.\nThey are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations.\nHowever, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\nProperty\nLFM2-350M\nLFM2-700M\nLFM2-1.2B\nLFM2-2.6B\nParameters\n354,483,968\n742,489,344\n1,170,340,608\n2,569,272,320\nLayers\n16 (10 conv + 6 attn)\n16 (10 conv + 6 attn)\n16 (10 conv + 6 attn)\n30 (22 conv + 8 attn)\nContext length\n32,768 tokens\n32,768 tokens\n32,768 tokens\n32,768 tokens\nVocabulary size\n65,536\n65,536\n65,536\n65,536\nPrecision\nbfloat16\nbfloat16\nbfloat16\nbfloat16\nTraining budget\n10 trillion tokens\n10 trillion tokens\n10 trillion tokens\n10 trillion tokens\nLicense\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\nGeneration parameters: We recommend the following parameters:\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful assistant trained by Liquid AI.<|im_end|>\n<|im_start|>user\nWhat is C. elegans?<|im_end|>\n<|im_start|>assistant\nIt's a tiny nematode that lives in temperate soil environments.<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nTool use: It consists of four main steps:\nFunction definition: LFM2 takes JSON function definitions as input (JSON objects between <|tool_list_start|> and <|tool_list_end|> special tokens), usually in the system prompt\nFunction call: LFM2 writes Pythonic function calls (a Python list between <|tool_call_start|> and <|tool_call_end|> special tokens), as the assistant answer.\nFunction execution: The function call is executed and the result is returned (string between <|tool_response_start|> and <|tool_response_end|> special tokens), as a \"tool\" role.\nFinal answer: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\nHere is a simple example of a conversation using tool use:\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>[{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}]<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\nYou can directly pass tools as JSON schema or Python functions with .apply_chat_template() as shown in this page to automatically format the system prompt.\nArchitecture: Hybrid model with multiplicative gates and short convolutions: 10 double-gated short-range LIV convolution blocks and 6 grouped query attention (GQA) blocks.\nPre-training mixture: Approximately 75% English, 20% multilingual, and 5% code data sourced from the web and licensed materials.\nTraining approach:\nKnowledge distillation using LFM1-7B as teacher model\nVery large-scale SFT on 50% downstream tasks, 50% general domains\nCustom DPO with length normalization and semi-online datasets\nIterative model merging\nüèÉ How to run LFM2\n1. Transformers\nTo run LFM2, you need to install Hugging Face transformers v4.55 or a more recent version as follows:\npip install -U transformers\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel_id = \"LiquidAI/LFM2-1.2B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"bfloat16\",\n#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Generate answer\nprompt = \"What is C. elegans?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\ntokenize=True,\n).to(model.device)\noutput = model.generate(\ninput_ids,\ndo_sample=True,\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_new_tokens=512,\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n# <|startoftext|><|im_start|>user\n# What is C. elegans?<|im_end|>\n# <|im_start|>assistant\n# C. elegans, also known as Caenorhabditis elegans, is a small, free-living\n# nematode worm (roundworm) that belongs to the phylum Nematoda.\nYou can directly run and test the model with this Colab notebook.\n2. vLLM\nYou need to install vLLM v0.10.2 or a more recent version as follows:\nuv pip install vllm==0.10.2 --extra-index-url https://wheels.vllm.ai/0.10.2/ --torch-backend=auto\nHere is an example of how to use it for inference:\nfrom vllm import LLM, SamplingParams\nprompts = [\n\"What is C. elegans?\",\n\"Say hi in JSON format\",\n\"Define AI in Spanish\"\n]\nsampling_params = SamplingParams(\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05\n)\nllm = LLM(model=\"LiquidAI/LFM2-1.2B\")\noutputs = llm.generate(prompts, sampling_params)\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n3. llama.cpp\nYou can run LFM2 with llama.cpp using its GGUF checkpoint. Find more information in the model card.\nüîß How to fine-tune LFM2\nWe recommend fine-tuning LFM2 models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (Unsloth)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Unsloth.\nSFT (Axolotl)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Axolotl.\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nüìà Performance\nLFM2 outperforms similar-sized models across different evaluation categories.\n1. Automated benchmarks\nModel\nMMLU\nGPQA\nIFEval\nIFBench\nGSM8K\nMGSM\nMMMLU\nLFM2-350M\n43.43\n27.46\n65.12\n16.41\n30.1\n29.52\n37.99\nLFM2-700M\n49.9\n28.48\n72.23\n20.56\n46.4\n45.36\n43.28\nLFM2-1.2B\n55.23\n31.47\n74.89\n20.7\n58.3\n55.04\n46.73\nQwen3-0.6B\n44.93\n22.14\n64.24\n19.75\n36.47\n41.28\n30.84\nQwen3-1.7B\n59.11\n27.72\n73.98\n21.27\n51.4\n66.56\n46.51\nLlama-3.2-1B-Instruct\n46.6\n28.84\n52.39\n16.86\n35.71\n29.12\n38.15\ngemma-3-1b-it\n40.08\n21.07\n62.9\n17.72\n59.59\n43.6\n34.43\n2. LLM-as-a-Judge\n3. Inference\nThroughput comparison on CPU in ExecuTorch\nThroughput comparison on CPU in Llama.cpp\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "moonshotai/Kimi-K2-Instruct": "A newer version of this model is available:\nmoonshotai/Kimi-K2-Instruct-0905\n0. Changelog\n2025.8.11\n2025.7.18\n2025.7.15\n1. Model Introduction\nKey Features\nModel Variants\nInstruction model evaluation results\nBase model evaluation results\n2. Model Summary\nInstruction model evaluation results\nBase model evaluation results\n3. Evaluation Results\nInstruction model evaluation results\nBase model evaluation results\n4. Deployment\n5. Model Usage\nChat Completion\nTool Calling\n6. License\n7. Third Party Notices\n7. Contact Us\nüì∞¬†¬†Tech Blog ¬†¬†¬† | ¬†¬†¬† üìÑ¬†¬†Paper\n0. Changelog\n2025.8.11\nMessages with name field are now supported. We‚Äôve also moved the chat template to a standalone file for easier viewing.\n2025.7.18\nWe further modified our chat template to improve its robustness. The default system prompt has also been updated.\n2025.7.15\nWe have updated our tokenizer implementation. Now special tokens like [EOS] can be encoded to their token ids.\nWe fixed a bug in the chat template that was breaking multi-turn tool calls.\n1. Model Introduction\nKimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\nKey Features\nLarge-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\nMuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\nAgentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.\nModel Variants\nKimi-K2-Base: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.\nKimi-K2-Instruct: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.\n2. Model Summary\nArchitecture\nMixture-of-Experts (MoE)\nTotal Parameters\n1T\nActivated Parameters\n32B\nNumber of Layers (Dense layer included)\n61\nNumber of Dense Layers\n1\nAttention Hidden Dimension\n7168\nMoE Hidden Dimension (per Expert)\n2048\nNumber of Attention Heads\n64\nNumber of Experts\n384\nSelected Experts per Token\n8\nNumber of Shared Experts\n1\nVocabulary Size\n160K\nContext Length\n128K\nAttention Mechanism\nMLA\nActivation Function\nSwiGLU\n3. Evaluation Results\nInstruction model evaluation results\nBenchmark\nMetric\nKimi K2 Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B (non-thinking)\nClaude Sonnet 4 (w/o extended thinking)\nClaude Opus 4 (w/o extended thinking)\nGPT-4.1\nGemini 2.5 Flash  Preview (05-20)\nCoding Tasks\nLiveCodeBench v6(Aug 24 - May 25)\nPass@1\n53.7\n46.9\n37.0\n48.5\n47.4\n44.7\n44.7\nOJBench\nPass@1\n27.1\n24.0\n11.3\n15.3\n19.6\n19.5\n19.5\nMultiPL-E\nPass@1\n85.7\n83.1\n78.2\n88.6\n89.6\n86.7\n85.6\nSWE-bench Verified (Agentless Coding)\nSingle Patch w/o Test (Acc)\n51.8\n36.6\n39.4\n50.2\n53.0\n40.8\n32.6\nSWE-bench Verified  (Agentic Coding)\nSingle Attempt (Acc)\n65.8\n38.8\n34.4\n72.7*\n72.5*\n54.6\n‚Äî\nMultiple Attempts (Acc)\n71.6\n‚Äî\n‚Äî\n80.2\n79.4*\n‚Äî\n‚Äî\nSWE-bench Multilingual (Agentic Coding)\nSingle Attempt (Acc)\n47.3\n25.8\n20.9\n51.0\n‚Äî\n31.5\n‚Äî\nTerminalBench\nInhouse Framework (Acc)\n30.0\n‚Äî\n‚Äî\n35.5\n43.2\n8.3\n‚Äî\nTerminus (Acc)\n25.0\n16.3\n6.6\n‚Äî\n‚Äî\n30.3\n16.8\nAider-Polyglot\nAcc\n60.0\n55.1\n61.8\n56.4\n70.7\n52.4\n44.0\nTool Use Tasks\nTau2 retail\nAvg@4\n70.6\n69.1\n57.0\n75.0\n81.8\n74.8\n64.3\nTau2 airline\nAvg@4\n56.5\n39.0\n26.5\n55.5\n60.0\n54.5\n42.5\nTau2 telecom\nAvg@4\n65.8\n32.5\n22.1\n45.2\n57.0\n38.6\n16.9\nAceBench\nAcc\n76.5\n72.7\n70.5\n76.2\n75.6\n80.1\n74.5\nMath & STEM Tasks\nAIME 2024\nAvg@64\n69.6\n59.4*\n40.1*\n43.4\n48.2\n46.5\n61.3\nAIME 2025\nAvg@64\n49.5\n46.7\n24.7*\n33.1*\n33.9*\n37.0\n46.6\nMATH-500\nAcc\n97.4\n94.0*\n91.2*\n94.0\n94.4\n92.4\n95.4\nHMMT 2025\nAvg@32\n38.8\n27.5\n11.9\n15.9\n15.9\n19.4\n34.7\nCNMO 2024\nAvg@16\n74.3\n74.7\n48.6\n60.4\n57.6\n56.6\n75.0\nPolyMath-en\nAvg@4\n65.1\n59.5\n51.9\n52.8\n49.8\n54.0\n49.9\nZebraLogic\nAcc\n89.0\n84.0\n37.7*\n73.7\n59.3\n58.5\n57.9\nAutoLogi\nAcc\n89.5\n88.9\n83.3\n89.8\n86.1\n88.2\n84.1\nGPQA-Diamond\nAvg@8\n75.1\n68.4*\n62.9*\n70.0*\n74.9*\n66.3\n68.2\nSuperGPQA\nAcc\n57.2\n53.7\n50.2\n55.7\n56.5\n50.8\n49.6\nHumanity's Last Exam(Text Only)\n-\n4.7\n5.2\n5.7\n5.8\n7.1\n3.7\n5.6\nGeneral Tasks\nMMLU\nEM\n89.5\n89.4\n87.0\n91.5\n92.9\n90.4\n90.1\nMMLU-Redux\nEM\n92.7\n90.5\n89.2\n93.6\n94.2\n92.4\n90.6\nMMLU-Pro\nEM\n81.1\n81.2*\n77.3\n83.7\n86.6\n81.8\n79.4\nIFEval\nPrompt Strict\n89.8\n81.1\n83.2*\n87.6\n87.4\n88.0\n84.3\nMulti-Challenge\nAcc\n54.1\n31.4\n34.0\n46.8\n49.0\n36.4\n39.5\nSimpleQA\nCorrect\n31.0\n27.7\n13.2\n15.9\n22.8\n42.3\n23.3\nLivebench\nPass@1\n76.4\n72.4\n67.6\n74.8\n74.6\n69.8\n67.8\n‚Ä¢ Bold denotes global SOTA, and underlined denotes open-source SOTA.\n‚Ä¢ Data points marked with * are taken directly from the model's tech report or blog.\n‚Ä¢ All metrics, except for SWE-bench Verified (Agentless), are evaluated with an 8k output token length. SWE-bench Verified (Agentless) is limited to a 16k output token length.\n‚Ä¢ Kimi K2 achieves 65.8% pass@1 on the SWE-bench Verified tests with bash/editor tools (single-attempt patches, no test-time compute). It also achieves a 47.3% pass@1 on the SWE-bench Multilingual tests under the same conditions. Additionally, we report results on SWE-bench Verified tests (71.6%) that leverage parallel test-time compute by sampling multiple sequences and selecting the single best via an internal scoring model.\n‚Ä¢ To ensure the stability of the evaluation, we employed avg@k on the AIME, HMMT, CNMO, PolyMath-en, GPQA-Diamond, EvalPlus, Tau2.\n‚Ä¢ Some data points have been omitted due to prohibitively expensive evaluation costs.\nBase model evaluation results\nBenchmark\nMetric\nShot\nKimi K2 Base\nDeepseek-V3-Base\nQwen2.5-72B\nLlama 4 Maverick\nGeneral Tasks\nMMLU\nEM\n5-shot\n87.8\n87.1\n86.1\n84.9\nMMLU-pro\nEM\n5-shot\n69.2\n60.6\n62.8\n63.5\nMMLU-redux-2.0\nEM\n5-shot\n90.2\n89.5\n87.8\n88.2\nSimpleQA\nCorrect\n5-shot\n35.3\n26.5\n10.3\n23.7\nTriviaQA\nEM\n5-shot\n85.1\n84.1\n76.0\n79.3\nGPQA-Diamond\nAvg@8\n5-shot\n48.1\n50.5\n40.8\n49.4\nSuperGPQA\nEM\n5-shot\n44.7\n39.2\n34.2\n38.8\nCoding Tasks\nLiveCodeBench v6\nPass@1\n1-shot\n26.3\n22.9\n21.1\n25.1\nEvalPlus\nPass@1\n-\n80.3\n65.6\n66.0\n65.5\nMathematics Tasks\nMATH\nEM\n4-shot\n70.2\n60.1\n61.0\n63.0\nGSM8k\nEM\n8-shot\n92.1\n91.7\n90.4\n86.3\nChinese Tasks\nC-Eval\nEM\n5-shot\n92.5\n90.0\n90.9\n80.9\nCSimpleQA\nCorrect\n5-shot\n77.6\n72.1\n50.5\n53.5\n‚Ä¢ We only evaluate open-source pretrained models in this work. We report results for Qwen2.5-72B because the base checkpoint for Qwen3-235B-A22B was not open-sourced at the time of our study.\n‚Ä¢ All models are evaluated using the same evaluation protocol.\n4. Deployment\nYou can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you.\nThe Anthropic-compatible API maps temperature by real_temperature = request_temperature * 0.6 for better compatible with existing applications.\nOur model checkpoints are stored in the block-fp8 format, you can find it on Huggingface.\nCurrently, Kimi-K2 is recommended to run on the following inference engines:\nvLLM\nSGLang\nKTransformers\nTensorRT-LLM\nDeployment examples for vLLM and SGLang can be found in the Model Deployment Guide.\n5. Model Usage\nChat Completion\nOnce the local inference service is up, you can interact with it through the chat endpoint:\ndef simple_chat(client: OpenAI, model_name: str):\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Please give a brief self-introduction.\"}]},\n]\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=messages,\nstream=False,\ntemperature=0.6,\nmax_tokens=256\n)\nprint(response.choices[0].message.content)\nThe recommended temperature for Kimi-K2-Instruct is temperature = 0.6.\nIf no special instructions are required, the system prompt above is a good default.\nTool Calling\nKimi-K2-Instruct has strong tool-calling capabilities.\nTo enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.\nThe following example demonstrates calling a weather tool end-to-end:\n# Your tool implementation\ndef get_weather(city: str) -> dict:\nreturn {\"weather\": \"Sunny\"}\n# Tool schema definition\ntools = [{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Retrieve current weather information. Call this when the user asks about the weather.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"city\"],\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"Name of the city\"\n}\n}\n}\n}\n}]\n# Map tool names to their implementations\ntool_map = {\n\"get_weather\": get_weather\n}\ndef tool_call_with_client(client: OpenAI, model_name: str):\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n{\"role\": \"user\", \"content\": \"What's the weather like in Beijing today? Use the tool to check.\"}\n]\nfinish_reason = None\nwhile finish_reason is None or finish_reason == \"tool_calls\":\ncompletion = client.chat.completions.create(\nmodel=model_name,\nmessages=messages,\ntemperature=0.6,\ntools=tools,          # tool list defined above\ntool_choice=\"auto\"\n)\nchoice = completion.choices[0]\nfinish_reason = choice.finish_reason\nif finish_reason == \"tool_calls\":\nmessages.append(choice.message)\nfor tool_call in choice.message.tool_calls:\ntool_call_name = tool_call.function.name\ntool_call_arguments = json.loads(tool_call.function.arguments)\ntool_function = tool_map[tool_call_name]\ntool_result = tool_function(**tool_call_arguments)\nprint(\"tool_result:\", tool_result)\nmessages.append({\n\"role\": \"tool\",\n\"tool_call_id\": tool_call.id,\n\"name\": tool_call_name,\n\"content\": json.dumps(tool_result)\n})\nprint(\"-\" * 100)\nprint(choice.message.content)\nThe tool_call_with_client function implements the pipeline from user query to tool execution.\nThis pipeline requires the inference engine to support Kimi-K2‚Äôs native tool-parsing logic.\nFor streaming output and manual tool-parsing, see the Tool Calling Guide.\n6. License\nBoth the code repository and the model weights are released under the Modified MIT License.\n7. Third Party Notices\nSee THIRD PARTY NOTICES\n7. Contact Us\nIf you have any questions, please reach out at support@moonshot.cn.",
    "t-tech/T-pro-it-2.0": "T-pro-it-2.0\nDescription\nüìö Dataset\nüìä Benchmarks\nSwitching Between Thinking and Non‚ÄëThinking Modes\nRecommended Generation Parameters\nüë®‚Äçüíª Examples of usage\nSGLang Usage\nHF Usage\nVLLM Usage\nLong Context Usage\nT-pro-it-2.0\nüö® Users are advised to exercise caution and are responsible for any additional training and oversight required to ensure the model's responses meet acceptable ethical and safety standards. The responsibility for incorporating this model into industrial or commercial solutions lies entirely with those who choose to deploy it.\nDescription\nT-pro-it-2.0 is a model built upon the Qwen 3 model family and incorporates both continual pre-training and alignment techniques.\nüìö Dataset\nInstruction Pre-Training:\n40B tokens of instruction data, with one-third focused on reasoning tasks.\nSupervised Fine-Tuning (SFT):\n~500K high-quality and diverse instructions with balanced complexity. Reasoning tasks make up about 20% of the dataset.\nPreference Tuning:\n~100K carefully selected instructions, filtered by length and type for general tasks and with domain-balanced selection for reasoning tasks.\nüìä Benchmarks\nModel\nMERA\nruMMLU\nRu Arena Hard\nru AIME 2025\nru LCB\nT-pro 2.0\n0.660\n0.790\n0.876\n0.646\n0.563\nQwen 3 32B\n0.584\n0.740\n0.836\n0.625\n0.537\nRuadapt 3 32B V2\n0.574\n0.737\n0.660\n0.450\n0.500\nDeepSeek-R1-Distill-Qwen-32B\n0.508\n0.702\n0.426\n0.402\n0.493\nGemma 3 27B\n0.577\n0.695\n0.759\n0.231\n0.261\nSwitching Between Thinking and Non‚ÄëThinking Modes\nTo enable or disable reasoning mode in HuggingFace, set the enable_thinking flag in tokenizer.apply_chat_template.For more details, see:\nSGLang Thinking/Non‚ÄëThinking Modes\nvLLM Thinking/Non‚ÄëThinking Modes\nRecommended Generation Parameters\nMode\nTemperature\npresence_penalty\nNo‚Äëthink (general requests)\n‚â§‚ÄØ0.3\n1.0\nThink mode (standard requests)\n‚âà‚ÄØ0.6\n1.0\nComplex reasoning requests\n‚â•‚ÄØ0.8\n1.0\nHybrid reasoning models need careful tuning of sampling hyperparameters, which vary by domain.\nUse lower temperature for straightforward queries and higher temperature for complex 'think-mode' tasks.\nA presence_penalty between 0 and 2 can help avoid repetitive outputs.\nüë®‚Äçüíª Examples of usage\nSGLang Usage\nFor better quality and stable performance, we recommend SGLang as your inference framework.\nTo run an inference server for T-pro-it-2.0, start by launching the SGLang server:\npython -m sglang.launch_server \\\n--model-path t-tech/T-pro-it-2.0 \\\n--reasoning-parser qwen3\nOnce the server is up and listening on localhost:30000, you can send chat-based requests via the OpenAI Python client.\nimport openai\nclient = openai.OpenAI(\nbase_url=\"http://127.0.0.1:30000/v1\",\napi_key=\"ANY\"  # the server ignores the API key\n)\nprompt = (\n\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã—á–∏—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞–ª ‚à´_0^1 x¬≤‚ÄØeÀ£‚ÄØdx, \"\n\"–ø–æ—à–∞–≥–æ–≤–æ –æ–±—ä—è—Å–Ω–∏ —Ä–µ—à–µ–Ω–∏–µ –∏ —É–∫–∞–∂–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\"\n)\ncompletion = client.chat.completions.create(\nmodel=\"ANY\",  # the server ignores the model name\nmessages=[\n{\"role\": \"system\", \"content\": \"–¢—ã T-pro, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n],\n# REQUIRED: sampling params from the \"Recommended Generation Parameters\" table\ntemperature=0.6,\npresence_penalty=1.0,\n)\n# The generated reply is in `completion.choices[0].message.content`\nprint(completion.choices[0].message.content)\nNote: It is obligatory to include both temperature and presence_penalty in every completion call.\nHF Usage\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntorch.manual_seed(42)\nmodel_name = \"t-tech/T-pro-it-2.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nprompt = (\n\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã—á–∏—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞–ª ‚à´_0^1 x¬≤‚ÄØeÀ£‚ÄØdx, \"\n\"–ø–æ—à–∞–≥–æ–≤–æ –æ–±—ä—è—Å–Ω–∏ —Ä–µ—à–µ–Ω–∏–µ –∏ —É–∫–∞–∂–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\"\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"–¢—ã T-pro, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\nOutput:\n<think>\n–•–æ—Ä–æ—à–æ, –º–Ω–µ –Ω—É–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞–ª –æ—Ç 0 –¥–æ 1 —Ñ—É–Ω–∫—Ü–∏–∏ x¬≤ * e^x dx. –Ø –ø–æ–º–Ω—é, —á—Ç–æ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞–ª–æ–≤ —Ç–∞–∫–æ–≥–æ –≤–∏–¥–∞, –≥–¥–µ –µ—Å—Ç—å –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–Ω–æ–≥–æ—á–ª–µ–Ω–∞ –∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã, –æ–±—ã—á–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º. –î–∞–≤–∞–π—Ç–µ –≤—Å–ø–æ–º–Ω—é —Ñ–æ—Ä–º—É–ª—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º: ‚à´u dv = uv - ‚à´v du.\n–ü–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å, —á—Ç–æ –≤–∑—è—Ç—å –∑–∞ u, –∞ —á—Ç–æ –∑–∞ dv. –û–±—ã—á–Ω–æ –≤ —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –º–Ω–æ–≥–æ—á–ª–µ–Ω (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ x¬≤) –±–µ—Ä—É—Ç –∑–∞ u, –ø–æ—Ç–æ–º—É —á—Ç–æ –ø—Ä–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–∏ –µ–≥–æ —Å—Ç–µ–ø–µ–Ω—å –ø–æ–Ω–∏–∂–∞–µ—Ç—Å—è, –∏ –≤ –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤ –æ–Ω –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É, —á—Ç–æ —É–ø—Ä–æ—Å—Ç–∏—Ç –∏–Ω—Ç–µ–≥—Ä–∞–ª. –¢–æ–≥–¥–∞ dv –±—É–¥–µ—Ç –æ—Å—Ç–∞–≤—à–∞—è—Å—è —á–∞—Å—Ç—å, —Ç–æ –µ—Å—Ç—å e^x dx.\n–ò—Ç–∞–∫, –ø—É—Å—Ç—å u = x¬≤, —Ç–æ–≥–¥–∞ du = 2x dx. –ê dv = e^x dx, –∑–Ω–∞—á–∏—Ç, v = ‚à´e^x dx = e^x.\n–¢–µ–ø–µ—Ä—å –ø—Ä–∏–º–µ–Ω–∏–º —Ñ–æ—Ä–º—É–ª—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º: ‚à´x¬≤ e^x dx = uv - ‚à´v du = x¬≤ e^x - ‚à´e^x * 2x dx.\n–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –æ—Å—Ç–∞–ª—Å—è –∏–Ω—Ç–µ–≥—Ä–∞–ª ‚à´2x e^x dx. –ï–≥–æ —Ç–æ–∂–µ –Ω—É–∂–Ω–æ –≤–∑—è—Ç—å –ø–æ —á–∞—Å—Ç—è–º. –û–ø—è—Ç—å –≤—ã–±–∏—Ä–∞–µ–º u –∏ dv. –¢–µ–ø–µ—Ä—å u = 2x, —Ç–æ–≥–¥–∞ du = 2 dx, –∞ dv = e^x dx, –∑–Ω–∞—á–∏—Ç, v = e^x.\n–ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—É–ª—É —Å–Ω–æ–≤–∞: ‚à´2x e^x dx = 2x e^x - ‚à´e^x * 2 dx = 2x e^x - 2‚à´e^x dx.\n–ò–Ω—Ç–µ–≥—Ä–∞–ª ‚à´e^x dx —Ä–∞–≤–µ–Ω e^x + C. –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ: 2x e^x - 2e^x + C.\n–¢–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∏–Ω—Ç–µ–≥—Ä–∞–ª—É: x¬≤ e^x - (2x e^x - 2e^x) + C = x¬≤ e^x - 2x e^x + 2e^x + C.\n–¢–µ–ø–µ—Ä—å –Ω—É–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞–ª –æ—Ç 0 –¥–æ 1. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ–¥—Å—Ç–∞–≤–∏–º –≤–µ—Ä—Ö–Ω–∏–π –∏ –Ω–∏–∂–Ω–∏–π –ø—Ä–µ–¥–µ–ª—ã –≤ –ø–µ—Ä–≤–æ–æ–±—Ä–∞–∑–Ω—É—é –∏ –≤—ã—á—Ç–µ–º.\n–°–Ω–∞—á–∞–ª–∞ –ø–æ–¥—Å—Ç–∞–≤–∏–º x = 1: (1¬≤ * e^1) - 2*1*e^1 + 2*e^1 = e - 2e + 2e = (1 - 2 + 2)e = 1*e = e.\n–¢–µ–ø–µ—Ä—å –ø–æ–¥—Å—Ç–∞–≤–∏–º x = 0: (0¬≤ * e^0) - 2*0*e^0 + 2*e^0 = 0 - 0 + 2*1 = 2.\n–í—ã—á–∏—Ç–∞–µ–º: e - 2.\n–ü—Ä–æ–≤–µ—Ä—é, –Ω–µ –æ—à–∏–±—Å—è –ª–∏ —è –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö. –ü—Ä–∏ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–µ x=1: 1*e - 2*1*e + 2*e = e - 2e + 2e = e. –î–∞, –≤–µ—Ä–Ω–æ. –ü—Ä–∏ x=0: 0 - 0 + 2*1 = 2. –¢–æ–≥–¥–∞ —Ä–∞–∑–Ω–æ—Å—Ç—å e - 2.\n–ù–æ –ø–æ–¥–æ–∂–¥–∏, –º–æ–∂–µ—Ç –±—ã—Ç—å —è –≥–¥–µ-—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏–ª –∑–Ω–∞–∫? –í –ø–µ—Ä–≤–æ–æ–±—Ä–∞–∑–Ω–æ–π –ø–æ—Å–ª–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º: x¬≤ e^x - 2x e^x + 2e^x. –î–∞, –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –¢–æ–≥–¥–∞ –ø—Ä–∏ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–µ 1: 1*e - 2*1*e + 2*e = (1 - 2 + 2)e = 1*e. –ü—Ä–∏ 0: 0 - 0 + 2*1 = 2. –¢–æ–≥–¥–∞ –æ—Ç–≤–µ—Ç e - 2.\n–ù–æ e –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–Ω–æ 2.718, –∑–Ω–∞—á–∏—Ç, e - 2 ‚âà 0.718. –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ –∏–Ω—Ç–µ–≥—Ä–∞–ª –æ—Ç 0 –¥–æ 1 x¬≤ e^x –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç.\n–ú–æ–∂–µ—Ç, —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥ –∏–ª–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–≤–æ–æ–±—Ä–∞–∑–Ω–æ–π? –í–æ–∑—å–º–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é –æ—Ç x¬≤ e^x - 2x e^x + 2e^x.\n–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è: d/dx [x¬≤ e^x] = 2x e^x + x¬≤ e^x.\nd/dx [-2x e^x] = -2 e^x - 2x e^x.\nd/dx [2e^x] = 2 e^x.\n–°–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Å–µ: 2x e^x + x¬≤ e^x - 2 e^x - 2x e^x + 2 e^x = x¬≤ e^x.\n–î–∞, –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ä–∞–≤–Ω–∞ –ø–æ–¥—ã–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. –ó–Ω–∞—á–∏—Ç, –ø–µ—Ä–≤–æ–æ–±—Ä–∞–∑–Ω–∞—è –Ω–∞–π–¥–µ–Ω–∞ –≤–µ—Ä–Ω–æ. –¢–æ–≥–¥–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞–ª —Ä–∞–≤–µ–Ω e - 2.\n–í—Ä–æ–¥–µ –≤—Å–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –í–æ–∑–º–æ–∂–Ω–æ, –º–æ–∂–Ω–æ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—É—é —Ñ–æ—Ä–º—É–ª—É –∏–ª–∏ —Ç–∞–±–ª–∏—Ü—É –∏–Ω—Ç–µ–≥—Ä–∞–ª–æ–≤, –Ω–æ –º–µ—Ç–æ–¥ –ø–æ —á–∞—Å—Ç—è–º —Å—Ä–∞–±–æ—Ç–∞–ª.\n–ò—Ç–∞–∫, –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: e - 2.\n</think>\n–î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–≥—Ä–∞–ª–∞\n‚à´01x2ex‚Äâdx\n\\int_0^1 x^2 e^x \\, dx\n‚à´01‚Äãx2exdx\n–∏—Å–ø–æ–ª—å–∑—É–µ–º **–º–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º**.\n### –®–∞–≥ 1: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º\n–§–æ—Ä–º—É–ª–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º:\n‚à´u‚Äâdv=uv‚àí‚à´v‚Äâdu.\n\\int u \\, dv = uv - \\int v \\, du.\n‚à´udv=uv‚àí‚à´vdu.\n–í—ã–±–∏—Ä–∞–µ–º:\n- $ u = x^2 $, —Ç–æ–≥–¥–∞ $ du = 2x \\, dx $,\n- $ dv = e^x \\, dx $, —Ç–æ–≥–¥–∞ $ v = e^x $.\n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –≤ —Ñ–æ—Ä–º—É–ª—É:\n‚à´x2ex‚Äâdx=x2ex‚àí‚à´ex‚ãÖ2x‚Äâdx=x2ex‚àí2‚à´xex‚Äâdx.\n\\int x^2 e^x \\, dx = x^2 e^x - \\int e^x \\cdot 2x \\, dx = x^2 e^x - 2 \\int x e^x \\, dx.\n‚à´x2exdx=x2ex‚àí‚à´ex‚ãÖ2xdx=x2ex‚àí2‚à´xexdx.\n### –®–∞–≥ 2: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –∏–Ω—Ç–µ–≥—Ä–∞–ª–∞\n–¢–µ–ø–µ—Ä—å –≤—ã—á–∏—Å–ª—è–µ–º $ \\int x e^x \\, dx $, —Å–Ω–æ–≤–∞ –ø—Ä–∏–º–µ–Ω—è—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —á–∞—Å—Ç—è–º:\n- $ u = x $, —Ç–æ–≥–¥–∞ $ du = dx $,\n- $ dv = e^x \\, dx $, —Ç–æ–≥–¥–∞ $ v = e^x $.\n–ü–æ–ª—É—á–∞–µ–º:\n‚à´xex‚Äâdx=xex‚àí‚à´ex‚Äâdx=xex‚àíex+C.\n\\int x e^x \\, dx = x e^x - \\int e^x \\, dx = x e^x - e^x + C.\n‚à´xexdx=xex‚àí‚à´exdx=xex‚àíex+C.\n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ:\n‚à´x2ex‚Äâdx=x2ex‚àí2(xex‚àíex)+C=x2ex‚àí2xex+2ex+C.\n\\int x^2 e^x \\, dx = x^2 e^x - 2 \\left( x e^x - e^x \\right) + C = x^2 e^x - 2x e^x + 2 e^x + C.\n‚à´x2exdx=x2ex‚àí2(xex‚àíex)+C=x2ex‚àí2xex+2ex+C.\n### –®–∞–≥ 3: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–≥—Ä–∞–ª–∞\n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–µ–ª—ã $ 0 $ –∏ $ 1 $:\n[x2ex‚àí2xex+2ex]01=(12e1‚àí2‚ãÖ1‚ãÖe1+2e1)‚àí(02e0‚àí2‚ãÖ0‚ãÖe0+2e0).\n\\left[ x^2 e^x - 2x e^x + 2 e^x \\right]_0^1 = \\left( 1^2 e^1 - 2 \\cdot 1 \\cdot e^1 + 2 e^1 \\right) - \\left( 0^2 e^0 - 2 \\cdot 0 \\cdot e^0 + 2 e^0 \\right).\n[x2ex‚àí2xex+2ex]01‚Äã=(12e1‚àí2‚ãÖ1‚ãÖe1+2e1)‚àí(02e0‚àí2‚ãÖ0‚ãÖe0+2e0).\n–£–ø—Ä–æ—â–∞–µ–º:\n- –ü—Ä–∏ $ x = 1 $:\n$$\ne - 2e + 2e = e.\n$$\n- –ü—Ä–∏ $ x = 0 $:\n$$\n0 - 0 + 2 \\cdot 1 = 2.\n$$\n–ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\ne‚àí2.\ne - 2.\ne‚àí2.\n### –û—Ç–≤–µ—Ç:\ne‚àí2\n\\boxed{e - 2}\ne‚àí2‚Äã\nVLLM Usage\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmodel_name = \"t-tech/T-pro-it-2.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, max_model_len=8192)\nsampling_params = SamplingParams(temperature=0.7,\nrepetition_penalty=1.05,\ntop_p=0.8, top_k=70,\nmax_tokens=512)\nprompt = (\n\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã—á–∏—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞–ª ‚à´_0^1 x¬≤‚ÄØeÀ£‚ÄØdx, \"\n\"–ø–æ—à–∞–≥–æ–≤–æ –æ–±—ä—è—Å–Ω–∏ —Ä–µ—à–µ–Ω–∏–µ –∏ —É–∫–∞–∂–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\"\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"–¢—ã T-pro, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\nprompt_token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\nLong Context Usage\nT-pro-it-2.0 natively supports a context length of 32,768 tokens.For conversations where the input significantly exceeds this limit, follow the recommendations from the Qwen3 model card on processing long texts.\nFor example, in SGLang, you can enable 128K context support with the following command:llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768"
}