{
    "KETI-AIR/ke-t5-base": "Model Card for ke-t5-base\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use [Optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nPreprocessing\nSpeeds, Sizes, Times\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nFactors\nMetrics\nResults\nModel Examination\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nHardware\nSoftware\nCitation\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nHow to Get Started with the Model\nModel Card for ke-t5-base\nModel Details\nModel Description\nThe developers of the Text-To-Text Transfer Transformer (T5) write:\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\nT5-Base is the checkpoint with 220 million parameters.\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.\nShared by [Optional]: Korea Electronics Technology Institute Artificial Intelligence Research Center\nModel type: Text Generation\n**Language(s) (NLP):**More information needed\nLicense: More information needed\nRelated Models:\nParent Model: T5\nResources for more information:\nGitHub Repo\nKE-T5 Github Repo\nPaper\nAssociated Paper\nBlog Post\nUses\nDirect Use\nThe developers write in a blog post that the model:\nOur text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself\nDownstream Use [Optional]\nMore information needed\nOut-of-Scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people.\nBias, Risks, and Limitations\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nTraining Details\nTraining Data\nThe model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\nThe model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.).\nSee the t5-base model card for further information.\nTraining Procedure\nPreprocessing\nMore information needed\nSpeeds, Sizes, Times\nMore information needed\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nThe developers evaluated the model on 24 tasks, see the research paper for full details.\nFactors\nMore information needed\nMetrics\nMore information needed\nResults\nFor full results for T5-Base, see the research paper, Table 14.\nModel Examination\nMore information needed\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nTechnical Specifications [optional]\nModel Architecture and Objective\nMore information needed\nCompute Infrastructure\nMore information needed\nHardware\nMore information needed\nSoftware\nMore information needed\nCitation\nBibTeX:\n@inproceedings{kim-etal-2021-model-cross,\ntitle = \"A Model of Cross-Lingual Knowledge-Grounded Response Generation for Open-Domain Dialogue Systems\",\nauthor = \"Kim, San  and\nJang, Jin Yea  and\nJung, Minyoung  and\nShin, Saim\",\nbooktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\nmonth = nov,\nyear = \"2021\",\naddress = \"Punta Cana, Dominican Republic\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2021.findings-emnlp.33\",\ndoi = \"10.18653/v1/2021.findings-emnlp.33\",\npages = \"352--365\",\nabstract = \"Research on open-domain dialogue systems that allow free topics is challenging in the field of natural language processing (NLP). The performance of the dialogue system has been improved recently by the method utilizing dialogue-related knowledge; however, non-English dialogue systems suffer from reproducing the performance of English dialogue systems because securing knowledge in the same language with the dialogue system is relatively difficult. Through experiments with a Korean dialogue system, this paper proves that the performance of a non-English dialogue system can be improved by utilizing English knowledge, highlighting the system uses cross-lingual knowledge. For the experiments, we 1) constructed a Korean version of the Wizard of Wikipedia dataset, 2) built Korean-English T5 (KE-T5), a language model pre-trained with Korean and English corpus, and 3) developed a knowledge-grounded Korean dialogue model based on KE-T5. We observed the performance improvement in the open-domain Korean dialogue model even only English knowledge was given. The experimental results showed that the knowledge inherent in cross-lingual language models can be helpful for generating responses in open dialogue systems.\",\n}\n@article{2020t5,\nauthor  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\ntitle   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\njournal = {Journal of Machine Learning Research},\nyear    = {2020},\nvolume  = {21},\nnumber  = {140},\npages   = {1-67},\nurl     = {http://jmlr.org/papers/v21/20-074.html}\n}\nAPA:\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\nGlossary [optional]\nMore information needed\nMore Information [optional]\nMore information needed\nModel Card Authors [optional]\nKorea Electronics Technology Institute Artificial Intelligence Research Center in collaboration with Ezi Ozoani and the Hugging Face team\nModel Card Contact\nMore information needed\nHow to Get Started with the Model\nUse the code below to get started with the model.\nClick to expand\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR/ke-t5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"KETI-AIR/ke-t5-base\")\nSee the Hugging Face T5 docs and a Colab Notebook created by the model developers for more examples.",
    "MoritzLaurer/DeBERTa-v3-base-mnli": "DeBERTa-v3-base-mnli-fever-anli\nModel description\nIntended uses & limitations\nTraining data\nTraining procedure\nEval results\nLimitations and bias\nBibTeX entry and citation info\nIdeas for cooperation or questions?\nDebugging and issues\nModel Recycling\nDeBERTa-v3-base-mnli-fever-anli\nModel description\nThis model was trained on the MultiNLI dataset, which consists of 392 702 NLI hypothesis-premise pairs.\nThe base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper. For a more powerful model, check out DeBERTa-v3-base-mnli-fever-anli which was trained on even more data.\nIntended uses & limitations\nHow to use the model\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel_name = \"MoritzLaurer/DeBERTa-v3-base-mnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\npremise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\nhypothesis = \"The movie was good.\"\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\nTraining data\nThis model was trained on the MultiNLI dataset, which consists of 392 702 NLI hypothesis-premise pairs.\nTraining procedure\nDeBERTa-v3-base-mnli was trained using the Hugging Face trainer with the following hyperparameters.\ntraining_args = TrainingArguments(\nnum_train_epochs=5,              # total number of training epochs\nlearning_rate=2e-05,\nper_device_train_batch_size=32,   # batch size per device during training\nper_device_eval_batch_size=32,    # batch size for evaluation\nwarmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\nweight_decay=0.06,               # strength of weight decay\nfp16=True                        # mixed precision training\n)\nEval results\nThe model was evaluated using the matched test set and achieves 0.90 accuracy.\nLimitations and bias\nPlease consult the original DeBERTa paper and literature on different NLI datasets for potential biases.\nBibTeX entry and citation info\nIf you want to cite this model, please cite the original DeBERTa paper, the respective NLI datasets and include a link to this model on the Hugging Face hub.\nIdeas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or LinkedIn\nDebugging and issues\nNote that DeBERTa-v3 was released recently and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers==4.13 might solve some issues.\nModel Recycling\nEvaluation on 36 datasets using MoritzLaurer/DeBERTa-v3-base-mnli as a base model yields average score of 80.01 in comparison to 79.04 by microsoft/deberta-v3-base.\nThe model is ranked 1st among all tested models for the microsoft/deberta-v3-base architecture as of 09/01/2023.\nResults:\n20_newsgroup\nag_news\namazon_reviews_multi\nanli\nboolq\ncb\ncola\ncopa\ndbpedia\nesnli\nfinancial_phrasebank\nimdb\nisear\nmnli\nmrpc\nmultirc\npoem_sentiment\nqnli\nqqp\nrotten_tomatoes\nrte\nsst2\nsst_5bins\nstsb\ntrec_coarse\ntrec_fine\ntweet_ev_emoji\ntweet_ev_emotion\ntweet_ev_hate\ntweet_ev_irony\ntweet_ev_offensive\ntweet_ev_sentiment\nwic\nwnli\nwsc\nyahoo_answers\n86.0196\n90.6333\n66.96\n60.0938\n83.792\n83.9286\n86.5772\n72\n79.2\n91.419\n85.1\n94.232\n71.5124\n89.4426\n90.4412\n63.7583\n86.5385\n93.8129\n91.9144\n89.8687\n85.9206\n95.4128\n57.3756\n91.377\n97.4\n91\n47.302\n83.6031\n57.6431\n77.1684\n83.3721\n70.2947\n71.7868\n67.6056\n74.0385\n71.7\nFor more information, see: Model Recycling",
    "Salesforce/codet5-base": "CodeT5 (base-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nEvaluation results\nEthical Considerations\nBibTeX entry and citation info\nCodeT5 (base-sized model)\nPre-trained CodeT5 model. It was introduced in the paper CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models\nfor Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi and first released in this repository.\nDisclaimer: The team releasing CodeT5 did not write a model card for this model so this model card has been written by the Hugging Face team (more specifically, nielsr).\nModel description\nFrom the abstract:\n\"We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code.\"\nIntended uses & limitations\nThis repository contains the pre-trained model only, so you can use this model for (among other tasks) masked span prediction, as shown in the code example below. However, the main use of this model is to fine-tune it for a downstream task of interest, such as:\ncode summarization\ncode generation\ncode translation\ncode refinement\ncode defect detection\ncode clone detection.\nSupervised datasets for code can be found here.\nSee the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model:\nfrom transformers import RobertaTokenizer, T5ForConditionalGeneration\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\ntext = \"def greet(user): print(f'hello <extra_id_0>!')\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n# simply generate a single sequence\ngenerated_ids = model.generate(input_ids, max_length=8)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n# this prints \"{user.username}\"\nTraining data\nThe CodeT5 model was pretrained on CodeSearchNet Husain et al., 2019. Additionally, the authors collected two datasets of C/CSharp from BigQuery1 to ensure that all downstream tasks have overlapped programming languages with the pre-training data. In total, around 8.35 million instances are used for pretraining.\nTraining procedure\nPreprocessing\nThis model uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the HuggingFace Tokenizers library. One can prepare text (or code) for the model using RobertaTokenizer, with the files from this repository.\nEvaluation results\nFor evaluation results on several downstream benchmarks, we refer to the paper.\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people‚Äôs lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nBibTeX entry and citation info\n@misc{wang2021codet5,\ntitle={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},\nauthor={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},\nyear={2021},\neprint={2109.00859},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "Shushant/nepaliBERT": "NEPALI BERT\nMasked Language Model for nepali language trained on nepali news scrapped from different nepali news website. The data set contained about 10 million of nepali sentences mainly related to nepali news.\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nData Description\nPaper and CItation Details\nPlain Text\nBibtex\nNEPALI BERT\nMasked Language Model for nepali language trained on nepali news scrapped from different nepali news website. The data set contained about 10 million of nepali sentences mainly related to nepali news.\nThis model is a fine-tuned version of Bert Base Uncased on dataset composed of different news scrapped from nepali news portals comprising of 4.6 GB of textual data.\nIt achieves the following results on the evaluation set:\nLoss: 1.0495\nModel description\nPretraining done on bert base architecture.\nIntended uses & limitations\nThis transformer model can be used for any NLP tasks related to Devenagari language. At the time of training, this is the state of the art model developed\nfor Devanagari dataset. Intrinsic evaluation with Perplexity of 8.56 achieves this state of the art whereas extrinsit evaluation done on sentiment analysis of Nepali tweets outperformed other existing\nmasked language models on Nepali dataset.\nTraining and evaluation data\nTHe training corpus is developed using 85467 news scrapped from different job portals. This is a preliminary dataset\nfor the experimentation. THe corpus size is about 4.3 GB of textual data. Similary evaluation data contains few news articles about 12 mb of textual data.\nTraining procedure\nFor the pretraining of masked language model, Trainer API from Huggingface is used. The pretraining took about 3 days 8 hours 57 minutes. Training was done on Tesla V100 GPU.\nWith 640 Tensor Cores, Tesla V100 is the world's first GPU to break the 100 teraFLOPS (TFLOPS) barrier of deep learning performance. This GPU was faciliated by Kathmandu University (KU) supercomputer.\nThanks to KU administration.\nUsage\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"Shushant/nepaliBERT\")\nfrom transformers import pipeline\nfill_mask = pipeline( \"fill-mask\", model=model, tokenizer=tokenizer, )\nfrom pprint import pprint pprint(fill_mask(f\"‡§§‡§ø‡§Æ‡•Ä‡§≤‡§æ‡§à ‡§ï‡§∏‡•ç‡§§‡•ã {tokenizer.mask_token}.\"))\nData Description\nTrained on about 4.6 GB of Nepali text corpus collected from various sources\nThese data were collected from nepali news site, OSCAR nepali corpus\nPaper and CItation Details\nIf you are interested to read the implementation details of this language model, you can read the full paper here.\nhttps://www.researchgate.net/publication/375019515_NepaliBERT_Pre-training_of_Masked_Language_Model_in_Nepali_Corpus\nPlain Text\nS. Pudasaini, S. Shakya, A. Tamang, S. Adhikari, S. Thapa and S. Lamichhane, \"NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus,\" 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Kirtipur, Nepal, 2023, pp. 325-330, doi: 10.1109/I-SMAC58438.2023.10290690.\nBibtex\n@INPROCEEDINGS{10290690,\nauthor={Pudasaini, Shushanta and Shakya, Subarna and Tamang, Aakash and Adhikari, Sajjan and Thapa, Sunil and Lamichhane, Sagar},\nbooktitle={2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)},\ntitle={NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus},\nyear={2023},\nvolume={},\nnumber={},\npages={325-330},\ndoi={10.1109/I-SMAC58438.2023.10290690}}",
    "UBC-NLP/AraT5-base": "AraT5-base\nAraT5: Text-to-Text Transformers for Arabic Language Generation\nHow to use AraT5 models\nAraT5 Models Checkpoints\nBibTex\nAcknowledgments\nAraT5-base\nAraT5: Text-to-Text Transformers for Arabic Language Generation\nThis is the repository accompanying our paper AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. In this is the repository we Introduce AraT5MSA, AraT5Tweet, and AraT5: three powerful Arabic-specific text-to-text Transformer based models;\nA new version of AraT5 comes out and we recommend using the AraT5v2-base-1024 instead of this version.\nHow to use AraT5 models\nBelow is an example for fine-tuning AraT5-base for News Title Generation on the Aranews dataset\n!python run_trainier_seq2seq_huggingface.py \\\n--learning_rate 5e-5 \\\n--max_target_length 128 --max_source_length 128 \\\n--per_device_train_batch_size 8 --per_device_eval_batch_size 8 \\\n--model_name_or_path \"UBC-NLP/AraT5-base\" \\\n--output_dir \"/content/AraT5_FT_title_generation\" --overwrite_output_dir \\\n--num_train_epochs 3 \\\n--train_file \"/content/ARGEn_title_genration_sample_train.tsv\" \\\n--validation_file \"/content/ARGEn_title_genration_sample_valid.tsv\" \\\n--task \"title_generation\" --text_column \"document\" --summary_column \"title\" \\\n--load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy epoch --logging_strategy epoch --predict_with_generate\\\n--do_train --do_eval\nFor more details about the fine-tuning example, please read this notebook\nIn addition, we release the fine-tuned checkpoint of the News Title Generation (NGT) which is described in the paper. The model available at Huggingface (UBC-NLP/AraT5-base-title-generation).\nFor more details, please visit our own GitHub.\nAraT5 Models Checkpoints\nAraT5 Pytorch and TensorFlow checkpoints are available on the Huggingface website for direct download and use exclusively for research. For commercial use, please contact the authors via email @ (muhammad.mageed[at]ubc[dot]ca).\nModel\nLink\nAraT5-base\nhttps://huggingface.co/UBC-NLP/AraT5-base\nAraT5-msa-base\nhttps://huggingface.co/UBC-NLP/AraT5-msa-base\nAraT5-tweet-base\nhttps://huggingface.co/UBC-NLP/AraT5-tweet-base\nAraT5-msa-small\nhttps://huggingface.co/UBC-NLP/AraT5-msa-small\nAraT5-tweet-small\nhttps://huggingface.co/UBC-NLP/AraT5-tweet-small\nBibTex\nIf you use our models for your scientific publication, or if you find the resources in this repository useful, please cite our papers as follows:\n(AraT5-base, AraT5-msa-base, AraT5-tweet-base, AraT5-msa-small, or AraT5-tweet-small)\n@inproceedings{nagoudi2022_arat5,\n@inproceedings{nagoudi-etal-2022-arat5,\ntitle = \"{A}ra{T}5: Text-to-Text Transformers for {A}rabic Language Generation\",\nauthor = \"Nagoudi, El Moatez Billah  and\nElmadany, AbdelRahim  and\nAbdul-Mageed, Muhammad\",\nbooktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\nmonth = may,\nyear = \"2022\",\naddress = \"Dublin, Ireland\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2022.acl-long.47\",\npages = \"628--647\",\nabstract = \"Transfer learning with a unified Transformer framework (T5) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach. Although a multilingual version of the T5 model (mT5) was also introduced, it is not clear how well it can fare on non-English tasks involving diverse data. To investigate this question, we apply mT5 on a language with a wide variety of dialects{--}Arabic. For evaluation, we introduce a novel benchmark for ARabic language GENeration (ARGEN), covering seven important tasks. For model comparison, we pre-train three powerful Arabic T5-style models and evaluate them on ARGEN. Although pre-trained with {\\textasciitilde}49 less data, our new models perform significantly better than mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new SOTAs. Our models also establish new SOTA on the recently-proposed, large Arabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al., 2021). Our new models are publicly available. We also link to ARGEN datasets through our repository: https://github.com/UBC-NLP/araT5.\",\n}\nAraT5v2-base-1024\n@inproceedings{elmadany-etal-2023-octopus,\ntitle = \"Octopus: A Multitask Model and Toolkit for {A}rabic Natural Language Generation\",\nauthor = \"Elmadany, AbdelRahim  and\nNagoudi, El Moatez Billah  and\nAbdul-Mageed, Muhammad\",\nbooktitle = \"Proceedings of ArabicNLP 2023\",\nmonth = dec,\nyear = \"2023\",\naddress = \"Singapore (Hybrid)\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2023.arabicnlp-1.20\",\ndoi = \"10.18653/v1/2023.arabicnlp-1.20\",\npages = \"232--243\",\n}\nAcknowledgments\nWe gratefully acknowledge support from the Natural Sciences and Engineering Research Council  of Canada, the  Social  Sciences and  Humanities  Research  Council  of  Canada, Canadian  Foundation for  Innovation,  ComputeCanada and UBC ARC-Sockeye. We  also  thank  the  Google TensorFlow Research Cloud (TFRC) program for providing us with free TPU access.",
    "Wikidepia/IndoT5-base-paraphrase": "Paraphrase Generation with IndoT5 Base\nModel in action\nLimitations\nAcknowledgement\nParaphrase Generation with IndoT5 Base\nIndoT5-base trained on translated PAWS.\nModel in action\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\nsentence = \"Anak anak melakukan piket kelas agar kebersihan kelas terjaga\"\ntext =  \"paraphrase: \" + sentence + \" </s>\"\nencoding = tokenizer(text, padding='longest', return_tensors=\"pt\")\noutputs = model.generate(\ninput_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"],\nmax_length=512,\ndo_sample=True,\ntop_k=200,\ntop_p=0.95,\nearly_stopping=True,\nnum_return_sequences=5\n)\nLimitations\nSometimes paraphrase contain date which doesnt exists in the original text :/\nAcknowledgement\nThanks to Tensorflow Research Cloud for providing TPU v3-8s.",
    "akdeniz27/xlm-roberta-base-turkish-ner": "Turkish Named Entity Recognition (NER) Model\nFine-tuning parameters:\nHow to use:\nReference test results:\nTurkish Named Entity Recognition (NER) Model\nThis model is the fine-tuned version of \"xlm-roberta-base\"\n(a multilingual version of RoBERTa)\nusing a reviewed version of well known Turkish NER dataset\n(https://github.com/stefan-it/turkish-bert/files/4558187/nerdata.txt).\nFine-tuning parameters:\ntask = \"ner\"\nmodel_checkpoint = \"xlm-roberta-base\"\nbatch_size = 8\nlabel_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\nmax_length = 512\nlearning_rate = 2e-5\nnum_train_epochs = 2\nweight_decay = 0.01\nHow to use:\nmodel = AutoModelForTokenClassification.from_pretrained(\"akdeniz27/xlm-roberta-base-turkish-ner\")\ntokenizer = AutoTokenizer.from_pretrained(\"akdeniz27/xlm-roberta-base-turkish-ner\")\nner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nner(\"<your text here>\")\nPls refer \"https://huggingface.co/transformers/_modules/transformers/pipelines/token_classification.html\" for entity grouping with aggregation_strategy parameter.\nReference test results:\naccuracy: 0.9919343118732742\nf1: 0.9492100796448622\nprecision: 0.9407349896480332\nrecall: 0.9578392621870883",
    "alvaroalon2/biobert_chemical_ner": "BioBERT model fine-tuned in NER task with BC5CDR-chemicals and BC4CHEMD corpus.\nThis was fine-tuned in order to use it in a BioNER/BioNEN system which is available at: https://github.com/librairy/bio-ner",
    "andi611/distilbert-base-uncased-ner-conll2003": "distilbert-base-uncased-ner\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\ndistilbert-base-uncased-ner\nThis model is a fine-tuned version of distilbert-base-uncased on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.0664\nPrecision: 0.9332\nRecall: 0.9423\nF1: 0.9377\nAccuracy: 0.9852\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 3e-05\ntrain_batch_size: 16\neval_batch_size: 32\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 5\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nPrecision\nRecall\nF1\nAccuracy\n0.2042\n1.0\n878\n0.0636\n0.9230\n0.9253\n0.9241\n0.9822\n0.0428\n2.0\n1756\n0.0577\n0.9286\n0.9370\n0.9328\n0.9841\n0.0199\n3.0\n2634\n0.0606\n0.9364\n0.9401\n0.9383\n0.9851\n0.0121\n4.0\n3512\n0.0641\n0.9339\n0.9380\n0.9360\n0.9847\n0.0079\n5.0\n4390\n0.0664\n0.9332\n0.9423\n0.9377\n0.9852\nFramework versions\nTransformers 4.8.2\nPytorch 1.8.1+cu111\nDatasets 1.8.0\nTokenizers 0.10.3",
    "bhadresh-savani/bert-base-uncased-emotion": "bert-base-uncased-emotion\nModel description:\nModel Performance Comparision on Emotion Dataset from Twitter:\nHow to Use the model:\nDataset:\nTraining procedure\nEval results\nReference:\nbert-base-uncased-emotion\nModel description:\nBert is a Transformer Bidirectional Encoder based Architecture trained on MLM(Mask Language Modeling) objective\nbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer with below training parameters\nlearning rate 2e-5,\nbatch size 64,\nnum_train_epochs=8,\nModel Performance Comparision on Emotion Dataset from Twitter:\nModel\nAccuracy\nF1 Score\nTest Sample per Second\nDistilbert-base-uncased-emotion\n93.8\n93.79\n398.69\nBert-base-uncased-emotion\n94.05\n94.06\n190.152\nRoberta-base-emotion\n93.95\n93.97\n195.639\nAlbert-base-v2-emotion\n93.6\n93.65\n182.794\nHow to Use the model:\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)\nprediction = classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\nprint(prediction)\n\"\"\"\noutput:\n[[\n{'label': 'sadness', 'score': 0.0005138228880241513},\n{'label': 'joy', 'score': 0.9972520470619202},\n{'label': 'love', 'score': 0.0007443308713845909},\n{'label': 'anger', 'score': 0.0007404946954920888},\n{'label': 'fear', 'score': 0.00032938539516180754},\n{'label': 'surprise', 'score': 0.0004197491507511586}\n]]\n\"\"\"\nDataset:\nTwitter-Sentiment-Analysis.\nTraining procedure\nColab Notebook\nfollow the above notebook by changing the model name from distilbert to bert\nEval results\n{\n'test_accuracy': 0.9405,\n'test_f1': 0.9405920712282673,\n'test_loss': 0.15769127011299133,\n'test_runtime': 10.5179,\n'test_samples_per_second': 190.152,\n'test_steps_per_second': 3.042\n}\nReference:\nNatural Language Processing with Transformer By Lewis Tunstall, Leandro von Werra, Thomas Wolf",
    "blanchefort/rubert-base-cased-sentiment-rusentiment": "RuBERT for Sentiment Analysis\nLabels\nHow to use\nDataset used for model training\nRuBERT for Sentiment Analysis\nThis is a DeepPavlov/rubert-base-cased-conversational model trained on RuSentiment.\nLabels\n0: NEUTRAL\n1: POSITIVE\n2: NEGATIVE\nHow to use\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment-rusentiment')\nmodel = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment-rusentiment', return_dict=True)\n@torch.no_grad()\ndef predict(text):\ninputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**inputs)\npredicted = torch.nn.functional.softmax(outputs.logits, dim=1)\npredicted = torch.argmax(predicted, dim=1).numpy()\nreturn predicted\nDataset used for model training\nRuSentiment\nA. Rogers A. Romanov A. Rumshisky S. Volkova M. Gronas A. Gribov RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian. Proceedings of COLING 2018.",
    "cambridgeltl/SapBERT-from-PubMedBERT-fulltext": "SapBERT-PubMedBERT\nExpected input and output\nExtracting embeddings from SapBERT\nCitation\ndatasets:\nUMLS\n[news] A cross-lingual extension of SapBERT will appear in the main onference of ACL 2021!\n[news] SapBERT will appear in the conference proceedings of NAACL 2021!\nSapBERT-PubMedBERT\nSapBERT by Liu et al. (2020). Trained with UMLS 2020AA (English only), using microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model.\nExpected input and output\nThe input should be a string of biomedical entity names, e.g., \"covid infection\" or \"Hydroxychloroquine\". The [CLS] embedding of the last layer is regarded as the output.\nExtracting embeddings from SapBERT\nThe following script converts a list of strings (entity names) into embeddings.\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")\nmodel = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()\n# replace with your own list of entity names\nall_names = [\"covid-19\", \"Coronavirus infection\", \"high fever\", \"Tumor of posterior wall of oropharynx\"]\nbs = 128 # batch size during inference\nall_embs = []\nfor i in tqdm(np.arange(0, len(all_names), bs)):\ntoks = tokenizer.batch_encode_plus(all_names[i:i+bs],\npadding=\"max_length\",\nmax_length=25,\ntruncation=True,\nreturn_tensors=\"pt\")\ntoks_cuda = {}\nfor k,v in toks.items():\ntoks_cuda[k] = v.cuda()\ncls_rep = model(**toks_cuda)[0][:,0,:] # use CLS representation as the embedding\nall_embs.append(cls_rep.cpu().detach().numpy())\nall_embs = np.concatenate(all_embs, axis=0)\nFor more details about training and eval, see SapBERT github repo.\nCitation\n@inproceedings{liu-etal-2021-self,\ntitle = \"Self-Alignment Pretraining for Biomedical Entity Representations\",\nauthor = \"Liu, Fangyu  and\nShareghi, Ehsan  and\nMeng, Zaiqiao  and\nBasaldella, Marco  and\nCollier, Nigel\",\nbooktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\nmonth = jun,\nyear = \"2021\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2021.naacl-main.334\",\npages = \"4228--4238\",\nabstract = \"Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.\",\n}",
    "almanach/camembert-large": "CamemBERT: a Tasty French Language Model\nIntroduction\nPre-trained models\nHow to use CamemBERT with HuggingFace\nAuthors\nCitation\nCamemBERT: a Tasty French Language Model\nIntroduction\nCamemBERT is a state-of-the-art language model for French based on the RoBERTa model.\nIt is now available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data and pretraining data source domains.\nPre-trained models\nModel\n#params\nArch.\nTraining data\ncamembert-base\n110M\nBase\nOSCAR (138 GB of text)\ncamembert/camembert-large\n335M\nLarge\nCCNet (135 GB of text)\ncamembert/camembert-base-ccnet\n110M\nBase\nCCNet (135 GB of text)\ncamembert/camembert-base-wikipedia-4gb\n110M\nBase\nWikipedia (4 GB of text)\ncamembert/camembert-base-oscar-4gb\n110M\nBase\nSubsample of OSCAR (4 GB of text)\ncamembert/camembert-base-ccnet-4gb\n110M\nBase\nSubsample of CCNet (4 GB of text)\nHow to use CamemBERT with HuggingFace\nLoad CamemBERT and its sub-word tokenizer :\nfrom transformers import CamembertModel, CamembertTokenizer\n# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-large\")\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-large\")\ncamembert.eval()  # disable dropout (or leave in train mode to finetune)\nFilling masks using pipeline\nfrom transformers import pipeline\ncamembert_fill_mask  = pipeline(\"fill-mask\", model=\"camembert/camembert-large\", tokenizer=\"camembert/camembert-large\")\nresults = camembert_fill_mask(\"Le camembert est <mask> :)\")\n# results\n#[{'sequence': '<s> Le camembert est bon :)</s>', 'score': 0.15560828149318695, 'token': 305},\n#{'sequence': '<s> Le camembert est excellent :)</s>', 'score': 0.06821336597204208, 'token': 3497},\n#{'sequence': '<s> Le camembert est d√©licieux :)</s>', 'score': 0.060438305139541626, 'token': 11661},\n#{'sequence': '<s> Le camembert est ici :)</s>', 'score': 0.02023460529744625, 'token': 373},\n#{'sequence': '<s> Le camembert est meilleur :)</s>', 'score': 0.01778135634958744, 'token': 876}]\nExtract contextual embedding features from Camembert output\nimport torch\n# Tokenize in sub-words with SentencePiece\ntokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n# ['‚ñÅJ', \"'\", 'aime', '‚ñÅle', '‚ñÅcam', 'ember', 't', '‚ñÅ!']\n# 1-hot encode and add special starting and end tokens\nencoded_sentence = tokenizer.encode(tokenized_sentence)\n# [5, 133, 22, 1250, 16, 12034, 14324, 81, 76, 6]\n# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n# Feed tokens to Camembert as a torch tensor (batch dim 1)\nencoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\nembeddings, _ = camembert(encoded_sentence)\n# embeddings.detach()\n# torch.Size([1, 10, 1024])\n#tensor([[[-0.1284,  0.2643,  0.4374,  ...,  0.1627,  0.1308, -0.2305],\n#         [ 0.4576, -0.6345, -0.2029,  ..., -0.1359, -0.2290, -0.6318],\n#         [ 0.0381,  0.0429,  0.5111,  ..., -0.1177, -0.1913, -0.1121],\n#         ...,\nExtract contextual embedding features from all Camembert layers\nfrom transformers import CamembertConfig\n# (Need to reload the model with new config)\nconfig = CamembertConfig.from_pretrained(\"camembert/camembert-large\", output_hidden_states=True)\ncamembert = CamembertModel.from_pretrained(\"camembert/camembert-large\", config=config)\nembeddings, _, all_layer_embeddings = camembert(encoded_sentence)\n#  all_layer_embeddings list of len(all_layer_embeddings) == 25 (input embedding layer + 24 self attention layers)\nall_layer_embeddings[5]\n# layer 5 contextual embedding : size torch.Size([1, 10, 1024])\n#tensor([[[-0.0600,  0.0742,  0.0332,  ..., -0.0525, -0.0637, -0.0287],\n#         [ 0.0950,  0.2840,  0.1985,  ...,  0.2073, -0.2172, -0.6321],\n#         [ 0.1381,  0.1872,  0.1614,  ..., -0.0339, -0.2530, -0.1182],\n#         ...,\nAuthors\nCamemBERT was trained and evaluated by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su√°rez*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.\nCitation\nIf you use our work, please cite:\n@inproceedings{martin2020camembert,\ntitle={CamemBERT: a Tasty French Language Model},\nauthor={Martin, Louis and Muller, Benjamin and Su{\\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de la Clergerie, {\\'E}ric Villemonte and Seddah, Djam{\\'e} and Sagot, Beno{\\^\\i}t},\nbooktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\nyear={2020}\n}",
    "cardiffnlp/twitter-roberta-base-sentiment": "Twitter-roBERTa-base for Sentiment Analysis\nExample of classification\nBibTeX entry and citation info\nTwitter-roBERTa-base for Sentiment Analysis\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English (for a similar multilingual model, see XLM-T).\nReference Paper: TweetEval (Findings of EMNLP 2020).\nGit Repo: Tweeteval official repository.\nLabels:\n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\nNew! We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets.\nSee twitter-roberta-base-sentiment-latest and TweetNLP for more details.\nExample of classification\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\nnew_text = []\nfor t in text.split(\" \"):\nt = '@user' if t.startswith('@') and len(t) > 1 else t\nt = 'http' if t.startswith('http') else t\nnew_text.append(t)\nreturn \" \".join(new_text)\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\nhtml = f.read().decode('utf-8').split(\"\\n\")\ncsvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\ntext = \"Good night üòä\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Good night üòä\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\nl = labels[ranking[i]]\ns = scores[ranking[i]]\nprint(f\"{i+1}) {l} {np.round(float(s), 4)}\")\nOutput:\n1) positive 0.8466\n2) neutral 0.1458\n3) negative 0.0076\nBibTeX entry and citation info\nPlease cite the reference paper if you use this model.\n@inproceedings{barbieri-etal-2020-tweeteval,\ntitle = \"{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification\",\nauthor = \"Barbieri, Francesco  and\nCamacho-Collados, Jose  and\nEspinosa Anke, Luis  and\nNeves, Leonardo\",\nbooktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2020.findings-emnlp.148\",\ndoi = \"10.18653/v1/2020.findings-emnlp.148\",\npages = \"1644--1650\"\n}",
    "cardiffnlp/twitter-roberta-base": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTwitter-roBERTa-base\nPreprocess Text\nExample Masked Language Model\nExample Tweet Embeddings\nExample Feature Extraction\nBibTeX entry and citation info\nTwitter-roBERTa-base\nThis is a RoBERTa-base model trained on ~58M tweets on top of the original RoBERTa-base checkpoint, as described and evaluated in the TweetEval benchmark (Findings of EMNLP 2020).\nTo evaluate this and other LMs on Twitter-specific data, please refer to the Tweeteval official repository.\nPreprocess Text\nReplace usernames and links for placeholders: \"@user\" and \"http\".\ndef preprocess(text):\nnew_text = []\nfor t in text.split(\" \"):\nt = '@user' if t.startswith('@') and len(t) > 1 else t\nt = 'http' if t.startswith('http') else t\nnew_text.append(t)\nreturn \" \".join(new_text)\nExample Masked Language Model\nfrom transformers import pipeline, AutoTokenizer\nimport numpy as np\nMODEL = \"cardiffnlp/twitter-roberta-base\"\nfill_mask = pipeline(\"fill-mask\", model=MODEL, tokenizer=MODEL)\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ndef print_candidates():\nfor i in range(5):\ntoken = tokenizer.decode(candidates[i]['token'])\nscore = np.round(candidates[i]['score'], 4)\nprint(f\"{i+1}) {token} {score}\")\ntexts = [\n\"I am so <mask> üòä\",\n\"I am so <mask> üò¢\"\n]\nfor text in texts:\nt = preprocess(text)\nprint(f\"{'-'*30}\\n{t}\")\ncandidates = fill_mask(t)\nprint_candidates()\nOutput:\n------------------------------\nI am so <mask> üòä\n1)  happy 0.402\n2)  excited 0.1441\n3)  proud 0.143\n4)  grateful 0.0669\n5)  blessed 0.0334\n------------------------------\nI am so <mask> üò¢\n1)  sad 0.2641\n2)  sorry 0.1605\n3)  tired 0.138\n4)  sick 0.0278\n5)  hungry 0.0232\nExample Tweet Embeddings\nfrom transformers import AutoTokenizer, AutoModel, TFAutoModel\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nfrom collections import defaultdict\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModel.from_pretrained(MODEL)\ndef get_embedding(text):\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\nfeatures = model(**encoded_input)\nfeatures = features[0].detach().cpu().numpy()\nfeatures_mean = np.mean(features[0], axis=0)\nreturn features_mean\nMODEL = \"cardiffnlp/twitter-roberta-base\"\nquery = \"The book was awesome\"\ntweets = [\"I just ordered fried chicken üê£\",\n\"The movie was great\",\n\"What time is the next game?\",\n\"Just finished reading 'Embeddings in NLP'\"]\nd = defaultdict(int)\nfor tweet in tweets:\nsim = 1-cosine(get_embedding(query),get_embedding(tweet))\nd[tweet] = sim\nprint('Most similar to: ',query)\nprint('----------------------------------------')\nfor idx,x in enumerate(sorted(d.items(), key=lambda x:x[1], reverse=True)):\nprint(idx+1,x[0])\nOutput:\nMost similar to:  The book was awesome\n----------------------------------------\n1 The movie was great\n2 Just finished reading 'Embeddings in NLP'\n3 I just ordered fried chicken üê£\n4 What time is the next game?\nExample Feature Extraction\nfrom transformers import AutoTokenizer, AutoModel, TFAutoModel\nimport numpy as np\nMODEL = \"cardiffnlp/twitter-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ntext = \"Good night üòä\"\ntext = preprocess(text)\n# Pytorch\nmodel = AutoModel.from_pretrained(MODEL)\nencoded_input = tokenizer(text, return_tensors='pt')\nfeatures = model(**encoded_input)\nfeatures = features[0].detach().cpu().numpy()\nfeatures_mean = np.mean(features[0], axis=0)\n#features_max = np.max(features[0], axis=0)\n# # Tensorflow\n# model = TFAutoModel.from_pretrained(MODEL)\n# encoded_input = tokenizer(text, return_tensors='tf')\n# features = model(encoded_input)\n# features = features[0].numpy()\n# features_mean = np.mean(features[0], axis=0)\n# #features_max = np.max(features[0], axis=0)\nBibTeX entry and citation info\nPlease cite the reference paper if you use this model.\n@inproceedings{barbieri-etal-2020-tweeteval,\ntitle = \"{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification\",\nauthor = \"Barbieri, Francesco  and\nCamacho-Collados, Jose  and\nEspinosa Anke, Luis  and\nNeves, Leonardo\",\nbooktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2020.findings-emnlp.148\",\ndoi = \"10.18653/v1/2020.findings-emnlp.148\",\npages = \"1644--1650\"\n}",
    "ckiplab/bert-base-chinese-ner": "CKIP BERT Base Chinese\nHomepage\nContributers\nUsage\nCKIP BERT Base Chinese\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\nÈÄôÂÄãÂ∞àÊ°àÊèê‰æõ‰∫ÜÁπÅÈ´î‰∏≠ÊñáÁöÑ transformers Ê®°ÂûãÔºàÂåÖÂê´ ALBERT„ÄÅBERT„ÄÅGPT2ÔºâÂèäËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂ∑•ÂÖ∑ÔºàÂåÖÂê´Êñ∑Ë©û„ÄÅË©ûÊÄßÊ®ôË®ò„ÄÅÂØ¶È´îËæ®Ë≠òÔºâ„ÄÇ\nHomepage\nhttps://github.com/ckiplab/ckip-transformers\nContributers\nMu Yang at CKIP (Author & Maintainer)\nUsage\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\nË´ã‰ΩøÁî® BertTokenizerFast ËÄåÈùû AutoTokenizer„ÄÇ\nfrom transformers import (\nBertTokenizerFast,\nAutoModel,\n)\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ner')\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\nÊúâÈóúÂÆåÊï¥‰ΩøÁî®ÊñπÊ≥ïÂèäÂÖ∂‰ªñË≥áË®äÔºåË´ãÂèÉË¶ã https://github.com/ckiplab/ckip-transformers „ÄÇ",
    "cointegrated/rubert-tiny2": "This is an updated version of cointegrated/rubert-tiny: a small Russian BERT-based encoder with high-quality sentence embeddings. This post in Russian gives more details.\nThe differences from the previous version include:\na larger vocabulary: 83828 tokens instead of 29564;\nlarger supported sequences: 2048 instead of 512;\nsentence embeddings approximate LaBSE closer than before;\nmeaningful segment embeddings (tuned on the NLI task)\nthe model is focused only on Russian.\nThe model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.\nSentence embeddings can be produced as follows:\n# pip install transformers sentencepiece\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\nmodel = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n# model.cuda()  # uncomment it if you have a GPU\ndef embed_bert_cls(text, model, tokenizer):\nt = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\nwith torch.no_grad():\nmodel_output = model(**{k: v.to(model.device) for k, v in t.items()})\nembeddings = model_output.last_hidden_state[:, 0, :]\nembeddings = torch.nn.functional.normalize(embeddings)\nreturn embeddings[0].cpu().numpy()\nprint(embed_bert_cls('–ø—Ä–∏–≤–µ—Ç –º–∏—Ä', model, tokenizer).shape)\n# (312,)\nAlternatively, you can use the model with sentence_transformers:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('cointegrated/rubert-tiny2')\nsentences = [\"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä\", \"hello world\", \"–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π –≤—Å–µ–ª–µ–Ω–Ω–∞—è\"]\nembeddings = model.encode(sentences)\nprint(embeddings)",
    "cross-encoder/nli-deberta-v3-base": "Cross-Encoder for Natural Language Inference\nTraining Data\nPerformance\nUsage\nUsage with Transformers AutoModel\nZero-Shot Classification\nCross-Encoder for Natural Language Inference\nThis model was trained using SentenceTransformers Cross-Encoder class. This model is based on microsoft/deberta-v3-base\nTraining Data\nThe model was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\nPerformance\nAccuracy on SNLI-test dataset: 92.38\nAccuracy on  MNLI mismatched set: 90.04\nFor futher evaluation results, see SBERT.net - Pretrained Cross-Encoder.\nUsage\nPre-trained models can be used like this:\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\n#Convert scores to labels\nlabel_mapping = ['contradiction', 'entailment', 'neutral']\nlabels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\nUsage with Transformers AutoModel\nYou can use the model also directly with Transformers library (without SentenceTransformers library):\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-base')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-base')\nfeatures = tokenizer(['A man is eating pizza', 'A black race car starts up in front of a crowd of people.'], ['A man eats something', 'A man is driving down a lonely road.'],  padding=True, truncation=True, return_tensors=\"pt\")\nmodel.eval()\nwith torch.no_grad():\nscores = model(**features).logits\nlabel_mapping = ['contradiction', 'entailment', 'neutral']\nlabels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\nprint(labels)\nZero-Shot Classification\nThis model can also be used for zero-shot-classification:\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-deberta-v3-base')\nsent = \"Apple just announced the newest iPhone X\"\ncandidate_labels = [\"technology\", \"sports\", \"politics\"]\nres = classifier(sent, candidate_labels)\nprint(res)",
    "cross-encoder/nli-deberta-v3-large": "Cross-Encoder for Natural Language Inference\nTraining Data\nPerformance\nUsage\nUsage with Transformers AutoModel\nZero-Shot Classification\nCross-Encoder for Natural Language Inference\nThis model was trained using SentenceTransformers Cross-Encoder class. This model is based on microsoft/deberta-v3-large\nTraining Data\nThe model was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\nPerformance\nAccuracy on SNLI-test dataset: 92.20\nAccuracy on  MNLI mismatched set: 90.49\nFor futher evaluation results, see SBERT.net - Pretrained Cross-Encoder.\nUsage\nPre-trained models can be used like this:\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-large')\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\n#Convert scores to labels\nlabel_mapping = ['contradiction', 'entailment', 'neutral']\nlabels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\nUsage with Transformers AutoModel\nYou can use the model also directly with Transformers library (without SentenceTransformers library):\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large')\nfeatures = tokenizer(['A man is eating pizza', 'A black race car starts up in front of a crowd of people.'], ['A man eats something', 'A man is driving down a lonely road.'],  padding=True, truncation=True, return_tensors=\"pt\")\nmodel.eval()\nwith torch.no_grad():\nscores = model(**features).logits\nlabel_mapping = ['contradiction', 'entailment', 'neutral']\nlabels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\nprint(labels)\nZero-Shot Classification\nThis model can also be used for zero-shot-classification:\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-deberta-v3-large')\nsent = \"Apple just announced the newest iPhone X\"\ncandidate_labels = [\"technology\", \"sports\", \"politics\"]\nres = classifier(sent, candidate_labels)\nprint(res)",
    "ctl/wav2vec2-large-xlsr-cantonese": "Wav2Vec2-Large-XLSR-53-Cantonese\nUsage\nEvaluation\nTraining\nWav2Vec2-Large-XLSR-53-Cantonese\nFine-tuned facebook/wav2vec2-large-xlsr-53 on Cantonese using the Common Voice.\nWhen using this model, make sure that your speech input is sampled at 16kHz.\nUsage\nThe model can be used directly (without a language model) as follows:\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\ntest_dataset = load_dataset(\"common_voice\", \"zh-HK\", split=\"test[:2%]\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"ctl/wav2vec2-large-xlsr-cantonese\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"ctl/wav2vec2-large-xlsr-cantonese\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\nspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\nbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\nreturn batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\nlogits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\nEvaluation\nThe model can be evaluated as follows on the Chinese (Hong Kong) test data of Common Voice.\n!pip install jiwer\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\nimport argparse\nlang_id = \"zh-HK\"\nmodel_id = \"ctl/wav2vec2-large-xlsr-cantonese\"\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\"\\‚Äú\\%\\‚Äò\\‚Äù\\ÔøΩ\\Ôºé\\‚ãØ\\ÔºÅ\\Ôºç\\Ôºö\\‚Äì\\„ÄÇ\\„Äã\\,\\Ôºâ\\,\\Ôºü\\Ôºõ\\ÔΩû\\~\\‚Ä¶\\Ô∏∞\\Ôºå\\Ôºà\\„Äç\\‚Äß\\„Ää\\Ôπî\\„ÄÅ\\‚Äî\\Ôºè\\,\\„Äå\\Ôπñ\\¬∑\\']'\ntest_dataset = load_dataset(\"common_voice\", f\"{lang_id}\", split=\"test\")\ncer = load_metric(\"cer\")\nprocessor = Wav2Vec2Processor.from_pretrained(f\"{model_id}\")\nmodel = Wav2Vec2ForCTC.from_pretrained(f\"{model_id}\")\nmodel.to(\"cuda\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\nbatch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\nspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\nbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\nreturn batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\ninputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\nlogits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\npred_ids = torch.argmax(logits, dim=-1)\nbatch[\"pred_strings\"] = processor.batch_decode(pred_ids)\nreturn batch\nresult = test_dataset.map(evaluate, batched=True, batch_size=16)\nprint(\"CER: {:2f}\".format(100 * cer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\nTest Result: 15.51 %\nTraining\nThe Common Voice train, validation were used for training.\nThe script used for training will be posted here",
    "dccuchile/bert-base-spanish-wwm-uncased": "BETO: Spanish BERT\nDownload\nBenchmarks\nExample of use\nAcknowledgments\nCitation\nLicense Disclaimer\nReferences\nBETO: Spanish BERT\nBETO is a BERT model trained on a big Spanish corpus. BETO is of size similar to a BERT-Base and was trained with the Whole Word Masking technique. Below you find Tensorflow and Pytorch checkpoints for the uncased and cased versions, as well as some results for Spanish benchmarks comparing BETO with Multilingual BERT as well as other (not BERT-based) models.\nDownload\nBETO uncased\ntensorflow_weights\npytorch_weights\nvocab, config\nBETO cased\ntensorflow_weights\npytorch_weights\nvocab, config\nAll models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.\nBenchmarks\nThe following table shows some BETO results in the Spanish version of every task.\nWe compare BETO (cased and uncased) with the Best Multilingual BERT results that\nwe found in the literature (as of October 2019).\nThe table also shows some alternative methods for the same tasks (not necessarily BERT-based methods).\nReferences for all methods can be found here.\nTask\nBETO-cased\nBETO-uncased\nBest Multilingual BERT\nOther results\nPOS\n98.97\n98.44\n97.10 [2]\n98.91 [6], 96.71 [3]\nNER-C\n88.43\n82.67\n87.38 [2]\n87.18 [3]\nMLDoc\n95.60\n96.12\n95.70 [2]\n88.75 [4]\nPAWS-X\n89.05\n89.55\n90.70 [8]\nXNLI\n82.01\n80.15\n78.50 [2]\n80.80 [5], 77.80 [1], 73.15 [4]\nExample of use\nFor further details on how to use BETO you can visit the ü§óHuggingface Transformers library, starting by the Quickstart section.\nBETO models can be accessed simply as 'dccuchile/bert-base-spanish-wwm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased' by using the Transformers library.\nAn example on how to download and use the models in this page can be found in this colab notebook.\n(We will soon add a more detailed step-by-step tutorial in Spanish for newcommers üòâ)\nAcknowledgments\nWe thank Adereso for kindly providing support for traininig BETO-uncased, and the Millennium Institute for Foundational Research on Data\nthat provided support for training BETO-cased. Also thanks to Google for helping us with the TensorFlow Research Cloud program.\nCitation\nSpanish Pre-Trained BERT Model and Evaluation Data\nTo cite this resource in a publication please use the following:\n@inproceedings{CaneteCFP2020,\ntitle={Spanish Pre-Trained BERT Model and Evaluation Data},\nauthor={Ca√±ete, Jos√© and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P√©rez, Jorge},\nbooktitle={PML4DC at ICLR 2020},\nyear={2020}\n}\nLicense Disclaimer\nThe license CC BY 4.0 best describes our intentions for our work. However we are not sure that all the datasets used to train BETO have licenses compatible with CC BY 4.0 (specially for commercial use). Please use at your own discretion and verify that the licenses of the original text resources match your needs.\nReferences\n[1] Original Multilingual BERT\n[2] Multilingual BERT on \"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\"\n[3] Multilingual BERT on \"How Multilingual is Multilingual BERT?\"\n[4] LASER\n[5] XLM (MLM+TLM)\n[6] UDPipe on \"75 Languages, 1 Model: Parsing Universal Dependencies Universally\"\n[7] Multilingual BERT on \"Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation\"\n[8] Multilingual BERT on \"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification\"",
    "deepmind/multimodal-perceiver": "Perceiver IO for multimodal autoencoding\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nPerceiver IO for multimodal autoencoding\nPerceiver IO model trained on Kinetics-700-2020 for auto-encoding videos that consist of images, audio and a class label. It was introduced in the paper Perceiver IO: A General Architecture for Structured Inputs & Outputs by Jaegle et al. and first released in this repository.\nThe goal of multimodal autoencoding is to learn a model that can accurately reconstruct multimodal inputs in the presence of a bottleneck induced by an architecture.\nDisclaimer: The team releasing Perceiver IO did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nPerceiver IO is a transformer encoder model that can be applied on any modality (text, images, audio, video, ...). The core idea is to employ the self-attention mechanism on a not-too-large set of latent vectors (e.g. 256 or 512), and only use the inputs to perform cross-attention with the latents. This allows for the time and memory requirements of the self-attention mechanism to not depend on the size of the inputs.\nTo decode, the authors employ so-called decoder queries, which allow to flexibly decode the final hidden states of the latents to produce outputs of arbitrary size and semantics. For multimodal autoencoding, the output contains the reconstructions of the 3 modalities: images, audio and the class label.\nPerceiver IO architecture.\nAs the time and memory requirements of the self-attention mechanism don't depend on the size of the inputs, the Perceiver IO authors can train the model by padding the inputs (images, audio, class label) with modality-specific embeddings and serialize all of them into a 2D input array (i.e. concatenate along the time dimension). Decoding the final hidden states of the latents is done by using queries containing Fourier-based position embeddings (for video and audio) and modality embeddings.\nIntended uses & limitations\nYou can use the raw model for multimodal autoencoding. Note that by masking the class label during evaluation, the auto-encoding model becomes a video classifier.\nSee the [model hub](https://huggingface.co/models search=deepmind/perceiver) to look for other versions on a task that may interest you.\nHow to use\nWe refer to the tutorial notebook regarding using the Perceiver for multimodal autoencoding.\nTraining data\nThis model was trained on Kinetics-700-200, a dataset consisting of videos that belong to one of 700 classes.\nTraining procedure\nPreprocessing\nThe authors train on 16 frames at 224x224 resolution, preprocessed into 50k 4x4 patches as well as 30k raw audio samples, patched into a total of 1920 16-dimensional vectors and one 700-dimensional one-hot representation of the class label.\nPretraining\nHyperparameter details can be found in Appendix F of the paper.\nEvaluation results\nFor evaluation results, we refer to table 5 of the paper.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2107-14795,\nauthor    = {Andrew Jaegle and\nSebastian Borgeaud and\nJean{-}Baptiste Alayrac and\nCarl Doersch and\nCatalin Ionescu and\nDavid Ding and\nSkanda Koppula and\nDaniel Zoran and\nAndrew Brock and\nEvan Shelhamer and\nOlivier J. H{\\'{e}}naff and\nMatthew M. Botvinick and\nAndrew Zisserman and\nOriol Vinyals and\nJo{\\~{a}}o Carreira},\ntitle     = {Perceiver {IO:} {A} General Architecture for Structured Inputs {\\&}\nOutputs},\njournal   = {CoRR},\nvolume    = {abs/2107.14795},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2107.14795},\neprinttype = {arXiv},\neprint    = {2107.14795},\ntimestamp = {Tue, 03 Aug 2021 14:53:34 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2107-14795.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "dennlinger/roberta-cls-consec": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nAbout this model: Topical Change Detection in Documents\nLoad the model\nInput Format\nTraining objective\nPerformance\nAbout this model: Topical Change Detection in Documents\nThis network has been fine-tuned for the task described in the paper Topical Change Detection in Documents via Embeddings of Long Sequences and is our best-performing base-transformer model. You can find more detailed information in our GitHub page for the paper here, or read the paper itself. The weights are based on RoBERTa-base.\nLoad the model\nThe preferred way is through pipelines\nfrom transformers import pipeline\npipe = pipeline(\"text-classification\", model=\"dennlinger/roberta-cls-consec\")\npipe(\"{First paragraph} [SEP] {Second paragraph}\")\nInput Format\nThe model expects two segments that are separated with the [SEP] token. In our training setup, we had entire paragraphs as samples (or up to 512 tokens across two paragraphs), specifically trained on a Terms of Service data set. Note that this might lead to poor performance on \"general\" topics, such as news articles or Wikipedia.\nTraining objective\nThe training task is to determine whether two text segments (paragraphs) belong to the same topical section or not. This can be utilized to create a topical segmentation of a document by consecutively predicting the \"coherence\" of two segments.If you are experimenting via the Huggingface Model API, the following are interpretations of the LABELs:\nLABEL_0: Two input segments separated by [SEP] do not belong to the same topic.\nLABEL_1: Two input segments separated by [SEP] do belong to the same topic.\nPerformance\nThe results of this model can be found in the paper. We average over models from five different random seeds, which is why the specific results for this model might be different from the exact values in the paper.\nNote that this model is not trained to work on classifying single texts, but only works with two (separated) inputs.",
    "dmis-lab/biobert-v1.1": "No model card",
    "edugp/kenlm": "KenLM models\nDependencies\nExample:\nKenLM models\nThis repo contains several KenLM models trained on different tokenized datasets and languages.KenLM models are probabilistic n-gram languge models that models. One use case of these models consist on fast perplexity estimation for filtering or sampling large datasets. For example, one could use a KenLM model trained on French Wikipedia to run inference on a large dataset and filter out samples that are very unlike to appear on Wikipedia (high perplexity), or very simple non-informative sentences that could appear repeatedly (low perplexity).\nAt the root of this repo you will find different directories named after the dataset models were trained on (e.g. wikipedia, oscar). Within each directory, you will find several models trained on different language subsets of the dataset (e.g. en (English), es (Spanish), fr (French)). For each language you will find three different files\n{language}.arpa.bin: The trained KenLM model binary\n{language}.sp.model: The trained SentencePiece model used for tokenization\n{language}.sp.vocab: The vocabulary file for the SentencePiece model\nThe models have been trained using some of the preprocessing steps from cc_net, in particular replacing numbers with zeros and normalizing punctuation. So, it is important to keep the default values for the parameters: lower_case, remove_accents, normalize_numbers and punctuation when using the pre-trained models in order to replicate the same pre-processing steps at inference time.\nDependencies\nKenLM: pip install https://github.com/kpu/kenlm/archive/master.zip\nSentencePiece: pip install sentencepiece\nExample:\nfrom model import KenlmModel\n# Load model trained on English wikipedia\nmodel = KenlmModel.from_pretrained(\"wikipedia\", \"en\")\n# Get perplexity\nmodel.get_perplexity(\"I am very perplexed\")\n# 341.3 (low perplexity, since sentence style is formal and with no grammar mistakes)\nmodel.get_perplexity(\"im hella trippin\")\n# 46793.5 (high perplexity, since the sentence is colloquial and contains grammar mistakes)\nIn the example above we see that, since Wikipedia is a collection of encyclopedic articles, a KenLM model trained on it will naturally give lower perplexity scores to sentences with formal language and no grammar mistakes than colloquial sentences with grammar mistakes.",
    "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition": "Speech Emotion Recognition By Fine-Tuning Wav2Vec 2.0\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nCitation\nContact\nFramework versions\nSpeech Emotion Recognition By Fine-Tuning Wav2Vec 2.0\nThe model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task.\nThe dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are:\nemotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\nIt achieves the following results on the evaluation set:\nLoss: 0.5023\nAccuracy: 0.8223\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0001\ntrain_batch_size: 4\neval_batch_size: 4\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 8\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 3\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\n2.0752\n0.21\n30\n2.0505\n0.1359\n2.0119\n0.42\n60\n1.9340\n0.2474\n1.8073\n0.63\n90\n1.5169\n0.3902\n1.5418\n0.84\n120\n1.2373\n0.5610\n1.1432\n1.05\n150\n1.1579\n0.5610\n0.9645\n1.26\n180\n0.9610\n0.6167\n0.8811\n1.47\n210\n0.8063\n0.7178\n0.8756\n1.68\n240\n0.7379\n0.7352\n0.8208\n1.89\n270\n0.6839\n0.7596\n0.7118\n2.1\n300\n0.6664\n0.7735\n0.4261\n2.31\n330\n0.6058\n0.8014\n0.4394\n2.52\n360\n0.5754\n0.8223\n0.4581\n2.72\n390\n0.4719\n0.8467\n0.3967\n2.93\n420\n0.5023\n0.8223\nCitation\n@misc {enrique_hern√°ndez_calabr√©s_2024,\nauthor       = { {Enrique Hern√°ndez Calabr√©s} },\ntitle        = { wav2vec2-lg-xlsr-en-speech-emotion-recognition (Revision 17cf17c) },\nyear         = 2024,\nurl          = { https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition },\ndoi          = { 10.57967/hf/2045 },\npublisher    = { Hugging Face }\n}\nContact\nAny doubt, contact me on Twitter.\nFramework versions\nTransformers 4.8.2\nPytorch 1.9.0+cu102\nDatasets 1.9.0\nTokenizers 0.10.3",
    "facebook/contriever": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nUsage (HuggingFace Transformers)\nThis model has been trained without supervision following the approach described in Towards Unsupervised Dense Information Retrieval with Contrastive Learning. The associated GitHub repository is available here https://github.com/facebookresearch/contriever.\nUsage (HuggingFace Transformers)\nUsing the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding.\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\nmodel = AutoModel.from_pretrained('facebook/contriever')\nsentences = [\n\"Where was Marie Curie born?\",\n\"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n\"Born in Paris on 15 May 1859, Pierre Curie was the son of Eug√®ne Curie, a doctor of French Catholic origin from Alsace.\"\n]\n# Apply tokenizer\ninputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\noutputs = model(**inputs)\n# Mean pooling\ndef mean_pooling(token_embeddings, mask):\ntoken_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\nsentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\nreturn sentence_embeddings\nembeddings = mean_pooling(outputs[0], inputs['attention_mask'])",
    "facebook/convnext-large-224": "ConvNeXT (large-sized model)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nConvNeXT (large-sized model)\nConvNeXT model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper A ConvNet for the 2020s by Liu et al. and first released in this repository.\nDisclaimer: The team releasing ConvNeXT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and \"modernized\" its design by taking the Swin Transformer as inspiration.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import ConvNextImageProcessor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\nprocessor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-large-224\")\nmodel = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-large-224\")\ninputs = processor(image, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\nFor more code examples, we refer to the documentation.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2201-03545,\nauthor    = {Zhuang Liu and\nHanzi Mao and\nChao{-}Yuan Wu and\nChristoph Feichtenhofer and\nTrevor Darrell and\nSaining Xie},\ntitle     = {A ConvNet for the 2020s},\njournal   = {CoRR},\nvolume    = {abs/2201.03545},\nyear      = {2022},\nurl       = {https://arxiv.org/abs/2201.03545},\neprinttype = {arXiv},\neprint    = {2201.03545},\ntimestamp = {Thu, 20 Jan 2022 14:21:35 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2201-03545.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "facebook/dino-vitb8": "Vision Transformer (base-sized model, patch size 8) trained using DINO\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nVision Transformer (base-sized model, patch size 8) trained using DINO\nVision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.\nDisclaimer: The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels.\nImages are presented to the model as a sequence of fixed-size patches (resolution 8x8), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nNote that this model does not include any fine-tuned heads.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model:\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('facebook/dino-vitb8')\nmodel = ViTModel.from_pretrained('facebook/dino-vitb8')\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2104-14294,\nauthor    = {Mathilde Caron and\nHugo Touvron and\nIshan Misra and\nHerv{\\'{e}} J{\\'{e}}gou and\nJulien Mairal and\nPiotr Bojanowski and\nArmand Joulin},\ntitle     = {Emerging Properties in Self-Supervised Vision Transformers},\njournal   = {CoRR},\nvolume    = {abs/2104.14294},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2104.14294},\narchivePrefix = {arXiv},\neprint    = {2104.14294},\ntimestamp = {Tue, 04 May 2021 15:12:43 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2104-14294.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "facebook/dino-vits16": "Vision Transformer (small-sized model, patch size 16) trained using DINO\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nVision Transformer (small-sized model, patch size 16) trained using DINO\nVision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.\nDisclaimer: The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nNote that this model does not include any fine-tuned heads.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model:\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('facebook/dino-vits16')\nmodel = ViTModel.from_pretrained('facebook/dino-vits16')\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2104-14294,\nauthor    = {Mathilde Caron and\nHugo Touvron and\nIshan Misra and\nHerv{\\'{e}} J{\\'{e}}gou and\nJulien Mairal and\nPiotr Bojanowski and\nArmand Joulin},\ntitle     = {Emerging Properties in Self-Supervised Vision Transformers},\njournal   = {CoRR},\nvolume    = {abs/2104.14294},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2104.14294},\narchivePrefix = {arXiv},\neprint    = {2104.14294},\ntimestamp = {Tue, 04 May 2021 15:12:43 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2104-14294.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}"
}