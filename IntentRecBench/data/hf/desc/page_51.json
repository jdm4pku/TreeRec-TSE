{
    "keras-io/multimodal-entailment": "Tensorflow Keras Implementation of Multimodal entailment.\nBackground Information\nIntroduction\nWhat is multimodal entailment?\nTensorflow Keras Implementation of Multimodal entailment.\nThis repo contains the models Multimodal Entailment.\nCredits: Sayak Paul - Original Author\nHF Contribution: Rishav Chandra Varma\nBackground Information\nIntroduction\nIn this example, we will build and train a model for predicting multimodal entailment. We will be using the multimodal entailment dataset recently introduced by Google Research.\nWhat is multimodal entailment?\nOn social media platforms, to audit and moderate content we may want to find answers to the following questions in near real-time:\nDoes a given piece of information contradict the other?\nDoes a given piece of information imply the other?\nIn NLP, this task is called analyzing textual entailment. However, that's only when the information comes from text content. In practice, it's often the case the information available comes not just from text content, but from a multimodal combination of text, images, audio, video, etc. Multimodal entailment is simply the extension of textual entailment to a variety of new input modalities.",
    "kha-white/manga-ocr-base": "Manga OCR\nManga OCR\nOptical character recognition for Japanese text, with the main focus being Japanese manga.\nIt uses Vision Encoder Decoder framework.\nManga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality\ntext recognition, robust against various scenarios specific to manga:\nboth vertical and horizontal text\ntext with furigana\ntext overlaid on images\nwide variety of fonts and font styles\nlow quality images\nCode is available here.",
    "martin-ha/toxic-comment-model": "Model description\nHow to use\nLimitations and Bias\nTraining data\nTraining procedure\nEvaluation results\nModel description\nThis model is a fine-tuned version of the DistilBERT model to classify toxic comments.\nHow to use\nYou can use the model with the following code.\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\nmodel_path = \"martin-ha/toxic-comment-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\npipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))\nLimitations and Bias\nThis model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics here. But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.\nsubgroup\nsubgroup_size\nsubgroup_auc\nbpsn_auc\nbnsp_auc\nmuslim\n108\n0.689\n0.811\n0.88\njewish\n40\n0.749\n0.86\n0.825\nhomosexual_gay_or_lesbian\n56\n0.795\n0.706\n0.972\nblack\n84\n0.866\n0.758\n0.975\nwhite\n112\n0.876\n0.784\n0.97\nfemale\n306\n0.898\n0.887\n0.948\nchristian\n231\n0.904\n0.917\n0.93\nmale\n225\n0.922\n0.862\n0.967\npsychiatric_or_mental_illness\n26\n0.924\n0.907\n0.95\nThe table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence \"Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion.\" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.\nTraining data\nThe training data comes this Kaggle competition. We use 10% of the train.csv data to train the model.\nTraining procedure\nYou can see this documentation and codes for how we train the model. It takes about 3 hours in a P-100 GPU.\nEvaluation results\nThe model achieves 94% accuracy and 0.59 f1-score in a 10000 rows held-out test set.",
    "mental/mental-roberta-base": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nMentalRoBERTa\nUsage\nPaper\nSocial Impact\nMentalRoBERTa\nMentalRoBERTa is a model initialized with RoBERTa-Base (cased_L-12_H-768_A-12) and trained with mental health-related posts collected from Reddit.\nWe follow the standard pretraining protocols of BERT and RoBERTa with Huggingface‚Äôs Transformers library.\nWe use four Nvidia Tesla v100 GPUs to train the two language models. We set the batch size to 16 per GPU, evaluate every 1,000 steps, and train for 624,000 iterations. Training with four GPUs takes around eight days.\nMore domain-specific pretrained models for mental health are available at https://huggingface.co/AIMH\nUsage\nLoad the model via Huggingface‚Äôs Transformers library:\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"mental/mental-roberta-base\")\nmodel = AutoModel.from_pretrained(\"mental/mental-roberta-base\")\nTo minimize the influence of worrying mask predictions, this model is gated.  To download a gated model, you‚Äôll need to be authenticated.\nKnow more about gated models.\nPaper\nFor more details, refer to the paper MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare.\n@inproceedings{ji2022mentalbert,\ntitle     = {{MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare}},\nauthor    = {Shaoxiong Ji and Tianlin Zhang and Luna Ansari and Jie Fu and Prayag Tiwari and Erik Cambria},\nyear      = {2022},\nbooktitle = {Proceedings of LREC}\n}\nSocial Impact\nWe train and release masked language models for mental health to facilitate the automatic detection of mental disorders in online social content for non-clinical use.\nThe models may help social workers find potential individuals in need of early prevention.\nHowever, the model predictions are not psychiatric diagnoses.\nWe recommend anyone who suffers from mental health issues to call the local mental health helpline and seek professional help if possible.\nData privacy is an important issue, and we try to minimize the privacy impact when using social posts for model training.\nDuring the data collection process, we only use anonymous posts that are manifestly available to the public.\nWe do not collect user profiles even though they are also manifestly public online.\nWe have not attempted to identify the anonymous users or interact with any anonymous users.\nThe collected data are stored securely with password protection even though they are collected from the open web.\nThere might also be some bias, fairness, uncertainty, and interpretability issues during the data collection and model training.\nEvaluation of those issues is essential in future research.",
    "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext": "MSR BiomedBERT (abstracts + full text)\nCitation\nMSR BiomedBERT (abstracts + full text)\nThis model was previously named \"PubMedBERT (abstracts + full text)\".\nYou can either adopt the new model name \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\" or update your transformers library to version 4.22+ if you need to refer to the old name.\nPretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.\nBiomedBERT is pretrained from scratch using abstracts from PubMed and full-text articles from PubMedCentral. This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the Biomedical Language Understanding and Reasoning Benchmark.\nCitation\nIf you find BiomedBERT useful in your research, please cite the following paper:\n@misc{pubmedbert,\nauthor = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},\ntitle = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},\nyear = {2020},\neprint = {arXiv:2007.15779},\n}",
    "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract": "MSR BiomedBERT (abstracts only)\nCitation\nMSR BiomedBERT (abstracts only)\nThis model was previously named \"PubMedBERT (abstracts)\".\nYou can either adopt the new model name \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\" or update your transformers library to version 4.22+ if you need to refer to the old name.\nPretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.\nThis BiomedBERT is pretrained from scratch using abstracts from PubMed. This model achieves state-of-the-art performance on several biomedical NLP tasks, as shown on the Biomedical Language Understanding and Reasoning Benchmark.\nCitation\nIf you find BiomedBERT useful in your research, please cite the following paper:\n@misc{pubmedbert,\nauthor = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},\ntitle = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},\nyear = {2020},\neprint = {arXiv:2007.15779},\n}",
    "microsoft/DialogRPT-updown": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nDemo\nDialog Ranking Pretrained Transformers\nContact:\nCitation:\nDialogRPT-updown\nDialog Ranking Pretrained Transformers\nContact:\nCitation:\nDemo\nPlease try this ‚û§‚û§‚û§ Colab Notebook Demo (click me!)\nContext\nResponse\nupdown score\nI love NLP!\nHere‚Äôs a free textbook (URL) in case anyone needs it.\n0.613\nI love NLP!\nMe too!\n0.111\nThe updown score predicts how likely the response is getting upvoted.\nDialogRPT-updown\nDialog Ranking Pretrained Transformers\nHow likely a dialog response is upvoted üëç and/or gets replied üí¨?\nThis is what DialogRPT is learned to predict.\nIt is a set of dialog response ranking models proposed by Microsoft Research NLP Group trained on 100 + millions of human feedback data.\nIt can be used to improve existing dialog generation model (e.g., DialoGPT) by re-ranking the generated response candidates.\nQuick Links:\nEMNLP'20 Paper\nDataset, training, and evaluation\nColab Notebook Demo\nWe considered the following tasks and provided corresponding pretrained models. This page is for the updown task, and other model cards can be found in table below.\nTask\nDescription\nPretrained model\nHuman feedback\ngiven a context and its two human responses, predict...\nupdown\n... which gets more upvotes?\nthis model\nwidth\n... which gets more direct replies?\nmodel card\ndepth\n... which gets longer follow-up thread?\nmodel card\nHuman-like (human vs fake)\ngiven a context and one human response, distinguish it with...\nhuman_vs_rand\n... a random human response\nmodel card\nhuman_vs_machine\n... a machine generated response\nmodel card\nContact:\nPlease create an issue on our repo\nCitation:\n@inproceedings{gao2020dialogrpt,\ntitle={Dialogue Response RankingTraining with Large-Scale Human Feedback Data},\nauthor={Xiang Gao and Yizhe Zhang and Michel Galley and Chris Brockett and Bill Dolan},\nyear={2020},\nbooktitle={EMNLP}\n}",
    "microsoft/Multilingual-MiniLM-L12-H384": "MiniLM: Small and Fast Pre-trained Models for Language Understanding and Generation\nMultilingual Pretrained Model\nCross-Lingual Natural Language Inference - XNLI\nCross-Lingual Question Answering - MLQA\nCitation\nMiniLM: Small and Fast Pre-trained Models for Language Understanding and Generation\nMiniLM is a distilled model from the paper \"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\".\nPlease find the information about preprocessing, training and full details of the MiniLM in the original MiniLM repository.\nPlease note: This checkpoint uses BertModel with XLMRobertaTokenizer so AutoTokenizer won't work with this checkpoint!\nMultilingual Pretrained Model\nMultilingual-MiniLMv1-L12-H384: 12-layer, 384-hidden, 12-heads, 21M Transformer parameters, 96M embedding parameters\nMultilingual MiniLM uses the same tokenizer as XLM-R. But the Transformer architecture of our model is the same as BERT. We provide the fine-tuning code on XNLI based on huggingface/transformers. Please replace run_xnli.py in transformers with ours to fine-tune multilingual MiniLM.\nWe evaluate the multilingual MiniLM on cross-lingual natural language inference benchmark (XNLI) and cross-lingual question answering benchmark (MLQA).\nCross-Lingual Natural Language Inference - XNLI\nWe evaluate our model on cross-lingual transfer from English to other languages. Following Conneau et al. (2019), we select the best single model on the joint dev set of all the languages.\nModel\n#Layers\n#Hidden\n#Transformer Parameters\nAverage\nen\nfr\nes\nde\nel\nbg\nru\ntr\nar\nvi\nth\nzh\nhi\nsw\nur\nmBERT\n12\n768\n85M\n66.3\n82.1\n73.8\n74.3\n71.1\n66.4\n68.9\n69.0\n61.6\n64.9\n69.5\n55.8\n69.3\n60.0\n50.4\n58.0\nXLM-100\n16\n1280\n315M\n70.7\n83.2\n76.7\n77.7\n74.0\n72.7\n74.1\n72.7\n68.7\n68.6\n72.9\n68.9\n72.5\n65.6\n58.2\n62.4\nXLM-R Base\n12\n768\n85M\n74.5\n84.6\n78.4\n78.9\n76.8\n75.9\n77.3\n75.4\n73.2\n71.5\n75.4\n72.5\n74.9\n71.1\n65.2\n66.5\nmMiniLM-L12xH384\n12\n384\n21M\n71.1\n81.5\n74.8\n75.7\n72.9\n73.0\n74.5\n71.3\n69.7\n68.8\n72.1\n67.8\n70.0\n66.2\n63.3\n64.2\nThis example code fine-tunes 12-layer multilingual MiniLM on XNLI.\n# run fine-tuning on XNLI\nDATA_DIR=/{path_of_data}/\nOUTPUT_DIR=/{path_of_fine-tuned_model}/\nMODEL_PATH=/{path_of_pre-trained_model}/\npython ./examples/run_xnli.py --model_type minilm \\\n--output_dir ${OUTPUT_DIR} --data_dir ${DATA_DIR} \\\n--model_name_or_path microsoft/Multilingual-MiniLM-L12-H384 \\\n--tokenizer_name xlm-roberta-base \\\n--config_name ${MODEL_PATH}/multilingual-minilm-l12-h384-config.json \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_gpu_train_batch_size 128 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 5 \\\n--per_gpu_eval_batch_size 32 \\\n--weight_decay 0.001 \\\n--warmup_steps 500 \\\n--save_steps 1500 \\\n--logging_steps 1500 \\\n--eval_all_checkpoints \\\n--language en \\\n--fp16 \\\n--fp16_opt_level O2\nCross-Lingual Question Answering - MLQA\nFollowing Lewis et al. (2019b), we adopt SQuAD 1.1 as training data and use MLQA English development data for early stopping.\nModel F1 Score\n#Layers\n#Hidden\n#Transformer Parameters\nAverage\nen\nes\nde\nar\nhi\nvi\nzh\nmBERT\n12\n768\n85M\n57.7\n77.7\n64.3\n57.9\n45.7\n43.8\n57.1\n57.5\nXLM-15\n12\n1024\n151M\n61.6\n74.9\n68.0\n62.2\n54.8\n48.8\n61.4\n61.1\nXLM-R Base (Reported)\n12\n768\n85M\n62.9\n77.8\n67.2\n60.8\n53.0\n57.9\n63.1\n60.2\nXLM-R Base (Our fine-tuned)\n12\n768\n85M\n64.9\n80.3\n67.0\n62.7\n55.0\n60.4\n66.5\n62.3\nmMiniLM-L12xH384\n12\n384\n21M\n63.2\n79.4\n66.1\n61.2\n54.9\n58.5\n63.1\n59.0\nCitation\nIf you find MiniLM useful in your research, please cite the following paper:\n@misc{wang2020minilm,\ntitle={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},\nauthor={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},\nyear={2020},\neprint={2002.10957},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "microsoft/beit-base-finetuned-ade-640-640": "BEiT (base-sized model, fine-tuned on ADE20k)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nBEiT (base-sized model, fine-tuned on ADE20k)\nBEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ADE20k (an important benchmark for semantic segmentation of images) at resolution 640x640. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.\nDisclaimer: The team releasing BEiT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). In contrast to the original ViT model, BEiT is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. The pre-training objective for the model is to predict visual tokens from the encoder of OpenAI's DALL-E's VQ-VAE, based on masked patches.\nNext, the model was fine-tuned in a supervised fashion on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings, and perform classification of images by mean-pooling the final hidden states of the patches, instead of placing a linear layer on top of the final hidden state of the [CLS] token.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: for semantic segmentation, one can just add one of the decode heads available in the mmseg library for example, and fine-tune the model in a supervised fashion on annotated images. This is what the authors did: they fine-tuned BEiT with an UperHead segmentation decode head, allowing it to obtain SOTA results on important benchmarks such as ADE20k and CityScapes.\nIntended uses & limitations\nYou can use the raw model for semantic segmentation of images. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model for semantic segmentation:\nfrom transformers import BeitFeatureExtractor, BeitForSemanticSegmentation\nfrom datasets import load_dataset\nfrom PIL import Image\n# load ADE20k image\nds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\nimage = Image.open(ds[0]['file'])\nfeature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\nmodel = BeitForSemanticSegmentation.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# logits are of shape (batch_size, num_labels, height/4, width/4)\nlogits = outputs.logits\nCurrently, both the feature extractor and model support PyTorch.\nTraining data\nThis BEiT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, and fine-tuned on ADE20k, a dataset consisting of thousands of annotated images and 150 classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are cropped and padded to the same resolution (640x640) and normalized across the RGB channels with the ImageNet mean and standard deviation.\nPretraining\nFor all pre-training related hyperparameters, we refer to page 15 of the original paper.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to tables 1 and 2 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\nauthor    = {Hangbo Bao and\nLi Dong and\nFuru Wei},\ntitle     = {BEiT: {BERT} Pre-Training of Image Transformers},\njournal   = {CoRR},\nvolume    = {abs/2106.08254},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2106.08254},\narchivePrefix = {arXiv},\neprint    = {2106.08254},\ntimestamp = {Tue, 29 Jun 2021 16:55:04 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "microsoft/mdeberta-v3-base": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nCitation\nDeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nDeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data.\nIn DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.\nPlease check the official repository for more implementation details and updates.\nmDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.\nThe mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.\nFine-tuning on NLU tasks\nWe present the dev results on XNLI with zero-shot cross-lingual transfer setting, i.e. training with English data only, test on other languages.\nModel\navg\nen\nfr\nes\nde\nel\nbg\nru\ntr\nar\nvi\nth\nzh\nhi\nsw\nur\nXLM-R-base\n76.2\n85.8\n79.7\n80.7\n78.7\n77.5\n79.6\n78.1\n74.2\n73.8\n76.5\n74.6\n76.7\n72.4\n66.5\n68.3\nmDeBERTa-base\n79.8+/-0.2\n88.2\n82.6\n84.4\n82.7\n82.3\n82.4\n80.8\n79.5\n78.5\n78.1\n76.4\n79.5\n75.9\n73.9\n72.4\nFine-tuning with HF transformers\n#!/bin/bash\ncd transformers/examples/pytorch/text-classification/\npip install datasets\noutput_dir=\"ds_results\"\nnum_gpus=8\nbatch_size=4\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\nrun_xnli.py \\\n--model_name_or_path microsoft/mdeberta-v3-base \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--train_language en \\\n--language en \\\n--evaluation_strategy steps \\\n--max_seq_length 256 \\\n--warmup_steps 3000 \\\n--per_device_train_batch_size ${batch_size} \\\n--learning_rate 2e-5 \\\n--num_train_epochs 6 \\\n--output_dir $output_dir \\\n--overwrite_output_dir \\\n--logging_steps 1000 \\\n--logging_dir $output_dir\nCitation\nIf you find DeBERTa useful for your work, please cite the following papers:\n@misc{he2021debertav3,\ntitle={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},\nauthor={Pengcheng He and Jianfeng Gao and Weizhu Chen},\nyear={2021},\neprint={2111.09543},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}",
    "microsoft/swin-tiny-patch4-window7-224": "Swin Transformer (tiny-sized model)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nSwin Transformer (tiny-sized model)\nSwin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository.\nDisclaimer: The team releasing Swin Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.\nSource\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nFor more code examples, we refer to the documentation.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2103-14030,\nauthor    = {Ze Liu and\nYutong Lin and\nYue Cao and\nHan Hu and\nYixuan Wei and\nZheng Zhang and\nStephen Lin and\nBaining Guo},\ntitle     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},\njournal   = {CoRR},\nvolume    = {abs/2103.14030},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2103.14030},\neprinttype = {arXiv},\neprint    = {2103.14030},\ntimestamp = {Thu, 08 Apr 2021 07:53:26 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "microsoft/trocr-large-handwritten": "TrOCR (large-sized model, fine-tuned on IAM)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nTrOCR (large-sized model, fine-tuned on IAM)\nTrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\nDisclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\nIntended uses & limitations\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model in PyTorch:\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n# load image from the IAM database\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nBibTeX entry and citation info\n@misc{li2021trocr,\ntitle={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},\nauthor={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\nyear={2021},\neprint={2109.10282},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "microsoft/trocr-small-handwritten": "TrOCR (small-sized model, fine-tuned on IAM)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nTrOCR (small-sized model, fine-tuned on IAM)\nTrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\nModel description\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\nIntended uses & limitations\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model in PyTorch:\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n# load image from the IAM database\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nBibTeX entry and citation info\n@misc{li2021trocr,\ntitle={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},\nauthor={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\nyear={2021},\neprint={2109.10282},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "microsoft/trocr-small-printed": "TrOCR (small-sized model, fine-tuned on SROIE)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nTrOCR (small-sized model, fine-tuned on SROIE)\nTrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\nModel description\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\nIntended uses & limitations\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model in PyTorch:\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n# load image from the IAM database (actually this model is meant to be used on printed text)\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nBibTeX entry and citation info\n@misc{li2021trocr,\ntitle={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},\nauthor={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\nyear={2021},\neprint={2109.10282},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "microsoft/wavlm-base-plus-sv": "WavLM-Base-Plus for Speaker Verification\nFine-tuning details\nUsage\nSpeaker Verification\nLicense\nWavLM-Base-Plus for Speaker Verification\nMicrosoft's WavLM\nThe model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss. When using the model, make sure that your speech input is also sampled at 16kHz.\nNote: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.\nThe model was pre-trained on:\n60,000 hours of Libri-Light\n10,000 hours of GigaSpeech\n24,000 hours of VoxPopuli\nPaper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing\nAuthors: Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei\nAbstract\nSelf-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.\nThe original model can be found under https://github.com/microsoft/unilm/tree/master/wavlm.\nFine-tuning details\nThe model is fine-tuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin Softmax loss\nX-Vectors: Robust DNN Embeddings for Speaker Recognition\nUsage\nSpeaker Verification\nfrom transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\nfrom datasets import load_dataset\nimport torch\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-plus-sv')\nmodel = WavLMForXVector.from_pretrained('microsoft/wavlm-base-plus-sv')\n# audio files are decoded on the fly\naudio = [x[\"array\"] for x in dataset[:2][\"audio\"]]\ninputs = feature_extractor(audio, padding=True, return_tensors=\"pt\")\nembeddings = model(**inputs).embeddings\nembeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n# the resulting embeddings can be used for cosine similarity-based retrieval\ncosine_sim = torch.nn.CosineSimilarity(dim=-1)\nsimilarity = cosine_sim(embeddings[0], embeddings[1])\nthreshold = 0.86  # the optimal threshold is dataset-dependent\nif similarity < threshold:\nprint(\"Speakers are not the same!\")\nLicense\nThe official license can be found here",
    "microsoft/wavlm-base": "YAML Metadata\nError:\n\"datasets\" must be a string\nWavLM-Base\nUsage\nSpeech Recognition\nSpeech Classification\nSpeaker Verification\nSpeaker Diarization\nContribution\nLicense\nWavLM-Base\nMicrosoft's WavLM\nThe base model pretrained on 16kHz sampled speech audio. When using the model, make sure that your speech input is also sampled at 16kHz.\nNote: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.\nThe model was pre-trained on 960h of Librispeech.\nPaper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing\nAuthors: Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei\nAbstract\nSelf-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.\nThe original model can be found under https://github.com/microsoft/unilm/tree/master/wavlm.\nUsage\nThis is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be\nused in inference. The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the SUPERB benchmark.\nNote: The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence\nof phonemes before fine-tuning.\nSpeech Recognition\nTo fine-tune the model for speech recognition, see the official speech recognition example.\nSpeech Classification\nTo fine-tune the model for speech classification, see the official audio classification example.\nSpeaker Verification\nTODO\nSpeaker Diarization\nTODO\nContribution\nThe model was contributed by cywang and patrickvonplaten.\nLicense\nThe official license can be found here",
    "microsoft/wavlm-large": "WavLM-Large\nUsage\nSpeech Recognition\nSpeech Classification\nSpeaker Verification\nSpeaker Diarization\nContribution\nLicense\nWavLM-Large\nMicrosoft's WavLM\nThe large model pretrained on 16kHz sampled speech audio. When using the model, make sure that your speech input is also sampled at 16kHz.\nNote: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.\nThe model was pre-trained on:\n60,000 hours of Libri-Light\n10,000 hours of GigaSpeech\n24,000 hours of VoxPopuli\nPaper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing\nAuthors: Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei\nAbstract\nSelf-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.\nThe original model can be found under https://github.com/microsoft/unilm/tree/master/wavlm.\nUsage\nThis is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be\nused in inference. The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the SUPERB benchmark.\nNote: The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence\nof phonemes before fine-tuning.\nSpeech Recognition\nTo fine-tune the model for speech recognition, see the official speech recognition example.\nSpeech Classification\nTo fine-tune the model for speech classification, see the official audio classification example.\nSpeaker Verification\nTODO\nSpeaker Diarization\nTODO\nContribution\nThe model was contributed by cywang and patrickvonplaten.\nLicense\nThe official license can be found here",
    "monologg/kobert": "KoBERT\nHow to use\nReference\nKoBERT\nHow to use\nIf you want to import KoBERT tokenizer with AutoTokenizer, you should give trust_remote_code=True.\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained(\"monologg/kobert\")\ntokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\nReference\nhttps://github.com/SKTBrain/KoBERT",
    "moussaKam/barthez": "A french sequence to sequence pretrained model based on BART.\nBARThez is pretrained by learning to reconstruct a corrupted input sentence. A corpus of 66GB of french raw text is used to carry out the pretraining.\nUnlike already existing BERT-based French language models such as CamemBERT and FlauBERT, BARThez is particularly well-suited for generative tasks (such as abstractive summarization), since not only its encoder but also its decoder is pretrained.\nIn addition to BARThez that is pretrained from scratch, we continue the pretraining of a multilingual BART mBART which boosted its performance in both discriminative and generative tasks. We call the french adapted version mBARThez.\nModel\nArchitecture\n#layers\n#params\nBARThez\nBASE\n12\n165M\nmBARThez\nLARGE\n24\n458M\npaper: https://arxiv.org/abs/2010.12321 github: https://github.com/moussaKam/BARThez\n@article{eddine2020barthez,\ntitle={BARThez: a Skilled Pretrained French Sequence-to-Sequence Model},\nauthor={Eddine, Moussa Kamal and Tixier, Antoine J-P and Vazirgiannis, Michalis},\njournal={arXiv preprint arXiv:2010.12321},\nyear={2020}\n}",
    "moussaKam/mbarthez": "A french sequence to sequence pretrained model based on BART.\nBARThez is pretrained by learning to reconstruct a corrupted input sentence. A corpus of 66GB of french raw text is used to carry out the pretraining.\nUnlike already existing BERT-based French language models such as CamemBERT and FlauBERT, BARThez is particularly well-suited for generative tasks (such as abstractive summarization), since not only its encoder but also its decoder is pretrained.\nIn addition to BARThez that is pretrained from scratch, we continue the pretraining of a multilingual BART mBART which boosted its performance in both discriminative and generative tasks. We call the french adapted version mBARThez.\nModel\nArchitecture\n#layers\n#params\nBARThez\nBASE\n12\n165M\nmBARThez\nLARGE\n24\n458M\npaper: https://arxiv.org/abs/2010.12321 github: https://github.com/moussaKam/BARThez\n@article{eddine2020barthez,\ntitle={BARThez: a Skilled Pretrained French Sequence-to-Sequence Model},\nauthor={Eddine, Moussa Kamal and Tixier, Antoine J-P and Vazirgiannis, Michalis},\njournal={arXiv preprint arXiv:2010.12321},\nyear={2020}\n}",
    "mrm8488/bert-spanish-cased-finetuned-ner": "Spanish BERT (BETO) + NER\nDetails of the downstream task (NER) - Dataset\nMetrics on evaluation set:\nComparison:\nModel in action\nSpanish BERT (BETO) + NER\nThis model is a fine-tuned on NER-C version of the Spanish BERT cased (BETO) for NER downstream task.\nDetails of the downstream task (NER) - Dataset\nDataset:  CONLL Corpora ES\nI preprocessed the dataset and split it as train / dev (80/20)\nDataset\n# Examples\nTrain\n8.7 K\nDev\n2.2 K\nFine-tune on NER script provided by Huggingface\nLabels covered:\nB-LOC\nB-MISC\nB-ORG\nB-PER\nI-LOC\nI-MISC\nI-ORG\nI-PER\nO\nMetrics on evaluation set:\nMetric\n# score\nF1\n90.17\nPrecision\n89.86\nRecall\n90.47\nComparison:\nModel\n# F1 score\nSize(MB)\nbert-base-spanish-wwm-cased (BETO)\n88.43\n421\nbert-spanish-cased-finetuned-ner (this one)\n90.17\n420\nBest Multilingual BERT\n87.38\n681\nTinyBERT-spanish-uncased-finetuned-ner\n70.00\n55\nModel in action\nFast usage with pipelines:\nfrom transformers import pipeline\nnlp_ner = pipeline(\n\"ner\",\nmodel=\"mrm8488/bert-spanish-cased-finetuned-ner\",\ntokenizer=(\n'mrm8488/bert-spanish-cased-finetuned-ner',\n{\"use_fast\": False}\n))\ntext = 'Mis amigos est√°n pensando viajar a Londres este verano'\nnlp_ner(text)\n#Output: [{'entity': 'B-LOC', 'score': 0.9998720288276672, 'word': 'Londres'}]\nCreated by Manuel Romero/@mrm8488\nMade with ‚ô• in Spain",
    "mrm8488/t5-base-finetuned-emotion": "T5-base fine-tuned for Emotion Recognition üòÇüò¢üò°üòÉüòØ\nDetails of T5\nDetails of the downstream task (Sentiment Recognition) - Dataset üìö\nModel fine-tuning üèãÔ∏è‚Äç\nTest set metrics üßæ\nModel in Action üöÄ\nT5-base fine-tuned for Emotion Recognition üòÇüò¢üò°üòÉüòØ\nGoogle's T5 base fine-tuned on emotion recognition dataset for Emotion Recognition downstream task.\nDetails of T5\nThe T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ‚ÄúColossal Clean Crawled Corpus‚Äù, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\nDetails of the downstream task (Sentiment Recognition) - Dataset üìö\nElvis Saravia has gathered a great dataset for emotion recognition. It allows to classifiy the text into one of the following 6 emotions:\nsadness üò¢\njoy üòÉ\nlove ü•∞\nanger üò°\nfear üò±\nsurprise üòØ\nModel fine-tuning üèãÔ∏è‚Äç\nThe training script is a slightly modified version of this Colab Notebook created by Suraj Patil, so all credits to him!\nTest set metrics üßæ\nprecision\nrecall\nf1-score\nsupport\nanger\n0.93\n0.92\n0.93\n275\nfear\n0.91\n0.87\n0.89\n224\njoy\n0.97\n0.94\n0.95\n695\nlove\n0.80\n0.91\n0.85\n159\nsadness\n0.97\n0.97\n0.97\n521\nsurpirse\n0.73\n0.89\n0.80\n66\naccuracy\n0.93\n2000\nmacro avg\n0.89\n0.92\n0.90\n2000\nweighted avg\n0.94\n0.93\n0.93\n2000\nModel in Action üöÄ\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\nmodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\ndef get_emotion(text):\ninput_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\noutput = model.generate(input_ids=input_ids,\nmax_length=2)\ndec = [tokenizer.decode(ids) for ids in output]\nlabel = dec[0]\nreturn label\nget_emotion(\"i feel as if i havent blogged in ages are at least truly blogged i am doing an update cute\") # Output: 'joy'\nget_emotion(\"i have a feeling i kinda lost my best friend\") # Output: 'sadness'\nCreated by Manuel Romero/@mrm8488 | LinkedIn\nMade with ‚ô• in Spain",
    "ntu-spml/distilhubert": "DistilHuBERT\nUsage\nDistilHuBERT\nDistilHuBERT by NTU Speech Processing & Machine Learning Lab\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\nNote: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.\nPaper: DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\nAbstract\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https://github.com/s3prl/s3prl/tree/master/s3prl/upstream/distiller .\nUsage\nSee this blog for more information on how to fine-tune the model. Note that the class Wav2Vec2ForCTC has to be replaced by HubertForCTC.",
    "nvidia/segformer-b0-finetuned-ade-512-512": "SegFormer (b0-sized) model fine-tuned on ADE20k\nModel description\nIntended uses & limitations\nHow to use\nLicense\nBibTeX entry and citation info\nSegFormer (b0-sized) model fine-tuned on ADE20k\nSegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\nDisclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nSegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.\nIntended uses & limitations\nYou can use the raw model for semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\nFor more code examples, we refer to the documentation.\nLicense\nThe license for this model can be found here.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2105-15203,\nauthor    = {Enze Xie and\nWenhai Wang and\nZhiding Yu and\nAnima Anandkumar and\nJose M. Alvarez and\nPing Luo},\ntitle     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\nTransformers},\njournal   = {CoRR},\nvolume    = {abs/2105.15203},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2105.15203},\neprinttype = {arXiv},\neprint    = {2105.15203},\ntimestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "oliverguhr/german-sentiment-bert": "German Sentiment Classification with Bert\nUsing the Python package\nOutput class probabilities\nModel and Data\nCite\nGerman Sentiment Classification with Bert\nThis model was trained for sentiment classification of German language texts. To achieve the best results all model inputs needs to be preprocessed with the same procedure, that was applied during the training. To simplify the usage of the model,\nwe provide a Python package that bundles the code need for the preprocessing and inferencing.\nThe model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.\nYou can find more information about the dataset and the training process in the paper.\nUsing the Python package\nTo get started install the package from pypi:\npip install germansentiment\nfrom germansentiment import SentimentModel\nmodel = SentimentModel()\ntexts = [\n\"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n\"Total awesome!\",\"nicht so schlecht wie erwartet\",\n\"Der Test verlief positiv.\",\"Sie f√§hrt ein gr√ºnes Auto.\"]\nresult = model.predict_sentiment(texts)\nprint(result)\nThe code above will output following list:\n[\"negative\",\"negative\",\"positive\",\"positive\",\"neutral\", \"neutral\"]\nOutput class probabilities\nfrom germansentiment import SentimentModel\nmodel = SentimentModel()\nclasses, probabilities = model.predict_sentiment([\"das ist super\"], output_probabilities = True)\nprint(classes, probabilities)\n['positive'] [[['positive', 0.9761366844177246], ['negative', 0.023540444672107697], ['neutral', 0.00032294404809363186]]]\nModel and Data\nIf you are interested in code and data that was used to train this model please have a look at this repository and our paper. Here is a table of the F1 scores that this model achieves on different datasets. Since we trained this model with a newer version of the transformer library, the results are slightly better than reported in the paper.\nDataset\nF1 micro Score\nholidaycheck\n0.9568\nscare\n0.9418\nfilmstarts\n0.9021\ngermeval\n0.7536\nPotTS\n0.6780\nemotions\n0.9649\nsb10k\n0.7376\nLeipzig Wikipedia Corpus 2016\n0.9967\nall\n0.9639\nCite\nFor feedback and questions contact me view mail or Twitter @oliverguhr. Please cite us if you found this useful:\n@InProceedings{guhr-EtAl:2020:LREC,\nauthor    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  B√∂hme, Hans Joachim},\ntitle     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\nbooktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\nmonth          = {May},\nyear           = {2020},\naddress        = {Marseille, France},\npublisher      = {European Language Resources Association},\npages     = {1620--1625},\nurl       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}",
    "openai/clip-vit-base-patch16": "Model Card: CLIP\nModel Details\nModel Date\nModel Type\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nOut-of-Scope Use Cases\nData\nData Mission Statement\nPerformance and Limitations\nPerformance\nLimitations\nBias and Fairness\nFeedback\nWhere to send questions or comments about the model\nModel Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.\nModel Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they‚Äôre being deployed within.\nModel Date\nJanuary 2021\nModel Type\nThe base model uses a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\nDocuments\nBlog Post\nCLIP Paper\nUse with Transformers\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nOut-of-Scope Use Cases\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP‚Äôs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\nData\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as YFCC100M. A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\nData Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\nPerformance and Limitations\nPerformance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanford Cars\nFGVC Aircraft\nVOC2007\nDTD\nOxford-IIIT Pet dataset\nCaltech101\nFlowers102\nMNIST\nSVHN\nIIIT5K\nHateful Memes\nSST-2\nUCF101\nKinetics700\nCountry211\nCLEVR Counting\nKITTI Distance\nSTL-10\nRareAct\nFlickr30\nMSCOCO\nImageNet\nImageNet-A\nImageNet-R\nImageNet Sketch\nObjectNet (ImageNet Overlap)\nYoutube-BB\nImageNet-Vid\nLimitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\nBias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from Fairface into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‚ÄòMiddle Eastern‚Äô having the highest accuracy (98.4%) and ‚ÄòWhite‚Äô having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\nFeedback\nWhere to send questions or comments about the model\nPlease use this Google Form",
    "openai/imagegpt-large": "ImageGPT (large-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nImageGPT (large-sized model)\nImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.\nDisclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels.\nThe goal for the model is simply to predict the next pixel value, given the previous ones.\nBy pre-training the model, it learns an inner representation of images that can then be used to:\nextract features useful for downstream tasks: one can either use ImageGPT to produce fixed image features, in order to train a linear model (like a sklearn logistic regression model or SVM). This is also referred to as \"linear probing\".\nperform (un)conditional image generation.\nIntended uses & limitations\nYou can use the raw model for either feature extractor or (un) conditional image generation. See the model hub to all ImageGPT variants.\nHow to use\nHere is how to use this model in PyTorch to perform unconditional image generation:\nfrom transformers import ImageGPTImageProcessor, ImageGPTForCausalImageModeling\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nprocessor = ImageGPTImageProcessor.from_pretrained('openai/imagegpt-large')\nmodel = ImageGPTForCausalImageModeling.from_pretrained('openai/imagegpt-large')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# unconditional generation of 8 images\nbatch_size = 8\ncontext = torch.full((batch_size, 1), model.config.vocab_size - 1) #initialize with SOS token\ncontext = torch.tensor(context).to(device)\noutput = model.generate(pixel_values=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40)\nclusters = processor.clusters\nn_px = processor.size\nsamples = output[:,1:].cpu().detach().numpy()\nsamples_img = [np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [n_px, n_px, 3]).astype(np.uint8) for s in samples] # convert color cluster tokens back to pixels\nf, axes = plt.subplots(1, batch_size, dpi=300)\nfor img, ax in zip(samples_img, axes):\nax.axis('off')\nax.imshow(img)\nTraining data\nThe ImageGPT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes.\nTraining procedure\nPreprocessing\nImages are first resized/rescaled to the same resolution (32x32) and normalized across the RGB channels. Next, color-clustering is performed. This means that every pixel is turned into one of 512 possible cluster values. This way, one ends up with a sequence of 32x32 = 1024 pixel values, rather than 32x32x3 = 3072, which is prohibitively large for Transformer-based models.\nPretraining\nTraining details can be found in section 3.4 of v2 of the paper.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to the original paper.\nBibTeX entry and citation info\n@InProceedings{pmlr-v119-chen20s,\ntitle = \t {Generative Pretraining From Pixels},\nauthor =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},\nbooktitle = \t {Proceedings of the 37th International Conference on Machine Learning},\npages = \t {1691--1703},\nyear = \t {2020},\neditor = \t {III, Hal Daum√É¬© and Singh, Aarti},\nvolume = \t {119},\nseries = \t {Proceedings of Machine Learning Research},\nmonth = \t {13--18 Jul},\npublisher =    {PMLR},\npdf = \t {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},\nurl = \t {https://proceedings.mlr.press/v119/chen20s.html\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "persiannlp/mt5-base-parsinlu-sentiment-analysis": "Sentiment Analysis (ÿ¢ŸÜÿßŸÑ€åÿ≤ ÿßÿ≠ÿ≥ÿßÿ≥ÿßÿ™)\nSentiment Analysis (ÿ¢ŸÜÿßŸÑ€åÿ≤ ÿßÿ≠ÿ≥ÿßÿ≥ÿßÿ™)\nThis is a mT5 model for sentiment analysis.\nHere is an example of how you can run this model:\nimport torch\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\nimport numpy as np\nmodel_name_or_path = \"persiannlp/mt5-base-parsinlu-sentiment-analysis\"\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\ndef model_predict(text_a, text_b):\nfeatures = tokenizer( [(text_a, text_b)], padding=\"max_length\", truncation=True, return_tensors='pt')\noutput = model(**features)\nlogits = output[0]\nprobs = torch.nn.functional.softmax(logits, dim=1).tolist()\nidx = np.argmax(np.array(probs))\nprint(labels[idx], probs)\ndef run_model(context, query, **generator_args):\ninput_ids = tokenizer.encode(context + \"<sep>\" + query, return_tensors=\"pt\")\nres = model.generate(input_ids, **generator_args)\noutput = tokenizer.batch_decode(res, skip_special_tokens=True)\nprint(output)\nreturn output\nrun_model(\n\"€å⁄© ŸÅ€åŸÑŸÖ ÿ∂ÿπ€åŸÅ ÿ®€å ŸÖÿ≠ÿ™Ÿàÿß ÿ®ÿØŸàŸÜ ŸÅ€åŸÑŸÖŸÜÿßŸÖŸá . ÿ¥ŸàÿÆ€å Ÿáÿß€å ÿ≥ÿÆ€åŸÅ .\",\n\"ŸÜÿ∏ÿ± ÿ¥ŸÖÿß ÿØÿ± ŸÖŸàÿ±ÿØ ÿØÿßÿ≥ÿ™ÿßŸÜÿå ŸÅ€åŸÑŸÖŸÜÿßŸÖŸáÿå ÿØ€åÿßŸÑŸà⁄Ø Ÿáÿß Ÿà ŸÖŸàÿ∂Ÿàÿπ ŸÅ€åŸÑŸÖ  ŸÑŸàŸÜŸá ÿ≤ŸÜÿ®Ÿàÿ± ⁄Ü€åÿ≥ÿ™ÿü\"\n)\nrun_model(\n\"ŸÅ€åŸÑŸÖ ÿ™ÿß Ÿàÿ≥ÿ∑ ŸÅ€åŸÑŸÖ €åÿπŸÜ€å ÿØŸÇ€åŸÇÿß ÿ™ÿß ÿ¨ÿß€å€å ⁄©Ÿá ŸÖÿπŸÑŸàŸÖ ŸÖ€åÿ¥Ÿá ÿ®⁄ÜŸá Ÿáÿß€å ÿßŸÖŸÑÿ¥€å ÿØŸÜÿ®ÿßŸÑ ÿ±ÿ∂ÿßŸÜ ÿÆ€åŸÑ€å ÿÆŸàÿ® Ÿà ÿ¨ÿ∞ÿßÿ® Ÿæ€åÿ¥ ŸÖ€åÿ±Ÿá ŸàŸÑ€å ÿØŸÇ€åŸÇÿß ÿßÿ≤ ŸáŸÖŸàŸÜÿ¨ÿßÿ¥ ÿ≥⁄©ÿ™Ÿá ŸÖ€åÿ≤ŸÜŸá Ÿà ÿÆŸÑÿßÿµ...\",\n\"ŸÜÿ∏ÿ± ÿ¥ŸÖÿß ÿ®Ÿá ÿµŸàÿ±ÿ™ ⁄©ŸÑ€å ÿØÿ± ŸÖŸàÿ±ÿØ ŸÅ€åŸÑŸÖ  ⁄òŸÜ ÿÆŸà⁄© ⁄Ü€åÿ≥ÿ™ÿü\"\n)\nrun_model(\n\"ÿßÿµŸÑÿß ÿ®Ÿá Ÿá€å⁄Ü ÿπŸÜŸàÿßŸÜ ÿπŸÑÿßŸÇŸá ŸÜÿØÿßÿ¥ÿ™ŸÖ ÿßÿ¨ÿ±ÿß€å ŸÖ€å ÿ≥€å ÿ≥€å Ÿæ€å ŸÜÿ¥ÿ≥ÿ™Ÿá ŸÖ€åŸÖ€åÿ±ÿØ ÿ±Ÿà€å Ÿæÿ±ÿØŸá ÿ≥€åŸÜŸÖÿß ÿ®ÿ®€åŸÜŸÖ  ÿØ€åÿßŸÑŸà⁄Ø Ÿáÿß€å ÿ™⁄©ÿ±ÿßÿ±€å   ŸáŸÑ€å⁄©ŸàŸæÿ™ÿ±  ŸÖÿßÿ¥€åŸÜ  ÿ¢ŸÑŸÜÿØŸÑŸàŸÜ  ŸÑÿ¶ŸàŸÜ  ŸæÿßŸæ€åŸàŸÜ  ÿ¢ÿÆŸá ⁄Üÿ±ÿßÿßÿßÿßÿßÿßÿßÿßÿßÿßÿßÿßÿßÿßÿß   ŸáŸÖŸàŸÜ ÿ≠ÿ≥€å ⁄©Ÿá ÿ™Ÿà€å ÿ™ÿßŸÑÿßÿ± Ÿàÿ≠ÿØÿ™ ÿ®ÿπÿØ ÿßÿ≤ ŸÜ€åŸÖ ÿ≥ÿßÿπÿ™ ÿ®Ÿá ÿ≥ÿ±ŸÖ ÿßŸàŸÖÿØ ÿßŸÖÿ¥ÿ® ÿ™Ÿà€å ÿ≥ÿßŸÑŸÜ ÿ≥€åŸÜŸÖÿß ÿ™ÿ¨ÿ±ÿ®Ÿá ⁄©ÿ±ÿØŸÖ ÿåÿ≠ÿ≥ ⁄Øÿ±€åÿ≤ ÿßÿ≤ ÿ≥ÿßŸÑŸÜ.......‚Å¶ ‚Å¶(„Éé‡≤†Áõä‡≤†)„Éé‚Å© \",\n\" ŸÜÿ∏ÿ± ÿ¥ŸÖÿß ÿØÿ± ŸÖŸàÿ±ÿØ ÿµÿØÿß⁄Øÿ∞ÿßÿ±€å Ÿà ÿ¨ŸÑŸàŸá Ÿáÿß€å ÿµŸàÿ™€å ŸÅ€åŸÑŸÖ  ŸÖÿ≥ÿÆÿ±Ÿá‚Äåÿ®ÿßÿ≤ ⁄Ü€åÿ≥ÿ™ÿü\"\n)\nrun_model(\n\" ⁄ØŸàŸÑ ŸÜÿÆŸàÿ±€åÿØ ÿß€åŸÜ ÿ±ŸÜ⁄Øÿßÿ±ŸÜ⁄Ø ŸÖ€åŸÜŸà ŸÜ€åÿ≥ÿ™ ÿ®ÿ±ÿß€å ÿ¥ÿ±⁄©ÿ™ ⁄Øÿ±ÿ¨€åŸá Ÿà ŸÖÿ™ÿßÿ≥ŸÅÿßŸÜŸá ÿß€åŸÜ ŸÖÿ≠ÿµŸàŸÑÿ¥ ÿßÿµŸÑÿß ŸÖÿ≤Ÿá ÿ±ŸÜ⁄Øÿßÿ±ŸÜ⁄Ø€å ⁄©Ÿá ÿßŸÜÿ™ÿ∏ÿßÿ± ÿØÿßÿ±€åÿØ ÿ±Ÿà ŸÜŸÖ€åÿØŸá \",\n\" ŸÜÿ∏ÿ± ÿ¥ŸÖÿß ÿØÿ± ŸÖŸàÿ±ÿØ ÿπÿ∑ÿ±ÿå ÿ®Ÿàÿå Ÿà ÿ∑ÿπŸÖ ÿß€åŸÜ ÿ®€åÿ≥⁄©Ÿà€åÿ™ Ÿà Ÿà€åŸÅÿ± ⁄Ü€åÿ≥ÿ™ÿü\"\n)\nrun_model(\n\"ÿØÿ± ŸÖŸÇÿß€åÿ≥Ÿá ÿ®ÿß ÿ≥ÿß€åÿ± ÿ®ÿ±ŸÜÿØŸáÿß€å ŸÖŸàÿ¨ŸàÿØ ÿØÿ± ÿ®ÿßÿ≤ÿßÿ± ÿ®ÿß ÿ™Ÿàÿ¨Ÿá ÿ®Ÿá ÿ≠ÿ±ÿßÿ¨€å ⁄©Ÿá ÿØÿßÿ¥ÿ™ ÿßÿ±ÿ≤ÿßŸÜÿ™ÿ± ÿ®\",\n\" ÿ¥ŸÖÿß ÿØÿ± ŸÖŸàÿ±ÿØ ŸÇ€åŸÖÿ™ Ÿà ÿßÿ±ÿ≤ÿ¥ ÿÆÿ±€åÿØ ÿß€åŸÜ ÿ≠ÿ®Ÿàÿ®ÿßÿ™ Ÿà ÿ≥Ÿà€åÿß ⁄Ü€åÿ≥ÿ™ÿü\"\n)\nrun_model(\n\"ŸÖŸÜ Ÿæÿ≥ÿ±ŸÖ ÿπÿßÿ¥ŸÇ ÿß€åŸÜÿßÿ≥ ŸàŸÑ€å ÿØ€å⁄ØŸá ÿ®Ÿá ÿÆÿßÿ∑ÿ± ÿ≠ŸÅÿ∏ ŸÖÿ≠€åÿ∑ ÿ≤€åÿ≥ÿ™ ŸÅŸÇÿ∑ ÿ≤ŸÖÿßŸÜŸáÿß€å€å ⁄©Ÿá ŸÖÿ¨ÿ®Ÿàÿ± ÿ®ÿßÿ¥ŸÖ ÿ¥€åÿ± ÿØŸàŸÜŸá ÿß€å ŸÖ€åÿÆÿ±ŸÖ Ÿà ÿ≥ÿπ€å ŸÖ€å⁄©ŸÜŸÖ ÿØ€å⁄ØŸá ⁄©ŸÖÿ™ÿ± ÿ¥€åÿ± ÿ®ÿß ÿ®ÿ≥ÿ™Ÿá ÿ®ŸÜÿØ€å ÿ™ÿ™ÿ±ÿßŸæ⁄© ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜŸÖ \",\n\"ŸÜÿ∏ÿ± ÿ¥ŸÖÿß ÿ®Ÿá ÿµŸàÿ±ÿ™ ⁄©ŸÑ€å ÿØÿ± ŸÖŸàÿ±ÿØ ÿß€åŸÜ ÿ¥€åÿ± ⁄Ü€åÿ≥ÿ™ÿü\"\n)\nFor more details, visit this page: https://github.com/persiannlp/parsinlu/",
    "prajjwal1/bert-tiny": "The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the official Google BERT repository.\nThis is one of the smaller pre-trained BERT variants, together with bert-mini bert-small and bert-medium. They were introduced in the study Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (arxiv), and ported to HF for the study Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics (arXiv). These models are supposed to be trained on a downstream task.\nIf you use the model, please consider citing both the papers:\n@misc{bhargava2021generalization,\ntitle={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics},\nauthor={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},\nyear={2021},\neprint={2110.01518},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@article{DBLP:journals/corr/abs-1908-08962,\nauthor    = {Iulia Turc and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {Well-Read Students Learn Better: The Impact of Student Initialization\non Knowledge Distillation},\njournal   = {CoRR},\nvolume    = {abs/1908.08962},\nyear      = {2019},\nurl       = {http://arxiv.org/abs/1908.08962},\neprinttype = {arXiv},\neprint    = {1908.08962},\ntimestamp = {Thu, 29 Aug 2019 16:32:34 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\nConfig of this model:\nprajjwal1/bert-tiny (L=2, H=128) Model Link\nOther models to check out:\nprajjwal1/bert-mini (L=4, H=256) Model Link\nprajjwal1/bert-small (L=4, H=512) Model Link\nprajjwal1/bert-medium (L=8, H=512) Model Link\nOriginal Implementation and more info can be found in this Github repository.\nTwitter: @prajjwal_1",
    "pyannote/embedding": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThe collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nüéπ Speaker embedding\nBasic usage\nAdvanced usage\nRunning on GPU\nExtract embedding from an excerpt\nExtract embeddings using a sliding window\nCitation\nUsing this open-source model in production?Consider switching to pyannoteAI for better and faster options.\nüéπ Speaker embedding\nRelies on pyannote.audio 2.1: see installation instructions.\nThis model is based on the canonical x-vector TDNN-based architecture, but with filter banks replaced with trainable SincNet features. See XVectorSincNet architecture for implementation details.\nBasic usage\n# 1. visit hf.co/pyannote/embedding and accept user conditions\n# 2. visit hf.co/settings/tokens to create an access token\n# 3. instantiate pretrained model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\"pyannote/embedding\",\nuse_auth_token=\"ACCESS_TOKEN_GOES_HERE\")\nfrom pyannote.audio import Inference\ninference = Inference(model, window=\"whole\")\nembedding1 = inference(\"speaker1.wav\")\nembedding2 = inference(\"speaker2.wav\")\n# `embeddingX` is (1 x D) numpy array extracted from the file as a whole.\nfrom scipy.spatial.distance import cdist\ndistance = cdist(embedding1, embedding2, metric=\"cosine\")[0,0]\n# `distance` is a `float` describing how dissimilar speakers 1 and 2 are.\nUsing cosine distance directly, this model reaches 2.8% equal error rate (EER) on VoxCeleb 1 test set.This is without voice activity detection (VAD) nor probabilistic linear discriminant analysis (PLDA).\nExpect even better results when adding one of those.\nAdvanced usage\nRunning on GPU\nimport torch\ninference.to(torch.device(\"cuda\"))\nembedding = inference(\"audio.wav\")\nExtract embedding from an excerpt\nfrom pyannote.audio import Inference\nfrom pyannote.core import Segment\ninference = Inference(model, window=\"whole\")\nexcerpt = Segment(13.37, 19.81)\nembedding = inference.crop(\"audio.wav\", excerpt)\n# `embedding` is (1 x D) numpy array extracted from the file excerpt.\nExtract embeddings using a sliding window\nfrom pyannote.audio import Inference\ninference = Inference(model, window=\"sliding\",\nduration=3.0, step=1.0)\nembeddings = inference(\"audio.wav\")\n# `embeddings` is a (N x D) pyannote.core.SlidingWindowFeature\n# `embeddings[i]` is the embedding of the ith position of the\n# sliding window, i.e. from [i * step, i * step + duration].\nCitation\n@inproceedings{Bredin2020,\nTitle = {{pyannote.audio: neural building blocks for speaker diarization}},\nAuthor = {{Bredin}, Herv{\\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},\nBooktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},\nAddress = {Barcelona, Spain},\nMonth = {May},\nYear = {2020},\n}\n@inproceedings{Coria2020,\nauthor=\"Coria, Juan M. and Bredin, Herv{\\'e} and Ghannay, Sahar and Rosset, Sophie\",\neditor=\"Espinosa-Anke, Luis and Mart{\\'i}n-Vide, Carlos and Spasi{\\'{c}}, Irena\",\ntitle=\"{A Comparison of Metric Learning Loss Functions for End-To-End Speaker Verification}\",\nbooktitle=\"Statistical Language and Speech Processing\",\nyear=\"2020\",\npublisher=\"Springer International Publishing\",\npages=\"137--148\",\nisbn=\"978-3-030-59430-5\"\n}"
}