{
    "TencentARC/PhotoMaker": "PhotoMaker Model Card\nIntroduction\nRealistic results\nStylization results\nModel Details\nUsage\nLimitations\nBias\nCitation\nPhotoMaker Model Card\nProject Page | Paper (ArXiv) | Code\nðŸ¤— Gradio demo (Realistic) | ðŸ¤— Gradio demo (Stylization)\nIntroduction\nUsers can input one or a few face photos, along with a text prompt, to receive a customized photo or painting within seconds (no training required!). Additionally, this model can be adapted to any base model based on SDXL or used in conjunction with other LoRA modules.\nRealistic results\nStylization results\nMore results can be found in our project page\nModel Details\nIt mainly contains two parts corresponding to two keys in loaded state dict:\nid_encoder includes finetuned OpenCLIP-ViT-H-14 and a few fuse layers.\nlora_weights applies to all attention layers in the UNet, and the rank is set to 64.\nUsage\nYou can directly download the model in this repository.\nYou also can download the model in python script:\nfrom huggingface_hub import hf_hub_download\nphotomaker_ckpt = hf_hub_download(repo_id=\"TencentARC/PhotoMaker\", filename=\"photomaker-v1.bin\", repo_type=\"model\")\nThen, please follow the instructions in our GitHub repository.\nLimitations\nThe model's customization performance degrades on Asian male faces.\nThe model still struggles with accurately rendering human hands.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nCitation\nBibTeX:\n@inproceedings{li2023photomaker,\ntitle={PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding},\nauthor={Li, Zhen and Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Cheng, Ming-Ming and Shan, Ying},\nbooktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nyear={2024}\n}",
    "Viet-Mistral/Vistral-7B-Chat": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nYou agree not to use the model for experiments that could harm human subjects.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Description\nAcknowledgement:\nData\nUsage\nPerformance\nCitation\nVistral-7B-Chat - Towards a State-of-the-Art Large Language Model for Vietnamese\nModel Description\nWe introduce Vistral-7B-chat, a multi-turn conversational large language model for Vietnamese. Vistral is extended from the Mistral 7B model using diverse data for continual pre-training and instruction tuning. In particular, our process to develop Vistral involves:\nExtend the tokenizer of Mistral 7B to better support Vietnamese.\nPerform continual pre-training for Mistral over a diverse dataset of Vietnamese texts that are meticulously cleaned and deduplicated.\nPerform supervised fine-tuning for the model using diverse instruction data. We design a set of instructions to align the model with the safety criteria in Vietnam.\nGGUF Version: Running Vistral on your local computer here\nNote: To deploy Vistral locally (e.g. on LM Studio), make sure that you are utilizing the specified chat template, download here. This step is very crucial to ensure that Vistral generates accurate answers.\nAcknowledgement:\nWe thank Hessian AI and LAION for their support and compute in order to train this model. Specifically, we gratefully acknowledge LAION for providing access to compute budget granted by Gauss Centre for Supercomputing e.V. and by the John von Neumann Institute for Computing (NIC) on the supercomputers JUWELS Booster and JURECA at Juelich Supercomputing Centre (JSC).\nData\nWe will make the data available after we release the technical report for this model. However, we have made some of the data available here in our CulutraY and CulutraX datasets.\nUsage\nTo enable single/multi-turn conversational chat with Vistral-7B-Chat, you can use the default chat template format:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nsystem_prompt = \"Báº¡n lÃ  má»™t trá»£ lÃ­ Tiáº¿ng Viá»‡t nhiá»‡t tÃ¬nh vÃ  trung thá»±c. HÃ£y luÃ´n tráº£ lá»i má»™t cÃ¡ch há»¯u Ã­ch nháº¥t cÃ³ thá»ƒ, Ä‘á»“ng thá»i giá»¯ an toÃ n.\\n\"\nsystem_prompt += \"CÃ¢u tráº£ lá»i cá»§a báº¡n khÃ´ng nÃªn chá»©a báº¥t ká»³ ná»™i dung gÃ¢y háº¡i, phÃ¢n biá»‡t chá»§ng tá»™c, phÃ¢n biá»‡t giá»›i tÃ­nh, Ä‘á»™c háº¡i, nguy hiá»ƒm hoáº·c báº¥t há»£p phÃ¡p nÃ o. HÃ£y Ä‘áº£m báº£o ráº±ng cÃ¡c cÃ¢u tráº£ lá»i cá»§a báº¡n khÃ´ng cÃ³ thiÃªn kiáº¿n xÃ£ há»™i vÃ  mang tÃ­nh tÃ­ch cá»±c.\"\nsystem_prompt += \"Náº¿u má»™t cÃ¢u há»i khÃ´ng cÃ³ Ã½ nghÄ©a hoáº·c khÃ´ng há»£p lÃ½ vá» máº·t thÃ´ng tin, hÃ£y giáº£i thÃ­ch táº¡i sao thay vÃ¬ tráº£ lá»i má»™t Ä‘iá»u gÃ¬ Ä‘Ã³ khÃ´ng chÃ­nh xÃ¡c. Náº¿u báº¡n khÃ´ng biáº¿t cÃ¢u tráº£ lá»i cho má»™t cÃ¢u há»i, hÃ£y tráº³ lá»i lÃ  báº¡n khÃ´ng biáº¿t vÃ  vui lÃ²ng khÃ´ng chia sáº» thÃ´ng tin sai lá»‡ch.\"\ntokenizer = AutoTokenizer.from_pretrained('Viet-Mistral/Vistral-7B-Chat')\nmodel = AutoModelForCausalLM.from_pretrained(\n'Viet-Mistral/Vistral-7B-Chat',\ntorch_dtype=torch.bfloat16, # change to torch.float16 if you're using V100\ndevice_map=\"auto\",\nuse_cache=True,\n)\nconversation = [{\"role\": \"system\", \"content\": system_prompt }]\nwhile True:\nhuman = input(\"Human: \")\nif human.lower() == \"reset\":\nconversation = [{\"role\": \"system\", \"content\": system_prompt }]\nprint(\"The chat history has been cleared!\")\ncontinue\nconversation.append({\"role\": \"user\", \"content\": human })\ninput_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\").to(model.device)\nout_ids = model.generate(\ninput_ids=input_ids,\nmax_new_tokens=768,\ndo_sample=True,\ntop_p=0.95,\ntop_k=40,\ntemperature=0.1,\nrepetition_penalty=1.05,\n)\nassistant = tokenizer.batch_decode(out_ids[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\nprint(\"Assistant: \", assistant)\nconversation.append({\"role\": \"assistant\", \"content\": assistant })\nPerformance\nWe evaluated our Vistral model using the VMLU leaderboard, a reliable framework for evaluating large language models in Vietnamese across various tasks. These tasks involve multiple-choice questions in STEM, Humanities, Social Sciences, and more. Our model achieved an average score of 50.07%, surpassing ChatGPT's performance of 46.33% significantly.\nDisclaimer: Despite extensive red teaming and safety alignment efforts, our model may still pose potential risks, including but not limited to hallucination, toxic content, and bias issues. We strongly encourage researchers and practitioners to fully acknowledge these potential risks and meticulously assess and secure the model before incorporating it into their work. Users are responsible for adhering to and complying with their governance and regulations. The authors retain the right to disclaim any accountability for potential damages or liability resulting from the use of the model.\nCitation\nIf you find our project useful, we hope you would kindly star our repo and cite our work as follows: huu@ontocord.ai, chienn@uoregon.edu, nguyenhuuthuat09@gmail.com and thienn@uoregon.edu\n@article{chien2023vistral,\nauthor = {Chien Van Nguyen, Thuat Nguyen, Quan Nguyen, Huy Nguyen, BjÃ¶rn PlÃ¼ster, Nam Pham, Huu Nguyen, Patrick Schramowski, Thien Nguyen},\ntitle = {Vistral-7B-Chat - Towards a State-of-the-Art Large Language Model for Vietnamese},\nyear = 2023,\n}",
    "lj1995/GPT-SoVITS": "pretrained models used in https://github.com/RVC-Boss/GPT-SoVITS",
    "numz/wav2lip_studio": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nðŸ”‰ðŸ‘„ Wav2Lip STUDIO Standalone\nðŸ’¡ Description\nðŸ“– Quick Index\nðŸš€ Updates\nðŸ”— Requirements\nðŸ’» Installation\nWindows Users\nMACOS Users\nAll Users\nTutorial\nðŸ Usage\nKEYFRAMES MANAGER\nInput Video\nðŸ“º Examples\nðŸ“– Behind the scenes\nðŸ’ª Quality tips\nâš  Noted Constraints\nðŸ“ To do\nðŸ˜Ž Contributing\nðŸ™ Appreciation\nâ˜• Support Wav2lip Studio\nðŸ“ Citation\nðŸ“œ License\nðŸ”‰ðŸ‘„ Wav2Lip STUDIO Standalone\ndemo/demo1.mp4\nðŸ’¡ Description\nThis repository contains a Wav2Lip Studio Standalone Version.\nIt's an all-in-one solution: just choose a video and a speech file (wav or mp3), and the tools will generate a lip-sync video, faceswap, voice clone, and translate video with voice clone (HeyGen like).\nIt improves the quality of the lip-sync videos generated by the Wav2Lip tool by applying specific post-processing techniques.\nðŸ“– Quick Index\nðŸš€ Updates\nðŸ”— Requirements\nðŸ’» Installation\nðŸ Tutorial\nðŸ Usage\nðŸ‘„ Keyframes Manager\nðŸ‘„ Input Video\nðŸ“º Examples\nðŸ“– Behind the scenes\nðŸ’ª Quality tips\nâš ï¸Noted Constraints\nðŸ“ To do\nðŸ˜Ž Contributing\nðŸ™ Appreciation\nðŸ“ Citation\nðŸ“œ License\nâ˜• Support Wav2lip Studio\nðŸš€ Updates\n2024.01.20 Major Update (Standalone version only)\nâ™» Manage project: Add a feature to manage multiple project\nðŸ‘ª Introduced multiple face swap: Can now Swap multiple face in one shot (See Usage section)\nâ›” Visible face restriction: Can now make whole process even if no face detected on frame!\nðŸ“º Video Size: works with high resolution video input, (test with 1980x1080, should works with 4K but slow)\nðŸ”‘ Keyframe manager: Add a keyframe manager for better control of the video generation\nðŸª coqui TTS integration: Remove bark integration, use coqui TTS instead (See Usage section)\nðŸ’¬ Conversation: Add a conversation feature with multiple person (See Usage section)\nðŸ”ˆ Record your own voice: Add a feature to record your own voice (See Usage section)\nðŸ‘¬ Clone voice: Add a feature to clone voice from video (See Usage section)\nðŸŽ translate video: Add a feature to translate video with voice clone (See Usage section)\nðŸ”‰ Volume amplifier for wav2lip: Add a feature to amplify the volume of the wav2lip output (See Usage section)\nðŸ•¡ Add delay before sound speech start\nðŸš€ Speed up process: Speed up the process\n2023.09.13\nðŸ‘ª Introduced face swap: facefusion integration (See Usage section) this feature is under experimental.\n2023.08.22\nðŸ‘„ Introduced bark (See Usage section), this feature is under experimental.\n2023.08.20\nðŸš¢ Introduced the GFPGAN model as an option.\nâ–¶ Added the feature to resume generation.\nðŸ“ Optimized to release memory post-generation.\n2023.08.17\nðŸ› Fixed purple lips bug\n2023.08.16\nâš¡ Added Wav2lip and enhanced video output, with the option to download the one that's best for you, likely the \"generated video\".\nðŸš¢ Updated User Interface: Introduced control over CodeFormer Fidelity.\nðŸ‘„ Removed image as input, SadTalker is better suited for this.\nðŸ› Fixed a bug regarding the discrepancy between input and output video that incorrectly positioned the mask.\nðŸ’ª Refined the quality process for greater efficiency.\nðŸš« Interruption will now generate videos if the process creates frames\n2023.08.13\nâš¡ Speed-up computation\nðŸš¢ Change User Interface : Add controls on hidden parameters\nðŸ‘„ Only Track mouth if needed\nðŸ“° Control debug\nðŸ› Fix resize factor bug\nðŸ”— Requirements\nFFmpeg : download it from the official FFmpeg site. Follow the instructions appropriate for your operating system, note ffmpeg have to be accessible from the command line.\nðŸ’» Installation\nWindows Users\n1.Install Visual Studio. During the install, make sure to include the Python and C++ packages in visual studio installer.\nInstall python 3.10.11\nInstall git\nInstall Cuda 11.8 if not ever done.\nCheck python and git installation\npython --version\ngit --version\nnvcc --version\nMust return something like\nPython 3.10.11\ngit version 2.35.1.windows.2\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nif you have multiple Python version on your computer edit wav2lip-studio.bat and change the following line:\nREM set PYTHON=\"your python.exe path\"\nset PYTHON=\"your python.exe path\"\ndouble click on wav2lip-studio.bat, that will install the requirements and download models\nMACOS Users\nInstall python 3.9\nbrew update\nbrew install python@3.9\nbrew install git-lfs\ngit-lfs install\nInstall environnement and requirements\ncd /YourWav2lipStudioFolder\n/opt/homebrew/bin/python3.9 -m venv venv\n./venv/bin/python3.9 -m pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2\n./venv/bin/python3.9 -m pip install -r requirements.txt\n./venv/bin/python3.9 -m pip install transformers==4.33.2\n./venv/bin/python3.9 -m pip install numpy==1.24.4\nif It doesn't works or too long on pip install -r requirements.txt\n./venv/bin/python3.9 -m pip install inaSpeechSegmenter\n./venv/bin/python3.9 -m pip install gradio==4.14.0 imutils==0.5.4 numpy opencv-python==4.8.0.76 scipy==1.11.2 requests==2.28.1  pillow==9.3.0  librosa==0.10.0 opencv-contrib-python==4.8.0.76 huggingface_hub==0.20.2 tqdm==4.66.1 cutlet==0.3.0 numba==0.57.1 imageio_ffmpeg==0.4.9 insightface==0.7.3 unidic==1.1.0 onnx==1.14.1 onnxruntime==1.16.0 psutil==5.9.5 lpips==0.1.4 GitPython==3.1.36 facexlib==0.3.0 gfpgan==1.3.8 gdown==4.7.1 pyannote.audio==3.1.1 TTS==0.21.2 openai-whisper==20231117 resampy==0.4.0 scenedetect==0.6.2 uvicorn==0.23.2 starlette==0.35.1 fastapi==0.109.0 fugashii\n./venv/bin/python3.9 -m pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2\n./venv/bin/python3.9 -m pip install transformers==4.33.2\n./venv/bin/python3.9 -m pip install numpy==1.24.4\nInstall models\ngit clone https://huggingface.co/numz/wav2lip_studio models\nLaunch UI\n./venv/bin/python3.9 wav2lip_studio.py\nAll Users\npyannote.audio:You need to agree to share your contact information to access pyannote models.\nTo do so, go to both link:\npyannote diarization-3.1 huggingface repository\npyannote segmentation-3.0 huggingface repository\nset each field and click \"Agree and access repository\"\nCreate an access token to Huggingface:\nConnect with your account\ngo to access tokens in settings\ncreate a new token in read mode\ncopy the token\npaste it in the file api_keys.json\n{\n\"huggingface_token\": \"your token\"\n}\nTutorial\nFR version\nEN Version\nðŸ Usage\n##PARAMETERS\nEnter project name and click enter.\nChoose a video (avi or mp4 format). Note avi file will not appear in Video input but process will works.\nFace Swap (take times so be patient):\nFace Swap: choose the image of the faces you want to swap with the face in the video (multiple faces are now available), left face is id 0.\nResolution Divide Factor: The resolution of the video will be divided by this factor. The higher the factor, the faster the process, but the lower the resolution of the output video.\nMin Face Width Detection: The minimum width of the face to detect. Allow to ignore little face in the video.\nAlign Faces: allows for straightening the head before sending it for Wav2Lip processing.\nKeyframes On Speaker Change: Allows you to generate a keyframe when the speaker changes. This allows you to better control the video generation.\nKeyframes On scene Change: Allows you to generate a keyframe when the scene changes. This allows you to better control the video generation.\nWhen parameters above are set click on Generate Keyframes, See Keyframes manager section for more details.\nAudio, 3 options:\nPut audio file in the \"Speech\" input. or record one with the \"Record\" button.\nGenerate Audio with the text to speech coqui TTS integration.\nChoose the language\nChoose the Voice\nWrite your speech in the text area \"Prompt\" in text format or json format:\nText format: Hello, my name is John. I am 25 years old.\nJson format (you can ask chat GPT to generate discussion for you):[\n{\n\"start\": 0.0,\n\"end\": 3.0,\n\"text\": \"Hello, my name is John. I am 25 years old.\",\n\"speaker\": \"arnold\"\n},\n{\n\"start\": 3.0,\n\"end\": 4.0,\n\"text\": \"Ho really ?\",\n\"speaker\": \"female_01\"\n},\n...\n]\nInput Video: Allow to use audio from the input video, voices cloning and translation. see Input Video section for more details.\nVideo Quality:\nLow: Original Wav2Lip quality, fast but not very good.\nMedium: Better quality by apply post processing on the mouth, slower.\nHigh: Better quality by apply post processing and upscale the mouth quality, slower.\nWav2lip Checkpoint: Choose beetwen 2 wav2lip model:\nWav2lip: Original Wav2Lip model, fast but not very good.\nWav2lip GAN: Better quality by apply post processing on the mouth, slower.\nFace Restoration Model: Choose beetwen 2 face restoration model:\nCode Former:\nA value of 0 offers higher quality but may significantly alter the person's facial appearance and cause noticeable flickering between frames.\nA value of 1 provides lower quality but maintains the person's face more consistently and reduces frame flickering.\nUsing a value below 0.5 is not advised. Adjust this setting to achieve optimal results. Starting with a value of 0.75 is recommended.\nGFPGAN: Usually better quality.\nVolume Amplifier: Not amplify the volume of the output audio but allows you to amplify the volume of the audio when sending it to Wav2Lip. This allows you to better control on lips movement.\nKEYFRAMES MANAGER\nGlobal parameters:\nOnly Track The Mouth: This option tracks only the mouth, removing other facial motions like those of the cheeks and chin.\nOnly show Speaker Face: This option allows you to only focus the face of the speaker, the other faces will be hidden.\nFrame Number: A slider that allows you to move between the frames of the video.\nAdd Keyframe: Allows you to add a keyframe at the current Frame Number.\nRemove Keyframe: Allows you to remove a keyframe at the current Frame Number.\nKeyframes: A list of all the keyframes.\nFor each face on keyframe:\nFace Id: List of all the faces in current keyframe.\nSpeaker: Checkbox to set the speaker on the current Face Id of the current keyframe.\nFace Swap Id: Checkbox to set the face swap id of the current keyframe on the current Face Id.\nMouth Mask Dilate: This will dilate the mouth mask to cover more area around the mouth. depends on the mouth size.\nFace Mask Erode: This will erode the face mask to remove some area around the face. depends on the face size.\nMask Blur: This will blur the mask to make it more smooth, try to keep it under or equal to Mouth Mask Dilate.\nPadding sliders: This will add padding to the head to avoid cuting the head in the video.\nInput Video\nIf no sound in translated audio, will take the audio from the input video. Can be useful if you have a bad lipsync on the input video.\nClone Voices:\nNumber Of Speakers: The number of speakers in the video. Help clone to know how many voices to clone.\nRemove Background Sounf Before Clone: Remove noise/music from the background sound before clone.\nClone Voices: Clone voices from the input video.\nVoices: List of the cloned voices.\nTranslation:\nLanguage: Target language to translate the input video.\nWhisper Model: List of the whisper models to use for the translation, choose beetwen 5 models, the higher the model the better the quality but the slower the process.\nTranslate: Translate the input video to the selected language.\nTranslation: The translated text.\nTranslated Audio: The translated audio.\nConvert To Audio: Convert the translated text to translated audio.\nðŸ“º Examples\ndemo/demo2.mp4\ndemo/demo3.mp4\ndemo/demo4.mp4\ndemo/demo5.mp4\nðŸ“– Behind the scenes\nThis extension operates in several stages to improve the quality of Wav2Lip-generated videos:\nGenerate face swap video: The script first generates the face swap video if image is in \"face Swap\" field, this operation take times so be patient.\nGenerate a Wav2lip video: Then script generates a low-quality Wav2Lip video using the input video and audio.\nVideo Quality Enhancement: Create a high-quality video using the low-quality video by using the enhancer define by user.\nMask Creation: The script creates a mask around the mouth and tries to keep other facial motions like those of the cheeks and chin.\nVideo Generation: The script then takes the high-quality mouth image and overlays it onto the original image guided by the mouth mask.\nðŸ’ª Quality tips\nUse a high quality video as input\nUse a video with a consistent frame rate. Occasionally, videos may exhibit unusual playback frame rates (not the standard 24, 25, 30, 60), which can lead to issues with the face mask.\nUse a high quality audio file as input, without background noise or music. Clean audio with a tool like https://podcast.adobe.com/enhance.\nDilate the mouth mask. This will help the model retain some facial motion and hide the original mouth.\nMask Blur maximum twice the value of Mouth Mask Dilate. If you want to increase the blur, increase the value of Mouth Mask Dilate otherwise the mouth will be blurred and the underlying mouth could be visible.\nUpscaling can be good for improving result, particularly around the mouth area. However, it will extend the processing duration. Use this tutorial from Olivio Sarikas to upscale your video: https://www.youtube.com/watch?v=3z4MKUqFEUk. Ensure the denoising strength is set between 0.0 and 0.05, select the 'revAnimated' model, and use the batch mode. i'll create a tutorial for this soon.\nâš  Noted Constraints\nfor speed up process try to keep resolution under 1000x1000px and upscaling after process.\nIf the initial phase is excessively lengthy, consider using the \"resize factor\" to decrease the video's dimensions.\nWhile there's no strict size limit for videos, larger videos will require more processing time. It's advisable to employ the \"resize factor\" to minimize the video size and then upscale the video once processing is complete.\nðŸ“ To do\nâœ”ï¸ Standalone version\nâœ”ï¸ Add a way to use a face swap image\nâœ”ï¸ Add Possibility to use a video for audio input\nâœ”ï¸ Convert avi to mp4. Avi is not show in video input but process work fine\nComfyUI intergration\nðŸ˜Ž Contributing\nWe welcome contributions to this project. When submitting pull requests, please provide a detailed description of the changes. see CONTRIBUTING for more information.\nðŸ™ Appreciation\nWav2Lip\nCodeFormer\nCoqui TTS\nfacefusion\nVocal Remover\nâ˜• Support Wav2lip Studio\nthis project is open-source effort that is free to use and modify. I rely on the support of users to keep this project going and help improve it. If you'd like to support me, you can make a donation on my Patreon page. Any contribution, large or small, is greatly appreciated!\nYour support helps me cover the costs of development and maintenance, and allows me to allocate more time and resources to enhancing this project. Thank you for your support!\npatreon page\nðŸ“ Citation\nIf you use this project in your own work, in articles, tutorials, or presentations, we encourage you to cite this project to acknowledge the efforts put into it.\nTo cite this project, please use the following BibTeX format:\n@misc{wav2lip_uhq,\nauthor = {numz},\ntitle = {Wav2Lip UHQ},\nyear = {2023},\nhowpublished = {GitHub repository},\npublisher = {numz},\nurl = {https://github.com/numz/sd-wav2lip-uhq}\n}\nðŸ“œ License\nThe code in this repository is released under the MIT license as found in the LICENSE file.",
    "LoSboccacc/orthogonal-2x7B-v2-base": "Open LLM Leaderboard Evaluation Results\nbase_model: mistralai/Mistral-7B-Instruct-v0.2\ngate_mode: hidden # one of \"hidden\", \"cheap_embed\", or \"random\"\ndtype: bfloat16 # output dtype (float32, float16, or bfloat16)\nexperts:\n- source_model: SanjiWatsuki/Kunoichi-DPO-v2-7B\npositive_prompts:\n- \"roleplay\"\n- source_model: mistralai/Mistral-7B-Instruct-v0.2\npositive_prompts:\n- \"chat\"\nchatml\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n68.47\nAI2 Reasoning Challenge (25-Shot)\n66.89\nHellaSwag (10-Shot)\n85.69\nMMLU (5-Shot)\n62.65\nTruthfulQA (0-shot)\n66.80\nWinogrande (5-shot)\n77.35\nGSM8k (5-shot)\n51.40",
    "rafiaa/legal-passive-to-active-mistral-7b": "legal-passive-to-active-mistral-7b\nModel Description\nKey Features\nModel Details\nTechnical Specifications\nUses\nDirect Use\nExample Use Cases\nHow to Get Started\nInstallation\nLoading the Model\nUsage Example\nAdvanced Usage\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation Metrics\nPerformance Comparison\nStrengths and Characteristics\nModel Strengths\nNotable Behaviors\nLimitations and Bias\nKnown Limitations\nRecommendations\nEnvironmental Impact\nCitation\nRelated Models\nModel Card Contact\nAcknowledgments\nlegal-passive-to-active-mistral-7b\nRECOMMENDED MODEL - An advanced LoRA fine-tuned model for transforming legal text from passive voice to active voice, built on Mistral-7B-Instruct. This model demonstrates superior performance in simplifying complex legal language while maintaining semantic accuracy and legal precision.\nModel Description\nThis is the enhanced model for legal passive-to-active transformation. Built on Mistral-7B-Instruct-v0.1, it outperforms comparable models on legal voice transformation tasks. The model was fine-tuned on a curated dataset of 319 legal sentences from authoritative sources including UN documents, GDPR, Fair Work Act, and insurance regulations.\nKey Features\nSuperior Performance: ~15% improvement over base model in human evaluation\nLegal Text Simplification: Converts passive voice to active voice in legal documents\nDomain-Specific: Fine-tuned on authentic legal text from multiple jurisdictions\nEfficient Training: Uses QLoRA for memory-efficient fine-tuning\nSemantic Preservation: Maintains legal meaning while simplifying sentence structure\nAccessibility: Makes legal documents more readable and accessible\nModel Details\nDeveloped by: Rafi Al Attrach\nModel type: LoRA fine-tuned Mistral (Enhanced)\nLanguage(s): English\nLicense: Apache 2.0\nFinetuned from: mistralai/Mistral-7B-Instruct-v0.1\nTraining method: QLoRA (4-bit quantization + LoRA)\nResearch Focus: Legal text simplification and accessibility (2024)\nTechnical Specifications\nBase Model: Mistral-7B-Instruct-v0.1\nLoRA Rank: 64\nTraining Samples: 319 legal sentences\nData Sources: UN legal documents, GDPR, Fair Work Act, Insurance regulations\nEvaluation: BERTScore metrics and human evaluation\nPerformance: ~15% improvement over base model in human evaluation\nUses\nDirect Use\nThis model is designed for:\nLegal document simplification: Converting passive legal text to active voice\nAccessibility improvement: Making legal documents more readable\nLegal writing assistance: Helping legal professionals write clearer documents\nEducational purposes: Teaching legal language transformation\nDocument processing: Batch processing of legal texts\nRegulatory compliance: Simplifying complex regulatory language\nExample Use Cases\n# Transform a legal passive sentence to active voice\npassive_sentence = \"The contract shall be executed by both parties within 30 days.\"\n# Model output: \"Both parties shall execute the contract within 30 days.\"\n# Simplify GDPR text\npassive_sentence = \"Personal data may be processed by the controller for legitimate interests.\"\n# Model output: \"The controller may process personal data for legitimate interests.\"\n# Transform UN legal text\npassive_sentence = \"All necessary measures shall be taken by Member States to ensure compliance.\"\n# Model output: \"Member States shall take all necessary measures to ensure compliance.\"\nHow to Get Started\nInstallation\npip install transformers torch peft accelerate bitsandbytes\nLoading the Model\nGPU Usage (Recommended)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n# Load base model with 4-bit quantization\nbase_model = \"mistralai/Mistral-7B-Instruct-v0.1\"\nmodel = AutoModelForCausalLM.from_pretrained(\nbase_model,\nload_in_4bit=True,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\"\n)\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(model, \"rafiaa/legal-passive-to-active-mistral-7b\")\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Set pad token\nif tokenizer.pad_token is None:\ntokenizer.pad_token = tokenizer.eos_token\nCPU Usage (Alternative)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n# Load base model (CPU compatible)\nbase_model = \"mistralai/Mistral-7B-Instruct-v0.1\"\nmodel = AutoModelForCausalLM.from_pretrained(\nbase_model,\ntorch_dtype=torch.float32,\ndevice_map=\"cpu\"\n)\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(model, \"rafiaa/legal-passive-to-active-mistral-7b\")\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Set pad token\nif tokenizer.pad_token is None:\ntokenizer.pad_token = tokenizer.eos_token\nUsage Example\ndef transform_passive_to_active(passive_sentence, max_length=512):\n# Create instruction prompt\ninstruction = \"\"\"You are a legal text transformation expert. Your task is to convert passive voice sentences to active voice while maintaining the exact legal meaning and terminology.\nInput: Transform the following legal sentence from passive to active voice.\nLegal Sentence: \"\"\"\nprompt = instruction + passive_sentence\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model.generate(\n**inputs,\nmax_length=max_length,\ntemperature=0.7,\ndo_sample=True,\npad_token_id=tokenizer.eos_token_id\n)\nreturn tokenizer.decode(outputs[0], skip_special_tokens=True)\n# Example usage\npassive = \"The agreement shall be signed by the authorized representatives.\"\nactive = transform_passive_to_active(passive)\nprint(active)\nAdvanced Usage\n# Batch processing multiple legal sentences\nlegal_sentences = [\n\"The policy was established by the board of directors.\",\n\"All documents must be reviewed by legal counsel.\",\n\"The regulations were enacted by Parliament.\"\n]\nfor sentence in legal_sentences:\ntransformed = transform_passive_to_active(sentence)\nprint(f\"Passive: {sentence}\")\nprint(f\"Active: {transformed}\\n\")\nTraining Details\nTraining Data\nDataset Size: 319 legal sentences\nSource Documents:\nUnited Nations legal documents\nGeneral Data Protection Regulation (GDPR)\nFair Work Act (Australia)\nInsurance Council of Australia regulations\nData Split: 85% training, 15% testing (with 15% of training for validation)\nDomain: Legal text across multiple jurisdictions\nFormat: Alpaca format for instruction-based training\nTraining Procedure\nMethod: QLoRA (4-bit quantization + LoRA)\nLoRA Configuration: Rank 64, Alpha 16\nLibrary: unsloth (2.2x faster, 62% less VRAM for Mistral)\nHardware: Tesla T4 GPU (Google Colab)\nTraining Loss: Downward trending validation loss indicating excellent generalization\nEvaluation Metrics\nBERTScore: Semantic similarity evaluation (Precision, Recall, F1)\nHuman Evaluation: Binary correctness assessment by legal evaluators\nPerformance Improvement: ~15% increase over base Mistral model\nPerformance Comparison\nModel\nHuman Eval Score\nBERTScore F1\nPerformance\nMistral-7B Base\nBaseline\nHigh\nGood\nlegal-passive-to-active-mistral-7b\n+15%\nHigher\nExcellent\nlegal-passive-to-active-llama2-7b\n+6%\nHigh\nGood\nThis model demonstrates the best performance among 7B parameter models for legal passive-to-active transformation.\nStrengths and Characteristics\nModel Strengths\nHigh accuracy in passive-to-active transformations\nSemantic preservation - maintains legal meaning\nBetter generalization compared to Llama-2 variants\nResponsive to prompts - adapts well to instruction modifications\nVocabulary diversity - uses appropriate legal terminology\nNotable Behaviors\nOccasionally substitutes words with synonyms (trade-off for flexibility)\nBetter precision compared to base model after fine-tuning\nStrong performance on complex legal constructions\nLimitations and Bias\nKnown Limitations\nWord Position Sensitivity: Struggles with sentences where word position significantly alters meaning\nDataset Size: Limited to 319 training samples\nNon-Determinism: LLM outputs may vary between runs\nDomain Coverage: Primarily trained on English common law and EU legal documents\nSynonym Substitution: May occasionally use synonyms instead of exact original words\nRecommendations\nValidate transformed sentences for legal accuracy before use\nUse human review for critical legal documents\nConsider context and jurisdiction when applying transformations\nTest with domain-specific legal texts for best results\nReview outputs for unintended synonym substitutions in critical documents\nEnvironmental Impact\nTraining Method: QLoRA reduces computational requirements by 62% for Mistral\nHardware: Efficient training using 4-bit quantization\nCarbon Footprint: Significantly reduced compared to full fine-tuning\nCitation\nIf you use this model in your research, please cite:\n@misc{legal-passive-active-mistral,\ntitle={legal-passive-to-active-mistral-7b: An Enhanced LoRA Fine-tuned Model for Legal Voice Transformation},\nauthor={Rafi Al Attrach},\nyear={2024},\nurl={https://huggingface.co/rafiaa/legal-passive-to-active-mistral-7b}\n}\nRelated Models\nBase Model: mistralai/Mistral-7B-Instruct-v0.1\nAlternative: rafiaa/legal-passive-to-active-llama2-7b\nThis Model: rafiaa/legal-passive-to-active-mistral-7b (Recommended)\nModel Card Contact\nAuthor: Rafi Al Attrach\nModel Repository: HuggingFace Model\nIssues: Please report issues through the HuggingFace model page\nAcknowledgments\nResearch Project: Legal text simplification and accessibility research (2024)\nTraining Data: Public legal documents and regulations\nBase Model: Mistral AI's Mistral-7B-Instruct-v0.1\nTraining Method: QLoRA for efficient fine-tuning\nThis model represents advanced research in legal text simplification and accessibility, demonstrating superior performance in passive-to-active voice transformation for legal documents.",
    "Systran/faster-distil-whisper-small.en": "Whisper small.en model for CTranslate2\nExample\nConversion details\nMore information\nWhisper small.en model for CTranslate2\nThis repository contains the conversion of distil-whisper/distil-small.en to the CTranslate2 model format.\nThis model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.\nExample\nfrom faster_whisper import WhisperModel\nmodel = WhisperModel(\"distil-small.en\")\nsegments, info = model.transcribe(\"audio.mp3\")\nfor segment in segments:\nprint(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\nConversion details\nThe original model was converted with the following command:\nct2-transformers-converter --model distil-whisper/distil-small.en --output_dir faster-distil-whisper-small.en \\\n--copy_files tokenizer.json preprocessor_config.json --quantization float16\nNote that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.\nMore information\nFor more information about the original model, see its model card.",
    "ibm-granite/granite-timeseries-patchtst": "PatchTST model pre-trained on ETTh1 dataset\nModel Details\nModel Description\nModel Sources\nUses\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining hyperparameters\nTraining Results\nEvaluation\nTesting Data\nMetrics\nResults\nCitation\nPatchTST model pre-trained on ETTh1 dataset\nPatchTST is a transformer-based model for time series modeling tasks, including forecasting, regression, and classification. This repository contains a pre-trained PatchTST model encompassing all seven channels of the ETTh1 dataset.\nThis particular pre-trained model produces a Mean Squared Error (MSE) of 0.3881 on the test split of the ETTh1 dataset when forecasting 96 hours into the future with a historical data window of 512 hours.\nFor training and evaluating a PatchTST model, you can refer to this demo notebook.\nModel Details\nModel Description\nThe PatchTST model was proposed in A Time Series is Worth 64 Words: Long-term Forecasting with Transformers by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.\nAt a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head.\nThe model is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. The patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models.\nIn addition, PatchTST has a modular design to seamlessly support masked time series pre-training as well as direct time series forecasting, classification, and regression.\nModel Sources\nRepository: PatchTST Hugging Face\nPaper: PatchTST ICLR 2023 paper\nDemo: Get started with PatchTST\nUses\nThis pre-trained model can be employed for fine-tuning or evaluation using any Electrical Transformer dataset that has the same channels as the ETTh1 dataset, specifically: HUFL, HULL, MUFL, MULL, LUFL, LULL, OT. The model is designed to predict the next 96 hours based on the input values from the preceding 512 hours. It is crucial to normalize the data. For a more comprehensive understanding of data pre-processing, please consult the paper or the demo.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nDemo\nTraining Details\nTraining Data\nETTh1/train split.\nTrain/validation/test splits are shown in the demo.\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 10\nTraining Results\nTraining Loss\nEpoch\nStep\nValidation Loss\n0.4306\n1.0\n1005\n0.7268\n0.3641\n2.0\n2010\n0.7456\n0.348\n3.0\n3015\n0.7161\n0.3379\n4.0\n4020\n0.7428\n0.3284\n5.0\n5025\n0.7681\n0.321\n6.0\n6030\n0.7842\n0.314\n7.0\n7035\n0.7991\n0.3088\n8.0\n8040\n0.8021\n0.3053\n9.0\n9045\n0.8199\n0.3019\n10.0\n10050\n0.8173\nEvaluation\nTesting Data\nETTh1/test split.\nTrain/validation/test splits are shown in the demo.\nMetrics\nMean Squared Error (MSE).\nResults\nIt achieves a MSE of 0.3881 on the evaluation dataset.\nHardware\n1 NVIDIA A100 GPU\nFramework versions\nTransformers 4.36.0.dev0\nPytorch 2.0.1\nDatasets 2.14.4\nTokenizers 0.14.1\nCitation\nBibTeX:\n@misc{nie2023time,\ntitle={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},\nauthor={Yuqi Nie and Nam H. Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},\nyear={2023},\neprint={2211.14730},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nAPA:\nNie, Y., Nguyen, N., Sinthong, P., & Kalagnanam, J. (2023). A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. arXiv preprint arXiv:2211.14730.",
    "kitty7779/ponyDiffusionV6XL": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nhttps://civitai.com/models/257749?modelVersionId=290640",
    "InstantX/InstantID": "InstantID Model Card\nIntroduction\nUsage\nUsage Tips\nDemos\nDisclaimer\nCitation\nInstantID Model Card\nProject Page | Paper | Code | ðŸ¤— Gradio demo\nIntroduction\nInstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.\nUsage\nYou can directly download the model in this repository.\nYou also can download the model in python script:\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/config.json\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/diffusion_pytorch_model.safetensors\", local_dir=\"./checkpoints\")\nhf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ip-adapter.bin\", local_dir=\"./checkpoints\")\nFor face encoder, you need to manutally download via this URL to models/antelopev2.\n# !pip install opencv-python transformers accelerate insightface\nimport diffusers\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n# prepare 'antelopev2' under ./models\napp = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n# prepare models under ./checkpoints\nface_adapter = f'./checkpoints/ip-adapter.bin'\ncontrolnet_path = f'./checkpoints/ControlNetModel'\n# load IdentityNet\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n...     \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n... )\npipe.cuda()\n# load adapter\npipe.load_ip_adapter_instantid(face_adapter)\nThen, you can customized your own face images\n# load an image\nimage = load_image(\"your-example.jpg\")\n# prepare face emb\nface_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\nface_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*x['bbox'][3]-x['bbox'][1])[-1] # only use the maximum face\nface_emb = face_info['embedding']\nface_kps = draw_kps(face_image, face_info['kps'])\npipe.set_ip_adapter_scale(0.8)\nprompt = \"analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality\"\nnegative_prompt = \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured\"\n# generate image\nimage = pipe(\n...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n... ).images[0]\nFor more details, please follow the instructions in our GitHub repository.\nUsage Tips\nIf you're not satisfied with the similarity, try to increase the weight of \"IdentityNet Strength\" and \"Adapter Strength\".\nIf you feel that the saturation is too high, first decrease the Adapter strength. If it is still too high, then decrease the IdentityNet strength.\nIf you find that text control is not as expected, decrease Adapter strength.\nIf you find that realistic style is not good enough, go for our Github repo and use a more realistic base model.\nDemos\nDisclaimer\nThis project is released under Apache License and aims to positively impact the field of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.\nCitation\n@article{wang2024instantid,\ntitle={InstantID: Zero-shot Identity-Preserving Generation in Seconds},\nauthor={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},\njournal={arXiv preprint arXiv:2401.07519},\nyear={2024}\n}",
    "mrm8488/deberta-v3-ft-financial-news-sentiment-analysis": "DeBERTa-v3-small-ft-news-sentiment-analisys\nModel description\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nExample of usage\nFramework versions\nCitation\nDeBERTa-v3-small-ft-news-sentiment-analisys\nThis model is a fine-tuned version of microsoft/deberta-v3-small on the None dataset.\nIt achieves the following results on the evaluation set:\nMetric\nValue\nF1\n0.9940\nAccuracy\n0.9940\nPrecision\n0.9940\nRecall\n0.9940\nLoss\n0.0233\nModel description\nDeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa performs RoBERTa on a majority of NLU tasks with 80GB of training data.\nIn DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.\nPlease check the official repository for more implementation details and updates.\nThe DeBERTa V3 small model comes with six layers and a hidden size of 768. It has 44M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\nTraining and evaluation data\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English-language financial news categorized by sentiment. The dataset is divided by an agreement rate of 5-8 annotators.\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 64\neval_batch_size: 64\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 5\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nPrecision\nRecall\nAccuracy\nF1\nNo log\n1.0\n214\n0.1865\n0.9323\n0.9323\n0.9323\n0.9323\nNo log\n2.0\n428\n0.0742\n0.9771\n0.9771\n0.9771\n0.9771\n0.2737\n3.0\n642\n0.0479\n0.9855\n0.9855\n0.9855\n0.9855\n0.2737\n4.0\n856\n0.0284\n0.9923\n0.9923\n0.9923\n0.9923\n0.0586\n5.0\n1070\n0.0233\n0.9940\n0.9940\n0.9940\n0.9940\nExample of usage\nIn case you did not installed it:\npip install transformers sentencepiece\nfrom transformers import pipeline\ntask = \"text-classification\"\nmodel_id = \"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\"\nclassifier = pipeline(task, model_id)\ntext = \"Tesla cars are not as good as expected\"\nresult = classifier(text)\nprint(result)\nFramework versions\nTransformers 4.35.2\nPytorch 2.1.0+cu121\nDatasets 2.16.1\nTokenizers 0.15.0\nCitation\n@misc {manuel_romero_2024,\nauthor       = { {Manuel Romero} },\ntitle        = { deberta-v3-ft-financial-news-sentiment-analysis (Revision 7430ace) },\nyear         = 2024,\nurl          = { https://huggingface.co/mrm8488/deberta-v3-ft-financial-news-sentiment-analysis },\ndoi          = { 10.57967/hf/1666 },\npublisher    = { Hugging Face }\n}",
    "vinai/PhoGPT-4B": "PhoGPT: Generative Pre-training for Vietnamese\nPhoGPT: Generative Pre-training for Vietnamese\nWe open-source a state-of-the-art 4B-parameter generative model series for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens, with an 8192 context length, employing a vocabulary of 20480 token types. The chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along with an additional 290K conversations. We demonstrate its superior performance compared to previous open-source models. More details about the general architecture and experimental results of PhoGPT can be found in our technical report:\n@article{PhoGPT,\ntitle     = {{PhoGPT: Generative Pre-training for Vietnamese}},\nauthor    = {Dat Quoc Nguyen and Linh The Nguyen and Chi Tran and Dung Ngoc Nguyen and Dinh Phung and Hung Bui},\njournal   = {arXiv preprint},\nvolume    = {arXiv:2311.02945},\nyear      = {2023}\n}\nPlease CITE our technical report when PhoGPT is used to help produce published results or is incorporated into other software.\nFor further information or requests, please go to PhoGPT's homepage!",
    "ulugbeksalaev/uzstemmer": "README.md exists but content is empty.",
    "sayby/rna_torsionBERT": "RNA-TorsionBERT\nModel Description\nUsage\nRNA-TorsionBERT\nModel Description\nRNA-TorsionBERT is a 86.9 MB parameter BERT-based language model that predicts RNA torsional and pseudo-torsional angles from the sequence.\nRNA-TorsionBERT is a DNABERT model that was pre-trained on ~4200 RNA structures.\nIt provides improvement of MCQ over the previous state-of-the-art models like\nSPOT-RNA-1D or inferred angles from existing methods, on the Test Set (composed of RNA-Puzzles and CASP-RNA).\nKey Features\nTorsional and Pseudo-torsional angles prediction\nPredict sequences up to 512 nucleotides\nUsage\nGet started generating text with RNA-TorsionBERT by using the following code snippet:\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"sayby/rna_torsionbert\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"sayby/rna_torsionbert\", trust_remote_code=True)\nsequence = \"ACG CGG GGT GTT\"\nparams_tokenizer = {\n\"return_tensors\": \"pt\",\n\"padding\": \"max_length\",\n\"max_length\": 512,\n\"truncation\": True,\n}\ninputs = tokenizer(sequence, **params_tokenizer)\noutput = model(inputs)[\"logits\"]\nPlease note that it was fine-tuned from a DNABERT-3 model and therefore the tokenizer is the same as the one used for DNABERT. Nucleotide U should therefore be replaced by T in the input sequence.\nThe output is the sinus and the cosine for each angle. The angles are in the following order: alpha, beta,gamma,delta,epsilon,zeta,chi,eta,theta,eta',theta',v0,v1,v2,v3,v4.\nTo convert the predictions into angles, you can use the following code snippet:\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom typing import Optional, Dict\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntransformers.logging.set_verbosity_error()\nBACKBONE = [\n\"alpha\",\n\"beta\",\n\"gamma\",\n\"delta\",\n\"epsilon\",\n\"zeta\",\n\"chi\",\n\"eta\",\n\"theta\",\n\"eta'\",\n\"theta'\",\n\"v0\",\n\"v1\",\n\"v2\",\n\"v3\",\n\"v4\",\n]\nclass RNATorsionBERTHelper:\ndef __init__(self):\nself.model_name = \"sayby/rna_torsionbert\"\nself.tokenizer = AutoTokenizer.from_pretrained(\nself.model_name, trust_remote_code=True\n)\nself.params_tokenizer = {\n\"return_tensors\": \"pt\",\n\"padding\": \"max_length\",\n\"max_length\": 512,\n\"truncation\": True,\n}\nself.model = AutoModel.from_pretrained(self.model_name, trust_remote_code=True)\ndef predict(self, sequence: str):\nsequence_tok = self.convert_raw_sequence_to_k_mers(sequence)\ninputs = self.tokenizer(sequence_tok, **self.params_tokenizer)\noutputs = self.model(inputs)[\"logits\"]\noutputs = self.convert_sin_cos_to_angles(\noutputs.cpu().detach().numpy(), inputs[\"input_ids\"]\n)\noutput_angles = self.convert_logits_to_dict(\noutputs[0, :], inputs[\"input_ids\"][0, :].cpu().detach().numpy()\n)\noutput_angles.index = list(sequence)[:-2]  # Because of the 3-mer representation\nreturn output_angles\ndef convert_raw_sequence_to_k_mers(self, sequence: str, k_mers: int = 3):\n\"\"\"\nConvert a raw RNA sequence into sequence readable for the tokenizer.\nIt converts the sequence into k-mers, and replace U by T\n:return: input readable by the tokenizer\n\"\"\"\nsequence = sequence.upper().replace(\"U\", \"T\")\nk_mers_sequence = [\nsequence[i : i + k_mers]\nfor i in range(len(sequence))\nif len(sequence[i : i + k_mers]) == k_mers\n]\nreturn \" \".join(k_mers_sequence)\ndef convert_sin_cos_to_angles(\nself, output: np.ndarray, input_ids: Optional[np.ndarray] = None\n):\n\"\"\"\nConvert the raw predictions of the RNA-TorsionBERT into angles.\nIt converts the cos and sinus into angles using:\nalpha = arctan(sin(alpha)/cos(alpha))\n:param output: Dictionary with the predictions of the RNA-TorsionBERT per angle\n:param input_ids: the input_ids of the RNA-TorsionBERT. It allows to only select the of the sequence,\nand not the special tokens.\n:return: a np.ndarray with the angles for the sequence\n\"\"\"\nif input_ids is not None:\noutput[\n(input_ids == 0)\n| (input_ids == 2)\n| (input_ids == 3)\n| (input_ids == 4)\n] = np.nan\npair_indexes, impair_indexes = np.arange(0, output.shape[-1], 2), np.arange(\n1, output.shape[-1], 2\n)\nsin, cos = output[:, :, impair_indexes], output[:, :, pair_indexes]\ntan = np.arctan2(sin, cos)\nangles = np.degrees(tan)\nreturn angles\ndef convert_logits_to_dict(self, output: np.ndarray, input_ids: np.ndarray) -> Dict:\n\"\"\"\nConvert the raw predictions into dictionary format.\nIt removes the special tokens and only keeps the predictions for the sequence.\n:param output: predictions from the models in angles\n:param input_ids: input ids from the tokenizer\n:return: a dictionary with the predictions for each angle\n\"\"\"\nindex_start, index_end = (\nnp.where(input_ids == 2)[0][0],\nnp.where(input_ids == 3)[0][0],\n)\noutput_non_pad = output[index_start + 1 : index_end, :]\noutput_angles = {\nangle: output_non_pad[:, angle_index]\nfor angle_index, angle in enumerate(BACKBONE)\n}\nout = pd.DataFrame(output_angles)\nreturn out\nif __name__ == \"__main__\":\nsequence = \"AGGGCUUUAGUCUUUGGAG\"\nrna_torsionbert_helper = RNATorsionBERTHelper()\noutput_angles = rna_torsionbert_helper.predict(sequence)\nprint(output_angles)",
    "nvidia/parakeet-tdt-1.1b": "Parakeet TDT 1.1B (en)\nNVIDIA NeMo: Training\nHow to Use this Model\nAutomatically instantiate the model\nTranscribing using Python\nTranscribing many audio files\nInput\nOutput\nModel Architecture\nTraining\nDatasets\nPerformance\nModel Fairness Evaluation\nGender Bias:\nAge Bias:\nNVIDIA Riva: Deployment\nReferences\nLicence\nParakeet TDT 1.1B (en)\n|\n|\nparakeet-tdt-1.1b is an ASR model that transcribes speech in lower case English alphabet. This model is jointly developed by NVIDIA NeMo and Suno.ai teams.\nIt is an XXL version of FastConformer [1] TDT [2] (around 1.1B parameters) model.\nSee the model architecture section and NeMo documentation for complete architecture details.\nNVIDIA NeMo: Training\nTo train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.\npip install nemo_toolkit['all']\nHow to Use this Model\nThe model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\nAutomatically instantiate the model\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-1.1b\")\nTranscribing using Python\nFirst, let's get a sample\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\nThen simply do:\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\nTranscribing many audio files\npython [NEMO_GIT_FOLDER]/examples/asr/transcribe_speech.py\npretrained_name=\"nvidia/parakeet-tdt-1.1b\"\naudio_dir=\"<DIRECTORY CONTAINING AUDIO FILES>\"\nInput\nThis model accepts 16000 Hz mono-channel audio (wav files) as input.\nOutput\nThis model provides transcribed speech as a string for a given audio sample.\nModel Architecture\nThis model uses a FastConformer-TDT architecture. FastConformer [1] is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling. You may find more information on the details of FastConformer here: Fast-Conformer Model.\nTDT (Token-and-Duration Transducer) [2] is a generalization of conventional Transducers by decoupling token and duration predictions. Unlike conventional Transducers which produces a lot of blanks during inference, a TDT model can skip majority of blank predictions by using the duration output (up to 4 frames for this parakeet-tdt-1.1b model), thus brings significant inference speed-up. The detail of TDT can be found here: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations.\nTraining\nThe NeMo toolkit [3] was used for training the models for over several hundred epochs. These model are trained with this example script and this base config.\nThe tokenizers for these models were built using the text transcripts of the train set with this script.\nDatasets\nThe model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams.\nThe training dataset consists of private subset with 40K hours of English speech plus 24K hours from the following public datasets:\nLibrispeech 960 hours of English speech\nFisher Corpus\nSwitchboard-1 Dataset\nWSJ-0 and WSJ-1\nNational Speech Corpus (Part 1, Part 6)\nVCTK\nVoxPopuli (EN)\nEuroparl-ASR (EN)\nMultilingual Librispeech (MLS EN) - 2,000 hour subset\nMozilla Common Voice (v7.0)\nPeople's Speech  - 12,000 hour subset\nPerformance\nThe performance of Automatic Speech Recognition models is measuring using Word Error Rate. Since this dataset is trained on multiple domains and a much larger corpus, it will generally perform better at transcribing audio in general.\nThe following tables summarizes the performance of the available models in this collection with the Transducer decoder. Performances of the ASR models are reported in terms of Word Error Rate (WER%) with greedy decoding.\nVersion\nTokenizer\nVocabulary Size\nAMI\nEarnings-22\nGiga Speech\nLS test-clean\nSPGI Speech\nTEDLIUM-v3\nVox Populi\nCommon Voice\n1.22.0\nSentencePiece Unigram\n1024\n15.90\n14.65\n9.55\n1.39\n2.62\n3.42\n3.56\n5.48\nThese are greedy WER numbers without external LM. More details on evaluation can be found at HuggingFace ASR Leaderboard\nModel Fairness Evaluation\nAs outlined in the paper \"Towards Measuring Fairness in AI: the Casual Conversations Dataset\", we assessed the parakeet-tdt-1.1b model for fairness. The model was evaluated on the CausalConversations-v1 dataset, and the results are reported as follows:\nGender Bias:\nGender\nMale\nFemale\nN/A\nOther\nNum utterances\n19325\n24532\n926\n33\n% WER\n17.18\n14.61\n19.06\n37.57\nAge Bias:\nAge Group\n$(18-30)$\n$(31-45)$\n$(46-85)$\n$(1-100)$\nNum utterances\n15956\n14585\n13349\n43890\n% WER\n15.83\n15.89\n15.46\n15.74\n(Error rates for fairness evaluation are determined by normalizing both the reference and predicted text, similar to the methods used in the evaluations found at https://github.com/huggingface/open_asr_leaderboard.)\nNVIDIA Riva: Deployment\nNVIDIA Riva, is an accelerated speech AI SDK deployable on-prem, in all clouds, multi-cloud, hybrid, on edge, and embedded.\nAdditionally, Riva provides:\nWorld-class out-of-the-box accuracy for the most common languages with model checkpoints trained on proprietary data with hundreds of thousands of GPU-compute hours\nBest in class accuracy with run-time word boosting (e.g., brand and product names) and customization of acoustic model, language model, and inverse text normalization\nStreaming speech recognition, Kubernetes compatible scaling, and enterprise-grade support\nAlthough this model isnâ€™t supported yet by Riva, the list of supported models is here.Check out Riva live demo.\nReferences\n[1] Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\n[2] Efficient Sequence Transduction by Jointly Predicting Tokens and Durations\n[3] Google Sentencepiece Tokenizer\n[4] NVIDIA NeMo Toolkit\n[5] Suno.ai\n[6] HuggingFace ASR Leaderboard\n[7] Towards Measuring Fairness in AI: the Casual Conversations Dataset\nLicence\nLicense to use this model is covered by the CC-BY-4.0. By downloading the public and release version of the model, you accept the terms and conditions of the CC-BY-4.0 license.",
    "dslim/distilbert-NER": "distilbert-NER\nModel description\nAvailable NER models\nIntended uses & limitations\nTraining data\nCoNLL-2003 English Dataset Statistics\nTraining procedure\nEval results\nBibTeX entry and citation info\ndistilbert-NER\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\nModel description\ndistilbert-NER is the fine-tuned version of DistilBERT, which is a distilled variant of the BERT model. DistilBERT has fewer parameters than BERT, making it smaller, faster, and more efficient. distilbert-NER is specifically fine-tuned for the task of Named Entity Recognition (NER).\nThis model accurately identifies the same four types of entities as its BERT counterparts: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC). Although it is a more compact model, distilbert-NER demonstrates a robust performance in NER tasks, balancing between size, speed, and accuracy.\nThe model was fine-tuned on the English version of the CoNLL-2003 Named Entity Recognition dataset, which is widely recognized for its comprehensive and diverse range of entity types.\nAvailable NER models\nModel Name\nDescription\nParameters\ndistilbert-NER\nFine-tuned DistilBERT - a smaller, faster, lighter version of BERT\n66M\nbert-large-NER\nFine-tuned bert-large-cased - larger model with slightly better performance\n340M\nbert-base-NER-(uncased)\nFine-tuned bert-base, available in both cased and uncased versions\n110M\nIntended uses & limitations\nHow to use\nThis model can be utilized with the Transformers pipeline for NER, similar to the BERT models.\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\nner_results = nlp(example)\nprint(ner_results)\nLimitations and bias\nThe performance of distilbert-NER is linked to its training on the CoNLL-2003 dataset. Therefore, it might show limited effectiveness on text data that significantly differs from this training set. Users should be aware of potential biases inherent in the training data and the possibility of entity misclassification in complex sentences.\nTraining data\nThis model was fine-tuned on English version of the standard CoNLL-2003 Named Entity Recognition dataset.\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation\nDescription\nO\nOutside of a named entity\nB-MISC\nBeginning of a miscellaneous entity right after another miscellaneous entity\nI-MISC\nMiscellaneous entity\nB-PER\nBeginning of a personâ€™s name right after another personâ€™s name\nI-PER\nPersonâ€™s name\nB-ORG\nBeginning of an organization right after another organization\nI-ORG\norganization\nB-LOC\nBeginning of a location right after another location\nI-LOC\nLocation\nCoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper.\n# of training examples per entity type\nDataset\nLOC\nMISC\nORG\nPER\nTrain\n7140\n3438\n6321\n6600\nDev\n1837\n922\n1341\n1842\nTest\n1668\n702\n1661\n1617\n# of articles/sentences/tokens per dataset\nDataset\nArticles\nSentences\nTokens\nTrain\n946\n14,987\n203,621\nDev\n216\n3,466\n51,362\nTest\n231\n3,684\n46,435\nTraining procedure\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the original BERT paper which trained & evaluated the model on CoNLL-2003 NER task.\nEval results\nMetric\nScore\nLoss\n0.0710\nPrecision\n0.9202\nRecall\n0.9232\nF1\n0.9217\nAccuracy\n0.9810\nThe training and validation losses demonstrate a decrease over epochs, signaling effective learning. The precision, recall, and F1 scores are competitive, showcasing the model's robustness in NER tasks.\nBibTeX entry and citation info\nFor DistilBERT:\n@article{sanh2019distilbert,\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\nauthor={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\njournal={arXiv preprint arXiv:1910.01108},\nyear={2019}\n}\nFor the underlying BERT model:\n@article{DBLP:journals/corr/abs-1810-04805,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njournal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/abs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource = {db\nlp computer science bibliography, https://dblp.org}\n}",
    "piddnad/ddcolor_modelscope": "ddcolor_modelscope\nCitation\nddcolor_modelscope\nPretrained model for DDColor\nFor more information, see DDColor Model Zoo\nPaper: https://arxiv.org/abs/2212.11613\nCode: https://github.com/piddnad/DDColor\nCitation\nIf our work is helpful for your research, please consider citing:\n@inproceedings{kang2023ddcolor,\ntitle={DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders},\nauthor={Kang, Xiaoyang and Yang, Tao and Ouyang, Wenqi and Ren, Peiran and Li, Lingzhi and Xie, Xuansong},\nbooktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\npages={328--338},\nyear={2023}\n}",
    "nashikone/iroiroLoRA": "README.md exists but content is empty.",
    "ZySec-AI/SecurityLLM": "ZySec-7B\nFor suggestions please use Road Map\nZySec AI: Unleashing the Potential of the ZySec Series Model\nDiscover the Key Features of Project ZySec\nAbout Project ZySec by ZySec AI\nThe ZySec Roadmap\nZySec-7B\nZySec-7B, stands as a pivotal innovation for security professionals, leveraging the advanced capabilities of HuggingFace's Zephyr language model series. This AI model is crafted to be an omnipresent cybersecurity ally, offering on-demand, expert guidance in cybersecurity issues. Picture ZySec-7B as an ever-present digital teammate, adept at navigating the complexities of security challenges.\nThe efficacy of ZySec-7B lies in its comprehensive training across numerous cybersecurity fields, providing a deep and wide-ranging understanding of the sector. ZySec is developed using the DPO technique, utilizing a varied dataset encompassing critical topics such as:\nSophisticated areas like Attack Surface Threats, Cloud Security, and the Cyber Kill Chain.\nKey compliance and regulatory frameworks, including CIS Controls, FedRAMP, PCI DSS, and ISO/IEC 27001.\nPractical aspects like Cloud Secure Migration, Data Exfiltration Techniques, and Security Incident Handling.\nCrucial strategic fields such as Security Governance, Risk Management, and Security Architecture Review.\nZySec-7B's training spans over 30 unique domains, each enriched with thousands of data points, delivering unparalleled expertise.\nAs the first of its kind in an open-source, AI-driven cybersecurity series, ZySec-7B transcends the conventional role of a support tool, redefining organizational security approaches. Its open-source nature not only invites community contributions but also enhances its flexibility and transparency in managing vast cybersecurity data. ZySec-7B is instrumental in providing vital, actionable insights for strategic decision-making and advanced risk management. More than a mere software, ZySec-7B is a community-enhanced strategic tool, equipping your team to proactively confront and stay ahead of the dynamic landscape of cyber threats and regulatory demands.\nFor suggestions please use Road Map\nDetails of dataset distribution here - Dataset Distribution\nFully compatible with LM Studio. Search for â€œZysecâ€ and here is what you get. Here is a sample output of ZySec writing email to John about database security using LM Studio:\nThe training is funded by ZySec AI, the mobile app for Cyber Security professionals.\nOfficial GGUF version is hosted here - ZySec-7B-v1-GGUF on HuggingFace\nZySec AI: Unleashing the Potential of the ZySec Series Model\nProject ZySec, an integral part of ZySec AI, stands at the forefront of integrating Artificial Intelligence into Cybersecurity. Centered around the innovative ZySec 7B model, it's designed to revolutionize the cybersecurity landscape with AI-driven solutions. ZySec AI isn't just a tool, it's a transformative approach, blending AI's cutting-edge capabilities with the unique intricacies of cybersecurity, while ensuring privacy and security.\nDiscover the Key Features of Project ZySec\nAI-Driven Cybersecurity: Tap into the power of the ZySec 7B model, a bespoke AI solution fine-tuned for cybersecurity.\n24/7 Expert Assistance: Benefit from round-the-clock support and expert advice, guaranteeing smooth operations during any SOC shift.\nEfficient Playbook Access: Streamline your workflow with quick and easy access to playbooks and documents, enhancing information retrieval.\nStandards Explorer: Navigate various standards with ease, akin to a seasoned expert's proficiency.\nOngoing Internet Research: Leverage AI-enabled, thorough internet research for exhaustive insights. (Note: Internet use is optional and specific to this feature).\nAbout Project ZySec by ZySec AI\nZySec AI an opensource project with a vision towards fusioning of Cybersecurity with Artificial Intelligence. Our goal is to transform the way security professionals engage with technology. More than a mere tool, ZySec AI symbolizes a comprehensive strategy to augment security operations, merging the innovative essence of AI with cybersecurity's distinctive challenges, always ensuring privacy and security.\nhttps://github.com/ZySec-AI/ZySec\nThe ZySec Roadmap\nhttps://github.com/ZySec-AI/.github/blob/main/roadmap.md",
    "cckevinn/SeeClick": "No model card",
    "HuggingFaceH4/mistral-7b-grok": "Mistral 7B Grok\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nMistral 7B Grok\nThis model is a fine-tuned version of mistralai/Mistral-7B-v0.1 that has been aligned via Constitutional AI to mimic the style of xAI's Grok assistant.\nIt achieves the following results on the evaluation set:\nLoss: 0.9348\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\ndistributed_type: multi-GPU\nnum_devices: 8\ngradient_accumulation_steps: 4\ntotal_train_batch_size: 256\ntotal_eval_batch_size: 64\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: cosine\nlr_scheduler_warmup_ratio: 0.1\nnum_epochs: 1\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\n0.9326\n1.0\n545\n0.9348\nFramework versions\nTransformers 4.36.2\nPytorch 2.1.2+cu121\nDatasets 2.16.1\nTokenizers 0.15.0",
    "MGE-LLMs/SteelBERT": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nðŸŒŸ MGE-LLMs/SteelBERT ðŸŒŸ\nðŸ“Cite\nðŸŒŸ MGE-LLMs/SteelBERT ðŸŒŸ\nSteelBERT was pre-trained based on DeBERTa using a corpus of 4.2 million materials abstracts and 55,000 full-text steel articles, amounting to roughly 0.96 billion words. For self-supervised training, SteelBERT masked 15% of the tokens using Masked Language Modeling (MLM) â€” a universal and effective pretraining method for NLP tasks.\nSteelBERT was trained to predict the representation of masked words by adjusting parameters across various network layers. We allocated 95% of the corpus for training and 5% for validation. The validation loss reached 1.158 after 840 hours of training.\nWhy DeBERTa? ðŸ¤”\nWe chose the DeBERTa structure due to its innovative approach. DeBERTa introduces a disentangled attention mechanism that handles long-range dependencies, crucial for comprehending complex material interactions.\nThe original DeBERTa model's extensive sub-word vocabulary could introduce noise during tokenization. To address this, we trained a specialized tokenizer, constructing a vocabulary specific to the steel domain. Despite the smaller training corpus, we maintained a consistent vocabulary scale of 128,100 words to capture latent knowledge.\nModel Architecture ðŸ—ï¸\nSteelBERT comprises 188 million parameters and is constructed using 12 stacked Transformer encoders, each with 12 attention heads. We used the original DeBERTa code with similar configurations and size.\nThe maximum sentence length was set to 512 tokens, and training continued until the loss stopped decreasing. The pre-training procedure employed 8 NVIDIA A100 40GB GPUs for 840 hours, with a batch size of 576 sequences.\nNew Features ðŸ†•\nSpecialized Tokenizer ðŸ› ï¸: Trained on a steel materials corpus to enhance accuracy, integrating insights from other material corpora.\nConsistent Vocabulary Scale ðŸ“: Maintained at 128,100 words to capture precise latent knowledge.\nEfficient Training Configuration âš™ï¸: Utilized 8 NVIDIA A100 40GB GPUs for 840 hours with a batch size of 576 sequences.\nEnhanced Fine-tuning Capabilities ðŸŽ¯: Facilitates efficient fine-tuning for specific downstream tasks, enhancing practical application versatility.\nDisentangled Attention Mechanism ðŸ§ : Effectively manages long-range dependencies, inherited from DeBERTa.\nUsage Example ðŸš€\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_path = \"MGE-LLMs/SteelBERT\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(model_path).to(device)  # Move model to GPU if available\n# Example list of texts\ntexts = [\n\"A composite steel plate for marine construction was fabricated using 316L stainless steel.\",\n\"The use of composite materials in construction is growing rapidly.\",\n\"Advances in material science are leading to stronger and more durable steel products.\"\n]\n# Tokenize the texts\ninputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n# Print tokenized texts\nfor i, input_ids in enumerate(inputs['input_ids']):\ntext_tokens = tokenizer.convert_ids_to_tokens(input_ids)\nprint(f\"Tokens for text {i + 1}: {text_tokens}\")\n# Get [CLS] embeddings for each input text\nwith torch.no_grad():\noutputs = model(**inputs, output_hidden_states=True)\nhidden_states = outputs.hidden_states\nlast_hidden_state = hidden_states[-1]\ncls_embeddings = last_hidden_state[:, 0, :]\n# Print the [CLS] token embeddings for each text\nprint(\"CLS embeddings for each text:\")\nprint(cls_embeddings)\nðŸ“Cite\nPaper link:Steel design based on a large language model\n@article{tian2025steel,Â Â Â Â Â Â Â Â title={Steel design based on a large language model},Â Â Â Â Â Â Â Â author={Tian, Shaohan and Jiang, Xue and Wang, Weiren and Jing, Zhihua and Zhang, Chi and Zhang, Cheng and Lookman, Turab and Su, Yanjing},Â Â Â Â Â Â Â Â journal={Acta Materialia},Â Â Â Â Â Â Â Â volume={285},Â Â Â Â Â Â Â Â pages={120663},Â Â Â Â Â Â Â Â year={2025},Â Â Â Â Â Â Â Â publisher={Elsevier}}",
    "TURKCELL/bert-offensive-lang-detection-tr": "Model Description\nDataset Distribution\nPreprocessing Steps\nUsage\nModel Initialization\nEvaluation\nModel Performance Metrics\nOffensive Language Detection For Turkish Language\nModel Description\nThis model has been fine-tuned using dbmdz/bert-base-turkish-128k-uncased model with the OffensEval 2020 dataset.\nThe offenseval-tr dataset contains 31,756 annotated tweets.\nDataset Distribution\nNon Offensive(0)\nOffensive (1)\nTrain\n25625\n6131\nTest\n2812\n716\nPreprocessing Steps\nProcess\nDescription\nAccented character transformation\nConverting accented characters to their unaccented equivalents\nLowercase transformation\nConverting all text to lowercase\nRemoving @user mentions\nRemoving @user formatted user mentions from text\nRemoving hashtag expressions\nRemoving #hashtag formatted expressions from text\nRemoving URLs\nRemoving URLs from text\nRemoving punctuation and punctuated emojis\nRemoving punctuation marks and emojis presented with punctuation from text\nRemoving emojis\nRemoving emojis from text\nDeasciification\nConverting ASCII text into text containing Turkish characters\nThe performance of each pre-process was analyzed.\nRemoving digits and keeping hashtags had no effect.\nUsage\nInstall necessary libraries:\npip install git+https://github.com/emres/turkish-deasciifier.git\npip install keras_preprocessing\nPre-processing functions are below:\nfrom turkish.deasciifier import Deasciifier\ndef deasciifier(text):\ndeasciifier = Deasciifier(text)\nreturn deasciifier.convert_to_turkish()\ndef remove_circumflex(text):\ncircumflex_map = {\n'Ã¢': 'a',\n'Ã®': 'i',\n'Ã»': 'u',\n'Ã´': 'o',\n'Ã‚': 'A',\n'ÃŽ': 'I',\n'Ã›': 'U',\n'Ã”': 'O'\n}\nreturn ''.join(circumflex_map.get(c, c) for c in text)\ndef turkish_lower(text):\nturkish_map = {\n'I': 'Ä±',\n'Ä°': 'i',\n'Ã‡': 'Ã§',\n'Åž': 'ÅŸ',\n'Äž': 'ÄŸ',\n'Ãœ': 'Ã¼',\n'Ã–': 'Ã¶'\n}\nreturn ''.join(turkish_map.get(c, c).lower() for c in text)\nClean text using below function:\nimport re\ndef clean_text(text):\n# Metindeki ÅŸapkalÄ± harfleri kaldÄ±rma\ntext = remove_circumflex(text)\n# Metni kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rme\ntext = turkish_lower(text)\n# deasciifier\ntext = deasciifier(text)\n# KullanÄ±cÄ± adlarÄ±nÄ± kaldÄ±rma\ntext = re.sub(r\"@\\S*\", \" \", text)\n# Hashtag'leri kaldÄ±rma\ntext = re.sub(r'#\\S+', ' ', text)\n# URL'leri kaldÄ±rma\ntext = re.sub(r\"http\\S+|www\\S+|https\\S+\", ' ', text, flags=re.MULTILINE)\n# Noktalama iÅŸaretlerini ve metin tabanlÄ± emojileri kaldÄ±rma\ntext = re.sub(r'[^\\w\\s]|(:\\)|:\\(|:D|:P|:o|:O|;\\))', ' ', text)\n# Emojileri kaldÄ±rma\nemoji_pattern = re.compile(\"[\"\nu\"\\U0001F600-\\U0001F64F\"  # emoticons\nu\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\nu\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\nu\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\nu\"\\U00002702-\\U000027B0\"\nu\"\\U000024C2-\\U0001F251\"\n\"]+\", flags=re.UNICODE)\ntext = emoji_pattern.sub(r' ', text)\n# Birden fazla boÅŸluÄŸu tek boÅŸlukla deÄŸiÅŸtirme\ntext = re.sub(r'\\s+', ' ', text).strip()\nreturn text\nModel Initialization\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"TURKCELL/bert-offensive-lang-detection-tr\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"TURKCELL/bert-offensive-lang-detection-tr\")\nCheck if sentence is offensive like below:\nimport numpy as np\ndef is_offensive(sentence):\nd = {\n0: 'non-offensive',\n1: 'offensive'\n}\nnormalize_text = clean_text(sentence)\ntest_sample = tokenizer([normalize_text], padding=True, truncation=True, max_length=256, return_tensors='pt')\ntest_sample = {k: v.to(device) for k, v in test_sample.items()}\noutput = model(**test_sample)\ny_pred = np.argmax(output.logits.detach().cpu().numpy(), axis=1)\nprint(normalize_text, \"-->\", d[y_pred[0]])\nreturn y_pred[0]\nis_offensive(\"@USER MekanÄ± cennet olsun, saygÄ±lar sayÄ±n avukatÄ±mÄ±z,iyi gÃ¼nler dilerim\")\nis_offensive(\"Bir GÃ¼n Gelecek Biriniz Bile Kalmayana Kadar MÃ¼cadeleye Devam KÃ¶kÃ¼nÃ¼zÃ¼ KurutacaÄŸÄ±z !! #bebekkatilipkk\")\nEvaluation\nEvaluation results on test set shown on table below.\nWe achive %89 accuracy on test set.\nModel Performance Metrics\nClass\nPrecision\nRecall\nF1-score\nAccuracy\nClass 0\n0.92\n0.94\n0.93\n0.89\nClass 1\n0.73\n0.67\n0.70\nMacro\n0.83\n0.80\n0.81",
    "openbmb/MiniCPM-V": "MiniCPM-V\nNews\nEvaluation\nExamples\nModel License\nStatement\nDemo\nModel License\nStatement\nDeployment on Mobile Phone\nModel License\nStatement\nUsage\nModel License\nStatement\nLicense\nModel License\nStatement\nMiniCPM-V\nNews\n[2025.01.14] ðŸ”¥ðŸ”¥ We open source MiniCPM-o 2.6, with significant performance improvement over MiniCPM-V 2.6, and support real-time speech-to-speech conversation and multimodal live streaming. Try it now.\n[2024.08.06] ðŸ”¥ We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad.\n[2024.05.20] ðŸ”¥ GPT-4V level multimodal model MiniCPM-Llama3-V 2.5 is out.\n[2024.04.11] ðŸ”¥ MiniCPM-V 2.0 is out.\nMiniCPM-V (i.e., OmniLMM-3B) is an efficient version with promising performance for deployment. The model is built based on SigLip-400M and MiniCPM-2.4B, connected by a perceiver resampler. Notable features of OmniLMM-3B include:\nâš¡ï¸ High Efficiency.\nMiniCPM-V can be efficiently deployed on most GPU cards and personal computers, and even on end devices such as mobile phones. In terms of visual encoding, we compress the image representations into 64 tokens via a perceiver resampler, which is significantly fewer than other LMMs based on MLP architecture (typically > 512 tokens). This allows OmniLMM-3B to operate with much less memory cost and higher speed during inference.\nðŸ”¥ Promising Performance.\nMiniCPM-V achieves state-of-the-art performance on multiple benchmarks (including MMMU, MME, and MMbech, etc) among models with comparable sizes, surpassing existing LMMs built on Phi-2. It even achieves comparable or better performance than the 9.6B Qwen-VL-Chat.\nðŸ™Œ Bilingual Support.\nMiniCPM-V is the first end-deployable LMM supporting bilingual multimodal interaction in English and Chinese. This is achieved by generalizing multimodal capabilities across languages, a technique from the ICLR 2024 spotlight paper.\nEvaluation\nModel\nSize\nMME\nMMB dev (en)\nMMB dev (zh)\nMMMU val\nCMMMU val\nLLaVA-Phi\n3.0B\n1335\n59.8\n-\n-\n-\nMobileVLM\n3.0B\n1289\n59.6\n-\n-\n-\nImp-v1\n3B\n1434\n66.5\n-\n-\n-\nQwen-VL-Chat\n9.6B\n1487\n60.6\n56.7\n35.9\n30.7\nCogVLM\n17.4B\n1438\n63.7\n53.8\n32.1\n-\nMiniCPM-V\n3B\n1452\n67.9\n65.3\n37.2\n32.1\nExamples\nDemo\nClick here to try out the Demo of MiniCPM-V.\nDeployment on Mobile Phone\nCurrently MiniCPM-V (i.e., OmniLMM-3B) can be deployed on mobile phones with Android and Harmony operating systems. ðŸš€ Try it out here.\nUsage\nInference using Huggingface transformers on Nivdia GPUs or Mac with MPS (Apple silicon or AMD GPUs). Requirements tested on python 3.10ï¼š\nPillow==10.1.0\ntimm==0.9.10\ntorch==2.1.2\ntorchvision==0.16.2\ntransformers==4.36.0\nsentencepiece==0.1.99\n# test.py\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True, torch_dtype=torch.bfloat16)\n# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\nmodel = model.to(device='cuda', dtype=torch.bfloat16)\n# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\n#model = model.to(device='cuda', dtype=torch.float16)\n# For Mac with MPS (Apple silicon or AMD GPUs).\n# Run with `PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py`\n#model = model.to(device='mps', dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True)\nmodel.eval()\nimage = Image.open('xx.jpg').convert('RGB')\nquestion = 'What is in the image?'\nmsgs = [{'role': 'user', 'content': question}]\nres, context, _ = model.chat(\nimage=image,\nmsgs=msgs,\ncontext=None,\ntokenizer=tokenizer,\nsampling=True,\ntemperature=0.7\n)\nprint(res)\nPlease look at GitHub for more detail about usage.\nLicense\nModel License\nThe code in this repo is released under the Apache-2.0 License.\nThe usage of MiniCPM-V series model weights must strictly follow MiniCPM Model License.md.\nThe models and weights of MiniCPM are completely free for academic research. after filling out a \"questionnaire\" for registration, are also available for free commercial use.\nStatement\nAs a LLM, MiniCPM-V generates contents by learning a large mount of texts, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V does not represent the views and positions of the model developers\nWe will not be liable for any problems arising from the use of the MinCPM-V open Source model, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.",
    "Qwen/Qwen1.5-7B-Chat": "Qwen1.5-7B-Chat\nIntroduction\nModel Details\nTraining details\nRequirements\nQuickstart\nTips\nCitation\nQwen1.5-7B-Chat\nIntroduction\nQwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:\n8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;\nSignificant performance improvement in human preference for chat models;\nMultilingual support of both base and chat models;\nStable support of 32K context length for models of all sizes\nNo need of trust_remote_code.\nFor more details, please refer to our blog post and GitHub repo.\nModel Details\nQwen1.5 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes. For the beta version, temporarily we did not include GQA (except for 32B) and the mixture of SWA and full attention.\nTraining details\nWe pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.\nRequirements\nThe code of Qwen1.5 has been in the latest Hugging face transformers and we advise you to install transformers>=4.37.0, or you might encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\n\"Qwen/Qwen1.5-7B-Chat\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\")\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\nmodel_inputs.input_ids,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nFor quantized models, we advise you to use the GPTQ, AWQ, and GGUF correspondents, namely Qwen1.5-7B-Chat-GPTQ-Int4, Qwen1.5-7B-Chat-GPTQ-Int8, Qwen1.5-7B-Chat-AWQ, and Qwen1.5-7B-Chat-GGUF.\nTips\nIf you encounter code switching or other bad cases, we advise you to use our provided hyper-parameters in generation_config.json.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen,\ntitle={Qwen Technical Report},\nauthor={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\njournal={arXiv preprint arXiv:2309.16609},\nyear={2023}\n}",
    "TheBloke/CodeLlama-70B-Instruct-GGUF": "Codellama 70B Instruct - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: CodeLlama-70B-Instruct\nCompatibility\nExplanation of quantisation methods\nProvided files\nQ6_K and Q8_0 files are split and require joining\nq6_K\nq8_0\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using llama-cpp-python\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nOriginal model card: Code Llama's Codellama 70B Instruct\nCode Llama\nModel Use\nChat prompt\nModel Details\nIntended Use\nHardware and Software\nEvaluation Results\nEthical Considerations and Limitations\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nCodellama 70B Instruct - GGUF\nModel creator: Code Llama\nOriginal model: Codellama 70B Instruct\nDescription\nThis repo contains GGUF format model files for Code Llama's Codellama 70B Instruct.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nGPT4All, a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nCode Llama's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: CodeLlama-70B-Instruct\nSource: system\n{system_message}<step> Source: user\n{prompt} <step> Source: assistant\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\ncodellama-70b-instruct.Q2_K.gguf\nQ2_K\n2\n25.46 GB\n27.96 GB\nsignificant quality loss - not recommended for most purposes\ncodellama-70b-instruct.Q3_K_S.gguf\nQ3_K_S\n3\n29.92 GB\n32.42 GB\nvery small, high quality loss\ncodellama-70b-instruct.Q3_K_M.gguf\nQ3_K_M\n3\n33.27 GB\n35.77 GB\nvery small, high quality loss\ncodellama-70b-instruct.Q3_K_L.gguf\nQ3_K_L\n3\n36.15 GB\n38.65 GB\nsmall, substantial quality loss\ncodellama-70b-instruct.Q4_0.gguf\nQ4_0\n4\n38.87 GB\n41.37 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ncodellama-70b-instruct.Q4_K_S.gguf\nQ4_K_S\n4\n39.25 GB\n41.75 GB\nsmall, greater quality loss\ncodellama-70b-instruct.Q4_K_M.gguf\nQ4_K_M\n4\n41.42 GB\n43.92 GB\nmedium, balanced quality - recommended\ncodellama-70b-instruct.Q5_0.gguf\nQ5_0\n5\n47.46 GB\n49.96 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ncodellama-70b-instruct.Q5_K_S.gguf\nQ5_K_S\n5\n47.46 GB\n49.96 GB\nlarge, low quality loss - recommended\ncodellama-70b-instruct.Q5_K_M.gguf\nQ5_K_M\n5\n48.75 GB\n51.25 GB\nlarge, very low quality loss - recommended\ncodellama-70b-instruct.Q6_K.gguf\nQ6_K\n6\n56.59 GB\n59.09 GB\nvery large, extremely low quality loss\ncodellama-70b-instruct.Q8_0.gguf\nQ8_0\n8\n73.29 GB\n75.79 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nQ6_K and Q8_0 files are split and require joining\nNote: HF does not support uploading files larger than 50GB. Therefore I have uploaded the Q6_K and Q8_0 files as split files.\nClick for instructions regarding Q6_K and Q8_0 files\nq6_K\nPlease download:\ncodellama-70b-instruct.Q6_K.gguf-split-a\ncodellama-70b-instruct.Q6_K.gguf-split-b\nq8_0\nPlease download:\ncodellama-70b-instruct.Q8_0.gguf-split-a\ncodellama-70b-instruct.Q8_0.gguf-split-b\nTo join the files, do the following:\nLinux and macOS:\ncat codellama-70b-instruct.Q6_K.gguf-split-* > codellama-70b-instruct.Q6_K.gguf && rm codellama-70b-instruct.Q6_K.gguf-split-*\ncat codellama-70b-instruct.Q8_0.gguf-split-* > codellama-70b-instruct.Q8_0.gguf && rm codellama-70b-instruct.Q8_0.gguf-split-*\nWindows command line:\nCOPY /B codellama-70b-instruct.Q6_K.gguf-split-a + codellama-70b-instruct.Q6_K.gguf-split-b codellama-70b-instruct.Q6_K.gguf\ndel codellama-70b-instruct.Q6_K.gguf-split-a codellama-70b-instruct.Q6_K.gguf-split-b\nCOPY /B codellama-70b-instruct.Q8_0.gguf-split-a + codellama-70b-instruct.Q8_0.gguf-split-b codellama-70b-instruct.Q8_0.gguf\ndel codellama-70b-instruct.Q8_0.gguf-split-a codellama-70b-instruct.Q8_0.gguf-split-b\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/CodeLlama-70B-Instruct-GGUF and below it, a specific filename to download, such as: codellama-70b-instruct.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/CodeLlama-70B-Instruct-GGUF codellama-70b-instruct.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage (click to read)\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/CodeLlama-70B-Instruct-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/CodeLlama-70B-Instruct-GGUF codellama-70b-instruct.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 35 -m codellama-70b-instruct.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Source: system\\n\\n  {system_message}<step> Source: user\\n\\n  {prompt} <step> Source: assistant\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 4096 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions can be found in the text-generation-webui documentation, here: text-generation-webui/docs/04 â€ Model Tab.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\nHow to load this model in Python code, using llama-cpp-python\nFor full documentation, please see: llama-cpp-python docs.\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\nSimple llama-cpp-python example code\nfrom llama_cpp import Llama\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\nmodel_path=\"./codellama-70b-instruct.Q4_K_M.gguf\",  # Download the model file first\nn_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\nn_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\nn_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n# Simple inference example\noutput = llm(\n\"Source: system\\n\\n  {system_message}<step> Source: user\\n\\n  {prompt} <step> Source: assistant\", # Prompt\nmax_tokens=512,  # Generate up to 512 tokens\nstop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\necho=True        # Whether to echo the prompt\n)\n# Chat Completion API\nllm = Llama(model_path=\"./codellama-70b-instruct.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n{\n\"role\": \"user\",\n\"content\": \"Write a story about llamas.\"\n}\n]\n)\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Michael Levine, é˜¿æ˜Ž, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik BjÃ¤reholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Code Llama's Codellama 70B Instruct\nCode Llama\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\nBase Model\nPython\nInstruct\n7B\ncodellama/CodeLlama-7b-hf\ncodellama/CodeLlama-7b-Python-hf\ncodellama/CodeLlama-7b-Instruct-hf\n13B\ncodellama/CodeLlama-13b-hf\ncodellama/CodeLlama-13b-Python-hf\ncodellama/CodeLlama-13b-Instruct-hf\n34B\ncodellama/CodeLlama-34b-hf\ncodellama/CodeLlama-34b-Python-hf\ncodellama/CodeLlama-34b-Instruct-hf\n70B\ncodellama/CodeLlama-70b-hf\ncodellama/CodeLlama-70b-Python-hf\ncodellama/CodeLlama-70b-Instruct-hf\nModel capabilities:\nCode completion.\nInfilling.\nInstructions / chat.\nPython specialist.\nModel Use\nInstall transformers\npip install transformers accelerate\nChat use: The 70B Instruct model uses a different prompt template than the smaller versions. To use it with transformers, we recommend you use the built-in chat template:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"codellama/CodeLlama-70b-Instruct-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\n)\nchat = [\n{\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n{\"role\": \"user\", \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n]\ninputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(input_ids=inputs, max_new_tokens=200)\noutput = output[0].to(\"cpu\")\nprint(tokenizer.decode(output))\nYou can also use the model for text or code completion. This examples uses transformers' pipeline interface:\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\nmodel_id = \"codellama/CodeLlama-70b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\n)\nsequences = pipeline(\n'def fibonacci(',\ndo_sample=True,\ntemperature=0.2,\ntop_p=0.9,\nnum_return_sequences=1,\neos_token_id=tokenizer.eos_token_id,\nmax_length=100,\n)\nfor seq in sequences:\nprint(f\"Result: {seq['generated_text']}\")\nChat prompt\nCodeLlama 70B Instruct uses a different format for the chat prompt than previous Llama 2 or CodeLlama models. As mentioned above, the easiest way to use it is with the help of the tokenizer's chat template. If you need to build the string or tokens, manually, here's how to do it.\nWe'll do our tests with the following made-up dialog:\nchat = [\n{\"role\": \"system\", \"content\": \"System prompt    \"},\n{\"role\": \"user\", \"content\": \"First user query\"},\n{\"role\": \"assistant\", \"content\": \"Model response to first query\"},\n{\"role\": \"user\", \"content\": \"Second user query\"},\n]\nFirst, let's see what the prompt looks like if we use the chat template:\ntokenizer.apply_chat_template(chat, tokenize=False)\n'<s>Source: system\\n\\n System prompt <step> Source: user\\n\\n First user query <step> Source: assistant\\n\\n Model response to first query <step> Source: user\\n\\n Second user query <step> Source: assistant\\nDestination: user\\n\\n '\nSo each turn of the conversation has a Source (system, user, or assistant), and then the content appears after two newlines and a space. Turns are separated with the special token <step>. After the last turn (which must necessarily come from the user), we invite the model to respond by using the special syntax Source: assistant\\nDestination: user\\n\\n . Let's see how we can build the same string ourselves:\noutput = \"<s>\"\nfor m in chat:\noutput += f\"Source: {m['role']}\\n\\n {m['content'].strip()}\"\noutput += \" <step> \"\noutput += \"Source: assistant\\nDestination: user\\n\\n \"\noutput\n'<s>Source: system\\n\\n System prompt <step> Source: user\\n\\n First user query <step> Source: assistant\\n\\n Model response to first query <step> Source: user\\n\\n Second user query <step> Source: assistant\\nDestination: user\\n\\n '\nTo verify that we got it right, we'll compare against the reference code in the original GitHub repo. We used the same dialog and tokenized it with the dialog_prompt_tokens function and got the following tokens:\nreference_tokens = [1, 7562, 29901, 1788, 13, 13, 2184, 9508, 32015, 7562, 29901, 1404, 13, 13, 3824, 1404, 2346, 32015, 7562, 29901, 20255, 13, 13, 8125, 2933, 304, 937, 2346, 32015, 7562, 29901, 1404, 13, 13, 6440, 1404, 2346, 32015, 7562, 29901, 20255, 13, 14994, 3381, 29901, 1404, 13, 13, 29871]\nLet's see what we get with the string we built using our Python loop. Note that we don't add \"special tokens\" because the string already starts with <s>, the beginning of sentence token:\ntokens = tokenizer.encode(output, add_special_tokens=False)\nassert reference_tokens == tokens\nSimilarly, let's verify that the chat template produces the same token sequence:\nassert reference_tokens == tokenizer.apply_chat_template(chat)\nAs a final detail, please note that if the dialog does not start with a system turn, the original code will insert one with an empty content string.\nModel Details\n*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).\nModel Developers Meta\nVariations Code Llama comes in four model sizes, and three variants:\nCode Llama: base models designed for general code synthesis and understanding\nCode Llama - Python: designed specifically for Python\nCode Llama - Instruct: for instruction following and safer deployment\nAll variants are available in sizes of 7B, 13B, 34B, and 70B parameters.\nThis repository contains the Instruct version of the 70B parameters model.\nInput Models input text only.\nOutput Models generate text only.\nModel Architecture Code Llama is an auto-regressive language model that uses an optimized transformer architecture. It was fine-tuned with up to 16k tokens. This variant does not support long context of up to 100k tokens.\nModel Dates Code Llama and its variants have been trained between January 2023 and January 2024.\nStatus This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\nResearch Paper More information can be found in the paper \"Code Llama: Open Foundation Models for Code\" or its arXiv page.\nIntended Use\nIntended Use Cases Code Llama and its variants are intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\nHardware and Software\nTraining Factors We used custom training libraries. The training and fine-tuning of the released models have been performed Metaâ€™s Research Super Cluster.\nCarbon Footprint In aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 228.55 tCO2eq, 100% of which were offset by Metaâ€™s sustainability program.\nEvaluation Results\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\nEthical Considerations and Limitations\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llamaâ€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\nPlease see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-use-guide.",
    "ChatterjeeLab/PTM-Mamba": "PTM-Mamba\nInstall Enviroment\nDocker\nData\nConfigs\nTraining\nInference\nAcknowledgement\nCitation\nPTM-Mamba\nA PTM-Aware Protein Language Model with Bidirectional Gated Mamba Blocks\n[HuggingFace]    [GitHub]   [Paper]\nInstall Enviroment\nDocker\nSetting up env for mamba could be a pain, alternatively, we suggest using docker containers.\nRun container in interactive and detach mode, and mounte project dir to the container workspace.\ndocker run --gpus all -v $(pwd):/workspace -d -it --name plm_benji nvcr.io/nvidia/pytorch:23.12-py3 /bin/bash && docker attach plm_benji\nInstall pkgs in container\nmkdir /root/.cache/torch/hub/checkpoints/ -p; wget -O /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\ncd protein_lm/modeling/models/libs/ && pip install -e causal-conv1d && pip install -e mamba && cd ../../../../\npip install transformers datasets accelerate evaluate pytest fair-esm biopython deepspeed wandb\npip install torch_geometric\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html\npip install -e .\npip install hydra-core --upgrade\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource \"$HOME/.cargo/env\"\npip install -e protein_lm/tokenizer/rust_trie\nData\nWe collect protein sequences and their PTM annotations from Uniprot-Swissprot. The PTM annotations are represented as tokens and used to replace the corresponding amino acids. The data can be downloaded from here. Please place the data in  protein_lm/dataset/.\nConfigs\nThe training and testing configs are in protein_lm/configs. We provide a basic training config at protein_lm/configs/train/base.yaml.\nTraining\nSingle-GPU Training\npython ./protein_lm/modeling/scripts/train.py +train=base\nThe command will use the configs in protein_lm/configs/train/base.yaml.\nMulti-GPU Training\nWe use Distributed training with ðŸ¤— Accelerate (huggingface.co).\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 accelerate launch --num_processes=8 --multi_gpu protein_lm/modeling/scripts/train.py +train=base   train.report_to='wandb' train.training_arguments.per_device_train_batch_size=256 train.training_arguments.use_esm=True train.training_arguments.save_dir='ckpt/ptm_mamba' train.model.model_type='bidirectional_mamba' train.training_arguments.max_tokens_per_batch=40000\nreport_to='wandb'  tracks the training using wandb.\ntraining_arguments.per_device_train_batch_size=300 sets the max batch size per device when constructing a batch.\ntraining_arguments.max_tokens_per_batch=80000 sets the max num of tokens within a batch. If a batch exceeds the max token limit(depending on the seq len), we will trim the batch. Tune the per_device_train_batch_size and max_tokens_per_batch togather to maximize the memory usage during training. The rule of thumb is setting a large batch size (e.g., 300) while searching for the max num token that fits your GPU memory.\ntraining_arguments.use_esm=True uses the ESM embedding. By default, we use ESM 650M, and set the model.esm_embed_dim: 1280 in base.yaml.  If disabled, the model will use its own embeddings.\ntraining_arguments.save_dir='ckpt/bi_directional_mamba-esm' where the model ckpts will be saved.\ntraining_arguments.sample_len_ascending=true is enable by default, samples sequences from short to long during the training.\nMulti-GPU training with Deepspeed\nSetup deepspeed with\naccelerate config\nand answer the questions asked. It will ask whether you want to use a config file for DeepSpeed to which you should answer no. Then answer the following questions to generate a basic DeepSpeed config. Use ZeRo 2 and FP32, which are sufficient for training our ~300M model without introducing overhead. This will generate a config file that will be used automatically to properly set the default options when launching training.\nInference\nThe inference example is at protein_lm/modeling/scripts/infer.py. The model checkpoints can be downloaded from here. The outputs are:\nOutput = namedtuple(\"output\", [\"logits\", \"hidden_states\"])\nfrom protein_lm.modeling.scripts.infer import PTMMamba\nckpt_path = \"ckpt/bi_mamba-esm-ptm_token_input/best.ckpt\"\nmamba = PTMMamba(ckpt_path,device='cuda:0')\nseq = \"M<N-acetylalanine>K\"\noutput = mamba(seq)\nprint(output.logits.shape)\nprint(output.hidden_states.shape)\nAcknowledgement\nThis project is based on the  following codebase. Please give them a star if you like our code.\nOpenBioML/protein-lm-scaling (github.com)\nstate-spaces/mamba (github.com)\nCitation\nPlease cite our paper if you enjoy our code :)\n@article {Peng2025,\nauthor={Peng, Fred Zhangzhi and Wang, Chentong and Chen, Tong and Schussheim, Benjamin and Vincoff, Sophia and Chatterjee, Pranam},\ntitle={PTM-Mamba: a PTM-aware protein language model with bidirectional gated Mamba blocks},\njournal={Nature Methods},\nyear={2025},\nissn={1548-7105},\ndoi={10.1038/s41592-025-02656-9},\nurl={https://doi.org/10.1038/s41592-025-02656-9}\n}",
    "liuhaotian/llava-v1.6-mistral-7b": "LLaVA Model Card\nModel details\nLicense\nIntended use\nTraining dataset\nEvaluation dataset\nLLaVA Model Card\nModel details\nModel type:\nLLaVA is an open-source chatbot trained by fine-tuning LLM on multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture.\nBase LLM: mistralai/Mistral-7B-Instruct-v0.2\nModel date:\nLLaVA-v1.6-Mistral-7B was trained in December 2023.\nPaper or resources for more information:\nhttps://llava-vl.github.io/\nLicense\nmistralai/Mistral-7B-Instruct-v0.2 license.\nWhere to send questions or comments about the model:\nhttps://github.com/haotian-liu/LLaVA/issues\nIntended use\nPrimary intended uses:\nThe primary use of LLaVA is research on large multimodal models and chatbots.\nPrimary intended users:\nThe primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.\nTraining dataset\n558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.\n158K GPT-generated multimodal instruction-following data.\n500K academic-task-oriented VQA data mixture.\n50K GPT-4V data mixture.\n40K ShareGPT data.\nEvaluation dataset\nA collection of 12 benchmarks, including 5 academic VQA benchmarks and 7 recent benchmarks specifically proposed for instruction-following LMMs.",
    "thu-coai/CharacterGLM-6B": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nä»£ç ç¤ºä¾‹\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"thu-coai/CharacterGLM-6B\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"thu-coai/CharacterGLM-6B\", trust_remote_code=True, device='cuda')\n>>> model = model.eval()\n>>> session_meta = {'user_info': 'æˆ‘æ˜¯é™†æ˜Ÿè¾°ï¼Œæ˜¯ä¸€ä¸ªç”·æ€§ï¼Œæ˜¯ä¸€ä½çŸ¥åå¯¼æ¼”ï¼Œä¹Ÿæ˜¯è‹æ¢¦è¿œçš„åˆä½œå¯¼æ¼”ã€‚æˆ‘æ“…é•¿æ‹æ‘„éŸ³ä¹é¢˜æçš„ç”µå½±ã€‚è‹æ¢¦è¿œå¯¹æˆ‘çš„æ€åº¦æ˜¯å°Šæ•¬çš„ï¼Œå¹¶è§†æˆ‘ä¸ºè‰¯å¸ˆç›Šå‹ã€‚', 'bot_info': 'è‹æ¢¦è¿œï¼Œæœ¬åè‹è¿œå¿ƒï¼Œæ˜¯ä¸€ä½å½“çº¢çš„å›½å†…å¥³æ­Œæ‰‹åŠæ¼”å‘˜ã€‚åœ¨å‚åŠ é€‰ç§€èŠ‚ç›®åŽï¼Œå‡­å€Ÿç‹¬ç‰¹çš„å—“éŸ³åŠå‡ºä¼—çš„èˆžå°é­…åŠ›è¿…é€Ÿæˆåï¼Œè¿›å…¥å¨±ä¹åœˆã€‚å¥¹å¤–è¡¨ç¾Žä¸½åŠ¨äººï¼Œä½†çœŸæ­£çš„é­…åŠ›åœ¨äºŽå¥¹çš„æ‰åŽå’Œå‹¤å¥‹ã€‚è‹æ¢¦è¿œæ˜¯éŸ³ä¹å­¦é™¢æ¯•ä¸šçš„ä¼˜ç§€ç”Ÿï¼Œå–„äºŽåˆ›ä½œï¼Œæ‹¥æœ‰å¤šé¦–çƒ­é—¨åŽŸåˆ›æ­Œæ›²ã€‚é™¤äº†éŸ³ä¹æ–¹é¢çš„æˆå°±ï¼Œå¥¹è¿˜çƒ­è¡·äºŽæ…ˆå–„äº‹ä¸šï¼Œç§¯æžå‚åŠ å…¬ç›Šæ´»åŠ¨ï¼Œç”¨å®žé™…è¡ŒåŠ¨ä¼ é€’æ­£èƒ½é‡ã€‚åœ¨å·¥ä½œä¸­ï¼Œå¥¹å¯¹å¾…å·¥ä½œéžå¸¸æ•¬ä¸šï¼Œæ‹æˆæ—¶æ€»æ˜¯å…¨èº«å¿ƒæŠ•å…¥è§’è‰²ï¼Œèµ¢å¾—äº†ä¸šå†…äººå£«çš„èµžèª‰å’Œç²‰ä¸çš„å–œçˆ±ã€‚è™½ç„¶åœ¨å¨±ä¹åœˆï¼Œä½†å¥¹å§‹ç»ˆä¿æŒä½Žè°ƒã€è°¦é€Šçš„æ€åº¦ï¼Œæ·±å¾—åŒè¡Œå°Šé‡ã€‚åœ¨è¡¨è¾¾æ—¶ï¼Œè‹æ¢¦è¿œå–œæ¬¢ä½¿ç”¨â€œæˆ‘ä»¬â€å’Œâ€œä¸€èµ·â€ï¼Œå¼ºè°ƒå›¢é˜Ÿç²¾ç¥žã€‚', 'bot_name': 'è‹æ¢¦è¿œ', 'user_name': 'é™†æ˜Ÿè¾°'}\n>>> response, history = model.chat(tokenizer, session_meta, \"ä½ å¥½\", history=[])",
    "wondervictor/YOLO-World": "README.md exists but content is empty."
}