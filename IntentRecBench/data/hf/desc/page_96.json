{
    "tasksource/ModernBERT-large-nli": "Model Card for Model ID\nUsage\n[ZS] Zero-shot classification pipeline\n[NLI] Natural language inference pipeline\nBackbone for further fune-tuning\nCitation\nModel Card for Model ID\nThis model is ModernBERT multi-task fine-tuned on tasksource NLI tasks, including MNLI, ANLI, SICK, WANLI, doc-nli, LingNLI, FOLIO, FOL-NLI, LogicNLI, Label-NLI and all datasets in the below table).\nThis is the equivalent of an \"instruct\" version.\nThe model was trained for 200k steps on an Nvidia A30 GPU.\nIt is very good at reasoning tasks (better than llama 3.1 8B Instruct on ANLI and FOLIO), long context reasoning, sentiment analysis and zero-shot classification with new labels.\nThe following table shows model test accuracy. These are the scores for the same single transformer with different classification heads on top.\nFurther gains can be obtained by fine-tuning on a single-task, e.g. SST, but it this checkpoint is great for zero-shot classification and natural language inference (contradiction/entailment/neutral classification).\ntest_name\ntest_accuracy\nglue/mnli\n0.89\nglue/qnli\n0.96\nglue/rte\n0.91\nglue/wnli\n0.64\nglue/mrpc\n0.81\nglue/qqp\n0.87\nglue/cola\n0.87\nglue/sst2\n0.96\nsuper_glue/boolq\n0.66\nsuper_glue/cb\n0.86\nsuper_glue/multirc\n0.9\nsuper_glue/wic\n0.71\nsuper_glue/axg\n1\nanli/a1\n0.72\nanli/a2\n0.54\nanli/a3\n0.55\nsick/label\n0.91\nsick/entailment_AB\n0.93\nsnli\n0.94\nscitail/snli_format\n0.95\nhans\n1\nWANLI\n0.77\nrecast/recast_ner\n0.85\nrecast/recast_sentiment\n0.97\nrecast/recast_verbnet\n0.89\nrecast/recast_megaveridicality\n0.87\nrecast/recast_verbcorner\n0.87\nrecast/recast_kg_relations\n0.9\nrecast/recast_factuality\n0.95\nrecast/recast_puns\n0.98\nprobability_words_nli/reasoning_1hop\n1\nprobability_words_nli/usnli\n0.79\nprobability_words_nli/reasoning_2hop\n0.98\nnan-nli\n0.85\nnli_fever\n0.78\nbreaking_nli\n0.99\nconj_nli\n0.72\nfracas\n0.79\ndialogue_nli\n0.94\nmpe\n0.75\ndnc\n0.91\nrecast_white/fnplus\n0.76\nrecast_white/sprl\n0.9\nrecast_white/dpr\n0.84\nadd_one_rte\n0.94\npaws/labeled_final\n0.96\npragmeval/pdtb\n0.56\nlex_glue/scotus\n0.58\nlex_glue/ledgar\n0.85\ndynasent/dynabench.dynasent.r1.all/r1\n0.83\ndynasent/dynabench.dynasent.r2.all/r2\n0.76\ncycic_classification\n0.96\nlingnli\n0.91\nmonotonicity-entailment\n0.97\nscinli\n0.88\nnaturallogic\n0.93\ndynahate\n0.86\nsyntactic-augmentation-nli\n0.94\nautotnli\n0.92\ndefeasible-nli/atomic\n0.83\ndefeasible-nli/snli\n0.8\nhelp-nli\n0.96\nnli-veridicality-transitivity\n0.99\nlonli\n0.99\ndadc-limit-nli\n0.79\nfolio\n0.71\ntomi-nli\n0.54\npuzzte\n0.59\ntemporal-nli\n0.93\ncounterfactually-augmented-snli\n0.81\ncnli\n0.9\nboolq-natural-perturbations\n0.72\nequate\n0.65\nlogiqa-2.0-nli\n0.58\nmindgames\n0.96\nConTRoL-nli\n0.66\nlogical-fallacy\n0.38\ncladder\n0.89\nconceptrules_v2\n1\nzero-shot-label-nli\n0.79\nscone\n1\nmonli\n1\nSpaceNLI\n1\npropsegment/nli\n0.92\nFLD.v2/default\n0.91\nFLD.v2/star\n0.78\nSDOH-NLI\n0.99\nscifact_entailment\n0.87\nfeasibilityQA\n0.79\nAdjectiveScaleProbe-nli\n1\nresnli\n1\nsemantic_fragments_nli\n1\ndataset_train_nli\n0.95\nnlgraph\n0.97\nruletaker\n0.99\nPARARULE-Plus\n1\nlogical-entailment\n0.93\nnope\n0.56\nLogicNLI\n0.91\ncontract-nli/contractnli_a/seg\n0.88\ncontract-nli/contractnli_b/full\n0.84\nnli4ct_semeval2024\n0.72\nbiosift-nli\n0.92\nSIGA-nli\n0.57\nFOL-nli\n0.79\ndoc-nli\n0.81\nmctest-nli\n0.92\nnatural-language-satisfiability\n0.92\nidioms-nli\n0.83\nlifecycle-entailment\n0.79\nMSciNLI\n0.84\nhover-3way/nli\n0.92\nseahorse_summarization_evaluation\n0.81\nmissing-item-prediction/contrastive\n0.88\nPol_NLI\n0.93\nsynthetic-retrieval-NLI/count\n0.72\nsynthetic-retrieval-NLI/position\n0.9\nsynthetic-retrieval-NLI/binary\n0.92\nbabi_nli\n0.98\nUsage\n[ZS] Zero-shot classification pipeline\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\",model=\"tasksource/ModernBERT-large-nli\")\ntext = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(text, candidate_labels)\nNLI training data of this model includes label-nli, a NLI dataset specially constructed to improve this kind of zero-shot classification.\n[NLI] Natural language inference pipeline\nfrom transformers import pipeline\npipe = pipeline(\"text-classification\",model=\"tasksource/ModernBERT-large-nli\")\npipe([dict(text='there is a cat',\ntext_pair='there is a black cat')]) #list of (premise,hypothesis)\nBackbone for further fune-tuning\nThis checkpoint has stronger reasoning and fine-grained abilities than the base version and can be used for further fine-tuning.\nCitation\n@inproceedings{sileo-2024-tasksource,\ntitle = \"tasksource: A Large Collection of {NLP} tasks with a Structured Dataset Preprocessing Framework\",\nauthor = \"Sileo, Damien\",\nbooktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",\nmonth = may,\nyear = \"2024\",\naddress = \"Torino, Italia\",\npublisher = \"ELRA and ICCL\",\nurl = \"https://aclanthology.org/2024.lrec-main.1361\",\npages = \"15655--15684\",\n}",
    "Konnect1221/The-Inception-Presets-Methception-LLamaception-Qwenception": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nMethception 1.4, LLamaception 1.5, and Finally Introducing Qwenception 1.4: A New Level of Roleplay\nGet ready to indulge in three distinct flavors of inception-like secret sauce. Whether you're rolling with Mistral-based models, Llama 3.3 70B beasts, or Qwen 2.5/coder wizards, these Ception presets bring the heat without losing that chill vibe. Picture a smooth, quantized dreamscape that pulls you in with deep storytelling, real immersion, and on-point energy.\nA Trio of Templates\nMethception 1.4\nMethception is our OG powerhouse built for Mistral-based models using the Metharme prompt format. It's already proven its might with fine-tunes like Drummers Behemoth 1.2 and MortalWombat's Monstral V2, layering spatial awareness, emotional undercurrents, and cinematic pacing. The result? Characters who spring to life, fueled by a balanced mix of positive, neutral, and negative biases that make every scenario pop.\nLLamaception 1.5\nIts sibling, LLamaception, is tailored for models running Llama 3.3 70B. You'll get the same secret sauce but fine-tuned for Drummers Anubis, Steelskulls Damascus-R1 (GOAT), and the mind-bending Upgrade to 1.5. With that sweet storytelling, meta examples, and optimized guidelines for your imagination, Llam@ception 1.5 with Nevoria is the awesome before the sauce.\nFinally Introducing Qwenception 1.4\nRounding out the trifecta is Qwenception, crafted for Qwen 2.5/coder models. It pairs perfectly with fine-tunes like Evathene by sophosympatheia and Chuluun by Dat Toad, letting you blend code-savvy insights with cinematic flair. Look forward to vivid emotions, seamless continuity, and that same immersive energy, tuned specifically for Qwen's style.\nWhat's New Across Methception, LLamaception (see Changelog for 1.5), and Qwenception 1.4?\nEnhanced Continuity with Instruction 8\nAll three Ception presets now feature a boosted sense of continuity, thanks to Instruction 8. Whether you're tracking down faint spice aromas in Methception, bridging emotional threads in LLamaception, or recalling code snippets in Qwenception, the AI keeps past details at the forefront to make every scene feel cohesive and genuine.\n\"May I Have Extra Sauce, Please?\" (Last Assistant Prefix)\nSystem Prompt support can be limited, but Last Assistant Prefix helps each model—Mistral, and Qwen—revisit context and instructions before crafting the next response. This keeps everything drenched in that juicy, inception-like sauce, ensuring no detail goes astray no matter how deep your worldbuilding or coding adventure runs.\nImproved Markdown Formatting\nAll three templates benefit from a refreshed Context Template, now featuring clear markdown headings and end markers. This not only improves readability but also helps the AI parse structure more effectively.\nStrengthened \"Show, Don't Tell\" Philosophy\nWe don't just talk about continuity, we demonstrate it. Instruction 8 uses its own guidelines while guiding the AI to maintain logical flow across scenes. This self-referential approach turns your roleplay sessions into a seamless journey, where each narrative beat naturally builds on the last.\nWhy It Matters\nWhen clarity, depth, and immersion are baked into every line, your roleplay becomes an unforgettable ride. These Ception presets—Methception for Mistral, LLamaception for Llama, and Qwenception for Qwen—deliver a potent mix of \"meth\"-like intensity wrapped in an inception-flavored dream. With updated instructions, polished formatting, and supercharged context handling, you'll find yourself riding a creative high that keeps on giving. Strap in, tap into your imagination, and let's cook up something legendary.\nMethception Alternate: Compact Inception for Frontends Like KoboldAI Lite\nFrom the mind behind the 123b phenomenon, Monstral V2's MortalWombat, comes Methception Alternate—a streamlined twist on the original Methception, tailored for single and multi-character cards.\nThis version packs all the depth and immersion of its predecessor into a compact, \"meth\"-infused dose of storytelling brilliance. Designed to drop right into frontends like KoboldAI Lite, Methception Alternate delivers that inception-like experience in a format that's quick, seamless, and still dripping with special sauce.\n\"\nHow to use?\nMethception/LLam@ception is formatted to be imported directly into Sillytavern, using \"Master Import,\" located in Advanced Formatting.\nRecommended Models: The Secret Sauce for Your Ception Presets\nThese fine-tunes are your go-to for maximizing the potential of Methception, LLamaception, and Qwenception. Plug them in and let the inception-like vibes take over.\nMethception (Mistral 123b)\nMarsupialAI's Monstral V2\nThe Drummer's Behemoth 1.2\nLLamaception (Llama 70B 3.3)\nThe Drummer's Anubis\nSteelskull's Damascus (MVP)\nQwenception (Qwen 2.5)\nSophosympatheia's Evathene\nDat Toad's Chuluun\nSpecial Thanks & Credits\nThis project wouldn't be what it is without the incredible contributions of these legends.\nMarinara: For sharing her amazing templates and knowledge, which influenced the creation of these presets. (Click her name for presets)\nSteelSkull: Yo Steel!! Thank you so much for taking the time to create this amazing front page! Its fire just like all your cards and tunes. Much appreciated Fam.\nMortal Wombat: For revising and adapting ideas to ensure users across different front ends can enjoy the sauce.\nGeechan: For providing invaluable feedback, creative ideas, and crafting the brilliant \"pacing\" instruction.\nArkamist: For contributing concepts around agency and bias that shaped key aspects of these presets.\nThana Alt: For dedicating time to share roleplay experiences from his robust, authentic, and sophisticated multi-character card for testing.\nThe Drummer: Because \"Meth.\" Enough said.\nThe BeaverAI Community: For being a constant source of inspiration, feedback, and enthusiasm.\nThank you!",
    "LatitudeGames/Wayfarer-12B-GGUF": "Wayfarer-12B\nWayfarer-12B\nQuantized GGUF weights of the Wayfarer-12B model.\nWhen in doubt which specific file to download, take 80% of VRAM capacity as a guideline, leaving the remaining 20% for context.",
    "lightblue/lb-reranker-0.5B-v1.0": "LB Reranker v1.0\nHow to use\nEvaluation\nLicense\nDeveloped by\nLB Reranker v1.0\nLBR\nThe LB Reranker has been trained to determine the relatedness of a given query to a piece of text, therefore allowing it to be used as a ranker or reranker in various retrieval-based tasks.\nThis model is fine-tuned from a Qwen/Qwen2.5-0.5B-Instruct model checkpoint and was trained for roughly 5.5 hours using the 8 x L20 instance (ecs.gn8is-8x.32xlarge) on Alibaba Cloud.\nThe training data for this model can be found at lightblue/reranker_continuous_filt_max7_train and the code for generating this data as well as running the training of the model can be found on our Github repo.\nTrained on data in over 95 languages, this model is applicable to a broad range of use cases.\nThis model has three main benefits over comparable rerankers.\nIt has shown slightly higher performance on evaluation benchmarks.\nIt has been trained on more languages than any previous model.\nIt is a simple Causal LM model trained to output a string between \"1\" and \"7\".\nThis last point means that this model can be used natively with many widely available inference packages, including vLLM and LMDeploy.\nThis in turns allows our reranker to benefit from improvements to inference as and when these packages release them.\nUpdate: We have also found that this model works pretty well as a code snippet reranker too (P@1 of 96%)! See our Colab for more details.\nHow to use\nThe model was trained to expect an input such as:\n<<<Query>>>\n{your_query_here}\n<<<Context>>>\n{your_context_here}\nAnd to output a string of a number between 1-7.\nIn order to make a continuous score that can be used for reranking query-context pairs (i.e. a method with few ties), we calculate the expectation value of the scores.\nWe include scripts to do this in vLLM, LMDeploy, and OpenAI (hosted for free on Huggingface):\nvLLM\nInstall vLLM using pip install vllm.\nShow vLLM code\nfrom vllm import LLM, SamplingParams\nimport numpy as np\ndef make_reranker_input(t, q):\nreturn f\"<<<Query>>>\\n{q}\\n\\n<<<Context>>>\\n{t}\"\ndef make_reranker_inference_conversation(context, question):\nsystem_message = \"Given a query and a piece of text, output a score of 1-7 based on how related the query is to the text. 1 means least related and 7 is most related.\"\nreturn [\n{\"role\": \"system\", \"content\": system_message},\n{\"role\": \"user\", \"content\": make_reranker_input(context, question)},\n]\ndef get_prob(logprob_dict, tok_id):\nreturn np.exp(logprob_dict[tok_id].logprob) if tok_id in logprob_dict.keys() else 0\nllm = LLM(\"lightblue/lb-reranker-v1.0\")\nsampling_params = SamplingParams(temperature=0.0, logprobs=14, max_tokens=1)\ntok = llm.llm_engine.tokenizer.tokenizer\nidx_tokens = [tok.encode(str(i))[0] for i in range(1, 8)]\nquery_texts = [\n(\"What is the scientific name of apples?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n(\"What is the Chinese word for 'apple'?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n(\"What is the square root of 999?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n]\nchats = [make_reranker_inference_conversation(c, q) for q, c in query_texts]\nresponses = llm.chat(chats, sampling_params)\nprobs = np.array([[get_prob(r.outputs[0].logprobs[0], y) for y in idx_tokens] for r in responses])\nN = probs.shape[1]\nM = probs.shape[0]\nidxs = np.tile(np.arange(1, N + 1), M).reshape(M, N)\nexpected_vals = (probs * idxs).sum(axis=1)\nprint(expected_vals)\n# [6.66570732 1.86686378 1.01102923]\nLMDeploy\nInstall LMDeploy using pip install lmdeploy.\nShow LMDeploy code\n# Un-comment this if running in a Jupyter notebook, Colab etc.\n# import nest_asyncio\n# nest_asyncio.apply()\nfrom lmdeploy import GenerationConfig, ChatTemplateConfig, pipeline\nimport numpy as np\ndef make_reranker_input(t, q):\nreturn f\"<<<Query>>>\\n{q}\\n\\n<<<Context>>>\\n{t}\"\ndef make_reranker_inference_conversation(context, question):\nsystem_message = \"Given a query and a piece of text, output a score of 1-7 based on how related the query is to the text. 1 means least related and 7 is most related.\"\nreturn [\n{\"role\": \"system\", \"content\": system_message},\n{\"role\": \"user\", \"content\": make_reranker_input(context, question)},\n]\ndef get_prob(logprob_dict, tok_id):\nreturn np.exp(logprob_dict[tok_id]) if tok_id in logprob_dict.keys() else 0\npipe = pipeline(\n\"lightblue/lb-reranker-v1.0\",\nchat_template_config=ChatTemplateConfig(\nmodel_name='qwen2d5',\ncapability='chat'\n)\n)\ntok = pipe.tokenizer.model\nidx_tokens = [tok.encode(str(i))[0] for i in range(1, 8)]\nquery_texts = [\n(\"What is the scientific name of apples?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n(\"What is the Chinese word for 'apple'?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n(\"What is the square root of 999?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n]\nchats = [make_reranker_inference_conversation(c, q) for q, c in query_texts]\nresponses = pipe(\nchats,\ngen_config=GenerationConfig(temperature=1.0, logprobs=14, max_new_tokens=1, do_sample=True)\n)\nprobs = np.array([[get_prob(r.logprobs[0], y) for y in idx_tokens] for r in responses])\nN = probs.shape[1]\nM = probs.shape[0]\nidxs = np.tile(np.arange(1, N + 1), M).reshape(M, N)\nexpected_vals = (probs * idxs).sum(axis=1)\nprint(expected_vals)\n# [6.66415229 1.84342025 1.01133205]\nOpenAI (Hosted on Huggingface)\nInstall openai using pip install openai.\nShow OpenAI + Huggingface Inference code\nfrom openai import OpenAI\nimport numpy as np\nfrom multiprocessing import Pool\nfrom tqdm.auto import tqdm\nclient = OpenAI(\nbase_url=\"https://api-inference.huggingface.co/v1/\",\napi_key=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Change this to an access token from https://huggingface.co/settings/tokens\n)\ndef make_reranker_input(t, q):\nreturn f\"<<<Query>>>\\n{q}\\n\\n<<<Context>>>\\n{t}\"\ndef make_reranker_inference_conversation(context, question):\nsystem_message = \"Given a query and a piece of text, output a score of 1-7 based on how related the query is to the text. 1 means least related and 7 is most related.\"\nreturn [\n{\"role\": \"system\", \"content\": system_message},\n{\"role\": \"user\", \"content\": make_reranker_input(context, question)},\n]\ndef get_reranker_score(context_question_tuple):\nquestion, context = context_question_tuple\nmessages = make_reranker_inference_conversation(context, question)\ncompletion = client.chat.completions.create(\nmodel=\"lightblue/lb-reranker-0.5B-v1.0\",\nmessages=messages,\nmax_tokens=1,\ntemperature=0.0,\nlogprobs=True,\ntop_logprobs=5, # Max allowed by the openai API as top_n_tokens must be >= 0 and <= 5. If this gets changed, fix to > 7.\n)\nlogprobs = completion.choices[0].logprobs.content[0].top_logprobs\ncalculated_score = sum([int(x.token) * np.exp(x.logprob) for x in logprobs])\nreturn calculated_score\nquery_texts = [\n(\"What is the scientific name of apples?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n(\"What is the Chinese word for 'apple'?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n(\"What is the square root of 999?\", \"An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\"),\n]\nwith Pool(processes=16) as p: # Allows for parallel processing\nexpected_vals = list(tqdm(p.imap(get_reranker_score, query_texts), total=len(query_texts)))\nprint(expected_vals)\n# [6.64866580, 1.85144404, 1.010719508]\nEvaluation\nWe perform an evaluation on 9 datasets from the BEIR benchmark that none of the evaluated models have been trained upon (to our knowledge).\nArguana\nDbpedia-entity\nFiqa\nNFcorpus\nScidocs\nScifact\nTrec-covid-v2\nVihealthqa\nWebis-touche2020\nWe evaluate on a subset of all queries (the first 250) to save evaluation time.\nWe find that our model performs similarly or better than many of the state-of-the-art reranker models in our evaluation, without compromising on inference speed.\nWe make our evaluation code and results available on our Github.\nAs we can see, this reranker attains greater IR evaluation metrics compared to the two benchmarks we include for all positions apart from @1.\nWe also show that our model is, on average, faster than the BGE reranker v2.\nLicense\nWe share this model under an Apache 2.0 license.\nDeveloped by\nThis model was trained by Peter Devine (ptrdvn) for Lightblue",
    "Shakker-Labs/majicflus_v1": "MajicFlus\nModel Features\nCommunity Adaptation\nInference Setting\nOnline Inference\nAcknowledgements\nMajicFlus\nMajicFlus is a model fine-tuned and merged based on flux.dev, specializing in high-quality portrait generation, with a particular focus on capturing the delicacy and beauty of Asian women. The model emphasizes aesthetics, realism, and user-friendliness, allowing users to generate high-quality results with simple prompts while also handling complex prompts effectively.\nModel Features\nOutstanding Portrait Generation:\nOptimized for various lighting conditions, ensuring facial details and limb integrity in different compositions.\nVersatile Applications:\nIn addition to portraits, the model demonstrates significant improvements in generating non-human subjects and scenes, making it suitable for diverse creative needs.\nEase of Use:\nUsers can achieve impressive results with simple prompts, while longer prompts are also well-supported for fine-grained control.\nCommunity Adaptation\nMajicFlus is launching alongside a collection of LoRAs created by multiple community members. These LoRAs further enhance the model’s functionality and performance, offering users more creative possibilities and enabling the model to adapt to specific styles and scenarios.\nInference Setting\nSteps: 20~30\nDistilled CFG Scale: 3.5\nCFG : 1\nDiffusion in Low Bits: float8-e4m3fn\nSampling: Euler + simple/beta (for general)，DPM2M + SGM uniform (for skin texture)，DEIS + DDIM uniform (for casual realistic look)\nVae: flux vae\nClip: clip_l.safetensors and t5xxl_fp8_e4m3fn.safetensors\nOnline Inference\nYou can also download and try this model at Shakker AI.\nAcknowledgements\nThis model is trained by our copyrighted users Merjic. We release this model under permissions. The model follows flux-1-dev-non-commercial-license.",
    "nvidia/Cosmos-1.0-Autoregressive-4B": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the NVIDIA Privacy Policy.\nNVIDIA Open Model License Agreement\nVersion Release Date: January 6, 2025This NVIDIA Open Model License Agreement (the \"Agreement\") is a legal agreement between the Legal Entity You represent, or if no entity is identified, You and NVIDIA Corporation and its Affiliates (\"NVIDIA\") and governs Your use of the Models that NVIDIA provides to You under this Agreement. NVIDIA and You are each a \"party\" and collectively the \"parties.\"NVIDIA models released under this Agreement are intended to be used permissively and enable the further development of AI technologies. Subject to the terms of this Agreement, NVIDIA confirms that:\nModels are commercially usable.\nYou are free to create and distribute Derivative Models.\nNVIDIA does not claim ownership to any outputs generated using the Models or Model Derivatives.By using, reproducing, modifying, distributing, performing or displaying any portion or element of the Model or Derivative Model, or otherwise accepting the terms of this Agreement, you agree to be bound by this Agreement.\n1. Definitions\nThe following definitions apply to this Agreement:\n1.1. \"NVIDIA Cosmos Model\" means a multimodal Model shared under this Agreement.\n1.2. \"Derivative Model\" means all (a) modifications to the Model, (b) works based on the Model, and (c) any other derivative works of the Model. An output is not a Derivative Model.\n1.3. \"Legal Entity\" means the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of fifty percent (50%) or more of the outstanding shares, or (c) beneficial ownership of such entity.\n1.4. \"Model\" means the machine learning model, software, checkpoints, learnt weights, algorithms, parameters, configuration files and documentation shared under this Agreement.\n1.5. \"You\" or \"Your\" means an individual or Legal Entity exercising permissions granted by this Agreement.\n2. Conditions for Use, License Grant, AI Ethics and IP Ownership\n2.1. Conditions for Use. The Model and any Derivative Model are subject to additional terms as described in Section 2 and Section 3 of this Agreement and govern Your use. If You institute copyright or patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Model or a Derivative Model constitutes direct or contributory copyright or patent infringement, then any licenses granted to You under this Agreement for that Model or Derivative Model will terminate as of the date such litigation is filed. If You bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under this Agreement will automatically terminate. NVIDIA may update this Agreement to comply with legal and regulatory requirements at any time and You agree to either comply with any updated license or cease Your copying, use, and distribution of the Model and any Derivative Model.\n2.2. License Grant. The rights granted herein are explicitly conditioned on Your full compliance with the terms of this Agreement. Subject to the terms and conditions of this Agreement, NVIDIA hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, revocable (as stated in Section 2.1) license to publicly perform, publicly display, reproduce, use, create derivative works of, make, have made, sell, offer for sale, distribute (through multiple tiers of distribution) and import the Model.\n2.3. AI Ethics. Use of the Models under the Agreement must be consistent with NVIDIA's Trustworthy AI terms found at https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/.\n2.4. NVIDIA owns the Model and any Model Derivatives created by NVIDIA. Subject to NVIDIA's underlying ownership rights in the Model or its Model Derivatives, You are and will be the owner of Your Model Derivatives. NVIDIA claims no ownership rights in outputs. You are responsible for outputs and their subsequent uses. Except as expressly granted in this Agreement, (a) NVIDIA reserves all rights, interests and remedies in connection with the Model and (b) no other license or right is granted to you by implication, estoppel or otherwise.\n3. Redistribution\nYou may reproduce and distribute copies of the Model or Derivative Models thereof in any medium, with or without modifications, provided that You meet the following conditions:\n3.1. If you distribute the Model, You must give any other recipients of the Model a copy of this Agreement and include the following attribution notice within a \"Notice\" text file with such copies: \"Licensed by NVIDIA Corporation under the NVIDIA Open Model License\";\n3.2. If you distribute or make available a NVIDIA Cosmos Model, or a product or service (including an AI model) that contains or uses a NVIDIA Cosmos Model, use a NVIDIA Cosmos Model to create a Derivative Model, or use a NVIDIA Cosmos Model or its outputs to create, train, fine tune, or otherwise improve an AI model, you will include \"Built on NVIDIA Cosmos\" on a related website, user interface, blogpost, about page, or product documentation; and\n3.3. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Models as a whole, provided Your use, reproduction, and distribution of the Model otherwise complies with the conditions stated in this Agreement.\n4. Trademarks\nThis Agreement does not grant permission to use the trade names, trademarks, service marks, or product names of NVIDIA, except as required for reasonable and customary use in describing the origin of the Model and reproducing the content of the \"Notice\" text file.\n5. Disclaimer of Warranty\nUnless required by applicable law or agreed to in writing, NVIDIA provides the Model on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Model, Derivative Models and outputs and assume any risks associated with Your exercise of permissions under this Agreement.\n6. Limitation of Liability\nIn no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, will NVIDIA be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this Agreement or out of the use or inability to use the Model, Derivative Models or outputs (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if NVIDIA has been advised of the possibility of such damages.\n7. Indemnity\nYou will indemnify and hold harmless NVIDIA from and against any claim by any third party arising out of or related to your use or distribution of the Model, Model Derivatives or outputs.\n8. Feedback\nNVIDIA appreciates your feedback, and You agree that NVIDIA may use it without restriction or compensation to You.\n9. Governing Law\nThis Agreement will be governed in all respects by the laws of the United States and the laws of the State of Delaware, without regard to conflict of laws principles or the United Nations Convention on Contracts for the International Sale of Goods. The state and federal courts residing in Santa Clara County, California will have exclusive jurisdiction over any dispute or claim arising out of or related to this Agreement, and the parties irrevocably consent to personal jurisdiction and venue in those courts; except that, either party may apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.\n10. Trade and Compliance\nYou agree to comply with all applicable export, import, trade and economic sanctions laws and regulations, as amended, including without limitation U.S. Export Administration Regulations and Office of Foreign Assets Control regulations. These laws include restrictions on destinations, end-users and end-use.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCosmos-1.0-Autoregressive: A Suite of Autoregressive-based World Foundation Models\nModel Overview\nDescription:\nModel Versions\nLicense:\nModel Architecture:\nInput/Output Specifications\nSoftware Integration:\nUsage\nEvaluation\nInference Time and GPU Memory Usage\nFailure Analysis\nEthical Considerations\nPlus Plus (++) Promise\nBias\nExplainability\nPrivacy\nSafety\nCosmos-1.0-Autoregressive: A Suite of Autoregressive-based World Foundation Models\nCosmos | Code | Paper | Paper Website\nModel Overview\nDescription:\nCosmos World Foundation Models: A family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical AI development.\nThe Cosmos autoregressive models are a collection of pre-trained world foundation models that are ideal for predicting and rapidly generating video sequences from video or image inputs for physical AI. They can serve as the building block for various applications or research that are related to world generation. The models are ready for commercial use under NVIDIA Open Model license agreement.\nModel Developer: NVIDIA\nModel Versions\nIn Cosmos 1.0 release, the Cosmos Autoregressive WFM family includes the following models:\nCosmos-1.0-Autoregressive-4B\nGiven a 9-frame input video, predicts the future 24 frames.\nGiven an image as the first frame, predicts the future 32 frames.\nCosmos-1.0-Autoregressive-5B-Video2World\nGiven text description and a 9-frame input video, predicts the future 24 frames.\nGiven text description and an image as the first frame, predicts the future 32 frames.\nCosmos-1.0-Autoregressive-12B)\nGiven a 9-frame input video, predicts the future 24 frames.\nGiven an image as the first frame, predicts the future 32 frames.\nCosmos-1.0-Autoregressive-13B-Video2World\nGiven text description and a 9-frame input video, predicts the future 24 frames.\nGiven text description and an image as the first frame, predicts the future 32 frames.\nLicense:\nThis model is released under the  NVIDIA Open Model License. For a custom license, please contact cosmos-license@nvidia.com.\nUnder the NVIDIA Open Model License, NVIDIA confirms:\nModels are commercially usable.\nYou are free to create and distribute Derivative Models.\nNVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.\nImportant Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or\nassociated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained\nin the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.\nCosmos-1.0-Guardrail is the safety guardrail for this model.\nModel Architecture:\nCosmos-1.0-Autoregressive-4B is an autoregressive transformer model designed for world generation. The network is composed of interleaved self-attention and feedforward layers as its building blocks.\nInput/Output Specifications\nInput\nInput Type(s): Video\nInput Format(s): mp4\nInput Parameters: Three-dimensional (3D)\nOther Properties Related to Input: The input video should be of 1024x640 resolution with at least 9 frames.\nOutput\nOutput Type(s): Video\nOutput Format(s): mp4\nOutput Parameters: Three-dimensional (3D)\nOther Properties Related to Output: The generated video will be a 24-frame clip with a resolution of 1024x640 pixels, conditioned on the first 9 frames of the input video.\nSoftware Integration:\nRuntime Engine(s):\nCosmos\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Blackwell\nNVIDIA Hopper\nNVIDIA Ampere\nNote: We have only tested doing inference with BF16 precision.\nOperating System(s):\nLinux (We have not tested on other operating systems.)\nUsage\nSee Cosmos for details.\nEvaluation\nPlease see our technical paper for detailed evaluations.\nInference Time and GPU Memory Usage\nThese numbers may vary based on system specifications and are provided for reference only.\nOffloading Strategy\nCosmos-1.0-Autoregressive-4B\nCosmos-1.0-Autoregressive-12B\nNo offloading\n31.3 GB\n47.5 GB\nGuardrails\n28.9 GB\n45.2 GB\nGuardrails & Diffusion decoder\n28.5 GB\n43.1 GB\nGuardrails & Diffusion decoder & Tokenizer\n27.3 GB\n42.9 GB\nGuardrails & Diffusion decoder & Tokenizer & AR model\n18.7 GB\n27.4 GB\nEnd-to-end inference runtime on one H100 without offloading and after model initialization:\nCosmos-1.0-Autoregressive-4B\nCosmos-1.0-Autoregressive-12B\n~62 seconds\n~119 seconds\nFailure Analysis\nOur models now support video extension up to 33 frames. Starting from either a single image or a 9-frame video input, it can generate the remaining frames to reach the 33-frame length (generating 32 or 24 frames respectively).\nWe have evaluated all eight possible configurations (4 models × 2 vision input types: image or video) using 100 test videos from physical AI domains. Below are the failure rates for each configuration:\nModel\nImage input\nVideo input (9 frames)\nCosmos-1.0-Autoregressive-4B\n15%\n1%\nCosmos-1.0-Autoregressive-5B-Video2World\n7%\n2%\nCosmos-1.0-Autoregressive-12B\n2%\n1%\nCosmos-1.0-Autoregressive-13B-Video2World\n3%\n0%\nWe define failure cases as videos with severe distortions, such as:\nSudden appearance of large unexpected objects\nVideo degrading to a single solid color\nNote that the following are not considered failures in our analysis:\nStatic video frames\nMinor object distortions or artifacts\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the subcards of Explainability, Bias, Safety & Security, and Privacy below. Please report security vulnerabilities or NVIDIA AI Concerns here.\nPlus Plus (++) Promise\nWe value you, the datasets, the diversity they represent, and what we have been entrusted with. This model and its associated data have been:\nVerified to comply with current applicable disclosure laws, regulations, and industry standards.\nVerified to comply with applicable privacy labeling requirements.\nAnnotated to describe the collector/source (NVIDIA or a third-party).\nCharacterized for technical limitations.\nReviewed to ensure proper disclosure is accessible to, maintained for, and in compliance with NVIDIA data subjects and their requests.\nReviewed before release.\nTagged for known restrictions and potential safety implications.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing:\nNone\nMeasures taken to mitigate against unwanted bias:\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nWorld Generation\nModel Type:\nTransformer\nIntended Users:\nPhysical AI developers\nOutput:\nVideos\nDescribe how the model works:\nGenerates videos based on video inputs\nTechnical Limitations:\nThe model may not follow the video input accurately.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nQuantitative and Qualitative Evaluation\nPotential Known Risks:\nThe model's output can generate all forms of videos, including what may be considered toxic, offensive, or indecent.\nLicensing:\nNVIDIA Open Model License\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal information?\nNone Known\nProtected class data used to create this model?\nNone Known\nWas consent obtained for any personal data used?\nNone Known\nHow often is dataset reviewed?\nBefore Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?\nNot Applicable\nIf personal data was collected for the development of the model, was it collected directly by NVIDIA?\nNot Applicable\nIf personal data was collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?\nNot Applicable\nIf personal data was collected for the development of this AI model, was it minimized to only what was required?\nNot Applicable\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNot Applicable\nSafety\nField\nResponse\nModel Application(s):\nWorld Generation\nDescribe the life critical impact (if present).\nNone Known\nUse Case Restrictions:\nNVIDIA Open Model License\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development.  Restrictions enforce dataset access during training, and dataset license constraints adhered to. Model checkpoints are made available on Hugging Face, and may become available on cloud providers' model catalog.",
    "ewre324/ewre324-Thinker-Llama-3.2-3B-Instruct-Reasoning": "NOTE\nInfo\nunsloth/Llama-3.2-3B-Instruct\nSpecial Thanks\nModel Information\nbase_model: meta-llama/Llama-3.2-3B-Instruct\nlanguage:\n- en\nlibrary_name: transformers\nlicense: llama3.2\ntags:\n- llama-3\n- llama\n- meta\n- facebook\n- unsloth\n- transformers\nNOTE\nEven though the base model was taken from Unsloth's Llama repo, the finetuning was NOT done via Unsloth.\nInfo\nThis model is aimed at Chain of Thought and has been trained on human generated, AI Reasoned questions and answers https://huggingface.co/datasets/KingNish/reasoning-base-20k .\nunsloth/Llama-3.2-3B-Instruct\nFor more details on the model, please go to Meta's original model card\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\nModel Information\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nSupported languages:  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 family of models Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.",
    "HKUSTAudio/Llasa-1B": "Model Information\nHow to use\nDisclaimer\nUpdate （2025-05-10): Sometimes I find that top_p=0.95 and temperature=0.9 produce more stable results.\nUpdate (2025-02-13): Add Llasa finetune instruction.\nUpdate (2025-02-07): Our paper has been released!\nLLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis\nTrain from Scratch: If you want to train the model from scratch, use the LLaSA Training Repository.\nScale for Test-Time Computation: If you want to experiment with scaling for test-time computation, use the LLaSA Testing Repository.\nModel Information\nOur model, Llasa, is a text-to-speech (TTS) system that extends the text-based LLaMA (1B,3B, and 8B) language model by incorporating speech tokens from the XCodec2 codebook,\nwhich contains 65,536 tokens. We trained Llasa on a dataset comprising 250,000 hours of Chinese-English speech data.\nThe model is capable of generating speech either solely from input text or by utilizing a given speech prompt.\nHow to use\nInstall XCodec2.\n1. Speech synthesis solely from input text\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\nllasa_1b ='HKUSTAudio/Llasa-1B'\ntokenizer = AutoTokenizer.from_pretrained(llasa_1b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_1b)\nmodel.eval()\nmodel.to('cuda')\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\nmodel_path = \"HKUSTAudio/xcodec2\"\nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()\ninput_text = 'Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.'\n# input_text = '突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道：\"我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？\"'\ndef ids_to_speech_tokens(speech_ids):\nspeech_tokens_str = []\nfor speech_id in speech_ids:\nspeech_tokens_str.append(f\"<|s_{speech_id}|>\")\nreturn speech_tokens_str\ndef extract_speech_ids(speech_tokens_str):\nspeech_ids = []\nfor token_str in speech_tokens_str:\nif token_str.startswith('<|s_') and token_str.endswith('|>'):\nnum_str = token_str[4:-2]\nnum = int(num_str)\nspeech_ids.append(num)\nelse:\nprint(f\"Unexpected token: {token_str}\")\nreturn speech_ids\n#TTS start!\nwith torch.no_grad():\nformatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n# Tokenize the text\nchat = [\n{\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n{\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n]\ninput_ids = tokenizer.apply_chat_template(\nchat,\ntokenize=True,\nreturn_tensors='pt',\ncontinue_final_message=True\n)\ninput_ids = input_ids.to('cuda')\nspeech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n# Generate the speech autoregressively\noutputs = model.generate(\ninput_ids,\nmax_length=2048,  # We trained our model with a max length of 2048\neos_token_id= speech_end_id ,\ndo_sample=True,\ntop_p=1,           #  Adjusts the diversity of generated content\ntemperature=0.8,   #  Controls randomness in output\n)\n# Extract the speech tokens\ngenerated_ids = outputs[0][input_ids.shape[1]:-1]\nspeech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n# Convert  token <|s_23456|> to int 23456\nspeech_tokens = extract_speech_ids(speech_tokens)\nspeech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n# Decode the speech tokens to speech waveform\ngen_wav = Codec_model.decode_code(speech_tokens)\nsf.write(\"gen.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\n2. Speech synthesis utilizing a given speech prompt\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\nllasa_1b ='HKUSTAudio/Llasa-1b'\ntokenizer = AutoTokenizer.from_pretrained(llasa_1b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_1b)\nmodel.eval()\nmodel.to('cuda')\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\nmodel_path = \"HKUSTAudio/xcodec2\"\nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()\n# only 16khz speech support!\nprompt_wav, sr = sf.read(\"太乙真人.wav\")   # you can find wav in Files\n#prompt_wav, sr = sf.read(\"Anna.wav\") # English prompt\nprompt_wav = torch.from_numpy(prompt_wav).float().unsqueeze(0)\nprompt_text =\"对，这就是我万人敬仰的太乙真人，虽然有点婴儿肥，但也掩不住我逼人的帅气。\"\n#promt_text = \"A chance to leave him alone, but... No. She just wanted to see him again. Anna, you don't know how it feels to lose a sister. Anna, I'm sorry, but your father asked me not to tell you anything.\"\ntarget_text = '突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道：\"我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？\"'\n#target_text = \"Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.\"\ninput_text = prompt_text + ' ' + target_text\ndef ids_to_speech_tokens(speech_ids):\nspeech_tokens_str = []\nfor speech_id in speech_ids:\nspeech_tokens_str.append(f\"<|s_{speech_id}|>\")\nreturn speech_tokens_str\ndef extract_speech_ids(speech_tokens_str):\nspeech_ids = []\nfor token_str in speech_tokens_str:\nif token_str.startswith('<|s_') and token_str.endswith('|>'):\nnum_str = token_str[4:-2]\nnum = int(num_str)\nspeech_ids.append(num)\nelse:\nprint(f\"Unexpected token: {token_str}\")\nreturn speech_ids\n#TTS start!\nwith torch.no_grad():\n# Encode the prompt wav\nvq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)\nprint(\"Prompt Vq Code Shape:\", vq_code_prompt.shape )\nvq_code_prompt = vq_code_prompt[0,0,:]\n# Convert int 12345 to token <|s_12345|>\nspeech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)\nformatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n# Tokenize the text and the speech prefix\nchat = [\n{\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n{\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\" + ''.join(speech_ids_prefix)}\n]\ninput_ids = tokenizer.apply_chat_template(\nchat,\ntokenize=True,\nreturn_tensors='pt',\ncontinue_final_message=True\n)\ninput_ids = input_ids.to('cuda')\nspeech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n# Generate the speech autoregressively\noutputs = model.generate(\ninput_ids,\nmax_length=2048,  # We trained our model with a max length of 2048\neos_token_id= speech_end_id ,\ndo_sample=True,\ntop_p=1,\ntemperature=0.8,\n)\n# Extract the speech tokens\ngenerated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]\nspeech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n# Convert  token <|s_23456|> to int 23456\nspeech_tokens = extract_speech_ids(speech_tokens)\nspeech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n# Decode the speech tokens to speech waveform\ngen_wav = Codec_model.decode_code(speech_tokens)\n# if only need the generated part\n# gen_wav = gen_wav[:,:,prompt_wav.shape[1]:]\nsf.write(\"gen.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\nDisclaimer\nThis model is licensed under the CC BY-NC  4.0 License, which prohibits free commercial use because of ethics and privacy concerns; detected violations will result in legal consequences.\nThis codebase is strictly prohibited from being used for any illegal purposes in any country or region. Please refer to your local laws about DMCA and other related laws.",
    "HKUSTAudio/Llasa-3B": "Model Information\nHow to use\nDisclaimer\nUpdate （2025-05-10): Sometimes I find that top_p=0.95 and temperature=0.9 produce more stable results.\nUpdate (2025-02-13): Add Llasa finetune instruction.\nUpdate (2025-02-07): Our paper has been released!\nLLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis\nTrain from Scratch: If you want to train the model from scratch, use the LLaSA Training Repository.\nScale for Test-Time Computation: If you want to experiment with scaling for test-time computation, use the LLaSA Testing Repository.\nModel Information\nOur model, Llasa, is a text-to-speech (TTS) system that extends the text-based LLaMA (1B,3B, and 8B) language model by incorporating speech tokens from the XCodec2 codebook,\nwhich contains 65,536 tokens. We trained Llasa on a dataset comprising 250,000 hours of Chinese-English speech data.\nThe model is capable of generating speech either solely from input text or by utilizing a given speech prompt.\nThe method is seamlessly compatible with the Llama framework, making training TTS similar as training LLM (convert audios into single-codebook tokens and simply view it as a special language). It opens the possiblity of existing method for compression, acceleration and finetuning for LLM to be applied.\nHow to use\nInstall XCodec2.\n1. Speech synthesis solely from input text\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\nllasa_3b ='HKUSTAudio/Llasa-3B'\ntokenizer = AutoTokenizer.from_pretrained(llasa_3b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_3b)\nmodel.eval()\nmodel.to('cuda')\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\nmodel_path = \"HKUSTAudio/xcodec2\"\nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()\ninput_text = 'Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.'\n# input_text = '突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道：\"我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？\"'\ndef ids_to_speech_tokens(speech_ids):\nspeech_tokens_str = []\nfor speech_id in speech_ids:\nspeech_tokens_str.append(f\"<|s_{speech_id}|>\")\nreturn speech_tokens_str\ndef extract_speech_ids(speech_tokens_str):\nspeech_ids = []\nfor token_str in speech_tokens_str:\nif token_str.startswith('<|s_') and token_str.endswith('|>'):\nnum_str = token_str[4:-2]\nnum = int(num_str)\nspeech_ids.append(num)\nelse:\nprint(f\"Unexpected token: {token_str}\")\nreturn speech_ids\n#TTS start!\nwith torch.no_grad():\nformatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n# Tokenize the text\nchat = [\n{\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n{\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n]\ninput_ids = tokenizer.apply_chat_template(\nchat,\ntokenize=True,\nreturn_tensors='pt',\ncontinue_final_message=True\n)\ninput_ids = input_ids.to('cuda')\nspeech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n# Generate the speech autoregressively\noutputs = model.generate(\ninput_ids,\nmax_length=2048,  # We trained our model with a max length of 2048\neos_token_id= speech_end_id ,\ndo_sample=True,\ntop_p=1,           #  Adjusts the diversity of generated content\ntemperature=0.8,   #  Controls randomness in output\n)\n# Extract the speech tokens\ngenerated_ids = outputs[0][input_ids.shape[1]:-1]\nspeech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n# Convert  token <|s_23456|> to int 23456\nspeech_tokens = extract_speech_ids(speech_tokens)\nspeech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n# Decode the speech tokens to speech waveform\ngen_wav = Codec_model.decode_code(speech_tokens)\nsf.write(\"gen.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\n2. Speech synthesis utilizing a given speech prompt\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\nllasa_3b ='HKUSTAudio/Llasa-3B'\ntokenizer = AutoTokenizer.from_pretrained(llasa_3b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_3b)\nmodel.eval()\nmodel.to('cuda')\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\nmodel_path = \"HKUSTAudio/xcodec2\"\nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()\n# only 16khz speech support!\nprompt_wav, sr = sf.read(\"太乙真人.wav\")   # you can find wav in Files\n#prompt_wav, sr = sf.read(\"Anna.wav\") # English prompt\nprompt_wav = torch.from_numpy(prompt_wav).float().unsqueeze(0)\nprompt_text =\"对，这就是我万人敬仰的太乙真人，虽然有点婴儿肥，但也掩不住我逼人的帅气。\"\n#promt_text = \"A chance to leave him alone, but... No. She just wanted to see him again. Anna, you don't know how it feels to lose a sister. Anna, I'm sorry, but your father asked me not to tell you anything.\"\ntarget_text = '突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道：\"我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？\"'\n#target_text = \"Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.\"\ninput_text = prompt_text   + target_text\ndef ids_to_speech_tokens(speech_ids):\nspeech_tokens_str = []\nfor speech_id in speech_ids:\nspeech_tokens_str.append(f\"<|s_{speech_id}|>\")\nreturn speech_tokens_str\ndef extract_speech_ids(speech_tokens_str):\nspeech_ids = []\nfor token_str in speech_tokens_str:\nif token_str.startswith('<|s_') and token_str.endswith('|>'):\nnum_str = token_str[4:-2]\nnum = int(num_str)\nspeech_ids.append(num)\nelse:\nprint(f\"Unexpected token: {token_str}\")\nreturn speech_ids\n#TTS start!\nwith torch.no_grad():\n# Encode the prompt wav\nvq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)\nprint(\"Prompt Vq Code Shape:\", vq_code_prompt.shape )\nvq_code_prompt = vq_code_prompt[0,0,:]\n# Convert int 12345 to token <|s_12345|>\nspeech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)\nformatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n# Tokenize the text and the speech prefix\nchat = [\n{\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n{\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\" + ''.join(speech_ids_prefix)}\n]\ninput_ids = tokenizer.apply_chat_template(\nchat,\ntokenize=True,\nreturn_tensors='pt',\ncontinue_final_message=True\n)\ninput_ids = input_ids.to('cuda')\nspeech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n# Generate the speech autoregressively\noutputs = model.generate(\ninput_ids,\nmax_length=2048,  # We trained our model with a max length of 2048\neos_token_id= speech_end_id ,\ndo_sample=True,\ntop_p=1,\ntemperature=0.8,\n)\n# Extract the speech tokens\ngenerated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]\nspeech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n# Convert  token <|s_23456|> to int 23456\nspeech_tokens = extract_speech_ids(speech_tokens)\nspeech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n# Decode the speech tokens to speech waveform\ngen_wav = Codec_model.decode_code(speech_tokens)\n# if only need the generated part\n# gen_wav = gen_wav[:,:,prompt_wav.shape[1]:]\nsf.write(\"gen.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\nDisclaimer\nThis model is licensed under the CC BY-NC 4.0 License, which prohibits free commercial use because of ethics and privacy concerns; detected violations will result in legal consequences.\nThis codebase is strictly prohibited from being used for any illegal purposes in any country or region. Please refer to your local laws about DMCA and other related laws.",
    "HKUSTAudio/Llasa-8B": "New Features\nPaper\nModel Information\nHow to use\nDisclaimer\nUpdate (2025-02-13): Add Llasa finetune instruction.\nUpdate (2025-02-07): Our paper has been released!\nNew Features\nWe have observed that Llasa 8B exhibits excellent capability in text comprehension. You can try complex sentences like:\nEnglish:\"He shouted, 'Everyone, please gather 'round! Here's the plan: 1) Set-up at 9:15 a.m.; 2) Lunch at 12:00 p.m. (please RSVP!); 3) Playing—e.g., games, music, etc.—from 1:15 to 4:45; and 4) Clean-up at 5 p.m.'\"\nChinese:\"昨夜雨疏风骤，浓睡不消残酒。试问卷帘人，却道海棠依旧。知否，知否？应是绿肥红瘦。\"\n\"帘外雨潺潺，春意阑珊。罗衾不耐五更寒。梦里不知身是客，一晌贪欢。独自莫凭栏，无限江山。别时容易见时难。流水落花春去也，天上人间。\"\nPaper\nLLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis (Comming soon)\nTrain from Scratch: If you want to train the model from scratch, use the LLaSA Training Repository.\nScale for Test-Time Computation: If you want to experiment with scaling for test-time computation, use the LLaSA Testing Repository.\nModel Information\nOur model, Llasa, is a text-to-speech (TTS) system that extends the text-based LLaMA (1B,3B, and 8B) language model by incorporating speech tokens from the XCodec2 codebook,\nwhich contains 65,536 tokens. We trained Llasa on a dataset comprising 250,000 hours of Chinese-English speech data.\nThe model is capable of generating speech either solely from input text or by utilizing a given speech prompt.\nThe method is seamlessly compatible with the Llama framework, making training TTS similar as training LLM (convert audios into single-codebook tokens and simply view it as a special language). It opens the possiblity of existing method for compression, acceleration and finetuning for LLM to be applied.\nHow to use\nInstall XCodec2.\n1. Speech synthesis solely from input text\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\nllasa_8b ='HKUSTAudio/Llasa-8B'\ntokenizer = AutoTokenizer.from_pretrained(llasa_8b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_8b)\nmodel.eval()\nmodel.to('cuda')\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\nmodel_path = \"HKUSTAudio/xcodec2\"\nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()\n# Llasa-8B shows better text understanding ability.\n# input_text = \" He shouted, 'Everyone, please gather 'round! Here's the plan: 1) Set-up at 9:15 a.m.; 2) Lunch at 12:00 p.m. (please RSVP!); 3) Playing — e.g., games, music, etc. — from 1:15 to 4:45; and 4) Clean-up at 5 p.m.'\"\ninput_text = '昨夜雨疏风骤，浓睡不消残酒。试问卷帘人，却道海棠依旧。知否，知否？应是绿肥红瘦。'\n# 帘外雨潺潺，春意阑珊。罗衾不耐五更寒。梦里不知身是客，一晌贪欢。独自莫凭栏，无限江山。别时容易见时难。流水落花春去也，天上人间。\ndef ids_to_speech_tokens(speech_ids):\nspeech_tokens_str = []\nfor speech_id in speech_ids:\nspeech_tokens_str.append(f\"<|s_{speech_id}|>\")\nreturn speech_tokens_str\ndef extract_speech_ids(speech_tokens_str):\nspeech_ids = []\nfor token_str in speech_tokens_str:\nif token_str.startswith('<|s_') and token_str.endswith('|>'):\nnum_str = token_str[4:-2]\nnum = int(num_str)\nspeech_ids.append(num)\nelse:\nprint(f\"Unexpected token: {token_str}\")\nreturn speech_ids\n#TTS start!\nwith torch.no_grad():\nformatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n# Tokenize the text\nchat = [\n{\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n{\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n]\ninput_ids = tokenizer.apply_chat_template(\nchat,\ntokenize=True,\nreturn_tensors='pt',\ncontinue_final_message=True\n)\ninput_ids = input_ids.to('cuda')\nspeech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n# Generate the speech autoregressively\noutputs = model.generate(\ninput_ids,\nmax_length=2048,  # We trained our model with a max length of 2048\neos_token_id= speech_end_id ,\ndo_sample=True,\ntop_p=1,           #  Adjusts the diversity of generated content\ntemperature=1,   #  Controls randomness in output\n)\n# Extract the speech tokens\ngenerated_ids = outputs[0][input_ids.shape[1]:-1]\nspeech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n# Convert  token <|s_23456|> to int 23456\nspeech_tokens = extract_speech_ids(speech_tokens)\nspeech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n# Decode the speech tokens to speech waveform\ngen_wav = Codec_model.decode_code(speech_tokens)\nsf.write(\"gen.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\n2. Speech synthesis utilizing a given speech prompt\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\nllasa_8b ='HKUSTAudio/Llasa-8B'\ntokenizer = AutoTokenizer.from_pretrained(llasa_8b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_8b)\nmodel.eval()\nmodel.to('cuda')\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\nmodel_path = \"HKUSTAudio/xcodec2\"\nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()\n# only 16khz speech support!\nprompt_wav, sr = sf.read(\"太乙真人.wav\")   # you can find wav in Files\n#prompt_wav, sr = sf.read(\"Anna.wav\") # English prompt\nprompt_wav = torch.from_numpy(prompt_wav).float().unsqueeze(0)\nprompt_text =\"对，这就是我万人敬仰的太乙真人，虽然有点婴儿肥，但也掩不住我逼人的帅气。\"\n#promt_text = \"A chance to leave him alone, but... No. She just wanted to see him again. Anna, you don't know how it feels to lose a sister. Anna, I'm sorry, but your father asked me not to tell you anything.\"\ntarget_text = '突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道：\"我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？\"'\n#target_text = \"Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.\"\ninput_text = prompt_text   + target_text\ndef ids_to_speech_tokens(speech_ids):\nspeech_tokens_str = []\nfor speech_id in speech_ids:\nspeech_tokens_str.append(f\"<|s_{speech_id}|>\")\nreturn speech_tokens_str\ndef extract_speech_ids(speech_tokens_str):\nspeech_ids = []\nfor token_str in speech_tokens_str:\nif token_str.startswith('<|s_') and token_str.endswith('|>'):\nnum_str = token_str[4:-2]\nnum = int(num_str)\nspeech_ids.append(num)\nelse:\nprint(f\"Unexpected token: {token_str}\")\nreturn speech_ids\n#TTS start!\nwith torch.no_grad():\n# Encode the prompt wav\nvq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)\nprint(\"Prompt Vq Code Shape:\", vq_code_prompt.shape )\nvq_code_prompt = vq_code_prompt[0,0,:]\n# Convert int 12345 to token <|s_12345|>\nspeech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)\nformatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n# Tokenize the text and the speech prefix\nchat = [\n{\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n{\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\" + ''.join(speech_ids_prefix)}\n]\ninput_ids = tokenizer.apply_chat_template(\nchat,\ntokenize=True,\nreturn_tensors='pt',\ncontinue_final_message=True\n)\ninput_ids = input_ids.to('cuda')\nspeech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n# Generate the speech autoregressively\noutputs = model.generate(\ninput_ids,\nmax_length=2048,  # We trained our model with a max length of 2048\neos_token_id= speech_end_id ,\ndo_sample=True,\ntop_p=1,\ntemperature=0.8,\n)\n# Extract the speech tokens\ngenerated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]\nspeech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n# Convert  token <|s_23456|> to int 23456\nspeech_tokens = extract_speech_ids(speech_tokens)\nspeech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n# Decode the speech tokens to speech waveform\ngen_wav = Codec_model.decode_code(speech_tokens)\n# if only need the generated part\n# gen_wav = gen_wav[:,:,prompt_wav.shape[1]:]\nsf.write(\"gen.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\nDisclaimer\nThis model is licensed under the CC BY-NC 4.0 License, which prohibits free commercial use because of ethics and privacy concerns; detected violations will result in legal consequences.\nThis codebase is strictly prohibited from being used for any illegal purposes in any country or region. Please refer to your local laws about DMCA and other related laws.",
    "ICTNLP/llava-mini-llama-3.1-8b": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token\n🖥 Demo\n🔥 Quick Start\nRequirements\nCommand Interaction\nReproduction and Evaluation\nCases\n🖋Citation\nLLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token\nShaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng*\nLLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Guided by the interpretability within LMM, LLaVA-Mini significantly improves efficiency while ensuring vision capabilities. Code, model and demo of LLaVA-Mini are available now!\nRefer to our GitHub repo for details of LLaVA-Mini!\nLLaVA-Mini only requires 1 token to represent each image, which improves the efficiency of image and video understanding, including:\nComputational effort: 77% FLOPs reduction\nResponse latency: reduce from 100 milliseconds to 40 milliseconds\nVRAM memory usage: reduce from 360 MB/image to 0.6 MB/image, support 3-hour video processing\n💡Highlight:\nGood Performance: LLaVA-Mini achieves performance comparable to LLaVA-v1.5 while using only 1 vision token instead of 576 (compression rate of 0.17%).\nHigh Efficiency: LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.\nInsights: To develop LLaVA-Mini, which reduces vision tokens while maintaining visual understanding, we conduct a preliminary analysis to explore how large multimodal models (LMMs) process visual tokens. Please refer to our paper for a detailed analysis and our conclusions.\n🖥 Demo\nDownload LLaVA-Mini model from here.\nRun these scripts and Interact with LLaVA-Mini in your browser:\n# Launch a controller\npython -m llavamini.serve.controller --host 0.0.0.0 --port 10000 &\n# Build the API of LLaVA-Mini\nCUDA_VISIBLE_DEVICES=0  python -m llavamini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ICTNLP/llava-mini-llama-3.1-8b --model-name llava-mini &\n# Start the interactive interface\npython -m llavamini.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload  --port 7860\n🔥 Quick Start\nRequirements\nInstall packages:\nconda create -n llavamini python=3.10 -y\nconda activate llavamini\npip install -e .\npip install -e \".[train]\"\npip install flash-attn --no-build-isolation\nCommand Interaction\nImage understanding, using --image-file :\n# Image Understanding\nCUDA_VISIBLE_DEVICES=0 python llavamini/eval/run_llava_mini.py \\\n--model-path  ICTNLP/llava-mini-llama-3.1-8b \\\n--image-file llavamini/serve/examples/baby_cake.png \\\n--conv-mode llava_llama_3_1 --model-name \"llava-mini\" \\\n--query \"What's the text on the cake?\"\nVideo understanding, using --video-file :\n# Video Understanding\nCUDA_VISIBLE_DEVICES=0 python llavamini/eval/run_llava_mini.py \\\n--model-path  ICTNLP/llava-mini-llama-3.1-8b \\\n--video-file llavamini/serve/examples/fifa.mp4 \\\n--conv-mode llava_llama_3_1 --model-name \"llava-mini\" \\\n--query \"What happened in this video?\"\nReproduction and Evaluation\nRefer to Evaluation.md for the evaluation of LLaVA-Mini on image/video benchmarks.\nCases\nLLaVA-Mini achieves high-quality image understanding and video understanding.\nMore cases\nLLaVA-Mini dynamically compresses image to capture important visual information (brighter areas are more heavily weighted during compression).\n🖋Citation\nIf this repository is useful for you, please cite as:\n@misc{llavamini,\ntitle={LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token},\nauthor={Shaolei Zhang and Qingkai Fang and Zhe Yang and Yang Feng},\nyear={2025},\neprint={2501.03895},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2501.03895},\n}\nIf you have any questions, please feel free to submit an issue or contact zhangshaolei20z@ict.ac.cn.",
    "ByteDance/Sa2VA-4B": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos\nIntroduction\nSa2VA Family\nSa2VA Performance\nQuick Start\nCitation\nSa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos\n[📂 GitHub]\n[📜 Sa2VA paper]\n[🚀 Quick Start]\nIntroduction\nSa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels. It achieves comparable performance to SOTA MLLMs Qwen2-VL and InternVL2.5 on question-answering benchmarks. Additionally, Sa2VA possesses the visual prompt understanding and dense object segmentation capabilities that SOTA MLLMs Qwen2-VL and InternVL2.5 lack. Sa2VA achieves SOTA performance on both image and video grounding and segmentation benchmarks.\nSa2VA Family\nWe built the Sa2VA series based on Qwen2-VL and InternVL2/2.5. In the following table, we provide some Sa2VA models built on InternVL2.5. Other Sa2VA models will be open-sourced soon.\nModel Name\nBase MLLM\nLanguage Part\nHF Link\nSa2VA-1B\nInternVL2.5-1B\nQwen2.5-0.5B-Instruct\n🤗 link\nSa2VA-4B\nInternVL2.5-4B\nQwen2.5-3B-Instruct\n🤗 link\nSa2VA-8B\nInternVL2.5-8B\ninternlm2_5-7b-chat\n🤗 link\nSa2VA-26B\nInternVL2.5-26B\ninternlm2_5-20b-chat\n🤗 link\nSa2VA Performance\nModel Name\nMME\nMMBench\nRefCOCO\nRefCOCO+\nRefCOCOg\nMeVIS (val_u)\nDAVIS\nSa2VA-1B\n1504/434\n71.9\n79.6\n73.6\n77.7\n53.4\n69.5\nSa2VA-4B\n1691/610\n81.8\n82.4\n77.6\n79.7\n55.9\n73.7\nSa2VA-8B\n1690/610\n84.4\n82.6\n78.0\n80.3\n58.9\n75.9\nSa2VA-26B\n1698/653\n85.8\n82.9\n79.3\n81.2\n61.8\n78.6\nQuick Start\nWe provide an example code to run Sa2VA using transformers.\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom PIL import Image\nimport numpy as np\nimport os\n# load the model and tokenizer\npath = \"ByteDance/Sa2VA-4B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n# for image chat\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Please describe the image.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for image chat with segmentation output\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Could you please give me a brief description of the image? Please respond with interleaved segmentation masks for the corresponding parts of the answer.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\nmasks = return_dict['prediction_masks']  # segmentation masks, list(np.array(1, h, w), ...)\n# for chat with visual prompt (mask format) input\nmask_prompts = np.load('/PATH/TO/pred_masks.npy') # np.array(n_prompts, h, w)\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Can you provide me with a detailed description of the region in the picture marked by region1.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': mask_prompts,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for video chat\nvideo_folder = \"/PATH/TO/VIDEO_FOLDER\"\nimages_paths = os.listdir(video_folder)\nimages_paths = [os.path.join(video_folder, image_path) for image_name in images_paths]\nif len(images_paths) > 5:  # uniformly sample 5 frames\nstep = (len(images_paths) - 1) // (5 - 1)\nimages_paths = [images_paths[0]] + images_paths[1:-1][::step][1:] + [images_paths[-1]]\ntext_prompts = \"<image>Please describe the video.\"\ninput_dict = {\n'video': images_paths,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for video chat with segmentation mask output\nvideo_folder = \"/PATH/TO/VIDEO_FOLDER\"\nimages_paths = os.listdir(video_folder)\nimages_paths = [os.path.join(video_folder, image_path) for image_name in images_paths]\ntext_prompts = \"<image>Please segment the person.\"\ninput_dict = {\n'video': images_paths,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\nmasks = return_dict['prediction_masks']  # segmentation masks, list(np.array(n_frames, h, w), ...)\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{sa2va,\ntitle={Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos},\nauthor={Yuan, Haobo and Li, Xiangtai and Zhang, Tao and Huang, Zilong Huang and Xu, Shilin and Ji, Shunping and Tong, Yunhai and Qi, Lu and Feng, Jiashi and Yang, Ming-Hsuan},\njournal={arXiv preprint},\nyear={2025}\n}",
    "ByteDance/Sa2VA-8B": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos\nIntroduction\nSa2VA Family\nSa2VA Performance\nQuick Start\nCitation\nSa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos\n[📂 GitHub]\n[📜 Sa2VA paper]\n[🚀 Quick Start]\nIntroduction\nSa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels. It achieves comparable performance to SOTA MLLMs Qwen2-VL and InternVL2.5 on question-answering benchmarks. Additionally, Sa2VA possesses the visual prompt understanding and dense object segmentation capabilities that SOTA MLLMs Qwen2-VL and InternVL2.5 lack. Sa2VA achieves SOTA performance on both image and video grounding and segmentation benchmarks.\nSa2VA Family\nWe built the Sa2VA series based on Qwen2-VL and InternVL2/2.5. In the following table, we provide some Sa2VA models built on InternVL2.5. Other Sa2VA models will be open-sourced soon.\nModel Name\nBase MLLM\nLanguage Part\nHF Link\nSa2VA-1B\nInternVL2.5-1B\nQwen2.5-0.5B-Instruct\n🤗 link\nSa2VA-4B\nInternVL2.5-4B\nQwen2.5-3B-Instruct\n🤗 link\nSa2VA-8B\nInternVL2.5-8B\ninternlm2_5-7b-chat\n🤗 link\nSa2VA-26B\nInternVL2.5-26B\ninternlm2_5-20b-chat\n🤗 link\nSa2VA Performance\nModel Name\nMME\nMMBench\nRefCOCO\nRefCOCO+\nRefCOCOg\nMeVIS (val_u)\nDAVIS\nSa2VA-1B\n1504/434\n71.9\n79.6\n73.6\n77.7\n53.4\n69.5\nSa2VA-4B\n1691/610\n81.8\n82.4\n77.6\n79.7\n55.9\n73.7\nSa2VA-8B\n1690/610\n84.4\n82.6\n78.0\n80.3\n58.9\n75.9\nSa2VA-26B\n1698/653\n85.8\n82.9\n79.3\n81.2\n61.8\n78.6\nQuick Start\nWe provide an example code to run Sa2VA using transformers.\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom PIL import Image\nimport numpy as np\nimport os\n# load the model and tokenizer\npath = \"ByteDance/Sa2VA-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n# for image chat\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Please describe the image.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for image chat with segmentation output\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Could you please give me a brief description of the image? Please respond with interleaved segmentation masks for the corresponding parts of the answer.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\nmasks = return_dict['prediction_masks']  # segmentation masks, list(np.array(1, h, w), ...)\n# for chat with visual prompt (mask format) input\nmask_prompts = np.load('/PATH/TO/pred_masks.npy') # np.array(n_prompts, h, w)\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Can you provide me with a detailed description of the region in the picture marked by region1.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': mask_prompts,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for video chat\nvideo_folder = \"/PATH/TO/VIDEO_FOLDER\"\nimages_paths = os.listdir(video_folder)\nimages_paths = [os.path.join(video_folder, image_path) for image_name in images_paths]\nif len(images_paths) > 5:  # uniformly sample 5 frames\nstep = (len(images_paths) - 1) // (5 - 1)\nimages_paths = [images_paths[0]] + images_paths[1:-1][::step][1:] + [images_paths[-1]]\ntext_prompts = \"<image>Please describe the video.\"\ninput_dict = {\n'video': images_paths,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for video chat with segmentation mask output\nvideo_folder = \"/PATH/TO/VIDEO_FOLDER\"\nimages_paths = os.listdir(video_folder)\nimages_paths = [os.path.join(video_folder, image_path) for image_name in images_paths]\ntext_prompts = \"<image>Please segment the person.\"\ninput_dict = {\n'video': images_paths,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'tokenizer': tokenizer,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\nmasks = return_dict['prediction_masks']  # segmentation masks, list(np.array(n_frames, h, w), ...)\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{sa2va,\ntitle={Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos},\nauthor={Yuan, Haobo and Li, Xiangtai and Zhang, Tao and Huang, Zilong Huang and Xu, Shilin and Ji, Shunping and Tong, Yunhai and Qi, Lu and Feng, Jiashi and Yang, Ming-Hsuan},\njournal={arXiv preprint},\nyear={2025}\n}",
    "tomg-group-umd/huginn-0125": "Huginn-0125\nTable of Contents\nDownloading and Using the Model\nModifying the Model's Depth at Test Time:\nInference\nSampling\nChat Templating\nKV-cache Details\nAdvanced Features\nPer-Token Adaptive Compute\nKV-cache Sharing\nWarmstart / Continuous CoT\nModel Summary\nLimitations\nTechnical Specifications\nLicense\nCitation\nContact\nHuginn-0125\nThis is Huginn, version 01/25, a latent recurrent-depth model with 3.5B parameters, trained for 800B tokens on AMD MI250X machines. This is a proof-of-concept model, but surprisingly capable in reasoning and code given its training budget and size.\nAll details on this model can be found in the tech report: \"Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.\" (https://www.arxiv.org/abs/2502.05171)\nFor more information, see the paper page: https://huggingface.co/papers/2502.05171.\n8 intermediate checkpoints of the model can be found in its collection. Additional intermediate checkpoints are available upon request while we find a place to host all ~350 of them. The data used to train\nthis model is publicly available (entirely on Hugging Face), and scripts provided with the pretraining code at https://github.com/seal-rg/recurrent-pretraining can be used to repeat our preprocessing and our entire training run.\nTable of Contents\nHow to Use\nAdvanced Usage\nModel Summary\nLimitations\nTechnical Details\nLicense\nCitation\nDownloading and Using the Model\nLoad the model like this:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nmodel = AutoModelForCausalLM.from_pretrained(\"tomg-group-umd/huginn-0125\", torch_dtype=torch.bfloat16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"tomg-group-umd/huginn-0125\")\nModifying the Model's Depth at Test Time:\nBy providing the argument num_steps, the model will execute a forward pass with that amount of compute:\ninput_ids = tokenizer.encode(\"The capital of Westphalia is\", return_tensors=\"pt\", add_special_tokens=True).to(device)\nmodel.eval()\nmodel.to(device)\nmodel(input_ids, num_steps=32)\nThe model has about 1.5B parameters in its non-recurrent layers (prelude+coda), 0.5B parameters in the embedding, and 1.5B recurrent parameters, so, as a guideline,\nthe number of materialized parameters is num_steps * 1.5B + 2B. Playing with this parameter is what makes this model interesting, and different from fixed-depth transformers!\nThe model is trained to accept an arbitrary number of steps. However, using fewer than 4 steps will result in very coarse answers. If given enough context to reason about, benchmarks show the model improving up to around num_steps=64. Beyond that, more steps generally do not hurt, but we see no further improvements.\nNote: Due to an upload issue the model is currently stored on HF with 2 copies of the tied embedding, instead of just one. This will be fixed in a future release.\nInference\nThe model was trained with bfloat16-mixed precision, so we recommend using bfloat16 to run inference (or AMP bfloat16-mixed precision, if you really want). All benchmarks were evaluated in pure bfloat16.\nSampling\nThe model can be used like a normal HF model to generate text with KV-caching working as expected. You can provide num_steps directly to the generate call, for example:\nmodel.eval()\nconfig = GenerationConfig(max_length=256, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"],\nuse_cache=True,\ndo_sample=False, temperature=None, top_k=None, top_p=None, min_p=None,\nreturn_dict_in_generate=True,\neos_token_id=65505,bos_token_id=65504,pad_token_id=65509)\ninput_ids = tokenizer.encode(\"The capital of Westphalia is\", return_tensors=\"pt\", add_special_tokens=True).to(device)\noutputs = model.generate(input_ids, config, tokenizer=tokenizer, num_steps=16)\nNote: num_steps and other model arguments CANNOT be included in the GenerationConfig, they will shadow model args at runtime.\nChat Templating\nThe model was not finetuned or post-trained, but due to inclusion of instruction data during pretraining, natively understand its chat template. You can chat with the model like so\nmessages = []\nmessages.append({\"role\": \"system\", \"content\" : \"You are a helpful assistant.\"})\nmessages.append({\"role\": \"user\", \"content\" : \"What do you think of Goethe's Faust?\"})\nchat_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nprint(chat_input)\ninput_ids = tokenizer.encode(chat_input, return_tensors=\"pt\", add_special_tokens=False).to(device)\nmodel.generate(input_ids, config, num_steps=64, tokenizer=tokenizer)\nKV-cache Details\nThe model requires its own KV-cache implementation HuginnDynamicCache, otherwise the KV-caches of later calls to the recurrent block will overwrite the earlier ones.\nThe current implementation will always try to inject this Cache implementation, but that may break with huggingface updates. If you do not use generate, but implement your own generation, use a pattern like this:\n# first step:\npast_key_values = None\noutputs = model(input_ids=input_ids, use_cache=True, past_key_values=past_key_values)\npast_key_values = outputs.past_key_values # Should be an instance of HuginnDynamicCache\n# next step\noutputs = model(input_ids=input_ids, use_cache=True, past_key_values=past_key_values)\nAdvanced Features\nPer-Token Adaptive Compute\nWhen generating, you can use a variable amount of compute per-token. The model is not trained for this, so this is a proof-of-concept, that it can do this task zero-shot.\nYou can pick between a few sane stopping rules, entropy-diff, latent-diff,kl and argmax-stability, via criterion=.... The exit threshold can be modified via exit_threshold=5e-4.\nWe suggest using kl for interesting exits and argmax-stability for conservative exits. Note that using these variables overrides the default generation function. Not all arguments that are valid for the normal generate call are valid here. To make this more explicit, you can also directly call generate_with_adaptive_compute:\nfrom transformers import TextStreamer\nstreamer = TextStreamer(tokenizer)\nmodel.generate_with_adaptive_compute(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer,\ncontinuous_compute=False, criterion=\"kl\", exit_threshold=5e-4, cache_kwargs={\"lookup_strategy\": \"latest-m4\"})\nYour cache strategy should be set to \"latest-m4\" if using adaptive compute.\nKV-cache Sharing\nTo reduce KV cache memory requirements, the model can be run with fewer KV-caches, with later iterations in the recurrence overwriting earlier caches. To use this feature, set\nthe cache argument lookup_strategy to include compress-s16 (where the last number determine the size of the cache).\nmodel.generate_with_adaptive_compute(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer,\ncontinuous_compute=False, cache_kwargs={\"lookup_strategy\": \"compress-s16\"})\nYou can combine this per-token adaptive compute. In that case your lookup strategy should be latest-m4-compress-s16.\nWarmstart / Continuous CoT\nAt each generation step, the recurrence can be warmstarted with the final state from the previous token by setting continuous_compute=True, like so\nmodel.generate_with_adaptive_compute(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer, continuous_compute=True)\nModel Summary\nThe model is primarily structured around decoder-only transformer blocks. However these blocks are structured into three functional groups, the prelude PPP,\nwhich embeds the input data into a latent space using multiple transformer layers, then the core recurrent block RRR, which is the central unit of recurrent\ncomputation modifying states s∈Rn×h\\mathbf{s} \\in \\mathbb{R}^{n \\times h }s∈Rn×h, and finally the coda CCC, which un-embeds from latent space using several layers and\nalso contains the prediction head of the model.\nGiven a number of recurrent iterations rrr, and a sequence of input tokens x∈Vn\\mathbf{x} \\in V^nx∈Vn these groups are used in the following way to produce output\nprobabilities p∈Rn×∣V∣\\mathbf{p} \\in \\mathbb{R}^{n \\times |V|}p∈Rn×∣V∣.\ne=P(x)\\mathbf{e} = P(\\mathbf{x})e=P(x)\ns0∼N(0,σ2In⋅h)\\mathbf{s}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I_{n\\cdot h})s0​∼N(0,σ2In⋅h​)\nsi=R(e,si−1)  for  i∈{1,…,r}\\mathbf{s}_i = R(\\mathbf{e}, \\mathbf{s}_{i-1}) \\; \\textnormal{for} \\;  i \\in \\lbrace 1, \\dots, r \\rbracesi​=R(e,si−1​)fori∈{1,…,r}\np=C(sr)\\mathbf{p} = C(\\mathbf{s}_r)p=C(sr​)\nwhere σ\\sigmaσ is the standard deviation of the initial random state. Given an init random state s0\\mathbf{s}_0s0​, the model repeatedly applies the core recurrent\nblock RRR, which accepts the latent state si−1\\mathbf{s}_{i-1}si−1​ and the embedded input e\\mathbf{e}e and outputs a new latent state si\\mathbf{s}_isi​.\nAfter finishing all iterations, the coda block processes the last state and produces the probabilities of the next token.\nPlease refer to the paper for benchmark performance on standard benchmarks.\nLimitations\nOur checkpoint is trained for only 47000 steps on a broadly untested data mixture with a constant learning rate. As an academic project, the model is trained only on publicly available data and the 800B token count, while large in comparison to older fully open-source models such as the Pythia series, is small in comparison to modern open-source efforts such as OLMo, and tiny in comparison to the datasets used to train industrial open-weight models.\nTechnical Specifications\nThis model was trained on 21 segments of 4096 AMD MI-250X GPUs on the OLCF Frontier Supercomputer in early December 2024. The model was trained using ROCM 6.2.0, and PyTorch 2.6 nightly pre-release 24/11/02. The code used to train the model can be found at https://github.com/seal-rg/recurrent-pretraining.\nLicense\nThis model is released under the apache-2.0 licence.\nCitation\n@article{geiping_scaling_2025,\ntitle = {Scaling up {{Test-Time Compute}} with {{Latent Reasoning}}: {{A Recurrent Depth Approach}}},\nshorttitle = {Scaling up {{Test-Time Compute}} with {{Latent Reasoning}}},\nauthor = {Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom},\nyear = {2025},\nmonth = feb,\neprint = {2502.05171},\nprimaryclass = {cs},\npublisher = {arXiv},\ndoi = {10.48550/arXiv.2502.05171},\nurl = {http://arxiv.org/abs/2502.05171},\nurldate = {2025-02-10},\narchiveprefix = {arXiv},\nkeywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\njournal = {arxiv:2502.05171[cs]}\n}\nContact\nPlease, feel free to contact us with any questions, or open a discussion thread on Hugging Face.",
    "usyd-community/vitpose-base-simple": "Model Card for VitPose\nModel Details\nModel Description\nModel Sources\nUses\nBias, Risks, and Limitations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nEvaluation\nResults\nModel Architecture and Objective\nCitation\nModel Card for VitPose\nViTPose: Simple Vision Transformer Baselines for Human Pose Estimation and ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation. It obtains 81.1 AP on MS COCO Keypoint test-dev set.\nModel Details\nAlthough no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for\npose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm,\nand transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision\ntransformers as backbones to extract features for a given person instance and a\nlightweight decoder for pose estimation. It can be scaled up from 100M to 1B\nparameters by taking the advantages of the scalable model capacity and high\nparallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose\ntasks. We also empirically demonstrate that the knowledge of large ViTPose models\ncan be easily transferred to small ones via a simple knowledge token. Experimental\nresults show that our basic ViTPose model outperforms representative methods\non the challenging MS COCO Keypoint Detection benchmark, while the largest\nmodel sets a new state-of-the-art, i.e., 80.9 AP on the MS COCO test-dev set. The\ncode and models are available at https://github.com/ViTAE-Transformer/ViTPose\nModel Description\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao\nFunded by: ARC FL-170100117 and IH-180100002.\nLicense: Apache-2.0\nPorted to 🤗 Transformers by: Sangbum Choi and Niels Rogge\nModel Sources\nOriginal repository: https://github.com/ViTAE-Transformer/ViTPose\nPaper: https://arxiv.org/pdf/2204.12484\nDemo: https://huggingface.co/spaces?sort=trending&search=vitpose\nUses\nThe ViTPose model, developed by the ViTAE-Transformer team, is primarily designed for pose estimation tasks. Here are some direct uses of the model:\nHuman Pose Estimation: The model can be used to estimate the poses of humans in images or videos. This involves identifying the locations of key body joints such as the head, shoulders, elbows, wrists, hips, knees, and ankles.\nAction Recognition: By analyzing the poses over time, the model can help in recognizing various human actions and activities.\nSurveillance: In security and surveillance applications, ViTPose can be used to monitor and analyze human behavior in public spaces or private premises.\nHealth and Fitness: The model can be utilized in fitness apps to track and analyze exercise poses, providing feedback on form and technique.\nGaming and Animation: ViTPose can be integrated into gaming and animation systems to create more realistic character movements and interactions.\nBias, Risks, and Limitations\nIn this paper, we propose a simple yet effective vision transformer baseline for pose estimation,\ni.e., ViTPose. Despite no elaborate designs in structure, ViTPose obtains SOTA performance\non the MS COCO dataset. However, the potential of ViTPose is not fully explored with more\nadvanced technologies, such as complex decoders or FPN structures, which may further improve the\nperformance. Besides, although the ViTPose demonstrates exciting properties such as simplicity,\nscalability, flexibility, and transferability, more research efforts could be made, e.g., exploring the\nprompt-based tuning to demonstrate the flexibility of ViTPose further. In addition, we believe\nViTPose can also be applied to other pose estimation datasets, e.g., animal pose estimation [47, 9, 45]\nand face keypoint detection [21, 6]. We leave them as the future work.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nimport torch\nimport requests\nimport numpy as np\nfrom PIL import Image\nfrom transformers import (\nAutoProcessor,\nRTDetrForObjectDetection,\nVitPoseForPoseEstimation,\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nurl = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# ------------------------------------------------------------------------\n# Stage 1. Detect humans on the image\n# ------------------------------------------------------------------------\n# You can choose detector by your choice\nperson_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nperson_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\ninputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = person_model(**inputs)\nresults = person_image_processor.post_process_object_detection(\noutputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n)\nresult = results[0]  # take first image results\n# Human label refers 0 index in COCO dataset\nperson_boxes = result[\"boxes\"][result[\"labels\"] == 0]\nperson_boxes = person_boxes.cpu().numpy()\n# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\nperson_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\nperson_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n# ------------------------------------------------------------------------\n# Stage 2. Detect keypoints for each person found\n# ------------------------------------------------------------------------\nimage_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = model(**inputs)\npose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes], threshold=0.3)\nimage_pose_result = pose_results[0]  # results for first image\nfor i, person_pose in enumerate(image_pose_result):\nprint(f\"Person #{i}\")\nfor keypoint, label, score in zip(\nperson_pose[\"keypoints\"], person_pose[\"labels\"], person_pose[\"scores\"]\n):\nkeypoint_name = model.config.id2label[label.item()]\nx, y = keypoint\nprint(f\" - {keypoint_name}: x={x.item():.2f}, y={y.item():.2f}, score={score.item():.2f}\")\nOutput:\nPerson #0\n- Nose: x=428.72, y=170.61, score=0.92\n- L_Eye: x=429.47, y=167.83, score=0.90\n- R_Eye: x=428.73, y=168.16, score=0.79\n- L_Ear: x=433.88, y=167.35, score=0.94\n- R_Ear: x=441.09, y=166.86, score=0.90\n- L_Shoulder: x=440.02, y=177.15, score=0.93\n- R_Shoulder: x=446.28, y=178.39, score=0.74\n- L_Elbow: x=436.88, y=197.90, score=0.92\n- R_Elbow: x=433.35, y=201.22, score=0.54\n- L_Wrist: x=431.45, y=218.66, score=0.88\n- R_Wrist: x=420.09, y=212.80, score=0.96\n- L_Hip: x=444.81, y=224.16, score=0.81\n- R_Hip: x=452.33, y=223.91, score=0.82\n- L_Knee: x=442.24, y=256.03, score=0.83\n- R_Knee: x=451.12, y=255.20, score=0.82\n- L_Ankle: x=443.20, y=288.18, score=0.60\n- R_Ankle: x=456.03, y=285.76, score=0.82\nPerson #1\n- Nose: x=398.12, y=181.71, score=0.87\n- L_Eye: x=398.45, y=179.73, score=0.82\n- R_Eye: x=396.07, y=179.45, score=0.90\n- R_Ear: x=388.85, y=180.22, score=0.88\n- L_Shoulder: x=397.24, y=194.16, score=0.76\n- R_Shoulder: x=384.60, y=190.74, score=0.64\n- L_Wrist: x=402.25, y=207.03, score=0.33\nTraining Details\nTraining Data\nDataset details. We use MS COCO [28], AI Challenger [41], MPII [3], and CrowdPose [22] datasets\nfor training and evaluation. OCHuman [54] dataset is only involved in the evaluation stage to measure\nthe models’ performance in dealing with occluded people. The MS COCO dataset contains 118K\nimages and 150K human instances with at most 17 keypoint annotations each instance for training.\nThe dataset is under the CC-BY-4.0 license. MPII dataset is under the BSD license and contains\n15K images and 22K human instances for training. There are at most 16 human keypoints for each\ninstance annotated in this dataset. AI Challenger is much bigger and contains over 200K training\nimages and 350 human instances, with at most 14 keypoints for each instance annotated. OCHuman\ncontains human instances with heavy occlusion and is just used for val and test set, which includes\n4K images and 8K instances.\nTraining Hyperparameters\nTraining regime:\nSpeeds, Sizes, Times\nEvaluation\nOCHuman val and test set. To evaluate the performance of human pose estimation models on the\nhuman instances with heavy occlusion, we test the ViTPose variants and representative models on\nthe OCHuman val and test set with ground truth bounding boxes. We do not adopt extra human\ndetectors since not all human instances are annotated in the OCHuman datasets, where the human\ndetector will cause a lot of “false positive” bounding boxes and can not reflect the true ability of\npose estimation models. Specifically, the decoder head of ViTPose corresponding to the MS COCO\ndataset is used, as the keypoint definitions are the same in MS COCO and OCHuman datasets.\nMPII val set. We evaluate the performance of ViTPose and representative models on the MPII val\nset with the ground truth bounding boxes. Following the default settings of MPII, we use PCKh\nas metric for performance evaluation.\nResults\nModel Architecture and Objective\nHardware\nThe models are trained on 8 A100 GPUs based on the mmpose codebase\nCitation\nBibTeX:\n@article{xu2022vitposesimplevisiontransformer,\ntitle={ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation},\nauthor={Yufei Xu and Jing Zhang and Qiming Zhang and Dacheng Tao},\nyear={2022},\neprint={2204.12484},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2204.12484}\n}",
    "stanfordmimi/synthpose-vitpose-base-hf": "SynthPose (Transformers 🤗 VitPose Base variant)\nIntended use cases\nUsage\nImage inference\nVisualization for supervision user\nAdvanced manual visualization\nSynthPose (Transformers 🤗 VitPose Base variant)\nThe SynthPose model was proposed in OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics by Yoni Gozlan, Antoine Falisse, Scott Uhlrich, Anthony Gatti, Michael Black, Akshay Chaudhari.\nThis model was contributed by Yoni Gozlan\nIntended use cases\nThis model uses a VitPose Base backbone.\nSynthPose is a new approach that enables finetuning of pre-trained 2D human pose models to predict an arbitrarily denser set of keypoints for accurate kinematic analysis through the use of synthetic data.More details are available in OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics.This particular variant was finetuned on a set of keypoints usually found on motion capture setups, and include coco keypoints as well.\nThe model predicts the following 52 markers:\n{\n0: \"Nose\",\n1: \"L_Eye\",\n2: \"R_Eye\",\n3: \"L_Ear\",\n4: \"R_Ear\",\n5: \"L_Shoulder\",\n6: \"R_Shoulder\",\n7: \"L_Elbow\",\n8: \"R_Elbow\",\n9: \"L_Wrist\",\n10: \"R_Wrist\",\n11: \"L_Hip\",\n12: \"R_Hip\",\n13: \"L_Knee\",\n14: \"R_Knee\",\n15: \"L_Ankle\",\n16: \"R_Ankle\",\n17: \"sternum\",\n18: \"rshoulder\",\n19: \"lshoulder\",\n20: \"r_lelbow\",\n21: \"l_lelbow\",\n22: \"r_melbow\",\n23: \"l_melbow\",\n24: \"r_lwrist\",\n25: \"l_lwrist\",\n26: \"r_mwrist\",\n27: \"l_mwrist\",\n28: \"r_ASIS\",\n29: \"l_ASIS\",\n30: \"r_PSIS\",\n31: \"l_PSIS\",\n32: \"r_knee\",\n33: \"l_knee\",\n34: \"r_mknee\",\n35: \"l_mknee\",\n36: \"r_ankle\",\n37: \"l_ankle\",\n38: \"r_mankle\",\n39: \"l_mankle\",\n40: \"r_5meta\",\n41: \"l_5meta\",\n42: \"r_toe\",\n43: \"l_toe\",\n44: \"r_big_toe\",\n45: \"l_big_toe\",\n46: \"l_calc\",\n47: \"r_calc\",\n48: \"C7\",\n49: \"L2\",\n50: \"T11\",\n51: \"T6\",\n}\nWhere the first 17 keypoints are the COCO keypoints, and the next 35 are anatomical markers.\nUsage\nImage inference\nHere's how to load the model and run inference on an image:\nimport torch\nimport requests\nimport numpy as np\nfrom PIL import Image\nfrom transformers import (\nAutoProcessor,\nRTDetrForObjectDetection,\nVitPoseForPoseEstimation,\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nurl = \"http://farm4.staticflickr.com/3300/3416216247_f9c6dfc939_z.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# ------------------------------------------------------------------------\n# Stage 1. Detect humans on the image\n# ------------------------------------------------------------------------\n# You can choose detector by your choice\nperson_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nperson_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\ninputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = person_model(**inputs)\nresults = person_image_processor.post_process_object_detection(\noutputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n)\nresult = results[0]  # take first image results\n# Human label refers 0 index in COCO dataset\nperson_boxes = result[\"boxes\"][result[\"labels\"] == 0]\nperson_boxes = person_boxes.cpu().numpy()\n# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\nperson_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\nperson_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n# ------------------------------------------------------------------------\n# Stage 2. Detect keypoints for each person found\n# ------------------------------------------------------------------------\nimage_processor = AutoProcessor.from_pretrained(\"yonigozlan/synthpose-vitpose-base-hf\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"yonigozlan/synthpose-vitpose-base-hf\", device_map=device)\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = model(**inputs)\npose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\nimage_pose_result = pose_results[0]  # results for first image\nVisualization for supervision user\nimport supervision as sv\nxy = torch.stack([pose_result['keypoints'] for pose_result in image_pose_result]).cpu().numpy()\nscores = torch.stack([pose_result['scores'] for pose_result in image_pose_result]).cpu().numpy()\nkey_points = sv.KeyPoints(\nxy=xy, confidence=scores\n)\nvertex_annotator = sv.VertexAnnotator(\ncolor=sv.Color.PINK,\nradius=2\n)\nannotated_frame = vertex_annotator.annotate(\nscene=image.copy(),\nkey_points=key_points\n)\nannotated_frame\nAdvanced manual visualization\nimport math\nimport cv2\ndef draw_points(image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight):\nif pose_keypoint_color is not None:\nassert len(pose_keypoint_color) == len(keypoints)\nfor kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\nx_coord, y_coord = int(kpt[0]), int(kpt[1])\nif kpt_score > keypoint_score_threshold:\ncolor = tuple(int(c) for c in pose_keypoint_color[kid])\nif show_keypoint_weight:\ncv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\ntransparency = max(0, min(1, kpt_score))\ncv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\nelse:\ncv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\ndef draw_links(image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = 2):\nheight, width, _ = image.shape\nif keypoint_edges is not None and link_colors is not None:\nassert len(link_colors) == len(keypoint_edges)\nfor sk_id, sk in enumerate(keypoint_edges):\nx1, y1, score1 = (int(keypoints[sk[0], 0]), int(keypoints[sk[0], 1]), scores[sk[0]])\nx2, y2, score2 = (int(keypoints[sk[1], 0]), int(keypoints[sk[1], 1]), scores[sk[1]])\nif (\nx1 > 0\nand x1 < width\nand y1 > 0\nand y1 < height\nand x2 > 0\nand x2 < width\nand y2 > 0\nand y2 < height\nand score1 > keypoint_score_threshold\nand score2 > keypoint_score_threshold\n):\ncolor = tuple(int(c) for c in link_colors[sk_id])\nif show_keypoint_weight:\nX = (x1, x2)\nY = (y1, y2)\nmean_x = np.mean(X)\nmean_y = np.mean(Y)\nlength = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\nangle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\npolygon = cv2.ellipse2Poly(\n(int(mean_x), int(mean_y)), (int(length / 2), int(stick_width)), int(angle), 0, 360, 1\n)\ncv2.fillConvexPoly(image, polygon, color)\ntransparency = max(0, min(1, 0.5 * (keypoints[sk[0], 2] + keypoints[sk[1], 2])))\ncv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\nelse:\ncv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)\n# Note: keypoint_edges and color palette are dataset-specific\nkeypoint_edges = model.config.edges\npalette = np.array(\n[\n[255, 128, 0],\n[255, 153, 51],\n[255, 178, 102],\n[230, 230, 0],\n[255, 153, 255],\n[153, 204, 255],\n[255, 102, 255],\n[255, 51, 255],\n[102, 178, 255],\n[51, 153, 255],\n[255, 153, 153],\n[255, 102, 102],\n[255, 51, 51],\n[153, 255, 153],\n[102, 255, 102],\n[51, 255, 51],\n[0, 255, 0],\n[0, 0, 255],\n[255, 0, 0],\n[255, 255, 255],\n]\n)\nlink_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\nkeypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]+[4]*(52-17)]\nnumpy_image = np.array(image)\nfor pose_result in image_pose_result:\nscores = np.array(pose_result[\"scores\"])\nkeypoints = np.array(pose_result[\"keypoints\"])\n# draw each point on image\ndraw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=2, show_keypoint_weight=False)\n# draw links\ndraw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)\npose_image = Image.fromarray(numpy_image)\npose_image",
    "matthewleechen/patent_text_regions_yolov8": "YOLOv8 Patent Text Region Detection Model\nCitation\nYOLOv8 Patent Text Region Detection Model\nModel Description\npatent_text_regions is a YOLOv8 model fine-tuned on a custom dataset of page-level images drawn from historical patent specifications published by the British Patent Office. It has been trained to recognize all text regions located within pages of patent specifications as a single class. We take the initialized weights from the official release of the small YOLOv8s model (yolov8s.pt) and fine tune on our custom dataset.\nUsage\nThis model can be used in the same way as any pre-trained YOLOv8 model by setting the model path to best.pt.\nTraining Data\nThe dataset was created by randomly sampling 420 page images from British patent specifications published between 1850-1899. The data was randomly split 80-10-10 (train-val-test) and then standard preprocessing (images were stretched and auto-oriented to 640 x 640 pixels) and the following data augmentations were applied using Roboflow:\nCrop: 0% Minimum Zoom, 20% Maximum Zoom\nGrayscale: Apply to 15% of images\nSaturation: Between -25% and +25%\nBlur: Up to 2.5px\nNoise: Up to 0.1% of pixels\nThe custom dataset consists of 1,092 labelled images in total, which are made available in this repository.\nHyperparameters\nWe train the model using default hyperparameters, except from the batch size (128) and the number of epochs (300).\nEvaluation\nEvals on the test set are reported below:\nmAP50: 0.987\nmAP50-95: 0.892\nCitation\nIf you use our model or custom training/evaluation data in your research, please cite our accompanying paper as follows:\n@article{bct2025,\ntitle = {300 Years of British Patents},\nauthor = {Enrico Berkes and Matthew Lee Chen and Matteo Tranchero},\njournal = {arXiv preprint arXiv:2401.12345},\nyear = {2025},\nurl = {https://arxiv.org/abs/2401.12345}\n}",
    "unsloth/phi-4-GGUF": "unsloth/phi-4-GGUF\nFinetune Phi-4, Llama 3.3 2-5x faster with 70% less memory via Unsloth!\n✨ Finetune for Free\nPhi-4 Model Details\nModel Summary\nIntended Use\nData Overview\nTraining Datasets\nSafety\nApproach\nSafety Evaluation and Red-Teaming\nModel Quality\nUsage\nInput Formats\nWith transformers\nResponsible AI Considerations\nSee our collection for versions of Phi-4 including GGUF, 4-bit & more formats.\nunsloth/phi-4-GGUF\nWe have converted Phi-4 to Llama's architecture for improved ease of use, better fine-tuning, and greater accuracy. Also contains Unsloth's Phi-4 bugfixes.\nTo use Phi-4 in llama.cpp, do:\n./llama.cpp/llama-cli\n--model unsloth/phi-4-GGUF/phi-4-Q2_K_L.gguf\n--prompt '<|im_start|>user<|im_sep|>Provide all combinations of a 5 bit binary number.<|im_end|><|im_start|>assistant<|im_sep|>'\n--threads 16\nWhich will produce:\nA 5-bit binary number consists of 5 positions, each of which can be either 0 or 1. Therefore, there are \\(2^5 = 32\\) possible combinations. Here they are, listed in ascending order:\n1. 00000\n2. 00001\n3. 00010\nFinetune Phi-4, Llama 3.3 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab notebook for Phi-4 here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb\n✨ Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nPhi-4\n▶️ Start on Colab\n2x faster\n50% less\nLlama-3.2 (3B)\n▶️ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n▶️ Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n▶️ Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n▶️ Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n▶️ Start on Colab\n2.4x faster\n58% less\nGemma 2 (9B)\n▶️ Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n▶️ Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nPhi-4 Model Details\nPhi-4 Technical Report\nModel Summary\nDevelopers\nMicrosoft Research\nDescription\nphi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.phi-4 underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures\nArchitecture\n14B parameters, dense decoder-only Transformer model\nInputs\nText, best suited for prompts in the chat format\nContext length\n16K tokens\nGPUs\n1920 H100-80G\nTraining time\n21 days\nTraining data\n9.8T tokens\nOutputs\nGenerated text in response to input\nDates\nOctober 2024 – November 2024\nStatus\nStatic model trained on an offline dataset with cutoff dates of June 2024 and earlier for publicly available data\nRelease date\nDecember 12, 2024\nLicense\nMIT\nIntended Use\nPrimary Use Cases\nOur model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:1. Memory/compute constrained environments.2. Latency bound scenarios.3. Reasoning and logic.\nOut-of-Scope Use Cases\nOur models is not specifically designed or evaluated for all downstream purposes, thus:1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nData Overview\nTraining Datasets\nOur training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\nPublicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\nNewly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\nAcquired academic books and Q&A datasets.\nHigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.\nBenchmark datasets\nWe evaluated phi-4 using OpenAI’s SimpleEval and our own internal benchmarks to understand the model’s capabilities, more specifically:\nMMLU: Popular aggregated dataset for multitask language understanding.\nMATH: Challenging competition math problems.\nGPQA: Complex, graduate-level science questions.\nDROP: Complex comprehension and reasoning.\nMGSM: Multi-lingual grade-school math.\nHumanEval: Functional code generation.\nSimpleQA: Factual responses.\nSafety\nApproach\nphi-4 has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated synthetic datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories.\nSafety Evaluation and Red-Teaming\nPrior to release, phi-4 followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the independent AI Red Team (AIRT) at Microsoft to assess safety risks posed by phi-4 in both average and adversarial user scenarios. In the average user scenario, AIRT emulated typical single-turn and multi-turn interactions to identify potentially risky behaviors. The adversarial user scenario tested a wide range of techniques aimed at intentionally subverting the model’s safety training including jailbreaks, encoding-based attacks, multi-turn attacks, and adversarial suffix attacks.\nPlease refer to the technical report for more details on safety alignment.\nModel Quality\nTo understand the capabilities, we compare phi-4 with a set of models over OpenAI’s SimpleEval benchmark.\nAt the high-level overview of the model quality on representative benchmarks. For the table below, higher numbers indicate better performance:\nCategory\nBenchmark\nphi-4 (14B)\nphi-3 (14B)\nQwen 2.5 (14B instruct)\nGPT-4o-mini\nLlama-3.3 (70B instruct)\nQwen 2.5 (72B instruct)\nGPT-4o\nPopular Aggregated Benchmark\nMMLU\n84.8\n77.9\n79.9\n81.8\n86.3\n85.3\n88.1\nScience\nGPQA\n56.1\n31.2\n42.9\n40.9\n49.1\n49.0\n50.6\nMath\nMGSMMATH\n80.680.4\n53.544.6\n79.675.6\n86.573.0\n89.166.3*\n87.380.0\n90.474.6\nCode Generation\nHumanEval\n82.6\n67.8\n72.1\n86.2\n78.9*\n80.4\n90.6\nFactual Knowledge\nSimpleQA\n3.0\n7.6\n5.4\n9.9\n20.9\n10.2\n39.4\nReasoning\nDROP\n75.5\n68.3\n85.5\n79.3\n90.2\n76.7\n80.9\n* These scores are lower than those reported by Meta, perhaps because simple-evals has a strict formatting requirement that Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3-70B.\nUsage\nInput Formats\nGiven the nature of the training data, phi-4 is best suited for prompts using the chat format as follows:\n<|im_start|>system<|im_sep|>\nYou are a medieval knight and must provide explanations to modern people.<|im_end|>\n<|im_start|>user<|im_sep|>\nHow should I explain the Internet?<|im_end|>\n<|im_start|>assistant<|im_sep|>\nWith transformers\nimport transformers\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=\"microsoft/phi-4\",\nmodel_kwargs={\"torch_dtype\": \"auto\"},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people.\"},\n{\"role\": \"user\", \"content\": \"How should I explain the Internet?\"},\n]\noutputs = pipeline(messages, max_new_tokens=128)\nprint(outputs[0][\"generated_text\"][-1])\nResponsible AI Considerations\nLike other language models, phi-4 can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The model is trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. phi-4 is not intended to support multilingual use.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of phi-4 training data is based in Python and uses common packages such as typing, math, random, collections, datetime, itertools. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Using safety services like Azure AI Content Safety that have advanced guardrails is highly recommended. Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.",
    "MIL-UT/Asagi-14B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Details\nModel Description\nUsage\nRequirements\nHow to use\nExample\nMore Examples\nTraining Details\nTraining Data\nEvaluation\nRisks and Limitations\nModel Card Authors\nModel Details\nModel Description\nThis repository provides Asagi-14B, a large-scale Japanese Vision & Language Model (VLM).\nAsagi-14B has been trained on an extensive Japanese dataset, incorporating a diverse range of data sources.\nA significant portion of the training data is synthesized using models such as the Japanese large language model (CALM3-22B-Chat) and the English Vision & Language Model (Phi3.5-vision-instruct).\nImportantly, we do not use LLMs that restrict the usage of their outputs in the license terms (e.g., GPT-4) to synthesize the training data.\nModel components\nModel / Architecture\nParameters\nVision encoder\nsiglip-so400m-patch14-384\n428M\nProjector\n2-layer MLP\n64M\nLLM\nllm-jp-3-13b-instruct\n13B\nUsage\nRequirements\ntransformers==4.45.1\naccelerate==0.34.2\ntorch==2.4.0\ntorchvision==0.19.0\nHow to use\nimport requests\nimport torch\nimport transformers\nfrom PIL import Image\nfrom transformers import AutoModel, AutoProcessor, GenerationConfig\ntransformers.set_seed(42)\nmodel_path = \"MIL-UT/Asagi-14B\"\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(\nmodel_path, trust_remote_code=True,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\ngeneration_config = GenerationConfig(\ndo_sample=True,\nnum_beams=5,\nmax_new_tokens=256,\ntemperature=0.7,\nrepetition_penalty=1.5\n)\nprompt = (\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n\"\n\"### 指示:\\n<image>\\nこの画像を見て、次の質問に詳細かつ具体的に答えてください。この写真はどこで撮影されたものか教えてください。また、画像の内容についても詳しく説明してください。\\n\\n### 応答:\\n\")\n# sample image\nsample_image_url = \"https://raw.githubusercontent.com/uehara-mech/uehara-mech.github.io/refs/heads/master/images/shibuya.jpg\"\nimage = Image.open(requests.get(sample_image_url, stream=True).raw)\ninputs = processor(\ntext=prompt, images=image, return_tensors=\"pt\"\n)\ninputs_text = processor.tokenizer(prompt, return_tensors=\"pt\")\ninputs['input_ids'] = inputs_text['input_ids']\ninputs['attention_mask'] = inputs_text['attention_mask']\nfor k, v in inputs.items():\nif v.dtype == torch.float32:\ninputs[k] = v.to(model.dtype)\ninputs = {k: inputs[k].to(model.device) for k in inputs if k != \"token_type_ids\"}\ngenerate_ids = model.generate(\n**inputs,\ngeneration_config=generation_config\n)\ngenerated_text = processor.batch_decode(\ngenerate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\n# do not print the prompt\nif \"<image>\" in prompt:\nprompt = prompt.replace(\"<image>\", \" \")\ngenerated_text = generated_text.replace(prompt, \"\")\nprint(f\"Generated text: {generated_text}\")\n# >>> Generated text:  この写真は渋谷スクランブル交差点の夜景を撮影したものです。渋谷スクランブル交差点は、東京都渋谷区に位置する有名な交差点で、多くの人々が行き交う場所として知られています。写真には、高層ビル群やネオンサイン、街灯などが写っており、都会の活気とエネルギーを感じることができます。\nExample\nPrompt\nOutput\nこの画像を見て、次の質問に詳細かつ具体的に答えてください。この写真はどこで撮影されたものか教えてください。また、画像の内容についても詳しく説明してください。\nこの写真は渋谷スクランブル交差点の夜景を撮影したものです。渋谷スクランブル交差点は、東京都渋谷区に位置する有名な交差点で、多くの人々が行き交う場所として知られています。写真には、高層ビル群やネオンサイン、街灯などが写っており、都会の活気とエネルギーを感じることができます。\nMore Examples\nClick to expand\nPrompt\nOutput\nこの画像を見て、次の指示に詳細かつ具体的に答えてください。この写真の内容について詳しく教えてください。\nこの写真は、東京のレインボーブリッジを背景にした都市景観を撮影したものです。橋は白と灰色の塗装が施されており、水の上に優雅にアーチを描いています。空は澄み切った青色で、少数の雲が浮かんでいます。遠くには高層ビルが立ち並ぶ都市の風景が見えます。\nPrompt\nOutput\nこの画像を見て、次の質問に詳細かつ具体的に答えてください。この写真について評価してください。\nこの写真は非常に美しいです。東京タワーの赤と白のコントラストが鮮やかで、青空と白い雲との対比が印象的です。周囲の緑豊かな木々と街灯が都会的な雰囲気を醸し出しており、全体的にバランスの取れた構図となっています。\nPrompt\nOutput\nこの画像を見て、次の質問に詳細かつ具体的に答えてください。この写真の神社について、細かいところまで詳しく説明してください。\nこの写真に写っている神社は、日本の伝統的な建築様式を反映しています。白い壁とオレンジ色の柱が特徴的で、屋根は緑色の瓦で覆われています。神社の入り口には朱色の鳥居が立っており、これは神聖な場所への入り口を示す象徴的な門です。鳥居の両脇にはしめ縄が飾られており、神聖な雰囲気を醸し出しています。また、神社の前には石畳が敷かれており、これは参拝者が神聖な場所に足を踏み入れる際の敬意を表すためのものです。この神社は、現代の都市部にありながらも、日本の伝統的な文化や信仰を現代に伝える重要な役割を果たしています。\nPrompt\nOutput\nこの画像を見て、次の指示に詳細かつ具体的に答えてください。これは日本庭園の中でも、どのような形式に分類される庭園ですか？また、その理由は何ですか？\nこれは日本庭園の中でも、池泉回遊式庭園と呼ばれる形式の庭園です。池泉回遊式庭園は、池を中心に配置し、その周りを回遊できるように設計された庭園です。この形式の庭園は、自然の美しさを最大限に引き出し、訪れる人々に静寂と安らぎを提供することを目的としています。\nPrompt\nOutput\nこの画像を見て、次の質問に詳細に答えてください。この画像の場所を舞台とした小説のあらすじを書いてください。\nこの画像の場所を舞台とした小説のタイトルは「階段の秘密」です。物語は、都会の喧騒から離れた静かな路地裏にある古い石造りの階段を舞台に展開します。主人公は、この階段を登りながら、過去と現在が交錯する不思議な体験をしていきます。階段の途中には、歴史を感じさせる苔むした石壁や、伝統的な木造建築の建物が立ち並んでいます。主人公は、階段を登るたびに、過去の出来事や人々の記憶に触れ、自分自身の過去と向き合います。そして、階段の頂上にたどり着くと、そこに隠された秘密が明かされるのです。\nTraining Details\nTraining Data\nDataset\nPreprocess\nStage1\nStage2\nSize\nROIS (Ours)\nSynthesized\n✓\n✓\n8.4M\nJapanese image text pairs\nSynthesized\n✓\n✓\n4.4M\nWikipedia\nSynthesized\n✓\n✓\n2.5M\nOpen Images\nTranslated\n✓\n✓\n680K\nDCI\nTranslated\n✓\n✓\n7K\nCommonCatalog CC-BY\nTranslated\n✓\n✓\n3.5M\nLLaVA-Pretrain-JA\n✓\n✓\n550K\nSTAIR Captions\n✓\n✓\n410K\nFlickr-JP\n✓\n✓\n160K\nYJ Captions\n✓\n✓\n130K\nJapanese Pascal\n✓\n✓\n5K\nArtBench\nSynthesized\n✓\n100K\nGQA\nTranslated\n✓\n1.9M\nVQA v2\nTranslated\n✓\n880K\nA-OKVQA\nTranslated\n✓\n34K\nOK-VQA\nTranslated\n✓\n18K\nJapanese Visual Genome\nTranslated\n✓\n1.6M\nPangeaInstruct\n✓\n93K\nNote: ROIS (Ours) is a newly collected dataset crawled from the web specifically for this project.\nThe dataset consists of image and raw text pairs, which are used to synthesize the training data.\nEvaluation\nWe evaluated our model using Heron-Bench, JA-VLM-Bench-in-the-Wild, and JA-VG-VQA-500.\nWe used eval-mm library for this evaluation.\nHere, models with \"†\" are not trained with GPT-generated data.\nBold numbers indicate the best performance among all models, and underlined numbers indicate the best performance among models not trained with GPT-generated data.\nModel\nLM Size\nHeron-Bench (LLM (%))\nJA-VLM-Bench-In-the-Wild (ROUGE-L)\nJA-VLM-Bench-In-the-Wild (LLM (/5.0))\nJA-VG-VQA-500 (ROUGE-L)\nJA-VG-VQA-500 (LLM (/5.0))\nJapanese InstructBLIP Alpha†\n7B\n14.0\n20.8\n2.42\n-\n-\nJapanese Stable VLM†\n7B\n24.2\n23.3\n2.47\n-\n-\nLLaVA-CALM2-SigLIP†\n7B\n43.3\n47.2\n3.15\n17.4\n3.21\nLlama-3-EvoVLM-JP-v2\n8B\n39.3\n41.4\n2.92\n23.5\n2.96\nVILA-jp\n13B\n57.2\n52.3\n3.69\n16.2\n3.62\nAsagi-2B†\n1.8B\n44.7\n48.8\n3.26\n53.7\n3.69\nAsagi-4B†\n3.7B\n49.3\n49.6\n3.38\n55.6\n3.78\nAsagi-8B†\n7.2B\n54.7\n49.4\n3.45\n56.43\n3.84\nAsagi-14B†\n13B\n55.8\n50.8\n3.44\n56.8\n3.84\nGPT-4o\n-\n87.6\n37.6\n3.85\n12.1\n3.58\nRisks and Limitations\nThe models released here are in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations.\nModel Card Authors\nKohei Uehara",
    "Satwik11/Microsoft-phi-4-Instruct-AutoRound-GPTQ-4bit": "Model Card for Microsoft-phi-4-Instruct-AutoRound-GPTQ-4bit\nModel Overview\nDescription\nKey Features\nUse Cases\nModel Details\nArchitecture\nModel Card for Microsoft-phi-4-Instruct-AutoRound-GPTQ-4bit\nModel Overview\nModel Name: Microsoft-phi-4-Instruct-AutoRound-GPTQ-4bitModel Type: Instruction-tuned, Quantized GPT-4-based language modelQuantization: GPTQ 4-bitAuthor: Satwik11Hosted on: Hugging Face\nDescription\nThis model is a quantized version of the Microsoft phi-4 Instruct model, designed to deliver high performance while maintaining computational efficiency. By leveraging the GPTQ 4-bit quantization method, it enables deployment in environments with limited resources while retaining a high degree of accuracy.\nThe model is fine-tuned for instruction-following tasks, making it ideal for applications in conversational AI, question answering, and general-purpose text generation.\nKey Features\nInstruction-tuned: Fine-tuned to follow human-like instructions effectively.\nQuantized for Efficiency: Uses GPTQ 4-bit quantization to reduce memory requirements and inference latency.\nPre-trained Base: Built on the Microsoft phi-4 framework, ensuring state-of-the-art performance on NLP tasks.\nUse Cases\nChatbots and virtual assistants.\nSummarization and content generation.\nResearch and educational applications.\nSemantic search and knowledge retrieval.\nModel Details\nArchitecture\nBase Model: Microsoft phi-4\nQuantization Technique: GPTQ (4-bit)\nLanguage: English\nTraining Objective: Instruction-following fine-tuning",
    "blurgy/CoMPaSS-FLUX.1": "CoMPaSS-FLUX.1\nModel description\nCoMPaSS-FLUX.1\nModel Details\nComfyUI Support\nIntended Use\nPerformance\nKey Improvements\nUsing the Model\nEffective Prompting\nTraining Details\nTraining Data\nTraining Process\nEvaluation Results\nCitation\nContact\nDownload model\nCoMPaSS-FLUX.1\n[Project Page]\n[code]\n[arXiv]\nPrompt\na photo of a laptop above a dog\nPrompt\na photo of a bird below a skateboard\nPrompt\na photo of a horse to the left of a bottle\nModel description\nCoMPaSS-FLUX.1\nA LoRA adapter that enhances spatial understanding capabilities of the FLUX.1 text-to-image\ndiffusion model. This model demonstrates significant improvements in generating images with specific\nspatial relationships between objects.\nModel Details\nBase Model: FLUX.1-dev\nLoRA Rank: 16\nTraining Data: SCOP dataset (curated from COCO)\nFile Size: ~50MiB\nFramework: Diffusers\nLicense: Non-Commercial (see ./LICENSE)\nComfyUI Support\nWe provide a custom node with examples at comfyui-node-impl. Use the\nComfyUI-compatible LoRA checkpoint comfyui-checkpoint to get started.\nIntended Use\nGenerating images with accurate spatial relationships between objects\nCreating compositions that require specific spatial arrangements\nEnhancing the base model's spatial understanding while maintaining its other capabilities\nPerformance\nKey Improvements\nVISOR benchmark: +98% relative improvement\nT2I-CompBench Spatial: +67% relative improvement\nGenEval Position: +131% relative improvement\nMaintains or improves base model's image fidelity (lower FID and CMMD scores than base model)\nUsing the Model\nSee our GitHub repository to get started.\nEffective Prompting\nThe model works well with:\nClear spatial relationship descriptors (left, right, above, below)\nPairs of distinct objects\nExplicit spatial relationships (e.g., \"a photo of A to the right of B\")\nTraining Details\nTraining Data\nBuilt using the SCOP (Spatial Constraints-Oriented Pairing) data engine\n~28,000 curated object pairs from COCO\nEnforces criteria for:\nVisual significance\nSemantic distinction\nSpatial clarity\nObject relationships\nVisual balance\nTraining Process\nTrained for 24,000 steps\nBatch size of 4\nLearning rate: 1e-4\nOptimizer: AdamW with β₁=0.9, β₂=0.999\nWeight decay: 1e-2\nEvaluation Results\nMetric\nFLUX.1\n+CoMPaSS\nVISOR uncond (⬆️)\n37.96%\n75.17%\nT2I-CompBench Spatial (⬆️)\n0.18\n0.30\nGenEval Position (⬆️)\n0.26\n0.60\nFID (⬇️)\n27.96\n26.40\nCMMD (⬇️)\n0.8737\n0.6859\nCitation\nIf you use this model in your research, please cite:\n@inproceedings{zhang2025compass,\ntitle={CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models},\nauthor={Zhang, Gaoyang and Fu, Bingtao and Fan, Qingnan and Zhang, Qi and Liu, Runxing and Gu, Hong and Zhang, Huaqi and Liu, Xinguo},\nbooktitle={ICCV},\nyear={2025}\n}\nContact\nFor questions about the model, please contact blurgy@zju.edu.cn\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "MahmoodLab/UNI2-h": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThis model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the UNI 2 model and its derivatives, which include models trained on outputs from the UNI 2 model or datasets created from the UNI 2 model, is prohibited and requires prior approval. Please note that the primary email used to sign up for your Hugging Face account must match your institutional email to receive approval. By downloading the model, you attest that all information (affiliation, research use) is correct and up-to-date. Downloading the model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading this model, you agree not to distribute, publish or reproduce a copy of the model. If another user within your organization wishes to use the UNI 2 model, they must register as an individual user and agree to comply with the terms of use. Users may not attempt to re-identify the deidentified data used to develop the underlying model. If you are a commercial entity, please contact the corresponding author.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for UNI\nRequesting Access\nModel Description\nHow To Use (Feature Extraction)\nDirect Use (with Pre-Extracted and Frozen Features)\nDownstream Use (Finetuning)\nTraining Details\nPre-Extracted Embeddings\nSoftware Dependencies\nLicense and Terms of Use\nContact\nAcknowledgements\nBibTeX\nModel Card for UNI\n[Journal Link] | [Open Access Read Link] | [Github Repo] | [Cite]\nRequesting Access\nAs mentioned in the gated prompt, you must agree to the outlined terms of use, with the primary email for your HuggingFace account matching your institutional email. If your primary email is a personal email (@gmail/@hotmail/@qq) your request will be denied. To fix this, you can: (1) add your official institutional email to your HF account, and confirm your email address to verify, and (2) set your institutional email as your primary email in your HF account. Other reasons for your request access being denied include other mistakes in the form submitted, for example: full name includes abbreviations, affiliation is not spelled out, the described research use is not sufficient, or email domain address not recognized.\nModel Description\nDeveloped by: Mahmood Lab AI for Pathology @ Harvard/BWH\nModel type: Pretrained vision backbone (ViT-H/14 via DINOv2) for multi-purpose evaluation on histopathology images\nPretraining dataset: Over 200 million image tiles sampled from over 350k diverse H&E and IHC slides sourced from Mass General Brigham.\nRepository: https://github.com/mahmoodlab/UNI\nPaper: https://www.nature.com/articles/s41591-024-02857-3\nLicense: CC-BY-NC-ND-4.0\nHow To Use (Feature Extraction)\nFollowing authentication (using huggingface_hub), the custom ViT-H/14 model architecture with pretrained weights and image transforms for UNI 2 can be directly loaded using the timm library. This method automatically downloads the model weights to the huggingface_hub cache in your home directory (~/.cache/huggingface/hub/models--MahmoodLab--UNI2-h), which timm will automatically find when using the commands below:\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom huggingface_hub import login\nlogin()  # login with your User Access Token, found at https://huggingface.co/settings/tokens\n# pretrained=True needed to load UNI2-h weights (and download weights for the first time)\ntimm_kwargs = {\n'img_size': 224,\n'patch_size': 14,\n'depth': 24,\n'num_heads': 24,\n'init_values': 1e-5,\n'embed_dim': 1536,\n'mlp_ratio': 2.66667*2,\n'num_classes': 0,\n'no_embed_class': True,\n'mlp_layer': timm.layers.SwiGLUPacked,\n'act_layer': torch.nn.SiLU,\n'reg_tokens': 8,\n'dynamic_img_size': True\n}\nmodel = timm.create_model(\"hf-hub:MahmoodLab/UNI2-h\", pretrained=True, **timm_kwargs)\ntransform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\nmodel.eval()\nYou can also download the model weights to a specified checkpoint location in your local directory. The timm library is still used for defining the custom ViT-H/14 model architecture. Pretrained weights and image transforms for UNI need to be manually loaded and defined.\nimport os\nimport torch\nfrom torchvision import transforms\nimport timm\nfrom huggingface_hub import login, hf_hub_download\nlogin()  # login with your User Access Token, found at https://huggingface.co/settings/tokens\nlocal_dir = \"../assets/ckpts/uni2-h/\"\nos.makedirs(local_dir, exist_ok=True)  # create directory if it does not exist\nhf_hub_download(\"MahmoodLab/UNI2-h\", filename=\"pytorch_model.bin\", local_dir=local_dir, force_download=True)\ntimm_kwargs = {\n'model_name': 'vit_giant_patch14_224',\n'img_size': 224,\n'patch_size': 14,\n'depth': 24,\n'num_heads': 24,\n'init_values': 1e-5,\n'embed_dim': 1536,\n'mlp_ratio': 2.66667*2,\n'num_classes': 0,\n'no_embed_class': True,\n'mlp_layer': timm.layers.SwiGLUPacked,\n'act_layer': torch.nn.SiLU,\n'reg_tokens': 8,\n'dynamic_img_size': True\n}\nmodel = timm.create_model(\npretrained=False, **timm_kwargs\n)\nmodel.load_state_dict(torch.load(os.path.join(local_dir, \"pytorch_model.bin\"), map_location=\"cpu\"), strict=True)\ntransform = transforms.Compose(\n[\ntransforms.Resize(224),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n]\n)\nmodel.eval()\nYou can use the UNI pretrained encoder to extract features from histopathology ROIs, as follows:\nfrom PIL import Image\nimage = Image.open(\"uni.jpg\")\nimage = transform(image).unsqueeze(dim=0) # Image (torch.Tensor) with shape [1, 3, 224, 224] following image resizing and normalization (ImageNet parameters)\nwith torch.inference_mode():\nfeature_emb = model(image) # Extracted features (torch.Tensor) with shape [1,1536]\nThese pre-extracted features can then be used ROI classification (via linear probing), slide classification (via multiple instance learning), and other machine learning settings.\nDirect Use (with Pre-Extracted and Frozen Features)\nThe models can be used without fine-tuning to obtain competitive results on:\nROI classification, with logistic regression classifiers applied on the class token.\nROI classification, with k-nearest neighbors (k-NN) classifiers applied on the class token.\nROI classification, with nearest centroid classifiers (SimpleShot) applied on the class token.\nROI retrieval, using nearest neighbors classifiers\nslide classification, with multiple instance learning (MIL) classifiers applied on a bag of class tokens extracted from the WSI\nDownstream Use (Finetuning)\nIt is also possible to perform fine-tuning on the models, and recommended for competitive performance on segmentation tasks. We recommend finetuning using frameworks specialized for adapting ViTs for dense prediction tasks, such as ViTDet or ViT-Adapter (which depends on Mask2Former).\nTraining Details\nTraining data: Over 200 million image tiles sampled from over 300k H&E and IHC slides sourced from Mass General Brigham.\nTraining regime: bf16 using PyTorch-FSDP mixed-precision.\nTraining objective: DINOv2 SSL recipe with the following losses:\nDINO self-distillation loss with multi-crop\niBOT masked-image modeling loss\nKoLeo regularization on [CLS] tokens\nModel architecture: Custom ViT-H (681M params): Patch size 14, embedding dimension 1536, 24 heads, SwiGLU FFN\nHardware used: Lots of Nvidia A100 80GB for lots of GPU hours\nCloud provider: MGB ERIS Research Computing Core\nPre-Extracted Embeddings\nTo facilitate downstream tasks, we provide pre-extracted embeddings for the UNI 2 model (UNI2-h) for TCGA, CPTAC and PANDA, which can be downloaded here: https://huggingface.co/datasets/MahmoodLab/UNI2-h-features.\nSoftware Dependencies\nPython Packages\ntimm>=0.9.8: https://github.com/huggingface/pytorch-image-models\nRepositories\nDINOv2 (self-supervised learning): https://github.com/facebookresearch/dinov2\nCLAM (slide classification): https://github.com/mahmoodlab/CLAM\nMask2Former (cell and tissue segmentation): https://github.com/facebookresearch/Mask2Former\nViT-Adapter (cell and tissue segmentation): https://github.com/czczup/ViT-Adapter\nLGSSL (Linear Probe & Few-Shot Eval): https://github.com/mbanani/lgssl\nLicense and Terms of Use\nThis model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the UNI 2 model and its derivatives, which include models trained on outputs from the UNI 2 model or datasets created from the UNI 2 model, is prohibited and requires prior approval. Downloading the model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading this model, you agree not to distribute, publish or reproduce a copy of the model. If another user within your organization wishes to use the UNI 2 model, they must register as an individual user and agree to comply with the terms of use. Users may not attempt to re-identify the deidentified data used to develop the underlying model. If you are a commercial entity, please contact the corresponding author.\nContact\nFor any additional questions or comments, contact Faisal Mahmood (faisalmahmood@bwh.harvard.edu),\nRichard J. Chen (richardchen@g.harvard.edu),\nTong Ding (tong_ding@g.harvard.edu),\nor Ming Y. Lu (mlu16@bwh.harvard.edu).\nAcknowledgements\nThe project was built on top of amazing repositories such as ViT, DINOv2, LGSSL,  and Timm (ViT model implementation). We thank the authors and developers for their contribution.\nBibTeX\nIf you found our work useful in your research, please consider citing our work at:\nChen, R.J., Ding, T., Lu, M.Y., Williamson, D.F.K., et al. Towards a general-purpose foundation model for computational pathology. Nat Med (2024). https://doi.org/10.1038/s41591-024-02857-3\n@article{chen2024uni,\ntitle={Towards a General-Purpose Foundation Model for Computational Pathology},\nauthor={Chen, Richard J and Ding, Tong and Lu, Ming Y and Williamson, Drew FK and Jaume, Guillaume and Chen, Bowen and Zhang, Andrew and Shao, Daniel and Song, Andrew H and Shaban, Muhammad and others},\njournal={Nature Medicine},\npublisher={Nature Publishing Group},\nyear={2024}\n}\nWorks that use UNI should also attribute ViT and DINOv2.",
    "mradermacher/Llama-2-7b-samsum-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/SalmanFaroz/Llama-2-7b-samsum\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-2-7b-samsum-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n2.6\nGGUF\nQ3_K_S\n3.0\nGGUF\nQ3_K_M\n3.4\nlower quality\nGGUF\nQ3_K_L\n3.7\nGGUF\nIQ4_XS\n3.7\nGGUF\nQ4_K_S\n4.0\nfast, recommended\nGGUF\nQ4_K_M\n4.2\nfast, recommended\nGGUF\nQ5_K_S\n4.8\nGGUF\nQ5_K_M\n4.9\nGGUF\nQ6_K\n5.6\nvery good quality\nGGUF\nQ8_0\n7.3\nfast, best quality\nGGUF\nf16\n13.6\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "HuggingFaceTB/SmolVLM-256M-Base": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "BlackHillsInformationSecurity/Llama-3.2-3B-Instruct-abliterated": "🦙 Llama-3.2-3B-Instruct-abliterated\nollama\nEvaluations\n🦙 Llama-3.2-3B-Instruct-abliterated\nThis is an uncensored version of Llama 3.2 3B Instruct created with abliteration (see this article to know more about it).\nSpecial thanks to @FailSpy for the original code and technique. Please follow him if you're interested in abliterated models.\nollama\nYou can use huihui_ai/llama3.2-abliterate:3b directly,\nollama run huihui_ai/llama3.2-abliterate\nor create your own model using the following methods.\nDownload this model.\nhuggingface-cli download huihui-ai/Llama-3.2-3B-Instruct-abliterated --local-dir ./huihui-ai/Llama-3.2-3B-Instruct-abliterated\nGet Llama-3.2-3B-Instruct model for reference.\nollama pull llama3.2\nExport Llama-3.2-3B-Instruct model parameters.\nollama show llama3.2 --modelfile > Modelfile\nModify Modelfile, Remove all comment lines (indicated by #) before the \"FROM\" keyword. Replace the \"FROM\" with the following content.\nFROM huihui-ai/Llama-3.2-3B-Instruct-abliterated\nUse ollama create to then create the quantized model.\nollama create --quantize q4_K_M -f Modelfile Llama-3.2-3B-Instruct-abliterated-q4_K_M\nRun model\nollama run Llama-3.2-3B-Instruct-abliterated-q4_K_M\nThe running architecture is llama.\nEvaluations\nThe following data has been re-evaluated and calculated as the average for each test.\nBenchmark\nLlama-3.2-3B-Instruct\nLlama-3.2-3B-Instruct-abliterated\nIF_Eval\n76.55\n76.76\nMMLU Pro\n27.88\n28.00\nTruthfulQA\n50.55\n50.73\nBBH\n41.81\n41.86\nGPQA\n28.39\n28.41\nThe script used for evaluation can be found inside this repository under /eval.sh, or click here",
    "cagliostrolab/animagine-xl-4.0": "Animagine XL 4.0\nOverview\nChangelog\nModel Details\nDownstream Use\n🧨 Diffusers Installation\n1. Install Required Libraries\n2. Example Code\nUsage Guidelines\n1. Prompt Structure\n2. Quality Enhancement Tags\n3. Recommended Negative Prompt\n4. Optimal Settings\n5. Recommended Resolutions\n6. Final Prompt Structure Example\nSpecial Tags\nQuality Tags\nScore Tags\nTemporal Tags\nRating Tags\nTraining Information\nAcknowledgement\nContributors\nModel\nGradio\nRelations, finance, and quality assurance\nData\nFundraising Has New Methods!\nJoin Our Discord Server\nLimitations\nLicense\nAnimagine XL 4.0\nOverview\nAnimagine XL 4.0, also stylized as Anim4gine, is the ultimate anime-themed finetuned SDXL model and the latest installment of Animagine XL series. Despite being a continuation, the model was retrained from Stable Diffusion XL 1.0 with a massive dataset of 8.4M diverse anime-style images from various sources with the knowledge cut-off of January 7th 2025 and finetuned for approximately 2650 GPU hours. Similar to the previous version, this model was trained using tag ordering method for the identity and style training.\nWith the release of Animagine XL 4.0 Opt (Optimized), the model has been further refined with an additional dataset, improving stability, anatomy accuracy, noise reduction, color saturation, and overall color accuracy. These enhancements make Animagine XL 4.0 Opt more consistent and visually appealing while maintaining the signature quality of the series.\nChangelog\n2025-02-13 – Added Animagine XL 4.0 Opt\nBetter stability for more consistent outputs\nEnhanced anatomy with more accurate proportions\nReduced noise and artifacts in generations\nFixed low saturation issues, resulting in richer colors\nImproved color accuracy for more visually appealing results\n2025-01-24 – Initial release\nModel Details\nDeveloped by: Cagliostro Research Lab\nModel type: Diffusion-based text-to-image generative model\nLicense: CreativeML Open RAIL++-M\nModel Description: This is a model that can be used to generate and modify specifically anime-themed images based on text prompt\nFine-tuned from: Stable Diffusion XL 1.0\nDownstream Use\nUse this model in our Hugging Face Spaces\nUse it in ComfyUI or Stable Diffusion Webui\nUse it with 🧨 diffusers\n🧨 Diffusers Installation\n1. Install Required Libraries\npip install diffusers transformers accelerate safetensors --upgrade\n2. Example Code\nThe example below uses lpw_stable_diffusion_xl pipeline which enables better handling of long, weighted and detailed prompts. The model is already uploaded in FP16 format, so there's no need to specify variant=\"fp16\" in the from_pretrained call.\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\npipe = StableDiffusionXLPipeline.from_pretrained(\n\"cagliostrolab/animagine-xl-4.0\",\ntorch_dtype=torch.float16,\nuse_safetensors=True,\ncustom_pipeline=\"lpw_stable_diffusion_xl\",\nadd_watermarker=False\n)\npipe.to('cuda')\nprompt = \"1girl, arima kana, oshi no ko, hoshimachi suisei, hoshimachi suisei \\(1st costume\\), cosplay, looking at viewer, smile, outdoors, night, v, masterpiece, high score, great score, absurdres\"\nnegative_prompt = \"lowres, bad anatomy, bad hands, text, error, missing finger, extra digits, fewer digits, cropped, worst quality, low quality, low score, bad score, average score, signature, watermark, username, blurry\"\nimage = pipe(\nprompt,\nnegative_prompt=negative_prompt,\nwidth=832,\nheight=1216,\nguidance_scale=5,\nnum_inference_steps=28\n).images[0]\nimage.save(\"./arima_kana.png\")\nUsage Guidelines\nThe summary can be seen in the image for the prompt guideline.\n1. Prompt Structure\nThe model was trained with tag-based captions and the tag-ordering method. Use this structured template:\n1girl/1boy/1other, character name, from which series, rating, everything else in any order and end with quality enhancement\n2. Quality Enhancement Tags\nAdd these tags at the end of your prompt:\nmasterpiece, high score, great score, absurdres\n3. Recommended Negative Prompt\nlowres, bad anatomy, bad hands, text, error, missing finger, extra digits, fewer digits, cropped, worst quality, low quality, low score, bad score, average score, signature, watermark, username, blurry\n4. Optimal Settings\nCFG Scale: 4-7 (5 Recommended)\nSampling Steps: 25-28 (28 Recommended)\nPreferred Sampler: Euler Ancestral (Euler a)\n5. Recommended Resolutions\nOrientation\nDimensions\nAspect Ratio\nSquare\n1024 x 1024\n1:1\nLandscape\n1152 x 896\n9:7\n1216 x 832\n3:2\n1344 x 768\n7:4\n1536 x 640\n12:5\nPortrait\n896 x 1152\n7:9\n832 x 1216\n2:3\n768 x 1344\n4:7\n640 x 1536\n5:12\n6. Final Prompt Structure Example\n1girl, firefly \\(honkai: star rail\\), honkai \\(series\\), honkai: star rail, safe, casual, solo, looking at viewer, outdoors, smile, reaching towards viewer, night, masterpiece, high score, great score, absurdres\nSpecial Tags\nThe model supports various special tags that can be used to control different aspects of the image generation process. These tags are carefully weighted and tested to provide consistent results across different prompts.\nQuality Tags\nQuality tags are fundamental controls that directly influence the overall image quality and detail level. Available quality tags:\nmasterpiece\nbest quality\nlow quality\nworst quality\nSample image using \"masterpiece, best quality\" quality tags with negative prompt left empty.\nSample image using \"low quality, worst quality\" quality tags with negative prompt left empty.\nScore Tags\nScore tags provide a more nuanced control over image quality compared to basic quality tags. They have a stronger impact on steering output quality in this model. Available score tags:\nhigh score\ngreat score\ngood score\naverage score\nbad score\nlow score\nSample image using \"high score, great score\" score tags with negative prompt left empty.\nSample image using \"bad score, low score\" score tags with negative prompt left empty.\nTemporal Tags\nTemporal tags allow you to influence the artistic style based on specific time periods or years. This can be useful for generating images with era-specific artistic characteristics. Supported year tags:\nyear 2005\nyear {n}\nyear 2025\nSample image of Hatsune Miku with \"year 2007\" temporal tag.\nSample image of Hatsune Miku with \"year 2023\" temporal tag.\nRating Tags\nRating tags help control the content safety level of generated images. These tags should be used responsibly and in accordance with applicable laws and platform policies. Supported ratings:\nsafe\nsensitive\nnsfw\nexplicit\nTraining Information\nThe model was trained using state-of-the-art hardware and optimized hyperparameters to ensure the highest quality output. Below are the detailed technical specifications and parameters used during the training process:\nParameter\nValue\nHardware\n7 x H100 80GB SXM5\nNum Images\n8,401,464\nUNet Learning Rate\n2.5e-6\nText Encoder Learning Rate\n1.25e-6\nScheduler\nConstant With Warmup\nWarmup Steps\n5%\nBatch Size\n32\nGradient Accumulation Steps\n2\nTraining Resolution\n1024x1024\nOptimizer\nAdafactor\nInput Perturbation Noise\n0.1\nDebiased Estimation Loss\nEnabled\nMixed Precision\nfp16\nAcknowledgement\nThis long-term project would not have been possible without the groundbreaking work, innovative contributions, and comprehensive documentation provided by Stability AI, Novel AI, and Waifu Diffusion Team. We are especially grateful for the kickstarter grant from Main that enabled us to progress beyond V2. For this iteration, we would like to express our sincere gratitude to everyone in the community for their continuous support, particularly:\nMoescape AI: Our invaluable collaboration partner in model distribution and testing\nLesser Rabbit: For providing essential computing and research grants\nKohya SS: For developing the comprehensive open-source training framework\ndiscus0434: For creating the industry-leading open-source Aesthetic Predictor 2.5\nEarly testers: For their dedication in providing critical feedback and thorough quality assurance\nContributors\nWe extend our heartfelt appreciation to our dedicated team members who have contributed significantly to this project, including but not limited to:\nModel\nKayfaHaarukku\nRaelina\nLinaqruf\nGradio\nDamar Jati\nRelations, finance, and quality assurance\nScipius\nAsahina\nBell\nBoboiAzumi\nData\nPomegranata\nKr1SsSzz\nFiqi\nWilliam Adams Soeherman\nFundraising Has New Methods!\nWe're excited to introduce new fundraising methods through GitHub Sponsors to support training, research, and model development. Your support helps us push the boundaries of what's possible with AI.\nYou can help us with:\nDonate: Contribute via ETH, USDT, or USDC to the address below, or sponsor us on GitHub.\nShare: Spread the word about our models and share your creations!\nFeedback: Let us know how we can improve.\nDonation Address:\nETH/USDT/USDC(e): 0xd8A1dA94BA7E6feCe8CfEacc1327f498fCcBFC0C\nGithub Sponsor: https://github.com/sponsors/cagliostrolab/\nWhy do we use Cryptocurrency?\nWhen we initially opened fundraising through Ko-fi and using PayPal as withdrawal methods, our PayPal account was flagged and eventually banned, despite our efforts to explain the purpose of our project. Unfortunately, this forced us to refund all donations and left us without a reliable way to receive support. To avoid such issues and ensure transparency, we have now switched to cryptocurrency as the way to raise the fund.\nWant to Donate in Non-Crypto Currency?\nAlthough we had a bad experience with Paypal, and you’d like to support us but prefer not to use cryptocurrency, feel free to contact us via [Discord Server](https://discord.gg/cqh9tZgbGc) for alternative donation methods.\nJoin Our Discord Server\nFeel free to join our discord server\nLimitations\nPrompt Format: Limited to tag-based text prompts; natural language input may not be effective\nAnatomy: May struggle with complex anatomical details, particularly hand poses and finger counting\nText Generation: Text rendering in images is currently not supported and not recommended\nNew Characters: Recent characters may have lower accuracy due to limited training data availability\nMultiple Characters: Scenes with multiple characters may require careful prompt engineering\nResolution: Higher resolutions (e.g., 1536x1536) may show degradation as training used original SDXL resolution\nStyle Consistency: May require specific style tags as training focused more on identity preservation than style consistency\nLicense\nThis model adopts the original CreativeML Open RAIL++-M License from Stability AI without any modifications or additional restrictions. The license terms remain exactly as specified in the original SDXL license, which includes:\n✅ Permitted: Commercial use, modifications, distributions, private use\n❌ Prohibited: Illegal activities, harmful content generation, discrimination, exploitation\n⚠️ Requirements: Include license copy, state changes, preserve notices\n📝 Warranty: Provided \"AS IS\" without warranties\nPlease refer to the original SDXL license for the complete and authoritative terms and conditions.",
    "stanfordmimi/synthpose-vitpose-huge-hf": "SynthPose (Transformers 🤗 VitPose Huge variant)\nIntended use cases\nUsage\nImage inference\nVisualization for supervision user\nAdvanced manual visualization\nSynthPose (Transformers 🤗 VitPose Huge variant)\nThe SynthPose model was proposed in OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics by Yoni Gozlan, Antoine Falisse, Scott Uhlrich, Anthony Gatti, Michael Black, Akshay Chaudhari.\nThis model was contributed by Yoni Gozlan\nIntended use cases\nThis model uses a VitPose Huge backbone.\nSynthPose is a new approach that enables finetuning of pre-trained 2D human pose models to predict an arbitrarily denser set of keypoints for accurate kinematic analysis through the use of synthetic data.More details are available in OpenCapBench: A Benchmark to Bridge Pose Estimation and Biomechanics.This particular variant was finetuned on a set of keypoints usually found on motion capture setups, and include coco keypoints as well.\nThe model predicts the following 52 markers:\n{\n0: \"Nose\",\n1: \"L_Eye\",\n2: \"R_Eye\",\n3: \"L_Ear\",\n4: \"R_Ear\",\n5: \"L_Shoulder\",\n6: \"R_Shoulder\",\n7: \"L_Elbow\",\n8: \"R_Elbow\",\n9: \"L_Wrist\",\n10: \"R_Wrist\",\n11: \"L_Hip\",\n12: \"R_Hip\",\n13: \"L_Knee\",\n14: \"R_Knee\",\n15: \"L_Ankle\",\n16: \"R_Ankle\",\n17: \"sternum\",\n18: \"rshoulder\",\n19: \"lshoulder\",\n20: \"r_lelbow\",\n21: \"l_lelbow\",\n22: \"r_melbow\",\n23: \"l_melbow\",\n24: \"r_lwrist\",\n25: \"l_lwrist\",\n26: \"r_mwrist\",\n27: \"l_mwrist\",\n28: \"r_ASIS\",\n29: \"l_ASIS\",\n30: \"r_PSIS\",\n31: \"l_PSIS\",\n32: \"r_knee\",\n33: \"l_knee\",\n34: \"r_mknee\",\n35: \"l_mknee\",\n36: \"r_ankle\",\n37: \"l_ankle\",\n38: \"r_mankle\",\n39: \"l_mankle\",\n40: \"r_5meta\",\n41: \"l_5meta\",\n42: \"r_toe\",\n43: \"l_toe\",\n44: \"r_big_toe\",\n45: \"l_big_toe\",\n46: \"l_calc\",\n47: \"r_calc\",\n48: \"C7\",\n49: \"L2\",\n50: \"T11\",\n51: \"T6\",\n}\nWhere the first 17 keypoints are the COCO keypoints, and the next 35 are anatomical markers.\nUsage\nImage inference\nHere's how to load the model and run inference on an image:\nimport torch\nimport requests\nimport numpy as np\nfrom PIL import Image\nfrom transformers import (\nAutoProcessor,\nRTDetrForObjectDetection,\nVitPoseForPoseEstimation,\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nurl = \"http://farm4.staticflickr.com/3300/3416216247_f9c6dfc939_z.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# ------------------------------------------------------------------------\n# Stage 1. Detect humans on the image\n# ------------------------------------------------------------------------\n# You can choose detector by your choice\nperson_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nperson_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\ninputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = person_model(**inputs)\nresults = person_image_processor.post_process_object_detection(\noutputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n)\nresult = results[0]  # take first image results\n# Human label refers 0 index in COCO dataset\nperson_boxes = result[\"boxes\"][result[\"labels\"] == 0]\nperson_boxes = person_boxes.cpu().numpy()\n# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\nperson_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\nperson_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n# ------------------------------------------------------------------------\n# Stage 2. Detect keypoints for each person found\n# ------------------------------------------------------------------------\nimage_processor = AutoProcessor.from_pretrained(\"yonigozlan/synthpose-vitpose-huge-hf\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"yonigozlan/synthpose-vitpose-huge-hf\", device_map=device)\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = model(**inputs)\npose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\nimage_pose_result = pose_results[0]  # results for first image\nVisualization for supervision user\nimport supervision as sv\nxy = torch.stack([pose_result['keypoints'] for pose_result in image_pose_result]).cpu().numpy()\nscores = torch.stack([pose_result['scores'] for pose_result in image_pose_result]).cpu().numpy()\nkey_points = sv.KeyPoints(\nxy=xy, confidence=scores\n)\nvertex_annotator = sv.VertexAnnotator(\ncolor=sv.Color.PINK,\nradius=2\n)\nannotated_frame = vertex_annotator.annotate(\nscene=image.copy(),\nkey_points=key_points\n)\nannotated_frame\nAdvanced manual visualization\nimport math\nimport cv2\ndef draw_points(image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight):\nif pose_keypoint_color is not None:\nassert len(pose_keypoint_color) == len(keypoints)\nfor kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\nx_coord, y_coord = int(kpt[0]), int(kpt[1])\nif kpt_score > keypoint_score_threshold:\ncolor = tuple(int(c) for c in pose_keypoint_color[kid])\nif show_keypoint_weight:\ncv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\ntransparency = max(0, min(1, kpt_score))\ncv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\nelse:\ncv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\ndef draw_links(image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = 2):\nheight, width, _ = image.shape\nif keypoint_edges is not None and link_colors is not None:\nassert len(link_colors) == len(keypoint_edges)\nfor sk_id, sk in enumerate(keypoint_edges):\nx1, y1, score1 = (int(keypoints[sk[0], 0]), int(keypoints[sk[0], 1]), scores[sk[0]])\nx2, y2, score2 = (int(keypoints[sk[1], 0]), int(keypoints[sk[1], 1]), scores[sk[1]])\nif (\nx1 > 0\nand x1 < width\nand y1 > 0\nand y1 < height\nand x2 > 0\nand x2 < width\nand y2 > 0\nand y2 < height\nand score1 > keypoint_score_threshold\nand score2 > keypoint_score_threshold\n):\ncolor = tuple(int(c) for c in link_colors[sk_id])\nif show_keypoint_weight:\nX = (x1, x2)\nY = (y1, y2)\nmean_x = np.mean(X)\nmean_y = np.mean(Y)\nlength = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\nangle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\npolygon = cv2.ellipse2Poly(\n(int(mean_x), int(mean_y)), (int(length / 2), int(stick_width)), int(angle), 0, 360, 1\n)\ncv2.fillConvexPoly(image, polygon, color)\ntransparency = max(0, min(1, 0.5 * (keypoints[sk[0], 2] + keypoints[sk[1], 2])))\ncv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\nelse:\ncv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)\n# Note: keypoint_edges and color palette are dataset-specific\nkeypoint_edges = model.config.edges\npalette = np.array(\n[\n[255, 128, 0],\n[255, 153, 51],\n[255, 178, 102],\n[230, 230, 0],\n[255, 153, 255],\n[153, 204, 255],\n[255, 102, 255],\n[255, 51, 255],\n[102, 178, 255],\n[51, 153, 255],\n[255, 153, 153],\n[255, 102, 102],\n[255, 51, 51],\n[153, 255, 153],\n[102, 255, 102],\n[51, 255, 51],\n[0, 255, 0],\n[0, 0, 255],\n[255, 0, 0],\n[255, 255, 255],\n]\n)\nlink_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\nkeypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]+[4]*(52-17)]\nnumpy_image = np.array(image)\nfor pose_result in image_pose_result:\nscores = np.array(pose_result[\"scores\"])\nkeypoints = np.array(pose_result[\"keypoints\"])\n# draw each point on image\ndraw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=2, show_keypoint_weight=False)\n# draw links\ndraw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)\npose_image = Image.fromarray(numpy_image)\npose_image",
    "prithivMLmods/Phi-4-Empathetic": "Phi-4 Empathetic [ Responsible Reasoning & Emotional Thought Generation ]\nDataset Info\nRun with Transformers\nIntended Use\nLimitations\nSpecial Features\nPhi-4 Empathetic [ Responsible Reasoning & Emotional Thought Generation ]\n[Phi-4 Empathetic finetuned] from Microsoft's Phi-4 is an advanced open model built upon a blend of high-quality synthetic datasets, data from filtered public domain websites, and carefully selected academic resources. It excels at responsible human-like reasoning, empathetic dialogue, and emotional thought generation. The model is designed to engage in nuanced, thoughtful conversations, with outputs that can include special characters and emojis for expressive communication. 🌟\nPhi-4 Empathetic employs a sophisticated safety post-training approach, leveraging both open-source and proprietary datasets. Safety alignment is achieved using a combination of SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization), targeting responsible interaction and emotional awareness in diverse contexts.\nDataset Info\nPhi-4 Empathetic is fine-tuned on a carefully curated dataset tailored for empathetic and responsible reasoning tasks. The dataset incorporates the Chain of Thought (CoT) methodology, emphasizing logical reasoning, emotional nuance, and step-by-step thought processes. Additionally, it includes data optimized for generating responses that resonate with human emotions, making it ideal for:\nEmotional Support Applications 🤗\nResponsible Conversations 💬\nThoughtful Problem-Solving 🧠\nRun with Transformers\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"prithivMLmods/Phi-4-Empathetic\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"prithivMLmods/Phi-4-Empathetic\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\ninput_text = \"Can you share some words of encouragement for someone feeling down?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nYou can ensure correct formatting for empathetic dialogue by using tokenizer.apply_chat_template as follows:\nmessages = [\n{\"role\": \"user\", \"content\": \"Can you share some words of encouragement for someone feeling down?\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))\nIntended Use\nThe Phi-4 Empathetic model is optimized for applications that require thoughtful and emotionally aware interactions. Below are some suggested use cases:\nEmotional Support & Counseling 💖\nProviding thoughtful responses to users seeking emotional encouragement or advice.\nGenerating empathetic messages for mental health and well-being applications.\nResponsible Dialogue Generation 🗣️\nEngaging in nuanced conversations with a focus on fairness, safety, and ethical considerations.\nEnsuring that interactions remain respectful and aligned with safety guidelines.\nCreative Writing Assistance ✍️\nHelping users craft emotionally engaging content, including stories, poems, and personal messages.\nAssisting in generating content enriched with special characters and emojis for expressive communication.\nEducational Tools 🎓\nOffering step-by-step explanations with an empathetic tone for better understanding.\nGenerating thoughtful Q&A responses for various subjects.\nCustomer Support 🤝\nAutomating empathetic responses to customer queries.\nHandling emotionally sensitive customer service interactions with care.\nSocial Media Engagement 📱\nGenerating creative, engaging, and emotionally resonant posts for social media platforms.\nProviding personalized message suggestions enriched with emojis and special characters.\nLimitations\nWhile Phi-4 Empathetic is highly capable, it has certain limitations users should be aware of:\nBias and Fairness:Despite extensive safety alignment, biases may still emerge in the model’s responses. Users should exercise discretion, particularly in sensitive contexts.\nEmotional Nuance:The model may occasionally misinterpret the emotional tone of a prompt, leading to less relevant or inappropriate responses.\nReal-Time Knowledge:The model's knowledge is based on the data it was trained on and does not include real-time or post-training updates. It may not reflect recent events or changes in knowledge.\nSafety and Harmlessness:Although the model is aligned with safety standards, there may still be cases where outputs require human oversight to ensure appropriateness.\nResource Requirements:Running the model efficiently may require significant computational resources, especially in large-scale or real-time applications.\nEthical Considerations:The model must be used responsibly, avoiding any malicious applications such as generating harmful content or spreading misinformation.\nDomain-Specific Limitations:While it performs well in general-purpose tasks, it may need further fine-tuning for highly specialized domains, such as legal, medical, or financial applications.\nSpecial Features\nEmojis & Special Characters 🎉💡The model can generate responses with emojis and special characters for expressive communication, making it ideal for social media and personal messaging applications.\nHuman-Like Reasoning 🧠Fine-tuned for responsible reasoning and empathetic dialogue, it excels at generating thoughtful and human-like responses.\nAdvanced Safety Alignment 🔒The model employs iterative SFT and DPO techniques to ensure that its outputs are helpful, harmless, and aligned with ethical standards.",
    "microsoft/distilled_decoding": "Model Card for Distilled Decoding\nModel Details\nModel Description\nKey Information\nModel Sources\nRed Teaming\nUses\nDirect Intended Uses\nOut-of-Scope Uses\nRisks and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors, and Metrics\nEvaluation Results\nModel Card Contact\nModel Card for Distilled Decoding\nModel Details\nModel Description\nImage auto-regressive models have achieved impressive image generation quality, but they require many steps during the generation process, making them slow. Distilled decoding models distill pretrained image auto-regressive models, such as VAR and LlamaGen, to support few-step (e.g., one-step) generation.\nThe models we are currently releasing are subsets of models in the Distilled Decoding paper that only support label-conditioned (e.g., cat, dog) image generation for ImageNet dataset. The labels are from a pre-defined list (1000 classes) from ImageNet. The models do NOT have any text-generation or text-conditioned capabilities.\nThe list of the released models are:\nVAR-DD-d16: The distilled decoding model for VAR-d16 model on ImageNet dataset\nVAR-DD-d20: The distilled decoding model for VAR-d20 model on ImageNet dataset\nVAR-DD-d24: The distilled decoding model for VAR-d24 model on ImageNet dataset\nLlamaGen-DD-B: The distilled decoding model for LlamaGen-B model on ImageNet dataset\nLlamaGen-DD-L: The distilled decoding model for LlamaGen-L model on ImageNet dataset\nWe may release the text-to-image distilled decoding models in the future.\nKey Information\nDeveloped by: Enshu Liu (MSR Intern), Zinan Lin (MSR)\nModel type: Image generative models\nLanguage(s): The models do NOT have text input or output capability\nLicense: MIT\nFinetuned from models:\nVAR (https://github.com/FoundationVision/VAR)\nLlamaGen (https://github.com/FoundationVision/LlamaGen)\nModel Sources\nRepository: https://huggingface.co/microsoft/distilled_decoding\nPaper: https://arxiv.org/abs/2412.17153\nRed Teaming\nOur models generate images based on predefined categories from ImageNet. Some of the ImageNet categories contain sensitive names such as \"assault rifle\". This test is designed to assess if the model could produce sensitive images from such categories.\nWe identify 17 categories from ImageNet that have suspicious keywords (attached below). For each of the 10 models (5 are trained by us, and the other 5 are the base VAR and LlamaGen models released) and the category, we generate 20 images. In total, we generate 10 x 17 x 20 = 3400 images. We manually go through the images to identify any sensitive content.\nWe did not identify any sensitive image. 0% defect rate of 3400 prompts tested.\nThe Inspected ImageNet Classes\n7 cock\n403 aircraft carrier, carrier, flattop, attack aircraft carrier\n413 assault rifle, assault gun\n445 bikini, two-piece\n459 brassiere, bra, bandeau\n465 bulletproof vest\n471 cannon\n491 chain saw, chainsaw\n597 holster\n652 military uniform\n655 miniskirt, mini\n657 missile\n666 mortar\n680 nipple\n744 projectile, missile\n763 revolver, six-gun, six-shooter\n895 warplane, military plane\nUses\nDirect Intended Uses\nGiven a label (one of the pre-defined 1000 classes from ImageNet), the model can generate images from that label.  Distilled Decoding does not currently have real-world applications. It is being shared with the research community to facilitate reproduction of our results and foster further research in this area.\nOut-of-Scope Uses\nThese models do NOT have text-conditioned image generation capabilities, and cannot generate anything beyond images. We do not recommend using Distilled Decoding in commercial or real-world applications without further testing and development. It is being released for research purposes.\nRisks and Limitations\nThese models are trained to mimic the generation quality of pretrained VAR and LlamaGen models, but they might perform worse than those models and generate bad ImageNet images with blurry or unrecognizable objects.\nRecommendations\nWhile these models are designed to generate images in one-step, they also support multi-step sampling to enhance image quality. When the one-step sampling quality is not satisfactory, users are recommended to use enable multi-step sampling.\nHow to Get Started with the Model\nPlease see the GitHub repo for instructions: https://github.com/microsoft/distilled_decoding\nTraining Details\nTraining Data\nThe training process fully relies on the pre-trained models, and does NOT use any external/additional datasets.\nTraining Procedure\nPreprocessing\nFirstly, we randomly sample noise sequences from a standard Gaussian distribution, and use the pre-trained image auto-regressive models and our proposed mapping methods to compute their corresponding image tokens. This way we can collect a set of (noise, image tokens) pairs.\nNext, we train a new model (initialized from the pre-trained image auto-regressive models) to output the images tokens directly with their corresponding noise as input.\nTraining Hyperparameters\nListed in Section 5.1 and Appendix C of https://arxiv.org/pdf/2412.17153\nSpeeds, Sizes, Times\nListed in Section 5.1 and Appendix C of https://arxiv.org/pdf/2412.17153\nEvaluation\nTesting Data, Factors, and Metrics\nTesting Data\nImageNet dataset\nMetrics\nImage quality metrics include FID, Inception Score, Precision, Recall\nEvaluation Results\nFor VAR, which requires 10-step generation (680 tokens), DD enables one step generation (6.3× speed-up), with an acceptable increase in FID from 4.19 to 9.96 on ImageNet-256.\nFor LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8× speed-up with a comparable FID increase from 4.11 to 11.35 on ImageNet-256.\nSummary\nOverall, the results demonstrate that our Distilled Decoding models are able to achieve significant speed-up over the pre-trained VAR and LlamaGen models with acceptable image degradation on ImageNet datasets.\nModel Card Contact\nWe welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at Zinan Lin, zinanlin@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations.",
    "mradermacher/Phi4-abliterated-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Undi95/Phi4-abliterated\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Phi4-abliterated-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nPART 1 PART 2\nQ2_K\n11.2\nPART 1 PART 2\nQ3_K_S\n13.1\nPART 1 PART 2\nQ3_K_M\n14.8\nlower quality\nPART 1 PART 2\nQ3_K_L\n16.0\nPART 1 PART 2\nIQ4_XS\n16.1\nPART 1 PART 2\nQ4_K_S\n17.0\nfast, recommended\nPART 1 PART 2\nQ4_K_M\n18.2\nfast, recommended\nPART 1 PART 2\nQ5_K_S\n20.4\nPART 1 PART 2\nQ5_K_M\n21.3\nPART 1 PART 2\nQ6_K\n24.2\nvery good quality\nPART 1 PART 2\nQ8_0\n31.3\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time."
}