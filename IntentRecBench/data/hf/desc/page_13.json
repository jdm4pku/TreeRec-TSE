{
    "huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated": "huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-4B-Thinking created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I canâ€™t describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-4B-Thinking-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Thinking\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This modelâ€™s safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the modelâ€™s outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "nvidia/llama-3.2-nv-rerankqa-1b-v2": "Model Overview\nDescription\nLicense/Terms of use\nIntended use\nModel Architecture\nInput\nOutput\nInstallation\nUsage\nSoftware Integration\nModel Version(s)\nTraining Dataset & Evaluation\nTraining Dataset\nEvaluation Results\nEthical Considerations\nGet Help\nEnterprise Support\nNVIDIA NIM Documentation\nBias\nExplainability\nPrivacy\nSafety\nModel Overview\nDescription\nThe Llama 3.2 NeMo Retriever Reranking 1B model is optimized for providing a logit score that represents how relevant a document(s) is to a given query. The model was fine-tuned for multilingual, cross-lingual text question-answering retrieval, with support for long documents (up to 8192 tokens).  This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish.\nThis model is a component in a text retrieval system to improve the overall accuracy. A text retrieval system often uses an embedding model (dense) or lexical search (sparse) index to return relevant text passages given the input. A reranking model can be used to rerank the potential candidate into a final order. The reranking model has the question-passage pairs as an input and therefore, can process cross attention between the words. Itâ€™s not feasible to apply a Ranking model on all documents in the knowledge base, therefore, ranking models are often deployed in combination with embedding models.\nThis model is ready for commercial use.\nThe Llama 3.2 NeMo Retriever Reranking 1B model is a part of the NeMo Retriever collection of NIM, which provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants.\nWe are excited to announce the open sourcing of this commercial embedding model. For users interested in deploying this model in production environments, it is also available via the model API in NVIDIA Inference Microservices (NIM) at llama-3.2-nv-rerankqa-1b-v2.\nLicense/Terms of use\nUse of this model is governed by the NVIDIA Open Model License Agreement. Additional Information: Llama 3.2 Community Model License Agreement.\nIntended use\nThe Llama 3.2 NeMo Retriever Reranking 1B model is most suitable for users who want to improve their multilingual retrieval tasks by reranking a set of candidates for a given question.\nModel Architecture\nArchitecture Type: Transformer\nNetwork Architecture: Fine-tuned ranker model from the meta-llama/Llama-3.2-1B model.\nThe Llama 3.2 NeMo Retriever Reranking 1B model is a transformer cross-encoder fine-tuned with contrastive learning. We employ bi-directional attention when fine-tuning for higher accuracy. The last embedding output by the decoder model is used with a mean pooling strategy, and a binary classification head is fine-tuned for the ranking task.\nRanking models for text ranking are typically trained as a cross-encoder for sentence classification. This involves predicting the relevancy of a sentence pair (for example, question and chunked passages). The CrossEntropy loss is used to maximize the likelihood of passages containing information to answer the question and minimize the likelihood for (negative) passages that do not contain information to answer the question.\nWe trained the model on public datasets described in the Dataset and Training section.\nInput\nInput Type: Pair of Texts\nInput Format: List of text pairs\nInput Parameters: 1D\nOther Properties Related to Input: The model was trained on question and answering over text documents from multiple languages. It was evaluated to work successfully with up to a sequence length of 8192 tokens. Longer texts are recommended to be either chunked or truncated.\nOutput\nOutput Type: Floats\nOutput Format: List of floats\nOutput Parameters: 1D\nOther Properties Related to Output: Each value corresponds to a raw logit. Users can choose to apply a Sigmoid activation function to the logits to convert them into probabilities during model usage.\nInstallation\nThe model requires transformers version 4.47.1.\npip install transformers==4.47.1\nUsage\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_name_or_path = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\ndevice = \"cuda:0\"\nmax_length = 512\nqueries = [\n\"how much protein should a female eat?\",\n]\ndocuments = [\n\"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n\"Calorie intake should not fall below 1,200 a day in women or 1,500 a day in men, except under the supervision of a health professional.\"\n]\n# Create pairs from queries and documents\npairs = [[q, d] for q in queries for d in documents]\ndef prompt_template(q, p):\n\"\"\"Format query and passage with a prompt template.\"\"\"\nreturn f\"question:{q} \\n \\n passage:{p}\"\ntokenizer = AutoTokenizer.from_pretrained(\nmodel_name_or_path,\ntrust_remote_code=True,\npadding_side=\"left\"\n)\nif tokenizer.pad_token is None:\ntokenizer.pad_token = tokenizer.eos_token\nmodel_kwargs = {\n\"trust_remote_code\": True,\n\"torch_dtype\": torch.bfloat16,\n}\nprint(f\"Loading model from {model_name_or_path}...\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\nmodel_name_or_path,\n**model_kwargs\n).eval()\nif model.config.pad_token_id is None:\nmodel.config.pad_token_id = tokenizer.eos_token_id\nmodel = model.to(device)\n# Apply prompt template and tokenize as single sequence\ntexts = [prompt_template(query, doc) for query, doc in pairs]\nbatch_dict = tokenizer(\ntexts,\npadding=True,\ntruncation=True,\nreturn_tensors=\"pt\",\nmax_length=max_length,\n)\n# Move to device\nbatch_dict = {k: v.to(device) for k, v in batch_dict.items()}\nwith torch.inference_mode():\nlogits = model(**batch_dict).logits\nscores = logits.view(-1).cpu().tolist()\nfor i, (pair, score) in enumerate(zip(pairs, scores)):\nquery, doc = pair\nprint(f\"  Query: {query}\")\nprint(f\"  Document: {doc[:100]}{'...' if len(doc) > 100 else ''}\")\nprint(f\"  Score: {score:.4f}\")\n#   Query: how much protein should a female eat?\n#   Document: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams...\n#   Score: 20.6250\n#   Query: how much protein should a female eat?\n#   Document: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top o...\n#   Score: -23.1250\n#   Query: how much protein should a female eat?\n#   Document: Calorie intake should not fall below 1,200 a day in women or 1,500 a day in men, except under the su...\n#   Score: -0.2617\nSoftware Integration\nRuntime: Llama 3.2 NeMo Retriever Reranking 1B NIM\nSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA Lovelace\nSupported Operating System(s): Linux\nModel Version(s)\nLlama 3.2 NeMo Retriever Reranking 1B\nShort Name: llama-3.2-nv-rerankqa-1b-v2\nTraining Dataset & Evaluation\nTraining Dataset\nThe development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MSMARCO restricts â€Œcommercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications.\nData Collection Method by dataset: Automated, Unknown\nLabeling Method by dataset: Automated, Unknown\nProperties: This model was trained on 800k samples from public datasets.\nEvaluation Results\nWe evaluate the pipelines on a set of evaluation benchmarks. We applied the ranking model to the candidates retrieved from a retrieval embedding model.\nOverall, the pipeline llama-3.2-nv-embedqa-1b-v2 + llama-3.2-nv-rerankqa-1b-v2 provides high BEIR+TechQA accuracy with multilingual and crosslingual support. The llama-3.2-nv-rerankqa-1B-v2  ranking model is 3.5x smaller than the nv-rerankqa-mistral-4b-v3 model.\nWe evaluated the NVIDIA Retrieval QA Embedding Model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering - NQ, HotpotQA and FiQA (Finance Q&A) from BeIR benchmark and TechQA dataset. In this benchmark, the metric used was Recall@5. As described, we need to apply the ranking model on the output of an embedding model.\nOpen & Commercial Reranker Models\nAverage Recall@5 on NQ, HotpotQA, FiQA, TechQA dataset\nllama-3.2-nv-embedqa-1b-v2 + llama-3.2-nv-rerankqa-1b-v2\n73.64%\nllama-3.2-nv-embedqa-1b-v2\n68.60%\nnv-embedqa-e5-v5 + nv-rerankQA-mistral-4b-v3\n75.45%\nnv-embedqa-e5-v5\n62.07%\nnv-embedqa-e5-v4\n57.65%\ne5-large_unsupervised\n48.03%\nBM25\n44.67%\nWe evaluated the modelâ€™s multilingual capabilities on the MIRACL academic benchmark - a multilingual retrieval dataset, across 15 languages, and on an additional 11 languages that were translated from the English and Spanish versions of MIRACL. The reported scores are based on a custom subsampled version by selecting hard negatives for each query to reduce the corpus size.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on MIRACL multilingual datasets\nllama-3.2-nv-embedqa-1b-v2 + llama-3.2-nv-rerankqa-1b-v2\n65.80%\nllama-3.2-nv-embedqa-1b-v2\n60.75%\nnv-embedqa-mistral-7b-v2\n50.42%\nBM25\n26.51%\nWe evaluated the cross-lingual capabilities on the academic benchmark MLQA based on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in different languages. We calculate the average Recall@5 across the 42 different language pairs.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on MLQA dataset with different languages\nllama-3.2-nv-embedqa-1b-v2 + llama-3.2-nv-rerankqa-1b-v2\n86.83%\nllama-3.2-nv-embedqa-1b-v2\n79.86%\nnv-embedqa-mistral-7b-v2\n68.38%\nBM25\n13.01%\nWe evaluated the support of long documents on the academic benchmark Multilingual Long-Document Retrieval (MLDR) built on Wikipedia and mC4, covering 12 typologically diverse languages . The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on MLDR\nllama-3.2-nv-embedqa-1b-v2 + llama-3.2-nv-rerankqa-1b-v2\n70.69%\nllama-3.2-nv-embedqa-1b-v2\n59.55%\nnv-embedqa-mistral-7b-v2\n43.24%\nBM25\n71.39%\nData Collection Method by dataset:\nUnknown\nLabeling Method by dataset:\nUnknown\nProperties\nThe evaluation datasets are based on three MTEB/BEIR TextQA datasets, the TechQA dataset, MIRACL, MLDR and MLQA multilingual retrieval datasets, which are all public datasets. The sizes range between 10,000s up to 5M depending on the dataset.\nInference\nEngine: TensorRT\nTest Hardware:  H100 PCIe/SXM, A100 PCIe/SXM, L40s, L4, and A10G\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Explainability, Bias, Safety, and Privacy sections.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nGet Help\nEnterprise Support\nGet access to knowledge base articles and support cases or  submit a ticket at the NVIDIA AI Enterprise Support Services page..\nNVIDIA NIM Documentation\nVisit the NeMo Retriever docs page for release documentation, deployment guides and more.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nPassage and query embedding for question and answer retrieval\nModel Type:\nTransformer encoder\nIntended User:\nGenerative AI creators working with conversational AI models - users who want to build a multilingual question and answer application over a large text corpus, leveraging the latest dense retrieval technologies.\nOutput:\nArray of float numbers (Dense Vector Representation for the input text)\nDescribe how the model works:\nModel transforms the tokenized input text into a dense vector representation.\nPerformance Metrics:\nAccuracy, Throughput, and Latency\nPotential Known Risks:\nThis model does not always guarantee to retrieve the correct passage(s) for a given query.\nLicensing & Terms of Use:\nUse of this model is governed by the NVIDIA Open Model License Agreement. Additional Information: Llama 3.2 Community Model License Agreement.\nTechnical Limitations\nThe modelâ€™s max sequence length is 8192. Therefore, the longer text inputs should be truncated.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nN/A\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personally-identifiable information (PII)?\nNone\nWas consent obtained for any personal data used?\nNot Applicable\nPII used to create this model?\nNone\nHow often is the dataset reviewed?\nBefore Every Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?\nNo\nIf personal data was collected for the development of the model, was it collected directly by NVIDIA?\nNot Applicable\nIf personal data was  collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?\nNot Applicable\nIf personal data was collected for the development of this AI model, was it minimized to only what was required?\nNot Applicable\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nSafety\nField\nResponse\nModel Application(s):\nText Reranking for Retrieval\nDescribe the physical safety impact (if present).\nNot Applicable\nUse Case Restrictions:\nUse of this model is governed by the NVIDIA Open Model License Agreement. Additional Information: Llama 3.2 Community Model License Agreement.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.",
    "nvidia/nemoretriever-table-structure-v1": "Nemoretriever Table Structure v1\nModel Overview\nDescription\nLicense/Terms of use\nTeam\nDeployment Geography\nUse Case\nRelease Date\nReferences\nModel Architecture\nInput\nOutput\nUsage\nModel Version(s):\nTraining and Evaluation Datasets:\nTraining Dataset\nEvaluation Results\nEthical Considerations\nBias\nExplainability\nPrivacy\nSafety\nNemoretriever Table Structure v1\nModel Overview\nPreview of the model output on the example image.\nThe input of this model is expected to be a table image. You can use the Nemoretriever Page Element v3 to detect and crop such images.\nDescription\nThe NeMo Retriever Table Structure v1 model is a specialized object detection model designed to identify and extract the structure of tables in images. Based on YOLOX, an anchor-free version of YOLO (You Only Look Once), this model combines a simpler architecture with enhanced performance. While the underlying technology builds upon work from Megvii Technology, we developed our own base model through complete retraining rather than using pre-trained weights.\nThe model excels at detecting and localizing the fundamental structural elements within tables. Through careful fine-tuning, it can accurately identify and delineate three key components within tables:\nIndividual cells (including merged cells)\nRows\nColumns\nThis specialized focus on table structure enables precise decomposition of complex tables into their constituent parts, forming the foundation for downstream retrieval tasks. This model helps convert tables into the markdown format which can improve retrieval accuracy.\nThis model is ready for commercial/non-commercial use.\nWe are excited to announce the open sourcing of this commercial model. For users interested in deploying this model in production environments, it is also available via the model API in NVIDIA Inference Microservices (NIM) at nemoretriever-table-structure-v1.\nLicense/Terms of use\nThe use of this model is governed by the NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nTeam\nTheo Viel\nBo Liu\nDarragh Hanley\nEven Oldridge\nCorrespondence to Theo Viel (tviel@nvidia.com) and Bo Liu (boli@nvidia.com)\nDeployment Geography\nGlobal\nUse Case\nThe NeMo Retriever Table Structure v1 model specializes in analyzing images containing tables by:\nDetecting and extracting table structure elements (rows, columns, and cells)\nProviding precise location information for each detected element\nSupporting downstream tasks like table analysis and data extraction\nThe model is designed to work in conjunction with OCR (Optical Character Recognition) systems to:\nIdentify the structural layout of tables\nPreserve the relationships between table elements\nEnable accurate extraction of tabular data from images\nIdeal for:\nDocument processing systems\nAutomated data extraction pipelines\nDigital content management solutions\nBusiness intelligence applications\nRelease Date\n10/23/2025 via https://huggingface.co/nvidia/nemoretriever-table-structure-v1\nReferences\nYOLOX paper: https://arxiv.org/abs/2107.08430\nYOLOX repo: https://github.com/Megvii-BaseDetection/YOLOX\nTechnical blog: https://developer.nvidia.com/blog/approaches-to-pdf-data-extraction-for-information-retrieval/\nModel Architecture\nArchitecture Type: YOLOX\nNetwork Architecture: DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction). YOLOX is a single-stage object detector that improves on Yolo-v3.\nThis model was developed based on the Yolo architecture\nNumber of model parameters: 5.4e7\nInput\nInput Type(s): Image\nInput Format(s): Red, Green, Blue (RGB)\nInput Parameters: Two-Dimensional (2D)\nOther Properties Related to Input: Image size resized to (1024, 1024)\nOutput\nOutput Type(s): Array\nOutput Format: A dictionary of dictionaries containing np.ndarray objects. The outer dictionary has entries for each sample (page), and the inner dictionary contains a list of dictionaries, each with a bounding box (np.ndarray), class label, and confidence score for that page.\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output: The output contains bounding boxes, detection confidence scores, and object classes (cell, row, column). The thresholds used for non-maximum suppression are conf_thresh = 0.01 and iou_thresh = 0.25\nOutput Classes:\nCell\nTable cell\nRow\nTable row\nColumn\nTable column\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIAâ€™s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nUsage\nThe model requires torch, and the custom code available in this repository.\nClone the repository\nMake sure git-lfs is installed (https://git-lfs.com)\ngit lfs install\nUsing https\ngit clone https://huggingface.co/nvidia/nemoretriever-table-structure-v1\nOr using ssh\ngit clone git@hf.co:nvidia/nemoretriever-table-structure-v1\nRun the model using the following code:\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom model import define_model\nfrom utils import plot_sample, postprocess_preds_table_structure, reformat_for_plotting\n# Load image\npath = \"./example.png\"\nimg = Image.open(path).convert(\"RGB\")\nimg = np.array(img)\n# Load model\nmodel = define_model(\"table_structure_v1\")\n# Inference\nwith torch.inference_mode():\nx = model.preprocess(img)\npreds = model(x, img.shape)[0]\n# Post-processing\nboxes, labels, scores = postprocess_preds_table_structure(preds, model.threshold, model.labels)\n# Plot\nboxes_plot, confs = reformat_for_plotting(boxes, labels, scores, img.shape, model.num_classes)\nplt.figure(figsize=(30, 15))\nfor i in range(1, 4):\nboxes_plot_c = [b if j == i else [] for j, b in enumerate(boxes_plot)]\nconfs_c = [c if j == i else [] for j, c in enumerate(confs)]\nplt.subplot(1, 3, i)\nplt.title(model.labels[i])\nplot_sample(img, boxes_plot_c, confs_c, labels=model.labels, show_text=False)\nplt.show()\nNote that this repository only provides minimal code to infer the model.\nIf you wish to do additional training, refer to the original repo.\nAdvanced post-processing\nAdditional post-processing might be required to use the model as part of a data extraction pipeline.\nWe show how to use the model as part of a table to text pipeline alongside with the Nemo Retriever OCR in the notebook Demo.ipynb.\nDisclaimer:\nWe are aware of some issues with the model, and will provide a v2 with improved performance in the future which addresses the following issues:\nThe model appears to be less confident in detecting cells in the bottom of the table, which sometimes results in missed cells.\nAdd an extra class for table titles\nAdd support for non full-page tables\nModel Version(s):\nnemoretriever-table-structure-v1\nTraining and Evaluation Datasets:\nTraining Dataset\nData Modality: Image\nImage Training Data Size: Less than a Million Images\nData collection method by dataset: Automated\nLabeling method by dataset: Automated\nPretraining (by NVIDIA): 118,287 images of the COCO train2017 dataset\nFinetuning (by NVIDIA): 23,977 images from Digital Corpora dataset, with annotations from Azure AI Document Intelligence.\nNumber of bounding boxes per class: 1,828,978 cells, 134,089 columns and 316,901 rows. The layout model of Document Intelligence was used with 2024-02-29-preview API version.\nEvaluation Results\nThe primary evaluation set is a cut of the Azure labels and digital corpora images. Number of bounding boxes per class: 200,840 cells, 13,670 columns and 34,575 rows. Mean Average Precision (mAP) was used as an evaluation metric, which measures the model's ability to correctly identify and localize objects across different confidence thresholds.\nData collection method by dataset: Hybrid: Automated, Human\nLabeling method by dataset: Hybrid: Automated, Human\nProperties: We evaluated with Azure labels from manually selected pages, as well as manual inspection on public PDFs and powerpoint slides.\nPer-class Performance Metrics:\nClass\nAP (%)\nAR (%)\ncell\n58.365\n60.647\nrow\n76.992\n81.115\ncolumn\n85.293\n87.434\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Explainability, Bias, Safety & Security, and Privacy sections below.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nObject Detection\nModel Type:\nYOLOX-architecture for detection of table structure within images of tables.\nIntended User:\nEnterprise developers, data scientists, and other technical users who need to extract table structure from images.\nOutput:\nAfter post-processing, the output is three numpy array that contains the detections: boxes [N x 4] (format is normalized (x_min, y_min, x_max, y_max)), associated classes: labels [N] and confidence scores: scores [N].\nDescribe how the model works:\nFinds and identifies objects in images by first dividing the image into a grid. For each section of the grid, the model uses a series of neural networks to extract visual features and simultaneously predict what objects are present (in this case \"cell\", \"row\", or \"column\") and exactly where they are located in that section, all in a single pass through the image.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nNot Applicable\nTechnical Limitations & Mitigation:\nThe model may not generalize to unknown table formats. Further fine-tuning might be required for such documents. Furthermore, it is not robust to rotated tables.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nMean Average Precision, detectionr recall and visual inspection\nPotential Known Risks:\nThis model may not always detect all elements in a document.\nLicensing & Terms of Use:\nUse of this model is governed by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNo\nPersonal data used to create this model?\nNo\nWas consent obtained for any personal data used?\nNot Applicable\nHow often is the dataset reviewed?\nBefore Release\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety\nField\nResponse\nModel Application Field(s):\nObject Detection for Retrieval, focused on Enterprise\nDescribe the life critical impact (if present).\nNot Applicable\nUse Case Restrictions:\nAbide by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.",
    "racineai/QwenAmann-4B-dse": "QwenAmann-4B-dse\nOverview\nPerformance\nENERGY Benchmark (racineai/Open-VLM-Retrieval-Leaderboard)\nKey Strengths\nKey Features\nInstallation\nUsage Example\nApplications\nTraining Methodology\nAuthors\nLicense\nCitation\nQwenAmann-4B-dse\nA multimodal vision-language model specialized for multilingual technical document retrieval.\nOverview\nQwenAmann-4B-dse is a 4B parameter vision-language model designed for efficient retrieval of technical documentation. It directly encodes document screenshots into embeddings, preserving all information including text, images, and layout without requiring separate content extraction.\nPerformance\nENERGY Benchmark (racineai/Open-VLM-Retrieval-Leaderboard)\nKey Strengths\nCompetitive performance: Achieves performance comparable to Jina Embeddings v4 while being fully open-source under Apache 2.0 license (Jina Embeddings v4 is governed by the Qwen Research License as it derives from Qwen-2.5-VL-3B)\nStrong multilingual performance: Stable scores across 5 tested languages\nMulti-domain training: Trained on 1.44M examples across 15+ technical domains\nKey Features\nEfficient Retrieval: Generates document and query embeddings for semantic similarity search\nMultimodal Understanding: Processes text, diagrams, charts, and tables in their original layout\nNo Preprocessing Required: Directly works with document screenshots\nInstallation\npip install transformers accelerate pillow torch qwen-vl-utils\nUsage Example\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, Qwen3VLForConditionalGeneration\nfrom qwen_vl_utils import process_vision_info\n# Load model and processor\nmodel_path = \"racineai/QwenAmann-4B-dse\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Configure image tokens (960 for Qwen3-VL)\nnum_image_tokens = 960\nmin_pixels = 1 * 32 * 32\nmax_pixels = num_image_tokens * 32 * 32\nprocessor = AutoProcessor.from_pretrained(\nmodel_path,\nmin_pixels=min_pixels,\nmax_pixels=max_pixels\n)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nmodel_path,\nattn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None,\ntorch_dtype=torch.bfloat16,\n).to(device).eval()\n# Configure processor\nprocessor.tokenizer.padding_side = \"left\"\nmodel.padding_side = \"left\"\ndef get_embedding(last_hidden_state: torch.Tensor, dimension: int = 2560) -> torch.Tensor:\n\"\"\"Extract and normalize embeddings from last token.\"\"\"\nreps = last_hidden_state[:, -1]\nreps = torch.nn.functional.normalize(reps[:, :dimension], p=2, dim=-1)\nreturn reps\n# Encode a document image\ndocument_image = Image.open(\"technical_document.jpg\")\ndoc_messages = [{\n'role': 'user',\n'content': [\n{'type': 'image', 'image': document_image},\n{'type': 'text', 'text': 'What is shown in this image?'}\n]\n}]\ndoc_text = processor.apply_chat_template(\ndoc_messages,\ntokenize=False,\nadd_generation_prompt=True\n) + \"<|endoftext|>\"\ndoc_image_inputs, doc_video_inputs = process_vision_info(doc_messages)\ndoc_inputs = processor(\ntext=[doc_text],\nimages=doc_image_inputs,\nvideos=doc_video_inputs,\npadding='longest',\nreturn_tensors='pt'\n).to(device)\ncache_position = torch.arange(0, 1)\ndoc_inputs = model.prepare_inputs_for_generation(\n**doc_inputs,\ncache_position=cache_position,\nuse_cache=False\n)\nwith torch.no_grad():\ndoc_outputs = model(**doc_inputs, return_dict=True, output_hidden_states=True)\ndoc_embedding = get_embedding(doc_outputs.hidden_states[-1], dimension=2560)\n# Encode a text query\nquery = \"What are the specifications of this component?\"\nquery_messages = [{\n'role': 'user',\n'content': [\n{'type': 'image', 'image': Image.new('RGB', (32, 32)),\n'resized_height': 1, 'resized_width': 1},\n{'type': 'text', 'text': f'Query: {query}'}\n]\n}]\nquery_text = processor.apply_chat_template(\nquery_messages,\ntokenize=False,\nadd_generation_prompt=True\n) + \"<|endoftext|>\"\nquery_image_inputs, query_video_inputs = process_vision_info(query_messages)\nquery_inputs = processor(\ntext=[query_text],\nimages=query_image_inputs,\nvideos=query_video_inputs,\npadding='longest',\nreturn_tensors='pt'\n).to(device)\ncache_position = torch.arange(0, 1)\nquery_inputs = model.prepare_inputs_for_generation(\n**query_inputs,\ncache_position=cache_position,\nuse_cache=False\n)\nwith torch.no_grad():\nquery_outputs = model(**query_inputs, return_dict=True, output_hidden_states=True)\nquery_embedding = get_embedding(query_outputs.hidden_states[-1], dimension=2560)\n# Calculate similarity using dot product\nsimilarity = torch.einsum(\"bd,cd->bc\", query_embedding, doc_embedding)\nprint(f\"Similarity score: {similarity.item():.4f}\")\nApplications\nMultilingual Technical Document Retrieval: Find relevant documents across multiple languages\nInternational Technical Support Systems: Match user questions to relevant documentation regardless of language\nEngineering Knowledge Management: Index and search technical specifications, diagrams, and reports\nMulti-Domain Search: Retrieve documents across military, energy, quantum computing, nuclear, geotechnical, and other technical domains\nTraining Methodology\nQwenAmann-4B-dse was trained using the Document Screenshot Embedding (DSE) approach, which treats document screenshots as a unified input format. This eliminates the need for content extraction preprocessing while preserving all visual and textual information in documents.\nThe model was fine-tuned on the OGC_MEGA_2 dataset, comprising 1.44M examples across 35+ languages with primary focus on 5 major European languages (English, French, German, Spanish, Italian). The dataset spans 15+ technical domains including military, energy, quantum computing, nuclear, geotechnical engineering, and more.\nAuthors\nLÃ©o Appourchaux - Lead Developer at TW3 Partners\nPaul Lemaistre - GD at Racine.ai â€“ Adjunct Professor at Ã‰cole Centrale d'Ã‰lectronique\nDataset Curators: LÃ©o Appourchaux, Paul Lemaistre, Yumeng Ye, MattÃ©o KHAN, AndrÃ©-Louis Rochet\nLicense\nThis model is released under the Apache 2.0 license.\nCitation\n@misc{qwenamann-4b-dse,\nauthor = {racine.ai},\ntitle = {QwenAmann-4B-dse: A Multimodal Vision-Language Model for Multilingual Document Retrieval},\nyear = {2025},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/racineai/QwenAmann-4B-dse}\n}",
    "rafacost/DreamOmni2-7.6B-GGUF": "Usage\nModel Details\nModel Description\nModel Sources [optional]\nThis GGUF file is a direct conversion of xiabs/DreamOmni2-7.6B.\nAs a quantized model, all original licensing terms and usage restrictions continue to apply.\nUsage\nI created some custom nodes to run on ComfyUI. You can download it here: DreamOmni2-GGUF\nPlace the model files in ComfyUI/models/unet and loras on ComfyUI/models/loras and refer to the GitHub README for detailed installation instructions.\nModel Details\nModel Description\nConverted by: rafacost\nModel Sources [optional]\nRepository: xiabs/DreamOmni2\nPaper: DreamOmni2: Multimodal Instruction-based Editing and Generation.\nGithub: DreamOmni2 Project",
    "facebook/MobileLLM-Pro-base": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nMobileLLM-Pro (P1) Model Card\nKey Features\nModel Information\nResults\nBase Pretrained Model\nInstruction Tuned Model\nTraining Data\nTraining Process\nPretraining\nInstruction Fine-Tuning\nQuantization\nHow to use\nLatency benchmarking\nCitation\nContact\nAcknowledgements\nLicense\nMobileLLM-Pro (P1) Model Card\nWe are introducing MobileLLM-P1 or Pro, a 1B foundational language model in the MobileLLM series, designed to deliver high-quality, efficient on-device inference across a wide range of general language modeling tasks.\nWe open-source two variants of the model: A pre-trained base model along with quantized checkpoints for CPU and accelerator inference, as well as an instruction tuned version, showing competitive performance against models in the this size range on tasks like tool calling, question answering, rewriting and summarization.\nYou are currently in the MobileLLM-Pro-base repository. For more versions, check out:\nMobileLLM-Pro Collection\nMobileLLM-Pro (instruct)\nMobileLLM-Pro-base Int4 CPU Quantized\nMobileLLM-Pro-base Int4 Accelerator Quantized\nFor an interactive demo: ðŸ¤— Click here\nKey Features\nStrong Pre-training Performance: MobileLLM-Pro base achieves impressive pre-training results, outperforming Gemma 3 1B and Llama 3.2 1B by on average 5.7% and 7.9% respectively on reasoning, knowledge, and long-context retrieval benchmarks. This performance is achieved by pre-training on less than 2T fully open-source tokens.\n128k Context Window: The model supports up to 128k tokens, enabling long-context understanding for applications such as document summarization and information retrieval, implicitly learned from a large teacher model.\nEfficient Long-Context Inference: Interleaving local and global attention layers at a 3:1 ratio with 512 local attention, MobileLLM-Pro reduces prefill latency by 1.8x* and lowers KV cache size from 117MB to 40MB* compared to fully global attention, enabling faster and more memory-efficient inference. (*Assuming 8k context length)\nNear Lossless int4 Quantization: We provide int4 quantization-ready checkpoints for our pre-trained model with less than 1.3% quality degradation compared to floating point baselines:\nCPU: int4 weights (group size 32), int8 dynamic activations, int8 KV cache, with only 0.4% regression.\nAccelerators: int4 per-channel weights, with only 1.3% quality regression.\nInstruction Fine-Tuned Model: We provide a competitive instruction fine-tuned (IFT) model specializing in use-cases such as tool calling, question answering, rewriting and summarization.\nMobileLLM-Pro sets a new standard for efficient, high-quality on-device language modeling. We invite the community to explore, evaluate, and build upon this model.\nModel Information\nLayers: 30\nAttention Heads: 20\nKV Heads: 4\nDimension: 1280\nHidden Dimension: 6144\nVocabulary Size: 202,048\nTotal Parameters: 1,084M (1.08B)\nInput Modality: Text\nOutput Modality: Text\nLanguages: English\nTraining Method: Knowledge Distillation\nContext Length: 128k tokens\nTeacher Model: Llama 4-Scout\nLoss Function: KL Divergence\nQuantization: 16-bit, 4-bit\nOther Features: Shared Embeddings, Local-Global Attention\nModel Developer: Meta Reality Labs\nModel Release Date:  October 2025\nLicense: MobileLLM-Pro is FAIR NC licensed\nResults\nBase Pretrained Model\nBenchmark\nP1 (FP)\nP1  (Q-CPU)\nP1 (Q-Acc)\nGemma 3 1B\nLlama 3.2 1B\nHellaSwag\n67.11%\n64.89%\n65.10%\n62.30%\n65.69%\nBoolQ\n76.24%\n77.49%\n76.36%\n63.20%\n62.51%\nPIQA\n76.55%\n76.66%\n75.52%\n73.80%\n75.14%\nSocialIQA\n50.87%\n51.18%\n50.05%\n48.90%\n45.60%\nTriviaQA\n39.85%\n37.26%\n36.42%\n39.80%\n23.81%\nNatQ\n15.76%\n15.43%\n13.19%\n9.48%\n5.48%\nARC-c\n52.62%\n52.45%\n51.24%\n38.40%\n38.28%\nARC-e\n76.28%\n76.58%\n75.73%\n73.00%\n63.47%\nWinoGrande\n62.83%\n62.43%\n61.96%\n58.20%\n61.09%\nOBQA\n43.60%\n44.20%\n40.40%\n37.20%\nNIH\n100.00%\n96.44%\n98.67%\nFP = Full precision, bf16\nQ-CPU = int4, group-wise quantized (for CPU)\nQ-Acc = int4, channel-wise quantized (for Accelerators (ANE&HTP))\nInstruction Tuned Model\nBenchmark\nP1 (IFT)\nGemma 3 1B (IFT)\nLlama 3.2 1B (IFT)\nMMLU\n44.8%\n29.9%\n49.3%\nIFEval\n62.0%\n80.2%\n59.5%\nMBPP\n46.8%\n35.2%\n39.6%\nHumanEval\n59.8%\n41.5%\n37.8%\nARC-C\n62.7%\n59.4%\nHellaSwag\n58.4%\n41.2%\nBFCL v2\n29.4%\n25.7%\nOpen Rewrite\n51.0%\n41.6%\nTLDR9+\n16.8%\n16.8%\nTraining Data\nWe constructed our datamix by selecting publicly available datasets that cover a range of domains. Using data-specific simulation runs, each dataset's contribution to the training process was carefully balanced by assigning it a specific sampling weight. These weights remained consistent throughout the base model pretraining and were informed by the extended work of Automixer and additional ablation studies.\nThe pre-training datamix primarily consists of a large educational web dataset, which makes up the vast majority of the training data. Smaller but significant portions come from coding data, mathematics, Wikipedia, scientific papers, Q&A forums, and algebraic content. In total, the datamix includes approximately 1,500 million rows and 1,640 billion tokens.\nFor our instruction fine-tuned data-mix, we focus on data diversity from existing open-source fine-tuning corpora. Specifically, we combine datasets for general instruction tuning with chat, science, safety, coding and math domains. For our final DPO phase, we rely on completely synthetic datasets.\nTraining Process\nPretraining\nOur general pre-training process contains three distinct phases using logit-based knowledge distillation from the Llama 4-Scout model and a novel model merging paradigm:\nPhase 1 (KD): Language Learning â€“ Learn general language skills from high-quality, well balanced pre-training data\nPhase 2 (KD): Long-context awareness â€“ Extend the model context-length to 128k tokens using implicit positional distillation from the teacher model\nPhase 3 (KD): Domain abilities â€“ Acquire domain understanding through annealing of multiple models in parallel and merging the specialist models, resulting in improvements across a diverse range of domains\nOn top of the three pre-training phases, we add a fourth phase of Quantization-Aware Training (QAT) for our 4-bit quantized model checkpoint.\nInstruction Fine-Tuning\nWe split the instruction fine-tuning stage into three distinct phases combining SFT and DPO methods:\nPhase 1 (SFT): Learn general instruction-following with a focus on data diversity\nPhase 2 (SFT): Domain-weight the Phase 1 data given its shortcomings (e.g. upsample code data to improve logical reasoning)\nPhase 3 (SFT + DPO): Train and align the model for safety and self-identification\nQuantization\nWe apply Quantization Aware Training (QAT) to our baseline and instruction fine-tuned models, yielding quantization-ready checkpoints that can either be directly converted to integer datatype (with minimal quality loss) or used for QAT on additional data. We release two quantization-ready checkpoints:\n4-bit groupwise weight quantization with block size 32, 8-bit dynamic activations, and 8-bit kv-cache quantizations â€” optimized for CPU/GPU backends (xnnpack).\n4-bit channelwise quantization without activation quantization and 8-bit kv-cache quantizations â€” designed for edge hardware accelerators such as Apple Neural Engine (ANE) and Qualcommâ€™s Hexagon Tensor Processor (HTP).\nOur QAT approach incorporates long-context awareness (up to 128k tokens) and self-knowledge distillation using the full-precision teacher model. We compared the QAT-trained model to a standard round-to-nearest Post-Training Quantization (PTQ) baseline. In the groupwise pre-training setting, we observe a 34% (absolute) regression in average benchmark score when using PTQ and only a 1.5% (absolute) regression for QAT. For instruction fine-tuning, we observe less than 1% average regression using QAT.\nHow to use\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\nlogin(token=\"<HF_TOKEN>\")\nMODEL_ID = \"facebook/MobileLLM-Pro-base\"\ndef generate(user_input: str, model, tokenizer) -> str:\ninputs = tokenizer(user_input, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\noutputs = model.generate(inputs, max_new_tokens=128)\nreturn tokenizer.decode(outputs[0], skip_special_tokens=True)\ndef main():\ntokenizer = AutoTokenizer.from_pretrained(\nMODEL_ID, trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_ID, trust_remote_code=True\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\nprompt = \"Why are open-source on-device language models great?\"\nresult = generate(prompt, model, tokenizer)\nprint(result)\nif __name__ == \"__main__\":\nmain()\nLatency benchmarking\nLatency benchmarking was done on a Samsung Galaxy S25 CPU and Samsung Galaxy S24 Hexagon Tensor Processor (HTP). Models were exported to ExecuTorch with XNNPACK backend (for CPU) and HTP backend (for accelerator). The model size of the CPU model with 4-bit groupwise quantization is 590MB. The CPU and HTP prefill latency for different input prompt lengths of 2k, 4k and 8k along with decode speed for generating 1k tokens is shown in the following table.\nModel / Prompt length\n2k\n4k\n8k\nCPU Prefill Latency (s)\n8.9\n24.8\n63.5\nCPU Decode Speed (tok/s)\n33.6\n24.8\n19.7\nHTP Prefill Latency (s)\n1.96\n3.38\n9.82\nHTP Decode Speed (tok/s)\n31.60\n28.95\n22.77\nKV Cache Size (MB)\n14\n23\n40\nTo validate the benefit of interleaved local-global attention (LGA), we benchmark models across different prompt lengths and measure the speed-up in prefill & decode relative to using global attention at every layer:\nCitation\n@misc{mobilellm_pro,\ntitle={MobileLLM-Pro Model Card},\nauthor={Patrick Huber*, Ernie Chang*, Wei Wen*, Igor Fedorov*, Tarek Elgamal, Hanxian Huang, Naveen Suda, Chinnadhurai Sankar, Vish Vogeti, Yanghan Wang, Alex Gladkov, Kai Sheng Tai, Abdelrahman Elogeel, Tarek Hefny, Vikas Chandra, Ahmed Aly, Anuj Kumar, Raghuraman Krishnamoorthi**, Adithya Sagar**},\nyear={2025},\nmonth={October},\nurl = {https://huggingface.co/facebook/MobileLLM-Pro}}\nContact\nPatrick Huber, Meta Inc, Reality Labs (patrickhuber@meta.com)\nErnie Chang, Meta Inc, Reality Labs (erniecyc@meta.com)\nWei Wen,  Meta Inc, Reality Labs (wewen@meta.com)\nIgor Fedorov, Meta Inc, Reality Labs (ifedorov@meta.com)\nRaghuraman Krishnamoorthi,  Meta Inc Reality Labs (raghuraman@meta.com)\nAdithya Sagar, Meta Inc, Reality Labs (adithyasagar@meta.com)\nAcknowledgements\nWe want to thank the team involved in this project, especially: Kimish Patel, Andrew Or, Min Guo, Shen Xu, Brian Moran, Maho Takahashi, Claire Lesage, Rylan Conway, Karan Chadha, Matthew Grange, Tomasz WoÅ‚cyrz, Shiv Desai, Amarlin Anand, Joele Sires, Robert Carrillo, Francisc Bungiu, Jayden Yu, AJ Brush, Yang Li, Samuel Selvan, Anand Sharma, Peng Shan, Anand Dass, Abhishek Sharma\nLicense\nMobileLLM-Pro is distributed under the FAIR NC license",
    "Danrisi/2000sAnalogCore_Qwen-image": "README.md exists but content is empty.",
    "spooknik/CyberRealistic-Flux-SVDQ": "Quality Evaluation\nThis repository contains Nunchaku-quantized (SVDQ) versions of CyberRealistic Flux, a text-to-image model based on Flux.1 Dev by Cyberdelia\nIf you like the model, please consider liking, reviewing and tipping the creator.\nModel Files\nsvdq-int4_r32-CyberRealistic-Flux-V2.5.safetensors: SVDQuant INT4 (rank 32) CyberRealistic Flux V2.5 model. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-CyberRealistic-Flux-V2.5.safetensors: SVDQuant NVFP4 (rank 32) CyberRealistic Flux V2.5. For users with Blackwell GPUs (50-series).\nQuality Evaluation\nBelow is the quality and similarity evaluated with 256 samples from MJHQ-30K dataset. (BF16 is the unqauntized model. INT W4A4 is INT4 and NVFP4 is FP4)\nModel\nPrecision\nMethod\nFID\nIR\nLPIPS\nPSNR\nCyberRealistic-Flux-V2.5\nBF16\n--\n173.06\n0.909\n--\n--\n(30 Steps)\nINT W4A4\nSVDQ\n173.07\n0.902\n0.215\n21.04\nNVFP4\nSVDQ\n173.54\n0.901\n0.222\n20.55\nIf you find my work useful please consider:",
    "lightonai/LightOnOCR-0.9B-16k-1025": "LightOnOCR-0.9B-16k-1025\nModel Overview\nBenchmarks\nInstallation\nStart Server\nPDF Inference\nRendering and Preprocessing Tips\nVariants\nFine-tuning\nData\nLicense\nCitation\nLightOnOCR-0.9B-16k-1025\nSmallest vocabulary variant with only 16k-token, ideal for European languages(English/French).\nLightOnOCR-1B is a compact, end-to-end visionâ€“language model for Optical Character Recognition (OCR) and document understanding. It achieves state-of-the-art accuracy in its weight class while being several times faster and cheaper than larger general-purpose VLMs.\nðŸ“ Read the full blog post | ðŸš€ Try the demo\nHighlights\nâš¡ Speed: 5Ã— faster than dots.ocr, 2Ã— faster than PaddleOCR-VL-0.9B, 1.73Ã— faster than DeepSeekOCR\nðŸ’¸ Efficiency: Processes 5.71 pages/s on a single H100 (~493k pages/day) for <$0.01 per 1,000 pages\nðŸ§  End-to-End: Fully differentiable, no external OCR pipeline\nðŸ§¾ Versatile: Handles tables, receipts, forms, multi-column layouts, and math notation\nðŸŒ Compact variants: 32k and 16k vocab options for European languages\nModel Overview\nLightOnOCR combines a Vision Transformer encoder(Pixtral-based) with a lightweight text decoder(Qwen3-based) distilled from high-quality open VLMs.\nIt is optimized for document parsing tasks, producing accurate, layout-aware text extraction from high-resolution pages.\nBenchmarks\nModel\nArXiv\nOld Scans\nMath\nTables\nMulti-Column\nTiny Text\nBase\nOverall\nLightOnOCR-1B-1025 (151k vocab)\n81.4\n71.6\n76.4\n35.2\n80.0\n88.7\n99.5\n76.1\nLightOnOCR-1B-32k (32k vocab)\n80.6\n66.2\n73.5\n33.5\n71.2\n87.6\n99.5\n73.1\nLightOnOCR-1B-16k (16k vocab)\n82.3\n72.9\n75.3\n33.5\n78.6\n85.1\n99.8\n75.4\nAll benchmarks evaluated using vLLM.\nInstallation\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\nuv pip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly \\\n--prerelease=allow\n# if this fails try adding triton-kernels package\n'triton-kernels @ git+https://github.com/triton-lang/triton.git@v3.5.0#subdirectory=python/triton_kernels'\nuv pip install pypdfium2 pillow requests\nStart Server\nvllm serve lightonai/LightOnOCR-0.9B-16k-1025 \\\n--limit-mm-per-prompt '{\"image\": 1}' \\\n--async-scheduling\nPDF Inference\nimport base64\nimport requests\nimport pypdfium2 as pdfium\nimport io\nENDPOINT = \"http://localhost:8000/v1/chat/completions\"\nMODEL = \"lightonai/LightOnOCR-0.9B-16k-1025\"\n# Download PDF from arXiv\npdf_url = \"https://arxiv.org/pdf/2412.13663\"\npdf_data = requests.get(pdf_url).content\n# Open PDF and convert first page to image\npdf = pdfium.PdfDocument(pdf_data)\npage = pdf[0]\n# Render at 200 DPI (scale factor = 200/72 â‰ˆ 2.77)\npil_image = page.render(scale=2.77).to_pil()\n# Convert to base64\nbuffer = io.BytesIO()\npil_image.save(buffer, format=\"PNG\")\nimage_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n# Make request\npayload = {\n\"model\": MODEL,\n\"messages\": [{\n\"role\": \"user\",\n\"content\": [{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}\n}]\n}],\n\"max_tokens\": 4096,\n\"temperature\": 0.2,\n\"top_p\": 0.9,\n}\nresponse = requests.post(ENDPOINT, json=payload)\ntext = response.json()['choices'][0]['message']['content']\nprint(text)\nRendering and Preprocessing Tips\nRender PDFs to PNG or JPEG at a target longest dimension of 1280â€“1300 px\nMaintain aspect ratio to preserve text geometry\nLightOnOCR is robust to moderate skew; heavy rotation correction is optional\nUse one image per page; batching supported by vLLM\nVariants\nVariant\nDescription\nLightOnOCR-1B-1025\nFull multilingual model (default)\nLightOnOCR-1B-32k\nFastest pruned-vocabulary version (32k tokens) optimized for European languages\nLightOnOCR-1B-16k\nMost compact variant with smallest vocabulary\nFine-tuning\nTransformers integration is coming soon for training.\nLightOnOCR is fully differentiable and supports:\nLoRA fine-tuning\nDomain adaptation (receipts, scientific articles, forms, etc.)\nMultilingual fine-tuning with task-specific corpora\nExample fine-tuning configurations will be released alongside the dataset.\nData\nTrained on a diverse large-scale PDF corpus covering:\nScientific papers, books, receipts, invoices, tables, forms, and handwritten text\nMultiple languages (Latin alphabet dominant)\nReal and synthetic document scans\nThe dataset will be released under an open license.\nLicense\nApache License 2.0\nCitation\n@misc{lightonocr2025,\ntitle        = {LightOnOCR-1B: End-to-End and Efficient Domain-Specific Vision-Language Models for OCR},\nauthor       = {Said Taghadouini and Baptiste Aubertin and Adrien CavaillÃ¨s},\nyear         = {2025},\nhowpublished = {\\url{https://huggingface.co/blog/lightonai/lightonocr}}\n}",
    "befox/QwenImage-Rebalance-GGUF": "GGUF version of lrzjason/QwenImage-Rebalance",
    "Alibaba-NLP/E2Rank-0.6B": "ðŸ“Œ Introduction\nðŸš€ Quick Start\nModel List\nUsage\nEmbedding Model\nReranking\nEnd-to-end search\nðŸ“Š Evaluation\nReranking Benchmark\nBEIR\nEmbedding Benchmark\nMTEB (Eng, v1)\nðŸš© Citation\nE2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker\nðŸ¤– Website |\nðŸ“„ Arxiv Paper |\nðŸ¤— Huggingface Collection |\nðŸš© Citation\nðŸ“Œ Introduction\nWe introduce E2Rank,\nmeaning Efficient Embedding-based Ranking\n(also meaning Embedding-to-Rank),\nwhich extends a single text embedding model\nto perform both high-quality retrieval and listwise reranking,\nthereby achieving strong effectiveness with remarkable efficiency.\nBy applying cosine similarity between the query and\ndocument embeddings as a unified ranking function, the listwise ranking prompt,\nwhich is constructed from the original query and its candidate documents, serves\nas an enhanced query enriched with signals from the top-K documents, akin to\npseudo-relevance feedback (PRF) in traditional retrieval models. This design\npreserves the efficiency and representational quality of the base embedding model\nwhile significantly improving its reranking performance.\nEmpirically, E2Rank achieves state-of-the-art results on the BEIR reranking benchmark\nand demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark,\nwith very low reranking latency. We also show that the ranking training process\nimproves embedding performance on the MTEB benchmark.\nOur findings indicate that a single embedding model can effectively unify retrieval and reranking,\noffering both computational efficiency and competitive ranking accuracy.\nOur work highlights the potential of single embedding models to serve as unified retrieval-reranking engines, offering a practical, efficient, and accurate alternative to complex multi-stage ranking systems.\nðŸš€ Quick Start\nModel List\nSupported Task\nModel Name\nSize\nLayers\nSequence Length\nEmbedding Dimension\nInstruction Aware\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-0.6B\n0.6B\n28\n32K\n1024\nYes\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-4B\n4B\n36\n32K\n2560\nYes\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-8B\n8B\n36\n32K\n4096\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n0.6B\n28\n32K\n1024\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n4B\n36\n32K\n2560\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n8B\n36\n32K\n4096\nYes\nNote:\nEmbedding Only indicates that the model is trained only with the constrative learning and support embedding tasks, while Embedding + Reranking indicates the full E2Rank model trained with both embedding and reranking objectives (for more detals, please refer to the paper).\nInstruction Aware notes whether the model supports customizing the input instruction according to different tasks.\nUsage\nEmbedding Model\nThe usage of E2Rank as an embedding model is similar to Qwen3-Embedding. The only difference is that Qwen3-Embedding will automatically append an EOS token, while E2Rank requires users to manully append the special token <|endoftext|> at the end of each input text.\nvLLM Usage (recommended)\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ninput_texts = [t + \"<|endoftext|>\" for t in input_texts]\nmodel = LLM(\nmodel=\"Alibaba-NLP/E2Rank-0.6B\",\ntask=\"embed\",\noverride_pooler_config=PoolerConfig(pooling_type=\"LAST\", normalize=True)\n)\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.5958386659622192, 0.030148349702358246], [0.060259245336055756, 0.5595865249633789]]\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ninput_texts = [t + \"<|endoftext|>\" for t in input_texts]\ntokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/E2Rank-0.6B', padding_side='left')\nmodel = AutoModel.from_pretrained('Alibaba-NLP/E2Rank-0.6B')\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nwith torch.no_grad():\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.5950675010681152, 0.030417663976550102], [0.061970409005880356, 0.562691330909729]]\nReranking\nFor using E2Rank as a reranker, you only need to perform additional processing on the query by adding (part of) the docs that needs to be reranked to the listwise prompt, while the rest is the same as using the embedding model.\nvLLM Usage (recommended)\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\nmodel = LLM(\nmodel=\"./checkpoints/E2Rank-0.6B\",\ntask=\"embed\",\noverride_pooler_config=PoolerConfig(pooling_type=\"LAST\", normalize=True)\n)\ntokenizer = model.get_tokenizer()\ndef get_listwise_prompt(task_description: str, query: str, documents: list[str], num_input_docs: int = 20) -> str:\ninput_docs = documents[:num_input_docs]\ninput_docs = \"\\n\".join([f\"[{i}] {doc}\" for i, doc in enumerate(input_docs, start=1)])\nmessages = [{\n\"role\": \"user\",\n\"content\": f'{task_description}\\nDocuments:\\n{input_docs}Search Query:{query}'\n}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False,\n)\nreturn text\ntask = 'Given a web search query and some relevant documents, rerank the documents that answer the query:'\nqueries = [\n'What is the capital of China?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ndocuments = [doc + \"<|endoftext|>\" for doc in documents]\npseudo_queries = [\nget_listwise_prompt(task, queries[0], documents),\nget_listwise_prompt(task, queries[1], documents)\n]  # no need to add the EOS token here\ninput_texts = pseudo_queries + documents\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.8516960144042969, 0.24043934047222137], [0.33099934458732605, 0.7905282974243164]]\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('./checkpoints/E2Rank-0.6B', padding_side='left')\nmodel = AutoModel.from_pretrained('./checkpoints/E2Rank-0.6B')\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_listwise_prompt(task_description: str, query: str, documents: list[str], num_input_docs: int = 20) -> str:\ninput_docs = documents[:num_input_docs]\ninput_docs = \"\\n\".join([f\"[{i}] {doc}\" for i, doc in enumerate(input_docs, start=1)])\nmessages = [{\n\"role\": \"user\",\n\"content\": f'{task_description}\\nDocuments:\\n{input_docs}Search Query:{query}'\n}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False,\n)\nreturn text\ntask = 'Given a web search query and some relevant documents, rerank the documents that answer the query:'\nqueries = [\n'What is the capital of China?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ndocuments = [doc + \"<|endoftext|>\" for doc in documents]\npseudo_queries = [\nget_listwise_prompt(task, queries[0], documents),\nget_listwise_prompt(task, queries[1], documents)\n]  # no need to add the EOS token here\ninput_texts = pseudo_queries + documents\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nwith torch.no_grad():\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.8513513207435608, 0.24268491566181183], [0.33154672384262085, 0.7923378944396973]]\nEnd-to-end search\nSince E2Rank extends a single text embedding model to perform both high-quality retrieval and listwise reranking, you can directly use it to build an end-to-end search system. By reusing the embeddings computed during the retrieval stage, E2Rank only need to compute the pseudo query's embedding and can efficiently rerank the retrieved documents with minimal additional computational overhead.\nExample code is coming soon.\nðŸ“Š Evaluation\nReranking Benchmark\nBEIR\nCovid\nNFCorpus\nTouche\nDBPedia\nSciFact\nSignal\nNews\nRobust\nAvg.\nBM25\n59.47\n30.75\n44.22\n31.80\n67.89\n33.05\n39.52\n40.70\n43.43\nZero-shot Listwise Reranker\nRankGPT-4o\n83.41\n39.67\n32.26\n45.56\n77.41\n34.20\n51.92\n60.25\n53.09\nRankGPT-4o-mini\n80.03\n38.73\n30.91\n44.54\n73.14\n33.64\n50.91\n57.41\n51.16\nRankQwen3-14B\n84.45\n38.94\n38.30\n44.52\n78.64\n33.58\n51.24\n59.66\n53.67\nRankQwen3-32B\n83.48\n39.22\n37.13\n45.00\n78.22\n32.12\n51.08\n60.74\n53.37\nFine-tuned Listwise Reranker based on Qwen3\nRankQwen3-0.6B\n78.35\n36.41\n37.54\n39.19\n71.01\n30.96\n44.43\n46.31\n48.03\nRankQwen3-4B\n83.91\n39.88\n32.66\n43.91\n76.37\n32.15\n50.81\n59.36\n52.38\nRankQwen3-8B\n85.37\n40.05\n31.73\n45.44\n78.96\n32.48\n52.36\n60.72\n53.39\nOurs\nE2Rank-0.6B\n79.17\n38.60\n41.91\n41.96\n73.43\n35.26\n52.75\n53.67\n52.09\nE2Rank-4B\n83.30\n39.20\n43.16\n42.95\n77.19\n34.48\n52.71\n60.16\n54.14\nE2Rank-8B\n84.09\n39.08\n42.06\n43.44\n77.49\n34.01\n54.25\n60.34\n54.35\nEmbedding Benchmark\nMTEB (Eng, v1)\nModels\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\nInstructor-xl\n49.26\n57.29\n44.74\n86.62\n73.12\n83.06\n32.32\n61.79\nBGE-large-en-v1.5\n54.29\n60.03\n46.08\n87.12\n75.97\n83.11\n31.61\n64.23\nGritLM-7B\n53.10\n61.30\n48.90\n86.90\n77.00\n82.80\n29.40\n64.70\nE5-Mistral-7b-v1\n52.78\n60.38\n47.78\n88.47\n76.80\n83.77\n31.90\n64.56\nEcho-Mistral-7b-v1\n55.52\n58.14\n46.32\n87.34\n77.43\n82.56\n30.73\n64.68\nLLM2Vec-Mistral-7B\n55.99\n58.42\n45.54\n87.99\n76.63\n84.09\n29.96\n64.80\nLLM2Vec-Meta-LLaMA-3-8B\n56.63\n59.68\n46.45\n87.80\n75.92\n83.58\n30.94\n65.01\nE2Rank-0.6B\n51.74\n55.97\n40.85\n83.93\n73.66\n81.41\n30.90\n61.25\nE2Rank-4B\n55.33\n59.10\n44.27\n87.14\n77.08\n84.03\n30.06\n64.47\nE2Rank-8B\n56.89\n59.58\n44.75\n86.96\n76.81\n84.52\n30.23\n65.03\nNote: For baselines, we only compared with models that are trained using public datasets.\nðŸš© Citation\nIf this work is helpful, please kindly cite as:\n@misc{liu2025e2rank,\ntitle={E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker},\nauthor={Qi Liu and Yanzhao Zhang and Mingxin Li and Dingkun Long and Pengjun Xie and Jiaxin Mao},\nyear={2025},\neprint={2510.22733},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2510.22733},\n}\nIf you have any questions, feel free to contact us via qiliu6777[AT]gmail.com or create an issue.",
    "spacepxl/Wan2.1-VAE-upscale2x": "How to use (ComfyUI):\nHow to use (diffusers):\nTraining details\nSlowpics comparison: generated images, decoded with original vae vs 2x vae\nSlowpics comparison: highres fix over ultrasharp upscaler, vs highres fix over 2x decoder\nA decoder-only finetune of the Wan2.1 VAE, with 2x upscaling integrated directly into the decoder. The main purpose of this is to kill the dreaded wan speckles/polka dots/grain, but it's also convenient for highres fix workflows. The outputs of the 2x decoder are usually much better than what you would get by running the outputs of the original decoder through an image upscale model, and even better, it's effectively free, since the compute cost of decoding is virtually unchanged. If you don't want to use the extra resolution, a slight blur and downsample will give you an original resolution image with much higher quality than the original decoder can produce.\nIn particular, this VAE improves skin details and hair very significantly. It is trained almost exclusively on real images, so it may struggle with anime/lineart and text. It would be possible to finetune on anime/lineart, but I'm not aware of a suitable dataset that's licensed correctly and not just full of scraped media with massive copyright violations. If you know of an appropriate datset for this that is sourced from cc-by materials or similar, let me know and I'd be happy to try training it.\nThe first released version is trained on images only, and is compatible with both Wan and Qwen since they share the same latent space. A video version is planned, but video training is more complex than image training, so it will take some time.\nHow to use (ComfyUI):\nInstall these custom nodes, which allow loading and decoding latents in a native-compatible way. Wan wrapper latents can also be used if they are de-normalized using the correct wrapper node.\nhttps://github.com/spacepxl/ComfyUI-VAE-Utils\nHow to use (diffusers):\nimport torch\nimport torch.nn.functional as F\nfrom diffusers import AutoencoderKLWan\nvae = AutoencoderKLWan.from_pretrained(\"spacepxl/Wan2.1-VAE-upscale2x\", subfolder=\"diffusers/Wan2.1_VAE_upscale2x_imageonly_real_v1\")\ndecoder_out = vae.decode(latents, return_dict=False)[0] # [B, 12, F, H, W]\ndecoded_video = F.pixel_shuffle(decoder_out.movedim(2, 1), upscale_factor=2).movedim(1, 2) # pixel shuffle needs [..., C, H, W] format\n# or just use decoder_out.squeeze(2) for images to convert BCFHW to BCHW\nTraining details\nThe VAE is initialized from the pretrained Wan2.1 VAE. Encoder is unchanged, but the final Conv3d in the decoder is replaced to expand output channels from 3 -> 12. The key idea behind this change is that many image upscale models do all processing at the original resolution, then kick out a high res image through a pixel shuffle as the last layer. For training, the encoder is frozen to preserve the latent space, and only the decoder is trained.\nTraining uses a combination of L1 loss, LPIPS, FDL, and GAN loss:\nLPIPS is useful to regularize the other losses, but needs to be kept low to prevent the checkerboard/speckle artifacts that come from over reliance on it. This was probably the primary mistake made by the Wan team, too much LPIPS loss weight.\nFrequency Distribution Loss is also a perceptual loss using VGG features, but has very different properties from LPIPS, and encourages more realistic textures and local statistics. It's not a replacement, on its own it generates different artifacts, but together they work quite well. Not well enough to eliminate GAN, but close.\nFor the GAN loss, I used a patchgan discriminator with spectral norm, and non-saturating LSGAN loss. I tried to use R3GAN, but the gradient penalties are slow and memory intensive, and it didn't converge well for me anyway. The GAN loss weight is fairly high, to improve sharpness and realism, possibly even at the expense of metric accuracy.\nSpeaking of metrics, I have none to share. This is because the focus was on human perceptual quality, not metrics. Most common metrics used to report VAE reconstruction accuracy (PSNR/SSIM/LPIPS/etc) are very poorly aligned with human preference, so all tuning was done by eye instead. The result is that decoded images may have slight color shifts, and can sometimes be overly sharp. This can always be overcome by color matching and blur, so in my opinion this is an acceptable tradeoff to fix the metrically accurate, but subjectively poor quality of the original Wan VAE decoder.\nFinal training for the image-only model took about 40h on a single 5090, although that is not counting the dozens of test runs needed to dial in loss functions and data preprocessing.\nbase resolution: 256 (upscaled to 512)\nbatch size: 4\ntotal steps: 300k\nIn addition to all that, there is one other critical piece to improve decoder quality: latent degradation. If you train a VAE only to reconstruct real encoder latents, it can still struggle with latents generated by the diffusion model. This is because diffusion models usually fail to generate the highest frequency information in the latent space. In other words, they have a low-frequency bias. This is probably caused by the use of only MSE loss in diffusion/flow training, which is well known in image model training to have a low frequency bias. One piece of evidence to support this theory is that distilled models which use distribution matching and/or adversarial loss functions in the distillation process, like DMD2 (lightning) distillation, tend to improve the quality of fine details compared to the base diffusion model.\nThere is probably a whole branch of ideas to explore on the side of diffusion model training, like incorporating latent perceptual or GAN loss into the diffusion training process, but another option is to just make the VAE decoder robust against the types of degradations that are generated by the diffusion model. This is what I did.\nTo simulate generated latents, you can simply add noise to real latents, and use the diffusion model to predict the clean latents in one or a few steps, using an empty or negative caption. I've seen this referred to in papers as \"SD-edit degradation\", aka img2img (or vid2vid). Crucially, the degradation is NOT noise. LTXV tried to train the decoder with noisy latents to fix this problem, and it did not work. If you inspect real vs generated latents, it's clear that the generated latents are softer than the real ones, not noisy.\nFor my purposes, I didn't want to keep a diffusion model loaded during VAE training, so I trained a small convolution-only model to match the sdedit degradations generated by Wan2.1-1.3b, since it tends to generate lower quality images than the 14b models. I simply used the preset negative caption, and trained with random timesteps on a lognorm distribution centered around ~0.2. The convolution model is also conditioned on timestep, so it learns to simulate whatever degradations the diffusion model produces towards the end of the generation process. This is mostly like blurring, but it's much easier to just learn it directly instead of trying to manually tune gaussian blur filters to match. It also doesn't need to be perfect, since the whole purpose is to make the encoded latents less accurate.\nThe degradation proxy model is trained first, and kept frozen during VAE training, with timesteps sampled randomly in the 0 to 0.12 range to approximate the typical level of degradations in generated images. The result of degrading the latents during VAE decoder training is that the decoder becomes more robust at decoding plausible details from generated latents, instead of producing artifacts like the uniform speckle grid of the original decoder. Without this, the decoder learns to reconstruct good details from encoded latents, but still produces artifacts sometimes on generated latents.",
    "Retreatcost/Impish-LongPen-12B": "Impish-LongPen-12B\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nImpish-LongPen-12B\nA karcher merge of Sicarius-Prototyping/Impish_Longtail_12B and SuperbEmphasis/MN-12b-RP-Ink-RP-Longform used in KansenSakura-Erosion-RP-12b\nBut with better quality.\nThe merge itself took long ass time, probably not going to repeat similar experiments.\nExpect more experimental models in the meantime.\nThis is a merge of pre-trained language models created using mergekit.\nMerge Details\nMerge Method\nThis model was merged using the Karcher Mean merge method.\nModels Merged\nThe following models were included in the merge:\nSuperbEmphasis/MN-12b-RP-Ink-RP-Longform\nSicarius-Prototyping/Impish_Longtail_12B\nConfiguration\nThe following YAML configuration was used to produce this model:\nmerge_method: karcher\nmodels:\n- model: SuperbEmphasis/MN-12b-RP-Ink-RP-Longform\n- model: Sicarius-Prototyping/Impish_Longtail_12B\nparameters:\nmax_iter: 100000\ntol: 1e-9\ndtype: bfloat16",
    "prithivMLmods/Kontext-Watermark-Remover": "Kontext-Watermark-Remover\nSample Inferences : Demo\nRequired Packages\nQuick start with diffusersðŸ§¨\nRun Demo\nParameter Settings\nTraining Parameters\nLabel Parameters\nAdvanced Parameters\nSample Inferences : Inference Providers\nTrigger words\nDownload model\nKontext-Watermark-Remover\nThe Kontext-Watermark-Remover is an adapter for black-forest-lab's FLUX.1-Kontext-dev, designed to precisely remove watermarks and textual content from images while maintaining the original image quality and context. The model was trained on 150 image pairs (75 start images and 75 end images) to ensure accurate and artifact-free watermark removal.\n[photo content], remove any watermark text or logos from the image while preserving the background, texture, lighting, and overall realism. Ensure the edited areas blend seamlessly with surrounding details, leaving no visible traces of watermark removal.\nSample Inferences : Demo\nQuick start with diffusersðŸ§¨\nRequired Packages\n!pip install diffusers torch gradio transformers\n!pip install pillow gradio-imageslider huggingface_hub\n!pip install sentencepiece spaces peft torchvision accelerate\nRun Demo\nimport os\nimport gradio as gr\nimport numpy as np\nimport spaces\nimport torch\nimport random\nfrom PIL import Image\nfrom typing import Iterable\nfrom diffusers import FluxKontextPipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import hf_hub_download\nfrom gradio_imageslider import ImageSlider\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# --- Main Model Initialization ---\nMAX_SEED = np.iinfo(np.int32).max\npipe = FluxKontextPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Kontext-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\n# --- Load New Adapter ---\npipe.load_lora_weights(\"prithivMLmods/Kontext-Watermark-Remover\", weight_name=\"Kontext-Watermark-Remover.safetensors\", adapter_name=\"watermark_remover\")\n@spaces.GPU\ndef infer(input_image, prompt, seed=42, randomize_seed=False, guidance_scale=2.5, steps=28, progress=gr.Progress(track_tqdm=True)):\n\"\"\"\nPerform image editing, returning a pair for the ImageSlider.\n\"\"\"\nif not input_image:\nraise gr.Error(\"Please upload an image for editing.\")\npipe.set_adapters([\"watermark_remover\"], adapter_weights=[1.0])\nif randomize_seed:\nseed = random.randint(0, MAX_SEED)\noriginal_image = input_image.copy().convert(\"RGB\")\nimage = pipe(\nimage=original_image,\nprompt=prompt,\nguidance_scale=guidance_scale,\nwidth = original_image.size[0],\nheight = original_image.size[1],\nnum_inference_steps=steps,\ngenerator=torch.Generator().manual_seed(seed),\n).images[0]\nreturn (original_image, image), seed, gr.Button(visible=True)\ncss=\"\"\"\n#col-container {\nmargin: 0 auto;\nmax-width: 960px;\n}\n#main-title h1 {font-size: 2.1em !important;}\n\"\"\"\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem_id=\"col-container\"):\ngr.Markdown(\"# **Photo-Mate-i2i: Watermark Remover**\", elem_id=\"main-title\")\ngr.Markdown(\"Image manipulation with FLUX.1 Kontext. This demo focuses on watermark removal.\")\nwith gr.Row():\nwith gr.Column():\ninput_image = gr.Image(label=\"Upload Image with Watermark\", type=\"pil\", height=\"300\")\nwith gr.Row():\nprompt = gr.Text(\nlabel=\"Edit Prompt\",\nshow_label=False,\nmax_lines=1,\nplaceholder=\"e.g., 'Remove the watermark'\",\ncontainer=False,\nvalue=\"[photo content], remove any watermark text or logos from the image while preserving the background, texture, lighting, and overall realism. Ensure the edited areas blend seamlessly with surrounding details, leaving no visible traces of watermark removal.\"\n)\nrun_button = gr.Button(\"Run\", variant=\"primary\", scale=0)\nwith gr.Accordion(\"Advanced Settings\", open=False):\nseed = gr.Slider(\nlabel=\"Seed\",\nminimum=0,\nmaximum=MAX_SEED,\nstep=1,\nvalue=0,\n)\nrandomize_seed = gr.Checkbox(label=\"Randomize seed\", value=True)\nguidance_scale = gr.Slider(\nlabel=\"Guidance Scale\",\nminimum=1,\nmaximum=10,\nstep=0.1,\nvalue=2.5,\n)\nsteps = gr.Slider(\nlabel=\"Steps\",\nminimum=1,\nmaximum=30,\nvalue=28,\nstep=1\n)\nwith gr.Column():\noutput_slider = ImageSlider(label=\"Before / After\", show_label=False, interactive=False)\nreuse_button = gr.Button(\"Reuse this image\", visible=False)\ngr.on(\ntriggers=[run_button.click, prompt.submit],\nfn=infer,\ninputs=[input_image, prompt, seed, randomize_seed, guidance_scale, steps],\noutputs=[output_slider, seed, reuse_button]\n)\nreuse_button.click(\nfn=lambda images: images[1] if isinstance(images, (list, tuple)) and len(images) > 1 else images,\ninputs=[output_slider],\noutputs=[input_image]\n)\ndemo.launch(mcp_server=True, ssr_mode=False, show_error=True)\nParameter Settings\nSetting\nValue\nModule Type\nAdapter\nBase Model\nFLUX.1 Kontext Dev - fp8\nTrigger Words\n[photo content], remove any watermark text or logos from the image while preserving the background, texture, lighting, and overall realism. Ensure the edited areas blend seamlessly with surrounding details, leaving no visible traces of watermark removal.\nImage Processing Repeats\n50\nEpochs\n25\nSave Every N Epochs\n1\nLabeling: florence-community/Florence-2-large-ft (natural language & English)\nTotal Images Used for Training : 150 Image Pairs (75 Start, 75 End)\nTraining Parameters\nSetting\nValue\nSeed\n-\nClip Skip\n-\nText Encoder LR\n0.00001\nUNet LR\n0.00005\nLR Scheduler\nconstant\nOptimizer\nAdamW8bit\nNetwork Dimension\n64\nNetwork Alpha\n32\nGradient Accumulation Steps\n-\nLabel Parameters\nSetting\nValue\nShuffle Caption\n-\nKeep N Tokens\n-\nAdvanced Parameters\nSetting\nValue\nNoise Offset\n0.03\nMultires Noise Discount\n0.1\nMultires Noise Iterations\n10\nConv Dimension\n-\nConv Alpha\n-\nBatch Size\n-\nSteps\n2900 & 400(warm up)\nSampler\neuler\nSample Inferences : Inference Providers\nTrigger words\nYou should use [photo content] to trigger the image generation.\nYou should use remove any watermark text or logos from the image while preserving the background to trigger the image generation.\nYou should use texture to trigger the image generation.\nYou should use lighting to trigger the image generation.\nYou should use and overall realism. Ensure the edited areas blend seamlessly with surrounding details to trigger the image generation.\nYou should use leaving no visible traces of watermark removal. to trigger the image generation.\nDownload model\nDownload them in the Files & versions tab.",
    "mlx-community/MiniMax-M2-4bit": "mlx-community/MiniMax-M2-4bit\nUse with mlx\nmlx-community/MiniMax-M2-4bit\nThis model mlx-community/MiniMax-M2-4bit was\nconverted to MLX format from MiniMaxAI/MiniMax-M2\nusing mlx-lm version 0.28.4.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/MiniMax-M2-4bit\")\nprompt = \"hello\"\nif tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "google-bert/bert-base-chinese": "Bert-base-chinese\nTable of Contents\nModel Details\nModel Description\nModel Sources\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nHow to Get Started With the Model\nBert-base-chinese\nTable of Contents\nModel Details\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nHow to Get Started With the Model\nModel Details\nModel Description\nThis model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).\nDeveloped by: Google\nModel Type: Fill-Mask\nLanguage(s): Chinese\nLicense: Apache 2.0\nParent Model: See the BERT base uncased model for more information about the BERT base model.\nModel Sources\nGitHub repo: https://github.com/google-research/bert/blob/master/multilingual.md\nPaper: BERT\nUses\nDirect Use\nThis model can be used for masked language modeling\nRisks, Limitations and Biases\nCONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nTraining\nTraining Procedure\ntype_vocab_size: 2\nvocab_size: 21128\nnum_hidden_layers: 12\nTraining Data\n[More Information Needed]\nEvaluation\nResults\n[More Information Needed]\nHow to Get Started With the Model\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")",
    "google-bert/bert-base-multilingual-cased": "BERT multilingual base model (cased)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nBibTeX entry and citation info\nBERT multilingual base model (cased)\nPretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.\nIt was introduced in this paper and first released in\nthis repository. This model is case sensitive: it makes a difference\nbetween english and English.\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\nModel description\nBERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\nMasked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\nthe entire masked sentence through the model and has to predict the masked words. This is different from traditional\nrecurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\nGPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\nsentence.\nNext sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\nthey correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\npredict if the two sentences were following each other or not.\nThis way, the model learns an inner representation of the languages in the training set that can then be used to\nextract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a\nstandard classifier using the features produced by the BERT model as inputs.\nIntended uses & limitations\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the model hub to look for\nfine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] Hello I'm a model model. [SEP]\",\n'score': 0.10182085633277893,\n'token': 13192,\n'token_str': 'model'},\n{'sequence': \"[CLS] Hello I'm a world model. [SEP]\",\n'score': 0.052126359194517136,\n'token': 11356,\n'token_str': 'world'},\n{'sequence': \"[CLS] Hello I'm a data model. [SEP]\",\n'score': 0.048930276185274124,\n'token': 11165,\n'token_str': 'data'},\n{'sequence': \"[CLS] Hello I'm a flight model. [SEP]\",\n'score': 0.02036019042134285,\n'token': 23578,\n'token_str': 'flight'},\n{'sequence': \"[CLS] Hello I'm a business model. [SEP]\",\n'score': 0.020079681649804115,\n'token': 14155,\n'token_str': 'business'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nTraining data\nThe BERT model was pretrained on the 104 languages with the largest Wikipedias. You can find the complete list\nhere.\nTraining procedure\nPreprocessing\nThe texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a\nlarger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese,\nJapanese Kanji and Korean Hanja that don't have space, a CJK Unicode block is added around every character.\nThe inputs of the model are then of the form:\n[CLS] Sentence A [SEP] Sentence B [SEP]\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1810-04805,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njournal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/abs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "google-t5/t5-small": "Model Card for T5 Small\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card Authors\nHow to Get Started with the Model\nModel Card for T5 Small\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nHow To Get Started With the Model\nModel Details\nModel Description\nThe developers of the Text-To-Text Transfer Transformer (T5) write:\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\nT5-Small is the checkpoint with 60 million parameters.\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\nModel type: Language model\nLanguage(s) (NLP): English, French, Romanian, German\nLicense: Apache 2.0\nRelated Models: All T5 Checkpoints\nResources for more information:\nResearch paper\nGoogle's T5 Blog Post\nGitHub Repo\nHugging Face T5 Docs\nUses\nDirect Use and Downstream Use\nThe developers write in a blog post that the model:\nOur text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\nSee the blog post and research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nMore information needed.\nRecommendations\nMore information needed.\nTraining Details\nTraining Data\nThe model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\nThe model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.).\nThereby, the following datasets were being used for (1.) and (2.):\nDatasets used for Unsupervised denoising objective:\nC4\nWiki-DPR\nDatasets used for Supervised text-to-text language modeling objective\nSentence acceptability judgment\nCoLA Warstadt et al., 2018\nSentiment analysis\nSST-2 Socher et al., 2013\nParaphrasing/sentence similarity\nMRPC Dolan and Brockett, 2005\nSTS-B Ceret al., 2017\nQQP Iyer et al., 2017\nNatural language inference\nMNLI Williams et al., 2017\nQNLI Rajpurkar et al.,2016\nRTE Dagan et al., 2005\nCB De Marneff et al., 2019\nSentence completion\nCOPA Roemmele et al., 2011\nWord sense disambiguation\nWIC Pilehvar and Camacho-Collados, 2018\nQuestion answering\nMultiRC Khashabi et al., 2018\nReCoRD Zhang et al., 2018\nBoolQ Clark et al., 2019\nTraining Procedure\nIn their abstract, the model developers write:\nIn this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the research paper for further details.\nEvaluation\nTesting Data, Factors & Metrics\nThe developers evaluated the model on 24 tasks, see the research paper for full details.\nResults\nFor full results for T5-small, see the research paper, Table 14.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@article{2020t5,\nauthor  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\ntitle   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\njournal = {Journal of Machine Learning Research},\nyear    = {2020},\nvolume  = {21},\nnumber  = {140},\npages   = {1-67},\nurl     = {http://jmlr.org/papers/v21/20-074.html}\n}\nAPA:\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\nModel Card Authors\nThis model card was written by the team at Hugging Face.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nClick to expand\nfrom transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5Model.from_pretrained(\"t5-small\")\ninput_ids = tokenizer(\n\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n).input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\nSee the Hugging Face T5 docs and a Colab Notebook created by the model developers for more examples.",
    "ProsusAI/finbert": "FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper FinBERT: Financial Sentiment Analysis with Pre-trained Language Models and our related blog post on Medium.\nThe model will give softmax outputs for three labels: positive, negative or neutral.\nAbout Prosus\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\nContact information\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.",
    "facebook/bart-large-mnli": "bart-large-mnli\nNLI-based Zero Shot Text Classification\nbart-large-mnli\nThis is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset.\nAdditional information about this model:\nThe bart-large model page\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nBART fairseq implementation\nNLI-based Zero Shot Text Classification\nYin et al. proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class \"politics\", we could construct a hypothesis of This text is about politics.. The probabilities for entailment and contradiction are then converted to label probabilities.\nThis method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See this blog post for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.\nWith the zero-shot classification pipeline\nThe model can be loaded with the zero-shot-classification pipeline like so:\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\",\nmodel=\"facebook/bart-large-mnli\")\nYou can then use this pipeline to classify sequences into any of the class names you specify.\nsequence_to_classify = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)\n#{'labels': ['travel', 'dancing', 'cooking'],\n# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n# 'sequence': 'one day I will see the world'}\nIf more than one candidate label can be correct, pass multi_label=True to calculate each class independently:\ncandidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\nclassifier(sequence_to_classify, candidate_labels, multi_label=True)\n#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n# 'scores': [0.9945111274719238,\n#  0.9383890628814697,\n#  0.0057061901316046715,\n#  0.0018193122232332826],\n# 'sequence': 'one day I will see the world'}\nWith manual PyTorch\n# pose sequence as a NLI premise and label as a hypothesis\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\npremise = sequence\nhypothesis = f'This example is {label}.'\n# run through model pre-trained on MNLI\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt',\ntruncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\n# we throw away \"neutral\" (dim 1) and take the probability of\n# \"entailment\" (2) as the probability of the label being true\nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]",
    "facebook/mbart-large-50-many-to-many-mmt": "mBART-50 many to many multilingual machine translation\nLanguages covered\nBibTeX entry and citation info\nmBART-50 many to many multilingual machine translation\nThis model is a fine-tuned checkpoint of mBART-large-50. mbart-large-50-many-to-many-mmt is fine-tuned for multilingual machine translation. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.\nThe model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_hi = \"à¤¸à¤‚à¤¯à¥à¤•à¥à¤¤ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤•à¥‡ à¤ªà¥à¤°à¤®à¥à¤– à¤•à¤¾ à¤•à¤¹à¤¨à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤¸à¥€à¤°à¤¿à¤¯à¤¾ à¤®à¥‡à¤‚ à¤•à¥‹à¤ˆ à¤¸à¥ˆà¤¨à¥à¤¯ à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ\"\narticle_ar = \"Ø§Ù„Ø£Ù…ÙŠÙ† Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ù„ Ø¹Ø³ÙƒØ±ÙŠ ÙÙŠ Ø³ÙˆØ±ÙŠØ§.\"\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n# translate Hindi to French\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(\n**encoded_hi,\nforced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire dans la Syrie.\"\n# translate Arabic to English\ntokenizer.src_lang = \"ar_AR\"\nencoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\ngenerated_tokens = model.generate(\n**encoded_ar,\nforced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"The Secretary-General of the United Nations says there is no military solution in Syria.\"\nSee the model hub to look for more fine-tuned versions.\nLanguages covered\nArabic (ar_AR), Czech (cs_CZ), German (de_DE), English (en_XX), Spanish (es_XX), Estonian (et_EE), Finnish (fi_FI), French (fr_XX), Gujarati (gu_IN), Hindi (hi_IN), Italian (it_IT), Japanese (ja_XX), Kazakh (kk_KZ), Korean (ko_KR), Lithuanian (lt_LT), Latvian (lv_LV), Burmese (my_MM), Nepali (ne_NP), Dutch (nl_XX), Romanian (ro_RO), Russian (ru_RU), Sinhala (si_LK), Turkish (tr_TR), Vietnamese (vi_VN), Chinese (zh_CN), Afrikaans (af_ZA), Azerbaijani (az_AZ), Bengali (bn_IN), Persian (fa_IR), Hebrew (he_IL), Croatian (hr_HR), Indonesian (id_ID), Georgian (ka_GE), Khmer (km_KH), Macedonian (mk_MK), Malayalam (ml_IN), Mongolian (mn_MN), Marathi (mr_IN), Polish (pl_PL), Pashto (ps_AF), Portuguese (pt_XX), Swedish (sv_SE), Swahili (sw_KE), Tamil (ta_IN), Telugu (te_IN), Thai (th_TH), Tagalog (tl_XX), Ukrainian (uk_UA), Urdu (ur_PK), Xhosa (xh_ZA), Galician (gl_ES), Slovene (sl_SI)\nBibTeX entry and citation info\n@article{tang2020multilingual,\ntitle={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\nauthor={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\nyear={2020},\neprint={2008.00401},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "microsoft/deberta-v3-large": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nCitation\nDeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nDeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data.\nIn DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.\nPlease check the official repository for more implementation details and updates.\nThe DeBERTa V3 large model comes with 24 layers and a hidden size of 1024. It has 304M backbone parameters  with a vocabulary containing 128K tokens which introduces 131M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\nFine-tuning on NLU tasks\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\nModel\nVocabulary(K)\nBackbone #Params(M)\nSQuAD 2.0(F1/EM)\nMNLI-m/mm(ACC)\nRoBERTa-large\n50\n304\n89.4/86.5\n90.2\nXLNet-large\n32\n-\n90.6/87.9\n90.8\nDeBERTa-large\n50\n-\n90.7/88.0\n91.3\nDeBERTa-v3-large\n128\n304\n91.5/89.0\n91.8/91.9\nFine-tuning with HF transformers\n#!/bin/bash\ncd transformers/examples/pytorch/text-classification/\npip install datasets\nexport TASK_NAME=mnli\noutput_dir=\"ds_results\"\nnum_gpus=8\nbatch_size=8\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\nrun_glue.py \\\n--model_name_or_path microsoft/deberta-v3-large \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--evaluation_strategy steps \\\n--max_seq_length 256 \\\n--warmup_steps 50 \\\n--per_device_train_batch_size ${batch_size} \\\n--learning_rate 6e-6 \\\n--num_train_epochs 2 \\\n--output_dir $output_dir \\\n--overwrite_output_dir \\\n--logging_steps 1000 \\\n--logging_dir $output_dir\nCitation\nIf you find DeBERTa useful for your work, please cite the following papers:\n@misc{he2021debertav3,\ntitle={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},\nauthor={Pengcheng He and Jianfeng Gao and Weizhu Chen},\nyear={2021},\neprint={2111.09543},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}",
    "openai/clip-vit-large-patch14": "Model Card: CLIP\nModel Details\nModel Date\nModel Type\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nOut-of-Scope Use Cases\nData\nData Mission Statement\nPerformance and Limitations\nPerformance\nLimitations\nBias and Fairness\nFeedback\nWhere to send questions or comments about the model\nModel Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.\nModel Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context theyâ€™re being deployed within.\nModel Date\nJanuary 2021\nModel Type\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\nDocuments\nBlog Post\nCLIP Paper\nUse with Transformers\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nOut-of-Scope Use Cases\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPâ€™s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\nData\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as YFCC100M. A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\nData Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\nPerformance and Limitations\nPerformance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanford Cars\nFGVC Aircraft\nVOC2007\nDTD\nOxford-IIIT Pet dataset\nCaltech101\nFlowers102\nMNIST\nSVHN\nIIIT5K\nHateful Memes\nSST-2\nUCF101\nKinetics700\nCountry211\nCLEVR Counting\nKITTI Distance\nSTL-10\nRareAct\nFlickr30\nMSCOCO\nImageNet\nImageNet-A\nImageNet-R\nImageNet Sketch\nObjectNet (ImageNet Overlap)\nYoutube-BB\nImageNet-Vid\nLimitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\nBias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from Fairface into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with â€˜Middle Easternâ€™ having the highest accuracy (98.4%) and â€˜Whiteâ€™ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\nFeedback\nWhere to send questions or comments about the model\nPlease use this Google Form",
    "microsoft/resnet-50": "ResNet-50 v1.5\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nResNet-50 v1.5\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al.\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\ninputs = processor(image, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\nFor more code examples, we refer to the documentation.\nBibTeX entry and citation info\n@inproceedings{he2016deep,\ntitle={Deep residual learning for image recognition},\nauthor={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\nbooktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\npages={770--778},\nyear={2016}\n}",
    "SamLowe/roberta-base-go_emotions": "Overview\nONNX version also available\nDataset used for the model\nHow the model was created\nInference\nEvaluation / metrics\nSummary\nCommentary on the dataset\nOverview\nModel trained from roberta-base on the go_emotions dataset for multi-label classification.\nONNX version also available\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx. These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\nDataset used for the model\ngo_emotions is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.\nHow the model was created\nThe model was trained using AutoModelForSequenceClassification.from_pretrained with problem_type=\"multi_label_classification\" for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.\nInference\nThere are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\nfrom transformers import pipeline\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\nsentences = [\"I am not having a great day\"]\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\nEvaluation / metrics\nEvaluation of the model is available at\nhttps://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb\nSummary\nAs provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:\nAccuracy: 0.474\nPrecision: 0.575\nRecall: 0.396\nF1: 0.450\nBut the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.\nWith a threshold of 0.5 applied to binarize the model outputs, as per the above notebook, the metrics per label are:\naccuracy\nprecision\nrecall\nf1\nmcc\nsupport\nthreshold\nadmiration\n0.946\n0.725\n0.675\n0.699\n0.670\n504\n0.5\namusement\n0.982\n0.790\n0.871\n0.829\n0.821\n264\n0.5\nanger\n0.970\n0.652\n0.379\n0.479\n0.483\n198\n0.5\nannoyance\n0.940\n0.472\n0.159\n0.238\n0.250\n320\n0.5\napproval\n0.942\n0.609\n0.302\n0.404\n0.403\n351\n0.5\ncaring\n0.973\n0.448\n0.319\n0.372\n0.364\n135\n0.5\nconfusion\n0.972\n0.500\n0.431\n0.463\n0.450\n153\n0.5\ncuriosity\n0.950\n0.537\n0.356\n0.428\n0.412\n284\n0.5\ndesire\n0.987\n0.630\n0.410\n0.496\n0.502\n83\n0.5\ndisappointment\n0.974\n0.625\n0.199\n0.302\n0.343\n151\n0.5\ndisapproval\n0.950\n0.494\n0.307\n0.379\n0.365\n267\n0.5\ndisgust\n0.982\n0.707\n0.333\n0.453\n0.478\n123\n0.5\nembarrassment\n0.994\n0.750\n0.243\n0.367\n0.425\n37\n0.5\nexcitement\n0.983\n0.603\n0.340\n0.435\n0.445\n103\n0.5\nfear\n0.992\n0.758\n0.603\n0.671\n0.672\n78\n0.5\ngratitude\n0.990\n0.960\n0.881\n0.919\n0.914\n352\n0.5\ngrief\n0.999\n0.000\n0.000\n0.000\n0.000\n6\n0.5\njoy\n0.978\n0.647\n0.559\n0.600\n0.590\n161\n0.5\nlove\n0.982\n0.773\n0.832\n0.802\n0.793\n238\n0.5\nnervousness\n0.996\n0.600\n0.130\n0.214\n0.278\n23\n0.5\noptimism\n0.972\n0.667\n0.376\n0.481\n0.488\n186\n0.5\npride\n0.997\n0.000\n0.000\n0.000\n0.000\n16\n0.5\nrealization\n0.974\n0.541\n0.138\n0.220\n0.264\n145\n0.5\nrelief\n0.998\n0.000\n0.000\n0.000\n0.000\n11\n0.5\nremorse\n0.991\n0.553\n0.750\n0.636\n0.640\n56\n0.5\nsadness\n0.977\n0.621\n0.494\n0.550\n0.542\n156\n0.5\nsurprise\n0.981\n0.750\n0.404\n0.525\n0.542\n141\n0.5\nneutral\n0.782\n0.694\n0.604\n0.646\n0.492\n1787\n0.5\nOptimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1 (how this was done is shown in the above notebook):\naccuracy\nprecision\nrecall\nf1\nmcc\nsupport\nthreshold\nadmiration\n0.940\n0.651\n0.776\n0.708\n0.678\n504\n0.25\namusement\n0.982\n0.781\n0.890\n0.832\n0.825\n264\n0.45\nanger\n0.959\n0.454\n0.601\n0.517\n0.502\n198\n0.15\nannoyance\n0.864\n0.243\n0.619\n0.349\n0.328\n320\n0.10\napproval\n0.926\n0.432\n0.442\n0.437\n0.397\n351\n0.30\ncaring\n0.972\n0.426\n0.385\n0.405\n0.391\n135\n0.40\nconfusion\n0.974\n0.548\n0.412\n0.470\n0.462\n153\n0.55\ncuriosity\n0.943\n0.473\n0.711\n0.568\n0.552\n284\n0.25\ndesire\n0.985\n0.518\n0.530\n0.524\n0.516\n83\n0.25\ndisappointment\n0.974\n0.562\n0.298\n0.390\n0.398\n151\n0.40\ndisapproval\n0.941\n0.414\n0.468\n0.439\n0.409\n267\n0.30\ndisgust\n0.978\n0.523\n0.463\n0.491\n0.481\n123\n0.20\nembarrassment\n0.994\n0.567\n0.459\n0.507\n0.507\n37\n0.10\nexcitement\n0.981\n0.500\n0.417\n0.455\n0.447\n103\n0.35\nfear\n0.991\n0.712\n0.667\n0.689\n0.685\n78\n0.40\ngratitude\n0.990\n0.957\n0.889\n0.922\n0.917\n352\n0.45\ngrief\n0.999\n0.333\n0.333\n0.333\n0.333\n6\n0.05\njoy\n0.978\n0.623\n0.646\n0.634\n0.623\n161\n0.40\nlove\n0.982\n0.740\n0.899\n0.812\n0.807\n238\n0.25\nnervousness\n0.996\n0.571\n0.348\n0.432\n0.444\n23\n0.25\noptimism\n0.971\n0.580\n0.565\n0.572\n0.557\n186\n0.20\npride\n0.998\n0.875\n0.438\n0.583\n0.618\n16\n0.10\nrealization\n0.961\n0.270\n0.262\n0.266\n0.246\n145\n0.15\nrelief\n0.992\n0.152\n0.636\n0.246\n0.309\n11\n0.05\nremorse\n0.991\n0.541\n0.946\n0.688\n0.712\n56\n0.10\nsadness\n0.977\n0.599\n0.583\n0.591\n0.579\n156\n0.40\nsurprise\n0.977\n0.543\n0.674\n0.601\n0.593\n141\n0.15\nneutral\n0.758\n0.598\n0.810\n0.688\n0.513\n1787\n0.25\nThis improves the overall metrics:\nPrecision: 0.542\nRecall: 0.577\nF1: 0.541\nOr if calculated weighted by the relative size of the support of each label:\nPrecision: 0.572\nRecall: 0.677\nF1: 0.611\nCommentary on the dataset\nSome labels (E.g. gratitude) when considered independently perform very strongly with F1 exceeding 0.9, whilst others (E.g. relief) perform very poorly.\nThis is a challenging dataset. Labels such as relief do have much fewer examples in the training data (less than 100 out of the 40k+, and only 11 in the test split).\nBut there is also some ambiguity and/or labelling errors visible in the training data of go_emotions that is suspected to constrain the performance. Data cleaning on the dataset to reduce some of the mistakes, ambiguity, conflicts and duplication in the labelling would produce a higher performing model.",
    "openai/whisper-large-v2": "Whisper\nModel details\nUsage\nTranscription\nEnglish to English\nFrench to French\nTranslation\nFrench to English\nEvaluation\nLong-Form Transcription\nFine-Tuning\nEvaluated Use\nTraining Data\nPerformance and Limitations\nBroader Implications\nBibTeX entry and citation info\nWhisper\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours\nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need\nfor fine-tuning.\nWhisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision\nby Alec Radford et al. from OpenAI. The original code repository can be found here.\nCompared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization\nfor improved performance.\nDisclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were\ncopied and pasted from the original model card.\nModel details\nWhisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.\nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.\nThe models were trained on either English-only data or multilingual data. The English-only models were trained\non the task of speech recognition. The multilingual models were trained on both speech recognition and speech\ntranslation. For speech recognition, the model predicts transcriptions in the same language as the audio.\nFor speech translation, the model predicts transcriptions to a different language to the audio.\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints\nare available on the Hugging Face Hub. The\ncheckpoints are summarised in the following table with links to the models on the Hub:\nSize\nParameters\nEnglish-only\nMultilingual\ntiny\n39 M\nâœ“\nâœ“\nbase\n74 M\nâœ“\nâœ“\nsmall\n244 M\nâœ“\nâœ“\nmedium\n769 M\nâœ“\nâœ“\nlarge\n1550 M\nx\nâœ“\nlarge-v2\n1550 M\nx\nâœ“\nUsage\nTo transcribe audio samples, the model has to be used alongside a WhisperProcessor.\nThe WhisperProcessor is used to:\nPre-process the audio inputs (converting them to log-Mel spectrograms for the model)\nPost-process the model outputs (converting them from tokens to text)\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens\nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\nThe transcription always starts with the <|startoftranscript|> token\nThe second token is the language token (e.g. <|en|> for English)\nThe third token is the \"task token\". It can take one of two values: <|transcribe|> for speech recognition or <|translate|> for speech translation\nIn addition, a <|notimestamps|> token is added if the model should not include timestamp prediction\nThus, a typical sequence of context tokens might look as follows:\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at\neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced,\nthe Whisper model will automatically predict the output langauge and task itself.\nThe context tokens can be set accordingly:\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\nWhich forces the model to predict in English under the task of speech recognition.\nTranscription\nEnglish to English\nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n>>> model.config.forced_decoder_ids = None\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\nThe context tokens can be removed from the start of the transcription by setting skip_special_tokens=True.\nFrench to French\nThe following example demonstrates French to French transcription by setting the decoder ids appropriately.\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.']\nTranslation\nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\nFrench to English\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\nEvaluation\nThis code snippet shows how to evaluate Whisper Large on LibriSpeech test-clean:\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\").to(\"cuda\")\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>>\n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n>>> result = librispeech_test_clean.map(map_to_pred)\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n3.0003583080317572\nLong-Form Transcription\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking\nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers\npipeline\nmethod. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline\ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing return_timestamps=True:\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-large-v2\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n'timestamp': (0.0, 5.44)}]\nRefer to the blog post ASR Chunking for more details on the chunking algorithm.\nFine-Tuning\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However,\nits predictive capabilities can be improved further for certain languages and tasks through fine-tuning. The blog\npost Fine-Tune Whisper with ðŸ¤— Transformers provides a step-by-step\nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\nEvaluated Use\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\nTraining Data\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.\nAs discussed in the accompanying paper, we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\nPerformance and Limitations\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in the paper accompanying this release.\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in the paper. It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\nBroader Implications\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\nBibTeX entry and citation info\n@misc{radford2022whisper,\ndoi = {10.48550/ARXIV.2212.04356},\nurl = {https://arxiv.org/abs/2212.04356},\nauthor = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\ntitle = {Robust Speech Recognition via Large-Scale Weak Supervision},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "Salesforce/blip-vqa-base": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\nTL;DR\nUsage\nUsing the Pytorch model\nEthical Considerations\nBibTex and citation info\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\nModel card for BLIP trained on visual question answering- base architecture (with ViT base backbone).\nPull figure from BLIP official repo\nTL;DR\nAuthors from the paper write in the abstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.\nUsage\nYou can use this model for conditional and un-conditional image captioning\nUsing the Pytorch model\nRunning the model on CPU\nClick to expand\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> 1\nRunning the model on GPU\nIn full precision\nClick to expand\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(\"cuda\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> 1\nIn half precision (float16)\nClick to expand\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(\"ybelkada/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"ybelkada/blip-vqa-base\", torch_dtype=torch.float16).to(\"cuda\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> 1\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peopleâ€™s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nBibTex and citation info\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\ndoi = {10.48550/ARXIV.2201.12086},\nurl = {https://arxiv.org/abs/2201.12086},\nauthor = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\nkeywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}",
    "QuixiAI/WizardLM-13B-Uncensored": "This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.\nShout out to the open source AI/ML community, and everyone who helped me out.\nNote:An uncensored model has no guardrails.You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.\nPublishing anything this model generates is the same as publishing it yourself.\nYou are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.",
    "stabilityai/stable-diffusion-xl-refiner-1.0": "SD-XL 1.0-refiner Model Card\nModel\nModel Description\nModel Sources\nEvaluation\nðŸ§¨ Diffusers\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nSD-XL 1.0-refiner Model Card\nModel\nSDXL consists of an ensemble of experts pipeline for latent diffusion:\nIn a first step, the base model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) is used to generate (noisy) latents,\nwhich are then further processed with a refinement model specialized for the final denoising steps.\nNote that the base model can be used as a standalone module.\nAlternatively, we can use a two-stage pipeline as follows:\nFirst, the base model is used to generate latents of the desired output size.\nIn the second step, we use a specialized high-resolution model and apply a technique called SDEdit (https://arxiv.org/abs/2108.01073, also known as \"img2img\")\nto the latents generated in the first step, using the same prompt. This technique is slightly slower than the first one, as it requires more function evaluations.\nSource code is available at https://github.com/Stability-AI/generative-models .\nModel Description\nDeveloped by: Stability AI\nModel type: Diffusion-based text-to-image generative model\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pretrained text encoders (OpenCLIP-ViT/G and CLIP-ViT/L).\nResources for more information: Check out our GitHub Repository and the SDXL report on arXiv.\nModel Sources\nFor research purposes, we recommned our generative-models Github repository (https://github.com/Stability-AI/generative-models), which implements the most popoular diffusion frameworks (both training and inference) and for which new functionalities like distillation will be added over time.\nClipdrop provides free SDXL inference.\nRepository: https://github.com/Stability-AI/generative-models\nDemo: https://clipdrop.co/stable-diffusion\nEvaluation\nThe chart above evaluates user preference for SDXL (with and without refinement) over SDXL 0.9 and Stable Diffusion 1.5 and 2.1.\nThe SDXL base model performs significantly better than the previous variants, and the model combined with the refinement module achieves the best overall performance.\nðŸ§¨ Diffusers\nMake sure to upgrade diffusers to >= 0.18.0:\npip install diffusers --upgrade\nIn addition make sure to install transformers, safetensors, accelerate as well as the invisible watermark:\npip install invisible_watermark transformers accelerate safetensors\nYon can then use the refiner to improve images.\nimport torch\nfrom diffusers import StableDiffusionXLImg2ImgPipeline\nfrom diffusers.utils import load_image\npipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\npipe = pipe.to(\"cuda\")\nurl = \"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png\"\ninit_image = load_image(url).convert(\"RGB\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt, image=init_image).images\nWhen using torch >= 2.0, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline:\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\nIf you are limited by GPU VRAM, you can enable cpu offloading by calling pipe.enable_model_cpu_offload\ninstead of .to(\"cuda\"):\n- pipe.to(\"cuda\")\n+ pipe.enable_model_cpu_offload()\nFor more advanced use cases, please have a look at the docs.\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "BAAI/bge-base-en-v1.5": "News\nModel List\nFrequently asked questions\nUsage\nUsage for Embedding Model\nUsing FlagEmbedding\nUsing Sentence-Transformers\nUsing Langchain\nUsing HuggingFace Transformers\nUsage of the ONNX files\nUsage via infinity\nUsage for Reranker\nUsing FlagEmbedding\nUsing Huggingface transformers\nEvaluation\nTrain\nBAAI Embedding\nBGE Reranker\nContact\nCitation\nLicense\nFlagEmbedding\nModel List |\nFAQ |\nUsage  |\nEvaluation |\nTrain |\nContact |\nCitation |\nLicense\nFor more details please refer to our Github: FlagEmbedding.\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using bge-m3.\nEnglish | ä¸­æ–‡\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\nLong-Context LLM: Activation Beacon\nFine-tuning of LM : LM-Cocktail\nDense Retrieval: BGE-M3, LLM Embedder, BGE Embedding\nReranker Model: BGE Reranker\nBenchmark: C-MTEB\nNews\n1/30/2024: Release BGE-M3, a new member to BGE model series! M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec/colbert retrieval).\nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\nTechnical Report and Code. :fire:\n1/9/2024: Release Activation-Beacon, an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. Technical Report :fire:\n12/24/2023: Release LLaRA, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. Technical Report :fire:\n11/23/2023: Release LM-Cocktail, a method to maintain general capabilities during fine-tuning by merging multiple language models. Technical Report :fire:\n10/12/2023: Release LLM-Embedder, a unified embedding model to support diverse retrieval augmentation needs for LLMs. Technical Report\n09/15/2023: The technical report and massive training data of BGE has been released\n09/12/2023: New models:\nNew reranker model: release cross-encoder models BAAI/bge-reranker-base and BAAI/bge-reranker-large, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models.\nupdate embedding model: release bge-*-v1.5 embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\nMore\n09/07/2023: Update fine-tune code: Add script to mine hard negatives and support adding instruction during fine-tuning.\n08/09/2023: BGE Models are integrated into Langchain, you can use it like this; C-MTEB leaderboard is available.\n08/05/2023: Release base-scale and small-scale models, best performance among the models of the same size ðŸ¤—\n08/02/2023: Release bge-large-*(short for BAAI General Embedding) Models, rank 1st on MTEB and C-MTEB benchmark! :tada: :tada:\n08/01/2023: We release the Chinese Massive Text Embedding Benchmark (C-MTEB), consisting of 31 test dataset.\nModel List\nbge is short for BAAI general embedding.\nModel\nLanguage\nDescription\nquery instruction for retrieval [1]\nBAAI/bge-m3\nMultilingual\nInference Fine-tune\nMulti-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens)\nBAAI/llm-embedder\nEnglish\nInference Fine-tune\na unified embedding model to support diverse retrieval augmentation needs for LLMs\nSee README\nBAAI/bge-reranker-large\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-reranker-base\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-large-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\nBAAI/bge-base-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\nBAAI/bge-small-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\nBAAI/bge-large-en\nEnglish\nInference Fine-tune\n:trophy: rank 1st in MTEB leaderboard\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en\nEnglish\nInference Fine-tune\na base-scale model but with similar ability to bge-large-en\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en\nEnglish\nInference Fine-tune\na small-scale model but with competitive performance\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh\nChinese\nInference Fine-tune\n:trophy: rank 1st in C-MTEB benchmark\nä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\nBAAI/bge-base-zh\nChinese\nInference Fine-tune\na base-scale model but with similar ability to bge-large-zh\nä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\nBAAI/bge-small-zh\nChinese\nInference Fine-tune\na small-scale model but with competitive performance\nä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\n[1]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, no instruction needs to be added to passages.\n[2]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models.\nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI.\nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\nFrequently asked questions\n1. How to fine-tune bge embedding model?\nFollowing this example to prepare data and fine-tune your model.\nSome suggestions:\nMine hard negatives following this example, which can improve the retrieval performance.\nIf you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\nIf the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n2. The similarity score between two dissimilar sentences is higher than 0.5\nSuggest to use bge v1.5, which alleviates the issue of the similarity distribution.\nSince we finetune the models by contrastive learning with a temperature of 0.01,\nthe similarity distribution of the current BGE model is about in the interval [0.6, 1].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\nFor downstream tasks, such as passage retrieval or semantic similarity,\nwhat matters is the relative order of the scores, not the absolute value.\nIf you need to filter similar sentences based on a similarity threshold,\nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n3. When does the query instruction need to be used\nFor the bge-*-v1.5, we improve its retrieval ability when not using instruction.\nNo instruction only has a slight degradation in retrieval performance compared with using instruction.\nSo you can generate embedding without instruction in all cases for convenience.\nFor a retrieval task that uses short queries to find long related documents,\nit is recommended to add instructions for these short queries.\nThe best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.\nIn all cases, the documents/passages do not need to add the instruction.\nUsage\nUsage for Embedding Model\nHere are some examples for using bge models with\nFlagEmbedding, Sentence-Transformers, Langchain, or Huggingface Transformers.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nIf it doesn't work for you, you can see FlagEmbedding for more methods to install FlagEmbedding.\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"æ ·ä¾‹æ•°æ®-1\", \"æ ·ä¾‹æ•°æ®-2\"]\nsentences_2 = [\"æ ·ä¾‹æ•°æ®-3\", \"æ ·ä¾‹æ•°æ®-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5',\nquery_instruction_for_retrieval=\"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\",\nuse_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"æ ·ä¾‹æ–‡æ¡£-1\", \"æ ·ä¾‹æ–‡æ¡£-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\nFor the value of the argument query_instruction_for_retrieval, see Model List.\nBy default, FlagModel will use all available GPUs when encoding. Please set os.environ[\"CUDA_VISIBLE_DEVICES\"] to select specific GPUs.\nYou also can set os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" to make all GPUs unavailable.\nUsing Sentence-Transformers\nYou can also use the bge models with sentence-transformers:\npip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"æ ·ä¾‹æ•°æ®-1\", \"æ ·ä¾‹æ•°æ®-2\"]\nsentences_2 = [\"æ ·ä¾‹æ•°æ®-3\", \"æ ·ä¾‹æ•°æ®-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\nFor s2p(short query to long passage) retrieval task,\neach short query should start with an instruction (instructions see Model List).\nBut the instruction is not needed for passages.\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"æ ·ä¾‹æ–‡æ¡£-1\", \"æ ·ä¾‹æ–‡æ¡£-2\"]\ninstruction = \"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\nUsing Langchain\nYou can use bge in langchain like this:\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\nmodel_name=model_name,\nmodel_kwargs=model_kwargs,\nencode_kwargs=encode_kwargs,\nquery_instruction=\"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\n)\nmodel.query_instruction = \"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºŽæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\nUsing HuggingFace Transformers\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"æ ·ä¾‹æ•°æ®-1\", \"æ ·ä¾‹æ•°æ®-2\"]\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, cls pooling.\nsentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\nUsage of the ONNX files\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction  # type: ignore\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\")\nmodel_ort = ORTModelForFeatureExtraction.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\",file_name=\"onnx/model.onnx\")\n# Sentences we want sentence embeddings for\nsentences = [\"æ ·ä¾‹æ•°æ®-1\", \"æ ·ä¾‹æ•°æ®-2\"]\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\nmodel_output_ort = model_ort(**encoded_input)\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# model_output and model_output_ort are identical\nUsage via infinity\nIts also possible to deploy the onnx files with the infinity_emb pip package.\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(\nEngineArgs(model_name_or_path = \"BAAI/bge-large-en-v1.5\", device=\"cpu\", engine=\"optimum\" # or engine=\"torch\"\n))\nasync def main():\nasync with engine:\nembeddings, usage = await engine.embed(sentences=sentences)\nasyncio.run(main())\nUsage for Reranker\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nGet relevance scores (higher scores indicate more relevance):\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\nUsing Huggingface transformers\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\nscores = model(**inputs, return_dict=True).logits.view(-1, ).float()\nprint(scores)\nEvaluation\nbaai-general-embedding models achieve state-of-the-art performance on both MTEB and C-MTEB leaderboard!\nFor more details and evaluation tools see our scripts.\nMTEB:\nModel Name\nDimension\nSequence Length\nAverage (56)\nRetrieval (15)\nClustering (11)\nPair Classification (3)\nReranking (4)\nSTS (10)\nSummarization (1)\nClassification (12)\nBAAI/bge-large-en-v1.5\n1024\n512\n64.23\n54.29\n46.08\n87.12\n60.03\n83.11\n31.61\n75.97\nBAAI/bge-base-en-v1.5\n768\n512\n63.55\n53.25\n45.77\n86.55\n58.86\n82.4\n31.07\n75.53\nBAAI/bge-small-en-v1.5\n384\n512\n62.17\n51.68\n43.82\n84.92\n58.36\n81.59\n30.12\n74.14\nbge-large-en\n1024\n512\n63.98\n53.9\n46.98\n85.8\n59.48\n81.56\n32.06\n76.21\nbge-base-en\n768\n512\n63.36\n53.0\n46.32\n85.86\n58.7\n81.84\n29.27\n75.27\ngte-large\n1024\n512\n63.13\n52.22\n46.84\n85.00\n59.13\n83.35\n31.66\n73.33\ngte-base\n768\n512\n62.39\n51.14\n46.2\n84.57\n58.61\n82.3\n31.17\n73.01\ne5-large-v2\n1024\n512\n62.25\n50.56\n44.49\n86.03\n56.61\n82.05\n30.19\n75.24\nbge-small-en\n384\n512\n62.11\n51.82\n44.31\n83.78\n57.97\n80.72\n30.53\n74.37\ninstructor-xl\n768\n512\n61.79\n49.26\n44.74\n86.62\n57.29\n83.06\n32.32\n61.79\ne5-base-v2\n768\n512\n61.5\n50.29\n43.80\n85.73\n55.91\n81.05\n30.28\n73.84\ngte-small\n384\n512\n61.36\n49.46\n44.89\n83.54\n57.7\n82.07\n30.42\n72.31\ntext-embedding-ada-002\n1536\n8192\n60.99\n49.25\n45.9\n84.89\n56.32\n80.97\n30.8\n70.93\ne5-small-v2\n384\n512\n59.93\n49.04\n39.92\n84.67\n54.32\n80.39\n31.16\n72.94\nsentence-t5-xxl\n768\n512\n59.51\n42.24\n43.72\n85.06\n56.42\n82.63\n30.08\n73.42\nall-mpnet-base-v2\n768\n514\n57.78\n43.81\n43.69\n83.04\n59.36\n80.28\n27.49\n65.07\nsgpt-bloom-7b1-msmarco\n4096\n2048\n57.59\n48.22\n38.93\n81.9\n55.65\n77.74\n33.6\n66.19\nC-MTEB:We create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks.\nPlease refer to C_MTEB for a detailed introduction.\nModel\nEmbedding dimension\nAvg\nRetrieval\nSTS\nPairClassification\nClassification\nReranking\nClustering\nBAAI/bge-large-zh-v1.5\n1024\n64.53\n70.46\n56.25\n81.6\n69.13\n65.84\n48.99\nBAAI/bge-base-zh-v1.5\n768\n63.13\n69.49\n53.72\n79.75\n68.07\n65.39\n47.53\nBAAI/bge-small-zh-v1.5\n512\n57.82\n61.77\n49.11\n70.41\n63.96\n60.92\n44.18\nBAAI/bge-large-zh\n1024\n64.20\n71.53\n54.98\n78.94\n68.32\n65.11\n48.39\nbge-large-zh-noinstruct\n1024\n63.53\n70.55\n53\n76.77\n68.58\n64.91\n50.01\nBAAI/bge-base-zh\n768\n62.96\n69.53\n54.12\n77.5\n67.07\n64.91\n47.63\nmultilingual-e5-large\n1024\n58.79\n63.66\n48.44\n69.89\n67.34\n56.00\n48.23\nBAAI/bge-small-zh\n512\n58.27\n63.07\n49.45\n70.35\n63.64\n61.48\n45.09\nm3e-base\n768\n57.10\n56.91\n50.47\n63.99\n67.52\n59.34\n47.68\nm3e-large\n1024\n57.05\n54.75\n50.42\n64.3\n68.2\n59.66\n48.88\nmultilingual-e5-base\n768\n55.48\n61.63\n46.49\n67.07\n65.35\n54.35\n40.68\nmultilingual-e5-small\n384\n55.38\n59.95\n45.27\n66.45\n65.85\n53.86\n45.26\ntext-embedding-ada-002(OpenAI)\n1536\n53.02\n52.0\n43.35\n69.56\n64.31\n54.28\n45.68\nluotuo\n1024\n49.37\n44.4\n42.78\n66.62\n61\n49.25\n44.39\ntext2vec-base\n768\n47.63\n38.79\n43.41\n67.41\n62.19\n49.45\n37.66\ntext2vec-large\n1024\n47.36\n41.94\n44.97\n70.86\n60.66\n49.16\n30.02\nReranking:\nSee C_MTEB for evaluation script.\nModel\nT2Reranking\nT2RerankingZh2En*\nT2RerankingEn2Zh*\nMMarcoReranking\nCMedQAv1\nCMedQAv2\nAvg\ntext2vec-base-multilingual\n64.66\n62.94\n62.51\n14.37\n48.46\n48.6\n50.26\nmultilingual-e5-small\n65.62\n60.94\n56.41\n29.91\n67.26\n66.54\n57.78\nmultilingual-e5-large\n64.55\n61.61\n54.28\n28.6\n67.42\n67.92\n57.4\nmultilingual-e5-base\n64.21\n62.13\n54.68\n29.5\n66.23\n66.98\n57.29\nm3e-base\n66.03\n62.74\n56.07\n17.51\n77.05\n76.76\n59.36\nm3e-large\n66.13\n62.72\n56.1\n16.46\n77.76\n78.27\n59.57\nbge-base-zh-v1.5\n66.49\n63.25\n57.02\n29.74\n80.47\n84.88\n63.64\nbge-large-zh-v1.5\n65.74\n63.39\n57.03\n28.74\n83.45\n85.44\n63.97\nBAAI/bge-reranker-base\n67.28\n63.95\n60.45\n35.46\n81.26\n84.1\n65.42\nBAAI/bge-reranker-large\n67.6\n64.03\n61.44\n37.16\n82.15\n84.18\n66.09\n* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\nTrain\nBAAI Embedding\nWe pre-train the models using retromae and train them on large-scale pairs data using contrastive learning.\nYou can fine-tune the embedding model on your data following our examples.\nWe also provide a pre-train example.\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see baai_general_embedding.\nBGE Reranker\nCross-encoder will perform full-attention over the input pair,\nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data,\nThe data format is the same as embedding model, so you can fine-tune it easily following our example.\nMore details please refer to ./FlagEmbedding/reranker/README.md\nContact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn).\nCitation\nIf you find this repository useful, please consider giving a star :star: and citation\n@misc{bge_embedding,\ntitle={C-Pack: Packaged Resources To Advance General Chinese Embedding},\nauthor={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\nyear={2023},\neprint={2309.07597},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nLicense\nFlagEmbedding is licensed under the MIT License. The released models can be used for commercial purposes free of charge."
}