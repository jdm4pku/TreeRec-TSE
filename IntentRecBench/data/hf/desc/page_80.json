{
    "ollieturnbull/p-IgGen-developable": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "hiieu/halong_embedding": "Halong Embedding\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nMatryoshka Embeddings Inference\nEvaluation\nMetrics\nCitation\nBibTeX\nHalong Embedding\nHalong Embedding is a Vietnamese text embedding focused on RAG and production efficiency:\nüìö Trained on a in house dataset consist of approximately 100,000 examples of question and related documents\nü™Ü Trained with a Matryoshka loss, allowing you to truncate embeddings with minimal performance loss: smaller embeddings are faster to compare.\nThis is a sentence-transformers model finetuned from intfloat/multilingual-e5-base. It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\nYou can find eval, fine-tune scripts here as well as my seminar\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: intfloat/multilingual-e5-base\nMaximum Sequence Length: 512 tokens\nOutput Dimensionality: 768 tokens\nSimilarity Function: Cosine Similarity\nLanguage: vi-focused, multilingual\nLicense: apache-2.0\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Normalize()\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\nimport torch\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"hiieu/halong_embedding\")\n# Define query and documents\nquery = \"B√≥ng ƒë√° c√≥ l·ª£i √≠ch g√¨ cho s·ª©c kh·ªèe?\"\ndocs = [\n\"B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn.\",\n\"B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi.\",\n\"Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω.\",\n\"B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n.\",\n\"B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠.\"\n]\n# Encode query and documents\nquery_embedding = model.encode([query])\ndoc_embeddings = model.encode(docs)\nsimilarities = model.similarity(query_embedding, doc_embeddings).flatten()\n# Sort documents by cosine similarity\nsorted_indices = torch.argsort(similarities, descending=True)\nsorted_docs = [docs[idx] for idx in sorted_indices]\nsorted_scores = [similarities[idx].item() for idx in sorted_indices]\n# Print sorted documents with their cosine scores\nfor doc, score in zip(sorted_docs, sorted_scores):\nprint(f\"Document: {doc} - Cosine Similarity: {score:.4f}\")\n# Document: B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn. - Cosine Similarity: 0.7318\n# Document: Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω. - Cosine Similarity: 0.6623\n# Document: B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠. - Cosine Similarity: 0.6102\n# Document: B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n. - Cosine Similarity: 0.4988\n# Document: B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi. - Cosine Similarity: 0.4828\nMatryoshka Embeddings Inference\nfrom sentence_transformers import SentenceTransformer\nimport torch.nn.functional as F\nimport torch\nmatryoshka_dim = 64\nmodel = SentenceTransformer(\n\"hiieu/halong_embedding\",\ntruncate_dim=matryoshka_dim,\n)\n# Define query and documents\nquery = \"B√≥ng ƒë√° c√≥ l·ª£i √≠ch g√¨ cho s·ª©c kh·ªèe?\"\ndocs = [\n\"B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn.\",\n\"B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi.\",\n\"Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω.\",\n\"B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n.\",\n\"B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠.\"\n]\n# Encode query and documents\nquery_embedding = model.encode([query])\ndoc_embeddings = model.encode(docs)\nsimilarities = model.similarity(query_embedding, doc_embeddings).flatten()\n# Sort documents by cosine similarity\nsorted_indices = torch.argsort(similarities, descending=True)\nsorted_docs = [docs[idx] for idx in sorted_indices]\nsorted_scores = [similarities[idx].item() for idx in sorted_indices]\n# Print sorted documents with their cosine scores\nfor doc, score in zip(sorted_docs, sorted_scores):\nprint(f\"Document: {doc} - Cosine Similarity: {score:.4f}\")\n# Document: B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn. - Cosine Similarity: 0.8045\n# Document: Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω. - Cosine Similarity: 0.7676\n# Document: B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠. - Cosine Similarity: 0.6758\n# Document: B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n. - Cosine Similarity: 0.5931\n# Document: B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi. - Cosine Similarity: 0.5105\nEvaluation\nMetrics\nInformation Retrieval\nDataset: Zalo legal retrieval dataet\nnote: We sampled 20% of the Zalo Legal train dataset for fast testing; our model did not train on this dataset.\nEvaluated with InformationRetrievalEvaluator\nModel\nAccuracy@1\nAccuracy@3\nAccuracy@5\nAccuracy@10\nPrecision@1\nPrecision@3\nPrecision@5\nPrecision@10\nRecall@1\nRecall@3\nRecall@5\nRecall@10\nNDCG@10\nMRR@10\nMAP@100\nvietnamese-bi-encoder\n0.8169\n0.9108\n0.9437\n0.9640\n0.8169\n0.3099\n0.1931\n0.0987\n0.8020\n0.9045\n0.9390\n0.9601\n0.8882\n0.8685\n0.8652\nsup-SimCSE-VietNamese-phobert-base\n0.5540\n0.7308\n0.7981\n0.8748\n0.5540\n0.2473\n0.1621\n0.0892\n0.5446\n0.7246\n0.7903\n0.8693\n0.7068\n0.6587\n0.6592\nhalong_embedding (768)\n0.8294\n0.9233\n0.9437\n0.9687\n0.8294\n0.3146\n0.1931\n0.0991\n0.8146\n0.9178\n0.9390\n0.9640\n0.8976\n0.8799\n0.8763\nhalong_embedding (512)\n0.8138\n0.9233\n0.9390\n0.9703\n0.8138\n0.3146\n0.1922\n0.0992\n0.7989\n0.9178\n0.9343\n0.9656\n0.8917\n0.8715\n0.8678\nhalong_embedding (256)\n0.7934\n0.8967\n0.9280\n0.9593\n0.7934\n0.3062\n0.1900\n0.0981\n0.7786\n0.8920\n0.9233\n0.9546\n0.8743\n0.8520\n0.8489\nhalong_embedding (128)\n0.7840\n0.8951\n0.9264\n0.9515\n0.7840\n0.3046\n0.1894\n0.0975\n0.7707\n0.8889\n0.9210\n0.9476\n0.8669\n0.8439\n0.8412\nhalong_embedding (64)\n0.6980\n0.8435\n0.8920\n0.9358\n0.6980\n0.2864\n0.1815\n0.0958\n0.6854\n0.8365\n0.8842\n0.9311\n0.8145\n0.7805\n0.7775\nCitation\nYou can cite our work as below:\n@misc{HalongEmbedding,\ntitle={HalongEmbedding: A Vietnamese Text Embedding},\nauthor={Ngo Hieu},\nyear={2024},\npublisher={Huggingface},\n}\nBibTeX\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nMatryoshkaLoss\n@misc{kusupati2024matryoshka,\ntitle={Matryoshka Representation Learning},\nauthor={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},\nyear={2024},\neprint={2205.13147},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nMultipleNegativesRankingLoss\n@misc{henderson2017efficient,\ntitle={Efficient Natural Language Response Suggestion for Smart Reply},\nauthor={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\nyear={2017},\neprint={1705.00652},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "mradermacher/Viet-Sailor-4B-Instruct-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/5CD-AI/Viet-Sailor-4B-Instruct\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.7\nGGUF\nIQ3_XS\n1.9\nGGUF\nIQ3_S\n2.0\nbeats Q3_K*\nGGUF\nQ3_K_S\n2.0\nGGUF\nIQ3_M\n2.0\nGGUF\nQ3_K_M\n2.1\nlower quality\nGGUF\nQ3_K_L\n2.3\nGGUF\nIQ4_XS\n2.3\nGGUF\nQ4_K_S\n2.4\nfast, recommended\nGGUF\nQ4_K_M\n2.6\nfast, recommended\nGGUF\nQ5_K_S\n2.9\nGGUF\nQ5_K_M\n2.9\nGGUF\nQ6_K\n3.3\nvery good quality\nGGUF\nQ8_0\n4.3\nfast, best quality\nGGUF\nf16\n8.0\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "ibm-granite/granite-20b-functioncalling": "Granite-20B-FunctionCalling\nModel Summary\nUsage\nIntended use\nGeneration\nGranite-20B-FunctionCalling\nModel Summary\nGranite-20B-FunctionCalling is a finetuned model based on IBM's granite-20b-code-instruct model to introduce function calling abilities into Granite model family. The model is trained using a multi-task training approach on seven fundamental tasks encompassed in function calling, those being Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation.\nDevelopers: IBM Research\nPaper: Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks\nRelease Date: July 9th, 2024\nLicense: Apache 2.0.\nUsage\nIntended use\nThe model is designed to respond to function calling related instructions.\nGeneration\nThis is a simple example of how to use Granite-20B-Code-FunctionCalling model.\nimport json\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # or \"cpu\"\nmodel_path = \"ibm-granite/granite-20b-functioncalling\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()\n# define the user query and list of available functions\nquery = \"What's the current weather in New York?\"\nfunctions = [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\"\n}\n},\n\"required\": [\"location\"]\n}\n},\n{\n\"name\": \"get_stock_price\",\n\"description\": \"Retrieves the current stock price for a given ticker symbol. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"ticker\": {\n\"type\": \"string\",\n\"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n}\n},\n\"required\": [\"ticker\"]\n}\n}\n]\n# serialize functions and define a payload to generate the input template\npayload = {\n\"functions_str\": [json.dumps(x) for x in functions],\n\"query\": query,\n}\ninstruction = tokenizer.apply_chat_template(payload, tokenize=False, add_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(instruction, return_tensors=\"pt\").to(device)\n# generate output tokens\noutputs = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into text\noutputs = tokenizer.batch_decode(outputs)\n# loop over the batch to print, in this example the batch size is 1\nfor output in outputs:\n# Each function call in the output will be preceded by the token \"<function_call>\" followed by a\n# json serialized function call of the format {\"name\": $function_name$, \"arguments\" {$arg_name$: $arg_val$}}\n# In this specific case, the output will be: <function_call> {\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"New York\"}}\nprint(output)",
    "Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier": "Model Details\nModel Description\nBias\nEvaluation Results\nCitation\nModel Details\nModel Description\nThe EmoSense-ID is a model designed to identify and analyze emotions in Indonesian texts based on Plutchik's eight basic emotions: Anticipation, Anger, Disgust, Fear, Joy, Sadness, Surprise, and Trust.\nThis model is developed using the NusaBERT-base  and trained using Indonesian tweets categorized into eight emotion categories. The evaluation results of this model can be utilized to analyze emotions in social media, providing insights into users' emotional responses.\nBias\nKeep in mind that this model is trained using certain data which may cause bias in the emotion classification process. Therefore, it is important to consider and account for such biases when using this model.\nEvaluation Results\nThe model was trained using the Hyperparameter Tuning technique with Optuna. In this process, Optuna conducted five trials to determine the optimal combination of learning rate (1e-6 to 1e-4) and weight decay (1e-6 to 1e-2). Each trial trained the BERT model with different hyperparameter configurations on the training dataset and then evaluated using the validation dataset. After all the experiments are completed, the best hyperparameter combination is used to train the final model.\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\nF1\nPrecision\nRecall\n1\n0.758400\n0.583508\n0.829932\n0.830203\n0.833136\n0.829932\n2\n0.370100\n0.394630\n0.866213\n0.865496\n0.870364\n0.866213\n3\n0.231500\n0.355294\n0.884354\n0.884585\n0.888140\n0.884354\n4\n0.071000\n0.322376\n0.902494\n0.902801\n0.904842\n0.902494\n5\n0.129900\n0.308596\n0.900227\n0.900340\n0.902132\n0.900227\nCitation\n@misc{Ardiyanto_Mikhael_2024,\nauthor    = {Mikhael Ardiyanto},\ntitle     = {EmoSense-ID},\nyear      = {2024},\nURL       = {Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier},\npublisher = {Hugging Face}\n}",
    "fal/AuraFlow": "AuraFlow\nUsage\nAuraFlow\nAuraFlow v0.1 is the fully open-sourced largest flow-based text-to-image generation model.\nThis model achieves state-of-the-art results on GenEval. Read our blog post for more technical details.\nThe model is currently in beta. We are working on improving it and the community's feedback is important.\nJoin fal's Discord to give us feedback and stay in touch with the model development.\nCredits: A huge thank you to @cloneofsimo and @isidentical for bringing this project to life. It's incredible what two cracked engineers can achieve in\nsuch a short period of time. We also extend our gratitude to the incredible researchers whose prior work laid the foundation for our efforts.\nUsage\n$ pip install transformers accelerate protobuf sentencepiece\n$ pip install git+https://github.com/huggingface/diffusers.git\nfrom diffusers import AuraFlowPipeline\nimport torch\npipeline = AuraFlowPipeline.from_pretrained(\n\"fal/AuraFlow\",\ntorch_dtype=torch.float16\n).to(\"cuda\")\nimage = pipeline(\nprompt=\"close-up portrait of a majestic iguana with vibrant blue-green scales, piercing amber eyes, and orange spiky crest. Intricate textures and details visible on scaly skin. Wrapped in dark hood, giving regal appearance. Dramatic lighting against black background. Hyper-realistic, high-resolution image showcasing the reptile's expressive features and coloration.\",\nheight=1024,\nwidth=1024,\nnum_inference_steps=50,\ngenerator=torch.Generator().manual_seed(666),\nguidance_scale=3.5,\n).images[0]",
    "ielzituh/Modelos-De-Iel-OUTROS": "",
    "karpathy/gpt2_1558M_final3_hf": "",
    "5CD-AI/Vietnamese-Sentiment-visobert": "",
    "KitsuneX07/Music_Source_Sepetration_Models": "",
    "OuteAI/Lite-Mistral-150M-v2-Instruct": "",
    "OuteAI/Lite-Mistral-150M-v2-Instruct-GGUF": "Lite-Mistral-150M-v2-Instruct\nImprovements from v1:\nChat format\nHow coherent is the 150M model?\nBenchmarks:\nRisk Disclaimer\nOuteAI\nüåé OuteAI.com\nü§ù Join our Discord\nùïè @OuteAI\nLite-Mistral-150M-v2-Instruct\nThis is a Lite series model based on the Mistral architecture, comprising approximately 157 million parameters.\nThe primary goal of this 150 million parameter model was to develop a compact and efficient model capable of operating on a wide range of devices, while maintaining a reasonable level of functionality and coherence for its small size. A smaller model scale may lead to challenges in preserving context over multi-turn conversations. Consequently, there is a risk of inconsistent or inaccurate responses.\nThe model was trained on ~8 billion tokens.\nLite-Mistral-150M-v2-Instruct\nLite-Mistral-150M-v2-Instruct-GGUF\nImprovements from v1:\nExtended Training: Further refinement of the model, resulting in improved benchmark performance and overall text generation quality.\nTokenizer changes.\nChat format\nThis model is sensitive to the chat template used. Ensure you use the correct template:\n<s>system\n[System message]</s>\n<s>user\n[Your question or message]</s>\n<s>assistant\n[The model's response]</s>\nHow coherent is the 150M model?\nLet's look at real-world examples:\nExample 1\n<s>user\nWhat is the capital of France?</s>\n<s>assistant\nThe capital of France is Paris.</s>\n<s>user\nWhat is the capital of Spain?</s>\n<s>assistant\nThe capital of Spain is Madrid, which is known for its beautiful architecture and rich history.</s>\nExample 2\n<s>user\nWho wrote the play \"Romeo and Juliet\"?</s>\n<s>assistant\nThe play \"Romeo and Juliet\" was written by William Shakespeare.</s>\nExample 3\n<s>system\nYou are a helpful assistant.</s>\n<s>user\nI'd like to learn about language models. Can you break down the concept for me?</s>\n<s>assistant\nOf course! Language models are computer programs designed to mimic human language, which is the way humans understand and interact with each other.\nThey use algorithms to analyze data and generate responses that are similar to how humans do on their own.</s>\nExample 4 (RP)\n[Input for the model]\n<s>user\nLet's roleplay. I'll be a customer at a restaurant, and you be the waiter. Ready?</s>\n<s>assistant\nCertainly! I'll play the role of a waiter. Please go ahead and start the roleplay when you're ready.</s>\n<s>user\nHello, I'd like to see the menu please.</s>\n<s>assistant\nGood evening, sir! Welcome to our restaurant. Here's our menu for today. May I recommend our chef's special, the grilled salmon with lemon butter sauce? Let me know if you have any questions about our dishes.</s>\n<s>user\nI'll have the salmon. What wines do you recommend to go with it?</s>\n[Model's response]\n<s> assistant\nTo go with your salmon, we suggest a pair of red-tailed or white-tailed jackfights. The grilled salmon is creamy and has a nice acidity. The lemon butter sauce is a great addition to this dish.</s>\nThe model shows some promise in understanding context related to simple requests, but it's clear that it still struggles with more complex or nuanced situations.\nBenchmarks:\nBenchmark\n5-shot (acc)\n5-shot (acc_norm)\n0-shot (acc)\n0-shot (acc_norm)\nARC Easy\n47.26\n46.30\n45.58\n39.52\nBoolQ\n43.33\n-\n44.16\n-\nHellaSWAG\n28.70\n30.73\n28.72\n31.01\nMMLU\n-\n-\n25.28\n-\nOpenBookQA\n16.00\n28.00\n18.20\n28.40\nPIQA\n62.79\n62.68\n62.02\n60.23\nWinogrande\n51.30\n-\n51.78\n-\nRisk Disclaimer\nBy using this model, you acknowledge that you understand and assume the risks associated with its use. You are solely responsible for ensuring compliance with all applicable laws and regulations. We disclaim any liability for problems arising from the use of this open-source model, including but not limited to direct, indirect, incidental, consequential, or punitive damages. We make no warranties, express or implied, regarding the model's performance, accuracy, or fitness for a particular purpose. Your use of this model is at your own risk, and you agree to hold harmless and indemnify us, our affiliates, and our contributors from any claims, damages, or expenses arising from your use of the model.",
    "TheDrummer/Big-Tiger-Gemma-27B-v1-GGUF": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nBig Tiger Gemma 27B v1\nBig Tiger Gemma 27B v1\nDecensored Gemma 27B. No refusals so far (other than some rare instances from 9B). No apparent brain damage.\nOriginal: https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v1\nGGUF: https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v1-GGUF\niMatrix: https://huggingface.co/MarsupialAI/Big-Tiger-Gemma-27B-v1_iMatrix_GGUF (Better PPL)\nEXL2: https://huggingface.co/collections/bullerwins/big-tiger-gemma-27b-v1-exl2-669413088437b4d339c32fbe\nIn memory of Tiger (the happy street cat on the right)",
    "meta-llama/Llama-3.1-70B": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3_1/use-policy\nProhibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.1 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.1 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nUse with llama3\nHardware and Software\nTraining Data\nBenchmark scores\nBase pretrained models\nInstruction tuned models\nMultilingual benchmarks\nResponsibility & Safety\nResponsible deployment\nLlama 3.1 instruct\nLlama 3.1 systems\nNew capabilities\nEvaluations\nCritical and other risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.1 (text only)\nA new mix of publicly available online data.\n8B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n405B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: July 23, 2024.\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Meta-Llama-3.1-70B, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\n>>> import transformers\n>>> import torch\n>>> model_id = \"meta-llama/Meta-Llama-3.1-70B\"\n>>> pipeline = transformers.pipeline(\n\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n>>> pipeline(\"Hey how are you doing today?\")\nUse with llama3\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Meta-Llama-3.1-70B --include \"original/*\" --local-dir Meta-Llama-3.1-70B\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions\n(tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions\n(tons CO2eq)\nLlama 3.1 8B\n1.46M\n700\n420\n0\nLlama 3.1 70B\n7.0M\n700\n2,040\n0\nLlama 3.1 405B\n30.84M\n700\n8,930\n0\nTotal\n39.3M\n11,390\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmark scores\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\nBase pretrained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B\nLlama 3.1 8B\nLlama 3 70B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n66.7\n66.7\n79.5\n79.3\n85.2\nMMLU-Pro (CoT)\n5\nmacro_avg/acc_char\n36.2\n37.1\n55.0\n53.8\n61.6\nAGIEval English\n3-5\naverage/acc_char\n47.1\n47.8\n63.0\n64.6\n71.6\nCommonSenseQA\n7\nacc_char\n72.6\n75.0\n83.8\n84.1\n85.8\nWinogrande\n5\nacc_char\n-\n60.5\n-\n83.3\n86.7\nBIG-Bench Hard (CoT)\n3\naverage/em\n61.1\n64.2\n81.3\n81.6\n85.9\nARC-Challenge\n25\nacc_char\n79.4\n79.7\n93.1\n92.9\n96.1\nKnowledge reasoning\nTriviaQA-Wiki\n5\nem\n78.5\n77.6\n89.7\n89.8\n91.8\nReading comprehension\nSQuAD\n1\nem\n76.4\n77.0\n85.6\n81.8\n89.3\nQuAC (F1)\n1\nf1\n44.4\n44.9\n51.1\n51.1\n53.6\nBoolQ\n0\nacc_char\n75.7\n75.0\n79.0\n79.4\n80.0\nDROP (F1)\n3\nf1\n58.4\n59.5\n79.7\n79.6\n84.8\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B Instruct\nLlama 3.1 8B Instruct\nLlama 3 70B Instruct\nLlama 3.1 70B Instruct\nLlama 3.1 405B Instruct\nGeneral\nMMLU\n5\nmacro_avg/acc\n68.5\n69.4\n82.0\n83.6\n87.3\nMMLU (CoT)\n0\nmacro_avg/acc\n65.3\n73.0\n80.9\n86.0\n88.6\nMMLU-Pro (CoT)\n5\nmicro_avg/acc_char\n45.5\n48.3\n63.4\n66.4\n73.3\nIFEval\n76.8\n80.4\n82.9\n87.5\n88.6\nReasoning\nARC-C\n0\nacc\n82.4\n83.4\n94.4\n94.8\n96.9\nGPQA\n0\nem\n34.6\n30.4\n39.5\n46.7\n50.7\nCode\nHumanEval\n0\npass@1\n60.4\n72.6\n81.7\n80.5\n89.0\nMBPP ++ base version\n0\npass@1\n70.6\n72.8\n82.5\n86.0\n88.6\nMultipl-E HumanEval\n0\npass@1\n-\n50.8\n-\n65.5\n75.2\nMultipl-E MBPP\n0\npass@1\n-\n52.4\n-\n62.0\n65.7\nMath\nGSM-8K (CoT)\n8\nem_maj1@1\n80.6\n84.5\n93.0\n95.1\n96.8\nMATH (CoT)\n0\nfinal_em\n29.1\n51.9\n51.0\n68.0\n73.8\nTool Use\nAPI-Bank\n0\nacc\n48.3\n82.6\n85.1\n90.0\n92.0\nBFCL\n0\nacc\n60.3\n76.1\n83.0\n84.8\n88.5\nGorilla Benchmark API Bench\n0\nacc\n1.7\n8.2\n14.7\n29.7\n35.3\nNexus (0-shot)\n0\nmacro_avg/acc\n18.1\n38.5\n47.8\n56.7\n58.7\nMultilingual\nMultilingual MGSM (CoT)\n0\nem\n-\n68.9\n-\n86.9\n91.6\nMultilingual benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.1 8B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n62.12\n80.13\n84.95\nSpanish\n62.45\n80.05\n85.08\nItalian\n61.63\n80.4\n85.04\nGerman\n60.59\n79.27\n84.36\nFrench\n62.34\n79.82\n84.66\nHindi\n50.88\n74.52\n80.31\nThai\n50.32\n72.95\n78.21\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.1 systems\nLarge language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teaming\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nOur study of Llama-3.1-405B‚Äôs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "brad-twinkl/controlnet-union-sdxl-1.0-promax": "ControlNet++: All-in-one ControlNet for image generations and editing!\nProMax Model has released!! 12 control + 5 advanced editing, just try it!!!\nControlNet++: All-in-one ControlNet for image generations and editing!\nProMax Model has released!! 12 control + 5 advanced editing, just try it!!!\nThis is just a copy of the original repo here by xinsir.\nThe only difference is I have renamed the Promax model to make it easier to use.",
    "lenamerkli/ingredient-scanner": "Ingredient Scanner\nAbstract\nTable of Contents\nReport\nModel Details\nVision Model\nLanguage Model\nUsage\nAnthropic API\nCitation\nIngredient Scanner\nAbstract\nWith the recent advancements in computer vision and optical character recognition and using a convolutional neural network to cut out the product from a picture, it has now become possible to reliably extract ingredient lists from the back of a product using the Anthropic API. Open-weight or even only on-device optical character recognition lacks the quality to be used in a production environment, although the progress in development is promising. The Anthropic API is also currently not feasible due to the high cost of 1 Swiss Franc per 100 pictures.\nThe training code and data is available on GitHub. This repository just contains an inference example and the report.\nThis is an entry for the 2024 Swiss AI competition.\nTable of Contents\nAbstract\nReport\nModel Details\nUsage\nCitation\nReport\nRead the full report here.\nModel Details\nThis repository consists of two models, one vision model and a large language model.\nVision Model\nCustom convolutional neural network based on ResNet18. It detects the four corner points and the upper and lower limits of a product.\nLanguage Model\nConverts the text from the optical character recognition engine which lies in-between the two models to JSON. It is fine-tuned from unsloth/Qwen2-0.5B-Instruct-bnb-4bit.\nUsage\nClone the repository and install the dependencies on any debian-based system:\ngit clone https://huggingface.co/lenamerkli/ingredient-scanner\ncd ingredient-scanner\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install -r requirements.txt\nNote: not all requirements are needed for inference, as both training and inference requirements are listed.\nSelect the OCR engine in main.py by uncommenting one of the lines 20 to 22:\n# ENGINE: list[str] = ['easyocr']\n# ENGINE: list[str] = ['anthropic', 'claude-3-5-sonnet-20240620']\n# ENGINE: list[str] = ['llama_cpp/v2/vision', 'qwen-vl-next_b2583']\nNote: Qwen-VL-Next is not an official qwen model. This is only to protect business secrets of a private model.\nRun the inference script:\npython3 main.py\nYou will be asked to enter the file path to a PNG image.\nAnthropic API\nIf you want to use the Anthropic API, create a .env file with the following content:\nANTHROPIC_API_KEY=YOUR_API_KEY\nCitation\nHere is how to cite this paper in the bibtex format:\n@misc{merkli2024ingriedient-scanner,\ntitle={Ingredient Scanner: Automating Reading of Ingredient Labels with Computer Vision},\nauthor={Lena Merkli and Sonja Merkli},\ndate={2024-07-16},\nurl={https://huggingface.co/lenamerkli/ingredient-scanner},\n}",
    "nvidia/bigvgan_v2_44khz_128band_512x": "BigVGAN: A Universal Neural Vocoder with Large-Scale Training\nSang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon\nNews\nInstallation\nUsage\nUsing Custom CUDA Kernel for Synthesis\nPretrained Models\nBigVGAN: A Universal Neural Vocoder with Large-Scale Training\nSang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon\n[Paper] - [Code] - [Showcase] - [Project Page] - [Weights] - [Demo]\nNews\nJul 2024 (v2.3):\nGeneral refactor and code improvements for improved readability.\nFully fused CUDA kernel of anti-alised activation (upsampling + activation + downsampling) with inference speed benchmark.\nJul 2024 (v2.2): The repository now includes an interactive local demo using gradio.\nJul 2024 (v2.1): BigVGAN is now integrated with ü§ó Hugging Face Hub with easy access to inference using pretrained checkpoints. We also provide an interactive demo on Hugging Face Spaces.\nJul 2024 (v2): We release BigVGAN-v2 along with pretrained checkpoints. Below are the highlights:\nCustom CUDA kernel for inference: we provide a fused upsampling + activation kernel written in CUDA for accelerated inference speed. Our test shows 1.5 - 3x faster speed on a single A100 GPU.\nImproved discriminator and loss: BigVGAN-v2 is trained using a multi-scale sub-band CQT discriminator and a multi-scale mel spectrogram loss.\nLarger training data: BigVGAN-v2 is trained using datasets containing diverse audio types, including speech in multiple languages, environmental sounds, and instruments.\nWe provide pretrained checkpoints of BigVGAN-v2 using diverse audio configurations, supporting up to 44 kHz sampling rate and 512x upsampling ratio.\nInstallation\nThis repository contains pretrained BigVGAN checkpoints with easy access to inference and additional huggingface_hub support.\nIf you are interested in training the model and additional functionalities, please visit the official GitHub repository for more information: https://github.com/NVIDIA/BigVGAN\ngit lfs install\ngit clone https://huggingface.co/nvidia/bigvgan_v2_44khz_128band_512x\nUsage\nBelow example describes how you can use BigVGAN: load the pretrained BigVGAN generator from Hugging Face Hub, compute mel spectrogram from input waveform, and generate synthesized waveform using the mel spectrogram as the model's input.\ndevice = 'cuda'\nimport torch\nimport bigvgan\nimport librosa\nfrom meldataset import get_mel_spectrogram\n# instantiate the model. You can optionally set use_cuda_kernel=True for faster inference.\nmodel = bigvgan.BigVGAN.from_pretrained('nvidia/bigvgan_v2_44khz_128band_512x', use_cuda_kernel=False)\n# remove weight norm in the model and set to eval mode\nmodel.remove_weight_norm()\nmodel = model.eval().to(device)\n# load wav file and compute mel spectrogram\nwav_path = '/path/to/your/audio.wav'\nwav, sr = librosa.load(wav_path, sr=model.h.sampling_rate, mono=True) # wav is np.ndarray with shape [T_time] and values in [-1, 1]\nwav = torch.FloatTensor(wav).unsqueeze(0) # wav is FloatTensor with shape [B(1), T_time]\n# compute mel spectrogram from the ground truth audio\nmel = get_mel_spectrogram(wav, model.h).to(device) # mel is FloatTensor with shape [B(1), C_mel, T_frame]\n# generate waveform from mel\nwith torch.inference_mode():\nwav_gen = model(mel) # wav_gen is FloatTensor with shape [B(1), 1, T_time] and values in [-1, 1]\nwav_gen_float = wav_gen.squeeze(0).cpu() # wav_gen is FloatTensor with shape [1, T_time]\n# you can convert the generated waveform to 16 bit linear PCM\nwav_gen_int16 = (wav_gen_float * 32767.0).numpy().astype('int16') # wav_gen is now np.ndarray with shape [1, T_time] and int16 dtype\nUsing Custom CUDA Kernel for Synthesis\nYou can apply the fast CUDA inference kernel by using a parameter use_cuda_kernel when instantiating BigVGAN:\nimport bigvgan\nmodel = bigvgan.BigVGAN.from_pretrained('nvidia/bigvgan_v2_44khz_128band_512x', use_cuda_kernel=True)\nWhen applied for the first time, it builds the kernel using nvcc and ninja. If the build succeeds, the kernel is saved to alias_free_activation/cuda/build and the model automatically loads the kernel. The codebase has been tested using CUDA 12.1.\nPlease make sure that both are installed in your system and nvcc installed in your system matches the version your PyTorch build is using.\nFor detail, see the official GitHub repository: https://github.com/NVIDIA/BigVGAN?tab=readme-ov-file#using-custom-cuda-kernel-for-synthesis\nPretrained Models\nWe provide the pretrained models on Hugging Face Collections.\nOne can download the checkpoints of the generator weight (named bigvgan_generator.pt) and its discriminator/optimizer states (named bigvgan_discriminator_optimizer.pt) within the listed model repositories.\nModel Name\nSampling Rate\nMel band\nfmax\nUpsampling Ratio\nParams\nDataset\nSteps\nFine-Tuned\nbigvgan_v2_44khz_128band_512x\n44 kHz\n128\n22050\n512\n122M\nLarge-scale Compilation\n5M\nNo\nbigvgan_v2_44khz_128band_256x\n44 kHz\n128\n22050\n256\n112M\nLarge-scale Compilation\n5M\nNo\nbigvgan_v2_24khz_100band_256x\n24 kHz\n100\n12000\n256\n112M\nLarge-scale Compilation\n5M\nNo\nbigvgan_v2_22khz_80band_256x\n22 kHz\n80\n11025\n256\n112M\nLarge-scale Compilation\n5M\nNo\nbigvgan_v2_22khz_80band_fmax8k_256x\n22 kHz\n80\n8000\n256\n112M\nLarge-scale Compilation\n5M\nNo\nbigvgan_24khz_100band\n24 kHz\n100\n12000\n256\n112M\nLibriTTS\n5M\nNo\nbigvgan_base_24khz_100band\n24 kHz\n100\n12000\n256\n14M\nLibriTTS\n5M\nNo\nbigvgan_22khz_80band\n22 kHz\n80\n8000\n256\n112M\nLibriTTS + VCTK + LJSpeech\n5M\nNo\nbigvgan_base_22khz_80band\n22 kHz\n80\n8000\n256\n14M\nLibriTTS + VCTK + LJSpeech\n5M\nNo",
    "ucd-dare/CarDreamer": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nüåç Learn to Drive in \"Dreams\": CarDreamer üöó\nCan world models imagine traffic dynamics for training autonomous driving agents? The answer is YES!\nüìö Open-Source World Model-Based Autonomous Driving Platform\nüìã Prerequisites\n:gear: Quick Start\n:mechanical_arm: Training\nCreating Tasks\nObservation Customization\n:hammer: System\nCredits\n:star2: Citation\nCredits\nüë• Contributors\nCredits\nüåç Learn to Drive in \"Dreams\": CarDreamer üöó\nGitHub Repo\nHuggingFace Checkpoints\n|\nCarDreamer API Documents\n|\nArXiv Pre-print\nUnleash the power of imagination and generalization of world models for self-driving cars.\nJuly 2024: Created a stop-sign task and a traffic-light task!\nJuly 2024: Uploaded all the task checkpoints to HuggingFace\nCan world models imagine traffic dynamics for training autonomous driving agents? The answer is YES!\nIntegrating the high-fidelity CARLA simulator with world models, we are able to train a world model that not only learns complex environment dynamics but also have an agent interact with the neural network \"simulator\" to learn to drive.\nSimply put, in CarDreamer the agent can learn to drive in a \"dream world\" from scratch, mastering maneuvers like overtaking and right turns, and avoiding collisions in heavy traffic‚Äîall within an imagined world!\nDive into our demos to see the agent skillfully navigating challenges and ensuring safe and efficient travel.\nüìö Open-Source World Model-Based Autonomous Driving Platform\nExplore world model based autonomous driving with CarDreamer, an open-source platform designed for the development and evaluation of world model based autonomous driving.\nüèôÔ∏è Built-in Urban Driving Tasks: flexible and customizable observation modality and observability; optimized rewards\nüîß Task Development Suite: create your own urban driving tasks with ease\nüåç Model Backbones: integrated state-of-the-art world models\nDocumentation: CarDreamer API Documents.\nLooking for more techincal details? Check our report here! Paper link\nüìã Prerequisites\nClone the repository:\ngit clone https://github.com/ucd-dare/CarDreamer\ncd CarDreamer\nDownload CARLA release of version 0.9.15 as we experiemented with this version. Set the following environment variables:\nexport CARLA_ROOT=\"</path/to/carla>\"\nexport PYTHONPATH=\"${CARLA_ROOT}/PythonAPI/carla\":${PYTHONPATH}\nInstall the package using flit. The --symlink flag is used to create a symlink to the package in the Python environment, so that changes to the package are immediately available without reinstallation. (--pth-file also works, as an alternative to --symlink.)\nconda create python=3.10 --name cardreamer\nconda activate cardreamer\npip install flit\nflit install --symlink\n:gear: Quick Start\n:mechanical_arm: Training\nFind README.md in the corresponding directory of the algorithm you want to use and follow the instructions. For example, to train DreamerV3 agents, use\nbash train_dm3.sh 2000 0 --task carla_four_lane --dreamerv3.logdir ./logdir/carla_four_lane\nThe command will launch CARLA at 2000 port, load task a built-in task named carla_four_lane, and start the visualization tool at port 9000 (2000+7000) which can be accessed through http://localhost:9000/. You can append flags to the command to overwrite yaml configurations.\nCreating Tasks\nThe section explains how to create CarDreamer tasks in a standalone mode without loading our integrated models. This can be helpful if you want to train and evaluate your own models other than our integrated DreamerV2 and DreamerV3 on CarDreamer tasks.\nCarDreamer offers a range of built-in task classes, which you can explore in the CarDreamer Docs: Tasks and Configurations.\nEach task class can be instantiated with various configurations. For instance, the right-turn task can be set up with simple, medium, or hard settings. These settings are defined in YAML blocks within tasks.yaml. The task creation API retrieves the given identifier (e.g., carla_four_lane_hard) from these YAML task blocks and injects the settings into the task class to create a gym task instance.\n# Create a gym environment with default task configurations\nimport car_dreamer\ntask, task_configs = car_dreamer.create_task('carla_four_lane_hard')\n# Or load default environment configurations without instantiation\ntask_configs = car_dreamer.load_task_configs('carla_right_turn_hard')\nTo create your own driving tasks using the development suite, refer to CarDreamer Docs: Customization.\nObservation Customization\nCarDreamer employs an Observer-Handler architecture to manage complex multi-modal observation spaces. Each handler defines its own observation space and lifecycle for stepping, resetting, or fetching information, similar to a gym environment. The agent communicates with the environment through an observer that manages these handlers.\nUsers can enable built-in observation handlers such as BEV, camera, LiDAR, and spectator in task configurations. Check out common.yaml for all available built-in handlers. Additionally, users can customize observation handlers and settings to suit their specific needs.\nHandler Implementation\nTo implement new handlers for different observation sources and modalities (e.g., text, velocity, locations, or even more complex data), CarDreamer provides two methods:\nRegister a callback as a SimpleHandler to fetch data at each step.\nFor observations requiring complex workflows that cannot be conveyed by a SimpleHandler, create an handler maintaining the full lifecycle of that observation, similar to our built-in message, BEV, spectator handlers.\nFor more details on defining new observation sources, see CarDreamer Docs: Defining a new observation source.\nObservation Handler Configurations\nEach handler can access yaml configurations for further customization. For example, a BEV handler setting can be defined as:\nbirdeye_view:\n# Specify the handler name used to produce `birdeye_view` observation\nhandler: birdeye\n# The observation key\nkey: birdeye_view\n# Define what to render in the birdeye view\nentities: [roadmap, waypoints, background_waypoints, fov_lines, ego_vehicle, background_vehicles]\n# ... other settings used by the BEV handler\nThe handler field specifies which handler implementation is used to manage that observation key. Then, users can simply enable this observation in the task settings.\nyour_task_name:\nenv:\nobservation.enabled: [camera, collision, spectator, birdeye_view]\nEnvironment & Observer Communications\nOne might need transfer information from the environements to a handler to compute their observations. E.g., a BEV handler might need a location to render the destination spot. These environment information can be accessed either through WorldManager APIs, or through environment state management.\nA WorldManager instance is passed in the handler during its initialization. The environment states are defined by an environment's get_state() API, and passed as parameters to handler's get_observation().\nclass MyHandler(BaseHandler):\ndef __init__(self, world: WorldManager, config):\nsuper().__init__(world, config)\nself._world = world\ndef get_observation(self, env_state: Dict) -> Tuple[Dict, Dict]:\n# Get the waypoints through environment states\nwaypoints = env_state.get(\"waypoints\")\n# Get actors through the world manager API\nactors = self._world.actors\n# ...\nclass MyEnv(CarlaBaseEnv):\n# ...\ndef get_state(self):\nreturn {\n# Expose the waypoints through get_state()\n'waypoints': self.waypoints,\n}\n:hammer: System\nTo easily customize your own driving tasks, and observation spaces, etc., please refer to our CarDreamer API Documents.\n:star2: Citation\nIf you find this repository useful, please cite this paper:\nPaper link\n@article{CarDreamer2024,\ntitle = {{CarDreamer: Open-Source Learning Platform for World Model based Autonomous Driving}},\nauthor = {Dechen Gao, Shuangyu Cai, Hanchu Zhou, Hang Wang, Iman Soltani, Junshan Zhang},\njournal = {arXiv preprint arXiv:2405.09111},\nyear = {2024},\nmonth = {May}\n}\nüë• Contributors\nCredits\nCarDreamer builds on the several projects within the autonomous driving and machine learning communities.\ngym-carla\nDreamerV2\nDreamerV3\nCuriousReplay",
    "TheDrummer/Tiger-Gemma-9B-v2": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTiger Gemma 9B v2\nLinks\nWhat's New with v2?\nTiger Gemma 9B v2\nDecensored Gemma 9B. No refusals so far. No apparent brain damage.\nLinks\nOriginal: https://huggingface.co/TheDrummer/Tiger-Gemma-9B-v2\nGGUF: https://huggingface.co/TheDrummer/Tiger-Gemma-9B-v2-GGUF\nWhat's New with v2?\nIt's a simple decensor but with a lighter touch, allowing it to be as Gemma as possible.\nIn memory of Tiger (the happy street cat on the right)",
    "meta-llama/Llama-3.1-70B-Instruct": "A newer version of this model is available:\nmeta-llama/Llama-3.3-70B-Instruct\nYou need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3_1/use-policy\nProhibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.1 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.1 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nTool use with transformers\nUse with bitsandbytes\nUse with llama\nHardware and Software\nTraining Data\nBenchmark scores\nBase pretrained models\nInstruction tuned models\nMultilingual benchmarks\nResponsibility & Safety\nResponsible deployment\nLlama 3.1 instruct\nLlama 3.1 systems\nNew capabilities\nEvaluations\nCritical and other risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.1 (text only)\nA new mix of publicly available online data.\n8B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n405B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: July 23, 2024.\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Meta-Llama-3.1-70B-Instruct, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nSee the snippet below for usage with Transformers:\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nTool use with transformers\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting here.\nTool use is also supported through chat templates in Transformers.\nHere is a quick example showing a single simple tool:\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n\"\"\"\nGet the current temperature at a location.\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nReturns:\nThe current temperature at the specified location in the specified units, as a float.\n\"\"\"\nreturn 22.  # A real function should probably actually get the temperature!\n# Next, create a chat and apply the chat template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n{\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\nand then call the tool and append the result, with the tool role, like so:\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the LLaMA prompt format docs and the Transformers tool use documentation.\nUse with bitsandbytes\nThe model checkpoints can be used in 8-bit and 4-bit for further memory optimisations using bitsandbytes and transformers\nSee the snippet below for usage:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nquantized_model = AutoModelForCausalLM.from_pretrained(\nmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nTo load in 4-bit simply pass load_in_4bit=True\nUse with llama\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-70B-Instruct\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions\n(tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions\n(tons CO2eq)\nLlama 3.1 8B\n1.46M\n700\n420\n0\nLlama 3.1 70B\n7.0M\n700\n2,040\n0\nLlama 3.1 405B\n30.84M\n700\n8,930\n0\nTotal\n39.3M\n11,390\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmark scores\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\nBase pretrained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B\nLlama 3.1 8B\nLlama 3 70B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n66.7\n66.7\n79.5\n79.3\n85.2\nMMLU-Pro (CoT)\n5\nmacro_avg/acc_char\n36.2\n37.1\n55.0\n53.8\n61.6\nAGIEval English\n3-5\naverage/acc_char\n47.1\n47.8\n63.0\n64.6\n71.6\nCommonSenseQA\n7\nacc_char\n72.6\n75.0\n83.8\n84.1\n85.8\nWinogrande\n5\nacc_char\n-\n60.5\n-\n83.3\n86.7\nBIG-Bench Hard (CoT)\n3\naverage/em\n61.1\n64.2\n81.3\n81.6\n85.9\nARC-Challenge\n25\nacc_char\n79.4\n79.7\n93.1\n92.9\n96.1\nKnowledge reasoning\nTriviaQA-Wiki\n5\nem\n78.5\n77.6\n89.7\n89.8\n91.8\nReading comprehension\nSQuAD\n1\nem\n76.4\n77.0\n85.6\n81.8\n89.3\nQuAC (F1)\n1\nf1\n44.4\n44.9\n51.1\n51.1\n53.6\nBoolQ\n0\nacc_char\n75.7\n75.0\n79.0\n79.4\n80.0\nDROP (F1)\n3\nf1\n58.4\n59.5\n79.7\n79.6\n84.8\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B Instruct\nLlama 3.1 8B Instruct\nLlama 3 70B Instruct\nLlama 3.1 70B Instruct\nLlama 3.1 405B Instruct\nGeneral\nMMLU\n5\nmacro_avg/acc\n68.5\n69.4\n82.0\n83.6\n87.3\nMMLU (CoT)\n0\nmacro_avg/acc\n65.3\n73.0\n80.9\n86.0\n88.6\nMMLU-Pro (CoT)\n5\nmicro_avg/acc_char\n45.5\n48.3\n63.4\n66.4\n73.3\nIFEval\n76.8\n80.4\n82.9\n87.5\n88.6\nReasoning\nARC-C\n0\nacc\n82.4\n83.4\n94.4\n94.8\n96.9\nGPQA\n0\nem\n34.6\n30.4\n39.5\n46.7\n50.7\nCode\nHumanEval\n0\npass@1\n60.4\n72.6\n81.7\n80.5\n89.0\nMBPP ++ base version\n0\npass@1\n70.6\n72.8\n82.5\n86.0\n88.6\nMultipl-E HumanEval\n0\npass@1\n-\n50.8\n-\n65.5\n75.2\nMultipl-E MBPP\n0\npass@1\n-\n52.4\n-\n62.0\n65.7\nMath\nGSM-8K (CoT)\n8\nem_maj1@1\n80.6\n84.5\n93.0\n95.1\n96.8\nMATH (CoT)\n0\nfinal_em\n29.1\n51.9\n51.0\n68.0\n73.8\nTool Use\nAPI-Bank\n0\nacc\n48.3\n82.6\n85.1\n90.0\n92.0\nBFCL\n0\nacc\n60.3\n76.1\n83.0\n84.8\n88.5\nGorilla Benchmark API Bench\n0\nacc\n1.7\n8.2\n14.7\n29.7\n35.3\nNexus (0-shot)\n0\nmacro_avg/acc\n18.1\n38.5\n47.8\n56.7\n58.7\nMultilingual\nMultilingual MGSM (CoT)\n0\nem\n-\n68.9\n-\n86.9\n91.6\nMultilingual benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.1 8B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n62.12\n80.13\n84.95\nSpanish\n62.45\n80.05\n85.08\nItalian\n61.63\n80.4\n85.04\nGerman\n60.59\n79.27\n84.36\nFrench\n62.34\n79.82\n84.66\nHindi\n50.88\n74.52\n80.31\nThai\n50.32\n72.95\n78.21\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.1 systems\nLarge language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teaming\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nOur study of Llama-3.1-405B‚Äôs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "maria26/Floor_Plan_LoRA": "Floor Plan Genrator\nModel description\nTrigger words\nDownload model\nFloor Plan Genrator\nPrompt\nFloor plan of a small apartment, few rooms, one bathroom, big kitchen, many windows.\nModel description\nThe model generates architectural floor plans in the style of the provided image from text descriptions and it is part of my Bachelor Thesis.\nThis project explored the use of diffusion models for generating architectural floor plans based on textual descriptions.\nGithHub repository:\nhttps://github.com/mariaaoprea/Diffusion-Models-for-floor-plan-drafting.git\nTrigger words\nYou should use prompts following this structure:\n\"Floor plan of a small/big apartment, few/many rooms, one/multiple bathrooms, small/big kitchen, few/many windows\"\nDownload model\nWeights for this model are available in Safetensors,PyTorch format.\nDownload them in the Files & versions tab.",
    "stabilityai/stable-fast-3d": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Fast 3D\nUsage\nIntended Uses\nOut-of-Scope Uses\nSafety\nContact\nStable Fast 3D\nStable Fast 3D (SF3D) is a large reconstruction model which takes in a single image of an object and generates a textured UV-unwrapped 3D mesh asset.\nPlease note: For individuals or organizations generating annual revenue of US $1,000,000 (or local currency equivalent) or more, regardless of the source of that revenue, you must obtain an enterprise commercial license directly from Stability AI before commercially using SF3D or any derivative work of SF3D or its outputs, such as \"fine tune\" models. You may submit a request for an Enterprise License at https://stability.ai/enterprise. Please refer to Stability AI's Community License, available at https://stability.ai/license, for more information.\nModel Description\nDeveloped by: Stability AI\nModel type: Transformer image-to-3D model\nModel details: This model is trained to create a 3D model from a single image in under one second. The asset is UV-unwrapped and textured and has a relatively low polygon count. We also perform a delighting step, enabling easier asset usage in downstream applications such as game engines or rendering work. The model also predicts per-object material parameters (roughness, metallic), enhancing reflective behaviors during rendering. The model expects an input size of 512x512 pixels.\nPlease check our tech report and video summary for details.\nLicense\nCommunity License: Free for research, non-commercial, and commercial use by organizations and individuals generating annual revenue of US $1,000,000 (or local currency equivalent) or less, regardless of the source of that revenue. If your annual revenue exceeds US $1M, any commercial use of this model or derivative works thereof requires obtaining an Enterprise License directly from Stability AI. You may submit a request for an Enterprise License at https://stability.ai/enterprise. Please refer to Stability AI's Community License, available at https://stability.ai/license, for more information.\nModel Sources\nRepository: https://github.com/Stability-AI/stable-fast-3d\nTech report: https://arxiv.org/pdf/2408.00653\nVideo summary: https://youtu.be/uT96UCBSBko\nProject page: https://stable-fast-3d.github.io\narXiv page: https://arxiv.org/abs/2408.00653\nTraining Dataset\nWe use renders from the Objaverse dataset, available under the Open Data Commons Attribution License. We utilize our enhanced rendering method, which more closely replicates the distribution of images found in the real world, significantly improving our model's ability to generalize. We filter objects based on the review of licenses and curate a subset suitable for our training needs.\nUsage\nFor usage instructions, please refer to our GitHub repository\nIntended Uses\nIntended uses include the following:\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on reconstruction models, including understanding the limitations of these models.\nAll uses of the model should be in accordance with our Acceptable Use Policy.\nOut-of-Scope Uses\nThe model was not trained to be factual or true representations of people or events. As such, using the model to generate such content is out-of-scope of the abilities of this model.\nSafety\nAs part of our safety-by-design and responsible AI deployment approach, we implement safety measures throughout the development of our models, from the time we begin pre-training a model to the ongoing development, fine-tuning, and deployment of each model. We have implemented a number of safety mitigations that are intended to reduce the risk of severe harms. However, we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.For more about our approach to Safety, please visit our Safety page.\nContact\nPlease report any issues with the model or contact us:\nSafety issues:  safety@stability.ai\nSecurity issues:  security@stability.ai\nPrivacy issues:  privacy@stability.ai\nLicense and general: https://stability.ai/license\nEnterprise license: https://stability.ai/enterprise",
    "OpenGVLab/InternVL2-40B-AWQ": "InternVL2-40B-AWQ\nIntroduction\nINT4 Weight-only Quantization and Deployment (W4A16)\nInference\nService\nLicense\nCitation\nInternVL2-40B-AWQ\n[üìÇ GitHub] [üìú InternVL 1.0] [üìú InternVL 1.5] [üìú Mini-InternVL] [üìú InternVL 2.5]\n[üÜï Blog] [üó®Ô∏è Chat Demo] [ü§ó HF Demo] [üöÄ Quick Start] [üìñ Documents]\nIntroduction\nINT4 Weight-only Quantization and Deployment (W4A16)\nLMDeploy adopts AWQ algorithm for 4bit weight-only quantization. By developed the high-performance cuda kernel, the 4bit quantized model inference achieves up to 2.4x faster than FP16.\nLMDeploy supports the following NVIDIA GPU for W4A16 inference:\nTuring(sm75): 20 series, T4\nAmpere(sm80,sm86): 30 series, A10, A16, A30, A100\nAda Lovelace(sm90): 40 series\nBefore proceeding with the quantization and inference, please ensure that lmdeploy is installed.\npip install lmdeploy>=0.5.3\nThis article comprises the following sections:\nInference\nService\nInference\nTrying the following codes, you can perform the batched offline inference with the quantized model:\nfrom lmdeploy import pipeline, TurbomindEngineConfig\nfrom lmdeploy.vl import load_image\nmodel = 'OpenGVLab/InternVL2-40B-AWQ'\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\nbackend_config = TurbomindEngineConfig(model_format='awq')\npipe = pipeline(model, backend_config=backend_config, log_level='INFO')\nresponse = pipe(('describe this image', image))\nprint(response.text)\nFor more information about the pipeline parameters, please refer to here.\nService\nLMDeploy's api_server enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:\nlmdeploy serve api_server OpenGVLab/InternVL2-40B-AWQ --server-port 23333 --model-format awq\nTo use the OpenAI-style interface, you need to install OpenAI:\npip install openai\nThen, use the code below to make the API call:\nfrom openai import OpenAI\nclient = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=[{\n'role':\n'user',\n'content': [{\n'type': 'text',\n'text': 'describe this image',\n}, {\n'type': 'image_url',\n'image_url': {\n'url':\n'https://modelscope.oss-cn-beijing.aliyuncs.com/resource/tiger.jpeg',\n},\n}],\n}],\ntemperature=0.8,\ntop_p=0.8)\nprint(response)\nLicense\nThis project is released under the MIT License. This project uses the pre-trained Nous-Hermes-2-Yi-34B as a component, which is licensed under the Apache License 2.0.\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{chen2024expanding,\ntitle={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\nauthor={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\njournal={arXiv preprint arXiv:2412.05271},\nyear={2024}\n}\n@article{gao2024mini,\ntitle={Mini-internvl: A flexible-transfer pocket multimodal model with 5\\% parameters and 90\\% performance},\nauthor={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},\njournal={arXiv preprint arXiv:2410.16261},\nyear={2024}\n}\n@article{chen2024far,\ntitle={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\nauthor={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\njournal={arXiv preprint arXiv:2404.16821},\nyear={2024}\n}\n@inproceedings{chen2024internvl,\ntitle={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\nauthor={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\npages={24185--24198},\nyear={2024}\n}",
    "Salesforce/xLAM-1b-fc-r": "Table of Contents\nModel Series\nRepository Overview\nFramework Versions\nBenchmark Results\nUsage\nBasic Usage with Huggingface\nUsage with vLLM\n1. Test Prompt Template\n2. Test xLAM Model with a Manually Served Endpoint\n3. Test xLAM Model by Directly Using vLLM Library\nCustomization\nLicense\nEthical Considerations\nCitation\n[Homepage]  |\n[APIGen Paper] |\n[ActionStudio Paper] |\n[Discord] |\n[Dataset] |\n[Github]\nWelcome to the xLAM model family! Large Action Models (LAMs) are advanced large language models designed to enhance decision-making and translate user intentions into executable actions that interact with the world. LAMs autonomously plan and execute tasks to achieve specific goals, serving as the brains of AI agents. They have the potential to automate workflow processes across various domains, making them invaluable for a wide range of applications.\nTable of Contents\nModel Series\nRepository Overview\nBenchmark Results\nUsage\nBasic Usage with Huggingface\nUsage with vLLM\nLicense\nCitation\nModel Series\nWe provide a series of xLAMs in different sizes to cater to various applications, including those optimized for function-calling and general agent applications:\nModel\n# Total Params\nContext Length\nRelease Date\nCategory\nDownload Model\nDownload GGUF files\nxLAM-7b-r\n7.24B\n32k\nSep. 5, 2024\nGeneral,  Function-calling\nü§ó Link\n--\nxLAM-8x7b-r\n46.7B\n32k\nSep. 5, 2024\nGeneral,  Function-calling\nü§ó Link\n--\nxLAM-8x22b-r\n141B\n64k\nSep. 5, 2024\nGeneral,  Function-calling\nü§ó Link\n--\nxLAM-1b-fc-r\n1.35B\n16k\nJuly 17, 2024\nFunction-calling\nü§ó Link\nü§ó Link\nxLAM-7b-fc-r\n6.91B\n4k\nJuly 17, 2024\nFunction-calling\nü§ó Link\nü§ó Link\nxLAM-v0.1-r\n46.7B\n32k\nMar. 18, 2024\nGeneral,  Function-calling\nü§ó Link\n--\nThe fc series of models are optimized for function-calling capability, providing fast, accurate, and structured responses based on input queries and available APIs. These models are fine-tuned based on the deepseek-coder models and are designed to be small enough for deployment on personal devices like phones or computers.\nWe also provide their quantized GGUF files for efficient deployment and execution. GGUF is a file format designed to efficiently store and load large language models, making GGUF ideal for running AI models on local devices with limited resources, enabling offline functionality and enhanced privacy.\nFor more details, check our GitHub and paper.\nRepository Overview\nThis repository is focused on our tiny xLAM-1b-fc-r model, which is optimized for function-calling and can be easily deployed on personal devices.\nFunction-calling, or tool use, is one of the key capabilities for AI agents. It requires the model not only understand and generate human-like text but also to execute functional API calls based on natural language instructions. This extends the utility of LLMs beyond simple conversation tasks to dynamic interactions with a variety of digital services and applications, such as retrieving weather information, managing social media platforms, and handling financial services.\nThe instructions will guide you through the setup, usage, and integration of xLAM-1b-fc-r with HuggingFace and vLLM.\nWe will first introduce the basic usage, and then walk through the provided tutorial and example scripts in the examples folder.\nFramework Versions\nTransformers 4.41.0\nPytorch 2.3.0+cu121\nDatasets 2.19.1\nTokenizers 0.19.1\nBenchmark Results\nWe mainly test our function-calling models on the Berkeley Function-Calling Leaderboard (BFCL), which offers a comprehensive evaluation framework for assessing LLMs' function-calling capabilities across various programming languages and application domains like Java, JavaScript, and Python.\nPerformance comparison on the BFCL benchmark as of date 07/18/2024. Evaluated with temperature=0.001 and top_p=1\nOur xLAM-7b-fc-r secures the 3rd place with an overall accuracy of 88.24% on the leaderboard, outperforming many strong models. Notably, our xLAM-1b-fc-r model is the only tiny model with less than 2B parameters on the leaderboard, but still achieves a competitive overall accuracy of 78.94% and outperforming GPT3-Turbo and many larger models.\nBoth models exhibit balanced performance across various categories, showing their strong function-calling capabilities despite their small sizes.\nSee our paper and Github repo for more detailed analysis.\nUsage\nBasic Usage with Huggingface\nTo use the xLAM-1b-fc-r model from Huggingface, please first install the transformers library:\npip install transformers>=4.41.0\nWe use the following example to illustrate how to use our model to perform function-calling tasks.\nPlease note that, our model works best with our provided prompt format.\nIt allows us to extract JSON output that is similar to the function-calling mode of ChatGPT.\nimport json\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntorch.random.manual_seed(0)\nmodel_name = \"Salesforce/xLAM-1b-fc-r\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Please use our provided instruction prompt for best performance\ntask_instruction = \"\"\"\nYou are an expert in composing functions. You are given a question and a set of possible functions.\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose.\nIf none of the functions can be used, point it out and refuse to answer.\nIf the given question lacks the parameters required by the function, also point it out.\n\"\"\".strip()\nformat_instruction = \"\"\"\nThe output MUST strictly adhere to the following JSON format, and NO other text MUST be included.\nThe example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make tool_calls an empty list '[]'.\n\n\"\"\".strip()\n# Define the input query and available tools\nquery = \"What's the weather like in New York in fahrenheit?\"\nget_weather_api = {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather for a location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, New York\"\n},\n\"unit\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The unit of temperature to return\"\n}\n},\n\"required\": [\"location\"]\n}\n}\nsearch_api = {\n\"name\": \"search\",\n\"description\": \"Search for information on the internet\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"query\": {\n\"type\": \"string\",\n\"description\": \"The search query, e.g. 'latest news on AI'\"\n}\n},\n\"required\": [\"query\"]\n}\n}\nopenai_format_tools = [get_weather_api, search_api]\n# Helper function to convert openai format tools to our more concise xLAM format\ndef convert_to_xlam_tool(tools):\n''''''\nif isinstance(tools, dict):\nreturn {\n\"name\": tools[\"name\"],\n\"description\": tools[\"description\"],\n\"parameters\": {k: v for k, v in tools[\"parameters\"].get(\"properties\", {}).items()}\n}\nelif isinstance(tools, list):\nreturn [convert_to_xlam_tool(tool) for tool in tools]\nelse:\nreturn tools\n# Helper function to build the input prompt for our model\ndef build_prompt(task_instruction: str, format_instruction: str, tools: list, query: str):\nprompt = f\"[BEGIN OF TASK INSTRUCTION]\\n{task_instruction}\\n[END OF TASK INSTRUCTION]\\n\\n\"\nprompt += f\"[BEGIN OF AVAILABLE TOOLS]\\n{json.dumps(xlam_format_tools)}\\n[END OF AVAILABLE TOOLS]\\n\\n\"\nprompt += f\"[BEGIN OF FORMAT INSTRUCTION]\\n{format_instruction}\\n[END OF FORMAT INSTRUCTION]\\n\\n\"\nprompt += f\"[BEGIN OF QUERY]\\n{query}\\n[END OF QUERY]\\n\\n\"\nreturn prompt\n# Build the input and start the inference\nxlam_format_tools = convert_to_xlam_tool(openai_format_tools)\ncontent = build_prompt(task_instruction, format_instruction, xlam_format_tools, query)\nmessages=[\n{ 'role': 'user', 'content': content}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\nThen you should be able to see the following output string in JSON format:\n{\"tool_calls\": [{\"name\": \"get_weather\", \"arguments\": {\"location\": \"New York\", \"unit\": \"fahrenheit\"}}]}\nWe highly recommend to use our provided prompt format and helper functions to yield the best function-calling performance of our model.\nUsage with vLLM\nWe provide example scripts to deploy our model with vllm and run inferences. First, install the required packages:\npip install vllm openai argparse jinja2\nThe example scripts are located in the examples folder.\n1. Test Prompt Template\nTo build prompts using the chat template and output formatted prompts ready for various test cases, run:\npython test_prompt_template.py --model\n2. Test xLAM Model with a Manually Served Endpoint\na. Serve the model with vLLM:\npython -m vllm.entrypoints.openai.api_server --model Salesforce/xLAM-1b-fc-r --served-model-name xlam-1b-fc-r --dtype bfloat16 --port 8001\nb. Run the test script:\npython test_xlam_model_with_endpoint.py --model_name xlam-1b-fc-r --port 8001 [OPTIONS]\nOptions:\n--temperature: Default 0.3\n--top_p: Default 1.0\n--max_tokens: Default 512\nThis test script provides a handler implementation that can be easily applied to your customized function-calling applications.\n3. Test xLAM Model by Directly Using vLLM Library\nTo test the xLAM model directly with the vLLM library, run:\npython test_xlam_model_with_vllm.py --model Salesforce/xLAM-1b-fc-r [OPTIONS]\nOptions are the same as for the endpoint test. This test script also provides a handler implementation that can be easily applied to your customized function-calling applications.\nCustomization\nThese examples are designed to be flexible and easily integrated into your own projects. Feel free to modify the scripts to suit your specific needs and applications. You can adjust test queries or API definitions in each script to test different scenarios or model capabilities.\nAdditional customization tips:\nModify the --dtype parameter when serving the model based on your GPU capacity.\nRefer to the vLLM documentation for more detailed configuration options.\nExplore the demo.ipynb file for a comprehensive description of the entire workflow, including how to execute APIs.\nThese resources provide a robust foundation for integrating xLAM models into your applications, allowing for tailored and efficient deployment.\nLicense\nxLAM-1b-fc-r is distributed under the CC-BY-NC-4.0 license, with additional terms specified in the Deepseek license.\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people‚Äôs lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nCitation\nIf you find this repo helpful, please cite our paper:\n@article{zhang2024xlam,\ntitle={xlam: A family of large action models to empower ai agent systems},\nauthor={Zhang, Jianguo and Lan, Tian and Zhu, Ming and Liu, Zuxin and Hoang, Thai and Kokane, Shirley and Yao, Weiran and Tan, Juntao and Prabhakar, Akshara and Chen, Haolin and others},\njournal={arXiv preprint arXiv:2409.03215},\nyear={2024}\n}\n@article{liu2024apigen,\ntitle={APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets},\nauthor={Liu, Zuxin and Hoang, Thai and Zhang, Jianguo and Zhu, Ming and Lan, Tian and Kokane, Shirley and Tan, Juntao and Yao, Weiran and Liu, Zhiwei and Feng, Yihao and others},\njournal={arXiv preprint arXiv:2406.18518},\nyear={2024}\n@article{zhang2025actionstudio,\ntitle={ActionStudio: A Lightweight Framework for Data and Training of Action Models},\nauthor={Zhang, Jianguo and Hoang, Thai and Zhu, Ming and Liu, Zuxin and Wang, Shiyu and Awalgaonkar, Tulika and Prabhakar, Akshara and Chen, Haolin and Yao, Weiran and Liu, Zhiwei and others},\njournal={arXiv preprint arXiv:2503.22673},\nyear={2025}\n}",
    "hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4": "Model Information\nModel Usage\nü§ó Transformers\nAutoAWQ\nü§ó Text Generation Inference (TGI)\nvLLM\nQuantization Reproduction\nThis repository is a community-driven quantized version of the original model meta-llama/Meta-Llama-3.1-405B-Instruct which is the FP16 half-precision official version released by Meta AI.\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nThis repository contains meta-llama/Meta-Llama-3.1-405B-Instruct quantized using AutoAWQ from FP16 down to INT4 using the GEMM kernels performing zero-point quantization with a group size of 128.\nModel Usage\nIn order to run the inference with Llama 3.1 405B Instruct AWQ in INT4, around 203 GiB of VRAM are needed only for loading the model checkpoint, without including the KV cache or the CUDA graphs, meaning that there should be a bit over that VRAM available.\nIn order to use the current quantized model, support is offered for different solutions as transformers, autoawq, or text-generation-inference.\nü§ó Transformers\nIn order to run the inference with Llama 3.1 405B Instruct AWQ in INT4, you need to install the following packages:\npip install -q --upgrade transformers autoawq accelerate\nTo run the inference on top of Llama 3.1 405B Instruct AWQ in INT4 precision, the AWQ model can be instantiated as any other causal language modeling model via AutoModelForCausalLM and run the inference normally.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\nmodel_id = \"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\"\nquantization_config = AwqConfig(\nbits=4,\nfuse_max_seq_len=512, # Note: Update this as per your use-case\ndo_fuse=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\ndevice_map=\"auto\",\nquantization_config=quantization_config\n)\nprompt = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n{\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n]\ninputs = tokenizer.apply_chat_template(\nprompt,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\n).to(\"cuda\")\noutputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\nprint(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])\nAutoAWQ\nIn order to run the inference with Llama 3.1 405B Instruct AWQ in INT4, you need to install the following packages:\npip install -q --upgrade transformers autoawq accelerate\nAlternatively, one may want to run that via AutoAWQ even though it's built on top of ü§ó transformers, which is the recommended approach instead as described above.\nimport torch\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoAWQForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\ndevice_map=\"auto\",\n)\nprompt = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n{\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n]\ninputs = tokenizer.apply_chat_template(\nprompt,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\n).to(\"cuda\")\noutputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\nprint(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])\nThe AutoAWQ script has been adapted from AutoAWQ/examples/generate.py.\nü§ó Text Generation Inference (TGI)\nTo run the text-generation-launcher with Llama 3.1 405B Instruct AWQ in INT4 with Marlin kernels for optimized inference speed, you will need to have Docker installed (see installation notes) and the huggingface_hub Python package as you need to login to the Hugging Face Hub.\npip install -q --upgrade huggingface_hub\nhuggingface-cli login\nThen you just need to run the TGI v2.2.0 (or higher) Docker container as follows:\ndocker run --gpus all --shm-size 1g -ti -p 8080:80 \\\n-v hf_cache:/data \\\n-e MODEL_ID=hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4 \\\n-e NUM_SHARD=8 \\\n-e QUANTIZE=awq \\\n-e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n-e MAX_INPUT_LENGTH=4000 \\\n-e MAX_TOTAL_TOKENS=4096 \\\nghcr.io/huggingface/text-generation-inference:2.2.0\nTGI will expose different endpoints, to see all the endpoints available check TGI OpenAPI Specification.\nTo send request to the deployed TGI endpoint compatible with OpenAI OpenAPI specification i.e. /v1/chat/completions:\ncurl 0.0.0.0:8080/v1/chat/completions \\\n-X POST \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"What is Deep Learning?\"\n}\n],\n\"max_tokens\": 128\n}'\nOr programatically via the huggingface_hub Python client as follows:\nimport os\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(base_url=\"http://0.0.0.0:8080\", api_key=os.getenv(\"HF_TOKEN\", \"-\"))\nchat_completion = client.chat.completions.create(\nmodel=\"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"What is Deep Learning?\"},\n],\nmax_tokens=128,\n)\nAlternatively, the OpenAI Python client can also be used (see installation notes) as follows:\nimport os\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://0.0.0.0:8080/v1\", api_key=os.getenv(\"OPENAI_API_KEY\", \"-\"))\nchat_completion = client.chat.completions.create(\nmodel=\"tgi\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"What is Deep Learning?\"},\n],\nmax_tokens=128,\n)\nvLLM\nTo run vLLM with Llama 3.1 405B Instruct AWQ in INT4, you will need to have Docker installed (see installation notes) and run the latest vLLM Docker container as follows:\ndocker run --runtime nvidia --gpus all --ipc=host -p 8000:8000 \\\n-v hf_cache:/root/.cache/huggingface \\\nvllm/vllm-openai:latest \\\n--model hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4 \\\n--tensor-parallel-size 8 \\\n--max-model-len 4096\nTo send request to the deployed vLLM endpoint compatible with OpenAI OpenAPI specification i.e. /v1/chat/completions:\ncurl 0.0.0.0:8000/v1/chat/completions \\\n-X POST \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"What is Deep Learning?\"\n}\n],\n\"max_tokens\": 128\n}'\nOr programatically via the openai Python client (see installation notes) as follows:\nimport os\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=os.getenv(\"VLLM_API_KEY\", \"-\"))\nchat_completion = client.chat.completions.create(\nmodel=\"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"What is Deep Learning?\"},\n],\nmax_tokens=128,\n)\nQuantization Reproduction\nIn order to quantize Llama 3.1 405B Instruct using AutoAWQ, you will need to use an instance with at least enough CPU RAM to fit the whole model i.e. ~800GiB, and an NVIDIA GPU with 80GiB of VRAM to quantize it.\nIn order to quantize Llama 3.1 405B Instruct, first install the following packages:\npip install -q --upgrade transformers autoawq accelerate\nThen run the following script, adapted from AutoAWQ/examples/quantize.py:\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\nmodel_path = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\nquant_path = \"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\"\nquant_config = {\n\"zero_point\": True,\n\"q_group_size\": 128,\n\"w_bit\": 4,\n\"version\": \"GEMM\",\n}\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(\nmodel_path, low_cpu_mem_usage=True, use_cache=False,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\nprint(f'Model is quantized and saved at \"{quant_path}\"')",
    "OpenGVLab/InternVL2-26B-AWQ": "A newer version of this model is available:\nOpenGVLab/InternVL2_5-26B-AWQ\nInternVL2-26B-AWQ\nIntroduction\nINT4 Weight-only Quantization and Deployment (W4A16)\nInference\nService\nLicense\nCitation\nInternVL2-26B-AWQ\n[üìÇ GitHub] [üìú InternVL 1.0] [üìú InternVL 1.5] [üìú Mini-InternVL] [üìú InternVL 2.5]\n[üÜï Blog] [üó®Ô∏è Chat Demo] [ü§ó HF Demo] [üöÄ Quick Start] [üìñ Documents]\nIntroduction\nINT4 Weight-only Quantization and Deployment (W4A16)\nLMDeploy adopts AWQ algorithm for 4bit weight-only quantization. By developed the high-performance cuda kernel, the 4bit quantized model inference achieves up to 2.4x faster than FP16.\nLMDeploy supports the following NVIDIA GPU for W4A16 inference:\nTuring(sm75): 20 series, T4\nAmpere(sm80,sm86): 30 series, A10, A16, A30, A100\nAda Lovelace(sm90): 40 series\nBefore proceeding with the quantization and inference, please ensure that lmdeploy is installed.\npip install lmdeploy>=0.5.3\nThis article comprises the following sections:\nInference\nService\nInference\nTrying the following codes, you can perform the batched offline inference with the quantized model:\nfrom lmdeploy import pipeline, TurbomindEngineConfig\nfrom lmdeploy.vl import load_image\nmodel = 'OpenGVLab/InternVL2-26B-AWQ'\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\nbackend_config = TurbomindEngineConfig(model_format='awq')\npipe = pipeline(model, backend_config=backend_config, log_level='INFO')\nresponse = pipe(('describe this image', image))\nprint(response.text)\nFor more information about the pipeline parameters, please refer to here.\nService\nLMDeploy's api_server enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:\nlmdeploy serve api_server OpenGVLab/InternVL2-26B-AWQ --server-port 23333 --model-format awq\nTo use the OpenAI-style interface, you need to install OpenAI:\npip install openai\nThen, use the code below to make the API call:\nfrom openai import OpenAI\nclient = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=[{\n'role':\n'user',\n'content': [{\n'type': 'text',\n'text': 'describe this image',\n}, {\n'type': 'image_url',\n'image_url': {\n'url':\n'https://modelscope.oss-cn-beijing.aliyuncs.com/resource/tiger.jpeg',\n},\n}],\n}],\ntemperature=0.8,\ntop_p=0.8)\nprint(response)\nLicense\nThis project is released under the MIT License. This project uses the pre-trained internlm2-chat-20b as a component, which is licensed under the Apache License 2.0.\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{chen2024expanding,\ntitle={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\nauthor={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\njournal={arXiv preprint arXiv:2412.05271},\nyear={2024}\n}\n@article{gao2024mini,\ntitle={Mini-internvl: A flexible-transfer pocket multimodal model with 5\\% parameters and 90\\% performance},\nauthor={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},\njournal={arXiv preprint arXiv:2410.16261},\nyear={2024}\n}\n@article{chen2024far,\ntitle={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\nauthor={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\njournal={arXiv preprint arXiv:2404.16821},\nyear={2024}\n}\n@inproceedings{chen2024internvl,\ntitle={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\nauthor={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\npages={24185--24198},\nyear={2024}\n}",
    "gaudi/opus-mt-en-ber-ctranslate2": "Repository General Information\nInspired by and derived from the work of Helsinki-NLP, CTranslate2, and michaelfeil!\nWhat is CTranslate2?\nCTranslate2 Benchmarks\nCPU Benchmarks for Generic Opus-MT Models\nGPU Benchmarks for Generic Opus-MT Models\nInternal Benchmarks\nct2-transformers-converter Command Used:\nCTranslate2 Installation\nct2-transformers-converter Command Used:\nCTranslate2 Converted Checkpoint Information:\nSample Code - ctranslate2\nSample Code - hf-hub-ctranslate2\nLicense and other remarks:\nRepository General Information\nInspired by and derived from the work of Helsinki-NLP, CTranslate2, and michaelfeil!\nLink to Original Model (Helsinki-NLP): Model Link\nThis respository was based on the work of CTranslate2.\nThis repository was based on the work of michaelfeil.\nWhat is CTranslate2?\nCTranslate2 is a C++ and Python library for efficient inference with Transformer models.\nCTranslate2 implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to accelerate and reduce the memory usage of Transformer models on CPU and GPU.\nCTranslate2 is one of the most performant ways of hosting translation models at scale. Current supported models include:\nEncoder-decoder models: Transformer base/big, M2M-100, NLLB, BART, mBART, Pegasus, T5, Whisper\nDecoder-only models: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, Mistral, Gemma, CodeGen, GPTBigCode, Falcon\nEncoder-only models: BERT, DistilBERT, XLM-RoBERTa\nThe project is production-oriented and comes with backward compatibility guarantees, but it also includes experimental features related to model compression and inference acceleration.\nCTranslate2 Benchmarks\nPlease note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings. Tested against newstest2014 (En -> De) dataset.\nThe benchmark reports the number of target tokens generated per second (higher is better). The results are aggregated over multiple runs. See the benchmark scripts for more details and reproduce these numbers.\nPlease note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings.\nCPU Benchmarks for Generic Opus-MT Models\nLibrary\nTokens per Second\nMax Memory Usage\nBLEU\nTransformers 4.26.1 (with PyTorch 1.13.1)\n147.3\n2332MB\n27.90\nMarian 1.11.0 (int16)\n330.2\n5901MB\n27.65\nMarian 1.11.0 (int8)\n355.8\n4763MB\n27.27\nCTranslate2 3.6.0 (int16)\n596.1\n660MB\n27.53\nCTranslate2 3.6.0 (int8)\n696.1\n516MB\n27.65\nGPU Benchmarks for Generic Opus-MT Models\nLibrary\nTokens per Second\nMax GPU Memory Usage\nMax Memory Usage\nBLEU\nTransformers 4.26.1 (with PyTorch 1.13.1)\n1022.9\n4097MB\n2109MB\n27.90\nMarian 1.11.0 (float16)\n3962.4\n3239MB\n1976MB\n27.94\nCTranslate2 3.6.0 (float16)\n9296.7\n909MB\n814MB\n27.9\nCTranslate2 3.6.0 (int8 + float16)\n8362.7\n813MB\n766MB\n27.9\nExecuted with 4 threads on a c5.2xlarge Amazon EC2 instance equipped with an Intel(R) Xeon(R) Platinum 8275CL CPU.\nSource to benchmark information can be found here.\nOriginal model BLEU scores can be found here.\nInternal Benchmarks\nInternal testing on our end showed inference times reduced by 6x-10x on average compared the vanilla checkpoints using the transformers library. A slight reduction on BLEU scores (~5%) was also identified in comparison to the vanilla checkpoints with a few exceptions. This is likely due to several factors, one being the quantization applied. Further testing is needed from our end to better assess the reduction in translation quality. The command used to compile the vanilla checkpoint into a CTranslate2 model can be found below. Modifying this command can yield differing balances between inferencing performance and translation quality.\nCTranslate2 Installation\npip install hf-hub-ctranslate2>=1.0.0 ctranslate2>=3.13.0\nct2-transformers-converter Command Used:\nct2-transformers-converter --model Helsinki-NLP/opus-mt-en-ber --output_dir ./ctranslate2/opus-mt-en-ber-ctranslate2 --force --copy_files README.md generation_config.json tokenizer_config.json vocab.json source.spm .gitattributes target.spm --quantization float16\nCTranslate2 Converted Checkpoint Information:\nCompatible With:\nctranslate2\nhf-hub-ctranslate2\nCompute Type:\ncompute_type=int8_float16 for device=\"cuda\"\ncompute_type=int8  for device=\"cpu\"\nSample Code - ctranslate2\nClone the repository to the working directory or wherever you wish to store the model artifacts.\ngit clone https://huggingface.co/gaudi/opus-mt-en-ber-ctranslate2\nTake the python code below and update the 'model_dir' variable to the location of the cloned repository.\nfrom ctranslate2 import Translator\nimport transformers\nmodel_dir = \"./opus-mt-en-ber-ctranslate2\" # Path to model directory.\ntranslator = Translator(\nmodel_path=model_dir,\ndevice=\"cuda\", # cpu, cuda, or auto.\ninter_threads=1, # Maximum number of parallel translations.\nintra_threads=4, # Number of OpenMP threads per translator.\ncompute_type=\"int8_float16\", # int8 for cpu or int8_float16 for cuda.\n)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_dir)\nsource = tokenizer.convert_ids_to_tokens(tokenizer.encode(\"XXXXXX, XXX XX XXXXXX.\"))\nresults = translator.translate_batch([source])\ntarget = results[0].hypotheses[0]\nprint(tokenizer.decode(tokenizer.convert_tokens_to_ids(target)))\nSample Code - hf-hub-ctranslate2\nDerived From michaelfeil:\nfrom hf_hub_ctranslate2 import TranslatorCT2fromHfHub, GeneratorCT2fromHfHub\nfrom transformers import AutoTokenizer\nmodel_name = \"gaudi/opus-mt-en-ber-ctranslate2\"\nmodel = TranslatorCT2fromHfHub(\nmodel_name_or_path=model_name,\ndevice=\"cuda\",\ncompute_type=\"int8_float16\",\ntokenizer=AutoTokenizer.from_pretrained(model_name)\n)\noutputs = model.generate(\ntext=[\"XXX XX XXX XXXXXXX XXXX?\", \"XX XX XXXX XX XXX!\"],\n)\nprint(outputs)\nLicense and other remarks:\nLicense conditions are intended to be idential to original huggingface repository by Helsinki-NLP.",
    "unsloth/Mistral-Nemo-Instruct-2407": "Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nFinetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Mistral Nemo 12b here: https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nGemma 7b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nLlama-2 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n43% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nCodeLlama 34b A100\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n27% less\nMistral 7b 1xT4\n‚ñ∂Ô∏è Start on Kaggle\n5x faster*\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "Na0s/Medical-Whisper-Large-v3": "med-whisper-large-final\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nPerformance Overview:\nFramework versions\nmed-whisper-large-final\nThis model is a fine-tuned version of openai/whisper-large-v3 on the primock_data dataset.\nModel description\nFine tuned version of whisper-large-v3 through transfer learning on Doctor/Patient consultations\nIntended uses & limitations\nMedical transcription\nTraining and evaluation data\nNa0s/Medical_Augmented_data\nTraining procedure\nExhaustive transfer learning\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 6\neval_batch_size: 6\nseed: 42\ngradient_accumulation_steps: 4\ntotal_train_batch_size: 24\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: constant_with_warmup\nlr_scheduler_warmup_steps: 50\ntraining_steps: 500\nmixed_precision_training: Native AMP\nPerformance Overview:\n| Model Name\nWER\nCER\nNumber of Parameters\nWhisper Tiny\n0.46\n0.27\n39M\nWhisper Base\n0.42\n0.26\n74M\nWhisper Small\n0.39\n0.26\n244M\nWhisper Medium\n0.37\n0.23\n769M\nWhisper Large v3\n0.33\n0.18\n1.55B\nWhisper Medical\n0.19\n0.10\n1.55B\nPerformance of foundation Whispers vs Medical Whisper on the Validation set.\nModel Name\nWER\nCER\nNumber of Parameters\nWhisper Medical\n0.24\n0.13\n1.55B\nTable: Performance of Whisper Medical on the Test set.\nFramework versions\nTransformers 4.42.4\nPytorch 2.3.1+cu121\nDatasets 2.20.0\nTokenizers 0.19.1",
    "stanfordmimi/Merlin": "Merlin: Vision Language Foundation Model for 3D Computed Tomography\n‚ö°Ô∏è Installation\nüìÅ Project Structure:\nüìé Citation\nMerlin: Vision Language Foundation Model for 3D Computed Tomography\nMerlin is a 3D VLM for computed tomography that leverages both structured electronic health records (EHR) and unstructured radiology reports for pretraining. The huggingface repository here provides the model weights and an example image file.\n[üíª Github] [üìÑ Paper]\n‚ö°Ô∏è Installation\nTo install Merlin, you can simply run:\npip install merlin-vlm\nFor an editable installation, use the following commands to clone and install this repository.\ngit clone https://github.com/StanfordMIMI/Merlin.git\ncd Merlin\npip install -e .\nFor usage instructions, please visit the github repository.\nüìÅ Project Structure:\n.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ i3_resnet_clinical_longformer_best_clip_04-02-2024_23-21-36_epoch_99.pt <Merlin weights>\n‚îú‚îÄ‚îÄ image1.nii.gz <Sample Image>\n‚îú‚îÄ‚îÄ resnet_gpt2_best_stanford_report_generation_average.pt <Merlin Radiology Report Generation Weights>\n‚îú‚îÄ‚îÄ resnet_clinical_longformer_five_year_disease_prediction <Five Year Disease Prediction Weights>\n‚îú‚îÄ‚îÄ nnUNetTrainerMerlin__nnUNetPlans__3d_fullres <nnUNet File>\nüìé Citation\nIf you find this repository useful for your work, please cite the cite the original paper:\n@article{blankemeier2024merlin,\ntitle={Merlin: A vision language foundation model for 3d computed tomography},\nauthor={Blankemeier, Louis and Cohen, Joseph Paul and Kumar, Ashwin and Van Veen, Dave and Gardezi, Syed Jamal Safdar and Paschali, Magdalini and Chen, Zhihong and Delbrouck, Jean-Benoit and Reis, Eduardo and Truyts, Cesar and others},\njournal={Research Square},\npages={rs--3},\nyear={2024}\n}"
}