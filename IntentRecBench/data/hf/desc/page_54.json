{
    "hakurei/waifu-diffusion": "waifu-diffusion v1.4 - Diffusion for Weebs\nGradio & Colab\nModel Description\nLicense\nDownstream Uses\nExample Code\nTeam Members and Acknowledgements\nwaifu-diffusion v1.4 - Diffusion for Weebs\nwaifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\nmasterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck\nOriginal Weights\nGradio & Colab\nWe also support a Gradio Web UI and Colab with Diffusers to run Waifu Diffusion:\nModel Description\nSee here for a full model overview.\nLicense\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies:\nYou can't use the model to deliberately produce nor share illegal or harmful outputs or content\nThe authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\nYou may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license here\nDownstream Uses\nThis model can be used for entertainment purposes and as a generative art assistant.\nExample Code\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\n'hakurei/waifu-diffusion',\ntorch_dtype=torch.float32\n).to('cuda')\nprompt = \"1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"\nwith autocast(\"cuda\"):\nimage = pipe(prompt, guidance_scale=6)[\"sample\"][0]\nimage.save(\"test.png\")\nTeam Members and Acknowledgements\nThis project would not have been possible without the incredible work by Stability AI and Novel AI.\nHaru\nSalt\nSta @ Bit192\nIn order to reach us, you can join our Discord server.",
    "Hoax0930/marian-finetuned-kde4-en-to-ja": "marian-finetuned-kde4-en-to-ja\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nmarian-finetuned-kde4-en-to-ja\nThis model is a fine-tuned version of Helsinki-NLP/opus-tatoeba-en-ja on the kde4 dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.9825\nBleu: 37.1098\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 32\neval_batch_size: 64\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 3\nmixed_precision_training: Native AMP\nTraining results\nFramework versions\nTransformers 4.21.2\nPytorch 1.12.1+cu113\nDatasets 2.4.0\nTokenizers 0.12.1",
    "nateraw/real-esrgan": "README.md exists but content is empty.",
    "IDEA-CCNL/Erlangshen-MegatronBert-3.9B-Chinese": "Erlangshen-MegatronBert-3.9B-Chinese\nç®€ä»‹ Brief Introduction\næ¨¡å‹åˆ†ç±» Model Taxonomy\næ¨¡å‹ä¿¡æ¯ Model Information\næ›´å¤šä¿¡æ¯ More Information\nä¸‹æ¸¸æ•ˆæœ Performance\nä½¿ç”¨ Usage\nå¼•ç”¨ Citation\nErlangshen-MegatronBert-3.9B-Chinese\nMain Page:Fengshenbang\nGithub: Fengshenbang-LM\nç®€ä»‹ Brief Introduction\nå–„äºå¤„ç†NLUä»»åŠ¡ï¼Œç°åœ¨æœ€å¤§çš„ï¼Œæ‹¥æœ‰39äº¿çš„ä¸­æ–‡BERTæ¨¡å‹ã€‚\nGood at solving NLU tasks, the largest Chinese BERT (39B) currently.\næ¨¡å‹åˆ†ç±» Model Taxonomy\néœ€æ±‚ Demand\nä»»åŠ¡ Task\nç³»åˆ— Series\næ¨¡å‹ Model\nå‚æ•° Parameter\né¢å¤– Extra\né€šç”¨ General\nè‡ªç„¶è¯­è¨€ç†è§£ NLU\näºŒéƒç¥ Erlangshen\nMegatronBERT\n3.9B\nä¸­æ–‡ Chinese\næ¨¡å‹ä¿¡æ¯ Model Information\nErlangshen-MegatronBert-3.9B-Chineseæ˜¯ä¸€ä¸ªæ¯”Erlangshen-MegatronBert-1.3Bæ‹¥æœ‰æ›´å¤šå‚æ•°çš„ç‰ˆæœ¬ï¼ˆ39äº¿ï¼‰ã€‚æˆ‘ä»¬éµå¾ªåŸæ¥çš„é¢„è®­ç»ƒæ–¹å¼åœ¨æ‚Ÿé“æ•°æ®é›†ï¼ˆ300Gç‰ˆæœ¬ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒé˜¶æ®µä¸­ä½¿ç”¨äº†å°ç¥æ¡†æ¶å¤§æ¦‚èŠ±è´¹äº†64å¼ A100ï¼ˆ40Gï¼‰çº¦30å¤©ã€‚\nErlangshen-MegatronBert-3.9B-Chinese (3.9B) is a larger version of Erlangshen-MegatronBert-1.3B. By following the original instructions, we apply WuDao Corpora (300 GB version) as the pretraining dataset. Specifically, we use the fengshen framework in the pre-training phase which cost about 30 days with 64 A100 (40G) GPUs.\næ›´å¤šä¿¡æ¯ More Information\nIDEAç ”ç©¶é™¢ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹äºŒéƒç¥ç™»é¡¶FewCLUEæ¦œå•\n2021å¹´11æœˆ10æ—¥ï¼ŒErlangshen-MegatronBERT-1.3Båœ¨FewCLUEä¸Šå–å¾—ç¬¬ä¸€ã€‚å…¶ä¸­ï¼Œå®ƒåœ¨CHIDF(æˆè¯­å¡«ç©º)å’ŒTNEWS(æ–°é—»åˆ†ç±»)å­ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºäººç±»è¡¨ç°ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨CHIDF(æˆè¯­å¡«ç©º), CSLDCP(å­¦ç§‘æ–‡çŒ®åˆ†ç±»), OCNLI(è‡ªç„¶è¯­è¨€æ¨ç†)ä»»åŠ¡ä¸­å‡ååˆ—å‰èŒ…ã€‚\nOn November 10, 2021, Erlangshen-MegatronBert-1.3B topped the FewCLUE benchmark. Among them, our Erlangshen outperformed human performance in CHIDF (idiom fill-in-the-blank) and TNEWS (news classification) subtasks. In addition, our Erlangshen ranked the top in CHIDF (idiom fill-in-the-blank), CSLDCP (subject literature classification), and OCNLI (natural language inference) tasks.\nä¸‹æ¸¸æ•ˆæœ Performance\nä¸‹æ¸¸ä¸­æ–‡ä»»åŠ¡çš„å¾—åˆ†ï¼ˆæ²¡æœ‰åšä»»ä½•æ•°æ®å¢å¼ºï¼‰:\nScores on downstream Chinese tasks (without any data augmentation):\nModel\nafqmc\ntnews\niflytek\nocnli\ncmnli\nwsc\ncsl\nroberta-wwm-ext-large\n0.7514\n0.5872\n0.6152\n0.777\n0.814\n0.8914\n0.86\nErlangshen-MegatronBert-1.3B\n0.7608\n0.5996\n0.6234\n0.7917\n0.81\n0.9243\n0.872\nErlangshen-MegatronBert-3.9B\n0.7561\n0.6048\n0.6204\n0.8278\n0.8517\n-\n-\nä½¿ç”¨ Usage\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, FillMaskPipeline\nimport torch\ntokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-3.9B-Chinese', use_fast=False)\nmodel=AutoModelForMaskedLM.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-3.9B-Chinese')\ntext = 'ç”Ÿæ´»çš„çœŸè°›æ˜¯[MASK]ã€‚'\nfillmask_pipe = FillMaskPipeline(model, tokenizer)\nprint(fillmask_pipe(text, top_k=10))\nå¼•ç”¨ Citation\nå¦‚æœæ‚¨åœ¨æ‚¨çš„å·¥ä½œä¸­ä½¿ç”¨äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¯ä»¥å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š\nIf you are using the resource for your work, please cite the our paper:\n@article{fengshenbang,\nauthor    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\ntitle     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\njournal   = {CoRR},\nvolume    = {abs/2209.02970},\nyear      = {2022}\n}\nä¹Ÿå¯ä»¥å¼•ç”¨æˆ‘ä»¬çš„ç½‘ç«™:\nYou can also cite our website:\n@misc{Fengshenbang-LM,\ntitle={Fengshenbang-LM},\nauthor={IDEA-CCNL},\nyear={2021},\nhowpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}",
    "bigscience/bloomz": "Table of Contents\nModel Summary\nUse\nIntended use\nHow to use\nCPU\nGPU\nGPU in 8bit\nLimitations\nTraining\nModel\nHardware\nSoftware\nEvaluation\nCitation\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nEvaluation\nCitation\nModel Summary\nWe present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.\nRepository: bigscience-workshop/xmtf\nPaper: Crosslingual Generalization through Multitask Finetuning\nPoint of Contact: Niklas Muennighoff\nLanguages: Refer to bloom for pretraining & xP3 for finetuning language proportions. It understands both pretraining & finetuning languages.\nBLOOMZ & mT0 Model Family:\nMultitask finetuned on xP3. Recommended for prompting in English.\nParameters\n300M\n580M\n1.2B\n3.7B\n13B\n560M\n1.1B\n1.7B\n3B\n7.1B\n176B\nFinetuned Model\nmt0-small\nmt0-base\nmt0-large\nmt0-xl\nmt0-xxl\nbloomz-560m\nbloomz-1b1\nbloomz-1b7\nbloomz-3b\nbloomz-7b1\nbloomz\nMultitask finetuned on xP3mt. Recommended for prompting in non-English.\nFinetuned Model\nmt0-xxl-mt\nbloomz-7b1-mt\nbloomz-mt\nMultitask finetuned on P3. Released for research purposes only. Strictly inferior to above models!\nFinetuned Model\nmt0-xxl-p3\nbloomz-7b1-p3\nbloomz-p3\nOriginal pretrained checkpoints. Not recommended.\nPretrained Model\nmt5-small\nmt5-base\nmt5-large\nmt5-xl\nmt5-xxl\nbloom-560m\nbloom-1b1\nbloom-1b7\nbloom-3b\nbloom-7b1\nbloom\nUse\nIntended use\nWe recommend using the model to perform tasks expressed in natural language. For example, given the prompt \"Translate to English: Je tâ€™aime.\", the model will most likely answer \"I love you.\". Some prompt ideas from our paper:\nä¸€ä¸ªä¼ å¥‡çš„å¼€ç«¯ï¼Œä¸€ä¸ªä¸ç­çš„ç¥è¯ï¼Œè¿™ä¸ä»…ä»…æ˜¯ä¸€éƒ¨ç”µå½±ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªèµ°è¿›æ–°æ—¶ä»£çš„æ ‡ç­¾ï¼Œæ°¸è¿œå½ªç‚³å²å†Œã€‚ä½ è®¤ä¸ºè¿™å¥è¯çš„ç«‹åœºæ˜¯èµæ‰¬ã€ä¸­ç«‹è¿˜æ˜¯æ‰¹è¯„?\nSuggest at least five related search terms to \"Máº¡ng neural nhÃ¢n táº¡o\".\nWrite a fairy tale about a troll saving a princess from a dangerous dragon. The fairy tale is a masterpiece that has achieved praise worldwide and its moral is \"Heroes Come in All Shapes and Sizes\". Story (in Spanish):\nExplain in a sentence in Telugu what is backpropagation in neural networks.\nFeel free to share your generations in the Community tab!\nHow to use\nCPU\nClick to expand\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigscience/bloomz\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\ninputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nGPU\nClick to expand\n# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigscience/bloomz\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\ninputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nGPU in 8bit\nClick to expand\n# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigscience/bloomz\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\ninputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nLimitations\nPrompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt \"Translate to English: Je t'aime\" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. \"Translate to English: Je t'aime.\", \"Translate to English: Je t'aime. Translation:\" \"What is \"Je t'aime.\" in English?\", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. \"Explain in a sentence in Telugu what is backpropagation in neural networks.\".\nTraining\nModel\nArchitecture: Same as bloom, also refer to the config.json file\nFinetuning steps: 498\nFinetuning tokens: 2.09 billion\nFinetuning layout: 72x pipeline parallel, 1x tensor parallel, 4x data parallel\nPrecision: bfloat16\nHardware\nCPUs: AMD CPUs with 512GB memory per node\nGPUs: 288 A100 80GB GPUs with 8 GPUs per node (36 nodes) using NVLink 4 inter-gpu connects, 4 OmniPath links\nCommunication: NCCL-communications network with a fully dedicated subnet\nSoftware\nOrchestration: Megatron-DeepSpeed\nOptimizer & parallelism: DeepSpeed\nNeural networks: PyTorch (pytorch-1.11 w/ CUDA-11.5)\nFP16 if applicable: apex\nEvaluation\nWe refer to Table 7 from our paper & bigscience/evaluation-results for zero-shot results on unseen tasks. The sidebar reports zero-shot performance of the best prompt per dataset config.\nCitation\n@article{muennighoff2022crosslingual,\ntitle={Crosslingual generalization through multitask finetuning},\nauthor={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},\njournal={arXiv preprint arXiv:2211.01786},\nyear={2022}\n}",
    "bigscience/bloomz-p3": "Table of Contents\nModel Summary\nUse\nIntended use\nHow to use\nCPU\nGPU\nGPU in 8bit\nLimitations\nTraining\nModel\nHardware\nSoftware\nEvaluation\nCitation\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nEvaluation\nCitation\nModel Summary\nWe present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.\nRepository: bigscience-workshop/xmtf\nPaper: Crosslingual Generalization through Multitask Finetuning\nPoint of Contact: Niklas Muennighoff\nLanguages: Refer to bloom for pretraining & xP3 for finetuning language proportions. It understands both pretraining & finetuning languages.\nBLOOMZ & mT0 Model Family:\nMultitask finetuned on xP3. Recommended for prompting in English.\nParameters\n300M\n580M\n1.2B\n3.7B\n13B\n560M\n1.1B\n1.7B\n3B\n7.1B\n176B\nFinetuned Model\nmt0-small\nmt0-base\nmt0-large\nmt0-xl\nmt0-xxl\nbloomz-560m\nbloomz-1b1\nbloomz-1b7\nbloomz-3b\nbloomz-7b1\nbloomz\nMultitask finetuned on xP3mt. Recommended for prompting in non-English.\nFinetuned Model\nmt0-xxl-mt\nbloomz-7b1-mt\nbloomz-mt\nMultitask finetuned on P3. Released for research purposes only. Strictly inferior to above models!\nFinetuned Model\nmt0-xxl-p3\nbloomz-7b1-p3\nbloomz-p3\nOriginal pretrained checkpoints. Not recommended.\nPretrained Model\nmt5-small\nmt5-base\nmt5-large\nmt5-xl\nmt5-xxl\nbloom-560m\nbloom-1b1\nbloom-1b7\nbloom-3b\nbloom-7b1\nbloom\nUse\nIntended use\nWe recommend using the model to perform tasks expressed in natural language. For example, given the prompt \"Translate to English: Je tâ€™aime.\", the model will most likely answer \"I love you.\". Some prompt ideas from our paper:\nä¸€ä¸ªä¼ å¥‡çš„å¼€ç«¯ï¼Œä¸€ä¸ªä¸ç­çš„ç¥è¯ï¼Œè¿™ä¸ä»…ä»…æ˜¯ä¸€éƒ¨ç”µå½±ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªèµ°è¿›æ–°æ—¶ä»£çš„æ ‡ç­¾ï¼Œæ°¸è¿œå½ªç‚³å²å†Œã€‚ä½ è®¤ä¸ºè¿™å¥è¯çš„ç«‹åœºæ˜¯èµæ‰¬ã€ä¸­ç«‹è¿˜æ˜¯æ‰¹è¯„?\nSuggest at least five related search terms to \"Máº¡ng neural nhÃ¢n táº¡o\".\nWrite a fairy tale about a troll saving a princess from a dangerous dragon. The fairy tale is a masterpiece that has achieved praise worldwide and its moral is \"Heroes Come in All Shapes and Sizes\". Story (in Spanish):\nExplain in a sentence in Telugu what is backpropagation in neural networks.\nFeel free to share your generations in the Community tab!\nHow to use\nCPU\nClick to expand\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigscience/bloomz-p3\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\ninputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nGPU\nClick to expand\n# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigscience/bloomz-p3\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\ninputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nGPU in 8bit\nClick to expand\n# pip install -q transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigscience/bloomz-p3\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_8bit=True)\ninputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nLimitations\nPrompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt \"Translate to English: Je t'aime\" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. \"Translate to English: Je t'aime.\", \"Translate to English: Je t'aime. Translation:\" \"What is \"Je t'aime.\" in English?\", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. \"Explain in a sentence in Telugu what is backpropagation in neural networks.\".\nTraining\nModel\nArchitecture: Same as bloom, also refer to the config.json file\nFinetuning steps: 498\nFinetuning tokens: 2.09 billion\nFinetuning layout: 72x pipeline parallel, 1x tensor parallel, 4x data parallel\nPrecision: bfloat16\nHardware\nCPUs: AMD CPUs with 512GB memory per node\nGPUs: 288 A100 80GB GPUs with 8 GPUs per node (36 nodes) using NVLink 4 inter-gpu connects, 4 OmniPath links\nCommunication: NCCL-communications network with a fully dedicated subnet\nSoftware\nOrchestration: Megatron-DeepSpeed\nOptimizer & parallelism: DeepSpeed\nNeural networks: PyTorch (pytorch-1.11 w/ CUDA-11.5)\nFP16 if applicable: apex\nEvaluation\nWe refer to Table 7 from our paper & bigscience/evaluation-results for zero-shot results on unseen tasks. The sidebar reports zero-shot performance of the best prompt per dataset config.\nCitation\n@article{muennighoff2022crosslingual,\ntitle={Crosslingual generalization through multitask finetuning},\nauthor={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},\njournal={arXiv preprint arXiv:2211.01786},\nyear={2022}\n}",
    "KoboldAI/OPT-2.7B-Erebus": "OPT 2.7B - Erebus\nModel description\nTraining data\nHow to use\nLimitations and biases\nLicense\nBibTeX entry and citation info\nOPT 2.7B - Erebus\nModel description\nThis is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the \"Adult\" theme. The name \"Erebus\" comes from the greek mythology, also named \"darkness\". This is in line with Shin'en, or \"deep abyss\". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.\nTraining data\nThe data can be divided in 6 different datasets:\nLiterotica (everything with 4.5/5 or higher)\nSexstories (everything with 90 or higher)\nDataset-G (private dataset of X-rated stories)\nDoc's Lab (all stories)\nPike Dataset (novels with \"adult\" rating)\nSoFurry (collection of various animals)\nThe dataset uses [Genre: <comma-separated list of genres>] for tagging.\nHow to use\nYou can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:\n>>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='KoboldAI/OPT-2.7B-Erebus')\n>>> generator(\"Welcome Captain Janeway, I apologize for the delay.\", do_sample=True, min_length=50)\n[{'generated_text': 'Welcome Captain Janeway, I apologize for the delay.\"\\nIt's all right,\" Janeway said. \"I'm certain that you're doing your best to keep me informed of what\\'s going on.\"'}]\nLimitations and biases\nBased on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!\nLicense\nOPT-6.7B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.\nBibTeX entry and citation info\n@misc{zhang2022opt,\ntitle={OPT: Open Pre-trained Transformer Language Models},\nauthor={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\nyear={2022},\neprint={2205.01068},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "neulab/codebert-python": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nCitation\nThis is a microsoft/codebert-base-mlm model, trained for 1,000,000 steps (with batch_size=32)  on Python code from the codeparrot/github-code-clean dataset, on the masked-language-modeling task.\nIt is intended to be used in CodeBERTScore: https://github.com/neulab/code-bert-score, but can be used for any other model or task.\nFor more information, see: https://github.com/neulab/code-bert-score\nCitation\nIf you use this model for research, please cite:\n@article{zhou2023codebertscore,\nurl = {https://arxiv.org/abs/2302.05527},\nauthor = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},\ntitle = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},\npublisher = {arXiv},\nyear = {2023},\n}",
    "openai/whisper-tiny": "Whisper\nModel details\nUsage\nTranscription\nEnglish to English\nFrench to French\nTranslation\nFrench to English\nEvaluation\nLong-Form Transcription\nFine-Tuning\nEvaluated Use\nTraining Data\nPerformance and Limitations\nBroader Implications\nBibTeX entry and citation info\nWhisper\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours\nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need\nfor fine-tuning.\nWhisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision\nby Alec Radford et al from OpenAI. The original code repository can be found here.\nDisclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were\ncopied and pasted from the original model card.\nModel details\nWhisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.\nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.\nThe models were trained on either English-only data or multilingual data. The English-only models were trained\non the task of speech recognition. The multilingual models were trained on both speech recognition and speech\ntranslation. For speech recognition, the model predicts transcriptions in the same language as the audio.\nFor speech translation, the model predicts transcriptions to a different language to the audio.\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints\nare available on the Hugging Face Hub. The\ncheckpoints are summarised in the following table with links to the models on the Hub:\nSize\nParameters\nEnglish-only\nMultilingual\ntiny\n39 M\nâœ“\nâœ“\nbase\n74 M\nâœ“\nâœ“\nsmall\n244 M\nâœ“\nâœ“\nmedium\n769 M\nâœ“\nâœ“\nlarge\n1550 M\nx\nâœ“\nlarge-v2\n1550 M\nx\nâœ“\nUsage\nTo transcribe audio samples, the model has to be used alongside a WhisperProcessor.\nThe WhisperProcessor is used to:\nPre-process the audio inputs (converting them to log-Mel spectrograms for the model)\nPost-process the model outputs (converting them from tokens to text)\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens\nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\nThe transcription always starts with the <|startoftranscript|> token\nThe second token is the language token (e.g. <|en|> for English)\nThe third token is the \"task token\". It can take one of two values: <|transcribe|> for speech recognition or <|translate|> for speech translation\nIn addition, a <|notimestamps|> token is added if the model should not include timestamp prediction\nThus, a typical sequence of context tokens might look as follows:\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at\neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced,\nthe Whisper model will automatically predict the output langauge and task itself.\nThe context tokens can be set accordingly:\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\nWhich forces the model to predict in English under the task of speech recognition.\nTranscription\nEnglish to English\nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n>>> model.config.forced_decoder_ids = None\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\nThe context tokens can be removed from the start of the transcription by setting skip_special_tokens=True.\nFrench to French\nThe following example demonstrates French to French transcription by setting the decoder ids appropriately.\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.']\nTranslation\nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\nFrench to English\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\nEvaluation\nThis code snippet shows how to evaluate Whisper Tiny on LibriSpeech test-clean:\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\").to(\"cuda\")\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>>\n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n>>> result = librispeech_test_clean.map(map_to_pred)\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n7.547098647858638\nLong-Form Transcription\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking\nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers\npipeline\nmethod. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline\ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing return_timestamps=True:\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-tiny\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n'timestamp': (0.0, 5.44)}]\nRefer to the blog post ASR Chunking for more details on the chunking algorithm.\nFine-Tuning\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However,\nits predictive capabilities can be improved further for certain languages and tasks through fine-tuning. The blog\npost Fine-Tune Whisper with ğŸ¤— Transformers provides a step-by-step\nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\nEvaluated Use\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\nTraining Data\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.\nAs discussed in the accompanying paper, we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\nPerformance and Limitations\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in the paper accompanying this release.\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in the paper. It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\nBroader Implications\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\nBibTeX entry and citation info\n@misc{radford2022whisper,\ndoi = {10.48550/ARXIV.2212.04356},\nurl = {https://arxiv.org/abs/2212.04356},\nauthor = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\ntitle = {Robust Speech Recognition via Large-Scale Weak Supervision},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "openai/whisper-base": "Whisper\nModel details\nUsage\nTranscription\nEnglish to English\nFrench to French\nTranslation\nFrench to English\nEvaluation\nLong-Form Transcription\nFine-Tuning\nEvaluated Use\nTraining Data\nPerformance and Limitations\nBroader Implications\nBibTeX entry and citation info\nWhisper\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours\nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need\nfor fine-tuning.\nWhisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision\nby Alec Radford et al from OpenAI. The original code repository can be found here.\nDisclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were\ncopied and pasted from the original model card.\nModel details\nWhisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.\nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.\nThe models were trained on either English-only data or multilingual data. The English-only models were trained\non the task of speech recognition. The multilingual models were trained on both speech recognition and speech\ntranslation. For speech recognition, the model predicts transcriptions in the same language as the audio.\nFor speech translation, the model predicts transcriptions to a different language to the audio.\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints\nare available on the Hugging Face Hub. The\ncheckpoints are summarised in the following table with links to the models on the Hub:\nSize\nParameters\nEnglish-only\nMultilingual\ntiny\n39 M\nâœ“\nâœ“\nbase\n74 M\nâœ“\nâœ“\nsmall\n244 M\nâœ“\nâœ“\nmedium\n769 M\nâœ“\nâœ“\nlarge\n1550 M\nx\nâœ“\nlarge-v2\n1550 M\nx\nâœ“\nUsage\nTo transcribe audio samples, the model has to be used alongside a WhisperProcessor.\nThe WhisperProcessor is used to:\nPre-process the audio inputs (converting them to log-Mel spectrograms for the model)\nPost-process the model outputs (converting them from tokens to text)\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens\nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\nThe transcription always starts with the <|startoftranscript|> token\nThe second token is the language token (e.g. <|en|> for English)\nThe third token is the \"task token\". It can take one of two values: <|transcribe|> for speech recognition or <|translate|> for speech translation\nIn addition, a <|notimestamps|> token is added if the model should not include timestamp prediction\nThus, a typical sequence of context tokens might look as follows:\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at\neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced,\nthe Whisper model will automatically predict the output langauge and task itself.\nThe context tokens can be set accordingly:\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\nWhich forces the model to predict in English under the task of speech recognition.\nTranscription\nEnglish to English\nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n>>> model.config.forced_decoder_ids = None\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\nThe context tokens can be removed from the start of the transcription by setting skip_special_tokens=True.\nFrench to French\nThe following example demonstrates French to French transcription by setting the decoder ids appropriately.\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.']\nTranslation\nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\nFrench to English\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\nEvaluation\nThis code snippet shows how to evaluate Whisper Base on LibriSpeech test-clean:\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>>\n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n>>> result = librispeech_test_clean.map(map_to_pred)\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n5.082316555716899\nLong-Form Transcription\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking\nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers\npipeline\nmethod. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline\ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing return_timestamps=True:\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-base\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n'timestamp': (0.0, 5.44)}]\nRefer to the blog post ASR Chunking for more details on the chunking algorithm.\nFine-Tuning\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However,\nits predictive capabilities can be improved further for certain languages and tasks through fine-tuning. The blog\npost Fine-Tune Whisper with ğŸ¤— Transformers provides a step-by-step\nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\nEvaluated Use\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\nTraining Data\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.\nAs discussed in the accompanying paper, we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\nPerformance and Limitations\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in the paper accompanying this release.\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in the paper. It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\nBroader Implications\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\nBibTeX entry and citation info\n@misc{radford2022whisper,\ndoi = {10.48550/ARXIV.2212.04356},\nurl = {https://arxiv.org/abs/2212.04356},\nauthor = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\ntitle = {Robust Speech Recognition via Large-Scale Weak Supervision},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "openai/whisper-medium": "Whisper\nModel details\nUsage\nTranscription\nEnglish to English\nFrench to French\nTranslation\nFrench to English\nEvaluation\nLong-Form Transcription\nFine-Tuning\nEvaluated Use\nTraining Data\nPerformance and Limitations\nBroader Implications\nBibTeX entry and citation info\nWhisper\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours\nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need\nfor fine-tuning.\nWhisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision\nby Alec Radford et al from OpenAI. The original code repository can be found here.\nDisclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were\ncopied and pasted from the original model card.\nModel details\nWhisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.\nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.\nThe models were trained on either English-only data or multilingual data. The English-only models were trained\non the task of speech recognition. The multilingual models were trained on both speech recognition and speech\ntranslation. For speech recognition, the model predicts transcriptions in the same language as the audio.\nFor speech translation, the model predicts transcriptions to a different language to the audio.\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints\nare available on the Hugging Face Hub. The\ncheckpoints are summarised in the following table with links to the models on the Hub:\nSize\nParameters\nEnglish-only\nMultilingual\ntiny\n39 M\nâœ“\nâœ“\nbase\n74 M\nâœ“\nâœ“\nsmall\n244 M\nâœ“\nâœ“\nmedium\n769 M\nâœ“\nâœ“\nlarge\n1550 M\nx\nâœ“\nlarge-v2\n1550 M\nx\nâœ“\nUsage\nTo transcribe audio samples, the model has to be used alongside a WhisperProcessor.\nThe WhisperProcessor is used to:\nPre-process the audio inputs (converting them to log-Mel spectrograms for the model)\nPost-process the model outputs (converting them from tokens to text)\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens\nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\nThe transcription always starts with the <|startoftranscript|> token\nThe second token is the language token (e.g. <|en|> for English)\nThe third token is the \"task token\". It can take one of two values: <|transcribe|> for speech recognition or <|translate|> for speech translation\nIn addition, a <|notimestamps|> token is added if the model should not include timestamp prediction\nThus, a typical sequence of context tokens might look as follows:\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at\neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced,\nthe Whisper model will automatically predict the output langauge and task itself.\nThe context tokens can be set accordingly:\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\nWhich forces the model to predict in English under the task of speech recognition.\nTranscription\nEnglish to English\nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n>>> model.config.forced_decoder_ids = None\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\nThe context tokens can be removed from the start of the transcription by setting skip_special_tokens=True.\nFrench to French\nThe following example demonstrates French to French transcription by setting the decoder ids appropriately.\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.']\nTranslation\nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\nFrench to English\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\nEvaluation\nThis code snippet shows how to evaluate Whisper Medium on LibriSpeech test-clean:\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\").to(\"cuda\")\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>>\n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n>>> result = librispeech_test_clean.map(map_to_pred)\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n2.900409225488902\nLong-Form Transcription\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking\nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers\npipeline\nmethod. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline\ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing return_timestamps=True:\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-medium\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n'timestamp': (0.0, 5.44)}]\nRefer to the blog post ASR Chunking for more details on the chunking algorithm.\nFine-Tuning\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However,\nits predictive capabilities can be improved further for certain languages and tasks through fine-tuning. The blog\npost Fine-Tune Whisper with ğŸ¤— Transformers provides a step-by-step\nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\nEvaluated Use\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\nTraining Data\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.\nAs discussed in the accompanying paper, we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\nPerformance and Limitations\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in the paper accompanying this release.\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in the paper. It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\nBroader Implications\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\nBibTeX entry and citation info\n@misc{radford2022whisper,\ndoi = {10.48550/ARXIV.2212.04356},\nurl = {https://arxiv.org/abs/2212.04356},\nauthor = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\ntitle = {Robust Speech Recognition via Large-Scale Weak Supervision},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "Sunbird/tts-tacotron2-lug": "Sunbird AI Text-to-Speech (TTS) model trained on Luganda text\nText-to-Speech (TTS) with Tacotron2 trained on Professional Studio Recordings\nInstall SpeechBrain\nPerform Text-to-Speech (TTS)\nInference on GPU\nSunbird AI Text-to-Speech (TTS) model trained on Luganda text\nText-to-Speech (TTS) with Tacotron2 trained on Professional Studio Recordings\nThis repository provides all the necessary tools for Text-to-Speech (TTS)  with SpeechBrain.\nThe pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.\nInstall SpeechBrain\npip install speechbrain\nPerform Text-to-Speech (TTS)\nimport torchaudio\nfrom speechbrain.pretrained import Tacotron2\nfrom speechbrain.pretrained import HIFIGAN\n# Intialize TTS (tacotron2) and Vocoder (HiFIGAN)\ntacotron2 = Tacotron2.from_hparams(source=\"/Sunbird/sunbird-lug-tts\", savedir=\"tmpdir_tts\")\nhifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"tmpdir_vocoder\")\n# Running the TTS\nmel_output, mel_length, alignment = tacotron2.encode_text(\"Mbagaliza Christmass Enungi Nomwaka Omugya Gubaberere Gwamirembe\")\n# Running Vocoder (spectrogram-to-waveform)\nwaveforms = hifi_gan.decode_batch(mel_output)\n# Save the waverform\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\nIf you want to generate multiple sentences in one-shot, you can do in this way:\nfrom speechbrain.pretrained import Tacotron2\ntacotron2 = Tacotron2.from_hparams(source=\"speechbrain/TTS_Tacotron2\", savedir=\"tmpdir\")\nitems = [\n\"Nsanyuse okukulaba\",\n\"Erinnya lyo ggwe ani?\",\n\"Mbagaliza Christmass Enungi Nomwaka Omugya Gubaberere Gwamirembe\"\n]\nmel_outputs, mel_lengths, alignments = tacotron2.encode_batch(items)\nInference on GPU\nTo perform inference on the GPU, add  run_opts={\"device\":\"cuda\"}  when calling the from_hparams method.",
    "huggingface/time-series-transformer-tourism-monthly": "Time Series Transformer (trained on monash_tsf/tourism-monthly)\nModel description\nUsage\nTime Series Transformer (trained on monash_tsf/tourism-monthly)\nTime Series Transformer model trained on the tourism-monthly dataset for 30 epochs.\nModel description\nThe Time Series Transformer is a vanilla encoder-decoder Transformer for time-series forecasting. The model is trained in the same way as one trains a Transformer for machine translation. At inference time, the model autoregressively generates samples, one time step at a time.\nUsage\nWe refer to the documentation regarding usage.",
    "facebook/esm2_t30_150M_UR50D": "ESM-2\nESM-2\nESM-2 is a state-of-the-art protein model trained on a masked language modelling objective. It is suitable for fine-tuning on a wide range of tasks that take protein sequences as input. For detailed information on the model architecture and training data, please refer to the accompanying paper. You may also be interested in some demo notebooks (PyTorch, TensorFlow) which demonstrate how to fine-tune ESM-2 models on your tasks of interest.\nSeveral ESM-2 checkpoints are available in the Hub with varying sizes. Larger sizes generally have somewhat better accuracy, but require much more memory and time to train:\nCheckpoint name\nNum layers\nNum parameters\nesm2_t48_15B_UR50D\n48\n15B\nesm2_t36_3B_UR50D\n36\n3B\nesm2_t33_650M_UR50D\n33\n650M\nesm2_t30_150M_UR50D\n30\n150M\nesm2_t12_35M_UR50D\n12\n35M\nesm2_t6_8M_UR50D\n6\n8M",
    "facebook/esm2_t33_650M_UR50D": "ESM-2\nESM-2\nESM-2 is a state-of-the-art protein model trained on a masked language modelling objective. It is suitable for fine-tuning on a wide range of tasks that take protein sequences as input. For detailed information on the model architecture and training data, please refer to the accompanying paper. You may also be interested in some demo notebooks (PyTorch, TensorFlow) which demonstrate how to fine-tune ESM-2 models on your tasks of interest.\nSeveral ESM-2 checkpoints are available in the Hub with varying sizes. Larger sizes generally have somewhat better accuracy, but require much more memory and time to train:\nCheckpoint name\nNum layers\nNum parameters\nesm2_t48_15B_UR50D\n48\n15B\nesm2_t36_3B_UR50D\n36\n3B\nesm2_t33_650M_UR50D\n33\n650M\nesm2_t30_150M_UR50D\n30\n150M\nesm2_t12_35M_UR50D\n12\n35M\nesm2_t6_8M_UR50D\n6\n8M",
    "sd-concepts-library/alisa": "alisa on Stable Diffusion\nalisa on Stable Diffusion\nThis is the <alisa-selezneva> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.\nHere is the new concept you will be able to use as an object:",
    "BlackKakapo/stsb-xlm-r-multilingual-ro": "stsb-xlm-r-multilingual-ro\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nTraining\nFull Model Architecture\nCiting & Authors\nBlackKakapo\nstsb-xlm-r-multilingual-ro\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. Fine Tune of stsb-xlm-r-multilingual for romanian language\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('BlackKakapo/stsb-xlm-r-multilingual-ro')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BlackKakapo/stsb-xlm-r-multilingual-ro')\nmodel = AutoModel.from_pretrained('BlackKakapo/stsb-xlm-r-multilingual-ro')\nTraining\nDataSet:\nSTS-ro\nThe text dataset is in Romanian (ro). Score is from 0 to 5, that why I divide score by 5, becouse the score for  EmbeddingSimilarityEvaluator (evaluator for finetune) need  to be from 0 to 1.\nDataset Structure:\n{\n'score': 1.5,\n'sentence1': 'Un bÄƒrbat cÃ¢ntÄƒ la harpÄƒ.',\n'sentence2': 'Un bÄƒrbat cÃ¢ntÄƒ la claviaturÄƒ.',\n}\nDataLoader:\ntorch.utils.data.dataloader.DataLoader of length 223 with parameters:\n{'batch_size': 32, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\nLoss:\nsentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss\nParameters of the fit()-Method:\n{\n\"epochs\": 10,\n\"evaluation_steps\": 0,\n\"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n\"max_grad_norm\": 1,\n\"optimizer_class\": \"<class 'torch.optim.adamw.AdamW'>\",\n\"optimizer_params\": {\n\"lr\": 2e-05\n},\n\"scheduler\": \"WarmupLinear\",\n\"steps_per_epoch\": null,\n\"warmup_steps\": 100,\n\"weight_decay\": 0.01\n}\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nBlackKakapo",
    "google/flan-t5-large": "Model Card for FLAN-T5 large\nTable of Contents\nTL;DR\nModel Details\nModel Description\nUsage\nUsing the Pytorch model\nRunning the model on a CPU\nRunning the model on a GPU\nRunning the model on a GPU using different precisions\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nEthical considerations and risks\nKnown Limitations\nSensitive Use:\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card for FLAN-T5 large\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nTL;DR\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.\nAs mentioned in the first few lines of the abstract :\nFlan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\nDisclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\nLicense: Apache 2.0\nRelated Models: All FLAN-T5 Checkpoints\nOriginal Checkpoints: All Original FLAN-T5 Checkpoints\nResources for more information:\nResearch paper\nGitHub Repo\nHugging Face FLAN-T5 Docs (Similar to T5)\nUsage\nFind below some example scripts on how to use the model in transformers:\nUsing the Pytorch model\nRunning the model on a CPU\nClick to expand\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU\nClick to expand\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nFP16\nClick to expand\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nINT8\nClick to expand\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", load_in_8bit=True)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nUses\nDirect Use and Downstream Use\nThe authors write in the original paper's model card that:\nThe primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\nSee the research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nThe information below in this section are copied from the model's official model card:\nLanguage models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\nEthical considerations and risks\nFlan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\nKnown Limitations\nFlan-T5 has not been tested in real world applications.\nSensitive Use:\nFlan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\nTraining Details\nTraining Data\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\nTraining Procedure\nAccording to the model card from the original paper:\nThese models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\nThe model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax.\nEvaluation\nTesting Data, Factors & Metrics\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\nFor full details, please check the research paper.\nResults\nFor full results for FLAN-T5-Large, see the research paper, Table 3.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips â‰¥ 4.\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\ndoi = {10.48550/ARXIV.2210.11416},\nurl = {https://arxiv.org/abs/2210.11416},\nauthor = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\nkeywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {Scaling Instruction-Finetuned Language Models},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}",
    "nghuyong/ernie-3.0-xbase-zh": "ERNIE-3.0-xbase-zh\nIntroduction\nReleased Model Info\nHow to use\nCitation\nERNIE-3.0-xbase-zh\nIntroduction\nERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\nMore detail: https://arxiv.org/abs/2107.02137\nReleased Model Info\nThis released pytorch model is converted from the officially released PaddlePaddle ERNIE model and\na series of experiments have been conducted to check the accuracy of the conversion.\nOfficial PaddlePaddle ERNIE repo:https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/ERNIE/contents.html\nPytorch Conversion repo:  https://github.com/nghuyong/ERNIE-Pytorch\nHow to use\nfrom transformers import BertTokenizer, ErnieModel\ntokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-xbase-zh\")\nmodel = ErnieModel.from_pretrained(\"nghuyong/ernie-3.0-xbase-zh\")\nCitation\n@article{sun2021ernie,\ntitle={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},\nauthor={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},\njournal={arXiv preprint arXiv:2107.02137},\nyear={2021}\n}",
    "studio-ousia/luke-japanese-base-lite": "luke-japanese\nExperimental results on JGLUE\nCitation\nluke-japanese\nluke-japanese is the Japanese version of LUKE (Language\nUnderstanding with Knowledge-based Embeddings), a pre-trained\nknowledge-enhanced contextualized representation of words and entities. LUKE\ntreats words and entities in a given text as independent tokens, and outputs\ncontextualized representations of them. Please refer to our\nGitHub repository for more details and\nupdates.\nThis model is a lightweight version which does not contain Wikipedia entity\nembeddings. Please use the\nfull version for\ntasks that use Wikipedia entities as inputs.\nluke-japaneseã¯ã€å˜èªã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®çŸ¥è­˜æ‹¡å¼µå‹è¨“ç·´æ¸ˆã¿ Transformer ãƒ¢ãƒ‡ãƒ«LUKEã®æ—¥æœ¬èªç‰ˆã§ã™ã€‚LUKE ã¯å˜èªã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ç‹¬ç«‹ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã„ã€ã“ã‚Œã‚‰ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¡¨ç¾ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€GitHub ãƒªãƒã‚¸ãƒˆãƒªã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\nã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Wikipedia ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å«ã¾ãªã„è»½é‡ç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Wikipedia ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ã†ã‚¿ã‚¹ã‚¯ã«ã¯ã€full versionã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\nExperimental results on JGLUE\nThe experimental results evaluated on the dev set of\nJGLUE are shown as follows:\nModel\nMARC-ja\nJSTS\nJNLI\nJCommonsenseQA\nacc\nPearson/Spearman\nacc\nacc\nLUKE Japanese base\n0.965\n0.916/0.877\n0.912\n0.842\nBaselines:\nTohoku BERT base\n0.958\n0.909/0.868\n0.899\n0.808\nNICT BERT base\n0.958\n0.910/0.871\n0.902\n0.823\nWaseda RoBERTa base\n0.962\n0.913/0.873\n0.895\n0.840\nXLM RoBERTa base\n0.961\n0.877/0.831\n0.893\n0.687\nThe baseline scores are obtained from\nhere.\nCitation\n@inproceedings{yamada2020luke,\ntitle={LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention},\nauthor={Ikuya Yamada and Akari Asai and Hiroyuki Shindo and Hideaki Takeda and Yuji Matsumoto},\nbooktitle={EMNLP},\nyear={2020}\n}",
    "nlpie/distil-clinicalbert": "Model Description\nDistillation Procedure\nInitialisation\nArchitecture\nCitation\nSupport\nModel Description\nDistilClinicalBERT is a distilled version of the BioClinicalBERT model which is distilled for 3 epochs using a total batch size of 192 on the MIMIC-III notes dataset.\nDistillation Procedure\nThis model uses a simple distillation technique, which tries to align the output distribution of the student model with the output distribution of the teacher based on the MLM objective. In addition, it optionally uses another alignment loss for aligning the last hidden state of the student and teacher.\nInitialisation\nFollowing DistilBERT, we initialise the student model by taking weights from every other layer of the teacher.\nArchitecture\nIn this model, the size of the hidden dimension and the embedding layer are both set to 768. The vocabulary size is 28996. The number of transformer layers is 6 and the expansion rate of the feed-forward layer is 4. Overall this model has around 65 million parameters.\nCitation\nIf you use this model, please consider citing the following paper:\n@article{rohanian2023lightweight,\ntitle={Lightweight transformers for clinical natural language processing},\nauthor={Rohanian, Omid and Nouriborji, Mohammadmahdi and Jauncey, Hannah and Kouchaki, Samaneh and Nooralahzadeh, Farhad and Clifton, Lei and Merson, Laura and Clifton, David A and ISARIC Clinical Characterisation Group and others},\njournal={Natural Language Engineering},\npages={1--28},\nyear={2023},\npublisher={Cambridge University Press}\n}\nSupport\nIf this model helps your work, you can keep the project running with a one-off or monthly contribution:https://github.com/sponsors/nlpie-research",
    "NlpHUST/ner-vietnamese-electra-base": "vietnamese-ner\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nFramework versions\nContact information\nvietnamese-ner\nThis model is a fine-tuned version of NlpHUST/electra-base-vn on an VLSP 2018 dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.0580\nLocation Precision: 0.9353\nLocation Recall: 0.9377\nLocation F1: 0.9365\nLocation Number: 2360\nMiscellaneous Precision: 0.5660\nMiscellaneous Recall: 0.6897\nMiscellaneous F1: 0.6218\nMiscellaneous Number: 174\nOrganization Precision: 0.8610\nOrganization Recall: 0.9068\nOrganization F1: 0.8833\nOrganization Number: 1878\nPerson Precision: 0.9692\nPerson Recall: 0.9637\nPerson F1: 0.9664\nPerson Number: 2121\nOverall Precision: 0.9122\nOverall Recall: 0.9307\nOverall F1: 0.9214\nOverall Accuracy: 0.9907\nModel description\nMore information needed\nHow to use\nYou can use this model with Transformers pipeline for NER.\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/ner-vietnamese-electra-base\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/ner-vietnamese-electra-base\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"LiÃªn quan vá»¥ viá»‡c CSGT bá»‹ tá»‘ Ä‘Ã¡nh dÃ¢n, trÃºng má»™t chÃ¡u nhá» Ä‘ang ngá»§, Ä‘ang lan truyá»n trÃªn máº¡ng xÃ£ há»™i, Äáº¡i tÃ¡ Nguyá»…n VÄƒn Táº£o, PhÃ³ GiÃ¡m Ä‘á»‘c CÃ´ng an tá»‰nh Tiá»n Giang vá»«a cÃ³ cuá»™c há»p cÃ¹ng Chá»‰ huy CÃ´ng an huyá»‡n ChÃ¢u ThÃ nh vÃ  má»™t sá»‘ Ä‘Æ¡n vá»‹ nghiá»‡p vá»¥ cáº¥p tá»‰nh Ä‘á»ƒ chá»‰ Ä‘áº¡o lÃ m rÃµ thÃ´ng tin.\"\nner_results = nlp(example)\nprint(ner_results)\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 16\neval_batch_size: 4\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 10.0\nFramework versions\nTransformers 4.20.1\nPytorch 1.8.0+cu111\nDatasets 2.4.0\nTokenizers 0.12.1\nContact information\nFor personal communication related to this project, please contact Nha Nguyen Van (nha282@gmail.com).",
    "AstraliteHeart/pony-diffusion-v2": "pony-diffusion-v2 - more anthro edition\nModel Description\nLicense\nDownstream Uses\nExample Code\nTeam Members and Acknowledgements\npony-diffusion-v2 - more anthro edition\nPony Diffusion V4 is now live!\npony-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality pony and furry SFW and NSFW images through fine-tuning.\nWARNING: Compared to v1 of this model, v2 is much more capable of producing NSFW content so it's recommended to use 'safe' tag in prompt in combination with negative prompt for image features you may want to suppress. v2 model also has a slight 3d bias so negative prompts like '3d' or 'sfm' should be used to better match v1 outputs.\nWith special thanks to Waifu-Diffusion for providing finetuning expertise and Novel AI for providing necessary compute.\nOriginal PyTorch Model Download Link\nReal-ESRGAN Model finetuned on pony faces\nModel Description\nThe model originally used for fine-tuning is an early finetuned checkpoint of waifu-diffusion on top of Stable Diffusion V1-4, which is a latent image diffusion model trained on LAION2B-en.\nThis particular checkpoint has been fine-tuned with a learning rate of 5.0e-6 for 4 epochs on approximately 450k pony and furry text-image pairs (using tags from derpibooru and e621) which all have score greater than 250.\nLicense\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies:\nYou can't use the model to deliberately produce nor share illegal or harmful outputs or content\nThe authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\nYou may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license here\nDownstream Uses\nThis model can be used for entertainment purposes and as a generative art assistant.\nExample Code\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\nmodel_id = \"AstraliteHeart/pony-diffusion-v2\"\ndevice = \"cuda\"\npipe = StableDiffusionPipeline.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nrevision=\"fp16\",\nscheduler=DDIMScheduler(\nbeta_start=0.00085,\nbeta_end=0.012,\nbeta_schedule=\"scaled_linear\",\nclip_sample=False,\nset_alpha_to_one=False,\n),\n)\npipe = pipe.to(device)\nprompt = \"pinkie pie anthro portrait wedding dress veil intricate highly detailed digital painting artstation concept art smooth sharp focus illustration Unreal Engine 5 8K\"\nwith autocast(\"cuda\"):\nimage = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]\nimage.save(\"cute_poner.png\")\nTeam Members and Acknowledgements\nThis project would not have been possible without the incredible work by the CompVis Researchers.\nWaifu-Diffusion for helping with finetuning and providing starting checkpoint\nNovel AI for providing compute\nIn order to reach us, you can join our Discord server.",
    "CIDAS/clipseg-rd64-refined": "CLIPSeg model\nIntended use cases\nUsage\nCLIPSeg model\nCLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by LÃ¼ddecke et al. and first released in this repository.\nIntended use cases\nThis model is intended for zero-shot and one-shot image segmentation.\nUsage\nRefer to the documentation.",
    "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb": "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nEvaluation Results\nTraining\nFull Model Architecture\nCiting & Authors\npritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. It has been trained over the SNLI, MNLI, SCINLI, SCITAIL, MEDNLI and STSB datasets for providing robust sentence embeddings.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')\nmodel = AutoModel.from_pretrained('pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nEvaluation Results\nFor an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net\nTraining\nThe model was trained with the parameters:\nDataLoader:\ntorch.utils.data.dataloader.DataLoader of length 90 with parameters:\n{'batch_size': 64, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\nLoss:\nsentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss\nParameters of the fit()-Method:\n{\n\"epochs\": 4,\n\"evaluation_steps\": 1000,\n\"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n\"max_grad_norm\": 1,\n\"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n\"optimizer_params\": {\n\"lr\": 2e-05\n},\n\"scheduler\": \"WarmupLinear\",\n\"steps_per_epoch\": null,\n\"warmup_steps\": 36,\n\"weight_decay\": 0.01\n}\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 100, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nIf you use the model kindly cite the following work\n@inproceedings{deka2022evidence,\ntitle={Evidence Extraction to Validate Medical Claims in Fake News Detection},\nauthor={Deka, Pritam and Jurek-Loughrey, Anna and others},\nbooktitle={International Conference on Health Information Science},\npages={3--15},\nyear={2022},\norganization={Springer}\n}",
    "OFA-Sys/chinese-clip-vit-base-patch16": "Chinese-CLIP-ViT-Base-Patch16\nIntroduction\nUse with the official API\nResults\nCitation\nChinese-CLIP-ViT-Base-Patch16\nIntroduction\nThis is the base-version of the Chinese CLIP, with ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. For more details, please refer to our technical report https://arxiv.org/abs/2211.01335 and our official github repo https://github.com/OFA-Sys/Chinese-CLIP (Welcome to star! ğŸ”¥ğŸ”¥)\nUse with the official API\nWe provide a simple code snippet to show how to use the API of Chinese-CLIP to compute the image & text embeddings and similarities.\nfrom PIL import Image\nimport requests\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\nmodel = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\nprocessor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\nurl = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# Squirtle, Bulbasaur, Charmander, Pikachu in English\ntexts = [\"æ°å°¼é¾Ÿ\", \"å¦™è›™ç§å­\", \"å°ç«é¾™\", \"çš®å¡ä¸˜\"]\n# compute image feature\ninputs = processor(images=image, return_tensors=\"pt\")\nimage_features = model.get_image_features(**inputs)\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n# compute text features\ninputs = processor(text=texts, padding=True, return_tensors=\"pt\")\ntext_features = model.get_text_features(**inputs)\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n# compute image-text similarity scores\ninputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1)  # probs: [[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]]\nHowever, if you are not satisfied with only using the API, feel free to check our github repo https://github.com/OFA-Sys/Chinese-CLIP for more details about training and inference.\nResults\nMUGE Text-to-Image Retrieval:\nSetupZero-shotFinetune\nMetricR@1R@5R@10MRR@1R@5R@10MR\nWukong42.769.078.063.252.777.985.672.1\nR2D249.575.783.269.560.182.989.477.5\nCN-CLIP63.084.189.278.868.988.793.183.6\nFlickr30K-CN Retrieval:\nTaskText-to-ImageImage-to-Text\nSetupZero-shotFinetuneZero-shotFinetune\nMetricR@1R@5R@10R@1R@5R@10R@1R@5R@10R@1R@5R@10\nWukong51.778.986.377.494.597.076.194.897.592.799.199.6\nR2D260.986.892.784.496.798.477.696.798.995.699.8100.0\nCN-CLIP71.291.495.583.896.998.681.697.598.895.399.7100.0\nCOCO-CN Retrieval:\nTaskText-to-ImageImage-to-Text\nSetupZero-shotFinetuneZero-shotFinetune\nMetricR@1R@5R@10R@1R@5R@10R@1R@5R@10R@1R@5R@10\nWukong53.480.290.174.094.498.155.281.090.673.394.098.0\nR2D256.485.093.179.196.598.963.389.395.779.397.198.7\nCN-CLIP69.289.996.181.596.999.163.086.692.983.597.399.2\nZero-shot Image Classification:\nTaskCIFAR10CIFAR100DTDEuroSATFERFGVCKITTIMNISTPCVOC\nGIT88.561.142.943.441.46.722.168.950.080.2\nALIGN94.976.866.152.150.825.041.274.055.283.0\nCLIP94.977.056.063.048.333.311.579.062.384.0\nWukong95.477.140.950.3------\nCN-CLIP96.079.751.252.055.126.249.979.463.584.9\nCitation\nIf you find Chinese CLIP helpful, feel free to cite our paper. Thanks for your support!\n@article{chinese-clip,\ntitle={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},\nauthor={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},\njournal={arXiv preprint arXiv:2211.01335},\nyear={2022}\n}",
    "nateraw/videomae-base-finetuned-ucf101-subset": "videomae-base-finetuned-ucf101-subset\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nvideomae-base-finetuned-ucf101-subset\nThis model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.5004\nAccuracy: 0.8516\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_ratio: 0.1\ntraining_steps: 148\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\n2.143\n0.26\n38\n1.8739\n0.5\n0.9177\n1.26\n76\n0.9706\n0.7857\n0.4706\n2.26\n114\n0.5701\n0.8429\n0.2895\n3.23\n148\n0.4121\n0.9143\nFramework versions\nTransformers 4.24.0\nPytorch 1.12.1+cu113\nDatasets 2.6.1\nTokenizers 0.13.2",
    "nateraw/videomae-base-finetuned-ucf101": "Model Card for videomae-base-finetuned-ucf101\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure [optional]\nPreprocessing\nSpeeds, Sizes, Times\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nFactors\nMetrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nHardware\nSoftware\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nHow to Get Started with the Model\nModel Card for videomae-base-finetuned-ucf101\nA WandB report here for metrics.\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nModel Examination\nEnvironmental Impact\nTechnical Specifications\nCitation\nGlossary\nMore Information\nModel Card Authors\nModel Card Contact\nHow To Get Started With the Model\nModel Details\nModel Description\nVideoMAE Base model fine tuned on UCF101\nDeveloped by: @nateraw\nShared by [optional]: [More Information Needed]\nModel type: fine-tuned\nLanguage(s) (NLP): en\nLicense: mit\nRelated Models [optional]: [More Information Needed]\nParent Model [optional]: MCG-NJU/videomae-base\nResources for more information: [More Information Needed]\nUses\nDirect Use\nThis model can be used for Video Action Recognition\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recomendations.\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure [optional]\nPreprocessing\nWe sampled clips from the videos of 64 frames, then took a uniform sample of those frames to get 16 frame inputs for the model. During training, we used PyTorchVideo's MixVideo to apply mixup/cutmix.\nSpeeds, Sizes, Times\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\nWe only trained/evaluated one fold from the UCF101 annotations. Unlike in the VideoMAE paper, we did not perform inference over multiple crops/segments of validation videos, so the results are likely slightly lower than what you would get if you did that too.\nEval Accuracy: 0.758209764957428\nEval Accuracy Top 5: 0.8983050584793091\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n@nateraw\nModel Card Contact\n@nateraw\nHow to Get Started with the Model\nUse the code below to get started with the model.\nClick to expand\nfrom decord import VideoReader, cpu\nimport torch\nimport numpy as np\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nfrom huggingface_hub import hf_hub_download\nnp.random.seed(0)\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\nconverted_len = int(clip_len * frame_sample_rate)\nend_idx = np.random.randint(converted_len, seg_len)\nstart_idx = end_idx - converted_len\nindices = np.linspace(start_idx, end_idx, num=clip_len)\nindices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\nreturn indices\n# video clip consists of 300 frames (10 seconds at 30 FPS)\nfile_path = hf_hub_download(\nrepo_id=\"nateraw/dino-clips\", filename=\"archery.mp4\", repo_type=\"space\"\n)\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n# sample 16 frames\nvideoreader.seek(0)\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\nvideo = videoreader.get_batch(indices).asnumpy()\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"nateraw/videomae-base-finetuned-ucf101\")\nmodel = VideoMAEForVideoClassification.from_pretrained(\"nateraw/videomae-base-finetuned-ucf101\")\ninputs = feature_extractor(list(video), return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 101 UCF101 classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])",
    "shi-labs/oneformer_ade20k_swin_large": "OneFormer\nModel description\nIntended uses & limitations\nHow to use\nCitation\nOneFormer\nOneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository.\nModel description\nOneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\nIntended uses & limitations\nYou can use this particular checkpoint for semantic, instance and panoptic segmentation. See the model hub to look for other fine-tuned versions on a different dataset.\nHow to use\nHere is how to use this model:\nfrom transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\nfrom PIL import Image\nimport requests\nurl = \"https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# Loading a single model for all three tasks\nprocessor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\nmodel = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\n# Semantic Segmentation\nsemantic_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\")\nsemantic_outputs = model(**semantic_inputs)\n# pass through image_processor for postprocessing\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# Instance Segmentation\ninstance_inputs = processor(images=image, task_inputs=[\"instance\"], return_tensors=\"pt\")\ninstance_outputs = model(**instance_inputs)\n# pass through image_processor for postprocessing\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\n# Panoptic Segmentation\npanoptic_inputs = processor(images=image, task_inputs=[\"panoptic\"], return_tensors=\"pt\")\npanoptic_outputs = model(**panoptic_inputs)\n# pass through image_processor for postprocessing\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\nFor more examples, please refer to the documentation.\nCitation\n@article{jain2022oneformer,\ntitle={{OneFormer: One Transformer to Rule Universal Image Segmentation}},\nauthor={Jitesh Jain and Jiachen Li and MangTik Chiu and Ali Hassani and Nikita Orlov and Humphrey Shi},\njournal={arXiv},\nyear={2022}\n}",
    "microsoft/biogpt": "BioGPT\nCitation\nBioGPT\nPre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n>>> from transformers import pipeline, set_seed\n>>> from transformers import BioGptTokenizer, BioGptForCausalLM\n>>> model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n>>> tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n>>> set_seed(42)\n>>> generator(\"COVID-19 is\", max_length=20, num_return_sequences=5, do_sample=True)\n[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},\n{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},\n{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},\n{'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},\n{'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import BioGptTokenizer, BioGptForCausalLM\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nBeam-search decoding:\nimport torch\nfrom transformers import BioGptTokenizer, BioGptForCausalLM, set_seed\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\nsentence = \"COVID-19 is\"\ninputs = tokenizer(sentence, return_tensors=\"pt\")\nset_seed(42)\nwith torch.no_grad():\nbeam_output = model.generate(**inputs,\nmin_length=100,\nmax_length=1024,\nnum_beams=5,\nearly_stopping=True\n)\ntokenizer.decode(beam_output[0], skip_special_tokens=True)\n'COVID-19 is a global pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the causative agent of coronavirus disease 2019 (COVID-19), which has spread to more than 200 countries and territories, including the United States (US), Canada, Australia, New Zealand, the United Kingdom (UK), and the United States of America (USA), as of March 11, 2020, with more than 800,000 confirmed cases and more than 800,000 deaths.'\nCitation\nIf you find BioGPT useful in your research, please cite the following paper:\n@article{10.1093/bib/bbac409,\nauthor = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},\ntitle = \"{BioGPT: generative pre-trained transformer for biomedical text generation and mining}\",\njournal = {Briefings in Bioinformatics},\nvolume = {23},\nnumber = {6},\nyear = {2022},\nmonth = {09},\nabstract = \"{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\\%, 38.42\\% and 40.76\\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}\",\nissn = {1477-4054},\ndoi = {10.1093/bib/bbac409},\nurl = {https://doi.org/10.1093/bib/bbac409},\nnote = {bbac409},\neprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},\n}"
}