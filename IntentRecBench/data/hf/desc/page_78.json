{
    "zjpshadow/CharacterGen": "This is the model parameters of CharacterGen.\nThanks for downloading the checkpoint~\nIf you find our work helpful, please consider citing\n@article\n{peng2024charactergen,\ntitle ={CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization},\nauthor ={Hao-Yang Peng and Jia-Peng Zhang and Meng-Hao Guo and Yan-Pei Cao and Shi-Min Hu},\njournal ={ACM Transactions on Graphics (TOG)},\nyear ={2024},\nvolume ={43},\nnumber ={4},\ndoi ={10.1145/3658217}\n}",
    "knowledgator/UTC-DeBERTa-large-v2": "UTC-DeBERTa-large - universal token classifier\nHow to run with utca:\nBenchmarking\nFuture reading\nFeedback\nJoin Our Discord\nUTC-DeBERTa-large - universal token classifier\nðŸš€ Meet the second version of our prompt-tuned universal token classification model ðŸš€\nThis line of models can perform various information extraction tasks by analysing input prompts and recognizing parts of texts that satisfy prompts. In comparison with the first version, the second one is more general and can recognised as entities, whole sentences, and even paragraphs.\nTo use a model, just specify a prompt, for example : â€œIdentify all positive aspects of the product mentioned by John: â€œ and put your target text.\nThis is a model based on DeBERTaV3-large that was trained on multiple token classification tasks or tasks that can be represented in this way.\nSuch multi-task fine-tuning enabled better generalization; even small models can be used for zero-shot named entity recognition and demonstrate good performance on reading comprehension tasks.\nThe model can be used for the following tasks:\nNamed entity recognition (NER);\nOpen information extraction;\nQuestion answering;\nRelation extraction;\nCoreference resolution;\nText cleaning;\nSummarization;\nHow to use\nThere are few ways how you can use this model, one of the way is to utilize token-classification pipeline from transformers:\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ndef process(text, prompt, treshold=0.5):\n\"\"\"\nProcesses text by preparing prompt and adjusting indices.\nArgs:\ntext (str): The text to process\nprompt (str): The prompt to prepend to the text\nReturns:\nlist: A list of dicts with adjusted spans and scores\n\"\"\"\n# Concatenate text and prompt for full input\ninput_ = f\"{prompt}\\n{text}\"\nresults = nlp(input_) # Run NLP on full input\nprocessed_results = []\nprompt_length = len(prompt) # Get prompt length\nfor result in results:\n# check whether score is higher than treshold\nif result['score']<treshold:\ncontinue\n# Adjust indices by subtracting prompt length\nstart = result['start'] - prompt_length\n# If indexes belongs to the prompt - continue\nif start<0:\ncontinue\nend = result['end'] - prompt_length\n# Extract span from original text using adjusted indices\nspan = text[start:end]\n# Create processed result dict\nprocessed_result = {\n'span': span,\n'start': start,\n'end': end,\n'score': result['score']\n}\nprocessed_results.append(processed_result)\nreturn processed_results\ntokenizer = AutoTokenizer.from_pretrained(\"knowledgator/UTC-DeBERTa-large-v2\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"knowledgator/UTC-DeBERTa-large-v2\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy = 'first')\nTo use the model for zero-shot named entity recognition, we recommend to utilize the following prompt:\nprompt = \"\"\"Identify the following entity classes in the text:\ncomputer\nText:\n\"\"\"\ntext = \"\"\"Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs (1955â€“2011) and Ronald Wayne to develop and sell Wozniak's Apple I personal computer.\nIt was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers.\nApple went public in 1980 to instant financial success.\"\"\"\nresults = process(text, prompt)\nprint(results)\nTo use the model for open information extracttion, put any prompt you want:\nprompt = \"\"\"Extract all positive aspects about the product\n\"\"\"\ntext = \"\"\"I recently purchased the Sony WH-1000XM4 Wireless Noise-Canceling Headphones from Amazon and I must say, I'm thoroughly impressed. The package arrived in New York within 2 days, thanks to Amazon Prime's expedited shipping.\nThe headphones themselves are remarkable. The noise-canceling feature works like a charm in the bustling city environment, and the 30-hour battery life means I don't have to charge them every day. Connecting them to my Samsung Galaxy S21 was a breeze, and the sound quality is second to none.\nI also appreciated the customer service from Amazon when I had a question about the warranty. They responded within an hour and provided all the information I needed.\nHowever, the headphones did not come with a hard case, which was listed in the product description. I contacted Amazon, and they offered a 10% discount on my next purchase as an apology.\nOverall, I'd give these headphones a 4.5/5 rating and highly recommend them to anyone looking for top-notch quality in both product and service.\"\"\"\nresults = process(text, prompt)\nprint(results)\nTo try the model in question answering, just specify question and text passage:\nquestion = \"\"\"Who are the founders of Microsoft?\"\"\"\ntext = \"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975 to develop and sell BASIC interpreters for the Altair 8800.\nDuring his career at Microsoft, Gates held the positions of chairman, chief executive officer, president and chief software architect, while also being the largest individual shareholder until May 2014.\"\"\"\ninput_ = f\"{question} {text}\"\nresults = process(text, question)\nprint(results)\nFor the text cleaning, please, specify the following prompt, it will recognize the part of the text that should be erased:\nprompt = \"\"\"Clean the following text extracted from the web matching not relevant parts:\"\"\"\ntext = \"\"\"The mechanism of action was characterized using native mass spectrometry, the thermal shift-binding assay, and enzymatic kinetic studies (Figure ). In the native mass spectrometry binding assay, compound 23R showed dose-dependent binding to SARS-CoV-2 Mpro, similar to the positive control GC376, with a binding stoichiometry of one drug per monomer (Figure A).\nSimilarly, compound 23R showed dose-dependent stabilization of the SARS-CoV-2 Mpro in the thermal shift binding assay with an apparent Kd value of 9.43 Î¼M, a 9.3-fold decrease compared to ML188 (1) (Figure B). In the enzymatic kinetic studies, 23R was shown to be a noncovalent inhibitor with a Ki value of 0.07 Î¼M (Figure C, D top and middle panels). In comparison, the Ki for the parent compound ML188 (1) is 2.29 Î¼M.\nThe Lineweaverâ€“Burk or double-reciprocal plot with different compound concentrations yielded an intercept at the Y-axis, suggesting that 23R is a competitive inhibitor similar to ML188 (1) (Figure C, D bottom panel). Buy our T-shirts for the lowerst prices you can find!!!  Overall, the enzymatic kinetic studies confirmed that compound 23R is a noncovalent inhibitor of SARS-CoV-2 Mpro.\"\"\"\nresults = process(text, prompt)\nprint(results)\nIt's possible to use the model for relation extraction, it allows in N*C operations to extract all relations between entities, where N - number of entities and C - number of classes:\nrex_prompt=\"\"\"\nIdentify target entity given the following relation: \"{}\" and the following source entity: \"{}\"\nText:\n\"\"\"\ntext = \"\"\"Dr. Paul Hammond, a renowned neurologist at Johns Hopkins University, has recently published a paper in the prestigious journal \"Nature Neuroscience\". \"\"\"\nentity = \"Paul Hammond\"\nrelation = \"worked at\"\nprompt = rex_prompt.format(relation, entity)\nresults = process(text, prompt)\nprint(results)\nTo find similar entities in the text, consider the following example:\nent_prompt = \"Find all '{}' mentions in the text:\"\ntext = \"\"\"Several studies have reported its pharmacological activities, including anti-inflammatory, antimicrobial, and antitumoral effects. The effect of E-anethole was studied in the osteosarcoma MG-63 cell line, and the antiproliferative activity was evaluated by an MTT assay. It showed a GI50 value of 60.25 Î¼M with apoptosis induction through the mitochondrial-mediated pathway. Additionally, it induced cell cycle arrest at the G0/G1 phase, up-regulated the expression of p53, caspase-3, and caspase-9, and down-regulated Bcl-xL expression. Moreover, the antitumoral activity of anethole was assessed against oral tumor Ca9-22 cells, and the cytotoxic effects were evaluated by MTT and LDH assays. It demonstrated a LD50 value of 8 Î¼M, and cellular proliferation was 42.7% and 5.2% at anethole concentrations of 3 Î¼M and 30 Î¼M, respectively. It was reported that it could selectively and in a dose-dependent manner decrease cell proliferation and induce apoptosis, as well as induce autophagy, decrease ROS production, and increase glutathione activity. The cytotoxic effect was mediated through NF-kB, MAP kinases, Wnt, caspase-3 and -9, and PARP1 pathways. Additionally, treatment with anethole inhibited cyclin D1 oncogene expression, increased cyclin-dependent kinase inhibitor p21WAF1, up-regulated p53 expression, and inhibited the EMT markers.\"\"\"\nentity = \"anethole\"\nprompt = ent_prompt.format(entity)\nresults = process(text, prompt)\nprint(results)\nWe significantly improved model summarization abilities in comparison to the first version, below is an example:\nprompt = \"Summarize the following text, highlighting the most important sentences:\"\ntext = \"\"\"Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs (1955â€“2011) and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. Apple went public in 1980 to instant financial success. The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement called \"1984\". By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT, taking some Apple employees with him.\nApple Inc. is an American multinational technology company headquartered in Cupertino, California. Apple is the world's largest technology company by revenue, with US$394.3 billion in 2022 revenue. As of March 2023, Apple is the world's biggest company by market capitalization. As of June 2022, Apple is the fourth-largest personal computer vendor by unit sales and the second-largest mobile phone manufacturer in the world. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Meta Platforms, and Microsoft.\nAs the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of the Microsoft Windows operating system on Intel-powered PC clones (also known as \"Wintel\"). In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone and iPad to critical acclaim, launching the \"Think different\" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health reasons, and died two months later, he was succeeded as CEO by Tim Cook\"\"\"\nresults = process(text, prompt)\nprint(results)\nHow to run with utca:\nFirst of all, you need to install the package:\npip install utca -U\nAfter that you to create predictor that will run UTC model:\nfrom utca.core import (\nAddData,\nRenameAttribute,\nFlush\n)\nfrom utca.implementation.predictors import (\nTokenSearcherPredictor, TokenSearcherPredictorConfig\n)\nfrom utca.implementation.tasks import (\nTokenSearcherNER,\nTokenSearcherNERPostprocessor,\n)\npredictor = TokenSearcherPredictor(\nTokenSearcherPredictorConfig(\ndevice=\"cuda:0\",\nmodel=\"knowledgator/UTC-DeBERTa-large-v2\"\n)\n)\nFor NER model you should create the following pipeline:\nner_task = TokenSearcherNER(\npredictor=predictor,\npostprocess=[TokenSearcherNERPostprocessor(\nthreshold=0.5\n)]\n)\nner_task = TokenSearcherNER()\npipeline = (\nAddData({\"labels\": [\"scientist\", \"university\", \"city\"]})\n| ner_task\n| Flush(keys=[\"labels\"])\n| RenameAttribute(\"output\", \"entities\")\n)\nAnd after that you can put your text for prediction and run the pipeline:\nres = pipeline.run({\n\"text\": \"\"\"Dr. Paul Hammond, a renowned neurologist at Johns Hopkins University, has recently published a paper in the prestigious journal \"Nature Neuroscience\".\nHis research focuses on a rare genetic mutation, found in less than 0.01% of the population, that appears to prevent the development of Alzheimer's disease. Collaborating with researchers at the University of California, San Francisco, the team is now working to understand the mechanism by which this mutation confers its protective effect.\nFunded by the National Institutes of Health, their research could potentially open new avenues for Alzheimer's treatment.\"\"\"\n})\nTo use utca for relation extraction construct the following pipeline:\nfrom utca.implementation.tasks import (\nTokenSearcherNER,\nTokenSearcherNERPostprocessor,\nTokenSearcherRelationExtraction,\nTokenSearcherRelationExtractionPostprocessor,\n)\npipe = (\nTokenSearcherNER( # TokenSearcherNER task produces classified entities that will be at the \"output\" key.\npredictor=predictor,\npostprocess=TokenSearcherNERPostprocessor(\nthreshold=0.5 # Entity threshold\n)\n)\n| RenameAttribute(\"output\", \"entities\") # Rename output entities from TokenSearcherNER task to use them as inputs in TokenSearcherRelationExtraction\n| TokenSearcherRelationExtraction( # TokenSearcherRelationExtraction is used for relation extraction.\npredictor=predictor,\npostprocess=TokenSearcherRelationExtractionPostprocessor(\nthreshold=0.5 # Relation threshold\n)\n)\n)\nTo run pipeline you need to specify parameters for entities and relations:\nr = pipe.run({\n\"text\": text, # Text to process\n\"labels\": [ # Labels used by TokenSearcherNER for entity extraction\n\"scientist\",\n\"university\",\n\"city\",\n\"research\",\n\"journal\",\n],\n\"relations\": [{ # Relation parameters\n\"relation\": \"published at\", # Relation label. Required parameter.\n\"pairs_filter\": [(\"scientist\", \"journal\")], # Optional parameter. It specifies possible members of relations by their entity labels.\n# Here, \"scientist\" is the entity label of the source, and \"journal\" is the target's entity label.\n# If provided, only specified pairs will be returned.\n},{\n\"relation\": \"worked at\",\n\"pairs_filter\": [(\"scientist\", \"university\"), (\"scientist\", \"other\")],\n\"distance_threshold\": 100, # Optional parameter. It specifies the max distance between spans in the text (i.e., the end of the span that is closer to the start of the text and the start of the next one).\n}]\n})\nprint(r[\"output\"])\nBenchmarking\nBelow is a table that highlights the performance of UTC models on the CrossNER dataset. The values represent the Micro F1 scores, with the estimation done at the word level.\nModel\nAI\nLiterature\nMusic\nPolitics\nScience\nUTC-DeBERTa-small\n0.8492\n0.8792\n0.864\n0.9008\n0.85\nUTC-DeBERTa-base\n0.8452\n0.8587\n0.8711\n0.9147\n0.8631\nUTC-DeBERTa-large\n0.8971\n0.8978\n0.9204\n0.9247\n0.8779\nFuture reading\nCheck our blogpost - \"As GPT4 but for token classification\", where we highlighted possible use-cases of the model and why next-token prediction is not the only way to achive amazing zero-shot capabilites.\nWhile most of the AI industry is focused on generative AI and decoder-based models, we are committed to developing encoder-based models.\nWe aim to achieve the same level of generalization for such models as their decoder brothers. Encoders have several wonderful properties, such as bidirectional attention, and they are the best choice for many information extraction tasks in terms of efficiency and controllability.\nFeedback\nWe value your input! Share your feedback and suggestions to help us improve our models.\nFill out the feedback form\nJoin Our Discord\nConnect with our community on Discord for news, support, and discussion about our models.\nJoin Discord",
    "mradermacher/DeepSeek-V2-Lite-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/ZZichen/DeepSeek-V2-Lite\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n6.5\nGGUF\nIQ3_XS\n7.2\nGGUF\nIQ3_S\n7.6\nbeats Q3_K*\nGGUF\nQ3_K_S\n7.6\nGGUF\nIQ3_M\n7.7\nGGUF\nQ3_K_M\n8.2\nlower quality\nGGUF\nQ3_K_L\n8.6\nGGUF\nIQ4_XS\n8.7\nGGUF\nQ4_K_S\n9.6\nfast, recommended\nGGUF\nQ4_K_M\n10.5\nfast, recommended\nGGUF\nQ5_K_S\n11.2\nGGUF\nQ5_K_M\n12.0\nGGUF\nQ6_K\n14.2\nvery good quality\nGGUF\nQ8_0\n16.8\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "jameslahm/yolov10n": "Model Description\nInstallation\nTraining and validation\nInference\nBibTeX Entry and Citation Info\nModel Description\nYOLOv10: Real-Time End-to-End Object Detection\narXiv: https://arxiv.org/abs/2405.14458v1\ngithub: https://github.com/THU-MIG/yolov10\nInstallation\npip install git+https://github.com/THU-MIG/yolov10.git\nTraining and validation\nfrom ultralytics import YOLOv10\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10n')\n# Training\nmodel.train(...)\n# after training, one can push to the hub\nmodel.push_to_hub(\"your-hf-username/yolov10-finetuned\")\n# Validation\nmodel.val(...)\nInference\nHere's an end-to-end example showcasing inference on a cats image:\nfrom ultralytics import YOLOv10\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10n')\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)\nwhich shows:\nBibTeX Entry and Citation Info\n@article{wang2024yolov10,\ntitle={YOLOv10: Real-Time End-to-End Object Detection},\nauthor={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},\njournal={arXiv preprint arXiv:2405.14458},\nyear={2024}\n}",
    "BSC-NLP4BIA/bsc-bio-ehr-es-carmen-drugtemist": "bsc-bio-ehr-es-carmen-drugtemist\nTable of contents\nModel description\nHow to use\nLimitations and bias\nAdditional information\nAuthors\nContact information\nFunding\nCiting information\nDisclaimer\nbsc-bio-ehr-es-carmen-drugtemist\nTable of contents\nClick to expand\nModel description\nHow to use\nLimitations and bias\nAdditional information\nAuthors\nContact information\nFunding\nCiting information\nDisclaimer\nModel description\nModel trained on the medication mentions of CARMEN-I and DrugTEMIST.\nHow to use\nâš  We recommend pre-tokenizing the input text into words instead of providing it directly to the model, as this is how the model was trained. Otherwise, the results and performance might get affected.\nA usage example can be found here.\nLimitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\nAdditional information\nAuthors\nNLP4BIA team at the Barcelona Supercomputing Center (nlp4bia@bsc.es).\nContact information\njan.rodriguez [at] bsc.es\nFunding\nThis project was partially funded by the Spanish Plan for the Advancement of Language Technology (Plan TL) in collaboration with the Barcelona Supercomputing Center (BSC) and the Hospital Clinic de Barcelona (HCB). On the BSC's side, we acknowledge additional funding by the Spanish National AI4ProfHealth project (PID2020-119266RA-I00 MICIU/AEI/10.13039/501100011033) and EU Horizon projects (AI4HF 101080430 and DataTools4Heart 101057849). On the HCB's side, the project was also supported by the Instituto de Salud Carlos III (ISCIII).\nCiting information\nPlease cite the following works:\n@article{LimaLopez2025,\nauthor       = {Salvador Lima-LÃ³pez and EulÃ lia FarrÃ©-Maduell and Luis Gasco and Jan RodrÃ­guez-Miret and Santiago Frid and Xavier Pastor and Xavier Borrat and Martin Krallinger},\ntitle        = {A textual dataset of de-identified health records in Spanish and Catalan for medical entity recognition and anonymization},\njournal      = {Scientific Data},\nvolume       = {12},\npages        = {Article 1088},\nyear         = {2025},\npublisher    = {Nature Publishing Group},\ndoi          = {10.1038/s41597-025-05320-1},\nurl          = {https://www.nature.com/articles/s41597-025-05320-1}\n}\n@misc{carmen_physionet,\nauthor = {Farre Maduell, Eulalia and Lima-Lopez, Salvador and Frid, Santiago Andres and Conesa, Artur and Asensio, Elisa and Lopez-Rueda, Antonio and Arino, Helena and Calvo, Elena and Bertran, Maria JesÃºs and Marcos, Maria Angeles and Nofre Maiz, Montserrat and TaÃ±Ã¡ Velasco, Laura and Marti, Antonia and Farreres, Ricardo and Pastor, Xavier and Borrat Frigola, Xavier and Krallinger, Martin},\ntitle = {{CARMEN-I: A resource of anonymized electronic health records in Spanish and Catalan for training and testing NLP tools (version 1.0.1)}},\nyear = {2024},\npublisher = {PhysioNet},\nurl = {https://doi.org/10.13026/x7ed-9r91}\n},\n@inproceedings{multicardioner2024overview,\ntitle = {{Overview of MultiCardioNER task at BioASQ 2024 on Medical Speciality and Language Adaptation of Clinical NER Systems for Spanish, English and Italian}},\nauthor = {Salvador Lima-L\\'opez and Eul\\`alia Farr\\'e-Maduell and Jan Rodr\\'iguez-Miret and Miguel Rodr\\'iguez-Ortega and Livia Lilli and Jacopo Lenkowicz and Giovanna Ceroni and Jonathan Kossoff and Anoop Shah and Anastasios Nentidis and Anastasia Krithara and Georgios Katsimpras and Georgios Paliouras and Martin Krallinger},\nbooktitle = {CLEF Working Notes},\nyear = {2024},\neditor = {Faggioli, Guglielmo and Ferro, Nicola and GaluÅ¡Ä\\'akov\\'a, Petra and Garc\\'ia Seco de Herrera, Alba}\n}\n@article{physionet,\nauthor = {Ary L. Goldberger  and Luis A. N. Amaral  and Leon Glass  and Jeffrey M. Hausdorff  and Plamen Ch. Ivanov  and Roger G. Mark  and Joseph E. Mietus  and George B. Moody  and Chung-Kang Peng  and H. Eugene Stanley },\ntitle = {PhysioBank, PhysioToolkit, and PhysioNet  },\njournal = {Circulation},\nvolume = {101},\nnumber = {23},\npages = {e215-e220},\nyear = {2000},\ndoi = {10.1161/01.CIR.101.23.e215},\nURL = {https://www.ahajournals.org/doi/abs/10.1161/01.CIR.101.23.e215}\n}\nDisclaimer\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\nWhen third parties deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of artificial intelligence.\nLos modelos publicados en este repositorio tienen una finalidad generalista y estÃ¡n a disposiciÃ³n de terceros. Estos modelos pueden tener sesgos y/u otro tipo de distorsiones indeseables.\nCuando terceros desplieguen o proporcionen sistemas y/o servicios a otras partes usando alguno de estos modelos (o utilizando sistemas basados en estos modelos) o se conviertan en usuarios de los modelos, deben tener en cuenta que es su responsabilidad mitigar los riesgos derivados de su uso y, en todo caso, cumplir con la normativa aplicable, incluyendo la normativa en materia de uso de inteligencia artificial.",
    "zai-org/glm-4-9b-chat": "GLM-4-9B-Chat\næ¨¡åž‹ä»‹ç»\nè¯„æµ‹ç»“æžœ\né•¿æ–‡æœ¬\nå¤šè¯­è¨€èƒ½åŠ›\nå·¥å…·è°ƒç”¨èƒ½åŠ›\nè¿è¡Œæ¨¡åž‹\nä½¿ç”¨ transformers åŽç«¯è¿›è¡ŒæŽ¨ç†:\nåè®®\nå¼•ç”¨\nGLM-4-9B-Chat\nRead this in English.\n2024/11/25, æˆ‘ä»¬å»ºè®®ä½¿ç”¨ä»Ž transformers>=4.46.0 å¼€å§‹ï¼Œä½¿ç”¨ glm-4-9b-chat-hf ä»¥å‡å°‘åŽç»­ transformers å‡çº§å¯¼è‡´çš„å…¼å®¹æ€§é—®é¢˜ã€‚\n2024/08/12, æœ¬ä»“åº“ä»£ç å·²æ›´æ–°å¹¶ä½¿ç”¨ transformers>=4.44.0, è¯·åŠæ—¶æ›´æ–°ä¾èµ–ã€‚\n2024/07/24ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸Žé•¿æ–‡æœ¬ç›¸å…³çš„æœ€æ–°æŠ€æœ¯è§£è¯»ï¼Œå…³æ³¨ è¿™é‡Œ æŸ¥çœ‹æˆ‘ä»¬åœ¨è®­ç»ƒ GLM-4-9B å¼€æºæ¨¡åž‹ä¸­å…³äºŽé•¿æ–‡æœ¬æŠ€æœ¯çš„æŠ€æœ¯æŠ¥å‘Š\næ¨¡åž‹ä»‹ç»\nGLM-4-9B æ˜¯æ™ºè°± AI æŽ¨å‡ºçš„æœ€æ–°ä¸€ä»£é¢„è®­ç»ƒæ¨¡åž‹ GLM-4 ç³»åˆ—ä¸­çš„å¼€æºç‰ˆæœ¬ã€‚\nåœ¨è¯­ä¹‰ã€æ•°å­¦ã€æŽ¨ç†ã€ä»£ç å’ŒçŸ¥è¯†ç­‰å¤šæ–¹é¢çš„æ•°æ®é›†æµ‹è¯„ä¸­ï¼ŒGLM-4-9B åŠå…¶äººç±»åå¥½å¯¹é½çš„ç‰ˆæœ¬ GLM-4-9B-Chat å‡è¡¨çŽ°å‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚\né™¤äº†èƒ½è¿›è¡Œå¤šè½®å¯¹è¯ï¼ŒGLM-4-9B-Chat è¿˜å…·å¤‡ç½‘é¡µæµè§ˆã€ä»£ç æ‰§è¡Œã€è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰å’Œé•¿æ–‡æœ¬æŽ¨ç†ï¼ˆæ”¯æŒæœ€å¤§ 128K\nä¸Šä¸‹æ–‡ï¼‰ç­‰é«˜çº§åŠŸèƒ½ã€‚\næœ¬ä»£æ¨¡åž‹å¢žåŠ äº†å¤šè¯­è¨€æ”¯æŒï¼Œæ”¯æŒåŒ…æ‹¬æ—¥è¯­ï¼ŒéŸ©è¯­ï¼Œå¾·è¯­åœ¨å†…çš„ 26 ç§è¯­è¨€ã€‚æˆ‘ä»¬è¿˜æŽ¨å‡ºäº†æ”¯æŒ 1M ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆçº¦ 200 ä¸‡ä¸­æ–‡å­—ç¬¦ï¼‰çš„æ¨¡åž‹ã€‚\nè¯„æµ‹ç»“æžœ\næˆ‘ä»¬åœ¨ä¸€äº›ç»å…¸ä»»åŠ¡ä¸Šå¯¹ GLM-4-9B-Chat æ¨¡åž‹è¿›è¡Œäº†è¯„æµ‹,å¹¶å¾—åˆ°äº†å¦‚ä¸‹çš„ç»“æžœ:\nModel\nAlignBench-v2\nMT-Bench\nIFEval\nMMLU\nC-Eval\nGSM8K\nMATH\nHumanEval\nNCB\nLlama-3-8B-Instruct\n5.12\n8.00\n68.58\n68.4\n51.3\n79.6\n30.0\n62.2\n24.7\nChatGLM3-6B\n3.97\n5.50\n28.1\n66.4\n69.0\n72.3\n25.7\n58.5\n11.3\nGLM-4-9B-Chat\n6.61\n8.35\n69.0\n72.4\n75.6\n79.6\n50.6\n71.8\n32.2\né•¿æ–‡æœ¬\nåœ¨ 1M çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹è¿›è¡Œå¤§æµ·æžé’ˆå®žéªŒï¼Œç»“æžœå¦‚ä¸‹ï¼š\nåœ¨ LongBench-Chat ä¸Šå¯¹é•¿æ–‡æœ¬èƒ½åŠ›è¿›è¡Œäº†è¿›ä¸€æ­¥è¯„æµ‹ï¼Œç»“æžœå¦‚ä¸‹:\nå¤šè¯­è¨€èƒ½åŠ›\nåœ¨å…­ä¸ªå¤šè¯­è¨€æ•°æ®é›†ä¸Šå¯¹ GLM-4-9B-Chat å’Œ Llama-3-8B-Instruct è¿›è¡Œäº†æµ‹è¯•ï¼Œæµ‹è¯•ç»“æžœåŠæ•°æ®é›†å¯¹åº”é€‰å–è¯­è¨€å¦‚ä¸‹è¡¨\nDataset\nLlama-3-8B-Instruct\nGLM-4-9B-Chat\nLanguages\nM-MMLU\n49.6\n56.6\nall\nFLORES\n25.0\n28.8\nru, es, de, fr, it, pt, pl, ja, nl, ar, tr, cs, vi, fa, hu, el, ro, sv, uk, fi, ko, da, bg, no\nMGSM\n54.0\n65.3\nzh, en, bn, de, es, fr, ja, ru, sw, te, th\nXWinograd\n61.7\n73.1\nzh, en, fr, jp, ru, pt\nXStoryCloze\n84.7\n90.7\nzh, en, ar, es, eu, hi, id, my, ru, sw, te\nXCOPA\n73.3\n80.1\nzh, et, ht, id, it, qu, sw, ta, th, tr, vi\nå·¥å…·è°ƒç”¨èƒ½åŠ›\næˆ‘ä»¬åœ¨ Berkeley Function Calling Leaderboard\nä¸Šè¿›è¡Œäº†æµ‹è¯•å¹¶å¾—åˆ°äº†ä»¥ä¸‹ç»“æžœï¼š\nModel\nOverall Acc.\nAST Summary\nExec Summary\nRelevance\nLlama-3-8B-Instruct\n58.88\n59.25\n70.01\n45.83\ngpt-4-turbo-2024-04-09\n81.24\n82.14\n78.61\n88.75\nChatGLM3-6B\n57.88\n62.18\n69.78\n5.42\nGLM-4-9B-Chat\n81.00\n80.26\n84.40\n87.92\næœ¬ä»“åº“æ˜¯ GLM-4-9B-Chat çš„æ¨¡åž‹ä»“åº“ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡é•¿åº¦ã€‚\nè¿è¡Œæ¨¡åž‹\næ›´å¤šæŽ¨ç†ä»£ç å’Œä¾èµ–ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ githubã€‚\nè¯·ä¸¥æ ¼æŒ‰ç…§ä¾èµ–å®‰è£…ï¼Œå¦åˆ™æ— æ³•æ­£å¸¸è¿è¡Œã€‚\nä½¿ç”¨ transformers åŽç«¯è¿›è¡ŒæŽ¨ç†:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat\", trust_remote_code=True)\nquery = \"ä½ å¥½\"\ninputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}],\nadd_generation_prompt=True,\ntokenize=True,\nreturn_tensors=\"pt\",\nreturn_dict=True\n)\ninputs = inputs.to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"THUDM/glm-4-9b-chat\",\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\ntrust_remote_code=True\n).to(device).eval()\ngen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\nwith torch.no_grad():\noutputs = model.generate(**inputs, **gen_kwargs)\noutputs = outputs[:, inputs['input_ids'].shape[1]:]\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nä½¿ç”¨ vLLMåŽç«¯è¿›è¡ŒæŽ¨ç†:\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n# GLM-4-9B-Chat-1M\n# max_model_len, tp_size = 1048576, 4\n# GLM-4-9B-Chat\n# å¦‚æžœé‡è§ OOM çŽ°è±¡ï¼Œå»ºè®®å‡å°‘max_model_lenï¼Œæˆ–è€…å¢žåŠ tp_size\nmax_model_len, tp_size = 131072, 1\nmodel_name = \"THUDM/glm-4-9b-chat\"\nprompt = [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}]\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nllm = LLM(\nmodel=model_name,\ntensor_parallel_size=tp_size,\nmax_model_len=max_model_len,\ntrust_remote_code=True,\nenforce_eager=True,\n# GLM-4-9B-Chat-1M å¦‚æžœé‡è§ OOM çŽ°è±¡ï¼Œå»ºè®®å¼€å¯ä¸‹è¿°å‚æ•°\n# enable_chunked_prefill=True,\n# max_num_batched_tokens=8192\n)\nstop_token_ids = [151329, 151336, 151338]\nsampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)\ninputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\noutputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\nåè®®\nGLM-4 æ¨¡åž‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª LICENSEã€‚\nå¼•ç”¨\nå¦‚æžœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ã€‚\n@misc{glm2024chatglm,\ntitle={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},\nauthor={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\nyear={2024},\neprint={2406.12793},\narchivePrefix={arXiv},\nprimaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}",
    "maastrichtlawtech/dpr-legal-french": "Citation\nCitation\n@article{louis2024know,\nauthor     = {Louis, Antoine and van Dijck, Gijs and Spanakis, Gerasimos},\ntitle      = {Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain},\njournal    = {CoRR},\nvolume     = {abs/2409.01357},\nyear       = {2024},\nurl        = {https://arxiv.org/abs/2409.01357},\neprinttype = {arXiv},\neprint     = {2409.01357},\n}",
    "AverageBusinessUser/aidapal": "aiDAPal is a fine tune of mistral7b-instruct to assist with analysis of Hex-Rays psuedocode. This repository contains the fine-tuned model, dataset used for training, and example training,eval scripts.\nThe associated aiDAPal IDA Pro plugin can be downloaded on Github - https://github.com/atredispartners/aidapal\nInformation on the process and background of this project can be seen on the associated blog post: https://atredis.com/blog/2024/6/3/how-to-train-your-large-language-model",
    "imlixinyang/director3d": "README.md exists but content is empty.",
    "rethinklab/Bench2DriveZoo": "We implement state-of-the-art end-to-end autonomous driving methods in Bench2Drive. This repository stores the checkpoint trained on Bench2Drive.\nPlease refer to https://github.com/Thinklab-SJTU/Bench2Drive for details about Bench2Drive and how to use these checkpoints for closed-loop evaluation. (Actually drive the car!)\nPlease refer to https://github.com/Thinklab-SJTU/Bench2Drive-Zoo regarding the training of models.",
    "llava-hf/LLaVA-NeXT-Video-7B-hf": "LLaVA-NeXT-Video Model Card\nðŸ“„ Model details\nðŸ“š Training dataset\nImage\nVideo\nðŸ“Š Evaluation dataset\nðŸš€ How to use the model\nInference with images as inputs\nInference with images and videos as inputs\nModel optimization\nðŸ”’ License\nâœï¸ Citation\nLLaVA-NeXT-Video Model Card\nCheck out also the Google Colab demo to run Llava on a free-tier Google Colab instance:\nDisclaimer: The team releasing LLaVa-NeXT-Video did not write a model card for this model so this model card has been written by the Hugging Face team.\nðŸ“„ Model details\nModel type:\nLLaVA-Next-Video is an open-source chatbot trained by fine-tuning LLM on multimodal instruction-following data. The model is buit on top of LLaVa-NeXT by tuning on a mix of video and image data to achieve better video understanding capabilities. The videos were sampled uniformly to be 32 frames per clip.\nThe model is a current SOTA among open-source models on VideoMME bench.\nBase LLM: lmsys/vicuna-7b-v1.5\nModel date:\nLLaVA-Next-Video-7B was trained in April 2024.\nPaper or resources for more information: https://github.com/LLaVA-VL/LLaVA-NeXT\nðŸ“š Training dataset\nImage\n558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.\n158K GPT-generated multimodal instruction-following data.\n500K academic-task-oriented VQA data mixture.\n50K GPT-4V data mixture.\n40K ShareGPT data.\nVideo\n100K VideoChatGPT-Instruct.\nðŸ“Š Evaluation dataset\nA collection of 4 benchmarks, including 3 academic VQA benchmarks and 1 captioning benchmark.\nðŸš€ How to use the model\nFirst, make sure to have transformers >= 4.42.0.\nThe model supports multi-visual and multi-prompt generation. Meaning that you can pass multiple images/videos in your prompt. Make sure also to follow the correct prompt template (USER: xxx\\nASSISTANT:) and add the token <image> or <video> to the location where you want to query images/videos:\nBelow is an example script to run generation in float16 precision on a GPU device:\nimport av\nimport torch\nimport numpy as np\nfrom huggingface_hub import hf_hub_download\nfrom transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration\nmodel_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n).to(0)\nprocessor = LlavaNextVideoProcessor.from_pretrained(model_id)\ndef read_video_pyav(container, indices):\n'''\nDecode the video with PyAV decoder.\nArgs:\ncontainer (`av.container.input.InputContainer`): PyAV container.\nindices (`List[int]`): List of frame indices to decode.\nReturns:\nresult (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n'''\nframes = []\ncontainer.seek(0)\nstart_index = indices[0]\nend_index = indices[-1]\nfor i, frame in enumerate(container.decode(video=0)):\nif i > end_index:\nbreak\nif i >= start_index and i in indices:\nframes.append(frame)\nreturn np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n# define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\")\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n{\"type\": \"video\"},\n],\n},\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\nvideo_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\ncontainer = av.open(video_path)\n# sample uniformly 8 frames from the video, can sample more for longer videos\ntotal_frames = container.streams.video[0].frames\nindices = np.arange(0, total_frames, total_frames / 8).astype(int)\nclip = read_video_pyav(container, indices)\ninputs_video = processor(text=prompt, videos=clip, padding=True, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nFrom transformers>=v4.48, you can also pass image/video url or local path to the conversation history, and let the chat template handle the rest.\nFor video you also need to indicate how many num_frames to sample from video, otherwise the whole video will be loaded.\nChat template will load the image/video for you and return inputs in torch.Tensor which you can pass directly to model.generate().\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n{\"type\": \"video\", \"path\": \"my_video.mp4\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image and video?\"},\n],\n},\n]\ninputs = processor.apply_chat_template(messages, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors\"pt\")\noutput = model.generate(**inputs, max_new_tokens=50)\nInference with images as inputs\nTo generate from images use the below code after loading the model as shown above:\nimport requests\nfrom PIL import Image\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What are these?\"},\n{\"type\": \"image\"},\n],\n},\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs_image = processor(text=prompt, images=raw_image, return_tensors='pt').to(0, torch.float16)\noutput = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nInference with images and videos as inputs\nTo generate from images and videos in one generate use the below code after loading the model as shown above:\nconversation_1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What's the content of the image>\"},\n{\"type\": \"image\"},\n],\n}\n]\nconversation_2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n{\"type\": \"video\"},\n],\n},\n]\nprompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\nprompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\ns = processor(text=[prompt_1, prompt_2], images=image, videos=clip, padding=True, return_tensors=\"pt\").to(model.device)\n# Generate\ngenerate_ids = model.generate(**inputs, max_new_tokens=100)\nout = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(out)\nModel optimization\n4-bit quantization through bitsandbytes library\nFirst make sure to install bitsandbytes, pip install bitsandbytes and make sure to have access to a CUDA compatible GPU device. Simply change the snippet above with:\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n+   load_in_4bit=True\n)\nUse Flash-Attention 2 to further speed-up generation\nFirst make sure to install flash-attn. Refer to the original repository of Flash Attention regarding that package installation. Simply change the snippet above with:\nmodel = LlavaNextVideoForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n+   use_flash_attention_2=True\n).to(0)\nðŸ”’ License\nLlama 2 is licensed under the LLAMA 2 Community License,\nCopyright (c) Meta Platforms, Inc. All Rights Reserved.\nâœï¸ Citation\nIf you find our paper and code useful in your research:\n@misc{zhang2024llavanextvideo,\ntitle={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},\nurl={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},\nauthor={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},\nmonth={April},\nyear={2024}\n}\n@misc{liu2024llavanext,\ntitle={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},\nurl={https://llava-vl.github.io/blog/2024-01-30-llava-next/},\nauthor={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},\nmonth={January},\nyear={2024}\n}",
    "RichardErkhov/openaccess-ai-collective_-_manticore-13b-chat-pyg-gguf": "No model card",
    "kuleshov-group/mdlm-owt": "Using MDLM\nModel Details\nCitation\nModel Card Contact\nUsing MDLM\nTo use the pre-trained model for masked language modeling, use the following snippet:\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n# See the `MDLM` collection page on the hub for list of available models.\ntokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\nmodel_name = 'kuleshov-group/mdlm-owt'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\nFor more details, please see our github repository: MDLM\nModel Details\nThe model, which has a context length of 1024 and is similar in size to GPT2-medium with approximately 130 million non-embedding parameters,\nwas trained using a forward diffusion process that generates inputs varying from fully masked to fully unmasked. Its objective is to\nreconstruct the original input from these varying levels of masking, outputting logits in the process.\nThe training regimen comprised one million steps on the OpenWebText corpus, involving the processing of a total of 33 billion tokens.\nFor more details, please see our paper: Simple and Effective Masked Diffusion Language Models.\nCitation\nPlease cite our work using the bibtex below:\nBibTeX:\n@misc{sahoo2024simple,\ntitle={Simple and Effective Masked Diffusion Language Models},\nauthor={Subham Sekhar Sahoo and Marianne Arriola and Yair Schiff and Aaron Gokaslan and Edgar Marroquin and Justin T Chiu and Alexander Rush and Volodymyr Kuleshov},\nyear={2024},\neprint={2406.07524},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nAPA:\n@software{Sahoo_Simple_and_Effective_2024,\nauthor = {Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin T and Rush, Alexander and Kuleshov, Volodymyr},\ndoi = {10.48550/arXiv.2406.07524},\nmonth = jun,\ntitle = {{Simple and Effective Masked Diffusion Language Models}},\nversion = {arXiv:2406.07524v1},\nyear = {2024}\n}\nModel Card Contact\nSubham Sekhar Sahoo (ssahoo@cs.cornell.edu)",
    "hugohrban/progen2-base": "Mirror of the base ProGen2-base model (with slightly modified configuration and forward pass) by Nijkamp, et al..\nSee also my github repo for an example of finetuning this model.\nExample usage:\nfrom transformers import AutoModelForCausalLM\nfrom tokenizers import Tokenizer\nimport torch\nimport torch.nn.functional as F\n# load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"hugohrban/progen2-base\", trust_remote_code=True)\ntokenizer = Tokenizer.from_pretrained(\"hugohrban/progen2-base\")\ntokenizer.no_padding()\n# prepare input\nprompt = \"1MEVVIVTGMSGAGK\"\ninput_ids = torch.tensor(tokenizer.encode(prompt).ids).to(model.device)\n# forward pass\nlogits = model(input_ids).logits\n# print output probabilities\nnext_token_logits = logits[-1, :]\nnext_token_probs = F.softmax(next_token_logits, dim=-1)\nfor i in range(tokenizer.get_vocab_size(with_added_tokens=False)):\nprint(f\"{tokenizer.id_to_token(i)}: {100 * next_token_probs[i].item():.2f} %\")",
    "mradermacher/RP-Stew-v2.5-34B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/MarinaraSpaghetti/RP-Stew-v2.5-34B\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/RP-Stew-v2.5-34B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n12.9\nGGUF\nIQ3_XS\n14.3\nGGUF\nQ3_K_S\n15.1\nGGUF\nIQ3_S\n15.1\nbeats Q3_K*\nGGUF\nIQ3_M\n15.7\nGGUF\nQ3_K_M\n16.8\nlower quality\nGGUF\nQ3_K_L\n18.2\nGGUF\nIQ4_XS\n18.7\nGGUF\nQ4_K_S\n19.7\nfast, recommended\nGGUF\nQ4_K_M\n20.8\nfast, recommended\nGGUF\nQ5_K_S\n23.8\nGGUF\nQ5_K_M\n24.4\nGGUF\nQ6_K\n28.3\nvery good quality\nGGUF\nQ8_0\n36.6\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "John6666/hassaku-xl-hentai-v13-sdxl": "Original model is here.",
    "ehristoforu/Visionix-alpha-inpainting": "VisioniX Alpha-inpainting - the most powerful realism-model\nAbout this model\nWhy is this model better than its competitors?\nWho is this model for?\nOptimal settings for this model\nEnd\nVisioniX Alpha-inpainting - the most powerful realism-model\nWe present the best realism model at the moment - VisioniX.\nAbout this model\nThis model was created through complex training on huge, ultra-realistic datasets.\nWhy is this model better than its competitors?\nAll, absolutely all realism models make one important mistake: they chase only super realism (super detailed skin and others) completely forgetting about general aesthetics, anatomy, etc.\nWho is this model for?\nThe main feature of this model is that the model can generate not only super realistic photos, but also realistic detailed art and much more, so the model is suitable for a large audience and can solve a wide range of problems. If this model still does not suit you, we recommend using FluentlyXL model.\nOptimal settings for this model\nSampler: DPM++ 3M SDE (Karras), DPM++ SDE (Karras)\nInference Steps: 22-25\nGuidance Scale (CFG): 5-7\nNegative Prompt: not or:\ncartoon, 3D, disfigured, bad, art, deformed, extra limbs, weird, blurry, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn, hands, poorly drawn face, mutation, ugly, bad, anatomy, bad proportions, extra limbs, clone, clone-faced, cross proportions, missing arms, malformed limbs, missing legs, mutated, hands, fused fingers, too many fingers, photo shop, video game, ugly, tiling, cross-eye, mutation of eyes, long neck, bonnet, hat, beanie, cap, B&W\nEnd\nAfter this model, you will not want to use the rest of the realism models, if you like the model, we ask you to leave a good review and a couple of your results in the review, thank you, this will greatly help in promoting this wonderful model ðŸ’–",
    "openvla/openvla-7b": "OpenVLA 7B\nModel Summary\nUses\nGetting Started\nCitation\nOpenVLA 7B\nOpenVLA 7B (openvla-7b) is an open vision-language-action model trained on 970K robot manipulation episodes from the Open X-Embodiment dataset.\nThe model takes language instructions and camera images as input and generates robot actions. It supports controlling multiple robots out-of-the-box, and can be quickly adapted for new robot domains via (parameter-efficient) fine-tuning.\nAll OpenVLA checkpoints, as well as our training codebase are released under an MIT License.\nFor full details, please read our paper and see our project page.\nModel Summary\nDeveloped by: The OpenVLA team consisting of researchers from Stanford, UC Berkeley, Google Deepmind, and the Toyota Research Institute.\nModel type: Vision-language-action (language, image => robot actions)\nLanguage(s) (NLP): en\nLicense: MIT\nFinetuned from: prism-dinosiglip-224px, a VLM trained from:\nVision Backbone: DINOv2 ViT-L/14 and SigLIP ViT-So400M/14\nLanguage Model: Llama-2\nPretraining Dataset: Open X-Embodiment -- specific component datasets can be found here.\nRepository: https://github.com/openvla/openvla\nPaper: OpenVLA: An Open-Source Vision-Language-Action Model\nProject Page & Videos: https://openvla.github.io/\nUses\nOpenVLA models take a language instruction and a camera image of a robot workspace as input, and predict (normalized) robot actions consisting of 7-DoF end-effector deltas\nof the form (x, y, z, roll, pitch, yaw, gripper). To execute on an actual robot platform, actions need to be un-normalized subject to statistics computed on a per-robot,\nper-dataset basis. See our repository for more information.\nOpenVLA models can be used zero-shot to control robots for specific combinations of embodiments and domains seen in the Open-X pretraining mixture (e.g., for\nBridgeV2 environments with a Widow-X robot). They can also be efficiently fine-tuned for new tasks and robot setups\ngiven minimal demonstration data; see here.\nOut-of-Scope: OpenVLA models do not zero-shot generalize to new (unseen) robot embodiments, or setups that are not represented in the pretraining mix; in these cases,\nwe suggest collecting a dataset of demonstrations on the desired setup, and fine-tuning OpenVLA models instead.\nGetting Started\nOpenVLA 7B can be used to control multiple robots for domains represented in the pretraining mixture out-of-the-box. For example,\nhere is an example for loading openvla-7b for zero-shot instruction following in the [BridgeV2 environments] with a Widow-X robot:\n# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nfrom PIL import Image\nimport torch\n# Load Processor & VLA\nprocessor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\nvla = AutoModelForVision2Seq.from_pretrained(\n\"openvla/openvla-7b\",\nattn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\ntrust_remote_code=True\n).to(\"cuda:0\")\n# Grab image input & format prompt\nimage: Image.Image = get_from_camera(...)\nprompt = \"In: What action should the robot take to {<INSTRUCTION>}?\\nOut:\"\n# Predict Action (7-DoF; un-normalize for BridgeV2)\ninputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\naction = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n# Execute...\nrobot.act(action, ...)\nFor more examples, including scripts for fine-tuning OpenVLA models on your own robot demonstration datasets, see our training repository.\nCitation\nBibTeX:\n@article{kim24openvla,\ntitle={OpenVLA: An Open-Source Vision-Language-Action Model},\nauthor={{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},\njournal = {arXiv preprint arXiv:2406.09246},\nyear={2024}\n}",
    "DAMO-NLP-SG/VideoLLaMA2-7B": "ðŸ“° News\nðŸŒŽ Model Zoo\nðŸš€ Main Results\nMulti-Choice Video QA & Video Captioning\nOpen-Ended Video QA\nðŸ¤– Inference with VideoLLaMA2\nCitation\nVideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs\nIf you like our project, please give us a star â­ on Github for the latest update.\nðŸ“° News\n[2024.06.12]  Release model weights and the first version of the technical report of VideoLLaMA 2.\n[2024.06.03]  Release training, evaluation, and serving codes of VideoLLaMA 2.\nðŸŒŽ Model Zoo\nModel Name\nType\nVisual Encoder\nLanguage Decoder\n# Training Frames\nVideoLLaMA2-7B-Base\nBase\nclip-vit-large-patch14-336\nMistral-7B-Instruct-v0.2\n8\nVideoLLaMA2-7B (This checkpoint)\nChat\nclip-vit-large-patch14-336\nMistral-7B-Instruct-v0.2\n8\nVideoLLaMA2-7B-16F-Base\nBase\nclip-vit-large-patch14-336\nMistral-7B-Instruct-v0.2\n16\nVideoLLaMA2-7B-16F\nChat\nclip-vit-large-patch14-336\nMistral-7B-Instruct-v0.2\n16\nVideoLLaMA2-8x7B-Base\nBase\nclip-vit-large-patch14-336\nMixtral-8x7B-Instruct-v0.1\n8\nVideoLLaMA2-8x7B\nChat\nclip-vit-large-patch14-336\nMixtral-8x7B-Instruct-v0.1\n8\nVideoLLaMA2-72B-Base\nBase\nclip-vit-large-patch14-336\nQwen2-72B-Instruct\n8\nVideoLLaMA2-72B\nChat\nclip-vit-large-patch14-336\nQwen2-72B-Instruct\n8\nðŸš€ Main Results\nMulti-Choice Video QA & Video Captioning\nOpen-Ended Video QA\nðŸ¤– Inference with VideoLLaMA2\nimport sys\nsys.path.append('./')\nfrom videollama2 import model_init, mm_infer\nfrom videollama2.utils import disable_torch_init\ndef inference():\ndisable_torch_init()\n# Video Inference\nmodal = 'video'\nmodal_path = 'assets/cat_and_chicken.mp4'\ninstruct = 'What animals are in the video, what are they doing, and how does the video feel?'\n# Image Inference\nmodal = 'image'\nmodal_path = 'assets/sora.png'\ninstruct = 'What is the woman wearing, what is she doing, and how does the image feel?'\nmodel_path = 'DAMO-NLP-SG/VideoLLaMA2-7B'\nmodel, processor, tokenizer = model_init(model_path)\noutput = mm_infer(processor[modal](modal_path), instruct, model=model, tokenizer=tokenizer, do_sample=False, modal=modal)\nprint(output)\nif __name__ == \"__main__\":\ninference()\nCitation\nIf you find VideoLLaMA useful for your research and applications, please cite using this BibTeX:\n@article{damonlpsg2024videollama2,\ntitle={VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs},\nauthor={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and Bing, Lidong},\njournal={arXiv preprint arXiv:2406.07476},\nyear={2024},\nurl = {https://arxiv.org/abs/2406.07476}\n}\n@article{damonlpsg2023videollama,\ntitle = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},\nauthor = {Zhang, Hang and Li, Xin and Bing, Lidong},\njournal = {arXiv preprint arXiv:2306.02858},\nyear = {2023},\nurl = {https://arxiv.org/abs/2306.02858}\n}",
    "zai-org/MSAGPT": "MSAGPT\nOverall Framework\nVisualized Cases\nGet Started:\nOption 1ï¼šDeploy MSAGPT by yourself\nOption 2ï¼šFinetuning MSAGPT\nHardware requirement\nLicense\nMSAGPT\nMSAGPT\nðŸ“– Paper: MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training\nMSAGPT is a powerful protein language model (PLM). MSAGPT has 3 billion parameters with three versions of the model, MSAGPT, MSAGPT-Sft, and MSAGPT-Dpo, supporting zero-shot and few-shot MSA generation.\nMSAGPT achieves state-of-the-art structural prediction performance on natural MSA-scarce scenarios.\nOverall Framework\nVisualized Cases\nVisualization of improved structure prediction compared with nature MSA.\nYellow: Ground truth;\nPurple: Predictions based on MSA generated by MSAGPT;\nCyan: Predictions from MSA generated by natural MSA.\nGet Started:\nOption 1ï¼šDeploy MSAGPT by yourself\nWe support GUI for model inference.\nFirst, we need to install the dependencies.\n# CUDA >= 11.8\npip install -r requirements.txt\nModel List\nYou can choose to manually download the necessary weights. Then UNZIP it and put it into the checkpoints folder.\nModel\nType\nSeq Length\nDownload\nMSAGPT\nBase\n16K\nðŸ¤— Huggingface ðŸ”¨ SwissArmyTransformer\nMSAGPT-SFT\nSft\n16K\nðŸ¤— Huggingface ðŸ”¨ SwissArmyTransformer\nMSAGPT-DPO\nRlhf\n16K\nðŸ¤— Huggingface ðŸ”¨ SwissArmyTransformer\nSituation 1.1 CLI (SAT version)\nRun CLI demo via:\n# Online Chat\nbash scripts/cli_sat.sh --from_pretrained ./checkpoints/MSAGPT-DPO --input-source chat --stream_chat --max-gen-length 1024\nThe program will automatically interact in the command line. You can generate replies entering the protein sequence you need to generate virtual MSAs (or add a few MSAs as a prompt, connected by \"<M>\"), for example: \"PEGKQGDPGIPGEPGPPGPPGPQGARGPPG<M>VTVEFVNSCLIGDMGVDGPPGQQGQPGPPG\", where \"PEGKQGDPGIPGEPGPPGPPGPQGARGPPG\" is the main sequence, and \"VTVEFVNSCLIGDMGVDGPPGQQGQPGPPG\" are MSA prompts, and pressing enter. Enter stop to stop the program. The chat CLI looks like:\nYou can also enable the offline generation by set the --input-source <your input file> and --output-path <your output path>.\nWe set an input file example: msa_input.\n# Offline Generation\nbash scripts/cli_sat.sh --from_pretrained ./checkpoints/MSAGPT-DPO --input-source <your input file> --output-path <your output path> --max-gen-length 1024\nSituation 1.2 CLI (Huggingface version)\n(TODO)\nSituation 1.3 Web Demo\n(TODO)\nOption 2ï¼šFinetuning MSAGPT\n(TODO)\nHardware requirement\nModel Inference:\nFor BF16: 1 * A100(80G)\nFinetuning:\nFor BF16: 4 * A100(80G) [Recommend].\nLicense\nThe code in this repository is open source under the Apache-2.0 license.\nIf you find our work helpful, please consider citing the our paper\n@article{chen2024msagpt,\ntitle={MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training},\nauthor={Chen, Bo and Bei, Zhilei and Cheng, Xingyi and Li, Pan and Tang, Jie and Song, Le},\njournal={arXiv preprint arXiv:2406.05347},\nyear={2024}\n}",
    "gustavecortal/Oneirogen-7B": "Presentation\nCode for generation\nInspiration\nTechnical aspects\nContact\nPresentation\nOneirogen (0.5B, 1.5B and 7B) is a language model for dream generation based on Qwen2. It was trained on DreamBank, a corpus of more than 27,000 dream narratives.\nOneirogen was used to produce The Android and The Machine, an English dataset composed of 10,000 real and 10,000 generated dreams.\nOneirogen can be used to generate novel dream narratives. It can also be used for dream analysis. For example, one could finetuned this model on Hall and Van de Castle annotations to predict character and emotion in dream narratives. I've introduced this task in this paper.\nGeneration examples are available on my website.\nCode for generation\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\nclass CustomStoppingCriteria(StoppingCriteria):\ndef __init__(self, stop_token, tokenizer):\nself.stop_token = stop_token\nself.tokenizer = tokenizer\ndef __call__(self, input_ids, scores, **kwargs):\ndecoded_output = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\nif self.stop_token in decoded_output:\nreturn True\nreturn False\nstop_token = \"END.\" # The model was trained with this special end of text token.\nstopping_criteria = StoppingCriteriaList([CustomStoppingCriteria(stop_token, tokenizer)])\ntokenizer = AutoTokenizer.from_pretrained(\"gustavecortal/oneirogen-7B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gustavecortal/oneirogen-7B\", torch_dtype=torch.float16)\nmodel.to(\"cuda\")\ntext = \"Dream:\" # The model was trained with this prefix\ninputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=256, top_k = 50, top_p = 0.95, do_sample = True, temperature=0.9, num_beams = 1, repetition_penalty= 1.11, stopping_criteria=stopping_criteria)\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])\nInspiration\nAn oneirogen, from the Greek Ã³neiros meaning \"dream\" and gen \"to create\", is a substance or other stimulus which produces or enhances dreamlike states of consciousness.\nThis model resonates with a speech called The Android and The Human given by science-fiction author Philip K. Dick:\nOur environment â€“ and I mean our man-made world of machines, artificial constructs, computers, electronic systems, interlinking homeostatic components â€“ all of this is in fact beginning more and more to possess what the earnest psychologists fear the primitive sees in his environment: animation. In a very real sense our environment is becoming alive, or at least quasi-alive, and in ways specifically and fundamentally analogous to ourselves... Rather than learning about ourselves by studying our constructs, perhaps we should make the attempt to comprehend what our constructs are up to by looking into what we ourselves are up to\nTechnical aspects\nOneirogen is a Qwen2 model finetuned on the DreamBank corpus using LoRA adaptation. A notebook to replicate the training will soon be available.\nThis work was performed using HPC resources (Jean Zay supercomputer) from GENCI-IDRIS (Grant 20XX-AD011014205).\nContact\nMail: gustave.cortal@ens-paris-saclay.fr\nX: @gustavecortal\nWebsite: gustavecortal.com",
    "bartowski/L3-8B-Stheno-v3.2-GGUF": "Llamacpp imatrix Quantizations of L3-8B-Stheno-v3.2\nPrompt format\nDownload a file (not the whole branch) from below:\nDownloading using huggingface-cli\nWhich file should I choose?\nLlamacpp imatrix Quantizations of L3-8B-Stheno-v3.2\nUsing llama.cpp release b3130 for quantization.\nOriginal model: https://huggingface.co/Sao10K/L3-8B-Stheno-v3.2\nAll quants made using imatrix option with dataset from here\nPrompt format\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nDescription\nL3-8B-Stheno-v3.2-Q8_0.gguf\nQ8_0\n8.54GB\nExtremely high quality, generally unneeded but max available quant.\nL3-8B-Stheno-v3.2-Q6_K.gguf\nQ6_K\n6.59GB\nVery high quality, near perfect, recommended.\nL3-8B-Stheno-v3.2-Q5_K_M.gguf\nQ5_K_M\n5.73GB\nHigh quality, recommended.\nL3-8B-Stheno-v3.2-Q5_K_S.gguf\nQ5_K_S\n5.59GB\nHigh quality, recommended.\nL3-8B-Stheno-v3.2-Q4_K_M.gguf\nQ4_K_M\n4.92GB\nGood quality, uses about 4.83 bits per weight, recommended.\nL3-8B-Stheno-v3.2-Q4_K_S.gguf\nQ4_K_S\n4.69GB\nSlightly lower quality with more space savings, recommended.\nL3-8B-Stheno-v3.2-IQ4_XS.gguf\nIQ4_XS\n4.44GB\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nL3-8B-Stheno-v3.2-Q3_K_L.gguf\nQ3_K_L\n4.32GB\nLower quality but usable, good for low RAM availability.\nL3-8B-Stheno-v3.2-Q3_K_M.gguf\nQ3_K_M\n4.01GB\nEven lower quality.\nL3-8B-Stheno-v3.2-IQ3_M.gguf\nIQ3_M\n3.78GB\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nL3-8B-Stheno-v3.2-Q3_K_S.gguf\nQ3_K_S\n3.66GB\nLow quality, not recommended.\nL3-8B-Stheno-v3.2-IQ3_XS.gguf\nIQ3_XS\n3.51GB\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nL3-8B-Stheno-v3.2-IQ3_XXS.gguf\nIQ3_XXS\n3.27GB\nLower quality, new method with decent performance, comparable to Q3 quants.\nL3-8B-Stheno-v3.2-Q2_K.gguf\nQ2_K\n3.17GB\nVery low quality but surprisingly usable.\nL3-8B-Stheno-v3.2-IQ2_M.gguf\nIQ2_M\n2.94GB\nVery low quality, uses SOTA techniques to also be surprisingly usable.\nL3-8B-Stheno-v3.2-IQ2_S.gguf\nIQ2_S\n2.75GB\nVery low quality, uses SOTA techniques to be usable.\nL3-8B-Stheno-v3.2-IQ2_XS.gguf\nIQ2_XS\n2.60GB\nVery low quality, uses SOTA techniques to be usable.\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/L3-8B-Stheno-v3.2-GGUF --include \"L3-8B-Stheno-v3.2-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/L3-8B-Stheno-v3.2-GGUF --include \"L3-8B-Stheno-v3.2-Q8_0.gguf/*\" --local-dir L3-8B-Stheno-v3.2-Q8_0\nYou can either specify a new local-dir (L3-8B-Stheno-v3.2-Q8_0) or download them all in place (./)\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "nvidia/mamba2-hybrid-8b-3t-4k": "An Empirical Study of Mamba-based Language Models\nOverview\nModel Version(s)\nToolkit\nCitations\nAn Empirical Study of Mamba-based Language Models\nDocumentation â€‚ Paper â€‚ Models\nOverview\nWe release the 8B-parameter Mamba-2 and Mamba-2-Hybrid model (made of Mamba-2, attention, and MLP layers) trained for the paper An Empirical Study of Mamba-based Language Models.. These models were trained for 3.5T tokens with a sequence length of 4K. These models can be compared to the released 8B-parameter Transformer trained on the same data with the same hyperparameters. We also release the 32K and 128K long-context extensions of Mamba-2-Hybrid.\nModel Version(s)\nmamba2-hybrid-8b-3t-4k: 8B-parameter base Mamba-2-Hybrid model trained on 3.5T tokens with 4K sequence length.\nToolkit\nMegatron-LM Framework\nCitations\nSee more details in our paper:\nAn Empirical Study of Mamba-based Language Models.\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro. (2024)\nPlease cite the paper as follows if you use the models from this repo:\n@article{waleffe2024anempirical,\ntitle   = {An Empirical Study of Mamba-based Language Models},\nauthor  = {Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro},\nyear    = {2024},\njournal = {arXiv preprint arXiv: 2406.07887}\n}",
    "nvidia/Nemotron-4-340B-Instruct": "Nemotron-4-340B-Instruct\nModel Overview\nLicense:\nIntended use\nRequired Hardware\nModel Architecture:\nPrompt Format\nSingle Turn\nMulti-Turn or Few-shot\nUsage\nEvaluation Results\nMT-Bench (GPT-4-Turbo)\nIFEval\nMMLU\nGSM8K\nHumanEval\nMBPP\nArena Hard\nAlpacaEval 2.0 LC\nTFEval\nAdversarial Testing and Red Teaming Efforts\nLimitations\nEthical Considerations\nNemotron-4-340B-Instruct\nModel Overview\nNemotron-4-340B-Instruct is a large language model (LLM) that can be used as part of a synthetic data generation pipeline to create training data that helps researchers and developers build their own LLMs. It is a fine-tuned version of the Nemotron-4-340B-Base model, optimized for English-based single and multi-turn chat use-cases. It supports a context length of 4,096 tokens.\nTry this model on build.nvidia.com now.\nThe base model was pre-trained on a corpus of 9 trillion tokens consisting of a diverse assortment of English based texts, 50+ natural languages, and 40+ coding languages. Subsequently the Nemotron-4-340B-Instruct model went through additional alignment steps including:\nSupervised Fine-tuning (SFT)\nDirect Preference Optimization (DPO)\nReward-aware Preference Optimization (RPO) (Additional in-house alignment technique)\nThroughout the alignment process, we relied on only approximately 20K human-annotated data while our data generation pipeline synthesized over 98% of the data used for supervised fine-tuning and preference fine-tuning (DPO & RPO). We provide comprehensive details about our synthetic data generation pipeline in the technical report.\nThis results in a model that is aligned for human chat preferences, improvements in mathematical reasoning, coding and instruction-following, and is capable of generating high quality synthetic data for a variety of use cases.\nUnder the NVIDIA Open Model License, NVIDIA confirms:\nModels are commercially usable.\nYou are free to create and distribute Derivative Models.\nNVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.\nLicense:\nNVIDIA Open Model License\nIntended use\nNemotron-4-340B-Instruct is a chat model intended for use for the English language.\nNemotron-4-340B-Instruct is designed for Synthetic Data Generation to enable developers and enterprises for building and customizing their own large language models and LLM applications.\nThe instruct model itself can be further customized using the NeMo Framework suite of customization tools including Parameter-Efficient Fine-Tuning (P-tuning, Adapters, LoRA, and more), and Model Alignment (SFT, SteerLM, RLHF, and more) using NeMo-Aligner. Refer to the documentation for examples.\nModel Developer: NVIDIA\nModel Dates: Nemotron-4-340B-Instruct was trained between December 2023 and May 2024.\nData Freshness: The pretraining data has a cutoff of June 2023.\nRequired Hardware\nBF16 Inference:\n8x H200 (1x H200 node)\n16x H100 (2x H100 nodes)\n16x A100 80GB (2x A100 80GB nodes)\nModel Architecture:\nNemotron-4-340B-Instruct is standard decoder-only Transformer, trained with a sequence length of 4096 tokens, uses Grouped-Query Attention (GQA), and Rotary Position Embeddings (RoPE).\nArchitecture Type: Transformer Decoder (auto-regressive language model)\nNetwork Architecture:\nNemotron-4\nPrompt Format\nNote: For Nemotron-4-340B-Instruct we recommend keeping the system prompt empty.\nSingle Turn\n<extra_id_0>System\n<extra_id_1>User\n{prompt}\n<extra_id_1>Assistant\nMulti-Turn or Few-shot\n<extra_id_0>System\n<extra_id_1>User\n{prompt 1}\n<extra_id_1>Assistant\n{response 1}\n<extra_id_1>User\n{prompt 2}\n<extra_id_1>Assistant\n{response 2}\n...\n<extra_id_1>User\n{prompt N}\n<extra_id_1>Assistant\nAn example of a formattable prompt template is available in the following section.\nUsage\nDeployment and inference with Nemotron-4-340B-Instruct can be done in three steps using NeMo Framework:\nCreate a Python script to interact with the deployed model.\nCreate a Bash script to start the inference server\nSchedule a Slurm job to distribute the model across 2 nodes and associate them with the inference server.\nDefine the Python script call_server.py\nimport json\nimport requests\nheaders = {\"Content-Type\": \"application/json\"}\ndef text_generation(data, ip='localhost', port=None):\nresp = requests.put(f'http://{ip}:{port}/generate', data=json.dumps(data), headers=headers)\nreturn resp.json()\ndef get_generation(prompt, greedy, add_BOS, token_to_gen, min_tokens, temp, top_p, top_k, repetition, batch=False):\ndata = {\n\"sentences\": [prompt] if not batch else prompt,\n\"tokens_to_generate\": int(token_to_gen),\n\"temperature\": temp,\n\"add_BOS\": add_BOS,\n\"top_k\": top_k,\n\"top_p\": top_p,\n\"greedy\": greedy,\n\"all_probs\": False,\n\"repetition_penalty\": repetition,\n\"min_tokens_to_generate\": int(min_tokens),\n\"end_strings\": [\"<|endoftext|>\", \"<extra_id_1>\", \"\\x11\", \"<extra_id_1>User\"],\n}\nsentences = text_generation(data, port=1424)['sentences']\nreturn sentences[0] if not batch else sentences\nPROMPT_TEMPLATE = \"\"\"<extra_id_0>System\n<extra_id_1>User\n{prompt}\n<extra_id_1>Assistant\n\"\"\"\nquestion = \"Write a poem on NVIDIA in the style of Shakespeare\"\nprompt = PROMPT_TEMPLATE.format(prompt=question)\nprint(prompt)\nresponse = get_generation(prompt, greedy=True, add_BOS=False, token_to_gen=1024, min_tokens=1, temp=1.0, top_p=1.0, top_k=0, repetition=1.0, batch=False)\nresponse = response[len(prompt):]\nif response.endswith(\"<extra_id_1>\"):\nresponse = response[:-len(\"<extra_id_1>\")]\nprint(response)\nGiven this Python script, create a Bash script which spins up the inference server within the NeMo container (docker pull nvcr.io/nvidia/nemo:24.05) and calls the Python script call_server.py. The Bash script nemo_inference.sh is as follows,\nNEMO_FILE=$1\nWEB_PORT=1424\ndepends_on () {\nHOST=$1\nPORT=$2\nSTATUS=$(curl -X PUT http://$HOST:$PORT >/dev/null 2>/dev/null; echo $?)\nwhile [ $STATUS -ne 0 ]\ndo\necho \"waiting for server ($HOST:$PORT) to be up\"\nsleep 10\nSTATUS=$(curl -X PUT http://$HOST:$PORT >/dev/null 2>/dev/null; echo $?)\ndone\necho \"server ($HOST:$PORT) is up running\"\n}\n/usr/bin/python3 /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_eval.py \\\ngpt_model_file=$NEMO_FILE \\\npipeline_model_parallel_split_rank=0 \\\nserver=True tensor_model_parallel_size=8 \\\ntrainer.precision=bf16 pipeline_model_parallel_size=2 \\\ntrainer.devices=8 \\\ntrainer.num_nodes=2 \\\nweb_server=False \\\nport=${WEB_PORT} &\nSERVER_PID=$!\nreadonly local_rank=\"${LOCAL_RANK:=${SLURM_LOCALID:=${OMPI_COMM_WORLD_LOCAL_RANK:-}}}\"\nif [ $SLURM_NODEID -eq 0 ] && [ $local_rank -eq 0 ]; then\ndepends_on \"0.0.0.0\" ${WEB_PORT}\necho \"start get json\"\nsleep 5\necho \"SLURM_NODEID: $SLURM_NODEID\"\necho \"local_rank: $local_rank\"\n/usr/bin/python3 /scripts/call_server.py\necho \"clean up dameons: $$\"\nkill -9 $SERVER_PID\npkill python\nfi\nwait\nLaunch nemo_inference.sh with a Slurm script defined like below, which starts a 2-node job for model inference.\n#!/bin/bash\n#SBATCH -A SLURM-ACCOUNT\n#SBATCH -p SLURM-PARITION\n#SBATCH -N 2\n#SBATCH -J generation\n#SBATCH --ntasks-per-node=8\n#SBATCH --gpus-per-node=8\nset -x\nRESULTS=<PATH_TO_YOUR_SCRIPTS_FOLDER>\nOUTFILE=\"${RESULTS}/slurm-%j-%n.out\"\nERRFILE=\"${RESULTS}/error-%j-%n.out\"\nMODEL=<PATH_TO>/Nemotron-4-340B-Instruct\nCONTAINER=\"nvcr.io/nvidia/nemo:24.05\"\nMOUNTS=\"--container-mounts=<PATH_TO_YOUR_SCRIPTS_FOLDER>:/scripts,MODEL:/model\"\nread -r -d '' cmd <<EOF\nbash /scripts/nemo_inference.sh /model\nEOF\nsrun -o $OUTFILE -e $ERRFILE --container-image=\"$CONTAINER\" $MOUNTS bash -c \"${cmd}\"\nEvaluation Results\nMT-Bench (GPT-4-Turbo)\nEvaluated using MT-Bench judging by GPT-4-0125-Preview as described in Appendix H in the HelpSteer2 Dataset Paper\ntotal\nwriting\nroleplay\nextraction\nstem\nhumanities\nreasoning\nmath\ncoding\nturn 1\nturn 2\n8.22\n8.70\n8.70\n9.20\n8.75\n8.95\n6.40\n8.40\n6.70\n8.61\n7.84\nIFEval\nEvaluated using the Instruction Following Eval (IFEval) introduced in Instruction-Following Evaluation for Large Language Models.\nPrompt-Strict Acc\nInstruction-Strict Acc\n79.9\n86.1\nMMLU\nEvaluated using the Multi-task Language Understanding benchmarks as introduced in Measuring Massive Multitask Language Understanding.\nMMLU 0-shot\n78.7\nGSM8K\nEvaluated using the Grade School Math 8K (GSM8K) benchmark as introduced in Training Verifiers to Solve Math Word Problems.\nGSM8K 0-shot\n92.3\nHumanEval\nEvaluated using the HumanEval benchmark as introduced in Evaluating Large Language Models Trained on Code.\nHumanEval 0-shot\n73.2\nMBPP\nEvaluated using the MBPP Dataset as introduced in the Program Synthesis with Large Language Models.\nMBPP 0-shot\n75.4\nArena Hard\nEvaluated using the Arena-Hard Pipeline from the LMSys Org.\nArena Hard\n54.2\nAlpacaEval 2.0 LC\nEvaluated using the AlpacaEval 2.0 LC (Length Controlled) as introduced in the paper: Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators\nAlpacaEval 2.0 LC\n41.5\nTFEval\nEvaluated using the CantTalkAboutThis Dataset as introduced in the CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues.\nDistractor F1\nOn-topic F1\n81.7\n97.7\nAdversarial Testing and Red Teaming Efforts\nThe Nemotron-4 340B-Instruct model underwent safety evaluation including adversarial testing via three distinct methods:\nGarak, is an automated LLM vulnerability scanner that probes for common weaknesses, including prompt injection and data leakage.\nAEGIS, is a content safety evaluation dataset and LLM based content safety classifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM interactions.\nHuman Content Red Teaming leveraging human interaction and evaluation of the models' responses.\nLimitations\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards here.  Please report security vulnerabilities or NVIDIA AI Concerns here.",
    "ERPFTW/odoo1": "No model card",
    "deepseek-ai/DeepSeek-Coder-V2-Base": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\n2. Model Downloads\n3. Chat Website\n4. API Platform\n5. How to run locally\nInference with Huggingface's Transformers\nInference with vLLM (recommended)\n6. License\n7. Contact\nAPI Platform |\nHow to Use |\nLicense |\nPaper LinkðŸ‘ï¸\nDeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\nWe present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.\nIn standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.  The list of supported programming languages can be found here.\n2. Model Downloads\nWe release the DeepSeek-Coder-V2 with 16B and 236B parameters based on the DeepSeekMoE framework, which has actived parameters of only 2.4B and 21B , including base and instruct models, to the public.\nModel\n#Total Params\n#Active Params\nContext Length\nDownload\nDeepSeek-Coder-V2-Lite-Base\n16B\n2.4B\n128k\nðŸ¤— HuggingFace\nDeepSeek-Coder-V2-Lite-Instruct\n16B\n2.4B\n128k\nðŸ¤— HuggingFace\nDeepSeek-Coder-V2-Base\n236B\n21B\n128k\nðŸ¤— HuggingFace\nDeepSeek-Coder-V2-Instruct\n236B\n21B\n128k\nðŸ¤— HuggingFace\n3. Chat Website\nYou can chat with the DeepSeek-Coder-V2 on DeepSeek's official website: coder.deepseek.com\n4. API Platform\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com, and you can also pay-as-you-go at an unbeatable price.\n5. How to run locally\nHere, we provide some examples of how to use DeepSeek-Coder-V2-Lite model. If you want to utilize DeepSeek-Coder-V2 in BF16 format for inference, 80GB*8 GPUs are required.\nInference with Huggingface's Transformers\nYou can directly employ Huggingface's Transformers for model inference.\nCode Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nCode Insertion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"\"\"<ï½œfimâ–beginï½œ>def quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[0]\nleft = []\nright = []\n<ï½œfimâ–holeï½œ>\nif arr[i] < pivot:\nleft.append(arr[i])\nelse:\nright.append(arr[i])\nreturn quick_sort(left) + [pivot] + quick_sort(right)<ï½œfimâ–endï½œ>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\nChat Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <ï½œendâ–ofâ–sentenceï½œ>  token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\nThe complete chat template can be found within tokenizer_config.json located in the huggingface model repository.\nAn example of chat template is as belows:\n<ï½œbeginâ–ofâ–sentenceï½œ>User: {user_message_1}\nAssistant: {assistant_message_1}<ï½œendâ–ofâ–sentenceï½œ>User: {user_message_2}\nAssistant:\nYou can also add an optional system message:\n<ï½œbeginâ–ofâ–sentenceï½œ>{system_message}\nUser: {user_message_1}\nAssistant: {assistant_message_1}<ï½œendâ–ofâ–sentenceï½œ>User: {user_message_2}\nAssistant:\nInference with vLLM (recommended)\nTo utilize vLLM for model inference, please merge this Pull Request into your vLLM codebase: https://github.com/vllm-project/vllm/pull/4650.\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmax_model_len, tp_size = 8192, 1\nmodel_name = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\nmessages_list = [\n[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n[{\"role\": \"user\", \"content\": \"write a quick sort algorithm in python.\"}],\n[{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n6. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-Coder-V2 Base/Instruct models is subject to the Model License. DeepSeek-Coder-V2 series (including Base and Instruct) supports commercial use.\n7. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "deepseek-ai/DeepSeek-Coder-V2-Lite-Base": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\n2. Model Downloads\n3. Chat Website\n4. API Platform\n5. How to run locally\nInference with Huggingface's Transformers\nInference with vLLM (recommended)\n6. License\n7. Contact\nAPI Platform |\nHow to Use |\nLicense |\nPaper LinkðŸ‘ï¸\nDeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\nWe present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.\nIn standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.  The list of supported programming languages can be found here.\n2. Model Downloads\nWe release the DeepSeek-Coder-V2 with 16B and 236B parameters based on the DeepSeekMoE framework, which has actived parameters of only 2.4B and 21B , including base and instruct models, to the public.\nModel\n#Total Params\n#Active Params\nContext Length\nDownload\nDeepSeek-Coder-V2-Lite-Base\n16B\n2.4B\n128k\nðŸ¤— HuggingFace\nDeepSeek-Coder-V2-Lite-Instruct\n16B\n2.4B\n128k\nðŸ¤— HuggingFace\nDeepSeek-Coder-V2-Base\n236B\n21B\n128k\nðŸ¤— HuggingFace\nDeepSeek-Coder-V2-Instruct\n236B\n21B\n128k\nðŸ¤— HuggingFace\n3. Chat Website\nYou can chat with the DeepSeek-Coder-V2 on DeepSeek's official website: coder.deepseek.com\n4. API Platform\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com, and you can also pay-as-you-go at an unbeatable price.\n5. How to run locally\nHere, we provide some examples of how to use DeepSeek-Coder-V2-Lite model. If you want to utilize DeepSeek-Coder-V2 in BF16 format for inference, 80GB*8 GPUs are required.\nInference with Huggingface's Transformers\nYou can directly employ Huggingface's Transformers for model inference.\nCode Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nCode Insertion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"\"\"<ï½œfimâ–beginï½œ>def quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[0]\nleft = []\nright = []\n<ï½œfimâ–holeï½œ>\nif arr[i] < pivot:\nleft.append(arr[i])\nelse:\nright.append(arr[i])\nreturn quick_sort(left) + [pivot] + quick_sort(right)<ï½œfimâ–endï½œ>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\nChat Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <ï½œendâ–ofâ–sentenceï½œ>  token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\nThe complete chat template can be found within tokenizer_config.json located in the huggingface model repository.\nAn example of chat template is as belows:\n<ï½œbeginâ–ofâ–sentenceï½œ>User: {user_message_1}\nAssistant: {assistant_message_1}<ï½œendâ–ofâ–sentenceï½œ>User: {user_message_2}\nAssistant:\nYou can also add an optional system message:\n<ï½œbeginâ–ofâ–sentenceï½œ>{system_message}\nUser: {user_message_1}\nAssistant: {assistant_message_1}<ï½œendâ–ofâ–sentenceï½œ>User: {user_message_2}\nAssistant:\nInference with vLLM (recommended)\nTo utilize vLLM for model inference, please merge this Pull Request into your vLLM codebase: https://github.com/vllm-project/vllm/pull/4650.\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmax_model_len, tp_size = 8192, 1\nmodel_name = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\nmessages_list = [\n[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n[{\"role\": \"user\", \"content\": \"write a quick sort algorithm in python.\"}],\n[{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n6. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-Coder-V2 Base/Instruct models is subject to the Model License. DeepSeek-Coder-V2 series (including Base and Instruct) supports commercial use.\n7. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "madebyollin/taesd3": "ðŸ° Tiny AutoEncoder for Stable Diffusion 3\nUsing in ðŸ§¨ diffusers\nðŸ° Tiny AutoEncoder for Stable Diffusion 3\nTAESD3 is very tiny autoencoder which uses the same \"latent API\" as Stable Diffusion 3's VAE.\nTAESD3 is useful for real-time previewing of the SD3 generation process.\nThis repo contains .safetensors versions of the TAESD3 weights.\nUsing in ðŸ§¨ diffusers\nimport torch\nfrom diffusers import StableDiffusion3Pipeline, AutoencoderTiny\npipe = StableDiffusion3Pipeline.from_pretrained(\n\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd3\", torch_dtype=torch.float16)\npipe.vae.config.shift_factor = 0.0\npipe = pipe.to(\"cuda\")\nprompt = \"slice of delicious New York-style berry cheesecake\"\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage.save(\"cheesecake.png\")",
    "karpathy/774M_export": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a ðŸ¤— transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "Alibaba-NLP/gte-Qwen2-7B-instruct": "gte-Qwen2-7B-instruct\nModel Information\nRequirements\nUsage\nSentence Transformers\nTransformers\nInfinity_emb\nEvaluation\nMTEB & C-MTEB\nGTE Models\nCloud API Services\nCommunity support\nFine-tuning\nCitation\ngte-Qwen2-7B-instruct\ngte-Qwen2-7B-instruct is the latest model in the gte (General Text Embedding) model family that ranks No.1 in both English and Chinese evaluations on the Massive Text Embedding Benchmark MTEB benchmark (as of June 16, 2024).\nRecently, the Qwen team released the Qwen2 series models, and we have trained the gte-Qwen2-7B-instruct model based on the Qwen2-7B LLM model. Compared to the gte-Qwen1.5-7B-instruct model, the gte-Qwen2-7B-instruct model uses the same training data and training strategies during the finetuning stage, with the only difference being the upgraded base model to Qwen2-7B. Considering the improvements in the Qwen2 series models compared to the Qwen1.5 series, we can also expect consistent performance enhancements in the embedding models.\nThe model incorporates several key advancements:\nIntegration of bidirectional attention mechanisms, enriching its contextual understanding.\nInstruction tuning, applied solely on the query side for streamlined efficiency\nComprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. This training leverages both weakly supervised and supervised data, ensuring the model's applicability across numerous languages and a wide array of downstream tasks.\nModel Information\nModel Size: 7B\nEmbedding Dimension: 3584\nMax Input Tokens: 32k\nRequirements\ntransformers>=4.39.2\nflash_attn>=2.5.6\nUsage\nSentence Transformers\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)\n# In case you want to reduce the maximum length:\nmodel.max_seq_length = 8192\nqueries = [\n\"how much protein should a female eat\",\n\"summit define\",\n]\ndocuments = [\n\"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n]\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\nscores = (query_embeddings @ document_embeddings.T) * 100\nprint(scores.tolist())\nObserve the config_sentence_transformers.json to see all pre-built prompt names. Otherwise, you can use model.encode(queries, prompt=\"Instruct: ...\\nQuery: \" to use a custom prompt of your choice.\nTransformers\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery: {query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'how much protein should a female eat'),\nget_detailed_instruct(task, 'summit define')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\ninput_texts = queries + documents\ntokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-7B-instruct', trust_remote_code=True)\nmodel = AutoModel.from_pretrained('Alibaba-NLP/gte-Qwen2-7B-instruct', trust_remote_code=True)\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\nInfinity_emb\nUsage via infinity, a MIT Licensed inference server.\n# requires ~16-32GB VRAM NVIDIA Compute Capability >= 8.0\ndocker run \\\n-v $PWD/data:/app/.cache --gpus \"0\" -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68-trt-onnx \\\nv2 --model-id Alibaba-NLP/gte-Qwen2-7B-instruct --revision \"refs/pr/38\" --dtype bfloat16 --batch-size 8 --device cuda --engine torch --port 7997 --no-bettertransformer\nEvaluation\nMTEB & C-MTEB\nYou can use the scripts/eval_mteb.py to reproduce the following result of gte-Qwen2-7B-instruct on MTEB(English)/C-MTEB(Chinese):\nModel Name\nMTEB(56)\nC-MTEB(35)\nMTEB-fr(26)\nMTEB-pl(26)\nbge-base-en-1.5\n64.23\n-\n-\n-\nbge-large-en-1.5\n63.55\n-\n-\n-\ngte-large-en-v1.5\n65.39\n-\n-\n-\ngte-base-en-v1.5\n64.11\n-\n-\n-\nmxbai-embed-large-v1\n64.68\n-\n-\n-\nacge_text_embedding\n-\n69.07\n-\n-\nstella-mrl-large-zh-v3.5-1792d\n-\n68.55\n-\n-\ngte-large-zh\n-\n66.72\n-\n-\nmultilingual-e5-base\n59.45\n56.21\n-\n-\nmultilingual-e5-large\n61.50\n58.81\n-\n-\ne5-mistral-7b-instruct\n66.63\n60.81\n-\n-\ngte-Qwen1.5-7B-instruct\n67.34\n69.52\n-\n-\nNV-Embed-v1\n69.32\n-\n-\n-\ngte-Qwen2-7B-instruct\n70.24\n72.05\n68.25\n67.86\ngte-Qwen2-1.5B-instruc(https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct)\n67.16\n67.65\n66.60\n64.04\nGTE Models\nThe gte series models have consistently released two types of models: encoder-only models (based on the BERT architecture) and decode-only models (based on the LLM architecture).\nModels\nLanguage\nMax Sequence Length\nDimension\nModel Size (Memory Usage, fp32)\nGTE-large-zh\nChinese\n512\n1024\n1.25GB\nGTE-base-zh\nChinese\n512\n512\n0.41GB\nGTE-small-zh\nChinese\n512\n512\n0.12GB\nGTE-large\nEnglish\n512\n1024\n1.25GB\nGTE-base\nEnglish\n512\n512\n0.21GB\nGTE-small\nEnglish\n512\n384\n0.10GB\nGTE-large-en-v1.5\nEnglish\n8192\n1024\n1.74GB\nGTE-base-en-v1.5\nEnglish\n8192\n768\n0.51GB\nGTE-Qwen1.5-7B-instruct\nMultilingual\n32000\n4096\n26.45GB\nGTE-Qwen2-7B-instruct\nMultilingual\n32000\n3584\n26.45GB\nGTE-Qwen2-1.5B-instruct\nMultilingual\n32000\n1536\n6.62GB\nCloud API Services\nIn addition to the open-source GTE series models, GTE series models are also available as commercial API services on Alibaba Cloud.\nEmbedding Models: Three versions of the text embedding models are available: text-embedding-v1/v2/v3, with v3 being the latest API service.\nReRank Models: The gte-rerank model service is available.\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\nCommunity support\nFine-tuning\nGTE models can be fine-tuned with a third party framework SWIFT.\npip install ms-swift -U\n# check: https://swift.readthedocs.io/en/latest/BestPractices/Embedding.html\nnproc_per_node=8\nNPROC_PER_NODE=$nproc_per_node \\\nUSE_HF=1 \\\nswift sft \\\n--model Alibaba-NLP/gte-Qwen2-7B-instruct \\\n--train_type lora \\\n--dataset 'sentence-transformers/stsb' \\\n--torch_dtype bfloat16 \\\n--num_train_epochs 10 \\\n--per_device_train_batch_size 2 \\\n--per_device_eval_batch_size 1 \\\n--gradient_accumulation_steps $(expr 64 / $nproc_per_node) \\\n--eval_steps 100 \\\n--save_steps 100 \\\n--eval_strategy steps \\\n--use_chat_template false \\\n--save_total_limit 5 \\\n--logging_steps 5 \\\n--output_dir output \\\n--warmup_ratio 0.05 \\\n--learning_rate 5e-6 \\\n--deepspeed zero3 \\\n--dataloader_num_workers 4 \\\n--task_type embedding \\\n--loss_type cosine_similarity \\\n--dataloader_drop_last true\nCitation\nIf you find our paper or models helpful, please consider cite:\n@article{li2023towards,\ntitle={Towards general text embeddings with multi-stage contrastive learning},\nauthor={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\njournal={arXiv preprint arXiv:2308.03281},\nyear={2023}\n}"
}