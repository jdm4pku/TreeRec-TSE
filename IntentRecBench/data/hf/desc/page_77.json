{
    "microsoft/rad-dino": "Model card for RAD-DINO\nModel description\nUses\nBiases, risks, and limitations\nInstallation\nUsage\nEncode an image\nWeights for fine-tuning\nConfigs and augmentation\nTraining details\nTraining data\nTraining procedure\nEvaluation\nEnvironmental impact\nCompute infrastructure\nCitation\nModel card contact\nModel card for RAD-DINO\nRAD-DINO is a vision transformer model trained to encode chest X-rays using the self-supervised learning method DINOv2.\nModel description\nRAD-DINO is described in detail in Exploring Scalable Medical Image Encoders Beyond Text Supervision (F. P√©rez-Garc√≠a, H. Sharma, S. Bond-Taylor, et al., 2024).\nDeveloped by: Microsoft Health Futures\nModel type: Vision transformer\nLicense: MIT\nFinetuned from model: dinov2-base\nUses\nRAD-DINO is shared for research purposes only.\nIt is not meant to be used for clinical practice.\nThe model is a vision backbone that can be plugged to other models for downstream tasks.\nSome potential uses are:\nImage classification, with a classifier trained on top of the CLS token\nImage segmentation, with a decoder trained using the patch tokens\nClustering, using the image embeddings directly\nImage retrieval, using nearest neighbors of the CLS token\nReport generation, with a language model to decode text\nFine-tuning RAD-DINO is typically not necessary to obtain good performance in downstream tasks.\nBiases, risks, and limitations\nRAD-DINO was trained with data from three countries, therefore it might be biased towards population in the training data.\nUnderlying biases of the training datasets may not be well characterized.\nInstallation\npip install rad-dino\nUsage\nEncode an image\n>>> from rad_dino import RadDino\n>>> from rad_dino.utils import download_sample_image\n>>> encoder = RadDino()\n>>> image = download_sample_image()\n>>> image\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2765x2505 at 0x7CCD5C014050>\n>>> cls_embeddings, patch_embeddings = encoder.extract_features(image)\n>>> cls_embeddings.shape, patch_embeddings.shape\n(torch.Size([1, 768]), torch.Size([1, 768, 37, 37]))\nWeights for fine-tuning\nWe have released a checkpoint compatible with the original DINOv2 code to help researchers fine-tune our model.\nWe can use the hub model and load the RAD-DINO weights.\nLet's clone the DINOv2 repository so we can import the code for the head.\ngit clone https://github.com/facebookresearch/dinov2.git\n>>> import torch\n>>> from rad_dino.utils import safetensors_to_state_dict\n>>> rad_dino_gh = torch.hub.load(\"./dinov2\", \"dinov2_vitb14\")\n>>> backbone_state_dict = safetensors_to_state_dict(\"backbone_compatible.safetensors\")\n>>> rad_dino_gh.load_state_dict(backbone_state_dict, strict=True)\n<All keys matched successfully>\nThe weights of the head are also released:\n>>> from dinov2.layers import DINOHead\n>>> rad_dino_head_gh = DINOHead(\n...    in_dim=768,\n...    out_dim=65536,\n...    hidden_dim=2048,\n...    bottleneck_dim=256,\n...    nlayers=3,\n... )\n>>> head_state_dict = safetensors_to_state_dict(\"dino_head.safetensors\")\n>>> rad_dino_head_gh.load_state_dict(head_state_dict, strict=True)\n<All keys matched successfully>\nConfigs and augmentation\nThe configuration files ssl_default_config.yaml and vitb14_cxr.yaml, and the augmentations module are also available in the repository to help researchers reproduce the training procedure with our hyperparameters.\nTraining details\nTraining data\nWe used images from five public, deidentified chest X-ray datasets to train this checkpoint of RAD-DINO.\nDataset\nNum. images\nMIMIC-CXR\n368 960\nCheXpert\n223 648\nNIH-CXR\n112 120\nPadChest\n136 787\nBRAX\n41 260\nTOTAL\n882 775\nImages in the validation and test sets used to train MAIRA were excluded from the training set of RAD-DINO.\nThe list of image files used for training is available at ./training_images.csv.\nNote this checkpoint is different from the one in the paper, where some private data was used (and fewer GPUs).\nThe checkpoint shared here is trained for 35 000 iterations (the total number of iterations in the run was 100 000, but we selected this checkpoint using linear probing on the validation sets of the evaluation datasets described in the paper).\nWe used 16 nodes with 4 A100 GPUs each, and a batch size of 40 images per GPU.\nTraining procedure\nWe refer to the manuscript for a detailed description of the training procedure.\nPreprocessing\nAll DICOM files were resized using B-spline interpolation so that their shorter size was 518, min-max scaled to [0, 255], and stored as PNG files.\nTraining hyperparameters\nTraining regime: fp16 using PyTorch-FSDP mixed-precision.\nEvaluation\nOur evaluation is best described in the manuscript.\nEnvironmental impact\nHardware type: NVIDIA A100 GPUs\nHours used: 40 hours/GPU √ó 16 nodes √ó 4 GPUs/node = 2560 GPU-hours\nCloud provider: Azure\nCompute region: West US 2\nCarbon emitted: 222 kg CO‚ÇÇ eq.\nCompute infrastructure\nRAD-DINO was trained on Azure Machine Learning.\nHardware\nWe used 16 Standard_NC96ads_A100_v4 nodes with four NVIDIA A100 (80 GB) GPUs each.\nSoftware\nWe leveraged the code in DINOv2 for training.\nWe used SimpleITK and Pydicom for processing of DICOM files.\nCitation\nBibTeX:\n@article{perez-garcia_exploring_2025,\ntitle = {Exploring scalable medical image encoders beyond text supervision},\nissn = {2522-5839},\nurl = {https://doi.org/10.1038/s42256-024-00965-w},\ndoi = {10.1038/s42256-024-00965-w},\njournal = {Nature Machine Intelligence},\nauthor = {P{\\'e}rez-Garc{\\'i}a, Fernando and Sharma, Harshita and Bond-Taylor, Sam and Bouzid, Kenza and Salvatelli, Valentina and Ilse, Maximilian and Bannur, Shruthi and Castro, Daniel C. and Schwaighofer, Anton and Lungren, Matthew P. and Wetscherek, Maria Teodora and Codella, Noel and Hyland, Stephanie L. and Alvarez-Valle, Javier and Oktay, Ozan},\nmonth = jan,\nyear = {2025},\n}\nAPA:\nP√©rez-Garc√≠a, F., Sharma, H., Bond-Taylor, S., Bouzid, K., Salvatelli, V., Ilse, M., Bannur, S., Castro, D. C., Schwaighofer, A., Lungren, M. P., Wetscherek, M. T., Codella, N., Hyland, S. L., Alvarez-Valle, J., & Oktay, O. (2025). Exploring scalable medical image encoders beyond text supervision. In Nature Machine Intelligence. Springer Science and Business Media LLC. https://doi.org/10.1038/s42256-024-00965-w\nModel card contact\nFernando P√©rez-Garc√≠a (fperezgarcia@microsoft.com).",
    "WangGuangyuLab/Loki": "Loki\nUser Manual and Notebooks\nSource Code\nInstallation (It takes about 5 mins to finish the installation on MacBook Pro)\nUsage\nSTbank\nPretrained weights\nlicense: bsd-3-clause\nLoki\nBuilding on OmiCLIP, a visual‚Äìomics foundation model designed to bridge omics data and hematoxylin and eosin (H&E) images, we developed the Loki platform, which has five key functions: tissue alignment using ST or H&E images, cell type decomposition of ST or H&E images using scRNA-seq as a reference, tissue annotation of ST or H&E images based on bulk RNA-seq or marker genes, ST gene expression prediction from H&E images, and histology image‚Äìtranscriptomics retrieval.\nPlease find our preprint here.\nUser Manual and Notebooks\nYou can view the Loki website and notebooks locally by dobule clicking the ./website/index.html file. It should show up in your default browser.\nThis README provides a quick overview of how to set up and use Loki.\nSource Code\nAll source code for Loki is contained in the ./src/loki directory.\nInstallation (It takes about 5 mins to finish the installation on MacBook Pro)\nCreate a Conda environment:\nconda create -n loki_env python=3.9\nconda activate loki_env\nNavigate to the Loki source directory and install Loki:\ncd ./src\npip install .\nUsage\nOnce Loki is installed, you can import it in your Python scripts or notebooks:\nimport loki.preprocess\nimport loki.utils\nimport loki.plot\nimport loki.align\nimport loki.annotate\nimport loki.decompose\nimport loki.retrieve\nimport loki.predex\nSTbank\nThe ST-bank database are avaliable from Google Drive link.\nThe links_to_raw_data.xlsx file includes the source paper names, doi links, and download links of the raw data.\nThe text.csv file includes the gene sentences with paired image patches.\nThe image.tar.gz includes the image patches.\nPretrained weights\nThe pretrained weights are avaliable in Loki/checkpoint.pt\nlicense: bsd-3-clause",
    "ebowwa/bad_llm_dpov03-gguf": "UNCENSORED AND QUICK MULTI_TURN LLAMA 3\nUploaded  model\nUNCENSORED AND QUICK MULTI_TURN LLAMA 3\nUploaded  model\nDeveloped by: ebowwa\nLicense: apache-2.0\nFinetuned from model : unsloth/llama-3-8b-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "mradermacher/dolphin-2.9.1-yi-1.5-34b-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/dphn/dolphin-2.9.1-yi-1.5-34b\nFor a convenient overview and download list, visit our model page for this model.\nstatic quants are available at https://huggingface.co/mradermacher/dolphin-2.9.1-yi-1.5-34b-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n7.6\nfor the desperate\nGGUF\ni1-IQ1_M\n8.3\nmostly desperate\nGGUF\ni1-IQ2_XXS\n9.4\nGGUF\ni1-IQ2_XS\n10.4\nGGUF\ni1-IQ2_S\n11.0\nGGUF\ni1-IQ2_M\n11.9\nGGUF\ni1-Q2_K\n12.9\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XXS\n13.4\nlower quality\nGGUF\ni1-IQ3_XS\n14.3\nGGUF\ni1-Q3_K_S\n15.1\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n15.1\nbeats Q3_K*\nGGUF\ni1-IQ3_M\n15.7\nGGUF\ni1-Q3_K_M\n16.8\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n18.2\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n18.6\nGGUF\ni1-Q4_0\n19.6\nfast, low quality\nGGUF\ni1-Q4_K_S\n19.7\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n20.8\nfast, recommended\nGGUF\ni1-Q5_K_S\n23.8\nGGUF\ni1-Q5_K_M\n24.4\nGGUF\ni1-Q6_K\n28.3\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "microsoft/Phi-3-medium-128k-instruct-onnx-cpu": "Phi-3 Medium-128K-Instruct ONNX CPU models\nONNX Models\nHow to Get Started with the Model\nHardware Supported\nModel Description\nAdditional Details\nPerformance Metrics\nAppendix\nModel Card Contact\nContributors\nPhi-3 Medium-128K-Instruct ONNX CPU models\nThis repository hosts the optimized versions of Phi-3-medium-128k-instruct to accelerate inference with ONNX Runtime for your CPU.\nPhi-3 Medium is a 14B parameter, lightweight, state-of-the-art open model trained with the Phi-3 datasets, which include both synthetic data and the filtered publicly available websites data, with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the medium version in two variants: 4K and 128K, which are the context lengths (in tokens) that they can support.\nThe base model has undergone a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context, and logical reasoning, Phi-3-Medium-128K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\nOptimized variants of the Phi-3 Medium models are published here in ONNX format and run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, and Linux, with the precision best suited to each of these targets.\nONNX Models\nHere are some of the optimized configurations we have added:\nONNX model for INT4 CPU: ONNX model for CPUs using int4 quantization via RTN.\nHow do you know which is the best ONNX model for you:\nAre you on a Windows machine with GPU?\nI don't know ‚Üí Review this guide to see whether you have a GPU in your Windows machine.\nYes ‚Üí Access the Hugging Face DirectML ONNX models and instructions at Phi-3-medium-128k-instruct-onnx-directml.\nNo ‚Üí Do you have a NVIDIA GPU?\nI don't know ‚Üí Review this guide to see whether you have a CUDA-capable GPU.\nYes ‚Üí Access the Hugging Face CUDA ONNX models and instructions at Phi-3-medium-128k-instruct-onnx-cuda for NVIDIA GPUs.\nNo ‚Üí Access the Hugging Face ONNX models for CPU devices and instructions at Phi-3-medium-128k-instruct-onnx-cpu.\nHow to Get Started with the Model\nTo support the Phi-3 models across a range of devices, platforms, and EP backends, we introduce a new API to wrap several aspects of generative AI inferencing. This API makes it easy to drag and drop LLMs straight into your app. To run the early version of these models with ONNX, follow the steps here. You can also test this with a chat app.\nHardware Supported\nThe models are tested on:\nIntel(R) Core(TM) i9-10920X CPU @ 3.50GHz\nMinimum Configuration Required:\nCPU machine with 16GB RAM\nModel Description\nDeveloped by:  Microsoft\nModel type: ONNX\nLanguage(s) (NLP): Python, C, C++\nLicense: MIT\nModel Description: This is a conversion of the Phi-3 Medium-128K-Instruct model for ONNX Runtime inference.\nAdditional Details\nPhi-3 Small, Medium, and Vision Blog and Phi-3 Mini Blog\nPhi-3 Model Blog Link\nPhi-3 Model Card\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPerformance Metrics\nThe model runs at ~20 tokens/sec on a Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz.\nAppendix\nModel Card Contact\nparinitarahi, kvaishnavi, natke\nContributors\nKunal Vaishnavi, Sunghoon Choi, Yufeng Li, Akshay Sonawane, Sheetal Arun Kadam, Rui Ren, Edward Chen, Scott McKay, Emma Ning, Natalie Kershaw, Parinita Rahi",
    "Mitsua/elan-mt-bt-en-ja": "ElanMT\nModel Details\nUsage\nTraining Data\nTraining Procedure\nEvaluation\nDataset\nResult\nDisclaimer\nElanMT\nElanMT-BT-en-ja is a English to Japanese translation model developed by ELAN MITSUA Project / Abstract Engine.\nElanMT-base-en-ja and ElanMT-base-ja-en are trained from scratch, exclusively on openly licensed corpora such as CC0, CC BY and CC BY-SA.\nThis model is a fine-tuned checkpoint of ElanMT-base-en-ja and is trained exclusively on openly licensed data and Wikipedia back translated data using ElanMT-base-ja-en.\nWeb crawled or other machine translated corpora are not used during the entire training procedure for the ElanMT models.\nDespite the relatively low resource training, thanks to back-translation and a newly built CC0 corpus,\nthe model achieved comparable performance to the currently available open translation models.\nModel Details\nThis is a translation model based on Marian MT 6-layer encoder-decoder transformer architecture with sentencepiece tokenizer.\nDeveloped by: ELAN MITSUA Project / Abstract Engine\nModel type: Translation\nSource Language: English\nTarget Language: Japanese\nLicense: CC BY-SA 4.0\nUsage\nInstall the python packages\npip install transformers accelerate sentencepiece\nThis model is verified on transformers==4.40.2\nRun\nfrom transformers import pipeline\ntranslator = pipeline('translation', model='Mitsua/elan-mt-bt-en-ja')\ntranslator('Hello. I am an AI.')\nFor longer multiple sentences, using pySBD is recommended.\npip install transformers accelerate sentencepiece pysbd\nimport pysbd\nseg_en = pysbd.Segmenter(language=\"en\", clean=False)\ntxt = 'Hello. I am an AI. How are you doing?'\nprint(translator(seg_en.segment(txt)))\nThis idea is from FuguMT repo.\nTraining Data\nWe heavily referred FuguMT author's blog post for dataset collection.\nMitsua/wikidata-parallel-descriptions-en-ja (CC0 1.0)\nWe newly built this 1.5M lines wikidata parallel corpus to augment the training data. This greatly improved the vocabulary on a word basis.\nThe Kyoto Free Translation Task (KFTT) (CC BY-SA 3.0)\nGraham Neubig, \"The Kyoto Free Translation Task,\" http://www.phontron.com/kftt, 2011.\nTatoeba (CC BY 2.0 FR / CC0 1.0)\nhttps://tatoeba.org/\nwikipedia-interlanguage-titles (The MIT License / CC BY-SA 4.0)\nWe built parallel titles based on 2024-05-06 wikipedia dump.\nWikiMatrix (CC BY-SA 4.0)\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong and Francisco Guzm√°n, \"WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia\"\nMDN Web Docs (The MIT / CC0 1.0 / CC BY-SA 2.5)\nhttps://github.com/mdn/translated-content\nWikimedia contenttranslation dump (CC BY-SA 4.0)\n2024-5-10 dump is used.\n*Even if the dataset itself is CC-licensed, we did not use it if the corpus contained in the dataset is based on web crawling, is based on unauthorized use of copyrighted works, or is based on the machine translation output of other translation models.\nTraining Procedure\nWe heavily referred \"Beating Edinburgh's WMT2017 system for en-de with Marian's Transformer model\"\nfor training process and hyperparameter tuning.\nTrains a sentencepiece tokenizer 32k vocab on 4M lines openly licensed corpus.\nTrains ja-en back-translation model on 4M lines openly licensed corpus for 6 epochs. = ElanMT-base-ja-en\nTrains en-ja base translation model on 4M lines openly licensed corpus for 6 epochs. = ElanMT-base-en-ja\nTranslates 20M lines ja Wikipedia to en using back-translation model.\nTrains 4 en-ja models, which is finetuned from ElanMT-base-en-ja checkpoint, on 24M lines training data augmented with back-translated data for 6 epochs.\nMerges 4 trained models that produces the best validation score on FLORES+ dev split.\nFinetunes the merged model on 1M lines high quality corpus subset for 5 epochs.\nEvaluation\nDataset\nFLORES+ (CC BY-SA 4.0) devtest split is used for evaluation.\nNTREX (CC BY-SA 4.0)\nResult\nModel\nParams\nFLORES+ BLEU\nFLORES+ chrf\nNTREX BLEU\nNTREX chrf\nElanMT-BT\n61M\n29.96\n38.43\n25.63\n35.41\nElanMT-base w/o back-translation\n61M\n26.55\n35.28\n23.04\n32.94\nElanMT-tiny\n15M\n25.93\n34.69\n22.78\n33.00\nstaka/fugumt-en-ja (*1)\n61M\n30.89\n38.38\n24.74\n34.23\nfacebook/mbart-large-50-many-to-many-mmt\n610M\n26.31\n34.37\n23.35\n32.66\nfacebook/nllb-200-distilled-600M\n615M\n17.09\n27.32\n14.92\n26.26\nfacebook/nllb-200-3.3B\n3B\n20.04\n30.33\n17.07\n28.46\ngoogle/madlad400-3b-mt\n3B\n24.62\n33.89\n23.64\n33.48\ngoogle/madlad400-7b-mt\n7B\n25.57\n34.59\n24.60\n34.43\n*1 tested on transformers==4.29.2 and num_beams=4\n*2 BLEU score is calculated by sacreBLEU with tokenize=ja-mecab\nDisclaimer\nThe translated result may be very incorrect, harmful or biased. The model was developed to investigate achievable performance with only a relatively small, licensed corpus, and is not suitable for use cases requiring high translation accuracy. Under Section 5 of the CC BY-SA 4.0 License, ELAN MITSUA Project / Abstract Engine is not responsible for any direct or indirect loss caused by the use of the model.\nÂÖçË≤¨‰∫ãÈ†ÖÔºöÁøªË®≥ÁµêÊûú„ÅØ‰∏çÊ≠£Á¢∫„Åß„ÄÅÊúâÂÆ≥„Åß„ÅÇ„Å£„Åü„Çä„Éê„Ç§„Ç¢„Çπ„Åå„Åã„Åã„Å£„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÊú¨„É¢„Éá„É´„ÅØÊØîËºÉÁöÑÂ∞èË¶èÊ®°„Åß„É©„Ç§„Çª„É≥„Çπ„Åï„Çå„Åü„Ç≥„Éº„Éë„Çπ„ÅÆ„Åø„ÅßÈÅîÊàêÂèØËÉΩ„Å™ÊÄßËÉΩ„ÇíË™øÊüª„Åô„Çã„Åü„ÇÅ„Å´ÈñãÁô∫„Åï„Çå„Åü„É¢„Éá„É´„Åß„ÅÇ„Çä„ÄÅÁøªË®≥„ÅÆÊ≠£Á¢∫ÊÄß„ÅåÂøÖË¶Å„Å™„É¶„Éº„Çπ„Ç±„Éº„Çπ„Åß„ÅÆ‰ΩøÁî®„Å´„ÅØÈÅ©„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇÁµµËóç„Éü„ÉÑ„Ç¢„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂèä„Å≥Ê†™Âºè‰ºöÁ§æ„Ç¢„Éñ„Çπ„Éà„É©„ÇØ„Éà„Ç®„É≥„Ç∏„É≥„ÅØCC BY-SA 4.0„É©„Ç§„Çª„É≥„ÇπÁ¨¨5Êù°„Å´Âü∫„Å•„Åç„ÄÅÊú¨„É¢„Éá„É´„ÅÆ‰ΩøÁî®„Å´„Çà„Å£„Å¶Áîü„Åò„ÅüÁõ¥Êé•ÁöÑ„Åæ„Åü„ÅØÈñìÊé•ÁöÑ„Å™ÊêçÂ§±„Å´ÂØæ„Åó„Å¶„ÄÅ‰∏ÄÂàá„ÅÆË≤¨‰ªª„ÇíË≤†„ÅÑ„Åæ„Åõ„Çì„ÄÇ",
    "PekingU/rtdetr_r18vd": "Model Card for RT-DETR\nTable of Contents\nModel Details\nModel Sources\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nPreprocessing\nTraining Hyperparameters\nEvaluation\nModel Architecture and Objective\nCitation\nModel Card Authors\nModel Card for RT-DETR\nTable of Contents\nModel Details\nModel Sources\nHow to Get Started with the Model\nTraining Details\nEvaluation\nModel Architecture and Objective\nCitation\nModel Details\nThe YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy.\nHowever, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS.\nRecently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS.\nNevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS.\nIn this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma.\nWe build RT-DETR in two steps, drawing on the advanced DETR:\nfirst we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy.\nSpecifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed.\nThen, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy.\nIn addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining.\nOur RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy.\nWe also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models).\nFurthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS.\nAfter pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: this https URL.\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub.\nDeveloped by: Yian Zhao and Sangbum Choi\nFunded by:  National Key R&D Program of China (No.2022ZD0118201), Natural Science Foundation of China (No.61972217, 32071459, 62176249, 62006133, 62271465),\nand the Shenzhen Medical Research Funds in China (No.\nB2302037).\nShared by: Sangbum Choi\nModel type: RT-DETR\nLicense: Apache-2.0\nModel Sources\nHF Docs: RT-DETR\nRepository: https://github.com/lyuwenyu/RT-DETR\nPaper: https://arxiv.org/abs/2304.08069\nDemo: RT-DETR Tracking\nHow to Get Started with the Model\nUse the code below to get started with the model.\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import RTDetrForObjectDetection, RTDetrImageProcessor\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r18vd\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r18vd\")\ninputs = image_processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\nfor result in results:\nfor score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\nscore, label = score.item(), label_id.item()\nbox = [round(i, 2) for i in box.tolist()]\nprint(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\nThis should output\nsofa: 0.97 [0.14, 0.38, 640.13, 476.21]\ncat: 0.96 [343.38, 24.28, 640.14, 371.5]\ncat: 0.96 [13.23, 54.18, 318.98, 472.22]\nremote: 0.95 [40.11, 73.44, 175.96, 118.48]\nremote: 0.92 [333.73, 76.58, 369.97, 186.99]\nTraining Details\nTraining Data\nThe RTDETR model was trained on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\nTraining Procedure\nWe conduct experiments on COCO and Objects365 datasets, where RT-DETR is trained on COCO train2017 and validated on COCO val2017 dataset.\nWe report the standard COCO metrics, including AP (averaged over uniformly sampled IoU thresholds ranging from 0.50-0.95 with a step size of 0.05),\nAP50, AP75, as well as AP at different scales: APS, APM, APL.\nPreprocessing\nImages are resized to 640x640 pixels and rescaled with image_mean=[0.485, 0.456, 0.406] and image_std=[0.229, 0.224, 0.225].\nTraining Hyperparameters\nTraining regime:\nEvaluation\nModel\n#Epochs\n#Params (M)\nGFLOPs\nFPS_bs=1\nAP (val)\nAP50 (val)\nAP75 (val)\nAP-s (val)\nAP-m (val)\nAP-l (val)\nRT-DETR-R18\n72\n20\n60.7\n217\n46.5\n63.8\n50.4\n28.4\n49.8\n63.0\nRT-DETR-R34\n72\n31\n91.0\n172\n48.5\n66.2\n52.3\n30.2\n51.9\n66.2\nRT-DETR R50\n72\n42\n136\n108\n53.1\n71.3\n57.7\n34.8\n58.0\n70.0\nRT-DETR R101\n72\n76\n259\n74\n54.3\n72.7\n58.6\n36.0\n58.8\n72.1\nRT-DETR-R18 (Objects 365 pretrained)\n60\n20\n61\n217\n49.2\n66.6\n53.5\n33.2\n52.3\n64.8\nRT-DETR-R50 (Objects 365 pretrained)\n24\n42\n136\n108\n55.3\n73.4\n60.1\n37.9\n59.9\n71.8\nRT-DETR-R101 (Objects 365 pretrained)\n24\n76\n259\n74\n56.2\n74.6\n61.3\n38.3\n60.5\n73.5\nModel Architecture and Objective\nOverview of RT-DETR. We feed the features from the last three stages of the backbone into the encoder. The efficient hybrid\nencoder transforms multi-scale features into a sequence of image features through the Attention-based Intra-scale Feature Interaction (AIFI)\nand the CNN-based Cross-scale Feature Fusion (CCFF). Then, the uncertainty-minimal query selection selects a fixed number of encoder\nfeatures to serve as initial object queries for the decoder. Finally, the decoder with auxiliary prediction heads iteratively optimizes object\nqueries to generate categories and boxes.\nCitation\nBibTeX:\n@misc{lv2023detrs,\ntitle={DETRs Beat YOLOs on Real-time Object Detection},\nauthor={Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},\nyear={2023},\neprint={2304.08069},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nModel Card Authors\nSangbum ChoiPavel Iakubovskii",
    "mistralai/Mistral-7B-v0.3": "Model Card for Mistral-7B-v0.3\nInstallation\nDownload\nDemo\nGenerate with transformers\nLimitations\nThe Mistral AI Team\nModel Card for Mistral-7B-v0.3\nThe Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.\nMistral-7B-v0.3 has the following changes compared to Mistral-7B-v0.2\nExtended vocabulary to 32768\nInstallation\nIt is recommended to use mistralai/Mistral-7B-v0.3 with mistral-inference. For HF transformers code snippets, please keep scrolling.\npip install mistral_inference\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\nDemo\nAfter installing mistral_inference, a mistral-demo CLI command should be available in your environment.\nmistral-demo $HOME/mistral_models/7B-v0.3\nShould give something along the following lines:\nThis is a test of the emergency broadcast system. This is only a test.\nIf this were a real emergency, you would be told what to do.\nThis is a test\n=====================\nThis is another test of the new blogging software. I‚Äôm not sure if I‚Äôm going to keep it or not. I‚Äôm not sure if I‚Äôm going to keep\n=====================\nThis is a third test, mistral AI is very good at testing. üôÇ\nThis is a third test, mistral AI is very good at testing. üôÇ\nThis\n=====================\nGenerate with transformers\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mistral-7B-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ninputs = tokenizer(\"Hello my name is\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLimitations\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timoth√©e Lacroix, Th√©ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit": "Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nFinetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\nWe have a Google Colab Tesla T4 notebook for Mistral v3 7b here: https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing\nFor conversational ShareGPT style and using Mistral v3 Instruct: https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "tianweiy/DMD2": "DMD2 Model Card\nContact\nUsage\nLicense\nCitation\nAcknowledgments\nDMD2 Model Card\nImproved Distribution Matching Distillation for Fast Image Synthesis,Tianwei Yin, Micha√´l Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fr√©do Durand, William T. Freeman\nContact\nFeel free to contact us if you have any questions about the paper!\nTianwei Yin tianweiy@mit.edu\nUsage\nWe can use the standard diffuser pipeline:\n4-step UNet generation\nimport torch\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel, LCMScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"tianweiy/DMD2\"\nckpt_name = \"dmd2_sdxl_4step_unet_fp16.bin\"\n# Load model.\nunet = UNet2DConditionModel.from_config(base_model_id, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(torch.load(hf_hub_download(repo_name, ckpt_name), map_location=\"cuda\"))\npipe = DiffusionPipeline.from_pretrained(base_model_id, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\nprompt=\"a photo of a cat\"\n# LCMScheduler's default timesteps are different from the one we used for training\nimage=pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0, timesteps=[999, 749, 499, 249]).images[0]\n4-step LoRA generation\nimport torch\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel, LCMScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"tianweiy/DMD2\"\nckpt_name = \"dmd2_sdxl_4step_lora_fp16.safetensors\"\n# Load model.\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora(lora_scale=1.0)  # we might want to make the scale smaller for community models\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\nprompt=\"a photo of a cat\"\n# LCMScheduler's default timesteps are different from the one we used for training\nimage=pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0, timesteps=[999, 749, 499, 249]).images[0]\n1-step UNet generation\nimport torch\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel, LCMScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"tianweiy/DMD2\"\nckpt_name = \"dmd2_sdxl_1step_unet_fp16.bin\"\n# Load model.\nunet = UNet2DConditionModel.from_config(base_model_id, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(torch.load(hf_hub_download(repo_name, ckpt_name), map_location=\"cuda\"))\npipe = DiffusionPipeline.from_pretrained(base_model_id, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\nprompt=\"a photo of a cat\"\nimage=pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0, timesteps=[399]).images[0]\n4-step T2I Adapter\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, AutoencoderKL, UNet2DConditionModel, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\nfrom controlnet_aux.canny import CannyDetector\nfrom huggingface_hub import hf_hub_download\nimport torch\n# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\nvae=AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"tianweiy/DMD2\"\nckpt_name = \"dmd2_sdxl_4step_unet_fp16.bin\"\n# Load model.\nunet = UNet2DConditionModel.from_config(base_model_id, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(torch.load(hf_hub_download(repo_name, ckpt_name), map_location=\"cuda\"))\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\nbase_model_id, unet=unet, vae=vae, adapter=adapter, torch_dtype=torch.float16, variant=\"fp16\",\n).to(\"cuda\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.enable_xformers_memory_efficient_attention()\ncanny_detector = CannyDetector()\nurl = \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\nimage = load_image(url)\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = canny_detector(image, detect_resolution=384, image_resolution=1024)#.resize((1024, 1024))\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\ngen_images = pipe(\nprompt=prompt,\nimage=image,\nnum_inference_steps=4,\nguidance_scale=0,\nadapter_conditioning_scale=0.8,\nadapter_conditioning_factor=0.5,\ntimesteps=[999, 749, 499, 249]\n).images[0]\ngen_images.save('out_canny.png')\nFor more information, please refer to the code repository\nLicense\nImproved Distribution Matching Distillation is released under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nCitation\nIf you find DMD2 useful or relevant to your research, please kindly cite our papers:\n@article{yin2024improved,\ntitle={Improved Distribution Matching Distillation for Fast Image Synthesis},\nauthor={Yin, Tianwei and Gharbi, Micha{\\\"e}l and Park, Taesung and Zhang, Richard and Shechtman, Eli and Durand, Fredo and Freeman, William T},\njournal={arXiv:2405.14867},\nyear={2024}\n}\n@inproceedings{yin2024onestep,\ntitle={One-step Diffusion with Distribution Matching Distillation},\nauthor={Yin, Tianwei and Gharbi, Micha{\\\"e}l and Zhang, Richard and Shechtman, Eli and Durand, Fr{\\'e}do and Freeman, William T and Park, Taesung},\nbooktitle={CVPR},\nyear={2024}\n}\nAcknowledgments\nThis work was done while Tianwei Yin was a full-time student at MIT. It was developed based on our reimplementation of the original DMD paper. This work was supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/), by NSF Grant 2105819, by NSF CISE award 1955864, and by funding from Google, GIST, Amazon, and Quanta Computer.",
    "waleko/TikZ-llava-1.5-7b": "Model Card for Model ID\nHow to Get Started with the Model\nTraining Details\nTraining Data\nModel Card for Model ID\nFine-tuned multimodal LLaVA model for TikZ diagram generation using hand-drawn sketches.\nHow to Get Started with the Model\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\npipe = pipeline(\"image-to-text\", model=\"waleko/TikZ-llava-1.5-7b\")\nurl = \"https://waleko.github.io/data/image.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"Assistant helps to write down the TikZ code for the user's image. USER: <image>\\nWrite down the TikZ code to draw the diagram shown in the image. ASSISTANT: \"\nprint(pipe(image, prompt=prompt)[0]['generated_text'])\nTraining Details\nTraining Data\nTrained on synthetic TikZ-short-code dataset.",
    "merve/paligemma_vqav2": "paligemma_vqav2\nHow to Use\nTraining hyperparameters\nTraining results\nFramework versions\npaligemma_vqav2\nThis model is a fine-tuned version of google/paligemma-3b-pt-224 on a small chunk of vq_av2 dataset.\nFine-tuning code is here.\nHow to Use\nBelow is the code to use this model. Also see inference notebook.\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nmodel_id = \"merve/paligemma_vqav2\"\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\nprompt = \"What is behind the cat?\"\nimage_file = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cat.png?download=true\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(prompt, raw_image.convert(\"RGB\"), return_tensors=\"pt\")\noutput = model.generate(**inputs, max_new_tokens=20)\nprint(processor.decode(output[0], skip_special_tokens=True)[len(prompt):])\n# gramophone\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 4\neval_batch_size: 8\nseed: 42\ngradient_accumulation_steps: 4\ntotal_train_batch_size: 16\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 2\nnum_epochs: 2\nTraining results\nFramework versions\nTransformers 4.42.0.dev0\nPytorch 2.3.0+cu121\nDatasets 2.19.1\nTokenizers 0.19.1",
    "internlm/internlm2-math-plus-1_8b": "InternLM-Math-Plus\nNews\nPerformance\nFormal Math Reasoning\nInformal Math Reasoning\nCitation and Tech Report\nInternLM-Math-Plus\nInternLM-Math\nPlus\nState-of-the-art bilingual open-sourced Math reasoning LLMs.\nA solver, prover, verifier, augmentor.\nüíª Github ü§ó Demo\nNews\n[2024.05.24] We release updated version InternLM2-Math-Plus with 4 sizes and state-of-the-art performances including 1.8B, 7B, 20B, and 8x22B. We improve informal math reasoning performance (chain-of-thought and code-intepreter) and formal math reasoning performance (LEAN 4 translation and LEAN 4 theorem proving) significantly.\n[2024.02.10] We add tech reports and citation reference.\n[2024.01.31] We add MiniF2F results with evaluation codes!\n[2024.01.29] We add checkpoints from ModelScope. Update results about majority voting and Code Intepreter. Tech report is on the way!\n[2024.01.26] We add checkpoints from OpenXLab, which ease Chinese users to download!\nPerformance\nFormal Math Reasoning\nWe evaluate the performance of InternLM2-Math-Plus on formal math reasoning benchmark MiniF2F-test. The evaluation setting is same as Llemma with LEAN 4.\nModels\nMiniF2F-test\nReProver\n26.5\nLLMStep\n27.9\nGPT-F\n36.6\nHTPS\n41.0\nLlemma-7B\n26.2\nLlemma-34B\n25.8\nInternLM2-Math-7B-Base\n30.3\nInternLM2-Math-20B-Base\n29.5\nInternLM2-Math-Plus-1.8B\n38.9\nInternLM2-Math-Plus-7B\n43.4\nInternLM2-Math-Plus-20B\n42.6\nInternLM2-Math-Plus-Mixtral8x22B\n37.3\nInformal Math Reasoning\nWe evaluate the performance of InternLM2-Math-Plus on informal math reasoning benchmark MATH and GSM8K. InternLM2-Math-Plus-1.8B outperforms MiniCPM-2B in the smallest size setting. InternLM2-Math-Plus-7B outperforms Deepseek-Math-7B-RL which is the state-of-the-art math reasoning open source model. InternLM2-Math-Plus-Mixtral8x22B achieves 68.5 on MATH (with Python) and 91.8 on GSM8K.\nModel\nMATH\nMATH-Python\nGSM8K\nMiniCPM-2B\n10.2\n-\n53.8\nInternLM2-Math-Plus-1.8B\n37.0\n41.5\n58.8\nInternLM2-Math-7B\n34.6\n50.9\n78.1\nDeepseek-Math-7B-RL\n51.7\n58.8\n88.2\nInternLM2-Math-Plus-7B\n53.0\n59.7\n85.8\nInternLM2-Math-20B\n37.7\n54.3\n82.6\nInternLM2-Math-Plus-20B\n53.8\n61.8\n87.7\nMixtral8x22B-Instruct-v0.1\n41.8\n-\n78.6\nEurux-8x22B-NCA\n49.0\n-\n-\nInternLM2-Math-Plus-Mixtral8x22B\n58.1\n68.5\n91.8\nWe also evaluate models on MathBench-A. InternLM2-Math-Plus-Mixtral8x22B has comparable performance compared to Claude 3 Opus.\nModel\nArithmetic\nPrimary\nMiddle\nHigh\nCollege\nAverage\nGPT-4o-0513\n77.7\n87.7\n76.3\n59.0\n54.0\n70.9\nClaude 3 Opus\n85.7\n85.0\n58.0\n42.7\n43.7\n63.0\nQwen-Max-0428\n72.3\n86.3\n65.0\n45.0\n27.3\n59.2\nQwen-1.5-110B\n70.3\n82.3\n64.0\n47.3\n28.0\n58.4\nDeepseek-V2\n82.7\n89.3\n59.0\n39.3\n29.3\n59.9\nLlama-3-70B-Instruct\n70.3\n86.0\n53.0\n38.7\n34.7\n56.5\nInternLM2-Math-Plus-Mixtral8x22B\n77.5\n82.0\n63.6\n50.3\n36.8\n62.0\nInternLM2-Math-20B\n58.7\n70.0\n43.7\n24.7\n12.7\n42.0\nInternLM2-Math-Plus-20B\n65.8\n79.7\n59.5\n47.6\n24.8\n55.5\nLlama3-8B-Instruct\n54.7\n71.0\n25.0\n19.0\n14.0\n36.7\nInternLM2-Math-7B\n53.7\n67.0\n41.3\n18.3\n8.0\n37.7\nDeepseek-Math-7B-RL\n68.0\n83.3\n44.3\n33.0\n23.0\n50.3\nInternLM2-Math-Plus-7B\n61.4\n78.3\n52.5\n40.5\n21.7\n50.9\nMiniCPM-2B\n49.3\n51.7\n18.0\n8.7\n3.7\n26.3\nInternLM2-Math-Plus-1.8B\n43.0\n43.3\n25.4\n18.9\n4.7\n27.1\nCitation and Tech Report\n@misc{ying2024internlmmath,\ntitle={InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning},\nauthor={Huaiyuan Ying and Shuo Zhang and Linyang Li and Zhejian Zhou and Yunfan Shao and Zhaoye Fei and Yichuan Ma and Jiawei Hong and Kuikun Liu and Ziyi Wang and Yudong Wang and Zijian Wu and Shuaibin Li and Fengzhe Zhou and Hongwei Liu and Songyang Zhang and Wenwei Zhang and Hang Yan and Xipeng Qiu and Jiayu Wang and Kai Chen and Dahua Lin},\nyear={2024},\neprint={2402.06332},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "shreyanshu09/block_diagram_global_information": "Block Diagram Global Information Extractor\nModel description\nTraining dataset\nHow to use\nContact\nLicense\nBlock Diagram Global Information Extractor\nIt was introduced in the paper \"Unveiling the Power of Integration: Block Diagram Summarization through Local-Global Fusion\" accepted at ACL 2024. The full code is available in this BlockNet github repository.\nModel description\nThis model is trained using a transformer encoder and decoder architecture, based on the configuration specified in Donut, to extract the overall summary of block diagram images. It supports both English and Korean languages. The straightforward architecture comprises a visual encoder module and a text decoder module, both based on the Transformer architecture.\nTraining dataset\n41,933 samples from the synthetic and real-world block diagrams in English language (BD-EnKo)\n33,101 samples from the synthetic and real-world block diagrams in Korean language (BD-EnKo)\n396 samples from real-world English block diagram dataset (CBD)\n357 samples from handwritten English block diagram dataset (FC_A)\n476 samples from handwritten English block diagram dataset (FC_B)\nHow to use\nHere is how to use this model in PyTorch:\nimport os\nfrom PIL import Image\nimport torch\nfrom donut import DonutModel\n# Load the pre-trained model\nmodel = DonutModel.from_pretrained(\"shreyanshu09/block_diagram_global_information\")\n# Move the model to GPU if available\nif torch.cuda.is_available():\nmodel.half()\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)\n# Function to process a single image\ndef process_image(image_path):\n# Load and process the image\nimage = Image.open(image_path)\ntask_name = os.path.basename('/block_diagram_global_information/dataset/c2t_data/')                  # Create empty folder anywhere\nresult = model.inference(image=image, prompt=f\"<s_{task_name}>\")[\"predictions\"][0]\n# Extract the relevant information from the result\nif 'c2t' in result:\nreturn result['c2t']\nelse:\nreturn result['text_sequence']\n# Example usage\nimage_path = 'image.png'                  # Input image file\nresult = process_image(image_path)\nContact\nIf you have any questions about this work, please contact Shreyanshu Bhushan using the following email addresses: shreyanshubhushan@gmail.com.\nLicense\nThe content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).",
    "2Noise/ChatTTS": "Clone the Repository\nModel Inference\nDisclaimer: For Academic Purposes Only\nWe are also training larger-scale models and need computational power and data support. If you can provide assistance, please contact OPEN-SOURCE@2NOISE.COM. Thank you very much.\nClone the Repository\nFirst, clone the Git repository:\ngit clone https://github.com/2noise/ChatTTS.git\nModel Inference\n# Import necessary libraries and configure settings\nimport torch\nimport torchaudio\ntorch._dynamo.config.cache_size_limit = 64\ntorch._dynamo.config.suppress_errors = True\ntorch.set_float32_matmul_precision('high')\nimport ChatTTS\nfrom IPython.display import Audio\n# Initialize and load the model:\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n# Define the text input for inference (Support Batching)\ntexts = [\n\"So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\",\n]\n# Perform inference and play the generated audio\nwavs = chat.infer(texts)\nAudio(wavs[0], rate=24_000, autoplay=True)\n# Save the generated audio\ntorchaudio.save(\"output.wav\", torch.from_numpy(wavs[0]), 24000)\nFor more usage examples, please refer to the example notebook, which includes parameters for finer control over the generated speech, such as specifying the speaker, adjusting speech speed, and adding laughter.\nDisclaimer: For Academic Purposes Only\nThe information provided in this document is for academic purposes only. It is intended for educational and research use, and should not be used for any commercial or legal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information.",
    "LyliaEngine/Pony_Diffusion_V6_XL": "Pony_Diffusion_V6_XL\nModel description\nSource\nCredit\nTrigger words\nDownload model\nPony_Diffusion_V6_XL\nPrompt\nscore_9, score_8_up, score_7_up, score_6_up, score_5_up, score_4_up, source_furry, beautiful female anthro shark portrait, dramatic lighting, dark background\nModel description\nPony Diffusion V6 is a versatile SDXL finetune capable of producing stunning SFW and NSFW visuals of various anthro, feral, or humanoids species and their interactions based on simple natural language prompts.\nCHECK \"ABOUT THIS VERSION\" ON THE RIGHT IF YOU ARE NOT ON \"V6\" FOR IMPORTANT INFORMATION.\nPlease join our Discord Server to support development of new versions of this model and get access to free SD bot and check out more examples of this model capabilities on our prompt sharing website or follow the author on Twitter.\nImportant information\nMake sure you load this model with clip skip 2 (or -2 in some software), otherwise you will be getting low quality blobs.\nThis model supports a wide array of styles and aesthetics but provides an opinionated default prompt template that allows generation of high quality samples with no negative prompt and otherwise default settings\nscore_9, score_8_up, score_7_up, score_6_up, score_5_up, score_4_up, just describe what you want, tag1, tag2\n(previous Pony Diffusion models used a simpler score_9 quality modifier, the longer version of V6 XL version is a training issue that was too late to correct during training, you can still use score_9 but it has a much weaker effect compared to full string. You can learn more about these tags here).\nThe model is designed to not need negative prompts in most cases and does not need other quality modifiers like \"hd\", \"masterpiece\", etc...\nOther special data selection tags include, 'source_pony', 'source_furry', 'source_cartoon' and 'source_anime' and ratings of 'rating_safe', 'rating_questionable' and 'rating_explicit'.\nThis model is capable of recognizing many popular and obscure characters and series.\nIf you are looking specifically for pony style, I recommend using one of the two following templates `anthro/feral pony, rest of the prompt` or `source_pony, rest of the prompt`.\nThis model is trained on combination of natural language prompts and tags and is capable of understanding both, so describing intended result using normal language works in most cases, although you can add some tags after the main prompt to boost them.\nUsing Euler a with 25 steps and resolution of 1024px is recommended although model generally can do most supported SDXL resolution.\nThis model will sometimes generate pseudo signatures that are hard to remove even with negative prompts, this is unfortunately a training issue that would be corrected in future models. If that's an issue for you I suggest trying V5.5 or inpainting.\nSpecial thanks\nIceman for helping to procure necessary training resources\nHaru for assistance with captioning efforts\nCookie for technical expertise in training\nPSAI Server Subscribers for supporting the project costs\nPSAI Server Moderators for being vigilant and managing the community\nTechnical details\nThe model has been trained on ~2.6M images aesthetically ranked based on authors personal preferences, with roughly 1:1 ratio between anime/cartoon/furry/pony datasets and 1:1 ratio between safe/questionable/explicit ratings. About 50% of all images has been captioned with high quality detailed captions, which results in very strong natural language capabilities.\nAll images has been trained with both captions (when available) and tags, artists' names have been removed and source data has been filtered based on our Opt-in/Opt-out program. Any explicit content involving underage characters has been filtered out.\nLicense\nThis model is licensed under a modified Fair AI Public License 1.0-SD (https://freedevproject.org/faipl-1.0-sd/) license.\nThe following modifications have been added to Fair AI Public License:\nYou are not permitted to run inference of this model on websites or applications allowing any form of monetization (paid inference, faster tiers, etc.). This applies to any derivative models or model merges.\nIf you want to use this model commercially, please reach us at contact@purplesmart.ai.\nExplicit permission for commercial inference has been granted to CivitAi and Hugging Face.\nSource\nhttps://civitai.com/models/257749\nCredit\nhttps://civitai.com/user/PurpleSmartAI\nTrigger words\nYou should use None to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "knifeayumu/StableDiffusionXL_Collection": "No model card",
    "mlabonne/Daredevil-8B-abliterated": "Daredevil-8B-abliterated\nüîé Applications\n‚ö° Quantization\nüèÜ Evaluation\nOpen LLM Leaderboard\nNous\nüå≥ Model family tree\nüíª Usage\nDaredevil-8B-abliterated\nAbliterated version of mlabonne/Daredevil-8B using failspy's notebook.\nIt based on the technique described in the blog post \"Refusal in LLMs is mediated by a single direction\".\nThanks to Andy Arditi, Oscar Balcells Obeso, Aaquib111, Wes Gurnee, Neel Nanda, and failspy.\nüîé Applications\nThis is an uncensored model. You can use it for any application that doesn't require alignment, like role-playing.\nTested on LM Studio using the \"Llama 3\" preset.\n‚ö° Quantization\nGGUF: https://huggingface.co/mlabonne/Daredevil-8B-abliterated-GGUF\nüèÜ Evaluation\nOpen LLM Leaderboard\nDaredevil-8B-abliterated is the second best-performing 8B model on the Open LLM Leaderboard in terms of MMLU score (27 May 24).\nNous\nEvaluation performed using LLM AutoEval. See the entire leaderboard here.\nModel\nAverage\nAGIEval\nGPT4All\nTruthfulQA\nBigbench\nmlabonne/Daredevil-8B üìÑ\n55.87\n44.13\n73.52\n59.05\n46.77\nmlabonne/Daredevil-8B-abliterated üìÑ\n55.06\n43.29\n73.33\n57.47\n46.17\nmlabonne/Llama-3-8B-Instruct-abliterated-dpomix üìÑ\n52.26\n41.6\n69.95\n54.22\n43.26\nmeta-llama/Meta-Llama-3-8B-Instruct üìÑ\n51.34\n41.22\n69.86\n51.65\n42.64\nfailspy/Meta-Llama-3-8B-Instruct-abliterated-v3 üìÑ\n51.21\n40.23\n69.5\n52.44\n42.69\nmlabonne/OrpoLlama-3-8B üìÑ\n48.63\n34.17\n70.59\n52.39\n37.36\nmeta-llama/Meta-Llama-3-8B üìÑ\n45.42\n31.1\n69.95\n43.91\n36.7\nüå≥ Model family tree\nüíª Usage\n!pip install -qU transformers accelerate\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\nmodel = \"mlabonne/Daredevil-8B-abliterated\"\nmessages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\ntokenizer = AutoTokenizer.from_pretrained(model)\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\n)\noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])",
    "StephArn/RoBERTaOPT3Labels": "Model Trained Using AutoTrain\nValidation Metrics\nFine-tuned from\nTrained on\nModel Trained Using AutoTrain\nProblem type: Text Classification\nValidation Metrics\nloss: 0.6056355834007263\nf1_macro: 0.6670922231156776\nf1_micro: 0.7154326494201606\nf1_weighted: 0.7146269676505387\nprecision_macro: 0.6692782206543363\nprecision_micro: 0.7154326494201606\nprecision_weighted: 0.7189916311836655\nrecall_macro: 0.6719451360857057\nrecall_micro: 0.7154326494201606\nrecall_weighted: 0.7154326494201606\naccuracy: 0.7154326494201606\nFine-tuned from\ncardiffnlp/twitter-roberta-base-sentiment-latest\nTrained on\nStephArn/TweetOPT-PES",
    "mlabonne/NeuralDaredevil-8B-abliterated": "NeuralDaredevil-8B-abliterated\nüîé Applications\n‚ö° Quantization\nüèÜ Evaluation\nOpen LLM Leaderboard\nNous\nüå≥ Model family tree\nüíª Usage\nNeuralDaredevil-8B-abliterated\nThis is a DPO fine-tune of mlabonne/Daredevil-8-abliterated, trained on one epoch of mlabonne/orpo-dpo-mix-40k.\nThe DPO fine-tuning successfully recovers the performance loss due to the abliteration process, making it an excellent uncensored model.\nüîé Applications\nNeuralDaredevil-8B-abliterated performs better than the Instruct model on my tests.\nYou can use it for any application that doesn't require alignment, like role-playing. Tested on LM Studio using the \"Llama 3\" and \"Llama 3 v2\" presets.\n‚ö° Quantization\nThanks to QuantFactory, ZeroWw, Zoyd, solidrust, and tarruda for providing these quants.\nGGUF: https://huggingface.co/QuantFactory/NeuralDaredevil-8B-abliterated-GGUF\nGGUF (FP16): https://huggingface.co/ZeroWw/NeuralDaredevil-8B-abliterated-GGUF\nEXL2: https://huggingface.co/Zoyd/mlabonne_NeuralDaredevil-8B-abliterated-4_0bpw_exl2\nAWQ: https://huggingface.co/solidrust/NeuralDaredevil-8B-abliterated-AWQ\nollama:\n16-bit: https://ollama.com/tarruda/neuraldaredevil-8b-abliterated\n8-bit: https://ollama.com/lstep/neuraldaredevil-8b-abliterated\n5-bit: https://ollama.com/closex/neuraldaredevil-8b-abliterated\nüèÜ Evaluation\nOpen LLM Leaderboard\nNeuralDaredevil-8B is the best-performing uncensored 8B model on the Open LLM Leaderboard (MMLU score).\nNous\nEvaluation performed using LLM AutoEval. See the entire leaderboard here.\nModel\nAverage\nAGIEval\nGPT4All\nTruthfulQA\nBigbench\nmlabonne/NeuralDaredevil-8B-abliterated üìÑ\n55.87\n43.73\n73.6\n59.36\n46.8\nmlabonne/Daredevil-8B üìÑ\n55.87\n44.13\n73.52\n59.05\n46.77\nmlabonne/Daredevil-8B-abliterated üìÑ\n55.06\n43.29\n73.33\n57.47\n46.17\nNousResearch/Hermes-2-Theta-Llama-3-8B üìÑ\n54.28\n43.9\n72.62\n56.36\n44.23\nopenchat/openchat-3.6-8b-20240522 üìÑ\n53.49\n44.03\n73.67\n49.78\n46.48\nmeta-llama/Meta-Llama-3-8B-Instruct üìÑ\n51.34\n41.22\n69.86\n51.65\n42.64\nmeta-llama/Meta-Llama-3-8B üìÑ\n45.42\n31.1\n69.95\n43.91\n36.7\nüå≥ Model family tree\nüíª Usage\n!pip install -qU transformers accelerate\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\nmodel = \"mlabonne/Daredevil-8B\"\nmessages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\ntokenizer = AutoTokenizer.from_pretrained(model)\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\n)\noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])",
    "LeeHarrold/detect_overlap_model": "No model card",
    "ShuklaGroupIllinois/LassoESM": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nLassoESM is a language model specifically tailored for lasso peptides, designed to improve the prediction of their properties.\nIt utilizes a domain adaptation approach to further pre-train ESM-2 (650 million parameters) model on lasso peptide datasets using the masked language modeling technique.",
    "mradermacher/Daredevil-8B-abliterated-dpomix-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated\nstatic quants are available at https://huggingface.co/mradermacher/Daredevil-8B-abliterated-dpomix-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n2.1\nfor the desperate\nGGUF\ni1-IQ1_M\n2.3\nmostly desperate\nGGUF\ni1-IQ2_XXS\n2.5\nGGUF\ni1-IQ2_XS\n2.7\nGGUF\ni1-IQ2_S\n2.9\nGGUF\ni1-IQ2_M\n3.0\nGGUF\ni1-Q2_K\n3.3\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XXS\n3.4\nlower quality\nGGUF\ni1-IQ3_XS\n3.6\nGGUF\ni1-Q3_K_S\n3.8\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n3.8\nbeats Q3_K*\nGGUF\ni1-IQ3_M\n3.9\nGGUF\ni1-Q3_K_M\n4.1\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n4.4\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n4.5\nGGUF\ni1-Q4_0\n4.8\nfast, low quality\nGGUF\ni1-Q4_K_S\n4.8\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n5.0\nfast, recommended\nGGUF\ni1-Q5_K_S\n5.7\nGGUF\ni1-Q5_K_M\n5.8\nGGUF\ni1-Q6_K\n6.7\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "stajerc/zephyr-7b-beta-Q4_K_M-GGUF": "miha-kac/zephyr-7b-beta-Q4_K_M-GGUF\nUse with llama.cpp\nmiha-kac/zephyr-7b-beta-Q4_K_M-GGUF\nThis model was converted to GGUF format from HuggingFaceH4/zephyr-7b-beta using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew.\nbrew install ggerganov/ggerganov/llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo miha-kac/zephyr-7b-beta-Q4_K_M-GGUF --model zephyr-7b-beta-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo miha-kac/zephyr-7b-beta-Q4_K_M-GGUF --model zephyr-7b-beta-q4_k_m.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\ngit clone https://github.com/ggerganov/llama.cpp && \\\ncd llama.cpp && \\\nmake && \\\n./main -m zephyr-7b-beta-q4_k_m.gguf -n 128",
    "fhswf/TrOCR_german_handwritten": "Model Card for TrOCR_german_handwritten\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nTraining Data\nEvaluation\nModel Card for TrOCR_german_handwritten\nModel Details\nTrOCR model fine-tuned on the german_handwriting. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\nDeveloped by: [More Information Needed]\nModel type: Transformer OCR\nLanguage(s) (NLP): German\nLicense: afl-3.0\nFinetuned from model [optional]: TrOCR_large_handwritten\nUses\nHere is how to use this model in PyTorch:\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n# load image from the IAM database\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprocessor = TrOCRProcessor.from_pretrained('fhswf/TrOCR_german_handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('fhswf/TrOCR_german_handwritten')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nBias, Risks, and Limitations\nYou can use the raw model for optical character recognition (OCR) on single text-line images of german handwriting.\nTraining Details\nTraining Data\nThis model was finetuned on german_handwriting.\nEvaluation\nLevenshtein: 1.85\nWER (Word Error Rate): 17.5%\nCER (Character Error Rate): 4.1%\nBibTeX:\n@misc{li2021trocr,\ntitle={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},\nauthor={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\nyear={2021},\neprint={2109.10282},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "mistralai/Codestral-22B-v0.1": "Model Card for Codestral-22B-v0.1\nEncode and Decode with mistral_common\nInference with mistral_inference\nInference with hugging face transformers\nInstallation\nDownload\nChat\nFill-in-the-middle (FIM)\nUsage with transformers library\nLimitations\nLicense\nThe Mistral AI Team\nModel Card for Codestral-22B-v0.1\nEncode and Decode with mistral_common\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nmistral_models_path = \"MISTRAL_MODELS_PATH\"\ntokenizer = MistralTokenizer.v3()\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nInference with mistral_inference\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nInference with hugging face transformers\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Codestral-22B-v0.1\")\nmodel.to(\"cuda\")\ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\nPRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mistral_common reference implementation are very welcome!\nCodestral-22B-v0.1 is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash (more details in the Blogpost). The model can be queried:\nAs instruct, for instance to answer any questions about a code snippet (write documentation, explain, factorize) or to generate code following specific indications\nAs Fill in the Middle (FIM), to predict the middle tokens between a prefix and a suffix (very useful for software development add-ons like in VS Code)\nInstallation\nIt is recommended to use mistralai/Codestral-22B-v0.1 with mistral-inference.\npip install mistral_inference\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', 'Codestral-22B-v0.1')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Codestral-22B-v0.1\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\nChat\nAfter installing mistral_inference, a mistral-chat CLI command should be available in your environment.\nmistral-chat $HOME/mistral_models/Codestral-22B-v0.1 --instruct --max_tokens 256\nWill generate an answer to \"Write me a function that computes fibonacci in Rust\" and should give something along the following lines:\nSure, here's a simple implementation of a function that computes the Fibonacci sequence in Rust. This function takes an integer `n` as an argument and returns the `n`th Fibonacci number.\nfn fibonacci(n: u32) -> u32 {\nmatch n {\n0 => 0,\n1 => 1,\n_ => fibonacci(n - 1) + fibonacci(n - 2),\n}\n}\nfn main() {\nlet n = 10;\nprintln!(\"The {}th Fibonacci number is: {}\", n, fibonacci(n));\n}\nThis function uses recursion to calculate the Fibonacci number. However, it's not the most efficient solution because it performs a lot of redundant calculations. A more efficient solution would use a loop to iteratively calculate the Fibonacci numbers.\nFill-in-the-middle (FIM)\nAfter installing mistral_inference and running pip install --upgrade mistral_common to make sure to have mistral_common>=1.2 installed:\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.tokens.instruct.request import FIMRequest\ntokenizer = MistralTokenizer.v3()\nmodel = Transformer.from_folder(\"~/codestral-22B-240529\")\nprefix = \"\"\"def add(\"\"\"\nsuffix = \"\"\"    return sum\"\"\"\nrequest = FIMRequest(prompt=prefix, suffix=suffix)\ntokens = tokenizer.encode_fim(request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nmiddle = result.split(suffix)[0].strip()\nprint(middle)\nShould give something along the following lines:\nnum1, num2):\n# Add two numbers\nsum = num1 + num2\n# return the sum\nUsage with transformers library\nThis model is also compatible with transformers library, first run pip install -U transformers then use the snippet below to quickly get started:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Codestral-22B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem.\nLimitations\nThe Codestral-22B-v0.1 does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nLicense\nCodestral-22B-v0.1 is released under the MNLP-0.1 license.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Jean-Malo Delignon, Jia Li, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickael Seznec, Nicolas Schuhl, Patrick von Platen, Romain Sauvestre, Pierre Stock, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Thibault Schueller, Timoth√©e Lacroix, Th√©ophile Gervet, Thomas Wang, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "QuantFactory/NeuralDaredevil-8B-abliterated-GGUF": "NeuralDaredevil-8B-abliterated-GGUF\nModel Description\nüîé Applications\nüèÜ Evaluation\nOpen LLM Leaderboard\nNous\nüå≥ Model family tree\nNeuralDaredevil-8B-abliterated-GGUF\nThis is quantized version of mlabonne/NeuralDaredevil-8B-abliterated created using llama.cpp\nModel Description\nThis is a DPO fine-tune of mlabonne/Daredevil-8-abliterated, trained on one epoch of mlabonne/orpo-dpo-mix-40k.\nThe DPO fine-tuning successfully recovers the performance loss due to the abliteration process, making it an excellent uncensored model.\nüîé Applications\nNeuralDaredevil-8B-abliterated performs better than the Instruct model on my tests.\nYou can use it for any application that doesn't require alignment, like role-playing. Tested on LM Studio using the \"Llama 3\" preset.\nüèÜ Evaluation\nOpen LLM Leaderboard\nNeuralDaredevil-8B is the best-performing uncensored 8B model on the Open LLM Leaderboard (MMLU score).\nNous\nEvaluation performed using LLM AutoEval. See the entire leaderboard here.\nModel\nAverage\nAGIEval\nGPT4All\nTruthfulQA\nBigbench\nmlabonne/NeuralDaredevil-8B-abliterated üìÑ\n55.87\n43.73\n73.6\n59.36\n46.8\nmlabonne/Daredevil-8B üìÑ\n55.87\n44.13\n73.52\n59.05\n46.77\nmlabonne/Daredevil-8B-abliterated üìÑ\n55.06\n43.29\n73.33\n57.47\n46.17\nNousResearch/Hermes-2-Theta-Llama-3-8B üìÑ\n54.28\n43.9\n72.62\n56.36\n44.23\nopenchat/openchat-3.6-8b-20240522 üìÑ\n53.49\n44.03\n73.67\n49.78\n46.48\nmeta-llama/Meta-Llama-3-8B-Instruct üìÑ\n51.34\n41.22\n69.86\n51.65\n42.64\nmeta-llama/Meta-Llama-3-8B üìÑ\n45.42\n31.1\n69.95\n43.91\n36.7\nüå≥ Model family tree",
    "martintomov/comfy": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nComfy\nGetting Started\nRepository Structure\nContributing\nHappy experimenting! üöÄ\nComfy\nCollection of models, weights, and workflows used in my local ComfyUI environment. It serves as a place to store, manage, and share the various models and associated resources that I have found and utilized.\nGetting Started\nTo get started with this repository, you can clone it to your local machine using the following command:\ngit clone https://huggingface.co/martintomov/comfy\nEnsure you have ComfyUI and properly configured on your system. This repository assumes you are familiar with setting up and running models in your environment.\nRepository Structure\ncomfy/\n‚îú‚îÄ‚îÄ model-name/\n‚îÇ   ‚îú‚îÄ‚îÄ model_1.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ model_2.pth\n‚îÇ   ‚îú‚îÄ‚îÄ model_3.ckpt\n‚îÇ   ‚îú‚îÄ‚îÄ model_4.onnx\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ workflows/\n‚îÇ   ‚îú‚îÄ‚îÄ workflow1.json\n‚îÇ   ‚îú‚îÄ‚îÄ workflow2.json\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ README.md\nContributing\nIf you have models, weights, or workflows that you would like to share, contributions are welcome!\nHappy experimenting! üöÄ",
    "AdapterOcean/expert_beefc778e5304bc1b75dab72e73d1360": "No model card",
    "knowledgator/gliner-multitask-large-v0.5": "Supported tasks:\nInstallation\nConstruct relations extraction pipeline with utca\nBenchmarks:\nJoin Our Discord\nCitation:\nüöÄ Meet the first multi-task prompt-tunable GLiNER model üöÄ\nGLiNER-Multitask is a model designed to extract various pieces of information from plain text based on a user-provided custom prompt. This versatile model leverages a bidirectional transformer encoder, similar to BERT, which ensures both high generalization and compute efficiency despite its compact size.\nThe gliner-multitask-large variant achieves state-of-the-art performance on NER zero-shot benchmarks, demonstrating its robustness and flexibility. It excels not only in named entity recognition but also in handling various other information extraction tasks, making it a powerful tool for diverse natural language processing applications.\nSupported tasks:\nNamed Entity Recognition (NER): Identifies and categorizes entities such as names, organizations, dates, and other specific items in the text.\nRelation Extraction: Detects and classifies relationships between entities within the text.\nSummarization: Extract the most important sentences that summarize the input text, capturing the essential information.\nSentiment Extraction: Identify parts of the text that signalize a positive, negative, or neutral sentiment;\nKey-Phrase Extraction: Identifies and extracts important phrases and keywords from the text.\nQuestion-answering: Finding an answer in the text given a question;\nOpen Information Extraction: Extracts pieces of text given an open prompt from a user, for example, product description extraction;\nInstallation\nTo use this model, you must install the GLiNER Python library:\npip install gliner\nOnce you've downloaded the GLiNER library, you can import the GLiNER class. You can then load this model using GLiNER.from_pretrained.\nHow to use for NER:\nfrom gliner import GLiNER\nmodel = GLiNER.from_pretrained(\"knowledgator/gliner-multitask-large-v0.5\")\ntext = \"\"\"\nMicrosoft was founded by Bill Gates and Paul Allen on April 4, 1975 to develop and sell BASIC interpreters for the Altair 8800. During his career at Microsoft, Gates held the positions of chairman, chief executive officer, president and chief software architect, while also being the largest individual shareholder until May 2014.\n\"\"\"\nlabels = [\"founder\", \"computer\", \"software\", \"position\", \"date\"]\nentities = model.predict_entities(text, labels)\nfor entity in entities:\nprint(entity[\"text\"], \"=>\", entity[\"label\"])\nHow to use for relation extraction:\ntext = \"\"\"\nMicrosoft was founded by Bill Gates and Paul Allen on April 4, 1975 to develop and sell BASIC interpreters for the Altair 8800. During his career at Microsoft, Gates held the positions of chairman, chief executive officer, president and chief software architect, while also being the largest individual shareholder until May 2014.\n\"\"\"\nlabels = [\"Microsoft <> founder\", \"Microsoft <> inception date\", \"Bill Gates <> held position\"]\nentities = model.predict_entities(text, labels)\nfor entity in entities:\nprint(entity[\"label\"], \"=>\", entity[\"text\"])\nConstruct relations extraction pipeline with utca\nFirst of all, we need import neccessary components of the library and initalize predictor - GLiNER model and construct pipeline that combines NER and realtions extraction:\nfrom utca.core import RenameAttribute\nfrom utca.implementation.predictors import (\nGLiNERPredictor,\nGLiNERPredictorConfig\n)\nfrom utca.implementation.tasks import (\nGLiNER,\nGLiNERPreprocessor,\nGLiNERRelationExtraction,\nGLiNERRelationExtractionPreprocessor,\n)\npredictor = GLiNERPredictor( # Predictor manages the model that will be used by tasks\nGLiNERPredictorConfig(\nmodel_name = \"knowledgator/gliner-multitask-large-v0.5\", # Model to use\ndevice = \"cuda:0\", # Device to use\n)\n)\npipe = (\nGLiNER( # GLiNER task produces classified entities that will be at the \"output\" key.\npredictor=predictor,\npreprocess=GLiNERPreprocessor(threshold=0.7) # Entities threshold\n)\n| RenameAttribute(\"output\", \"entities\") # Rename output entities from GLiNER task to use them as inputs in GLiNERRelationExtraction\n| GLiNERRelationExtraction( # GLiNERRelationExtraction is used for relation extraction.\npredictor=predictor,\npreprocess=(\nGLiNERPreprocessor(threshold=0.5) # Relations threshold\n| GLiNERRelationExtractionPreprocessor()\n)\n)\n)\nTo run pipeline we need to specify entity types and relations with their parameters:\nr = pipe.run({\n\"text\": text, # Text to process\n\"labels\": [\"organisation\", \"founder\", \"position\", \"date\"],\n\"relations\": [{ # Relation parameters\n\"relation\": \"founder\", # Relation label. Required parameter.\n\"pairs_filter\": [(\"organisation\", \"founder\")], # Optional parameter. It specifies possible members of relations by their entity labels.\n\"distance_threshold\": 100, # Optional parameter. It specifies the max distance between spans in the text (i.e., the end of the span that is closer to the start of the text and the start of the next one).\n}, {\n\"relation\": \"inception date\",\n\"pairs_filter\": [(\"organisation\", \"date\")],\n}, {\n\"relation\": \"held position\",\n\"pairs_filter\": [(\"founder\", \"position\")],\n}]\n})\nprint(r[\"output\"])\nHow to use for open information extraction:\nprompt = \"\"\"Find all positive aspects about the product:\\n\"\"\"\ntext = \"\"\"\nI recently purchased the Sony WH-1000XM4 Wireless Noise-Canceling Headphones from Amazon and I must say, I'm thoroughly impressed. The package arrived in New York within 2 days, thanks to Amazon Prime's expedited shipping.\nThe headphones themselves are remarkable. The noise-canceling feature works like a charm in the bustling city environment, and the 30-hour battery life means I don't have to charge them every day. Connecting them to my Samsung Galaxy S21 was a breeze, and the sound quality is second to none.\nI also appreciated the customer service from Amazon when I had a question about the warranty. They responded within an hour and provided all the information I needed.\nHowever, the headphones did not come with a hard case, which was listed in the product description. I contacted Amazon, and they offered a 10% discount on my next purchase as an apology.\nOverall, I'd give these headphones a 4.5/5 rating and highly recommend them to anyone looking for top-notch quality in both product and service.\n\"\"\"\ninput_ = prompt+text\nlabels = [\"match\"]\nmatches = model.predict_entities(input_, labels)\nfor match in matches:\nprint(match[\"text\"], \"=>\", match[\"score\"])\nHow to use for question-answering:\nquestion = \"Who was the CEO of Microsoft?\"\ntext = \"\"\"\nMicrosoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. During his career at Microsoft, Gates held the positions of chairman, chief executive officer, president and chief software architect, while also being the largest individual shareholder until May 2014.\n\"\"\"\nlabels = [\"answer\"]\ninput_ = question+text\nanswers = model.predict_entities(input_, labels)\nfor answer in answers:\nprint(answer[\"text\"], \"=>\", answer[\"score\"])\nHow to use for summarization:\nWith threshold parameters, you can control how much information you want to extract.\nprompt = \"Summarize the given text, highlighting the most important information:\\n\"\ntext = \"\"\"\nSeveral studies have reported its pharmacological activities, including anti-inflammatory, antimicrobial, and antitumoral effects.\nThe effect of E-anethole was studied in the osteosarcoma MG-63 cell line, and the antiproliferative activity was evaluated by an MTT assay.\nIt showed a GI50 value of 60.25 ŒºM with apoptosis induction through the mitochondrial-mediated pathway. Additionally, it induced cell cycle arrest at the G0/G1 phase, up-regulated the expression of p53, caspase-3, and caspase-9, and down-regulated Bcl-xL expression.\nMoreover, the antitumoral activity of anethole was assessed against oral tumor Ca9-22 cells, and the cytotoxic effects were evaluated by MTT and LDH assays.\nIt demonstrated a LD50 value of 8 ŒºM, and cellular proliferation was 42.7% and 5.2% at anethole concentrations of 3 ŒºM and 30 ŒºM, respectively.\nIt was reported that it could selectively and in a dose-dependent manner decrease cell proliferation and induce apoptosis, as well as induce autophagy, decrease ROS production, and increase glutathione activity. The cytotoxic effect was mediated through NF-kB, MAP kinases, Wnt, caspase-3 and -9, and PARP1 pathways. Additionally, treatment with anethole inhibited cyclin D1 oncogene expression, increased cyclin-dependent kinase inhibitor p21WAF1, up-regulated p53 expression, and inhibited the EMT markers.\n\"\"\"\nlabels = [\"summary\"]\ninput_ = prompt+text\nthreshold = 0.5\nsummaries = model.predict_entities(input_, labels, threshold=threshold)\nfor summary in summaries:\nprint(summary[\"text\"], \"=>\", summary[\"score\"])\nBenchmarks:\nOur multitask model demonstrates comparable performance on different zero-shot benchmarks to dedicated models to NER task (all labels were lowecased in this testing):\nModel\nDataset\nPrecision\nRecall\nF1 Score\nF1 Score (Decimal)\nnumind/NuNER_Zero-span\nCrossNER_AI\n63.82%\n56.82%\n60.12%\n0.6012\nCrossNER_literature\n73.53%\n58.06%\n64.89%\n0.6489\nCrossNER_music\n72.69%\n67.40%\n69.95%\n0.6995\nCrossNER_politics\n77.28%\n68.69%\n72.73%\n0.7273\nCrossNER_science\n70.08%\n63.12%\n66.42%\n0.6642\nmit-movie\n63.00%\n48.88%\n55.05%\n0.5505\nmit-restaurant\n54.81%\n37.62%\n44.62%\n0.4462\nAverage\n0.6196\nknowledgator/gliner-multitask-v0.5\nCrossNER_AI\n51.00%\n51.11%\n51.05%\n0.5105\nCrossNER_literature\n72.65%\n65.62%\n68.96%\n0.6896\nCrossNER_music\n74.91%\n73.70%\n74.30%\n0.7430\nCrossNER_politics\n78.84%\n77.71%\n78.27%\n0.7827\nCrossNER_science\n69.20%\n65.48%\n67.29%\n0.6729\nmit-movie\n61.29%\n52.59%\n56.60%\n0.5660\nmit-restaurant\n50.65%\n38.13%\n43.51%\n0.4351\nAverage\n0.6276\nurchade/gliner_large-v2.1\nCrossNER_AI\n54.98%\n52.00%\n53.45%\n0.5345\nCrossNER_literature\n59.33%\n56.47%\n57.87%\n0.5787\nCrossNER_music\n67.39%\n66.77%\n67.08%\n0.6708\nCrossNER_politics\n66.07%\n63.76%\n64.90%\n0.6490\nCrossNER_science\n61.45%\n62.56%\n62.00%\n0.6200\nmit-movie\n55.94%\n47.36%\n51.29%\n0.5129\nmit-restaurant\n53.34%\n40.83%\n46.25%\n0.4625\nAverage\n0.5754\nEmergentMethods/gliner_large_news-v2.1\nCrossNER_AI\n59.60%\n54.55%\n56.96%\n0.5696\nCrossNER_literature\n65.41%\n56.16%\n60.44%\n0.6044\nCrossNER_music\n67.47%\n63.08%\n65.20%\n0.6520\nCrossNER_politics\n66.05%\n60.07%\n62.92%\n0.6292\nCrossNER_science\n68.44%\n63.57%\n65.92%\n0.6592\nmit-movie\n65.85%\n49.59%\n56.57%\n0.5657\nmit-restaurant\n54.71%\n35.94%\n43.38%\n0.4338\nAverage\n0.5876\nJoin Our Discord\nConnect with our community on Discord for news, support, and discussion about our models. Join Discord.\nCitation:\n@misc{stepanov2024gliner,\ntitle={GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks},\nauthor={Ihor Stepanov and Mykhailo Shtopko},\nyear={2024},\neprint={2406.12925},\narchivePrefix={arXiv},\nprimaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}\n}"
}