{
    "NandemoGHS/Anime-Llasa-3B-Captions": "Anime-Llasa-3B-Captions\nOverview\nWhat's New: Fine-Tuning with Audio Metadata\nHow to Use: Controlling Speech Generation\n1. Using System Prompt Metadata\n2. Using In-Text Tags (Full-Width Parentheses)\nDemo\nLimitations\nTraining Data\nOld Versions\nLicense\nAnime-Llasa-3B-Captions\nOverview\nThis is Anime-Llasa-3B-Captions, a Text-to-Speech (TTS) model fine-tuned for Japanese, based on NandemoGHS/Anime-Llasa-3B.\nThis version has been further fine-tuned with additional data, incorporating detailed audio metadata generated by Gemini 2.5 Pro.\nWhat's New: Fine-Tuning with Audio Metadata\nThe key improvement in this model is its training methodology. I used Gemini 2.5 Pro to generate detailed metadata (captions, speaker profiles, emotions, etc.) for the audio data. The model was then fine-tuned on this dataset, learning to associate text with these rich descriptive tags.\nThis allows for highly controllable speech synthesis by specifying desired audio characteristics in the prompt.\nHow to Use: Controlling Speech Generation\nYou can control the generated speech in two main ways:\n1. Using System Prompt Metadata\nYou can guide the speech synthesis by providing specific tags in the system prompt. The model expects the following format (note: emotion tags are in English, while others should be in Japanese):\ncaption: (Required) A general description of the audio content.\nemotion: Emotion tag (e.g., angry, sad, happy, serious).\nprofile: Speaker profile (e.g., Ëã•„ÅÑÂ•≥ÊÄßÂ£∞, Â§ß‰∫∫„ÅÆÁî∑ÊÄßÂ£∞).\nmood: Mood (e.g., ÊÅ•„Åö„Åã„Åó„Åï, ÊÇ≤„Åó„Åø).\nspeed: Speaking speed (e.g., „ÇÜ„Å£„Åè„Çä, ÈÄü„ÅÑ).\nprosody: Prosody/Rhythm (e.g., Èúá„ÅàÂ£∞, Âπ≥Âù¶).\npitch_timbre: Pitch/Timbre (e.g., È´ò„ÇÅ, ‰Ωé„ÇÅ, ÊÅØÂ§ö„ÇÅ).\nstyle: Style (e.g., „Éä„É¨„Éº„Ç∑„Éß„É≥È¢®, ‰ºöË©±Ë™ø).\nnotes: Special notes (Ë∑ùÈõ¢ÊÑü„ÄÅ„Éñ„É¨„Çπ„Å™„Å©).\n2. Using In-Text Tags (Full-Width Parentheses)\nAdditionally, you can control the speech style directly within the transcription text by using full-width Japanese parentheses Ôºà„ÄÄÔºâ.\nFor example, adding ÔºàÂõÅ„ÅçÔºâ (whisper) to the text will prompt the model to generate that part of the speech in a whispering voice.\nExample Input Text:\n„Äå„Åì„Çå„ÅØ„ÉÜ„Çπ„Éà„Åß„Åô„ÄÇÔºàÂõÅ„ÅçÔºâËÅû„Åì„Åà„Åæ„Åô„ÅãÔºü„Äç\nDemo\nFor detailed usage instructions and to try the model, please see the Hugging Face Space:\nAnime-Llasa-3B-Captions-Demo\nLimitations\nPlease note that due to limitations in the amount and quality of the training data, the model cannot be controlled perfectly. The generated speech may not always reflect the specified tags precisely.\nTraining Data\nThe dataset used for this fine-tuning, which includes the Gemini 2.5 Pro generated captions, is available here:\nNandemoGHS/Galgame_Gemini_Captions\nOld Versions\nAnime-Llasa-3B\nLicense\nThis model is licensed under CC-BY-NC-4.0.\nAdditionally, as this model includes outputs from Gemini 2.5 Pro in its training data, any use that competes with Gemini is prohibited.",
    "ladaapp/lada": "README.md exists but content is empty.",
    "spooknik/Jib-Mix-Flux-SVDQ": "Quality Evaluation\nThis repository contains Nunchaku-quantized (SVDQ) versions of Jib Mix Flux, a text-to-image model based on Flux.1-Dev by J1B\nIf you like the model, please consider liking, reviewing and tipping the creator.\nModel Files\nsvdq-int4_r32-Jib-Mix-Flux-V12.safetensors: SVDQuant INT4 (rank 32) Jib Mix Flux V12 model. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-Jib-Mix-Flux-V12.safetensors: SVDQuant NVFP4 (rank 32) Jib Mix Flux V12 model. For users with Blackwell GPUs (50-series).\nQuality Evaluation\nBelow is the quality and similarity evaluated with 256 samples from MJHQ-30K dataset. (BF16 is the unqauntized model. INT W4A4 is INT4 and NVFP4 is FP4)\nModel\nPrecision\nMethod\nFID\nIR\nLPIPS\nPSNR\nJib Flux Mix V12 (25 steps)\nBF16\n--\n170.37\n0.946\n--\n--\nINT W4A4\nSVDQ\n171.64\n0.907\n0.253\n20.31\nNVFP4\nSVDQ\n170.99\n0.955\n0.220\n20.71\nIf you find these useful please consider:",
    "cerebras/GLM-4.6-REAP-218B-A32B": "GLM-4.6-REAP-218B-A32B-FP8\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.6-REAP-218B-A32B-FP8\n‚ú® Highlights\nIntroducing GLM-4.6-REAP-218B-A32B-FP8, a memory-efficient compressed variant of GLM-4.6-FP8 that maintains near-identical performance while being 40% lighter.\nNote: this is a BF16 version for more accurate downstream low-bit quantization. An FP8 version is also available on HF.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 355B model\n40% Memory Reduction: Compressed from 355B to 218B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nüìã Model Overview\nGLM-4.6-REAP-218B-A32B-FP8 has the following specifications:\nBase Model: GLM-4.6-FP8\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 40% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 218B total, 32B activated per token\nNumber of Layers: 92\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 96 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 202,752 tokens\nLicense: MIT\nüìä Evaluations\nTBD for BF16 model. Evalulation results available for the FP8 variant.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.6-REAP-218B-A32B-FP8 \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.6-FP8, with a 40% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nzai-org/GLM-4.6-FP8\nand distributed under the MIT license.\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "HuggingFaceH4/zephyr-7b-beta": "Model Card for Zephyr 7B Œ≤\nModel description\nModel Sources\nPerformance\nIntended uses & limitations\nBias, Risks, and Limitations\nTraining and evaluation data\nTraining hyperparameters\nTraining results\nFramework versions\nCitation\nOpen LLM Leaderboard Evaluation Results\nModel Card for Zephyr 7B Œ≤\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Œ≤ is the second model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of publicly available, synthetic datasets using Direct Preference Optimization (DPO). We found that removing the in-built alignment of these datasets boosted performance on MT Bench and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so. You can find more details in the technical report.\nModel description\nModel type: A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\nLanguage(s) (NLP): Primarily English\nLicense: MIT\nFinetuned from model: mistralai/Mistral-7B-v0.1\nModel Sources\nRepository: https://github.com/huggingface/alignment-handbook\nDemo: https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\nChatbot Arena: Evaluate Zephyr 7B against 10+ LLMs in the LMSYS arena: http://arena.lmsys.org\nPerformance\nAt the time of release, Zephyr-7B-Œ≤ is the highest ranked 7B chat model on the MT-Bench and AlpacaEval benchmarks:\nModel\nSize\nAlignment\nMT-Bench (score)\nAlpacaEval (win rate %)\nStableLM-Tuned-Œ±\n7B\ndSFT\n2.75\n-\nMPT-Chat\n7B\ndSFT\n5.42\n-\nXwin-LMv0.1\n7B\ndPPO\n6.19\n87.83\nMistral-Instructv0.1\n7B\n-\n6.84\n-\nZephyr-7b-Œ±\n7B\ndDPO\n6.88\n-\nZephyr-7b-Œ≤ ü™Å\n7B\ndDPO\n7.34\n90.60\nFalcon-Instruct\n40B\ndSFT\n5.17\n45.71\nGuanaco\n65B\nSFT\n6.41\n71.80\nLlama2-Chat\n70B\nRLHF\n6.86\n92.66\nVicuna v1.3\n33B\ndSFT\n7.12\n88.99\nWizardLM v1.0\n70B\ndSFT\n7.71\n-\nXwin-LM v0.1\n70B\ndPPO\n-\n95.57\nGPT-3.5-turbo\n-\nRLHF\n7.94\n89.37\nClaude 2\n-\nRLHF\n8.06\n91.36\nGPT-4\n-\nRLHF\n8.99\n95.28\nIn particular, on several categories of MT-Bench, Zephyr-7B-Œ≤ has strong performance compared to larger open models like Llama2-Chat-70B:\nHowever, on more complex tasks like coding and mathematics, Zephyr-7B-Œ≤ lags behind proprietary models and more research is needed to close the gap.\nIntended uses & limitations\nThe model was initially fine-tuned on a filtered and preprocessed of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT.\nWe then further aligned the model with ü§ó TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contains 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our demo to test its capabilities.\nYou can find the datasets used for training Zephyr-7B-Œ≤ here\nHere's how you can run the model using the pipeline() function from ü§ó Transformers:\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\nBias, Risks, and Limitations\nZephyr-7B-Œ≤ has not been aligned to human preferences for safety within the RLHF phase or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base model (mistralai/Mistral-7B-v0.1), however it is likely to have included a mix of Web data and technical sources like books and code. See the Falcon 180B model card for an example of this.\nTraining and evaluation data\nDuring DPO training, this model achieves the following results on the evaluation set:\nLoss: 0.7496\nRewards/chosen: -4.5221\nRewards/rejected: -8.3184\nRewards/accuracies: 0.7812\nRewards/margins: 3.7963\nLogps/rejected: -340.1541\nLogps/chosen: -299.4561\nLogits/rejected: -2.3081\nLogits/chosen: -2.3531\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-07\ntrain_batch_size: 2\neval_batch_size: 4\nseed: 42\ndistributed_type: multi-GPU\nnum_devices: 16\ntotal_train_batch_size: 32\ntotal_eval_batch_size: 64\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_ratio: 0.1\nnum_epochs: 3.0\nTraining results\nThe table below shows the full set of DPO training metrics:\nTraining Loss\nEpoch\nStep\nValidation Loss\nRewards/chosen\nRewards/rejected\nRewards/accuracies\nRewards/margins\nLogps/rejected\nLogps/chosen\nLogits/rejected\nLogits/chosen\n0.6284\n0.05\n100\n0.6098\n0.0425\n-0.1872\n0.7344\n0.2297\n-258.8416\n-253.8099\n-2.7976\n-2.8234\n0.4908\n0.1\n200\n0.5426\n-0.0279\n-0.6842\n0.75\n0.6563\n-263.8124\n-254.5145\n-2.7719\n-2.7960\n0.5264\n0.15\n300\n0.5324\n0.0414\n-0.9793\n0.7656\n1.0207\n-266.7627\n-253.8209\n-2.7892\n-2.8122\n0.5536\n0.21\n400\n0.4957\n-0.0185\n-1.5276\n0.7969\n1.5091\n-272.2460\n-254.4203\n-2.8542\n-2.8764\n0.5362\n0.26\n500\n0.5031\n-0.2630\n-1.5917\n0.7812\n1.3287\n-272.8869\n-256.8653\n-2.8702\n-2.8958\n0.5966\n0.31\n600\n0.5963\n-0.2993\n-1.6491\n0.7812\n1.3499\n-273.4614\n-257.2279\n-2.8778\n-2.8986\n0.5014\n0.36\n700\n0.5382\n-0.2859\n-1.4750\n0.75\n1.1891\n-271.7204\n-257.0942\n-2.7659\n-2.7869\n0.5334\n0.41\n800\n0.5677\n-0.4289\n-1.8968\n0.7969\n1.4679\n-275.9378\n-258.5242\n-2.7053\n-2.7265\n0.5251\n0.46\n900\n0.5772\n-0.2116\n-1.3107\n0.7344\n1.0991\n-270.0768\n-256.3507\n-2.8463\n-2.8662\n0.5205\n0.52\n1000\n0.5262\n-0.3792\n-1.8585\n0.7188\n1.4793\n-275.5552\n-258.0276\n-2.7893\n-2.7979\n0.5094\n0.57\n1100\n0.5433\n-0.6279\n-1.9368\n0.7969\n1.3089\n-276.3377\n-260.5136\n-2.7453\n-2.7536\n0.5837\n0.62\n1200\n0.5349\n-0.3780\n-1.9584\n0.7656\n1.5804\n-276.5542\n-258.0154\n-2.7643\n-2.7756\n0.5214\n0.67\n1300\n0.5732\n-1.0055\n-2.2306\n0.7656\n1.2251\n-279.2761\n-264.2903\n-2.6986\n-2.7113\n0.6914\n0.72\n1400\n0.5137\n-0.6912\n-2.1775\n0.7969\n1.4863\n-278.7448\n-261.1467\n-2.7166\n-2.7275\n0.4655\n0.77\n1500\n0.5090\n-0.7987\n-2.2930\n0.7031\n1.4943\n-279.8999\n-262.2220\n-2.6651\n-2.6838\n0.5731\n0.83\n1600\n0.5312\n-0.8253\n-2.3520\n0.7812\n1.5268\n-280.4902\n-262.4876\n-2.6543\n-2.6728\n0.5233\n0.88\n1700\n0.5206\n-0.4573\n-2.0951\n0.7812\n1.6377\n-277.9205\n-258.8084\n-2.6870\n-2.7097\n0.5593\n0.93\n1800\n0.5231\n-0.5508\n-2.2000\n0.7969\n1.6492\n-278.9703\n-259.7433\n-2.6221\n-2.6519\n0.4967\n0.98\n1900\n0.5290\n-0.5340\n-1.9570\n0.8281\n1.4230\n-276.5395\n-259.5749\n-2.6564\n-2.6878\n0.0921\n1.03\n2000\n0.5368\n-1.1376\n-3.1615\n0.7812\n2.0239\n-288.5854\n-265.6111\n-2.6040\n-2.6345\n0.0733\n1.08\n2100\n0.5453\n-1.1045\n-3.4451\n0.7656\n2.3406\n-291.4208\n-265.2799\n-2.6289\n-2.6595\n0.0972\n1.14\n2200\n0.5571\n-1.6915\n-3.9823\n0.8125\n2.2908\n-296.7934\n-271.1505\n-2.6471\n-2.6709\n0.1058\n1.19\n2300\n0.5789\n-1.0621\n-3.8941\n0.7969\n2.8319\n-295.9106\n-264.8563\n-2.5527\n-2.5798\n0.2423\n1.24\n2400\n0.5455\n-1.1963\n-3.5590\n0.7812\n2.3627\n-292.5599\n-266.1981\n-2.5414\n-2.5784\n0.1177\n1.29\n2500\n0.5889\n-1.8141\n-4.3942\n0.7969\n2.5801\n-300.9120\n-272.3761\n-2.4802\n-2.5189\n0.1213\n1.34\n2600\n0.5683\n-1.4608\n-3.8420\n0.8125\n2.3812\n-295.3901\n-268.8436\n-2.4774\n-2.5207\n0.0889\n1.39\n2700\n0.5890\n-1.6007\n-3.7337\n0.7812\n2.1330\n-294.3068\n-270.2423\n-2.4123\n-2.4522\n0.0995\n1.45\n2800\n0.6073\n-1.5519\n-3.8362\n0.8281\n2.2843\n-295.3315\n-269.7538\n-2.4685\n-2.5050\n0.1145\n1.5\n2900\n0.5790\n-1.7939\n-4.2876\n0.8438\n2.4937\n-299.8461\n-272.1744\n-2.4272\n-2.4674\n0.0644\n1.55\n3000\n0.5735\n-1.7285\n-4.2051\n0.8125\n2.4766\n-299.0209\n-271.5201\n-2.4193\n-2.4574\n0.0798\n1.6\n3100\n0.5537\n-1.7226\n-4.2850\n0.8438\n2.5624\n-299.8200\n-271.4610\n-2.5367\n-2.5696\n0.1013\n1.65\n3200\n0.5575\n-1.5715\n-3.9813\n0.875\n2.4098\n-296.7825\n-269.9498\n-2.4926\n-2.5267\n0.1254\n1.7\n3300\n0.5905\n-1.6412\n-4.4703\n0.8594\n2.8291\n-301.6730\n-270.6473\n-2.5017\n-2.5340\n0.085\n1.76\n3400\n0.6133\n-1.9159\n-4.6760\n0.8438\n2.7601\n-303.7296\n-273.3941\n-2.4614\n-2.4960\n0.065\n1.81\n3500\n0.6074\n-1.8237\n-4.3525\n0.8594\n2.5288\n-300.4951\n-272.4724\n-2.4597\n-2.5004\n0.0755\n1.86\n3600\n0.5836\n-1.9252\n-4.4005\n0.8125\n2.4753\n-300.9748\n-273.4872\n-2.4327\n-2.4716\n0.0746\n1.91\n3700\n0.5789\n-1.9280\n-4.4906\n0.8125\n2.5626\n-301.8762\n-273.5149\n-2.4686\n-2.5115\n0.1348\n1.96\n3800\n0.6015\n-1.8658\n-4.2428\n0.8281\n2.3769\n-299.3976\n-272.8936\n-2.4943\n-2.5393\n0.0217\n2.01\n3900\n0.6122\n-2.3335\n-4.9229\n0.8281\n2.5894\n-306.1988\n-277.5699\n-2.4841\n-2.5272\n0.0219\n2.07\n4000\n0.6522\n-2.9890\n-6.0164\n0.8281\n3.0274\n-317.1334\n-284.1248\n-2.4105\n-2.4545\n0.0119\n2.12\n4100\n0.6922\n-3.4777\n-6.6749\n0.7969\n3.1972\n-323.7187\n-289.0121\n-2.4272\n-2.4699\n0.0153\n2.17\n4200\n0.6993\n-3.2406\n-6.6775\n0.7969\n3.4369\n-323.7453\n-286.6413\n-2.4047\n-2.4465\n0.011\n2.22\n4300\n0.7178\n-3.7991\n-7.4397\n0.7656\n3.6406\n-331.3667\n-292.2260\n-2.3843\n-2.4290\n0.0072\n2.27\n4400\n0.6840\n-3.3269\n-6.8021\n0.8125\n3.4752\n-324.9908\n-287.5042\n-2.4095\n-2.4536\n0.0197\n2.32\n4500\n0.7013\n-3.6890\n-7.3014\n0.8125\n3.6124\n-329.9841\n-291.1250\n-2.4118\n-2.4543\n0.0182\n2.37\n4600\n0.7476\n-3.8994\n-7.5366\n0.8281\n3.6372\n-332.3356\n-293.2291\n-2.4163\n-2.4565\n0.0125\n2.43\n4700\n0.7199\n-4.0560\n-7.5765\n0.8438\n3.5204\n-332.7345\n-294.7952\n-2.3699\n-2.4100\n0.0082\n2.48\n4800\n0.7048\n-3.6613\n-7.1356\n0.875\n3.4743\n-328.3255\n-290.8477\n-2.3925\n-2.4303\n0.0118\n2.53\n4900\n0.6976\n-3.7908\n-7.3152\n0.8125\n3.5244\n-330.1224\n-292.1431\n-2.3633\n-2.4047\n0.0118\n2.58\n5000\n0.7198\n-3.9049\n-7.5557\n0.8281\n3.6508\n-332.5271\n-293.2844\n-2.3764\n-2.4194\n0.006\n2.63\n5100\n0.7506\n-4.2118\n-7.9149\n0.8125\n3.7032\n-336.1194\n-296.3530\n-2.3407\n-2.3860\n0.0143\n2.68\n5200\n0.7408\n-4.2433\n-7.9802\n0.8125\n3.7369\n-336.7721\n-296.6682\n-2.3509\n-2.3946\n0.0057\n2.74\n5300\n0.7552\n-4.3392\n-8.0831\n0.7969\n3.7439\n-337.8013\n-297.6275\n-2.3388\n-2.3842\n0.0138\n2.79\n5400\n0.7404\n-4.2395\n-7.9762\n0.8125\n3.7367\n-336.7322\n-296.6304\n-2.3286\n-2.3737\n0.0079\n2.84\n5500\n0.7525\n-4.4466\n-8.2196\n0.7812\n3.7731\n-339.1662\n-298.7007\n-2.3200\n-2.3641\n0.0077\n2.89\n5600\n0.7520\n-4.5586\n-8.3485\n0.7969\n3.7899\n-340.4545\n-299.8206\n-2.3078\n-2.3517\n0.0094\n2.94\n5700\n0.7527\n-4.5542\n-8.3509\n0.7812\n3.7967\n-340.4790\n-299.7773\n-2.3062\n-2.3510\n0.0054\n2.99\n5800\n0.7520\n-4.5169\n-8.3079\n0.7812\n3.7911\n-340.0493\n-299.4038\n-2.3081\n-2.3530\nFramework versions\nTransformers 4.35.0.dev0\nPytorch 2.0.1+cu118\nDatasets 2.12.0\nTokenizers 0.14.0\nCitation\nIf you find Zephyr-7B-Œ≤ is useful in your work, please cite it with:\n@misc{tunstall2023zephyr,\ntitle={Zephyr: Direct Distillation of LM Alignment},\nauthor={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl√©mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\nyear={2023},\neprint={2310.16944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nIf you use the UltraChat or UltraFeedback datasets, please cite the original works:\n@misc{ding2023enhancing,\ntitle={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},\nauthor={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\nyear={2023},\neprint={2305.14233},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@misc{cui2023ultrafeedback,\ntitle={UltraFeedback: Boosting Language Models with High-quality Feedback},\nauthor={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},\nyear={2023},\neprint={2310.01377},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n52.15\nARC (25-shot)\n62.03\nHellaSwag (10-shot)\n84.36\nMMLU (5-shot)\n61.07\nTruthfulQA (0-shot)\n57.45\nWinogrande (5-shot)\n77.74\nGSM8K (5-shot)\n12.74\nDROP (3-shot)\n9.66",
    "stable-diffusion-v1-5/stable-diffusion-v1-5": "Stable Diffusion v1-5 Model Card\nModel Details\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nSafety Module\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v1-5 Model Card\n‚ö†Ô∏è This repository is a mirror of the now deprecated ruwnayml/stable-diffusion-v1-5, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in red or green\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at ü§ó's Stable Diffusion blog.\nThe Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2\ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.\nYou can use this both with the üß®Diffusers library and RunwayML GitHub repository (now deprecated), ComfyUI, Automatic1111, SD.Next, InvokeAI.\nUse with Diffusers\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nFor more detailed instructions, use-cases and examples in JAX follow the instructions here\nUse with GitHub Repository (now deprecated), ComfyUI or Automatic1111\nDownload the weights\nv1-5-pruned-emaonly.safetensors - ema-only weight. uses less VRAM - suitable for inference\nv1-5-pruned.safetensors - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\nFollow instructions here. (now deprecated)\nUse locally with ComfyUI, AUTOMATIC1111, SD.Next, InvokeAI\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (CLIP ViT-L/14) as suggested in the Imagen paper.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and\ntasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is taken from the DALLE-MINI model card, but applies in the same way to Stable Diffusion v1.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a large-scale dataset\nLAION-5B which contains adult material\nand is not fit for product use without additional safety mechanisms and\nconsiderations.\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\nThe training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion v1 was trained on subsets of LAION-2B(en),\nwhich consists of images that are primarily limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nSafety Module\nThe intended use of this model is with the Safety Checker in Diffusers.\nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the CLIPTextModel after generation of the images.\nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-2B (en) and subsets thereof (see next section)\nTraining Procedure\nStable Diffusion v1-5 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through a ViT-L/14 text-encoder.\nThe non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\nCurrently six Stable Diffusion checkpoints are provided, which were trained as follows.\nstable-diffusion-v1-1: 237,000 steps at resolution 256x256 on laion2B-en.\n194,000 steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).\nstable-diffusion-v1-2: Resumed from stable-diffusion-v1-1.\n515,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator).\nstable-diffusion-v1-3: Resumed from stable-diffusion-v1-2 - 195,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nstable-diffusion-v1-4 Resumed from stable-diffusion-v1-2 - 225,000 steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nstable-diffusion-v1-5 Resumed from stable-diffusion-v1-2 - 595,000 steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nstable-diffusion-inpainting Resumed from stable-diffusion-v1-5 - then 440,000 steps of inpainting training at resolution 512x512 on ‚Äúlaion-aesthetics v2 5+‚Äù and 10% dropping of the text-conditioning. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 2\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PNDM/PLMS sampling\nsteps show the relative improvements of the checkpoints:\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 150000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 11250 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach and Patrick Esser and is based on the DALL-E Mini model card.",
    "sesame/csm-1b": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCSM 1B\nUsage\nGenerate a sentence\nCSM sounds best when provided with context\nBatched Inference üì¶\nMaking The Model Go Brrr üèéÔ∏è\nFine-tuning & training üìâ\nFAQ\nMisuse and abuse ‚ö†Ô∏è\nCSM 1B\n2025/05/20 - CSM is availabile natively in Hugging Face Transformers ü§ó as of version 4.52.1\n2025/03/13 - We are releasing the 1B CSM variant. The checkpoint is hosted on Hugging Face.\nCSM (Conversational Speech Model) is a speech generation model from Sesame that generates RVQ audio codes from text and audio inputs. The model architecture employs a Llama backbone and a smaller audio decoder that produces Mimi audio codes.\nA fine-tuned variant of CSM powers the interactive voice demo shown in our blog post.\nA hosted HuggingFace space is also available for testing audio generation.\nUsage\nGenerate a sentence\nimport torch\nfrom transformers import CsmForConditionalGeneration, AutoProcessor\nmodel_id = \"sesame/csm-1b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# load the model and the processor\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n# prepare the inputs\ntext = \"[0]Hello from Sesame.\" # `[0]` for speaker id 0\ninputs = processor(text, add_special_tokens=True).to(device)\n# another equivalent way to prepare the inputs\nconversation = [\n{\"role\": \"0\", \"content\": [{\"type\": \"text\", \"text\": \"Hello from Sesame.\"}]},\n]\ninputs = processor.apply_chat_template(\nconversation,\ntokenize=True,\nreturn_dict=True,\n).to(device)\n# infer the model\naudio = model.generate(**inputs, output_audio=True)\nprocessor.save_audio(audio, \"example_without_context.wav\")\nCSM sounds best when provided with context\nimport torch\nfrom transformers import CsmForConditionalGeneration, AutoProcessor\nfrom datasets import load_dataset, Audio\nmodel_id = \"sesame/csm-1b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# load the model and the processor\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n# prepare the inputs\nds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n# ensure the audio is 24kHz\nds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\nconversation = []\n# 1. context\nfor text, audio, speaker_id in zip(ds[:4][\"text\"], ds[:4][\"audio\"], ds[:4][\"speaker_id\"]):\nconversation.append(\n{\n\"role\": f\"{speaker_id}\",\n\"content\": [{\"type\": \"text\", \"text\": text}, {\"type\": \"audio\", \"path\": audio[\"array\"]}],\n}\n)\n# 2. text prompt\nconversation.append({\"role\": f\"{ds[4]['speaker_id']}\", \"content\": [{\"type\": \"text\", \"text\": ds[4][\"text\"]}]})\ninputs = processor.apply_chat_template(\nconversation,\ntokenize=True,\nreturn_dict=True,\n).to(device)\n# infer the model\naudio = model.generate(**inputs, output_audio=True)\nprocessor.save_audio(audio, \"example_with_context.wav\")\nBatched Inference üì¶\nCSM supports batched inference:\ncode snippet\nimport torch\nfrom transformers import CsmForConditionalGeneration, AutoProcessor\nfrom datasets import load_dataset, Audio\nmodel_id = \"sesame/csm-1b\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# load the model and the processor\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n# prepare the inputs\nds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n# ensure the audio is 24kHz\nds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n# here a batch with two prompts\nconversation = [\n[\n{\n\"role\": f\"{ds[0]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[0][\"text\"]},\n{\"type\": \"audio\", \"path\": ds[0][\"audio\"][\"array\"]},\n],\n},\n{\n\"role\": f\"{ds[1]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[1][\"text\"]},\n],\n},\n],\n[\n{\n\"role\": f\"{ds[0]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[0][\"text\"]},\n],\n}\n],\n]\ninputs = processor.apply_chat_template(\nconversation,\ntokenize=True,\nreturn_dict=True,\n).to(device)\naudio = model.generate(**inputs, output_audio=True)\nprocessor.save_audio(audio, [f\"speech_batch_idx_{i}.wav\" for i in range(len(audio))])\nMaking The Model Go Brrr üèéÔ∏è\nCSM supports full-graph compilation with CUDA graphs!\ncode snippet\nimport torch\nimport copy\nfrom transformers import CsmForConditionalGeneration, AutoProcessor\nfrom datasets import load_dataset\nmodel_id = \"sesame/csm-1b\"\ndevice = \"cuda\"\n# set logs to ensure no recompilation and graph breaks\ntorch._logging.set_logs(graph_breaks=True, recompiles=True, cudagraphs=True)\n# load the model and the processor\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n# use static cache, enabling automatically torch compile with fullgraph and reduce-overhead\nmodel.generation_config.max_length = 250 # big enough to avoid recompilation\nmodel.generation_config.max_new_tokens = None # would take precedence over max_length\nmodel.generation_config.cache_implementation = \"static\"\nmodel.depth_decoder.generation_config.cache_implementation = \"static\"\n# generation kwargs\ngen_kwargs = {\n\"do_sample\": False,\n\"depth_decoder_do_sample\": False,\n\"temperature\": 1.0,\n\"depth_decoder_temperature\": 1.0,\n}\n# Define a timing decorator\nclass TimerContext:\ndef __init__(self, name=\"Execution\"):\nself.name = name\nself.start_event = None\nself.end_event = None\ndef __enter__(self):\n# Use CUDA events for more accurate GPU timing\nself.start_event = torch.cuda.Event(enable_timing=True)\nself.end_event = torch.cuda.Event(enable_timing=True)\nself.start_event.record()\nreturn self\ndef __exit__(self, *args):\nself.end_event.record()\ntorch.cuda.synchronize()\nelapsed_time = self.start_event.elapsed_time(self.end_event) / 1000.0\nprint(f\"{self.name} time: {elapsed_time:.4f} seconds\")\n# prepare the inputs\nds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\nconversation = [\n{\n\"role\": f\"{ds[0]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[0][\"text\"]},\n{\"type\": \"audio\", \"path\": ds[0][\"audio\"][\"array\"]},\n],\n},\n{\n\"role\": f\"{ds[1]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[1][\"text\"]},\n{\"type\": \"audio\", \"path\": ds[1][\"audio\"][\"array\"]},\n],\n},\n{\n\"role\": f\"{ds[2]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[2][\"text\"]},\n],\n},\n]\npadded_inputs_1 = processor.apply_chat_template(\nconversation,\ntokenize=True,\nreturn_dict=True,\n).to(device)\nprint(\"\\n\" + \"=\"*50)\nprint(\"First generation - compiling and recording CUDA graphs...\")\nwith TimerContext(\"First generation\"):\n_ = model.generate(**padded_inputs_1, **gen_kwargs)\nprint(\"=\"*50)\nprint(\"\\n\" + \"=\"*50)\nprint(\"Second generation - fast !!!\")\nwith TimerContext(\"Second generation\"):\n_ = model.generate(**padded_inputs_1, **gen_kwargs)\nprint(\"=\"*50)\n# now with different inputs\nconversation = [\n{\n\"role\": f\"{ds[0]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[2][\"text\"]},\n{\"type\": \"audio\", \"path\": ds[2][\"audio\"][\"array\"]},\n],\n},\n{\n\"role\": f\"{ds[1]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[3][\"text\"]},\n{\"type\": \"audio\", \"path\": ds[3][\"audio\"][\"array\"]},\n],\n},\n{\n\"role\": f\"{ds[2]['speaker_id']}\",\n\"content\": [\n{\"type\": \"text\", \"text\": ds[4][\"text\"]},\n],\n},\n]\npadded_inputs_2 = processor.apply_chat_template(\nconversation,\ntokenize=True,\nreturn_dict=True,\n).to(device)\nprint(\"\\n\" + \"=\"*50)\nprint(\"Generation with other inputs!\")\nwith TimerContext(\"Generation with different inputs\"):\n_ = model.generate(**padded_inputs_2, **gen_kwargs)\nprint(\"=\"*50)\nFine-tuning & training üìâ\nCSM can be fine-tuned using Transformers' Trainer.\ncode snippet\nfrom datasets import load_dataset, Audio\nfrom transformers import (\nCsmForConditionalGeneration,\nTrainingArguments,\nCsmProcessor,\nTrainer\n)\nprocessor = CsmProcessor.from_pretrained(\"sesame/csm-1b\")\nmodel = CsmForConditionalGeneration.from_pretrained(\"sesame/csm-1b\")\nmodel.train()\nmodel.codec_model.eval()\nds = load_dataset(\"eustlb/dailytalk-conversations-grouped\", split=\"train\")\nds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\ndef data_collator(samples):\nconversations = []\nfor sample in samples:\nconcatenated_audio_array = sample[\"audio\"][\"array\"]\naudio = [concatenated_audio_array[s: e] for s, e in sample[\"audio_cut_idxs\"]]\nconversation = []\nfor speaker_id, text, audio in zip(sample[\"speaker_ids\"], sample[\"texts\"], audio):\nconversation.append({\n\"role\": f\"{speaker_id}\",\n\"content\": [\n{\"type\": \"text\", \"text\": text},\n{\"type\": \"audio\", \"audio\": audio}\n]\n})\nconversations.append(conversation)\ninputs = processor.apply_chat_template(\nconversations,\ntokenize=True,\nreturn_dict=True,\noutput_labels=True,\n)\nreturn inputs\ntraining_args = TrainingArguments(\n\"test-trainer\",\nremove_unused_columns=False,\ngradient_checkpointing=True,\n)\ntrainer = Trainer(\nmodel,\ntraining_args,\ntrain_dataset=ds,\ndata_collator=data_collator,\n)\ntrainer.train()\nFAQ\nDoes this model come with any voices?\nThe model open sourced here is a base generation model. It is capable of producing a variety of voices, but it has not been fine-tuned on any specific voice.\nCan I converse with the model?\nCSM is trained to be an audio generation model and not a general purpose multimodal LLM. It cannot generate text. We suggest using a separate LLM for text generation.\nDoes it support other languages?\nThe model has some capacity for non-English languages due to data contamination in the training data, but it likely won't do well.\nMisuse and abuse ‚ö†Ô∏è\nThis project provides a high-quality speech generation model for research and educational purposes. While we encourage responsible and ethical use, we explicitly prohibit the following:\nImpersonation or Fraud: Do not use this model to generate speech that mimics real individuals without their explicit consent.\nMisinformation or Deception: Do not use this model to create deceptive or misleading content, such as fake news or fraudulent calls.\nIllegal or Harmful Activities: Do not use this model for any illegal, harmful, or malicious purposes.\nBy using this model, you agree to comply with all applicable laws and ethical guidelines. We are not responsible for any misuse, and we strongly condemn unethical applications of this technology.\nAuthors\nJohan Schalkwyk, Ankit Kumar, Dan Lyth, Sefik Emre Eskimez, Zack Hodari, Cinjon Resnick, Ramon Sanabria, Raven Jiang, and the Sesame team.",
    "google/gemma-3n-E4B-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3n model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nThis repository corresponds to the launch version of Gemma 3n E4B IT (Instruct), to be used with Hugging Face transformers,\nsupporting text, audio, and vision (image and video) inputs.\nGemma 3n models have multiple architecture innovations:\nThey are available in two sizes based on effective parameters. While the raw parameter count of this model is 8B, the architecture design allows the model to be run with a memory footprint comparable to a traditional 4B model by offloading low-utilization matrices from the accelerator.\nThey use a MatFormer architecture that allows nesting sub-models within the E4B model. We provide one sub-model (an E2B), or you can access a spectrum of custom-sized models using the Mix-and-Match method.\nLearn more about these techniques in the technical blog post\nand the Gemma documentation.\nGemma 3n model card\nModel Page: Gemma 3n\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on HuggingFace\nGemma on Vertex Model Garden\nTerms of Use: TermsAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3n models are designed for efficient execution on low-resource devices.\nThey are capable of multimodal input, handling text, image, video, and audio\ninput, and generating text outputs, with open weights for pre-trained and\ninstruction-tuned variants. These models were trained with data in over 140\nspoken languages.\nGemma 3n models use selective parameter activation technology to reduce resource\nrequirements. This technique allows the models to operate at an effective size\nof 2B and 4B parameters, which is lower than the total number of parameters they\ncontain. For more information on Gemma 3n's efficient parameter management\ntechnology, see the\nGemma 3n\npage.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be\nsummarized\nImages, normalized to 256x256, 512x512, or 768x768 resolution\nand encoded to 256 tokens each\nAudio data encoded to 6.25 tokens per second from a single channel\nTotal input context of 32K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output length up to 32K tokens, subtracting the request\ninput tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running\nthe model. First, install the Transformers library. Gemma 3n is supported\nstarting from transformers 4.53.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nYou can initialize the model and processor for inference with pipeline as\nfollows.\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/gemma-3n-e4b-it\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16,\n)\nWith instruction-tuned models, you need to use chat templates to process our\ninputs first. Then, you can pass it to the pipeline.\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n{\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\nRunning the model on a single GPU\nfrom transformers import AutoProcessor, Gemma3nForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/gemma-3n-e4b-it\"\nmodel = Gemma3nForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\nCitation\n@article{gemma_3n_2025,\ntitle={Gemma 3n},\nurl={https://ai.google.dev/gemma/docs/gemma-3n},\npublisher={Google DeepMind},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset that includes a wide variety of sources\ntotalling approximately 11 trillion tokens. The knowledge cutoff date for the\ntraining data was June 2024. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model\nis exposed to a broad range of linguistic styles, topics, and vocabulary.\nThe training dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn\nlogical reasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nAudio: A diverse set of sound samples enables the model to recognize\nspeech, transcribe text from recordings, and identify information in audio data.\nThe combination of these diverse data sources is crucial for training a\npowerful multimodal model that can handle a wide variety of different tasks and\ndata formats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material)\nfiltering was applied at multiple stages in the data preparation process to\nensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit\n(TPU) hardware (TPUv4p, TPUv5p\nand TPUv5e). Training generative models requires significant computational\npower. TPUs, designed specifically for matrix operations common in machine\nlearning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training generative models. They can speed up\ntraining considerably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and\nML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models:\n\"the 'single controller' programming model of Jax and Pathways allows a single\nPython process to orchestrate the entire training run, dramatically simplifying\nthe development workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with IT are for\ninstruction-tuned models. Evaluation results marked with PT are for\npre-trained models.\nReasoning and factuality\nBenchmark\nMetric\nn-shot\nE2B PT\nE4B PT\nHellaSwag\nAccuracy\n10-shot\n72.2\n78.6\nBoolQ\nAccuracy\n0-shot\n76.4\n81.6\nPIQA\nAccuracy\n0-shot\n78.9\n81.0\nSocialIQA\nAccuracy\n0-shot\n48.8\n50.0\nTriviaQA\nAccuracy\n5-shot\n60.8\n70.2\nNatural Questions\nAccuracy\n5-shot\n15.5\n20.9\nARC-c\nAccuracy\n25-shot\n51.7\n61.6\nARC-e\nAccuracy\n0-shot\n75.8\n81.6\nWinoGrande\nAccuracy\n5-shot\n66.8\n71.7\nBIG-Bench Hard\nAccuracy\nfew-shot\n44.3\n52.9\nDROP\nToken F1 score\n1-shot\n53.9\n60.8\nMultilingual\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMGSM\nAccuracy\n0-shot\n53.1\n60.7\nWMT24++ (ChrF)\nCharacter-level F-score\n0-shot\n42.7\n50.1\nInclude\nAccuracy\n0-shot\n38.6\n57.2\nMMLU (ProX)\nAccuracy\n0-shot\n8.1\n19.9\nOpenAI MMLU\nAccuracy\n0-shot\n22.3\n35.6\nGlobal-MMLU\nAccuracy\n0-shot\n55.1\n60.3\nECLeKTic\nECLeKTic score\n0-shot\n2.5\n1.9\nSTEM and code\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nGPQA Diamond\nRelaxedAccuracy/accuracy\n0-shot\n24.8\n23.7\nLiveCodeBench v5\npass@1\n0-shot\n18.6\n25.7\nCodegolf v2.2\npass@1\n0-shot\n11.0\n16.8\nAIME 2025\nAccuracy\n0-shot\n6.7\n11.6\nAdditional benchmarks\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMMLU\nAccuracy\n0-shot\n60.1\n64.9\nMBPP\npass@1\n3-shot\n56.6\n63.6\nHumanEval\npass@1\n0-shot\n66.5\n75.0\nLiveCodeBench\npass@1\n0-shot\n13.2\n13.2\nHiddenMath\nAccuracy\n0-shot\n27.7\n37.7\nGlobal-MMLU-Lite\nAccuracy\n0-shot\n59.0\n64.5\nMMLU (Pro)\nAccuracy\n0-shot\n40.5\n50.6\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models'\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: Generate creative text formats such as\npoems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational\ninterfaces for customer service, virtual assistants, or interactive\napplications.\nText Summarization: Generate concise summaries of a text\ncorpus, research papers, or reports.\nImage Data Extraction: Extract, interpret, and summarize\nvisual data for text communications.\nAudio Data Extraction: Transcribe spoken language, translate speech\nto text in other languages, and analyze sound-based data.\nResearch and Education\nNatural Language Processing (NLP) and generative model\nResearch: These models can serve as a foundation for researchers to\nexperiment with generative models and NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language\nlearning experiences, aiding in grammar correction or providing writing\npractice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of data by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of generative models raises several ethical concerns. In\ncreating an open model, we have carefully considered the following:\nBias and Fairness\nGenerative models trained on large-scale, real-world text and image data\ncan reflect socio-cultural biases embedded in the training material.\nThese models underwent careful scrutiny, input data pre-processing\ndescribed and posterior evaluations reported in this card.\nMisinformation and Misuse\nGenerative models can be misused to generate text that is\nfalse, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making generative model technology accessible to\ndevelopers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\ngenerative models. Educational resources and reporting mechanisms for users\nto flag misuse are provided. Prohibited uses of Gemma models are outlined\nin the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of\ncertain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\ngenerative model implementations designed from the ground up for responsible AI\ndevelopment compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "zeroentropy/zerank-1": "Releasing zeroentropy/zerank-1\nHow to Use\nEvaluations\nReleasing zeroentropy/zerank-1\nIn search enginers, rerankers are crucial for improving the accuracy of your retrieval system.\nHowever, SOTA rerankers are closed-source and proprietary. At ZeroEntropy, we've trained a SOTA reranker outperforming closed-source competitors, and we're launching our model here on HuggingFace.\nThis reranker outperforms proprietary rerankers such as cohere-rerank-v3.5 and Salesforce/LlamaRank-v1 across a wide variety of domains, including finance, legal, code, STEM, medical, and conversational data.\nAt ZeroEntropy we've developed an innovative multi-stage pipeline that models query-document relevance scores as adjusted Elo ratings. See our Technical Report (Coming soon!) for more details.\nSince we're a small company, this model is only released under a non-commercial license. If you'd like a commercial license, please contact us at founders@zeroentropy.dev and we'll get you a license ASAP.\nFor this model's smaller twin, see zerank-1-small, which we've fully open-sourced under an Apache 2.0 License.\nHow to Use\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder(\"zeroentropy/zerank-1\", trust_remote_code=True)\nquery_documents = [\n(\"What is 2+2?\", \"4\"),\n(\"What is 2+2?\", \"The answer is definitely 1 million\"),\n]\nscores = model.predict(query_documents)\nprint(scores)\nThe model can also be inferenced using ZeroEntropy's /models/rerank endpoint.\nEvaluations\nNDCG@10 scores between zerank-1 and competing closed-source proprietary rerankers. Since we are evaluating rerankers, OpenAI's text-embedding-3-small is used as an initial retriever for the Top 100 candidate documents.\nTask\nEmbedding\ncohere-rerank-v3.5\nSalesforce/Llama-rank-v1\nzerank-1-small\nzerank-1\nCode\n0.678\n0.724\n0.694\n0.730\n0.754\nConversational\n0.250\n0.571\n0.484\n0.556\n0.596\nFinance\n0.839\n0.824\n0.828\n0.861\n0.894\nLegal\n0.703\n0.804\n0.767\n0.817\n0.821\nMedical\n0.619\n0.750\n0.719\n0.773\n0.796\nSTEM\n0.401\n0.510\n0.595\n0.680\n0.694\nComparing BM25 and Hybrid Search without and with zerank-1:",
    "Wan-AI/Wan2.2-TI2V-5B": "Wan2.2\nVideo Demos\nüî• Latest News!!\nCommunity Works\nüìë Todo List\nRun Wan2.2\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.2\nüíú Wan ¬†¬† ÔΩú ¬†¬† üñ•Ô∏è GitHub ¬†¬†  | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Technical Report ¬†¬† | ¬†¬† üìë Blog ¬†¬† | ¬†¬†üí¨ WeChat Group¬†¬† | ¬†¬† üìñ Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nüëç Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nüëç Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nüëç Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nüëç Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nThis repository contains our TI2V-5B model, built with the advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can runs on single consumer-grade GPU such as the 4090. It is one of the fastest 720P@24fps models available, meeting the needs of both industrial applications and academic research.\nVideo Demos\nYour browser does not support the video tag.\nüî• Latest News!!\nJul 28, 2025: üëã We've released the inference code and model weights of Wan2.2.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\nüìë Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nü§ó Huggingface     ü§ñ ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nüí°Note:\nThe TI2V-5B model supports 720P video generation at 24 FPS.\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./Wan2.2-TI2V-5B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-TI2V-5B --local_dir ./Wan2.2-TI2V-5B\nRun Text-Image-to-Video Generation\nThis repository supports the Wan2.2-TI2V-5B Text-Image-to-Video model and can support video generation at 720P resolutions.\nSingle-GPU Text-to-Video inference\npython generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --offload_model True --convert_model_dtype --t5_cpu --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\"\nüí°Unlike other tasks, the 720P resolution of the Text-Image-to-Video task is 1280*704 or 704*1280.\nThis command can run on a GPU with at least 24GB VRAM (e.g, RTX 4090 GPU).\nüí°If you are running on a GPU with at least 80GB VRAM, you can remove the --offload_model True, --convert_model_dtype and --t5_cpu options to speed up execution.\nSingle-GPU Image-to-Video inference\npython generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --offload_model True --convert_model_dtype --t5_cpu --image examples/i2v_input.JPG --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nüí°If the image parameter is configured, it is an Image-to-Video generation; otherwise, it defaults to a Text-to-Video generation.\nüí°Similar to Image-to-Video, the size parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\ntorchrun --nproc_per_node=8 generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --dit_fsdp --t5_fsdp --ulysses_size 8 --image examples/i2v_input.JPG --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nThe process of prompt extension can be referenced here.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "QuantStack/Wan2.2-T2V-A14B-GGUF": "This GGUF file is a direct conversion of Wan-AI/Wan2.2-T2V-A14B\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96\nPlace model files in ComfyUI/models/unet see the GitHub readme for further installation instructions.",
    "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF": "Qwen3-Coder-30B-A3B-Instruct\nHighlights\nModel Overview\nQuickstart\nAgentic Coding\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3-Coder correctly - Read our Guide.\nSee Unsloth Dynamic 2.0 GGUFs for our quantization benchmarks.\n‚ú® Read our Qwen3-Coder Guide here!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\n‚ñ∂Ô∏è Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\n‚ñ∂Ô∏è Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen3-Coder-30B-A3B-Instruct\nHighlights\nQwen3-Coder is available in multiple sizes. Today, we're excited to introduce Qwen3-Coder-30B-A3B-Instruct. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:\nSignificant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks.\nLong-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding.\nAgentic Coding supporting for most platform such as Qwen Code, CLINE, featuring a specially designed function call format.\nModel Overview\nQwen3-Coder-30B-A3B-Instruct has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nWe advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Coding\nQwen3-Coder excels in tool calling capabilities.\nYou can simply define or use any tools as following example.\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\nreturn num ** 2\n# Define Tools\ntools=[\n{\n\"type\":\"function\",\n\"function\":{\n\"name\": \"square_the_number\",\n\"description\": \"output the square of the number.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"input_num\"],\n\"properties\": {\n'input_num': {\n'type': 'number',\n'description': 'input_num is a number that will be squared'\n}\n},\n}\n}\n}\n]\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n# Use a custom endpoint compatible with OpenAI API\nbase_url='http://localhost:8000/v1',  # api_base\napi_key=\"EMPTY\"\n)\nmessages = [{'role': 'user', 'content': 'square the number 1024'}]\ncompletion = client.chat.completions.create(\nmessages=messages,\nmodel=\"Qwen3-Coder-30B-A3B-Instruct\",\nmax_tokens=65536,\ntools=tools,\n)\nprint(completion.choice[0])\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05.\nAdequate Output Length: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "google/gemma-3-270m": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach, for the 4B, 12B, and 27B sizes.\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B and 270M sizes.\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context up to 128K tokens for the 4B, 12B, and 27B sizes,\nand 32K tokens for the 1B and 270M sizes per request, subtracting the\nrequest input tokens\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://arxiv.org/abs/2503.19786},\npublisher={Google DeepMind},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens,\nthe 1B with 2 trillion tokens, and the 270M with 6 trillion tokens. The\nknowledge cutoff date for the training data was August 2024. Here are the key\ncomponents:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation. Evaluation results marked\nwith IT are for instruction-tuned models. Evaluation results marked with\nPT are for pre-trained models.\nGemma 3 270M\nBenchmark\nn-shot\nGemma 3 PT 270M\nHellaSwag\n10-shot\n40.9\nBoolQ\n0-shot\n61.4\nPIQA\n0-shot\n67.7\nTriviaQA\n5-shot\n15.4\nARC-c\n25-shot\n29.0\nARC-e\n0-shot\n57.7\nWinoGrande\n5-shot\n52.0\nBenchmark\nn-shot\nGemma 3 IT 270m\nHellaSwag\n0-shot\n37.7\nPIQA\n0-shot\n66.2\nARC-c\n0-shot\n28.2\nWinoGrande\n0-shot\n52.3\nBIG-Bench Hard\nfew-shot\n26.7\nIF Eval\n0-shot\n51.2\nGemma 3 1B, 4B, 12B & 27B\nReasoning and factuality\nBenchmark\nn-shot\nGemma 3 IT 1B\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nGPQA Diamond\n0-shot\n19.2\n30.8\n40.9\n42.4\nSimpleQA\n0-shot\n2.2\n4.0\n6.3\n10.0\nFACTS Grounding\n-\n36.4\n70.1\n75.8\n74.9\nBIG-Bench Hard\n0-shot\n39.1\n72.2\n85.7\n87.6\nBIG-Bench Extra Hard\n0-shot\n7.2\n11.0\n16.3\n19.3\nIFEval\n0-shot\n80.2\n90.2\n88.9\n90.4\nBenchmark\nn-shot\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nn-shot\nGemma 3 IT 1B\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nMMLU (Pro)\n0-shot\n14.7\n43.6\n60.6\n67.5\nLiveCodeBench\n0-shot\n1.9\n12.6\n24.6\n29.7\nBird-SQL (dev)\n-\n6.4\n36.3\n47.9\n54.4\nMath\n0-shot\n48.0\n75.6\n83.8\n89.0\nHiddenMath\n0-shot\n15.8\n43.0\n54.5\n60.3\nMBPP\n3-shot\n35.2\n63.2\n73.0\n74.4\nHumanEval\n0-shot\n41.5\n71.3\n85.4\n87.8\nNatural2Code\n0-shot\n56.0\n70.3\n80.7\n84.5\nGSM8K\n0-shot\n62.8\n89.2\n94.4\n95.9\nBenchmark\nn-shot\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nn-shot\nGemma 3 IT 1B\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nGlobal-MMLU-Lite\n0-shot\n34.2\n54.5\n69.5\n75.1\nECLeKTic\n0-shot\n1.4\n4.6\n10.3\n16.7\nWMT24++\n0-shot\n35.9\n46.8\n51.6\n53.4\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nMMMU (val)\n48.8\n59.6\n64.9\nDocVQA\n75.8\n87.1\n86.6\nInfoVQA\n50.0\n64.9\n70.6\nTextVQA\n57.8\n67.7\n65.1\nAI2D\n74.8\n84.2\n84.5\nChartQA\n68.8\n75.7\n78.0\nVQAv2 (val)\n62.4\n71.6\n71.0\nMathVista (testmini)\n50.0\n62.9\n67.6\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "nvidia/DLER-R1-1.5B-Research": "Model Overview\nDescription:\nEvaluation Results:\nEnvironment Setup\nLicense/Terms of Use\nInference:\nCitation\nModel Overview\nDLER-R1-1.5B\nüöÄ The leading efficient reasoning model for cutting-edge research and development üåü\nDescription:\nDLER-Qwen-R1-1.5B is an ultra-efficient 1.5B open-weight reasoning model designed for challenging tasks such as mathematics, programming, and scientific problem-solving. It is trained with the DLER algorithm on agentica-org/DeepScaleR-Preview-Dataset. Compared to DeepSeek‚Äôs 1.5B model, DLER-Qwen-R1-1.5B achieves substantial efficiency gains, reducing the average response length by nearly 80% across diverse mathematical benchmarks with better accuracy.\nThis model is for research and development only.\nEvaluation Results:\nModel\nMATH\nLength\nAIME\nLength\nAMC\nLength\nMinerva\nLength\nOlympiad\nLength\nTotal Avg Length\nDeepseek-R1-1.5B\n84.31\n5500\n29.79\n16916\n61.97\n10967\n38.41\n7494\n44.07\n11620\n10499\nDLER-R1-1.5B\n86.95 (+2.64%)\n1652 (-70%)\n34.375 (+4.59%)\n3551 (-80%)\n70.48 (+8.51%)\n2537 (-77%)\n43.58 (+5.18%)\n2029 (-73%)\n48.314 (+4.24%)\n2563 (-78%)\n2466 (-77%)\nEnvironment Setup\npip install transformers==4.51.3\nInference:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForCausalLM.from_pretrained('nvidia/DLER-R1-1.5B-Research').to(device)\ntokenizer = AutoTokenizer.from_pretrained('nvidia/DLER-R1-1.5B-Research')\nmessages = [\n{\"role\": \"user\", \"content\": \"Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$\"+\" Let's think step by step and output the final answer within \\\\boxed{}.\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\noutputs = model.generate(\ntokenized_chat,\nmax_new_tokens=10000,\neos_token_id=tokenizer.eos_token_id\n)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLicense/Terms of Use\nNSCLv1\nCitation\nIf you find our model helpful, please cite the following paper:\n@article{liu2025dler,\ntitle={DLER: Doing Length pEnalty Right-Incentivizing More Intelligence per Token via Reinforcement Learning},\nauthor={Liu, Shih-Yang and Dong, Xin and Lu, Ximing and Diao, Shizhe and Liu, Mingjie and Chen, Min-Hung and Yin, Hongxu and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Choi, Yejin and others},\njournal={arXiv preprint arXiv:2510.15110},\nyear={2025}\n}",
    "nvidia/DLER-Llama-Nemotron-8B-Merge-Research": "Model Overview\nDescription:\nEvaluation Results:\nEnvironment Setup\nLicense/Terms of Use\nInference:\nEthical Considerations:\nCitation\nModel Overview\nDLER-Llama-Nemotron-8B-Merge\nüöÄ The leading efficient reasoning model for cutting-edge research and development üåü\nDescription:\nDLER-Llama-3.1-Nemotron-8B is an ultra-efficient 8B open-weight reasoning model designed for challenging tasks such as mathematics, programming, and scientific problem-solving. It is first trained with the DLER algorithm on agentica-org/DeepScaleR-Preview-Dataset and then enhanced using a weight-merging technique to merge with the base model to mitigate accuracy degradation. Compared to the Llama-3.1-Nemotron-8B model, DLER-Llama-Nemotron-8B-Merge achieves substantial efficiency gains, reducing the average response length by nearly 50% across diverse mathematical benchmarks without sacrificing accuracy.\nThis model is for research and development only.\nEvaluation Results:\nModel\nMATH\nLength\nAIME\nLength\nAMC\nLength\nMinerva\nLength\nOlympiad\nLength\nTotal Avg Length\nLlama-3.1-Nemotron-Nano-8B-v1\n95.4\n3069\n66.4\n9899\n88.25\n6228\n52.38\n4031\n64.33\n6755\n5996\nDLER-Llama-Nemotron-8B-Merge\n95.2\n1995\n66.7\n5013\n89.23\n3358\n53.19\n2301\n65.39\n3520\n3237 (-46%)\nEnvironment Setup\npip install transformers==4.51.3\nInference:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/DLER-Llama-Nemotron-8B-Merge-Research\")\nmodel = AutoModelForCausalLM.from_pretrained(\"nvidia/DLER-Llama-Nemotron-8B-Merge-Research\").to(device)\nmessages = [{\"role\": \"system\", \"content\": \"detailed thinking on\"}, {\"role\": \"user\", \"content\": \"Below is a math question. I want you to reason through the steps and then give a final answer. Your final answer should be in \\\\boxed{}.\\nQuestion: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$\"}]\ntokenized_chat = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\").to(model.device)\noutputs = model.generate(\ntokenized_chat,\nmax_new_tokens=10000,\neos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=False))\nLicense/Terms of Use\nNSCLv1\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nCitation\nIf you find our model helpful, please cite the following paper:\n@article{liu2025dler,\ntitle={DLER: Doing Length pEnalty Right-Incentivizing More Intelligence per Token via Reinforcement Learning},\nauthor={Liu, Shih-Yang and Dong, Xin and Lu, Ximing and Diao, Shizhe and Liu, Mingjie and Chen, Min-Hung and Yin, Hongxu and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Choi, Yejin and others},\njournal={arXiv preprint arXiv:2510.15110},\nyear={2025}\n}",
    "lodestones/Chroma1-Radiance": "Chroma Radiance Architecture\nChroma Radiance Architecture",
    "zai-org/GLM-4.6-FP8": "GLM-4.6-FP8\nModel Introduction\nInference\nRecommended Evaluation Parameters\nEvaluation\nGLM-4.6-FP8\nüëã Join our Discord community.\nüìñ Check out the GLM-4.6 technical blog, technical report(GLM-4.5), and Zhipu AI technical documentation.\nüìç Use GLM-4.6 API services on Z.ai API Platform.\nüëâ One click to GLM-4.6.\nModel Introduction\nCompared with GLM-4.5, GLM-4.6  brings several key improvements:\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code„ÄÅCline„ÄÅRoo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4.\nInference\nBoth GLM-4.5 and GLM-4.6 use the same inference method.\nyou can check our github for more detail.\nRecommended Evaluation Parameters\nFor general evaluations, we recommend using a sampling temperature of 1.0.\nFor code-related evaluation tasks (such as LCB), it is further recommended to set:\ntop_p = 0.95\ntop_k = 40\nEvaluation\nFor tool-integrated reasoning, please refer to this doc.\nFor search benchmark, we design a specific format for searching toolcall in thinking mode to support search agent, please refer to this. for the detailed template.",
    "nvidia/omni-embed-nemotron-3b": "Omni-Embed-Nemotron-3B\nDescription\nLicense/Terms of Use\nTeam\nCitation\nDeployment Geography\nUse Case\nRelease Date\nModel Architecture\nInput\nOutput\nUsage\nSoftware Integration:\nModel Version(s)\nTraining and Evaluation Datasets\nTraining Dataset\nEvaluation Dataset\nModel performance comparison on Video retrieval datasets (LPM and FineVideo) using NDCG@10 and NDCG@5 metrics:\nLPM performance (NDCG@10) modalities breakdown\nFineVideo performance (NDCG@10) modalities breakdown\nEvaluation of embedding models across text retrieval benchmarks. Results are reported using nDCG@10.\nEvaluation of baseline models and our models on ViDoRe V1 (as of September 30th). Results are presented using nDCG@5 metrics.\nInference:\nEthical Considerations\nBias\nExplainability\nPrivacy\nSafety And Security\nOmni-Embed-Nemotron-3B\nDescription\nNV-QwenOmni-Embed-3B-v1 is a versatile multimodal embedding model capable of encoding content across multiple modalities, including text, image, audio, and video, either individually or in combination, and supports retrieval using queries that can also be multimodal. It is designed to serve as a foundational component in multi-modal Retrieval-Augmented Generation (RAG) systems.\nThe foundational Qwen Omni model (Qwen/Qwen2.5-Omni-3B) is based on the Thinker-Talker architecture. We only leverage the Thinker component to encode and understand diverse modalities. In this implementation, we do not include the Talker component, as the model focuses on multimodal understanding rather than response generation.\nThis model is for research and development only.\nFor more technical details, please refer to our technical report: Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video\nLicense/Terms of Use\nGoverning Terms for nvidia/omni-embed-nemotron-3b model: NVIDIA OneWay Noncommercial License.\nADDITIONAL INFORMATION: Qwen RESEARCH LICENSE AGREEMENT\nThis project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use.\nTeam\nMengyao Xu\nGabriel Moreira\nRadek Osmulski\nRonay Ak\nYauhen Babakhin\nBo Liu\nEven Oldridge\nBenedikt Schifferer\nCorrespondence to Mengyao Xu (mengyaox@nvidia.com) and Benedikt Schifferer (bschifferer@nvidia.com)\nCitation\n@article{xu2025omni,\ntitle={Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video},\nauthor={Xu, Mengyao and Zhou, Wenfei and Babakhin, Yauhen and Moreira, Gabriel and Ak, Ronay and Osmulski, Radek and Liu, Bo and Oldridge, Even and Schifferer, Benedikt},\njournal={arXiv preprint arXiv:2510.03458},\nyear={2025}\n}\n@misc{moreira2025nvretrieverimprovingtextembedding,\ntitle={NV-Retriever: Improving text embedding models with effective hard-negative mining},\nauthor={Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge},\nyear={2025},\neprint={2407.15831},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2407.15831},\n}\nDeployment Geography\nGlobal\nUse Case\nNV-Omni-Embed is intended for researchers and developers building retrieval-based applications that require understanding and retrieve information across multiple modalities. It is particularly useful in multimodal RAG systems, where queries and documents may include combinations of text, images, audio, and videos. Potential applications include multimedia search engines, cross-modal retrieval systems, and conversational AI with rich input understanding.\nRelease Date\nHuggingface 10/1/2025 via [https://huggingface.co/nvidia/omni-embed-nemotron-3b]\nModel Architecture\nArchitecture Type: Transformer\nNetwork Architecture: Qwen/Qwen2.5-Omni-3B\nNV-QwenOmni-Embed-3B-v1 is a transformer-based multimodal embedding model built on top of the Thinker component from Qwen/Qwen2.5-Omni-3B. Unlike the original Thinker-Talker architecture, this model does not include the Talker module, as it is designed specifically for multimodal understanding and retrieval rather than response generation. Number of model parameters is 4.7B.\nThe model incorporates a vision encoder, an audio encoder, and a large language model (LLM) from the Qwen architecture to process diverse modalities. Unlike the Omni model, which interleaves audio and video tokens with TMRoPE, our retrieval encoder keeps the two streams separate. Audio and video are encoded independently, preserving their full temporal structure without interleaving. Our experiments show this design improves retrieval performance.\nNV-QwenOmni-Embed-3B-v1 is trained using a bi-encoder architecture where queries and candidate inputs are embedded independently. A contrastive learning objective is employed to align relevant query-content pairs while pushing apart unrelated ones in the shared embedding space.\nInput\nProperty\nQuery\nDocument\nInput Type\nText | Image | Audio | Video | Any combination\nText | Image | Audio | Video | Any combination\nInput Format\nList of strings, image tensors, audio arrays, or video clips\nList of text strings, images, audio, or video clips\nText\nImage\nVideo\nAudio\nInput Parameter\nstr, list[str], or pre-tokenized list[list[str]]; encoded to token IDs; per-sample 1D; batched 2D [batch, seq_len]\nPIL.Image, np.ndarray, or torch.Tensor; per-sample 3D; batched 4D\nnp.ndarray, torch.Tensor, list of frames per-sample 4D; batched 5D; or file (like .mp4)\n1D waveform (np.ndarray or torch.Tensor) per-sample 1D; batched 2D [batch, num_samples], or file\nOther Properties: The model's maximum context length is 32768 tokens.\nOutput\nOutput Type: Floats\nOutput Format: List of float arrays\nOutput Parameters: A tensor of floats equivalent to [batchsize x 2048]\nOther Properties Related to Output: Model outputs embedding vectors of dimension 2048 for each input.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nUsage\nThe model requires transformers version 4.51.3\npip install git+https://github.com/huggingface/transformers.git@v4.51.3-Qwen2.5-Omni-preview\nimport torch\nfrom qwen_omni_utils import process_mm_info\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoProcessor\nmodel_name_or_path = \"nvidia/omni-embed-nemotron-3b\"\nmodel = AutoModel.from_pretrained(\nmodel_name_or_path,\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ntrust_remote_code=True,\n)\nmodel = model.to(\"cuda:0\")\nmodel.eval()\ndocuments = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"passage: This is a passage to be embedded\"\n},\n{\n\"type\": \"video\",\n\"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"\n},\n{\n\"type\": \"audio\",\n\"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"\n}\n]\n},\n]\nprocessor = AutoProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\ndocuments_texts = processor.apply_chat_template(documents, add_generation_prompt=False, tokenize=False)\naudio, images, videos = process_mm_info(documents, use_audio_in_video=False)\nvideos_kwargs = {\n\"min_pixels\": 32*14*14,\n\"max_pixels\": 64*28*28,\n\"use_audio_in_video\": False,\n}\ntext_kwargs = {\n\"truncation\": True,\n\"padding\": True,\n\"max_length\": 204800,\n}\nbatch_dict = processor(\ntext=documents_texts,\nimages=images,\nvideos=videos,\naudio=audio,\nreturn_tensors=\"pt\",\ntext_kwargs=text_kwargs,\nvideos_kwargs=videos_kwargs,\naudio_kwargs={\"max_length\": 2048000},\n)\nbatch_dict = {k: v.to(model.device) for k, v in batch_dict.items()}\nlast_hidden_states = model(**batch_dict, output_hidden_states=True).hidden_states[-1]\n# Average Pooling\nattention_mask = batch_dict[\"attention_mask\"]\nlast_hidden_states_masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nembedding = last_hidden_states_masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\nembedding = F.normalize(embedding, dim=-1)\nprint(embedding)\nprint(embedding.shape)\nSoftware Integration:\nRuntime Engine(s): TensorRT, Triton\nSupported Hardware Microarchitecture Compatibility: A100 40GB, A100 80GB, H100 80GB\nSupported Operating System(s): Linux\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\nModel Version(s)\nNvidia Omni Embed Nemotron 3B\nShort name: omni-embed-nemotron-3b-v1\nTraining and Evaluation Datasets\nTraining Dataset\nData Modality:\nImage\nText\nImage Training Data Size: 1 Million to 1 Billion Images\nText Training Data Size: Less than a Billion Tokens\nThe model was trained on publicly available datasets, includingHotpotQA, MIRACL, Natural Questions (NQ), Stack Exchange, SQuAD, Tiger Math/Stack, DocMatix-IR, Vidore-ColPali-Training, and Wiki-SS-NQ.\nData Collection Method by dataset: Hybrid: Automated, Human, Synthetic\nLabeling Method by dataset: Hybrid: Automated, Human, Synthetic\nProperties: 1M samples from public datasets.\nEvaluation Dataset\nWe evaluate our model on multiple benchmarks covering different modalities. For text retrieval, we select some text retrieval datasets from MTEB. For image retrieval, evaluation is conducted on the public ViDoRe V1 dataset. Since no established video retrieval benchmarks exist, we construct two custom evaluation sets based on the LPM dataset and FineVideo. To provide fair comparison with the state-of-the-art text-only baselines, we use the speech-to-text transcripts released with FineVideo and the transcripts from LPM as the input corpus for standard text retrieval models.\nData Collection Method by dataset: Hybrid: Automated, Human, Synthetic\nLabeling Method by dataset: Hybrid: Automated, Human, Synthetic\nProperties: More details on ViDoRe V1 can be found on their leaderboard: Visual Document Retrieval Benchmark.\nModel performance comparison on Video retrieval datasets (LPM and FineVideo) using NDCG@10 and NDCG@5 metrics:\nModel\nNDCG@10 LPM\nNDCG@10 FineVideo\nNDCG@10 Avg\nNDCG@5 LPM\nNDCG@5 FineVideo\nNDCG@5 Avg\nQwen/Qwen3-Embedding-4B\n0.8634\n0.5405\n0.7020\n0.8518\n0.5264\n0.6891\nintfloat/multilingual-e5-large-instruct\n0.7952\n0.4456\n0.6204\n0.7759\n0.4300\n0.6030\nstella_en_1.5B_v5\n0.8522\n0.5359\n0.6941\n0.8404\n0.5206\n0.6805\nnvidia/omni-embed-nemotron-3b\n0.8465\n0.5662\n0.7064\n0.8355\n0.5486\n0.6921\nMultimodal retrieval performance across input modalities on LPM and FineVideo using NDCG@10. Baselines support text only; multimodal settings apply to Omni.\nLPM performance (NDCG@10) modalities breakdown\nModel\nText (Transcript+OCR)\nAudio-Only\nVideo-Only\nAudio+Video Fusion\nAudio+Video Separately\nQwen/Qwen3-Embedding-4B\n0.8634\nN/A\nN/A\nN/A\nN/A\nintfloat/multilingual-e5-large-instruct\n0.7952\nN/A\nN/A\nN/A\nN/A\nstella_en_1.5B_v5\n0.8522\nN/A\nN/A\nN/A\nN/A\nnvidia/omni-embed-nemotron-3b\n0.8636\n0.8238\n0.7365\n0.8373\n0.8465\nFineVideo performance (NDCG@10) modalities breakdown\nModel\nText (Transcript)\nAudio-Only\nVideo-Only\nAudio+Video Fusion\nAudio+Video Separately\nQwen/Qwen3-Embedding-4B\n0.5405\nN/A\nN/A\nN/A\nN/A\nintfloat/multilingual-e5-large-instruct\n0.4456\nN/A\nN/A\nN/A\nN/A\nstella_en_1.5B_v5\n0.5359\nN/A\nN/A\nN/A\nN/A\nnvidia/omni-embed-nemotron-3b\n0.6082\n0.5407\n0.4488\n0.4700\n0.5662\nEvaluation of embedding models across text retrieval benchmarks. Results are reported using nDCG@10.\nModel\nAvg.\nNQ\nFiQA-2018\nSciFact\nSCIDOCS\nArguAna\nNFCorpus\nQuora\nLegalBench-CorpLobby\nCQAdupGaming\nCQAdupUnix\nQwen/Qwen3-Embedding-4B\n0.6654\n0.6313\n0.6122\n0.7833\n0.3144\n0.7564\n0.4110\n0.8806\n0.9542\n0.7151\n0.5960\nintfloat/multilingual-e5-large-instruct\n0.5900\n0.6350\n0.4865\n0.7162\n0.1924\n0.5848\n0.3634\n0.8926\n0.9425\n0.6396\n0.4473\nstella_en_1.5B_v5\n0.6050\n0.7180\n0.5996\n0.8009\n0.2677\n0.5706\n0.4200\n0.9003\n0.9468\n0.5359\n0.2903\nnvidia/omni-embed-nemotron-3b\n0.6059\n0.6808\n0.5382\n0.7405\n0.2163\n0.5891\n0.3644\n0.8347\n0.9413\n0.6432\n0.5102\nEvaluation of baseline models and our models on ViDoRe V1 (as of September 30th). Results are presented using nDCG@5 metrics.\nModel\nSize (M)\nAvg.\nArxivQA\nDocVQA\nInfoVQA\nShift Project\nAI\nEnergy\nGov. Reports\nHealthcare\nTabFQuad\nTAT-DQA\nnvidia/llama-nemoretriever-colembed-1b-v1\n2418\n90.5\n87.6\n64.5\n93.6\n92.3\n100\n96.6\n96.7\n99.6\n94.3\n79.8\nnvidia/llama-nemoretriever-colembed-3b-v1\n4407\n91.0\n88.4\n66.2\n94.9\n90.7\n99.6\n96.6\n97.8\n99.3\n95.9\n80.6\nnomic-ai/colnomic-embed-multimodal-3b\n3000\n89.9\n88.2\n61.3\n92.8\n90.2\n96.3\n97.3\n96.6\n98.3\n94.5\n83.1\nvidore/colqwen2.5-v0.2\n3000\n89.6\n89.1\n63.5\n92.6\n88.0\n99.6\n95.8\n96.6\n98.0\n90.8\n82.1\nvidore/colqwen2-v1.0\n2210\n89.2\n88.0\n61.5\n92.5\n89.9\n99.0\n95.9\n95.5\n98.8\n89.0\n82.2\nvidore/colpali-v1.3\n2920\n84.7\n83.7\n58.7\n85.7\n76.5\n96.6\n94.6\n95.9\n97.4\n86.7\n70.7\nvidore/colpali-v1.2\n2920\n83.4\n77.9\n56.5\n82.4\n78.3\n97.5\n94.4\n94.9\n95.4\n88.4\n68.1\nnvidia/omni-embed-nemotron-3b\n4703\n85.7\n85.3\n59.2\n89.2\n78.6\n98.1\n93.5\n95.4\n95.8\n91.0\n69.7\nInference:\nAcceleration Engine: Not Applicable\nTest Hardware: A100 40GB, A100 80GB, H100 80GB\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report model quality, risk, security vulnerabilities or NVIDIA AI Concerns here.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nMulti-modality corpus and query embedding for question and answer retrieval.\nModel Type:\nTransformer encoder.\nIntended User:\nCreators of generative AI focused on conversational models, as well as users aiming to develop question-and-answer applications, can benefit from leveraging the dense retrieval technologies. These applications can efficiently handle large, multi-modal corpora, including images, text, videos, and audio.\nOutput:\nArray of float numbers (Dense vector for input content, which may include multi-modal corpora).\nDescribe how the model works:\nModel transforms the input into a dense vector representation.\nPerformance Metrics:\nAccuracy\nPotential Known Risks:\nThis model does not guarantee to always retrieve the correct corpus for a given query.\nLicensing & Terms of Use:\nGoverning Terms:Your use of the software container and model is governed by the NVIDIA Software and Model Evaluation LicenseAdditional Information:Qwen RESEARCH LICENSE AGREEMENT\nTechnical Limitations:\nThe model's max sequence length is 32768. Longer sequence inputs should be truncated.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nN/A\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNone\nPersonal data used to create this model?\nNone\nHow often is dataset reviewed?\nDataset is initially reviewed upon addition, and subsequent reviews are conducted as needed or upon request for changes.\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety And Security\nField\nResponse\nModel Application(s):\nMulti-modal Corpus Embedding for Retrieval. The model processes input from various modalities‚Äîtext, image, audio, and video‚Äîeither independently or in combination.\nUse Case Restrictions:\nGoverning Terms:Your use of the model is governed by the NVIDIA Open License Agreement. Additional Information: Qwen RESEARCH LICENSE AGREEMENT.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.\nDescribe the life critical impact (if present)\nNot applicable",
    "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8": "Qwen3-VL-235B-A22B-Instruct-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nCitation\nQwen3-VL-235B-A22B-Instruct-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-235B-A22B model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-235B-A22B-Instruct-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8": "Qwen3-VL-30B-A3B-Instruct-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nCitation\nQwen3-VL-30B-A3B-Instruct-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-30B-A3B-Instruct model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-30B-A3B-Instruct-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "batwBMW/Magma-R1": "üåü Overview\nüî• News\nüöÄ Updates\nüìä Results\nTable 1. Performance comparison of GUI agents on AndroidControl-Curated\nTable 2. Ablation analysis of the benchmark purification process on the Hard subset\nüöÄ Setup & Installation\nüß™ Evaluation\nüõ†Ô∏è Methodology\nSystematic Benchmark Purification: The AndroidControl-Curated Pipeline\nTraining Paradigm of Magma-R1: Optimization via GRPO\nüìö Citation Information\nüôè Acknowledgments\nAndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification\nThis is the official repository for the paper AndroidControl-Curated.\nüåü Overview\nOn-device virtual assistants like Siri and Google Assistant are increasingly pivotal, yet their capabilities are hamstrung by a reliance on rigid, developer-dependent APIs. GUI agents offer a powerful, API-independent alternative, but their adoption is hindered by the perception of poor performance, as ~3B parameter models score as low as 60% on benchmarks like AndroidControl, far from viability for real-world use.\nOur research reveals that issue lies not only with the models but with the benchmarks themselves. We identified notable shortcomings in AndroidControl, including ambiguities and factual errors, which systematically underrates agent capabilities. To address this critical oversight, we enhanced AndroidControl into AndroidControl-Curated, a refined version of the benchmark improved through a rigorous purification pipeline.\nOn this enhanced benchmark, state-of-the-art models achieve success rates nearing 80% on complex tasks, reflecting that on-device GUI agents are actually closer to practical deployment than previously thought. We also trained our new SOTA model, Magma-R1, on just 2,400 curated samples, which matches the performance of previous models trained on over 31,000 samples.\nOverview of our integrated pipeline for Magma-R1 training and AndroidControl-Curated creation.\nüî• News\nüî• 2025/10/21 Our paper \"AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification\" released.\nüöÄ Updates\n2025/10/21 The source code for AndroidControl-Curated and Magma-R1 has been released.\nüìä Results\nTable 1. Performance comparison of GUI agents on AndroidControl-Curated\nGrounding Accuracy (GA) for all models is evaluated using our proposed E_bbox. The best results are in bold, and the second best are underlined. \"-\" indicates results to be added.\nModel\nAndroidControl-Curated-Easy\nAndroidControl-Curated-Hard\nType (%)\nGrounding (%)\nSR (%)\nType (%)\nGrounding (%)\nSR (%)\nProprietary Models\nGPT-4o\n74.3\n0.0\n19.4\n66.3\n0.0\n20.8\nOpen-source Models\nOS-Atlas-4B\n91.9\n83.8\n80.6\n84.7\n73.8\n67.5\nUI-R1\n62.2\n93.6\n58.9\n54.4\n79.3\n43.6\nGUI-R1-3B\n69.5\n94.7\n67.1\n63.1\n80.3\n54.4\nGUI-R1-7B\n74.9\n95.9\n72.7\n66.5\n82.6\n57.5\nInfi-GUI-R1 (trained on 31k origin data)\n90.2\n93.7\n87.2\n78.5\n72.8\n70.7\nQwen3-VL-30B\n82.8\n80.7\n70.5\n85.9\n78.9\n70.0\nQwen3-VL-235B\n85.1\n82.9\n74.5\n88.2\n83.6\n76.5\nOurs\nMagma-R1\n91.3\n94.2\n88.0\n84.2\n84.8\n75.3\nTable 2. Ablation analysis of the benchmark purification process on the Hard subset\nSR Impr. (G) shows the SR gain from AndroidControl to AndroidControl-Curated-Box. SR Impr. (T) shows the SR gain from AndroidControl-Curated-Box to the final AndroidControl-Curated. Best results are in bold, second best are underlined.\nModel\nAndroidControl\nAndroidControl-Curated-Box\nAndroidControl-Curated\nType (%)\nGrounding (%)\nSR (%)\nType (%)\nGrounding (%)\nSR (%)\nSR Impr. (G)\nType (%)\nGrounding (%)\nSR (%)\nSR Impr. (T)\nGUI-R1-3B\n57.2\n59.0\n41.5\n59.3\n74.0\n49.4\n+7.9\n63.1\n80.3\n54.4\n+5.0\nGUI-R1-7B\n62.5\n65.1\n46.3\n63.3\n76.9\n53.2\n+6.9\n66.5\n82.6\n57.5\n+4.3\nInfi-GUI-R1\n77.0\n57.0\n59.0\n77.7\n69.5\n67.6\n+8.6\n78.5\n72.8\n70.7\n+3.1\nQwen3-VL-235B\n67.3\n78.3\n61.2\n82.9\n79.9\n71.7\n+10.5\n88.2\n83.6\n76.5\n+4.8\nMagma-R1\n78.2\n58.2\n57.6\n80.0\n77.1\n69.1\n+11.5\n84.2\n84.8\n75.3\n+6.2\nüöÄ Setup & Installation\nClone the repository:\ngit clone https://github.com/batechworks/AndroidControl_Curated.git\ncd AndroidControl_Curated\nInstall dependencies:\nWe recommend using a virtual environment (e.g., conda or venv).\npip install -r requirement.py\nüß™ Evaluation\nTo reproduce the results on AndroidControl-Curated:\nDownload the benchmark data:\nDownload the processed test set from Hugging Face and place it in the benchmark_resource/ directory. The directory should contain the following files:\nandroid_control_high_bbox.json\nandroid_control_high_point.json\nandroid_control_low_bbox.json\nandroid_control_low_point.json\nandroid_control_high_task-improved.json\nDownload the model:\nDownload the Magma-R1 model weights from Hugging Face and place them in your desired location.\nRun the evaluation script:\nExecute the following command, making sure to update the paths to your model and the benchmark image directory.\npython eval/evaluate_actions_androidControl_vllm.py \\\n--model_path /path/to/your/Magma-R1-model \\\n--save_name Your_Results.xlsx \\\n--image_dir /path/to/your/benchmark_images_directory\nüõ†Ô∏è Methodology\nOur methodology consists of two main parts:\nSystematic Benchmark Purification: The AndroidControl-Curated Pipeline\nStage 1: From Coordinate Matching to Intent Alignment in Grounding Evaluation\nReplace overly strict point-based matching with bounding-box-based intent alignment\nEvaluate whether predicted points fall within target UI element bounding boxes\nStage 2: Task-Level Correction via LLM-Human Collaboration\nHigh-risk sample identification via execution consensus failure\nAutomated causal attribution and correction with LLMs\nRigorous human expert verification\nTraining Paradigm of Magma-R1: Optimization via GRPO\nDense Rewards: Gaussian kernel-based grounding reward for continuous feedback\nBalanced Learning: Action type proportional optimization to address class imbalance\nEfficient Training: Generative REINFORCE with Policy Optimization (GRPO)\nüìö Citation Information\nIf you find this work useful, a citation to the following paper would be appreciated:\n@article{leung2025androidcontrolcurated,\ntitle={AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification},\nauthor={LEUNG Ho Fai (Kevin) and XI XiaoYan (Sibyl) and ZUO Fei (Eric)},\njournal={arXiv preprint arXiv:XXXX.XXXXX},\nyear={2025},\ninstitution={BMW ArcherMind Information Technology Co. Ltd. (BA TechWorks)}\n}\nüôè Acknowledgments\nWe thank the anonymous reviewers for their valuable feedback and suggestions. This work was made possible by the generous support of several organizations. We extend our sincere gratitude to ArcherMind for providing the high-performance computing resources essential for our experiments. We would also like to acknowledge the BMW Group for their significant administrative support. Furthermore, we are grateful to BA Techworks for invaluable technical support and collaboration throughout this project.",
    "IDEA-Research/Rex-Omni": "üöÄ Quick Start\nInstallation\n2. Quick Start: Using Rex-Omni for Detection\n3. Tutorials\nüìÑ License\nüîó Links\nüìß Contact\n7. Citation\nThis model is Rex-Omni, a 3B-parameter Multimodal Large Language Model (MLLM) presented in the paper \"Detect Anything via Next Point Prediction\". It is compatible with the Hugging Face transformers library and is licensed under the IDEA License 1.0.\nDetect Anything via Next Point Prediction\nRex-Omni is a 3B-parameter Multimodal Large Language Model (MLLM) that redefines object detection and a wide range of other visual perception tasks as a simple next-token prediction problem.\nüöÄ Quick Start\nInstallation\nconda create -n rexomni -m python=3.10\npip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu124\ngit clone https://github.com/IDEA-Research/Rex-Omni.git\ncd Rex-Omni\npip install -v -e .\n2. Quick Start: Using Rex-Omni for Detection\nfrom PIL import Image\nfrom rex_omni import RexOmniWrapper, RexOmniVisualize\n# Initialize model\nmodel = RexOmniWrapper(\nmodel_path=\"IDEA-Research/Rex-Omni\",\nbackend=\"transformers\"  # or \"vllm\"\n)\n# Load image\nimage = Image.open(\"your_image.jpg\")\n# Object Detection\nresults = model.inference(\nimages=image,\ntask=\"detection\",\ncategories=[\"person\", \"car\", \"dog\"]\n)\nresult = results[0]\n# 4) Visualize\nvis = RexOmniVisualize(\nimage=image,\npredictions=result[\"extracted_predictions\"],\nfont_size=20,\ndraw_width=5,\nshow_labels=True,\n)\nvis.save(\"visualize.jpg\")\n3. Tutorials\nWe provide a series of tutorials to help you get started with Rex-Omni.\nDetection Example\nPointing Example\nOCR Example\nKeypointing Example\nVisual Prompting Example\nBatch Inference Example\nüìÑ License\nRex-Omni is licensed under the IDEA License 1.0, Copyright (c) IDEA. All Rights Reserved. This model is based on Qwen, which is licensed under the Qwen RESEARCH LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\nüîó Links\nüè† Homepage\nüéÆ Demo\nüìß Contact\nFor questions and feedback, please contact us at:\nEmail: jiangqing@idea.edu.cn\nGitHub Issues: IDEA-Research/Rex-Omni\n7. Citation\nRex-Omni comes from a series of prior works. If you‚Äôre interested, you can take a look.\nRexThinker\nRexSeek\nChatRex\nDINO-X\nGrounidng DINO 1.5\nT-Rex2\nT-Rex\n@misc{jiang2025detectpointprediction,\ntitle={Detect Anything via Next Point Prediction},\nauthor={Qing Jiang and Junan Huo and Xingyu Chen and Yuda Xiong and Zhaoyang Zeng and Yihao Chen and Tianhe Ren and Junzhi Yu and Lei Zhang},\nyear={2025},\neprint={2510.12798},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.12798},\n}",
    "lapa-llm/lapa-v0.1.2-instruct": "Gemma 3 model card\nModel Information\nDescription\nKey Achievements\nBenchmark Results\nApplication Possibilities\nNext Steps\nAcknowledgment to Sponsors\nLinks:\nTeam\nInputs and outputs\nUsage\nCitation\nGemma 3 model card\nModel Information\nIntroducing Lapa LLM v0.1.2 ‚Äî the most efficient Ukrainian open-source language model\nDescription\nDemo page: https://huggingface.co/spaces/lapa-llm/lapaLink to Lapa Models: https://huggingface.co/collections/lapa-llm/lapa-v012-release\nTBD: Datasets would be published soon.\nToday, we proudly present Lapa LLM ‚Äî a cutting-edge open large language model based on Gemma-3-12B with a focus on Ukrainian language processing. The project is the result of many months of work by a team of Ukrainian researchers in artificial intelligence from the Ukrainian Catholic University, AGH University of Krakow, Igor Sikorsky Kyiv Polytechnic Institute, and Lviv Polytechnic, who united to create the best model for Ukrainian language processing.\nThe model is named in honor of Valentyn Lapa, who together with Oleksiy Ivakhnenko created the Group Method of Data Handling, which is a predecessor to Deep Learning (source).\nThe project's goal is to create the best model for Ukrainian language processing with open datasets for pretraining and instruction tuning.\nKey Achievements\nBest tokenizer for the Ukrainian language\nThanks to a SOTA method for tokenizer adaptation developed by Mykola Haltiuk as part of this project, it was possible to replace 80,000 tokens out of 250,000 with Ukrainian ones without loss of model quality, thus making Lapa LLM the fastest model for working with the Ukrainian language. Compared to the original Gemma 3, for working with Ukrainian, the model requires 1.5 times fewer tokens, thus performing three times fewer computations to achieve better results.\nMost efficient instruction-tuned model on the market\nOur instruction version of the model in some benchmark categories is only slightly behind the current leader ‚Äî MamayLM. The team is actively working on new datasets to further improve benchmark scores, which we plan to surpass in the v1.0 model.\nBenchmark Results\nBest English-to-Ukrainian translator with a result of 33 BLEU on FLORES and vice versa, which allows for natural and cost-effective translation of new NLP datasets into Ukrainian\nOne of the best models for image processing in Ukrainian in its size class, as measured on the MMZNO benchmark\nOne of the best models for Summarization and Q&A, which means excellent performance for RAG\nTests on propaganda and disinformation questions show the effectiveness of the filtering approach at the pretraining stage and during instruction fine-tuning\nModel measurements and comparisons will be published as part of the Ukrainian LLM Leaderboard project; subscribe to the Telegram channel for further news.\nLeader in pretraining results\nLapa LLM demonstrates the best performance in pretraining benchmarks for Ukrainian language processing, which opens opportunities for use by other researchers to adapt for their own tasks.\nThe model was trained on data evaluated by various quality assessment models - evaluation of propaganda and disinformation presence, readability, grammar assessment, etc. In the final stages of training, the model was trained on high-quality materials provided for commercial use by the Open Data division of Harvard Library.\nMaximum openness and transparency\nUnlike most available models, Lapa LLM is a maximally open project:\nThe model is available for commercial use\nApproximately 25 datasets for model training have been published\nMethods for filtering and processing data are disclosed, including for detecting disinformation and propaganda\nOpen source code for the model\nDocumentation of the training process is available\nThis openness allows for the development of the Ukrainian NLP community and helps businesses obtain a tool for the most efficient Ukrainian language processing in terms of both computation and results.\nApplication Possibilities\nLapa LLM opens wide possibilities for:\nProcessing sensitive documents without transferring data to external servers\nWorking with Ukrainian texts taking into account cultural and historical context without code-switching to Russian or other languages\nBuilding RAG systems and chatbots that write in proper Ukrainian\nDeveloping specialized solutions through the ability to fine-tune for specific tasks\nMachine translation with the best translation quality from English to Ukrainian and vice versa among all models, including API providers\nNext Steps\nComplete development of the reasoning model\nWe are collecting community feedback on the model's performance, so we look forward to receiving it on GitHub or HuggingFace!\nCollecting additional datasets for image processing in Ukrainian\nCollecting additional datasets for instruction following and programming\nAcknowledgment to Sponsors\nThe creation of Lapa LLM was made possible thanks to the support of our partners and sponsors, primarily the startup Comand.AI, which provided computational resources for training the model. We also want to thank the company ELEKS, which supported this project through a grant dedicated to the memory of Oleksiy Skrypnyk, and the startup HuggingFace, which provided a free corporate subscription to the team for storing models and datasets.\nLinks:\nTry the model: https://huggingface.co/spaces/lapa-llm/lapaCode: https://github.com/lapa-llm/lapa-llm\nSubscribe to the Telegram channel for further news about the project: https://t.me/pehade_blog\nTeam\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nYou can initialize the model and processor for inference with pipeline as follows.\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"lapa-llm/lapa-v0.1.2-instruct\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16\n)\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n{\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"lapa-llm/lapa-v0.1.2-instruct\"\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\nmodel_id, device_map=\"auto\"\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n{\"type\": \"text\", \"text\": \"–û–ø–∏—à–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è\"}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=True,\nreturn_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\nCitation\nTBD",
    "dx8152/White_film_to_rendering": "Welcome everyone to use Lora in Qwen-Edit for Image Fusion. It's an excellent performer!\n2509 is not very compatible with this lora, it is recommended to use Qwen-Edit\nTrigger Words:ÁôΩËÜúËΩ¨ÊùêË¥®\nInstructions: Download the Lora file to the models/loras folder.\nYou'll also need this Lora to use it: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main\nHere's a tutorial on using Lora: https://youtu.be/y9NWDq-yfkA\nFor communication/collaboration, please join our Discord group: https://discord.gg/Qbq3VdjK",
    "Falconsai/nsfw_image_detection": "Model Card: Fine-Tuned Vision Transformer (ViT) for NSFW Image Classification\nModel Description\nIntended Uses & Limitations\nIntended Uses\nHow to use\nLimitations\nTraining Data\nTraining Stats\nReferences\nModel Card: Fine-Tuned Vision Transformer (ViT) for NSFW Image Classification\nModel Description\nThe Fine-Tuned Vision Transformer (ViT) is a variant of the transformer encoder architecture, similar to BERT, that has been adapted for image classification tasks. This specific model, named \"google/vit-base-patch16-224-in21k,\" is pre-trained on a substantial collection of images in a supervised manner, leveraging the ImageNet-21k dataset. The images in the pre-training dataset are resized to a resolution of 224x224 pixels, making it suitable for a wide range of image recognition tasks.\nDuring the training phase, meticulous attention was given to hyperparameter settings to ensure optimal model performance. The model was fine-tuned with a judiciously chosen batch size of 16. This choice not only balanced computational efficiency but also allowed for the model to effectively process and learn from a diverse array of images.\nTo facilitate this fine-tuning process, a learning rate of 5e-5 was employed. The learning rate serves as a critical tuning parameter that dictates the magnitude of adjustments made to the model's parameters during training. In this case, a learning rate of 5e-5 was selected to strike a harmonious balance between rapid convergence and steady optimization, resulting in a model that not only learns swiftly but also steadily refines its capabilities throughout the training process.\nThis training phase was executed using a proprietary dataset containing an extensive collection of 80,000 images, each characterized by a substantial degree of variability. The dataset was thoughtfully curated to include two distinct classes, namely \"normal\" and \"nsfw.\" This diversity allowed the model to grasp nuanced visual patterns, equipping it with the competence to accurately differentiate between safe and explicit content.\nThe overarching objective of this meticulous training process was to impart the model with a deep understanding of visual cues, ensuring its robustness and competence in tackling the specific task of NSFW image classification. The result is a model that stands ready to contribute significantly to content safety and moderation, all while maintaining the highest standards of accuracy and reliability.\nIntended Uses & Limitations\nIntended Uses\nNSFW Image Classification: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.\nHow to use\nHere is how to use this model to classifiy an image based on 1 of 2 classes (normal,nsfw):\n# Use a pipeline as a high-level helper\nfrom PIL import Image\nfrom transformers import pipeline\nimg = Image.open(\"<path_to_image_file>\")\nclassifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\nclassifier(img)\n# Load model directly\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\nimg = Image.open(\"<path_to_image_file>\")\nmodel = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\nprocessor = ViTImageProcessor.from_pretrained('Falconsai/nsfw_image_detection')\nwith torch.no_grad():\ninputs = processor(images=img, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_label = logits.argmax(-1).item()\nmodel.config.id2label[predicted_label]\nRun Yolo Version\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport onnxruntime as ort\nimport json # Added import for json\n# Predict using YOLOv9 model\ndef predict_with_yolov9(image_path, model_path, labels_path, input_size):\n\"\"\"\nRun inference using the converted YOLOv9 model on a single image.\nArgs:\nimage_path (str): Path to the input image file.\nmodel_path (str): Path to the ONNX model file.\nlabels_path (str): Path to the JSON file containing class labels.\ninput_size (tuple): The expected input size (height, width) for the model.\nReturns:\nstr: The predicted class label.\nPIL.Image.Image: The original loaded image.\n\"\"\"\ndef load_json(file_path):\nwith open(file_path, \"r\") as f:\nreturn json.load(f)\n# Load labels\nlabels = load_json(labels_path)\n# Preprocess image\noriginal_image = Image.open(image_path).convert(\"RGB\")\nimage_resized = original_image.resize(input_size, Image.Resampling.BILINEAR)\nimage_np = np.array(image_resized, dtype=np.float32) / 255.0\nimage_np = np.transpose(image_np, (2, 0, 1))  # [C, H, W]\ninput_tensor = np.expand_dims(image_np, axis=0).astype(np.float32)\n# Load YOLOv9 model\nsession = ort.InferenceSession(model_path)\ninput_name = session.get_inputs()[0].name\noutput_name = session.get_outputs()[0].name # Assuming classification output\n# Run inference\noutputs = session.run([output_name], {input_name: input_tensor})\npredictions = outputs[0]\n# Postprocess predictions (assuming classification output)\n# Adapt this section if your model output is different (e.g., detection boxes)\npredicted_index = np.argmax(predictions)\npredicted_label = labels[str(predicted_index)] # Assumes labels are indexed by string numbers\nreturn predicted_label, original_image\n# Display prediction for a single image\ndef display_single_prediction(image_path, model_path, labels_path, input_size):\n\"\"\"\nPredicts the class for a single image and displays the image with its prediction.\nArgs:\nimage_path (str): Path to the input image file.\nmodel_path (str): Path to the ONNX model file.\nlabels_path (str): Path to the JSON file containing class labels.\ninput_size (tuple): The expected input size (height, width) for the model.\n\"\"\"\ntry:\n# Run prediction\nprediction, img = predict_with_yolov9(image_path, model_path, labels_path, input_size)\n# Display image and prediction\nfig, ax = plt.subplots(1, 1, figsize=(8, 8)) # Create a single plot\nax.imshow(img)\nax.set_title(f\"Prediction: {prediction}\", fontsize=14)\nax.axis(\"off\") # Hide axes ticks and labels\nplt.tight_layout()\nplt.show()\nexcept FileNotFoundError:\nprint(f\"Error: Image file not found at {image_path}\")\nexcept Exception as e:\nprint(f\"An error occurred: {e}\")\n# --- Main Execution ---\n# Paths and parameters - **MODIFY THESE**\nsingle_image_path = \"path/to/your/single_image.jpg\"  # <--- Replace with the actual path to your image file\nmodel_path = \"path/to/your/yolov9_model.onnx\"    # <--- Replace with the actual path to your ONNX model\nlabels_path = \"path/to/your/labels.json\"        # <--- Replace with the actual path to your labels JSON file\ninput_size = (224, 224)                         # Standard input size, adjust if your model differs\n# Check if the image file exists before proceeding (optional but recommended)\nif os.path.exists(single_image_path):\n# Run prediction and display for the single image\ndisplay_single_prediction(single_image_path, model_path, labels_path, input_size)\nelse:\nprint(f\"Error: The specified image file does not exist: {single_image_path}\")\nLimitations\nSpecialized Task Fine-Tuning: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.\nUsers interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.\nTraining Data\nThe model's training data includes a proprietary dataset comprising approximately 80,000 images. This dataset encompasses a significant amount of variability and consists of two distinct classes: \"normal\" and \"nsfw.\" The training process on this data aimed to equip the model with the ability to distinguish between safe and explicit content effectively.\nTraining Stats\n- 'eval_loss': 0.07463177293539047,\n- 'eval_accuracy': 0.980375,\n- 'eval_runtime': 304.9846,\n- 'eval_samples_per_second': 52.462,\n- 'eval_steps_per_second': 3.279\nNote: It's essential to use this model responsibly and ethically, adhering to content guidelines and applicable regulations when implementing it in real-world applications, particularly those involving potentially sensitive content.\nFor more details on model fine-tuning and usage, please refer to the model's documentation and the model hub.\nReferences\nHugging Face Model Hub\nVision Transformer (ViT) Paper\nImageNet-21k Dataset\nDisclaimer: The model's performance may be influenced by the quality and representativeness of the data it was fine-tuned on. Users are encouraged to assess the model's suitability for their specific applications and datasets.",
    "mistralai/Mistral-7B-Instruct-v0.2": "A newer version of this model is available:\nmistralai/Mistral-7B-Instruct-v0.3\nModel Card for Mistral-7B-Instruct-v0.2\nEncode and Decode with mistral_common\nInference with mistral_inference\nInference with hugging face transformers\nInstruction format\nTroubleshooting\nLimitations\nThe Mistral AI Team\nModel Card for Mistral-7B-Instruct-v0.2\nEncode and Decode with mistral_common\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nmistral_models_path = \"MISTRAL_MODELS_PATH\"\ntokenizer = MistralTokenizer.v1()\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nInference with mistral_inference\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nInference with hugging face transformers\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\nPRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mistral_common reference implementation are very welcome!\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n32k context window (vs 8k context in v0.1)\nRope-theta = 1e6\nNo Sliding-Window Attention\nFor full details of this model please read our paper and release blog post.\nInstruction format\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\nE.g.\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\nThis format is available as a chat template via the apply_chat_template() method:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmessages = [\n{\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n{\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n{\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\nTroubleshooting\nIf you see the following error:\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\nThis should not be required after transformers-v4.33.4.\nLimitations\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.",
    "litert-community/Gemma3-1B-IT": "Access Gemma3-1B-IT on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma3-1B-IT on Hugging Face, you are required to review and agree to the gemma license. To do this, please ensure you are logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nlitert-community/Gemma3-1B-IT\nUse the models\nColab\nCustomize\nAndroid via Google AI Edge Gallery and MediaPipe\nAndroid or Desktop via LiteRT LM\niOS via MediaPipe\nPerformance\nAndroid via Google AI Edge Gallery and MediaPipe\nAndroid via LiteRT LM\nWeb\nThis repository corresponds to Gemma 3 models. You can try it out with:\nGoogle AI Edge Gallery for Android through Open Beta in the Play Store\nGoogle AI Edge Gallery for Android through GitHub\nGoogle AI Studio\nlitert-community/Gemma3-1B-IT\nThis model provides a few variants of\ngoogle/Gemma-3-1B-IT that are ready for\ndeployment on Android using the\nLiteRT (fka TFLite) stack and\nMediaPipe LLM Inference API.\nUse the models\nColab\nDisclaimer: The target deployment surface for the LiteRT models is\nAndroid/iOS/Web and the stack has been optimized for performance on these\ntargets. Trying out the system in Colab is an easier way to familiarize yourself\nwith the LiteRT stack, with the caveat that the performance (memory and latency)\non Colab could be much worse than on a local device.\nCustomize\nFine tune Gemma 3 1B and deploy with either LiteRT or Mediapipe LLM Inference API:\nAndroid via Google AI Edge Gallery and MediaPipe\nDownload and install\nthe apk.\nFollow the instructions in the app.\nTo build the demo app from source, please follow the instructions\nfrom the GitHub repository.\nAndroid or Desktop via LiteRT LM\nFollow the LitRT LM instructions to build our Open Source LiteRT LM runtime to run LiteRT models.\niOS via MediaPipe\nClone the MediaPipe samples\nrepository and follow the instructions\nto build the LLM Inference iOS Sample App using XCode.\nRun the app via the iOS simulator or deploy to an iOS device.\nPerformance\nAndroid via Google AI Edge Gallery and MediaPipe\nNote that all benchmark stats are from a Samsung S24 Ultra and use models with multiple prefill signatures.\nBackend\nQuantization scheme\nContext length\nPrefill (tokens/sec)\nDecode (tokens/sec)\nTime-to-first-token (sec)\nCPU Memory (RSS in MB)\nGPU Memory (RSS in MB)\nModel size (MB)\nCPU\nfp32 (baseline)\n1280\n49 tk/s\n10 tk/s\n5.59 s\n4,123 MB\n3,824 MB\nüîó\ndynamic_int4 (block size 128)\n1280\n138 tk/s\n50 tk/s\n2.33 s\n982 MB\n657 MB\nüîó\n4096\n87 tk/s\n37 tk/s\n3.40 s\n1,145 MB\n657 MB\nüîó\ndynamic_int4 (block size 32)\n1280\n107 tk/s\n48 tk/s\n3.49 s\n1,045 MB\n688 MB\nüîó\n4096\n79 tk/s\n36 tk/s\n4.40 s\n1,210 MB\n688 MB\nüîó\ndynamic_int4 QAT\n2048\n322 tk/s\n47 tk/s\n3.10 s\n1,138 MB\n529 MB\nüîó\ndynamic_int8\n1280\n177 tk/s\n33 tk/s\n1.69 s\n1,341 MB\n1,005 MB\nüîó\n4096\n123 tk/s\n29 tk/s\n2.34 s\n1,504 MB\n1,005 MB\nüîó\nGPU\ndynamic_int4 QAT\n2048\n2585 tk/s\n56 tk/s\n4.50 s\n1,205 MB\n529 MB\nüîó\ndynamic_int8\n1280\n1191 tk/s\n24 tk/s\n4.68 s\n2,164 MB\n1,059 MB\n1,005 MB\nüîó\n4096\n814 tk/s\n24 tk/s\n4.99 s\n2,167 MB\n1,181 MB\n1,005 MB\nüîó\nFor the list of supported quantization schemes see supported-schemes.\nFor these models, we are using prefill signature lengths of 32, 128, 512 and 1280.\nModel Size: measured by the size of the .tflite flatbuffer (serialization\nformat for LiteRT models)\nMemory: indicator of peak RAM usage\nThe inference on CPU is accelerated via the LiteRT\nXNNPACK delegate with 4 threads\nBenchmark is run with cache enabled and initialized. During the first run,\nthe time to first token may differ.\nAndroid via LiteRT LM\nNote that all benchmark stats are from a Samsung S24 Ultra and use models with multiple prefill signatures.\nBackend\nQuantization scheme\nContext length\nPrefill (tokens/sec)\nDecode (tokens/sec)\nTime-to-first-token (sec)\nCPU Memory (RSS in MB)\nGPU Memory (RSS in MB)\nModel size (MB)\nCPU\ndynamic_int4 QAT\n2048\n379 tk/s\n55 tk/s\n1,009 MB\n529 MB\nüîó\nGPU\ndynamic_int4 QAT\n2048\n2531 tk/s\n49 tk/s\n1,205 MB\n529 MB\nüîó\nFor the list of supported quantization schemes see supported-schemes.\nFor these models, we are using prefill signature lengths of 32, 128, 512 and 1280.\nModel Size: measured by the size of the .tflite flatbuffer (serialization\nformat for LiteRT models)\nMemory: indicator of peak RAM usage\nThe inference on CPU is accelerated via the LiteRT\nXNNPACK delegate with 4 threads\nBenchmark is run with cache enabled and initialized. During the first run,\nthe time to first token may differ.\nAndroid via LiteRT LM with NPU\nNote that the benchmark stats are from a Samsung S25 Ultra and use models with 128 token prefill chunks towards 1024 tokens.\nBackend\nQuantization scheme\nContext length\nPrefill (tokens/sec)\nDecode (tokens/sec)\nTime-to-first-token (sec)\nCPU Memory (RSS in MB)\nGPU Memory (RSS in MB)\nModel size (MB)\nNPU\na16w4 QAT\n1280\n5836 tk/s\n85 tk/s\n626 MB\n689 MB\nüîó\nModel Size: measured by the size of the .tflite flatbuffer (serialization\nformat for LiteRT models)\nMemory: indicator of peak RAM usage from malloc.\nWeb\nNote that all benchmark stats are from a MacBook Pro 2024 (Apple M4 Max chip) running with 1280 KV cache size, 1024 tokens prefill, 256 tokens decode.\nBackend\nQuantization scheme\nPrecision\nPrefill (tokens/sec)\nDecode (tokens/sec)\nTime-to-first-token (sec)\nCPU Memory\nGPU Memory\nModel size (MB)\nGPU\ndynamic_int4\nF16\n4339 tk/s\n133 tk/s\n0.51 s\n460 MB\n1,331 MB\n700 MB\nüîó\nF32\n2837 tk/s\n134 tk/s\n0.49 s\n481 MB\n1,331 MB\n700 MB\nüîó\ndynamic_int4 QAT\nF16\n1702 tk/s\n77 tk/s\n529 MB\nüîó\ndynamic_int8\nF16\n4321 tk/s\n126 tk/s\n0.6 s\n471 MB\n1,740 MB\n1,011 MB\nüîó\nF32\n2805 tk/s\n129 tk/s\n0.58 s\n474 MB\n1,740 MB\n1,011 MB\nüîó\nModel size: measured by the size of the .tflite flatbuffer (serialization format for LiteRT models)\ndynamic_int4: quantized model with int4 weights and float activations.\ndynamic_int8: quantized model with int8 weights and float activations.\na16w4: quantized model with int4 weights and int16 activations.",
    "Qwen/Qwen3-1.7B": "Qwen3-1.7B\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nBest Practices\nCitation\nQwen3-1.7B\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-1.7B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 1.7B\nNumber of Paramaters (Non-Embedding): 1.4B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 16 for Q and 8 for KV\nContext Length: 32,768\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nIf you encounter significant endless repetitions, please refer to the Best Practices section for optimal sampling parameters, and set the presence_penalty to 1.5.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-1.7B --reasoning-parser qwen3\nvLLM:vllm serve Qwen/Qwen3-1.7B --enable-reasoning --reasoning-parser deepseek_r1\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by SGLang and vLLM.\nPlease refer to our documentation for SGLang and vLLM users.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-1.7B\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-1.7B',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "ByteDance/Dolphin": "Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting\nModel Description\nüìë Overview\nModel Architecture\nUsage\nLicense\nCitation\nAcknowledgements\nDolphin: Document Image Parsing via Heterogeneous Anchor Prompting\nModel Description\nDolphin (Document Image Parsing via Heterogeneous Anchor Prompting) is a novel multimodal document image parsing model that follows an analyze-then-parse paradigm. It addresses the challenges of complex document understanding through a two-stage approach designed to handle intertwined elements such as text paragraphs, figures, formulas, and tables.\nüìë Overview\nDocument image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:\nüîç Stage 1: Comprehensive page-level layout analysis by generating element sequence in natural reading order\nüß© Stage 2: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts\nDolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.\nModel Architecture\nDolphin is built on a vision-encoder-decoder architecture using transformers:\nVision Encoder: Based on Swin Transformer for extracting visual features from document images\nText Decoder: Based on MBart for decoding text from visual features\nPrompt-based interface: Uses natural language prompts to control parsing tasks\nThe model is implemented as a Hugging Face VisionEncoderDecoderModel for easy integration with the Transformers ecosystem.\nUsage\nOur demo will be released in these days. Please keep tuned! üî•\nPlease refer to our GitHub repository for detailed usage.\nPage-wise parsing: for an entire document image\nElement-wise parsing: for an element (paragraph, table, formula) image\nLicense\nThis model is released under the MIT License.\nCitation\n@inproceedings{dolphin2025,\ntitle={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},\nauthor={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and Tang, Jingqun and Liu, Hao and Huang, Can},\nyear={2025},\nbooktitle={Proceedings of the 65rd Annual Meeting of the Association for Computational Linguistics (ACL)}\n}\nAcknowledgements\nThis model builds on several open-source projects including:\nHugging Face Transformers\nDonut\nNougat\nSwin Transformer",
    "MiniMaxAI/MiniMax-M1-80k": "MiniMax-M1\n1. Model Overview\n2. Evaluation\nSWE-bench methodology\nTAU-bench methodology\n3. Recommendations for Minimax-M1 Model Usage\n3.1. Inference Parameters\n3.2. System Prompt\n4. Deployment Guide\n5. Function Calling\n6. Chatbot & API\n7. Citation\n8. Contact Us\nMiniMax-M1\n1. Model Overview\nWe introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.\nMiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning\nattention mechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters activated\nper token. Consistent with MiniMax-Text-01, the M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism\nin MiniMax-M1 enables efficient scaling of test-time compute ‚Äì For example, compared to DeepSeek\nR1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1\nparticularly suitable for complex tasks that require processing long inputs and thinking extensively.\nMiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems ranging from\ntraditional mathematical reasoning to sandbox-based, real-world software engineering environments.\nWe develop an efficient RL scaling framework for M1 highlighting two perspectives: (1) We propose\nCISPO, a novel algorithm that clips importance sampling weights instead of token updates, which\noutperforms other competitive RL variants; (2) Our hybrid-attention design naturally enhances the\nefficiency of RL, where we address unique challenges when scaling RL with the hybrid architecture. We\ntrain two versions of MiniMax-M1 models with 40K and\n80K thinking budgets respectively. Experiments\non standard benchmarks show that our models outperform other strong open-weight models such as\nthe original DeepSeek-R1 and Qwen3-235B, particularly on complex software engineering, tool using,\nand long context tasks. With efficient scaling of test-time compute, MiniMax-M1 serves as a strong\nfoundation for next-generation language model agents to reason and tackle real-world challenges.\nBenchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.\n2. Evaluation\nPerformance of MiniMax-M1 on core benchmarks.\nCategory\nTask\nMiniMax-M1-80K\nMiniMax-M1-40K\nQwen3-235B-A22B\nDeepSeek-R1-0528\nDeepSeek-R1\nSeed-Thinking-v1.5\nClaude 4 Opus\nGemini 2.5 Pro (06-05)\nOpenAI-o3\nExtended Thinking\n80K\n40K\n32k\n64k\n32k\n32k\n64k\n64k\n100k\nMathematics\nAIME 2024\n86.0\n83.3\n85.7\n91.4\n79.8\n86.7\n76.0\n92.0\n91.6\nAIME 2025\n76.9\n74.6\n81.5\n87.5\n70.0\n74.0\n75.5\n88.0\n88.9\nMATH-500\n96.8\n96.0\n96.2\n98.0\n97.3\n96.7\n98.2\n98.8\n98.1\nGeneral Coding\nLiveCodeBench (24/8~25/5)\n65.0\n62.3\n65.9\n73.1\n55.9\n67.5\n56.6\n77.1\n75.8\nFullStackBench\n68.3\n67.6\n62.9\n69.4\n70.1\n69.9\n70.3\n--\n69.3\nReasoning & Knowledge\nGPQA Diamond\n70.0\n69.2\n71.1\n81.0\n71.5\n77.3\n79.6\n86.4\n83.3\nHLE (no tools)\n8.4*\n7.2*\n7.6*\n17.7*\n8.6*\n8.2\n10.7\n21.6\n20.3\nZebraLogic\n86.8\n80.1\n80.3\n95.1\n78.7\n84.4\n95.1\n91.6\n95.8\nMMLU-Pro\n81.1\n80.6\n83.0\n85.0\n84.0\n87.0\n85.0\n86.0\n85.0\nSoftware Engineering\nSWE-bench Verified\n56.0\n55.6\n34.4\n57.6\n49.2\n47.0\n72.5\n67.2\n69.1\nLong Context\nOpenAI-MRCR (128k)\n73.4\n76.1\n27.7\n51.5\n35.8\n54.3\n48.9\n76.8\n56.5\nOpenAI-MRCR (1M)\n56.2\n58.6\n--\n--\n--\n--\n--\n58.8\n--\nLongBench-v2\n61.5\n61.0\n50.1\n52.1\n58.3\n52.5\n55.6\n65.0\n58.8\nAgentic Tool Use\nTAU-bench (airline)\n62.0\n60.0\n34.7\n53.5\n--\n44.0\n59.6\n50.0\n52.0\nTAU-bench (retail)\n63.5\n67.8\n58.6\n63.9\n--\n55.7\n81.4\n67.0\n73.9\nFactuality\nSimpleQA\n18.5\n17.9\n11.0\n27.8\n30.1\n12.9\n--\n54.0\n49.4\nGeneral Assistant\nMultiChallenge\n44.7\n44.7\n40.0\n45.0\n40.7\n43.0\n45.8\n51.8\n56.5\n* conducted on the text-only HLE subset.\nOur models are evaluated with temperature=1.0, top_p=0.95.\nSWE-bench methodology\nWe report results derived from the Agentless scaffold. Departing from the original pipeline, our methodology employs a two-stage localization process (without any embedding-based retrieval mechanisms): initial coarse-grained file localization followed by fine-grained localization to specific files and code elements. The values for our models are calculated on the subset of n=486 verified tasks which work on our infrastructure. The excluded 14 test cases that were incompatible with our internal infrastructure are:\n\"astropy__astropy-7606\",\n\"astropy__astropy-8707\",\n\"astropy__astropy-8872\",\n\"django__django-10097\",\n\"matplotlib__matplotlib-20488\",\n\"psf__requests-2317\",\n\"psf__requests-2931\",\n\"psf__requests-5414\",\n\"pylint-dev__pylint-6528\",\n\"pylint-dev__pylint-7277\",\n\"sphinx-doc__sphinx-10435\",\n\"sphinx-doc__sphinx-7985\",\n\"sphinx-doc__sphinx-8269\",\n\"sphinx-doc__sphinx-8475\"\nTAU-bench methodology\nWe evaluate TAU-Bench with GPT-4.1 as user model and without any custom tools. The maximum number of interaction steps is 40.\nOur general system prompt is:\n- In each round, you need to carefully examine the tools provided to you to determine if any can be used.\n- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies.\n3. Recommendations for Minimax-M1 Model Usage\nTo achieve the best results with the Minimax-M1 model, we suggest focusing on two key points: Inference Parameters and the System Prompt.\n3.1. Inference Parameters\nTemperature: 1.0\nTop_p: 0.95\nThis setting is optimal for encouraging creativity and diversity in the model's responses. It allows the model to explore a wider range of linguistic possibilities, preventing outputs that are too rigid or repetitive, while still maintaining strong logical coherence.\n3.2. System Prompt\nTailoring your system prompt to the specific task is crucial for guiding the model effectively. Below are suggested settings for different scenarios.\nA. General-Purpose Scenarios\nFor common tasks like summarization, translation, Q&A, or creative writing:\nYou are a helpful assistant.\nB. Web Development Scenarios\nFor complex tasks like generating code for web pages:\nYou are a web development engineer, writing web pages according to the instructions below. You are a powerful code editing assistant capable of writing code and creating artifacts in conversations with users, or modifying and updating existing artifacts as requested by users.\nAll code is written in a single code block to form a complete code file for display, without separating HTML and JavaScript code. An artifact refers to a runnable complete code snippet, you prefer to integrate and output such complete runnable code rather than breaking it down into several code blocks. For certain types of code, they can render graphical interfaces in a UI window. After generation, please check the code execution again to ensure there are no errors in the output.\nOutput only the HTML, without any additional descriptive text. Make the UI looks modern and beautiful.\nC. Mathematical Scenarios\nWhen dealing with problems that require calculation or logical deduction:\nPlease reason step by step, and put your final answer within \\boxed{}.\n4. Deployment Guide\nDownload the model from HuggingFace repository:\nMiniMax-M1-40k\nMiniMax-M1-80k\nFor production deployment, we recommend using vLLM to serve MiniMax-M1. vLLM provides excellent performance for serving large language models with the following features:\nüî• Outstanding service throughout performance\n‚ö° Efficient and intelligent memory management\nüì¶ Powerful batch request processing capability\n‚öôÔ∏è Deeply optimized underlying performance\nFor detailed vLLM deployment instructions, please refer to our vLLM Deployment Guide. Special Note: Using vLLM versions below 0.9.2 may result in incompatibility or incorrect precision for the model.\nAlternatively, you can also deploy using Transformers directly. For detailed Transformers deployment instructions, you can see our MiniMax-M1 Transformers Deployment Guide.\n5. Function Calling\nThe MiniMax-M1 model supports function calling capabilities, enabling the model to identify when external functions need to be called and output function call parameters in a structured format. MiniMax-M1 Function Call Guide provides detailed instructions on how to use the function calling feature of MiniMax-M1.\n6. Chatbot & API\nFor general use and evaluation, we provide a Chatbot with online search capabilities and the online API for developers. For general use and evaluation, we provide the MiniMax MCP Server with video generation, image generation, speech synthesis, and voice cloning for developers.\n7. Citation\n@misc{minimax2025minimaxm1scalingtesttimecompute,\ntitle={MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention},\nauthor={MiniMax},\nyear={2025},\neprint={2506.13585},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2506.13585},\n}\n8. Contact Us\nContact us at model@minimax.io."
}