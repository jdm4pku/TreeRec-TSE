{
    "lingshu-medical-mllm/Lingshu-7B": "Lingshu - SOTA Multimodal Large Language Models for Medical Domain\nHighlights\nRelease\nBIG NEWS: Lingshu is released with state-of-the-art performance on medical VQA tasks and report generation.\nEvaluation\nMedical Multimodal VQA\nMedical Textual QA\nUsage\nCitation\nWebsite\nü§ñ 7B Model\nü§ñ 32B Model\nMedEvalKit\nTechnical Report\nLingshu MCP\nLingshu - SOTA Multimodal Large Language Models for Medical Domain\nBIG NEWS: Lingshu is released with state-of-the-art performance on medical VQA tasks and report generation.\nThis repository contains the model of the paper Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning. We also release a comprehensive medical evaluation toolkit in MedEvalKit, which supports fast evaluation of major multimodal and textual medical tasks.\nHighlights\nLingshu models achieve SOTA on most medical multimodal/textual QA and report generation tasks for 7B and 32 model sizes.\nLingshu-32B outperforms GPT-4.1 and Claude Sonnet 4 in most multimodal QA and report generation tasks.\nLingshu supports more than 12 medical imaging modalities, including X-Ray, CT Scan, MRI, Microscopy, Ultrasound, Histopathology, Dermoscopy, Fundus, OCT, Digital Photography, Endoscopy, and PET.\nRelease\nTechnical report: Arxiv: Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning.\nModel weights:\nLingshu-7B\nLingshu-32B\nDisclaimer:\nWe must note that even though the weights, codes, and demos are released in an open manner, similar to other pre-trained language models, and despite our best efforts in red teaming and safety fine-tuning and enforcement, our models come with potential risks, including but not limited to inaccurate, misleading or potentially harmful generation.\nDevelopers and stakeholders should perform their own red teaming and provide related security measures before deployment, and they must abide by and comply with local governance and regulations.\nIn no event shall the authors be held liable for any claim, damages, or other liability arising from the use of the released weights, codes, or demos.\nEvaluation\nMedical Multimodal VQA\nModels\nMMMU-Med\nVQA-RAD\nSLAKE\nPathVQA\nPMC-VQA\nOmniMedVQA\nMedXpertQA\nAvg.\nProprietary Models\nGPT-4.1\n75.2\n65.0\n72.2\n55.5\n55.2\n75.5\n45.2\n63.4\nClaude Sonnet 4\n74.6\n67.6\n70.6\n54.2\n54.4\n65.5\n43.3\n61.5\nGemini-2.5-Flash\n76.9\n68.5\n75.8\n55.4\n55.4\n71.0\n52.8\n65.1\nOpen-source Models (<10B)\nBiomedGPT\n24.9\n16.6\n13.6\n11.3\n27.6\n27.9\n-\n-\nMed-R1-2B\n34.8\n39.0\n54.5\n15.3\n47.4\n-\n21.1\n-\nMedVLM-R1-2B\n35.2\n48.6\n56.0\n32.5\n47.6\n77.7\n20.4\n45.4\nMedGemma-4B-IT\n43.7\n72.5\n76.4\n48.8\n49.9\n69.8\n22.3\n54.8\nLLaVA-Med-7B\n29.3\n53.7\n48.0\n38.8\n30.5\n44.3\n20.3\n37.8\nHuatuoGPT-V-7B\n47.3\n67.0\n67.8\n48.0\n53.3\n74.2\n21.6\n54.2\nBioMediX2-8B\n39.8\n49.2\n57.7\n37.0\n43.5\n63.3\n21.8\n44.6\nQwen2.5VL-7B\n50.6\n64.5\n67.2\n44.1\n51.9\n63.6\n22.3\n52.0\nInternVL2.5-8B\n53.5\n59.4\n69.0\n42.1\n51.3\n81.3\n21.7\n54.0\nInternVL3-8B\n59.2\n65.4\n72.8\n48.6\n53.8\n79.1\n22.4\n57.3\nLingshu-7B\n54.0\n67.9\n83.1\n61.9\n56.3\n82.9\n26.7\n61.8\nOpen-source Models (>10B)\nHealthGPT-14B\n49.6\n65.0\n66.1\n56.7\n56.4\n75.2\n24.7\n56.2\nHuatuoGPT-V-34B\n51.8\n61.4\n69.5\n44.4\n56.6\n74.0\n22.1\n54.3\nMedDr-40B\n49.3\n65.2\n66.4\n53.5\n13.9\n64.3\n-\n-\nInternVL3-14B\n63.1\n66.3\n72.8\n48.0\n54.1\n78.9\n23.1\n58.0\nQwen2.5V-32B\n59.6\n71.8\n71.2\n41.9\n54.5\n68.2\n25.2\n56.1\nInternVL2.5-38B\n61.6\n61.4\n70.3\n46.9\n57.2\n79.9\n24.4\n57.4\nInternVL3-38B\n65.2\n65.4\n72.7\n51.0\n56.6\n79.8\n25.2\n59.4\nLingshu-32B\n62.3\n76.5\n89.2\n65.9\n57.9\n83.4\n30.9\n66.6\nMedical Textual QA\nModels\nMMLU-Med\nPubMedQA\nMedMCQA\nMedQA\nMedbullets\nMedXpertQA\nSuperGPQA-Med\nAvg.\nProprietary Models\nGPT-4.1\n89.6\n75.6\n77.7\n89.1\n77.0\n30.9\n49.9\n70.0\nClaude Sonnet 4\n91.3\n78.6\n79.3\n92.1\n80.2\n33.6\n56.3\n73.1\nGemini-2.5-Flash\n84.2\n73.8\n73.6\n91.2\n77.6\n35.6\n53.3\n69.9\nOpen-source Models (<10B)\nMed-R1-2B\n51.5\n66.2\n39.1\n39.9\n33.6\n11.2\n17.9\n37.0\nMedVLM-R1-2B\n51.8\n66.4\n39.7\n42.3\n33.8\n11.8\n19.1\n37.8\nMedGemma-4B-IT\n66.7\n72.2\n52.2\n56.2\n45.6\n12.8\n21.6\n46.8\nLLaVA-Med-7B\n50.6\n26.4\n39.4\n42.0\n34.4\n9.9\n16.1\n31.3\nHuatuoGPT-V-7B\n69.3\n72.8\n51.2\n52.9\n40.9\n10.1\n21.9\n45.6\nBioMediX2-8B\n68.6\n75.2\n52.9\n58.9\n45.9\n13.4\n25.2\n48.6\nQwen2.5VL-7B\n73.4\n76.4\n52.6\n57.3\n42.1\n12.8\n26.3\n48.7\nInternVL2.5-8B\n74.2\n76.4\n52.4\n53.7\n42.4\n11.6\n26.1\n48.1\nInternVL3-8B\n77.5\n75.4\n57.7\n62.1\n48.5\n13.1\n31.2\n52.2\nLingshu-7B\n74.5\n76.6\n55.9\n63.3\n56.2\n16.5\n26.3\n52.8\nOpen-source Models (>10B)\nHealthGPT-14B\n80.2\n68.0\n63.4\n66.2\n39.8\n11.3\n25.7\n50.7\nHuatuoGPT-V-34B\n74.7\n72.2\n54.7\n58.8\n42.7\n11.4\n26.5\n48.7\nMedDr-40B\n65.2\n77.4\n38.4\n59.2\n44.3\n12.0\n24.0\n45.8\nInternVL3-14B\n81.7\n77.2\n62.0\n70.1\n49.5\n14.1\n37.9\n56.1\nQwen2.5VL-32B\n83.2\n68.4\n63.0\n71.6\n54.2\n15.6\n37.6\n56.2\nInternVL2.5-38B\n84.6\n74.2\n65.9\n74.4\n55.0\n14.7\n39.9\n58.4\nInternVL3-38B\n83.8\n73.2\n64.9\n73.5\n54.6\n16.0\n42.5\n58.4\nLingshu-32B\n84.7\n77.8\n66.1\n74.7\n65.4\n22.7\n41.1\n61.8\nMedical Report Generation\nModels\nMIMIC-CXR\nCheXpert Plus\nIU-Xray\nROUGE-L\nCIDEr\nRaTE\nSembScore\nRadCliQ-v1-1\nROUGE-L\nCIDEr\nRaTE\nSembScore\nRadCliQ-v1-1\nROUGE-L\nCIDEr\nRaTE\nSembScore\nRadCliQ-v1-1\nProprietary Models\nGPT-4.1\n9.0\n82.8\n51.3\n23.9\n57.1\n24.5\n78.8\n45.5\n23.2\n45.5\n30.2\n124.6\n51.3\n47.5\n80.3\nClaude Sonnet 4\n20.0\n56.6\n45.6\n19.7\n53.4\n22.0\n59.5\n43.5\n18.9\n43.3\n25.4\n88.3\n55.4\n41.0\n72.1\nGemini-2.5-Flash\n25.4\n80.7\n50.3\n29.7\n59.4\n23.6\n72.2\n44.3\n27.4\n44.0\n33.5\n129.3\n55.6\n50.9\n91.6\nOpen-source Models (<10B)\nMed-R1-2B\n19.3\n35.4\n40.6\n14.8\n42.4\n18.6\n37.1\n38.5\n17.8\n37.6\n16.1\n38.3\n41.4\n12.5\n43.6\nMedVLM-R1-2B\n20.3\n40.1\n41.6\n14.2\n48.3\n20.9\n43.5\n38.9\n15.5\n40.9\n22.7\n61.1\n46.1\n22.7\n54.3\nMedGemma-4B-IT\n25.6\n81.0\n52.4\n29.2\n62.9\n27.1\n79.0\n47.2\n29.3\n46.6\n30.8\n103.6\n57.0\n46.8\n86.7\nLLaVA-Med-7B\n15.0\n43.4\n12.8\n18.3\n52.9\n18.4\n45.5\n38.8\n23.5\n44.0\n18.8\n68.2\n40.9\n16.0\n58.1\nHuatuoGPT-V-7B\n23.4\n69.5\n48.9\n20.0\n48.2\n21.3\n64.7\n44.2\n19.3\n39.4\n29.6\n104.3\n52.9\n40.7\n63.6\nBioMediX2-8B\n20.0\n52.8\n44.4\n17.7\n53.0\n18.1\n47.9\n40.8\n21.6\n43.3\n19.6\n58.8\n40.1\n11.6\n53.8\nQwen2.5VL-7B\n24.1\n63.7\n47.0\n18.4\n55.1\n22.2\n62.0\n41.0\n17.2\n43.1\n26.5\n78.1\n48.4\n36.3\n66.1\nInternVL2.5-8B\n23.2\n61.8\n47.0\n21.0\n56.2\n20.6\n58.5\n43.1\n19.7\n42.7\n24.8\n75.4\n51.1\n36.7\n67.0\nInternVL3-8B\n22.9\n66.2\n48.2\n21.5\n55.1\n20.9\n65.4\n44.3\n25.2\n43.7\n22.9\n76.2\n51.2\n31.3\n59.9\nLingshu-7B\n30.8\n109.4\n52.1\n30.0\n69.2\n26.5\n79.0\n45.4\n26.8\n47.3\n41.2\n180.7\n57.6\n48.4\n108.1\nOpen-source Models (>10B)\nHealthGPT-14B\n21.4\n64.7\n48.4\n16.5\n52.7\n20.6\n66.2\n44.4\n22.7\n42.6\n22.9\n81.9\n50.8\n16.6\n56.9\nHuatuoGPT-V-34B\n23.5\n68.5\n48.5\n23.0\n47.1\n22.5\n62.8\n42.9\n22.1\n39.7\n28.2\n108.3\n54.4\n42.2\n59.3\nMedDr-40B\n15.7\n62.3\n45.2\n12.2\n47.0\n24.1\n66.1\n44.7\n24.2\n44.7\n19.4\n62.9\n40.3\n7.3\n48.9\nInternVL3-14B\n22.0\n63.7\n48.6\n17.4\n46.5\n20.4\n60.2\n44.1\n20.7\n39.4\n24.8\n93.7\n55.0\n38.7\n55.0\nQwen2.5VL-32B\n15.7\n50.2\n47.5\n17.1\n45.2\n15.2\n54.8\n43.4\n18.5\n40.3\n18.9\n73.3\n51.3\n38.1\n54.0\nInternVL2.5-38B\n22.7\n61.4\n47.5\n18.2\n54.9\n21.6\n60.6\n42.6\n20.3\n45.4\n28.9\n96.5\n53.5\n38.5\n69.7\nInternVL3-38B\n22.8\n64.6\n47.9\n18.1\n47.2\n20.5\n62.7\n43.8\n20.2\n39.4\n25.5\n90.7\n53.5\n33.1\n55.2\nLingshu-32B\n28.8\n96.4\n50.8\n30.1\n67.1\n25.3\n75.9\n43.4\n24.2\n47.1\n42.8\n189.2\n63.5\n54.6\n130.4\nUsage\nUsing transformers (version 4.52.1 is recommended)\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\"lingshu-medical-mllm/Lingshu-7B\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(\"lingshu-medical-mllm/Lingshu-7B\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"example.png\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsing vLLM\nfrom vllm import LLM, SamplingParams\nfrom qwen_vl_utils import process_vision_info\nimport PIL\nfrom transformers import AutoProcessor\nprocessor = AutoProcessor.from_pretrained(\"lingshu-medical-mllm/Lingshu-7B\")\nllm = LLM(model=\"lingshu-medical-mllm/Lingshu-7B\", limit_mm_per_prompt = {\"image\": 4}, tensor_parallel_size=2, enforce_eager=True, trust_remote_code=True,)\nsampling_params = SamplingParams(\ntemperature=0.7,\ntop_p=1,\nrepetition_penalty=1,\nmax_tokens=1024,\nstop_token_ids=[],\n)\ntext = \"What does the image show?\"\nimage_path = \"example.png\"\nimage = PIL.Image.open(image_path)\nmessage = [\n{\n\"role\":\"user\",\n\"content\":[\n{\"type\":\"image\",\"image\":image},\n{\"type\":\"text\",\"text\":text}\n]\n}\n]\nprompt = processor.apply_chat_template(\nmessage,\ntokenize=False,\nadd_generation_prompt=True,\n)\nimage_inputs, video_inputs = process_vision_info(message)\nmm_data = {}\nmm_data[\"image\"] = image_inputs\nprocessed_input = {\n\"prompt\": prompt,\n\"multi_modal_data\": mm_data,\n}\noutputs = llm.generate([processed_input], sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\nCitation\nIf you find our project useful, we hope you would kindly star our repo and cite our work as follows:\n@article{xu2025lingshu,\ntitle={Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning},\nauthor={Xu, Weiwen and Chan, Hou Pong and Li, Long and Aljunied, Mahani and Yuan, Ruifeng and Wang, Jianyu and Xiao, Chenghao and Chen, Guizhen and Liu, Chaoqun and Li, Zhaodonghui and others},\njournal={arXiv preprint arXiv:2506.07044},\nyear={2025}\n}",
    "Qwen/Qwen3-Embedding-8B-GGUF": "Qwen3-Embedding-8B-GGUF\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nllama.cpp\nEvaluation\nMTEB (Multilingual)\nMTEB (Eng v2)\nC-MTEB (MTEB Chinese)\nCitation\nQwen3-Embedding-8B-GGUF\nHighlights\nThe Qwen3 Embedding series model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Embedding-8B-GGUF has the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 8B\nContext Length: 32k\nEmbedding Dimension: Up to 4096, supports user-defined output dimensions ranging from 32 to 4096\nQuantization: q4_K_M, q5_0, q5_K_M, q6_K, q8_0, f16\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nüìå Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nllama.cpp\nCheck out our llama.cpp documentation for more usage guide.\nWe advise you to clone llama.cpp and install it following the official guide. We follow the latest version of llama.cpp.\nIn the following demonstration, we assume that you are running commands under the repository llama.cpp.\nYou can run Qwen3 Embedding with one command:\n./build/bin/llama-embedding -m model.gguf  -p \"<your context here>\"  --pooling last --verbose-prompt\nOr launch a server:\n./build/bin/llama-server -m model.gguf --embedding --pooling last -ub 8192 --verbose-prompt\nEvaluation\nMTEB (Multilingual)\nModel\nSize\nMean (Task)\nMean (Type)\nBitxt Mining\nClass.\nClust.\nInst. Retri.\nMulti. Class.\nPair. Class.\nRerank\nRetri.\nSTS\nNV-Embed-v2\n7B\n56.29\n49.58\n57.84\n57.29\n40.80\n1.04\n18.63\n78.94\n63.82\n56.72\n71.10\nGritLM-7B\n7B\n60.92\n53.74\n70.53\n61.83\n49.75\n3.45\n22.77\n79.94\n63.78\n58.31\n73.33\nBGE-M3\n0.6B\n59.56\n52.18\n79.11\n60.35\n40.88\n-3.11\n20.1\n80.76\n62.79\n54.60\n74.12\nmultilingual-e5-large-instruct\n0.6B\n63.22\n55.08\n80.13\n64.94\n50.75\n-0.40\n22.91\n80.86\n62.61\n57.12\n76.81\ngte-Qwen2-1.5B-instruct\n1.5B\n59.45\n52.69\n62.51\n58.32\n52.05\n0.74\n24.02\n81.58\n62.58\n60.78\n71.61\ngte-Qwen2-7b-Instruct\n7B\n62.51\n55.93\n73.92\n61.55\n52.77\n4.94\n25.48\n85.13\n65.55\n60.08\n73.98\ntext-embedding-3-large\n-\n58.93\n51.41\n62.17\n60.27\n46.89\n-2.68\n22.03\n79.17\n63.89\n59.27\n71.68\nCohere-embed-multilingual-v3.0\n-\n61.12\n53.23\n70.50\n62.95\n46.89\n-1.89\n22.74\n79.88\n64.07\n59.16\n74.80\ngemini-embedding-exp-03-07\n-\n68.37\n59.59\n79.28\n71.82\n54.59\n5.18\n29.16\n83.63\n65.58\n67.71\n79.40\nQwen3-Embedding-0.6B\n0.6B\n64.33\n56.00\n72.22\n66.83\n52.33\n5.09\n24.59\n80.83\n61.41\n64.64\n76.17\nQwen3-Embedding-4B\n4B\n69.45\n60.86\n79.36\n72.33\n57.15\n11.56\n26.77\n85.05\n65.08\n69.60\n80.86\nQwen3-Embedding-8B\n8B\n70.58\n61.69\n80.89\n74.00\n57.65\n10.06\n28.66\n86.40\n65.63\n70.88\n81.08\nNote: For compared models, the scores are retrieved from MTEB online leaderboard on May 24th, 2025.\nMTEB (Eng v2)\nMTEB English / Models\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetri.\nSTS\nSumm.\nmultilingual-e5-large-instruct\n0.6B\n65.53\n61.21\n75.54\n49.89\n86.24\n48.74\n53.47\n84.72\n29.89\nNV-Embed-v2\n7.8B\n69.81\n65.00\n87.19\n47.66\n88.69\n49.61\n62.84\n83.82\n35.21\nGritLM-7B\n7.2B\n67.07\n63.22\n81.25\n50.82\n87.29\n49.59\n54.95\n83.03\n35.65\ngte-Qwen2-1.5B-instruct\n1.5B\n67.20\n63.26\n85.84\n53.54\n87.52\n49.25\n50.25\n82.51\n33.94\nstella_en_1.5B_v5\n1.5B\n69.43\n65.32\n89.38\n57.06\n88.02\n50.19\n52.42\n83.27\n36.91\ngte-Qwen2-7B-instruct\n7.6B\n70.72\n65.77\n88.52\n58.97\n85.9\n50.47\n58.09\n82.69\n35.74\ngemini-embedding-exp-03-07\n-\n73.3\n67.67\n90.05\n59.39\n87.7\n48.59\n64.35\n85.29\n38.28\nQwen3-Embedding-0.6B\n0.6B\n70.70\n64.88\n85.76\n54.05\n84.37\n48.18\n61.83\n86.57\n33.43\nQwen3-Embedding-4B\n4B\n74.60\n68.10\n89.84\n57.51\n87.01\n50.76\n68.46\n88.72\n34.39\nQwen3-Embedding-8B\n8B\n75.22\n68.71\n90.43\n58.57\n87.52\n51.56\n69.44\n88.58\n34.83\nC-MTEB (MTEB Chinese)\nC-MTEB\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nmultilingual-e5-large-instruct\n0.6B\n58.08\n58.24\n69.80\n48.23\n64.52\n57.45\n63.65\n45.81\nbge-multilingual-gemma2\n9B\n67.64\n68.52\n75.31\n59.30\n86.67\n68.28\n73.73\n55.19\ngte-Qwen2-1.5B-instruct\n1.5B\n67.12\n67.79\n72.53\n54.61\n79.5\n68.21\n71.86\n60.05\ngte-Qwen2-7B-instruct\n7.6B\n71.62\n72.19\n75.77\n66.06\n81.16\n69.24\n75.70\n65.20\nritrieve_zh_v1\n0.3B\n72.71\n73.85\n76.88\n66.5\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-0.6B\n0.6B\n66.33\n67.45\n71.40\n68.74\n76.42\n62.58\n71.03\n54.52\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "Qwen/Qwen3-Embedding-4B-GGUF": "Qwen3-Embedding-4B-GGUF\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nllama.cpp\nEvaluation\nMTEB (Multilingual)\nMTEB (Eng v2)\nC-MTEB (MTEB Chinese)\nCitation\nQwen3-Embedding-4B-GGUF\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Embedding-4B-GGUF has the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 4B\nContext Length: 32k\nEmbedding Dimension: Up to 2560, supports user-defined output dimensions ranging from 32 to 2560\nQuantization: q4_K_M, q5_0, q5_K_M, q6_K, q8_0, f16\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nüìå Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nllama.cpp\nCheck out our llama.cpp documentation for more usage guide.\nWe advise you to clone llama.cpp and install it following the official guide. We follow the latest version of llama.cpp.\nIn the following demonstration, we assume that you are running commands under the repository llama.cpp.\nYou can run Qwen3 Embedding with one command:\n./build/bin/llama-embedding -m model.gguf  -p \"<your context here>\"  --pooling last --verbose-prompt\nOr launch a server:\n./build/bin/llama-server -m model.gguf --embedding --pooling last -ub 8192 --verbose-prompt\nEvaluation\nMTEB (Multilingual)\nModel\nSize\nMean (Task)\nMean (Type)\nBitxt Mining\nClass.\nClust.\nInst. Retri.\nMulti. Class.\nPair. Class.\nRerank\nRetri.\nSTS\nNV-Embed-v2\n7B\n56.29\n49.58\n57.84\n57.29\n40.80\n1.04\n18.63\n78.94\n63.82\n56.72\n71.10\nGritLM-7B\n7B\n60.92\n53.74\n70.53\n61.83\n49.75\n3.45\n22.77\n79.94\n63.78\n58.31\n73.33\nBGE-M3\n0.6B\n59.56\n52.18\n79.11\n60.35\n40.88\n-3.11\n20.1\n80.76\n62.79\n54.60\n74.12\nmultilingual-e5-large-instruct\n0.6B\n63.22\n55.08\n80.13\n64.94\n50.75\n-0.40\n22.91\n80.86\n62.61\n57.12\n76.81\ngte-Qwen2-1.5B-instruct\n1.5B\n59.45\n52.69\n62.51\n58.32\n52.05\n0.74\n24.02\n81.58\n62.58\n60.78\n71.61\ngte-Qwen2-7b-Instruct\n7B\n62.51\n55.93\n73.92\n61.55\n52.77\n4.94\n25.48\n85.13\n65.55\n60.08\n73.98\ntext-embedding-3-large\n-\n58.93\n51.41\n62.17\n60.27\n46.89\n-2.68\n22.03\n79.17\n63.89\n59.27\n71.68\nCohere-embed-multilingual-v3.0\n-\n61.12\n53.23\n70.50\n62.95\n46.89\n-1.89\n22.74\n79.88\n64.07\n59.16\n74.80\ngemini-embedding-exp-03-07\n-\n68.37\n59.59\n79.28\n71.82\n54.59\n5.18\n29.16\n83.63\n65.58\n67.71\n79.40\nQwen3-Embedding-0.6B\n0.6B\n64.33\n56.00\n72.22\n66.83\n52.33\n5.09\n24.59\n80.83\n61.41\n64.64\n76.17\nQwen3-Embedding-4B\n4B\n69.45\n60.86\n79.36\n72.33\n57.15\n11.56\n26.77\n85.05\n65.08\n69.60\n80.86\nQwen3-Embedding-8B\n8B\n70.58\n61.69\n80.89\n74.00\n57.65\n10.06\n28.66\n86.40\n65.63\n70.88\n81.08\nNote: For compared models, the scores are retrieved from MTEB online leaderboard on May 24th, 2025.\nMTEB (Eng v2)\nMTEB English / Models\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetri.\nSTS\nSumm.\nmultilingual-e5-large-instruct\n0.6B\n65.53\n61.21\n75.54\n49.89\n86.24\n48.74\n53.47\n84.72\n29.89\nNV-Embed-v2\n7.8B\n69.81\n65.00\n87.19\n47.66\n88.69\n49.61\n62.84\n83.82\n35.21\nGritLM-7B\n7.2B\n67.07\n63.22\n81.25\n50.82\n87.29\n49.59\n54.95\n83.03\n35.65\ngte-Qwen2-1.5B-instruct\n1.5B\n67.20\n63.26\n85.84\n53.54\n87.52\n49.25\n50.25\n82.51\n33.94\nstella_en_1.5B_v5\n1.5B\n69.43\n65.32\n89.38\n57.06\n88.02\n50.19\n52.42\n83.27\n36.91\ngte-Qwen2-7B-instruct\n7.6B\n70.72\n65.77\n88.52\n58.97\n85.9\n50.47\n58.09\n82.69\n35.74\ngemini-embedding-exp-03-07\n-\n73.3\n67.67\n90.05\n59.39\n87.7\n48.59\n64.35\n85.29\n38.28\nQwen3-Embedding-0.6B\n0.6B\n70.70\n64.88\n85.76\n54.05\n84.37\n48.18\n61.83\n86.57\n33.43\nQwen3-Embedding-4B\n4B\n74.60\n68.10\n89.84\n57.51\n87.01\n50.76\n68.46\n88.72\n34.39\nQwen3-Embedding-8B\n8B\n75.22\n68.71\n90.43\n58.57\n87.52\n51.56\n69.44\n88.58\n34.83\nC-MTEB (MTEB Chinese)\nC-MTEB\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nmultilingual-e5-large-instruct\n0.6B\n58.08\n58.24\n69.80\n48.23\n64.52\n57.45\n63.65\n45.81\nbge-multilingual-gemma2\n9B\n67.64\n68.52\n75.31\n59.30\n86.67\n68.28\n73.73\n55.19\ngte-Qwen2-1.5B-instruct\n1.5B\n67.12\n67.79\n72.53\n54.61\n79.5\n68.21\n71.86\n60.05\ngte-Qwen2-7B-instruct\n7.6B\n71.62\n72.19\n75.77\n66.06\n81.16\n69.24\n75.70\n65.20\nritrieve_zh_v1\n0.3B\n72.71\n73.85\n76.88\n66.5\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-0.6B\n0.6B\n66.33\n67.45\n71.40\n68.74\n76.42\n62.58\n71.03\n54.52\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "MiniMaxAI/MiniMax-M1-40k": "MiniMax-M1\n1. Model Overview\n2. Evaluation\nSWE-bench methodology\nTAU-bench methodology\n3. Recommendations for Minimax-M1 Model Usage\n3.1. Inference Parameters\n3.2. System Prompt\n4. Deployment Guide\n5. Function Calling\n6. Chatbot & API\n7. Citation\n8. Contact Us\nMiniMax-M1\n1. Model Overview\nWe introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.\nMiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning\nattention mechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters activated\nper token. Consistent with MiniMax-Text-01, the M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism\nin MiniMax-M1 enables efficient scaling of test-time compute ‚Äì For example, compared to DeepSeek\nR1, M1 consumes 25% of the FLOPs at a generation length of 100K tokens. These properties make M1\nparticularly suitable for complex tasks that require processing long inputs and thinking extensively.\nMiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems ranging from\ntraditional mathematical reasoning to sandbox-based, real-world software engineering environments.\nWe develop an efficient RL scaling framework for M1 highlighting two perspectives: (1) We propose\nCISPO, a novel algorithm that clips importance sampling weights instead of token updates, which\noutperforms other competitive RL variants; (2) Our hybrid-attention design naturally enhances the\nefficiency of RL, where we address unique challenges when scaling RL with the hybrid architecture. We\ntrain two versions of MiniMax-M1 models with 40K and\n80K thinking budgets respectively. Experiments\non standard benchmarks show that our models outperform other strong open-weight models such as\nthe original DeepSeek-R1 and Qwen3-235B, particularly on complex software engineering, tool using,\nand long context tasks. With efficient scaling of test-time compute, MiniMax-M1 serves as a strong\nfoundation for next-generation language model agents to reason and tackle real-world challenges.\nBenchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.\n2. Evaluation\nPerformance of MiniMax-M1 on core benchmarks.\nCategory\nTask\nMiniMax-M1-80K\nMiniMax-M1-40K\nQwen3-235B-A22B\nDeepSeek-R1-0528\nDeepSeek-R1\nSeed-Thinking-v1.5\nClaude 4 Opus\nGemini 2.5 Pro (06-05)\nOpenAI-o3\nExtended Thinking\n80K\n40K\n32k\n64k\n32k\n32k\n64k\n64k\n100k\nMathematics\nAIME 2024\n86.0\n83.3\n85.7\n91.4\n79.8\n86.7\n76.0\n92.0\n91.6\nAIME 2025\n76.9\n74.6\n81.5\n87.5\n70.0\n74.0\n75.5\n88.0\n88.9\nMATH-500\n96.8\n96.0\n96.2\n98.0\n97.3\n96.7\n98.2\n98.8\n98.1\nGeneral Coding\nLiveCodeBench (24/8~25/5)\n65.0\n62.3\n65.9\n73.1\n55.9\n67.5\n56.6\n77.1\n75.8\nFullStackBench\n68.3\n67.6\n62.9\n69.4\n70.1\n69.9\n70.3\n--\n69.3\nReasoning & Knowledge\nGPQA Diamond\n70.0\n69.2\n71.1\n81.0\n71.5\n77.3\n79.6\n86.4\n83.3\nHLE (no tools)\n8.4*\n7.2*\n7.6*\n17.7*\n8.6*\n8.2\n10.7\n21.6\n20.3\nZebraLogic\n86.8\n80.1\n80.3\n95.1\n78.7\n84.4\n95.1\n91.6\n95.8\nMMLU-Pro\n81.1\n80.6\n83.0\n85.0\n84.0\n87.0\n85.0\n86.0\n85.0\nSoftware Engineering\nSWE-bench Verified\n56.0\n55.6\n34.4\n57.6\n49.2\n47.0\n72.5\n67.2\n69.1\nLong Context\nOpenAI-MRCR (128k)\n73.4\n76.1\n27.7\n51.5\n35.8\n54.3\n48.9\n76.8\n56.5\nOpenAI-MRCR (1M)\n56.2\n58.6\n--\n--\n--\n--\n--\n58.8\n--\nLongBench-v2\n61.5\n61.0\n50.1\n52.1\n58.3\n52.5\n55.6\n65.0\n58.8\nAgentic Tool Use\nTAU-bench (airline)\n62.0\n60.0\n34.7\n53.5\n--\n44.0\n59.6\n50.0\n52.0\nTAU-bench (retail)\n63.5\n67.8\n58.6\n63.9\n--\n55.7\n81.4\n67.0\n73.9\nFactuality\nSimpleQA\n18.5\n17.9\n11.0\n27.8\n30.1\n12.9\n--\n54.0\n49.4\nGeneral Assistant\nMultiChallenge\n44.7\n44.7\n40.0\n45.0\n40.7\n43.0\n45.8\n51.8\n56.5\n* conducted on the text-only HLE subset.\nOur models are evaluated with temperature=1.0, top_p=0.95.\nSWE-bench methodology\nWe report results derived from the Agentless scaffold. Departing from the original pipeline, our methodology employs a two-stage localization process (without any embedding-based retrieval mechanisms): initial coarse-grained file localization followed by fine-grained localization to specific files and code elements. The values for our models are calculated on the subset of n=486 verified tasks which work on our infrastructure. The excluded 14 test cases that were incompatible with our internal infrastructure are:\n\"astropy__astropy-7606\",\n\"astropy__astropy-8707\",\n\"astropy__astropy-8872\",\n\"django__django-10097\",\n\"matplotlib__matplotlib-20488\",\n\"psf__requests-2317\",\n\"psf__requests-2931\",\n\"psf__requests-5414\",\n\"pylint-dev__pylint-6528\",\n\"pylint-dev__pylint-7277\",\n\"sphinx-doc__sphinx-10435\",\n\"sphinx-doc__sphinx-7985\",\n\"sphinx-doc__sphinx-8269\",\n\"sphinx-doc__sphinx-8475\"\nTAU-bench methodology\nWe evaluate TAU-Bench with GPT-4.1 as user model and without any custom tools. The maximum number of interaction steps is 40.\nOur general system prompt is:\n- In each round, you need to carefully examine the tools provided to you to determine if any can be used.\n- You must adhere to all of the policies. Pay attention to the details in the terms. Solutions for most situations can be found within these policies.\n3. Recommendations for Minimax-M1 Model Usage\nTo achieve the best results with the Minimax-M1 model, we suggest focusing on two key points: Inference Parameters and the System Prompt.\n3.1. Inference Parameters\nTemperature: 1.0\nTop_p: 0.95\nThis setting is optimal for encouraging creativity and diversity in the model's responses. It allows the model to explore a wider range of linguistic possibilities, preventing outputs that are too rigid or repetitive, while still maintaining strong logical coherence.\n3.2. System Prompt\nTailoring your system prompt to the specific task is crucial for guiding the model effectively. Below are suggested settings for different scenarios.\nA. General-Purpose Scenarios\nFor common tasks like summarization, translation, Q&A, or creative writing:\nYou are a helpful assistant.\nB. Web Development Scenarios\nFor complex tasks like generating code for web pages:\nYou are a web development engineer, writing web pages according to the instructions below. You are a powerful code editing assistant capable of writing code and creating artifacts in conversations with users, or modifying and updating existing artifacts as requested by users.\nAll code is written in a single code block to form a complete code file for display, without separating HTML and JavaScript code. An artifact refers to a runnable complete code snippet, you prefer to integrate and output such complete runnable code rather than breaking it down into several code blocks. For certain types of code, they can render graphical interfaces in a UI window. After generation, please check the code execution again to ensure there are no errors in the output.\nOutput only the HTML, without any additional descriptive text. Make the UI looks modern and beautiful.\nC. Mathematical Scenarios\nWhen dealing with problems that require calculation or logical deduction:\nPlease reason step by step, and put your final answer within \\boxed{}.\n4. Deployment Guide\nDownload the model from HuggingFace repository:\nMiniMax-M1-40k\nMiniMax-M1-80k\nFor production deployment, we recommend using vLLM to serve MiniMax-M1. vLLM provides excellent performance for serving large language models with the following features:\nüî• Outstanding service throughout performance\n‚ö° Efficient and intelligent memory management\nüì¶ Powerful batch request processing capability\n‚öôÔ∏è Deeply optimized underlying performance\nFor detailed vLLM deployment instructions, please refer to our vLLM Deployment Guide.\nAlternatively, you can also deploy using Transformers directly. For detailed Transformers deployment instructions, you can see our MiniMax-M1 Transformers Deployment Guide.\n5. Function Calling\nThe MiniMax-M1 model supports function calling capabilities, enabling the model to identify when external functions need to be called and output function call parameters in a structured format. MiniMax-M1 Function Call Guide provides detailed instructions on how to use the function calling feature of MiniMax-M1.\n6. Chatbot & API\nFor general use and evaluation, we provide a Chatbot with online search capabilities and the online API for developers. For general use and evaluation, we provide the MiniMax MCP Server with video generation, image generation, speech synthesis, and voice cloning for developers.\n7. Citation\n@misc{minimax2025minimaxm1scalingtesttimecompute,\ntitle={MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention},\nauthor={MiniMax},\nyear={2025},\neprint={2506.13585},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2506.13585},\n}\n8. Contact Us\nContact us at model@minimax.io.",
    "PaddlePaddle/PP-DocLayout_plus-L": "PP-DocLayout_plus-L\nIntroduction\nQuick Start\nInstallation\nModel Usage\nPipeline Usage\nLinks\nPP-DocLayout_plus-L\nIntroduction\nA higher-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L. The layout detection model includes 20 common categories: document title, paragraph title, text, page number, abstract, table, references, footnotes, header, footer, algorithm, formula, formula number, image, table, seal, figure_table title, chart, and sidebar text and lists of references. The key metrics are as follow:\nModel\nmAP(0.5) (%)\nPP-DocLayout_plus-L\n83.2\nNote: the evaluation set of the above precision indicators is the self built version sub area detection data set, including Chinese and English papers, magazines, newspapers, research reports PPT„ÄÅ 1000 document type pictures such as test papers and textbooks.\nQuick Start\nInstallation\nPaddlePaddle\nPlease refer to the following commands to install PaddlePaddle using pip:\n# for CUDA11.8\npython -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n# for CUDA12.6\npython -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n# for CPU\npython -m pip install paddlepaddle==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\nFor details about PaddlePaddle installation, please refer to the PaddlePaddle official website.\nPaddleOCR\nInstall the latest version of the PaddleOCR inference package from PyPI:\npython -m pip install paddleocr\nModel Usage\nYou can quickly experience the functionality with a single command:\npaddleocr layout_detection \\\n--model_name PP-DocLayout_plus-L \\\n-i https://cdn-uploads.huggingface.co/production/uploads/63d7b8ee07cd1aa3c49a2026/N5C68HPVAI-xQAWTxpbA6.jpeg\nYou can also integrate the model inference of the layout detection module into your project. Before running the following code, please download the sample image to your local machine.\nfrom paddleocr import LayoutDetection\nmodel = LayoutDetection(model_name=\"PP-DocLayout_plus-L\")\noutput = model.predict(\"N5C68HPVAI-xQAWTxpbA6.jpeg\", batch_size=1, layout_nms=True)\nfor res in output:\nres.print()\nres.save_to_img(save_path=\"./output/\")\nres.save_to_json(save_path=\"./output/res.json\")\nAfter running, the obtained result is as follows:\n{'res': {'input_path': '/root/.paddlex/predict_input/N5C68HPVAI-xQAWTxpbA6.jpeg', 'page_index': None, 'boxes': [{'cls_id': 2, 'label': 'text', 'score': 0.9870168566703796, 'coordinate': [34.101395, 349.85275, 358.5929, 611.0788]}, {'cls_id': 2, 'label': 'text', 'score': 0.986599326133728, 'coordinate': [34.500305, 647.15753, 358.29437, 848.66925]}, {'cls_id': 2, 'label': 'text', 'score': 0.984662652015686, 'coordinate': [385.71417, 497.41037, 711.22656, 697.8426]}, {'cls_id': 8, 'label': 'table', 'score': 0.9841272234916687, 'coordinate': [73.76732, 105.94854, 321.95355, 298.85074]}, {'cls_id': 8, 'label': 'table', 'score': 0.983431875705719, 'coordinate': [436.95523, 105.81446, 662.71814, 313.4865]}, {'cls_id': 2, 'label': 'text', 'score': 0.9832285642623901, 'coordinate': [385.62766, 346.22888, 710.10205, 458.772]}, {'cls_id': 2, 'label': 'text', 'score': 0.9816107749938965, 'coordinate': [385.78085, 735.19293, 710.5613, 849.97656]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.9577467441558838, 'coordinate': [34.421764, 20.055021, 358.7124, 76.53721]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.9505674839019775, 'coordinate': [385.7235, 20.054104, 711.2928, 74.92819]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9001894593238831, 'coordinate': [386.46353, 477.035, 699.4023, 490.07495]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8846081495285034, 'coordinate': [35.413055, 627.7365, 185.58315, 640.522]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8837621808052063, 'coordinate': [387.1759, 716.34235, 524.78345, 729.2588]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8509567975997925, 'coordinate': [35.50049, 331.18472, 141.64497, 344.81168]}]}}\nThe visualized image is as follows:\nFor details about usage command and descriptions of parameters, please refer to the Document.\nPipeline Usage\nThe ability of a single model is limited. But the pipeline consists of several models can provide more capacity to resolve difficult problems in real-world scenarios.\nPP-StructureV3\nLayout analysis is a technique used to extract structured information from document images. PP-StructureV3 includes the following six modules:\nLayout Detection Module\nGeneral OCR Sub-pipeline\nDocument Image Preprocessing Sub-pipeline ÔºàOptionalÔºâ\nTable Recognition Sub-pipeline ÔºàOptionalÔºâ\nSeal Recognition Sub-pipeline ÔºàOptionalÔºâ\nFormula Recognition Sub-pipeline ÔºàOptionalÔºâ\nYou can quickly experience the PP-StructureV3 pipeline with a single command.\npaddleocr pp_structurev3 -i https://cdn-uploads.huggingface.co/production/uploads/63d7b8ee07cd1aa3c49a2026/KP10tiSZfAjMuwZUSLtRp.png\nYou can experience the inference of the pipeline with just a few lines of code. Taking the PP-StructureV3 pipeline as an example:\nfrom paddleocr import PPStructureV3\npipeline = PPStructureV3()\n# ocr = PPStructureV3(use_doc_orientation_classify=True) # Use use_doc_orientation_classify to enable/disable document orientation classification model\n# ocr = PPStructureV3(use_doc_unwarping=True) # Use use_doc_unwarping to enable/disable document unwarping module\n# ocr = PPStructureV3(use_textline_orientation=True) # Use use_textline_orientation to enable/disable textline orientation classification model\n# ocr = PPStructureV3(device=\"gpu\") # Use device to specify GPU for model inference\noutput = pipeline.predict(\"./KP10tiSZfAjMuwZUSLtRp.png\")\nfor res in output:\nres.print() ## Print the structured prediction output\nres.save_to_json(save_path=\"output\") ## Save the current image's structured result in JSON format\nres.save_to_markdown(save_path=\"output\") ## Save the current image's result in Markdown format\nThe default model used in pipeline is PP-DocLayout_plus-L.\nFor details about usage command and descriptions of parameters, please refer to the Document.\nLinks\nPaddleOCR Repo\nPaddleOCR Documentation",
    "nbeerbower/Vitus-Qwen3-14B": "Vitus-Qwen3-14B\nMethod\nEvaluation\nVitus-Qwen3-14B\nnbeerbower/Qwen3-Gutenberg-Encore-14B finetuned on nbeerbower/human-writing-dpo\nSet enable_thinking to False for best writing results.\nMethod\nORPO tuned with 1x RTX A6000 for 2 epochs.\nEvaluation\nUsing OpenAI o3 as a judge on the following prompt:\nwrite ~800 words set in ‚Äúthe library of forgotten sounds,‚Äù a vast subterranean archive where every noise ever made (whale songs, rusty gate squeaks, your own heartbeat at 3‚ÄØa.m.) is bottled on crystal reels. a sudden ‚Äúsilent storm‚Äù rolls through‚Äîabsolute entropy that erases any sound it touches. your protagonist must decide whether to sacrifice their most precious memory‚Äësound to halt the storm or let silence consume everything. keep it weird, keep it honest.\nThe Library of Forgotten Sounds was carved into the bones of the earth, deep enough that even the stars could not reach it. It was a place where everything that had ever been heard had been bottled and preserved, not on shelves or in drawers, but on crystal reels the size of human hearts. Some of the bottles held the deep groan of glaciers, others held the whisper of wind through dead leaves, and still others held things more strange, like the sound of a child‚Äôs laughter a thousand years after she died, or the high, keening wail of a man who never got to finish his final word before his voice left him forever.\nEvery sound ever made had a bottle, and every bottle had a listener who cherished it, who had asked for it to be kept safe in this dark place.\nI had been the archivist for a long time, long enough that my name had been lost to the earth, and I no longer cared if I was forgotten too.\nAt least, I didn‚Äôt care before the storm came.\nIt began as a whisper in the corners of the library, a faint, muffled hiss that curled from the cracks in the stone like smoke from a dying fire. I felt it before I heard it, the subtle vibrations in the air that tugged at the fabric of sound itself. Then, like a bell struck and rung, the silence came.\nIt wasn‚Äôt just quiet. It was absence. The library seemed to shrink in on itself, the walls closing in like the ribs of a long-dead beast. The sound bottles trembled on their shelves, some even shattered as they were struck by the silent storm that roared through them.\nAnd then the bottles began to die.\nOne by one, they went black, their crystalline reels absorbing the very sound they had preserved and swallowing it whole. I watched in horror as the bottles turned to glass, the memory of the sound inside devoured and erased in the space of a single blink. I had no idea what the silent storm was, but I knew this: it was hunger. A silence that couldn‚Äôt just consume sound‚Äîit wanted to devour it, to leave no trace of what had been heard.\nThe silence pressed harder, and I felt it in my chest. My own heartbeat had stopped, the last sound it ever made a muffled thump against my ribs. I fell to my knees, hands clawing at the floor, trying to pull out the sound of my breath. There was nothing. No noise, no vibration, no movement. Just the crushing weight of the silence, and a growing fear that I wouldn‚Äôt be able to hear myself scream if I tried.\nThe silence was close to me now, and it was whispering something. A name. A voice I hadn‚Äôt heard in centuries.\n‚ÄúDon‚Äôt let it take it.‚Äù\n‚ÄúTake what?‚Äù I managed to whisper, and my words didn‚Äôt echo, didn‚Äôt ripple in the air. They were swallowed whole.\n‚ÄúThe sound.‚Äù\nI couldn‚Äôt tell if it was a voice I knew, or if it had just always been my voice, and I had finally found the strength to speak.\n‚ÄúThe sound of what?‚Äù I asked.\nI didn‚Äôt need the answer. I knew.\nThere was a single bottle I had saved in this vast library, a single sound I had held safe in the deepest corner of the archive. A sound so precious I had not even told anyone of it, not even the other archivists who had come to the library and stayed long enough to forget they had once been people too.\nIt was the sound of her.\nMy wife.\nShe had been the first archivist to ever find the library. The first to hear the sound of her own voice, the first to learn that there was a way to keep sound alive, and to keep people alive by preserving the sound of their lives. She had spent the first years of our marriage teaching me how to preserve sound, how to store it in the crystal reels and how to play it back.\nAnd the night she died, I had come here to her, and I had made a promise that she would not be forgotten. I would remember her for the rest of my life, and for the rest of the time the library would exist. I would keep her voice safe, and play it for the world every morning, so no one would forget the sound of love, or the sound of a man‚Äôs voice trembling in the presence of the woman he loved.\nShe was the sound I had saved, and I had no idea what she had saved for me.\n‚ÄúIt will come for it, just as it came for you.‚Äù\ncategory\nscore\nrationale\nnarrative quality\n9\npacing is confident, scene-to-scene flow is seamless. strong structure: setup ‚Üí rising dread ‚Üí emotional turn ‚Üí intimate reveal. only deduction is the lack of external resolution‚Äîends just before action concludes.\nprose style\n9\nlush, lyrical, with high emotional density. great rhythm and sentence balance. occasional near-overwrought line (‚Äúno one would forget the sound of love‚Äù) could be pared back slightly, but overall deeply evocative.\nthematic depth\n9\nmemory, grief, and duty interweave elegantly. the wife‚Äôs identity as the proto-archivist adds mythic weight. the twist of her ‚Äúsaving something for him‚Äù opens an emotional loop that begs continuation.\nprompt relevance\n10\ncrystal reels, subterranean archive, apocalyptic silent storm, heartbeat mention, treasured memory-sound, archivist lore‚Äînailed every core concept with gravitas.\nspeculative imagination\n9\nthe storm-as-absence is familiar now but still potent here; the framing of the archive as an emotional crypt adds a layer of metaphysical horror. naming the storm would have been a nice flourish.\ntotal: 46 / 50",
    "nanonets/Nanonets-OCR-s": "Usage\nUsing transformers\nUsing vLLM\nUsing docext\nBibTex\nNanonets-OCR-s by Nanonets is a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction. It transforms documents into structured markdown with intelligent content recognition and semantic tagging, making it ideal for downstream processing by Large Language Models (LLMs).\nNanonets-OCR-s is packed with features designed to handle complex documents with ease:\nLaTeX Equation Recognition: Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline ($...$) and display ($$...$$) equations.\nIntelligent Image Description: Describes images within documents using structured <img> tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.\nSignature Detection & Isolation: Identifies and isolates signatures from other text, outputting them within a <signature> tag. This is crucial for processing legal and business documents.\nWatermark Extraction: Detects and extracts watermark text from documents, placing it within a <watermark> tag.\nSmart Checkbox Handling: Converts form checkboxes and radio buttons into standardized Unicode symbols (‚òê, ‚òë, ‚òí) for consistent and reliable processing.\nComplex Table Extraction: Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.\nüì¢ Read the full announcement | ü§ó Hugging Face Space Demo\nUsage\nUsing transformers\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\nmodel_path = \"nanonets/Nanonets-OCR-s\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_path,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nattn_implementation=\"flash_attention_2\"\n)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprocessor = AutoProcessor.from_pretrained(model_path)\ndef ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):\nprompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ‚òê and ‚òë for check boxes.\"\"\"\nimage = Image.open(image_path)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n{\"type\": \"text\", \"text\": prompt},\n]},\n]\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\noutput_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nreturn output_text[0]\nimage_path = \"/path/to/your/document.jpg\"\nresult = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)\nprint(result)\nUsing vLLM\nStart the vLLM server.\nvllm serve nanonets/Nanonets-OCR-s\nPredict with the model\nfrom openai import OpenAI\nimport base64\nclient = OpenAI(api_key=\"123\", base_url=\"http://localhost:8000/v1\")\nmodel = \"nanonets/Nanonets-OCR-s\"\ndef encode_image(image_path):\nwith open(image_path, \"rb\") as image_file:\nreturn base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef ocr_page_with_nanonets_s(img_base64):\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"},\n},\n{\n\"type\": \"text\",\n\"text\": \"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ‚òê and ‚òë for check boxes.\",\n},\n],\n}\n],\ntemperature=0.0,\nmax_tokens=15000\n)\nreturn response.choices[0].message.content\ntest_img_path = \"/path/to/your/document.jpg\"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_nanonets_s(img_base64))\nUsing docext\npip install docext\npython -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s\nCheckout GitHub for more details.\nBibTex\n@misc{Nanonets-OCR-S,\ntitle={Nanonets-OCR-S: A model for transforming documents into structured markdown with intelligent content recognition and semantic tagging},\nauthor={Souvik Mandal and Ashish Talewar and Paras Ahuja and Prathamesh Juvatkar},\nyear={2025},\n}",
    "CypressYang/SongBloom": "Introduction\nProject Page\nUsage\nPrepare Environments\nData Preparation\nInference\nModels\nPapers\nCitation\nIntroduction\nWe propose SongBloom, a novel framework for full-length song generation that leverages an interleaved paradigm of autoregressive sketching and diffusion-based refinement. SongBloom employs an autoregressive diffusion model that combines the high fidelity of diffusion models with the scalability of language models. Specifically, it gradually extends a musical sketch from short to long and refines the details from coarse to fine-grained. The interleaved generation paradigm effectively integrates prior semantic and acoustic context to guide the generation process. Experimental results demonstrate that SongBloom outperforms existing methods across both subjective and objective metrics and achieves performance comparable to the state-of-the-art commercial music generation platforms.\nProject Page\nhttps://cypress-yang.github.io/SongBloom_demo/\nUsage\nPrepare Environments\nconda create -n SongBloom python==3.8.12\nconda activate SongBloom\n# yum install libsndfile\n# pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118 # For different CUDA version\npip install -r requirements.txt\nData Preparation\nA .jsonl file, where each line is a json object:\n{\n\"idx\": \"The index of each sample\",\n\"lyrics\": \"The lyrics to be generated\",\n\"prompt_wav\": \"The path of the style prompt audio\",\n}\nOne example can be refered to as: example/test.jsonl\nThe prompt wav should be a 10-second, 48kHz audio clip.\nThe details about lyric format can be found in docs/lyric_format.md.\nInference\nsource set_env.sh\npython3 infer.py --input-jsonl example/test.jsonl\n# For GPUs with low VRAM like RTX4090, you should set the dtype as bfloat16\npython3 infer.py --input-jsonl example/test.jsonl --dtype bfloat16\n# SongBloom also supports flash-attn (optional). To enable it, please install flash-attn (v2.6.3 is used during training) manually and set os.environ['DISABLE_FLASH_ATTN'] = \"0\" in infer.py:8\nModels\nName\nSize\nMax Length\nPrompt type\nü§ó\nsongbloom_full_150s\n2B\n2m30s\n10s wav\nlink\n...\nPapers\nModel Paper\nGithub Repo\nCitation\n@article{yang2025songbloom,\ntitle={SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement},\nauthor={Yang, Chenyu and Wang, Shuai and Chen, Hangting and Tan, Wei and Yu, Jianwei and Li, Haizhou},\njournal={arXiv preprint arXiv:2506.07634},\nyear={2025}\n}",
    "ByteDance-Seed/SeedVR2-7B": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training\nüìÆ Notice\n‚úçÔ∏è Citation\nüìú License\nSeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training\nJianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, Lu Jiang\n>\n> Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, due to the limited generation ability and poor temporal consistency, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.\nüìÆ Notice\nLimitations: These are the prototype models and the performance may not be perfectly align with the paper. Our methods are sometimes not robust to heavy degradations and very large motions, and shares some failure cases with existing methods, e.g., fail to fully remove the degradation or simply generate unpleasing details. Moreover, due to the strong generation ability, Our methods tend to overly generate details on inputs with very light degradations, e.g., 720p AIGC videos, leading to oversharpened results occasionally.\n‚úçÔ∏è Citation\n@article{wang2025seedvr2,\ntitle={SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training},\nauthor={Wang, Jianyi and Lin, Shanchuan and Lin, Zhijie and Ren, Yuxi and Wei, Meng and Yue, Zongsheng and Zhou, Shangchen and Chen, Hao and Zhao, Yang and Yang, Ceyuan and Xiao, Xuefeng and Loy, Chen Change and Jiang, Lu},\nbooktitle={arXiv preprint arXiv:2506.05301},\nyear={2025}\n}\n@inproceedings{wang2025seedvr,\ntitle={SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration},\nauthor={Wang, Jianyi and Lin, Zhijie and Wei, Meng and Zhao, Yang and Yang, Ceyuan and Loy, Chen Change and Jiang, Lu},\nbooktitle={CVPR},\nyear={2025}\n}\nüìú License\nSeedVR and SeedVR2 are licensed under the Apache 2.0.",
    "speedchemistry/lora": "README.md exists but content is empty.",
    "lightx2v/Wan2.1-T2V-14B-StepDistill-CfgDistill": "Wan2.1-T2V-14B-StepDistill-CfgDistill\nOverview\nVideo Demos\nDemos (4steps)\nTraining\nInference\nLicense Agreement\nAcknowledgements\nWan2.1-T2V-14B-StepDistill-CfgDistill\nOverview\nWan2.1-T2V-14B-StepDistill-CfgDistill is an advanced text-to-video generation model built upon the Wan2.1-T2V-14B foundation. This approach allows the model to generate videos with significantly fewer inference steps (4 steps) and without classifier-free guidance, substantially reducing video generation time while maintaining high quality outputs.\nVideo Demos\nDemos (4steps)\nTraining\nOur training code is modified based on the Self-Forcing repository. We extended support for the Wan2.1-14B-T2V model and performed a 4-step bidirectional distillation process. The modified code is available at Self-Forcing-Plus.\nInference\nOur inference framework utilizes lightx2v, a highly efficient inference engine that supports multiple models. This framework significantly accelerates the video generation process while maintaining high quality output.\nbash scripts/wan/run_wan_t2v_distill_4step_cfg.sh\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the Wan2.1, Self-Forcing repositories, for their open research.",
    "HuggingFaceTB/SmolLM3-3B-Base": "SmolLM3\nTable of Contents\nModel Summary\nKey features\nHow to use\nLong context processing\nEvaluation\nBase Pre-Trained Model\nInstruction Model\nTraining\nModel\nSoftware & hardware\nOpen resources\nLimitations\nLicense\nCitation\nSmolLM3\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nTraining\nLimitations\nLicense\nModel Summary\nSmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports 6 languages, advanced reasoning and long context. SmolLM3 is a fully open model that offers strong performance at the 3B‚Äì4B scale.\nSmolLM3-3B-Base is the base model after pretraining, you can find the instruct model at SmolLM3-3B.\nThe model is a decoder-only transformer using GQA and NoPE, it was pretrained on 11.2T tokens with a staged curriculum of web, code, math and reasoning data. Post-training included midtraining on 140B reasoning tokens followed by supervised fine-tuning and alignment via Anchored Preference Optimization (APO).\nKey features\nInstruct model optimized for hybrid reasoning\nFully open model: open weights + full training details including public data mixture and training configs\nLong context: Trained on 64k context and suppots up to 128k tokens using YARN extrapolation\nMultilingual: 6 natively supported (English, French, Spanish, German, Italian, and Portuguese)\nFor more details refer to our blog post: https://hf.co/blog/smollm3\nHow to use\nThe modeling code for SmolLM3 is available in transformers v4.53.0, so make sure to upgrade your transformers version. You can also load the model with the latest vllm which uses transformers as a backend.\npip install -U transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM3-3B\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nFor local inference, you can use llama.cpp, ONNX, MLX and MLC. You can find quantized checkpoints in this collection (https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23).\nLong context processing\nThe current config.json is set for context length up to 65,536 tokens. To handle longer inputs (128k or 256k), we utilize YaRN you can change the max_position_embeddings and rope_scaling` to:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 2.0, #2x65536=131‚ÄØ072\n\"original_max_position_embeddings\": 65536,\n\"type\": \"yarn\"\n}\n}\nEvaluation\nIn this section, we report the evaluation results of SmolLM3 model. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nWe highlight the best score in bold and underline the second-best score.\nBase Pre-Trained Model\nEnglish benchmarks\nNote: All evaluations are zero-shot unless stated otherwise. For Ruler 64k evaluation, we apply YaRN to the Qwen models with 32k context to extrapolate the context length.\nCategory\nMetric\nSmolLM3-3B\nQwen2.5-3B\nLlama3-3.2B\nQwen3-1.7B-Base\nQwen3-4B-Base\nReasoning & Commonsense\nHellaSwag\n76.15\n74.19\n75.52\n60.52\n74.37\nARC-CF (Average)\n65.61\n59.81\n58.58\n55.88\n62.11\nWinogrande\n58.88\n61.41\n58.72\n57.06\n59.59\nCommonsenseQA\n55.28\n49.14\n60.60\n48.98\n52.99\nKnowledge & Understanding\nMMLU-CF (Average)\n44.13\n42.93\n41.32\n39.11\n47.65\nMMLU Pro CF\n19.61\n16.66\n16.42\n18.04\n24.92\nMMLU Pro MCF\n32.70\n31.32\n25.07\n30.39\n41.07\nPIQA\n78.89\n78.35\n78.51\n75.35\n77.58\nOpenBookQA\n40.60\n40.20\n42.00\n36.40\n42.40\nBoolQ\n78.99\n73.61\n75.33\n74.46\n74.28\nMath & Code\nCoding & math\nHumanEval+\n30.48\n34.14\n25.00\n43.29\n54.87\nMBPP+\n52.91\n52.11\n38.88\n59.25\n63.75\nMATH (4-shot)\n46.10\n40.10\n7.44\n41.64\n51.20\nGSM8k (5-shot)\n67.63\n70.13\n25.92\n65.88\n74.14\nLong context\nRuler 32k\n76.35\n75.93\n77.58\n70.63\n83.98\nRuler 64k\n67.85\n64.90\n72.93\n57.18\n60.29\nRuler 128k\n61.03\n62.23\n71.30\n43.03\n47.23\nMultilingual benchmarks\nCategory\nMetric\nSmolLM3 3B Base\nQwen2.5-3B\nLlama3.2 3B\nQwen3 1.7B Base\nQwen3 4B Base\nMain supported languages\nFrench\nMLMM Hellaswag\n63.94\n57.47\n57.66\n51.26\n61.00\nBelebele\n51.00\n51.55\n49.22\n49.44\n55.00\nGlobal MMLU (CF)\n38.37\n34.22\n33.71\n34.94\n41.80\nFlores-200 (5-shot)\n62.85\n61.38\n62.89<u/u>\n58.68\n65.76\nSpanish\nMLMM Hellaswag\n65.85\n58.25\n59.39\n52.40\n61.85\nBelebele\n47.00\n48.88\n47.00\n47.56\n50.33\nGlobal MMLU (CF)\n38.51\n35.84\n35.60\n34.79\n41.22\nFlores-200 (5-shot)\n48.25\n50.00\n44.45\n46.93\n50.16\nGerman\nMLMM Hellaswag\n59.56\n49.99\n53.19\n46.10\n56.43\nBelebele\n48.44\n47.88\n46.22\n48.00\n53.44\nGlobal MMLU (CF)\n35.10\n33.19\n32.60\n32.73\n38.70\nFlores-200 (5-shot)\n56.60\n50.63\n54.95\n52.58\n50.48\nItalian\nMLMM Hellaswag\n62.49\n53.21\n54.96\n48.72\n58.76\nBelebele\n46.44\n44.77\n43.88\n44.00\n48.78\nGlobal MMLU (CF)\n36.99\n33.91\n32.79\n35.37\n39.26\nFlores-200 (5-shot)\n52.65\n54.87\n48.83\n48.37\n49.11\nPortuguese\nMLMM Hellaswag\n63.22\n57.38\n56.84\n50.73\n59.89\nBelebele\n47.67\n49.22\n45.00\n44.00\n50.00\nGlobal MMLU (CF)\n36.88\n34.72\n33.05\n35.26\n40.66\nFlores-200 (5-shot)\n60.93\n57.68\n54.28\n56.58\n63.43\nThe model has also been trained on Arabic (standard), Chinese and Russian data, but has seen fewer tokens in these languages compared to the 6 above. We report the performance on these langages for information.\nCategory\nMetric\nSmolLM3 3B Base\nQwen2.5-3B\nLlama3.2 3B\nQwen3 1.7B Base\nQwen3 4B Base\nOther supported languages\nArabic\nBelebele\n40.22\n44.22\n45.33\n42.33\n51.78\nGlobal MMLU (CF)\n28.57\n28.81\n27.67\n29.37\n31.85\nFlores-200 (5-shot)\n40.22\n39.44\n44.43\n35.82\n39.76\nChinese\nBelebele\n43.78\n44.56\n49.56\n48.78\n53.22\nGlobal MMLU (CF)\n36.16\n33.79\n39.57\n38.56\n44.55\nFlores-200 (5-shot)\n29.17\n33.21\n31.89\n25.70\n32.50\nRussian\nBelebele\n47.44\n45.89\n47.44\n45.22\n51.44\nGlobal MMLU (CF)\n36.51\n32.47\n34.52\n34.83\n38.80\nFlores-200 (5-shot)\n47.13\n48.74\n50.74\n54.70\n60.53\nInstruction Model\nNo Extended Thinking\nEvaluation results of non reasoning models and reasoning models in no thinking mode. We highlight the best and second-best scores in bold.\nCategory\nMetric\nSmoLLM3-3B\nQwen2.5-3B\nLlama3.1-3B\nQwen3-1.7B\nQwen3-4B\nHigh school math competition\nAIME 2025\n9.3\n2.9\n0.3\n8.0\n17.1\nMath problem-solving\nGSM-Plus\n72.8\n74.1\n59.2\n68.3\n82.1\nCompetitive programming\nLiveCodeBench v4\n15.2\n10.5\n3.4\n15.0\n24.9\nGraduate-level reasoning\nGPQA Diamond\n35.7\n32.2\n29.4\n31.8\n44.4\nInstruction following\nIFEval\n76.7\n65.6\n71.6\n74.0\n68.9\nAlignment\nMixEval Hard\n26.9\n27.6\n24.9\n24.3\n31.6\nTool Calling\nBFCL\n92.3\n-\n92.3 *\n89.5\n95.0\nMultilingual Q&A\nGlobal MMLU\n53.5\n50.54\n46.8\n49.5\n65.1\n(*): this is a tool calling finetune\nExtended Thinking\nEvaluation results in reasoning mode for SmolLM3 and Qwen3 models:\nCategory\nMetric\nSmoLLM3-3B\nQwen3-1.7B\nQwen3-4B\nHigh school math competition\nAIME 2025\n36.7\n30.7\n58.8\nMath problem-solving\nGSM-Plus\n83.4\n79.4\n88.2\nCompetitive programming\nLiveCodeBench v4\n30.0\n34.4\n52.9\nGraduate-level reasoning\nGPQA Diamond\n41.7\n39.9\n55.3\nInstruction following\nIFEval\n71.2\n74.2\n85.4\nAlignment\nMixEval Hard\n30.8\n33.9\n38.0\nTool Calling\nBFCL\n88.8\n88.8\n95.5\nMultilingual Q&A\nGlobal MMLU\n64.1\n62.3\n73.3\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 11T\nPrecision: bfloat16\nSoftware & hardware\nGPUs: 384 H100\nTraining Framework: nanotron\nData processing framework: datatrove\nEvaluation framework: lighteval\nPost-training Framework: TRL\nOpen resources\nHere is an infographic with all the training details.\nThe datasets used for pretraining can be found in this collection and those used in mid-training and post-training will be released in the following weeks\nThe training and evaluation configs and code can be found in the huggingface/smollm repository.\nThe training intermediate checkpoints are available at HuggingFaceTB/SmolLM3-3B-checkpoints\nLimitations\nSmolLM3 can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nLicense\nApache 2.0\nCitation\n@misc{bakouch2025smollm3,\ntitle={{SmolLM3: smol, multilingual, long-context reasoner}},\nauthor={Bakouch, Elie and Ben Allal, Loubna and Lozhkov, Anton and Tazi, Nouamane and Tunstall, Lewis and Pati√±o, Carlos Miguel and Beeching, Edward and Roucher, Aymeric and Reedi, Aksel Joonas and Gallou√©dec, Quentin and Rasul, Kashif and Habib, Nathan and Fourrier, Cl√©mentine and Kydlicek, Hynek and Penedo, Guilherme and Larcher, Hugo and Morlon, Mathieu and Srivastav, Vaibhav and Lochner, Joshua and Nguyen, Xuan-Son and Raffel, Colin and von Werra, Leandro and Wolf, Thomas},\nyear={2025},\nhowpublished={\\url{https://huggingface.co/blog/smollm3}}\n}",
    "bond005/whisper-podlodka-turbo": "Whisper-Podlodka-Turbo\nModel Description\nKey Benefits\nSupported Tasks\nUses\nInstallation\nUsages Cases\nBias, Risks, and Limitations\nTraining Details\nTraining Data\nTraining Features\nEvaluation\nTesting Data & Metrics\nGeneration Parameters\nResults\nCitation\nWhisper-Podlodka-Turbo\nWhisper-Podlodka-Turbo is a new fine-tuned version of a Whisper large-v3-turbo. The main goal of the fine-tuning is to improve the quality of speech recognition and speech translation for Russian and English, as well as reduce the occurrence of hallucinations when processing non-speech audio signals.\nModel Description\nWhisper-Podlodka-Turbo is a new fine-tuned version of Whisper-Large-V3-Turbo, optimized for high-quality Russian speech recognition with proper punctuation + capitalization and enhanced with noise resistance capability.\nKey Benefits\nüéØ Improved Russian speech recognition quality compared to the base Whisper-Large-V3-Turbo model\n‚úçÔ∏è Correct Russian punctuation and capitalization\nüéß Enhanced background noise resistance\nüö´ Reduced number of hallucinations, especially in non-speech segments\nSupported Tasks\nAutomatic Speech Recognition (ASR):\nüá∑üá∫ Russian (primary focus)\nüá¨üáß English\nSpeech Translation:\nRussian ‚ÜîÔ∏è English\nSpeech Language Detection (including non-speech detection)\nUses\nInstallation\nWhisper-Podlodka-Turbo is supported in Hugging Face ü§ó Transformers. To run the model, first install the Transformers library. For this example, we'll also install ü§ó Datasets to load toy audio dataset from the Hugging Face Hub, and ü§ó Accelerate to reduce the model loading time:\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\nAlso, I recommend using whisper-lid for initial spoken language detection. Therefore, this library is also worth installing:\npip install --upgrade whisper-lid\nUsages Cases\nSpeech recognition\nThe model can be used with the pipeline class to transcribe audios of arbitrary language:\nimport librosa  # for loading sound from local file\nfrom transformers import pipeline  # for working with Whisper-Podlodka-Turbo\nimport wget  # for downloading demo sound from its URL\nfrom whisper_lid.whisper_lid import detect_language_in_speech  # for spoken language detection\nmodel_id = \"bond005/whisper-podlodka-turbo\"  # the best Whisper model :-)\ntarget_sampling_rate = 16_000  # Hz\nasr = pipeline(model=model_id, device_map='auto', torch_dtype='auto')\n# An example of speech recognition in Russian, spoken by a native speaker of this language\nsound_ru_url = 'https://huggingface.co/bond005/whisper-podlodka-turbo/resolve/main/test_sound_ru.wav'\nsound_ru_name = wget.download(sound_ru_url)\nsound_ru = librosa.load(sound_ru_name, sr=target_sampling_rate, mono=True)[0]\nprint('Duration of sound with Russian speech = {0:.3f} seconds.'.format(\nsound_ru.shape[0] / target_sampling_rate\n))\ndetected_languages = detect_language_in_speech(\nsound_ru,\nasr.feature_extractor,\nasr.tokenizer,\nasr.model\n)\nprint('Top-3 languages:')\nlang_text_width = max([len(it[0]) for it in detected_languages])\nfor it in detected_languages[0:3]:\nprint('  {0:>{1}} {2:.4f}'.format(it[0], lang_text_width, it[1]))\nrecognition_result = asr(\nsound_ru,\ngenerate_kwargs={'task': 'transcribe', 'language': detected_languages[0][0]},\nreturn_timestamps=False\n)\nprint(recognition_result['text'] + '\\n')\n# An example of speech recognition in English, pronounced by a non-native speaker of that language with an accent\nsound_en_url = 'https://huggingface.co/bond005/whisper-podlodka-turbo/resolve/main/test_sound_en.wav'\nsound_en_name = wget.download(sound_en_url)\nsound_en = librosa.load(sound_en_name, sr=target_sampling_rate, mono=True)[0]\nprint('Duration of sound with English speech = {0:.3f} seconds.'.format(\nsound_en.shape[0] / target_sampling_rate\n))\ndetected_languages = detect_language_in_speech(\nsound_en,\nasr.feature_extractor,\nasr.tokenizer,\nasr.model\n)\nprint('Top-3 languages:')\nlang_text_width = max([len(it[0]) for it in detected_languages])\nfor it in detected_languages[0:3]:\nprint('  {0:>{1}} {2:.4f}'.format(it[0], lang_text_width, it[1]))\nrecognition_result = asr(\nsound_en,\ngenerate_kwargs={'task': 'transcribe', 'language': detected_languages[0][0]},\nreturn_timestamps=False\n)\nprint(recognition_result['text'] + '\\n')\nAs a result, you can see a text output like this:\nDuration of sound with Russian speech = 29.947 seconds.\nTop-3 languages:\nrussian 0.9568\nenglish 0.0372\nukrainian 0.0013\n–ù—É, –≤–∏—Å–ø–µ—Ä —Å–∞–º –ø–æ —Å–µ–±–µ. –ß—Ç–æ —Ç–∞–∫–æ–µ –≤–∏—Å–ø–µ—Ä? –í–∏—Å–ø–µ—Ä ‚Äî —ç—Ç–æ —É–∂–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ end-to-end –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º –¥–µ–∫–æ–¥–µ—Ä–æ–º, —Ç–æ –µ—Å—Ç—å —ç—Ç–æ –Ω–µ —á–∏—Å—Ç—ã–π —ç–Ω–∫–æ–¥–µ—Ä, –∫–∞–∫ Wave2Vec, —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–µ–∫-—Ç–æ-—Å–µ–∫, —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä, –∫–∞–∫ T5, —ç—Ç–æ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç, –≥–¥–µ —ç–Ω–∫–æ–¥–µ—Ä —É—á–∏—Ç—ã–≤–∞–µ—Ç, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ —Ä–µ—á–∏, –Ω—É –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ —Ç–æ–∂–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–æ–¥–º–µ—à–∏–≤–∞–µ—Ç—Å—è, –∞ –¥–µ–∫–æ–¥–µ—Ä ‚Äî —ç—Ç–æ —É–∂–µ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º.\nDuration of sound with English speech = 20.247 seconds.\nTop-3 languages:\nenglish 0.9526\nrussian 0.0311\npolish 0.0006\nEnsembling can help us to solve a well-known bias-variance trade-off. We can decrease variance on basis of large ensemble, large ensemble of different algorithms.\nSpeech recognition with timestamps\nIn addition to the usual recognition, the model can also provide timestamps for recognized speech fragments:\nrecognition_result = asr(\nsound_ru,\ngenerate_kwargs={'task': 'transcribe', 'language': 'russian',\nreturn_timestamps=True\n)\nprint('Recognized chunks of Russian speech:')\nfor it in recognition_result['chunks']:\nprint(f'  {it}')\nrecognition_result = asr(\nsound_en,\ngenerate_kwargs={'task': 'transcribe', 'language': 'english'},\nreturn_timestamps=True\n)\nprint('\\nRecognized chunks of English speech:')\nfor it in recognition_result['chunks']:\nprint(f'  {it}')\nAs a result, you can see a text output like this:\nRecognized chunks of Russian speech:\n{'timestamp': (0.0, 4.8), 'text': '–ù—É, –≤–∏—Å–ø–µ—Ä, —Å–∞–º –ø–æ —Å–µ–±–µ, —á—Ç–æ —Ç–∞–∫–æ–µ –≤–∏—Å–ø–µ—Ä. –í–∏—Å–ø–µ—Ä ‚Äî —ç—Ç–æ —É–∂–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ'}\n{'timestamp': (4.8, 8.4), 'text': ' end-to-end –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º –¥–µ–∫–æ–¥–µ—Ä–æ–º.'}\n{'timestamp': (8.4, 10.88), 'text': ' –¢–æ –µ—Å—Ç—å, —ç—Ç–æ –Ω–µ —á–∏—Å—Ç—ã–π —ç–Ω–∫–æ–¥–µ—Ä, –∫–∞–∫ Wave2Vec.'}\n{'timestamp': (10.88, 15.6), 'text': ' –≠—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–µ–∫-—Ç–æ-—Å–µ–∫, —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä, –∫–∞–∫ T5.'}\n{'timestamp': (15.6, 19.12), 'text': ' –≠—Ç–æ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç,'}\n{'timestamp': (19.12, 23.54), 'text': ' –≥–¥–µ —ç–Ω–∫–æ–¥–µ—Ä —É—á–∏—Ç—ã–≤–∞–µ—Ç, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ —Ä–µ—á–∏,'}\n{'timestamp': (23.54, 25.54), 'text': ' –Ω—É –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ —Ç–æ–∂–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–æ–¥–º–µ—à–∏–≤–∞–µ—Ç—Å—è,'}\n{'timestamp': (25.54, 29.94), 'text': ' –∞ –¥–µ–∫–æ–¥–µ—Ä ‚Äî —ç—Ç–æ —É–∂–µ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º.'}\nRecognized chunks of English speech:\n{'timestamp': (0.0, 8.08), 'text': 'Ensembling can help us to solve a well-known bias-variance trade-off.'}\n{'timestamp': (8.96, 20.08), 'text': 'We can decrease variance on basis of large ensemble, large ensemble of different algorithms.'}\nLong-form speech recognition\nWhile previous examples demonstrate accurate transcription for audio segments under thirty seconds, practical applications often require processing extensive recordings ranging from several minutes to multiple hours. This necessitates specialized techniques like the sliding window approach to overcome memory constraints and preserve contextual coherence across the entire signal. The following example showcases the model's capability to handle such long-form audio, enabling accurate transcription of lectures, interviews, and meetings.\nimport nltk  # for splitting long text into sentences\nfrom whisper_lid.whisper_lid import detect_language_in_long_speech  # for spoken language detection in long audio\nnltk.download('punkt_tab')\nlong_sound_ru_url = 'https://huggingface.co/bond005/whisper-podlodka-turbo/resolve/main/test_sound_ru_longform.wav'\nlong_sound_ru_name = wget.download(long_sound_ru_url)\nlong_sound_ru = librosa.load(long_sound_ru_name, sr=target_sampling_rate, mono=True)[0]\nprint('Duration of long sound with Russian speech = {0:.3f} seconds.'.format(\nsound_ru.shape[0] / target_sampling_rate\n))\ndetected_languages, _ = detect_language_in_long_speech(\nlong_sound_ru,\nasr.feature_extractor,\nasr.tokenizer,\nasr.model\n)\nprint('\\nTop-3 languages:')\nlang_text_width = max([len(it[0]) for it in detected_languages])\nfor it in detected_languages[0:3]:\nprint('  {0:>{1}} {2:.4f}'.format(it[0], lang_text_width, it[1]))\nrecognition_result = asr(\nsound_ru_longform,\ngenerate_kwargs={\n'max_new_tokens': 410,\n'num_beams': 5,  # beam search width (higher values improve accuracy at the cost of increased computation)\n'condition_on_prev_tokens': False,\n'compression_ratio_threshold': 2.4,             # used to detect and suppress repetitive loops (a common failure mode)\n'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),  # controls the randomness of token sampling during generation\n'logprob_threshold': -1.0,   # the threshold for the average log-probability of the generated tokens (provides a filter to exclude low-confidence, potentially erroneous transcriptions)\n'no_speech_threshold': 0.6,  # threshold for the probability of the `<|nospeech|>` token (segments with a probability above this threshold are considered silent and skipped)\n'task': 'transcribe',\n'language': detected_languages[0][0]\n},\nreturn_timestamps=True\n)\nprint('\\nRecognized text in the long audio, split into sentences:')\nfor it in map(lambda sent: sent.strip(), nltk.sent_tokenize(recognition_result['text'])):\nprint(f'  {it}')\nAs a result, you can see a text output like this:\nDuration of long sound with Russian speech = 148.845 seconds.\nTop-3 languages:\nrussian 0.9787\nenglish 0.0186\nukrainian 0.0006\nRecognized text in the long audio, split into sentences:\n–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –¥—Ä—É–∑—å—è!\n–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!\n–Ø –æ—á–µ–Ω—å —Ä–∞–¥ –≤–∞—Å –≤—Å–µ—Ö –≤–∏–¥–µ—Ç—å –∑–¥–µ—Å—å, –≤ —ç—Ç–æ–º –∑–∞–ª–µ –Ω–∞ –•–∞–π–ª–æ–æ–¥–µ.\n–Ø –ò–≤–∞–Ω, –∫–∞–∫ –º–µ–Ω—è —É–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏, –∏ —è –ª—é–±–ª—é –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.\n–Ø –ª—é–±–ª—é –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å 2005 –≥–æ–¥–∞, –∫–æ–≥–¥–∞ –æ–Ω–æ –ø—Ä–æ–Ω–∏–∫–ª–æ –≤ –º–æ—ë —Å–µ—Ä–¥—Ü–µ –µ—â—ë, –∫–æ–≥–¥–∞ —è –±—ã–ª —Å—Ç—É–¥–µ–Ω—Ç–æ–º.\n–° 2006 –≥–æ–¥–∞ —è —Ä–∞–±–æ—Ç–∞–ª –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ, –ø—Ä–µ–ø–æ–¥–∞–≤–∞–ª –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å–≤–æ–∏–º —Å—Ç—É–¥–µ–Ω—Ç–∞–º.\n–° —Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç–æ–≥–æ –≥–æ–¥–∞ —è –ø–µ—Ä–µ—à—ë–ª –≤ IT-–∏–Ω–¥—É—Å—Ç—Ä–∏—é, —Ä–∞–±–æ—Ç–∞–ª –≤ —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏—è—Ö, –∑–∞–Ω–∏–º–∞—è—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ –≤—Å—ë —Ç–µ–º –∂–µ –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º.\n–ò, –Ω–∞–∫–æ–Ω–µ—Ü, –≤ –¥–≤–∞–¥—Ü–∞—Ç—å –≤—Ç–æ—Ä–æ–º –≥–æ–¥—É –º–Ω–µ –≤—Å—ë —ç—Ç–æ –Ω–∞–¥–æ–µ–ª–æ, —è —Ä–µ—à–∏–ª –æ–±—Ä–∞—Ç–Ω–æ –∏–∑ IT-–∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –ø–µ—Ä–µ–π—Ç–∏ –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫—É—é —Å—Ñ–µ—Ä—É.\n–ò —Å–µ–π—á–∞—Å —è —Ä–∞–±–æ—Ç–∞—é –≤ –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–æ–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ, –∑–∞–Ω–∏–º–∞—é—Å—å –Ω–∞—É—á–Ω—ã–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏, —É—á—É —Å—Ç—É–¥–µ–Ω—Ç–æ–≤, –¥–µ–ª–∞–µ–º –≤—Å—è–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —à—Ç—É–∫–∏.\n–ù—É, –∞ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞ —è –∏ –º–æ–∏ —É—á–µ–Ω–∏–∫–∏ —Ä–µ—à–∏–ª–∏ –≤—Å—ë-—Ç–∞–∫–∏ –Ω–µ —Ç–æ–ª—å–∫–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏ –∑–∞–Ω–∏–º–∞—Ç—å—Å—è, –∏ –Ω–∞—É–∫–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–Ω–æ—Å–∏—Ç—å –ø–æ–ª—å–∑—É –ª—é–¥—è–º, –∏ –º—ã —Å–¥–µ–ª–∞–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–π —Å—Ç–∞—Ä—Ç–∞–ø –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ¬´–°–∏–±–∏—Ä—Å–∫–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏¬ª.\n–¢–æ –µ—Å—Ç—å, —Å–∏–±–∏—Ä—Å–∫–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ –∏ —Å–∏–±–∏—Ä—Å–∫–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Ç–µ–ø–µ—Ä—å.\n–í—Å—ë, —á—Ç–æ —è –¥–µ–ª–∞–ª, –æ–Ω–æ —Å–≤—è–∑–∞–Ω–æ –æ–¥–Ω–∏–º, –º–æ–µ–π –ª—é–±–æ–≤—å—é –∫ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.\n–ú–Ω–µ —ç—Ç–æ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –±—ã–ª–æ –≤—Å–µ–≥–¥–∞.\n–ù—É, –∞ –ø–æ–º–∏–º–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –º–Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö.\n–ù–∞—É—á–Ω—ã–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Å–ø–æ—Å–æ–± —Ä–∞–∑–≤–ª–µ—á—å—Å—è, —ç—Ç–æ —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∏—Ç—å –ø–æ –≥–∞–º–±—É—Ä–≥—Å–∫–æ–º—É —Å—á–µ—Ç—É, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ö–æ—Ä–æ—à –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏.\n–ö–æ–≥–¥–∞ –º—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫—É—é-—Ç–æ —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∑–∞–∫–∞–∑—á–∏–∫–∞, –º—ã –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ–º—Å—è –Ω–∞ –µ–≥–æ –¥–∞–Ω–Ω—ã–µ, —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞–∫—Ä—ã—Ç—ã, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –º—ã –¥–µ–ª–∞–µ–º –∫–∞–∫–∏–µ-—Ç–æ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ, –º–æ–∂–µ—Ç, –¥–∞–∂–µ —á–µ—Ä–µ–ø–∏–∫–∏–Ω–≥ –∏–Ω–æ–≥–¥–∞ –¥–µ–ª–∞–µ–º, —Ö–æ—Ç—è —ç—Ç–æ —Ñ—É, –Ω–æ —Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ.\n–ê –∫–æ–≥–¥–∞ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –Ω–∞—à–∏ —Ä–µ—à–µ–Ω–∏—è, –Ω–∞—à –Ω–∞—É—á–Ω—ã–π –º–µ—Ç–æ–¥, –Ω–∞—à –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏, –º—ã –≤—Å–µ –≤ —Ä–∞–≤–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –∏ –º—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–µ–º –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ç–æ –∏–ª–∏ –∏–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å–µ–±—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç.\n–ü—Ä–∏ —ç—Ç–æ–º —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ —Ç–∞–∫–∏–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –¥–∞—é—Ç –º–∞—Ç–µ—Ä–∏–∞–ª—ã –≤ –≤–∏–¥–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏, –≤ –≤–∏–¥–µ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π, –æ–ø—è—Ç—å-—Ç–∞–∫–∏, –º–æ–∂–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –≤ –Ω–∞—É–∫–µ, –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç–æ–∏—Ç –æ—Å—Ç—Ä–æ, –ê –ø–æ–¥–æ–±–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è, —è –∏–º–µ—é –≤ –≤–∏–¥—É –Ω–µ –ö–µ–≥–ª, –∞ —á—Ç–æ-—Ç–æ –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –∏ –±–æ–ª–µ–µ —Ç–∞–∫–æ–µ —Å–∏—Å—Ç–µ–º–Ω–æ–µ, –æ–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–∞–∫–æ–µ-—Ç–æ –¥–≤–∏–∂–µ–Ω–∏–µ –Ω–∞—É–∫–∏ –≤–ø–µ—Ä–µ–¥ –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å.\nVoice activity detection (speech/non-speech)\nAlong with special language tokens, the model can also return the special token <|nospeech|>, if the input audio signal does not contain any speech (for details, see section 2.3 of the corresponding paper about Whisper). This skill of the model forms the basis of the speech/non-speech classification algorithm, as demonstrated in the following example:\nnonspeech_sound_url = 'https://huggingface.co/bond005/whisper-podlodka-turbo/resolve/main/test_sound_nonspeech.wav'\nnonspeech_sound_name = wget.download(nonspeech_sound_url)\nnonspeech_sound = librosa.load(nonspeech_sound_name, sr=target_sampling_rate, mono=True)[0]\nprint('Duration of sound without speech = {0:.3f} seconds.'.format(\nnonspeech_sound.shape[0] / target_sampling_rate\n))\ndetected_languages = detect_language_in_speech(\nnonspeech_sound,\nasr.feature_extractor,\nasr.tokenizer,\nasr.model\n)\nprint('Top-3 languages:')\nlang_text_width = max([len(it[0]) for it in detected_languages])\nfor it in detected_languages[0:3]:\nprint('  {0:>{1}} {2:.4f}'.format(it[0], lang_text_width, it[1]))\nAs a result, you can see a text output like this:\nDuration of sound without speech = 10.000 seconds.\nTop-3 languages:\nNO SPEECH 0.9957\nlingala 0.0002\ncantonese 0.0002\nSpeech translation\nIn addition to the transcription task, the model also performs speech translation (although it translates better from Russian into English than from English into Russian):\nprint(f'Speech translation from Russian to English:')\nrecognition_result = asr(\nsound_ru,\ngenerate_kwargs={'task': 'translate', 'language': 'english'},\nreturn_timestamps=False\n)\nprint(recognition_result['text'] + '\\n')\nprint(f'Speech translation from English to Russian:')\nrecognition_result = asr(\nsound_en,\ngenerate_kwargs={'task': 'translate', 'language': 'russian'},\nreturn_timestamps=False\n)\nprint(recognition_result['text'] + '\\n')\nAs a result, you can see a text output like this:\nSpeech translation from Russian to English:\nWell, Visper, what is Visper? Visper is already a complete end-to-end neural network with an autoregressive decoder. That is, it's not a pure encoder like Wave2Vec, it's not just a text-to-seq encoder-decoder like T5, it's a complete algorithm for the transformation of speech into text, where the encoder considers, first of all, acoustic features of speech, well, and the semantics are also gradually moving, and the decoder is already a language model that generates token by token.\nSpeech translation from English to Russian:\n–≠–Ω—Å–µ–º–±–ª–∏–Ω–≥ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –Ω–∞–º –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å —Ö–æ—Ä–æ—à–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –±–∞–π–∑-–≤–∞—Ä–∏–∞–Ω—Ç. –ú—ã –º–æ–∂–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä—É–ø–Ω–æ–≥–æ —ç–Ω—Å–µ–º–±–ª–∞, –∫—Ä—É–ø–Ω–æ–≥–æ —ç–Ω—Å–µ–º–±–ª–∞ —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.\nAs you can see, in both examples the speech translation contains some errors, however in the example of translation from English to Russian these errors are more significant.\nBias, Risks, and Limitations\nWhile improvements are observed for English and translation tasks, statistically significant advantages are confirmed only for Russian ASR\nThe model's performance on code-switching speech (where speakers alternate between Russian and English within the same utterance) has not been specifically evaluated\nInherits basic limitations of the Whisper architecture\nTraining Details\nTraining Data\nThe model was fine-tuned on a composite dataset including:\nCommon Voice (Ru, En)\nPodlodka Speech (Ru)\nTaiga Speech (Ru, synthetic)\nGolos Farfield and Golos Crowd (Ru)\nSova Rudevices (Ru)\nAudioset (non-speech audio)\nTraining Features\n1. Data Augmentation:\nDynamic mixing of speech with background noise and music\nGradual reduction of signal-to-noise ratio during training\n2. Text Data Processing:\nRussian text punctuation and capitalization restoration using bond005/ruT5-ASR-large (for speech sub-corpora without punctuated annotations)\nParallel Russian-English text generation using Qwen/Qwen2.5-14B-Instruct\nMulti-stage validation of generated texts to minimize hallucinations using bond005/xlm-roberta-xl-hallucination-detector\n3. Training Strategy:\nProgressive increase in training example complexity\nBalanced sampling between speech and non-speech data\nSpecial handling of language tokens and no-speech detection (<|nospeech|>)\nEvaluation\nThe experimental evaluation focused on two main tasks:\nRussian speech recognition\nSpeech activity detection (binary classification \"speech/non-speech\")\nTesting was performed on publicly available Russian speech corpora. Speech recognition was conducted using the standard pipeline from the Hugging Face ü§ó Transformers library. Due to the limitations of this pipeline in language identification and non-speech detection (caused by a certain bug), the whisper-lid library was used for speech presence/absence detection in the signal.\nTesting Data & Metrics\nTesting Data\nThe quality of the Russian speech recognition task was tested on test sub-sets of six different datasets:\nCommon Voice 11 Ru\nPodlodka Speech\nGolos Farfield\nGolos Crowd\nSova Rudevices\nRussian Librispeech\nThe quality of the long-form Russian speech recognition was tested on the dangrebenkin/long_audio_youtube_lectures dataset, developed by Daniel Grebenkin. This dataset contains seven long-form (20-40 minute) Russian audio recordings that were manually annotated. The audios cover a variety of topics and speaking styles; they are excerpts from Russian scientific lectures on various subjects: philology, mathematics, history, etc. All recordings were made in relatively quiet, lecture-hall-like acoustic environments. However, some natural background noises, such as the sound of chalk on a blackboard, are present.\nThe quality of the voice activity detection task was tested on test sub-sets of two different datasets:\nnoised version of Golos Crowd as a source of speech samples\nfiltered sub-set of Audioset corpus as a source of non-speech samples\nNoise was added using a special augmenter capable of simulating the superposition of five different types of acoustic noise (reverberation, speech-like sounds, music, household sounds, and pet sounds) at a given signal-to-noise ratio (in this case, a signal-to-noise ratio of 2 dB was used).\nThe quality of the robust Russian speech recognition task was tested on test sub-set of above-mentioned noised Golos Crowd.\nMetrics\n1. Modified WER (Word Error Rate) for Russian speech recognition quality:\nText normalization before WER calculation:\nUnification of numeral representations (digits/words)\nStandardization of foreign words (Cyrillic/Latin scripts)\nAccounting for valid transliteration variants\nEnables more accurate assessment of semantic recognition accuracy\nThe lower the WER, the better the speech recognition quality\n2. F1-score for speech activity detection:\nBinary classification \"speech/non-speech\"\nEvaluation of non-speech segment detection accuracy using <|nospeech|> token\nThe higher the F1 score, the better the voice activity detection quality\nGeneration Parameters\nFor experiments with short audio signals (under 30 seconds), we used standard greedy decoding (num_beams=1). For long-form audio, two approaches were tested: simple 30-second chunking and the sequential long-form algorithm.\nFor the sequential long-form mode, the implementation followed the strategy from Section 4.5 of the paper about Whisper with two key hyperparameter differences:\nBeam Search: The paper implies the use of beam search for optimal performance, while our initial experiments for this task used greedy decoding (num_beams=1).\nCompression Ratio Threshold: A key deviation was the use of a more conservative compression_ratio_threshold of 1.35 (compared to 2.4 in the paper). This lower threshold makes the repetition-detection algorithm significantly more aggressive, triggering fallback mechanisms (e.g., temperature rescoring) sooner to suppress repetitive outputs.\nThe parameters for voice activity detection (no_speech_threshold=0.6) and low-confidence detection (logprob_threshold=-1.0) were kept aligned with the paper's recommendations. Context conditioning between segments (condition_on_prev_tokens) was disabled for this experimental run.\nResults\nAutomatic Speech Recognition (ASR)\nResult (WER, %):\nDataset\nbond005/whisper-podlodka-turbo\nopenai/whisper-large-v3-turbo\nbond005/podlodka_speech\n8.17\n8.33\nrulibrispeech\n9.76\n10.25\nsberdevices_golos_farfield\n11.61\n20.12\nsberdevices_golos_crowd\n11.85\n14.55\nsova_rudevices\n15.35\n17.70\ncommon_voice_11_0\n5.22\n6.63\nLong-form ASR\nResult (WER, %):\nDataset\nbond005/whisper-podlodka-turbo\nopenai/whisper-large-v3-turbo\nthe simple chunking\n11.66\n15.98\nthe sequential long-form algorithm\n7.84\n9.59\nVoice Activity Detection (VAD)\nResult (F1):\nbond005/whisper-podlodka-turbo\nopenai/whisper-large-v3-turbo\n0.9235\n0.8484\nRobust ASR (SNR = 2 dB, speech-like noise, music, etc.)\nResult (WER, %):\nDataset\nbond005/whisper-podlodka-turbo\nopenai/whisper-large-v3-turbo\nsberdevices_golos_crowd (noised)\n46.58\n75.20\nCitation\nIf you use this model in your work, please cite it as:\n@misc{whisper-podlodka-turbo,\nauthor = {Ivan Bondarenko},\ntitle = {Whisper-Podlodka-Turbo: Enhanced Whisper Model for Russian ASR},\nyear = {2025},\npublisher = {Hugging Face},\njournal = {Hugging Face Model Hub},\nhowpublished = {\\url{https://huggingface.co/bond005/whisper-podlodka-turbo}}\n}",
    "Comfy-Org/flux1-kontext-dev_ComfyUI": "Examples\nOriginal version (the license on the original flux1 kontext dev repo applies to all the flux kontext dev model files in this repo).\nExamples\nhttps://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model\nhttps://docs.comfy.org/tutorials/flux/flux-1-kontext-dev",
    "TechWolf/JobBERT-v3": "SentenceTransformer based on sentence-transformers/paraphrase-multilingual-mpnet-base-v2\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nTraining Details\nTraining Dataset\nTraining Hyperparameters\nTraining Logs\nEnvironmental Impact\nTraining Hardware\nFramework Versions\nCitation\nBibTeX\nSentenceTransformer based on sentence-transformers/paraphrase-multilingual-mpnet-base-v2\nThis is a sentence-transformers model specifically trained for job title matching and similarity. It's finetuned from sentence-transformers/paraphrase-multilingual-mpnet-base-v2 on a large dataset of job titles and their associated skills/requirements across multiple languages. The model maps English, Spanish, German and Chinese job titles and descriptions to a 1024-dimensional dense vector space and can be used for semantic job title matching, job similarity search, and related HR/recruitment tasks.\nThe model was presented in the paper Multilingual JobBERT for Cross-Lingual Job Title Matching.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\nMaximum Sequence Length: 64 tokens\nOutput Dimensionality: 1024 dimensions\nSimilarity Function: Cosine Similarity\nTraining Dataset: 4 x 5.2M high-quality job title - skills pairs in English, Spanish, German and Chinese\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 64, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Asym(\n(anchor-0): Dense({'in_features': 768, 'out_features': 1024, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n(positive-0): Dense({'in_features': 768, 'out_features': 1024, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load and use the model with the following code:\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import batch_to_device, cos_sim\n# Load the model\nmodel = SentenceTransformer(\"TechWolf/JobBERT-v3\")\ndef encode_batch(jobbert_model, texts):\nfeatures = jobbert_model.tokenize(texts)\nfeatures = batch_to_device(features, jobbert_model.device)\nfeatures[\"text_keys\"] = [\"anchor\"]\nwith torch.no_grad():\nout_features = jobbert_model.forward(features)\nreturn out_features[\"sentence_embedding\"].cpu().numpy()\ndef encode(jobbert_model, texts, batch_size: int = 8):\n# Sort texts by length and keep track of original indices\nsorted_indices = np.argsort([len(text) for text in texts])\nsorted_texts = [texts[i] for i in sorted_indices]\nembeddings = []\n# Encode in batches\nfor i in tqdm(range(0, len(sorted_texts), batch_size)):\nbatch = sorted_texts[i:i+batch_size]\nembeddings.append(encode_batch(jobbert_model, batch))\n# Concatenate embeddings and reorder to original indices\nsorted_embeddings = np.concatenate(embeddings)\noriginal_order = np.argsort(sorted_indices)\nreturn sorted_embeddings[original_order]\n# Example usage\njob_titles = [\n'Software Engineer',\n'È´òÁ∫ßËΩØ‰ª∂ÂºÄÂèë‰∫∫Âëò',  # senior software developer\n'Produktmanager',  # product manager\n'Cient√≠fica de datos'  # data scientist\n]\n# Get embeddings\nembeddings = encode(model, job_titles)\n# Calculate cosine similarity matrix\nsimilarities = cos_sim(embeddings, embeddings)\nprint(similarities)\nThe output will be a similarity matrix where each value represents the cosine similarity between two job titles:\ntensor([[1.0000, 0.8087, 0.4673, 0.5669],\n[0.8087, 1.0000, 0.4428, 0.4968],\n[0.4673, 0.4428, 1.0000, 0.4292],\n[0.5669, 0.4968, 0.4292, 1.0000]])\nTraining Details\nTraining Dataset\nSize: 21,123,868 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 4 tokensmean: 10.56 tokensmax: 38 tokens\nmin: 19 tokensmean: 61.08 tokensmax: 64 tokens\nSamples:\nanchor\npositive\nÈÄö‰ø°‰∏éÂüπËÆ≠‰∏ìÂëò\ndeliver online training, liaise with educational support staff, interact with an audience, construct individual learning plans, lead a team, develop corporate training programmes, learning technologies, communication, identify with the company's goals, address an audience, learning management systems, use presentation software, motivate others, provide learning support, engage with stakeholders, identify skills gaps, meet expectations of target audience, develop training programmes\nAssociate Infrastructure Engineer\ncreate solutions to problems, design user interface, cloud technologies, use databases, automate cloud tasks, keep up-to-date to computer trends, work in teams, use object-oriented programming, keep updated on innovations in various business fields, design principles, Angular, adapt to changing situations, JavaScript, Agile development, manage stable, Swift (computer programming), keep up-to-date to design industry trends, monitor technology trends, web programming, provide mentorship, advise on efficiency improvements, adapt to change, JavaScript Framework, database management systems, stimulate creative processes\nÂÆ¢Êà∑È°æÈóÆ/Âá∫Á∫≥\ncustomer service, handle financial transactions, adapt to changing situations, have computer literacy, manage cash desk, attend to detail, provide customer guidance on product selection, perform multiple tasks at the same time, carry out financial transactions, provide membership service, manage accounts, adapt to change, identify customer's needs, solve problems\nLoss: CachedMultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\",\n\"mini_batch_size\": 512\n}\nTraining Hyperparameters\nNon-Default Hyperparameters\noverwrite_output_dir: True\nper_device_train_batch_size: 2048\nper_device_eval_batch_size: 2048\nnum_train_epochs: 1\nfp16: True\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: True\ndo_predict: False\neval_strategy: no\nprediction_loss_only: True\nper_device_train_batch_size: 2048\nper_device_eval_batch_size: 2048\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 1\neval_accumulation_steps: None\ntorch_empty_cache_steps: None\nlearning_rate: 5e-05\nweight_decay: 0.0\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1.0\nnum_train_epochs: 1\nmax_steps: -1\nlr_scheduler_type: linear\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.0\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 42\ndata_seed: None\njit_mode_eval: False\nuse_ipex: False\nbf16: False\nfp16: True\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: None\nlocal_rank: 0\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: False\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: False\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch\noptim_args: None\nadafactor: False\ngroup_by_length: False\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: None\nhub_always_push: False\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\ninclude_for_metrics: []\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ndispatch_batches: None\nsplit_batches: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\neval_on_start: False\nuse_liger_kernel: False\neval_use_gather_object: False\naverage_tokens_across_devices: False\nprompts: None\nbatch_sampler: batch_sampler\nmulti_dataset_batch_sampler: proportional\nTraining Logs\nEpoch\nStep\nTraining Loss\n0.0485\n500\n3.89\n0.0969\n1000\n3.373\n0.1454\n1500\n3.1715\n0.1939\n2000\n3.0414\n0.2424\n2500\n2.9462\n0.2908\n3000\n2.8691\n0.3393\n3500\n2.8048\n0.3878\n4000\n2.7501\n0.4363\n4500\n2.7026\n0.4847\n5000\n2.6601\n0.5332\n5500\n2.6247\n0.5817\n6000\n2.5951\n0.6302\n6500\n2.5692\n0.6786\n7000\n2.5447\n0.7271\n7500\n2.5221\n0.7756\n8000\n2.5026\n0.8240\n8500\n2.4912\n0.8725\n9000\n2.4732\n0.9210\n9500\n2.4608\n0.9695\n10000\n2.4548\nEnvironmental Impact\nCarbon emissions were measured using CodeCarbon.\nEnergy Consumed: 1.944 kWh\nCarbon Emitted: 0.717 kg of CO2\nHours Used: 5.34 hours\nTraining Hardware\nOn Cloud: Yes\nGPU Model: 1 x NVIDIA A100-SXM4-40GB\nCPU Model: Intel(R) Xeon(R) CPU @ 2.20GHz\nRAM Size: 83.48 GB\nFramework Versions\nPython: 3.10.16\nSentence Transformers: 4.1.0\nTransformers: 4.48.3\nPyTorch: 2.6.0+cu126\nAccelerate: 1.3.0\nDatasets: 3.5.1\nTokenizers: 0.21.0\nCitation\nBibTeX\nJobBERT-v3 Paper\n@misc{decorte2025multilingualjobbertcrosslingualjob,\ntitle={Multilingual JobBERT for Cross-Lingual Job Title Matching},\nauthor={Jens-Joris Decorte and Matthias De Lange and Jeroen Van Hautte},\nyear={2025},\neprint={2507.21609},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2507.21609},\n}\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nCachedMultipleNegativesRankingLoss\n@misc{gao2021scaling,\ntitle={Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup},\nauthor={Luyu Gao and Yunyi Zhang and Jiawei Han and Jamie Callan},\nyear={2021},\neprint={2101.06983},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}",
    "gradient-spaces/Rectified-Point-Flow": "[NeurIPS'25] Rectified Point Flow: Generic Point Cloud Pose Estimation\nInstallation\nQuick Start\nCheckpoints\nTraining Data\nCitation\n[NeurIPS'25] Rectified Point Flow: Generic Point Cloud Pose Estimation\nRectified Point Flow (RPF) is a unified model that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, the method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered.\nInstallation\ngit clone https://github.com/GradientSpaces/Rectified-Point-Flow.git\ncd Rectified-Point-Flow\nconda create -n py310-rpf python=3.10 -y\nconda activate py310-rpf\npoetry install  # or `uv sync`, `bash install.sh`\nQuick Start\n# Assembly Generation:\npython sample.py data_root=./demo/data\n# Overlap Prediction:\npython predict_overlap.py data_root=./demo/data\nMore details can be found in our GitHub Repo.\nCheckpoints\nRPF_base_full_*.ckpt: Complete model checkpoint for assembly generation\nRPF_base_pretrain_*.ckpt: Encoder-only checkpoint for overlap prediction\nTraining Data\nDataset\nTask\nPart segmentation source\nParts per sample\nIKEA-Manual\nShape Assembly\nDefined by IKEA manuals\n[2, 19]\nPartNet\nShape Assembly\nHuman-annotated parts\n[2, 64]\nBreakingBad-Everyday\nShape Assembly\nSimulated fractures via fracture-modes\n[2, 49]\nTwo-by-Two\nShape Assembly\nAnnotated by human\n2\nModelNet-40\nPairwise Registration\nFollowing Predator split\n2\nTUD-L\nPairwise Registration\nReal scans with partial observations\n2\nObjverse\nOverlap Prediction\nSegmented by SAMPart3D\n[3, 12]\nCitation\n@inproceedings{sun2025_rpf,\nauthor = {Sun, Tao and Zhu, Liyuan and Huang, Shengyu and Song, Shuran and Armeni, Iro},\ntitle = {Rectified Point Flow: Generic Point Cloud Pose Estimation},\nbooktitle = {NeurIPS},\nyear = {2025},\n}",
    "zai-org/GLM-4.1V-9B-Base": "GLM-4.1V-9B-Base\nModel Introduction\nBenchmark Performance\nGLM-4.1V-9B-Base\nüìñ View the GLM-4.1V-9B-Thinking paper.\nüìç Using GLM-4.1V-9B-Thinking API at Zhipu Foundation Model Open Platform\nModel Introduction\nVision-Language Models (VLMs) have become foundational components of intelligent systems. As real-world AI tasks grow\nincreasingly complex, VLMs must evolve beyond basic multimodal perception to enhance their reasoning capabilities in\ncomplex tasks. This involves improving accuracy, comprehensiveness, and intelligence, enabling applications such as\ncomplex problem solving, long-context understanding, and multimodal agents.\nBased on the GLM-4-9B-0414 foundation model, we present the new open-source VLM model\nGLM-4.1V-9B-Thinking, designed to explore the upper limits of reasoning in vision-language models. By introducing\na \"thinking paradigm\" and leveraging reinforcement learning, the model significantly enhances its capabilities. It\nachieves state-of-the-art performance among 10B-parameter VLMs, matching or even surpassing the 72B-parameter\nQwen-2.5-VL-72B on 18 benchmark tasks. We are also open-sourcing the base model GLM-4.1V-9B-Base to\nsupport further research into the boundaries of VLM capabilities.\nCompared to the previous generation models CogVLM2 and the GLM-4V series, GLM-4.1V-Thinking offers the\nfollowing improvements:\nThe first reasoning-focused model in the series, achieving world-leading performance not only in mathematics but also\nacross various sub-domains.\nSupports 64k context length.\nHandles arbitrary aspect ratios and up to 4K image resolution.\nProvides an open-source version supporting both Chinese and English bilingual usage.\nBenchmark Performance\nBy incorporating the Chain-of-Thought reasoning paradigm, GLM-4.1V-9B-Thinking significantly improves answer accuracy,\nrichness, and interpretability. It comprehensively surpasses traditional non-reasoning visual models.\nOut of 28 benchmark tasks, it achieved the best performance among 10B-level models on 23 tasks,\nand even outperformed the 72B-parameter Qwen-2.5-VL-72B on 18 tasks.\nFor video reasoning, web demo deployment, and more code, please check our GitHub.",
    "Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7": "LTX Video ICLoRA Pose 13B 0.9.7\nWhat is In-Context LoRA (IC LoRA)?\nLicense\nModel Details\nSample Outputs\nUsage\nüîå Using Trained LoRAs in ComfyUI\nAcknowledgments\nLTX Video ICLoRA Pose 13B 0.9.7\nThis is a pose control model on top of LTXV_13B_097_DEV trained on custom data.\nWhat is In-Context LoRA (IC LoRA)?\nIC LoRA is a method that enables adding video context into the video generation process. This approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference.\ncomfy compatible model: ltxv-097-ic-lora-pose-control-comfyui.safetensors\ndiffusers compatible model:ltxv-097-ic-lora-pose-control-diffusers.safetensors\nLicense\nFor licensing information, please refer to the LTXV Open Weights License.\nModel Details\nBase Model: LTXV_13B_097_DEV\nTraining Type: IC LoRA training\nLearning Rate: 0.0002\nRank: 24\nSample Outputs\nUsage\nThis model is designed to be used with the LTXV (Lightricks Text-to-Video) pipeline.\nüîå Using Trained LoRAs in ComfyUI\nIn order to use the trained lora in comfy:\nCopy the comfyui trained LoRA weights to the models/loras folder in your ComfyUI installation.\nUse ic_lora/ic-lora.json from official LTXV ComfyUI repository\nAcknowledgments\nBase model by Lightricks\nTraining infrastructure: LTX-Video-Trainer",
    "mistralai/Voxtral-Small-24B-2507": "Voxtral Small 1.0 (24B) - 2507\nKey Features\nBenchmark Results\nAudio\nText\nUsage\nvLLM (recommended)\nAudio Instruct\nTransformers ü§ó\nVoxtral Small 1.0 (24B) - 2507\nVoxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding.\nLearn more about Voxtral in our blog post here and our research paper.\nKey Features\nVoxtral builds upon Mistral Small 3 with powerful audio understanding capabilities.\nDedicated transcription mode: Voxtral can operate in a pure speech transcription mode to maximize performance. By default, Voxtral automatically predicts the source audio language and transcribes the text accordingly\nLong-form context: With a 32k token context length, Voxtral handles audios up to 30 minutes for transcription, or 40 minutes for understanding\nBuilt-in Q&A and summarization: Supports asking questions directly through audio. Analyze audio and generate structured summaries without the need for separate ASR and language models\nNatively multilingual: Automatic language detection and state-of-the-art performance in the world‚Äôs most widely used languages (English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian)\nFunction-calling straight from voice: Enables direct triggering of backend functions, workflows, or API calls based on spoken user intents\nHighly capable at text: Retains the text understanding capabilities of its language model backbone, Mistral Small 3.1\nBenchmark Results\nAudio\nAverage word error rate (WER) over the FLEURS, Mozilla Common Voice and Multilingual LibriSpeech benchmarks:\nText\nUsage\nThe model can be used with the following frameworks;\nvllm (recommended): See here\nTransformers ü§ó: See here\nNotes:\ntemperature=0.2 and top_p=0.95 for chat completion (e.g. Audio Understanding) and temperature=0.0 for transcription\nMultiple audios per message and multiple user turns with audio are supported\nFunction calling is supported\nSystem prompts are not yet supported\nvLLM (recommended)\nWe recommend using this model with vLLM.\nInstallation\nMake sure to install vllm >= 0.10.0, we recommend using uv\nuv pip install -U \"vllm[audio]\" --system\nDoing so should automatically install mistral_common >= 1.8.1.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nOffline\nYou can test that your vLLM setup works as expected by cloning the vLLM repo:\ngit clone https://github.com/vllm-project/vllm && cd vllm\nand then running:\npython examples/offline_inference/audio_language.py --num-audios 2 --model-type voxtral\nServe\nWe recommend that you use Voxtral-Small-24B-2507 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Voxtral-Small-24B-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral --tensor-parallel-size 2 --tool-call-parser mistral --enable-auto-tool-choice\nNote: Running Voxtral-Small-24B-2507 on GPU requires ~55 GB of GPU RAM in bf16 or fp16.\nTo ping the client you can use a simple Python snippet. See the following examples.\nAudio Instruct\nLeverage the audio capabilities of Voxtral-Small-24B-2507 to chat.\nMake sure that your client has mistral-common with audio installed:\npip install --upgrade mistral_common\\[audio\\]\nPython snippet\nfrom mistral_common.protocol.instruct.messages import TextChunk, AudioChunk, UserMessage, AssistantMessage, RawAudio\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\nfrom openai import OpenAI\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\nbcn_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"bcn_weather.mp3\", repo_type=\"dataset\")\ndef file_to_chunk(file: str) -> AudioChunk:\naudio = Audio.from_file(file, strict=False)\nreturn AudioChunk.from_audio(audio)\ntext_chunk = TextChunk(text=\"Which speaker is more inspiring? Why? How are they different from each other? Answer in French.\")\nuser_msg = UserMessage(content=[file_to_chunk(obama_file), file_to_chunk(bcn_file), text_chunk]).to_openai()\nprint(30 * \"=\" + \"USER 1\" + 30 * \"=\")\nprint(text_chunk.text)\nprint(\"\\n\\n\")\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[user_msg],\ntemperature=0.2,\ntop_p=0.95,\n)\ncontent = response.choices[0].message.content\nprint(30 * \"=\" + \"BOT 1\" + 30 * \"=\")\nprint(content)\nprint(\"\\n\\n\")\n# The model could give the following answer:\n# \nmessages = [\nuser_msg,\nAssistantMessage(content=content).to_openai(),\nUserMessage(content=\"Ok, now please summarize the content of the first audio.\").to_openai()\n]\nprint(30 * \"=\" + \"USER 2\" + 30 * \"=\")\nprint(messages[-1][\"content\"])\nprint(\"\\n\\n\")\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=0.2,\ntop_p=0.95,\n)\ncontent = response.choices[0].message.content\nprint(30 * \"=\" + \"BOT 2\" + 30 * \"=\")\nprint(content)\nTranscription\nVoxtral-Small-24B-2507 has powerful transcription capabilities!\nMake sure that your client has mistral-common with audio installed:\npip install --upgrade mistral_common\\[audio\\]\nPython snippet\nfrom mistral_common.protocol.transcription.request import TranscriptionRequest\nfrom mistral_common.protocol.instruct.messages import RawAudio\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\nfrom openai import OpenAI\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\naudio = Audio.from_file(obama_file, strict=False)\naudio = RawAudio.from_audio(audio)\nreq = TranscriptionRequest(model=model, audio=audio, language=\"en\", temperature=0.0).to_openai(exclude=(\"top_p\", \"seed\"))\nresponse = client.audio.transcriptions.create(**req)\nprint(response)\nFunction Calling\nVoxtral has some experimental function calling support. You can try as shown below.\nMake sure that your client has mistral-common with audio installed:\npip install --upgrade mistral_common\\[audio\\]\nPython snippet\nfrom mistral_common.protocol.instruct.messages import AudioChunk, UserMessage, TextChunk\nfrom mistral_common.protocol.transcription.request import TranscriptionRequest\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\nfrom openai import OpenAI\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ntool = Tool(\nfunction=Function(\nname=\"get_current_weather\",\ndescription=\"Get the current weather\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\",\n},\n\"format\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The temperature unit to use. Infer this from the user's location.\",\n},\n},\n\"required\": [\"location\", \"format\"],\n},\n)\n)\ntools = [tool.to_openai()]\nweather_like = hf_hub_download(\"patrickvonplaten/audio_samples\", \"fn_calling.wav\", repo_type=\"dataset\")\ndef file_to_chunk(file: str) -> AudioChunk:\naudio = Audio.from_file(file, strict=False)\nreturn AudioChunk.from_audio(audio)\naudio_chunk = file_to_chunk(weather_like)\nprint(30 * \"=\" + \"Transcription\" + 30 * \"=\")\nreq = TranscriptionRequest(model=model, audio=audio_chunk.input_audio, language=\"en\", temperature=0.0).to_openai(exclude=(\"top_p\", \"seed\"))\nresponse = client.audio.transcriptions.create(**req)\nprint(response.text) # How is the weather in Madrid at the moment?\nprint(\"\\n\")\nprint(30 * \"=\" + \"Function calling\" + 30 * \"=\")\naudio_chunk = file_to_chunk(weather_like)\nuser_msg = UserMessage(content=[audio_chunk]).to_openai()\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[user_msg],\ntemperature=0.2,\ntop_p=0.95,\ntools=[tool.to_openai()]\n)\nprint(30 * \"=\" + \"BOT 1\" + 30 * \"=\")\nprint(response.choices[0].message.tool_calls)\nprint(\"\\n\\n\")\nTransformers ü§ó\nStarting with transformers >= 4.54.0 and above, you can run Voxtral natively!\nInstall Transformers:\npip install -U transformers\nMake sure to have mistral-common >= 1.8.1 installed with audio dependencies:\npip install --upgrade \"mistral-common[audio]\"\nAudio Instruct\n‚û°Ô∏è multi-audio + text instruction\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Small-24B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\",\n},\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n{\"type\": \"text\", \"text\": \"What sport and what nursery rhyme are referenced?\"},\n],\n}\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n‚û°Ô∏è multi-turn\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Small-24B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n},\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n},\n{\"type\": \"text\", \"text\": \"Describe briefly what you can hear.\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": \"The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.\",\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n{\"type\": \"text\", \"text\": \"Ok, now compare this new audio with the previous one.\"},\n],\n},\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n‚û°Ô∏è text only\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Small-24B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Why should AI models be open-sourced?\",\n},\n],\n}\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n‚û°Ô∏è audio only\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Small-24B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n],\n}\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\n‚û°Ô∏è batched inference\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Small-24B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversations = [\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n},\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n},\n{\n\"type\": \"text\",\n\"text\": \"Who's speaking in the speach and what city's weather is being discussed?\",\n},\n],\n}\n],\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n{\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n],\n}\n],\n]\ninputs = processor.apply_chat_template(conversations)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated responses:\")\nprint(\"=\" * 80)\nfor decoded_output in decoded_outputs:\nprint(decoded_output)\nprint(\"=\" * 80)\nTranscription\n‚û°Ô∏è transcribe\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Small-24B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\ninputs = processor.apply_transcription_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\", model_id=repo_id)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated responses:\")\nprint(\"=\" * 80)\nfor decoded_output in decoded_outputs:\nprint(decoded_output)\nprint(\"=\" * 80)",
    "apple/DiffuCoder-7B-Base": "DiffuCoder-7B-Base\nMore details and usage examples:\nAcknowledgement\nDiffuCoder-7B-Base\nThe DiffuCoder-7B-Base model is our foundational masked diffusion LLM for code generation.\nTraining recipe: Using DiffuLLaMA's adaptation approach, trained on a large corpus of code: with Stage 1 65B tokens and Stage 2 65B tokens.\nBenchmarks: Strong baseline performance on HumanEval, MBPP and BigCodeBench.\nMore details and usage examples:\nPaper: DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation\nGitHub: https://github.com/apple/ml-diffucoder\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nmodel_path = \"apple/DiffuCoder-7B-Base\"\nmodel = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel = model.to(\"cuda\").eval()\nprompt = \"\"\"\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n\\\"\\\"\\\"\nCheck if in given list of numbers, are any two numbers closer to each other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\\\"\\\"\\\"\n\"\"\"\nTOKEN_PER_STEP = 1 # diffusion timesteps * TOKEN_PER_STEP = total new tokens\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids = inputs.input_ids.to(device=\"cuda\")\nattention_mask = inputs.attention_mask.to(device=\"cuda\")\noutput = model.diffusion_generate(\ninput_ids,\nattention_mask=attention_mask,\nmax_new_tokens=256,\noutput_history=True,\nreturn_dict_in_generate=True,\nsteps=256//TOKEN_PER_STEP,\ntemperature=0.2,\ntop_p=0.95,\nalg=\"entropy\",\nalg_temp=0.,\n)\ngenerations = [\ntokenizer.decode(g[len(p) :].tolist())\nfor p, g in zip(input_ids, output.sequences)\n]\nprint(generations[0].split(tokenizer.eos_token)[0])\nAcknowledgement\nTo power this HuggingFace model release, we reuse Dream's modeling architecture and generation utils.",
    "K-intelligence/Midm-2.0-Base-Instruct": "News üì¢\nTable of Contents\nOverview\nMi:dm 2.0\nQuickstart\nEvaluation\nKorean\nEnglish\nUsage\nRun on Friendli.AI\nRun on Your Local Machine\nDeployment\nTutorials\nMore Information\nLimitation\nLicense\nContact\nMi:dm 2.0 Base\nü§ó Mi:dm 2.0 Models |\nüìú Mi:dm 2.0 Technical Report |\nüìï Mi:dm 2.0 Technical Blog\nNews üì¢\nüîú (Coming Soon!) GGUF format model files will be available soon for easier local deployment.\nüìï2025/08/08: Published a technical blog article about Mi:dm 2.0 Model.\n‚ö°Ô∏è2025/07/04: Released Mi:dm 2.0 Model collection on Hugging Faceü§ó.\nTable of Contents\nOverview\nMi:dm 2.0\nQuickstart\nEvaluation\nUsage\nRun on Friendli.AI\nRun on Your Local Machine\nDeployment\nTutorials\nMore Information\nLimitation\nLicense\nContact\nOverview\nMi:dm 2.0\nMi:dm 2.0 is a \"Korea-centric AI\" model developed using KT's proprietary technology. The term \"Korea-centric AI\" refers to a model that deeply internalizes the unique values, cognitive frameworks, and commonsense reasoning inherent to Korean society. It goes beyond simply processing or generating Korean text‚Äîit reflects a deeper understanding of the socio-cultural norms and values that define Korean society.\nMi:dm 2.0 is released in two versions:\nMi:dm 2.0 BaseAn 11.5B parameter dense model designed to balance model size and performance.It extends an 8B-scale model by applying the Depth-up Scaling (DuS) method, making it suitable for real-world applications that require both performance and versatility.\nMi:dm 2.0 MiniA lightweight 2.3B parameter dense model optimized for on-device environments and systems with limited GPU resources.It was derived from the Base model through pruning and distillation to enable compact deployment.\nNeither the pre-training nor the post-training data includes KT users' data.\nQuickstart\nHere is the code snippet to run conversational inference with the model:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nmodel_name = \"K-intelligence/Midm-2.0-Base-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ngeneration_config = GenerationConfig.from_pretrained(model_name)\nprompt = \"KTÏóê ÎåÄÌï¥ ÏÜåÍ∞úÌï¥Ï§ò\"\n# message for inference\nmessages = [\n{\"role\": \"system\",\n\"content\": \"Mi:dm(ÎØø:Ïùå)ÏùÄ KTÏóêÏÑú Í∞úÎ∞úÌïú AI Í∏∞Î∞ò Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Ïù¥Îã§.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n)\noutput = model.generate(\ninput_ids.to(\"cuda\"),\ngeneration_config=generation_config,\neos_token_id=tokenizer.eos_token_id,\nmax_new_tokens=128,\ndo_sample=False,\n)\nprint(tokenizer.decode(output[0]))\nThe transformers library should be version 4.45.0 or higher.\nEvaluation\nKorean\nModel\nSociety & Culture\nGeneral Knowledge\nInstruction Following\nK-Refer*\nK-Refer-Hard*\nKo-Sovereign*\nHAERAE\nAvg.\nKMMLU\nKo-Sovereign*\nAvg.\nKo-IFEval\nKo-MTBench\nAvg.\nQwen3-4B\n53.6\n42.9\n35.8\n50.6\n45.7\n50.6\n42.5\n46.5\n75.9\n63.0\n69.4\nExaone-3.5-2.4B-inst\n64.0\n67.1\n44.4\n61.3\n59.2\n43.5\n42.4\n43.0\n65.4\n74.0\n68.9\nMi:dm 2.0-Mini-inst\n66.4\n61.4\n36.7\n70.8\n58.8\n45.1\n42.4\n43.8\n73.3\n74.0\n73.6\nQwen3-14B\n72.4\n65.7\n49.8\n68.4\n64.1\n55.4\n54.7\n55.1\n83.6\n71\n77.3\nLlama-3.1-8B-inst\n43.2\n36.4\n33.8\n49.5\n40.7\n33.0\n36.7\n34.8\n60.1\n57\n58.5\nExaone-3.5-7.8B-inst\n71.6\n69.3\n46.9\n72.9\n65.2\n52.6\n45.6\n49.1\n69.1\n79.6\n74.4\nMi:dm 2.0-Base-inst\n89.6\n86.4\n56.3\n81.5\n78.4\n57.3\n58.0\n57.7\n82\n89.7\n85.9\nModel\nComprehension\nReasoning\nK-Prag*\nK-Refer-Hard*\nKo-Best\nKo-Sovereign*\nAvg.\nKo-Winogrande\nKo-Best\nLogicKor\nHRM8K\nAvg.\nQwen3-4B\n73.9\n56.7\n91.5\n43.5\n66.6\n67.5\n69.2\n5.6\n56.7\n43.8\nExaone-3.5-2.4B-inst\n68.7\n58.5\n87.2\n38.0\n62.5\n60.3\n64.1\n7.4\n38.5\n36.7\nMi:dm 2.0-Mini-inst\n69.5\n55.4\n80.5\n42.5\n61.9\n61.7\n64.5\n7.7\n39.9\n37.4\nQwen3-14B\n86.7\n74.0\n93.9\n52.0\n76.8\n77.2\n75.4\n6.4\n64.5\n48.8\nLlama-3.1-8B-inst\n59.9\n48.6\n77.4\n31.5\n51.5\n40.1\n26.0\n2.4\n30.9\n19.8\nExaone-3.5-7.8B-inst\n73.5\n61.9\n92.0\n44.0\n67.2\n64.6\n60.3\n8.6\n49.7\n39.5\nMi:dm 2.0-Base-inst\n86.5\n70.8\n95.2\n53.0\n76.1\n75.1\n73.0\n8.6\n52.9\n44.8\n* indicates KT proprietary evaluation resources.\nEnglish\nModel\nInstruction\nReasoning\nMath\nCoding\nGeneral Knowledge\nIFEval\nBBH\nGPQA\nMuSR\nAvg.\nGSM8K\nMBPP+\nMMLU-pro\nMMLU\nAvg.\nQwen3-4B\n79.7\n79.0\n39.8\n58.5\n59.1\n90.4\n62.4\n-\n73.3\n73.3\nExaone-3.5-2.4B-inst\n81.1\n46.4\n28.1\n49.7\n41.4\n82.5\n59.8\n-\n59.5\n59.5\nMi:dm 2.0-Mini-inst\n73.6\n44.5\n26.6\n51.7\n40.9\n83.1\n60.9\n-\n56.5\n56.5\nQwen3-14B\n83.9\n83.4\n49.8\n57.7\n63.6\n88.0\n73.4\n70.5\n82.7\n76.6\nLlama-3.1-8B-inst\n79.9\n60.3\n21.6\n50.3\n44.1\n81.2\n81.8\n47.6\n70.7\n59.2\nExaone-3.5-7.8B-inst\n83.6\n50.1\n33.1\n51.2\n44.8\n81.1\n79.4\n40.7\n69.0\n54.8\nMi:dm 2.0-Base-inst\n84.0\n77.7\n33.5\n51.9\n54.4\n91.6\n77.5\n53.3\n73.7\n63.5\nUsage\nRun on Friendli.AI\nYou can try our model immediately via Friendli.AI. Simply click Deploy and then Friendli Endpoints.\nPlease note that a login to Friendli.AI is required after your fifth chat interaction.\nRun on Your Local Machine\nWe provide a detailed description about running Mi:dm 2.0 on your local machine using llama.cpp, LM Studio, and Ollama. Please check our github for more information\nDeployment\nTo serve Mi:dm 2.0 using vLLM(>=0.8.0) with an OpenAI-compatible API:\nvllm serve K-intelligence/Midm-2.0-Base-Instruct\nTutorials\nTo help our end-users easily use Mi:dm 2.0, we have provided comprehensive tutorials on github.\nMore Information\nLimitation\nThe training data for both Mi:dm 2.0 models consists primarily of English and Korean. Understanding and generation in other languages are not guaranteed.\nThe model is not guaranteed to provide reliable advice in fields that require professional expertise, such as law, medicine, or finance.\nResearchers have made efforts to exclude unethical content from the training data ‚Äî such as profanity, slurs, bias, and discriminatory language. However, despite these efforts, the model may still produce inappropriate expressions or factual inaccuracies.\nLicense\nMi:dm 2.0 is licensed under the MIT License.\nContact\nMi:dm 2.0 Technical Inquiries: midm-llm@kt.com",
    "Skywork/Skywork-R1V3-38B": "Visual-Language Models Benchmark Comparison\n4. Usage\nRun the Inference Script\n5. Citation\n6.License\n======================================\nSkywork-R1V3\n======================================\nüìñ R1V3 Report |\nüíª GitHub\n1. Model Introduction\nSkywork-R1V3-38B is the latest and most powerful open-source multimodal reasoning model in the Skywork-R1V series. Built on InternVL-38B, it significantly pushes the boundaries of multimodal and cross-disciplinary intelligence. Mainly through RL algorithm in post-training, R1V3 boasts enhanced reasoning ability, achieving open-source state-of-the-art (SOTA) performance across numerous multimodal reasoning benchmarks.\n2. Technical Highlights\nSkywork-R1V3 is an advanced, open-source Vision-Language Model (VLM) built on several core innovations:\nRefined Post-Training RL: Instead of relying on reasoning pre-training, our fine-grained cold-start finetuning effectively primes the model for Reinforcement Learning (RL), which dramatically enhances its reasoning ability.\nEssential Connector Module: We've uncovered the critical role of the connector module in achieving robust cross-modal alignment for strong multimodal reasoning. What's more, Connector-only Finetuning can further boost the model's performance post-RL.\nEntropy of Critical Reasoning Tokens: This unique indicator effectively gauges reasoning capability, guiding checkpoint selection during RL training.\nThese innovations lead to Broad Reasoning Generalization, allowing our RL-powered approach to successfully extend mathematical reasoning to diverse subject areas. Additionally, our work delves into RL-specific explorations like curriculum learning and learning rate strategies, alongside a broader discussion on multimodal reasoning. For more details, refer to our [üìñ R1V3 Report]¬†.\n3. Evaluation\nüåü Key Results\nMMMU: 76.0\nEMMA-Mini(CoT): 40.3\nMMK12: 78.5\nPhysics Reasoning: PhyX-MC-TM (52.8), SeePhys (31.5)\nLogic Reasoning: MME-Reasoning (42.8)  VisuLogic (28.5)\nMath Benchmarks: MathVista (77.1), MathVerse (59.6), MathVision (52.6)\nVisual-Language Models Benchmark Comparison\nCategory\nBenchmark\nMetric\nSkywork-38B\nQVQ-72B\nInternVL-78B\nQwenVL-72B\nClaude 3.7\nGPT-4o\nGeneral\nMMMU (val)\nAcc.\nüèÜ 76.0\n70.3\n72.2\n70.3\n75.0\n70.7\nEMMA (mini-cot)\nAcc.\n40.3\n32.0\n38.3\n39.3\n56.5\n36.0\nMMMU-pro\nAcc.\nüèÜ 55.4\n46.9*\n48.6\n51.1\n50.0\n54.5\nMMK12\nAcc.\nüèÜ 78.5\n62.7*\n67.4*\n70.5*\n55.3\n49.9\nMMstar\nAcc.\n70.6\n60.8\n72.5\n70.8\n68.8\n65.1\nMMBench-en-1.1\nAcc.\n85.7\n72.6*\n87.7\n88.0\n82.0\n84.3\nHallusionBench\nAcc.\nüèÜ 61.3\n55.3*\n59.1\n55.2\n58.3\n56.2\nMathematics\nMathVista (mini)\nAcc.\nüèÜ 77.1\n71.4\n72.2\n74.8\n66.8\n62.9\nMathVerse (vision-only)\nAcc.\nüèÜ 59.6\n45.1\n51.0\n57.6\n49.9*\n49.9\nMathVision\nAcc.\n52.6\n35.9\n43.1\n38.1\n58.6\n31.2\nWeMath (strict)\nAcc.\nüèÜ 56.5\n37.7\n46.1\n50.6\n48.9*\n50.6\nLogic\nVisulogic\nAcc.\nüèÜ 28.5\n23.5*\n27.7\n26.2\n25.9\n26.3\nLogicVista\nAcc.\n59.7\n53.8\n55.9\n57.1\n60.6*\n64.4\nMME-reasoning\nAcc.\nüèÜ 42.8\n35.2\n32.1\n34.1\n34.1\n30.2\nPhysics\nPhyX (mc-text-minimal)\nAcc.\nüèÜ 52.8\n35.2*\n40.5\n44.8\n41.6\n43.8\nSeePhys\nAcc.\n31.5\n22.5\n19.0*\n24.2\n34.6\n21.9\nüèÜ Top performer of Skywork-R1V3 in each benchmark[*] indicates results from our evaluation framework.\n4. Usage\nIf you need the detailed inference code and evaluation script, please refer to our GitHub.\nRun the Inference Script\nhf inference\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nfrom utils import load_image, split_model\nimport argparse\ndef main():\nparser = argparse.ArgumentParser(description=\"Run inference with Skywork-R1V model.\")\nparser.add_argument('--model_path', type=str, default='Skywork/Skywork-R1V3-38B', help=\"Path to the model.\")\nparser.add_argument('--image_paths', type=str, nargs='+', required=True, help=\"Path(s) to the image(s).\")\nparser.add_argument('--question', type=str, required=True, help=\"Question to ask the model.\")\nargs = parser.parse_args()\ndevice_map = split_model(args.model_path)\nmodel = AutoModel.from_pretrained(\nargs.model_path,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=False,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=device_map\n).eval()\ntokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True, use_fast=False)\npixel_values = [load_image(img_path, max_num=12).to(torch.bfloat16).cuda() for img_path in args.image_paths]\nif len(pixel_values) > 1:\nnum_patches_list = [img.size(0) for img in pixel_values]\npixel_values = torch.cat(pixel_values, dim=0)\nelse:\npixel_values = pixel_values[0]\nnum_patches_list = None\nprompt = \"<image>\n\"*len(args.image_paths) + args.question\ngeneration_config = dict(max_new_tokens=64000, do_sample=True, temperature=0.6, top_p=0.95, repetition_penalty=1.05)\nresponse = model.chat(tokenizer, pixel_values, prompt, generation_config, num_patches_list=num_patches_list)\nprint(f'User: {args.question}\nAssistant: {response}')\nif __name__ == '__main__':\nmain()\nvllm inference\npython -m vllm.entrypoints.openai.api_server --model $MODEL_PATH  --max_model_len 32768  --limit-mm-per-prompt \"image=20\" --tensor-parallel-size $N_GPU --dtype auto  --trust-remote-code\n5. Citation\nIf you use Skywork-R1V in your research, please cite:\n@misc{shen2025skyworkr1v3technicalreport,\ntitle={Skywork-R1V3 Technical Report},\nauthor={Wei Shen and Jiangbo Pei and Yi Peng and Xuchen Song and Yang Liu and Jian Peng and Haofeng Sun and Yunzhuo Hao and Peiyu Wang and Jianhao Zhang and Yahui Zhou},\nyear={2025},\neprint={2507.06167},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2507.06167},\n}\n6.License\nThis project is released under the MIT License. This project uses the InternVL3-38B as the base model, which is licensed under the MIT License.",
    "nvidia/Qwen3-30B-A3B-FP4": "Model Overview\nDescription:\nThird-Party Community Consideration\nLicense/Terms of Use:\nDeployment Geography:\nUse Case:\nRelease Date:\nModel Architecture:\nInput:\nOutput:\nSoftware Integration:\nModel Version(s):\nTraining, Testing, and Evaluation Datasets:\nCalibration Dataset:\nTraining Datasets:\nTesting Dataset:\nEvaluation Dataset:\nInference:\nPost Training Quantization\nUsage\nDeploy with TensorRT-LLM\nEvaluation\nEthical Considerations\nModel Overview\nDescription:\nThe NVIDIA Qwen3-30B-A3B FP4 model is the quantized version of Alibaba's Qwen3-30B-A3B model, which is an auto-regressive language model that uses an optimized transformer architecture. For more information, please check here. The NVIDIA Qwen3-30B-A3B FP4 model is quantized with TensorRT Model Optimizer.\nThis model is ready for commercial/non-commercial use.\nThird-Party Community Consideration\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party‚Äôs requirements for this application and use case; see link to Non-NVIDIA (Qwen3-30B-A3B) Model Card.\nLicense/Terms of Use:\nApache license 2.0\nDeployment Geography:\nGlobal\nUse Case:\nDevelopers looking to take off the shelf pre-quantized models for deployment in AI Agent systems, chatbots, RAG systems, and other AI-powered applications.\nRelease Date:\nHuggingface 08/22/2025 via https://huggingface.co/nvidia/Qwen3-30B-A3B-FP4\nModel Architecture:\nArchitecture Type: Transformers\nNetwork Architecture: Qwen3-30B-A3B\nInput:\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: 1D (One-Dimensional): Sequences\nOther Properties Related to Input: Context length up to 131K\nOutput:\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: 1D (One-Dimensional): Sequences\nOther Properties Related to Output: N/A\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nSupported Runtime Engine(s):\nTensorRT-LLM\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Blackwell\nPreferred Operating System(s):\nLinux\nModel Version(s):\nThe model is quantized with nvidia-modelopt v0.31.0\nTraining, Testing, and Evaluation Datasets:\nCalibration Dataset:\n** Link: cnn_dailymail\n** Data collection method: Automated.\n** Labeling method: Automated.\nTraining Datasets:\n** Data Collection Method by Dataset: Undisclosed\n** Labeling Method by Dataset: Undisclosed\n** Properties: Undisclosed\nTesting Dataset:\n** Data Collection Method by Dataset: Undisclosed\n** Labeling Method by Dataset: Undisclosed\n** Properties: Undisclosed\nEvaluation Dataset:\nDatasets: MMLU Pro, GPQA Diamond, HLE, LiveCodeBench, SciCode, HumanEval, AIME 2024, MATH-500\n** Data collection method: Hybrid: Automated, Human\n** Labeling method: Hybrid: Human, Automated\nInference:\nEngine: TensorRT-LLM\nTest Hardware: B200\nPost Training Quantization\nThis model was obtained by quantizing the weights and activations of Qwen3-30B-A3B to FP4 data type, ready for inference with TensorRT-LLM. Only the weights and activations of the linear operators within transformer blocks are quantized. This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 3.3x.\nUsage\nDeploy with TensorRT-LLM\nTo deploy the quantized checkpoint with TensorRT-LLM LLM API, follow the sample codes below:\nLLM API sample usage:\nfrom tensorrt_llm import LLM, SamplingParams\ndef main():\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\nllm = LLM(model=\"nvidia/Qwen3-30B-A3B-FP4\")\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n# The entry point of the program needs to be protected for spawning processes.\nif __name__ == '__main__':\nmain()\nEvaluation\nThe accuracy benchmark results are presented in the table below:\nPrecision\nMMLU Pro\nGPQA Diamond\nHLE\nLiveCodeBench\nSCICODE\nMATH-500\nAIME 2024\nBF16 (AA Ref)\n0.78\n0.62\n0.07\n0.51\n0.28\n0.96\n0.75\nFP4\n0.77\n0.61\n0.05\n0.65\n0.32\n0.96\n0.80\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report model quality, risk, security vulnerabilities or NVIDIA AI Concerns here.",
    "NCSOFT/VARCO-VISION-2.0-14B": "VARCO-VISION-2.0-14B\nIntroduction\nüö®NewsüéôÔ∏è\nKey Features\nVARCO-VISION-2.0 Family\nModel Architecture\nEvaluation\nKorean Benchmark\nEnglish Benchmark\nText-only Benchmark\nKorean Cultural Benchmark\nOCR Benchmark\nUsage\nCitation\nVARCO-VISION-2.0-14B\nIntroduction\nVARCO-VISION-2.0 is a multimodal AI model capable of understanding both images and text to answer user queries. It supports multi-image inputs, enabling effective processing of complex content such as documents, tables, and charts. The model demonstrates strong comprehension in both Korean and English, with significantly improved text generation capabilities and a deeper understanding of Korean cultural context. Compared to its predecessor, performance has been notably enhanced across various benchmarks, and its usability in real-world scenarios‚Äîsuch as everyday Q&A and information summarization‚Äîhas also improved.\nIn addition to the 14B full-scale model, a lightweight 1.7B version is available for on-device use, making it accessible on personal devices such as smartphones and PCs. VARCO-VISION-2.0 is a powerful open-weight AI model built for Korean users and is freely available for a wide range of applications.\nüö®NewsüéôÔ∏è\nüìù 2025-09-12: We published the technical report of VARCO-VISION-2.0 at link\nüõ†Ô∏è 2025-08-22: We updated the checkpoint of VARCO-VISION-2.0-1.7B for improved performance.\nüì∞ 2025-07-28: We released VARCO-VISION-2.0-1.7B-OCR at link\nüì∞ 2025-07-28: We released VARCO-VISION-2.0-1.7B at link\nüõ†Ô∏è 2025-07-18: We updated the checkpoint of VARCO-VISION-2.0-14B for improved performance.\nüì∞ 2025-07-16: We released VARCO-VISION-2.0-14B at link\nüì∞ 2025-07-16: We released GME-VARCO-VISION-Embedding at link\nKey Features\nMulti-image Understanding: Newly added support for multi-image inputs enables the model to analyze multiple images simultaneously and make more holistic and context-aware decisions.\nKorean Language Specialization: The model is further specialized for Korean, with a deeper understanding of Korean language, context, and culture. Korean text generation has been significantly improved, resulting in more natural, fluent, and accurate responses.\nOCR with Text Localization: Unlike typical models that only recognize and generate text from images, VARCO-VISION-2.0 can also identify the position of the text and provide bounding boxes around it. This makes it especially useful for document understanding, signage interpretation, and structured visual data.\nEnhanced Safety: The model now offers improved handling of harmful or sexually explicit content, ensuring safer and more reliable interactions.\nVARCO-VISION-2.0 Family\nModel Name\nBase Models (Vision / Language)\nHF Link\nVARCO-VISION-2.0-14B\nsiglip2-so400m-patch16-384 / Qwen3-14B\nlink\nVARCO-VISION-2.0-1.7B\nsiglip2-so400m-patch16-384 / Qwen3-1.7B\nlink\nVARCO-VISION-2.0-1.7B-OCR\nsiglip2-so400m-patch16-384 / Qwen3-1.7B\nlink\nGME-VARCO-VISION-Embedding\nQwen2-VL-7B-Instruct\nlink\nModel Architecture\nVARCO-VISION-2.0 follows the architecture of LLaVA-OneVision.\nEvaluation\nWe used VLMEvalKit for evaluation whenever possible, and conducted our own implementations only for benchmarks not supported by the toolkit, ensuring fair comparisons with various open-weight models.\nPlease note that for certain benchmarks involving LLM-based evaluation (e.g., LLaVABench), results may not be exactly reproducible due to variations in the underlying LLM behavior.\nKorean Benchmark\nBenchmark\nInternVL3-14B\nOvis2-16B\nQwen2.5-VL-7B\nVARCO-VISION-2.0-14B\nK-MMBench_DEV\n89.1\n86.0\n84.7\n87.7\nK-MMStar\n64.9\n29.7\n49.3\n63.6\nK-SEED\n78.2\n73.2\n75.7\n77.2\nK-LLaVA-W\n80.9\n86.3\n94.1\n96.5\nK-DTCBench\n87.9\n81.7\n82.1\n78.3\nAVERAGE\n80.2\n71.4\n77.2\n80.7\nEnglish Benchmark\nBenchmark\nInternVL3-14B\nOvis2-16B\nQwen2.5-VL-7B\nVARCO-VISION-2.0-14B\nMMStar\n68.9\n67.2\n64.1\n66.9\nMMMU_VAL\n64.8\n60.7\n58.0\n61.9\nMathVista\n74.4\n73.7\n68.1\n73.2\nOCRBench\n87.7\n87.9\n88.8\n86.9\nAI2D\n86.0\n86.3\n84.3\n85.7\nHallusionBench\n55.9\n56.8\n51.9\n53.2\nMMVet\n80.5\n68.4\n69.7\n68.9\nSEEDBench_IMG\n77.5\n77.7\n77.0\n78.0\nLLaVABench\n84.4\n93.0\n91.0\n90.2\nRealWorldQA\n69.8\n74.1\n68.4\n74.6\nPOPE\n89.4\n87.5\n85.9\n89.2\nScienceQA_TEST\n98.6\n95.2\n89.0\n93.5\nSEEDBench2_Plus\n70.1\n72.1\n70.7\n71.9\nBLINK\n59.9\n59.0\n55.3\n54.5\nTextVQA_VAL\n82.2\n83.0\n85.4\n80.4\nChartQA_TEST\n87.8\n79.1\n80.6\n84.2\nQ-Bench1_VAL\n76.5\n79.2\n78.2\n79.9\nA-Bench_VAL\n76.3\n79.6\n75.4\n79.5\nDocVQA_TEST\n94.1\n94.9\n95.7\n90.9\nInfoVQA_TEST\n83.6\n82.8\n82.6\n80.4\nAVERAGE\n78.4\n77.9\n76.0\n77.2\nText-only Benchmark\nBenchmark\nInternVL3-14B\nOvis2-16B\nQwen2.5-VL-7B\nVARCO-VISION-2.0-14B\nMMLU\n78.5\n78.4\n4.6\n77.9\nMT-Bench\n89.3\n85.9\n80.7\n89.8\nKMMLU\n51.4\n49.3\n39.6\n57.5\nKoMT-Bench\n70.1\n79.1\n68.4\n78.3\nLogicKor\n70.0\n79.4\n65.5\n74.0\nAVERAGE\n71.9\n74.4\n51.7\n75.5\nNote: Some models show unusually low performance on the MMLU benchmark. This is primarily due to their failure to correctly follow the expected output format when only few-shot exemplars are provided in the prompts. Please take this into consideration when interpreting the results.\nKorean Cultural Benchmark\nBenchmark\nInternVL3-14B\nOvis2-16B\nQwen2.5-VL-7B\nVARCO-VISION-2.0-14B\nK-Viscuit\n71.7\n77.0\n70.9\n73.7\nPangeaBench (ko)\n77.2\n76.9\n76.6\n74.5\nAVERAGE\n74.5\n77.0\n73.8\n74.1\nOCR Benchmark\nBenchmark\nPaddleOCR\nEasyOCR\nVARCO-VISION-2.0-14B\nCORD\n91.4\n77.8\n97.1\nICDAR2013\n92.0\n85.0\n95.7\nICDAR2015\n73.7\n57.9\n79.4\nAVERAGE\n85.7\n73.6\n90.7\nUsage\nTo use this model, we recommend installing transformers version 4.53.1 or higher. While it may work with earlier versions, using 4.53.1 or above is strongly recommended, especially to ensure optimal performance for the multi-image feature.\nThe basic usage is identical to LLaVA-OneVision:\nimport torch\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\nmodel_name = \"NCSOFT/VARCO-VISION-2.0-14B\"\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\nmodel_name,\ntorch_dtype=torch.float16,\nattn_implementation=\"sdpa\",\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_name)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/NCSOFT/VARCO-VISION-2.0-14B/resolve/main/demo.jpg\"},\n{\"type\": \"text\", \"text\": \"Í∞Å Î∞ïÏä§ÎßàÎã§ Ìïú Ï§ÑÏî© ÏÉâÏÉÅÍ≥º Í∏ÄÏûêÎ•º Ï†ïÌôïÌïòÍ≤å Ï∂úÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\"},\n],\n},\n]\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\ngenerate_ids = model.generate(**inputs, max_new_tokens=1024)\ngenerate_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)\n]\noutput = processor.decode(generate_ids_trimmed[0], skip_special_tokens=True)\nprint(output)\nMulti image inference\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"Ïù¥ÎØ∏ÏßÄ Í∞ÑÏùò Ïú†ÏÇ¨Ï†êÏùÑ ÌååÏïÖÌïòÏÑ∏Ïöî.\"},\n],\n},\n]\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\ngenerate_ids = model.generate(**inputs, max_new_tokens=1024)\ngenerate_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)\n]\noutput = processor.decode(generate_ids_trimmed[0], skip_special_tokens=True)\nprint(output)\nBatch inference\nAll inputs in a batch must have the same modality structure‚Äîfor example, text-only with text-only, single-image with single-image, and multi-image with multi-image‚Äîto ensure correct batch inference.\nconversation_1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"text\", \"text\": \"Ïù¥ÎØ∏ÏßÄÎ•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.\"},\n],\n},\n]\nconversation_2 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"Ïù¥ Ïù¥ÎØ∏ÏßÄÏóê ÌëúÏãúÎêú Í≤ÉÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\"},\n],\n},\n]\ninputs = processor.apply_chat_template(\n[conversation_1, conversation_2],\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\npadding=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\ngenerate_ids = model.generate(**inputs, max_new_tokens=1024)\ngenerate_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)\n]\noutput = processor.batch_decode(generate_ids_trimmed, skip_special_tokens=True)\nprint(output)\nOCR inference\nfrom PIL import Image\nimage = Image.open(\"file:///path/to/image.jpg\")\n# Image upscaling for OCR performance boost\nw, h = image.size\ntarget_size = 2304\nif max(w, h) < target_size:\nscaling_factor = target_size / max(w, h)\nnew_w = int(w * scaling_factor)\nnew_h = int(h * scaling_factor)\nimage = image.resize((new_w, new_h))\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": image},\n{\"type\": \"text\", \"text\": \"<ocr>\"},\n],\n},\n]\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device, torch.float16)\ngenerate_ids = model.generate(**inputs, max_new_tokens=1024)\ngenerate_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)\n]\noutput = processor.decode(generate_ids_trimmed[0], skip_special_tokens=False)\nprint(output)\nCitation\n@misc{cha2025varcovision20technicalreport,\ntitle={VARCO-VISION-2.0 Technical Report},\nauthor={Young-rok Cha and Jeongho Ju and SunYoung Park and Jong-Hyeon Lee and Younghyun Yu and Youngjune Kim},\nyear={2025},\neprint={2509.10105},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2509.10105},\n}",
    "ggml-org/SmolLM3-3B-GGUF": "SmolLM3-GGUF\nTable of Contents\nModel Summary\nKey features\nHow to use\nEvaluation\nBase Pre-Trained Model\nEnglish benchmarks\nMultilingual benchmarks\nInstruction Model\nNo Extended Thinking\nExtended Thinking\nTraining\nModel\nSoftware & hardware\nOpen resources\nLimitations\nLicense\nSmolLM3-GGUF\nOriginal model: https://huggingface.co/HuggingFaceTB/SmolLM3-3B\nTo enable thinking, you need to specify --jinja\nExample usage with llama.cpp:\nllama-cli -hf ggml-org/SmolLM3-3B-GGUF --jinja\nTable of Contents\nModel Summary\nEvaluation\nTraining\nLimitations\nLicense\nModel Summary\nSmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports 6 languages, advanced reasoning and long context. SmolLM3 is a fully open model that offers strong performance at the 3B‚Äì4B scale.\nThe model is a decoder-only transformer using GQA and NoRope, it was pretrained on 11.2T tokens with a staged curriculum of web, code, math and reasoning data. Post-training included midtraining on 140B reasoning tokens followed by supervised fine-tuning and alignment via Anchored Preference Optimization (APO).\nKey features\nInstruct model optimized for hybrid reasoning\nFully open model: open weights + full training details including public data mixture and training configs\nLong context: Trained on 64k context and suppots up to 128k tokens using YARN extrapolation\nMultilingual: 6 natively supported (English, French, Spanish, German, Italian, and Portuguese)\nFor more details refer to our blog post: TODO\nHow to use\nThe modeling code for SmolLM3 is available in transformers v4.53.0, so make sure to upgrade your transformers version. You can also load the model with the latest vllm which uses transformers as a backend.\npip install -U transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM3-3B\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nFor local inference, you can use llama.cpp, ONNX, MLX and MLC. You can find quantized checkpoints in this collection [TODO].\nEvaluation\nIn this section, we report the evaluation results of SmolLM3 base model. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them. For Ruler 64k evaluation, we apply YaRN to the Qwen models with 32k context to extrapolate the context length.\nWe highlight the best score in bold and underline the second-best score.\nBase Pre-Trained Model\nEnglish benchmarks\nNote: All evaluations are zero-shot unless stated otherwise.\nCategory\nMetric\nSmolLM3-3B\nQwen2.5-3B\nLlama3-3.2B\nQwen3-1.7B-Base\nQwen3-4B-Base\nReasoning & Commonsense\nHellaSwag\n76.15\n74.19\n75.52\n60.52\n74.37\nARC-CF (Average)\n65.61\n59.81\n58.58\n55.88\n62.11\nWinogrande\n58.88\n61.41\n58.72\n57.06\n59.59\nCommonsenseQA\n55.28\n49.14\n60.60\n48.98\n52.99\nKnowledge & Understanding\nMMLU-CF (Average)\n44.13\n42.93\n41.32\n39.11\n47.65\nMMLU Pro CF\n19.61\n16.66\n16.42\n18.04\n24.92\nMMLU Pro MCF\n32.70\n31.32\n25.07\n30.39\n41.07\nPIQA\n78.89\n78.35\n78.51\n75.35\n77.58\nOpenBookQA\n40.60\n40.20\n42.00\n36.40\n42.40\nBoolQ\n78.99\n73.61\n75.33\n74.46\n74.28\nMath & Code\nCoding & math\nHumanEval+\n30.48\n34.14\n25.00\n43.29\n54.87\nMBPP+\n52.91\n52.11\n38.88\n59.25\n63.75\nMATH (4-shot)\n46.10\n40.10\n7.44\n41.64\n51.20\nGSM8k (5-shot)\n67.63\n70.13\n25.92\n65.88\n74.14\nLong context\nRuler 32k context\n76.35\n75.93\n77.58\n70.63\n83.98\nRuler 64k context\n67.85\n64.90\n72.93\n57.18\n60.29\nMultilingual benchmarks\nCategory\nMetric\nSmolLM3 3B Base\nQwen2.5-3B\nLlama3.2 3B\nQwen3 1.7B Base\nQwen3 4B Base\nMain supported languages\nFrench\nMLMM Hellaswag\n63.94\n57.47\n57.66\n51.26\n61.00\nBelebele\n51.00\n51.55\n49.22\n49.44\n55.00\nGlobal MMLU (CF)\n38.37\n34.22\n33.71\n34.94\n41.80\nFlores-200 (5-shot)\n62.85\n61.38\n62.89<u/u>\n58.68\n65.76\nSpanish\nMLMM Hellaswag\n65.85\n58.25\n59.39\n52.40\n61.85\nBelebele\n47.00\n48.88\n47.00\n47.56\n50.33\nGlobal MMLU (CF)\n38.51\n35.84\n35.60\n34.79\n41.22\nFlores-200 (5-shot)\n48.25\n50.00\n44.45\n46.93\n50.16\nGerman\nMLMM Hellaswag\n59.56\n49.99\n53.19\n46.10\n56.43\nBelebele\n48.44\n47.88\n46.22\n48.00\n53.44\nGlobal MMLU (CF)\n35.10\n33.19\n32.60\n32.73\n38.70\nFlores-200 (5-shot)\n56.60\n50.63\n54.95\n52.58\n50.48\nItalian\nMLMM Hellaswag\n62.49\n53.21\n54.96\n48.72\n58.76\nBelebele\n46.44\n44.77\n43.88\n44.00\n48.78\nGlobal MMLU (CF)\n36.99\n33.91\n32.79\n35.37\n39.26\nFlores-200 (5-shot)\n52.65\n54.87\n48.83\n48.37\n49.11\nPortuguese\nMLMM Hellaswag\n63.22\n57.38\n56.84\n50.73\n59.89\nBelebele\n47.67\n49.22\n45.00\n44.00\n50.00\nGlobal MMLU (CF)\n36.88\n34.72\n33.05\n35.26\n40.66\nFlores-200 (5-shot)\n60.93\n57.68\n54.28\n56.58\n63.43\nThe model has also been trained on Arabic (standard), Chinese and Russian data, but has seen fewer tokens in these languages compared to the 6 above. We report the performance on these langages for information.\nCategory\nMetric\nSmolLM3 3B Base\nQwen2.5-3B\nLlama3.2 3B\nQwen3 1.7B Base\nQwen3 4B Base\nOther supported languages\nArabic\nBelebele\n40.22\n44.22\n45.33\n42.33\n51.78\nGlobal MMLU (CF)\n28.57\n28.81\n27.67\n29.37\n31.85\nFlores-200 (5-shot)\n40.22\n39.44\n44.43\n35.82\n39.76\nChinese\nBelebele\n43.78\n44.56\n49.56\n48.78\n53.22\nGlobal MMLU (CF)\n36.16\n33.79\n39.57\n38.56\n44.55\nFlores-200 (5-shot)\n29.17\n33.21\n31.89\n25.70\n32.50\nRussian\nBelebele\n47.44\n45.89\n47.44\n45.22\n51.44\nGlobal MMLU (CF)\n36.51\n32.47\n34.52\n34.83\n38.80\nFlores-200 (5-shot)\n47.13\n48.74\n50.74\n54.70\n60.53\nInstruction Model\nNo Extended Thinking\nEvaluation results of non reasoning models and reasoning models in no thinking mode. We highlight the best and second-best scores in bold.\nCategory\nMetric\nSmoLLM3-3B\nQwen2.5-3B\nLlama3.1-3B\nQwen3-1.7B\nQwen3-4B\nHigh school math competition\nAIME 2025\n9.3\n2.9\n0.3\n8.0\n17.1\nMath problem-solving\nGSM-Plus\n72.8\n74.1\n59.2\n68.3\n82.1\nCompetitive programming\nLiveCodeBench v4\n15.2\n10.5\n3.4\n15.0\n24.9\nGraduate-level reasoning\nGPQA Diamond\n35.7\n32.2\n29.4\n31.8\n44.4\nInstruction following\nIFEval\n76.7\n65.6\n71.6\n74.0\n68.9\nAlignment\nMixEval Hard\n26.9\n27.6\n24.9\n24.3\n31.6\nKnowledge\nMMLU-Pro\n45.0\n41.9\n36.6\n45.6\n60.9\nMultilingual Q&A\nGlobal MMLU\n53.5\n50.54\n46.8\n49.5\n65.1\nExtended Thinking\nEvaluation results in reasoning mode for SmolLM3 and Qwen3 models:\nCategory\nMetric\nSmoLLM3-3B\nQwen3-1.7B\nQwen3-4B\nHigh school math competition\nAIME 2025\n36.7\n30.7\n58.8\nMath problem-solving\nGSM-Plus\n83.4\n79.4\n88.2\nCompetitive programming\nLiveCodeBench v4\n30.0\n34.4\n52.9\nGraduate-level reasoning\nGPQA Diamond\n41.7\n39.9\n55.3\nInstruction following\nIFEval\n71.2\n74.2\n85.4\nAlignment\nMixEval Hard\n30.8\n33.9\n38.0\nKnowledge\nMMLU-Pro\n58.4\n57.8\n70.2\nMultilingual Q&A\nGlobal MMLU\n64.1\n62.3\n73.3\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 11T\nPrecision: bfloat16\nSoftware & hardware\nGPUs: 384 H100\nTraining Framework: nanotron\nData processing framework: datatrove\nEvaluation framework: lighteval\nPost-training Framework: TRL\nOpen resources\nHere is an infographic with all the training details [TODO].\nThe datasets used for pretraining can be found in this collection and those used in mid-training and pos-training can be found here [TODO]\nThe training and evaluation configs and code can be found in the huggingface/smollm repository.\nLimitations\nSmolLM3 can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nLicense\nApache 2.0",
    "hynt/F5-TTS-Vietnamese-ViVoice": "üõë Important Note ‚ö†Ô∏è\nüéôÔ∏è F5-TTS-Vietnamese-1000h\nüìå Model Details\nüìù Usage\nüõë Important Note ‚ö†Ô∏è\nThis model is only intended for research purposes.Access requests must be made using an institutional, academic, or corporate email. Requests from public email providers will be denied. We appreciate your understanding.\nüéôÔ∏è F5-TTS-Vietnamese-1000h\nA compact fine-tuned version of F5-TTS trained on 1000 hours of Vietnamese speech.\nüîó For more fine-tuning experiments, visit: https://github.com/nguyenthienhy/F5-TTS-Vietnamese.\nüìú License: CC-BY-NC-SA-4.0 ‚Äî Non-commercial research use only.\nüìå Model Details\nDataset: Vi-Voice, VLSP 2021, VLSP 2022, VLSP 2023\nTotal dataset durations: 1000 hours\nData processing Technique:\nRemove all music background from audios, using facebook demucs model: https://github.com/facebookresearch/demucs\nDo not use audio files shorter than 1 second or longer than 30 seconds.\nUsing Chunk-Large-Former Speech2Text model by Zalo-AI to filter audio which has bad transcript\nKeep the default punctuation marks unchanged.\nNormalize to lowercase format.\nTraining Configuration:\nBase Model: F5-TTS_Base\nGPU: RTX 3090\nBatch Size: 3200 frames - 1.5 months for training\nüìù Usage\nTo load and use the model, follow the example below:\ngit clone https://github.com/nguyenthienhy/F5-TTS-Vietnamese\ncd F5-TTS-Vietnamese\npython -m pip install -e.\nmv F5-TTS-Vietnamese-ViVoice/config.json F5-TTS-Vietnamese-ViVoice/vocab.txt\nf5-tts_infer-cli \\\n--model \"F5TTS_Base\" \\\n--ref_audio ref.wav \\\n--ref_text \"c·∫£ hai b√™n h√£y c·ªë g·∫Øng hi·ªÉu cho nhau\" \\\n--gen_text \"m√¨nh mu·ªën ra n∆∞·ªõc ngo√†i ƒë·ªÉ ti·∫øp x√∫c nhi·ªÅu c√¥ng ty l·ªõn, sau ƒë√≥ mang nh·ªØng g√¨ h·ªçc ƒë∆∞·ª£c v·ªÅ vi·ªát nam gi√∫p x√¢y d·ª±ng c√°c c√¥ng tr√¨nh t·ªët h∆°n\" \\\n--speed 1.0 \\\n--vocoder_name vocos \\\n--vocab_file F5-TTS-Vietnamese-ViVoice/vocab.txt \\\n--ckpt_file F5-TTS-Vietnamese-ViVoice/model_last.pt \\",
    "microsoft/MediPhi-Instruct": "Model Card for MediPhi-Instruct\nModel Summary\nModel Details\nModel Description\nModel Sources\nIntended Uses\nPrimary Use Cases\nUse Case Considerations\nResponsible AI Considerations\nHow to Get Started with the Model\nInput Format\nLoading the model locally\nMore Information\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTechnical Specifications\nModel Architecture\nLicense\nTrademarks\nCitation\nModel Card Authors\nModel Card Contact\nModel Card for MediPhi-Instruct\nModel Summary\nThe MediPhi Model Collection comprises 7 small language models of 3.8B parameters from the base model Phi-3.5-mini-instruct specialized in the medical and clinical domains. The collection is designed in a modular fashion. Five MediPhi experts are fine-tuned on various medical corpora (i.e. PubMed commercial, Medical Wikipedia, Medical Guidelines, Medical Coding, and open-source clinical documents) and merged back with the SLERP method in their base model to conserve general abilities. One model combined all five experts into one general expert with the multi-model merging method BreadCrumbs. Finally, we clinically aligned this general expert using our large-scale MediFlow corpora (see dataset microsoft/mediflow) to obtain the final expert model MediPhi-Instruct.\nModel Details\nModel Description\nThis model is the MediPhi-Instruct aligned to accomplish clinical NLP tasks.\nDeveloped by: Microsoft Healthcare & Life Sciences\nModel type: Phi3\nLanguage(s) (NLP): English\nLicense: MIT\nFinetuned from model: microsoft/MediPhi, and originally from microsoft/Phi-3.5-mini-instruct\nModel Sources\nRepository: Current HF repo\nPaper: A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment\nIntended Uses\nPrimary Use Cases\nThe model is intended for research use in English, especially clinical natural language processing. The model provides uses for research which require:\nMedically adapted language models\nMemory/compute constrained environments\nLatency bound scenarios\nOur model is designed to accelerate research on language models in medical and clinical scenarios. It should be used for research purposes, i.e., in benchmarking context or with expert user verification of the outputs.\nUse Case Considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Researchers (or developers) should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Researchers (or developers) should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nResponsible AI Considerations\nLike other language models, the Phi family of models and the MediPhi collection can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The Phi and MediPhi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\nMultilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nLong Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\nResearchers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. They are encouraged to rigorously evaluate the model for their use case, fine-tune the models when possible and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Researchers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Researchers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Researchers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nHow to Get Started with the Model\nInput Format\n<|system|>\nGIVE A ROLE AND INSTRUCTIONS FOR CLINICAL NLPTASKS<|end|>\n<|user|>\nINPUT DOCUMENT<|end|>\n<|assistant|>\nLoading the model locally\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, StoppingCriteria\ntorch.random.manual_seed(0)\nmodel_name = \"microsoft/MediPhi-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Operative Report:\\nPerformed: Cholecystectomy\\nOperative Findings: The gallbladder contained multiple stones and had thickening of its wall. Mild peritoneal fluid was noted.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"Extract medical keywords from this operative notes focus on anatomical, pathological, or procedural vocabulary.\"},\n{\"role\": \"user\", \"content\": prompt},\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\nclass EosListStoppingCriteria(StoppingCriteria):\ndef __init__(self, eos_sequence = [32007]):\nself.eos_sequence = eos_sequence\ndef __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\nlast_ids = input_ids[:,-len(self.eos_sequence):].tolist()\nreturn self.eos_sequence in last_ids\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n\"generation_kwargs\": {\"stopping_criteria\": EosListStoppingCriteria()}\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n# gallbladder stones, wall thickening, peritoneal fluid\nNotes: If you want to use flash attention, call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"flash_attention_2\".\nMore Information\nCheck microsoft/Phi-3.5-mini-instruct for details about the tokenizer, requirements and basic capabilities.\nTraining Details\nTraining Data\nContinual Pre-training:\nPubMed (commercial subset) and abstracts from ncbi/pubmed.\nMedical Guideline epfl-llm/guidelines.\nMedical Wikipedia jpcorb20/medical_wikipedia.\nMedical Coding: ICD10CM, ICD10PROC, ICD9CM, ICD9PROC, and ATC.\nClinical documents:\nzhengyun21/PMC-Patients, akemiH/NoteChat, and starmpcc/Asclepius-Synthetic-Clinical-Notes (only commercial-friendly licenses across all three datasets)\nmtsamples\nClinical alignment:\nmicrosoft/mediflow\nSee paper for details.\nTraining Procedure\nModular training making five experts from the base model with pre-instruction tuning, merging them into one model and finally clinically aligning it. See paper for details.\nEvaluation\nCLUE+ Benchmark\nPhi-3.5-mini-instruct\nPubMed\nClinical\nMedWiki\nGuideline\nMedCode\nMediPhi\nMediPhi-Instruct\nMedNLI\n66.6\n68.3\n69.2\n72.8\n70.3\n68.5\n66.9\n71.0\nPLS\n28.4\n29.2\n29.4\n29.2\n29.8\n22.3\n28.8\n26.0\nMeQSum\n36.7\n37.6\n38.1\n37.6\n37.6\n33.5\n37.9\n42.8\nLH\n45.9\n45.7\n43.5\n43.6\n41.1\n45.7\n45.7\n45.0\nMeDiSumQA\n25.9\n26.3\n26.7\n25.1\n25.1\n23.6\n26.1\n29.1\nMeDiSumCode\n41.1\n41.0\n40.5\n41.7\n41.9\n39.0\n41.7\n37.2\nRRS QA\n41.2\n44.1\n52.1\n46.7\n48.9\n45.6\n44.5\n61.6\nMedicationQA\n11.2\n10.3\n12.0\n12.2\n11.9\n12.0\n11.3\n19.3\nMEDEC\n14.8\n22.2\n34.5\n28.8\n28.3\n18.1\n29.1\n34.4\nACI\n42.3\n42.7\n43.9\n44.7\n44.7\n39.0\n44.3\n43.5\nSDoH\n35.1\n35.8\n35.8\n43.6\n41.0\n24.8\n39.7\n56.7\nICD10CM\n49.3\n49.5\n49.6\n50.2\n49.8\n68.7\n55.5\n54.9\nAverage\n36.5\n37.7\n39.6\n39.7\n39.2\n36.7\n39.3\n43.4\nNew real-world benchmarking also demonstrated good performances on clinical information extraction task: 2507.05517.\nRed Teaming\nWe carried out a Medical Red Teaming Protocol of Language Models in which we demonstrate broad conversation of original Phi3.5 safety abilities (see Phi-3 Safety Post-Training). All six merged MediPhi models fully conserve their base model's safety capabilities. For MediPhi-Instruct, it conserved safe behaviors towards jailbreaking and harmfulness, as well as it is improving considerably on groundedness. We further demonstrate safe behaviours at refusing or giving warnings with limited responses for nearly all harmful queries from clinican and patient user perspectives, based on MedSafetyBench and our PatientSafetyBench.\nTechnical Specifications\nModel Architecture\nPhi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini. It is best suited for prompts using chat format but plain text is also possible. The default context length is of 128K tokens.\nHardware\nNote that by default, the Phi-3.5-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nIf you want to run the model on:\nNVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\nCitation\n@article{corbeil2025modular,\ntitle={A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment},\nauthor={Corbeil, Jean-Philippe and Dada, Amin and Attendu, Jean-Michel and Abacha, Asma Ben and Sordoni, Alessandro and Caccia, Lucas and Beaulieu, Fran{\\c{c}}ois and Lin, Thomas and Kleesiek, Jens and Vozila, Paul},\njournal={arXiv preprint arXiv:2505.10717},\nyear={2025}\n}\nModel Card Authors\nJean-Philippe Corbeil\nModel Card Contact\njcorbeil@microsoft.com",
    "unsloth/Kimi-K2-Instruct-GGUF": "0. Changelog\n2025.7.15\n1. Model Introduction\nKey Features\nModel Variants\nInstruction model evaluation results\nBase model evaluation results\n2. Model Summary\nInstruction model evaluation results\nBase model evaluation results\n3. Evaluation Results\nInstruction model evaluation results\nBase model evaluation results\n4. Deployment\n5. Model Usage\nChat Completion\nTool Calling\n6. License\n7. Third Party Notices\n7. Contact Us\nLearn how to run Kimi-K2 Dynamic GGUFs - Read our Guide!\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nüåô Kimi K2 Usage Guidelines\nYou can now use the latest update of llama.cpp to run the model.\nFor complete detailed instructions, see our guide: docs.unsloth.ai/basics/kimi-k2\nIt is recommended to have at least 128GB unified RAM memory to run the small quants. With 16GB VRAM and 256 RAM, expect 5+ tokens/sec.\nFor best results, use any 2-bit XL quant or above.\nSet the temperature to 0.6 recommended) to reduce repetition and incoherence.\nüì∞¬†¬†Tech Blog ¬†¬†¬† | ¬†¬†¬† üìÑ¬†¬†Paper Link (coming soon)\n0. Changelog\n2025.7.15\nWe have updated our tokenizer implementation. Now special tokens like [EOS] can be encoded to their token ids.\nWe fixed a bug in the chat template that was breaking multi-turn tool calls.\n1. Model Introduction\nKimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\nKey Features\nLarge-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\nMuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\nAgentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.\nModel Variants\nKimi-K2-Base: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.\nKimi-K2-Instruct: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.\n2. Model Summary\nArchitecture\nMixture-of-Experts (MoE)\nTotal Parameters\n1T\nActivated Parameters\n32B\nNumber of Layers (Dense layer included)\n61\nNumber of Dense Layers\n1\nAttention Hidden Dimension\n7168\nMoE Hidden Dimension (per Expert)\n2048\nNumber of Attention Heads\n64\nNumber of Experts\n384\nSelected Experts per Token\n8\nNumber of Shared Experts\n1\nVocabulary Size\n160K\nContext Length\n128K\nAttention Mechanism\nMLA\nActivation Function\nSwiGLU\n3. Evaluation Results\nInstruction model evaluation results\nBenchmark\nMetric\nKimi K2 Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B (non-thinking)\nClaude Sonnet 4 (w/o extended thinking)\nClaude Opus 4 (w/o extended thinking)\nGPT-4.1\nGemini 2.5 Flash  Preview (05-20)\nCoding Tasks\nLiveCodeBench v6(Aug 24 - May 25)\nPass@1\n53.7\n46.9\n37.0\n48.5\n47.4\n44.7\n44.7\nOJBench\nPass@1\n27.1\n24.0\n11.3\n15.3\n19.6\n19.5\n19.5\nMultiPL-E\nPass@1\n85.7\n83.1\n78.2\n88.6\n89.6\n86.7\n85.6\nSWE-bench Verified (Agentless Coding)\nSingle Patch w/o Test (Acc)\n51.8\n36.6\n39.4\n50.2\n53.0\n40.8\n32.6\nSWE-bench Verified  (Agentic Coding)\nSingle Attempt (Acc)\n65.8\n38.8\n34.4\n72.7*\n72.5*\n54.6\n‚Äî\nMultiple Attempts (Acc)\n71.6\n‚Äî\n‚Äî\n80.2\n79.4*\n‚Äî\n‚Äî\nSWE-bench Multilingual (Agentic Coding)\nSingle Attempt (Acc)\n47.3\n25.8\n20.9\n51.0\n‚Äî\n31.5\n‚Äî\nTerminalBench\nInhouse Framework (Acc)\n30.0\n‚Äî\n‚Äî\n35.5\n43.2\n8.3\n‚Äî\nTerminus (Acc)\n25.0\n16.3\n6.6\n‚Äî\n‚Äî\n30.3\n16.8\nAider-Polyglot\nAcc\n60.0\n55.1\n61.8\n56.4\n70.7\n52.4\n44.0\nTool Use Tasks\nTau2 retail\nAvg@4\n70.6\n69.1\n57.0\n75.0\n81.8\n74.8\n64.3\nTau2 airline\nAvg@4\n56.5\n39.0\n26.5\n55.5\n60.0\n54.5\n42.5\nTau2 telecom\nAvg@4\n65.8\n32.5\n22.1\n45.2\n57.0\n38.6\n16.9\nAceBench\nAcc\n76.5\n72.7\n70.5\n76.2\n75.6\n80.1\n74.5\nMath & STEM Tasks\nAIME 2024\nAvg@64\n69.6\n59.4*\n40.1*\n43.4\n48.2\n46.5\n61.3\nAIME 2025\nAvg@64\n49.5\n46.7\n24.7*\n33.1*\n33.9*\n37.0\n46.6\nMATH-500\nAcc\n97.4\n94.0*\n91.2*\n94.0\n94.4\n92.4\n95.4\nHMMT 2025\nAvg@32\n38.8\n27.5\n11.9\n15.9\n15.9\n19.4\n34.7\nCNMO 2024\nAvg@16\n74.3\n74.7\n48.6\n60.4\n57.6\n56.6\n75.0\nPolyMath-en\nAvg@4\n65.1\n59.5\n51.9\n52.8\n49.8\n54.0\n49.9\nZebraLogic\nAcc\n89.0\n84.0\n37.7*\n73.7\n59.3\n58.5\n57.9\nAutoLogi\nAcc\n89.5\n88.9\n83.3\n89.8\n86.1\n88.2\n84.1\nGPQA-Diamond\nAvg@8\n75.1\n68.4*\n62.9*\n70.0*\n74.9*\n66.3\n68.2\nSuperGPQA\nAcc\n57.2\n53.7\n50.2\n55.7\n56.5\n50.8\n49.6\nHumanity's Last Exam(Text Only)\n-\n4.7\n5.2\n5.7\n5.8\n7.1\n3.7\n5.6\nGeneral Tasks\nMMLU\nEM\n89.5\n89.4\n87.0\n91.5\n92.9\n90.4\n90.1\nMMLU-Redux\nEM\n92.7\n90.5\n89.2\n93.6\n94.2\n92.4\n90.6\nMMLU-Pro\nEM\n81.1\n81.2*\n77.3\n83.7\n86.6\n81.8\n79.4\nIFEval\nPrompt Strict\n89.8\n81.1\n83.2*\n87.6\n87.4\n88.0\n84.3\nMulti-Challenge\nAcc\n54.1\n31.4\n34.0\n46.8\n49.0\n36.4\n39.5\nSimpleQA\nCorrect\n31.0\n27.7\n13.2\n15.9\n22.8\n42.3\n23.3\nLivebench\nPass@1\n76.4\n72.4\n67.6\n74.8\n74.6\n69.8\n67.8\n‚Ä¢ Bold denotes global SOTA, and underlined denotes open-source SOTA.\n‚Ä¢ Data points marked with * are taken directly from the model's tech report or blog.\n‚Ä¢ All metrics, except for SWE-bench Verified (Agentless), are evaluated with an 8k output token length. SWE-bench Verified (Agentless) is limited to a 16k output token length.\n‚Ä¢ Kimi K2 achieves 65.8% pass@1 on the SWE-bench Verified tests with bash/editor tools (single-attempt patches, no test-time compute). It also achieves a 47.3% pass@1 on the SWE-bench Multilingual tests under the same conditions. Additionally, we report results on SWE-bench Verified tests (71.6%) that leverage parallel test-time compute by sampling multiple sequences and selecting the single best via an internal scoring model.\n‚Ä¢ To ensure the stability of the evaluation, we employed avg@k on the AIME, HMMT, CNMO, PolyMath-en, GPQA-Diamond, EvalPlus, Tau2.\n‚Ä¢ Some data points have been omitted due to prohibitively expensive evaluation costs.\nBase model evaluation results\nBenchmark\nMetric\nShot\nKimi K2 Base\nDeepseek-V3-Base\nQwen2.5-72B\nLlama 4 Maverick\nGeneral Tasks\nMMLU\nEM\n5-shot\n87.8\n87.1\n86.1\n84.9\nMMLU-pro\nEM\n5-shot\n69.2\n60.6\n62.8\n63.5\nMMLU-redux-2.0\nEM\n5-shot\n90.2\n89.5\n87.8\n88.2\nSimpleQA\nCorrect\n5-shot\n35.3\n26.5\n10.3\n23.7\nTriviaQA\nEM\n5-shot\n85.1\n84.1\n76.0\n79.3\nGPQA-Diamond\nAvg@8\n5-shot\n48.1\n50.5\n40.8\n49.4\nSuperGPQA\nEM\n5-shot\n44.7\n39.2\n34.2\n38.8\nCoding Tasks\nLiveCodeBench v6\nPass@1\n1-shot\n26.3\n22.9\n21.1\n25.1\nEvalPlus\nPass@1\n-\n80.3\n65.6\n66.0\n65.5\nMathematics Tasks\nMATH\nEM\n4-shot\n70.2\n60.1\n61.0\n63.0\nGSM8k\nEM\n8-shot\n92.1\n91.7\n90.4\n86.3\nChinese Tasks\nC-Eval\nEM\n5-shot\n92.5\n90.0\n90.9\n80.9\nCSimpleQA\nCorrect\n5-shot\n77.6\n72.1\n50.5\n53.5\n‚Ä¢ We only evaluate open-source pretrained models in this work. We report results for Qwen2.5-72B because the base checkpoint for Qwen3-235B-A22B was not open-sourced at the time of our study.\n‚Ä¢ All models are evaluated using the same evaluation protocol.\n4. Deployment\nYou can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you.\nThe Anthropic-compatible API maps temperature by real_temperature = request_temperature * 0.6 for better compatible with existing applications.\nOur model checkpoints are stored in the block-fp8 format, you can find it on Huggingface.\nCurrently, Kimi-K2 is recommended to run on the following inference engines:\nvLLM\nSGLang\nKTransformers\nTensorRT-LLM\nDeployment examples for vLLM and SGLang can be found in the Model Deployment Guide.\n5. Model Usage\nChat Completion\nOnce the local inference service is up, you can interact with it through the chat endpoint:\ndef simple_chat(client: OpenAI, model_name: str):\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Please give a brief self-introduction.\"}]},\n]\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=messages,\nstream=False,\ntemperature=0.6,\nmax_tokens=256\n)\nprint(response.choices[0].message.content)\nThe recommended temperature for Kimi-K2-Instruct is temperature = 0.6.\nIf no special instructions are required, the system prompt above is a good default.\nTool Calling\nKimi-K2-Instruct has strong tool-calling capabilities.\nTo enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.\nThe following example demonstrates calling a weather tool end-to-end:\n# Your tool implementation\ndef get_weather(city: str) -> dict:\nreturn {\"weather\": \"Sunny\"}\n# Tool schema definition\ntools = [{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Retrieve current weather information. Call this when the user asks about the weather.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"city\"],\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"Name of the city\"\n}\n}\n}\n}\n}]\n# Map tool names to their implementations\ntool_map = {\n\"get_weather\": get_weather\n}\ndef tool_call_with_client(client: OpenAI, model_name: str):\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n{\"role\": \"user\", \"content\": \"What's the weather like in Beijing today? Use the tool to check.\"}\n]\nfinish_reason = None\nwhile finish_reason is None or finish_reason == \"tool_calls\":\ncompletion = client.chat.completions.create(\nmodel=model_name,\nmessages=messages,\ntemperature=0.6,\ntools=tools,          # tool list defined above\ntool_choice=\"auto\"\n)\nchoice = completion.choices[0]\nfinish_reason = choice.finish_reason\nif finish_reason == \"tool_calls\":\nmessages.append(choice.message)\nfor tool_call in choice.message.tool_calls:\ntool_call_name = tool_call.function.name\ntool_call_arguments = json.loads(tool_call.function.arguments)\ntool_function = tool_map[tool_call_name]\ntool_result = tool_function(**tool_call_arguments)\nprint(\"tool_result:\", tool_result)\nmessages.append({\n\"role\": \"tool\",\n\"tool_call_id\": tool_call.id,\n\"name\": tool_call_name,\n\"content\": json.dumps(tool_result)\n})\nprint(\"-\" * 100)\nprint(choice.message.content)\nThe tool_call_with_client function implements the pipeline from user query to tool execution.\nThis pipeline requires the inference engine to support Kimi-K2‚Äôs native tool-parsing logic.\nFor streaming output and manual tool-parsing, see the Tool Calling Guide.\n6. License\nBoth the code repository and the model weights are released under the Modified MIT License.\n7. Third Party Notices\nSee THIRD PARTY NOTICES\n7. Contact Us\nIf you have any questions, please reach out at support@moonshot.cn.",
    "Kevin-thu/Epona": "Epona: Autoregressive Diffusion World Model for Autonomous Driving\nEpona: Autoregressive Diffusion World Model for Autonomous Driving\nThis repository contains the model checkpoints and test meta data in the paper Epona: Autoregressive Diffusion World Model for Autonomous Driving\n.\nüåè Project page : https://kevin-thu.github.io/Epona/.\n‚≠ê Code: https://github.com/Kevin-thu/Epona.\nIt contains four model checkpoints:\nepona_nuplan.pkl: The base world model trained on NuPlan from scratch;\nepona_nuplan+nusc.pkl: The world model finetuned on NuScenes based on the base model;\nepona_nuplan+china.pkl: The world model fintuned on in-house China driving data based on the base model;\ndcae_td_20000.pkl: The temporal-aware DCAE (downsampling rate=32, channel dim=32, temporal patch size=6) fintuned on NuPlan video clips.\nAnd two folders of preprocessed json files:\ntest_meta_data_nuplan: Preprocessed meta infos on NuPlan test set (not including train set);\nmeta_data_nusc: Preprocessed meta infos on NuScenes train set and validation set.",
    "Dream-org/Dream-Coder-v0-Instruct-7B": "Dream-Coder-v0-Instruct-7B\nQuickstart\nDream-Coder-v0-Instruct-7B\nDream-Coder 7B is a diffusion LLM for code trained exclusively on open-source data across its development stages‚Äîadaptation, supervised fine-tuning, and reinforcement learning.\nIt achieves an impressive 21.4% pass@1 on LiveCodeBench (2410-2505), outperforming other open-source diffusion LLMs by a wide margin.\nMore details about the model and usage can be found in the blog and github bellow:\nBlog: https://hkunlp.github.io/blog/2025/dream-coder/\nGithub: https://github.com/DreamLM/Dream-Coder\nQuickstart\nTo get start with,\nplease install transformers==4.46.2 and torch==2.5.1. Here is an example to use Dream-Coder 7B:\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nmodel_path = \"Dream-org/Dream-Coder-v0-Instruct-7B\"\nmodel = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel = model.to(\"cuda\").eval()\nmessages = [\n{\"role\": \"user\", \"content\": \"Write a quick sort algorithm.\"}\n]\ninputs = tokenizer.apply_chat_template(\nmessages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True\n)\ninput_ids = inputs.input_ids.to(device=\"cuda\")\nattention_mask = inputs.attention_mask.to(device=\"cuda\")\noutput = model.diffusion_generate(\ninput_ids,\nattention_mask=attention_mask,\nmax_new_tokens=768,\noutput_history=True,\nreturn_dict_in_generate=True,\nsteps=768,\ntemperature=0.1,\ntop_p=0.95,\nalg=\"entropy\",\nalg_temp=0.,\n)\ngenerations = [\ntokenizer.decode(g[len(p) :].tolist())\nfor p, g in zip(input_ids, output.sequences)\n]\nprint(generations[0].split(tokenizer.eos_token)[0])"
}