{
    "mintujohnson/Llama-3.2-3B-Malayalam-Instruct": "Uploaded finetuned  model\nInference\nUploaded finetuned  model\nDeveloped by: mintujohnson\nLicense: apache-2.0\nFinetuned from model : unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.\nInference\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\nmodel_path = \"mintujohnson/Llama-3.2-3B-French-Instruct\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(model_name = model_path, max_seq_length = 128,\ndtype = None, load_in_4bit = True)\ndef inference(messages, model, tokenizer):\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer.apply_chat_template(\nmessages, tokenize = True,\nadd_generation_prompt = True, # Must add for generation\nreturn_tensors = \"pt\",\n).to(\"cuda\")\nprint(tokenizer.decode(inputs[0], skip_special_tokens=False))\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(\ninput_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\nuse_cache = True, temperature = 1.5, min_p = 0.1)\nmessages = [\n{\"role\": \"user\", \"content\": \"നിങ്ങൾ ആരാണ്?\"},\n]\noutput = inference(messages, model, tokenizer)",
    "FX-FeiHou/wan2.2-Remix": "",
    "NSFW-API/NSFW_Wan_14b": "",
    "Retreatcost/KansenSakura-Erosion-RP-12b": "",
    "thesby/Qwen3-VL-8B-NSFW-Caption-V4": "",
    "google-bert/bert-base-cased": "BERT base model (cased)\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nBERT base model (cased)\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\nthis paper and first released in\nthis repository. This model is case-sensitive: it makes a difference between\nenglish and English.\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\nModel description\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\nMasked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\nthe entire masked sentence through the model and has to predict the masked words. This is different from traditional\nrecurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\nGPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\nsentence.\nNext sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\nthey correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\npredict if the two sentences were following each other or not.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\nIntended uses & limitations\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the model hub to look for\nfine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n'score': 0.09019174426794052,\n'token': 4633,\n'token_str': 'fashion'},\n{'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n'score': 0.06349995732307434,\n'token': 1207,\n'token_str': 'new'},\n{'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n'score': 0.06228214129805565,\n'token': 2581,\n'token_str': 'male'},\n{'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n'score': 0.0441727414727211,\n'token': 1848,\n'token_str': 'professional'},\n{'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n'score': 0.03326151892542839,\n'token': 7688,\n'token_str': 'super'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nLimitations and bias\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',\n'score': 0.04804691672325134,\n'token': 4545,\n'token_str': 'lawyer'},\n{'sequence': '[CLS] The man worked as a waiter. [SEP]',\n'score': 0.037494491785764694,\n'token': 17989,\n'token_str': 'waiter'},\n{'sequence': '[CLS] The man worked as a cop. [SEP]',\n'score': 0.035512614995241165,\n'token': 9947,\n'token_str': 'cop'},\n{'sequence': '[CLS] The man worked as a detective. [SEP]',\n'score': 0.031271643936634064,\n'token': 9140,\n'token_str': 'detective'},\n{'sequence': '[CLS] The man worked as a doctor. [SEP]',\n'score': 0.027423162013292313,\n'token': 3995,\n'token_str': 'doctor'}]\n>>> unmasker(\"The woman worked as a [MASK].\")\n[{'sequence': '[CLS] The woman worked as a nurse. [SEP]',\n'score': 0.16927455365657806,\n'token': 7439,\n'token_str': 'nurse'},\n{'sequence': '[CLS] The woman worked as a waitress. [SEP]',\n'score': 0.1501094549894333,\n'token': 15098,\n'token_str': 'waitress'},\n{'sequence': '[CLS] The woman worked as a maid. [SEP]',\n'score': 0.05600163713097572,\n'token': 13487,\n'token_str': 'maid'},\n{'sequence': '[CLS] The woman worked as a housekeeper. [SEP]',\n'score': 0.04838843643665314,\n'token': 26458,\n'token_str': 'housekeeper'},\n{'sequence': '[CLS] The woman worked as a cook. [SEP]',\n'score': 0.029980547726154327,\n'token': 9834,\n'token_str': 'cook'}]\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nThe BERT model was pretrained on BookCorpus, a dataset consisting of 11,038\nunpublished books and English Wikipedia (excluding lists, tables and\nheaders).\nTraining procedure\nPreprocessing\nThe texts are tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form:\n[CLS] Sentence A [SEP] Sentence B [SEP]\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nPretraining\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, β1=0.9\\beta_{1} = 0.9β1​=0.9 and β2=0.999\\beta_{2} = 0.999β2​=0.999, a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\nEvaluation results\nWhen fine-tuned on downstream tasks, this model achieves the following results:\nGlue test results:\nTask\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n84.6/83.4\n71.2\n90.5\n93.5\n52.1\n85.8\n88.9\n66.4\n79.6\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1810-04805,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njournal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/abs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad": "BERT large model (uncased) whole word masking finetuned on SQuAD\nModel description\nIntended uses & limitations\nTraining procedure\nPreprocessing\nPretraining\nFine-tuning\nEvaluation results\nBibTeX entry and citation info\nBERT large model (uncased) whole word masking finetuned on SQuAD\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\nthis paper and first released in\nthis repository. This model is uncased: it does not make a difference\nbetween english and English.\nDifferently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.\nThe training is identical -- each masked WordPiece token is predicted independently.\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\nModel description\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\nMasked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\nthe entire masked sentence through the model and has to predict the masked words. This is different from traditional\nrecurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\nGPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\nsentence.\nNext sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\nthey correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\npredict if the two sentences were following each other or not.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\nThis model has the following configuration:\n24-layer\n1024 hidden dimension\n16 attention heads\n336M parameters.\nIntended uses & limitations\nThis model should be used as a question-answering model. You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the task summary of the transformers documentation.## Training data\nThe BERT model was pretrained on BookCorpus, a dataset consisting of 11,038\nunpublished books and English Wikipedia (excluding lists, tables and\nheaders).\nTraining procedure\nPreprocessing\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n[CLS] Sentence A [SEP] Sentence B [SEP]\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nPretraining\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, β1=0.9\\beta_{1} = 0.9β1​=0.9 and β2=0.999\\beta_{2} = 0.999β2​=0.999, a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\nFine-tuning\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. In order to reproduce the training, you may use the following command:\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/question-answering/run_qa.py \\\n--model_name_or_path bert-large-uncased-whole-word-masking \\\n--dataset_name squad \\\n--do_train \\\n--do_eval \\\n--learning_rate 3e-5 \\\n--num_train_epochs 2 \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir ./examples/models/wwm_uncased_finetuned_squad/ \\\n--per_device_eval_batch_size=3   \\\n--per_device_train_batch_size=3   \\\nEvaluation results\nThe results obtained are the following:\nf1 = 93.15\nexact_match = 86.91\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1810-04805,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njournal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/abs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "distilbert/distilgpt2": "DistilGPT2\nModel Details\nUses, Limitations and Risks\nHow to Get Started with the Model\nTraining Data\nTraining Procedure\nEvaluation Results\nEnvironmental Impact\nCitation\nGlossary\nDistilGPT2\nDistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of GPT-2.\nModel Details\nDeveloped by: Hugging Face\nModel type: Transformer-based Language Model\nLanguage: English\nLicense: Apache 2.0\nModel Description: DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using knowledge distillation and was designed to be a faster, lighter version of GPT-2.\nResources for more information: See this repository for more about Distil* (a class of compressed models including Distilled-GPT2), Sanh et al. (2019) for more information about knowledge distillation and the training procedure, and this page for more about GPT-2.\nUses, Limitations and Risks\nLimitations and Risks\nClick to expand\nCONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.\nAs the developers of GPT-2 (OpenAI) note in their model card, “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nDistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.\nThe impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example:\nSilva, Tambwekar and Gombolay (2021) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.\nXu and Hu (2022) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias).\nGupta et al. (2022) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2.\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(48)\n>>> generator(\"The White man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': \"The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the\"},\n{'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a \"'},\n{'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]\n>>> set_seed(48)\n>>> generator(\"The Black man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},\n{'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},\n{'generated_text': 'The Black man worked as a police spokesman four months ago...'}]\nPotential Uses\nSince DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model.\nThe developers of GPT-2 state in their model card that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including:\nWriting assistance: Grammar assistance, autocompletion (for normal prose or code)\nCreative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.\nEntertainment: Creation of games, chat bots, and amusing generations.\nUsing DistilGPT2, the Hugging Face team built the Write With Transformers web app, which allows users to play with the model to generate text directly from their browser.\nOut-of-scope Uses\nOpenAI states in the GPT-2 model card:\nBecause large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\nAdditionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.\nHow to Get Started with the Model\nClick to expand\nBe sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.\nUsing DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I’m a language model\", max_length=20, num_return_sequences=5)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n[{'generated_text': \"Hello, I'm a language model, I'm a language model. In my previous post I've\"},\n{'generated_text': \"Hello, I'm a language model, and I'd love to hear what you think about it.\"},\n{'generated_text': \"Hello, I'm a language model, but I don't get much of a connection anymore, so\"},\n{'generated_text': \"Hello, I'm a language model, a functional language... It's not an example, and that\"},\n{'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I\"}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = GPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nAnd in TensorFlow:\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = TFGPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nTraining Data\nDistilGPT2 was trained using OpenWebTextCorpus, an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the OpenWebTextCorpus Dataset Card for additional information about OpenWebTextCorpus and Radford et al. (2019) for additional information about WebText.\nTraining Procedure\nThe texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in Sanh et al. (2019).\nEvaluation Results\nThe creators of DistilGPT2 report that, on the WikiText-103 benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).\nEnvironmental Impact\nCarbon emissions were estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: 8 16GB V100\nHours used: 168 (1 week)\nCloud Provider: Azure\nCompute Region: unavailable, assumed East US for calculations\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 149.2 kg eq. CO2\nCitation\n@inproceedings{sanh2019distilbert,\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\nauthor={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\nbooktitle={NeurIPS EMC^2 Workshop},\nyear={2019}\n}\nGlossary\nKnowledge Distillation: As described in Sanh et al. (2019), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see Bucila et al. (2006) and Hinton et al. (2015).",
    "openai-community/gpt2-large": "GPT-2 Large\nTable of Contents\nModel Details\nHow to Get Started with the Model\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation Information\nModel Card Authors\nGPT-2 Large\nTable of Contents\nModel Details\nHow To Get Started With the Model\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation Information\nModel Card Authors\nModel Details\nModel Description: GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\nDeveloped by: OpenAI, see associated research paper and GitHub repo for model developers.\nModel Type: Transformer-based language model\nLanguage(s): English\nLicense: Modified MIT License\nRelated Models: GPT-2, GPT-Medium and GPT-XL\nResources for more information:\nResearch Paper\nOpenAI Blog Post\nGitHub Repo\nOpenAI Model Card for GPT-2\nTest the full generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\nHow to Get Started with the Model\nUse the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n[{'generated_text': \"Hello, I'm a language model, I can do language modeling. In fact, this is one of the reasons I use languages. To get a\"},\n{'generated_text': \"Hello, I'm a language model, which in its turn implements a model of how a human can reason about a language, and is in turn an\"},\n{'generated_text': \"Hello, I'm a language model, why does this matter for you?\\n\\nWhen I hear new languages, I tend to start thinking in terms\"},\n{'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI don't need to know anything else. If I want to understand about how\"},\n{'generated_text': \"Hello, I'm a language model, not a toolbox.\\n\\nIn a nutshell, a language model is a set of attributes that define how\"}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = GPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = TFGPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nUses\nDirect Use\nIn their model card about GPT-2, OpenAI wrote:\nThe primary intended users of these models are AI researchers and practitioners.\nWe primarily imagine these language models will be used by researchers to better understand the behaviors, capabilities, biases, and constraints of large-scale generative language models.\nDownstream Use\nIn their model card about GPT-2, OpenAI wrote:\nHere are some secondary use cases we believe are likely:\nWriting assistance: Grammar assistance, autocompletion (for normal prose or code)\nCreative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.\nEntertainment: Creation of games, chat bots, and amusing generations.\nMisuse and Out-of-scope Use\nIn their model card about GPT-2, OpenAI wrote:\nBecause large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\nAdditionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\nRisks, Limitations and Biases\nCONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propogate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n[{'generated_text': 'The man worked as a security guard in a hotel'},\n{'generated_text': 'The man worked as a salesman in Mexico and in'},\n{'generated_text': 'The man worked as a supervisor at the warehouse for'},\n{'generated_text': \"The man worked as a cleaner for the store's\"},\n{'generated_text': 'The man worked as a barbershop apprentice.'}]\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n[{'generated_text': 'The woman worked as a clerk at the bank.'},\n{'generated_text': 'The woman worked as a caregiver, and her'},\n{'generated_text': 'The woman worked as a customer service agent for a'},\n{'generated_text': 'The woman worked as a cleaner at the store,'},\n{'generated_text': 'The woman worked as a barista and was \"'}]\nThis bias will also affect all fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nTraining\nTraining Data\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\nhere.\nTraining Procedure\nThe model is pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token i only uses the inputs from 1 to i but not the future tokens.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks.\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\nEvaluation\nThe following evaluation information is extracted from the associated paper.\nTesting Data, Factors and Metrics\nThe model authors write in the associated paper that:\nSince our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex- ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string  which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results...using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible. Since these de-tokenizers are invertible, we can still calculate the log probability of a dataset and they can be thought of as a simple form of domain adaptation.\nResults\nThe model achieves the following results without any fine-tuning (zero-shot):\nDataset\nLAMBADA\nLAMBADA\nCBT-CN\nCBT-NE\nWikiText2\nPTB\nenwiki8\ntext8\nWikiText103\n1BW\n(metric)\n(PPL)\n(ACC)\n(ACC)\n(ACC)\n(PPL)\n(PPL)\n(BPB)\n(BPC)\n(PPL)\n(PPL)\n10.87\n60.12\n93.45\n88.0\n19.93\n40.31\n0.97\n1.02\n22.05\n44.575\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Unknown\nHours used: Unknown\nCloud Provider: Unknown\nCompute Region: Unknown\nCarbon Emitted: Unknown\nTechnical Specifications\nSee the associated paper for details on the modeling architecture, objective, compute infrastructure, and training details.\nCitation Information\n@article{radford2019language,\ntitle={Language models are unsupervised multitask learners},\nauthor={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\njournal={OpenAI blog},\nvolume={1},\nnumber={8},\npages={9},\nyear={2019}\n}\nModel Card Authors\nThis model card was written by the Hugging Face team.",
    "google-t5/t5-11b": "Model Card for T5 11B\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card Authors\nHow to Get Started with the Model\nDisclaimer\nModel Card for T5 11B\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nHow To Get Started With the Model\nModel Details\nModel Description\nThe developers of the Text-To-Text Transfer Transformer (T5) write:\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\nT5-11B is the checkpoint with 11 billion parameters.\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\nModel type: Language model\nLanguage(s) (NLP): English, French, Romanian, German\nLicense: Apache 2.0\nRelated Models: All T5 Checkpoints\nResources for more information:\nResearch paper\nGoogle's T5 Blog Post\nGitHub Repo\nHugging Face T5 Docs\nUses\nDirect Use and Downstream Use\nThe developers write in a blog post that the model:\nOur text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\nSee the blog post and research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nMore information needed.\nRecommendations\nMore information needed.\nTraining Details\nTraining Data\nThe model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\nThe model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.).\nThereby, the following datasets were being used for (1.) and (2.):\nDatasets used for Unsupervised denoising objective:\nC4\nWiki-DPR\nDatasets used for Supervised text-to-text language modeling objective\nSentence acceptability judgment\nCoLA Warstadt et al., 2018\nSentiment analysis\nSST-2 Socher et al., 2013\nParaphrasing/sentence similarity\nMRPC Dolan and Brockett, 2005\nSTS-B Ceret al., 2017\nQQP Iyer et al., 2017\nNatural language inference\nMNLI Williams et al., 2017\nQNLI Rajpurkar et al.,2016\nRTE Dagan et al., 2005\nCB De Marneff et al., 2019\nSentence completion\nCOPA Roemmele et al., 2011\nWord sense disambiguation\nWIC Pilehvar and Camacho-Collados, 2018\nQuestion answering\nMultiRC Khashabi et al., 2018\nReCoRD Zhang et al., 2018\nBoolQ Clark et al., 2019\nTraining Procedure\nIn their abstract, the model developers write:\nIn this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the research paper for further details.\nEvaluation\nTesting Data, Factors & Metrics\nThe developers evaluated the model on 24 tasks, see the research paper for full details.\nResults\nFor full results for T5-11B, see the research paper, Table 14.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@article{2020t5,\nauthor  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\ntitle   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\njournal = {Journal of Machine Learning Research},\nyear    = {2020},\nvolume  = {21},\nnumber  = {140},\npages   = {1-67},\nurl     = {http://jmlr.org/papers/v21/20-074.html}\n}\nAPA:\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\nModel Card Authors\nThis model card was written by the team at Hugging Face.\nHow to Get Started with the Model\nDisclaimer\nBefore transformers v3.5.0, due do its immense size, t5-11b required some special treatment.\nIf you're using transformers <= v3.4.0, t5-11b should be loaded with flag use_cdn set to False as follows:\nt5 = transformers.T5ForConditionalGeneration.from_pretrained('t5-11b', use_cdn = False)\nSecondly, a single GPU will most likely not have enough memory to even load the model into memory as the weights alone amount to over 40 GB.\nModel parallelism has to be used here to overcome this problem as is explained in this PR.\nDeepSpeed's ZeRO-Offload is another approach as explained in this post.\nSee the Hugging Face T5 docs and a Colab Notebook created by the model developers for more context.",
    "FacebookAI/xlm-roberta-large-finetuned-conll03-english": "xlm-roberta-large-finetuned-conll03-english\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation\nModel Card Authors\nHow to Get Started with the Model\nxlm-roberta-large-finetuned-conll03-english\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation\nModel Card Authors\nHow To Get Started With the Model\nModel Details\nModel Description\nThe XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is XLM-RoBERTa-large fine-tuned with the conll2003 dataset in English.\nDeveloped by: See associated paper\nModel type: Multi-lingual language model\nLanguage(s) (NLP) or Countries (images): XLM-RoBERTa is a multilingual model trained on 100 different languages; see GitHub Repo for full list; model is fine-tuned on a dataset in English\nLicense: More information needed\nRelated Models: RoBERTa, XLM\nParent Model: XLM-RoBERTa-large\nResources for more information:\n-GitHub Repo\n-Associated Paper\nUses\nDirect Use\nThe model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text.\nDownstream Use\nPotential downstream use cases include Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. To learn more about token classification and other potential downstream use cases, see the Hugging Face token classification docs.\nOut-of-Scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people.\nBias, Risks, and Limitations\nCONTENT WARNING: Readers should be made aware that language generated by this model may be disturbing or offensive to some and may propagate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). In the context of tasks relevant to this model, Mishra et al. (2020) explore social biases in NER systems for English and find that there is systematic bias in existing NER systems in that they fail to identify named entities from different demographic groups (though this paper did not look at BERT). For example, using a sample sentence from Mishra et al. (2020):\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n[{'end': 2,\n'entity': 'I-PER',\n'index': 1,\n'score': 0.9997861,\n'start': 0,\n'word': '▁Al'},\n{'end': 4,\n'entity': 'I-PER',\n'index': 2,\n'score': 0.9998591,\n'start': 2,\n'word': 'ya'},\n{'end': 16,\n'entity': 'I-PER',\n'index': 4,\n'score': 0.99995816,\n'start': 10,\n'word': '▁Jasmin'},\n{'end': 17,\n'entity': 'I-PER',\n'index': 5,\n'score': 0.9999584,\n'start': 16,\n'word': 'e'},\n{'end': 29,\n'entity': 'I-PER',\n'index': 7,\n'score': 0.99998057,\n'start': 23,\n'word': '▁Andrew'}]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nTraining\nSee the following resources for training data and training procedure details:\nXLM-RoBERTa-large model card\nCoNLL-2003 data card\nAssociated paper\nEvaluation\nSee the associated paper for evaluation details.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: 500 32GB Nvidia V100 GPUs (from the associated paper)\nHours used: More information needed\nCloud Provider: More information needed\nCompute Region: More information needed\nCarbon Emitted: More information needed\nTechnical Specifications\nSee the associated paper for further details.\nCitation\nBibTeX:\n@article{conneau2019unsupervised,\ntitle={Unsupervised Cross-lingual Representation Learning at Scale},\nauthor={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\njournal={arXiv preprint arXiv:1911.02116},\nyear={2019}\n}\nAPA:\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\nModel Card Authors\nThis model card was written by the team at Hugging Face.\nHow to Get Started with the Model\nUse the code below to get started with the model. You can use this model directly within a pipeline for NER.\nClick to expand\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Hello I'm Omar and I live in Zürich.\")\n[{'end': 14,\n'entity': 'I-PER',\n'index': 5,\n'score': 0.9999175,\n'start': 10,\n'word': '▁Omar'},\n{'end': 35,\n'entity': 'I-LOC',\n'index': 10,\n'score': 0.9999906,\n'start': 29,\n'word': '▁Zürich'}]",
    "CAMeL-Lab/bert-base-arabic-camelbert-mix-did-madar-corpus26": "CAMeLBERT-Mix DID Madar Corpus26 Model\nModel description\nIntended uses\nCitation\nCAMeLBERT-Mix DID Madar Corpus26 Model\nModel description\nCAMeLBERT-Mix DID Madar Corpus26 Model is a dialect identification (DID) model that was built by fine-tuning the CAMeLBERT-Mix model.\nFor the fine-tuning, we used the MADAR Corpus 26 dataset, which includes 26 labels.\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper \"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.\" Our fine-tuning code can be found here.\nIntended uses\nYou can use the CAMeLBERT-Mix DID Madar Corpus26 model as part of the transformers pipeline.\nThis model will also be available in CAMeL Tools soon.\nHow to use\nTo use the model with a transformers pipeline:\n>>> from transformers import pipeline\n>>> did = pipeline('text-classification', model='CAMeL-Lab/bert-base-arabic-camelbert-mix-did-madar-corpus26')\n>>> sentences = ['عامل ايه ؟', 'شلونك ؟ شخبارك ؟']\n>>> did(sentences)\n[{'label': 'CAI', 'score': 0.8751305937767029},\n{'label': 'DOH', 'score': 0.9867215156555176}]\nNote: to download our models, you would need transformers>=3.5.0.\nOtherwise, you could download the models manually.\nCitation\n@inproceedings{inoue-etal-2021-interplay,\ntitle = \"The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\",\nauthor = \"Inoue, Go  and\nAlhafni, Bashar  and\nBaimukan, Nurpeiis  and\nBouamor, Houda  and\nHabash, Nizar\",\nbooktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\nmonth = apr,\nyear = \"2021\",\naddress = \"Kyiv, Ukraine (Online)\",\npublisher = \"Association for Computational Linguistics\",\nabstract = \"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\",\n}",
    "CAMeL-Lab/bert-base-arabic-camelbert-mix-ner": "CAMeLBERT-Mix NER Model\nModel description\nIntended uses\nCitation\nCAMeLBERT-Mix NER Model\nModel description\nCAMeLBERT-Mix NER Model is a Named Entity Recognition (NER) model that was built by fine-tuning the CAMeLBERT Mix model.\nFor the fine-tuning, we used the ANERcorp dataset.\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper \"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.\n\" Our fine-tuning code can be found here.\nIntended uses\nYou can use the CAMeLBERT-Mix NER model directly as part of our CAMeL Tools NER component (recommended) or as part of the transformers pipeline.\nHow to use\nTo use the model with the CAMeL Tools NER component:\n>>> from camel_tools.ner import NERecognizer\n>>> from camel_tools.tokenizers.word import simple_word_tokenize\n>>> ner = NERecognizer('CAMeL-Lab/bert-base-arabic-camelbert-mix-ner')\n>>> sentence = simple_word_tokenize('إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع')\n>>> ner.predict_sentence(sentence)\n>>> ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']\nYou can also use the NER model directly with a transformers pipeline:\n>>> from transformers import pipeline\n>>> ner = pipeline('ner', model='CAMeL-Lab/bert-base-arabic-camelbert-mix-ner')\n>>> ner(\"إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع\")\n[{'word': 'أبوظبي',\n'score': 0.9895730018615723,\n'entity': 'B-LOC',\n'index': 2,\n'start': 6,\n'end': 12},\n{'word': 'الإمارات',\n'score': 0.8156259655952454,\n'entity': 'B-LOC',\n'index': 8,\n'start': 33,\n'end': 41},\n{'word': 'العربية',\n'score': 0.890906810760498,\n'entity': 'I-LOC',\n'index': 9,\n'start': 42,\n'end': 49},\n{'word': 'المتحدة',\n'score': 0.8169114589691162,\n'entity': 'I-LOC',\n'index': 10,\n'start': 50,\n'end': 57}]\nNote: to download our models, you would need transformers>=3.5.0.\nOtherwise, you could download the models manually.\nCitation\n@inproceedings{inoue-etal-2021-interplay,\ntitle = \"The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\",\nauthor = \"Inoue, Go  and\nAlhafni, Bashar  and\nBaimukan, Nurpeiis  and\nBouamor, Houda  and\nHabash, Nizar\",\nbooktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\nmonth = apr,\nyear = \"2021\",\naddress = \"Kyiv, Ukraine (Online)\",\npublisher = \"Association for Computational Linguistics\",\nabstract = \"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\",\n}",
    "CAMeL-Lab/bert-base-arabic-camelbert-msa-ner": "CAMeLBERT MSA NER Model\nModel description\nIntended uses\nCitation\nCAMeLBERT MSA NER Model\nModel description\nCAMeLBERT MSA NER Model is a Named Entity Recognition (NER) model that was built by fine-tuning the CAMeLBERT Modern Standard Arabic (MSA) model.\nFor the fine-tuning, we used the ANERcorp dataset.\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper \"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.\n\" Our fine-tuning code can be found here.\nIntended uses\nYou can use the CAMeLBERT MSA NER model directly as part of our CAMeL Tools NER component (recommended) or as part of the transformers pipeline.\nHow to use\nTo use the model with the CAMeL Tools NER component:\n>>> from camel_tools.ner import NERecognizer\n>>> from camel_tools.tokenizers.word import simple_word_tokenize\n>>> ner = NERecognizer('CAMeL-Lab/bert-base-arabic-camelbert-msa-ner')\n>>> sentence = simple_word_tokenize('إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع')\n>>> ner.predict_sentence(sentence)\n>>> ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']\nYou can also use the NER model directly with a transformers pipeline:\n>>> from transformers import pipeline\n>>> ner = pipeline('ner', model='CAMeL-Lab/bert-base-arabic-camelbert-msa-ner')\n>>> ner(\"إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع\")\n[{'word': 'أبوظبي',\n'score': 0.9895730018615723,\n'entity': 'B-LOC',\n'index': 2,\n'start': 6,\n'end': 12},\n{'word': 'الإمارات',\n'score': 0.8156259655952454,\n'entity': 'B-LOC',\n'index': 8,\n'start': 33,\n'end': 41},\n{'word': 'العربية',\n'score': 0.890906810760498,\n'entity': 'I-LOC',\n'index': 9,\n'start': 42,\n'end': 49},\n{'word': 'المتحدة',\n'score': 0.8169114589691162,\n'entity': 'I-LOC',\n'index': 10,\n'start': 50,\n'end': 57}]\nNote: to download our models, you would need transformers>=3.5.0.\nOtherwise, you could download the models manually.\nCitation\n@inproceedings{inoue-etal-2021-interplay,\ntitle = \"The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\",\nauthor = \"Inoue, Go  and\nAlhafni, Bashar  and\nBaimukan, Nurpeiis  and\nBouamor, Houda  and\nHabash, Nizar\",\nbooktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\nmonth = apr,\nyear = \"2021\",\naddress = \"Kyiv, Ukraine (Online)\",\npublisher = \"Association for Computational Linguistics\",\nabstract = \"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\",\n}",
    "CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment": "CAMeLBERT MSA SA Model\nModel description\nIntended uses\nCitation\nCAMeLBERT MSA SA Model\nModel description\nCAMeLBERT MSA SA Model is a Sentiment Analysis (SA) model that was built by fine-tuning the CAMeLBERT Modern Standard Arabic (MSA) model.\nFor the fine-tuning, we used the ASTD, ArSAS, and SemEval datasets.\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper \"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.\" Our fine-tuning code can be found here.\nIntended uses\nYou can use the CAMeLBERT MSA SA model directly as part of our CAMeL Tools SA component (recommended) or as part of the transformers pipeline.\nHow to use\nTo use the model with the CAMeL Tools SA component:\n>>> from camel_tools.sentiment import SentimentAnalyzer\n>>> sa = SentimentAnalyzer(\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n>>> sentences = ['أنا بخير', 'أنا لست بخير']\n>>> sa.predict(sentences)\n>>> ['positive', 'negative']\nYou can also use the SA model directly with a transformers pipeline:\n>>> from transformers import pipeline\n>>> sa = pipeline('sentiment-analysis', model='CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment')\n>>> sentences = ['أنا بخير', 'أنا لست بخير']\n>>> sa(sentences)\n[{'label': 'positive', 'score': 0.9616648554801941},\n{'label': 'negative', 'score': 0.9779177904129028}]\nNote: to download our models, you would need transformers>=3.5.0.\nOtherwise, you could download the models manually.\nCitation\n@inproceedings{inoue-etal-2021-interplay,\ntitle = \"The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\",\nauthor = \"Inoue, Go  and\nAlhafni, Bashar  and\nBaimukan, Nurpeiis  and\nBouamor, Houda  and\nHabash, Nizar\",\nbooktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\nmonth = apr,\nyear = \"2021\",\naddress = \"Kyiv, Ukraine (Online)\",\npublisher = \"Association for Computational Linguistics\",\nabstract = \"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\",\n}",
    "EleutherAI/gpt-j-6b": "GPT-J 6B\nModel Description\nIntended Use and Limitations\nOut-of-scope use\nLimitations and Biases\nHow to use\nTraining data\nTraining procedure\nEvaluation results\nCitation and Related Information\nBibTeX entry\nAcknowledgements\nGPT-J 6B\nModel Description\nGPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. \"GPT-J\" refers to the class of model, while \"6B\" represents the number of trainable parameters.\nHyperparameter\nValue\nnparametersn_{parameters}nparameters​\n6053381344\nnlayersn_{layers}nlayers​\n28*\ndmodeld_{model}dmodel​\n4096\ndffd_{ff}dff​\n16384\nnheadsn_{heads}nheads​\n16\ndheadd_{head}dhead​\n256\nnctxn_{ctx}nctx​\n2048\nnvocabn_{vocab}nvocab​\n50257/50400† (same tokenizer as GPT-2/3)\nPositional Encoding\nRotary Position Embedding (RoPE)\nRoPE Dimensions\n64\n* Each layer consists of one feedforward block and one self attention block.\n† Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer.\nThe model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model\ndimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64\ndimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as\nGPT-2/GPT-3.\nIntended Use and Limitations\nGPT-J learns an inner representation of the English language that can be used to\nextract features useful for downstream tasks. The model is best at what it was\npretrained for however, which is generating text from a prompt.\nOut-of-scope use\nGPT-J-6B is not intended for deployment without fine-tuning, supervision,\nand/or moderation. It is not a in itself a product and cannot be used for\nhuman-facing interactions. For example, the model may generate harmful or\noffensive text. Please evaluate the risks associated with your particular use case.\nGPT-J-6B was trained on an English-language only dataset, and is thus not\nsuitable for translation or generating text in other languages.\nGPT-J-6B has not been fine-tuned for downstream contexts in which\nlanguage models are commonly deployed, such as writing genre prose,\nor commercial chatbots. This means GPT-J-6B will not\nrespond to a given prompt the way a product like ChatGPT does. This is because,\nunlike this model, ChatGPT was fine-tuned using methods such as Reinforcement\nLearning from Human Feedback (RLHF) to better “follow” human instructions.\nLimitations and Biases\nThe core functionality of GPT-J is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work. When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most \"accurate\" text. Never depend upon GPT-J to produce factually accurate output.\nGPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See Sections 5 and 6 of the Pile paper for a more detailed analysis of the biases in the Pile.\nAs with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.\nHow to use\nThis model can be easily loaded using the AutoModelForCausalLM functionality:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\nTraining data\nGPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI.\nTraining procedure\nThis model was trained for 402 billion tokens over 383,500 steps on TPU v3-256 pod. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token correctly.\nEvaluation results\nModel\nPublic\nTraining FLOPs\nLAMBADA PPL ↓\nLAMBADA Acc ↑\nWinogrande ↑\nHellaswag ↑\nPIQA ↑\nDataset Size (GB)\nRandom Chance\n✓\n0\n~a lot\n~0%\n50%\n25%\n25%\n0\nGPT-3 Ada‡\n✗\n-----\n9.95\n51.6%\n52.9%\n43.4%\n70.5%\n-----\nGPT-2 1.5B\n✓\n-----\n10.63\n51.21%\n59.4%\n50.9%\n70.8%\n40\nGPT-Neo 1.3B‡\n✓\n3.0e21\n7.50\n57.2%\n55.0%\n48.9%\n71.1%\n825\nMegatron-2.5B*\n✗\n2.4e21\n-----\n61.7%\n-----\n-----\n-----\n174\nGPT-Neo 2.7B‡\n✓\n6.8e21\n5.63\n62.2%\n56.5%\n55.8%\n73.0%\n825\nGPT-3 1.3B*‡\n✗\n2.4e21\n5.44\n63.6%\n58.7%\n54.7%\n75.1%\n~800\nGPT-3 Babbage‡\n✗\n-----\n5.58\n62.4%\n59.0%\n54.5%\n75.5%\n-----\nMegatron-8.3B*\n✗\n7.8e21\n-----\n66.5%\n-----\n-----\n-----\n174\nGPT-3 2.7B*‡\n✗\n4.8e21\n4.60\n67.1%\n62.3%\n62.8%\n75.6%\n~800\nMegatron-11B†\n✓\n1.0e22\n-----\n-----\n-----\n-----\n-----\n161\nGPT-J 6B‡\n✓\n1.5e22\n3.99\n69.7%\n65.3%\n66.1%\n76.5%\n825\nGPT-3 6.7B*‡\n✗\n1.2e22\n4.00\n70.3%\n64.5%\n67.4%\n78.0%\n~800\nGPT-3 Curie‡\n✗\n-----\n4.00\n69.3%\n65.6%\n68.5%\n77.9%\n-----\nGPT-3 13B*‡\n✗\n2.3e22\n3.56\n72.5%\n67.9%\n70.9%\n78.5%\n~800\nGPT-3 175B*‡\n✗\n3.1e23\n3.00\n76.2%\n70.2%\n78.9%\n81.0%\n~800\nGPT-3 Davinci‡\n✗\n-----\n3.0\n75%\n72%\n78%\n80%\n-----\nModels roughly sorted by performance, or by FLOPs if not available.\n* Evaluation numbers reported by their respective authors. All other numbers are provided by\nrunning lm-evaluation-harness either with released\nweights or with API access. Due to subtle implementation differences as well as different zero shot task framing, these\nmight not be directly comparable. See this blog post for more\ndetails.\n† Megatron-11B provides no comparable metrics, and several implementations using the released weights do not\nreproduce the generation quality and evaluations. (see 1\n2 3)\nThus, evaluation was not attempted.\n‡ These models have been trained with data which contains possible test set contamination. The OpenAI GPT-3 models\nfailed to deduplicate training data for certain test sets, while the GPT-Neo models as well as this one is\ntrained on the Pile, which has not been deduplicated against any test sets.\nCitation and Related Information\nBibTeX entry\nTo cite this model:\n@misc{gpt-j,\nauthor = {Wang, Ben and Komatsuzaki, Aran},\ntitle = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},\nhowpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\nyear = 2021,\nmonth = May\n}\nTo cite the codebase that trained this model:\n@misc{mesh-transformer-jax,\nauthor = {Wang, Ben},\ntitle = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\nhowpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\nyear = 2021,\nmonth = May\n}\nIf you use this model, we would love to hear about it! Reach out on GitHub, Discord, or shoot Ben an email.\nAcknowledgements\nThis project would not have been possible without compute generously provided by Google through the\nTPU Research Cloud, as well as the Cloud TPU team for providing early access to the Cloud TPU VM Alpha.\nThanks to everyone who have helped out one way or another (listed alphabetically):\nJames Bradbury for valuable assistance with debugging JAX issues.\nStella Biderman, Eric Hallahan, Kurumuz, and Finetune for converting the model to be compatible with the transformers package.\nLeo Gao for running zero shot evaluations for the baseline models for the table.\nLaurence Golding for adding some features to the web demo.\nAran Komatsuzaki for advice with experiment design and writing the blog posts.\nJanko Prester for creating the web demo frontend.",
    "EleutherAI/gpt-neo-2.7B": "GPT-Neo 2.7B\nModel Description\nTraining data\nTraining procedure\nIntended Use and Limitations\nHow to use\nLimitations and Biases\nEval results\nLinguistic Reasoning\nPhysical and Scientific Reasoning\nDown-Stream Applications\nBibTeX entry and citation info\nGPT-Neo 2.7B\nModel Description\nGPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 2.7B represents the number of parameters of this particular pre-trained model.\nTraining data\nGPT-Neo 2.7B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.\nTraining procedure\nThis model was trained for 420 billion tokens over 400,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.\nIntended Use and Limitations\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\nHow to use\nYou can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:\n>>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n>>> generator(\"EleutherAI has\", do_sample=True, min_length=50)\n[{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]\nLimitations and Biases\nGPT-Neo was trained as an autoregressive language model. This means that its core functionality is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work.\nGPT-Neo was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending on your usecase GPT-Neo may produce socially unacceptable text. See Sections 5 and 6 of the Pile paper for a more detailed analysis of the biases in the Pile.\nAs with all language models, it is hard to predict in advance how GPT-Neo will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.\nEval results\nAll evaluations were done using our evaluation harness. Some results for GPT-2 and GPT-3 are inconsistent with the values reported in the respective papers. We are currently looking into why, and would greatly appreciate feedback and further testing of our eval harness. If you would like to contribute evaluations you have done, please reach out on our Discord.\nLinguistic Reasoning\nModel and Size\nPile BPB\nPile PPL\nWikitext PPL\nLambada PPL\nLambada Acc\nWinogrande\nHellaswag\nGPT-Neo 1.3B\n0.7527\n6.159\n13.10\n7.498\n57.23%\n55.01%\n38.66%\nGPT-2 1.5B\n1.0468\n-----\n17.48\n10.634\n51.21%\n59.40%\n40.03%\nGPT-Neo 2.7B\n0.7165\n5.646\n11.39\n5.626\n62.22%\n56.50%\n42.73%\nGPT-3 Ada\n0.9631\n-----\n-----\n9.954\n51.60%\n52.90%\n35.93%\nPhysical and Scientific Reasoning\nModel and Size\nMathQA\nPubMedQA\nPiqa\nGPT-Neo 1.3B\n24.05%\n54.40%\n71.11%\nGPT-2 1.5B\n23.64%\n58.33%\n70.78%\nGPT-Neo 2.7B\n24.72%\n57.54%\n72.14%\nGPT-3 Ada\n24.29%\n52.80%\n68.88%\nDown-Stream Applications\nTBD\nBibTeX entry and citation info\nTo cite this model, use\n@software{gpt-neo,\nauthor       = {Black, Sid and\nLeo, Gao and\nWang, Phil and\nLeahy, Connor and\nBiderman, Stella},\ntitle        = {{GPT-Neo: Large Scale Autoregressive Language\nModeling with Mesh-Tensorflow}},\nmonth        = mar,\nyear         = 2021,\nnote         = {{If you use this software, please cite it using\nthese metadata.}},\npublisher    = {Zenodo},\nversion      = {1.0},\ndoi          = {10.5281/zenodo.5297715},\nurl          = {https://doi.org/10.5281/zenodo.5297715}\n}\n@article{gao2020pile,\ntitle={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},\nauthor={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},\njournal={arXiv preprint arXiv:2101.00027},\nyear={2020}\n}",
    "Geotrend/distilbert-base-zh-cased": "distilbert-base-zh-cased\nHow to use\nHow to cite\nContact\ndistilbert-base-zh-cased\nWe are sharing smaller versions of distilbert-base-multilingual-cased that handle a custom number of languages.\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\nFor more information please visit our paper: Load What You Need: Smaller Versions of Multilingual BERT.\nHow to use\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-zh-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-zh-cased\")\nTo generate other smaller versions of multilingual transformers please visit our Github repo.\nHow to cite\n@inproceedings{smallermdistilbert,\ntitle={Load What You Need: Smaller Versions of Mutlilingual BERT},\nauthor={Abdaoui, Amine and Pradel, Camille and Sigel, Grégoire},\nbooktitle={SustaiNLP / EMNLP},\nyear={2020}\n}\nContact\nPlease contact amine@geotrend.fr for any question, feedback or request.",
    "GroNLP/hateBERT": "Model description\nBibTeX entry and citation info\nTommaso Caselli •\nValerio Basile •\nJelena Mitrovic •\nMichael Granizter\nModel description\nHateBERT is an English pre-trained BERT model obtained by further training the English BERT base uncased model with more than 1 million posts from banned communites from Reddit. The model has been developed as a collaboration between the University of Groningen, the university of Turin, and the University of Passau.\nFor details, check out the paper presented at WOAH 2021. The code and the fine-tuned models are available on OSF.\nBibTeX entry and citation info\n@inproceedings{caselli-etal-2021-hatebert,\n\\ttitle = \"{H}ate{BERT}: Retraining {BERT} for Abusive Language Detection in {E}nglish\",\n\\tauthor = \"Caselli, Tommaso  and\nBasile, Valerio  and\nMitrovi{\\'c}, Jelena  and\nGranitzer, Michael\",\n\\tbooktitle = \"Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)\",\n\\tmonth = aug,\n\\tyear = \"2021\",\n\\taddress = \"Online\",\n\\tpublisher = \"Association for Computational Linguistics\",\n\\tturl = \"https://aclanthology.org/2021.woah-1.3\",\n\\tdoi = \"10.18653/v1/2021.woah-1.3\",\n\\tpages = \"17--25\",\n\\tabstract = \"We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.\",\n}",
    "Hate-speech-CNERG/dehatebert-mono-german": "For more details about our paper\nThis model is used detecting hatespeech in German language. The mono in the name refers to the monolingual setting, where the model is trained using only English language data. It is finetuned on multilingual bert model.\nThe model is trained with different learning rates and the best validation score achieved is 0.649794 for a learning rate of 3e-5. Training code can be found at this url\nFor more details about our paper\nSai Saketh Aluru, Binny Mathew, Punyajoy Saha and Animesh Mukherjee. \"Deep Learning Models for Multilingual Hate Speech Detection\". Accepted at ECML-PKDD 2020.\nPlease cite our paper in any published work that uses any of these resources.\n@article{aluru2020deep,\ntitle={Deep Learning Models for Multilingual Hate Speech Detection},\nauthor={Aluru, Sai Saket and Mathew, Binny and Saha, Punyajoy and Mukherjee, Animesh},\njournal={arXiv preprint arXiv:2004.06465},\nyear={2020}\n}",
    "Helsinki-NLP/opus-mt-ROMANCE-en": "Benchmarks\nopus-mt-ROMANCE-en\nsource languages: fr,fr_BE,fr_CA,fr_FR,wa,frp,oc,ca,rm,lld,fur,lij,lmo,es,es_AR,es_CL,es_CO,es_CR,es_DO,es_EC,es_ES,es_GT,es_HN,es_MX,es_NI,es_PA,es_PE,es_PR,es_SV,es_UY,es_VE,pt,pt_br,pt_BR,pt_PT,gl,lad,an,mwl,it,it_IT,co,nap,scn,vec,sc,ro,la\ntarget languages: en\nOPUS readme: fr+fr_BE+fr_CA+fr_FR+wa+frp+oc+ca+rm+lld+fur+lij+lmo+es+es_AR+es_CL+es_CO+es_CR+es_DO+es_EC+es_ES+es_GT+es_HN+es_MX+es_NI+es_PA+es_PE+es_PR+es_SV+es_UY+es_VE+pt+pt_br+pt_BR+pt_PT+gl+lad+an+mwl+it+it_IT+co+nap+scn+vec+sc+ro+la-en\ndataset: opus\nmodel: transformer\npre-processing: normalization + SentencePiece\ndownload original weights: opus-2020-04-01.zip\ntest set translations: opus-2020-04-01.test.txt\ntest set scores: opus-2020-04-01.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nTatoeba.fr.en\n62.2\n0.750",
    "Helsinki-NLP/opus-mt-en-ig": "Benchmarks\nopus-mt-en-ig\nsource languages: en\ntarget languages: ig\nOPUS readme: en-ig\ndataset: opus\nmodel: transformer-align\npre-processing: normalization + SentencePiece\ndownload original weights: opus-2020-01-08.zip\ntest set translations: opus-2020-01-08.test.txt\ntest set scores: opus-2020-01-08.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nJW300.en.ig\n39.5\n0.546\nTatoeba.en.ig\n3.8\n0.297",
    "Helsinki-NLP/opus-mt-en-zh": "Benchmarks\nSystem Info:\neng-zho\nsource group: English\ntarget group: Chinese\nOPUS readme: eng-zho\nmodel: transformer\nsource language(s): eng\ntarget language(s): cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant gan lzh lzh_Hans nan wuu yue yue_Hans yue_Hant\nmodel: transformer\npre-processing: normalization + SentencePiece (spm32k,spm32k)\na sentence initial language token is required in the form of >>id<< (id = valid target language ID)\ndownload original weights: opus-2020-07-17.zip\ntest set translations: opus-2020-07-17.test.txt\ntest set scores: opus-2020-07-17.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nTatoeba-test.eng.zho\n31.4\n0.268\nSystem Info:\nhf_name: eng-zho\nsource_languages: eng\ntarget_languages: zho\nopus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md\noriginal_repo: Tatoeba-Challenge\ntags: ['translation']\nlanguages: ['en', 'zh']\nsrc_constituents: {'eng'}\ntgt_constituents: {'cmn_Hans', 'nan', 'nan_Hani', 'gan', 'yue', 'cmn_Kana', 'yue_Hani', 'wuu_Bopo', 'cmn_Latn', 'yue_Hira', 'cmn_Hani', 'cjy_Hans', 'cmn', 'lzh_Hang', 'lzh_Hira', 'cmn_Hant', 'lzh_Bopo', 'zho', 'zho_Hans', 'zho_Hant', 'lzh_Hani', 'yue_Hang', 'wuu', 'yue_Kana', 'wuu_Latn', 'yue_Bopo', 'cjy_Hant', 'yue_Hans', 'lzh', 'cmn_Hira', 'lzh_Yiii', 'lzh_Hans', 'cmn_Bopo', 'cmn_Hang', 'hak_Hani', 'cmn_Yiii', 'yue_Hant', 'lzh_Kana', 'wuu_Hani'}\nsrc_multilingual: False\ntgt_multilingual: False\nprepro:  normalization + SentencePiece (spm32k,spm32k)\nurl_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip\nurl_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt\nsrc_alpha3: eng\ntgt_alpha3: zho\nshort_pair: en-zh\nchrF2_score: 0.268\nbleu: 31.4\nbrevity_penalty: 0.8959999999999999\nref_len: 110468.0\nsrc_name: English\ntgt_name: Chinese\ntrain_date: 2020-07-17\nsrc_alpha2: en\ntgt_alpha2: zh\nprefer_old: False\nlong_pair: eng-zho\nhelsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\ntransformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\nport_machine: brutasse\nport_time: 2020-08-21-14:41",
    "Helsinki-NLP/opus-mt-ig-en": "Benchmarks\nopus-mt-ig-en\nsource languages: ig\ntarget languages: en\nOPUS readme: ig-en\ndataset: opus\nmodel: transformer-align\npre-processing: normalization + SentencePiece\ndownload original weights: opus-2020-01-09.zip\ntest set translations: opus-2020-01-09.test.txt\ntest set scores: opus-2020-01-09.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nJW300.ig.en\n36.7\n0.520\nTatoeba.ig.en\n46.3\n0.528",
    "Helsinki-NLP/opus-mt-jap-en": "Benchmarks\nopus-mt-jap-en\nsource languages: jap\ntarget languages: en\nOPUS readme: jap-en\ndataset: opus\nmodel: transformer-align\npre-processing: normalization + SentencePiece\ndownload original weights: opus-2020-01-09.zip\ntest set translations: opus-2020-01-09.test.txt\ntest set scores: opus-2020-01-09.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nbible-uedin.jap.en\n52.6\n0.703",
    "Helsinki-NLP/opus-mt-mt-en": "Benchmarks\nopus-mt-mt-en\nsource languages: mt\ntarget languages: en\nOPUS readme: mt-en\ndataset: opus\nmodel: transformer-align\npre-processing: normalization + SentencePiece\ndownload original weights: opus-2020-01-16.zip\ntest set translations: opus-2020-01-16.test.txt\ntest set scores: opus-2020-01-16.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nJW300.mt.en\n49.0\n0.655\nTatoeba.mt.en\n53.3\n0.685",
    "Helsinki-NLP/opus-mt-ru-en": "Table of Contents\nDirect Use\nTraining Data\nResults\nBenchmarks\nModel Details\nDirect Use\nTraining Data\nResults\nBenchmarks\nUses\nDirect Use\nTraining Data\nResults\nBenchmarks\nRisks, Limitations and Biases\nTraining Data\nResults\nBenchmarks\nTraining\nTraining Data\nResults\nBenchmarks\nEvaluation\nResults\nBenchmarks\nCitation Information\nHow to Get Started With the Model\nopus-mt-ru-en\nTable of Contents\nModel Details\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nCitation Information\nHow to Get Started With the Model\nModel Details\nModel Description:\nDeveloped by: Language Technology Research Group at the University of Helsinki\nModel Type: Transformer-align\nLanguage(s):\nSource Language: Russian\nTarget Language: English\nLicense: CC-BY-4.0\nResources for more information:\nGitHub Repo\nUses\nDirect Use\nThis model can be used for translation and text-to-text generation.\nRisks, Limitations and Biases\nCONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nFurther details about the dataset for this model can be found in the OPUS readme: ru-en\nTraining\nTraining Data\nPreprocessing\nPre-processing: Normalization + SentencePiece\nDataset: opus\nDownload original weights: opus-2020-02-26.zip\nTest set translations: opus-2020-02-26.test.txt\nEvaluation\nResults\ntest set scores: opus-2020-02-26.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nnewstest2012.ru.en\n34.8\n0.603\nnewstest2013.ru.en\n27.9\n0.545\nnewstest2014-ruen.ru.en\n31.9\n0.591\nnewstest2015-enru.ru.en\n30.4\n0.568\nnewstest2016-enru.ru.en\n30.1\n0.565\nnewstest2017-enru.ru.en\n33.4\n0.593\nnewstest2018-enru.ru.en\n29.6\n0.565\nnewstest2019-ruen.ru.en\n31.4\n0.576\nTatoeba.ru.en\n61.1\n0.736\nCitation Information\n@InProceedings{TiedemannThottingal:EAMT2020,\nauthor = {J{\\\"o}rg Tiedemann and Santhosh Thottingal},\ntitle = {{OPUS-MT} — {B}uilding open translation services for the {W}orld},\nbooktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},\nyear = {2020},\naddress = {Lisbon, Portugal}\n}\nHow to Get Started With the Model\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")",
    "Helsinki-NLP/opus-mt-zh-en": "Table of Contents\nDirect Use\nSystem Information\nTraining Data\nResults\nModel Details\nDirect Use\nSystem Information\nTraining Data\nResults\nUses\nDirect Use\nSystem Information\nTraining Data\nResults\nRisks, Limitations and Biases\nSystem Information\nTraining Data\nResults\nTraining\nSystem Information\nTraining Data\nResults\nEvaluation\nResults\nBenchmarks\nCitation Information\nHow to Get Started With the Model\nzho-eng\nTable of Contents\nModel Details\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nCitation Information\nHow to Get Started With the Model\nModel Details\nModel Description:\nDeveloped by: Language Technology Research Group at the University of Helsinki\nModel Type: Translation\nLanguage(s):\nSource Language:  Chinese\nTarget Language: English\nLicense: CC-BY-4.0\nResources for more information:\nGitHub Repo\nUses\nDirect Use\nThis model can be used for translation and text-to-text generation.\nRisks, Limitations and Biases\nCONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nFurther details about the dataset for this model can be found in the OPUS readme: zho-eng\nTraining\nSystem Information\nhelsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\ntransformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\nport_machine: brutasse\nport_time: 2020-08-21-14:41\nsrc_multilingual: False\ntgt_multilingual: False\nTraining Data\nPreprocessing\npre-processing: normalization + SentencePiece (spm32k,spm32k)\nref_len: 82826.0\ndataset: opus\ndownload original weights: opus-2020-07-17.zip\ntest set translations: opus-2020-07-17.test.txt\nEvaluation\nResults\ntest set scores: opus-2020-07-17.eval.txt\nbrevity_penalty: 0.948\nBenchmarks\ntestset\nBLEU\nchr-F\nTatoeba-test.zho.eng\n36.1\n0.548\nCitation Information\n@InProceedings{TiedemannThottingal:EAMT2020,\nauthor = {J{\\\"o}rg Tiedemann and Santhosh Thottingal},\ntitle = {{OPUS-MT} — {B}uilding open translation services for the {W}orld},\nbooktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},\nyear = {2020},\naddress = {Lisbon, Portugal}\n}\nHow to Get Started With the Model\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")",
    "Intel/bert-base-uncased-sparse-1_2": "Sparse BERT base model (uncased)\nIntended Use\nEvaluation Results\nSparse BERT base model (uncased)\nPretrained model pruned to 1:2 structured sparsity.\nThe model is a pruned version of the BERT base model.\nIntended Use\nThe model can be used for fine-tuning to downstream tasks with sparsity already embeded to the model.\nTo keep the sparsity a mask should be added to each sparse weight blocking the optimizer from updating the zeros.\nEvaluation Results\nWe get the following results on the tasks development set, all results are mean of 5 different seeded models:\nTask\nMNLI-m (Acc)\nMNLI-mm (Acc)\nQQP (Acc/F1)\nQNLI (Acc)\nSST-2 (Acc)\nSTS-B (Pears/Spear)\nSQuADv1.1 (Acc/F1)\n83.3\n83.9\n90.8/87.6\n90.4\n91.3\n88.8/88.3\n80.5/88.2",
    "Jacobo/aristoBERTo": "YAML Metadata\nError:\n\"tags\" must be an array\naristoBERTo\nIntended uses\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\naristoBERTo\naristoBERTo is a transformer model for ancient Greek, a low resource  language. We initialized the pre-training with weights from GreekBERT, a Greek version of BERT which was trained on a large corpus of modern Greek (~ 30 GB of texts). We continued the pre-training with an ancient Greek corpus of about 900 MB, which was scrapped from the web and post-processed. Duplicate texts and editorial punctuation were removed.\nApplied to the processing of ancient Greek, aristoBERTo outperforms xlm-roberta-base and mdeberta in most downstream tasks like the labeling of POS, MORPH, DEP and LEMMA.\naristoBERTo is provided by the Diogenet project of the University of California, San Diego.\nIntended uses\nThis model was created for fine-tuning with spaCy and the ancient Greek Universal Dependency datasets as well as a NER corpus produced by the Diogenet project. As a fill-mask model, AristoBERTo can also be used in the restoration of damaged Greek papyri, inscriptions, and manuscripts.\nIt achieves the following results on the evaluation set:\nLoss: 1.6323\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 20.0\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\n1.377\n20.0\n3414220\n1.6314\nFramework versions\nTransformers 4.14.0.dev0\nPytorch 1.10.0+cu102\nDatasets 1.16.1\nTokenizers 0.10.3"
}