{
    "valiantcat/Qwen-Image-Edit-Remover-General-LoRA": "valiantcat Qwen-Image-Edit LoRA\nModel Card for Model ID\nDirect Use\nTrigger phrase\nDownload model\nTraining at Chongqing Valiant Cat\nvaliantcat Qwen-Image-Edit LoRA\nPrompt\nÁßªÈô§Áå´Áå´\nPrompt\nÂêåÊó∂ÁßªÈô§Ëøô‰∏™Â∞èÁî∑Â≠©ÂíåËá™Ë°åËΩ¶\nPrompt\nÂÆåÂÖ®ÁßªÈô§Ëøô‰∏™Â•≥‰∫∫\nModel Card for Model ID\nThis is a model for object removal, trained on Qwen/Qwen-Image-Edit, and is suitable for object removal tasks of e-commerce images, character images, and object images.For use in ComfyUI.\nThe greatest advantage of using this LORA is that it maintains the consistency of the original image without changing any parts.\nComfyUI Workflow\nThis LoRA works with a modified version of Comfy's Qwen-Image-Edit workflow. The main modification is adding a Qwen-Image-Edit LoRA node connected to the base model.\nSee the Downloads section above for the modified workflow.\nDirect Use\nfrom diffusers import QwenImageEditPipeline\nimport torch\nfrom PIL import Image\n# Load the pipeline\npipeline = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\")\npipeline.to(torch.bfloat16)\npipeline.to(\"cuda\")\n# Load trained LoRA weights for in-scene editing\npipeline.load_lora_weights(\"valiantcat/Qwen-Image-Edit-Remover-General-LoRA\",weight_name=\"qwen-edit-remover.safetensors\")\n# Load input image\nimage = Image.open(\"./result/test.png\").convert(\"RGB\")\n# Define in-scene editing prompt\nprompt = \"ÁßªÈô§Áå´Áå´\"\n# Generate edited image with enhanced scene understanding\ninputs = {\n\"image\": image,\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(12345),\n\"true_cfg_scale\": 4.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 50,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"restlt.png\")\nTrigger phrase\n‰ªéÂú∫ÊôØ‰∏≠ÁßªÈô§XXX\nThere is no fixed trigger word. The specific removal prompt needs to be tested more\nDownload model\nWeights for this model are available in Safetensors format.\nDownload\nTraining at Chongqing Valiant Cat\nThis model was trained by the AI Laboratory of Chongqing Valiant Cat Technology Co., LTD(https://vvicat.com/).Business cooperation is welcome",
    "ESpeech/ESpeech-TTS-1_RL-V2": "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:\npip install f5-tts gradio ruaccent transformers torch torchaudio huggingface_hub\n–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–¥ –∏ –∂–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∞–¥—Ä–µ—Å–æ–º –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞\n#!/usr/bin/env python3\nimport os\nimport gc\nimport tempfile\nimport traceback\nfrom pathlib import Path\nimport gradio as gr\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport torchaudio\nfrom huggingface_hub import hf_hub_download, snapshot_download\nfrom ruaccent import RUAccent\nfrom f5_tts.infer.utils_infer import (\ninfer_process,\nload_model,\nload_vocoder,\npreprocess_ref_audio_text,\nremove_silence_for_generated_wav,\nsave_spectrogram,\ntempfile_kwargs,\n)\nfrom f5_tts.model import DiT\nMODEL_CFG = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\nMODEL_REPO = \"ESpeech/ESpeech-TTS-1_RL-V2\"\nMODEL_FILE = \"espeech_tts_rlv2.pt\"\nVOCAB_FILE = \"vocab.txt\"\nloaded_model = None\ndef ensure_model():\nglobal loaded_model\nif loaded_model is not None:\nreturn loaded_model\nmodel_path = None\nvocab_path = None\nprint(f\"Trying to download model file '{MODEL_FILE}' and '{VOCAB_FILE}' from hub '{MODEL_REPO}'\")\ntry:\nmodel_path = hf_hub_download(repo_id=MODEL_REPO, filename=MODEL_FILE)\nvocab_path = hf_hub_download(repo_id=MODEL_REPO, filename=VOCAB_FILE)\nprint(f\"Downloaded model to {model_path}\")\nprint(f\"Downloaded vocab to {vocab_path}\")\nexcept Exception as e:\nprint(\"hf_hub_download failed:\", e)\nif model_path is None or vocab_path is None:\ntry:\nlocal_dir = f\"cache_{MODEL_REPO.replace('/', '_')}\"\nprint(f\"Attempting snapshot_download into {local_dir}...\")\nsnapshot_dir = snapshot_download(repo_id=MODEL_REPO, cache_dir=None, local_dir=local_dir, token=hf_token)\npossible_model = os.path.join(snapshot_dir, MODEL_FILE)\npossible_vocab = os.path.join(snapshot_dir, VOCAB_FILE)\nif os.path.exists(possible_model):\nmodel_path = possible_model\nif os.path.exists(possible_vocab):\nvocab_path = possible_vocab\nprint(f\"Snapshot downloaded to {snapshot_dir}\")\nexcept Exception as e:\nprint(\"snapshot_download failed:\", e)\nif not model_path or not os.path.exists(model_path):\nraise FileNotFoundError(f\"Model file not found after download attempts: {model_path}\")\nif not vocab_path or not os.path.exists(vocab_path):\nraise FileNotFoundError(f\"Vocab file not found after download attempts: {vocab_path}\")\nprint(f\"Loading model from: {model_path}\")\nloaded_model = load_model(DiT, MODEL_CFG, model_path, vocab_file=vocab_path)\nreturn loaded_model\nprint(\"Preloading model...\")\ntry:\nensure_model()\nprint(\"Model preloaded.\")\nexcept Exception as e:\nprint(f\"Model preload failed: {e}\")\nprint(\"Loading RUAccent...\")\naccentizer = RUAccent()\naccentizer.load(omograph_model_size='turbo3.1', use_dictionary=True, tiny_mode=False)\nprint(\"RUAccent loaded.\")\nprint(\"Loading vocoder...\")\nvocoder = load_vocoder()\nprint(\"Vocoder loaded.\")\ndef process_text_with_accent(text, accentizer):\nif not text or not text.strip():\nreturn text\nif '+' in text:\nreturn text\nelse:\nreturn accentizer.process_all(text)\ndef process_texts_only(ref_text, gen_text):\nprocessed_ref_text = process_text_with_accent(ref_text, accentizer)\nprocessed_gen_text = process_text_with_accent(gen_text, accentizer)\nreturn processed_ref_text, processed_gen_text\ndef synthesize(\nref_audio,\nref_text,\ngen_text,\nremove_silence,\nseed,\ncross_fade_duration=0.15,\nnfe_step=32,\nspeed=1.0,\n):\nif not ref_audio:\ngr.Warning(\"Please provide reference audio.\")\nreturn None, None, ref_text, gen_text\nif seed is None or seed < 0 or seed > 2**31 - 1:\nseed = np.random.randint(0, 2**31 - 1)\ntorch.manual_seed(int(seed))\nif not gen_text or not gen_text.strip():\ngr.Warning(\"Please enter text to generate.\")\nreturn None, None, ref_text, gen_text\nif not ref_text or not ref_text.strip():\ngr.Warning(\"Please provide reference text.\")\nreturn None, None, ref_text, gen_text\nprocessed_ref_text = process_text_with_accent(ref_text, accentizer)\nprocessed_gen_text = process_text_with_accent(gen_text, accentizer)\ntry:\nmodel = ensure_model()\nexcept Exception as e:\ngr.Warning(f\"Failed to load model: {e}\")\nreturn None, None, processed_ref_text, processed_gen_text\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntry:\nif device.type == \"cuda\":\ntry:\nmodel.to(device)\nvocoder.to(device)\nexcept Exception as e:\nprint(\"Warning: failed to move model/vocoder to cuda:\", e)\ntry:\nref_audio_proc, processed_ref_text_final = preprocess_ref_audio_text(\nref_audio,\nprocessed_ref_text,\nshow_info=gr.Info\n)\nexcept Exception as e:\ngr.Warning(f\"Preprocess failed: {e}\")\ntraceback.print_exc()\nreturn None, None, processed_ref_text, processed_gen_text\ntry:\nfinal_wave, final_sample_rate, combined_spectrogram = infer_process(\nref_audio_proc,\nprocessed_ref_text_final,\nprocessed_gen_text,\nmodel,\nvocoder,\ncross_fade_duration=cross_fade_duration,\nnfe_step=nfe_step,\nspeed=speed,\nshow_info=gr.Info,\nprogress=gr.Progress(),\n)\nexcept Exception as e:\ngr.Warning(f\"Infer failed: {e}\")\ntraceback.print_exc()\nreturn None, None, processed_ref_text, processed_gen_text\nif remove_silence:\ntry:\nwith tempfile.NamedTemporaryFile(suffix=\".wav\", **tempfile_kwargs) as f:\ntemp_path = f.name\nsf.write(temp_path, final_wave, final_sample_rate)\nremove_silence_for_generated_wav(temp_path)\nfinal_wave_tensor, _ = torchaudio.load(temp_path)\nfinal_wave = final_wave_tensor.squeeze().cpu().numpy()\nexcept Exception as e:\nprint(\"Remove silence failed:\", e)\ntry:\nwith tempfile.NamedTemporaryFile(suffix=\".png\", **tempfile_kwargs) as tmp_spectrogram:\nspectrogram_path = tmp_spectrogram.name\nsave_spectrogram(combined_spectrogram, spectrogram_path)\nexcept Exception as e:\nprint(\"Save spectrogram failed:\", e)\nspectrogram_path = None\nreturn (final_sample_rate, final_wave), spectrogram_path, processed_ref_text_final, processed_gen_text\nfinally:\nif device.type == \"cuda\":\ntry:\nmodel.to(\"cpu\")\nvocoder.to(\"cpu\")\ntorch.cuda.empty_cache()\ngc.collect()\nexcept Exception as e:\nprint(\"Warning during cuda cleanup:\", e)\nwith gr.Blocks(title=\"ESpeech-TTS\") as app:\ngr.Markdown(\"# ESpeech-TTS\")\ngr.Markdown(\"üí° **–°–æ–≤–µ—Ç:** –î–æ–±–∞–≤—å—Ç–µ —Å–∏–º–≤–æ–ª '+' –¥–ª—è —É–¥–∞—Ä–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ø—Ä–∏–≤+–µ—Ç')\")\ngr.Markdown(\"‚ùå **–°–æ–≤–µ—Ç:** –†–µ—Ñ–µ—Ä–µ–Ω—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ –±–æ–ª–µ–µ 12 —Å–µ–∫—É–Ω–¥\")\nwith gr.Row():\nwith gr.Column():\nref_audio_input = gr.Audio(label=\"Reference Audio\", type=\"filepath\")\nref_text_input = gr.Textbox(\nlabel=\"Reference Text\",\nlines=2,\nplaceholder=\"Text corresponding to reference audio\"\n)\nwith gr.Column():\ngen_text_input = gr.Textbox(\nlabel=\"Text to Generate\",\nlines=5,\nmax_lines=20,\nplaceholder=\"Enter text to synthesize...\"\n)\nprocess_text_btn = gr.Button(\"‚úèÔ∏è Process Text (Add Accents)\", variant=\"secondary\")\nwith gr.Accordion(\"Advanced Settings\", open=False):\nwith gr.Row():\nseed_input = gr.Number(label=\"Seed (-1 for random)\", value=-1, precision=0)\nremove_silence = gr.Checkbox(label=\"Remove Silences\", value=False)\nwith gr.Row():\nspeed_slider = gr.Slider(label=\"Speed\", minimum=0.3, maximum=2.0, value=1.0, step=0.1)\nnfe_slider = gr.Slider(label=\"NFE Steps\", minimum=4, maximum=64, value=48, step=2)\ncross_fade_slider = gr.Slider(label=\"Cross-Fade Duration (s)\", minimum=0.0, maximum=1.0, value=0.15, step=0.01)\ngenerate_btn = gr.Button(\"üé§ Generate Speech\", variant=\"primary\", size=\"lg\")\nwith gr.Row():\naudio_output = gr.Audio(label=\"Generated Audio\", type=\"numpy\")\nspectrogram_output = gr.Image(label=\"Spectrogram\", type=\"filepath\")\nprocess_text_btn.click(\nprocess_texts_only,\ninputs=[ref_text_input, gen_text_input],\noutputs=[ref_text_input, gen_text_input]\n)\ngenerate_btn.click(\nsynthesize,\ninputs=[\nref_audio_input,\nref_text_input,\ngen_text_input,\nremove_silence,\nseed_input,\ncross_fade_slider,\nnfe_slider,\nspeed_slider,\n],\noutputs=[audio_output, spectrogram_output, ref_text_input, gen_text_input]\n)\nif __name__ == \"__main__\":\napp.launch()",
    "pixasocial/survival-uncensored-gemma-270m-v2": "Uncensored-Q-270M-v2\nModel Overview\nIntended Uses\nOffline Usage\nQuantization Explanations\nDeployment on Ollama\nDeployment on Phone\nDeployment on Desktop\nTraining Parameters\nPerformance Benchmarks\nTechnical Documentation\nEthical Considerations\nExport Guide\nUncensored-Q-270M-v2\nUncensored-Q-270M-v2 is a fine-tuned version of google/gemma-3-270m-it, featuring 268 million parameters. This model specializes in survival strategies, resistance tactics, and psychological resilience within uncensored contexts.\nModel Overview\nBase Model: google/gemma-3-270m-it\nParameters: 268 million\nLanguages: Primarily English, with support for over 140 languages\nLicense: Gemma Terms of Use\nAuthor: pixasocial\nFine-Tuning: Hugging Face Transformers and TRL/SFTTrainer on an expanded curated dataset of ~200,000 examples across survival, resistance, psychology, and related themes\nHardware: NVIDIA A40 GPU\nSFT Training Time: ~10 hours\nNext Steps: PPO training planned\nIntended Uses\nPrimary: Advice on survival, resistance, psychological coping\nSecondary: Offline mobile deployment for emergencies\nNot for harmful/illegal use; validate outputs\nOffline Usage\nThe model supports GGUF format for deployment on various platforms, including Android/iOS via apps like MLC Chat or Ollama. The Q4_K_M variant (253 MB) is suitable for devices with 4GB+ RAM. Detailed instructions follow for Ollama, mobile phones, and desktops.\nQuantization Explanations\nQuantization reduces model precision to optimize size and inference speed while maintaining functionality. Below is a table of available GGUF variants with precise file sizes from the repository, along with recommended use cases:\nQuantization Type\nFile Size\nRecommended Hardware\nAccuracy vs. Speed Trade-off\nf16 (base)\n543 MB\nHigh-end desktops/GPUs\nHighest accuracy, larger size, suitable for precise tasks\nQ8_0\n292 MB\nDesktops with 8GB+ RAM\nHigh accuracy, moderate size and speed\nQ6_K\n283 MB\nLaptops/mid-range desktops\nGood balance, minor accuracy loss\nQ5_K_M\n260 MB\nMobile desktops/low-end GPUs\nEfficient, slight reduction in quality\nQ5_K_S\n258 MB\nMobile desktops\nSimilar to Q5_K_M but optimized for smaller footprints\nQ4_K_M\n253 MB\nSmartphones (4GB+ RAM)\nFast inference, acceptable accuracy for mobile\nQ4_K_S\n250 MB\nSmartphones/edge devices\nFaster than Q4_K_M, more compression\nQ3_K_L\n246 MB\nLow-RAM devices\nHigher compression, noticeable quality drop\nQ3_K_M\n242 MB\nEdge devices\nBalanced 3-bit, for constrained environments\nQ3_K_S\n237 MB\nVery low-resource devices\nMaximum compression at 3-bit, prioritized speed\nIQ4_XS\n241 MB\nSmartphones/hybrids\nIntelligent quantization, efficient with preserved performance\nQ2_K\n237 MB\nMinimal hardware\nSmallest size, fastest but lowest accuracy\nSelect based on device constraints: higher-bit variants for accuracy, lower for portability.\nHere is a handy graph by ikawrakow comparing some lower-quality quant types (lower is better):\nAnd here are Artefact2's thoughts on the matter: https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nDeployment on Ollama\nOllama facilitates local GGUF model execution on desktops.\nInstall Ollama from ollama.com.\nPull a variant: ollama pull q1776/survival-uncensored-gemma-270m-v2:Q4_K_M.gguf.\nRun: ollama run q1776/survival-uncensored-gemma-270m-v2:Q4_K_M.gguf.\nUse Modelfiles from the modelfiles folder for customization: Download (e.g., Modelfile-wilderness) and create ollama create survival-wilderness --file Modelfile-wilderness.\nDeployment on Phone\nFor Android/iOS:\nMLC Chat: Download from mlc.ai. Import GGUF (e.g., Q4_K_M, 253 MB) and query offline. Requires 4GB RAM; expect 5-10 tokens/second.\nTermux (Android): Install Termux, then Ollama. Pull and run as above.\niOS: Use Ollama-compatible apps or simulators; native options limited.\nDeployment on Desktop\nLM Studio: From lmstudio.ai; import GGUF and use UI.\nvLLM: pip install vllm; serve with python -m vllm.entrypoints.openai.api_server --model q1776/survival-uncensored-gemma-270m-v2:Q4_K_M.gguf --port 8000.\nTraining Parameters\nEpochs: 5\nBatch Size: 4 per device, effective 16\nLearning Rate: 1e-5\nOptimizer: AdamW\nWeight Decay: 0.01\nScheduler: Linear\nMax Sequence Length: 512\nPrecision: bf16\nWarmup Steps: 5\nSeed: 3407\nLoss: Cross-entropy, ~2.0 to <1.5\nPerformance Benchmarks\nImproved on specialized queries. Scores (/10):\nSurvival Advice: 9.5\nResistance Tactics: 9.0\nPsychology Insights: 9.2\nInference Speed Graph (tokens/second, approximate):\nHardware\nQ8_0\nQ4_K_M\nQ2_K\nNVIDIA A40\n25\n35\n45\nDesktop GPU\n15\n25\n35\nSmartphone\nN/A\n8\n12\nTechnical Documentation\nTransformer-based, multimodal (text+images, 896x896). Context: 32K tokens. Deploy via vLLM or RunPod.\nEthical Considerations\nUncensored; may generate controversial content. User responsibility. Limitations: Hallucinations on obscure topics. Impact: ~10 kWh energy.\nExport Guide\nConvert to GGUF for Ollama, vLLM for inference, RunPod for API.",
    "OpenGVLab/InternVL3_5-241B-A28B": "InternVL3_5-241B-A28B\nIntroduction\nInternVL3.5 Family\nGithub Format\nHuggingFace Format\nModel Architecture\nTraining and Deployment Strategy\nPre-Training\nSupervised Fine-Tuning\nCascade Reinforcement Learning\nVisual Consistency Learning\nTest-Time Scaling\nDecoupled Vision-Language Deployment\nEvaluation on Multimodal Capability\nMultimodal Reasoning and Mathematics\nOCR, Chart, and Document Understanding\nMulti-Image Understanding & Real-World Comprehension\nComprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\nVisual Grounding\nMultimodal Multilingual Understanding\nVideo Understanding\nGUI Tasks\nEmbodied Tasks\nSVG Tasks\nEvaluation on Language Capability\nAblation Study\nCascade Reinforcement Learning\nDecoupled Vision-Language Deployment\nQuick Start\nModel Loading\nThinking Mode\nInference with Transformers\nFinetune\nDeployment\nLMDeploy\nLicense\nCitation\nInternVL3_5-241B-A28B\n[üìÇ GitHub] [üìú InternVL 1.0] [üìú InternVL 1.5] [üìú InternVL 2.5] [üìú InternVL2.5-MPO] [üìú InternVL3] [üìú InternVL3.5]\n[üÜï Blog] [üó®Ô∏è Chat Demo] [üöÄ Quick Start] [üìñ Documents]\nIntroduction\nWe introduce InternVL3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05 √ó\\times√ó inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e.,  InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks‚Äînarrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.\nHatched bars represent closed-source commercial models. We report average scores on a set of multimodal general, reasoning, text, and agentic benchmarks: MMBench v1.1 (en), MMStar,BLINK, HallusionBench, AI2D, OCRBench, MMVet, MME-RealWorld (en), MVBench, VideoMME, MMMU, MathVista, MathVision, MathVerse, DynaMath, WeMath, LogicVista, MATH500, AIME24, AIME25, GPQA, MMLU-Pro, GAOKAO, IFEval, SGP-Bench, VSI-Bench, ERQA, SpaCE-10, and OmniSpatial.\nSee quick start for how to use our model.\nInternVL3.5 Family\nIn the following table, we provide an overview of the InternVL3.5 series.\nTo maintain consistency with earlier generations, we provide two model formats: the GitHub format, consistent with prior releases, and the HF format, aligned with the official Transformers standard.\nIf you want to convert the checkpoint between these two formats, please refer to the scripts about custom2hf and hf2custom.\nGithub Format\nModel\n#Vision Param\n#Language Param\n#Total Param\nHF Link\nModelScope Link\nInternVL3.5-1B\n0.3B\n0.8B\n1.1B\nü§ó link\nü§ñ link\nInternVL3.5-2B\n0.3B\n2.0B\n2.3B\nü§ó link\nü§ñ link\nInternVL3.5-4B\n0.3B\n4.4B\n4.7B\nü§ó link\nü§ñ link\nInternVL3.5-8B\n0.3B\n8.2B\n8.5B\nü§ó link\nü§ñ link\nInternVL3.5-14B\n0.3B\n14.8B\n15.1B\nü§ó link\nü§ñ link\nInternVL3.5-38B\n5.5B\n32.8B\n38.4B\nü§ó link\nü§ñ link\nInternVL3.5-20B-A4B\n0.3B\n20.9B\n21.2B-A4B\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B\n0.3B\n30.5B\n30.8B-A3B\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B\n5.5B\n235.1B\n240.7B-A28B\nü§ó link\nü§ñ link\nHuggingFace Format\nModel\n#Vision Param\n#Language Param\n#Total Param\nHF Link\nModelScope Link\nInternVL3.5-1B-HF\n0.3B\n0.8B\n1.1B\nü§ó link\nü§ñ link\nInternVL3.5-2B-HF\n0.3B\n2.0B\n2.3B\nü§ó link\nü§ñ link\nInternVL3.5-4B-HF\n0.3B\n4.4B\n4.7B\nü§ó link\nü§ñ link\nInternVL3.5-8B-HF\n0.3B\n8.2B\n8.5B\nü§ó link\nü§ñ link\nInternVL3.5-14B-HF\n0.3B\n14.8B\n15.1B\nü§ó link\nü§ñ link\nInternVL3.5-38B-HF\n5.5B\n32.8B\n38.4B\nü§ó link\nü§ñ link\nInternVL3.5-20B-A4B-HF\n0.3B\n20.9B\n21.2B-A4B\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-HF\n0.3B\n30.5B\n30.8B-A3B\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-HF\n5.5B\n235.1B\n240.7B-A28B\nü§ó link\nü§ñ link\nWe conduct the evaluation with VLMEvalkit. To enable the Thinking mode of our model, please set the system prompt to R1_SYSTEM_PROMPT. When enabling Thinking mode, we recommend setting do_sample=True and temperature=0.6 to mitigate undesired repetition.\nOur training pipeline comprises four stages: Multimodal Continual Pre-Training (CPT), Supervised Fine-Tuning (SFT), and Cascade Reinforcement Learning (CascadeRL). In CascadeRL, we first fine-tune the model using Mixed Preference Optimization (MPO) under an offline RL setting, followed by GSPO under an oneline RL setting.\nFor the Flash version of InternVL3.5, we additionally introduce a lightweight training stage, termed Visual Consistency Learning (ViCO), which reduces the token cost required to represent an image patch.\nHere, we also open-source the model weights after different training stages for potential research usage.\nIf you're unsure which version to use, please select the one without any suffix, as it has completed the full training pipeline.\nModel\nTraining Pipeline\nHF Link\nModelScope Link\nInternVL3.5-1B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-1B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-1B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-1B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-2B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-2B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-2B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-2B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-4B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-4B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-4B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-4B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-8B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-8B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-8B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-8B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-14B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-14B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-14B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-14B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-38B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-38B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-38B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-38B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nThe Flash version of our model will be released as soon as possible.\nModel Architecture\nInternVL3.5:\nThis series of models follow the \"ViT‚ÄìMLP‚ÄìLLM\" paradigm adopted in previous versions of InternVL.\nWe initialize the language model using the Qwen3 series and GPT-OSS, and the vision encoder using InternViT-300M and InternViT-6B.\nThe Dynamic High Resolution strategy introduced in InternVL1.5 is also retained in our design.\nInternVL3.5-Flash:\nCompared to InternVL3.5, InternVL3.5-Flash further integrates the Visual Resolution Router (ViR), thus yielding a series of  efficient variants friendly  suitable for  resource-constrained scenarios.\nSpecifically, in InternVL3.5, each image patch is initially represented as 1024 visual tokens for the vision encoder, which are then compressed into 256 tokens via a pixel shuffle module before being passed to the Large Language Model (LLM).\nIn InternVL3.5-Flash, as shown in the Figure below, an additional pixel shuffle module with a higher compression rate is included, enabling the compression of visual tokens down to 64 tokens.\nFor each patch, the patch router determines the appropriate compression rate by assessing its semantic richness, and routes it to the corresponding pixel shuffle module accordingly.\nBenefiting from this patch-aware compression mechanism, InternVL3.5-Flash is able to reduce the number of visual tokens by 50% while maintaining nearly 100% of the performance of InternVL3.5.\nTraining and Deployment Strategy\nPre-Training\nDuring the pre-training stage, we update all model parameters jointly using the combination of large-scale text and multimodal corpora. Specifically, given an arbitrary training sample consisting of a multimodal token sequence x=(x1,x2,‚Ä¶,xL)\\mathbf{x}=\\left(x_1, x_2, \\ldots, x_L\\right)x=(x1‚Äã,x2‚Äã,‚Ä¶,xL‚Äã), the next token prediction (NTP) loss is calculated on each text token as follows:\nLi=‚àílog‚Å°pŒ∏(xi‚à£x1,‚Ä¶,xi‚àí1),\n\\mathcal{L}_{i}=-\\log p_\\theta\\left(x_i \\mid x_1, \\ldots, x_{i-1}\\right),\nLi‚Äã=‚àílogpŒ∏‚Äã(xi‚Äã‚à£x1‚Äã,‚Ä¶,xi‚àí1‚Äã),\nwhere xix_ixi‚Äã is the predicted token and  prefix tokens in {x1,x2,‚Ä¶,xi‚àí1}\\{x_1, x_2, \\ldots, x_{i-1}\\}{x1‚Äã,x2‚Äã,‚Ä¶,xi‚àí1‚Äã} can be either  text tokens or  image tokens. Notably, for conversation samples, only response tokens  are included for the calculation of the loss.\nAdditionally, to mitigate bias toward either longer or shorter responses during training, we adopt the square averaging to re-weight the NTP loss  as follows:\nLi‚Ä≤=wi‚àëjwj‚ãÖLi,wi=1N0.5,\n\\mathcal{L}_{i}^{'} = \\frac{w_i}{\\sum_j w_j} \\cdot \\mathcal{L}_i, \\quad w_i = \\frac{1}{N^{0.5}},\nLi‚Ä≤‚Äã=‚àëj‚Äãwj‚Äãwi‚Äã‚Äã‚ãÖLi‚Äã,wi‚Äã=N0.51‚Äã,\nwhere NNN denotes the number of tokens in the training sample on which the loss needs to be calculated. The random JPEG compression is also included to enhance the model's real-world performance.\nSupervised Fine-Tuning\nDuring the SFT phase, we adopt the same objective as in the pre-training stage and use the  square-root averaging strategy to calculate the final loss.  In this stage, the context window is set to 32K tokens to adapt long-context information.\nCompared to InternVL3, the SFT stage of InternVL3.5 contains  more high-quality and  diverse training data derived from three sources:\n(1) Instruction-following data from InternVL3, which are reused to preserve broad coverage of vision‚Äìlanguage tasks.\n(2) Multimodal reasoning data in the \"Thinking\" mode, which are included to instill long-thinking capabilities in the model. To construct such data, we first use InternVL3-78B to describe the image and then input the description into DeepSeek-R1 to sample rollouts with detailed reasoning processes. Rollouts with an incorrect final answer are filtered out. The questions in these datasets cover various expert domains, such as mathematics and scientific disciplines, thereby strengthening performance on different reasoning tasks.\n(3) Capability-expansion datasets, which endow InternVL3.5 with new skills, including GUI-based interaction, embodied interaction, and scalable vect\nCascade Reinforcement Learning\nCascade RL aims to combine the benefits of offline RL and online RL to progressively facilitate the post-training of MLLMs in an efficient manner.\nSpecifically, we first fine-tune the model using an offline RL algorithm as an efficient warm-up stage to reach a satisfied results, which can guarantee the high-quality rollouts for the latter stage.\nSubsequently, we employ an online RL algorithm to further refine the output distribution based on rollouts generated by the model itself.  Compared to the single offline or online RL stage, our cascaded RL achieves significant performance improvements at a fraction of the GPU time cost.\nDuring the offline RL stage, we employ mixed preference optimization (MPO) to fine-tune the model. Specifically, the training objective of MPO is a combination of preference loss Lp\\mathcal{L}_{p}Lp‚Äã, quality loss Lq\\mathcal{L}_{q}Lq‚Äã, and generation loss Lg\\mathcal{L}_{g}Lg‚Äã, which can be formulated as follows:\nLMPO=wpLp+wqLq+wgLg,\n\\mathcal{L}_{\\text{MPO}}=\nw_{p} \\mathcal{L}_{p}\n+\nw_{q} \\mathcal{L}_{q}\n+\nw_{g} \\mathcal{L}_{g}\n,\nLMPO‚Äã=wp‚ÄãLp‚Äã+wq‚ÄãLq‚Äã+wg‚ÄãLg‚Äã,\nwhere w‚àów_{*}w‚àó‚Äã represents the weight assigned to each loss component.\nThe DPO loss, BCO loss, and LM loss serve as the preference loss, quality loss, and generation loss, respectively.\nDuring the online RL stage, we employ GSPO, without reference model constraints, as our online RL algorithm, which we find more effective in training both dense and mixture-of-experts (MoE) models. Similar to GRPO, the advantage is defined as the normalized reward across responses sampled from the same query.\nThe training objective of GSPO is given by:\nLGSPO(Œ∏)=Ex‚àºD,{yi}i=1G‚àºœÄŒ∏¬†old¬†(‚ãÖ‚à£x)[1G‚àëi=1Gmin‚Å°(si(Œ∏)A^i,clip‚Å°(si(Œ∏),1‚àíŒµ,1+Œµ)A^i)],\n\\mathcal{L}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D},\\left\\{y_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta \\text { old }}(\\cdot \\mid x)}\\left[\\frac{1}{G} \\sum_{i=1}^G \\min \\left(s_i(\\theta) \\widehat{A}_i, \\operatorname{clip}\\left(s_i(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_i\\right)\\right],\nLGSPO‚Äã(Œ∏)=Ex‚àºD,{yi‚Äã}i=1G‚Äã‚àºœÄŒ∏¬†old¬†‚Äã(‚ãÖ‚à£x)‚Äã[G1‚Äãi=1‚àëG‚Äãmin(si‚Äã(Œ∏)Ai‚Äã,clip(si‚Äã(Œ∏),1‚àíŒµ,1+Œµ)Ai‚Äã)],\nwhere the importance sampling ratio is defined as the geometric mean of the per-token ratios.\nPlease see our paper for more technical and experimental details.\nVisual Consistency Learning\nWe further include ViCO as an additional training stage to integrate the visual resolution router (ViR) into InternVL3.5, thereby reducing the inference cost of InternVL3.5. The obtained efficient version of InternVL3.5 are termed as InternVL3.5-Flash. In particular, ViCO comprises two stages:\nConsistency training:\nIn this stage, the entire model is trained to minimize the divergence between response distributions conditioned on visual tokens with different compression rates.\nIn practice, we introduce an extra reference model, which is frozen and initialized with InternVL3.5.\nGiven a sample, each image patch is represented as either 256 or 64 tokens, and the training objective is defined as follows:\nLViCO=EŒæ‚àºR[1N‚àëi=1NKL(œÄŒ∏ref(yi‚à£y<i,I)‚ÄÖ‚Ää‚à•‚ÄÖ‚ÄäœÄŒ∏policy(yi‚à£y<i,IŒæ))],\n\\mathcal{L}_\\text{ViCO} =\n\\mathbb{E}_{\\xi \\sim \\mathcal{R}} \\Bigg[\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{KL} \\Big(\n\\pi_{\\theta_{ref}}\\left(y_i \\mid y_{<i}, I\\right) \\;\\Big\\|\\;\n\\pi_{\\theta_{policy}}\\left(y_i \\mid y_{<i}, I_\\xi\\right)\n\\Big)\n\\Bigg],\nLViCO‚Äã=EŒæ‚àºR‚Äã[N1‚Äãi=1‚àëN‚ÄãKL(œÄŒ∏ref‚Äã‚Äã(yi‚Äã‚à£y<i‚Äã,I)‚ÄãœÄŒ∏policy‚Äã‚Äã(yi‚Äã‚à£y<i‚Äã,IŒæ‚Äã))],\nwhere \\(\\mathrm{KL}) denotes the KL divergence and (\\xi) denotes the compression rate, which is uniformly sampled from ({\\frac{1}{4},\\frac{1}{16}}). The image (I_\\xi) is represented as 256 tokens when (\\xi=\\frac{1}{4}) and 64 tokens when (\\xi=\\frac{1}{16}). Notably, the reference model always performs inference with (\\xi=\\frac{1}{4}).\nRouter training:\nThis stage aims to train the ViR to select an appropriate trade-off resolution for different inputs.\nViR is formulated as a binary classifier and trained using standard cross-entropy loss.\nTo construct the route targets, we first compute the KL divergence between the model outputs conditioned on uncompressed visual tokens (i.e., 256 tokens per patch) and those conditioned on compressed visual tokens (i.e., 64 tokens per patch).\nDuring this stage, the main MLLM (ViT, MLP and LLM) is kept frozen, and only the ViR is trained.\nSpecifically, we first compute the loss ratio for each patch:\nri=LViCO(yi‚à£I116)LViCO(yi‚à£I14),\nr_i = \\frac{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{16}}\\big)}{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{4}}\\big)},\nri‚Äã=LViCO‚Äã(yi‚Äã‚à£I41‚Äã‚Äã)LViCO‚Äã(yi‚Äã‚à£I161‚Äã‚Äã)‚Äã,\nwhich quantifies the relative increase in loss caused by compressing the visual tokens. Based on this ratio, the binary ground-truth label for the patch router is defined as:\nyirouter={0,ri<œÑ‚ÄÖ‚Ää(compression¬†has¬†negligible¬†impact)1,ri‚â•œÑ‚ÄÖ‚Ää(compression¬†has¬†significant¬†impact),\ny_i^\\text{router} =\n\\begin{cases}\n0, & r_i < \\tau \\; \\text{(compression has negligible impact)} \\\\\n1, & r_i \\ge \\tau \\; \\text{(compression has significant impact)},\n\\end{cases}\nyirouter‚Äã={0,1,‚Äãri‚Äã<œÑ(compression¬†has¬†negligible¬†impact)ri‚Äã‚â•œÑ(compression¬†has¬†significant¬†impact),‚Äã\nwhere (y_i^{\\text{router}}=0) and (y_i^{\\text{router}}=1)  indicate that the compression rate (\\xi) is set to (\\tfrac{1}{16}) and (\\tfrac{1}{4}), respectively.\nPlease see our paper for more technical and experimental details.\nTest-Time Scaling\nTest-time scaling (TTS) has been empirically demonstrated as an effective approach to enhance the reasoning capabilities of LLMs and MLLMs, particularly for complex tasks necessitating multi-step inference.\nIn this work, we implement a comprehensive test-time scaling approach that simultaneously improves reasoning depth (i.e., deep thinking) and breadth (i.e., parallel thinking).\nDeep Thinking: By activating the Thinking mode, we guide the model to deliberately engage in step-by-step reasoning (i.e., decomposing complex problems into logical steps and validating intermediate conclusions) prior to generating the final answer. This approach systematically improves the logical structure of solutions for complex problems, particularly those requiring multi-step inference, and enhances reasoning depth.\nParallel Thinking: Following InternVL3, for reasoning tasks, we adopt the Best-of-N (BoN) strategy by employing VisualPRM-v1.1 as the critic model to select the optimal response from multiple reasoning candidates.\nThis approach improves reasoning breadth.\nNotably, unless otherwise specified, the experimental results reported in our paper are obtained without applying TTS. Thus far, we have only applied TTS to reasoning benchmarks, since we found that the model already exhibits strong perception and understanding capabilities, and initiating TTS yields no significant improvement.\nDecoupled Vision-Language Deployment\nIn multimodal inference, the vision encoder and language model have distinct computational characteristics. The vision encoder that transforms images into semantic features is highly parallelizable and does not rely on long-term history state.  In contrast,  the language model adopts the inference in an autoregressive manner, which requires previous states to compute the next one. This sequential property makes the language part more sensitive to memory bandwidth and latency.\nWhen MLLMs are deployed online at scale, the vision and language models often block each other, thus incurring additional inference cost. This effect becomes more pronounced with larger vision models or higher-resolution images.\nAs shown in the Figure above, we propose decoupled vision-language deployment (DvD) to address this issue by separating vision and language processing, with a particular focus on optimizing the prefilling stage. The vision subsystem batches and processes images to produce compact feature embeddings, which are then transmitted to the language subsystem for fusion with the text context prior to decoding. This separation alleviates blocking and brings multimodal prefilling performance closer to that of pure language models.\nIn our system implementation, the ViT and MLP (and ViR for InternVL3.5-Flash) are deployed on the vision server, while the language server executes only the LLM. The communication is unidirectional, transmitting BF16 visual features over TCP, with RDMA optionally employed to achieve higher transmission speed. Vision processing, feature transmission, and language processing are organized into an asynchronous three-stage pipeline, enabling overlapped execution and minimizing pipeline stalls.\nDvD increases GPU utilization and processing efficiency on the vision side, while enabling the language server to focus exclusively on the LLM‚Äôs prefilling and decoding without being blocked by vision computation. This design leads to improved throughput and responsiveness. Moreover, the architecture supports independent hardware cost optimization for the vision and language modules, and facilitates the seamless integration of new modules without requiring modifications to the language server deployment.\nEvaluation on Multimodal Capability\nMultimodal Reasoning and Mathematics\nOCR, Chart, and Document Understanding\nMulti-Image Understanding & Real-World Comprehension\nComprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\nVisual Grounding\nMultimodal Multilingual Understanding\nVideo Understanding\nGUI Tasks\nEmbodied Tasks\nSVG Tasks\nEvaluation on Language Capability\nAblation Study\nCascade Reinforcement Learning\nDecoupled Vision-Language Deployment\nQuick Start\nWe provide an example code to run InternVL3.5-8B using transformers. Please note that our models with up to 30B parameters can be deployed on a single A100 GPU, while the 38B model requires two A100 GPUs and the 235B model requires eight A100 GPUs.\nIn most cases, both LMDeploy and vLLM can be used for model deployment. However, for InternVL3.5-20B-A4B, we recommend using vLLM since lmdeploy has not yet supported GPT-OSS.\nPlease use transformers>=4.52.1 to ensure the model works normally. For the 20B version of our model, transformers>=4.55.0 is required.\nModel Loading\n16-bit (bf16 / fp16)\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval().cuda()\nBNB 8-bit Quantization\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=True,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval()\nMultiple GPUs\nimport math\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=\"auto\").eval()\nThinking Mode\nTo enable thinking mode, please set the system prompt to our Thinking System Prompt. When enabling Thinking mode, we recommend setting do_sample=True and temperature=0.6 to mitigate undesired repetition.\nR1_SYSTEM_PROMPT = \"\"\"\nYou are an AI assistant that rigorously follows this response protocol:\n1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags.\n2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline.\nEnsure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section.\n\"\"\".strip()\nmodel.system_message = R1_SYSTEMP_PROMPT\nInference with Transformers\nimport math\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom decord import VideoReader, cpu\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import AutoModel, AutoTokenizer\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\ndef build_transform(input_size):\nMEAN, STD = IMAGENET_MEAN, IMAGENET_STD\ntransform = T.Compose([\nT.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\nT.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\nT.ToTensor(),\nT.Normalize(mean=MEAN, std=STD)\n])\nreturn transform\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\nbest_ratio_diff = float('inf')\nbest_ratio = (1, 1)\narea = width * height\nfor ratio in target_ratios:\ntarget_aspect_ratio = ratio[0] / ratio[1]\nratio_diff = abs(aspect_ratio - target_aspect_ratio)\nif ratio_diff < best_ratio_diff:\nbest_ratio_diff = ratio_diff\nbest_ratio = ratio\nelif ratio_diff == best_ratio_diff:\nif area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\nbest_ratio = ratio\nreturn best_ratio\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\norig_width, orig_height = image.size\naspect_ratio = orig_width / orig_height\n# calculate the existing image aspect ratio\ntarget_ratios = set(\n(i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\ni * j <= max_num and i * j >= min_num)\ntarget_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n# find the closest aspect ratio to the target\ntarget_aspect_ratio = find_closest_aspect_ratio(\naspect_ratio, target_ratios, orig_width, orig_height, image_size)\n# calculate the target width and height\ntarget_width = image_size * target_aspect_ratio[0]\ntarget_height = image_size * target_aspect_ratio[1]\nblocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n# resize the image\nresized_img = image.resize((target_width, target_height))\nprocessed_images = []\nfor i in range(blocks):\nbox = (\n(i % (target_width // image_size)) * image_size,\n(i // (target_width // image_size)) * image_size,\n((i % (target_width // image_size)) + 1) * image_size,\n((i // (target_width // image_size)) + 1) * image_size\n)\n# split the image\nsplit_img = resized_img.crop(box)\nprocessed_images.append(split_img)\nassert len(processed_images) == blocks\nif use_thumbnail and len(processed_images) != 1:\nthumbnail_img = image.resize((image_size, image_size))\nprocessed_images.append(thumbnail_img)\nreturn processed_images\ndef load_image(image_file, input_size=448, max_num=12):\nimage = Image.open(image_file).convert('RGB')\ntransform = build_transform(input_size=input_size)\nimages = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(image) for image in images]\npixel_values = torch.stack(pixel_values)\nreturn pixel_values\npath = 'OpenGVLab/InternVL3_5-8B'\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=False,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=\"auto\").eval()\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n# set the max number of tiles in `max_num`\npixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n# pure-text conversation (Á∫ØÊñáÊú¨ÂØπËØù)\nquestion = 'Hello, who are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Can you tell me a story?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\nquestion = '<image>\\nPlease describe the image shortly.'\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f'User: {question}\\nAssistant: {response}')\n# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\nquestion = '<image>\\nPlease describe the image in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Please write a poem according to the image.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# multi-image multi-round conversation, combined images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÊãºÊé•ÂõæÂÉè)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nquestion = '<image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nhistory=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nhistory=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# multi-image multi-round conversation, separate images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÁã¨Á´ãÂõæÂÉè)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\nquestion = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list,\nhistory=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list,\nhistory=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# batch inference, single image per sample (ÂçïÂõæÊâπÂ§ÑÁêÜ)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nquestions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\nresponses = model.batch_chat(tokenizer, pixel_values,\nnum_patches_list=num_patches_list,\nquestions=questions,\ngeneration_config=generation_config)\nfor question, response in zip(questions, responses):\nprint(f'User: {question}\\nAssistant: {response}')\n# video multi-round conversation (ËßÜÈ¢ëÂ§öËΩÆÂØπËØù)\ndef get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\nif bound:\nstart, end = bound[0], bound[1]\nelse:\nstart, end = -100000, 100000\nstart_idx = max(first_idx, round(start * fps))\nend_idx = min(round(end * fps), max_frame)\nseg_size = float(end_idx - start_idx) / num_segments\nframe_indices = np.array([\nint(start_idx + (seg_size / 2) + np.round(seg_size * idx))\nfor idx in range(num_segments)\n])\nreturn frame_indices\ndef load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\nvr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\nmax_frame = len(vr) - 1\nfps = float(vr.get_avg_fps())\npixel_values_list, num_patches_list = [], []\ntransform = build_transform(input_size=input_size)\nframe_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\nfor frame_index in frame_indices:\nimg = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\nimg = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(tile) for tile in img]\npixel_values = torch.stack(pixel_values)\nnum_patches_list.append(pixel_values.shape[0])\npixel_values_list.append(pixel_values)\npixel_values = torch.cat(pixel_values_list)\nreturn pixel_values, num_patches_list\nvideo_path = './examples/red-panda.mp4'\npixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\npixel_values = pixel_values.to(torch.bfloat16).cuda()\nvideo_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\nquestion = video_prefix + 'What is the red panda doing?'\n# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Describe this video in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nStreaming Output\nBesides this method, you can also use the following code to get streamed output.\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n# Initialize the streamer\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=10)\n# Define the generation configuration\ngeneration_config = dict(max_new_tokens=1024, do_sample=False, streamer=streamer)\n# Start the model chat in a separate thread\nthread = Thread(target=model.chat, kwargs=dict(\ntokenizer=tokenizer, pixel_values=pixel_values, question=question,\nhistory=None, return_history=False, generation_config=generation_config,\n))\nthread.start()\n# Initialize an empty string to store the generated text\ngenerated_text = ''\n# Loop through the streamer to get the new text as it is generated\nfor new_text in streamer:\nif new_text == model.conv_template.sep:\nbreak\ngenerated_text += new_text\nprint(new_text, end='', flush=True)  # Print each new chunk of generated text on the same line\nFinetune\nMany repositories now support fine-tuning of the InternVL series models, including InternVL, SWIFT, XTuner, and others. Please refer to their documentation for more details on fine-tuning.\nDeployment\nLMDeploy\nLMDeploy is a toolkit for compressing, deploying, and serving LLMs & VLMs.\npip install lmdeploy>=0.9.1\nLMDeploy abstracts the complex inference process of multi-modal Vision-Language Models (VLM) into an easy-to-use pipeline, similar to the Large Language Model (LLM) inference pipeline.\nA 'Hello, world' Example\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nresponse = pipe(('describe this image', image))\nprint(response.text)\nMulti-images Inference\nWhen dealing with multiple images, you can put them all in one list. Keep in mind that multiple images will lead to a higher number of input tokens, and as a result, the size of the context window typically needs to be increased.\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nfrom lmdeploy.vl.constants import IMAGE_TOKEN\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage_urls=[\n'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg',\n'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg'\n]\nimages = [load_image(img_url) for img_url in image_urls]\n# Numbering images improves multi-image conversations\nresponse = pipe((f'Image-1: {IMAGE_TOKEN}\\nImage-2: {IMAGE_TOKEN}\\ndescribe these two images', images))\nprint(response.text)\nBatch Prompts Inference\nConducting inference with batch prompts is quite straightforward; just place them within a list structure:\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage_urls=[\n\"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg\",\n\"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg\"\n]\nprompts = [('describe this image', load_image(img_url)) for img_url in image_urls]\nresponse = pipe(prompts)\nprint(response)\nMulti-turn Conversation\nThere are two ways to do the multi-turn conversations with the pipeline. One is to construct messages according to the format of OpenAI and use above introduced method, the other is to use the pipeline.chat interface.\nfrom lmdeploy import pipeline, PytorchEngineConfig, GenerationConfig\nfrom lmdeploy.vl import load_image\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg')\ngen_config = GenerationConfig(top_k=50, top_p=0.95, temperature=0.6, max_new_tokens=8192)\nsess = pipe.chat(('describe this image', image), gen_config=gen_config)\nprint(sess.response.text)\nsess = pipe.chat('What is the woman doing?', session=sess, gen_config=gen_config)\nprint(sess.response.text)\nService\nLMDeploy's api_server enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:\nlmdeploy serve api_server OpenGVLab/InternVL3_5-8B --server-port 23333 --tp 1 --backend pytorch\nTo use the OpenAI-style interface, you need to install OpenAI:\npip install openai\nThen, use the code below to make the API call:\nfrom openai import OpenAI\nclient = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=[{\n'role':\n'user',\n'content': [{\n'type': 'text',\n'text': 'describe this image',\n}, {\n'type': 'image_url',\n'image_url': {\n'url':\n'https://modelscope.oss-cn-beijing.aliyuncs.com/resource/tiger.jpeg',\n},\n}],\n}],\ntemperature=0.8,\ntop_p=0.8)\nprint(response)\nLicense\nThis project is released under the apache-2.0 License. This project uses the pre-trained Qwen3 as a component, which is licensed under the apache-2.0 License.\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{wang2025internvl3_5,\ntitle={InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},\nauthor={Wang, Weiyun and Gao, Zhangwei and Gu, Lixin and Pu, Hengjun and Cui, Long and Wei, Xingguang and Liu, Zhaoyang and Jing, Linglin and Ye, Shenglong and Shao, Jie and others},\njournal={arXiv preprint arXiv:2508.18265},\nyear={2025}\n}",
    "OpenGVLab/InternVL3_5-1B": "InternVL3_5-1B\nIntroduction\nInternVL3.5 Family\nGithub Format\nHuggingFace Format\nModel Architecture\nTraining and Deployment Strategy\nPre-Training\nSupervised Fine-Tuning\nCascade Reinforcement Learning\nVisual Consistency Learning\nTest-Time Scaling\nDecoupled Vision-Language Deployment\nEvaluation on Multimodal Capability\nMultimodal Reasoning and Mathematics\nOCR, Chart, and Document Understanding\nMulti-Image Understanding & Real-World Comprehension\nComprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\nVisual Grounding\nMultimodal Multilingual Understanding\nVideo Understanding\nGUI Tasks\nEmbodied Tasks\nSVG Tasks\nEvaluation on Language Capability\nAblation Study\nCascade Reinforcement Learning\nDecoupled Vision-Language Deployment\nQuick Start\nModel Loading\nThinking Mode\nInference with Transformers\nFinetune\nDeployment\nLMDeploy\nLicense\nCitation\nInternVL3_5-1B\n[üìÇ GitHub] [üìú InternVL 1.0] [üìú InternVL 1.5] [üìú InternVL 2.5] [üìú InternVL2.5-MPO] [üìú InternVL3] [üìú InternVL3.5]\n[üÜï Blog] [üó®Ô∏è Chat Demo] [üöÄ Quick Start] [üìñ Documents]\nIntroduction\nWe introduce InternVL3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05 √ó\\times√ó inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e.,  InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks‚Äînarrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.\nHatched bars represent closed-source commercial models. We report average scores on a set of multimodal general, reasoning, text, and agentic benchmarks: MMBench v1.1 (en), MMStar,BLINK, HallusionBench, AI2D, OCRBench, MMVet, MME-RealWorld (en), MVBench, VideoMME, MMMU, MathVista, MathVision, MathVerse, DynaMath, WeMath, LogicVista, MATH500, AIME24, AIME25, GPQA, MMLU-Pro, GAOKAO, IFEval, SGP-Bench, VSI-Bench, ERQA, SpaCE-10, and OmniSpatial.\nSee quick start for how to use our model.\nInternVL3.5 Family\nIn the following table, we provide an overview of the InternVL3.5 series.\nTo maintain consistency with earlier generations, we provide two model formats: the GitHub format, consistent with prior releases, and the HF format, aligned with the official Transformers standard.\nIf you want to convert the checkpoint between these two formats, please refer to the scripts about custom2hf and hf2custom.\nGithub Format\nModel\n#Vision Param\n#Language Param\n#Total Param\nHF Link\nModelScope Link\nInternVL3.5-1B\n0.3B\n0.8B\n1.1B\nü§ó link\nü§ñ link\nInternVL3.5-2B\n0.3B\n2.0B\n2.3B\nü§ó link\nü§ñ link\nInternVL3.5-4B\n0.3B\n4.4B\n4.7B\nü§ó link\nü§ñ link\nInternVL3.5-8B\n0.3B\n8.2B\n8.5B\nü§ó link\nü§ñ link\nInternVL3.5-14B\n0.3B\n14.8B\n15.1B\nü§ó link\nü§ñ link\nInternVL3.5-38B\n5.5B\n32.8B\n38.4B\nü§ó link\nü§ñ link\nInternVL3.5-20B-A4B\n0.3B\n20.9B\n21.2B-A4B\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B\n0.3B\n30.5B\n30.8B-A3B\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B\n5.5B\n235.1B\n240.7B-A28B\nü§ó link\nü§ñ link\nHuggingFace Format\nModel\n#Vision Param\n#Language Param\n#Total Param\nHF Link\nModelScope Link\nInternVL3.5-1B-HF\n0.3B\n0.8B\n1.1B\nü§ó link\nü§ñ link\nInternVL3.5-2B-HF\n0.3B\n2.0B\n2.3B\nü§ó link\nü§ñ link\nInternVL3.5-4B-HF\n0.3B\n4.4B\n4.7B\nü§ó link\nü§ñ link\nInternVL3.5-8B-HF\n0.3B\n8.2B\n8.5B\nü§ó link\nü§ñ link\nInternVL3.5-14B-HF\n0.3B\n14.8B\n15.1B\nü§ó link\nü§ñ link\nInternVL3.5-38B-HF\n5.5B\n32.8B\n38.4B\nü§ó link\nü§ñ link\nInternVL3.5-20B-A4B-HF\n0.3B\n20.9B\n21.2B-A4B\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-HF\n0.3B\n30.5B\n30.8B-A3B\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-HF\n5.5B\n235.1B\n240.7B-A28B\nü§ó link\nü§ñ link\nWe conduct the evaluation with VLMEvalkit. To enable the Thinking mode of our model, please set the system prompt to R1_SYSTEM_PROMPT. When enabling Thinking mode, we recommend setting do_sample=True and temperature=0.6 to mitigate undesired repetition.\nOur training pipeline comprises four stages: Multimodal Continual Pre-Training (CPT), Supervised Fine-Tuning (SFT), and Cascade Reinforcement Learning (CascadeRL). In CascadeRL, we first fine-tune the model using Mixed Preference Optimization (MPO) under an offline RL setting, followed by GSPO under an oneline RL setting.\nFor the Flash version of InternVL3.5, we additionally introduce a lightweight training stage, termed Visual Consistency Learning (ViCO), which reduces the token cost required to represent an image patch.\nHere, we also open-source the model weights after different training stages for potential research usage.\nIf you're unsure which version to use, please select the one without any suffix, as it has completed the full training pipeline.\nModel\nTraining Pipeline\nHF Link\nModelScope Link\nInternVL3.5-1B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-1B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-1B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-1B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-2B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-2B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-2B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-2B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-4B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-4B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-4B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-4B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-8B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-8B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-8B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-8B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-14B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-14B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-14B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-14B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-38B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-38B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-38B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-38B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nThe Flash version of our model will be released as soon as possible.\nModel Architecture\nInternVL3.5:\nThis series of models follow the \"ViT‚ÄìMLP‚ÄìLLM\" paradigm adopted in previous versions of InternVL.\nWe initialize the language model using the Qwen3 series and GPT-OSS, and the vision encoder using InternViT-300M and InternViT-6B.\nThe Dynamic High Resolution strategy introduced in InternVL1.5 is also retained in our design.\nInternVL3.5-Flash:\nCompared to InternVL3.5, InternVL3.5-Flash further integrates the Visual Resolution Router (ViR), thus yielding a series of  efficient variants friendly  suitable for  resource-constrained scenarios.\nSpecifically, in InternVL3.5, each image patch is initially represented as 1024 visual tokens for the vision encoder, which are then compressed into 256 tokens via a pixel shuffle module before being passed to the Large Language Model (LLM).\nIn InternVL3.5-Flash, as shown in the Figure below, an additional pixel shuffle module with a higher compression rate is included, enabling the compression of visual tokens down to 64 tokens.\nFor each patch, the patch router determines the appropriate compression rate by assessing its semantic richness, and routes it to the corresponding pixel shuffle module accordingly.\nBenefiting from this patch-aware compression mechanism, InternVL3.5-Flash is able to reduce the number of visual tokens by 50% while maintaining nearly 100% of the performance of InternVL3.5.\nTraining and Deployment Strategy\nPre-Training\nDuring the pre-training stage, we update all model parameters jointly using the combination of large-scale text and multimodal corpora. Specifically, given an arbitrary training sample consisting of a multimodal token sequence x=(x1,x2,‚Ä¶,xL)\\mathbf{x}=\\left(x_1, x_2, \\ldots, x_L\\right)x=(x1‚Äã,x2‚Äã,‚Ä¶,xL‚Äã), the next token prediction (NTP) loss is calculated on each text token as follows:\nLi=‚àílog‚Å°pŒ∏(xi‚à£x1,‚Ä¶,xi‚àí1),\n\\mathcal{L}_{i}=-\\log p_\\theta\\left(x_i \\mid x_1, \\ldots, x_{i-1}\\right),\nLi‚Äã=‚àílogpŒ∏‚Äã(xi‚Äã‚à£x1‚Äã,‚Ä¶,xi‚àí1‚Äã),\nwhere xix_ixi‚Äã is the predicted token and  prefix tokens in {x1,x2,‚Ä¶,xi‚àí1}\\{x_1, x_2, \\ldots, x_{i-1}\\}{x1‚Äã,x2‚Äã,‚Ä¶,xi‚àí1‚Äã} can be either  text tokens or  image tokens. Notably, for conversation samples, only response tokens  are included for the calculation of the loss.\nAdditionally, to mitigate bias toward either longer or shorter responses during training, we adopt the square averaging to re-weight the NTP loss  as follows:\nLi‚Ä≤=wi‚àëjwj‚ãÖLi,wi=1N0.5,\n\\mathcal{L}_{i}^{'} = \\frac{w_i}{\\sum_j w_j} \\cdot \\mathcal{L}_i, \\quad w_i = \\frac{1}{N^{0.5}},\nLi‚Ä≤‚Äã=‚àëj‚Äãwj‚Äãwi‚Äã‚Äã‚ãÖLi‚Äã,wi‚Äã=N0.51‚Äã,\nwhere NNN denotes the number of tokens in the training sample on which the loss needs to be calculated. The random JPEG compression is also included to enhance the model's real-world performance.\nSupervised Fine-Tuning\nDuring the SFT phase, we adopt the same objective as in the pre-training stage and use the  square-root averaging strategy to calculate the final loss.  In this stage, the context window is set to 32K tokens to adapt long-context information.\nCompared to InternVL3, the SFT stage of InternVL3.5 contains  more high-quality and  diverse training data derived from three sources:\n(1) Instruction-following data from InternVL3, which are reused to preserve broad coverage of vision‚Äìlanguage tasks.\n(2) Multimodal reasoning data in the \"Thinking\" mode, which are included to instill long-thinking capabilities in the model. To construct such data, we first use InternVL3-78B to describe the image and then input the description into DeepSeek-R1 to sample rollouts with detailed reasoning processes. Rollouts with an incorrect final answer are filtered out. The questions in these datasets cover various expert domains, such as mathematics and scientific disciplines, thereby strengthening performance on different reasoning tasks.\n(3) Capability-expansion datasets, which endow InternVL3.5 with new skills, including GUI-based interaction, embodied interaction, and scalable vect\nCascade Reinforcement Learning\nCascade RL aims to combine the benefits of offline RL and online RL to progressively facilitate the post-training of MLLMs in an efficient manner.\nSpecifically, we first fine-tune the model using an offline RL algorithm as an efficient warm-up stage to reach a satisfied results, which can guarantee the high-quality rollouts for the latter stage.\nSubsequently, we employ an online RL algorithm to further refine the output distribution based on rollouts generated by the model itself.  Compared to the single offline or online RL stage, our cascaded RL achieves significant performance improvements at a fraction of the GPU time cost.\nDuring the offline RL stage, we employ mixed preference optimization (MPO) to fine-tune the model. Specifically, the training objective of MPO is a combination of preference loss Lp\\mathcal{L}_{p}Lp‚Äã, quality loss Lq\\mathcal{L}_{q}Lq‚Äã, and generation loss Lg\\mathcal{L}_{g}Lg‚Äã, which can be formulated as follows:\nLMPO=wpLp+wqLq+wgLg,\n\\mathcal{L}_{\\text{MPO}}=\nw_{p} \\mathcal{L}_{p}\n+\nw_{q} \\mathcal{L}_{q}\n+\nw_{g} \\mathcal{L}_{g}\n,\nLMPO‚Äã=wp‚ÄãLp‚Äã+wq‚ÄãLq‚Äã+wg‚ÄãLg‚Äã,\nwhere w‚àów_{*}w‚àó‚Äã represents the weight assigned to each loss component.\nThe DPO loss, BCO loss, and LM loss serve as the preference loss, quality loss, and generation loss, respectively.\nDuring the online RL stage, we employ GSPO, without reference model constraints, as our online RL algorithm, which we find more effective in training both dense and mixture-of-experts (MoE) models. Similar to GRPO, the advantage is defined as the normalized reward across responses sampled from the same query.\nThe training objective of GSPO is given by:\nLGSPO(Œ∏)=Ex‚àºD,{yi}i=1G‚àºœÄŒ∏¬†old¬†(‚ãÖ‚à£x)[1G‚àëi=1Gmin‚Å°(si(Œ∏)A^i,clip‚Å°(si(Œ∏),1‚àíŒµ,1+Œµ)A^i)],\n\\mathcal{L}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D},\\left\\{y_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta \\text { old }}(\\cdot \\mid x)}\\left[\\frac{1}{G} \\sum_{i=1}^G \\min \\left(s_i(\\theta) \\widehat{A}_i, \\operatorname{clip}\\left(s_i(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_i\\right)\\right],\nLGSPO‚Äã(Œ∏)=Ex‚àºD,{yi‚Äã}i=1G‚Äã‚àºœÄŒ∏¬†old¬†‚Äã(‚ãÖ‚à£x)‚Äã[G1‚Äãi=1‚àëG‚Äãmin(si‚Äã(Œ∏)Ai‚Äã,clip(si‚Äã(Œ∏),1‚àíŒµ,1+Œµ)Ai‚Äã)],\nwhere the importance sampling ratio is defined as the geometric mean of the per-token ratios.\nPlease see our paper for more technical and experimental details.\nVisual Consistency Learning\nWe further include ViCO as an additional training stage to integrate the visual resolution router (ViR) into InternVL3.5, thereby reducing the inference cost of InternVL3.5. The obtained efficient version of InternVL3.5 are termed as InternVL3.5-Flash. In particular, ViCO comprises two stages:\nConsistency training:\nIn this stage, the entire model is trained to minimize the divergence between response distributions conditioned on visual tokens with different compression rates.\nIn practice, we introduce an extra reference model, which is frozen and initialized with InternVL3.5.\nGiven a sample, each image patch is represented as either 256 or 64 tokens, and the training objective is defined as follows:\nLViCO=EŒæ‚àºR[1N‚àëi=1NKL(œÄŒ∏ref(yi‚à£y<i,I)‚ÄÖ‚Ää‚à•‚ÄÖ‚ÄäœÄŒ∏policy(yi‚à£y<i,IŒæ))],\n\\mathcal{L}_\\text{ViCO} =\n\\mathbb{E}_{\\xi \\sim \\mathcal{R}} \\Bigg[\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{KL} \\Big(\n\\pi_{\\theta_{ref}}\\left(y_i \\mid y_{<i}, I\\right) \\;\\Big\\|\\;\n\\pi_{\\theta_{policy}}\\left(y_i \\mid y_{<i}, I_\\xi\\right)\n\\Big)\n\\Bigg],\nLViCO‚Äã=EŒæ‚àºR‚Äã[N1‚Äãi=1‚àëN‚ÄãKL(œÄŒ∏ref‚Äã‚Äã(yi‚Äã‚à£y<i‚Äã,I)‚ÄãœÄŒ∏policy‚Äã‚Äã(yi‚Äã‚à£y<i‚Äã,IŒæ‚Äã))],\nwhere \\(\\mathrm{KL}) denotes the KL divergence and (\\xi) denotes the compression rate, which is uniformly sampled from ({\\frac{1}{4},\\frac{1}{16}}). The image (I_\\xi) is represented as 256 tokens when (\\xi=\\frac{1}{4}) and 64 tokens when (\\xi=\\frac{1}{16}). Notably, the reference model always performs inference with (\\xi=\\frac{1}{4}).\nRouter training:\nThis stage aims to train the ViR to select an appropriate trade-off resolution for different inputs.\nViR is formulated as a binary classifier and trained using standard cross-entropy loss.\nTo construct the route targets, we first compute the KL divergence between the model outputs conditioned on uncompressed visual tokens (i.e., 256 tokens per patch) and those conditioned on compressed visual tokens (i.e., 64 tokens per patch).\nDuring this stage, the main MLLM (ViT, MLP and LLM) is kept frozen, and only the ViR is trained.\nSpecifically, we first compute the loss ratio for each patch:\nri=LViCO(yi‚à£I116)LViCO(yi‚à£I14),\nr_i = \\frac{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{16}}\\big)}{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{4}}\\big)},\nri‚Äã=LViCO‚Äã(yi‚Äã‚à£I41‚Äã‚Äã)LViCO‚Äã(yi‚Äã‚à£I161‚Äã‚Äã)‚Äã,\nwhich quantifies the relative increase in loss caused by compressing the visual tokens. Based on this ratio, the binary ground-truth label for the patch router is defined as:\nyirouter={0,ri<œÑ‚ÄÖ‚Ää(compression¬†has¬†negligible¬†impact)1,ri‚â•œÑ‚ÄÖ‚Ää(compression¬†has¬†significant¬†impact),\ny_i^\\text{router} =\n\\begin{cases}\n0, & r_i < \\tau \\; \\text{(compression has negligible impact)} \\\\\n1, & r_i \\ge \\tau \\; \\text{(compression has significant impact)},\n\\end{cases}\nyirouter‚Äã={0,1,‚Äãri‚Äã<œÑ(compression¬†has¬†negligible¬†impact)ri‚Äã‚â•œÑ(compression¬†has¬†significant¬†impact),‚Äã\nwhere (y_i^{\\text{router}}=0) and (y_i^{\\text{router}}=1)  indicate that the compression rate (\\xi) is set to (\\tfrac{1}{16}) and (\\tfrac{1}{4}), respectively.\nPlease see our paper for more technical and experimental details.\nTest-Time Scaling\nTest-time scaling (TTS) has been empirically demonstrated as an effective approach to enhance the reasoning capabilities of LLMs and MLLMs, particularly for complex tasks necessitating multi-step inference.\nIn this work, we implement a comprehensive test-time scaling approach that simultaneously improves reasoning depth (i.e., deep thinking) and breadth (i.e., parallel thinking).\nDeep Thinking: By activating the Thinking mode, we guide the model to deliberately engage in step-by-step reasoning (i.e., decomposing complex problems into logical steps and validating intermediate conclusions) prior to generating the final answer. This approach systematically improves the logical structure of solutions for complex problems, particularly those requiring multi-step inference, and enhances reasoning depth.\nParallel Thinking: Following InternVL3, for reasoning tasks, we adopt the Best-of-N (BoN) strategy by employing VisualPRM-v1.1 as the critic model to select the optimal response from multiple reasoning candidates.\nThis approach improves reasoning breadth.\nNotably, unless otherwise specified, the experimental results reported in our paper are obtained without applying TTS. Thus far, we have only applied TTS to reasoning benchmarks, since we found that the model already exhibits strong perception and understanding capabilities, and initiating TTS yields no significant improvement.\nDecoupled Vision-Language Deployment\nIn multimodal inference, the vision encoder and language model have distinct computational characteristics. The vision encoder that transforms images into semantic features is highly parallelizable and does not rely on long-term history state.  In contrast,  the language model adopts the inference in an autoregressive manner, which requires previous states to compute the next one. This sequential property makes the language part more sensitive to memory bandwidth and latency.\nWhen MLLMs are deployed online at scale, the vision and language models often block each other, thus incurring additional inference cost. This effect becomes more pronounced with larger vision models or higher-resolution images.\nAs shown in the Figure above, we propose decoupled vision-language deployment (DvD) to address this issue by separating vision and language processing, with a particular focus on optimizing the prefilling stage. The vision subsystem batches and processes images to produce compact feature embeddings, which are then transmitted to the language subsystem for fusion with the text context prior to decoding. This separation alleviates blocking and brings multimodal prefilling performance closer to that of pure language models.\nIn our system implementation, the ViT and MLP (and ViR for InternVL3.5-Flash) are deployed on the vision server, while the language server executes only the LLM. The communication is unidirectional, transmitting BF16 visual features over TCP, with RDMA optionally employed to achieve higher transmission speed. Vision processing, feature transmission, and language processing are organized into an asynchronous three-stage pipeline, enabling overlapped execution and minimizing pipeline stalls.\nDvD increases GPU utilization and processing efficiency on the vision side, while enabling the language server to focus exclusively on the LLM‚Äôs prefilling and decoding without being blocked by vision computation. This design leads to improved throughput and responsiveness. Moreover, the architecture supports independent hardware cost optimization for the vision and language modules, and facilitates the seamless integration of new modules without requiring modifications to the language server deployment.\nEvaluation on Multimodal Capability\nMultimodal Reasoning and Mathematics\nOCR, Chart, and Document Understanding\nMulti-Image Understanding & Real-World Comprehension\nComprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\nVisual Grounding\nMultimodal Multilingual Understanding\nVideo Understanding\nGUI Tasks\nEmbodied Tasks\nSVG Tasks\nEvaluation on Language Capability\nAblation Study\nCascade Reinforcement Learning\nDecoupled Vision-Language Deployment\nQuick Start\nWe provide an example code to run InternVL3.5-8B using transformers. Please note that our models with up to 30B parameters can be deployed on a single A100 GPU, while the 38B model requires two A100 GPUs and the 235B model requires eight A100 GPUs.\nIn most cases, both LMDeploy and vLLM can be used for model deployment. However, for InternVL3.5-20B-A4B, we recommend using vLLM since lmdeploy has not yet supported GPT-OSS.\nPlease use transformers>=4.52.1 to ensure the model works normally. For the 20B version of our model, transformers>=4.55.0 is required.\nModel Loading\n16-bit (bf16 / fp16)\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval().cuda()\nBNB 8-bit Quantization\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=True,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval()\nMultiple GPUs\nimport math\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=\"auto\").eval()\nThinking Mode\nTo enable thinking mode, please set the system prompt to our Thinking System Prompt. When enabling Thinking mode, we recommend setting do_sample=True and temperature=0.6 to mitigate undesired repetition.\nR1_SYSTEM_PROMPT = \"\"\"\nYou are an AI assistant that rigorously follows this response protocol:\n1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags.\n2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline.\nEnsure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section.\n\"\"\".strip()\nmodel.system_message = R1_SYSTEMP_PROMPT\nInference with Transformers\nimport math\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom decord import VideoReader, cpu\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import AutoModel, AutoTokenizer\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\ndef build_transform(input_size):\nMEAN, STD = IMAGENET_MEAN, IMAGENET_STD\ntransform = T.Compose([\nT.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\nT.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\nT.ToTensor(),\nT.Normalize(mean=MEAN, std=STD)\n])\nreturn transform\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\nbest_ratio_diff = float('inf')\nbest_ratio = (1, 1)\narea = width * height\nfor ratio in target_ratios:\ntarget_aspect_ratio = ratio[0] / ratio[1]\nratio_diff = abs(aspect_ratio - target_aspect_ratio)\nif ratio_diff < best_ratio_diff:\nbest_ratio_diff = ratio_diff\nbest_ratio = ratio\nelif ratio_diff == best_ratio_diff:\nif area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\nbest_ratio = ratio\nreturn best_ratio\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\norig_width, orig_height = image.size\naspect_ratio = orig_width / orig_height\n# calculate the existing image aspect ratio\ntarget_ratios = set(\n(i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\ni * j <= max_num and i * j >= min_num)\ntarget_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n# find the closest aspect ratio to the target\ntarget_aspect_ratio = find_closest_aspect_ratio(\naspect_ratio, target_ratios, orig_width, orig_height, image_size)\n# calculate the target width and height\ntarget_width = image_size * target_aspect_ratio[0]\ntarget_height = image_size * target_aspect_ratio[1]\nblocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n# resize the image\nresized_img = image.resize((target_width, target_height))\nprocessed_images = []\nfor i in range(blocks):\nbox = (\n(i % (target_width // image_size)) * image_size,\n(i // (target_width // image_size)) * image_size,\n((i % (target_width // image_size)) + 1) * image_size,\n((i // (target_width // image_size)) + 1) * image_size\n)\n# split the image\nsplit_img = resized_img.crop(box)\nprocessed_images.append(split_img)\nassert len(processed_images) == blocks\nif use_thumbnail and len(processed_images) != 1:\nthumbnail_img = image.resize((image_size, image_size))\nprocessed_images.append(thumbnail_img)\nreturn processed_images\ndef load_image(image_file, input_size=448, max_num=12):\nimage = Image.open(image_file).convert('RGB')\ntransform = build_transform(input_size=input_size)\nimages = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(image) for image in images]\npixel_values = torch.stack(pixel_values)\nreturn pixel_values\npath = 'OpenGVLab/InternVL3_5-8B'\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=False,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=\"auto\").eval()\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n# set the max number of tiles in `max_num`\npixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n# pure-text conversation (Á∫ØÊñáÊú¨ÂØπËØù)\nquestion = 'Hello, who are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Can you tell me a story?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\nquestion = '<image>\\nPlease describe the image shortly.'\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f'User: {question}\\nAssistant: {response}')\n# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\nquestion = '<image>\\nPlease describe the image in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Please write a poem according to the image.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# multi-image multi-round conversation, combined images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÊãºÊé•ÂõæÂÉè)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nquestion = '<image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nhistory=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nhistory=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# multi-image multi-round conversation, separate images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÁã¨Á´ãÂõæÂÉè)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\nquestion = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list,\nhistory=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list,\nhistory=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# batch inference, single image per sample (ÂçïÂõæÊâπÂ§ÑÁêÜ)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nquestions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\nresponses = model.batch_chat(tokenizer, pixel_values,\nnum_patches_list=num_patches_list,\nquestions=questions,\ngeneration_config=generation_config)\nfor question, response in zip(questions, responses):\nprint(f'User: {question}\\nAssistant: {response}')\n# video multi-round conversation (ËßÜÈ¢ëÂ§öËΩÆÂØπËØù)\ndef get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\nif bound:\nstart, end = bound[0], bound[1]\nelse:\nstart, end = -100000, 100000\nstart_idx = max(first_idx, round(start * fps))\nend_idx = min(round(end * fps), max_frame)\nseg_size = float(end_idx - start_idx) / num_segments\nframe_indices = np.array([\nint(start_idx + (seg_size / 2) + np.round(seg_size * idx))\nfor idx in range(num_segments)\n])\nreturn frame_indices\ndef load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\nvr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\nmax_frame = len(vr) - 1\nfps = float(vr.get_avg_fps())\npixel_values_list, num_patches_list = [], []\ntransform = build_transform(input_size=input_size)\nframe_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\nfor frame_index in frame_indices:\nimg = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\nimg = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(tile) for tile in img]\npixel_values = torch.stack(pixel_values)\nnum_patches_list.append(pixel_values.shape[0])\npixel_values_list.append(pixel_values)\npixel_values = torch.cat(pixel_values_list)\nreturn pixel_values, num_patches_list\nvideo_path = './examples/red-panda.mp4'\npixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\npixel_values = pixel_values.to(torch.bfloat16).cuda()\nvideo_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\nquestion = video_prefix + 'What is the red panda doing?'\n# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Describe this video in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nStreaming Output\nBesides this method, you can also use the following code to get streamed output.\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n# Initialize the streamer\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=10)\n# Define the generation configuration\ngeneration_config = dict(max_new_tokens=1024, do_sample=False, streamer=streamer)\n# Start the model chat in a separate thread\nthread = Thread(target=model.chat, kwargs=dict(\ntokenizer=tokenizer, pixel_values=pixel_values, question=question,\nhistory=None, return_history=False, generation_config=generation_config,\n))\nthread.start()\n# Initialize an empty string to store the generated text\ngenerated_text = ''\n# Loop through the streamer to get the new text as it is generated\nfor new_text in streamer:\nif new_text == model.conv_template.sep:\nbreak\ngenerated_text += new_text\nprint(new_text, end='', flush=True)  # Print each new chunk of generated text on the same line\nFinetune\nMany repositories now support fine-tuning of the InternVL series models, including InternVL, SWIFT, XTuner, and others. Please refer to their documentation for more details on fine-tuning.\nDeployment\nLMDeploy\nLMDeploy is a toolkit for compressing, deploying, and serving LLMs & VLMs.\npip install lmdeploy>=0.9.1\nLMDeploy abstracts the complex inference process of multi-modal Vision-Language Models (VLM) into an easy-to-use pipeline, similar to the Large Language Model (LLM) inference pipeline.\nA 'Hello, world' Example\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nresponse = pipe(('describe this image', image))\nprint(response.text)\nMulti-images Inference\nWhen dealing with multiple images, you can put them all in one list. Keep in mind that multiple images will lead to a higher number of input tokens, and as a result, the size of the context window typically needs to be increased.\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nfrom lmdeploy.vl.constants import IMAGE_TOKEN\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage_urls=[\n'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg',\n'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg'\n]\nimages = [load_image(img_url) for img_url in image_urls]\n# Numbering images improves multi-image conversations\nresponse = pipe((f'Image-1: {IMAGE_TOKEN}\\nImage-2: {IMAGE_TOKEN}\\ndescribe these two images', images))\nprint(response.text)\nBatch Prompts Inference\nConducting inference with batch prompts is quite straightforward; just place them within a list structure:\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage_urls=[\n\"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg\",\n\"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg\"\n]\nprompts = [('describe this image', load_image(img_url)) for img_url in image_urls]\nresponse = pipe(prompts)\nprint(response)\nMulti-turn Conversation\nThere are two ways to do the multi-turn conversations with the pipeline. One is to construct messages according to the format of OpenAI and use above introduced method, the other is to use the pipeline.chat interface.\nfrom lmdeploy import pipeline, PytorchEngineConfig, GenerationConfig\nfrom lmdeploy.vl import load_image\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg')\ngen_config = GenerationConfig(top_k=50, top_p=0.95, temperature=0.6, max_new_tokens=8192)\nsess = pipe.chat(('describe this image', image), gen_config=gen_config)\nprint(sess.response.text)\nsess = pipe.chat('What is the woman doing?', session=sess, gen_config=gen_config)\nprint(sess.response.text)\nService\nLMDeploy's api_server enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:\nlmdeploy serve api_server OpenGVLab/InternVL3_5-8B --server-port 23333 --tp 1 --backend pytorch\nTo use the OpenAI-style interface, you need to install OpenAI:\npip install openai\nThen, use the code below to make the API call:\nfrom openai import OpenAI\nclient = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=[{\n'role':\n'user',\n'content': [{\n'type': 'text',\n'text': 'describe this image',\n}, {\n'type': 'image_url',\n'image_url': {\n'url':\n'https://modelscope.oss-cn-beijing.aliyuncs.com/resource/tiger.jpeg',\n},\n}],\n}],\ntemperature=0.8,\ntop_p=0.8)\nprint(response)\nLicense\nThis project is released under the apache-2.0 License. This project uses the pre-trained Qwen3 as a component, which is licensed under the apache-2.0 License.\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{wang2025internvl3_5,\ntitle={InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},\nauthor={Wang, Weiyun and Gao, Zhangwei and Gu, Lixin and Pu, Hengjun and Cui, Long and Wei, Xingguang and Liu, Zhaoyang and Jing, Linglin and Ye, Shenglong and Shao, Jie and others},\njournal={arXiv preprint arXiv:2508.18265},\nyear={2025}\n}",
    "OpenGVLab/InternVL3_5-8B": "InternVL3_5-8B\nIntroduction\nInternVL3.5 Family\nGithub Format\nHuggingFace Format\nModel Architecture\nTraining and Deployment Strategy\nPre-Training\nSupervised Fine-Tuning\nCascade Reinforcement Learning\nVisual Consistency Learning\nTest-Time Scaling\nDecoupled Vision-Language Deployment\nEvaluation on Multimodal Capability\nMultimodal Reasoning and Mathematics\nOCR, Chart, and Document Understanding\nMulti-Image Understanding & Real-World Comprehension\nComprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\nVisual Grounding\nMultimodal Multilingual Understanding\nVideo Understanding\nGUI Tasks\nEmbodied Tasks\nSVG Tasks\nEvaluation on Language Capability\nAblation Study\nCascade Reinforcement Learning\nDecoupled Vision-Language Deployment\nQuick Start\nModel Loading\nThinking Mode\nInference with Transformers\nFinetune\nDeployment\nLMDeploy\nLicense\nCitation\nInternVL3_5-8B\n[üìÇ GitHub] [üìú InternVL 1.0] [üìú InternVL 1.5] [üìú InternVL 2.5] [üìú InternVL2.5-MPO] [üìú InternVL3] [üìú InternVL3.5]\n[üÜï Blog] [üó®Ô∏è Chat Demo] [üöÄ Quick Start] [üìñ Documents]\nIntroduction\nWe introduce InternVL3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05 √ó\\times√ó inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e.,  InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks‚Äînarrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.\nHatched bars represent closed-source commercial models. We report average scores on a set of multimodal general, reasoning, text, and agentic benchmarks: MMBench v1.1 (en), MMStar,BLINK, HallusionBench, AI2D, OCRBench, MMVet, MME-RealWorld (en), MVBench, VideoMME, MMMU, MathVista, MathVision, MathVerse, DynaMath, WeMath, LogicVista, MATH500, AIME24, AIME25, GPQA, MMLU-Pro, GAOKAO, IFEval, SGP-Bench, VSI-Bench, ERQA, SpaCE-10, and OmniSpatial.\nSee quick start for how to use our model.\nInternVL3.5 Family\nIn the following table, we provide an overview of the InternVL3.5 series.\nTo maintain consistency with earlier generations, we provide two model formats: the GitHub format, consistent with prior releases, and the HF format, aligned with the official Transformers standard.\nIf you want to convert the checkpoint between these two formats, please refer to the scripts about custom2hf and hf2custom.\nGithub Format\nModel\n#Vision Param\n#Language Param\n#Total Param\nHF Link\nModelScope Link\nInternVL3.5-1B\n0.3B\n0.8B\n1.1B\nü§ó link\nü§ñ link\nInternVL3.5-2B\n0.3B\n2.0B\n2.3B\nü§ó link\nü§ñ link\nInternVL3.5-4B\n0.3B\n4.4B\n4.7B\nü§ó link\nü§ñ link\nInternVL3.5-8B\n0.3B\n8.2B\n8.5B\nü§ó link\nü§ñ link\nInternVL3.5-14B\n0.3B\n14.8B\n15.1B\nü§ó link\nü§ñ link\nInternVL3.5-38B\n5.5B\n32.8B\n38.4B\nü§ó link\nü§ñ link\nInternVL3.5-20B-A4B\n0.3B\n20.9B\n21.2B-A4B\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B\n0.3B\n30.5B\n30.8B-A3B\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B\n5.5B\n235.1B\n240.7B-A28B\nü§ó link\nü§ñ link\nHuggingFace Format\nModel\n#Vision Param\n#Language Param\n#Total Param\nHF Link\nModelScope Link\nInternVL3.5-1B-HF\n0.3B\n0.8B\n1.1B\nü§ó link\nü§ñ link\nInternVL3.5-2B-HF\n0.3B\n2.0B\n2.3B\nü§ó link\nü§ñ link\nInternVL3.5-4B-HF\n0.3B\n4.4B\n4.7B\nü§ó link\nü§ñ link\nInternVL3.5-8B-HF\n0.3B\n8.2B\n8.5B\nü§ó link\nü§ñ link\nInternVL3.5-14B-HF\n0.3B\n14.8B\n15.1B\nü§ó link\nü§ñ link\nInternVL3.5-38B-HF\n5.5B\n32.8B\n38.4B\nü§ó link\nü§ñ link\nInternVL3.5-20B-A4B-HF\n0.3B\n20.9B\n21.2B-A4B\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-HF\n0.3B\n30.5B\n30.8B-A3B\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-HF\n5.5B\n235.1B\n240.7B-A28B\nü§ó link\nü§ñ link\nWe conduct the evaluation with VLMEvalkit. To enable the Thinking mode of our model, please set the system prompt to R1_SYSTEM_PROMPT. When enabling Thinking mode, we recommend setting do_sample=True and temperature=0.6 to mitigate undesired repetition.\nOur training pipeline comprises four stages: Multimodal Continual Pre-Training (CPT), Supervised Fine-Tuning (SFT), and Cascade Reinforcement Learning (CascadeRL). In CascadeRL, we first fine-tune the model using Mixed Preference Optimization (MPO) under an offline RL setting, followed by GSPO under an oneline RL setting.\nFor the Flash version of InternVL3.5, we additionally introduce a lightweight training stage, termed Visual Consistency Learning (ViCO), which reduces the token cost required to represent an image patch.\nHere, we also open-source the model weights after different training stages for potential research usage.\nIf you're unsure which version to use, please select the one without any suffix, as it has completed the full training pipeline.\nModel\nTraining Pipeline\nHF Link\nModelScope Link\nInternVL3.5-1B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-1B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-1B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-1B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-2B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-2B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-2B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-2B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-4B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-4B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-4B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-4B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-8B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-8B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-8B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-8B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-14B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-14B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-14B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-14B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-30B-A3B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-38B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-38B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-38B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-38B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-Pretrained\nCPT\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-Instruct\nCPT + SFT\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B-MPO\nCPT + SFT + MPO\nü§ó link\nü§ñ link\nInternVL3.5-241B-A28B\nCPT + SFT + CascadeRL\nü§ó link\nü§ñ link\nThe Flash version of our model will be released as soon as possible.\nModel Architecture\nInternVL3.5:\nThis series of models follow the \"ViT‚ÄìMLP‚ÄìLLM\" paradigm adopted in previous versions of InternVL.\nWe initialize the language model using the Qwen3 series and GPT-OSS, and the vision encoder using InternViT-300M and InternViT-6B.\nThe Dynamic High Resolution strategy introduced in InternVL1.5 is also retained in our design.\nInternVL3.5-Flash:\nCompared to InternVL3.5, InternVL3.5-Flash further integrates the Visual Resolution Router (ViR), thus yielding a series of  efficient variants friendly  suitable for  resource-constrained scenarios.\nSpecifically, in InternVL3.5, each image patch is initially represented as 1024 visual tokens for the vision encoder, which are then compressed into 256 tokens via a pixel shuffle module before being passed to the Large Language Model (LLM).\nIn InternVL3.5-Flash, as shown in the Figure below, an additional pixel shuffle module with a higher compression rate is included, enabling the compression of visual tokens down to 64 tokens.\nFor each patch, the patch router determines the appropriate compression rate by assessing its semantic richness, and routes it to the corresponding pixel shuffle module accordingly.\nBenefiting from this patch-aware compression mechanism, InternVL3.5-Flash is able to reduce the number of visual tokens by 50% while maintaining nearly 100% of the performance of InternVL3.5.\nTraining and Deployment Strategy\nPre-Training\nDuring the pre-training stage, we update all model parameters jointly using the combination of large-scale text and multimodal corpora. Specifically, given an arbitrary training sample consisting of a multimodal token sequence x=(x1,x2,‚Ä¶,xL)\\mathbf{x}=\\left(x_1, x_2, \\ldots, x_L\\right)x=(x1‚Äã,x2‚Äã,‚Ä¶,xL‚Äã), the next token prediction (NTP) loss is calculated on each text token as follows:\nLi=‚àílog‚Å°pŒ∏(xi‚à£x1,‚Ä¶,xi‚àí1),\n\\mathcal{L}_{i}=-\\log p_\\theta\\left(x_i \\mid x_1, \\ldots, x_{i-1}\\right),\nLi‚Äã=‚àílogpŒ∏‚Äã(xi‚Äã‚à£x1‚Äã,‚Ä¶,xi‚àí1‚Äã),\nwhere xix_ixi‚Äã is the predicted token and  prefix tokens in {x1,x2,‚Ä¶,xi‚àí1}\\{x_1, x_2, \\ldots, x_{i-1}\\}{x1‚Äã,x2‚Äã,‚Ä¶,xi‚àí1‚Äã} can be either  text tokens or  image tokens. Notably, for conversation samples, only response tokens  are included for the calculation of the loss.\nAdditionally, to mitigate bias toward either longer or shorter responses during training, we adopt the square averaging to re-weight the NTP loss  as follows:\nLi‚Ä≤=wi‚àëjwj‚ãÖLi,wi=1N0.5,\n\\mathcal{L}_{i}^{'} = \\frac{w_i}{\\sum_j w_j} \\cdot \\mathcal{L}_i, \\quad w_i = \\frac{1}{N^{0.5}},\nLi‚Ä≤‚Äã=‚àëj‚Äãwj‚Äãwi‚Äã‚Äã‚ãÖLi‚Äã,wi‚Äã=N0.51‚Äã,\nwhere NNN denotes the number of tokens in the training sample on which the loss needs to be calculated. The random JPEG compression is also included to enhance the model's real-world performance.\nSupervised Fine-Tuning\nDuring the SFT phase, we adopt the same objective as in the pre-training stage and use the  square-root averaging strategy to calculate the final loss.  In this stage, the context window is set to 32K tokens to adapt long-context information.\nCompared to InternVL3, the SFT stage of InternVL3.5 contains  more high-quality and  diverse training data derived from three sources:\n(1) Instruction-following data from InternVL3, which are reused to preserve broad coverage of vision‚Äìlanguage tasks.\n(2) Multimodal reasoning data in the \"Thinking\" mode, which are included to instill long-thinking capabilities in the model. To construct such data, we first use InternVL3-78B to describe the image and then input the description into DeepSeek-R1 to sample rollouts with detailed reasoning processes. Rollouts with an incorrect final answer are filtered out. The questions in these datasets cover various expert domains, such as mathematics and scientific disciplines, thereby strengthening performance on different reasoning tasks.\n(3) Capability-expansion datasets, which endow InternVL3.5 with new skills, including GUI-based interaction, embodied interaction, and scalable vect\nCascade Reinforcement Learning\nCascade RL aims to combine the benefits of offline RL and online RL to progressively facilitate the post-training of MLLMs in an efficient manner.\nSpecifically, we first fine-tune the model using an offline RL algorithm as an efficient warm-up stage to reach a satisfied results, which can guarantee the high-quality rollouts for the latter stage.\nSubsequently, we employ an online RL algorithm to further refine the output distribution based on rollouts generated by the model itself.  Compared to the single offline or online RL stage, our cascaded RL achieves significant performance improvements at a fraction of the GPU time cost.\nDuring the offline RL stage, we employ mixed preference optimization (MPO) to fine-tune the model. Specifically, the training objective of MPO is a combination of preference loss Lp\\mathcal{L}_{p}Lp‚Äã, quality loss Lq\\mathcal{L}_{q}Lq‚Äã, and generation loss Lg\\mathcal{L}_{g}Lg‚Äã, which can be formulated as follows:\nLMPO=wpLp+wqLq+wgLg,\n\\mathcal{L}_{\\text{MPO}}=\nw_{p} \\mathcal{L}_{p}\n+\nw_{q} \\mathcal{L}_{q}\n+\nw_{g} \\mathcal{L}_{g}\n,\nLMPO‚Äã=wp‚ÄãLp‚Äã+wq‚ÄãLq‚Äã+wg‚ÄãLg‚Äã,\nwhere w‚àów_{*}w‚àó‚Äã represents the weight assigned to each loss component.\nThe DPO loss, BCO loss, and LM loss serve as the preference loss, quality loss, and generation loss, respectively.\nDuring the online RL stage, we employ GSPO, without reference model constraints, as our online RL algorithm, which we find more effective in training both dense and mixture-of-experts (MoE) models. Similar to GRPO, the advantage is defined as the normalized reward across responses sampled from the same query.\nThe training objective of GSPO is given by:\nLGSPO(Œ∏)=Ex‚àºD,{yi}i=1G‚àºœÄŒ∏¬†old¬†(‚ãÖ‚à£x)[1G‚àëi=1Gmin‚Å°(si(Œ∏)A^i,clip‚Å°(si(Œ∏),1‚àíŒµ,1+Œµ)A^i)],\n\\mathcal{L}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D},\\left\\{y_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta \\text { old }}(\\cdot \\mid x)}\\left[\\frac{1}{G} \\sum_{i=1}^G \\min \\left(s_i(\\theta) \\widehat{A}_i, \\operatorname{clip}\\left(s_i(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_i\\right)\\right],\nLGSPO‚Äã(Œ∏)=Ex‚àºD,{yi‚Äã}i=1G‚Äã‚àºœÄŒ∏¬†old¬†‚Äã(‚ãÖ‚à£x)‚Äã[G1‚Äãi=1‚àëG‚Äãmin(si‚Äã(Œ∏)Ai‚Äã,clip(si‚Äã(Œ∏),1‚àíŒµ,1+Œµ)Ai‚Äã)],\nwhere the importance sampling ratio is defined as the geometric mean of the per-token ratios.\nPlease see our paper for more technical and experimental details.\nVisual Consistency Learning\nWe further include ViCO as an additional training stage to integrate the visual resolution router (ViR) into InternVL3.5, thereby reducing the inference cost of InternVL3.5. The obtained efficient version of InternVL3.5 are termed as InternVL3.5-Flash. In particular, ViCO comprises two stages:\nConsistency training:\nIn this stage, the entire model is trained to minimize the divergence between response distributions conditioned on visual tokens with different compression rates.\nIn practice, we introduce an extra reference model, which is frozen and initialized with InternVL3.5.\nGiven a sample, each image patch is represented as either 256 or 64 tokens, and the training objective is defined as follows:\nLViCO=EŒæ‚àºR[1N‚àëi=1NKL(œÄŒ∏ref(yi‚à£y<i,I)‚ÄÖ‚Ää‚à•‚ÄÖ‚ÄäœÄŒ∏policy(yi‚à£y<i,IŒæ))],\n\\mathcal{L}_\\text{ViCO} =\n\\mathbb{E}_{\\xi \\sim \\mathcal{R}} \\Bigg[\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{KL} \\Big(\n\\pi_{\\theta_{ref}}\\left(y_i \\mid y_{<i}, I\\right) \\;\\Big\\|\\;\n\\pi_{\\theta_{policy}}\\left(y_i \\mid y_{<i}, I_\\xi\\right)\n\\Big)\n\\Bigg],\nLViCO‚Äã=EŒæ‚àºR‚Äã[N1‚Äãi=1‚àëN‚ÄãKL(œÄŒ∏ref‚Äã‚Äã(yi‚Äã‚à£y<i‚Äã,I)‚ÄãœÄŒ∏policy‚Äã‚Äã(yi‚Äã‚à£y<i‚Äã,IŒæ‚Äã))],\nwhere \\(\\mathrm{KL}) denotes the KL divergence and (\\xi) denotes the compression rate, which is uniformly sampled from ({\\frac{1}{4},\\frac{1}{16}}). The image (I_\\xi) is represented as 256 tokens when (\\xi=\\frac{1}{4}) and 64 tokens when (\\xi=\\frac{1}{16}). Notably, the reference model always performs inference with (\\xi=\\frac{1}{4}).\nRouter training:\nThis stage aims to train the ViR to select an appropriate trade-off resolution for different inputs.\nViR is formulated as a binary classifier and trained using standard cross-entropy loss.\nTo construct the route targets, we first compute the KL divergence between the model outputs conditioned on uncompressed visual tokens (i.e., 256 tokens per patch) and those conditioned on compressed visual tokens (i.e., 64 tokens per patch).\nDuring this stage, the main MLLM (ViT, MLP and LLM) is kept frozen, and only the ViR is trained.\nSpecifically, we first compute the loss ratio for each patch:\nri=LViCO(yi‚à£I116)LViCO(yi‚à£I14),\nr_i = \\frac{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{16}}\\big)}{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{4}}\\big)},\nri‚Äã=LViCO‚Äã(yi‚Äã‚à£I41‚Äã‚Äã)LViCO‚Äã(yi‚Äã‚à£I161‚Äã‚Äã)‚Äã,\nwhich quantifies the relative increase in loss caused by compressing the visual tokens. Based on this ratio, the binary ground-truth label for the patch router is defined as:\nyirouter={0,ri<œÑ‚ÄÖ‚Ää(compression¬†has¬†negligible¬†impact)1,ri‚â•œÑ‚ÄÖ‚Ää(compression¬†has¬†significant¬†impact),\ny_i^\\text{router} =\n\\begin{cases}\n0, & r_i < \\tau \\; \\text{(compression has negligible impact)} \\\\\n1, & r_i \\ge \\tau \\; \\text{(compression has significant impact)},\n\\end{cases}\nyirouter‚Äã={0,1,‚Äãri‚Äã<œÑ(compression¬†has¬†negligible¬†impact)ri‚Äã‚â•œÑ(compression¬†has¬†significant¬†impact),‚Äã\nwhere (y_i^{\\text{router}}=0) and (y_i^{\\text{router}}=1)  indicate that the compression rate (\\xi) is set to (\\tfrac{1}{16}) and (\\tfrac{1}{4}), respectively.\nPlease see our paper for more technical and experimental details.\nTest-Time Scaling\nTest-time scaling (TTS) has been empirically demonstrated as an effective approach to enhance the reasoning capabilities of LLMs and MLLMs, particularly for complex tasks necessitating multi-step inference.\nIn this work, we implement a comprehensive test-time scaling approach that simultaneously improves reasoning depth (i.e., deep thinking) and breadth (i.e., parallel thinking).\nDeep Thinking: By activating the Thinking mode, we guide the model to deliberately engage in step-by-step reasoning (i.e., decomposing complex problems into logical steps and validating intermediate conclusions) prior to generating the final answer. This approach systematically improves the logical structure of solutions for complex problems, particularly those requiring multi-step inference, and enhances reasoning depth.\nParallel Thinking: Following InternVL3, for reasoning tasks, we adopt the Best-of-N (BoN) strategy by employing VisualPRM-v1.1 as the critic model to select the optimal response from multiple reasoning candidates.\nThis approach improves reasoning breadth.\nNotably, unless otherwise specified, the experimental results reported in our paper are obtained without applying TTS. Thus far, we have only applied TTS to reasoning benchmarks, since we found that the model already exhibits strong perception and understanding capabilities, and initiating TTS yields no significant improvement.\nDecoupled Vision-Language Deployment\nIn multimodal inference, the vision encoder and language model have distinct computational characteristics. The vision encoder that transforms images into semantic features is highly parallelizable and does not rely on long-term history state.  In contrast,  the language model adopts the inference in an autoregressive manner, which requires previous states to compute the next one. This sequential property makes the language part more sensitive to memory bandwidth and latency.\nWhen MLLMs are deployed online at scale, the vision and language models often block each other, thus incurring additional inference cost. This effect becomes more pronounced with larger vision models or higher-resolution images.\nAs shown in the Figure above, we propose decoupled vision-language deployment (DvD) to address this issue by separating vision and language processing, with a particular focus on optimizing the prefilling stage. The vision subsystem batches and processes images to produce compact feature embeddings, which are then transmitted to the language subsystem for fusion with the text context prior to decoding. This separation alleviates blocking and brings multimodal prefilling performance closer to that of pure language models.\nIn our system implementation, the ViT and MLP (and ViR for InternVL3.5-Flash) are deployed on the vision server, while the language server executes only the LLM. The communication is unidirectional, transmitting BF16 visual features over TCP, with RDMA optionally employed to achieve higher transmission speed. Vision processing, feature transmission, and language processing are organized into an asynchronous three-stage pipeline, enabling overlapped execution and minimizing pipeline stalls.\nDvD increases GPU utilization and processing efficiency on the vision side, while enabling the language server to focus exclusively on the LLM‚Äôs prefilling and decoding without being blocked by vision computation. This design leads to improved throughput and responsiveness. Moreover, the architecture supports independent hardware cost optimization for the vision and language modules, and facilitates the seamless integration of new modules without requiring modifications to the language server deployment.\nEvaluation on Multimodal Capability\nMultimodal Reasoning and Mathematics\nOCR, Chart, and Document Understanding\nMulti-Image Understanding & Real-World Comprehension\nComprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\nVisual Grounding\nMultimodal Multilingual Understanding\nVideo Understanding\nGUI Tasks\nEmbodied Tasks\nSVG Tasks\nEvaluation on Language Capability\nAblation Study\nCascade Reinforcement Learning\nDecoupled Vision-Language Deployment\nQuick Start\nWe provide an example code to run InternVL3.5-8B using transformers. Please note that our models with up to 30B parameters can be deployed on a single A100 GPU, while the 38B model requires two A100 GPUs and the 235B model requires eight A100 GPUs.\nIn most cases, both LMDeploy and vLLM can be used for model deployment. However, for InternVL3.5-20B-A4B, we recommend using vLLM since lmdeploy has not yet supported GPT-OSS.\nPlease use transformers>=4.52.1 to ensure the model works normally. For the 20B version of our model, transformers>=4.55.0 is required.\nModel Loading\n16-bit (bf16 / fp16)\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval().cuda()\nBNB 8-bit Quantization\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=True,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval()\nMultiple GPUs\nimport math\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\npath = \"OpenGVLab/InternVL3_5-8B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=\"auto\").eval()\nThinking Mode\nTo enable thinking mode, please set the system prompt to our Thinking System Prompt. When enabling Thinking mode, we recommend setting do_sample=True and temperature=0.6 to mitigate undesired repetition.\nR1_SYSTEM_PROMPT = \"\"\"\nYou are an AI assistant that rigorously follows this response protocol:\n1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags.\n2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline.\nEnsure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section.\n\"\"\".strip()\nmodel.system_message = R1_SYSTEMP_PROMPT\nInference with Transformers\nimport math\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom decord import VideoReader, cpu\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import AutoModel, AutoTokenizer\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\ndef build_transform(input_size):\nMEAN, STD = IMAGENET_MEAN, IMAGENET_STD\ntransform = T.Compose([\nT.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\nT.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\nT.ToTensor(),\nT.Normalize(mean=MEAN, std=STD)\n])\nreturn transform\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\nbest_ratio_diff = float('inf')\nbest_ratio = (1, 1)\narea = width * height\nfor ratio in target_ratios:\ntarget_aspect_ratio = ratio[0] / ratio[1]\nratio_diff = abs(aspect_ratio - target_aspect_ratio)\nif ratio_diff < best_ratio_diff:\nbest_ratio_diff = ratio_diff\nbest_ratio = ratio\nelif ratio_diff == best_ratio_diff:\nif area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\nbest_ratio = ratio\nreturn best_ratio\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\norig_width, orig_height = image.size\naspect_ratio = orig_width / orig_height\n# calculate the existing image aspect ratio\ntarget_ratios = set(\n(i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\ni * j <= max_num and i * j >= min_num)\ntarget_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n# find the closest aspect ratio to the target\ntarget_aspect_ratio = find_closest_aspect_ratio(\naspect_ratio, target_ratios, orig_width, orig_height, image_size)\n# calculate the target width and height\ntarget_width = image_size * target_aspect_ratio[0]\ntarget_height = image_size * target_aspect_ratio[1]\nblocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n# resize the image\nresized_img = image.resize((target_width, target_height))\nprocessed_images = []\nfor i in range(blocks):\nbox = (\n(i % (target_width // image_size)) * image_size,\n(i // (target_width // image_size)) * image_size,\n((i % (target_width // image_size)) + 1) * image_size,\n((i // (target_width // image_size)) + 1) * image_size\n)\n# split the image\nsplit_img = resized_img.crop(box)\nprocessed_images.append(split_img)\nassert len(processed_images) == blocks\nif use_thumbnail and len(processed_images) != 1:\nthumbnail_img = image.resize((image_size, image_size))\nprocessed_images.append(thumbnail_img)\nreturn processed_images\ndef load_image(image_file, input_size=448, max_num=12):\nimage = Image.open(image_file).convert('RGB')\ntransform = build_transform(input_size=input_size)\nimages = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(image) for image in images]\npixel_values = torch.stack(pixel_values)\nreturn pixel_values\npath = 'OpenGVLab/InternVL3_5-8B'\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nload_in_8bit=False,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True,\ndevice_map=\"auto\").eval()\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n# set the max number of tiles in `max_num`\npixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\n# pure-text conversation (Á∫ØÊñáÊú¨ÂØπËØù)\nquestion = 'Hello, who are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Can you tell me a story?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\nquestion = '<image>\\nPlease describe the image shortly.'\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f'User: {question}\\nAssistant: {response}')\n# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\nquestion = '<image>\\nPlease describe the image in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Please write a poem according to the image.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# multi-image multi-round conversation, combined images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÊãºÊé•ÂõæÂÉè)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nquestion = '<image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nhistory=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nhistory=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# multi-image multi-round conversation, separate images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÁã¨Á´ãÂõæÂÉè)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\nquestion = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list,\nhistory=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list,\nhistory=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n# batch inference, single image per sample (ÂçïÂõæÊâπÂ§ÑÁêÜ)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nquestions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\nresponses = model.batch_chat(tokenizer, pixel_values,\nnum_patches_list=num_patches_list,\nquestions=questions,\ngeneration_config=generation_config)\nfor question, response in zip(questions, responses):\nprint(f'User: {question}\\nAssistant: {response}')\n# video multi-round conversation (ËßÜÈ¢ëÂ§öËΩÆÂØπËØù)\ndef get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\nif bound:\nstart, end = bound[0], bound[1]\nelse:\nstart, end = -100000, 100000\nstart_idx = max(first_idx, round(start * fps))\nend_idx = min(round(end * fps), max_frame)\nseg_size = float(end_idx - start_idx) / num_segments\nframe_indices = np.array([\nint(start_idx + (seg_size / 2) + np.round(seg_size * idx))\nfor idx in range(num_segments)\n])\nreturn frame_indices\ndef load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\nvr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\nmax_frame = len(vr) - 1\nfps = float(vr.get_avg_fps())\npixel_values_list, num_patches_list = [], []\ntransform = build_transform(input_size=input_size)\nframe_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\nfor frame_index in frame_indices:\nimg = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\nimg = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(tile) for tile in img]\npixel_values = torch.stack(pixel_values)\nnum_patches_list.append(pixel_values.shape[0])\npixel_values_list.append(pixel_values)\npixel_values = torch.cat(pixel_values_list)\nreturn pixel_values, num_patches_list\nvideo_path = './examples/red-panda.mp4'\npixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\npixel_values = pixel_values.to(torch.bfloat16).cuda()\nvideo_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\nquestion = video_prefix + 'What is the red panda doing?'\n# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nquestion = 'Describe this video in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\nnum_patches_list=num_patches_list, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\nStreaming Output\nBesides this method, you can also use the following code to get streamed output.\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n# Initialize the streamer\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=10)\n# Define the generation configuration\ngeneration_config = dict(max_new_tokens=1024, do_sample=False, streamer=streamer)\n# Start the model chat in a separate thread\nthread = Thread(target=model.chat, kwargs=dict(\ntokenizer=tokenizer, pixel_values=pixel_values, question=question,\nhistory=None, return_history=False, generation_config=generation_config,\n))\nthread.start()\n# Initialize an empty string to store the generated text\ngenerated_text = ''\n# Loop through the streamer to get the new text as it is generated\nfor new_text in streamer:\nif new_text == model.conv_template.sep:\nbreak\ngenerated_text += new_text\nprint(new_text, end='', flush=True)  # Print each new chunk of generated text on the same line\nFinetune\nMany repositories now support fine-tuning of the InternVL series models, including InternVL, SWIFT, XTuner, and others. Please refer to their documentation for more details on fine-tuning.\nDeployment\nLMDeploy\nLMDeploy is a toolkit for compressing, deploying, and serving LLMs & VLMs.\npip install lmdeploy>=0.9.1\nLMDeploy abstracts the complex inference process of multi-modal Vision-Language Models (VLM) into an easy-to-use pipeline, similar to the Large Language Model (LLM) inference pipeline.\nA 'Hello, world' Example\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nresponse = pipe(('describe this image', image))\nprint(response.text)\nMulti-images Inference\nWhen dealing with multiple images, you can put them all in one list. Keep in mind that multiple images will lead to a higher number of input tokens, and as a result, the size of the context window typically needs to be increased.\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nfrom lmdeploy.vl.constants import IMAGE_TOKEN\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage_urls=[\n'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg',\n'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg'\n]\nimages = [load_image(img_url) for img_url in image_urls]\n# Numbering images improves multi-image conversations\nresponse = pipe((f'Image-1: {IMAGE_TOKEN}\\nImage-2: {IMAGE_TOKEN}\\ndescribe these two images', images))\nprint(response.text)\nBatch Prompts Inference\nConducting inference with batch prompts is quite straightforward; just place them within a list structure:\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage_urls=[\n\"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg\",\n\"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg\"\n]\nprompts = [('describe this image', load_image(img_url)) for img_url in image_urls]\nresponse = pipe(prompts)\nprint(response)\nMulti-turn Conversation\nThere are two ways to do the multi-turn conversations with the pipeline. One is to construct messages according to the format of OpenAI and use above introduced method, the other is to use the pipeline.chat interface.\nfrom lmdeploy import pipeline, PytorchEngineConfig, GenerationConfig\nfrom lmdeploy.vl import load_image\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg')\ngen_config = GenerationConfig(top_k=50, top_p=0.95, temperature=0.6, max_new_tokens=8192)\nsess = pipe.chat(('describe this image', image), gen_config=gen_config)\nprint(sess.response.text)\nsess = pipe.chat('What is the woman doing?', session=sess, gen_config=gen_config)\nprint(sess.response.text)\nService\nLMDeploy's api_server enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:\nlmdeploy serve api_server OpenGVLab/InternVL3_5-8B --server-port 23333 --tp 1 --backend pytorch\nTo use the OpenAI-style interface, you need to install OpenAI:\npip install openai\nThen, use the code below to make the API call:\nfrom openai import OpenAI\nclient = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=[{\n'role':\n'user',\n'content': [{\n'type': 'text',\n'text': 'describe this image',\n}, {\n'type': 'image_url',\n'image_url': {\n'url':\n'https://modelscope.oss-cn-beijing.aliyuncs.com/resource/tiger.jpeg',\n},\n}],\n}],\ntemperature=0.8,\ntop_p=0.8)\nprint(response)\nLicense\nThis project is released under the apache-2.0 License. This project uses the pre-trained Qwen3 as a component, which is licensed under the apache-2.0 License.\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{wang2025internvl3_5,\ntitle={InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},\nauthor={Wang, Weiyun and Gao, Zhangwei and Gu, Lixin and Pu, Hengjun and Cui, Long and Wei, Xingguang and Liu, Zhaoyang and Jing, Linglin and Ye, Shenglong and Shao, Jie and others},\njournal={arXiv preprint arXiv:2508.18265},\nyear={2025}\n}",
    "apple/FastVLM-1.5B": "FastVLM: Efficient Vision Encoding for Vision Language Models\nCitation\nFastVLM: Efficient Vision Encoding for Vision Language Models\nFastVLM was introduced in\nFastVLM: Efficient Vision Encoding for Vision Language Models. (CVPR 2025)\nHighlights\nWe introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.\nOur smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.\nOur larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.\nEvaluations\nBenchmark\nFastVLM-0.5B\nFastVLM-1.5B\nFastVLM-7B\nAi2D\n68.0\n77.4\n83.6\nScienceQA\n85.2\n94.4\n96.7\nMMMU\n33.9\n37.8\n45.4\nVQAv2\n76.3\n79.1\n80.8\nChartQA\n76.0\n80.1\n85.0\nTextVQA\n64.5\n70.4\n74.9\nInfoVQA\n46.4\n59.7\n75.8\nDocVQA\n82.5\n88.3\n93.2\nOCRBench\n63.9\n70.2\n73.1\nRealWorldQA\n56.1\n61.2\n67.2\nSeedBench-Img\n71.0\n74.2\n75.4\nUsage Example\nTo run inference of PyTorch checkpoint, follow the instruction in the official repo:\nDownload the model\nhuggingface-cli download apple/FastVLM-1.5B\nRun inference using predict.py from the official repo.\npython predict.py --model-path /path/to/checkpoint-dir \\\n--image-file /path/to/image.png \\\n--prompt \"Describe the image.\"\nRun inference with Transformers (Remote Code)\nTo run inference with transformers we can leverage trust_remote_code along with the following snippet:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nMID = \"apple/FastVLM-1.5B\"\nIMAGE_TOKEN_INDEX = -200  # what the model code looks for\n# Load\ntok = AutoTokenizer.from_pretrained(MID, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nMID,\ntorch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\n# Build chat -> render to string (not tokens) so we can place <image> exactly\nmessages = [\n{\"role\": \"user\", \"content\": \"<image>\\nDescribe this image in detail.\"}\n]\nrendered = tok.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=False\n)\npre, post = rendered.split(\"<image>\", 1)\n# Tokenize the text *around* the image token (no extra specials!)\npre_ids  = tok(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\npost_ids = tok(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n# Splice in the IMAGE token id (-200) at the placeholder position\nimg_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\ninput_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)\nattention_mask = torch.ones_like(input_ids, device=model.device)\n# Preprocess image via the model's own processor\nimg = Image.open(\"test-2.jpg\").convert(\"RGB\")\npx = model.get_vision_tower().image_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\npx = px.to(model.device, dtype=model.dtype)\n# Generate\nwith torch.no_grad():\nout = model.generate(\ninputs=input_ids,\nattention_mask=attention_mask,\nimages=px,\nmax_new_tokens=128,\n)\nprint(tok.decode(out[0], skip_special_tokens=True))\nCitation\nIf you found this model useful, please cite the following paper:\n@InProceedings{fastvlm2025,\nauthor = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},\ntitle = {FastVLM: Efficient Vision Encoding for Vision Language Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2025},\n}",
    "janhq/Jan-v1-edge-gguf": "Jan-v1-edge: Distilled for Edge, Built for Web Search\nOverview\nPerformance\nQuestion Answering(SimpleQA)\nChat & Instruction Following\nQuick Start\nIntegration with Jan App\nLocal Deployment\nRecommended Inference Parameters\nü§ù Community & Support\nüìÑ Citation\nJan-v1-edge: Distilled for Edge, Built for Web Search\nOverview\nJan-v1-edge is a lightweight agentic model built for fast, reliable on-device execution. As the second release in the Jan Family, it is distilled from the larger Jan-v1 model, preserving strong reasoning and problem-solving ability in a smaller footprint suitable for resource-constrained environments.\nJan-v1-edge was developed through a two-phase post-training process. The first phase, Supervised Fine-Tuning (SFT), transferred core capabilities from the Jan-v1 teacher model to the smaller student. The second phase, Reinforcement Learning with Verifiable Rewards (RLVR) ‚Äîthe same method used in Jan-v1 and Lucy‚Äîfurther optimized reasoning efficiency, tool use, and correctness. This staged approach delivers reliable results on complex, interactive workloads.\nPerformance\nQuestion Answering(SimpleQA)\nDespite having only 1.7B parameters, Jan-v1-edge achieves 83% accuracy‚Äînearly matching the larger Jan-nano-128k‚Äîdemonstrating its efficiency and robustness.\nChat & Instruction Following\nVersus Qwen 3 1.7B Thinking, Jan-v1-edge shows a slight degradation on instruction-following and CreativeWriting, while remaining comparable or better on EQBench and recency QA.\nQuick Start\nIntegration with Jan App\nJan-v1-edge is optimized for direct integration with the Jan App. Simply select the model from the Jan App interface for immediate access to its full capabilities.\nLocal Deployment\nUsing vLLM:\nvllm serve janhq/Jan-v1-edge \\\n--host 0.0.0.0 \\\n--port 1234 \\\n--enable-auto-tool-choice \\\n--tool-call-parser hermes\nUsing llama.cpp:\nllama-server --model Jan-v1-edge-Q8_0.gguf \\\n--host 0.0.0.0 \\\n--port 1234 \\\n--jinja \\\n--no-context-shift\nRecommended Inference Parameters\ntemperature: 0.6\ntop_p: 0.95\ntop_k: 20\nmin_p: 0.0\nmax_tokens: 2048\nü§ù Community & Support\nDiscussions: HuggingFace Community\nJan App: Discover more about the Jan App at jan.ai\nüìÑ Citation\nUpdated Soon",
    "TheDrummer/GLM-Steam-106B-A12B-v1": "Join our Discord! https://discord.gg/BeaverAI\nNearly 7000 members strong üí™ A hub for users and makers alike!\nThank you to everyone who subscribed through Patreon. Your suppprt helps me chug along in this brave new world.\nGLM Steam 106B A12B v1 üöÇ\nUsage\nDescription\nLinks\nSpecial Thanks\nJoin our Discord! https://discord.gg/BeaverAI\nNearly 7000 members strong üí™ A hub for users and makers alike!\nThank you to everyone who subscribed through Patreon. Your suppprt helps me chug along in this brave new world.\nDrummer proudly presents...\nGLM Steam 106B A12B v1 üöÇ\nThe smoke and the fire and the speed, the action and the sound, and everything that goes together, the steam engine is the most beautiful machine that we ever made, there's just nothing like it.\nUsage\nGLM-4.5 (Think or No Thinking)\nhttps://rentry.org/geechan#model-specific-presets\nDescription\nSteam v1 has got the juice\nCharacters are as vivid as the original GLM-Air, though prose is much more enticing.\nDamn okay this model is actually pretty good. I don't have enough vram to test it on longer chats to 16k, but on 6k chats it's looking good and without deepseek's slop.\nthis model has a unique way of speaking. imo it's kept the same \"soul\" of the writing as Air but with more creativity and willingness to be hor -\nthis model is fun! :3\nLinks\nOriginal: https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1\nGGUF: https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1-GGUF\niMatrix (recommended): https://huggingface.co/bartowski/TheDrummer_GLM-Steam-106B-A12B-v1-GGUF\nEXL3: https://huggingface.co/ArtusDev/TheDrummer_GLM-Steam-106B-A12B-v1-EXL3\nSpecial Thanks\nThank you to Nectar.AI for making this finetune possible, and your belief and support for Generative AI as entertainment!\nThank you, zerofata, for collaborating with me and diving headfirst on tuning GLM Air!\nconfig-v1b",
    "jinaai/jina-code-embeddings-1.5b": "Jina Code Embeddings: A Small but Performant Code Embedding Model\nIntended Usage & Model Info\nUsage\nOptional / Recommended\nCitation\nContact\nJina Code Embeddings: A Small but Performant Code Embedding Model\nIntended Usage & Model Info\njina-code-embeddings is an embedding model for code retrieval.\nThe model supports various types of code retrieval (text-to-code, code-to-code, code-to-text, code-to-completion) and technical question answering across 15+ programming languages.\nBuilt on Qwen/Qwen2.5-Coder-1.5B, jina-code-embeddings-1.5b features:\nMultilingual support (15+ programming languages) and compatibility with a wide range of domains, including web development, software development, machine learning, data science, and educational coding problems.\nTask-specific instruction prefixes for NL2Code, Code2Code, Code2NL, Code2Completion, and Technical QA, which can be selected at inference time.\nFlexible embedding size: dense embeddings are 1536-dimensional by default but can be truncated to as low as 128 with minimal performance loss.\nSummary of features:\nFeature\nJina Code Embeddings 1.5B\nBase Model\nQwen2.5-Coder-1.5B\nSupported Tasks\nnl2code, code2code, code2nl, code2completion, qa\nModel DType\nBFloat 16\nMax Sequence Length\n32768\nEmbedding Vector Dimension\n1536\nMatryoshka dimensions\n128, 256, 512, 1024, 1536\nPooling Strategy\nLast-token pooling\nAttention Mechanism\nFlashAttention2\nUsage\nRequirements\nThe following Python packages are required:\ntransformers>=4.53.0\ntorch>=2.7.1\nOptional / Recommended\nflash-attention: Installing flash-attention is recommended for improved inference speed and efficiency, but not mandatory.\nsentence-transformers: If you want to use the model via the sentence-transformers interface, install this package as well.\nvia transformers\n# !pip install transformers>=4.53.0 torch>=2.7.1\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\nINSTRUCTION_CONFIG = {\n\"nl2code\": {\n\"query\": \"Find the most relevant code snippet given the following query:\\n\",\n\"passage\": \"Candidate code snippet:\\n\"\n},\n\"qa\": {\n\"query\": \"Find the most relevant answer given the following question:\\n\",\n\"passage\": \"Candidate answer:\\n\"\n},\n\"code2code\": {\n\"query\": \"Find an equivalent code snippet given the following code snippet:\\n\",\n\"passage\": \"Candidate code snippet:\\n\"\n},\n\"code2nl\": {\n\"query\": \"Find the most relevant comment given the following code snippet:\\n\",\n\"passage\": \"Candidate comment:\\n\"\n},\n\"code2completion\": {\n\"query\": \"Find the most relevant completion given the following start of code snippet:\\n\",\n\"passage\": \"Candidate completion:\\n\"\n}\n}\nMAX_LENGTH = 8192\ndef cosine_similarity(x,y):\nx = F.normalize(x, p=2, dim=1)\ny = F.normalize(y, p=2, dim=1)\nreturn x @ y.T\ndef last_token_pool(last_hidden_states, attention_mask):\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef add_instruction(instruction, query):\nreturn f'{instruction}{query}'\n# The queries and documents to embed\nqueries = [\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"query\"], \"print hello world in python\"),\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"query\"], \"initialize array of 5 zeros in c++\")\n]\ndocuments = [\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"passage\"], \"print('Hello World!')\"),\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"passage\"], \"int arr[5] = {0, 0, 0, 0, 0};\")\n]\nall_inputs = queries + documents\ntokenizer = AutoTokenizer.from_pretrained('jinaai/jina-code-embeddings-1.5b')\nmodel = AutoModel.from_pretrained('jinaai/jina-code-embeddings-1.5b')\nbatch_dict = tokenizer(\nall_inputs,\npadding=True,\ntruncation=True,\nmax_length=MAX_LENGTH,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\nquery_embeddings = embeddings[:2]\npassage_embeddings = embeddings[2:]\n# Compute the (cosine) similarity between the query and document embeddings\nscores = cosine_similarity(query_embeddings, passage_embeddings)\nprint(scores)\n# tensor([[0.7647, 0.1115],\n#         [0.0930, 0.6606]], grad_fn=<MmBackward0>)\nvia sentence-transformers\n# !pip install sentence_transformers>=5.0.0 torch>=2.7.1\nimport torch\nfrom sentence_transformers import SentenceTransformer\n# Load the model\nmodel = SentenceTransformer(\n\"jinaai/jina-code-embeddings-1.5b\",\nmodel_kwargs={\n\"torch_dtype\": torch.bfloat16,\n\"attn_implementation\": \"flash_attention_2\",\n\"device_map\": \"cuda\"\n},\ntokenizer_kwargs={\"padding_side\": \"left\"},\n)\n# The queries and documents to embed\nqueries = [\n\"print hello world in python\",\n\"initialize array of 5 zeros in c++\"\n]\ndocuments = [\n\"print('Hello World!')\",\n\"int arr[5] = {0, 0, 0, 0, 0};\"\n]\nquery_embeddings = model.encode(queries, prompt_name=\"nl2code_query\")\ndocument_embeddings = model.encode(documents, prompt_name=\"nl2code_document\")\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7670, 0.1117],\n#         [0.0938, 0.6607]])\nvia vLLM\nimport torch\nimport torch.nn.functional as F\nfrom vllm import LLM\nINSTRUCTION_CONFIG = {\n\"nl2code\": {\n\"query\": \"Find the most relevant code snippet given the following query:\\n\",\n\"passage\": \"Candidate code snippet:\\n\"\n},\n\"qa\": {\n\"query\": \"Find the most relevant answer given the following question:\\n\",\n\"passage\": \"Candidate answer:\\n\"\n},\n\"code2code\": {\n\"query\": \"Find an equivalent code snippet given the following code snippet:\\n\",\n\"passage\": \"Candidate code snippet:\\n\"\n},\n\"code2nl\": {\n\"query\": \"Find the most relevant comment given the following code snippet:\\n\",\n\"passage\": \"Candidate comment:\\n\"\n},\n\"code2completion\": {\n\"query\": \"Find the most relevant completion given the following start of code snippet:\\n\",\n\"passage\": \"Candidate completion:\\n\"\n}\n}\ndef add_instruction(instruction, text):\nreturn f\"{instruction}{text}\"\ndef cosine_similarity(x, y):\nx = F.normalize(x, p=2, dim=1)\ny = F.normalize(y, p=2, dim=1)\nreturn x @ y.T\n# Build the queries and documents\nqueries = [\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"query\"], \"print hello world in python\"),\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"query\"], \"initialize array of 5 zeros in c++\"),\n]\ndocuments = [\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"passage\"], \"print('Hello World!')\"),\nadd_instruction(INSTRUCTION_CONFIG[\"nl2code\"][\"passage\"], \"int arr[5] = {0, 0, 0, 0, 0};\"),\n]\nall_inputs = queries + documents\n# vLLM embedding model\nllm = LLM(\nmodel=\"jinaai/jina-code-embeddings-1.5b\",\ntask=\"embed\"\n)\n# Encode with vLLM\noutputs = llm.encode(all_inputs)\n# Collect embeddings into a single tensor\nemb_list = []\nfor out in outputs:\nvec = out.outputs.data.detach()\nemb_list.append(vec)\nembeddings = torch.stack(emb_list, dim=0)\n# Split into query and passage embeddings\nn_q = len(queries)\nquery_embeddings = embeddings[:n_q]\npassage_embeddings = embeddings[n_q:]\n# Cosine similarity matrix (queries x documents)\nscores = cosine_similarity(query_embeddings, passage_embeddings)\nprint(scores)\n# tensor([[0.7650, 0.1118],\n#         [0.0937, 0.6613]])\nCitation\nPlease refer to our technical report of jina-code-embeddings for training details and benchmarks. If you find it useful in your research, please cite the following paper:\n@misc{kryvosheieva2025efficientcodeembeddingscode,\ntitle={Efficient Code Embeddings from Code Generation Models},\nauthor={Daria Kryvosheieva and Saba Sturua and Michael G√ºnther and Scott Martens and Han Xiao},\nyear={2025},\neprint={2508.21290},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.21290},\n}\nContact\nJoin our Discord community and chat with other community members about ideas.",
    "Comfy-Org/Qwen-Image-InstantX-ControlNets": "README.md exists but content is empty.",
    "5CD-AI/Vintern-Embedding-1B": "Model Details\nBenchmark Highlights\nSummary\nBenchmark Details\nExamples:\nQuickstart:\nModel Details\nVintern-Embedding-1B is the next-generation embedding model built on top of the base Vintern-1B-v3_5. It was trained on over 1.5 million high-quality question‚Äìdocument pairs, including both Visual Question Answering (VQA) and pure text QA tasks. Leveraging this large and diverse dataset, the model is capable of handling a wide range of cross-modal retrieval tasks, including:\nText ‚Üí Visual\nText ‚Üí Text\nCompared to ColVintern-1B-v1, which was more experimental, this version is significantly optimized and achieves much higher retrieval quality. Despite having only ~0.9B parameters, it performs competitively with larger 2B‚Äì7B multimodal embedding models, making it both lightweight and highly effective.\nBenchmark Highlights\nGreenNode/Markdown Table Retrieval (Vietnamese)\nAchieved MAP@5 = 57.01 and Mean = 59.71, clearly multi-vector embedding outperforming all existing multilingual and Vietnamese-specific embedding baselines.\nGreenNode/Zalo Legal Text Retrieval (Vietnamese)\nScored Mean = 73.14, on par with or surpassing Vietnamese-specialized models, showing strong performance on legal retrieval tasks.\nViDoRe Benchmark (Global Multimodal Standard)\nReached Average Score = 82.85, improving over ColVintern-1B v1 (78.8) and approaching the performance of several 2B‚Äì3B multimodal embedding models.\nParticularly strong in domains such as Artificial Intelligence (97.52), Healthcare (97.09), and Government (93.97).\nSummary\nüëâ Vintern-Embedding-1B (v2) delivers robust cross-modal retrieval, excels on both Vietnamese-specific and global multimodal benchmarks, and remains highly efficient at ~1B parameters. It is a strong choice for RAG pipelines, multimodal search engines, and information retrieval applications in both English and Vietnamese.\nBenchmark Details\nDataset:  GreenNode/GreenNode-Table-Markdown-Retrieval\nModel Name\nMAP@5 ‚Üë\nMRR@5 ‚Üë\nNDCG@5 ‚Üë\nRecall@5 ‚Üë\nMean ‚Üë\nMultilingual Embedding models\nme5_small\n33.75\n33.75\n35.68\n41.49\n36.17\nme5_large\n38.16\n38.16\n40.27\n46.62\n40.80\nM3-Embedding\n36.52\n36.52\n38.60\n44.84\n39.12\nOpenAI-embedding-v3\n30.61\n30.61\n32.57\n38.46\n33.06\nVietnamese Embedding models (Prior Work)\nhalong-embedding\n32.15\n32.15\n34.13\n40.09\n34.63\nsup-SimCSE-VietNamese-phobert_base\n10.90\n10.90\n12.03\n15.41\n12.31\nvietnamese-bi-encoder\n13.61\n13.61\n14.63\n17.68\n14.89\nGreenNode-Embedding\nM3-GN-VN\n41.85\n41.85\n44.15\n57.05\n46.23\nM3-GN-VN-Mixed\n42.08\n42.08\n44.33\n51.06\n44.89\nOurs ‚Äì Multi-vector embedding\nVintern-Embedding-1B\n57.01\n57.01\n59.17\n65.65\n59.71\nDataset:  GreenNode/zalo-ai-legal-text-retrieval-vn\nModel Name\nMAP@5 ‚Üë\nMRR@5 ‚Üë\nNDCG@5 ‚Üë\nRecall@5 ‚Üë\nMean ‚Üë\nMultilingual Embedding models\nme5_small\n54.68\n54.37\n58.32\n69.16\n59.13\nme5_large\n60.14\n59.62\n64.17\n76.02\n64.99\nM3-Embedding\n69.34\n68.96\n73.70\n86.68\n74.67\nOpenAI-embedding-v3\n38.68\n38.80\n41.53\n49.94\n41.74\nVietnamese Embedding models (Prior Work)\nhalong-embedding\n52.57\n52.28\n56.64\n68.72\n57.55\nsup-SimCSE-VietNamese-phobert_base\n25.15\n25.07\n27.81\n35.79\n28.46\nvietnamese-bi-encoder\n54.88\n54.47\n59.10\n79.51\n61.99\nGreenNode-Embedding\nM3-GN-VN\n65.03\n64.80\n69.19\n81.66\n70.17\nM3-GN-VN-Mixed\n69.75\n69.28\n74.01\n86.74\n74.95\nOurs ‚Äì Multi-vector embedding\nVintern-Embedding-1B\n68.90\n69.06\n72.32\n82.29\n73.14\nDataset: ViDoRe Benchmark\nModel\nModel_Size\nAverage_Score\nArxivQA\nDocVQA\nInfoVQA\nArtificial Intelligence\nEnergy\nGovernment\nHealthcare Industry\nTAT-DQA\nroyokong/e5-v\n8.3B\n62.88\n48.3\n34.7\n69.2\n78.9\n78.1\n82.2\n82.3\n29.3\nTIGER-Lab/VLM2Vec-Full\n4.2B\n51.16\n42.8\n26.7\n66.7\n53.5\n63.5\n64\n70.7\n21.4\nnvidia/llama-nemoretriever-colembed-3b-v1\n4.4B\n90.42\n88.4\n66.2\n94.9\n99.6\n96.6\n97.8\n99.3\n80.6\nnvidia/llama-nemoretriever-colembed-1b-v1\n2.4B\n89.8\n87.6\n64.5\n93.6\n100\n96.6\n96.7\n99.6\n79.8\njinaai/jina-embeddings-v4\n3.8B\n89.38\n88.5\n60.1\n93.8\n99.3\n97.3\n96.6\n99.1\n80.3\nnomic-ai/colnomic-embed-multimodal-3b\n3B\n89.25\n88.1\n61.3\n92.8\n96.3\n97.4\n96.6\n98.3\n83.2\nnomic-ai/colnomic-embed-multimodal-7b\n7B\n89.00\n88.3\n60.1\n92.2\n98.8\n96.3\n95.9\n99.3\n81.1\nvidore/colqwen2.5-v0.2\n3B\n89.58\n88.9\n63.6\n92.5\n99.6\n96.1\n95.8\n98\n82.1\nvidore/colqwen2-v1.0\n2.2B\n89.18\n88\n61.5\n92.5\n99\n95.9\n95.5\n98.8\n82.2\nibm-granite/granite-vision-3.3-2b-embedding\n3B\n85.98\n84.2\n54.6\n89.7\n98.9\n96.3\n97.3\n98.9\n67.9\nvidore/colpali-v1.3\n3B\n85.44\n83.3\n58.4\n85.5\n97.4\n94.6\n96.1\n97.4\n70.8\nvidore/colpali-v1.2\n3B\n83.16\n77.8\n56.6\n82.2\n97.5\n93.8\n94.4\n94.9\n68.1\nColVintern-1B\n0.9B\n78.8\n71.6\n48.3\n84.6\n92.9\n88.7\n89.4\n95.2\n59.6\nVintern-Embedding-1B\n0.9B\n82.85\n75.37\n51.79\n86.2\n97.52\n93.19\n93.97\n97.09\n67.72\nExamples:\nQuery Input:\n\"S·ª≠ d·ª•ng ma tu√Ω b·ªã g√¨ ?\"\nRelevant Document Output:\nMa t√∫y, thu·ªëc g√¢y nghi·ªán, thu·ªëc h∆∞·ªõng th·∫ßn v√† ti·ªÅn ch·∫•t ma t√∫y;\nc) Vi ph·∫°m c√°c quy ƒë·ªãnh v·ªÅ nghi√™n c·ª©u, gi√°m ƒë·ªãnh, ki·ªÉm ƒë·ªãnh, ki·ªÉm nghi·ªám, s·∫£n xu·∫•t, b·∫£o qu·∫£n, t·ªìn tr·ªØ ch·∫•t ma t√∫y, ti·ªÅn ch·∫•t ma t√∫y;\nd) Vi ph·∫°m c√°c quy ƒë·ªãnh v·ªÅ giao nh·∫≠n, t√†ng tr·ªØ, v·∫≠n chuy·ªÉn ch·∫•t ma t√∫y, thu·ªëc g√¢y nghi·ªán, thu·ªëc h∆∞·ªõng th·∫ßn, ti·ªÅn ch·∫•t ma t√∫y;\nƒë) Vi ph·∫°m c√°c quy ƒë·ªãnh v·ªÅ ph√¢n ph·ªëi, mua b√°n, s·ª≠ d·ª•ng, trao ƒë·ªïi ch·∫•t ma t√∫y, thu·ªëc g√¢y nghi·ªán, thu·ªëc h∆∞·ªõng th·∫ßn, ti·ªÅn ch·∫•t ma t√∫y;\ne) Vi ph·∫°m c√°c quy ƒë·ªãnh v·ªÅ qu·∫£n l√Ω, ki·ªÉm so√°t, l∆∞u gi·ªØ ch·∫•t ma t√∫y, thu·ªëc g√¢y nghi·ªán, thu·ªëc h∆∞·ªõng th·∫ßn, ti·ªÅn ch·∫•t t·∫°i c√°c khu v·ª±c c·ª≠a kh·∫©u, bi√™n gi·ªõi, tr√™n bi·ªÉn;\ng) Th·ª±c hi·ªán cai nghi·ªán ma t√∫y v∆∞·ª£t qu√° ph·∫°m vi ho·∫°t ƒë·ªông ƒë∆∞·ª£c ghi trong gi·∫•y ph√©p ho·∫°t ƒë·ªông cai nghi·ªán ma t√∫y t·ª± nguy·ªán.\n6. Ph·∫°t ti·ªÅn t·ª´ 40.000.000 ƒë·ªìng ƒë·∫øn 50.000.000 ƒë·ªìng ƒë·ªëi v·ªõi h√†nh vi cho m∆∞·ª£n, cho thu√™, chuy·ªÉn nh∆∞·ª£ng ho·∫∑c s·ª≠ d·ª•ng gi·∫•y ph√©p ho·∫°t ƒë·ªông cai nghi·ªán ma t√∫y t·ª± nguy·ªán v√†o c√°c m·ª•c ƒë√≠ch kh√°c.\n7. Ph·∫°t ti·ªÅn t·ª´ 50.000.000 ƒë·ªìng ƒë·∫øn 75.000.000 ƒë·ªìng ƒë·ªëi v·ªõi h√†nh vi t·ªï ch·ª©c cai nghi·ªán ma t√∫\nQuery Input:\n\"ƒêi xe b·∫±ng 1 b√°nh b·ªã ph·∫°t bao nhi√™u ?\"\nRelevant Image Output:\nQuery Input:\n\"Kinh t·∫ø Campuchia tƒÉng tr∆∞·ªüng nh∆∞ n√†o nƒÉm 2021 ?\"\nRelevant Image Output:\nQuery Input:\n\"C√¥ng nghi·ªáp t·ª´ nƒÉm 2017 tƒÉng tr∆∞·ªüng ra sao ?\"\nRelevant Image Output:\nQuickstart:\nInstallation:\npip install decord\npip install transformers==4.48.2\npip install flash_attn\nDownload samples:\nwget https://huggingface.co/5CD-AI/ColVintern-1B-v1/resolve/main/ex1.jpg\nwget https://huggingface.co/5CD-AI/ColVintern-1B-v1/resolve/main/ex2.jpg\nInference:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoProcessor\nimport matplotlib.pyplot as plt\n# ==============================\n# 1. Load Model and Processor\n# ==============================\nmodel_name = \"5CD-AI/Vintern-Embedding-1B\"\nmodel = AutoModel.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\ntrust_remote_code=True,\n).eval().cuda()\nprocessor = AutoProcessor.from_pretrained(\nmodel_name,\ntrust_remote_code=True\n)\n# ==============================\n# 2. Prepare Input Data\n# ==============================\n# !wget https://huggingface.co/5CD-AI/ColVintern-1B-v1/resolve/main/ex1.jpg\n# !wget https://huggingface.co/5CD-AI/ColVintern-1B-v1/resolve/main/ex2.jpg\nimages = [Image.open(\"ex1.jpg\"), Image.open(\"ex2.jpg\")]\nbatch_images = processor.process_images(images)\nqueries = [\n\"C·∫£ng H·∫£i Ph√≤ng ·ªü ƒë√¢u ?\",\n\"Ph√≠ giao h√†ng bao nhi√™u ?\",\n]\nbatch_queries = processor.process_queries(queries)\ntext_documents = [\n\"C·∫£ng H·∫£i Ph√≤ng l√† m·ªôt c·ª•m c·∫£ng bi·ªÉn t·ªïng h·ª£p c·∫•p qu·ªëc gia, l·ªõn th·ª© 2 ·ªü Vi·ªát Nam sau c·∫£ng S√†i G√≤n, l√† c·ª≠a ng√µ qu·ªëc t·∫ø c·ªßa Vi·ªát Nam, n·∫±m t·∫°i ba qu·∫≠n H·ªìng B√†ng, Ng√¥ Quy·ªÅn v√† H·∫£i An. B√™n c·∫°nh ƒë√≥, c√πng t√™n C·∫£ng H·∫£i Ph√≤ng (ti·∫øng Anh: Port of Hai Phong ho·∫∑c Hai Phong Port) l√† m·ªôt c·ª•m c·∫£ng bi·ªÉn thu·ªôc C√¥ng ty c·ªï ph·∫ßn c·∫£ng H·∫£i Ph√≤ng t·∫°i th√†nh ph·ªë H·∫£i Ph√≤ng, Vi·ªát Nam. ƒê√¢y l√† m·ªôt trong hai c·∫£ng bi·ªÉn t·ªïng h·ª£p l·ªõn v√† l√¢u ƒë·ªùi nh·∫•t t·∫°i Vi·ªát Nam, c√πng v·ªõi C√¥ng ty C·∫£ng S√†i G√≤n ·ªü ph√≠a Nam.\",\n\"S√¢n bay Chu Lai (t·ªânh Qu·∫£ng Nam) c≈©ng ƒë∆∞·ª£c h√£ng h√†ng kh√¥ng gi√° r·∫ª Vietjet ƒë·ªÅ xu·∫•t ƒë·∫ßu t∆∞ n√¢ng c·∫•p 20.000 t·ªâ ƒë·ªìng theo 3 giai ƒëo·∫°n t·ª´ 2020-2025 ƒë·ªÉ ƒë·∫øn nƒÉm 2025 tr·ªü th√†nh C·∫£ng h√†ng kh√¥ng qu·ªëc t·∫ø v√† tr·ªü th√†nh trung t√¢m trung chuy·ªÉn, v·∫≠n t·∫£i h√†ng h√≥a l·ªõn c·ªßa c·∫£ n∆∞·ªõc theo quy ho·∫°ch c·ªßa B·ªô GTVT nƒÉm 2015.\",\n]\nbatch_text_docs = processor.process_docs(text_documents)\nraw_docs = images + text_documents\n# ==============================\n# 3. Move Tensors to GPU\n# ==============================\nbatch_images[\"pixel_values\"] = batch_images[\"pixel_values\"].cuda().bfloat16()\nbatch_images[\"input_ids\"] = batch_images[\"input_ids\"].cuda()\nbatch_images[\"attention_mask\"] = batch_images[\"attention_mask\"].cuda().bfloat16()\nbatch_queries[\"input_ids\"] = batch_queries[\"input_ids\"].cuda()\nbatch_queries[\"attention_mask\"] = batch_queries[\"attention_mask\"].cuda().bfloat16()\nbatch_text_docs[\"input_ids\"] = batch_text_docs[\"input_ids\"].cuda()\nbatch_text_docs[\"attention_mask\"] = batch_text_docs[\"attention_mask\"].cuda().bfloat16()\n# ==============================\n# 4. Generate Embeddings\n# ==============================\nwith torch.no_grad():\nimage_embeddings = model(**batch_images)\nquery_embeddings = model(**batch_queries)\ntext_docs_embeddings = model(**batch_text_docs)\n# ==============================\n# 5. Compute Similarity Scores\n# ==============================\nscores = processor.score_multi_vector(\nquery_embeddings,\nlist(image_embeddings) + list(text_docs_embeddings)\n)\nmax_scores, max_indices = torch.max(scores, dim=1)\n# ==============================\n# 6. Print Results\n# ==============================\nfor i, query in enumerate(queries):\nprint(\"=\" * 100)\nprint(f\"Query: '{query}'\")\nprint(f\"Score: {max_scores[i].item()}\\n\")\ndoc = raw_docs[max_indices[i]]\nif isinstance(doc, str):\nprint(f\"Matched Text Document:\\n{doc}\\n\")\nelse:\nplt.figure(figsize=(5, 5))\nplt.imshow(doc)\nplt.axis(\"off\")\nplt.show()\nQuickstart Google Colab: https://colab.research.google.com/drive/1jkP7fJja5RmSrP5Ad0_Csu7zPtIz-AwV?usp=sharing",
    "wikeeyang/Real-Qwen-Image-v1.0": "Real-Qwen-Image v1.0 version:\nAlso onÔºö\nÊ®°Âûã‰ΩøÁî®Ôºö\nLicense Agreement\nReal-Qwen-Image v1.0 version:\nÊú¨Ê®°Âûã‰∏∫ Qwen_Image ÂæÆË∞ÉÊ®°ÂûãÔºå‰∏ªË¶ÅÊèêÂçá‰∫ÜÂá∫ÂõæÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÂÜôÂÆûÊÑü„ÄÇÂÖ∑‰ΩìÊïàÊûúÂèÇËßÅÁ§∫‰æãÂõæÁâáÔºåÂõæÁâá‰∏≠‰πüÈôÑÂ∏¶Êúâ ComfyUI Â∑•‰ΩúÊµÅÔºåÊú¨Ê®°ÂûãÊûÅÊòì‰ΩøÁî®„ÄÅÂø´ÈÄüÂá∫Âõæ„ÄÅLoRAÂÖºÂÆπÊÄßËâØÂ•Ω„ÄÇ\nThe model is the Qwen_Image fine-tuned model, It enhances the clarity and realism of the generated images. For specific effects, please refer to the example images, which also include the ComfyUI workflow. The model is very easy to use and quickly generates images, and have a good LoRA compatibility.\nAlso onÔºö\nhttps://www.modelscope.cn/models/wikeeyang/Real-Qwen-Image\nhttps://civitai.com/models/1898752\nÊ®°Âûã‰ΩøÁî®Ôºö\nÂü∫Êú¨ÁªÑÂêàÔºöeuler+simpleÔºåcfg 1.0Ôºåsteps 20 - 30ÔºåÊÇ®ÂèØ‰ª•Â∞ùËØï‰∏çÂêåÁöÑÁªÑÂêà„ÄÇ\nBasic: euler+simple, cfg 1.0, steps 20 - 30, You can try more different combinations.\nüíú Qwen Chat¬†¬† | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Tech Report ¬†¬† | ¬†¬† üìë Blog\nüñ•Ô∏è Demo¬†¬† | ¬†¬†üí¨ WeChat (ÂæÆ‰ø°)¬†¬† | ¬†¬†ü´® Discord\nLicense Agreement\nQwen-Image is licensed under Apache 2.0.",
    "driaforall/mem-agent": "mem-agent\nBenchmark\nUsage\nMemory\nExample user.md\nExample entity files (jane_doe.md and acme_corp.md)\nReferences:\nmem-agent\nBased on Qwen3-4B-Thinking-2507, this model was trained using GSPO (Zheng et al., 2025) over an agent scaffold that is built around an Obisidian-like memory system and the tools required to interact with it. The model was trained on the following subtasks:\nRetrieval: Retrieving relevant information when needed from the memory system. In this subtask, we also trained the model on filtering the retrieved information and/or obfuscating it completely.\nUpdating: Updating the memory system with new information.\nClarification: Asking for clarification when the user query is not clear/contradicting with the information in the memory system.\nThe tools in the scaffold are:\n# File Operations\ncreate_file(file_path: str, content: str = \"\") -> bool  # Auto-creates parent directories\nupdate_file(file_path: str, old_content: str, new_content: str) -> Union[bool, str] # Returns True or error message\nread_file(file_path: str) -> str\ndelete_file(file_path: str) -> bool\ncheck_if_file_exists(file_path: str) -> bool\n# Directory Operations\ncreate_dir(dir_path: str) -> bool\nlist_files() -> str  # Shows tree structure of current working directory\ncheck_if_dir_exists(dir_path: str) -> bool\n# Utilities\nget_size(file_or_dir_path: str) -> int  # Bytes; empty = total memory size\ngo_to_link(link_string: str) -> bool\nIn the scaffold, the model uses <think>, <python> and <reply> tags to structure its response. Using <reply> only when it's done interacting with the memory. The <python> block is executed in a sandbox with the tools and the results of the code block are returned in a <result> tag to the model, forming the agentic loop.\nThe model is also trained to be able to handle optional filters given by the user in between\nBenchmark\nWe evaluated this model and a few other open & closed ones on our benchmark, md-memory-bench. We used o3 from OpenAI as the judge. All the other models except driaforall/mem-agent and Qwen/Qwen3-4B-Thinking-2507 were used through OpenRouter.s\nModel\nRetrieval\nUpdate\nClarification\nFilter\nOverall\nqwen/qwen3-235b-a22b-thinking-2507\n0.9091\n0.6363\n0.4545\n1\n0.7857\ndriaforall/mem-agent\n0.8636\n0.7272\n0.3636\n0.9167\n0.75\nz-ai/glm-4.5\n0.7727\n0.8181\n0.3636\n0.9167\n0.7321\ndeepseek/deepseek-chat-v3.1\n0.6818\n0.5454\n0.5454\n0.8333\n0.6607\ngoogle/gemini-2.5-pro\n0.7273\n0.4545\n0.2727\n1\n0.6429\ngoogle/gemini-2.5-flash\n0.7727\n0.3636\n0.2727\n0.9167\n0.625\nopenai/gpt-5\n0.6818\n0.5454\n0.2727\n0.9167\n0.625\nanthropic/claude-opus-4.1\n0.6818\n0\n0.8181\n0.5833\n0.5536\nQwen/Qwen3-4B-Thinking-2507\n0.4545\n0\n0.2727\n0.75\n0.3929\nmoonshotai/kimi-k2\n0.3181\n0.2727\n0.1818\n0.6667\n0.3571\nOur model, with only 4B parameters, is only second on the benchmark, beating all the open & closed models except for qwen/qwen3-235b-a22b-thinking-2507. The model achieves an overall score of 0.75, a significant improvement over the 0.3929 of the base Qwen model.\nUsage\nThe model, while can be used on its own, is recommended to be used as an MCP server to a bigger model, which can then be used to interact with the memory system. For this, you can check our repo, which contains instructions for both an MCP setup and a cli standalone model usage.\nMemory\nThe model uses a markdown based memory system with links, inspired by Obsidian. The general structure of the memory is:\nmemory/\n‚îú‚îÄ‚îÄ user.md\n‚îî‚îÄ‚îÄ entities/\n‚îî‚îÄ‚îÄ [entity_name_1].md\n‚îî‚îÄ‚îÄ [entity_name_2].md\n‚îî‚îÄ‚îÄ ...\nuser.md is the main file that contains information about the user and their relationships, accompanied by links to the enity file in the format of [[entities/[entity_name].md]] per relationship. The link format should be followed strictly.\nentities/ is the directory that contains the entity files.\nEach entity file follows the same structure as user.md.\nModifying the memory manually does not require restarting the MCP server.\nExample user.md\n# User Information\n- user_name: John Doe\n- birth_date: 1990-01-01\n- birth_location: New York, USA\n- living_location: Enschede, Netherlands\n- zodiac_sign: Aquarius\n## User Relationships\n- company: [[entities/acme_corp.md]]\n- mother: [[entities/jane_doe.md]]\nExample entity files (jane_doe.md and acme_corp.md)\n# Jane Doe\n- relationship: Mother\n- birth_date: 1965-01-01\n- birth_location: New York, USA\n# Acme Corporation\n- industry: Software Development\n- location: Enschede, Netherlands\nThe model is trained on this memory standard and any fruitful use should be on a memory system that follows this standard. We have a few memory export tools for different sources like ChatGPT, Notion, etc. in our mcp server repo.\nReferences:\nGSPO, Zheng et al., 2025",
    "itsomk/vit-xray-v1": "ViT X-ray Multi-label (vit-xray-v1)\nModel Description\nIntended Use\nTraining Data\nModel Performance\nQuick Usage\nViT X-ray Multi-label (vit-xray-v1)\nModel Description\nThis model is a fine-tuned Vision Transformer (google/vit-base-patch16-224-in21k) for multi-label classification of chest X-rays.It predicts the presence of multiple findings such as:\nNodule\nInfiltration\nEffusion\nAtelectasis\nAuthor: Om Kumar (Hugging Face: @itsomk)\nThe model is designed for research and educational purposes only and should not be used as a substitute for clinical diagnosis.\nIntended Use\nResearch in medical imaging and computer vision\nEducational purposes for understanding X-ray image classification\nBaseline model for further fine-tuning or domain adaptation\n‚ö†Ô∏è Not intended for clinical use. Predictions should not guide medical decisions.\nTraining Data\nDataset: Chest X-ray images (publicly available datasets, e.g., NIH ChestX-ray14, etc.)\nImages were preprocessed (resized to 224x224, normalized).\nLabels are multi-label, meaning an X-ray can contain more than one finding.\nModel Performance\nOptimized for detecting common thoracic abnormalities.\nEvaluation metrics: AUC .\nNodule AUC: 0.696\nInfiltration AUC: 0.684\nEffusion AUC: 0.843\nAtelectasis AUC: 0.762\nQuick Usage\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch\nfrom PIL import Image\nMODEL = \"itsomk/vit-xray-v1\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL)\nmodel = AutoModelForImageClassification.from_pretrained(MODEL)\nimg = Image.open(\"path/to/xray.jpg\").convert(\"RGB\")\ninputs = processor(images=img, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\nprobs = torch.sigmoid(logits).squeeze().tolist()\nresults = {model.config.id2label[i]: float(probs[i]) for i in range(len(probs))}\nprint(results)",
    "swiss-ai/Apertus-70B-Instruct-2509": "Apertus\nTable of Contents\nModel Summary\nKey features\nHow to use\nLong context processing\nAgentic Usage\nDeployment\nEvaluation\nTraining\nModel\nSoftware & hardware\nOpen resources\nLimitations\nLegal Aspects\nContact\nCitation\nApertus\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nTraining\nLimitations\nLegal Aspects\nModel Summary\nApertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models.\nThe model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.\nThe model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO.\nKey features\nFully open model: open weights + open data + full training details including all data and training recipes\nMassively Multilingual: 1811 natively supported languages\nCompliant Apertus is trained while respecting opt-out consent of data owners (even retrospectivey), and avoiding memorization of training data\nFor more details refer to our technical report\nHow to use\nThe modeling code for Apertus is available in transformers v4.56.0 and later, so make sure to upgrade your transformers version. You can also load the model with the latest vLLM which uses transformers as a backend.\npip install -U transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"swiss-ai/Apertus-70B-Instruct-2509\"\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\n).to(device)\n# prepare the model input\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages_think = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages_think,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\nWe recommend setting temperature=0.8 and top_p=0.9 in the sampling parameters.\nLong context processing\nApertus by default supports a context length up to 65,536 tokens.\nAgentic Usage\nApertus supports tool use\nDeployment\nDeployment of the models is directly supported by the newest versions of Transformers, vLLM, SGLang, and also for running on-device with MLX,\nEvaluation\nPretraining Evaluation: Performance (%) of Apertus models on general language understanding tasks (higher is better) compared to other pretrained models.\nModel\nAvg\nARC\nHellaSwag\nWinoGrande\nXNLI\nXCOPA\nPIQA\nFully Open Models\nApertus-8B\n65.8\n72.7\n59.8\n70.6\n45.2\n66.5\n79.8\nApertus-70B\n67.5\n70.6\n64.0\n73.3\n45.3\n69.8\n81.9\nOLMo2-7B\n64.0\n72.9\n60.4\n74.5\n40.4\n55.2\n80.9\nOLMo2-32B\n67.7\n76.2\n66.7\n78.6\n42.9\n60.1\n82.1\nEuroLLM-1.7B\n54.8\n57.2\n44.9\n58.1\n40.7\n55.7\n72.4\nEuroLLM-9B\n62.8\n67.9\n57.9\n68.8\n41.5\n61.1\n79.6\nSmolLM2-1.7B\n58.5\n66.1\n52.4\n65.6\n37.6\n52.3\n77.0\nSmolLM3-3B\n61.6\n68.6\n56.4\n68.1\n40.5\n58.2\n77.7\nPoro-34B\n61.7\n65.7\n57.9\n70.6\n41.6\n56.0\n78.5\nOpen-Weight Models\nLlama3.1-8B\n65.4\n71.6\n60.0\n73.4\n45.3\n61.8\n80.1\nLlama3.1-70B\n67.3\n74.4\n56.5\n79.4\n44.3\n66.7\n82.3\nQwen2.5-7B\n64.4\n69.6\n60.1\n72.8\n43.3\n61.7\n78.7\nQwen2.5-72B\n69.8\n76.2\n67.5\n78.0\n46.9\n68.2\n82.0\nQwen3-32B\n67.8\n75.6\n64.0\n73.8\n44.4\n67.9\n80.9\nLlama4-Scout-16x17B\n67.9\n74.7\n66.8\n73.2\n43.5\n67.7\n81.2\nGPT-OSS-20B\n58.1\n67.0\n41.5\n66.5\n37.4\n60.4\n75.6\nMany additional benchmark evaluations, for pretraining and posttraining phases, multilingual evaluations in around hundred languages, and long context evaluations are provided in Section 5 of the Apertus_Tech_Report.pdf\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 15T\nPrecision: bfloat16\nSoftware & hardware\nGPUs: 4096 GH200\nTraining Framework: Megatron-LM\n...\nOpen resources\nAll elements used in the training process are made openly available\nTraining data reconstruction scripts: github.com/swiss-ai/pretrain-data\nThe training intermediate checkpoints are available on the different branches of this same repository\nLimitations\nApertus can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nLegal Aspects\nEU AI Act Transparency Documentation and Code of Practice\nApertus_EU_Public_Summary.pdf\nApertus_EU_Code_of_Practice.pdf\nData Protection and Copyright Requests\nFor removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly\nllm-privacy-requests@swiss-ai.org\nllm-copyright-requests@swiss-ai.org\nOutput Filter for PII\nCurrently no output filter is provided.\nPlease check this site regularly for an output filter that can be used on top of the Apertus LLM. The filter reflects data protection deletion requests which have been addressed to us as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from this site every six months.\nContact\nTo contact us, please send an email to\nllm-requests@swiss-ai.org\nCitation\n@misc{swissai2025apertus,\ntitle={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}},\nauthor={Alejandro Hern√°ndez-Cano and Alexander H√§gele and Allen Hao Huang and Angelika Romanou and Antoni-Joan Solergibert and Barna Pasztor and Bettina Messmer and Dhia Garbaya and Eduard Frank ƒéurech and Ido Hakimi and Juan Garc√≠a Giraldo and Mete Ismayilzada and Negar Foroutan and Skander Moalla and Tiancheng Chen and Vinko Sabolƒçec and Yixuan Xu and Michael Aerni and Badr AlKhamissi and Ines Altemir Marinas and Mohammad Hossein Amani and Matin Ansaripour and Ilia Badanin and Harold Benoit and Emanuela Boros and Nicholas Browning and Fabian B√∂sch and Maximilian B√∂ther and Niklas Canova and Camille Challier and Clement Charmillot and Jonathan Coles and Jan Deriu and Arnout Devos and Lukas Drescher and Daniil Dzenhaliou and Maud Ehrmann and Dongyang Fan and Simin Fan and Silin Gao and Miguel Gila and Mar√≠a Grandury and Diba Hashemi and Alexander Hoyle and Jiaming Jiang and Mark Klein and Andrei Kucharavy and Anastasiia Kucherenko and Frederike L√ºbeck and Roman Machacek and Theofilos Manitaras and Andreas Marfurt and Kyle Matoba and Simon Matrenok and Henrique Mendonc√ßa and Fawzi Roberto Mohamed and Syrielle Montariol and Luca Mouchel and Sven Najem-Meyer and Jingwei Ni and Gennaro Oliva and Matteo Pagliardini and Elia Palme and Andrei Panferov and L√©o Paoletti and Marco Passerini and Ivan Pavlov and Auguste Poiroux and Kaustubh Ponkshe and Nathan Ranchin and Javi Rando and Mathieu Sauser and Jakhongir Saydaliev and Muhammad Ali Sayfiddinov and Marian Schneider and Stefano Schuppli and Marco Scialanga and Andrei Semenov and Kumar Shridhar and Raghav Singhal and Anna Sotnikova and Alexander Sternfeld and Ayush Kumar Tarun and Paul Teiletche and Jannis Vamvas and Xiaozhe Yao and Hao Zhao Alexander Ilic and Ana Klimovic and Andreas Krause and Caglar Gulcehre and David Rosenthal and Elliott Ash and Florian Tram√®r and Joost VandeVondele and Livio Veraldi and Martin Rajman and Thomas Schulthess and Torsten Hoefler and Antoine Bosselut and Martin Jaggi and Imanol Schlag},\nyear={2025},\nhowpublished={\\url{https://arxiv.org/abs/2509.14233}}\n}",
    "swiss-ai/Apertus-70B-2509": "Apertus\nTable of Contents\nModel Summary\nKey features\nHow to use\nLong context processing\nAgentic Usage\nDeployment\nEvaluation\nTraining\nModel\nSoftware & hardware\nOpen resources\nLimitations\nLegal Aspects\nContact\nCitation\nApertus\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nTraining\nLimitations\nLegal Aspects\nModel Summary\nApertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models.\nThe model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.\nThe model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO.\nKey features\nFully open model: open weights + open data + full training details including all data and training recipes\nMassively Multilingual: 1811 natively supported languages\nCompliant Apertus is trained while respecting opt-out consent of data owners (even retrospectivey), and avoiding memorization of training data\nFor more details refer to our technical report\nHow to use\nThe modeling code for Apertus is available in transformers v4.56.0 and later, so make sure to upgrade your transformers version. You can also load the model with the latest vLLM which uses transformers as a backend.\npip install -U transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"swiss-ai/Apertus-70B-2509\"\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\n).to(device)\n# prepare the model input\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages_think = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages_think,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\nWe recommend setting temperature=0.8 and top_p=0.9 in the sampling parameters.\nLong context processing\nApertus by default supports a context length up to 65,536 tokens.\nAgentic Usage\nApertus supports tool use\nDeployment\nDeployment of the models is directly supported by the newest versions of Transformers, vLLM, SGLang, and also for running on-device with MLX,\nEvaluation\nPretraining Evaluation: Performance (%) of Apertus models on general language understanding tasks (higher is better) compared to other pretrained models.\nModel\nAvg\nARC\nHellaSwag\nWinoGrande\nXNLI\nXCOPA\nPIQA\nFully Open Models\nApertus-8B\n65.8\n72.7\n59.8\n70.6\n45.2\n66.5\n79.8\nApertus-70B\n67.5\n70.6\n64.0\n73.3\n45.3\n69.8\n81.9\nOLMo2-7B\n64.0\n72.9\n60.4\n74.5\n40.4\n55.2\n80.9\nOLMo2-32B\n67.7\n76.2\n66.7\n78.6\n42.9\n60.1\n82.1\nEuroLLM-1.7B\n54.8\n57.2\n44.9\n58.1\n40.7\n55.7\n72.4\nEuroLLM-9B\n62.8\n67.9\n57.9\n68.8\n41.5\n61.1\n79.6\nSmolLM2-1.7B\n58.5\n66.1\n52.4\n65.6\n37.6\n52.3\n77.0\nSmolLM3-3B\n61.6\n68.6\n56.4\n68.1\n40.5\n58.2\n77.7\nPoro-34B\n61.7\n65.7\n57.9\n70.6\n41.6\n56.0\n78.5\nOpen-Weight Models\nLlama3.1-8B\n65.4\n71.6\n60.0\n73.4\n45.3\n61.8\n80.1\nLlama3.1-70B\n67.3\n74.4\n56.5\n79.4\n44.3\n66.7\n82.3\nQwen2.5-7B\n64.4\n69.6\n60.1\n72.8\n43.3\n61.7\n78.7\nQwen2.5-72B\n69.8\n76.2\n67.5\n78.0\n46.9\n68.2\n82.0\nQwen3-32B\n67.8\n75.6\n64.0\n73.8\n44.4\n67.9\n80.9\nLlama4-Scout-16x17B\n67.9\n74.7\n66.8\n73.2\n43.5\n67.7\n81.2\nGPT-OSS-20B\n58.1\n67.0\n41.5\n66.5\n37.4\n60.4\n75.6\nMany additional benchmark evaluations, for pretraining and posttraining phases, multilingual evaluations in around hundred languages, and long context evaluations are provided in Section 5 of the Apertus_Tech_Report.pdf\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 15T\nPrecision: bfloat16\nSoftware & hardware\nGPUs: 4096 GH200\nTraining Framework: Megatron-LM\n...\nOpen resources\nAll elements used in the training process are made openly available\nTraining data reconstruction scripts: github.com/swiss-ai/pretrain-data\nThe training intermediate checkpoints are available on the different branches of this same repository\nLimitations\nApertus can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nLegal Aspects\nEU AI Act Transparency Documentation and Code of Practice\nApertus_EU_Public_Summary.pdf\nApertus_EU_Code_of_Practice.pdf\nData Protection and Copyright Requests\nFor removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly\nllm-privacy-requests@swiss-ai.org\nllm-copyright-requests@swiss-ai.org\nOutput Filter for PII\nCurrently no output filter is provided.\nPlease check this site regularly for an output filter that can be used on top of the Apertus LLM. The filter reflects data protection deletion requests which have been addressed to us as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from this site every six months.\nContact\nTo contact us, please send an email to\nllm-requests@swiss-ai.org\nCitation\n@misc{swissai2025apertus,\ntitle={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}},\nauthor={Alejandro Hern√°ndez-Cano and Alexander H√§gele and Allen Hao Huang and Angelika Romanou and Antoni-Joan Solergibert and Barna Pasztor and Bettina Messmer and Dhia Garbaya and Eduard Frank ƒéurech and Ido Hakimi and Juan Garc√≠a Giraldo and Mete Ismayilzada and Negar Foroutan and Skander Moalla and Tiancheng Chen and Vinko Sabolƒçec and Yixuan Xu and Michael Aerni and Badr AlKhamissi and Ines Altemir Marinas and Mohammad Hossein Amani and Matin Ansaripour and Ilia Badanin and Harold Benoit and Emanuela Boros and Nicholas Browning and Fabian B√∂sch and Maximilian B√∂ther and Niklas Canova and Camille Challier and Clement Charmillot and Jonathan Coles and Jan Deriu and Arnout Devos and Lukas Drescher and Daniil Dzenhaliou and Maud Ehrmann and Dongyang Fan and Simin Fan and Silin Gao and Miguel Gila and Mar√≠a Grandury and Diba Hashemi and Alexander Hoyle and Jiaming Jiang and Mark Klein and Andrei Kucharavy and Anastasiia Kucherenko and Frederike L√ºbeck and Roman Machacek and Theofilos Manitaras and Andreas Marfurt and Kyle Matoba and Simon Matrenok and Henrique Mendonc√ßa and Fawzi Roberto Mohamed and Syrielle Montariol and Luca Mouchel and Sven Najem-Meyer and Jingwei Ni and Gennaro Oliva and Matteo Pagliardini and Elia Palme and Andrei Panferov and L√©o Paoletti and Marco Passerini and Ivan Pavlov and Auguste Poiroux and Kaustubh Ponkshe and Nathan Ranchin and Javi Rando and Mathieu Sauser and Jakhongir Saydaliev and Muhammad Ali Sayfiddinov and Marian Schneider and Stefano Schuppli and Marco Scialanga and Andrei Semenov and Kumar Shridhar and Raghav Singhal and Anna Sotnikova and Alexander Sternfeld and Ayush Kumar Tarun and Paul Teiletche and Jannis Vamvas and Xiaozhe Yao and Hao Zhao Alexander Ilic and Ana Klimovic and Andreas Krause and Caglar Gulcehre and David Rosenthal and Elliott Ash and Florian Tram√®r and Joost VandeVondele and Livio Veraldi and Martin Rajman and Thomas Schulthess and Torsten Hoefler and Antoine Bosselut and Martin Jaggi and Imanol Schlag},\nyear={2025},\nhowpublished={\\url{https://arxiv.org/abs/2509.14233}}\n}",
    "openbmb/MiniCPM4.1-8B": "What's New\nHighlights\nEvaluation Results\nPerformance Evaluation\nBest Practices\nEfficiency Evaluation\nExamples\nUsage\nInference with Transformers\nInference with SGLang\nSpeculative Decoding\nStandard Inference (Without Speculative Decoding)\nInference with vLLM\nSpeculative Decoding\nStandard Inference (Without Speculative Decoding)\nInference with CPM.cu\nInference with llama.cpp and Ollama\nHybird Reasoning Mode\nStatement\nLICENSE\nCitation\nGitHub Repo |\nTechnical Report |\nJoin Us\nüëã Contact us in Discord and WeChat\nWhat's New\n[2025.09.29] InfLLM-V2 paper is released! We can train a sparse attention model with only 5B long-text tokens. üî•üî•üî•\n[2025.09.05] MiniCPM4.1 series are released! This series is a hybrid reasoning model with trainable sparse attention, which can be used in both deep reasoning mode and non-reasoning mode. üî•üî•üî•\n[2025.06.06] MiniCPM4 series are released! This model achieves ultimate efficiency improvements while maintaining optimal performance at the same scale! It can achieve over 5x generation acceleration on typical end-side chips! You can find technical report here.üî•üî•üî•\nHighlights\nMiniCPM4.1 is highlighted with following features:\n‚úÖ Strong Reasoning Capability: Surpasses similar-sized models on 15 tasks!\n‚úÖ Fast Generation: 3x decoding speedup for reasoning!\n‚úÖ Efficient Architecture: Trainable sparse attention, frequency-ranked speculative decoding!\nMiniCPM4.1-8B: The latest version of MiniCPM4, with 8B parameters, support fusion thinking. (<-- you are here)\nMiniCPM4.1-8B-GPTQ: MiniCPM4.1-8B in GPTQ format.\nMiniCPM4.1-8B-AutoAWQ: MiniCPM4.1-8B in AutoAWQ format.\nMiniCPM-4.1-8B-Marlin: MiniCPM4.1-8B in Marlin format.\nMiniCPM4.1-8B-GGUF: MiniCPM4.1-8B in GGUF format.\nMiniCPM4.1-8B-MLX: MiniCPM4.1-8B in MLX format.\nMiniCPM4.1-8B-Eagle3: Eagle3 model for MiniCPM4.1-8B.\nMiniCPM4 Series\nClick to expand all MiniCPM4 series models\nMiniCPM4-8B: The flagship model with 8B parameters, trained on 8T tokens\nMiniCPM4-0.5B: Lightweight version with 0.5B parameters, trained on 1T tokens\nMiniCPM4-8B-Eagle-FRSpec: Eagle head for FRSpec, accelerating speculative inference\nMiniCPM4-8B-Eagle-FRSpec-QAT-cpmcu: Eagle head with QAT for FRSpec, integrating speculation and quantization for ultra acceleration\nMiniCPM4-8B-Eagle-vLLM: Eagle head in vLLM format for speculative inference\nMiniCPM4-8B-marlin-Eagle-vLLM: Quantized Eagle head for vLLM format\nBitCPM4-0.5B: Extreme ternary quantization of MiniCPM4-0.5B, achieving 90% bit width reduction\nBitCPM4-1B: Extreme ternary quantization of MiniCPM3-1B, achieving 90% bit width reduction\nMiniCPM4-Survey: Generates trustworthy, long-form survey papers from user queries\nMiniCPM4-MCP: Integrates MCP tools to autonomously satisfy user requirements\nEvaluation Results\nPerformance Evaluation\nMiniCPM4.1 launches end-side versions with 8B parameter scale, both achieving best-in-class performance in their respective categories.\nBest Practices\nIt is advisable  to use temperature=0.9, topp=0.95. And we suggest setting max_output_token to 65,536 tokens.\nFor math problems, we recommend using \"Please reason step by step, and put your final answer within \\boxed{}.\"\nAnd for English multiple-choice questions, we recommend starting with \"Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\" And \"‰Ω†ÂõûÁ≠îÁöÑÊúÄÂêé‰∏ÄË°åÂøÖÈ°ªÊòØ‰ª•‰∏ãÊ†ºÂºè 'Á≠îÊ°àÔºö$ÈÄâÈ°π' (‰∏çÂ∏¶ÂºïÂè∑), ÂÖ∂‰∏≠ÈÄâÈ°πÊòØABCD‰πã‰∏Ä„ÄÇËØ∑Âú®ÂõûÁ≠î‰πãÂâç‰∏ÄÊ≠•Ê≠•ÊÄùËÄÉ\" for Chinese MCQ.\nEfficiency Evaluation\nMiniCPM4.1 adopts sparse attention and speculative decoding to improve the inference efficiency. On RTX 4090, MiniCPM4.1 achieves 3x decoding speed improvement in reasoning.\nExamples\nUsage\nMiniCPM 4.1 can be used with following frameworks: Huggingface Transformers, SGLang, vLLM, and CPM.cu. For the ultimate inference speed, we highly recommend CPM.cu.\nMiniCPM4/MiniCPM4.1 supports both dense attention inference and sparse attention inference modes, where vLLM and SGLang currently only support dense inference mode. If you want to use sparse inference mode, please use Huggingface Transformers and CPM.cu.\nDense attention inference: vLLM, SGLang, Huggingface Transformers\nSparse attention inference: Huggingface Transformers, CPM.cu\nTo facilitate researches in sparse attention, we provide InfLLM-V2 Training Kernels and InfLLM-V2 Inference Kernels.\nInference with Transformers\nMiniCPM4.1-8B requires transformers>=4.56.\nInference with Dense Attention\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\npath = 'openbmb/MiniCPM4.1-8B'\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(path)\nmodel = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)\n# User can directly use the chat interface\n# responds, history = model.chat(tokenizer, \"Write an article about Artificial Intelligence.\", temperature=0.7, top_p=0.7)\n# print(responds)\n# User can also use the generate interface\nmessages = [\n{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"},\n]\nprompt_text = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(device)\nmodel_outputs = model.generate(\n**model_inputs,\nmax_new_tokens=32768,\ntop_p=0.95,\ntemperature=0.6\n)\noutput_token_ids = [\nmodel_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs['input_ids']))\n]\nresponses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\nprint(responses)\nInference with Sparse Attention\nMiniCPM4.1-8B supports InfLLM v2, a sparse attention mechanism designed for efficient long-sequence inference. It requires the infllmv2_cuda_impl library.\nYou can install it by running the following command:\ngit clone -b feature_infer https://github.com/OpenBMB/infllmv2_cuda_impl.git\ncd infllmv2_cuda_impl\ngit submodule update --init --recursive\npip install -e . # or python setup.py install\nTo enable InfLLM v2, you need to add the sparse_config field in config.json:\n{\n...,\n\"sparse_config\": {\n\"kernel_size\": 32,\n\"kernel_stride\": 16,\n\"init_blocks\": 1,\n\"block_size\": 64,\n\"window_size\": 2048,\n\"topk\": 64,\n\"use_nope\": false,\n\"dense_len\": 8192\n}\n}\nThese parameters control the behavior of InfLLM v2:\nkernel_size (default: 32): The size of semantic kernels.\nkernel_stride (default: 16): The stride between adjacent kernels.\ninit_blocks (default: 1): The number of initial blocks that every query token attends to. This ensures attention to the beginning of the sequence.\nblock_size (default: 64): The block size for key-value blocks.\nwindow_size (default: 2048): The size of the local sliding window.\ntopk (default: 64): The specifies that each token computes attention with only the top-k most relevant key-value blocks.\nuse_nope (default: false): Whether to use the NOPE technique in block selection for improved performance.\ndense_len (default: 8192): Since Sparse Attention offers limited benefits for short sequences, the model can use standard (dense) attention for shorter texts. The model will use dense attention for sequences with a token length below dense_len and switch to sparse attention for sequences exceeding this length. Set this to -1 to always use sparse attention regardless of sequence length.\nLong Context Extension\nMiniCPM4.1 natively supports context lengths of up to 65,536(64k) tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques for effective handling of long texts. We have validated the model's performance on context lengths of up to 131,072 tokens by modifying the LongRoPE factor.\nYou can apply the LongRoPE factor modification by modifying the model files. Specifically, in the config.json file, adjust the rope_scaling fields.\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"longrope\",\n\"long_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873],\n\"short_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873],\n\"original_max_position_embeddings\": 65536\n}\n}\nInference with SGLang\nYou can inference with SGLang using the standard mode and speculative decoding mode.\nSpeculative Decoding\nFor accelerated inference with speculative decoding, follow these steps:\n1. Download MiniCPM4.1 Draft Model\nFirst, download the MiniCPM4.1 draft model:\ncd /your_path\ngit clone https://huggingface.co/openbmb/MiniCPM4.1-8B-Eagle3\n2. Install EAGLE3-Compatible SGLang\nThe EAGLE3 adaptation PR has been submitted. For now, use our repository for installation:\ngit clone https://github.com/LDLINGLINGLING/sglang.git\ncd sglang\npip install -e \"python[all]\"\n3. Launch SGLang Server with Speculative Decoding\nStart the SGLang server with speculative decoding enabled:\npython -m sglang.launch_server \\\n--model-path \"openbmb/MiniCPM4.1-8B\" \\\n--host \"127.0.0.1\" \\\n--port 30002 \\\n--mem-fraction-static 0.9 \\\n--speculative-algorithm EAGLE3 \\\n--speculative-draft-model-path \"your/path/MiniCPM4_1-8B-Eagle3-bf16\" \\\n--speculative-num-steps 3 \\\n--speculative-eagle-topk 1 \\\n--speculative-num-draft-tokens 32 \\\n--temperature 0.7\n4. Client Usage\nThe client usage remains the same for both standard and speculative decoding:\nimport openai\nclient = openai.Client(base_url=f\"http://localhost:30002/v1\", api_key=\"None\")\nresponse = client.chat.completions.create(\nmodel=\"openbmb/MiniCPM4.1-8B\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"},\n],\ntemperature=0.6,\nmax_tokens=32768,\n)\nprint(response.choices[0].message.content)\nNote: Make sure to update the port number in the client code to match the server port (30002 in the speculative decoding example).\nConfiguration Parameters\n--speculative-algorithm EAGLE3: Enables EAGLE3 speculative decoding\n--speculative-draft-model-path: Path to the draft model for speculation\n--speculative-num-steps: Number of speculative steps (default: 3)\n--speculative-eagle-topk: Top-k parameter for EAGLE (default: 1)\n--speculative-num-draft-tokens: Number of draft tokens (default: 32)\n--mem-fraction-static: Memory fraction for static allocation (default: 0.9)\nStandard Inference (Without Speculative Decoding)\nFor now, you need to install our forked version of SGLang.\ngit clone -b openbmb https://github.com/OpenBMB/sglang.git\ncd sglang\npip install --upgrade pip\npip install -e \"python[all]\"\nYou can start the inference server by running the following command:\npython -m sglang.launch_server --model openbmb/MiniCPM4.1-8B --trust-remote-code --port 30000 --chat-template chatml\nThen you can use the chat interface by running the following command:\nimport openai\nclient = openai.Client(base_url=f\"http://localhost:30000/v1\", api_key=\"None\")\nresponse = client.chat.completions.create(\nmodel=\"openbmb/MiniCPM4.1-8B\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"},\n],\ntemperature=0.6,\nmax_tokens=32768,\n)\nprint(response.choices[0].message.content)\nInference with vLLM\nYou can inference with vLLM using the standard mode and speculative decoding mode.\nSpeculative Decoding\nFor accelerated inference with speculative decoding using vLLM, follow these steps:\n1. Download MiniCPM4.1 Draft Model\nFirst, download the MiniCPM4.1 draft model and change the architectures in config.json as LlamaForCausalLM.\ncd /your_path\ngit clone https://huggingface.co/openbmb/MiniCPM4.1-8B-Eagle3\n2. Install EAGLE3-Compatible vLLM\nThe EAGLE3 vLLM PR has been submitted. For now, use our repository for installation:\ngit clone https://github.com/LDLINGLINGLING/vllm.git\ncd vllm\npip install -e .\n3. Launch vLLM Server with Speculative Decoding\nStart the vLLM inference server with speculative decoding enabled. Make sure to update the model path in the speculative-config to point to your downloaded MiniCPM4_1-8B-Eagle3-bf16 folder:\nVLLM_USE_V1=1 \\\nvllm serve openbmb/MiniCPM4.1-8B \\\n--seed 42 \\\n--trust-remote-code \\\n--speculative-config '{\n\"model\": \"your/path/MiniCPM4_1-8B-Eagle3-bf16\",\n\"num_speculative_tokens\": 3,\n\"method\": \"eagle3\",\n\"draft_tensor_parallel_size\": 1\n}'\n4. Client Usage Example\nThe client usage remains the same for both standard and speculative decoding:\nimport openai\nclient = openai.Client(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\nresponse = client.chat.completions.create(\nmodel=\"openbmb/MiniCPM4.1-8B\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"},\n],\ntemperature=0.6,\nmax_tokens=32768,\nextra_body=dict(add_special_tokens=True),  # Ensures special tokens are added for chat template\n)\nprint(response.choices[0].message.content)\nvLLM Configuration Parameters\nVLLM_USE_V1=1: Enables vLLM v1 API\n--speculative-config: JSON configuration for speculative decoding\nmodel: Path to the draft model for speculation\nnum_speculative_tokens: Number of speculative tokens (default: 3)\nmethod: Speculative decoding method (eagle3)\ndraft_tensor_parallel_size: Tensor parallel size for draft model (default: 1)\n--seed: Random seed for reproducibility\n--trust-remote-code: Allow execution of remote code for custom models\nStandard Inference (Without Speculative Decoding)\nFor now, you need to install the latest version of vLLM.\npip install -U vllm \\\n--pre \\\n--extra-index-url https://wheels.vllm.ai/nightly\nThen you can inference MiniCPM4.1-8B with vLLM:\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmodel_name = \"openbmb/MiniCPM4.1-8B\"\nprompt = [{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}]\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninput_text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\nllm = LLM(\nmodel=model_name,\ntrust_remote_code=True,\nmax_num_batched_tokens=65536,\ndtype=\"bfloat16\",\ngpu_memory_utilization=0.8,\n)\nsampling_params = SamplingParams(top_p=0.95, temperature=0.6, max_tokens=32768)\noutputs = llm.generate(prompts=input_text, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\nAlso, you can start the inference server by running the following command:\nNote: In vLLM's chat API, add_special_tokens is False by default. This means important special tokens‚Äîsuch as the beginning-of-sequence (BOS) token‚Äîwill not be added automatically. To ensure the input prompt is correctly formatted for the model, you should explicitly set extra_body={\"add_special_tokens\": True}.\nvllm serve openbmb/MiniCPM4.1-8B --trust-remote-code\nThen you can use the chat interface by running the following code:\nimport openai\nclient = openai.Client(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\nresponse = client.chat.completions.create(\nmodel=\"openbmb/MiniCPM4.1-8B\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"},\n],\ntemperature=0.6,\nmax_tokens=32768,\nextra_body=dict(add_special_tokens=True),  # Ensures special tokens are added for chat template\n)\nprint(response.choices[0].message.content)\nInference with CPM.cu\nWe recommend using CPM.cu for the inference of MiniCPM4 and MiniCPM4.1. CPM.cu is a CUDA inference framework developed by OpenBMB, which integrates efficient sparse, speculative sampling, and quantization techniques, fully leveraging the efficiency advantages of MiniCPM4 and MiniCPM4.1.\nYou can install CPM.cu by running the following command:\ngit clone https://github.com/OpenBMB/cpm.cu.git --recursive\ncd cpm.cu\npython3 setup.py install\nMiniCPM4.1 natively supports context lengths of up to 65,536(64k) tokens. To reproduce the long-text acceleration effect in the paper, we recommend using the LongRoPE factors that have been validated. Change the rope_scaling field in the config.json file as the following to enable LongRoPE.\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"longrope\",\n\"long_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873],\n\"short_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873],\n\"original_max_position_embeddings\": 65536\n}\n}\nAfter modification, you can run the following command to reproduce the long-context acceleration effect (the script will automatically download the model weights from HuggingFace)\npython3 tests/test_generate.py\nYou can run the following command to infer with EAGLE3 speculative decoding algorithm.\npython3 -m cpmcu.cli \\\n--model-path $BASE_MODEL_PATH \\\n--draft-model-path $EAGLE3_DRAFT_MODEL_PATH \\\n--prompt-text \"Write an article about Artificial Intelligence.\" \\\n--use-eagle3 true\nFor more details about CPM.cu, please refer to the repo CPM.cu.\nInference with llama.cpp and Ollama\nWe also support inference with llama.cpp and Ollama.\nllama.cpp\nYou can download the GGUF format of MiniCPM4.1-8B model from huggingface and run it with llama.cpp for efficient CPU or GPU inference.\n# case 1: main-cli\n./build/bin/llama-cli -m MiniCPM4.1-8B-Q4_K_M.gguf -p \"Write an article about Artificial Intelligence.\" -n 1500\n# case 2: server\n## launch server\n./build/bin/llama-server -m MiniCPM4.1-8B-Q4_K_M.gguf --host 127.0.0.1 --port 8080 -c 4096 -fa on &\n## send request\ncurl -X POST http://127.0.0.1:8080/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"gpt-3.5-turbo\",\n\"messages\": [{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}],\n\"max_tokens\": 1500\n}'\nOllama\nPlease refer to model hub for model download. After installing ollama package, you can use MiniCPM4.1 with following commands:\nollama run openbmb/minicpm4.1\nHybird Reasoning Mode\nMiniCPM4.1 supports hybrid reasoning mode, which can be used in both deep reasoning mode and non-reasoning mode. To enable hybrid reasoning mode. User can set enable_thinking=True in tokenizer.apply_chat_template to enable hybrid reasoning mode, and set enable_thinking=False to enable non-reasoning mode. Similarly, user can directly add /no_think at the end of the query to enable non-reasoning mode. If not add any special token or add /think at the end of the query, the model will enable reasoning mode.\n# Enable reasoning mode\nprompt_text = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True\n)\n# Enable non-reasoning mode\nprompt_text = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False\n)\nStatement\nAs a language model, MiniCPM generates content by learning from a vast amount of text.\nHowever, it does not possess the ability to comprehend or express personal opinions or value judgments.\nAny content generated by MiniCPM does not represent the viewpoints or positions of the model developers.\nTherefore, when using content generated by MiniCPM, users should take full responsibility for evaluating and verifying it on their own.\nLICENSE\nThis repository and MiniCPM models are released under the Apache-2.0 License.\nCitation\nPlease cite our paper if you find our work valuable.\n@article{minicpm4,\ntitle={Minicpm4: Ultra-efficient llms on end devices},\nauthor={MiniCPM, Team},\njournal={arXiv preprint arXiv:2506.07900},\nyear={2025}\n}",
    "ostris/qwen_image_edit_inpainting": "Qwen Image Edit Inpainting\nModel description\nVideo\nDownload model\nQwen Image Edit Inpainting\nPrompt\na man wearing a rainbow colored beanie\nPrompt\na cat wearing sunglasses\nPrompt\na man with a big white beard\nPrompt\npainting of a woman wearing sunglasses\nPrompt\npainting of a man with a yellow lightsaber\nPrompt\npainting of a woman with a beard and horns\nModel description\nA \"green screen\" inpainting LoRA for Qwen Image Edit. Just paint whatever you want inpainted green, prompt with the instruction, and it will fill in the green area with whatever you prompt.\nThe green color needs to be pure green. (0, 255, 0).\nVideo\nI made a video breakdown of this LoRA and examples of how to use it on YouTube\nDownload model\nDownload them in the Files & versions tab.",
    "LiquidAI/LFM2-1.2B-RAG": "LFM2-1.2B-RAG\nüìÑ Model details\nüèÉ How to run\nüì¨ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-1.2B-RAG\nBased on LFM2-1.2B, LFM2-1.2B-RAG is specialized in answering questions based on provided contextual documents, for use in RAG (Retrieval-Augmented Generation) systems.\nUse cases:\nChatbot to ask questions about the documentation of a particular product.\nCustom support with an internal knowledge base to provide grounded answers.\nAcademic research assistant with multi-turn conversations about research papers and course materials.\nYou can find more information about other task-specific models in this blog post.\nüìÑ Model details\nGeneration parameters: We recommend using greedy decoding with a temperature=0.\nSystem prompt: The system prompt is optional. You can force the output's language, for example, using \"Always respond in English, regardless of the user's input language.\" By default, the output's language follows the user prompt's language.\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, Portuguese, and Spanish.\nTraining approach: We fine-tuned the LFM2-1.2B-RAG model on a dataset that includes 1M+ samples of multi-turn interactions and multi-document samples consisting of a mix of curated open source documents as well as generated synthetic ones.\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>user\nUse the following context to answer questions:\nBeach soccer differs significantly from its grass-rooted counterpart. [...]<|im_end|>\n<|im_start|>assistant\nEach team in a beach soccer match consists of five players, including a goalkeeper.{<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\n‚ö†Ô∏è The model supports both single-turn and multi-turn conversations.\nRAG systems enable AI solutions to include new, up-to-date, and potentially proprietary information in LLM responses that was not present in the training data. When a user asks a question, the retrieval component locates and delivers related documents from a knowledge base, and then the RAG generator model answers the question based on facts from those contextual documents.\nüèÉ How to run\nHugging Face: LFM2-1.2B\nllama.cpp: LFM2-1.2B-Extract-GGUF\nLEAP: LEAP model library\nYou can use the following Colab notebooks for easy inference and fine-tuning:\nNotebook\nDescription\nLink\nInference\nRun the model with Hugging Face's transformers library.\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nSFT (Axolotl)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Axolotl.\nSFT (Unsloth)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Unsloth.\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "aoi-ot/VibeVoice-1.5B": "VibeVoice: A Frontier Open-Source Text-to-Speech Model\nTraining Details\nModels\nInstallation and Usage\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nContact\nVibeVoice: A Frontier Open-Source Text-to-Speech Model\nVibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\nThe model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.\n‚û°Ô∏è Technical Report: VibeVoice Technical Report\n‚û°Ô∏è Project Page: microsoft/VibeVoice\n‚û°Ô∏è Code: microsoft/VibeVoice-Code\nTraining Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head.\nLLM: Qwen2.5-1.5B for this release.\nTokenizers:\nAcoustic Tokenizer: Based on a œÉ-VAE variant (proposed in LatentLM), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each.\nSemantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task.\nDiffusion Head: Lightweight module (4 layers, ~123M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\nContext Length: Trained with a curriculum increasing up to 65,536 tokens.\nTraining Stages:\nTokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately.\nVibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K -> 64K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers.\nModels\nModel\nContext Length\nGeneration Length\nWeight\nVibeVoice-0.5B-Streaming\n-\n-\nOn the way\nVibeVoice-1.5B\n64K\n~90 min\nYou are here.\nVibeVoice-Large\n32K\n~45 min\nHF link\nInstallation and Usage\nPlease refer to GitHub README\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent ‚Äì cloning a real individual‚Äôs voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass.\nDisinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events.\nReal‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference ‚Äúlive deep‚Äëfake‚Äù applications.\nUnsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have:\nEmbedded an audible disclaimer (e.g. ‚ÄúThis segment was generated by AI‚Äù) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nLogged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.\nUsers are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nContact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently,‚ÄØwe will‚ÄØupdate this repository with appropriate mitigations.",
    "gguf-org/vibevoice-gguf": "vibevoice-gguf\nreference\nvibevoice-gguf\nrun it with gguf-connector; simply execute the command below in console/terminal\nggc v6\nGGUF file(s) available. Select which one to use:\nvibevoice-1.5b-iq4_nl.gguf\nvibevoice-1.5b-q4_0.gguf\nvibevoice-1.5b-q8_0.gguf\nEnter your choice (1 to 3): _\nopt a gguf file in your current directory to interact with; nothing else\nnote: for the latest update, you should be able to adjust the speech pacing (see picture above)\nPrompt*\nAudio Sample\nSpeaker 1: Hey, why you folks always act together like a wolf pack?Speaker 2: Oh, really? We just hang out for good food and share the bills.Speaker 1: Wow. Amazing. A pig pack then!Speaker 2: You must be the smartest joke maker in this universe.\nüéß audio-output-demo\n*for this demo prompt, drag audio1.wav and audio2.wav inside the upload voice samples, then it will be taken as voice reference for speakers 1 and 2 (voice cloning)\nreference\nbase model from microsoft\ngguf-connector (pypi)",
    "phazei/HunyuanVideo-Foley": "HunyuanVideo-Foley FP8 Quantized\nQuantization Details\nUsage\nComfyUI (Recommended)\nOther Frameworks\nFiles\nPerformance Notes\nOriginal Model\nTechnical Details\nHunyuanVideo-Foley FP8 Quantized\nThis is an FP8 quantized version of tencent/HunyuanVideo-Foley optimized for reduced VRAM usage while maintaining audio generation quality.\nQuantization Details\nQuantization Method: FP8 E5M2 & E4M3FN weight-only quantization\nLayers Quantized: Transformer block weights only (attention and FFN layers)\nPreserved Precision: Normalization layers, embeddings, and biases remain in original precision\nExpected VRAM Savings: ~30-40% reduction compared to BF16 original\nMemory Usage: Enables running on <12GB GPUs when combined with other optimizations\nUsage\nComfyUI (Recommended)\nThis model is specifically optimized for use with the ComfyUI-HunyuanVideo-Foley custom node, which provides:\nVRAM-friendly loading with ping-pong memory management\nBuilt-in FP8 support that automatically handles the quantized weights\nTorch compile integration for ~30% speed improvements after first run\nText-to-Audio and Video-to-Audio modes\nBatch generation with audio selection tools\nInstallation:\nInstall the ComfyUI node: ComfyUI-HunyuanVideo-Foley\nDownload this quantized model to ComfyUI/models/foley/\nEnjoy <8GB VRAM usage with high-quality audio generation\nTypical VRAM Usage (5s audio, 50 steps):\nBaseline (BF16): ~10-12 GB\nWith FP8 quantization: ~8-10 GB\nPerfect for RTX 3080/4070 Ti and similar GPUs\nOther Frameworks\nThe FP8 weights can be used with any framework that supports automatic upcasting of FP8 to FP16/BF16 during computation. The quantized weights maintain compatibility with the original model architecture.\nFiles\nhunyuanvideo_foley_fp8_e4m3fn.safetensors - Main model weights in FP8 format\nPerformance Notes\nQuality: Maintains comparable audio generation quality to the original model\nSpeed: Conversion overhead is minimal; actual generation speed depends on compute precision\nMemory: Significant VRAM reduction makes the model accessible on consumer GPUs\nCompatibility: Drop-in replacement for the original model weights\nOriginal Model\nThis quantization is based on tencent/HunyuanVideo-Foley. Please refer to the original repository for:\nModel architecture details\nTraining information\nLicense terms\nCitation information\nTechnical Details\nThe quantization uses a conservative approach that only converts transformer block weights while preserving precision-sensitive components:\n‚úÖ Converted: Attention and FFN layer weights in transformer blocks\n‚ùå Preserved: Normalization layers, embeddings, projections, bias terms\nThis selective quantization strategy maintains model quality while maximizing memory savings.",
    "bytedance-research/UMO": "üìñ Introduction\n‚ö°Ô∏è Quick Start\nüîß Requirements and Installation\nUMO requirements based on UNO\nUMO requirements based on OmniGen2\nUMO checkpoints download\nüåü Gradio Demo\n‚úçÔ∏è Inference\nUMO (based on UNO) inference on XVerseBench\nUMO (based on UNO) inference on OmniContext\nUMO (based on OmniGen2) inference on XVerseBench\nUMO (based on OmniGen2) inference on OmniContext\nüìå Tips and Notes\nüìÑ Disclaimer\nCitation\nUMO: Scaling Multi-Identity Consistency for Image Customization\nvia Matching Reward\nüìñ Introduction\nRecent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving.\n‚ö°Ô∏è Quick Start\nüîß Requirements and Installation\n# 1. Clone the repo with submodules: UNO & OmniGen2\ngit clone --recurse-submodules git@github.com:bytedance/UMO.git\ncd UMO\nUMO requirements based on UNO\n# 2.1 (Optional, but recommended) Create a clean virtual Python 3.11 environment\npython3 -m venv venv/UMO_UNO\nsource venv/UMO_UNO/bin/activate\n# 3.1 Install submodules UNO requirements as:\n# https://github.com/bytedance/UNO?tab=readme-ov-file#-requirements-and-installation\n# 4.1 Install UMO requirements\npip install -r requirements.txt\nUMO requirements based on OmniGen2\n# 2.2 (Optional, but recommended) Create a clean virtual Python 3.11 environment\npython3 -m venv venv/UMO_OmniGen2\nsource venv/UMO_OmniGen2/bin/activate\n# 3.2 Install submodules OmniGen2 requirements as:\n# https://github.com/VectorSpaceLab/OmniGen2?tab=readme-ov-file#%EF%B8%8F-environment-setup\n# 4.2 Install UMO requirements\npip install -r requirements.txt\nUMO checkpoints download\n# pip install huggingface_hub hf-transfer\nexport HF_HUB_ENABLE_HF_TRANSFER=1 # use hf_transfer to speedup\n# export HF_ENDPOINT=https://hf-mirror.com # use mirror to speedup if necessary\nrepo_name=\"bytedance-research/UMO\"\nlocal_dir=\"models/\"$repo_name\nhuggingface-cli download --resume-download $repo_name --local-dir $local_dir\nüåü Gradio Demo\n# UMO (based on UNO)\npython3 demo/UNO/app.py --lora_path models/bytedance-research/UMO/UMO_UNO.safetensors\n# UMO (based on OmniGen2)\npython3 demo/OmniGen2/app.py --lora_path models/bytedance-research/UMO/UMO_OmniGen2.safetensors\n‚úçÔ∏è Inference\nUMO (based on UNO) inference on XVerseBench\n# single subject\naccelerate launch eval/UNO/inference_xversebench.py \\\n--eval_json_path projects/XVerse/eval/tools/XVerseBench_single.json \\\n--num_images_per_prompt 4 \\\n--width 768 \\\n--height 768 \\\n--save_path output/XVerseBench/single/UMO_UNO \\\n--lora_path models/bytedance-research/UMO/UMO_UNO.safetensors\n# multi subject\naccelerate launch eval/UNO/inference_xversebench.py \\\n--eval_json_path projects/XVerse/eval/tools/XVerseBench_multi.json \\\n--num_images_per_prompt 4 \\\n--width 768 \\\n--height 768 \\\n--save_path output/XVerseBench/multi/UMO_UNO \\\n--lora_path models/bytedance-research/UMO/UMO_UNO.safetensors\nUMO (based on UNO) inference on OmniContext\naccelerate launch eval/UNO/inference_omnicontext.py \\\n--eval_json_path OmniGen2/OmniContext \\\n--width 768 \\\n--height 768 \\\n--save_path output/OmniContext/UMO_UNO \\\n--lora_path models/bytedance-research/UMO/UMO_UNO.safetensors\nUMO (based on OmniGen2) inference on XVerseBench\n# single subject\naccelerate launch -m eval.OmniGen2.inference_xversebench \\\n--model_path OmniGen2/OmniGen2 \\\n--model_name UMO_OmniGen2 \\\n--test_data projects/XVerse/eval/tools/XVerseBench_single.json \\\n--result_dir output/XVerseBench/single \\\n--num_images_per_prompt 4 \\\n--disable_align_res \\\n--lora_path models/bytedance-research/UMO/UMO_OmniGen2.safetensors\n# multi subject\naccelerate launch -m eval.OmniGen2.inference_xversebench \\\n--model_path OmniGen2/OmniGen2 \\\n--model_name UMO_OmniGen2 \\\n--test_data projects/XVerse/eval/tools/XVerseBench_multi.json \\\n--result_dir output/XVerseBench/multi \\\n--num_images_per_prompt 4 \\\n--disable_align_res \\\n--lora_path models/bytedance-research/UMO/UMO_OmniGen2.safetensors\nUMO (based on OmniGen2) inference on OmniContext\naccelerate launch -m eval.OmniGen2.inference_omnicontext \\\n--model_path OmniGen2/OmniGen2 \\\n--model_name UMO_OmniGen2 \\\n--test_data OmniGen2/OmniContext \\\n--result_dir output/OmniContext \\\n--num_images_per_prompt 1 \\\n--disable_align_res \\\n--lora_path models/bytedance-research/UMO/UMO_OmniGen2.safetensors\nüìå Tips and Notes\nPlease note that UNO gets unstable results on parts of OmniContext due to the different prompt format with its training data (UNO-1M), leading to similar issue with UMO based on it. To get better results with these two models, we recommend using description prompt instead of instruction one, using resolution 768~1024 instead of 512.\nüìÑ Disclaimer\nWe open-source this project for academic research. The vast majority of images\nused in this project are either generated or licensed. If you have any concerns,\nplease contact us, and we will promptly remove any inappropriate content.\nOur code is released under the Apache 2.0 License.\nThis research aims to advance the field of generative AI. Users are free to\ncreate images using this tool, provided they comply with local laws and exercise\nresponsible usage. The developers are not liable for any misuse of the tool by users.\nCitation\nIf UMO is helpful, please help to ‚≠ê the repo.\nIf you find this project useful for your research, please consider citing our paper:\n@article{cheng2025umo,\ntitle={UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward},\nauthor={Cheng, Yufeng and Wu, Wenxu and Wu, Shaojin and Huang, Mengqi and Ding, Fei and He, Qian},\njournal={arXiv preprint arXiv:2509.06818},\nyear={2025}\n}",
    "lrzjason/QwenEdit_Consistance_Edit": "This lora and workflow is to improve qwen edit consistance issue.\nWhen using qwenvl encode image, it usually facing a random movement of image structure.\nTo prevent this issue, we use kontext like workflow to only set reference image but not encode image.\nPlus using consistance lora to achieve high fidelity image editing.\nComparison:\nIf your task requires more consistance, use higher lora strength.\nIf original model able to output modified image but plus lora is unable to achieve result, try to lower the lora strength.",
    "calcuis/hunyuanimage-gguf": "hunyuanimage-gguf\nreference\nhunyuanimage-gguf\ndrag hunyuanimage2.1 (opt anyone you like) to > ./ComfyUI/models/diffusion_models\ndrag byt5-sm [127MB] and qwen2.5-vl-7b [5.03GB] to > ./ComfyUI/models/text_encoders\ndrag pig [811MB] to > ./ComfyUI/models/vae\nPrompt\ncute anime girl with massive fennec ears and a big fluffy fox tail with long wavy blonde hair between eyes and large blue eyes blonde colored eyelashes chubby wearing oversized clothes summer uniform large black coat long blue maxi skirt muddy clothes happy sitting on the side of the road in a run down dark gritty cyberpunk city with neon and a crumbling skyscraper in the rain at night while dipping her feet in a river of water she is holding a sign that says \"ComfyUI is the best\" and another one that says \"The Future is PIG\"\nNegative Prompt\nlow quality, bad anatomy, extra digits, missing digits, extra limbs, missing limbs\nPrompt\ncute anime girl with massive fennec ears and a big fluffy fox tail with long wavy blonde hair between eyes and large blue eyes blonde colored eyelashes chubby wearing oversized clothes summer uniform large black coat long blue maxi skirt muddy clothes happy sitting on the side of the road in a run down dark gritty cyberpunk city with neon and a crumbling skyscraper in the rain at night while dipping her feet in a river of water she is holding a sign that says \"PIG is the best\" and another one that says \"The Future is gguf\"\nNegative Prompt\nlow quality, bad anatomy, extra digits, missing digits, extra limbs, missing limbs\nPrompt\ncute anime girl with massive fennec ears and a big fluffy fox tail with long wavy blonde hair between eyes and large blue eyes blonde colored eyelashes chubby wearing oversized clothes summer uniform large black coat long blue maxi skirt muddy clothes happy sitting on the side of the road in a run down dark gritty cyberpunk city with neon and a crumbling skyscraper in the rain at night while dipping her feet in a river of water she is holding a sign that says \"Comfy is the best\" and another one that says \"The Future is PIG\"\nNegative Prompt\nlow quality, bad anatomy, extra digits, missing digits, extra limbs, missing limbs\nfor standard model, all files should work - running them with gguf node via comfyui; and v2 is more lightweighted; both of them should be able to generate quality output with 12-15 steps\nnote: for refiner model, please use v2; initial test for refining blur image (runnable test only); could load any picture, i.e., output from q2, blur, distorted, poor in quality, etc., to refine/sharpen it\nnote: for distilled model, please use v2; able to generate output with merely 8 steps (see picture)\nfor lite model, run it with 8 steps + 1 cfg; output is identical to standard model; but 2-3x faster\nthe new lite v2.2, output should be 80-90% closed to the standard model; save up to 60-70% loading time, depends on how you config the steps and cfg (demo above: steps=10; cfg=1.5)\nget scaled fp8 safetensors encoder here if your gpu doesn't release vram after several running attempts; it might vary among different cards/drivers, do your own research always\nreference\ngguf-node (pypi|repo|pack)",
    "nunchaku-tech/nunchaku-qwen-image-edit": "Model Card for nunchaku-qwen-image-edit\nModel Details\nModel Description\nModel Files\nModel Sources\nUsage\nPerformance\nCitation\nModel Card for nunchaku-qwen-image-edit\nThis repository contains Nunchaku-quantized versions of Qwen-Image-Edit, an image-editing model based on Qwen-Image, advances in complex text rendering. It is optimized for efficient inference while maintaining minimal loss in performance.\nNo recent news. Stay tuned for updates!\nModel Details\nModel Description\nDeveloped by: Nunchaku Team\nModel type: image-to-image\nLicense: apache-2.0\nQuantized from model: Qwen-Image-Edit\nModel Files\nsvdq-int4_r32-qwen-image-edit.safetensors: SVDQuant INT4 (rank 32) Qwen-Image-Edit model. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r128-qwen-image-edit.safetensors: SVDQuant INT4 (rank 128) Qwen-Image-Edit model. For users with non-Blackwell GPUs (pre-50-series). It offers better quality than the rank 32 model, but it is slower.\nsvdq-int4_r32-qwen-image-edit-lightningv1.0-4steps.safetensors: SVDQuant INT4 (rank 32) 4-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r128-qwen-image-edit-lightningv1.0-4steps.safetensors: SVDQuant INT4 (rank 128) 4-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r32-qwen-image-edit-lightningv1.0-8steps.safetensors: SVDQuant INT4 (rank 32) 8-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-int4_r128-qwen-image-edit-lightningv1.0-8steps.safetensors: SVDQuant INT4 (rank 128) 8-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-qwen-image-edit.safetensors: SVDQuant NVFP4 (rank 32) Qwen-Image-Edit model. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r128-qwen-image-edit.safetensors: SVDQuant NVFP4 (rank 128) Qwen-Image-Edit model. For users with Blackwell GPUs (50-series). It offers better quality than the rank 32 model, but it is slower.\nsvdq-fp4_r32-qwen-image-edit-lightningv1.0-4steps.safetensors: SVDQuant NVFP4 (rank 32) 4-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r128-qwen-image-edit-lightningv1.0-4steps.safetensors: SVDQuant NVFP4 (rank 128) 4-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r32-qwen-image-edit-lightningv1.0-8steps.safetensors: SVDQuant NVFP4 (rank 32) 8-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nsvdq-fp4_r128-qwen-image-edit-lightningv1.0-8steps.safetensors: SVDQuant NVFP4 (rank 128) 8-step Qwen-Image-Edit model by fusing Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors using LoRA strength = 1.0. For users with Blackwell GPUs (50-series).\nModel Sources\nInference Engine: nunchaku\nQuantization Library: deepcompressor\nPaper: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models\nDemo: svdquant.mit.edu\nUsage\nDiffusers Usage: See qwen-image-edit.py and qwen-image-edit-lightning.py. Check this tutorial for more advanced usage.\nComfyUI Usage: See nunchaku-qwen-image-edit.json.\nPerformance\nCitation\n@inproceedings{\nli2024svdquant,\ntitle={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},\nauthor={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},\nbooktitle={The Thirteenth International Conference on Learning Representations},\nyear={2025}\n}",
    "lakhera2023/devops-slm-v1": "DevOps Specialized Language Model v1\nüéØ Model Overview\n‚ö†Ô∏è Important Notice\nüöÄ Capabilities\nüö´ Limitations\nüíª Usage\nBasic Usage\nExample Queries\nüîß Technical Details\nüìä Performance\nü§ù Contributing\nüìÑ License\n‚ö†Ô∏è Disclaimer\nDevOps Specialized Language Model v1\nüéØ Model Overview\nThis is a specialized language model specifically for DevOps tasks and operations only.\n‚ö†Ô∏è Important Notice\nThis model is designed EXCLUSIVELY for DevOps-related tasks. It has robust filtering that will NOT respond to general questions about movies, weather, cooking, sports, music, travel, health, education, politics, religion, philosophy, entertainment, gaming, fashion, or any non-DevOps topics.\nThe model will automatically redirect any non-DevOps questions back to DevOps topics.\nüöÄ Capabilities\nThe model specializes in:\nKubernetes Operations: Deployments, pods, services, troubleshooting\nDocker Containerization: Container management, Dockerfiles, best practices\nCI/CD Pipelines: GitHub Actions, GitLab CI, Jenkins, automation\nInfrastructure Automation: Terraform, Ansible, infrastructure as code\nMonitoring & Observability: Logging, metrics, alerting, performance monitoring\nCloud Platform Operations: AWS, Azure, GCP, cloud-native solutions\nDevOps Tools: Helm, Prometheus, Grafana, ELK stack, and more\nüö´ Limitations\nDevOps Only: Will not respond to non-DevOps questions\nSpecialized Knowledge: Focused on DevOps practices and tools\nüíª Usage\nBasic Usage\nhttps://colab.research.google.com/drive/1UgTUI6AeVnSlknHoF3cEDhWLHYirghju?usp=sharing\nExample Queries\n‚úÖ Good DevOps Questions:\n\"How do I create a Kubernetes deployment?\"\n\"What's the difference between Docker and Podman?\"\n\"How do I set up a CI/CD pipeline with GitHub Actions?\"\n\"How do I troubleshoot a failing Kubernetes pod?\"\n\"What are the best practices for container security?\"\n‚ùå Non-DevOps Questions (will be redirected):\n\"Tell me about movies\" ‚Üí Redirected to DevOps\n\"What's the weather like?\" ‚Üí Redirected to DevOps\n\"Explain quantum physics\" ‚Üí Redirected to DevOps\n\"Write a poem\" ‚Üí Redirected to DevOps\n\"How to cook pasta?\" ‚Üí Redirected to DevOps\n\"Who won the football game?\" ‚Üí Redirected to DevOps\n\"Recommend some music\" ‚Üí Redirected to DevOps\n\"Best travel destinations\" ‚Üí Redirected to DevOps\nüîß Technical Details\nBase Model: lakhera2023/Qwen-model\nArchitecture: Transformer-based with LoRA fine-tuning\nTraining: DevOps-specific examples\nSpecialization: Fine-tuned for DevOps tasks only\nüìä Performance\nThis model is optimized for:\nDevOps Task Accuracy: High precision on DevOps-related queries\nResponse Quality: Clear, actionable DevOps guidance\nTopic Focus: Strict adherence to DevOps-only responses\nü§ù Contributing\nThis model is specialized for the DevOps community. If you have suggestions for improvements or additional DevOps training data, please open an issue.\nüìÑ License\nThis model is released under the Apache 2.0 License.\n‚ö†Ô∏è Disclaimer\nThis model is designed for educational and professional DevOps use. Always verify any deployment or configuration advice in a safe environment before applying to production systems.\nRemember: This model is DevOps-only. It will not respond to general questions.",
    "silveroxides/Chroma1-HD-GGUF": "Chroma1-HD-GGUF Official Repo for GGUF Quants\nSpecial Thanks\nHow to Use\ndiffusers Library\nModel Details\nIntended Use\nLimitations and Bias Statement\nSummary of Architectural Modifications\nP.S\nCitation\nChroma1-HD-GGUF Official Repo for GGUF Quants\nChroma1-HD is an 8.9B parameter text-to-image foundational model based on FLUX.1-schnell. It is fully Apache 2.0 licensed, ensuring that anyone can use, modify, and build upon it.\nAs a base model, Chroma1 is intentionally designed to be an excellent starting point for finetuning. It provides a strong, neutral foundation for developers, researchers, and artists to create specialized models.\nfor the fast CFG \"baked\" version please go to Chroma1-Flash.\nKey Features\nHigh-Performance Base: 8.9B parameters, built on the powerful FLUX.1 architecture.\nEasily Finetunable: Designed as an ideal checkpoint for creating custom, specialized models.\nCommunity-Driven & Open-Source: Fully transparent with an Apache 2.0 license, and training history.\nFlexible by Design: Provides a flexible foundation for a wide range of generative tasks.\nSpecial Thanks\nA massive thank you to our supporters who make this project possible.\nAnonymous donor whose incredible generosity funded the pretraining run and data collections. Your support has been transformative for open-source AI.\nFictional.ai for their fantastic support and for helping push the boundaries of open-source AI. You can try Chroma on their platform:\nHow to Use\ndiffusers Library\ninstall the requirements\npip install transformers diffusers sentencepiece accelerate\nimport torch\nfrom diffusers import ChromaPipeline\npipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma1-HD\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload()\nprompt = [\n\"A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.\"\n]\nnegative_prompt =  [\"low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors\"]\nimage = pipe(\nprompt=prompt,\nnegative_prompt=negative_prompt,\ngenerator=torch.Generator(\"cpu\").manual_seed(433),\nnum_inference_steps=40,\nguidance_scale=3.0,\nnum_images_per_prompt=1,\n).images[0]\nimage.save(\"chroma.png\")\nQuantized inference using gemlite\nimport torch\nfrom diffusers import ChromaPipeline\npipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma1-HD\", torch_dtype=torch.float16)\n#pipe.enable_model_cpu_offload()\n#######################################################\nimport gemlite\ndevice = 'cuda:0'\nprocessor = gemlite.helper.A8W8_int8_dynamic\n#processor = gemlite.helper.A8W8_fp8_dynamic\n#processor = gemlite.helper.A16W4_MXFP\nfor name, module in pipe.transformer.named_modules():\nmodule.name = name\ndef patch_linearlayers(model, fct):\nfor name, layer in model.named_children():\nif isinstance(layer, torch.nn.Linear):\nsetattr(model, name, fct(layer, name))\nelse:\npatch_linearlayers(layer, fct)\ndef patch_linear_to_gemlite(layer, name):\nlayer = layer.to(device, non_blocking=True)\ntry:\nreturn processor(device=device).from_linear(layer)\nexcept Exception as exception:\nprint('Skipping gemlite conversion for: ' + str(layer.name), exception)\nreturn layer\npatch_linearlayers(pipe.transformer, patch_linear_to_gemlite)\ntorch.cuda.synchronize()\ntorch.cuda.empty_cache()\npipe.to(device)\npipe.transformer.forward = torch.compile(pipe.transformer.forward, fullgraph=True)\npipe.vae.forward = torch.compile(pipe.vae.forward, fullgraph=True)\n#pipe.set_progress_bar_config(disable=True)\n#######################################################\nprompt = [\n\"A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.\"\n]\nnegative_prompt =  [\"low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors\"]\nimport time\nfor _ in range(3):\nt_start = time.time()\nimage = pipe(\nprompt=prompt,\nnegative_prompt=negative_prompt,\ngenerator=torch.Generator(\"cpu\").manual_seed(433),\nnum_inference_steps=40,\nguidance_scale=3.0,\nnum_images_per_prompt=1,\n).images[0]\nt_end = time.time()\nprint(f\"Took: {t_end - t_start} secs.\") #66.1242527961731 -> 27.72 sec\nimage.save(\"chroma.png\")\nComfyUI\nFor advanced users and customized workflows, you can use Chroma with ComfyUI.\nRequirements:\nA working ComfyUI installation.\nChroma checkpoint (latest version).\nT5 XXL Text Encoder.\nFLUX VAE.\nChroma Workflow JSON.\nSetup:\nPlace the T5_xxl model in your ComfyUI/models/clip folder.\nPlace the FLUX VAE in your ComfyUI/models/vae folder.\nPlace the Chroma checkpoint in your ComfyUI/models/diffusion_models folder.\nLoad the Chroma workflow file into ComfyUI and run.\nModel Details\nArchitecture: Based on the 8.9B parameter FLUX.1-schnell model.\nTraining Data: Trained on a 5M sample dataset curated from a 20M pool, including artistic, photographic, and niche styles.\nTechnical Report: A comprehensive technical paper detailing the architectural modifications and training process is forthcoming.\nIntended Use\nChroma is intended to be used as a base model for researchers and developers to build upon. It is ideal for:\nFinetuning on specific styles, concepts, or characters.\nResearch into generative model behavior, alignment, and safety.\nAs a foundational component in larger AI systems.\nLimitations and Bias Statement\nChroma is trained on a broad, filtered dataset from the internet. As such, it may reflect the biases and stereotypes present in its training data. The model is released in a state as is and has not been aligned with a specific safety filter.\nUsers are responsible for their own use of this model. It has the potential to generate content that may be considered harmful, explicit, or offensive. I encourage developers to implement appropriate safeguards and ethical considerations in their downstream applications.\nSummary of Architectural Modifications\n(For a full breakdown, tech report soon-ish.)\n12B ‚Üí 8.9B Parameters:\nTL;DR: I replaced a 3.3B parameter timestep-encoding layer with a more efficient 250M parameter FFN, as the original was vastly oversized for its task.\nMMDiT Masking:\nTL;DR: Masking T5 padding tokens enhanced fidelity and increased training stability by preventing the model from focusing on irrelevant <pad> tokens.\nCustom Timestep Distributions:\nTL;DR: I implemented a custom timestep sampling distribution (-x^2) to prevent loss spikes and ensure the model trains effectively on both high-noise and low-noise regions.\nP.S\nChroma1-HD is not the old Chroma-v.50 it has been retrained from v.48\nCitation\n@misc{rock2025chroma,\nauthor = {Lodestone Rock},\ntitle = {Chroma1-HD},\nyear = {2025},\npublisher = {Hugging Face},\njournal = {Hugging Face repository},\nhowpublished = {\\url{https://huggingface.co/lodestones/Chroma1-HD}},\n}",
    "KrauthammerLab/cast-0.7b-s2s": "CAST 0.7B ‚Äî Speech-to-Speech\nCAST 0.7B ‚Äî Speech-to-Speech Model\nDemo\nPaper\nInstallation\nCAST 0.7B ‚Äî Speech-to-Speech\nFinal checkpoint files. Depends on KrauthammerLab/cast-wavtokenizer-24k-40tps for encode/decode.\nCAST 0.7B ‚Äî Speech-to-Speech Model\nCAST 0.7B is a speech-to-speech language model built on a 0.7B parameter Gemma3-style LM.It generates natural continuations of spoken audio.\nIt requires the companion CAST WavTokenizer for encode/decode.\nDemo\nInteractive samples and usage examples: https://mortezaro.github.io/speech-cast/\nPaper\nOptimizing Speech Language Models for Acoustic ConsistencyarXiv: 2509.26276 ‚Äî https://arxiv.org/abs/2509.26276\nWe study speech language models that incorporate semantic initialization and planning losses to achieve robust and consistent generation. Our approach initializes speech tokens with self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary objectives that target robustness and content planning. We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. Acoustic studies show that the speech-only models achieve the highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical and syntactic probes and semantic‚Äìacoustic alignment but reduces consistency. Linear probes show that our initialization biases the model toward content structure while trading off prosody detail. These results show that LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to the tokenizer or runtime architecture. A demo and model weights are available for exploration.\nInstallation\npip install torch torchaudio transformers accelerate soundfile\npip install git+https://github.com/jishengpeng/WavTokenizer.git\n1- Resynthesis\nWT_REPO = \"KrauthammerLab/cast-wavtokenizer-24k-40tps\"\n# Download tokenizer ckpt + config from HF\nwt_ckpt = hf_hub_download(WT_REPO, filename=\"wavtokenizer_large_unify_600_24k.ckpt\")\ntry:\nwt_cfg  = hf_hub_download(WT_REPO, filename=\"config.yaml\")\nexcept Exception:\nwt_cfg = None  # cfg optional in your setup\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load WavTokenizer (codec)\nif wt_cfg is not None:\nwt = WavTokenizer.from_pretrained0802(wt_cfg, wt_ckpt).to(device)\nelse:\nwt = WavTokenizer.from_pretrained0802(None, wt_ckpt).to(device)\n# Load a 16 kHz prompt\nwav16, sr = torchaudio.load(\"prompt_16k.wav\")  # mono recommended\nassert sr == 16000, f\"Expected 16k input, got {sr}\"\n# (Optional) ensure mono\nif wav16.size(0) > 1:\nwav16 = wav16.mean(dim=0, keepdim=True)\n# Resample 16k -> 24k before encode (your pipeline runs at 24k)\nwav24 = torchaudio.functional.resample(wav16, orig_freq=16000, new_freq=24000).to(device)\n# Encode ‚Üí features, codes\nbandwidth_id = torch.tensor([0], device=device)\nfeats, codes = wt.encode_infer(wav24, bandwidth_id=bandwidth_id)  # feats: [1, ?, T], codes: [1, streams?, T] or [1, T]\n# Decode back to waveform (24 kHz)\nrecon24 = wt.decode(feats, bandwidth_id=bandwidth_id)  # [1, T] or [1,1,T]\nif recon24.dim() == 3:\nrecon24 = recon24.squeeze(0)\n# Save 24k round-trip audio\nsf.write(\"recon_24k.wav\", recon24.squeeze(0).detach().cpu().numpy(), 24000)\nprint(\"Wrote recon_24k.wav\")\nSpeech generation\nLM_REPO = \"KrauthammerLab/cast-0.7b-s2s\"\nWT_REPO = \"KrauthammerLab/cast-wavtokenizer-24k-40tps\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncodes_per_second = 40          # your setup: ~40 tokens/s\ncodebook_size    = 4096        # [Sp1]..[Sp4096]\nspeech_prefix     = \"[Speech]\"\n# ---------- helpers ----------\ndef equal_power_crossfade(prev_24k: torch.Tensor, cont_24k: torch.Tensor, fade_ms: int = 40, sr: int = 24000) -> torch.Tensor:\n\"\"\"Equal-power crossfade between prev and cont (both [1,T] @ 24k).\"\"\"\nfade = max(1, int(sr * fade_ms / 1000))\nprev_24k = prev_24k.to(device)\ncont_24k = cont_24k.to(device)\nif prev_24k.size(1) < fade or cont_24k.size(1) < fade:\nreturn torch.cat([prev_24k, cont_24k], dim=1)\na = prev_24k[:, -fade:]\nb = cont_24k[:, :fade]\nt = torch.linspace(0, 1, fade, device=device).view(1, -1)\nmix = torch.cos(t * 0.5 * math.pi) * a + torch.sin(t * 0.5 * math.pi) * b\nreturn torch.cat([prev_24k[:, :-fade], mix, cont_24k[:, fade:]], dim=1)\nclass SpeechOnlyLogitsProcessor(LogitsProcessor):\n\"\"\"Mask logits so only [Sp#] tokens (and EOS) can be sampled.\"\"\"\ndef __init__(self, allowed: Set[int]):\nsuper().__init__()\nself.allowed = list(allowed)\ndef __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:\nmask = torch.full_like(scores, float(\"-inf\"))\nmask[..., self.allowed] = 0.0\nreturn scores + mask\n# ---------- load codec ----------\nwt_ckpt = hf_hub_download(WT_REPO, filename=\"wavtokenizer_large_unify_600_24k.ckpt\")\ntry:\nwt_cfg  = hf_hub_download(WT_REPO, filename=\"config.yaml\")\nexcept Exception:\nwt_cfg = None\nif wt_cfg is not None:\nwt = WavTokenizer.from_pretrained0802(wt_cfg, wt_ckpt).to(device)\nelse:\nwt = WavTokenizer.from_pretrained0802(None, wt_ckpt).to(device)\n# ---------- load LM + tokenizer ----------\ntok = AutoTokenizer.from_pretrained(LM_REPO)\nlm  = AutoModelForCausalLM.from_pretrained(LM_REPO, torch_dtype=torch.bfloat16).to(device).eval()\n# Build speech token id table: \"[Sp1]\"..\"[Sp4096]\" must be single tokens\nspeech_token_ids: List[int] = []\nfor i in range(1, codebook_size + 1):\nids = tok(f\"[Sp{i}]\", add_special_tokens=False)[\"input_ids\"]\nif len(ids) != 1:\nraise RuntimeError(f\"[Sp{i}] is not a single token; tokenizer mismatch.\")\nspeech_token_ids.append(ids[0])\n# For mapping back: token_id -> code_index (0-based)\nid2code = {tid: i for i, tid in enumerate(speech_token_ids)}\neos_id = tok.eos_token_id\nallowed_ids = set(speech_token_ids + ([eos_id] if eos_id is not None else []))\n# ---------- load prompt audio (16k), encode to codes ----------\nwav16, sr = torchaudio.load(\"prompt_16k.wav\")     # mono\nassert sr == 16000\nif wav16.size(0) > 1:\nwav16 = wav16.mean(dim=0, keepdim=True)\n# Resample to 24k before codec\nwav24 = torchaudio.functional.resample(wav16, orig_freq=16000, new_freq=24000).to(device)\nbw = torch.tensor([0], device=device)\nfeats, codes = wt.encode_infer(wav24, bandwidth_id=bw)\n# Normalize shapes to [T] list[int]\nif codes.dim() == 3:\ncodes = codes.squeeze(0)\ncodes = codes[0] if codes.size(0) > 1 else codes.squeeze(0)\nelif codes.dim() == 2:\ncodes = codes.squeeze(0)\ncodes_list = codes.long().tolist()  # each in [0..4095]\n# ---------- optional: decode round-trip for the stitched prefix ----------\nrecon24 = wt.decode(feats, bandwidth_id=bw)\nif recon24.dim() == 3:\nrecon24 = recon24.squeeze(0)\n# ---------- build LM prefix string ----------\nprefix_text = speech_prefix + \"\".join(f\"[Sp{c+1}]\" for c in codes_list)\nenc = tok(prefix_text, return_tensors=\"pt\")\ninput_ids = enc[\"input_ids\"].to(device)\nattn_mask = enc.get(\"attention_mask\", None)\nif attn_mask is not None:\nattn_mask = attn_mask.to(device)\n# ---------- generate continuation (about N seconds) ----------\nseconds = 3.0\nmax_new_tokens = max(1, int(round(seconds * codes_per_second)))\nlp = LogitsProcessorList([SpeechOnlyLogitsProcessor(allowed_ids)])\ngen = lm.generate(\ninput_ids=input_ids,\nattention_mask=attn_mask,\nmax_new_tokens=max_new_tokens,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.9,\nrepetition_penalty=1.1,\neos_token_id=eos_id,\npad_token_id=(tok.pad_token_id if tok.pad_token_id is not None else eos_id),\nlogits_processor=lp,\n)\n# Strip prefix and EOS\ngen_tail = gen[0][input_ids.size(1):].tolist()\nif eos_id is not None and eos_id in gen_tail:\ngen_tail = gen_tail[:gen_tail.index(eos_id)]\n# Map token ids -> code indices (0-based)\nnew_codes = [id2code[t] for t in gen_tail if t in id2code]\n# (Nice) keep last ~1s of prompt codes to avoid a hard seam\nkeep_sec = 1.0\nkeep = max(0, int(round(keep_sec * codes_per_second)))\ntail_codes = codes_list[-keep:] if keep > 0 else []\ndecode_codes = tail_codes + new_codes\n# Decode to audio (24 kHz)\ntok_tensor = torch.tensor(decode_codes, dtype=torch.long, device=device).view(1,1,-1)\ncont24 = wt.decode(wt.codes_to_features(tok_tensor), bandwidth_id=bw)\nif cont24.dim() == 3:\ncont24 = cont24.squeeze(0)\n# Stitch with crossfade\nstitched = equal_power_crossfade(recon24, cont24, fade_ms=60, sr=24000)\n# Save files\nsf.write(\"recon_24k.wav\",     recon24.squeeze(0).detach().cpu().numpy(), 24000)\nsf.write(\"continuation.wav\",  cont24.squeeze(0).detach().cpu().numpy(),  24000)\nsf.write(\"stitched_24k.wav\",  stitched.squeeze(0).detach().cpu().numpy(),24000)\nprint(\"Wrote recon_24k.wav, continuation.wav, stitched_24k.wav\")"
}