{
    "AXERA-TECH/Pulsar2": "User Guide\nUser Guide\nÁÆÄ‰Ωì‰∏≠ÊñáÊñáÊ°£ ÈìæÊé•\nEnglish Guide Link",
    "starvector/starvector-8b-im2svg": "Model Card for StarVector\nModel Details\nModel Description\nModel Architecture\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data & Factors\nModels\nSummary\nBibTeX entry and citation info\nModel Card for StarVector\nStarVector is a foundation model for generating Scalable Vector Graphics (SVG) code from images and text. It utilizes a Vision-Language Modeling architecture to understand both visual and textual inputs, enabling high-quality vectorization and text-guided SVG creation.\nModel Details\nModel Description\nThis is the model card for the StarVector model, a ü§ó transformers model. StarVector is a foundation model for generating Scalable Vector Graphics (SVG) code from images and text. It utilizes a Vision-Language Modeling architecture to understand both visual and textual inputs, enabling high-quality vectorization and text-guided SVG creation.\nDeveloped by: ServiceNow Research, Mila - Quebec AI Institute, ETS, Montreal.\nShared by : Juan A Rodriguez, Abhay Puri, Shubham Agarwal, Issam H. Laradji, Sai Rajeswar, Pau Rodriguez, David Vazquez, Christopher Pal, Marco Pedersoli.\nModel type: Vision-Language Model for SVG Generation.\nLanguage(s) (NLP): English.\nLicense: Apache 2.0\nModel Architecture\nThe StarVector architecture integrates an image encoder and a Large Language Model (LLM) Adapter to generate SVG code from both image and text inputs. Images are first converted into embeddings using a Vision Transformer (ViT), after which the LLM Adapter maps these embeddings into the LLM's embedding space to create visual tokens. Text prompts are handled through the LLM‚Äôs tokenizer and embedder. This unified multimodal approach ensures precise and contextually rich SVG output.\nFigure 2: a) StarVector Architecture: StarVector projects images into embeddings via an image encoder, then maps these embeddings to the LLM hidden space using an LLM Adapter, generating Visual Tokens. Text conditioning is achieved with the LLM's tokenizer and embedder. The model learns to map token sequences (visual or textual) to SVG code. The symbol ‚äï denotes mutually exclusive operations (image-to- SVG or text-to-SVG), while ‚Äñ indicates sequence concatenation. Figure 2: b)Vision Model and Adapter: The image encoder employs a Vision Transformer (ViT) to process image patches sequentially. The LLM Adapter non-linearly projects embeddings into visual tokens for LLM integration.\nModel Sources\nRepository: https://github.com/joanrod/star-vector\nPaper: https://arxiv.org/abs/2312.11556\nUses\nDirect Use\nImage-to-SVG generation, Text-to-SVG generation.\nDownstream Use\nCreation of icons, logotypes, technical diagrams, and other vector graphics.\nOut-of-Scope Use\nGenerating realistic photographic images or complex 3D graphics.\nBias, Risks, and Limitations\nPotential biases may exist in the model due to the composition of the training data (SVG-Stack). The model's ability to perfectly vectorize all types of images and interpret all textual instructions may have limitations. Users should be aware of these potential issues, especially in critical applications.\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. Further investigation into the model's behavior across different types of inputs is recommended.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\nfrom starvector.data.util import process_and_rasterize_svg\nimport torch\nmodel_name = \"starvector/starvector-8b-im2svg\"\nstarvector = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True)\nprocessor = starvector.model.processor\ntokenizer = starvector.model.svg_transformer.tokenizer\nstarvector.cuda()\nstarvector.eval()\nimage_pil = Image.open('assets/examples/sample-18.png')\nimage = processor(image_pil, return_tensors=\"pt\")['pixel_values'].cuda()\nif not image.shape[0] == 1:\nimage = image.squeeze(0)\nbatch = {\"image\": image}\nraw_svg = starvector.generate_im2svg(batch, max_length=4000)[0]\nsvg, raster_image = process_and_rasterize_svg(raw_svg)\nTraining Details\nTraining Data\nSVG-Stack: A dataset of over 2 million SVG samples.\nTraining Procedure\nThe model utilizes a Vision-Language Modeling architecture. Images are projected into embeddings via an image encoder, then mapped to the LLM hidden space using an LLM Adapter, generating Visual Tokens. Text conditioning is achieved with the LLM's tokenizer and embedder. The model learns to map token sequences (visual or textual) to SVG code.\nEvaluation\nTesting Data & Factors\nTesting Data\nSVG-Bench\nFactors\nSVG-Stack, SVG-Fonts, SVG-Icons, SVG-Emoji, SVG-Diagrams.\nModels\nStarVector models achieve state-of-the-art performance on SVG generation tasks\nWe provide Hugging Face ü§ó model checkpoints for image2SVG vectorization, for üí´ StarVector-8B and üí´ StarVector-1B. These are the results on SVG-Bench, using the DinoScore metric.\nMethod\nSVG-Stack\nSVG-Fonts\nSVG-Icons\nSVG-Emoji\nSVG-Diagrams\nAutoTrace\n0.942\n0.954\n0.946\n0.975\n0.874\nPotrace\n0.898\n0.967\n0.972\n0.882\n0.875\nVTracer\n0.954\n0.964\n0.940\n0.981\n0.882\nIm2Vec\n0.692\n0.733\n0.754\n0.732\n-\nLIVE\n0.934\n0.956\n0.959\n0.969\n0.870\nDiffVG\n0.810\n0.821\n0.952\n0.814\n0.822\nGPT-4-V\n0.852\n0.842\n0.848\n0.850\n-\nüí´ StarVector-1B\n0.926\n0.978\n0.975\n0.929\n0.943\nüí´ StarVector-8B\n0.966\n0.982\n0.984\n0.981\n0.959\nNote: StarVector models will not work for natural images or illustrations, as they have not been trained on those images. They excel in vectorizing icons, logotypes, technical diagrams, graphs, and charts.\nAs shown in the table above, StarVector-8B achieves the highest performance across all benchmark datasets, demonstrating its effectiveness in generating high-quality SVG code from images. The model's ability to understand and reproduce complex vector graphics makes it particularly valuable for applications requiring precise vectorization of icons, logos, and technical diagrams.\nSummary\nStarVector represents a significant advancement in the field of vector graphics generation. By combining the power of vision-language models with a comprehensive training dataset, we've created a system that can accurately translate images into high-quality SVG code. The model's performance on SVG-Bench demonstrates its effectiveness across a wide range of vector graphics tasks.\nWe believe that StarVector will enable new applications in design, illustration, and technical documentation, making vector graphics more accessible and easier to create. We invite the research community to build upon our work and explore new directions in this exciting field.\nFor more details, please refer to our paper and explore our code repository.\nBibTeX entry and citation info\n@misc{rodriguez2024starvector,\ntitle={StarVector: Generating Scalable Vector Graphics Code from Images and Text},\nauthor={Juan A. Rodriguez and Abhay Puri and Shubham Agarwal and Issam H. Laradji and Pau Rodriguez and Sai Rajeswar and David Vazquez and Christopher Pal and Marco Pedersoli},\nyear={2024},\neprint={2312.11556},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2312.11556},\n}",
    "Rombo-Org/Rombo-LLM-V2.5-Qwen-32b": "Rombos-LLM-V2.5-Qwen-32b\nRombos-LLM-V2.5-Qwen-32b\nRombos-LLM-V2.5-Qwen-32b is a continues finetuned version of Qwen2.5-32B. I noticed recently that the Qwen team did not learn from my methods of continuous finetuning, the great benefits, and no downsides of it. So I took it upon myself to merge the instruct model with the base model myself using the Ties merge method\nThis version of the model shows higher performance than the original instruct and base models.\nQuants: (Coming soon)\nGGUF: https://huggingface.co/bartowski/Replete-LLM-V2.5-Qwen-32b-GGUF\nEXL2:\n(8-bit)\nhttps://huggingface.co/Apel-sin/rombos-llm-v2.5-qwen-32b-exl2\n(5-bit)\nhttps://huggingface.co/async0x42/Rombos-LLM-V2.5-Qwen-32b-exl2_5.0bpw\n(4.25-bit)\nhttps://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-32b-Exl2-4.25-bit",
    "MiniMaxAI/MiniMax-VL-01": "MiniMax-VL-01\n1. Introduction\n2. Evaluation\n3. Quickstart\n4. Deployment Guide\n5. Function Calling\n6. Citation\n7. Chatbot & API\n8. Contact Us\nWeChat\nMiniMax-VL-01\n1. Introduction\nWe are delighted to introduce our MiniMax-VL-01 model. It adopts the \"ViT-MLP-LLM\" framework, which is a commonly used technique in the field of multimodal large language models. The model is initialized and trained with three key parts: a 303-million-parameter Vision Transformer (ViT) for visual encoding, a randomly initialized two-layer MLP projector for image adaptation, and the MiniMax-Text-01 as the base LLM.\nMiniMax-VL-01 has a notable dynamic resolution feature. Input images are resized per a pre-set grid, with resolutions from 336√ó336 to 2016√ó2016, keeping a 336√ó336 thumbnail. The resized images are split into non-overlapping patches of the same size. These patches and the thumbnail are encoded separately and then combined for a full image representation.\nThe training data for MiniMax-VL-01 consists of caption, description, and instruction data. The Vision Transformer (ViT) is trained on 694 million image-caption pairs from scratch. Across four distinct stages of the training pipeline, a total of 512 billion tokens are processed, leveraging this vast amount of data to endow the model with strong capabilities.\nFinally, MiniMax-VL-01 has reached top-level performance on multimodal leaderboards, demonstrating its edge and dependability in complex multimodal tasks.\n2. Evaluation\nTasks\nGPT-4o(11-20)\nClaude-3.5-Sonnet (10-22)\nGemini-1.5-Pro (002)\nGemini-2.0-Flash (exp)\nQwen2-VL-72B-Inst.\nInternVL2.5-78B\nLLama-3.2-90B\nMiniMax-VL-01\nKnowledge\nMMMU*\n63.5\n72.0\n68.4\n70.6\n64.5\n66.5\n62.1\n68.5\nMMMU-Pro*\n54.5\n54.7\n50.9\n57.0\n43.2\n47.3\n36.0\n52.7\nVisual Q&A\nChartQA*relaxed\n88.1\n90.8\n88.7\n88.3\n91.2\n91.5\n85.5\n91.7\nDocVQA*\n91.1\n94.2\n91.5\n92.9\n97.1\n96.1\n90.1\n96.4\nOCRBench\n806\n790\n800\n846\n856\n847\n805\n865\nMathematics & Sciences\nAI2D*\n83.1\n82.0\n80.9\n85.1\n84.4\n86.8\n78.9\n83.3\nMathVista*\n62.1\n65.4\n70.6\n73.1\n69.6\n68.4\n57.3\n68.6\nOlympiadBenchfull\n25.2\n28.4\n32.1\n46.1\n21.9\n25.1\n19.3\n24.2\nLong Context\nM-LongDocacc\n41.4\n31.4\n26.2\n31.4\n11.6\n19.7\n13.9\n32.5\nComprehensive\nMEGA-Benchmacro\n49.4\n51.4\n45.9\n53.9\n46.8\n45.3\n19.9\n47.4\nUser Experience\nIn-house Benchmark\n62.3\n47.0\n49.2\n72.1\n40.6\n34.8\n13.6\n56.6\n* Evaluated following a 0-shot CoT setting.\n3. Quickstart\nHere we provide a simple example of loading the tokenizer and model to generate content.\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoConfig, QuantoConfig, GenerationConfig\nimport torch\nimport json\nimport os\nfrom PIL import Image\n# load hf config\nhf_config = AutoConfig.from_pretrained(\"MiniMaxAI/MiniMax-VL-01\", trust_remote_code=True)\n# quantization config, int8 is recommended\nquantization_config =  QuantoConfig(\nweights=\"int8\",\nmodules_to_not_convert=[\n\"vision_tower\",\n\"image_newline\",\n\"multi_modal_projector\",\n\"lm_head\",\n\"embed_tokens\",\n] + [f\"model.layers.{i}.coefficient\" for i in range(hf_config.text_config.num_hidden_layers)]\n+ [f\"model.layers.{i}.block_sparse_moe.gate\" for i in range(hf_config.text_config.num_hidden_layers)]\n)\n# set device map\nmodel_safetensors_index_path = os.path.join(\"MiniMax-VL-01\", \"model.safetensors.index.json\")\nwith open(model_safetensors_index_path, \"r\") as f:\nmodel_safetensors_index = json.load(f)\nweight_map = model_safetensors_index['weight_map']\nvision_map = {}\nfor key, value in weight_map.items():\nif 'vision_tower' in key or 'image_newline' in key or 'multi_modal_projector' in key:\nnew_key = key.replace('.weight','').replace('.bias','')\nif new_key not in vision_map:\nvision_map[new_key] = value\n# assume 8 GPUs\nworld_size = 8\ndevice_map = {\n'language_model.model.embed_tokens': 'cuda:0',\n'language_model.model.norm': f'cuda:{world_size - 1}',\n'language_model.lm_head': f'cuda:{world_size - 1}'\n}\nfor key, value in vision_map.items():\ndevice_map[key] = f'cuda:0'\ndevice_map['vision_tower.vision_model.post_layernorm'] = f'cuda:0'\nlayers_per_device = hf_config.text_config.num_hidden_layers // world_size\nfor i in range(world_size):\nfor j in range(layers_per_device):\ndevice_map[f'language_model.model.layers.{i * layers_per_device + j}'] = f'cuda:{i}'\n# load processor\nprocessor = AutoProcessor.from_pretrained(\"MiniMaxAI/MiniMax-VL-01\", trust_remote_code=True)\nmessages = [\n{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant created by MiniMax based on MiniMax-VL-01 model.\"}]},\n{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"placeholder\"},{\"type\": \"text\", \"text\": \"Describe this image.\"}]},\n]\nprompt = processor.tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nraw_image = Image.open(\"figures/image.jpg\")\n# tokenize and move to device\nmodel_inputs = processor(images=[raw_image], text=prompt, return_tensors='pt').to('cuda').to(torch.bfloat16)\n# load bfloat16 model, move to device, and apply quantization\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"MiniMaxAI/MiniMax-VL-01\",\ntorch_dtype=\"bfloat16\",\ndevice_map=device_map,\nquantization_config=quantization_config,\ntrust_remote_code=True,\noffload_buffers=True,\n)\ngeneration_config = GenerationConfig(\nmax_new_tokens=100,\neos_token_id=200020,\nuse_cache=True,\n)\n# generate response\ngenerated_ids = quantized_model.generate(**model_inputs, generation_config=generation_config)\nprint(f\"generated_ids: {generated_ids}\")\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n4. Deployment Guide\nFor production deployment, we recommend using vLLM to serve MiniMax-VL-01. vLLM provides excellent performance for serving large language models with the following features:\nüî• Outstanding service throughput performance‚ö° Efficient and intelligent memory managementüì¶ Powerful batch request processing capability‚öôÔ∏è Deeply optimized underlying performanceFor detailed deployment instructions, please refer to our vLLM Deployment Guide.\n5. Function Calling\nMiniMax-VL-01 supports Function Calling capability, enabling the model to intelligently identify when external functions need to be called and output parameters in structured JSON format. With Function Calling, you can:\nLet the model recognize implicit function call needs in user requests\nReceive structured parameter outputs for seamless application integration\nSupport various complex parameter types, including nested objects and arrays\nFunction Calling supports standard OpenAI-compatible format definitions and integrates seamlessly with the Transformers library. For detailed usage instructions, please refer to our Function Call Guide or Chinese Guide.\n6. Citation\n@misc{minimax2025minimax01scalingfoundationmodels,\ntitle={MiniMax-01: Scaling Foundation Models with Lightning Attention},\nauthor={MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},\nyear={2025},\neprint={2501.08313},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.08313},\n}\n7. Chatbot & API\nFor general use and evaluation, we provide a Chatbot with online search capabilities and the online API for developers. For general use and evaluation, we provide the MiniMax MCP Server with video generation, image generation, speech synthesis, and voice cloning for developers.\n8. Contact Us\nContact us at model@minimax.io.",
    "bunnycore/Phi-4-Stock-RP": "Training Data:\nMerge Method\nModels Merged\nConfiguration\nOpen LLM Leaderboard Evaluation Results\nPhi-4-Stock-RP is a phi4 based language model designed for reasoning and role-play scenarios. It leverages the capabilities of several pre-existing high-quality models, integrating them into a cohesive system that excels in reasoning, creative, narrative, and interactive text generation.\nTraining Data:\nSources:\nMerged from various pre-trained models, focusing on those with strong performance in text generation and understanding.\nEnhanced with a specialized LoRA trained on role-play dialogues, scenarios, and character interactions.\nModel Capabilities:\nRole-Playing: Capable of maintaining coherent characters, plots, and dialogues over extended interactions.\nCreative Writing: Assists in crafting stories, dialogues, and character development with a focus on immersion and narrative coherence.\nGeneral Language Understanding: Inherits general text comprehension and generation from the base models, making it versatile for various language tasks beyond RP.\n<|im_start|>system<|im_sep|> {system_message}<|im_end|> <|im_start|>user<|im_sep|> {prompt}<|im_end|> <|im_start|>assistant<|im_sep|>\nMerge Method\nThis model was merged using the passthrough merge method using bunnycore/Phi-4-Model-Stock + bunnycore/Phi-4-rp-v1-lora as a base.\nModels Merged\nThe following models were included in the merge:\nConfiguration\nThe following YAML configuration was used to produce this model:\nbase_model: bunnycore/Phi-4-Model-Stock+bunnycore/Phi-4-rp-v1-lora\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n- model: bunnycore/Phi-4-Model-Stock+bunnycore/Phi-4-rp-v1-lora\ntokenizer_source: unsloth/phi-4\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n38.73\nIFEval (0-Shot)\n63.99\nBBH (3-Shot)\n55.21\nMATH Lvl 5 (4-Shot)\n32.25\nGPQA (0-shot)\n14.43\nMuSR (0-shot)\n18.53\nMMLU-PRO (5-shot)\n47.96",
    "mradermacher/Phi-4-Empathetic-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/prithivMLmods/Phi-4-Empathetic\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Phi-4-Empathetic-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n5.7\nGGUF\nQ3_K_S\n6.6\nGGUF\nQ3_K_M\n7.3\nlower quality\nGGUF\nQ3_K_L\n7.9\nGGUF\nIQ4_XS\n8.2\nGGUF\nQ4_K_S\n8.5\nfast, recommended\nGGUF\nQ4_K_M\n9.0\nfast, recommended\nGGUF\nQ5_K_S\n10.3\nGGUF\nQ5_K_M\n10.5\nGGUF\nQ6_K\n12.1\nvery good quality\nGGUF\nQ8_0\n15.7\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "lightonai/modernbert-embed-large": "ModernBERT-embed-large\nPerformance\nUsage\nSentence Transformers\nTransformers\nTransformers.js\nTraining\nAcknowledgment\nCitation\nModernBERT-embed-large\nModernBERT-embed-large is an embedding model trained from ModernBERT-large, bringing the new advances of ModernBERT to embeddings!\nIndeed, ModernBERT is a base model trained for Masked Language Modeling and can not directly be used to perform tasks such as retrieval without further fine-tuning.\nModernBERT-embed-large is fine-tuned on the Nomic Embed weakly-supervised and supervised datasets and also supports Matryoshka Representation Learning dimensions of 256 to reduce memory with minimal performance loss.\nPerformance\nModel\nDimensions\nAverage (56)\nClassification (12)\nClustering (11)\nPair Classification (3)\nReranking (4)\nRetrieval (15)\nSTS (10)\nSummarization (1)\nnomic-embed-text-v1.5\n768\n62.28\n73.55\n43.93\n84.61\n55.78\n53.01\n81.94\n30.4\nmodernbert-embed-base\n768\n62.62\n74.31\n44.98\n83.96\n56.42\n52.89\n81.78\n31.39\nmodernbert-embed-large\n1024\n63,84\n75.03\n46.04\n85.31\n57.64\n54.36\n83.80\n28.31\nnomic-embed-text-v1.5\n256\n61.04\n72.1\n43.16\n84.09\n55.18\n50.81\n81.34\n30.05\nmodernbert-embed-base\n256\n61.17\n72.40\n43.82\n83.45\n55.69\n50.62\n81.12\n31.27\nmodernbert-embed-large\n256\n62.43\n73.60\n44.59\n84.89\n57.08\n51.72\n83.46\n29.03\nUsage\nYou can use these models directly with the latest transformers release and requires installing transformers>=4.48.0:\npip install transformers>=4.48.0\nReminder, this model is trained similarly to Nomic Embed and REQUIRES prefixes to be added to the input. For more information, see the instructions in Nomic Embed.\nMost use cases, adding search_query:  to the query and search_document:  to the documents will be sufficient.\nSentence Transformers\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"lightonai/modernbert-embed-large\")\nquery_embeddings = model.encode([\n\"search_query: What is TSNE?\",\n\"search_query: Who is Laurens van der Maaten?\",\n])\ndoc_embeddings = model.encode([\n\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n])\nprint(query_embeddings.shape, doc_embeddings.shape)\n# (2, 1024) (1, 1024)\nsimilarities = model.similarity(query_embeddings, doc_embeddings)\nprint(similarities)\n# tensor([[0.6518],\n#         [0.4237]])\nClick to see Sentence Transformers usage with Matryoshka Truncation\nIn Sentence Transformers, you can truncate embeddings to a smaller dimension by using the truncate_dim parameter when loading the SentenceTransformer model.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"lightonai/modernbert-embed-large\", truncate_dim=256)\nquery_embeddings = model.encode([\n\"search_query: What is TSNE?\",\n\"search_query: Who is Laurens van der Maaten?\",\n])\ndoc_embeddings = model.encode([\n\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n])\nprint(query_embeddings.shape, doc_embeddings.shape)\n# (2, 256) (1, 256)\nsimilarities = model.similarity(query_embeddings, doc_embeddings)\nprint(similarities)\n# tensor([[0.6835],\n#         [0.3982]])\nNote the small differences compared to the full 1024-dimensional similarities.\nTransformers\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]\ninput_mask_expanded = (\nattention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n)\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\ninput_mask_expanded.sum(1), min=1e-9\n)\nqueries = [\"search_query: What is TSNE?\", \"search_query: Who is Laurens van der Maaten?\"]\ndocuments = [\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\"]\ntokenizer = AutoTokenizer.from_pretrained(\"lightonai/modernbert-embed-large\")\nmodel = AutoModel.from_pretrained(\"lightonai/modernbert-embed-large\")\nencoded_queries = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\nqueries_outputs = model(**encoded_queries)\ndocuments_outputs = model(**encoded_documents)\nquery_embeddings = mean_pooling(queries_outputs, encoded_queries[\"attention_mask\"])\nquery_embeddings = F.normalize(query_embeddings, p=2, dim=1)\ndoc_embeddings = mean_pooling(documents_outputs, encoded_documents[\"attention_mask\"])\ndoc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)\nprint(query_embeddings.shape, doc_embeddings.shape)\n# torch.Size([2, 1024]) torch.Size([1, 1024])\nsimilarities = query_embeddings @ doc_embeddings.T\nprint(similarities)\n# tensor([[0.6518],\n#         [0.4237]])\nClick to see Transformers usage with Matryoshka Truncation\nIn transformers, you can truncate embeddings to a smaller dimension by slicing the mean pooled embeddings, prior to normalization.\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]\ninput_mask_expanded = (\nattention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n)\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\ninput_mask_expanded.sum(1), min=1e-9\n)\nqueries = [\"search_query: What is TSNE?\", \"search_query: Who is Laurens van der Maaten?\"]\ndocuments = [\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\"]\ntokenizer = AutoTokenizer.from_pretrained(\".\")\nmodel = AutoModel.from_pretrained(\".\")\ntruncate_dim = 256\nencoded_queries = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\nencoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\nqueries_outputs = model(**encoded_queries)\ndocuments_outputs = model(**encoded_documents)\nquery_embeddings = mean_pooling(queries_outputs, encoded_queries[\"attention_mask\"])\nquery_embeddings = query_embeddings[:, :truncate_dim]\nquery_embeddings = F.normalize(query_embeddings, p=2, dim=1)\ndoc_embeddings = mean_pooling(documents_outputs, encoded_documents[\"attention_mask\"])\ndoc_embeddings = doc_embeddings[:, :truncate_dim]\ndoc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)\nprint(query_embeddings.shape, doc_embeddings.shape)\n# torch.Size([2, 256]) torch.Size([1, 256])\nsimilarities = query_embeddings @ doc_embeddings.T\nprint(similarities)\n# tensor([[0.6835],\n#         [0.3982]])\nNote the small differences compared to the full 1024-dimensional similarities.\nTransformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nThen, you can compute embeddings as follows:\nimport { pipeline, matmul } from '@huggingface/transformers';\n// Create a feature extraction pipeline\nconst extractor = await pipeline(\n\"feature-extraction\",\n\"lightonai/modernbert-embed-large\",\n{ dtype: \"fp32\" }, // Supported options: \"fp32\", \"fp16\", \"q8\", \"q4\", \"q4f16\"\n);\n// Embed queries and documents\nconst query_embeddings = await extractor([\n\"search_query: What is TSNE?\",\n\"search_query: Who is Laurens van der Maaten?\",\n], { pooling: \"mean\", normalize: true },\n);\nconst doc_embeddings = await extractor([\n\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n], { pooling: \"mean\", normalize: true },\n);\n// Compute similarity scores\nconst similarities = await matmul(query_embeddings, doc_embeddings.transpose(1, 0));\nconsole.log(similarities.tolist());\nTraining\nWe train ModernBERT-embed-large using a multi-stage training pipeline. Starting from the pretrained ModernBERT-large model,\nthe first unsupervised contrastive stage trains on a dataset generated from weakly related text pairs, such as question-answer pairs from forums like StackExchange and Quora, title-body pairs from Amazon reviews, and summarizations from news articles.\nIn the second finetuning stage, higher quality labeled datasets such as search queries and answers from web searches are leveraged. Data curation and hard-example mining is crucial in this stage.\nFor more details, see the Nomic Embed Technical Report and corresponding blog post.\nTraining data to train the models is released in its entirety. For more details, see the contrastors repository\nAcknowledgment\nWe wanted to thank Zach Nussbaum from Nomic AI for building and sharing the Nomic Embed recipe and tools and its support during the training of this model!\nThe training has been run on Orange Business Cloud Avenue infrastructure.\nCitation\nIf you find the model, dataset, or training code useful, please considering citing ModernBERT as well as Nomic Embed:\n@misc{modernbert,\ntitle={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},\nauthor={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\nyear={2024},\neprint={2412.13663},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.13663},\n}\n@misc{nussbaum2024nomic,\ntitle={Nomic Embed: Training a Reproducible Long Context Text Embedder},\nauthor={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\nyear={2024},\neprint={2402.01613},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nAnd if you want to cite this fine-tuning in particular, please use:\n@misc{ModernBERT-embed-large,\ntitle={ModernBERT-embed-large},\nauthor={Chaffin, Antoine},\nurl={https://huggingface.co/lightonai/modernbert-embed-large},\nyear={2025}\n}",
    "Qwen/Qwen2.5-Math-PRM-7B": "Qwen2.5-Math-PRM-7B\nIntroduction\nModel Details\nRequirements\nQuick Start\nPrerequisites\nü§ó Hugging Face Transformers\nCitation\nQwen2.5-Math-PRM-7B\nIntroduction\nIn addition to the mathematical Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B, we release the Process Reward Model (PRM), namely Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B. PRMs emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), aiming to identify and mitigate intermediate errors in the reasoning processes. Our trained PRMs exhibit both impressive performance in the Best-of-N (BoN) evaluation and stronger error identification performance in ProcessBench.\nModel Details\nFor more details, please refer to our paper.\nRequirements\ntransformers>=4.40.0 for Qwen2.5-Math models. The latest version is recommended.\nüö® This is a must because `transformers` integrated Qwen2.5 codes since `4.37.0`.\nFor requirements on GPU memory and the respective throughput, see similar results of Qwen2 here.\nQuick Start\nQwen2.5-Math-PRM-7B is a process reward model typically used for offering feedback on the quality of reasoning and intermediate steps rather than generation.\nPrerequisites\nStep Separation: We recommend using double line breaks (\"\\n\\n\") to separate individual steps within the solution if using responses from Qwen2.5-Math-Instruct.\nReward Computation: After each step, we insert a special token \"<extra_0>\". For reward calculation, we extract the probability score of this token being classified as positive, resulting in a reward value between 0 and 1.\nü§ó Hugging Face Transformers\nHere we show a code snippet to show you how to use the Qwen2.5-Math-PRM-7B with transformers:\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nimport torch.nn.functional as F\ndef make_step_rewards(logits, token_masks):\nprobabilities = F.softmax(logits, dim=-1)\nprobabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\nall_scores_res = []\nfor i in range(probabilities.size(0)):\nsample = probabilities[i] # seq_len, num_labels\npositive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels\nnon_zero_elements_list = positive_probs.cpu().tolist()\nall_scores_res.append(non_zero_elements_list)\nreturn all_scores_res\nmodel_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\ndevice = \"auto\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\nmodel_name,\ndevice_map=device,\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\n).eval()\ndata = {\n\"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n\"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n\"response\": [\n\"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n\"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n\"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n\"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n]\n}\nmessages = [\n{\"role\": \"system\", \"content\": data['system']},\n{\"role\": \"user\", \"content\": data['query']},\n{\"role\": \"assistant\", \"content\": \"<extra_0>\".join(data['response']) + \"<extra_0>\"},\n]\nconversation_str = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=False\n)\ninput_ids = tokenizer.encode(\nconversation_str,\nreturn_tensors=\"pt\",\n).to(model.device)\noutputs = model(input_ids=input_ids)\nstep_sep_id = tokenizer.encode(\"<extra_0>\")[0]\ntoken_masks = (input_ids == step_sep_id)\nstep_reward = make_step_rewards(outputs[0], token_masks)\nprint(step_reward)  # [[1.0, 0.1904296875, 0.9765625, 1.0]]\nCitation\nIf you find our work helpful, feel free to give us a citation.\n@article{prmlessons,\ntitle={The Lessons of Developing Process Reward Models in Mathematical Reasoning},\nauthor={\nZhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin\n},\njournal={arXiv preprint arXiv:2501.07301},\nyear={2025}\n}",
    "haywoodsloan/ai-image-detector-dev-deploy": "Model Trained Using AutoTrain\nValidation Metrics\nModel Trained Using AutoTrain\nProblem type: Image Classification\nValidation Metrics\nloss: 0.08581268042325974\nf1: 0.9875742669136907\nprecision: 0.9817413946399085\nrecall: 0.9934768637532133\nauc: 0.9953790023764837\naccuracy: 0.981488090989126",
    "plimper/plimper-lora": "No model card",
    "Kim2091/2x-AnimeSharpV4": "2x-AnimeSharpV4 & Fast\n2x-AnimeSharpV4 & Fast\nScale: 2\nArchitecture: RCAN & RCAN PixelUnshuffle\nLinks: Github Release\nAuthor: Kim2091\nLicense: CC BY-NC-SA 4.0\nPurpose: Anime\nSubject:\nInput Type: Images\nDate: 1-7-25\nSize:\nI/O Channels: 3(RGB)->3(RGB)\nDataset: ModernAnimation1080_v3 & digital_art_v3\nDataset Size: 6k & 20k\nOTF (on the fly augmentations): No\nPretrained Model: 2x-AnimeSharpV3_RCAN & database's 12k PU checkpoint\nIterations: 100k RCAN & 400k RCAN PU\nBatch Size: 8\nGT Size: 64\nDescription: This is a successor to AnimeSharpV3 based on RCAN instead of ESRGAN. It outperforms both versions of AnimeSharpV3 in every capacity. It's sharper, retains even more detail, and has very few artifacts. It is extremely faithful to the input image, even with heavily compressed inputs.\nCurrently it is NOT compatible with chaiNNer, but will be available on the nightly build soon (hopefully).\nThe 2x-AnimeSharpV4_Fast_RCAN_PU model is trained on RCAN PixelUnshuffle. This is much faster, but comes at the cost of quality. I believe the model is ~95% the quality of the full V4 RCAN model, but ~6x faster in Pytorch and ~4x faster in TensorRT. This model is ideal for video processing, and as such was trained to handle MPEG2 & H264 compression.\nTo use the Pytorch version of the model right now, you can update your version of the spandrel library to 0.4.1 in ComfyUI\nAlternatively, the latest chaiNNer nightly supports it: https://github.com/chaiNNer-org/chaiNNer-nightly/releases\nComparisons:\nhttps://slow.pics/c/63Qu8HTN\nhttps://slow.pics/c/DBJPDJM9",
    "allenai/olmOCR-7B-0225-preview": "",
    "nvidia/llama-3.1-nemoguard-8b-content-safety": "",
    "nvidia/Llama-3.3-70B-Instruct-FP4": "",
    "mradermacher/Phi-4-Empathetic-i1-GGUF": "",
    "jingheya/lotus-depth-g-v2-1-disparity": "",
    "antonvinny/gs3-test": "whisper-tiny-gs3\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nwhisper-tiny-gs3\nThis model is a fine-tuned version of openai/whisper-small on the None dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.0001\nWer: 3.0326\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 3e-05\ntrain_batch_size: 16\neval_batch_size: 8\nseed: 42\noptimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 100\ntraining_steps: 600\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nWer\n0.0013\n50.0\n200\n0.0007\n3.0326\n0.0002\n100.0\n400\n0.0002\n3.0326\n0.0001\n150.0\n600\n0.0001\n3.0326\nFramework versions\nTransformers 4.48.0\nPytorch 2.5.1+cu121\nDatasets 3.2.0\nTokenizers 0.21.0",
    "AdamCodd/YOLOv11n-face-detection": "YOLOv11n-Face-Detection\nUsage\nLimitations\nYOLOv11n-Face-Detection\nA lightweight face detection model based on YOLO architecture (YOLOv11 nano), trained for 225 epochs on the WIDERFACE dataset.\nIt achieves the following results on the evaluation set:\n==================== Results ====================\nEasy   Val AP: 0.9420471677096086\nMedium Val AP: 0.9210357271019756\nHard   Val AP: 0.8099848364072022\n=================================================\nYOLO results:\nConfusion matrix:\n[[23577 2878]\n[16098 0]]\nUsage\nfrom huggingface_hub import hf_hub_download\nfrom ultralytics import YOLO\nmodel_path = hf_hub_download(repo_id=\"AdamCodd/YOLOv11n-face-detection\", filename=\"model.pt\")\nmodel = YOLO(model_path)\nresults = model.predict(\"/path/to/your/image\", save=True) # saves the result in runs/detect/predict\nLimitations\nPerformance may vary in extreme lighting conditions\nBest suited for frontal and slightly angled faces\nOptimal performance for faces occupying >20 pixels",
    "yeniguno/bert-uncased-intent-classification": "Model Card for Model ID\nHow to Get Started with the Model\nUses\nBias, Risks, and Limitations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nResults\nModel Card for Model ID\nThis is a fine-tuned BERT-based model for intent classification, capable of categorizing intents into 82 distinct labels. It was trained on a consolidated dataset of multilingual intent datasets.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel = AutoModelForSequenceClassification.from_pretrained(\"yeniguno/bert-uncased-intent-classification\")\ntokenizer = AutoTokenizer.from_pretrained(\"yeniguno/bert-uncased-intent-classification\")\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\ntext = \"Play the song, Sam.\"\nprediction = pipe(text)\nprint(prediction)\n#¬†[{'label': 'play_music', 'score': 0.9997674822807312}]\nUses\nThis model is intended for:\nNatural Language Understanding (NLU) tasks. Classifying user intents for applications such as:\nVoice assistants\nChatbots\nCustomer support automation\nConversational AI systems\nBias, Risks, and Limitations\nThe model's performance may degrade on intents that are underrepresented in the training data. Not optimized for languages other than English. Domain-specific intents not included in the dataset may require additional fine-tuning.\nTraining Details\nTraining Data\nhis model was trained on a combination of intent datasets from various sources:\nDatasets Used:\nmteb/amazon_massive_intent\nmteb/mtop_intent\nsonos-nlu-benchmark/snips_built_in_intents\nMozilla/smart_intent_dataset\nBhuvaneshwari/intent_classification\nclinc/clinc_oos\nEach dataset was preprocessed, and intent labels were consolidated into 82 unique classes.\nDataset Sizes:\nTrain size: 138228\nValidation size: 17279\nTest size: 17278\nTraining Procedure\nThe model was fine-tuned with the following hyperparameters:\nBase Model: bert-base-uncased Learning Rate: 3e-5 Batch Size: 32 Epochs: 4 Weight Decay: 0.01 Evaluation Strategy: Per epoch Mixed Precision: FP32 Hardware: A100\nEvaluation\nResults\nTraining and Validation:\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\nF1 Score\nPrecision\nRecall\n1\n0.1143\n0.1014\n97.38%\n97.33%\n97.36%\n97.38%\n2\n0.0638\n0.0833\n97.78%\n97.79%\n97.83%\n97.78%\n3\n0.0391\n0.0946\n97.98%\n97.98%\n97.99%\n97.98%\n4\n0.0122\n0.1013\n98.04%\n98.04%\n98.05%\n98.04%\nTest Results:\nMetric\nValue\nLoss\n0.0814\nAccuracy\n98.37%\nF1 Score\n98.37%\nPrecision\n98.38%\nRecall\n98.37%",
    "bytedance-research/pasa-7b-selector": "PaSa: An LLM Agent for Comprehensive Academic Paper Search\nPaSa: An LLM Agent for Comprehensive Academic Paper Search\nYichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, Weinan E\npaper link: https://arxiv.org/abs/2501.10120\nThis model is described in the paper PaSa: An LLM Agent for Comprehensive Academic Paper Search.\ngithub: https://github.com/bytedance/pasa",
    "bytedance-research/pasa-7b-crawler": "PaSa: An LLM Agent for Comprehensive Academic Paper Search\nPaSa: An LLM Agent for Comprehensive Academic Paper Search\nYichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, Weinan E\npaper link: https://arxiv.org/abs/2501.10120\ngithub: https://github.com/bytedance/pasa",
    "HuggingFaceTB/SmolVLM-256M-Instruct": "SmolVLM-256M\nModel Summary\nResources\nUses\nTechnical Summary\nHow to get started\nModel optimizations\nMisuse and Out-of-scope Use\nLicense\nTraining Details\nTraining Data\nEvaluation\nCitation information\nSmolVLM-256M\nSmolVLM-256M is the smallest multimodal model in the world. It accepts arbitrary sequences of image and text inputs to produce text outputs. It's designed for efficiency. SmolVLM can answer questions about images, describe visual content, or transcribe text. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks. It can run inference on one image with under 1GB of GPU RAM.\nModel Summary\nDeveloped by: Hugging Face ü§ó\nModel type: Multi-modal model (image+text)\nLanguage(s) (NLP): English\nLicense: Apache 2.0\nArchitecture: Based on Idefics3 (see technical summary)\nResources\nDemo: SmolVLM-256 Demo\nBlog: Blog post\nUses\nSmolVLM can be used for inference on multimodal (image + text) tasks where the input comprises text queries along with one or more images. Text and images can be interleaved arbitrarily, enabling tasks like image captioning, visual question answering, and storytelling based on visual content. The model does not support image generation.\nTo fine-tune SmolVLM on a specific task, you can follow the fine-tuning tutorial.\nTechnical Summary\nSmolVLM leverages the lightweight SmolLM2 language model to provide a compact yet powerful multimodal experience. It introduces several changes compared to the larger SmolVLM 2.2B model:\nImage compression: We introduce a more radical image compression compared to Idefics3 and SmolVLM-2.2B to enable the model to infer faster and use less RAM.\nVisual Token Encoding: SmolVLM-256 uses 64 visual tokens to encode image patches of size 512√ó512. Larger images are divided into patches, each encoded separately, enhancing efficiency without compromising performance.\nNew special tokens: We added new special tokens to divide the subimages. This allows for more efficient tokenization of the images.\nSmoller vision encoder: We went from a 400M parameter siglip vision encoder to a much smaller 93M encoder.\nLarger image patches: We are now passing patches of 512x512 to the vision encoder, instead of 384x384 like the larger SmolVLM. This allows the information to be encoded more efficiently.\nMore details about the training and architecture are available in our technical report.\nHow to get started\nYou can use transformers to load, infer and fine-tune SmolVLM.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load images\nimage = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-256M-Instruct\",\ntorch_dtype=torch.bfloat16,\n_attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n).to(DEVICE)\n# Create input messages\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Can you describe this image?\"}\n]\n},\n]\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=500)\ngenerated_texts = processor.batch_decode(\ngenerated_ids,\nskip_special_tokens=True,\n)\nprint(generated_texts[0])\n\"\"\"\nAssistant: The image depicts a large, historic statue of liberty, located in New York City. The statue is a green, cylindrical structure with a human figure at the top, holding a torch. The statue is situated on a pedestal that resembles the statue of liberty, which is located on a small island in the middle of a body of water. The water surrounding the island is calm, reflecting the blue sky and the statue.\nIn the background, there are several tall buildings, including the Empire State Building, which is visible in the distance. These buildings are made of glass and steel, and they are positioned in a grid-like pattern, giving them a modern look. The sky is clear, with a few clouds visible, indicating fair weather.\nThe statue is surrounded by trees, which are green and appear to be healthy. There are also some small structures, possibly houses or buildings, visible in the distance. The overall scene suggests a peaceful and serene environment, typical of a cityscape.\nThe image is taken during the daytime, likely during the day of the statue's installation. The lighting is bright, casting a strong shadow on the statue and the water, which enhances the visibility of the statue and the surrounding environment.\nTo summarize, the image captures a significant historical statue of liberty, situated on a small island in the middle of a body of water, surrounded by trees and buildings. The sky is clear, with a few clouds visible, indicating fair weather. The statue is green and cylindrical, with a human figure holding a torch, and is surrounded by trees, indicating a peaceful and well-maintained environment. The overall scene is one of tranquility and historical significance.\n\"\"\"\nWe also provide ONNX weights for the model, which you can run with ONNX Runtime as follows:\nClick here to see the sample code\nfrom transformers import AutoConfig, AutoProcessor\nfrom transformers.image_utils import load_image\nimport onnxruntime\nimport numpy as np\n# 1. Load models\n## Load config and processor\nmodel_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\nconfig = AutoConfig.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n## Load sessions\n## !wget https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/vision_encoder.onnx\n## !wget https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/embed_tokens.onnx\n## !wget https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/decoder_model_merged.onnx\nvision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\")\nembed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\")\ndecoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\")\n## Set config values\nnum_key_value_heads = config.text_config.num_key_value_heads\nhead_dim = config.text_config.head_dim\nnum_hidden_layers = config.text_config.num_hidden_layers\neos_token_id = config.text_config.eos_token_id\nimage_token_id = config.image_token_id\n# 2. Prepare inputs\n## Create input messages\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Can you describe this image?\"}\n]\n},\n]\n## Load image and apply processor\nimage = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"np\")\n## Prepare decoder inputs\nbatch_size = inputs['input_ids'].shape[0]\npast_key_values = {\nf'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\nfor layer in range(num_hidden_layers)\nfor kv in ('key', 'value')\n}\nimage_features = None\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nposition_ids = np.cumsum(inputs['attention_mask'], axis=-1)\n# 3. Generation loop\nmax_new_tokens = 1024\ngenerated_tokens = np.array([[]], dtype=np.int64)\nfor i in range(max_new_tokens):\ninputs_embeds = embed_session.run(None, {'input_ids': input_ids})[0]\nif image_features is None:\n## Only compute vision features if not already computed\nimage_features = vision_session.run(\n['image_features'],  # List of output names or indices\n{\n'pixel_values': inputs['pixel_values'],\n'pixel_attention_mask': inputs['pixel_attention_mask'].astype(np.bool_)\n}\n)[0]\n## Merge text and vision embeddings\ninputs_embeds[inputs['input_ids'] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])\nlogits, *present_key_values = decoder_session.run(None, dict(\ninputs_embeds=inputs_embeds,\nattention_mask=attention_mask,\nposition_ids=position_ids,\n**past_key_values,\n))\n## Update values for next generation loop\ninput_ids = logits[:, -1].argmax(-1, keepdims=True)\nattention_mask = np.ones_like(input_ids)\nposition_ids = position_ids[:, -1:] + 1\nfor j, key in enumerate(past_key_values):\npast_key_values[key] = present_key_values[j]\ngenerated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\nif (input_ids == eos_token_id).all():\nbreak\n## (Optional) Streaming\nprint(processor.decode(input_ids[0]), end='')\nprint()\n# 4. Output result\nprint(processor.batch_decode(generated_tokens))\nExample output:\nThe image depicts a large, historic statue of Liberty situated on a small island in a body of water. The statue is a green, cylindrical structure with a human figure at the top, which is the actual statue of Liberty. The statue is mounted on a pedestal that is supported by a cylindrical tower. The pedestal is rectangular and appears to be made of stone or a similar material. The statue is surrounded by a large, flat, rectangular area that is likely a base for the statue.\nIn the background, there is a cityscape with a variety of buildings, including skyscrapers and high-rise buildings. The sky is clear with a gradient of colors, transitioning from a pale blue at the top to a deeper blue at the bottom. The buildings are mostly modern, with a mix of glass and concrete. The buildings are densely packed, with many skyscrapers and high-rise buildings visible.\nThere are trees and greenery visible on the left side of the image, indicating that the statue is located near a park or a park area. The water in the foreground is calm, with small ripples indicating that the statue is in the water.\nThe overall scene suggests a peaceful and serene environment, likely a public park or a park area in a city. The statue is likely a representation of liberty, representing the city's commitment to freedom and democracy.\n### Analysis and Description:\n#### Statue of Liberty:\n- **Location**: The statue is located on a small island in a body of water.\n- **Statue**: The statue is a green cylindrical structure with a human figure at the top, which is the actual statue of Liberty.\n- **Pedestal**: The pedestal is rectangular and supports the statue.\n- **Pedestrian**: The pedestal is surrounded by a flat rectangular area.\n- **Water**: The water is calm, with small ripples indicating that the statue is in the water.\n#### Cityscape:\n- **Buildings**: The buildings are modern, with a mix of glass and concrete.\n- **Sky**: The sky is clear with a gradient of colors, transitioning from a pale blue at the top to a deeper blue at the bottom.\n- **Trees**: There are trees and greenery visible on the left side of the image, indicating that the statue is located near a park or a park area.\n#### Environment:\n- **Water**: The water is calm, with small ripples indicating that the statue is in the water.\n- **Sky**: The sky is clear with a gradient of colors, transitioning from a pale blue at the top to a deeper blue at the bottom.\n### Conclusion:\nThe image depicts a peaceful and serene public park or park area in a city, with the statue of Liberty prominently featured. The cityscape in the background includes modern buildings and a clear sky, suggesting a well-maintained public space.<end_of_utterance>\nModel optimizations\nPrecision: For better performance, load and run the model in half-precision (torch.bfloat16) if your hardware supports it.\nfrom transformers import AutoModelForVision2Seq\nimport torch\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\ntorch_dtype=torch.bfloat16\n).to(\"cuda\")\nYou can also load SmolVLM with 4/8-bit quantization using bitsandbytes, torchao or Quanto. Refer to this page for other options.\nfrom transformers import AutoModelForVision2Seq, BitsAndBytesConfig\nimport torch\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\nquantization_config=quantization_config,\n)\nVision Encoder Efficiency: Adjust the image resolution by setting size={\"longest_edge\": N*512} when initializing the processor, where N is your desired value. The default N=4 works well, which results in input images of\nsize 2048√ó2048. Decreasing N can save GPU memory and is appropriate for lower-resolution images. This is also useful if you want to fine-tune on videos.\nMisuse and Out-of-scope Use\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual's well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\nProhibited Uses:\nEvaluating or scoring individuals (e.g., in employment, education, credit)\nCritical automated decision-making\nGenerating unreliable factual content\nMalicious Activities:\nSpam generation\nDisinformation campaigns\nHarassment or abuse\nUnauthorized surveillance\nLicense\nSmolVLM is built upon SigLIP as image encoder and SmolLM2 for text decoder part.\nWe release the SmolVLM checkpoints under the Apache 2.0 license.\nTraining Details\nTraining Data\nThe training data comes from The Cauldron and Docmatix datasets, with emphasis on document understanding (25%) and image captioning (18%), while maintaining balanced coverage across other crucial capabilities like visual reasoning, chart comprehension, and general instruction following.\nEvaluation\nSize\nMathvista\nMMMU\nOCRBench\nMMStar\nAI2D\nChartQA_Test\nScience_QA\nTextVQA Val\nDocVQA Val\n256M\n35.9\n28.3\n52.6\n34.6\n47\n55.8\n73.6\n49.9\n58.3\n500M\n40.1\n33.7\n61\n38.3\n59.5\n63.2\n79.7\n60.5\n70.5\n2.2B\n43.9\n38.3\n65.5\n41.8\n64\n71.6\n84.5\n72.1\n79.7\nCitation information\nYou can cite us in the following way:\n@article{marafioti2025smolvlm,\ntitle={SmolVLM: Redefining small and efficient multimodal models},\nauthor={Andr√©s Marafioti and Orr Zohar and Miquel Farr√© and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\njournal={arXiv preprint arXiv:2504.05299},\nyear={2025}\n}",
    "huihui-ai/internlm3-8b-instruct-abliterated": "huihui-ai/internlm3-8b-instruct-abliterated\nUse with ollama\nhuihui-ai/internlm3-8b-instruct-abliterated\nThis is an uncensored version of internlm/internlm3-8b-instruct created with abliteration (see remove-refusals-with-transformers to know more about it).This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.\nUse with ollama\nYou can use huihui_ai/internlm3-abliterated directly\nollama run huihui_ai/internlm3-abliterated",
    "foduucom/Watermark_Removal": "Watermark Removal Model\nModel Summary\nModel Details\nModel Description\nUsage Guide\nInstallation Requirements\nModel Loading and Inference\nLimitations and Considerations\nTraining Details\nModel Evaluation\nCompute Infrastructure\nHardware\nSoftware\nModel Card Contact\nWatermark Removal Model\nModel Summary\nThe Watermark Removal model is an image processing model based on neural networks. It is designed to remove watermarks from images while preserving the original image quality. The model utilizes an encoder-decoder structure with skip connections to maintain fine details during the watermark removal process.\nModel Details\nModel Description\nDeveloped by: FODUU AI\nModel type: Computer Vision - Image Processing\nTask: Remove watermark from image\nUsage Guide\nInstallation Requirements\npip install torch torchvision\npip install Pillow matplotlib numpy\nor you can run :\npip install -r requirements.txt\nModel Loading and Inference\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nfrom watermark_remover import WatermarkRemover\nimport numpy as np\nimage_path = \"path to your test image\"  # Replace with the path to your test image\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load the trained model\nmodel = WatermarkRemover().to(device)\nmodel_path = \"path to your model.pth\"  # Replace with the path to your saved model\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\ntransform = transforms.Compose([transforms.Resize((256, 256)),\ntransforms.ToTensor(),])\nwatermarked_image = Image.open(image_path).convert(\"RGB\")\noriginal_size = watermarked_image.size\ninput_tensor = transform(watermarked_image).unsqueeze(0).to(device)\nwith torch.no_grad():\noutput_tensor = model(input_tensor)\npredicted_image = output_tensor.squeeze(0).cpu().permute(1, 2, 0).clamp(0, 1).numpy()\npredicted_pil = Image.fromarray((predicted_image * 255).astype(np.uint8))\npredicted_pil = predicted_pil.resize(original_size, Image.Resampling.LANCZOS)\npredicted_pil.save(\"predicted_image.jpg\", quality=100)\nLimitations and Considerations\nPerformance may vary depending on watermark complexity and opacity\nBest results achieved with semi-transparent watermarks\nModel trained on 256x256 images; performance may vary with different resolutions\nGPU recommended for faster inference\nTraining Details\nDataset: The model was trained on a custom dataset consisting of 20,000 images with watermarks in various styles and intensities.\nTraining Time: The model was trained for 200 epochs on an NVIDIA GeForce RTX 3060 GPU.\nLoss Function: The model uses a combination of MSE (Mean Squared Error) and perceptual loss to optimize watermark removal quality.\nModel Evaluation\nThe model has been evaluated using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) on a test set of watermarked images, achieving an average PSNR of 30.5 dB and an SSIM of 0.92.\nCompute Infrastructure\nHardware\nNVIDIA GeForce RTX 3060 card\nSoftware\nThe model was trained on Jupyter Notebook environment.\nModel Card Contact\nFor inquiries and contributions, please contact us at info@foduu.com\n@ModelCard{\nauthor    = {Nehul Agrawal and\nPriyal Mehta},\ntitle     = {Watermark Removal Using Neural Networks},\nyear      = {2025}\n}",
    "Minthy/ToriiGate-v0.4-2B-exl2-8bpw": "8bpw exl2 quant for ToriiGate-v0.4-2B",
    "Alibaba-NLP/gte-modernbert-base": "gte-modernbert-base\nModel Overview\nModel list\nUsage\nTraining Details\nEvaluation\nMTEB\nLoCo (Long Document Retrieval)(NDCG@10)\nCOIR (Code Retrieval Task)(NDCG@10)\nBEIR(NDCG@10)\nHiring\nCitation\ngte-modernbert-base\nWe are excited to introduce the gte-modernbert series of models, which are built upon the latest modernBERT pre-trained encoder-only foundation models. The gte-modernbert series models include both text embedding models and rerank models.\nThe gte-modernbert models demonstrates competitive performance in several text embedding and text retrieval evaluation tasks when compared to similar-scale models from the current open-source community. This includes assessments such as MTEB, LoCO, and COIR evaluation.\nModel Overview\nDeveloped by: Tongyi Lab, Alibaba Group\nModel Type: Text Embedding\nPrimary Language: English\nModel Size: 149M\nMax Input Length: 8192 tokens\nOutput Dimension: 768\nModel list\nModels\nLanguage\nModel Type\nModel Size\nMax Seq. Length\nDimension\nMTEB-en\nBEIR\nLoCo\nCoIR\ngte-modernbert-base\nEnglish\ntext embedding\n149M\n8192\n768\n64.38\n55.33\n87.57\n79.31\ngte-reranker-modernbert-base\nEnglish\ntext reranker\n149M\n8192\n-\n-\n56.19\n90.68\n79.99\nUsage\nFor transformers and sentence-transformers, if your GPU supports it, the efficient Flash Attention 2 will be used automatically if you have flash_attn installed. It is not mandatory.\npip install flash_attn\nUse with transformers\n# Requires transformers>=4.48.0\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\ninput_texts = [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Beijing\",\n\"sorting algorithms\"\n]\nmodel_path = \"Alibaba-NLP/gte-modernbert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(model_path)\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = outputs.last_hidden_state[:, 0]\n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n# [[42.89073944091797, 71.30911254882812, 33.664554595947266]]\nUse with sentence-transformers:\n# Requires transformers>=4.48.0\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\ninput_texts = [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Beijing\",\n\"sorting algorithms\"\n]\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-modernbert-base\")\nembeddings = model.encode(input_texts)\nprint(embeddings.shape)\n# (4, 768)\nsimilarities = cos_sim(embeddings[0], embeddings[1:])\nprint(similarities)\n# tensor([[0.4289, 0.7131, 0.3366]])\nUse with transformers.js:\n// npm i @huggingface/transformers\nimport { pipeline, matmul } from \"@huggingface/transformers\";\n// Create a feature extraction pipeline\nconst extractor = await pipeline(\n\"feature-extraction\",\n\"Alibaba-NLP/gte-modernbert-base\",\n{ dtype: \"fp32\" }, // Supported options: \"fp32\", \"fp16\", \"q8\", \"q4\", \"q4f16\"\n);\n// Embed queries and documents\nconst embeddings = await extractor(\n[\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Beijing\",\n\"sorting algorithms\",\n],\n{ pooling: \"cls\", normalize: true },\n);\n// Compute similarity scores\nconst similarities = (await matmul(embeddings.slice([0, 1]), embeddings.slice([1, null]).transpose(1, 0))).mul(100);\nconsole.log(similarities.tolist()); // [[42.89077377319336, 71.30916595458984, 33.66455841064453]]\nAdditionally, you can also deploy Alibaba-NLP/gte-modernbert-base with Text Embeddings Inference (TEI) as follows:\nCPU\ndocker run --platform linux/amd64 \\\n-p 8080:80 \\\n-v $PWD/data:/data \\\n--pull always \\\nghcr.io/huggingface/text-embeddings-inference:cpu-1.7 \\\n--model-id Alibaba-NLP/gte-modernbert-base\nGPU\ndocker run --gpus all \\\n-p 8080:80 \\\n-v $PWD/data:/data \\\n--pull always \\\nghcr.io/huggingface/text-embeddings-inference:1.7 \\\n--model-id Alibaba-NLP/gte-modernbert-base\nThen you can send requests to the deployed API via the OpenAI-compatible v1/embeddings route (more information about the OpenAI Embeddings API):\ncurl https://0.0.0.0:8080/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"input\": [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Beijing\",\n\"sorting algorithms\"\n],\n\"model\": \"Alibaba-NLP/gte-modernbert-base\",\n\"encoding_format\": \"float\"\n}'\nTraining Details\nThe gte-modernbert series of models follows the training scheme of the previous GTE models, with the only difference being that the pre-training language model base has been replaced from GTE-MLM to ModernBert. For more training details, please refer to our paper: mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval\nEvaluation\nMTEB\nThe results of other models are retrieved from MTEB leaderboard. Given that all models in the gte-modernbert series have a size of less than 1B parameters, we focused exclusively on the results of models under 1B from the MTEB leaderboard.\nModel Name\nParam Size (M)\nDimension\nSequence Length\nAverage (56)\nClass. (12)\nClust. (11)\nPair Class. (3)\nReran. (4)\nRetr. (15)\nSTS (10)\nSumm. (1)\nmxbai-embed-large-v1\n335\n1024\n512\n64.68\n75.64\n46.71\n87.2\n60.11\n54.39\n85\n32.71\nmultilingual-e5-large-instruct\n560\n1024\n514\n64.41\n77.56\n47.1\n86.19\n58.58\n52.47\n84.78\n30.39\nbge-large-en-v1.5\n335\n1024\n512\n64.23\n75.97\n46.08\n87.12\n60.03\n54.29\n83.11\n31.61\ngte-base-en-v1.5\n137\n768\n8192\n64.11\n77.17\n46.82\n85.33\n57.66\n54.09\n81.97\n31.17\nbge-base-en-v1.5\n109\n768\n512\n63.55\n75.53\n45.77\n86.55\n58.86\n53.25\n82.4\n31.07\ngte-large-en-v1.5\n409\n1024\n8192\n65.39\n77.75\n47.95\n84.63\n58.50\n57.91\n81.43\n30.91\nmodernbert-embed-base\n149\n768\n8192\n62.62\n74.31\n44.98\n83.96\n56.42\n52.89\n81.78\n31.39\nnomic-embed-text-v1.5\n768\n8192\n62.28\n73.55\n43.93\n84.61\n55.78\n53.01\n81.94\n30.4\ngte-multilingual-base\n305\n768\n8192\n61.4\n70.89\n44.31\n84.24\n57.47\n51.08\n82.11\n30.58\njina-embeddings-v3\n572\n1024\n8192\n65.51\n82.58\n45.21\n84.01\n58.13\n53.88\n85.81\n29.71\ngte-modernbert-base\n149\n768\n8192\n64.38\n76.99\n46.47\n85.93\n59.24\n55.33\n81.57\n30.68\nLoCo (Long Document Retrieval)(NDCG@10)\nModel Name\nDimension\nSequence Length\nAverage (5)\nQsmsumRetrieval\nSummScreenRetrieval\nQasperAbastractRetrieval\nQasperTitleRetrieval\nGovReportRetrieval\ngte-qwen1.5-7b\n4096\n32768\n87.57\n49.37\n93.10\n99.67\n97.54\n98.21\ngte-large-v1.5\n1024\n8192\n86.71\n44.55\n92.61\n99.82\n97.81\n98.74\ngte-base-v1.5\n768\n8192\n87.44\n49.91\n91.78\n99.82\n97.13\n98.58\ngte-modernbert-base\n768\n8192\n88.88\n54.45\n93.00\n99.82\n98.03\n98.70\ngte-reranker-modernbert-base\n-\n8192\n90.68\n70.86\n94.06\n99.73\n99.11\n89.67\nCOIR (Code Retrieval Task)(NDCG@10)\nModel Name\nDimension\nSequence Length\nAverage(20)\nCodeSearchNet-ccr-go\nCodeSearchNet-ccr-java\nCodeSearchNet-ccr-javascript\nCodeSearchNet-ccr-php\nCodeSearchNet-ccr-python\nCodeSearchNet-ccr-ruby\nCodeSearchNet-go\nCodeSearchNet-java\nCodeSearchNet-javascript\nCodeSearchNet-php\nCodeSearchNet-python\nCodeSearchNet-ruby\napps\ncodefeedback-mt\ncodefeedback-st\ncodetrans-contest\ncodetrans-dl\ncosqa\nstackoverflow-qa\nsynthetic-text2sql\ngte-modernbert-base\n768\n8192\n79.31\n94.15\n93.57\n94.27\n91.51\n93.93\n90.63\n88.32\n83.27\n76.05\n85.12\n88.16\n77.59\n57.54\n82.34\n85.95\n71.89\n35.46\n43.47\n91.2\n61.87\ngte-reranker-modernbert-base\n-\n8192\n79.99\n96.43\n96.88\n98.32\n91.81\n97.7\n91.96\n88.81\n79.71\n76.27\n89.39\n98.37\n84.11\n47.57\n83.37\n88.91\n49.66\n36.36\n44.37\n89.58\n64.21\nBEIR(NDCG@10)\nModel Name\nDimension\nSequence Length\nAverage(15)\nArguAna\nClimateFEVER\nCQADupstackAndroidRetrieval\nDBPedia\nFEVER\nFiQA2018\nHotpotQA\nMSMARCO\nNFCorpus\nNQ\nQuoraRetrieval\nSCIDOCS\nSciFact\nTouche2020\nTRECCOVID\ngte-modernbert-base\n768\n8192\n55.33\n72.68\n37.74\n42.63\n41.79\n91.03\n48.81\n69.47\n40.9\n36.44\n57.62\n88.55\n21.29\n77.4\n21.68\n81.95\ngte-reranker-modernbert-base\n-\n8192\n56.73\n69.03\n37.79\n44.68\n47.23\n94.54\n49.81\n78.16\n45.38\n30.69\n64.57\n87.77\n20.60\n73.57\n27.36\n79.89\nHiring\nWe have open positions for Research Interns and Full-Time Researchers to join our team at Tongyi Lab.\nWe are seeking passionate individuals with expertise in representation learning, LLM-driven information retrieval, Retrieval-Augmented Generation (RAG), and agent-based systems.\nOur team is located in the vibrant cities of Beijing and Hangzhou.\nIf you are driven by curiosity and eager to make a meaningful impact through your work, we would love to hear from you. Please submit your resume along with a brief introduction to dingkun.ldk@alibaba-inc.com.\nCitation\nIf you find our paper or models helpful, feel free to give us a cite.\n@inproceedings{zhang2024mgte,\ntitle={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\nauthor={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\nbooktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\npages={1393--1412},\nyear={2024}\n}\n@article{li2023towards,\ntitle={Towards general text embeddings with multi-stage contrastive learning},\nauthor={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\njournal={arXiv preprint arXiv:2308.03281},\nyear={2023}\n}",
    "deepseek-ai/DeepSeek-R1-Zero": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "hotchpotch/static-embedding-japanese": "static-embedding-japanese\nÂà©Áî®ÊñπÊ≥ï\nÂá∫ÂäõÊ¨°ÂÖÉ„ÇíÂ∞è„Åï„Åè„Åô„Çã\n„Å™„ÅúCPU„ÅßÊé®Ë´ñ„ÅåÈ´òÈÄü„Å™„ÅÆÔºü\nË©ï‰æ°ÁµêÊûú\nÊÉÖÂ†±Ê§úÁ¥¢„ÅßBM25„ÅÆÁΩÆ„ÅçÊèõ„Åà„Åå„Åß„Åç„Åù„ÅÜ„Åã?\n„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÁµêÊûú„ÅåÊÇ™„ÅÑ\nJQaRA, JaCWIR „Åß„ÅÆ„É™„É©„É≥„Ç≠„É≥„Ç∞„Çø„Çπ„ÇØË©ï‰æ°\nÂá∫ÂäõÊ¨°ÂÖÉ„ÅÆÂâäÊ∏õ\nStaticEmbdding Êó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÅÆÊ¨°ÂÖÉÂâäÊ∏õÁµêÊûú\nStaticEmbedding „É¢„Éá„É´„Çí‰Ωú„Å£„Å¶„Åø„Å¶\nStaticEmbedding Êó•Êú¨Ë™û„É¢„Éá„É´Â≠¶Áøí„ÅÆ„ÉÜ„ÇØ„Éã„Ç´„É´„Éé„Éº„Éà\n„Å™„Åú„ÅÜ„Åæ„ÅèÂ≠¶Áøí„Åß„Åç„Çã„ÅÆ„Åã\nÂ≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„Éà\nÊó•Êú¨Ë™û„Éà„Éº„ÇØ„Éä„Ç§„Ç∂\n„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø\nÂ≠¶Áøí„É™„ÇΩ„Éº„Çπ\n„Åï„Çâ„Å™„ÇãÊÄßËÉΩÂêë‰∏ä„Å∏\nÂ§ßÂÖÉ„ÅÆÂ≠¶Áøí„Ç≥„Éº„Éâ\n„É©„Ç§„Çª„É≥„Çπ\n‰ª•‰∏ã„ÅÆÊñáÁ´†„ÅØ„ÄÅË®ò‰∫ã„ÄÄ100ÂÄçÈÄü„ÅßÂÆüÁî®ÁöÑ„Å™ÊñáÁ´†„Éô„ÇØ„Éà„É´„Çí‰Ωú„Çå„Çã„ÄÅÊó•Êú¨Ë™û StaticEmbedding „É¢„Éá„É´„ÇíÂÖ¨Èñã „Åã„Çâ„ÅÆËª¢Ëºâ„Åß„Åô„ÄÇ\nstatic-embedding-japanese\nÊñáÁ´†„ÅÆÂØÜ„Éô„ÇØ„Éà„É´„ÅØ„ÄÅÊÉÖÂ†±Ê§úÁ¥¢„ÉªÊñáÁ´†Âà§Âà•„ÉªÈ°û‰ººÊñáÁ´†ÊäΩÂá∫„Å™„Å©„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™Áî®ÈÄî„Å´‰Ωø„ÅÜ„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ„Åó„Åã„Åó„Å™„Åå„ÇâÊúÄÂÖàÁ´Ø„ÅÆTransformer„É¢„Éá„É´„ÅØÂ∞è„Åï„ÅÑ„É¢„Éá„É´„Åß„ÇÇ„ÄÅ„Å®„Çä„Çè„ÅëCPUÁí∞Â¢É„Åß„ÅØÂá¶ÁêÜÈÄüÂ∫¶„ÅåÈÅÖ„ÅÑ„Åü„ÇÅÂÆüÁî®„Åß„Å™„ÅÑ„Åì„Å®„ÇÇ„Åó„Å∞„Åó„Å∞„ÅÇ„Çä„Åæ„Åô„ÄÇ\n„Åì„ÅÆË™≤È°å„ÇíËß£Ê±∫„Åô„ÇãÊñ∞„Åó„ÅÑ„Ç¢„Éó„É≠„Éº„ÉÅ„Å®„Åó„Å¶„ÄÅÂÖàÊó•ÂÖ¨Èñã„Åï„Çå„ÅüTransformer„É¢„Éá„É´„Äå„Åß„ÅØ„Å™„ÅÑ„Äç StaticEmbedding„É¢„Éá„É´„ÅØ„ÄÅ‰æã„Åà„Å∞ intfloat/multilingual-e5-small (‰ª•‰∏ãmE5-small)„Å®„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÊØîËºÉ„Åß„ÅØ85%„ÅÆ„Çπ„Ç≥„Ç¢„Å®„ÅÑ„ÅÜÊúÄ‰ΩéÂçÅÂàÜ„Å™ÊÄßËÉΩ„Åß„ÄÅ‰Ωï„Çà„ÇäCPU„ÅßÂãï‰ΩúÊôÇ„Å´126ÂÄçÈ´òÈÄü„Å´Êñá„Éô„ÇØ„Éà„É´„Çí‰ΩúÊàê„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Çã„ÄÅ„Å®„ÅÑ„ÅÜÈ©ö„Åç„ÅÆÈÄüÂ∫¶„Åß„Åô„ÄÇ\n„Å®„ÅÑ„ÅÜ„Çè„Åë„Åß„ÄÅÊó©ÈÄüÊó•Êú¨Ë™û(„Å®Ëã±Ë™û)„ÅßÂ≠¶Áøí„Åï„Åõ„Åü„É¢„Éá„É´ sentence-embedding-japanese „Çí‰ΩúÊàê„Åó„ÄÅÂÖ¨Èñã„Åó„Åæ„Åó„Åü„ÄÇ\nhttps://huggingface.co/hotchpotch/static-embedding-japanese\nÊó•Êú¨Ë™û„ÅÆÊñáÁ´†„Éô„ÇØ„Éà„É´„ÅÆÊÄßËÉΩ„ÇíË©ï‰æ°„Åô„Çã JMTEB „ÅÆÁµêÊûú„ÅØ‰ª•‰∏ã„Åß„Åô„ÄÇÁ∑èÂêà„Çπ„Ç≥„Ç¢„Åß„ÅØ mE5-small „Å´„ÅØËã•Âπ≤Âèä„Å∞„Å™„ÅÑ„Åæ„Åß„ÇÇ„ÄÅ„Çø„Çπ„ÇØ„Å´„Çà„Å£„Å¶„ÅØÂãù„Å£„Å¶„ÅÑ„Åü„Çä„Åó„Åæ„Åô„Åó„ÄÅ‰ªñ„ÅÆÊó•Êú¨Ë™ûbase„Çµ„Ç§„Ç∫bert„É¢„Éá„É´„Çà„Çä„ÇÇ„Çπ„Ç≥„Ç¢„ÅåÈ´ò„ÅÑ„Åì„Å®„ÇÇ„ÅÇ„Çã„Åê„Çâ„ÅÑ„ÄÅÊúÄ‰ΩéÈôêÂÆüÁî®„Åß„Åç„Åù„ÅÜ„Å™ÊÄßËÉΩ„ÅåÂá∫„Å¶„ÅÑ„Åæ„Åô„Å≠„ÄÇÊú¨ÂΩì„Å´„Åù„Çì„Å™„Å´ÊÄßËÉΩ„ÅåÂá∫„Çã„ÅÆ„ÅãÂÆüÈöõ„Å´Â≠¶Áøí„Åï„Åõ„Å¶„Åø„Çã„Åæ„Åß„ÅØÂçä‰ø°ÂçäÁñë„Åß„Åó„Åü„Åå„ÄÅÈ©ö„Åç„Åß„Åô„ÄÇ\nModel\nAvg(micro)\nRetrieval\nSTS\nClassification\nReranking\nClustering\nPairClassification\ntext-embedding-3-small\n69.18\n66.39\n79.46\n73.06\n92.92\n51.06\n62.27\nmultilingual-e5-small\n67.71\n67.27\n80.07\n67.62\n93.03\n46.91\n62.19\nstatic-embedding-japanese\n67.17\n67.92\n80.16\n67.96\n91.87\n40.39\n62.37\n„Å™„Åä„ÄÅStaticEmbedding Êó•Êú¨Ë™û„É¢„Éá„É´Â≠¶Áøí„Å™„Å©„ÅÆÊäÄË°ìÁöÑ„Å™„Åì„Å®„ÅØË®ò‰∫ã„ÅÆÂæåÂçä„Å´Êõ∏„ÅÑ„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅËààÂë≥„Åå„ÅÇ„ÇãÊñπ„ÅØ„Å©„ÅÜ„Åû„ÄÇ\nÂà©Áî®ÊñπÊ≥ï\nÂà©Áî®„ÅØÁ∞°Âçò„ÄÅSentenceTransformer „Çí‰Ωø„Å£„Å¶„ÅÑ„Å§„ÇÇ„ÅÆÊñπÊ≥ï„ÅßÊñáÁ´†„Éô„ÇØ„Éà„É´„Çí‰Ωú„Çå„Åæ„Åô„ÄÇ‰ªäÂõû„ÅØGPU„Çí‰Ωø„Çè„Åö„ÄÅCPU„ÅßÂÆüË°å„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ„Å™„Åä SentenceTransformer „ÅØ 3.3.1 „ÅßË©¶„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\npip install \"sentence-transformers>=3.3.1\"\nfrom sentence_transformers import SentenceTransformer\nmodel_name = \"hotchpotch/static-embedding-japanese\"\nmodel = SentenceTransformer(model_name, device=\"cpu\")\nquery = \"ÁæéÂë≥„Åó„ÅÑ„É©„Éº„É°„É≥Â±ã„Å´Ë°å„Åç„Åü„ÅÑ\"\ndocs = [\n\"Á¥†Êïµ„Å™„Ç´„Éï„Çß„ÅåËøëÊâÄ„Å´„ÅÇ„Çã„Çà„ÄÇËêΩ„Å°ÁùÄ„ÅÑ„ÅüÈõ∞Âõ≤Ê∞ó„Åß„ÇÜ„Å£„Åè„Çä„Åß„Åç„Çã„Åó„ÄÅÁ™ìÈöõ„ÅÆÂ∏≠„Åã„Çâ„ÅØÂÖ¨Âúí„ÅÆÊôØËâ≤„ÇÇË¶ã„Åà„Çã„Çì„Å†„ÄÇ\",\n\"Êñ∞ÈÆÆ„Å™È≠ö‰ªã„ÇíÊèê‰æõ„Åô„ÇãÂ∫ó„Åß„Åô„ÄÇÂú∞ÂÖÉ„ÅÆÊºÅÂ∏´„Åã„ÇâÁõ¥Êé•‰ªïÂÖ•„Çå„Å¶„ÅÑ„Çã„ÅÆ„ÅßÈÆÆÂ∫¶„ÅØÊäúÁæ§„Åß„Åô„Åó„ÄÅÊñôÁêÜ‰∫∫„ÅÆËÖï„ÇÇÁ¢∫„Åã„Åß„Åô„ÄÇ\",\n\"„ÅÇ„Åù„Åì„ÅØË°å„Åç„Å´„Åè„ÅÑ„Åë„Å©„ÄÅÈö†„Çå„ÅüË±öÈ™®„ÅÆÂêçÂ∫ó„Å†„Çà„ÄÇ„Çπ„Éº„Éó„ÅåÊúÄÈ´ò„Å†„Åó„ÄÅÈ∫∫„ÅÆÁ°¨„Åï„ÇÇÂ•Ω„Åø„ÄÇ\",\n\"„Åä„Åô„Åô„ÇÅ„ÅÆ‰∏≠ËèØ„Åù„Å∞„ÅÆÂ∫ó„ÇíÊïô„Åà„Å¶„ÅÇ„Åí„Çã„ÄÇ„Å®„Çä„Çè„Åë„ÉÅ„É£„Éº„Ç∑„É•„Éº„ÅåÊâã‰Ωú„Çä„ÅßÊüî„Çâ„Åã„Åè„Å¶„Ç∏„É•„Éº„Ç∑„Éº„Å™„Çì„Å†„ÄÇ\",\n]\nembeddings = model.encode([query] + docs)\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings[0], embeddings[1:])\nfor i, similarity in enumerate(similarities[0].tolist()):\nprint(f\"{similarity:.04f}: {docs[i]}\")\n(5, 1024)\n0.1040: Á¥†Êïµ„Å™„Ç´„Éï„Çß„ÅåËøëÊâÄ„Å´„ÅÇ„Çã„Çà„ÄÇËêΩ„Å°ÁùÄ„ÅÑ„ÅüÈõ∞Âõ≤Ê∞ó„Åß„ÇÜ„Å£„Åè„Çä„Åß„Åç„Çã„Åó„ÄÅÁ™ìÈöõ„ÅÆÂ∏≠„Åã„Çâ„ÅØÂÖ¨Âúí„ÅÆÊôØËâ≤„ÇÇË¶ã„Åà„Çã„Çì„Å†„ÄÇ\n0.2521: Êñ∞ÈÆÆ„Å™È≠ö‰ªã„ÇíÊèê‰æõ„Åô„ÇãÂ∫ó„Åß„Åô„ÄÇÂú∞ÂÖÉ„ÅÆÊºÅÂ∏´„Åã„ÇâÁõ¥Êé•‰ªïÂÖ•„Çå„Å¶„ÅÑ„Çã„ÅÆ„ÅßÈÆÆÂ∫¶„ÅØÊäúÁæ§„Åß„Åô„Åó„ÄÅÊñôÁêÜ‰∫∫„ÅÆËÖï„ÇÇÁ¢∫„Åã„Åß„Åô„ÄÇ\n0.4835: „ÅÇ„Åù„Åì„ÅØË°å„Åç„Å´„Åè„ÅÑ„Åë„Å©„ÄÅÈö†„Çå„ÅüË±öÈ™®„ÅÆÂêçÂ∫ó„Å†„Çà„ÄÇ„Çπ„Éº„Éó„ÅåÊúÄÈ´ò„Å†„Åó„ÄÅÈ∫∫„ÅÆÁ°¨„Åï„ÇÇÂ•Ω„Åø„ÄÇ\n0.3199: „Åä„Åô„Åô„ÇÅ„ÅÆ‰∏≠ËèØ„Åù„Å∞„ÅÆÂ∫ó„ÇíÊïô„Åà„Å¶„ÅÇ„Åí„Çã„ÄÇ„Å®„Çä„Çè„Åë„ÉÅ„É£„Éº„Ç∑„É•„Éº„ÅåÊâã‰Ωú„Çä„ÅßÊüî„Çâ„Åã„Åè„Å¶„Ç∏„É•„Éº„Ç∑„Éº„Å™„Çì„Å†„ÄÇ\n„Åì„ÅÆ„Çà„ÅÜ„Å´„ÄÅquery„Å´„Éû„ÉÉ„ÉÅ„Åô„ÇãÊñáÁ´†„ÅÆ„Çπ„Ç≥„Ç¢„ÅåÈ´ò„Åè„Å™„Çã„Çà„ÅÜ„Å´Ë®àÁÆó„Åß„Åç„Å¶„Åæ„Åô„Å≠„ÄÇ„Åì„ÅÆ‰æãÊñá„Åß„ÅØ„ÄÅ‰æã„Åà„Å∞BM25„Åß„ÅØquery„Å´Âê´„Åæ„Çå„Çã„Äå„É©„Éº„É°„É≥„Äç„ÅÆ„Çà„ÅÜ„Å™Áõ¥Êé•ÁöÑ„Å™ÂçòË™û„ÅåÊñáÁ´†„Å´Âá∫„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„ÅÜ„Åæ„Åè„Éû„ÉÉ„ÉÅ„Åï„Åõ„Çã„Åì„Å®„ÅåÈõ£„Åó„ÅÑ„Åß„Åó„Çá„ÅÜ„ÄÇ\nÁ∂ö„ÅÑ„Å¶„ÄÅÈ°û‰ººÊñáÁ´†„Çø„Çπ„ÇØ„ÅÆ‰æã„Åß„Åô„ÄÇ\nsentences = [\n\"ÊòéÊó•„ÅÆÂçàÂæå„Åã„ÇâÈõ®„ÅåÈôç„Çã„Åø„Åü„ÅÑ„Åß„Åô„ÄÇ\",\n\"Êù•ÈÄ±„ÅÆÊó•ÊõúÊó•„ÅØÂ§©Ê∞ó„ÅåËâØ„ÅÑ„Åù„ÅÜ„Å†„ÄÇ\",\n\"„ÅÇ„Åó„Åü„ÅÆÊòºÈÅé„Åé„Åã„ÇâÂÇò„ÅåÂøÖË¶Å„Å´„Å™„Çä„Åù„ÅÜ„ÄÇ\",\n\"ÈÄ±Êú´„ÅØÊô¥„Çå„Çã„Å®„ÅÑ„ÅÜ‰∫àÂ†±„ÅåÂá∫„Å¶„ÅÑ„Åæ„Åô„ÄÇ\",\n]\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# ‰∏Ä„Å§ÁõÆ„ÅÆÊñáÁ´†„Å®„ÄÅ„Åù„ÅÆ‰ªñ„ÅÆÊñáÁ´†„ÅÆÈ°û‰ººÂ∫¶„ÇíË°®Á§∫\nfor i, similarity in enumerate(similarities[0].tolist()):\nprint(f\"{similarity:.04f}: {sentences[i]}\")\ntensor([[1.0000, 0.2814, 0.3620, 0.2818],\n[0.2814, 1.0000, 0.2007, 0.5372],\n[0.3620, 0.2007, 1.0000, 0.1299],\n[0.2818, 0.5372, 0.1299, 1.0000]])\n1.0000: ÊòéÊó•„ÅÆÂçàÂæå„Åã„ÇâÈõ®„ÅåÈôç„Çã„Åø„Åü„ÅÑ„Åß„Åô„ÄÇ\n0.2814: Êù•ÈÄ±„ÅÆÊó•ÊõúÊó•„ÅØÂ§©Ê∞ó„ÅåËâØ„ÅÑ„Åù„ÅÜ„Å†„ÄÇ\n0.3620: „ÅÇ„Åó„Åü„ÅÆÊòºÈÅé„Åé„Åã„ÇâÂÇò„ÅåÂøÖË¶Å„Å´„Å™„Çä„Åù„ÅÜ„ÄÇ\n0.2818: ÈÄ±Êú´„ÅØÊô¥„Çå„Çã„Å®„ÅÑ„ÅÜ‰∫àÂ†±„ÅåÂá∫„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„Åì„Å°„Çâ„ÇÇ„ÄÅÈ°û‰ººÊñáÁ´†„ÅåÈ´ò„Çπ„Ç≥„Ç¢„Å´„Å™„ÇãÁµêÊûú„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ\n„Åæ„ÅüTransformer„É¢„Éá„É´„ÇíÂà©Áî®„Åó„Å¶CPU„ÅßÊñáÁ´†„Éô„ÇØ„Éà„É´„Çí‰Ωú„Å£„ÅüÂ†¥Âêà„ÄÅÂ∞ë„Å™„ÅÑÊñáÁ´†Èáè„Åß„ÇÇ„Å†„ÅÑ„Å∂ÊôÇÈñì„Åå„Åã„Åã„ÄÅ„Å®„ÅÑ„ÅÜÁµåÈ®ì„Çí„Åï„Çå„ÅüÊñπ„ÇÇÂ§ö„ÅÑ„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇStaticEmbedding „É¢„Éá„É´„Åß„ÅØCPU„Åå„Åù„Åì„Åù„ÅìÈÄü„Åë„Çå„Å∞‰∏ÄÁû¨„ÅßÁµÇ„Çè„Çã„ÅØ„Åö„ÄÇ„Åï„Åô„Åå100ÂÄçÈÄü„ÄÇ\nÂá∫ÂäõÊ¨°ÂÖÉ„ÇíÂ∞è„Åï„Åè„Åô„Çã\nÊ®ôÊ∫ñ„Åß‰Ωú„Çâ„Çå„ÇãÊñá„Éô„ÇØ„Éà„É´„ÅÆÊ¨°ÂÖÉ„ÅØ1024„Åß„Åô„Åå„ÄÅ„Åì„Çå„Çí„Åï„Çâ„Å´Â∞è„Åï„ÅèÊ¨°ÂÖÉÂâäÊ∏õ„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ‰æã„Åà„Å∞ 128 „ÇíÊåáÂÆö„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ\n# truncate_dim „ÅØ 32, 64, 128, 256, 512, 1024 „Åã„ÇâÊåáÂÆö\nmodel = SentenceTransformer(model_name, device=\"cpu\", truncate_dim=128)\nquery = \"ÁæéÂë≥„Åó„ÅÑ„É©„Éº„É°„É≥Â±ã„Å´Ë°å„Åç„Åü„ÅÑ\"\ndocs = [\n\"Á¥†Êïµ„Å™„Ç´„Éï„Çß„ÅåËøëÊâÄ„Å´„ÅÇ„Çã„Çà„ÄÇËêΩ„Å°ÁùÄ„ÅÑ„ÅüÈõ∞Âõ≤Ê∞ó„Åß„ÇÜ„Å£„Åè„Çä„Åß„Åç„Çã„Åó„ÄÅÁ™ìÈöõ„ÅÆÂ∏≠„Åã„Çâ„ÅØÂÖ¨Âúí„ÅÆÊôØËâ≤„ÇÇË¶ã„Åà„Çã„Çì„Å†„ÄÇ\",\n\"Êñ∞ÈÆÆ„Å™È≠ö‰ªã„ÇíÊèê‰æõ„Åô„ÇãÂ∫ó„Åß„Åô„ÄÇÂú∞ÂÖÉ„ÅÆÊºÅÂ∏´„Åã„ÇâÁõ¥Êé•‰ªïÂÖ•„Çå„Å¶„ÅÑ„Çã„ÅÆ„ÅßÈÆÆÂ∫¶„ÅØÊäúÁæ§„Åß„Åô„Åó„ÄÅÊñôÁêÜ‰∫∫„ÅÆËÖï„ÇÇÁ¢∫„Åã„Åß„Åô„ÄÇ\",\n\"„ÅÇ„Åù„Åì„ÅØË°å„Åç„Å´„Åè„ÅÑ„Åë„Å©„ÄÅÈö†„Çå„ÅüË±öÈ™®„ÅÆÂêçÂ∫ó„Å†„Çà„ÄÇ„Çπ„Éº„Éó„ÅåÊúÄÈ´ò„Å†„Åó„ÄÅÈ∫∫„ÅÆÁ°¨„Åï„ÇÇÂ•Ω„Åø„ÄÇ\",\n\"„Åä„Åô„Åô„ÇÅ„ÅÆ‰∏≠ËèØ„Åù„Å∞„ÅÆÂ∫ó„ÇíÊïô„Åà„Å¶„ÅÇ„Åí„Çã„ÄÇ„Å®„Çä„Çè„Åë„ÉÅ„É£„Éº„Ç∑„É•„Éº„ÅåÊâã‰Ωú„Çä„ÅßÊüî„Çâ„Åã„Åè„Å¶„Ç∏„É•„Éº„Ç∑„Éº„Å™„Çì„Å†„ÄÇ\",\n]\nembeddings = model.encode([query] + docs)\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings[0], embeddings[1:])\nfor i, similarity in enumerate(similarities[0].tolist()):\nprint(f\"{similarity:.04f}: {docs[i]}\")\n(5, 128)\n0.1464: Á¥†Êïµ„Å™„Ç´„Éï„Çß„ÅåËøëÊâÄ„Å´„ÅÇ„Çã„Çà„ÄÇËêΩ„Å°ÁùÄ„ÅÑ„ÅüÈõ∞Âõ≤Ê∞ó„Åß„ÇÜ„Å£„Åè„Çä„Åß„Åç„Çã„Åó„ÄÅÁ™ìÈöõ„ÅÆÂ∏≠„Åã„Çâ„ÅØÂÖ¨Âúí„ÅÆÊôØËâ≤„ÇÇË¶ã„Åà„Çã„Çì„Å†„ÄÇ\n0.3094: Êñ∞ÈÆÆ„Å™È≠ö‰ªã„ÇíÊèê‰æõ„Åô„ÇãÂ∫ó„Åß„Åô„ÄÇÂú∞ÂÖÉ„ÅÆÊºÅÂ∏´„Åã„ÇâÁõ¥Êé•‰ªïÂÖ•„Çå„Å¶„ÅÑ„Çã„ÅÆ„ÅßÈÆÆÂ∫¶„ÅØÊäúÁæ§„Åß„Åô„Åó„ÄÅÊñôÁêÜ‰∫∫„ÅÆËÖï„ÇÇÁ¢∫„Åã„Åß„Åô„ÄÇ\n0.5923: „ÅÇ„Åù„Åì„ÅØË°å„Åç„Å´„Åè„ÅÑ„Åë„Å©„ÄÅÈö†„Çå„ÅüË±öÈ™®„ÅÆÂêçÂ∫ó„Å†„Çà„ÄÇ„Çπ„Éº„Éó„ÅåÊúÄÈ´ò„Å†„Åó„ÄÅÈ∫∫„ÅÆÁ°¨„Åï„ÇÇÂ•Ω„Åø„ÄÇ\n0.3405: „Åä„Åô„Åô„ÇÅ„ÅÆ‰∏≠ËèØ„Åù„Å∞„ÅÆÂ∫ó„ÇíÊïô„Åà„Å¶„ÅÇ„Åí„Çã„ÄÇ„Å®„Çä„Çè„Åë„ÉÅ„É£„Éº„Ç∑„É•„Éº„ÅåÊâã‰Ωú„Çä„ÅßÊüî„Çâ„Åã„Åè„Å¶„Ç∏„É•„Éº„Ç∑„Éº„Å™„Çì„Å†„ÄÇ\n128Ê¨°ÂÖÉ„ÅÆ„Éô„ÇØ„Éà„É´„Å´„Å™„Çä„ÄÅÁµêÊûú„ÅÆ„Çπ„Ç≥„Ç¢„ÇÇËã•Âπ≤Â§â„Çè„Çä„Åæ„Åó„Åü„Å≠„ÄÇÊ¨°ÂÖÉ„ÅåÂ∞è„Åï„Åè„Å™„Å£„Åü„Åì„Å®„Åß„ÄÅÊÄßËÉΩ„ÅåÂ∞ë„ÄÖÂä£Âåñ„Åó„Å¶„ÅÑ„Åæ„Åô(ÂæåÂçä„Å´„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇíË®òËºâ)„ÄÇ„Åü„Å†1024Ê¨°ÂÖÉ„Åã„Çâ128Ê¨°ÂÖÉ„Å´Ê∏õ„Çã„Åì„Å®„Åß„ÄÅ‰øùÂ≠ò„Åô„Çã„Çπ„Éà„É¨„Éº„Ç∏„Çµ„Ç§„Ç∫„ÅåÊ∏õ„Å£„Åü„Çä„ÄÅÊ§úÁ¥¢ÊôÇ„Å™„Å©„Å´Âà©Áî®„Åô„ÇãÈ°û‰ººÂ∫¶Ë®àÁÆó„Ç≥„Çπ„Éà„ÅåÁ¥Ñ8ÂÄçÈÄü„Å´„Å™„Å£„Åü„Çä„Å®„Å™„Å£„Åü„Çä„Å®„ÄÅÁî®ÈÄî„Å´„Çà„Å£„Å¶„ÅØÂ∞è„Åï„ÅÑÊ¨°ÂÖÉ„ÅÆÊñπ„ÅåÂ¨â„Åó„ÅÑ„Åì„Å®„ÇÇÂ§ö„ÅÑ„Åß„Åó„Çá„ÅÜ„ÄÇ\n„Å™„ÅúCPU„ÅßÊé®Ë´ñ„ÅåÈ´òÈÄü„Å™„ÅÆÔºü\nStaticEmbedding „ÅØTransformer„É¢„Éá„É´„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Å§„Åæ„ÇäTrasformer„ÅÆÁâπÂæ¥„Åß„ÅÇ„Çã \"Attention Is All You Need\" „Å™„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆË®àÁÆó„Åå‰∏ÄÂàá„Å™„ÅÑ„ÅÆ„Åß„Åô„ÄÇÊñáÁ´†„Å´Âá∫„Å¶„Åè„ÇãÂçòË™û„Éà„Éº„ÇØ„É≥„Çí1024Ê¨°ÂÖÉ„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´‰øùÂ≠ò„Åó„Å¶„ÄÅÊñá„Éô„ÇØ„Éà„É´‰ΩúÊàêÊôÇ„Å´„ÅØ„Åù„Çå„ÅÆÂπ≥Âùá„Çí„Å®„Å£„Å¶„ÅÑ„Çã„Å†„Åë„Åß„Åô„ÄÇ„Å™„Åä„ÄÅ„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Åå„Å™„ÅÑ„ÅÆ„Åß„ÄÅÊñáËÑà„ÅÆÁêÜËß£„Å™„Å©„ÅØ„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ\n„Åæ„ÅüÂÜÖÈÉ®ÂÆüË£Ö„Åß„ÅØ PyTorch „ÅÆ nn.EmbeddingBag „Çí‰Ωø„Å£„Å¶„ÄÅÂÖ®„Å¶„ÇíÈÄ£Áµê„Åó„Åü„Éà„Éº„ÇØ„É≥„Å®„Ç™„Éï„Çª„ÉÉ„Éà„ÇíÊ∏°„Åó„Å¶Âá¶ÁêÜ„Åô„Çã„Åì„Å®„Åß„ÄÅPyTorch „ÅÆÊúÄÈÅ©Âåñ„ÅßÈ´òÈÄü„Å™CPU‰∏¶ÂàóÂá¶ÁêÜ„Å®„É°„É¢„É™„Ç¢„ÇØ„Çª„Çπ„Åå„Åï„Çå„Å¶„ÅÑ„Çã„Çà„ÅÜ„Åß„Åô„ÄÇ\nÂÖÉË®ò‰∫ã„ÅÆÈÄüÂ∫¶Ë©ï‰æ°ÁµêÊûú„Å´„Çà„Çã„Å®CPU„Åß„ÅØmE5-small„Å®ÊØî„Åπ„Å¶126ÂÄçÈÄü„Çâ„Åó„ÅÑ„Åß„Åô„Å≠„ÄÇ\nË©ï‰æ°ÁµêÊûú\nJMTEB„Åß„ÅÆÂÖ®„Å¶„ÅÆË©ï‰æ°ÁµêÊûú„ÅØ„Åì„Å°„ÇâJSON„Éï„Ç°„Ç§„É´„Å´Ë®òËºâ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇJMTEB Leaderboard„Åß‰ªñ„ÅÆ„É¢„Éá„É´„Å®Ë¶ãÊØî„Åπ„Çã„Å®„ÄÅÁõ∏ÂØæÁöÑ„Å™Â∑Æ„Åå„Çè„Åã„Çã„Åß„Åó„Çá„ÅÜ„ÄÇJMTEB„ÅÆÂÖ®‰Ωì„ÅÆË©ï‰æ°ÁµêÊûú„ÅØ„É¢„Éá„É´„Çµ„Ç§„Ç∫„ÇíËÄÉ„Åà„Çã„Å®„ÄÅ„Åô„Åì„Å∂„ÇãËâØÂ•Ω„Åß„Åô„ÄÇ„Å™„Åä„ÄÅJMTEB „ÅÆmr-tidy „Çø„Çπ„ÇØ„ÅØ700‰∏áÊñáÁ´†„ÅÆ„Éô„ÇØ„Éà„É´Âåñ„ÇíË°å„ÅÜ„ÅÆ„ÅßÂá¶ÁêÜ„Å´ÊôÇÈñì„Åå„Åã„Å™„Çä„Åã„Åã„Çã(„É¢„Éá„É´„Å´„ÇÇ„Çà„Çä„Åæ„Åô„ÅåRTX4090„Åß1~4ÊôÇÈñì„Åª„Å©)„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„ÇÇStaticEmbeddings„Åß„ÅØÈùûÂ∏∏„Å´ÈÄü„Åè„ÄÅRTX4090„Åß„ÅØÁ¥Ñ4ÂàÜ„ÅßÂá¶ÁêÜÁµÇ„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åó„Åü„ÄÇ\nÊÉÖÂ†±Ê§úÁ¥¢„ÅßBM25„ÅÆÁΩÆ„ÅçÊèõ„Åà„Åå„Åß„Åç„Åù„ÅÜ„Åã?\nJMTEB„ÅÆ‰∏≠„ÅÆÊÉÖÂ†±Ê§úÁ¥¢„Çø„Çπ„ÇØ„ÅÆRetrieval„ÅÆÁµêÊûú„ÇíË¶ã„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇStaticEmbedding „Åß„ÅØ mr-tidy „ÅÆÈ†ÖÁõÆ„ÅåËëó„Åó„ÅèÊÇ™„ÅÑ„Åß„Åô„Å≠„ÄÇmr-tidy„ÅØ‰ªñ„ÅÆ„Çø„Çπ„ÇØ„Å´ÊØî„Åπ„Å¶ÊñáÁ´†Èáè„ÅåÂúßÂÄíÁöÑ„Å´Â§ö„Åè(700‰∏áÊñáÁ´†)„ÄÅ„Å§„Åæ„ÇãÊâÄÂ§ßÈáè„ÅÆÊñáÁ´†„ÇíÊ§úÁ¥¢„Åô„Çã„Çà„ÅÜ„Å™„Çø„Çπ„ÇØ„Åß„ÅØÁµêÊûú„ÅåÊÇ™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åù„ÅÜ„Åß„Åô„ÄÇÊñáËÑà„ÇíÁÑ°Ë¶ñ„Åó„Åü„ÅüÂçòÁ¥î„Å™„Éà„Éº„ÇØ„É≥„ÅÆÂπ≥Âùá„Å™„ÅÆ„Åß„ÄÅÂ¢ó„Åà„Çå„Å∞Â¢ó„Åà„Çã„Åª„Å©‰ºº„ÅüÂπ≥Âùá„ÅÆÊñáÁ´†„ÅåÂá∫„Å¶„Åè„Çã„Å®„Åô„Çã„Å®„ÄÅ„Åù„ÅÜ„ÅÑ„ÅÜÁµêÊûú„Å´„ÇÇ„Å™„ÇäÂæó„Åù„ÅÜ„Åß„Åô„Å≠„ÄÇ\n„ÅÆ„Åß„ÄÅÂ§ßÈáè„ÅÆÊñáÁ´†„ÅÆÂ†¥Âêà„ÄÅBM25„Çà„Çä„ÇÇ„Å†„ÅÑ„Å∂ÊÄßËÉΩ„ÅåÊÇ™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åù„ÅÜ„Åß„Åô„ÄÇ„Åü„Å†„ÄÅÂ∞ë„Å™„ÅÑÊñáÁ´†„Åß„ÄÅ„Åö„Å∞„Çä„ÅÆÂçòË™û„Éû„ÉÉ„ÉÅ„ÅåÂ∞ë„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅBM25„Çà„Çä„ÇÇËâØÂ•Ω„Å™ÁµêÊûú„Å´„Å™„Çã„Åì„Å®„ÅåÂ§ö„Åù„ÅÜ„Åß„Åô„Å≠„ÄÇ\n„Å™„ÅäÊÉÖÂ†±Ê§úÁ¥¢„Çø„Çπ„ÇØ„ÅÆ jaqket „ÅÆÁµêÊûú„Åå‰ªñ„ÅÆ„É¢„Éá„É´„Å´ÂØæ„Åó„Å¶„ÇÑ„Åü„ÇâËâØ„ÅÑ„ÅÆ„ÅØ„ÄÅjaqket „ÅÆÂïèÈ°å„ÇíÂê´„ÇÄ JQaRa (dev, unused)„ÇíÂ≠¶Áøí„Åó„Å¶„ÅÑ„Çã„Åã„Çâ„Å®„ÅÑ„Å£„Å¶„ÇÇ„ÄÅÈ´ò„Åô„Åé„ÇãÊÑü„Åò„ÅßË¨é„Åß„Åô„ÄÇtest „ÅÆÊÉÖÂ†±„É™„Éº„ÇØ„ÅØ„Åó„Å¶„ÅÑ„Å™„ÅÑ„Å®„ÅØÊÄù„ÅÜ„ÅÆ„Åß„Åô„Åå‚Ä¶„ÄÇ\n„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÁµêÊûú„ÅåÊÇ™„ÅÑ\n„Åì„Å°„Çâ„ÇÇË©≥Á¥∞„ÅØËøΩ„Å£„Åã„Åë„Å¶„ÅÑ„Åæ„Åõ„Çì„Åå„ÄÅ„Çπ„Ç≥„Ç¢ÁöÑ„Å´„ÅØ‰ªñ„ÅÆ„É¢„Éá„É´„Çà„Çä„ÇÇ„Å†„ÅÑ„Å∂ÊÇ™„ÅÑÁµêÊûú„Åß„Åô„Å≠„ÄÇ„ÇØ„É©„ÇπÂàÜÈ°û„Çø„Çπ„ÇØ„ÅØÊÇ™„Åè„Å™„ÅÑ„ÅÆ„Åß‰∏çÊÄùË≠∞„Åß„Åô„ÄÇÂüã„ÇÅËæº„ÅøÁ©∫Èñì„Åå„Éû„Éà„É™„Éß„Éº„Ç∑„Ç´Ë°®ÁèæÂ≠¶Áøí„Åß‰Ωú„Çâ„Çå„ÅüÂΩ±Èüø„ÇÇ„ÅÇ„Çã„ÅÆ„Åß„Åó„Çá„ÅÜ„Åã„ÄÇ\nJQaRA, JaCWIR „Åß„ÅÆ„É™„É©„É≥„Ç≠„É≥„Ç∞„Çø„Çπ„ÇØË©ï‰æ°\nJQaRA „ÅÆÁµêÊûú„ÅØ„Åì„Å°„Çâ„ÄÇ\nmodel_names\nndcg@10\nmrr@10\nstatic-embedding-japanese\n0.4704\n0.6814\nbm25\n0.458\n0.702\nmultilingual-e5-small\n0.4917\n0.7291\nJaCWIR „ÅÆÁµêÊûú„ÅØ„Åì„Å°„Çâ„ÄÇ\nmodel_names\nmap@10\nhits@10\nstatic-embedding-japanese\n0.7642\n0.9266\nbm25\n0.8408\n0.9528\nmultilingual-e5-small\n0.869\n0.97\nJQaRa Ë©ï‰æ°„ÅØ BM25 „Çà„Çä„ÅØËã•Âπ≤ËâØ„Åè„ÄÅmE5-small „Çà„Çä„ÅØËã•Âπ≤‰Ωé„ÅÑ„ÄÅJaCWIR „ÅØ BM25, mE5„Çà„Çä„Å†„ÅÑ„Å∂‰Ωé„ÅÑÊÑü„Åò„ÅÆÁµêÊûú„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ\nJaCWIR „ÅØquery„Åã„ÇâÊé¢„Åó„ÅÇ„Å¶„ÇãÊñáÁ´†„Åå„ÄÅWebÊñáÁ´†„ÅÆ„Çø„Ç§„Éà„É´„Å®Ê¶ÇË¶ÅÊñá„Å™„ÅÆ„Åß„ÄÅ„ÅÑ„Çè„ÇÜ„Çã„ÄåÁ∂∫È∫ó„Å™„ÄçÊñáÁ´†„Åß„ÅØ„Å™„ÅÑ„Ç±„Éº„Çπ„ÇÇÂ§ö„ÅÑ„Åß„Åô„ÄÇtransformer„É¢„Éá„É´„ÅØ„Éé„Ç§„Ç∫„Å´Âº∑„ÅÑ„ÅÆ„Åß„ÄÅÂçòÁ¥î„Å™„Éà„Éº„ÇØ„É≥Âπ≥Âùá„ÅÆStaticEmbedding„Åß„ÅØ„Çπ„Ç≥„Ç¢„Å´Â∑Æ„Åå„Å§„Åë„Çâ„Çå„Çã„ÅÆ„ÇÇÁ¥çÂæó„Åß„Åô„Å≠„ÄÇBM25„ÅØÁâπÂæ¥ÁöÑ„Å™ÂçòË™û„ÅåÂá∫Áèæ„Åó„ÅüÊñáÁ´†„Å´„Éû„ÉÉ„ÉÅ„Åô„Çã„ÅÆ„Åß„ÄÅJaCWIR „Åß„ÇÇ„Éé„Ç§„Ç∫„Å®„Å™„Çã„Çà„ÅÜ„Å™ÊñáÁ´†‰∏ä„ÅÆÂçòË™û„ÅØ„ÇØ„Ç®„É™„Å´„Åù„ÇÇ„Åù„ÇÇ„Éû„ÉÉ„ÉÅ„Åó„Å™„ÅÑ„Åü„ÇÅ„ÄÅTransformer „É¢„Éá„É´„Å®Á´∂‰∫âÂäõ„ÅÆ„ÅÇ„ÇãÁµêÊßãËâØ„ÅÑÁµêÊûú„ÇíÊÆã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„Åì„ÅÆÁµêÊûú„Åã„Çâ„ÄÅStaticEmbedding „ÅØ Transformer / BM25 „Å´ÊØî„Åπ„ÄÅ„Éé„Ç§„Ç∫„ÇíÂ§ö„ÅèÂê´„ÇÄÊñáÁ´†„ÅÆÂ†¥Âêà„ÅØ„Çπ„Ç≥„Ç¢„ÅåÊÇ™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\nÂá∫ÂäõÊ¨°ÂÖÉ„ÅÆÂâäÊ∏õ\nStaticEmbedding „ÅßÂá∫Âäõ„Åï„Çå„ÇãÊ¨°ÂÖÉ„ÅØ„ÄÅÂ≠¶ÁøíÊ¨°Á¨¨„Åß„Åô„Åå‰ªäÂõû‰ΩúÊàê„Åó„Åü„ÇÇ„ÅÆ„ÅØ1024Ê¨°ÂÖÉ„Å®„Åù„Åì„Åù„Åì„ÅÆ„Çµ„Ç§„Ç∫„Åß„Åô„ÄÇÊ¨°ÂÖÉÊï∞„ÅåÂ§ß„Åç„ÅÑ„Å®„ÄÅÊé®Ë´ñÂæå„ÅÆ„Çø„Çπ„ÇØ(„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÇÑÊÉÖÂ†±Ê§úÁ¥¢„Å™„Å©)„Å´Ë®àÁÆó„Ç≥„Çπ„Éà„Åå„Åã„Åã„Å£„Å¶„Åó„Åæ„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åã„Åó„Å™„Åå„Çâ„ÄÅÂ≠¶ÁøíÊôÇ„Å´„Éû„Éà„É™„Éß„Éº„Ç∑„Ç´Ë°®ÁèæÂ≠¶Áøí(Matryoshka Representation Learning(MRL))„Çí„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ1024Ê¨°ÂÖÉ„Çí„Åï„Çâ„Å´Â∞è„Åï„Å™Ê¨°ÂÖÉ„Å∏„Å®Á∞°Âçò„Å´Ê¨°ÂÖÉÂâäÊ∏õ„Åå„Åß„Åç„Åæ„Åô„ÄÇ\nMRL„ÅØ„ÄÅÂ≠¶ÁøíÊôÇ„Å´ÂÖàÈ†≠„ÅÆ„Éô„ÇØ„Éà„É´„Åª„Å©ÈáçË¶Å„Å™Ê¨°ÂÖÉ„ÇíÊåÅ„Å£„Å¶„Åè„Çã„Åì„Å®„Åß„ÄÅ‰æã„Åà„Å∞1024Ê¨°ÂÖÉ„Åß„ÇÇÂÖàÈ†≠„ÅÆ32,64,128,256...Ê¨°ÂÖÉ„Å†„Åë„Çí‰Ωø„Å£„Å¶Âæå„Çç„ÇíÂàá„ÇäÊç®„Å¶„Çã„Å†„Åë„Åß„ÄÅ„ÅÇ„ÇãÁ®ãÂ∫¶ËâØÂ•Ω„Å™ÁµêÊûú„ÇíÁ§∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„Åì„ÅÆ„Ç∞„É©„ÉïÂèÇÁÖßÂÖÉ„ÅÆStaticEmbedding „ÅÆË®ò‰∫ã„Å´„Çà„Çã„Å®„ÄÅ128Ê¨°ÂÖÉ„Åß91.87%, 256Ê¨°ÂÖÉ„Åß95.79%, 512Ê¨°ÂÖÉ„Åß98.53%„ÅÆÊÄßËÉΩ„ÇíÁ∂≠ÊåÅ„Åó„Å¶„ÅÑ„Çã„Çà„ÅÜ„Åß„Åô„ÄÇÁ≤æÂ∫¶„Å´„Åù„Åì„Åæ„Åß„Ç∑„Éì„Ç¢„Åß„ÅØ„Å™„ÅÑ„Åå„ÄÅ„Åù„ÅÆÂæå„ÅÆË®àÁÆó„Ç≥„Çπ„Éà„Çí‰∏ã„Åí„Åü„ÅÑÂ†¥Âêà„ÄÅ„Ç¨„ÉÉ„Å®Ê¨°ÂÖÉÂâäÊ∏õ„Åó„Å¶‰Ωø„ÅÜ„ÄÅ„Å®„ÅÑ„ÅÜÁî®ÈÄî„Å´„ÇÇ‰Ωø„Åà„Åù„ÅÜ„Åß„Åô„Å≠„ÄÇ\nStaticEmbdding Êó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÅÆÊ¨°ÂÖÉÂâäÊ∏õÁµêÊûú\nJMTEB „Åß„ÅØ„ÄÅÂá∫ÂäõÊôÇ„Å´„É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíÂà∂Âæ°„Åß„Åç„Çã„Åü„ÇÅ„ÄÅtruncate_dim „Ç™„Éó„Ç∑„Éß„É≥„ÇíÊ∏°„Åô„Åì„Å®„Åß„ÄÅÊ¨°ÂÖÉÂâäÊ∏õ„Åó„ÅüÁµêÊûú„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇÇÁ∞°Âçò„Å´Ë®àÊ∏¨„Åß„Åç„Åæ„Åô„ÄÇÁ¥†Êô¥„Çâ„Åó„ÅÑ„Åß„Åô„Å≠„ÄÇ„Å®„ÅÑ„ÅÜ„Çè„Åë„Åß„ÄÅStaticEmbdding Êó•Êú¨Ë™û„É¢„Éá„É´„Åß„ÇÇ„ÄÅÊ¨°ÂÖÉÂâäÊ∏õ„Åó„ÅüÁµêÊûú„Åß„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí„Å®„Å£„Å¶„Åø„Åæ„Åó„Åü„ÄÇ\nÊ¨°ÂÖÉÊï∞\nAvg(micro)\n„Çπ„Ç≥„Ç¢Ââ≤Âêà(%)\nRetrieval\nSTS\nClassification\nReranking\nClustering\nPairClassification\n1024\n67.17\n100.00\n67.92\n80.16\n67.96\n91.87\n40.39\n62.37\n512\n66.57\n99.10\n67.63\n80.11\n65.66\n91.54\n41.25\n62.37\n256\n65.94\n98.17\n66.99\n79.93\n63.53\n91.73\n42.55\n62.37\n128\n64.25\n95.65\n64.87\n79.56\n60.52\n91.62\n41.81\n62.33\n64\n61.79\n91.98\n61.15\n78.34\n58.23\n91.50\n39.11\n62.35\n32\n57.93\n86.24\n53.35\n76.51\n55.95\n91.15\n38.20\n62.37\n„Çπ„Ç≥„Ç¢„ÅÆÂ§âÂåñ„ÇíË¶ã„Çã„Å®„ÄÅ512Ê¨°ÂÖÉ„Å∏„Å®Ê¨°ÂÖÉÂâäÊ∏õ„Åó„ÅüÂ†¥Âêà„ÅØ„ÇÑ„Åü„ÇâRetrieval, Classification,Reranking „ÅÆÊÄßËÉΩ„ÅåÊÇ™„Åè„Å™„Çä„Åæ„Åô„ÄÇ„ÇÄ„Åó„Çç256Ê¨°ÂÖÉ„Åæ„ÅßÊ¨°ÂÖÉÂâäÊ∏õ„Åó„Å¶„Åó„Åæ„Å£„ÅüÊñπ„ÅåËâØÂ•Ω„Å™ÁµêÊûú„Å´„ÄÇ256Ê¨°ÂÖÉ„Åß„ÅØ„ÄÅ„Çπ„Ç≥„Ç¢ÁöÑ„Å´„ÅØÊ¨°ÂÖÉÂâäÊ∏õ„Åô„ÇãÂâç„ÅÆ„É¢„Éá„É´„ÅÆ98.93%„Å™„Çì„Åß„Åô„Åå„ÄÅ„Åì„Çå„ÅØ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÅÆÁµêÊûú„Åå„Å™„Åú„Åã1024Ê¨°ÂÖÉ„Çà„Çä„ÇÇËâØ„Åè„Å™„Å£„Å¶„Åó„Åæ„Å£„Åü„Åü„ÇÅ„Åß„Åô„Å≠„ÄÇ\n512Ê¨°ÂÖÉ„Åß„ÅÆ„Çπ„Ç≥„Ç¢Ë®àÊ∏¨„ÅåÈñìÈÅï„Å£„Å¶„ÅÑ„Åü„ÅÆ„Åß‰øÆÊ≠£„Åó„Åæ„Åó„Åü„ÄÇ„Éû„Éà„É™„Éß„Éº„Ç∑„Ç´Ë°®ÁèæÂ≠¶Áøí„Åå„ÅÜ„Åæ„ÅèÂèçÊò†„Åï„Çå„ÄÅÊ¨°ÂÖÉÊï∞„ÇíÂâä„Çã„Å®Ëã•Âπ≤„ÅÆ„Çπ„Ç≥„Ç¢‰Ωé‰∏ã„ÅåË¶ã„Çâ„Çå„Åæ„Åô„Åå„ÄÅÊ¨°ÂÖÉÊï∞„ÅåÊ∏õ„Å£„Åü„Åü„ÇÅ„Åù„ÅÆÂæå„ÅÆ„Ç≥„Çπ„Éà„ÅåÊäë„Åà„Çâ„Çå„Åù„ÅÜ„Åß„Åô„Å≠„ÄÇ\n„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Çø„Çπ„ÇØ„Å´„Åä„ÅÑ„Å¶„ÅØ128Ê¨°ÂÖÉ„Åæ„ÅßÊ¨°ÂÖÉÂâäÊ∏õ„Åó„Å¶„ÇÇ1024Ê¨°ÂÖÉ„Çà„Çä„ÇÇ„Çπ„Ç≥„Ç¢„ÅåÈ´ò„ÅÑ„ÄÅ„Å®„ÅÑ„ÅÜÊú¨Êù•ÊÉÖÂ†±Èáè„ÇíÂâä„Çâ„Å™„ÅÑÊñπ„Åå„Çπ„Ç≥„Ç¢„ÅåËâØ„ÅÑ„Åè„Å™„Çä„Åù„ÅÜ„Å™„ÅÆ„Å´„ÄÅ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„Çø„Çπ„ÇØ„ÅÆ„Åø„ÅØÈÄÜ„Å´„Çπ„Ç≥„Ç¢„Åå‰∏ä„Åå„Å£„Å¶„Åó„Åæ„ÅÜËààÂë≥Ê∑±„ÅÑÁµêÊûú„Å®„Å™„Çä„Åæ„Åó„Åü‚Ä¶„ÄÇ„Éû„Éà„É™„Éß„Éº„Ç∑„Ç´Ë°®ÁèæÂ≠¶Áøí„Åß„ÅØ„ÄÅÂÖàÈ†≠„ÅÆÊ¨°ÂÖÉ„ÅÆÊñπ„ÅåÂÖ®‰ΩìÁöÑ„Å™ÁâπÂæ¥„ÇíË∏è„Åæ„Åà„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞Áî®ÈÄî„Å´„ÅØ(„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„ÇÇ„Çà„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„Åå)„ÄÅÁâπÂæ¥ÁöÑ„Å™Ââç„ÅÆÊñπ„ÅÆÊ¨°ÂÖÉ„ÅÆ„Åø„ÅßÂæå„Çç„ÅÆÊ¨°ÂÖÉ„Çí‰Ωø„Çè„Å™„ÅÑÊñπ„ÅåËâØË≥™„Å™ÁµêÊûú„ÅåÂæó„Çâ„Çå„Çã„ÄÅ„Å®„ÅÑ„ÅÜ„Åì„Å®„Å™„ÅÆ„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ\n„Å®„ÅÑ„ÅÜ„Çè„Åë„Åß„ÄÅstatic-embedding-japanese „É¢„Éá„É´„ÅßÊ¨°ÂÖÉÂâäÊ∏õ„Åô„ÇãÊôÇ„ÅØ„ÄÅ512,256,128Ê¨°ÂÖÉ„ÅÇ„Åü„Çä„ÅåÊÄßËÉΩ„Å®Ê¨°ÂÖÉÂâäÊ∏õ„ÅÆ„Éê„É©„É≥„Çπ„ÅåÂèñ„Çå„Å¶„Åù„ÅÜ„Åß„Åô„Å≠„ÄÇ\nStaticEmbedding „É¢„Éá„É´„Çí‰Ωú„Å£„Å¶„Åø„Å¶\nÊ≠£Áõ¥„ÄÅÂçòÁ¥î„Å™„Éà„Éº„ÇØ„É≥„ÅÆembeddings„ÅÆÂπ≥Âùá„Åß„Åù„Çì„Å™„Å´ÊÄßËÉΩÂá∫„Çã„ÅÆ„ÅãÂçä‰ø°ÂçäÁñë„Å†„Å£„Åü„ÅÆ„Åß„Åô„Åå„ÄÅÂÆüÈöõ„Å´Â≠¶Áøí„Åï„Åõ„Å¶„Åø„Å¶„Ç∑„É≥„Éó„É´„Å™„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Å™„ÅÆ„Å´ÊÄßËÉΩ„ÅÆÈ´ò„Åï„Å´„Å≥„Å£„Åè„Çä„Åó„Åæ„Åó„Åü„ÄÇTransformer ÂÖ®Áõõ„ÅÆ„Åì„ÅÆÊôÇ‰ª£„Å´„ÄÅÂè§„ÅçËâØ„ÅçÂçòË™ûÂüã„ÇÅËæº„Åø„ÅÆÊ¥ªÁî®„É¢„Éá„É´„Åß„ÄÅÂÆü‰∏ñÁïå„ÅßÂà©Ê¥ªÁî®„Åß„Åç„Åù„ÅÜ„Å™„É¢„Éá„É´„ÅÆÂá∫Áèæ„Å´È©ö„Åç„ÇíÈö†„Åõ„Åæ„Åõ„Çì„ÄÇ\nCPU„Åß„ÅÆÊé®Ë´ñÈÄüÂ∫¶„ÅåÈÄü„ÅÑÊñá„Éô„ÇØ„Éà„É´‰ΩúÊàê„É¢„Éá„É´„ÅØ„ÄÅ„É≠„Éº„Ç´„É´CPUÁí∞Â¢É„ÅßÂ§ßÈáè„ÅÆÊñáÁ´†„ÅÆÂ§âÊèõ„Å™„Å©„ÅØ„ÇÇ„Å®„Çà„Çä„ÄÅ„Ç®„ÉÉ„Ç∏„Éá„Éê„Ç§„Çπ„Å†„Å£„Åü„Çä„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅåÈÅÖ„ÅÑ(„É™„É¢„Éº„Éà„ÅÆÊé®Ë´ñ„Çµ„Éº„Éê„ÇíÂè©„Åë„Å™„ÅÑ)Áí∞Â¢É„Å†„Å£„Åü„Çä„ÄÅËâ≤„ÄÖ„Å®Ê¥ªÁî®„Åß„Åç„Åù„ÅÜ„Åß„Åô„Å≠„ÄÇ\nStaticEmbedding Êó•Êú¨Ë™û„É¢„Éá„É´Â≠¶Áøí„ÅÆ„ÉÜ„ÇØ„Éã„Ç´„É´„Éé„Éº„Éà\n„Å™„Åú„ÅÜ„Åæ„ÅèÂ≠¶Áøí„Åß„Åç„Çã„ÅÆ„Åã\nStaticEmbedding „ÅØÈùûÂ∏∏„Å´„Ç∑„É≥„Éó„É´„Åß„ÄÅÊñáÁ´†„Çí„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Åó„ÅüID„ÅßÂçòË™û„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„ÅåÊ†ºÁ¥ç„Åï„Çå„Å¶„ÅÑ„ÇãEmbeddingBag„ÉÜ„Éº„Éñ„É´„Åã„ÇâNÊ¨°ÂÖÉ(‰ªäÂõû„ÅØ1024Ê¨°ÂÖÉ)„ÅÆ„Éô„ÇØ„Éà„É´„ÇíÂèñÂæó„Åó„ÄÅ„Åù„ÅÆÂπ≥Âùá„ÇíÂèñ„Çã„Å†„Åë„Åß„Åô„ÄÇ\n„Åì„Çå„Åæ„Åß„ÄÅÂçòË™ûÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„Å®„ÅÑ„Åà„Å∞„ÄÅword2vec „ÇÑ GloVe „ÅÆ„Çà„ÅÜ„Å´ Skip-gram „ÇÑ CBOW „ÇíÁî®„ÅÑ„Å¶ÂçòË™û„ÅÆÂë®Ëæ∫„ÇíÂ≠¶Áøí„Åó„Å¶„Åç„Åæ„Åó„Åü„ÄÇ„Åó„Åã„Åó„ÄÅStaticEmbedding „Åß„ÅØÊñáÁ´†ÂÖ®‰Ωì„ÇíÁî®„ÅÑ„Å¶Â≠¶Áøí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅÂØæÁÖßÂ≠¶Áøí„Çí‰Ωø„Å£„Å¶Â§ßÈáè„ÅÆÊßò„ÄÖ„Å™ÊñáÁ´†„ÇíÂ∑®Â§ß„Éê„ÉÉ„ÉÅ„ÅßÂ≠¶Áøí„Åó„Å¶„Åä„Çä„ÄÅËâØ„ÅÑÂçòË™û„ÅÆÂüã„ÇÅËæº„ÅøË°®Áèæ„ÅÆÂ≠¶Áøí„Å´ÊàêÂäü„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÂØæÁÖßÂ≠¶Áøí„ÅØ„ÄÅÂü∫Êú¨ÁöÑ„Å´Ê≠£‰æã‰ª•Â§ñÂÖ®„Å¶„ÇíË≤†‰æã„Å®„Åó„Å¶Â≠¶Áøí„Åô„Çã„Åü„ÇÅ„ÄÅ‰æã„Åà„Å∞„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫2048„Å™„Çâ1„ÅÆÊ≠£‰æã„Å´ÂØæ„Åó„Å¶2047„ÅÆË≤†‰æã„Çí2048ÈÄö„Çä„ÄÅ„Å§„Åæ„Çä2048x2047„ÅßÁ¥Ñ400‰∏á„ÅÆÊØîËºÉ„ÇíÂ≠¶Áøí„Åó„Åæ„Åô„ÄÇ„Åù„ÅÆ„Åü„ÇÅ„ÄÅÂÖÉ„ÅÆÂçòË™ûÁ©∫Èñì„Å´ÂØæ„Åó„Å¶ÈÅ©Âàá„Å™Èáç„Åø„ÇíÊõ¥Êñ∞„Åó„Å™„Åå„Çâ„ÄÅÂ≠¶Áøí„ÇíÈÄ≤„ÇÅ„Çã„Åì„Å®„Åå„Åß„Åç„Çã„ÅÆ„Åß„Åô„ÄÇ\nÂ≠¶Áøí„Éá„Éº„Çø„Çª„ÉÉ„Éà\nÊó•Êú¨Ë™û„É¢„Éá„É´Â≠¶Áøí„Å´„ÅÇ„Åü„Çä„ÄÅÂØæÁÖßÂ≠¶Áøí„ÅßÂà©Áî®„Åß„Åç„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®„Åó„Å¶„ÄÅ‰ª•‰∏ã„Çí‰ΩúÊàê„Åó‰ΩøÁî®„Åó„Åæ„Åó„Åü„ÄÇ\nhotchpotch/sentence_transformer_japanese\nSentenceTransformer „ÅßÂ≠¶Áøí„Åó„ÇÑ„Åô„ÅÑ„Ç´„É©„É†Âêç„Å®ÊßãÈÄ†„Å´Êï¥„Åà„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n(anchor, positive), (anchor, positive, negative), (anchor, positive,¬†negative_1, ..., negative_n) „Å®„ÅÑ„Å£„ÅüÊßãÈÄ†„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n‰ª•‰∏ã„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂü∫„Å´ hotchpotch/sentence_transformer_japanese „Çí‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇÊØéÂ∫¶„Å™„Åå„Çâ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰ΩúËÄÖ„ÅÆÊñπ„ÄÖ„Éª„Å®„Çä„Çè„Åë hpprc Ê∞è„Å´ÊÑüË¨ù„Åß„Åô„ÄÇ\nhttps://huggingface.co/datasets/hpprc/emb\nhttps://huggingface.co/datasets/hotchpotch/hpprc_emb-scores „ÅÆ„É™„É©„É≥„Ç´„Éº„Çπ„Ç≥„Ç¢„Çí‰ΩøÁî®„Åó„ÄÅpositive(>=0.7) / negative(<=0.3) „ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÇíË°å„ÅÑ„Åæ„Åó„Åü„ÄÇ\nhttps://huggingface.co/datasets/hpprc/llmjp-kaken\nhttps://huggingface.co/datasets/hpprc/msmarco-ja\nhttps://huggingface.co/datasets/hotchpotch/msmarco-ja-hard-negatives¬†„ÅÆ„É™„É©„É≥„Ç´„Éº„Çπ„Ç≥„Ç¢„ÇíÁî®„ÅÑ„Å¶„ÄÅpositive(>=0.7) / negative(<=0.3) „ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÇíË°å„ÅÑ„Åæ„Åó„Åü„ÄÇ\nhttps://huggingface.co/datasets/hpprc/mqa-ja\nhttps://huggingface.co/datasets/hpprc/llmjp-warp-html\n‰∏äË®ò„ÅÆ‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰∏≠„Åß„ÄÅ‰ª•‰∏ã„Çí‰ΩøÁî®„Åó„Åæ„Åó„Åü„ÄÇ„Å™„Åä„ÄÅÊÉÖÂ†±Ê§úÁ¥¢„ÇíÂº∑Âåñ„Åó„Åü„Åã„Å£„Åü„Åü„ÇÅ„ÄÅÊÉÖÂ†±Ê§úÁ¥¢„Å´ÈÅ©„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Éá„Éº„Çø„ÅØ„Ç™„Éº„ÇÆ„É•„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„Åß‰ª∂Êï∞„ÇíÂ§ö„ÇÅ„Å´Â≠¶Áøí„Åï„Åõ„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nhttprc_auto-wiki-nli-triplet\nhttprc_auto-wiki-qa\nhttprc_auto-wiki-qa-nemotron\nhttprc_auto-wiki-qa-pair\nhttprc_baobab-wiki-retrieval\nhttprc_janli-triplet\nhttprc_jaquad\nhttprc_jqara\nhttprc_jsnli-triplet\nhttprc_jsquad\nhttprc_miracl\nhttprc_mkqa\nhttprc_mkqa-triplet\nhttprc_mr-tydi\nhttprc_nu-mnli-triplet\nhttprc_nu-snli-triplet\nhttprc_quiz-no-mori\nhttprc_quiz-works\nhttprc_snow-triplet\nhttprc_llmjp-kaken\nhttprc_llmjp_warp_html\nhttprc_mqa_ja\nhttprc_msmarco_ja\nËã±Ë™û„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nsentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\nsentence-transformers/squad\nsentence-transformers/all-nli\nsentence-transformers/trivia-qa\nnthakur/swim-ir-monolingual\nsentence-transformers/miracl\nsentence-transformers/mr-tydi\nÊó•Êú¨Ë™û„Éà„Éº„ÇØ„Éä„Ç§„Ç∂\nStaticEmbedding „ÇíÂ≠¶Áøí„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅHuggingFace „ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„É©„Ç§„Éñ„É©„É™„ÅÆ tokenizer.json ÂΩ¢Âºè„ÅßÂá¶ÁêÜÂèØËÉΩ„Å™„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Çí‰Ωø„ÅÜ„Å®Á∞°Âçò„Åù„ÅÜ„Å†„Å£„Åü„ÅÆ„Åß„ÄÅ hotchpotch/xlm-roberta-japanese-tokenizer „Å®„ÅÑ„ÅÜ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Çí‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇË™ûÂΩôÊï∞„ÅØ 32,768 „Åß„Åô„ÄÇ\n„Åì„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÅØ„ÄÅwikipedia Êó•Êú¨Ë™û„ÄÅwikipedia Ëã±Ë™û(„Çµ„É≥„Éó„É™„É≥„Ç∞)„ÄÅcc-100(Êó•Êú¨Ë™û, „Çµ„É≥„Éó„É™„É≥„Ç∞)(Ë®ÇÊ≠£:‰ΩúÊàê„Ç≥„Éº„Éâ„ÇíÁ¢∫Ë™ç„Åó„Åü„Å®„Åì„Çç„ÄÅwikipediaÊó•Êú¨Ë™û„ÅÆ„Åø„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Åæ„Åó„Åü)„ÅÆ„Éá„Éº„Çø„Çí unidic „ÅßÂàÜÂâ≤„Åó„ÄÅsentencepiece unigram „ÅßÂ≠¶Áøí„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇXLM-Roberta ÂΩ¢Âºè„ÅÆÊó•Êú¨Ë™û„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Å®„Åó„Å¶„ÇÇÊ©üËÉΩ„Åó„Åæ„Åô„ÄÇ‰ªäÂõû„ÅØ„Åì„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÇíÂà©Áî®„Åó„Åæ„Åó„Åü„ÄÇ\n„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø\nÂ§ßÂÖÉ„ÅÆÂ≠¶Áøí„Ç≥„Éº„Éâ„Å®„ÅÆÂ§âÊõ¥ÁÇπ„ÇÑ„É°„É¢„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„Åô„ÄÇ\nbatch_size „ÇíÂ§ßÂÖÉ„ÅÆ 2048 „Åã„Çâ 6072 „Å´Ë®≠ÂÆö„Åó„Åæ„Åó„Åü„ÄÇ\nÂØæÁÖßÂ≠¶Áøí„ÅßÂ∑®Â§ß„Å™„Éê„ÉÉ„ÉÅ„ÇíÂá¶ÁêÜ„Åô„Çã„Å®„Åç„ÄÅÂêå‰∏Ä„Éê„ÉÉ„ÉÅÂÜÖ„Å´„Éù„Ç∏„ÉÜ„Ç£„Éñ„Å®„Éç„Ç¨„ÉÜ„Ç£„Éñ„ÅåÂê´„Åæ„Çå„Çã„Å®Â≠¶Áøí„Å´ÊÇ™ÂΩ±Èüø„Çí‰∏é„Åà„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„ÇíÈò≤„Åê„Åü„ÇÅ„Å´ BatchSamplers.NO_DUPLICATES „Ç™„Éó„Ç∑„Éß„É≥„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅåÂ∑®Â§ß„Å†„Å®Âêå‰∏Ä„Éê„ÉÉ„ÉÅ„Å´Âê´„ÇÅ„Å™„ÅÑ„Åü„ÇÅ„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞Âá¶ÁêÜ„Å´ÊôÇÈñì„Åå„Åã„Åã„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n‰ªäÂõû„ÅØ BatchSamplers.NO_DUPLICATES „ÇíÊåáÂÆö„Åó„ÄÅRTX4090 „ÅÆ 24GB „Å´Âèé„Åæ„Çã 6072 „Å´Ë®≠ÂÆö„Åó„Åæ„Åó„Åü„ÄÇ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅØ„Åï„Çâ„Å´Â§ß„Åç„ÅÑÊñπ„ÅåÁµêÊûú„ÅåËâØ„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\nepochÊï∞„Çí1„Åã„Çâ2„Å´Â§âÊõ¥„Åó„Åæ„Åó„Åü\n1„Çà„Çä„ÇÇ2„ÅÆÊñπ„ÅåËâØ„ÅÑÁµêÊûú„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ„Åü„Å†„Åó„ÄÅ„Éá„Éº„Çø„Çµ„Ç§„Ç∫„Åå„ÇÇ„Å£„Å®Â§ß„Åç„Åë„Çå„Å∞„ÄÅ1„ÅÆÊñπ„ÅåËâØ„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n„Çπ„Ç±„Ç∏„É•„Éº„É©\nÊ®ôÊ∫ñ„ÅÆlinear„Åã„Çâ„ÄÅÁµåÈ®ìÂâá„Åß„Çà„ÇäËâØ„ÅÑ„Å®ÊÑü„Åò„Çãcosine„Å´Â§âÊõ¥„Åó„Åæ„Åó„Åü„ÄÇ\n„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂\nÊ®ôÊ∫ñ„ÅÆAdamW „ÅÆ„Åæ„Åæ„Åß„Åô„ÄÇadafactor„Å´Â§âÊõ¥„Åó„ÅüÂ†¥Âêà„ÄÅÂèéÊùü„ÅåÊÇ™„Åè„Å™„Çä„Åæ„Åó„Åü„ÄÇ\nlearning_rate\n2e-1 „ÅÆ„Åæ„Åæ„Åß„Åô„ÄÇÂÄ§„ÅåÂ∑®Â§ß„Åô„Åé„Çã„ÅÆ„Åß„ÅØ„Å™„ÅÑ„Åã„Å®ÁñëÂïè„Å´ÊÄù„ÅÑ„Åæ„Åó„Åü„Åå„ÄÅ‰Ωé„Åè„Åô„Çã„Å®ÁµêÊûú„ÅåÊÇ™Âåñ„Åó„Åæ„Åó„Åü„ÄÇ\ndataloader_prefetch_factor=4\ndataloader_num_workers=15\n„Éà„Éº„ÇØ„Éä„Ç§„Ç∫„Å®„Éê„ÉÉ„ÉÅ„Çµ„É≥„Éó„É©„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞„Å´ÊôÇÈñì„Åå„Åã„Åã„Çã„Åü„ÇÅ„ÄÅÂ§ß„Åç„ÇÅ„Å´Ë®≠ÂÆö„Åó„Åæ„Åó„Åü„ÄÇ\nÂ≠¶Áøí„É™„ÇΩ„Éº„Çπ\nCPU\nRyzen9 7950X\nGPU\nRTX4090\nmemory\n64GB\n„Åì„ÅÆ„Éû„Ç∑„É≥„É™„ÇΩ„Éº„Çπ„Åß„ÄÅ„Éï„É´„Çπ„ÇØ„É©„ÉÉ„ÉÅÂ≠¶Áøí„Å´„Åã„Åã„Å£„ÅüÊôÇÈñì„ÅØÁ¥Ñ4ÊôÇÈñì„Åß„Åó„Åü„ÄÇGPU„ÅÆ„Ç≥„Ç¢Ë≤†Ëç∑„ÅØÈùûÂ∏∏„Å´Â∞è„Åï„Åè„ÄÅ‰ªñ„ÅÆtransformer„É¢„Éá„É´„Åß„ÅØÂ≠¶ÁøíÊôÇ„Å´90%ÂâçÂæå„ÅßÂºµ„Çä‰ªò„Åè„ÅÆ„Å´ÂØæ„Åó„Å¶„ÄÅStaticEmbedding„Åß„ÅØ„Åª„Å®„Çì„Å©0%„Åß„Åó„Åü„ÄÇ„Åì„Çå„ÅØ„ÄÅÂ∑®Â§ß„Å™„Éê„ÉÉ„ÉÅ„ÇíGPU„É°„É¢„É™„Å´Ëª¢ÈÄÅ„Åô„ÇãÊôÇÈñì„ÅåÂ§ßÂçä„ÇíÂç†„ÇÅ„Å¶„ÅÑ„Çã„Åü„ÇÅ„Åã„Å®ÊÄù„Çè„Çå„Åæ„Åô„ÄÇ„Åù„ÅÆ„Åü„ÇÅ„ÄÅGPU„É°„É¢„É™„ÅÆÂ∏ØÂüüÂπÖ„ÅåÈÄü„Åè„Å™„Çå„Å∞„ÄÅÂ≠¶ÁøíÈÄüÂ∫¶„Åå„Åï„Çâ„Å´Âêë‰∏ä„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n„Åï„Çâ„Å™„ÇãÊÄßËÉΩÂêë‰∏ä„Å∏\n‰ªäÂõûÂà©Áî®„Åó„Åü„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÅØStaticEmbeddingÂêë„Åë„Å´ÁâπÂåñ„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅØ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„Çà„ÇäÈÅ©„Åó„Åü„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Çí‰ΩøÁî®„Åô„Çå„Å∞ÊÄßËÉΩ„ÅåÂêë‰∏ä„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Çí„Åï„Çâ„Å´Â∑®Â§ßÂåñ„Åô„Çã„Åì„Å®„Åß„ÄÅÂ≠¶Áøí„ÅÆÂÆâÂÆöÊÄß„ÅåÂêë‰∏ä„Åó„ÄÅÊÄßËÉΩÂêë‰∏ä„ÅåË¶ãËæº„ÇÅ„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ\n„Åæ„Åü„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™„Éâ„É°„Ç§„É≥„ÇÑÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂà©Áî®„Åô„Çã„Å™„Å©„ÄÅ„Çà„ÇäÂπÖÂ∫É„ÅÑÊñáÁ´†„É™„ÇΩ„Éº„Çπ„ÇíÂ≠¶Áøí„Å´ÁµÑ„ÅøËæº„ÇÄ„Åì„Å®„Åß„ÄÅ„Åï„Çâ„Å™„ÇãÊÄßËÉΩÂêë‰∏ä„ÅåÊúüÂæÖ„Åß„Åç„Åæ„Åô„ÄÇ\nÂ§ßÂÖÉ„ÅÆÂ≠¶Áøí„Ç≥„Éº„Éâ\nÂ≠¶Áøí„Å´‰ΩøÁî®„Åó„Åü„Ç≥„Éº„Éâ„ÅØ„ÄÅ‰ª•‰∏ã„Åß MIT „É©„Ç§„Çª„É≥„Çπ„ÅßÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Çπ„ÇØ„É™„Éó„Éà„ÇíÂÆüË°å„Åô„Çå„Å∞ÂÜçÁèæ„Åß„Åç„Çã„ÄÅ„ÅØ„Åö...!\nhttps://huggingface.co/hotchpotch/static-embedding-japanese/blob/main/trainer.py\n„É©„Ç§„Çª„É≥„Çπ\nstatic-embedding-japanese „ÅØ„É¢„Éá„É´Èáç„Åø„ÉªÂ≠¶Áøí„Ç≥„Éº„Éâ„Çí MIT „É©„Ç§„Çª„É≥„Çπ„ÅßÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    "deepseek-ai/DeepSeek-R1-Distill-Llama-8B": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF": "Llamacpp imatrix Quantizations of DeepSeek-R1-Distill-Llama-8B\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of DeepSeek-R1-Distill-Llama-8B\nUsing llama.cpp release b4514 for quantization.\nOriginal model: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system_prompt}<ÔΩúUserÔΩú>{prompt}<ÔΩúAssistantÔΩú>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nDeepSeek-R1-Distill-Llama-8B-f32.gguf\nf32\n32.13GB\nfalse\nFull F32 weights.\nDeepSeek-R1-Distill-Llama-8B-f16.gguf\nf16\n16.07GB\nfalse\nFull F16 weights.\nDeepSeek-R1-Distill-Llama-8B-Q8_0.gguf\nQ8_0\n8.54GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nDeepSeek-R1-Distill-Llama-8B-Q6_K_L.gguf\nQ6_K_L\n6.85GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q6_K.gguf\nQ6_K\n6.60GB\nfalse\nVery high quality, near perfect, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q5_K_L.gguf\nQ5_K_L\n6.06GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\nQ5_K_M\n5.73GB\nfalse\nHigh quality, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q5_K_S.gguf\nQ5_K_S\n5.60GB\nfalse\nHigh quality, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q4_K_L.gguf\nQ4_K_L\n5.31GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q4_1.gguf\nQ4_1\n5.13GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nDeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\nQ4_K_M\n4.92GB\nfalse\nGood quality, default size for most use cases, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q3_K_XL.gguf\nQ3_K_XL\n4.78GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nDeepSeek-R1-Distill-Llama-8B-Q4_K_S.gguf\nQ4_K_S\n4.69GB\nfalse\nSlightly lower quality with more space savings, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q4_0.gguf\nQ4_0\n4.68GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nDeepSeek-R1-Distill-Llama-8B-IQ4_NL.gguf\nIQ4_NL\n4.68GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nDeepSeek-R1-Distill-Llama-8B-IQ4_XS.gguf\nIQ4_XS\n4.45GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nDeepSeek-R1-Distill-Llama-8B-Q3_K_L.gguf\nQ3_K_L\n4.32GB\nfalse\nLower quality but usable, good for low RAM availability.\nDeepSeek-R1-Distill-Llama-8B-Q3_K_M.gguf\nQ3_K_M\n4.02GB\nfalse\nLow quality.\nDeepSeek-R1-Distill-Llama-8B-IQ3_M.gguf\nIQ3_M\n3.78GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nDeepSeek-R1-Distill-Llama-8B-Q2_K_L.gguf\nQ2_K_L\n3.69GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nDeepSeek-R1-Distill-Llama-8B-Q3_K_S.gguf\nQ3_K_S\n3.66GB\nfalse\nLow quality, not recommended.\nDeepSeek-R1-Distill-Llama-8B-IQ3_XS.gguf\nIQ3_XS\n3.52GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nDeepSeek-R1-Distill-Llama-8B-Q2_K.gguf\nQ2_K\n3.18GB\nfalse\nVery low quality but surprisingly usable.\nDeepSeek-R1-Distill-Llama-8B-IQ2_M.gguf\nIQ2_M\n2.95GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF --include \"DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF --include \"DeepSeek-R1-Distill-Llama-8B-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (DeepSeek-R1-Distill-Llama-8B-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski"
}