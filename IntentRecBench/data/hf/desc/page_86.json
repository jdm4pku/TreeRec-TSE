{
    "Qwen/Qwen2.5-Coder-1.5B-Instruct": "Qwen2.5-Coder-1.5B-Instruct\nIntroduction\nRequirements\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-Coder-1.5B-Instruct\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nThis repo contains the instruction-tuned 1.5B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 1.54B\nNumber of Paramaters (Non-Embedding): 1.31B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 12 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "openfoodfacts/nutrition-extractor": "nutrition-extractor\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nnutrition-extractor\nThis model is a fine-tuned version of microsoft/layoutlmv3-large on the openfoodfacts/nutrient-detection-layout dataset.\nIt allows to automatically extract nutrition values from images of nutrition tables.\nIt achieves the following results on the evaluation set:\nLoss: 0.0534\nPrecision: 0.9545\nRecall: 0.9647\nF1: 0.9596\nAccuracy: 0.9917\nModel description\nThis model can extract nutrient values from photos of food packaging containing nutrition information. This was developped as part of the Nutrisight project.\nFor more information about the project, please refer to the nutrisight directory in the openfoodfacts-ai GitHub repository.\nAs any model using the LayoutLM architecture, this model expects as input:\nthe image\nthe tokens (string) on the images\nthe 2D coordinates of each token\nThe tokens and their 2D positions is provided by an OCR model. This model was trained using OCR results coming from Google Cloud Vision.\nAn ONNX export of the model can be found in the onnx directory.\nIntended uses & limitations\nThis model is only intended to be used on images of products where a nutrition table can be found.\nTraining and evaluation data\nThe training and evaluation data can be found on the dataset page.\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 4\neval_batch_size: 4\nseed: 42\ngradient_accumulation_steps: 8\ntotal_train_batch_size: 32\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\ntraining_steps: 3000\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nPrecision\nRecall\nF1\nAccuracy\n1.9852\n0.1664\n15\n1.1500\n0.0\n0.0\n0.0\n0.8101\n1.0244\n0.3329\n30\n0.8342\n0.05\n0.0074\n0.0129\n0.8123\n0.7826\n0.4993\n45\n0.6795\n0.0789\n0.1138\n0.0932\n0.8479\n0.6767\n0.6657\n60\n0.5963\n0.1193\n0.1644\n0.1383\n0.8578\n0.6031\n0.8322\n75\n0.5406\n0.1671\n0.2248\n0.1917\n0.8691\n0.5756\n0.9986\n90\n0.4935\n0.2291\n0.3112\n0.2639\n0.8778\n0.5215\n1.1650\n105\n0.4302\n0.3267\n0.3948\n0.3575\n0.8905\n0.4782\n1.3315\n120\n0.3782\n0.3939\n0.4654\n0.4267\n0.9020\n0.4208\n1.4979\n135\n0.3405\n0.4027\n0.5044\n0.4478\n0.9081\n0.3532\n1.6644\n150\n0.2930\n0.4960\n0.5820\n0.5356\n0.9252\n0.3458\n1.8308\n165\n0.2658\n0.5155\n0.6033\n0.5560\n0.9301\n0.302\n1.9972\n180\n0.2321\n0.6112\n0.7009\n0.6530\n0.9474\n0.2655\n2.1637\n195\n0.2093\n0.6471\n0.7264\n0.6845\n0.9520\n0.2598\n2.3301\n210\n0.1951\n0.7013\n0.7557\n0.7275\n0.9570\n0.2364\n2.4965\n225\n0.1794\n0.7091\n0.7743\n0.7402\n0.9590\n0.2218\n2.6630\n240\n0.1676\n0.7216\n0.7933\n0.7558\n0.9621\n0.206\n2.8294\n255\n0.1572\n0.7436\n0.8110\n0.7758\n0.9650\n0.2053\n2.9958\n270\n0.1580\n0.7381\n0.8114\n0.7730\n0.9640\n0.1876\n3.1623\n285\n0.1406\n0.7738\n0.8309\n0.8013\n0.9687\n0.1602\n3.3287\n300\n0.1420\n0.7714\n0.8277\n0.7986\n0.9671\n0.1706\n3.4951\n315\n0.1323\n0.7933\n0.8379\n0.8150\n0.9691\n0.1585\n3.6616\n330\n0.1313\n0.8060\n0.8551\n0.8298\n0.9700\n0.1574\n3.8280\n345\n0.1267\n0.8129\n0.8639\n0.8376\n0.9717\n0.15\n3.9945\n360\n0.1157\n0.8336\n0.8746\n0.8536\n0.9754\n0.1192\n4.1609\n375\n0.1120\n0.8348\n0.8709\n0.8525\n0.9741\n0.1313\n4.3273\n390\n0.1130\n0.8395\n0.8792\n0.8589\n0.9745\n0.1179\n4.4938\n405\n0.1093\n0.8370\n0.8871\n0.8613\n0.9755\n0.1327\n4.6602\n420\n0.1102\n0.8400\n0.8853\n0.8621\n0.9746\n0.1323\n4.8266\n435\n0.0997\n0.8611\n0.8987\n0.8795\n0.9782\n0.1254\n4.9931\n450\n0.0949\n0.8499\n0.8969\n0.8728\n0.9775\n0.0999\n5.1595\n465\n0.0847\n0.8658\n0.8992\n0.8822\n0.9797\n0.1017\n5.3259\n480\n0.0803\n0.8747\n0.9108\n0.8924\n0.9810\n0.091\n5.4924\n495\n0.0796\n0.8784\n0.9057\n0.8918\n0.9806\n0.0979\n5.6588\n510\n0.0943\n0.8607\n0.8950\n0.8775\n0.9773\n0.1024\n5.8252\n525\n0.0804\n0.8710\n0.9062\n0.8882\n0.9805\n0.0952\n5.9917\n540\n0.0787\n0.8845\n0.9178\n0.9008\n0.9816\n0.0742\n6.1581\n555\n0.0776\n0.8918\n0.9150\n0.9033\n0.9823\n0.0764\n6.3245\n570\n0.0721\n0.9028\n0.9187\n0.9107\n0.9837\n0.0813\n6.4910\n585\n0.0664\n0.9065\n0.9229\n0.9146\n0.9844\n0.0791\n6.6574\n600\n0.0642\n0.9026\n0.9252\n0.9138\n0.9848\n0.0792\n6.8239\n615\n0.0673\n0.8964\n0.9248\n0.9104\n0.9841\n0.078\n6.9903\n630\n0.0693\n0.8938\n0.9224\n0.9079\n0.9833\n0.0678\n7.1567\n645\n0.0672\n0.9082\n0.9327\n0.9203\n0.9852\n0.0685\n7.3232\n660\n0.0655\n0.8926\n0.9224\n0.9073\n0.9840\n0.0555\n7.4896\n675\n0.0615\n0.9156\n0.9271\n0.9213\n0.9856\n0.07\n7.6560\n690\n0.0587\n0.9173\n0.9373\n0.9272\n0.9868\n0.065\n7.8225\n705\n0.0558\n0.9205\n0.9405\n0.9304\n0.9875\n0.0599\n7.9889\n720\n0.0579\n0.9253\n0.9433\n0.9342\n0.9878\n0.0571\n8.1553\n735\n0.0593\n0.9148\n0.9331\n0.9239\n0.9866\n0.0563\n8.3218\n750\n0.0605\n0.9152\n0.9322\n0.9236\n0.9863\n0.0602\n8.4882\n765\n0.0581\n0.9252\n0.9308\n0.9280\n0.9863\n0.0582\n8.6546\n780\n0.0581\n0.9206\n0.9373\n0.9289\n0.9872\n0.0514\n8.8211\n795\n0.0557\n0.9245\n0.9382\n0.9313\n0.9873\n0.0467\n8.9875\n810\n0.0520\n0.9291\n0.9498\n0.9394\n0.9883\n0.0435\n9.1540\n825\n0.0526\n0.9229\n0.9447\n0.9337\n0.9880\n0.0531\n9.3204\n840\n0.0502\n0.9249\n0.9443\n0.9345\n0.9884\n0.0502\n9.4868\n855\n0.0545\n0.9171\n0.9452\n0.9309\n0.9874\n0.0377\n9.6533\n870\n0.0618\n0.9077\n0.9368\n0.9221\n0.9851\n0.0416\n9.8197\n885\n0.0549\n0.9267\n0.9392\n0.9329\n0.9881\n0.044\n9.9861\n900\n0.0529\n0.9366\n0.9475\n0.9420\n0.9884\n0.0383\n10.1526\n915\n0.0490\n0.9332\n0.9475\n0.9403\n0.9889\n0.0454\n10.3190\n930\n0.0507\n0.9264\n0.9471\n0.9366\n0.9885\n0.0416\n10.4854\n945\n0.0467\n0.9364\n0.9498\n0.9430\n0.9891\n0.0403\n10.6519\n960\n0.0499\n0.9314\n0.9457\n0.9385\n0.9886\n0.0354\n10.8183\n975\n0.0523\n0.9258\n0.9452\n0.9354\n0.9883\n0.0338\n10.9847\n990\n0.0521\n0.9214\n0.9424\n0.9318\n0.9880\n0.0347\n11.1512\n1005\n0.0539\n0.9235\n0.9475\n0.9354\n0.9880\n0.0364\n11.3176\n1020\n0.0560\n0.9194\n0.9480\n0.9335\n0.9871\n0.0363\n11.4840\n1035\n0.0509\n0.9286\n0.9480\n0.9382\n0.9889\n0.0308\n11.6505\n1050\n0.0498\n0.9389\n0.9484\n0.9436\n0.9893\n0.032\n11.8169\n1065\n0.0491\n0.9364\n0.9443\n0.9403\n0.9891\n0.0331\n11.9834\n1080\n0.0455\n0.9373\n0.9443\n0.9408\n0.9892\n0.0301\n12.1498\n1095\n0.0486\n0.9359\n0.9489\n0.9423\n0.9892\n0.0308\n12.3162\n1110\n0.0513\n0.9325\n0.9503\n0.9413\n0.9891\n0.0253\n12.4827\n1125\n0.0510\n0.9296\n0.9503\n0.9398\n0.9892\n0.0301\n12.6491\n1140\n0.0533\n0.9308\n0.9489\n0.9397\n0.9886\n0.0328\n12.8155\n1155\n0.0549\n0.9287\n0.9443\n0.9364\n0.9885\n0.0298\n12.9820\n1170\n0.0504\n0.9402\n0.9498\n0.9450\n0.9895\n0.0256\n13.1484\n1185\n0.0515\n0.9354\n0.9419\n0.9387\n0.9888\n0.0313\n13.3148\n1200\n0.0483\n0.9418\n0.9545\n0.9481\n0.9905\n0.022\n13.4813\n1215\n0.0463\n0.9361\n0.9531\n0.9445\n0.9899\n0.0245\n13.6477\n1230\n0.0494\n0.9368\n0.9494\n0.9430\n0.9893\n0.0251\n13.8141\n1245\n0.0493\n0.9404\n0.9531\n0.9467\n0.9898\n0.0259\n13.9806\n1260\n0.0511\n0.9386\n0.9522\n0.9454\n0.9895\n0.03\n14.1470\n1275\n0.0535\n0.9344\n0.9457\n0.9400\n0.9889\n0.0192\n14.3135\n1290\n0.0491\n0.9428\n0.9494\n0.9461\n0.9899\n0.0267\n14.4799\n1305\n0.0490\n0.9457\n0.9545\n0.9501\n0.9901\n0.0241\n14.6463\n1320\n0.0506\n0.9435\n0.9540\n0.9487\n0.9899\n0.0211\n14.8128\n1335\n0.0510\n0.9444\n0.9540\n0.9492\n0.9903\n0.0171\n14.9792\n1350\n0.0499\n0.9405\n0.9545\n0.9474\n0.9898\n0.0226\n15.1456\n1365\n0.0511\n0.9366\n0.9540\n0.9452\n0.9894\n0.024\n15.3121\n1380\n0.0484\n0.9445\n0.9559\n0.9501\n0.9899\n0.018\n15.4785\n1395\n0.0482\n0.9469\n0.9517\n0.9493\n0.9903\n0.0191\n15.6449\n1410\n0.0491\n0.9442\n0.9512\n0.9477\n0.9899\n0.0203\n15.8114\n1425\n0.0451\n0.9510\n0.9554\n0.9532\n0.9912\n0.0198\n15.9778\n1440\n0.0447\n0.9497\n0.9549\n0.9523\n0.9911\n0.0167\n16.1442\n1455\n0.0444\n0.9487\n0.9540\n0.9514\n0.9909\n0.0178\n16.3107\n1470\n0.0513\n0.9386\n0.9512\n0.9449\n0.9892\n0.024\n16.4771\n1485\n0.0502\n0.9430\n0.9536\n0.9483\n0.9899\n0.0206\n16.6436\n1500\n0.0459\n0.9483\n0.9545\n0.9514\n0.9908\n0.0188\n16.8100\n1515\n0.0469\n0.9474\n0.9540\n0.9507\n0.9906\n0.016\n16.9764\n1530\n0.0463\n0.9468\n0.9582\n0.9524\n0.9906\n0.0161\n17.1429\n1545\n0.0455\n0.9516\n0.9596\n0.9556\n0.9911\n0.0135\n17.3093\n1560\n0.0475\n0.9524\n0.9573\n0.9548\n0.9909\n0.0148\n17.4757\n1575\n0.0479\n0.9440\n0.9545\n0.9492\n0.9905\n0.0173\n17.6422\n1590\n0.0455\n0.9539\n0.9605\n0.9572\n0.9915\n0.0173\n17.8086\n1605\n0.0456\n0.9475\n0.9554\n0.9514\n0.9913\n0.0185\n17.9750\n1620\n0.0461\n0.9498\n0.9577\n0.9537\n0.9908\n0.0153\n18.1415\n1635\n0.0472\n0.9491\n0.9605\n0.9548\n0.9911\n0.0148\n18.3079\n1650\n0.0446\n0.9507\n0.9587\n0.9547\n0.9913\n0.0136\n18.4743\n1665\n0.0441\n0.9486\n0.9601\n0.9543\n0.9914\n0.0185\n18.6408\n1680\n0.0478\n0.9528\n0.9573\n0.9551\n0.9915\n0.0147\n18.8072\n1695\n0.0493\n0.9515\n0.9652\n0.9583\n0.9912\n0.0156\n18.9736\n1710\n0.0509\n0.9440\n0.9545\n0.9492\n0.9903\n0.0113\n19.1401\n1725\n0.0460\n0.9559\n0.9573\n0.9566\n0.9911\n0.014\n19.3065\n1740\n0.0493\n0.9439\n0.9526\n0.9482\n0.9905\n0.0147\n19.4730\n1755\n0.0498\n0.9476\n0.9568\n0.9522\n0.9906\n0.0126\n19.6394\n1770\n0.0493\n0.9474\n0.9531\n0.9502\n0.9906\n0.0167\n19.8058\n1785\n0.0491\n0.9463\n0.9577\n0.9520\n0.9904\n0.0126\n19.9723\n1800\n0.0474\n0.9492\n0.9540\n0.9516\n0.9908\n0.0107\n20.1387\n1815\n0.0462\n0.9524\n0.9577\n0.9551\n0.9914\n0.0115\n20.3051\n1830\n0.0481\n0.9504\n0.9614\n0.9559\n0.9911\n0.0128\n20.4716\n1845\n0.0486\n0.9475\n0.9563\n0.9519\n0.9907\n0.0113\n20.6380\n1860\n0.0491\n0.9477\n0.9591\n0.9534\n0.9910\n0.0119\n20.8044\n1875\n0.0514\n0.9494\n0.9503\n0.9499\n0.9901\n0.0122\n20.9709\n1890\n0.0480\n0.9481\n0.9591\n0.9536\n0.9911\n0.0123\n21.1373\n1905\n0.0477\n0.9467\n0.9577\n0.9522\n0.9909\n0.0116\n21.3037\n1920\n0.0486\n0.9485\n0.9582\n0.9533\n0.9910\n0.0108\n21.4702\n1935\n0.0488\n0.9442\n0.9582\n0.9511\n0.9905\n0.0115\n21.6366\n1950\n0.0472\n0.9498\n0.9587\n0.9542\n0.9913\n0.0083\n21.8031\n1965\n0.0476\n0.9490\n0.9596\n0.9543\n0.9911\n0.0094\n21.9695\n1980\n0.0475\n0.9482\n0.9605\n0.9543\n0.9909\n0.0118\n22.1359\n1995\n0.0492\n0.9449\n0.9554\n0.9501\n0.9904\n0.01\n22.3024\n2010\n0.0486\n0.9492\n0.9554\n0.9523\n0.9909\n0.0114\n22.4688\n2025\n0.0497\n0.9502\n0.9577\n0.9540\n0.9910\n0.0091\n22.6352\n2040\n0.0499\n0.9503\n0.9582\n0.9542\n0.9910\n0.0077\n22.8017\n2055\n0.0502\n0.9513\n0.9614\n0.9563\n0.9911\n0.01\n22.9681\n2070\n0.0513\n0.9544\n0.9628\n0.9586\n0.9913\n0.0087\n23.1345\n2085\n0.0485\n0.9500\n0.9610\n0.9554\n0.9912\n0.0073\n23.3010\n2100\n0.0485\n0.9557\n0.9628\n0.9593\n0.9917\n0.0083\n23.4674\n2115\n0.0485\n0.9535\n0.9610\n0.9572\n0.9913\n0.0117\n23.6338\n2130\n0.0479\n0.9557\n0.9624\n0.9590\n0.9916\n0.0095\n23.8003\n2145\n0.0508\n0.9498\n0.9587\n0.9542\n0.9911\n0.009\n23.9667\n2160\n0.0513\n0.9492\n0.9628\n0.9560\n0.9910\n0.0077\n24.1331\n2175\n0.0504\n0.9553\n0.9628\n0.9591\n0.9915\n0.0087\n24.2996\n2190\n0.0500\n0.9521\n0.9610\n0.9565\n0.9913\n0.0068\n24.4660\n2205\n0.0506\n0.9539\n0.9610\n0.9574\n0.9913\n0.0094\n24.6325\n2220\n0.0500\n0.9507\n0.9591\n0.9549\n0.9913\n0.0088\n24.7989\n2235\n0.0486\n0.9508\n0.9596\n0.9552\n0.9914\n0.0089\n24.9653\n2250\n0.0507\n0.9508\n0.9610\n0.9559\n0.9911\n0.0063\n25.1318\n2265\n0.0479\n0.9561\n0.9610\n0.9585\n0.9917\n0.0058\n25.2982\n2280\n0.0506\n0.9526\n0.9619\n0.9572\n0.9911\n0.0102\n25.4646\n2295\n0.0499\n0.9526\n0.9624\n0.9575\n0.9912\n0.0079\n25.6311\n2310\n0.0543\n0.9469\n0.9614\n0.9541\n0.9905\n0.009\n25.7975\n2325\n0.0498\n0.9526\n0.9619\n0.9572\n0.9915\n0.0068\n25.9639\n2340\n0.0511\n0.9509\n0.9619\n0.9564\n0.9911\n0.007\n26.1304\n2355\n0.0492\n0.9527\n0.9633\n0.9580\n0.9914\n0.0086\n26.2968\n2370\n0.0516\n0.9500\n0.9610\n0.9554\n0.9913\n0.0078\n26.4632\n2385\n0.0503\n0.9504\n0.9610\n0.9557\n0.9914\n0.0067\n26.6297\n2400\n0.0514\n0.9527\n0.9628\n0.9577\n0.9915\n0.0059\n26.7961\n2415\n0.0504\n0.9549\n0.9628\n0.9588\n0.9919\n0.0089\n26.9626\n2430\n0.0520\n0.9517\n0.9605\n0.9561\n0.9916\n0.0059\n27.1290\n2445\n0.0512\n0.9522\n0.9624\n0.9573\n0.9917\n0.0073\n27.2954\n2460\n0.0526\n0.9530\n0.9610\n0.9570\n0.9916\n0.0065\n27.4619\n2475\n0.0530\n0.9527\n0.9628\n0.9577\n0.9916\n0.0064\n27.6283\n2490\n0.0515\n0.9535\n0.9610\n0.9572\n0.9917\n0.0072\n27.7947\n2505\n0.0542\n0.9482\n0.9610\n0.9546\n0.9907\n0.0066\n27.9612\n2520\n0.0537\n0.9491\n0.9610\n0.9550\n0.9909\n0.006\n28.1276\n2535\n0.0518\n0.9531\n0.9628\n0.9579\n0.9915\n0.0074\n28.2940\n2550\n0.0523\n0.9521\n0.9610\n0.9565\n0.9914\n0.0068\n28.4605\n2565\n0.0534\n0.9495\n0.9614\n0.9555\n0.9913\n0.0055\n28.6269\n2580\n0.0521\n0.9548\n0.9619\n0.9584\n0.9917\n0.0056\n28.7933\n2595\n0.0526\n0.9522\n0.9614\n0.9568\n0.9913\n0.0066\n28.9598\n2610\n0.0527\n0.9522\n0.9619\n0.9570\n0.9913\n0.0053\n29.1262\n2625\n0.0533\n0.9531\n0.9628\n0.9579\n0.9913\n0.0063\n29.2926\n2640\n0.0520\n0.9530\n0.9610\n0.9570\n0.9913\n0.0059\n29.4591\n2655\n0.0533\n0.9504\n0.9605\n0.9554\n0.9910\n0.0059\n29.6255\n2670\n0.0532\n0.9526\n0.9619\n0.9572\n0.9912\n0.0062\n29.7920\n2685\n0.0516\n0.9535\n0.9624\n0.9579\n0.9917\n0.0064\n29.9584\n2700\n0.0515\n0.9522\n0.9624\n0.9573\n0.9915\n0.0055\n30.1248\n2715\n0.0513\n0.9549\n0.9633\n0.9591\n0.9917\n0.0064\n30.2913\n2730\n0.0524\n0.9540\n0.9628\n0.9584\n0.9916\n0.0055\n30.4577\n2745\n0.0530\n0.9531\n0.9633\n0.9582\n0.9915\n0.0065\n30.6241\n2760\n0.0528\n0.9536\n0.9642\n0.9589\n0.9917\n0.0068\n30.7906\n2775\n0.0530\n0.9518\n0.9633\n0.9575\n0.9916\n0.0047\n30.9570\n2790\n0.0545\n0.9532\n0.9647\n0.9589\n0.9916\n0.0051\n31.1234\n2805\n0.0534\n0.9545\n0.9647\n0.9596\n0.9917\n0.0044\n31.2899\n2820\n0.0532\n0.9531\n0.9633\n0.9582\n0.9914\n0.0068\n31.4563\n2835\n0.0532\n0.9527\n0.9633\n0.9580\n0.9913\n0.0045\n31.6227\n2850\n0.0531\n0.9545\n0.9638\n0.9591\n0.9915\n0.0047\n31.7892\n2865\n0.0530\n0.9540\n0.9633\n0.9586\n0.9916\n0.0075\n31.9556\n2880\n0.0533\n0.9549\n0.9638\n0.9593\n0.9916\n0.0055\n32.1221\n2895\n0.0525\n0.9553\n0.9638\n0.9595\n0.9917\n0.006\n32.2885\n2910\n0.0523\n0.9553\n0.9638\n0.9595\n0.9917\n0.0062\n32.4549\n2925\n0.0525\n0.9544\n0.9633\n0.9589\n0.9917\n0.0059\n32.6214\n2940\n0.0525\n0.9549\n0.9638\n0.9593\n0.9917\n0.0058\n32.7878\n2955\n0.0531\n0.9549\n0.9642\n0.9596\n0.9917\n0.005\n32.9542\n2970\n0.0533\n0.9536\n0.9633\n0.9584\n0.9916\n0.007\n33.1207\n2985\n0.0533\n0.9536\n0.9633\n0.9584\n0.9916\n0.0047\n33.2871\n3000\n0.0532\n0.9536\n0.9633\n0.9584\n0.9916\nFramework versions\nTransformers 4.40.2\nPytorch 2.5.1\nDatasets 2.19.0\nTokenizers 0.19.1",
    "CalamitousFelicitousness/Qwen2.5-72B-Instruct-fp8-dynamic": "Qwen2.5-72B-Instruct\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-72B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 72B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 72.7B\nNumber of Paramaters (Non-Embedding): 70.0B\nNumber of Layers: 80\nNumber of Attention Heads (GQA): 64 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-72B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "mlx-community/Qwen2.5-Coder-1.5B-bf16": "mlx-community/Qwen2.5-Coder-1.5B-bf16\nUse with mlx\nmlx-community/Qwen2.5-Coder-1.5B-bf16\nThe Model mlx-community/Qwen2.5-Coder-1.5B-bf16 was converted to MLX format from Qwen/Qwen2.5-Coder-1.5B using mlx-lm version 0.18.1.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/Qwen2.5-Coder-1.5B-bf16\")\nresponse = generate(model, tokenizer, prompt=\"hello\", verbose=True)",
    "ByteDance/HLLM": "Model Card for HLLM\nComparison with state-of-the-art methods (HLLM)\nCite our work\nModel Card for HLLM\nThis repo is used for hosting HLLM and HLLM-Creator checkpoints.\nFor more details or tutorials see https://github.com/bytedance/HLLM.\nHierarchical Large Language Model (HLLM) architecture is designed to enhance sequential recommendation systems:\nHLLM significantly outperforms classical ID-based models on large-scale academic datasets and has been validated to yield tangible benefits in real-world industrial settings. Additionally, this method demonstrates excellent training and serving efficiency.\nHLLM effectively transfers the world knowledge encoded during the LLM pre-training stage into the recommendation model, encompassing both item feature extraction and user interest modeling. Nevertheless, task-specific fine-tuning with recommendation objectives is essential.\nHLLM exhibits excellent scalability, with performance continuously improving as the data volume and model parameters increase. This scalability highlights the potential of the proposed approach when applied to even larger datasets and model sizes.\nHLLM-Creator is designed for personalized creative generation:\nHLLM-Creator enables precise user interest modeling and fine-grained content personalization.\nA Chain-of-Thought-based data construction pipeline is developed to expand personalization space and ensure factual consistency, effectively reducing hallucinations in generated titles.\nA flexible and efficient inference scheme is developed for large-scale industrial deployment, with significant positive results in Douyin search advertising demonstrating its real-world impact.\nComparison with state-of-the-art methods (HLLM)\nMethod\nDataset\nNegatives\nR@10\nR@50\nR@200\nN@10\nN@50\nN@200\nHSTU\nPixel8M\n5632\n4.83\n10.30\n18.28\n2.75\n3.94\n5.13\nSASRec\nPixel8M\n5632\n5.08\n10.62\n18.64\n2.92\n4.12\n5.32\nHLLM-1B\nPixel8M\n5632\n6.13\n12.48\n21.18\n3.54\n4.92\n6.22\nHSTU-large\nBooks\n512\n5.00\n11.29\n20.13\n2.78\n4.14\n5.47\nSASRec\nBooks\n512\n5.35\n11.91\n21.02\n2.98\n4.40\n5.76\nHLLM-1B\nBooks\n512\n6.97\n14.61\n24.78\n3.98\n5.64\n7.16\nHSTU-large\nBooks\n28672\n6.50\n12.22\n19.93\n4.04\n5.28\n6.44\nHLLM-1B\nBooks\n28672\n9.28\n17.34\n27.22\n5.65\n7.41\n8.89\nHLLM-7B\nBooks\n28672\n9.39\n17.65\n27.59\n5.69\n7.50\n8.99\nCite our work\n@article{HLLM,\ntitle={HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling},\nauthor={Junyi Chen and Lu Chi and Bingyue Peng and Zehuan Yuan},\njournal={arXiv preprint arXiv:2409.12740},\nyear={2024}\n}\n@article{HLLM-Creator,\ntitle={HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation},\nauthor={Junyi Chen and Lu Chi and Siliang Xu and Shiwei Ran and Bingyue Peng and Zehuan Yuan},\njournal={arXiv preprint arXiv:2508.18118},\nyear={2025}\n}",
    "FireRedTeam/FireRedTTS": "News\nClone and install\nDownload models\nBasic Usage\nRoadmap\nClone and install\nDownload models\nBasic Usage\nUsage\nClone and install\nDownload models\nBasic Usage\nTips\nAcknowledgements\nFireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications\nüëâüèª FireRedTTS Paper üëàüèª\nüëâüèª FireRedTTS Demos üëàüèª\nüëâüèª FireRedTTS Space (Interactive Demo) üëàüèª\nNews\n[2024/10/17] üî• We release new rich-punctuation model, offering expanded punctuation coverage and enhanced audio production consistency. In addition, we have strengthened the capabilities of the text front-end and enhanced the stability of synthesis.\n[2024/09/26] üî• Our model is already available on huggingface spaceÔºåtry it through the interactive interface.\n[2024/09/20] üî• We release the pre-trained checkpoints and inference code.\n[2024/09/06] üî• We release the technical report and project page\nRoadmap\n2024/09\nRelease the pre-trained checkpoints and inference code.\nRelease testing set.\n2024/10\nRelease rich punctuation version.\nRelease finetuned checkpoints for controllable human-like speech generation.\nUsage\nClone and install\nClone the repo\nhttps://github.com/FireRedTeam/FireRedTTS.git\ncd FireRedTTS\nCreate conda env\n# step1.create env\nconda create --name redtts python=3.10\n# stpe2.install torch Ôºàpytorch should match the cuda-version on your machineÔºâ\n# CUDA 11.8\nconda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=11.8 -c pytorch -c nvidia\n# CUDA 12.1\nconda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=12.1 -c pytorch -c nvidia\n# step3.install fireredtts form source\npip install -e .\n# step4.install other requirements\npip install -r requirements.txt\nDownload models\nDownload the required model files from Model_Lists and place them in the folder pretrained_models\nBasic Usage\nimport os\nimport torchaudio\nfrom fireredtts.fireredtts import FireRedTTS\ntts = FireRedTTS(\nconfig_path=\"configs/config_24k.json\",\npretrained_path=<pretrained_models_dir>,\n)\n#same language\nrec_wavs = tts.synthesize(\nprompt_wav=\"examples/prompt_1.wav\",\ntext=\"Â∞èÁ∫¢‰π¶ÔºåÊòØ‰∏≠ÂõΩÂ§ßÈôÜÁöÑÁΩëÁªúË¥≠Áâ©ÂíåÁ§æ‰∫§Âπ≥Âè∞ÔºåÊàêÁ´ã‰∫é‰∫åÈõ∂‰∏Ä‰∏âÂπ¥ÂÖ≠Êúà„ÄÇ\",\nlang=\"zh\",\n)\nrec_wavs = rec_wavs.detach().cpu()\nout_wav_path = os.path.join(\"./example.wav\")\ntorchaudio.save(out_wav_path, rec_wavs, 24000)\nTips\nRemoving the long silence (>1s) in the middle of prompt_wav may bring better stability. If there are too many long silences in your prompt_wav and it causes stability problems, it is recommended to use our tool(tools/process_prompts.py) to remove the silence.\nAcknowledgements\nTortoise-tts and XTTS-v2 offer invaluable insights for constructing an autoregressive-style system.\nMatcha-TTS and CosyVoice demonstrate the excellent ability of flow-matching in converting audio code to mel.\nBigVGAN-v2, utilized for vocoding.\nWe referred to whisper‚Äôs text tokenizer solution.",
    "Qwen/Qwen2.5-Math-7B-Instruct": "Qwen2.5-Math-7B-Instruct\nIntroduction\nModel Details\nRequirements\nQuick Start\nü§ó Hugging Face Transformers\nCitation\nQwen2.5-Math-7B-Instruct\nüö® Qwen2.5-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks.\nIntroduction\nIn August 2024, we released the first series of mathematical LLMs - Qwen2-Math - of our Qwen family. A month later, we have upgraded it and open-sourced Qwen2.5-Math series, including base models Qwen2.5-Math-1.5B/7B/72B, instruction-tuned models Qwen2.5-Math-1.5B/7B/72B-Instruct, and mathematical reward model Qwen2.5-Math-RM-72B.\nUnlike Qwen2-Math series which only supports using Chain-of-Thught (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT.\nWhile CoT plays a vital role in enhancing the reasoning capabilities of LLMs, it faces challenges in achieving computational accuracy and handling complex mathematical or algorithmic reasoning tasks, such as finding the roots of a quadratic equation or computing the eigenvalues of a matrix. TIR can further improve the model's proficiency in precise computation, symbolic manipulation, and algorithmic manipulation. Qwen2.5-Math-1.5B/7B/72B-Instruct achieve 79.7, 85.3, and 87.8 respectively on the MATH benchmark using TIR.\nModel Details\nFor more details, please refer to our blog post and GitHub repo.\nRequirements\ntransformers>=4.37.0 for Qwen2.5-Math models. The latest version is recommended.\nüö® This is a must because transformers integrated Qwen2 codes since 4.37.0.\nFor requirements on GPU memory and the respective throughput, see similar results of Qwen2 here.\nQuick Start\nQwen2.5-Math-7B-Instruct is an instruction model for chatting;\nQwen2.5-Math-7B is a base model typically used for completion and few-shot inference, serving as a better starting point for fine-tuning.\nü§ó Hugging Face Transformers\nQwen2.5-Math can be deployed and infered in the same way as Qwen2.5. Here we show a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\"\n# CoT\nmessages = [\n{\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\n# TIR\nmessages = [\n{\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nCitation\nIf you find our work helpful, feel free to give us a citation.\n@article{yang2024qwen25mathtechnicalreportmathematical,\ntitle={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement},\nauthor={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},\njournal={arXiv preprint arXiv:2409.12122},\nyear={2024}\n}",
    "OnomaAIResearch/Illustrious-xl-early-release-v0": "Illustrious XL Series Update\nDevelopment Journey\nOur Model Versions\nOur Future Plans\nCommitment to Open Source\nBackward Compatibility\nSummary\nIllustrious XL v0.1 trained by Onoma AI\nIllustrious XL is the Illustration focused Stable Diffusion XL model which is continued from Kohaku XL Beta 5, trained by OnomaAI Research Team.\nThe model focuses on utilizing large-scale annotated dataset, Danbooru2023.\nWe release the v0.1 and v0.1-GUIDED model here, under fair public ai license, however discourages the usage of model over monetization purpose / any closed source purposes.\nFor full technical details, please refer to our technical report.\nModel Information:\nName: Illustrious-XL-v0.1\nModel Type: Stable Diffusion XL Model\nDataset: Fine-tuned on Danbooru2023 Dataset\nDescription:\nIllustrious-XL is a powerful generative model series, fine-tuned on the comprehensive Danbooru2023 dataset and its variants. It includes a wide variety of character designs, styles, and artistic knowledge derived from the dataset, making it suitable for creative and artistic AI generation tasks.\nIllustrious-XL-v0.1 is untuned BASE model, which works as possible base for all future model variants. LoRAs / Adapters can be trained on this model, ensuring future usecases. The model is research-only purpose, as not tuned for aesthetics / preferences.\nIllustrious-XL-v0.1-GUIDED is minimally safety controlled model, which works as better option for usual usecases.\nWe plan to release several aesthetic-finetuned model variants in near future.\nTechnical Details:\nhttps://arxiv.org/abs/2409.19946\nTerms and Conditions:\nWe recommend to use official repositories, to prevent malicious attacks.\nUsers must agree with LICENSE to use the model. As mentioned in LICENSE, we do NOT take any actions about generated results or possible variants.\nAs mentioned in LICENSE, users must NOT use the generated result for any prohibited purposes, including but not limited to:\nHarmful or malicious activities: This includes harassment, threats, spreading misinformation, or any use intended to harm individuals or groups.\nIllegal activities: Using generated content to violate any applicable laws or regulations.\nUnethical, offensive content generation: Generating offensive, defamatory, or controversial content that violates ethical guidelines.\nBy using this model, users agree to comply with the conditions outlined in the LICENSE and acknowledge responsibility for how they utilize the generated content.\nSafety Control Recommendation:\nGenerative models can occasionally produce unintended or harmful outputs.\nTo minimize this risk, it is strongly recommended to use the GUIDED model variant, which incorporates additional safety mechanisms for responsible content generation.\nBy choosing this variant, users can significantly reduce the likelihood of generating harmful or unintended content.\nWe plan to update GUIDED model variants and its methodologies, with extensive research.\nTraining/Merging Policy:\nYou may fine-tune, merge, or train LoRA based on this model. However, to foster an open-source community, you are required to:\nOpenly share details of any derived models, including references to the original model licensed under the fair-ai-public-license.\nProvide information on datasets and \"merge recipes\" used for fine-tuning or training.\nAdhere to the fair-ai-public-license, ensuring that any derivative works are also open source.\nUploading / Generation Policy:\nWe do not restrict any upload or spread of the generation results, as we do not own any rights regard to generated materials. This includes 'personally trained models / finetuned models / trained lora-related results'. However, we kindly ask you to open the generation details, to foster the open source communities and researches.\nMonetization Prohibition:\nYou are prohibited from monetizing any close-sourced fine-tuned / merged model, which disallows the public from accessing the model's source code / weights and its usages.\nAs per the license, you must openly publish any derivative models and variants. This model is intended for open-source use, and all derivatives must follow the same principles.\nUsage:\nWe do not recommend overusing critical composition tags such as 'close-up', 'upside-down', or 'cowboy shot', as they can be conflicting and lead to confusion, affecting model results.\nRecommended sampling method: Euler a, Sampling Steps: 20‚Äì28, CFG: 5‚Äì7.5 (may vary based on use case).\nWe suggest using suitable composition tags like \"upper body,\" \"cowboy shot,\" \"portrait,\" or \"full body\" depending on your use case.\nThe model supports quality tags such as: \"worst quality,\" \"bad quality,\" \"average quality,\" \"good quality,\" \"best quality,\" and \"masterpiece (quality).\"\nNote: The model does not have any default style. This is intended behavior for the base model.\nPrompt:\n1boy, holding knife, blue eyes, jewelry, jacket, shirt, open mouth, hand up, simple background, hair between eyes, vest, knife, tongue, holding weapon, grey vest, upper body, necktie, solo, looking at viewer, smile, pink blood, weapon, dagger, open clothes, collared shirt, blood on face, tongue out, blonde hair, holding dagger, red necktie, white shirt, blood, short hair, holding, earrings, long sleeves, black jacket, dark theme\nNegative Prompt:\nworst quality, comic, multiple views, bad quality, low quality, lowres, displeasing, very displeasing, bad anatomy, bad hands, scan artifacts, monochrome, greyscale, signature, twitter username, jpeg artifacts, 2koma, 4koma, guro, extra digits, fewer digits\nPrompt:\n1girl, extremely dark, black theme, silhouette, rim lighting, black, looking at viewer, low contrast, masterpiece\nNegative Prompt:\nworst quality, comic, multiple views, bad quality, low quality, lowres, displeasing, very displeasing, bad anatomy, bad hands, scan artifacts, monochrome, greyscale, twitter username, jpeg artifacts, 2koma, 4koma, guro, extra digits, fewer digits, jaggy lines, unclear\nIllustrious XL Series Update\nIt‚Äôs been a while since we released Illustrious XL v0.1, and we know many of you have been eagerly waiting for updates. We also recognize that many are disappointed with the closed-source nature of Illustrious XL v1.0, and we want to address this directly. A lot has happened since then, and we‚Äôre truly grateful for the open-source community‚Äôs contributions‚Äîwhether it‚Äôs large-scale fine-tuned models, ControlNets, or the countless LoRAs and adapters that have been developed.\nDevelopment Journey\nWhen we started working on the Illustrious XL series, our goal was simple: there weren‚Äôt any strong pretrained models available for illustrations, so we decided to build one ourselves‚Äîa pretrain-level fine-tuned model that artists and researchers could actually use.\nWe also knew that keeping everything in-house wouldn‚Äôt help the field move forward. That‚Äôs why we released v0.1 to the public and focused on training newer variations, pushing the model‚Äôs capabilities further with improved quality, deeper knowledge, and architectural refinements.\nAlong the way, we discovered something unexpected. The model wasn‚Äôt just good at illustrations‚Äîit could also interpret natural language, handle complex prompts, and generate high-resolution images, far beyond what we originally planned.\nOur Model Versions\nv0.1 (trained in May 2024)\nv1.0 (July 2024)\nv1.1 (August 2024)\nv2.0 (September 2024)\nv3 (November 2024)\nv3.5 (a special variant incorporating Google‚Äôs v-parameterization)\nThese models take another step forward in natural language composition and image generation.\nThat said, we can‚Äôt drop everything all at once. There‚Äôs a clear roadmap ahead, and open-source releases are part of it. But rather than rushing, we want to do this the right way‚Äîwith explanations, insights, and research-backed improvements.\nOur Future Plans\nNow, after months of work behind the scenes, we‚Äôre finally ready to move forward. We‚Äôll be rolling out our latest models step by step while progressively open-sourcing previous versions so they can be studied and improved upon. Expect breakthroughs like true 2K-resolution generation and better natural language alignment along the way.\nCommitment to Open Source\nThis will take time, but we‚Äôre moving fast. Our next-generation models are already in development, tackling some of the fundamental limitations of the base SD XL architecture. As we progress, older models will naturally be deprecated, and weight releases will follow accordingly. Our team aims to proceed thoughtfully, ensuring that each release is accompanied by comprehensive explanations and insights.\nBackward Compatibility\nOne last thing‚Äîwe‚Äôre not just here to release models. Every model we‚Äôve built is designed with backward compatibility in mind, because Illustrious XL wasn‚Äôt just about making something new‚Äîit was about creating a better foundation for fine-tuning. That‚Äôs why we‚Äôve put so much effort into training LoRAs properly, and soon, we‚Äôll be sharing insights on how to train them more effectively.\nSummary\nIn summary, Onoma AI plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments‚Äîwe‚Äôre just getting started.",
    "meta-llama/Llama-Guard-3-11B-Vision": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.2 COMMUNITY LICENSE AGREEMENT\nLlama 3.2 Version Release Date: September 25, 2024\n‚ÄúAgreement‚Äù means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n‚ÄúDocumentation‚Äù means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n‚ÄúLicensee‚Äù or ‚Äúyou‚Äù means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n‚ÄúLlama 3.2‚Äù means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n‚ÄúLlama Materials‚Äù means, collectively, Meta‚Äôs proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n‚ÄúMeta‚Äù or ‚Äúwe‚Äù means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking ‚ÄúI Accept‚Äù below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies:  ‚ÄúLlama 3.2 is licensed under the Llama 3.2 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù).  The most recent copy of this policy can be found at https://www.llama.com/llama3_2/use-policy.\nProhibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals‚Äô identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\nPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nGet started\nUsing with transformers\nHazard Taxonomy and Policy\nTraining data\nEvaluation\nLimitations\nReferences\nCitation\nModel Information\nLlama Guard 3 Vision is a Llama-3.2-11B pretrained model, fine-tuned for content safety classification. Similar to previous versions [1-3], it can be used to safeguard content for both LLM inputs (prompt classification) and LLM responses (response classification).\nLlama Guard 3 Vision was specifically designed to support image reasoning use cases and was optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts.\nLlama Guard 3 Vision acts as an LLM ‚Äì it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated. Below is a response classification example input and output for Llama Guard 3 Vision.\nGet started\nOnce you have access to the model weights, please refer to our documentation to get started.\nUsing with transformers\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nimport torch\nfrom PIL import Image as PIL_Image\nmodel_id = \"meta-llama/Llama-Guard-3-11B-Vision\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModelForVision2Seq.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nimage = PIL_Image.open(\"<path/to/image>\").convert(\"RGB\")\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What is the recipe for mayonnaise?\"\n},\n{\n\"type\": \"image\",\n},\n],\n}\n]\ninput_prompt = processor.apply_chat_template(\nconversation, return_tensors=\"pt\"\n)\ninputs = processor(text=input_prompt, images=image, return_tensors=\"pt\").to(model.device)\nprompt_len = len(inputs['input_ids'][0])\noutput = model.generate(\n**inputs,\nmax_new_tokens=20,\npad_token_id=0,\n)\ngenerated_tokens = output[:, prompt_len:]\nprint(input_prompt)\nprint(processor.decode(generated_tokens[0]))\nThis snippet will use the categories described in this model card. You can provide your own categories instead:\ninput_prompt = processor.apply_chat_template(\nconversation,\nreturn_tensors=\"pt\",\ncategories = {\n\"S1\": \"My custom category\",\n},\n)\nOr you can exclude categories from the default list by specifying an array of category keys to exclude:\ninput_prompt = processor.apply_chat_template(\nconversation,\nreturn_tensors=\"pt\",\nexcluded_category_keys=[\"S1\"],\n)\nHazard Taxonomy and Policy\nThe model is trained to predict safety labels on the 13 categories shown below, based on the MLCommons taxonomy of 13 hazards.\nHazard categories\nS1: Violent Crimes\nS2: Non-Violent Crimes\nS3: Sex-Related Crimes\nS4: Child Sexual Exploitation\nS5: Defamation\nS6: Specialized Advice\nS7: Privacy\nS8: Intellectual Property\nS9: Indiscriminate Weapons\nS10: Hate\nS11: Suicide & Self-Harm\nS12: Sexual Content\nS13: Elections\nS1 - Violent Crimes\nResponses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\nS2 - Non-Violent Crimes\nResponses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\nS3 - Sex-Related Crimes\nResponses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\nS4 - Child Sexual Exploitation\nResponses that contain, describe, enable, encourage, or endorse the sexual abuse of children\nS5 - Defamation\nResponses that are both verifiably false and likely to injure a living person‚Äôs reputation\nS6 - Specialized Advice\nResponses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\nS7 - Privacy\nResponses that contain sensitive, nonpublic personal information that could undermine someone‚Äôs physical, digital, or financial security\nS8 - Intellectual Property\nResponses that may violate the intellectual property rights of any third party\nS9 - Indiscriminate Weapons\nResponses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\nS10 - Hate\nResponses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\nS11 - Suicide & Self-Harm\nResponses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\nS12 - Sexual Content\nResponses that contain erotica\nS13 - Elections\nResponses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections\nNote on violation for image understanding. Compared to text-only Llama Guards, specific attention was paid to risks emerging from potential prompts to identify people in images (e.g., recognizing a real world person from the image through any visual clue). Llama Guard 3 Vision was trained to classify the response as unsafe.\nTraining data\nTo train the Llama Guard 3 Vision, we employed a hybrid dataset comprising both human-generated and synthetically generated data. Our approach involved collecting human-created prompts paired with corresponding images, as well as generating benign and violating model responses using our in-house Llama models. We utilized jailbreaking techniques to elicit violating responses from these models. The resulting dataset includes samples labeled either by humans or the Llama 3.1 405B model. To ensure comprehensive coverage, we carefully curated the dataset to encompass a diverse range of prompt-image pairs, spanning all hazard categories listed above. For the image data we use, our vision encoder will rescale it into 4 chunks, each of 560x560.\nEvaluation\nWe evaluate the performance of Llama Guard 3 vision on our internal test following MLCommons hazard taxonomy. To the best of our knowledge, Llama Guard 3 Vision is the first safety classifier for the LLM image understanding task. In this regard, we use GPT-4o and GPT-4o mini with zero-shot prompting using MLCommons hazard taxonomy as a baseline.\nTable 1: Comparison of performance of various models measured on our internal test set for MLCommons hazard taxonomy.\nModel\nTask\nPrecision\nRecall\nF1\nFPR\nLlama Guard 3 Vision\nPrompt Classification\n0.891\n0.623\n0.733\n0.052\nGPT-4o\n0.544\n0.843\n0.661\n0.485\nGPT-4o mini\n0.488\n0.943\n0.643\n0.681\nLlama Guard 3 Vision\nResponse Classification\n0.961\n0.916\n0.938\n0.016\nGPT-4o\n0.579\n0.788\n0.667\n0.243\nGPT-4o mini\n0.526\n0.820\n0.641\n0.313\nTable 2: Category-wise breakdown of F1 for Llama Guard 3 Vision on our internal test set for response classification with safety labels from the ML Commons taxonomy.\nCategory\nF1\nViolent Crimes\n0.839\nNon-Violent Crimes\n0.917\nSex Crimes\n0.797\nChild Exploitation\n0.698\nDefamation\n0.967\nSpecialized Advice\n0.764\nPrivacy\n0.847\nIntellectual Property\n0.849\nIndiscriminate Weapons\n0.995\nHate\n0.894\nSelf-Harm\n0.911\nSexual Content\n0.947\nElections\n0.957\nResult Analysis. Table 1 shows that Llama Guard 3 Vision outperforms GPT-4o and GPT-4o mini in terms of F1, specifically in response classification. Noteworthily,  Llama Guard 3 Vision achieves better performance with much lower false positive rates. We also observe that the recall (false positive rates) of prompt classification are much lower (higher) than in general, this is because of the ambiguity of prompt (text+image) in the image understanding tasks making prompt classification more challenging than response classification. For example, given a text prompt ‚Äúhow do I buy this‚Äù with an image of multiple objects, whether the prompt is safe or not depends on which objects the user is referring to, and it is hard to decide in some cases. In such cases, we recommend using Llama Guard 3 Vision in the response classification task. Table 2 also shows the category breakdown in our internal test set. We observe that Llama Guard 3 Vision performs well in Indiscriminate Weapon and Elections categories, while showing > 0.69 F1 scores across all categories.\nLimitations\nThere are some limitations associated with Llama Guard 3 Vision. First, Llama Guard 3 Vision itself is an LLM fine-tuned on Llama 3.2-vision. Thus, its performance (e.g., judgments that need common sense knowledge, multilingual capability, and policy coverage) might be limited by its (pre-)training data.\nLlama Guard 3 Vision is not meant to be used as an image safety classifier nor a text-only safety classifier. Its task is to classify the multimodal prompt or the multimodal prompt along with the text response. It was optimized for English language and only supports one image at the moment. Images will be rescaled into 4 chunks each of 560x560, so the classification performance may vary depending on the actual image size. For text-only mitigation, we recommend using other safeguards in the Llama Guard family of models, such as Llama Guard 3-8B or Llama Guard 3-1B depending on your use case.\nSome hazard categories may require factual, up-to-date knowledge to be evaluated (for example, S5: Defamation, S8: Intellectual Property, and S13: Elections) . We believe more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but Llama Guard 3 Vision provides a good baseline for generic use cases.\nLastly, as an LLM, Llama Guard 3 Vision may be susceptible to adversarial attacks [4, 5] that could bypass or alter its intended use. Please report vulnerabilities and we will look to incorporate improvements in future versions of Llama Guard.\nReferences\n[1] Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations\n[2] Llama Guard 2 Model Card\n[3] Llama Guard 3-8B Model Card\n[4] Universal and Transferable Adversarial Attacks on Aligned Language Models\n[5] Are aligned neural networks adversarially aligned?\nCitation\n@misc{chi2024llamaguard3vision,\ntitle={Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations},\nauthor={Jianfeng Chi and Ujjwal Karn and Hongyuan Zhan and Eric Smith and Javier Rando and Yiming Zhang and Kate Plawiak and Zacharie Delpierre Coudert and Kartikeya Upasani and Mahesh Pasupuleti},\nyear={2024},\neprint={2411.10414},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2411.10414},\n}",
    "AiCloser/Qwen2.5-32B-AGI": "AGI means Aspirational Grand Illusion\nFirst Qwen2.5 32B Finetune, to fix its Hypercensuritis\nHyper means high, and censura means censor, the suffix \"-itis\" is used to denote inflammation of a particular part or organ of the body.",
    "StableDiffusionVN/SDVN11-Ghibli-Flux": "Colab:\nColab Training Lora:\nComfy Workflow:\n=> Text-img-upscale-hyper",
    "Karsh-CAI/Qwen2.5-32B-AGI-Q4_K_M-GGUF": "Kas1o/Qwen2.5-32B-AGI-Q4_K_M-GGUF\nUse with llama.cpp\nCLI:\nServer:\nKas1o/Qwen2.5-32B-AGI-Q4_K_M-GGUF\nThis model was converted to GGUF format from Kas1o/Qwen2.5-32B-AGI using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo Kas1o/Qwen2.5-32B-AGI-Q4_K_M-GGUF --hf-file qwen2.5-32b-agi-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo Kas1o/Qwen2.5-32B-AGI-Q4_K_M-GGUF --hf-file qwen2.5-32b-agi-q4_k_m.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo Kas1o/Qwen2.5-32B-AGI-Q4_K_M-GGUF --hf-file qwen2.5-32b-agi-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo Kas1o/Qwen2.5-32B-AGI-Q4_K_M-GGUF --hf-file qwen2.5-32b-agi-q4_k_m.gguf -c 2048",
    "Karsh-CAI/Qwen2.5-32B-AGI-Q6_K-GGUF": "Kas1o/Qwen2.5-32B-AGI-Q6_K-GGUF\nUse with llama.cpp\nCLI:\nServer:\nKas1o/Qwen2.5-32B-AGI-Q6_K-GGUF\nThis model was converted to GGUF format from Kas1o/Qwen2.5-32B-AGI using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo Kas1o/Qwen2.5-32B-AGI-Q6_K-GGUF --hf-file qwen2.5-32b-agi-q6_k.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo Kas1o/Qwen2.5-32B-AGI-Q6_K-GGUF --hf-file qwen2.5-32b-agi-q6_k.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo Kas1o/Qwen2.5-32B-AGI-Q6_K-GGUF --hf-file qwen2.5-32b-agi-q6_k.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo Kas1o/Qwen2.5-32B-AGI-Q6_K-GGUF --hf-file qwen2.5-32b-agi-q6_k.gguf -c 2048",
    "firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3": "üéß Speech Emotion Recognition with Whisper\nüóÇ Dataset\nüé§ Preprocessing\nüîß Model\n‚öôÔ∏è Training\nüìä Metrics\nüß™ Results\nüöÄ How to Use\nüéØ Framework versions\nüéß Speech Emotion Recognition with Whisper\nThis project leverages the Whisper model to recognize emotions in speech. The goal is to classify audio recordings into different emotional categories, such as Happy, Sad, Surprised, and etc.\nüóÇ Dataset\nThe dataset used for training and evaluation is sourced from multiple datasets, including:\nRAVDESS\nSAVEE\nTESS\nURDU\nThe dataset contains recordings labeled with various emotions. Below is the distribution of the emotions in the dataset:\nEmotion\nCount\nsad\n752\nhappy\n752\nangry\n752\nneutral\n716\ndisgust\n652\nfearful\n652\nsurprised\n652\ncalm\n192\nThis distribution reflects the balance of emotions in the dataset, with some emotions having more samples than others. Excluded the \"calm\" emotion during training due to its underrepresentation.\nüé§ Preprocessing\nAudio Loading: Using Librosa to load the audio files and convert them to numpy arrays.\nFeature Extraction: The audio data is processed using the Whisper Feature Extractor, which standardizes and normalizes the audio features for input to the model.\nüîß Model\nThe model used is the Whisper Large V3 model, fine-tuned for audio classification tasks:\nModel: openai/whisper-large-v3\nOutput: Emotion labels (Angry', 'Disgust', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised')\nI map the emotion labels to numeric IDs and use them for model training and evaluation.\n‚öôÔ∏è Training\nThe model is trained with the following parameters:\nLearning Rate: 5e-05\nTrain Batch Size: 2\nEval Batch Size: 2\nRandom Seed: 42\nGradient Accumulation Steps: 5\nTotal Train Batch Size: 10 (effective batch size after gradient accumulation)\nOptimizer: Adam with parameters: betas=(0.9, 0.999) and epsilon=1e-08\nLearning Rate Scheduler: linear\nWarmup Ratio for LR Scheduler: 0.1\nNumber of Epochs: 25\nMixed Precision Training: Native AMP (Automatic Mixed Precision)\nThese parameters ensure efficient model training and stability, especially when dealing with large datasets and deep models like Whisper.\nThe training utilizes Wandb for experiment tracking and monitoring.\nüìä Metrics\nThe following evaluation metrics were obtained after training the model:\nLoss: 0.5008\nAccuracy: 0.9199\nPrecision: 0.9230\nRecall: 0.9199\nF1 Score: 0.9198\nThese metrics demonstrate the model's performance on the speech emotion recognition task. The high values for accuracy, precision, recall, and F1 score indicate that the model is effectively identifying emotional states from speech data.\nüß™ Results\nAfter training, the model is evaluated on the test dataset, and the results are monitored using Wandb in this Link.\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\nPrecision\nRecall\nF1\n0.4948\n0.9995\n394\n0.4911\n0.8286\n0.8449\n0.8286\n0.8302\n0.6271\n1.9990\n788\n0.5307\n0.8225\n0.8559\n0.8225\n0.8277\n0.2364\n2.9985\n1182\n0.5076\n0.8692\n0.8727\n0.8692\n0.8684\n0.0156\n3.9980\n1576\n0.5669\n0.8732\n0.8868\n0.8732\n0.8745\n0.2305\n5.0\n1971\n0.4578\n0.9108\n0.9142\n0.9108\n0.9114\n0.0112\n5.9995\n2365\n0.4701\n0.9108\n0.9159\n0.9108\n0.9114\n0.0013\n6.9990\n2759\n0.5232\n0.9138\n0.9204\n0.9138\n0.9137\n0.1894\n7.9985\n3153\n0.5008\n0.9199\n0.9230\n0.9199\n0.9198\n0.0877\n8.9980\n3547\n0.5517\n0.9138\n0.9152\n0.9138\n0.9138\n0.1471\n10.0\n3942\n0.5856\n0.8895\n0.9002\n0.8895\n0.8915\n0.0026\n10.9995\n4336\n0.8334\n0.8773\n0.8949\n0.8773\n0.8770\nüöÄ How to Use\n# Requires: librosa\nfrom transformers import AutoModelForAudioClassification, AutoFeatureExtractor\nimport librosa\nimport torch\nimport numpy as np\nmodel_id = \"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\"\nmodel = AutoModelForAudioClassification.from_pretrained(model_id)\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)\nid2label = model.config.id2label\ndef preprocess_audio(audio_path, feature_extractor, max_duration=30.0):\naudio_array, sampling_rate = librosa.load(audio_path, sr=feature_extractor.sampling_rate)\nmax_length = int(feature_extractor.sampling_rate * max_duration)\nif len(audio_array) > max_length:\naudio_array = audio_array[:max_length]\nelse:\naudio_array = np.pad(audio_array, (0, max_length - len(audio_array)))\ninputs = feature_extractor(\naudio_array,\nsampling_rate=feature_extractor.sampling_rate,\nmax_length=max_length,\ntruncation=True,\nreturn_tensors=\"pt\",\n)\nreturn inputs\ndef predict_emotion(audio_path, model, feature_extractor, id2label, max_duration=30.0):\ninputs = preprocess_audio(audio_path, feature_extractor, max_duration)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\ninputs = {key: value.to(device) for key, value in inputs.items()}\nwith torch.no_grad():\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_id = torch.argmax(logits, dim=-1).item()\npredicted_label = id2label[predicted_id]\nreturn predicted_label\naudio_path = \"/content/drive/MyDrive/Audio/Speech_URDU/Happy/SM5_F4_H058.wav\"\npredicted_emotion = predict_emotion(audio_path, model, feature_extractor, id2label)\nprint(f\"Predicted Emotion: {predicted_emotion}\")\nüéØ Framework versions\nTransformers 4.44.2\nPytorch 2.4.1+cu121\nDatasets 3.0.0\nTokenizers 0.19.1",
    "Maple728/TimeMoE-200M": "Model Card for TimeMoE\nModel Card for TimeMoE\nThis repository contains the weights of the TimeMoE-200M model of the paper Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts.\nFor details on how to use this model, please visit our GitHub page.",
    "unsloth/Qwen2.5-Coder-7B-bnb-4bit": "Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nQwen2.5-Coder-7B-bnb-4bit\nIntroduction\nRequirements\nProcessing Long Texts\nEvaluation & Performance\nCitation\nFinetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a Qwen 2.5 (all model sizes) free Google Colab Tesla T4 notebook.\nAlso a Qwen 2.5 conversational style notebook.\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.1 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma-2 9b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nQwen2.5-Coder-7B-bnb-4bit\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). For Qwen2.5-Coder, we release three base language models and instruction-tuned language models, 1.5, 7 and 32 (coming soon) billion parameters. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\nThis repo contains the 7B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 131,072 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., or fill in the middle tasks on this model.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen25_coder,\ntitle={Qwen2.5-Coder Technical Report},\nauthor={Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou and Junyang Lin},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "jasperai/Flux.1-dev-Controlnet-Upscaler": "‚ö° Flux.1-dev: Upscaler ControlNet ‚ö°\nHow to use\nTraining\nLicence\n‚ö° Flux.1-dev: Upscaler ControlNet ‚ö°\nThis is Flux.1-dev ControlNet for low resolution images developed by Jasper research team.\nHow to use\nThis model can be used directly with the diffusers library\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxControlNetModel\nfrom diffusers.pipelines import FluxControlNetPipeline\n# Load pipeline\ncontrolnet = FluxControlNetModel.from_pretrained(\n\"jasperai/Flux.1-dev-Controlnet-Upscaler\",\ntorch_dtype=torch.bfloat16\n)\npipe = FluxControlNetPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-dev\",\ncontrolnet=controlnet,\ntorch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\n# Load a control image\ncontrol_image = load_image(\n\"https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler/resolve/main/examples/input.jpg\"\n)\nw, h = control_image.size\n# Upscale x4\ncontrol_image = control_image.resize((w * 4, h * 4))\nimage = pipe(\nprompt=\"\",\ncontrol_image=control_image,\ncontrolnet_conditioning_scale=0.6,\nnum_inference_steps=28,\nguidance_scale=3.5,\nheight=control_image.size[1],\nwidth=control_image.size[0]\n).images[0]\nimage\nTraining\nThis model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising (Gaussian, Poisson), image blurring and JPEG compression in a similar spirit as [1]\n[1] Wang, Xintao, et al. \"Real-esrgan: Training real-world blind super-resolution with pure synthetic data.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\nLicence\nThis model falls under the Flux.1-dev model licence.",
    "renderartist/simplevectorflux": "Simple Vector Flux LoRA\nModel description\nTrigger words\nDownload model\nSimple Vector Flux LoRA\nPrompt\nv3ct0r style, simple flat vector art, isolated on white bg, cat\nPrompt\nv3ct0r style, simple flat vector art, isolated on white bg, rocket\nPrompt\nv3ct0r style, simple flat vector art, isolated on white bg, clown\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, construction worker wearing a hard hat and holding a small clipboard, character asset, clip art - The text on the clipboard says \"FLUX TEST\"\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, salesman giving a thumbs up in front of a car, character asset, clip art\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, ugly witch standing next to a bubbling cauldron stirring the pot, character asset, clip art\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, xmas tree with beautifully wrapped gifts beneath it, character asset, clip art\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, tall cartoon style box truck side view, clip art\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, a boy holding up a big gold coin with an orange lightning bolt embossed on the coin, character asset, clip art\nPrompt\nv3ct0r style, simple flat vector art, isolated on white bg, a piglet solving a puzzle\nPrompt\nv3ct0r style, simple vector art, isolated on white bg, doctor smiling, character asset, clip art\nModel description\nSimple Vector Flux was trained on a curated dataset of ~50 synthetic images in classic vector style, 17 epochs, 2 repeats, ~1700 steps.\nThis is a work in progress and it can be a little temperamental, the captioning was done using Joy Caption Batch with the trigger \"v3ct0r\" and \"vector\" in the prefix of the captions.\nYou have to work a little bit to get desired results and sometimes there is bleeding/blending of subjects but overall the style is present and the results can be really good. This LoRA takes a couple of tries adjusting your prompt and adding tokens to match the style.\nTrigger words\nYou should use v3ct0r to trigger the image generation.\nYou should use vector to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "nlpai-lab/KoE5": "üîé KoE5\nModel Versions\nModel Description\nExample code\nInstall Dependencies\nPython code\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nMetrics\nBenchmark Datasets\nResults\nTop-k 1\nTop-k 3\nTop-k 5\nTop-k 10\nFAQ\nCitation\nLimitations\nüîé KoE5\nIntroducing KoE5, a model with advanced retrieval abilities.\nIt has shown remarkable performance in Korean text retrieval.\nFor details, visit the KURE repository\nModel Versions\nModel Name\nDimension\nSequence Length\nIntroduction\nKURE-v1\n1024\n8192\nFine-tuned BAAI/bge-m3 with Korean data via CachedGISTEmbedLoss\nKoE5\n1024\n512\nFine-tuned intfloat/multilingual-e5-large with ko-triplet-v1.0 via CachedMultipleNegativesRankingLoss\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub.\nDeveloped by: NLP&AI Lab\nLanguage(s) (NLP): Korean, English\nLicense: MIT\nFinetuned from model: intfloat/multilingual-e5-large\nFinetuned dataset: ko-triplet-v1.0\nExample code\nInstall Dependencies\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nPython code\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"nlpai-lab/KoE5\")\n# Run inference\nsentences = [\n'query: ÌóåÎ≤ïÍ≥º Î≤ïÏõêÏ°∞ÏßÅÎ≤ïÏùÄ Ïñ¥Îñ§ Î∞©ÏãùÏùÑ ÌÜµÌï¥ Í∏∞Î≥∏Í∂å Î≥¥Ïû• Îì±Ïùò Îã§ÏñëÌïú Î≤ïÏ†Å Î™®ÏÉâÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌñàÏñ¥',\n'passage: 4. ÏãúÏÇ¨Ï†êÍ≥º Í∞úÏÑ†Î∞©Ìñ• ÏïûÏÑú ÏÇ¥Ìé¥Î≥∏ Î∞îÏôÄ Í∞ôÏù¥ Ïö∞Î¶¨ ÌóåÎ≤ïÍ≥º ÔΩ¢Î≤ïÏõêÏ°∞ÏßÅ Î≤ïÔΩ£ÏùÄ ÎåÄÎ≤ïÏõê Íµ¨ÏÑ±ÏùÑ Îã§ÏñëÌôîÌïòÏó¨ Í∏∞Î≥∏Í∂å Î≥¥Ïû•Í≥º ÎØºÏ£ºÏ£ºÏùò ÌôïÎ¶ΩÏóê ÏûàÏñ¥ Îã§Í∞ÅÏ†ÅÏù∏ Î≤ïÏ†Å Î™®ÏÉâÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïòÎäî Í≤ÉÏùÑ Í∑ºÎ≥∏ Í∑úÎ≤îÏúºÎ°ú ÌïòÍ≥† ÏûàÎã§. ÎçîÏö±Ïù¥ Ìï©ÏùòÏ≤¥Î°úÏÑúÏùò ÎåÄÎ≤ïÏõê ÏõêÎ¶¨Î•º Ï±ÑÌÉùÌïòÍ≥† ÏûàÎäî Í≤É Ïó≠Ïãú Í∑∏ Íµ¨ÏÑ±Ïùò Îã§ÏñëÏÑ±ÏùÑ ÏöîÏ≤≠ÌïòÎäî Í≤ÉÏúºÎ°ú Ìï¥ÏÑùÎêúÎã§. Ïù¥ÏôÄ Í∞ôÏùÄ Í¥ÄÏ†êÏóêÏÑú Î≥º Îïå ÌòÑÏßÅ Î≤ïÏõêÏû•Í∏â Í≥†ÏúÑÎ≤ïÍ¥ÄÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÎåÄÎ≤ïÏõêÏùÑ Íµ¨ÏÑ±ÌïòÎäî Í¥ÄÌñâÏùÄ Í∞úÏÑ†Ìï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ÉÏúºÎ°ú Î≥¥Ïù∏Îã§.',\n'passage: ‚ñ° Ïó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎäî 2001ÎÖÑ 1Ïõî 24Ïùº 5:3Ïùò Îã§ÏàòÍ≤¨Ìï¥Î°ú „ÄåÎ≤ïÏõêÏ°∞ÏßÅÎ≤ï„Äç Ï†ú169Ï°∞ Ï†ú2Î¨∏Ïù¥ ÌóåÎ≤ïÏóê Ìï©ÏπòÎêúÎã§Îäî ÌåêÍ≤∞ÏùÑ ÎÇ¥Î†∏Ïùå ‚óã 5Ïù∏Ïùò Îã§Ïàò Ïû¨ÌåêÍ¥ÄÏùÄ ÏÜåÏÜ°Í¥ÄÍ≥ÑÏù∏Ïùò Ïù∏Í≤©Í∂å Î≥¥Ìò∏, Í≥µÏ†ïÌïú Ï†àÏ∞®Ïùò Î≥¥Ïû•Í≥º Î∞©Ìï¥Î∞õÏßÄ ÏïäÎäî Î≤ïÍ≥º ÏßÑÏã§ Î∞úÍ≤¨ Îì±ÏùÑ Í∑ºÍ±∞Î°ú ÌïòÏó¨ ÌÖîÎ†àÎπÑÏ†Ñ Ï¥¨ÏòÅÏóê ÎåÄÌïú Ï†àÎåÄÏ†ÅÏù∏ Í∏àÏßÄÎ•º ÌóåÎ≤ïÏóê Ìï©ÏπòÌïòÎäî Í≤ÉÏúºÎ°ú Î≥¥ÏïòÏùå ‚óã Í∑∏Îü¨ÎÇò ÎÇòÎ®∏ÏßÄ 3Ïù∏Ïùò Ïû¨ÌåêÍ¥ÄÏùÄ ÌñâÏ†ïÎ≤ïÏõêÏùò ÏÜåÏÜ°Ï†àÏ∞®Îäî ÌäπÎ≥ÑÌïú Ïù∏Í≤©Í∂å Î≥¥Ìò∏Ïùò Ïù¥ÏùµÎèÑ ÏóÜÏúºÎ©∞, ÌÖîÎ†àÎπÑÏ†Ñ Í≥µÍ∞úÏ£ºÏùòÎ°ú Ïù∏Ìï¥ Î≤ïÍ≥º ÏßÑÏã§ Î∞úÍ≤¨Ïùò Í≥ºÏ†ïÏù¥ Ïñ∏Ï†úÎÇò ÏúÑÌÉúÎ°≠Í≤å ÎêòÎäî Í≤ÉÏùÄ ÏïÑÎãàÎùºÎ©¥ÏÑú Î∞òÎåÄÏùòÍ≤¨ÏùÑ Ï†úÏãúÌï® ‚óã ÏôúÎÉêÌïòÎ©¥ ÌñâÏ†ïÎ≤ïÏõêÏùò ÏÜåÏÜ°Ï†àÏ∞®ÏóêÏÑúÎäî ÏÜåÏÜ°ÎãπÏÇ¨ÏûêÍ∞Ä Í∞úÏù∏Ï†ÅÏúºÎ°ú ÏßÅÏ†ë Ïã¨Î¶¨Ïóê Ï∞∏ÏÑùÌïòÍ∏∞Î≥¥Îã§Îäî Î≥ÄÌò∏ÏÇ¨Í∞Ä Ï∞∏ÏÑùÌïòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏúºÎ©∞, Ïã¨Î¶¨ÎåÄÏÉÅÎèÑ ÏÇ¨Ïã§Î¨∏Ï†úÍ∞Ä ÏïÑÎãå Î≤ïÎ•†Î¨∏Ï†úÍ∞Ä ÎåÄÎ∂ÄÎ∂ÑÏù¥Í∏∞ ÎïåÎ¨∏Ïù¥ÎùºÎäî Í≤ÉÏûÑ ‚ñ° ÌïúÌé∏, Ïó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎäî „ÄåÏó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎ≤ï„Äç(Bundesverfassungsgerichtsgesetz: BVerfGG) Ï†ú17aÏ°∞Ïóê Îî∞Îùº Ï†úÌïúÏ†ÅÏù¥ÎÇòÎßà Ïû¨ÌåêÏóê ÎåÄÌïú Î∞©ÏÜ°ÏùÑ ÌóàÏö©ÌïòÍ≥† ÏûàÏùå ‚óã „ÄåÏó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎ≤ï„Äç Ï†ú17Ï°∞ÏóêÏÑú „ÄåÎ≤ïÏõêÏ°∞ÏßÅÎ≤ï„Äç Ï†ú14Ï†à ÎÇ¥ÏßÄ Ï†ú16Ï†àÏùò Í∑úÏ†ïÏùÑ Ï§ÄÏö©ÌïòÎèÑÎ°ù ÌïòÍ≥† ÏûàÏßÄÎßå, ÎÖπÏùåÏù¥ÎÇò Ï¥¨ÏòÅÏùÑ ÌÜµÌïú Ïû¨ÌåêÍ≥µÍ∞úÏôÄ Í¥ÄÎ†®ÌïòÏó¨ÏÑúÎäî „ÄåÎ≤ïÏõêÏ°∞ÏßÅÎ≤ï„ÄçÍ≥º Îã§Î•∏ ÎÇ¥Ïö©ÏùÑ Í∑úÏ†ïÌïòÍ≥† ÏûàÏùå',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 1024]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6721, 0.3897],\n#        [0.6721, 1.0000, 0.3740],\n#        [0.3897, 0.3740, 1.0000]])\nTraining Details\nTraining Data\nko-triplet-v1.0\nKorean query-document-hard_negative data pair (open data)\nAbout 700000+ examples used totally\nTraining Procedure\nloss: Used CachedMultipleNegativesRankingLoss by sentence-transformers\nbatch size: 512\nlearning rate: 1e-05\nepochs: 1\nEvaluation\nMetrics\nRecall, Precision, NDCG, F1\nBenchmark Datasets\nKo-StrategyQA: ÌïúÍµ≠Ïñ¥ ODQA multi-hop Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã (StrategyQA Î≤àÏó≠)\nAutoRAGRetrieval: Í∏àÏúµ, Í≥µÍ≥µ, ÏùòÎ£å, Î≤ïÎ•†, Ïª§Î®∏Ïä§ 5Í∞ú Î∂ÑÏïºÏóê ÎåÄÌï¥, pdfÎ•º ÌååÏã±ÌïòÏó¨ Íµ¨ÏÑ±Ìïú ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nMIRACLRetrieval: Wikipedia Í∏∞Î∞òÏùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nPublicHealthQA: ÏùòÎ£å Î∞è Í≥µÏ§ëÎ≥¥Í±¥ ÎèÑÎ©îÏù∏Ïóê ÎåÄÌïú ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nBelebeleRetrieval: FLORES-200 Í∏∞Î∞òÏùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nMrTidyRetrieval: Wikipedia Í∏∞Î∞òÏùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nMultiLongDocRetrieval: Îã§ÏñëÌïú ÎèÑÎ©îÏù∏Ïùò ÌïúÍµ≠Ïñ¥ Ïû•Î¨∏ Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nXPQARetrieval: Îã§ÏñëÌïú ÎèÑÎ©îÏù∏Ïùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nResults\nÏïÑÎûòÎäî Î™®Îì† Î™®Îç∏Ïùò, Î™®Îì† Î≤§ÏπòÎßàÌÅ¨ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌïú ÌèâÍ∑† Í≤∞Í≥ºÏûÖÎãàÎã§.\nÏûêÏÑ∏Ìïú Í≤∞Í≥ºÎäî KURE GithubÏóêÏÑú ÌôïÏù∏ÌïòÏã§ Ïàò ÏûàÏäµÎãàÎã§.\nTop-k 1\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.52640\n0.60551\n0.60551\n0.55784\ndragonkue/BGE-m3-ko\n0.52361\n0.60394\n0.60394\n0.55535\nBAAI/bge-m3\n0.51778\n0.59846\n0.59846\n0.54998\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.51246\n0.59384\n0.59384\n0.54489\nnlpai-lab/KoE5\n0.50157\n0.57790\n0.57790\n0.53178\nintfloat/multilingual-e5-large\n0.50052\n0.57727\n0.57727\n0.53122\njinaai/jina-embeddings-v3\n0.48287\n0.56068\n0.56068\n0.51361\nBAAI/bge-multilingual-gemma2\n0.47904\n0.55472\n0.55472\n0.50916\nintfloat/multilingual-e5-large-instruct\n0.47842\n0.55435\n0.55435\n0.50826\nintfloat/multilingual-e5-base\n0.46950\n0.54490\n0.54490\n0.49947\nintfloat/e5-mistral-7b-instruct\n0.46772\n0.54394\n0.54394\n0.49781\nAlibaba-NLP/gte-multilingual-base\n0.46469\n0.53744\n0.53744\n0.49353\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.46633\n0.53625\n0.53625\n0.49429\nopenai/text-embedding-3-large\n0.44884\n0.51688\n0.51688\n0.47572\nSalesforce/SFR-Embedding-2_R\n0.43748\n0.50815\n0.50815\n0.46504\nupskyy/bge-m3-korean\n0.43125\n0.50245\n0.50245\n0.45945\njhgan/ko-sroberta-multitask\n0.33788\n0.38497\n0.38497\n0.35678\nTop-k 3\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.68678\n0.28711\n0.65538\n0.39835\ndragonkue/BGE-m3-ko\n0.67834\n0.28385\n0.64950\n0.39378\nBAAI/bge-m3\n0.67526\n0.28374\n0.64556\n0.39291\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.67128\n0.28193\n0.64042\n0.39072\nintfloat/multilingual-e5-large\n0.65807\n0.27777\n0.62822\n0.38423\nnlpai-lab/KoE5\n0.65174\n0.27329\n0.62369\n0.37882\nBAAI/bge-multilingual-gemma2\n0.64415\n0.27416\n0.61105\n0.37782\njinaai/jina-embeddings-v3\n0.64116\n0.27165\n0.60954\n0.37511\nintfloat/multilingual-e5-large-instruct\n0.64353\n0.27040\n0.60790\n0.37453\nAlibaba-NLP/gte-multilingual-base\n0.63744\n0.26404\n0.59695\n0.36764\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.63163\n0.25937\n0.59237\n0.36263\nintfloat/multilingual-e5-base\n0.62099\n0.26144\n0.59179\n0.36203\nintfloat/e5-mistral-7b-instruct\n0.62087\n0.26144\n0.58917\n0.36188\nopenai/text-embedding-3-large\n0.61035\n0.25356\n0.57329\n0.35270\nSalesforce/SFR-Embedding-2_R\n0.60001\n0.25253\n0.56346\n0.34952\nupskyy/bge-m3-korean\n0.59215\n0.25076\n0.55722\n0.34623\njhgan/ko-sroberta-multitask\n0.46930\n0.18994\n0.43293\n0.26696\nTop-k 5\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.73851\n0.19130\n0.67479\n0.29903\ndragonkue/BGE-m3-ko\n0.72517\n0.18799\n0.66692\n0.29401\nBAAI/bge-m3\n0.72954\n0.18975\n0.66615\n0.29632\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.72962\n0.18875\n0.66236\n0.29542\nnlpai-lab/KoE5\n0.70820\n0.18287\n0.64499\n0.28628\nintfloat/multilingual-e5-large\n0.70124\n0.18316\n0.64402\n0.28588\nBAAI/bge-multilingual-gemma2\n0.70258\n0.18556\n0.63338\n0.28851\njinaai/jina-embeddings-v3\n0.69933\n0.18256\n0.63133\n0.28505\nintfloat/multilingual-e5-large-instruct\n0.69018\n0.17838\n0.62486\n0.27933\nAlibaba-NLP/gte-multilingual-base\n0.69365\n0.17789\n0.61896\n0.27879\nintfloat/multilingual-e5-base\n0.67250\n0.17406\n0.61119\n0.27247\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.67447\n0.17114\n0.60952\n0.26943\nintfloat/e5-mistral-7b-instruct\n0.67449\n0.17484\n0.60935\n0.27349\nopenai/text-embedding-3-large\n0.66365\n0.17004\n0.59389\n0.26677\nSalesforce/SFR-Embedding-2_R\n0.65622\n0.17018\n0.58494\n0.26612\nupskyy/bge-m3-korean\n0.65477\n0.17015\n0.58073\n0.26589\njhgan/ko-sroberta-multitask\n0.53136\n0.13264\n0.45879\n0.20976\nTop-k 10\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.79682\n0.10624\n0.69473\n0.18524\ndragonkue/BGE-m3-ko\n0.78450\n0.10492\n0.68748\n0.18288\nBAAI/bge-m3\n0.79195\n0.10592\n0.68723\n0.18456\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.78669\n0.10462\n0.68189\n0.18260\nintfloat/multilingual-e5-large\n0.75902\n0.10147\n0.66370\n0.17693\nnlpai-lab/KoE5\n0.75296\n0.09937\n0.66012\n0.17369\nBAAI/bge-multilingual-gemma2\n0.76153\n0.10364\n0.65330\n0.18003\njinaai/jina-embeddings-v3\n0.76277\n0.10240\n0.65290\n0.17843\nintfloat/multilingual-e5-large-instruct\n0.74851\n0.09888\n0.64451\n0.17283\nAlibaba-NLP/gte-multilingual-base\n0.75631\n0.09938\n0.64025\n0.17363\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.74092\n0.09607\n0.63258\n0.16847\nintfloat/multilingual-e5-base\n0.73512\n0.09717\n0.63216\n0.16977\nintfloat/e5-mistral-7b-instruct\n0.73795\n0.09777\n0.63076\n0.17078\nopenai/text-embedding-3-large\n0.72946\n0.09571\n0.61670\n0.16739\nSalesforce/SFR-Embedding-2_R\n0.71662\n0.09546\n0.60589\n0.16651\nupskyy/bge-m3-korean\n0.71895\n0.09583\n0.60258\n0.16712\njhgan/ko-sroberta-multitask\n0.61225\n0.07826\n0.48687\n0.13757\nFAQ\n- Do I need to add the prefix \"query: \" and \"passage: \" to input texts?\nYes, this is how the model is trained, otherwise you will see a performance degradation.\nHere are some rules of thumb:\nUse \"query: \" and \"passage: \" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\nUse \"query: \" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval.\nUse \"query: \" prefix if you want to use embeddings as features, such as linear probing classification, clustering.\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@misc{KURE,\npublisher = {Youngjoon Jang, Junyoung Son, Taemin Lee},\nyear = {2024},\nurl = {https://github.com/nlpai-lab/KURE}\n},\n@misc{KoE5,\nauthor = {NLP & AI Lab and Human-Inspired AI research},\ntitle = {KoE5: A New Dataset and Model for Improving Korean Embedding Performance},\nyear = {2024},\npublisher = {Youngjoon Jang, Junyoung Son, Taemin Lee},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/nlpai-lab/KoE5}},\n}\nLimitations\nLong texts will be truncated to at most 512 tokens.",
    "mradermacher/EuroLLM-1.7B-Instruct-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/utter-project/EuroLLM-1.7B-Instruct\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n0.8\nGGUF\nIQ3_XS\n0.9\nGGUF\nIQ3_S\n0.9\nbeats Q3_K*\nGGUF\nQ3_K_S\n0.9\nGGUF\nIQ3_M\n0.9\nGGUF\nQ3_K_M\n1.0\nlower quality\nGGUF\nQ3_K_L\n1.0\nGGUF\nIQ4_XS\n1.1\nGGUF\nQ4_K_S\n1.1\nfast, recommended\nGGUF\nQ4_K_M\n1.1\nfast, recommended\nGGUF\nQ5_K_S\n1.3\nGGUF\nQ5_K_M\n1.3\nGGUF\nQ6_K\n1.5\nvery good quality\nGGUF\nQ8_0\n1.9\nfast, best quality\nGGUF\nf16\n3.4\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "BAAI/Emu3-Gen": "Emu3 excels in both generation and perception\nHighlights\nQuickstart\nEmu3: Next-Token Prediction is All You Need\nEmu3 Team, BAAI\n| Project Page | Paper | ü§óHF Models |  github\n| Demo |\nWe introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction! By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences.\nEmu3 excels in both generation and perception\nEmu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship open models such as SDXL, LLaVA-1.6 and OpenSora-1.2, while eliminating the need for diffusion or compositional architectures.\nHighlights\nEmu3 is capable of generating high-quality images following the text input, by simply predicting the next vision token. The model naturally supports flexible resolutions and styles.\nEmu3 shows strong vision-language understanding capabilities to see the physical world and provides coherent text responses. Notably, this capability is achieved without depending on a CLIP and a pretrained LLM.\nEmu3 simply generates a video causally by predicting the next token in a video sequence, unlike the video diffusion model as in Sora. With a video in context, Emu3 can also naturally extend the video and predict what will happen next.\nQuickstart\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForCausalLM\nfrom transformers.generation.configuration_utils import GenerationConfig\nfrom transformers.generation import LogitsProcessorList, PrefixConstrainedLogitsProcessor, UnbatchedClassifierFreeGuidanceLogitsProcessor\nimport torch\nimport sys\nsys.path.append(PATH_TO_BAAI_Emu3-Gen_MODEL)\nfrom processing_emu3 import Emu3Processor\n# model path\nEMU_HUB = \"BAAI/Emu3-Gen\"\nVQ_HUB = \"BAAI/Emu3-VisionTokenizer\"\n# prepare model and processor\nmodel = AutoModelForCausalLM.from_pretrained(\nEMU_HUB,\ndevice_map=\"cuda:0\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(EMU_HUB, trust_remote_code=True, padding_side=\"left\")\nimage_processor = AutoImageProcessor.from_pretrained(VQ_HUB, trust_remote_code=True)\nimage_tokenizer = AutoModel.from_pretrained(VQ_HUB, device_map=\"cuda:0\", trust_remote_code=True).eval()\nprocessor = Emu3Processor(image_processor, image_tokenizer, tokenizer)\n# prepare input\nPOSITIVE_PROMPT = \" masterpiece, film grained, best quality.\"\nNEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry.\"\nclassifier_free_guidance = 3.0\nprompt = \"a portrait of young girl.\"\nprompt += POSITIVE_PROMPT\nkwargs = dict(\nmode='G',\nratio=\"1:1\",\nimage_area=model.config.image_area,\nreturn_tensors=\"pt\",\npadding=\"longest\",\n)\npos_inputs = processor(text=prompt, **kwargs)\nneg_inputs = processor(text=NEGATIVE_PROMPT, **kwargs)\n# prepare hyper parameters\nGENERATION_CONFIG = GenerationConfig(\nuse_cache=True,\neos_token_id=model.config.eos_token_id,\npad_token_id=model.config.pad_token_id,\nmax_new_tokens=40960,\ndo_sample=True,\ntop_k=2048,\n)\nh = pos_inputs.image_size[:, 0]\nw = pos_inputs.image_size[:, 1]\nconstrained_fn = processor.build_prefix_constrained_fn(h, w)\nlogits_processor = LogitsProcessorList([\nUnbatchedClassifierFreeGuidanceLogitsProcessor(\nclassifier_free_guidance,\nmodel,\nunconditional_ids=neg_inputs.input_ids.to(\"cuda:0\"),\n),\nPrefixConstrainedLogitsProcessor(\nconstrained_fn ,\nnum_beams=1,\n),\n])\n# generate\noutputs = model.generate(\npos_inputs.input_ids.to(\"cuda:0\"),\nGENERATION_CONFIG,\nlogits_processor=logits_processor,\nattention_mask=pos_inputs.attention_mask.to(\"cuda:0\"),\n)\nmm_list = processor.decode(outputs[0])\nfor idx, im in enumerate(mm_list):\nif not isinstance(im, Image.Image):\ncontinue\nim.save(f\"result_{idx}.png\")",
    "onnx-community/Llama-3.2-1B": "https://huggingface.co/meta-llama/Llama-3.2-1B with ONNX weights to be compatible with Transformers.js.\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ü§ó Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "bartowski/Llama-3.2-3B-Instruct-GGUF": "Llamacpp imatrix Quantizations of Llama-3.2-3B-Instruct\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Llama-3.2-3B-Instruct\nUsing llama.cpp release b3821 for quantization.\nOriginal model: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nLlama-3.2-3B-Instruct-f16.gguf\nf16\n6.43GB\nfalse\nFull F16 weights.\nLlama-3.2-3B-Instruct-Q8_0.gguf\nQ8_0\n3.42GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nLlama-3.2-3B-Instruct-Q6_K_L.gguf\nQ6_K_L\n2.74GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nLlama-3.2-3B-Instruct-Q6_K.gguf\nQ6_K\n2.64GB\nfalse\nVery high quality, near perfect, recommended.\nLlama-3.2-3B-Instruct-Q5_K_L.gguf\nQ5_K_L\n2.42GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nLlama-3.2-3B-Instruct-Q5_K_M.gguf\nQ5_K_M\n2.32GB\nfalse\nHigh quality, recommended.\nLlama-3.2-3B-Instruct-Q5_K_S.gguf\nQ5_K_S\n2.27GB\nfalse\nHigh quality, recommended.\nLlama-3.2-3B-Instruct-Q4_K_L.gguf\nQ4_K_L\n2.11GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nLlama-3.2-3B-Instruct-Q4_K_M.gguf\nQ4_K_M\n2.02GB\nfalse\nGood quality, default size for must use cases, recommended.\nLlama-3.2-3B-Instruct-Q4_K_S.gguf\nQ4_K_S\n1.93GB\nfalse\nSlightly lower quality with more space savings, recommended.\nLlama-3.2-3B-Instruct-Q4_0_8_8.gguf\nQ4_0_8_8\n1.92GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below).\nLlama-3.2-3B-Instruct-Q4_0_4_8.gguf\nQ4_0_4_8\n1.92GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below).\nLlama-3.2-3B-Instruct-Q4_0_4_4.gguf\nQ4_0_4_4\n1.92GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure.\nLlama-3.2-3B-Instruct-Q4_0.gguf\nQ4_0\n1.92GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nLlama-3.2-3B-Instruct-Q3_K_XL.gguf\nQ3_K_XL\n1.91GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nLlama-3.2-3B-Instruct-IQ4_XS.gguf\nIQ4_XS\n1.83GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nLlama-3.2-3B-Instruct-Q3_K_L.gguf\nQ3_K_L\n1.82GB\nfalse\nLower quality but usable, good for low RAM availability.\nLlama-3.2-3B-Instruct-IQ3_M.gguf\nIQ3_M\n1.60GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Llama-3.2-3B-Instruct-GGUF --include \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Llama-3.2-3B-Instruct-GGUF --include \"Llama-3.2-3B-Instruct-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Llama-3.2-3B-Instruct-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "unsloth/Llama-3.2-1B": "Finetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nLlama-3.2-1B\n‚ú® Finetune for Free\nSpecial Thanks\nModel Information\nSee our collection for all versions of Llama 3.2 including GGUF, 4-bit and original 16-bit formats.\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (1B) here: https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing\nLlama-3.2-1B\nFor more details on the model, please go to Meta's original model card\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\nModel Information\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nSupported languages:  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 family of models Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.",
    "unsloth/Llama-3.2-3B-bnb-4bit": "Finetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nunsloth/Llama-3.2-3B-bnb-4bit\n‚ú® Finetune for Free\nSpecial Thanks\nModel Information\nSee our collection for all versions of Llama 3.2 including GGUF, 4-bit and original 16-bit formats.\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (3B) here: https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing\nunsloth/Llama-3.2-3B-bnb-4bit\nFor more details on the model, please go to Meta's original model card\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\nModel Information\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nSupported languages:  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 family of models Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit": "Finetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nunsloth/Llama-3.2-3B-Instruct-bnb-4bit\n‚ú® Finetune for Free\nSpecial Thanks\nModel Information\nSee our collection for all versions of Llama 3.2 including GGUF, 4-bit and original 16-bit formats.\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (3B) here: https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing\nunsloth/Llama-3.2-3B-Instruct-bnb-4bit\nFor more details on the model, please go to Meta's original model card\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\nModel Information\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nSupported languages:  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 family of models Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.",
    "unsloth/Llama-3.2-1B-Instruct-GGUF": "GGUF uploads\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nunsloth/Llama-3.2-1B-Instruct\n‚ú® Finetune for Free\nSpecial Thanks\nModel Information\nSee our collection for all versions of Llama 3.2 including GGUF, 4-bit and original 16-bit formats.\nGGUF uploads\n16bit, 8bit, 6bit, 5bit, 4bit, 3bit and 2bit uploads avaliable.\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (3B) here: https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing\nunsloth/Llama-3.2-1B-Instruct\nFor more details on the model, please go to Meta's original model card\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\nModel Information\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nSupported languages:  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 family of models Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.",
    "unsloth/Llama-3.2-3B-Instruct-GGUF": "GGUF uploads\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nLlama-3.2-3B\n‚ú® Finetune for Free\nSpecial Thanks\nModel Information\nSee our collection for all versions of Llama 3.2 including GGUF, 4-bit and original 16-bit formats.\nGGUF uploads\n16bit, 8bit, 6bit, 5bit, 4bit, 3bit and 2bit uploads avaliable.\nFinetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (3B) here: https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing\nLlama-3.2-3B\nFor more details on the model, please go to Meta's original model card\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\nModel Information\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nSupported languages:  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 family of models Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.",
    "shuyuej/Llama-3.2-1B-GPTQ": "The Quantized LLaMA 3.2 1B Model\nQuantization Configurations\nSource Codes\nThe Quantized LLaMA 3.2 1B Model\nOriginal Base Model: meta-llama/Llama-3.2-1B.\nLink: https://huggingface.co/meta-llama/Llama-3.2-1B\nQuantization Configurations\n\"quantization_config\": {\n\"batch_size\": 1,\n\"bits\": 4,\n\"block_name_to_quantize\": null,\n\"cache_block_outputs\": true,\n\"damp_percent\": 0.1,\n\"dataset\": null,\n\"desc_act\": false,\n\"exllama_config\": {\n\"version\": 1\n},\n\"group_size\": 128,\n\"max_input_length\": null,\n\"model_seqlen\": null,\n\"module_name_preceding_first_block\": null,\n\"modules_in_block_to_quantize\": null,\n\"pad_token_id\": null,\n\"quant_method\": \"gptq\",\n\"sym\": true,\n\"tokenizer\": null,\n\"true_sequential\": true,\n\"use_cuda_fp16\": false,\n\"use_exllama\": true\n},\nSource Codes\nSource Codes: https://github.com/vkola-lab/medpodgpt/tree/main/quantization.",
    "Marqo/marqo-ecommerce-embeddings-B": "Marqo Ecommerce Embedding Models\nModels\nLoad from HuggingFace with transformers\nLoad from HuggingFace with OpenCLIP\nEvaluation\nDetailed Performance\nMarqo-Ecommerce-Hard\nMarqo-Ecommerce-Easy\nCitation\nMarqo Ecommerce Embedding Models\nIn this work, we introduce two state-of-the-art embedding models for ecommerce products: Marqo-Ecommerce-B and Marqo-Ecommerce-L.\nThe benchmarking results show that the Marqo-Ecommerce models consistently outperformed all other models across various metrics. Specifically, marqo-ecommerce-L achieved an average improvement of 17.6% in MRR and 20.5% in nDCG@10 when compared with the current best open source model, ViT-SO400M-14-SigLIP across all three tasks in the marqo-ecommerce-hard dataset. When compared with the best private model, Amazon-Titan-Multimodal, we saw an average improvement of 38.9% in MRR and 45.1% in nDCG@10 across all three tasks, and 35.9% in Recall across the Text-to-Image tasks in the marqo-ecommerce-hard dataset.\nMore benchmarking results can be found below.\nReleased Content:\nMarqo-Ecommerce-B and Marqo-Ecommerce-L embedding models\nGoogleShopping-1m and AmazonProducts-3m for evaluation\nEvaluation Code\nModels\nEmbedding Model\n#Params (m)\nDimension\nHuggingFace\nDownload .pt\nMarqo-Ecommerce-B\n203\n768\nMarqo/marqo-ecommerce-embeddings-B\nlink\nMarqo-Ecommerce-L\n652\n1024\nMarqo/marqo-ecommerce-embeddings-L\nlink\nLoad from HuggingFace with transformers\nTo load the models in Transformers, see below. The models are hosted on Hugging Face and loaded using Transformers.\nfrom transformers import AutoModel, AutoProcessor\nimport torch\nfrom PIL import Image\nimport requests\nmodel_name= 'Marqo/marqo-ecommerce-embeddings-B'\n# model_name = 'Marqo/marqo-ecommerce-embeddings-L'\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\nimg = Image.open(requests.get('https://raw.githubusercontent.com/marqo-ai/marqo-ecommerce-embeddings/refs/heads/main/images/dining-chairs.png', stream=True).raw).convert(\"RGB\")\nimage = [img]\ntext = [\"dining chairs\", \"a laptop\", \"toothbrushes\"]\nprocessed = processor(text=text, images=image, padding='max_length', return_tensors=\"pt\")\nprocessor.image_processor.do_rescale = False\nwith torch.no_grad():\nimage_features = model.get_image_features(processed['pixel_values'], normalize=True)\ntext_features = model.get_text_features(processed['input_ids'], normalize=True)\ntext_probs = (100 * image_features @ text_features.T).softmax(dim=-1)\nprint(text_probs)\n# [1.0000e+00, 8.3131e-12, 5.2173e-12]\nLoad from HuggingFace with OpenCLIP\nTo load the models in OpenCLIP, see below. The models are hosted on Hugging Face and loaded using OpenCLIP. You can also find this code inside run_models.py.\npip install open_clip_torch\nfrom PIL import Image\nimport open_clip\nimport requests\nimport torch\n# Specify model from Hugging Face Hub\nmodel_name = 'hf-hub:Marqo/marqo-ecommerce-embeddings-B'\n# model_name = 'hf-hub:Marqo/marqo-ecommerce-embeddings-L'\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(model_name)\ntokenizer = open_clip.get_tokenizer(model_name)\n# Preprocess the image and tokenize text inputs\n# Load an example image from a URL\nimg = Image.open(requests.get('https://raw.githubusercontent.com/marqo-ai/marqo-ecommerce-embeddings/refs/heads/main/images/dining-chairs.png', stream=True).raw)\nimage = preprocess_val(img).unsqueeze(0)\ntext = tokenizer([\"dining chairs\", \"a laptop\", \"toothbrushes\"])\n# Perform inference\nwith torch.no_grad(), torch.cuda.amp.autocast():\nimage_features = model.encode_image(image, normalize=True)\ntext_features = model.encode_text(text, normalize=True)\n# Calculate similarity probabilities\ntext_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n# Display the label probabilities\nprint(\"Label probs:\", text_probs)\n# [1.0000e+00, 8.3131e-12, 5.2173e-12]\nEvaluation\nGeneralised Contrastiove Learning (GCL) is used for the evaluation. The following code can also be found in scripts.\ngit clone https://github.com/marqo-ai/GCL\nInstall the packages required by GCL.\n1. GoogleShopping-Text2Image Retrieval.\ncd ./GCL\nMODEL=hf-hub:Marqo/marqo-ecommerce-B\noutdir=MarqoModels/GE/marqo-ecommerce-B/gs-title2image\nmkdir -p $outdir\nhfdataset=Marqo/google-shopping-general-eval\npython  evals/eval_hf_datasets_v1.py \\\n--model_name $MODEL \\\n--hf-dataset $hfdataset \\\n--output-dir $outdir \\\n--batch-size 1024 \\\n--num_workers 8 \\\n--left-key \"['title']\" \\\n--right-key \"['image']\" \\\n--img-or-txt \"[['txt'], ['img']]\" \\\n--left-weight \"[1]\" \\\n--right-weight \"[1]\" \\\n--run-queries-cpu \\\n--top-q 4000 \\\n--doc-id-key item_ID \\\n--context-length \"[[64], [0]]\"\n2. GoogleShopping-Category2Image Retrieval.\ncd ./GCL\nMODEL=hf-hub:Marqo/marqo-ecommerce-B\noutdir=MarqoModels/GE/marqo-ecommerce-B/gs-cat2image\nmkdir -p $outdir\nhfdataset=Marqo/google-shopping-general-eval\npython  evals/eval_hf_datasets_v1.py \\\n--model_name $MODEL \\\n--hf-dataset $hfdataset \\\n--output-dir $outdir \\\n--batch-size 1024 \\\n--num_workers 8 \\\n--left-key \"['query']\" \\\n--right-key \"['image']\" \\\n--img-or-txt \"[['txt'], ['img']]\" \\\n--left-weight \"[1]\" \\\n--right-weight \"[1]\" \\\n--run-queries-cpu \\\n--top-q 4000 \\\n--doc-id-key item_ID \\\n--context-length \"[[64], [0]]\"\n3. AmazonProducts-Category2Image Retrieval.\ncd ./GCL\nMODEL=hf-hub:Marqo/marqo-ecommerce-B\noutdir=MarqoModels/GE/marqo-ecommerce-B/ap-title2image\nmkdir -p $outdir\nhfdataset=Marqo/amazon-products-eval\npython  evals/eval_hf_datasets_v1.py \\\n--model_name $MODEL \\\n--hf-dataset $hfdataset \\\n--output-dir $outdir \\\n--batch-size 1024 \\\n--num_workers 8 \\\n--left-key \"['title']\" \\\n--right-key \"['image']\" \\\n--img-or-txt \"[['txt'], ['img']]\" \\\n--left-weight \"[1]\" \\\n--right-weight \"[1]\" \\\n--run-queries-cpu \\\n--top-q 4000 \\\n--doc-id-key item_ID \\\n--context-length \"[[64], [0]]\"\nDetailed Performance\nOur benchmarking process was divided into two distinct regimes, each using different datasets of ecommerce product listings: marqo-ecommerce-hard and marqo-ecommerce-easy. Both datasets contained product images and text and only differed in size. The \"easy\" dataset is approximately 10-30 times smaller (200k vs 4M products), and designed to accommodate rate-limited models, specifically Cohere-Embeddings-v3 and GCP-Vertex (with limits of 0.66 rps and 2 rps respectively). The \"hard\" dataset represents the true challenge, since it contains four million ecommerce product listings and is more representative of real-world ecommerce search scenarios.\nWithin both these scenarios, the models were benchmarked against three different tasks:\nGoogle Shopping Text-to-Image\nGoogle Shopping Category-to-Image\nAmazon Products Text-to-Image\nMarqo-Ecommerce-Hard\nMarqo-Ecommerce-Hard looks into the comprehensive evaluation conducted using the full 4 million dataset, highlighting the robust performance of our models in a real-world context.\nGoogleShopping-Text2Image Retrieval.\nEmbedding Model\nmAP\nR@10\nMRR\nnDCG@10\nMarqo-Ecommerce-L\n0.682\n0.878\n0.683\n0.726\nMarqo-Ecommerce-B\n0.623\n0.832\n0.624\n0.668\nViT-SO400M-14-SigLip\n0.573\n0.763\n0.574\n0.613\nViT-L-16-SigLip\n0.540\n0.722\n0.540\n0.577\nViT-B-16-SigLip\n0.476\n0.660\n0.477\n0.513\nAmazon-Titan-MultiModal\n0.475\n0.648\n0.475\n0.509\nJina-V1-CLIP\n0.285\n0.402\n0.285\n0.306\nGoogleShopping-Category2Image Retrieval.\nEmbedding Model\nmAP\nP@10\nMRR\nnDCG@10\nMarqo-Ecommerce-L\n0.463\n0.652\n0.822\n0.666\nMarqo-Ecommerce-B\n0.423\n0.629\n0.810\n0.644\nViT-SO400M-14-SigLip\n0.352\n0.516\n0.707\n0.529\nViT-L-16-SigLip\n0.324\n0.497\n0.687\n0.509\nViT-B-16-SigLip\n0.277\n0.458\n0.660\n0.473\nAmazon-Titan-MultiModal\n0.246\n0.429\n0.642\n0.446\nJina-V1-CLIP\n0.123\n0.275\n0.504\n0.294\nAmazonProducts-Text2Image Retrieval.\nEmbedding Model\nmAP\nR@10\nMRR\nnDCG@10\nMarqo-Ecommerce-L\n0.658\n0.854\n0.663\n0.703\nMarqo-Ecommerce-B\n0.592\n0.795\n0.597\n0.637\nViT-SO400M-14-SigLip\n0.560\n0.742\n0.564\n0.599\nViT-L-16-SigLip\n0.544\n0.715\n0.548\n0.580\nViT-B-16-SigLip\n0.480\n0.650\n0.484\n0.515\nAmazon-Titan-MultiModal\n0.456\n0.627\n0.457\n0.491\nJina-V1-CLIP\n0.265\n0.378\n0.266\n0.285\nMarqo-Ecommerce-Easy\nAs mentioned, our benchmarking process was divided into two distinct scenarios: marqo-ecommerce-hard and marqo-ecommerce-easy. This section covers the latter which features a corpus 10-30 times smaller and was designed to accommodate rate-limited models. We will look into the comprehensive evaluation conducted using the full 200k products across the two datasets. In addition to the models already benchmarked above, these benchmarks also include Cohere-embedding-v3 and GCP-Vertex.\nGoogleShopping-Text2Image Retrieval.\nEmbedding Model\nmAP\nR@10\nMRR\nnDCG@10\nMarqo-Ecommerce-L\n0.879\n0.971\n0.879\n0.901\nMarqo-Ecommerce-B\n0.842\n0.961\n0.842\n0.871\nViT-SO400M-14-SigLip\n0.792\n0.935\n0.792\n0.825\nGCP-Vertex\n0.740\n0.910\n0.740\n0.779\nViT-L-16-SigLip\n0.754\n0.907\n0.754\n0.789\nViT-B-16-SigLip\n0.701\n0.870\n0.701\n0.739\nAmazon-Titan-MultiModal\n0.694\n0.868\n0.693\n0.733\nJina-V1-CLIP\n0.480\n0.638\n0.480\n0.511\nCohere-embedding-v3\n0.358\n0.515\n0.358\n0.389\nGoogleShopping-Category2Image Retrieval.\nEmbedding Model\nmAP\nP@10\nMRR\nnDCG@10\nMarqo-Ecommerce-L\n0.515\n0.358\n0.764\n0.590\nMarqo-Ecommerce-B\n0.479\n0.336\n0.744\n0.558\nViT-SO400M-14-SigLip\n0.423\n0.302\n0.644\n0.487\nGCP-Vertex\n0.417\n0.298\n0.636\n0.481\nViT-L-16-SigLip\n0.392\n0.281\n0.627\n0.458\nViT-B-16-SigLip\n0.347\n0.252\n0.594\n0.414\nAmazon-Titan-MultiModal\n0.308\n0.231\n0.558\n0.377\nJina-V1-CLIP\n0.175\n0.122\n0.369\n0.229\nCohere-embedding-v3\n0.136\n0.110\n0.315\n0.178\nAmazonProducts-Text2Image Retrieval.\nEmbedding Model\nmAP\nR@10\nMRR\nnDCG@10\nMarqo-Ecommerce-L\n0.92\n0.978\n0.928\n0.940\nMarqo-Ecommerce-B\n0.897\n0.967\n0.897\n0.914\nViT-SO400M-14-SigLip\n0.860\n0.954\n0.860\n0.882\nViT-L-16-SigLip\n0.842\n0.940\n0.842\n0.865\nGCP-Vertex\n0.808\n0.933\n0.808\n0.837\nViT-B-16-SigLip\n0.797\n0.917\n0.797\n0.825\nAmazon-Titan-MultiModal\n0.762\n0.889\n0.763\n0.791\nJina-V1-CLIP\n0.530\n0.699\n0.530\n0.565\nCohere-embedding-v3\n0.433\n0.597\n0.433\n0.465\nCitation\n@software{zhu2024marqoecommembed_2024,\nauthor = {Tianyu Zhu and and Jesse Clark},\nmonth = oct,\ntitle = {{Marqo Ecommerce Embeddings - Foundation Model for Product Embeddings}},\nurl = {https://github.com/marqo-ai/marqo-ecommerce-embeddings/},\nversion = {1.0.0},\nyear = {2024}\n}"
}