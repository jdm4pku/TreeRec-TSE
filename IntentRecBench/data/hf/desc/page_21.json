{
    "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF": "Finetune your own Reasoning model like R1 with Unsloth!\n‚ú® Finetune for Free\nSpecial Thanks\nDeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nSee our collection for versions of Deepseek-R1 including GGUF & 4-bit formats.\nUnsloth's DeepSeek-R1 1.58-bit + 2-bit Dynamic Quants is selectively quantized, greatly improving accuracy over standard 1-bit/2-bit.\nInstructions to run this model in llama.cpp:\nYou can view more detailed instructions in our blog: unsloth.ai/blog/deepseek-r1\nDo not forget about <ÔΩúUserÔΩú> and <ÔΩúAssistantÔΩú> tokens! - Or use a chat template formatter\nObtain the latest llama.cpp at https://github.com/ggerganov/llama.cpp\nExample with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mode\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf \\\n--cache-type-k q8_0 \\\n--threads 16 \\\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>' \\\n-no-cnv\nExample output:\n<think>\nOkay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\nLet me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\nWait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\nI also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\nIs there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\nIf you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\n--cache-type-k q8_0\n--threads 16\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>'\n--n-gpu-layers 20 \\\n-no-cnv\nFinetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nGRPO with Phi-4 (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "Qwen/Qwen2.5-7B-Instruct-1M": "Qwen2.5-7B-Instruct-1M\nIntroduction\nRequirements\nQuickstart\nProcessing Ultra Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-7B-Instruct-1M\nIntroduction\nQwen2.5-1M is the long-context version of the Qwen2.5 series models, supporting a context length of up to 1M tokens. Compared to the Qwen2.5 128K version, Qwen2.5-1M demonstrates significantly improved performance in handling long-context tasks while maintaining its capability in short tasks.\nThe model has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 1,010,000 tokens and generation 8192 tokens\nWe recommend deploying with our custom vLLM, which introduces sparse attention and length extrapolation methods to ensure efficiency and accuracy for long-context tasks. For specific guidance, refer to this section.\nYou can also use the previous framework that supports Qwen2.5 for inference, but accuracy degradation may occur for sequences exceeding 262,144 tokens.\nFor more details, please refer to our blog, GitHub, Technical Report, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Ultra Long Texts\nTo enhance processing accuracy and efficiency for long sequences, we have developed an advanced inference framework based on vLLM, incorporating sparse attention and length extrapolation. This approach significantly improves model generation performance for sequences exceeding 256K tokens and achieves a 3 to 7 times speedup for sequences up to 1M tokens.\nHere we provide step-by-step instructions for deploying the Qwen2.5-1M models with our framework.\n1. System Preparation\nTo achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.\nEnsure your system meets the following requirements:\nCUDA Version: 12.1 or 12.3\nPython Version: >=3.9 and <=3.12\nVRAM Requirements:\nFor processing 1 million-token sequences:\nQwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs).\nQwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs).\nIf your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M for shorter tasks.\n2. Install Dependencies\nFor now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project.\ngit clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git\ncd vllm\npip install -e . -v\n3. Launch vLLM\nvLLM supports offline inference or launch an openai-like server.\nExample of Offline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-1M\")\n# Pass the default decoding hyperparameters of Qwen2.5-7B-Instruct\n# max_tokens is for the maximum length for generation.\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n# Input the model name or path. See below for parameter explanation (after the example of openai-like server).\nllm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct-1M\",\ntensor_parallel_size=4,\nmax_model_len=1010000,\nenable_chunked_prefill=True,\nmax_num_batched_tokens=131072,\nenforce_eager=True,\n# quantization=\"fp8\", # Enabling FP8 quantization for model weights can reduce memory usage.\n)\n# Prepare your prompts\nprompt = \"Tell me something about large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\n# generate outputs\noutputs = llm.generate([text], sampling_params)\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\nExample of Openai-like Server\nvllm serve Qwen/Qwen2.5-7B-Instruct-1M \\\n--tensor-parallel-size 4 \\\n--max-model-len 1010000 \\\n--enable-chunked-prefill --max-num-batched-tokens 131072 \\\n--enforce-eager \\\n--max-num-seqs 1\n# --quantization fp8  # Enabling FP8 quantization for model weights can reduce memory usage.\nThen you can use curl or python to interact with the deployed model.\nParameter Explanations:\n--tensor-parallel-size\nSet to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model.\n--max-model-len\nDefines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues.\n--max-num-batched-tokens\nSets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference.\nRecommend 131072 for optimal performance.\n--max-num-seqs\nLimits concurrent sequences processed.\nYou can also refer to our Documentation for usage of vLLM.\nTroubleshooting:\nEncountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\"\nThe VRAM reserved for the KV cache is insufficient. Consider reducing the max_model_len or increasing the tensor_parallel_size. Alternatively, you can reduce max_num_batched_tokens, although this may significantly slow down inference.\nEncountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\nThe VRAM reserved for activation weights is insufficient. You can try setting gpu_memory_utilization to 0.85 or lower, but be aware that this might reduce the VRAM available for the KV cache.\nEncountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\"\nThe input is too lengthy. Consider using a shorter sequence or increasing the max_model_len.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog and our technical report.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5-1m,\ntitle = {Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens},\nurl = {https://qwenlm.github.io/blog/qwen2.5-1m/},\nauthor = {Qwen Team},\nmonth = {January},\nyear = {2025}\n}\n@article{qwen2.5,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "Qwen/Qwen2.5-VL-72B-Instruct": "Qwen2.5-VL-72B-Instruct\nIntroduction\nEvaluation\nImage benchmark\nVideo benchmark\nAgent benchmark\nRequirements\nQuickstart\nUsing ü§ó  Transformers to Chat\nü§ñ ModelScope\nMore Usage Tips\nProcessing Long Texts\nCitation\nQwen2.5-VL-72B-Instruct\nIntroduction\nIn the past five months since Qwen2-VL‚Äôs release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\nKey Enhancements:\nUnderstand things visually: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\nBeing agentic: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\nUnderstanding long videos and capturing events: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\nCapable of visual localization in different formats: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\nGenerating structured outputs: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\nModel Architecture Updates:\nDynamic Resolution and Frame Rate Training for Video Understanding:\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\nStreamlined and Efficient Vision Encoder\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 72B Qwen2.5-VL model. For more information, visit our Blog and GitHub.\nEvaluation\nImage benchmark\nBenchmarks\nGPT4o\nClaude3.5 Sonnet\nGemini-2-flash\nInternVL2.5-78B\nQwen2-VL-72B\nQwen2.5-VL-72B\nMMMUval\n70.3\n70.4\n70.7\n70.1\n64.5\n70.2\nMMMU_Pro\n54.5\n54.7\n57.0\n48.6\n46.2\n51.1\nMathVista_MINI\n63.8\n65.4\n73.1\n76.6\n70.5\n74.8\nMathVision_FULL\n30.4\n38.3\n41.3\n32.2\n25.9\n38.1\nHallusion Bench\n55.0\n55.16\n57.4\n58.1\n55.16\nMMBench_DEV_EN_V11\n82.1\n83.4\n83.0\n88.5\n86.6\n88\nAI2D_TEST\n84.6\n81.2\n89.1\n88.1\n88.4\nChartQA_TEST\n86.7\n90.8\n85.2\n88.3\n88.3\n89.5\nDocVQA_VAL\n91.1\n95.2\n92.1\n96.5\n96.1\n96.4\nMMStar\n64.7\n65.1\n69.4\n69.5\n68.3\n70.8\nMMVet_turbo\n69.1\n70.1\n72.3\n74.0\n76.19\nOCRBench\n736\n788\n854\n877\n885\nOCRBench-V2(en/zh)\n46.5/32.3\n45.2/39.6\n51.9/43.1\n45/46.2\n47.8/46.1\n61.5/63.7\nCC-OCR\n66.6\n62.7\n73.0\n64.7\n68.7\n79.8\nVideo benchmark\nBenchmarks\nGPT4o\nGemini-1.5-Pro\nInternVL2.5-78B\nQwen2VL-72B\nQwen2.5VL-72B\nVideoMME w/o sub.\n71.9\n75.0\n72.1\n71.2\n73.3\nVideoMME w sub.\n77.2\n81.3\n74.0\n77.8\n79.1\nMVBench\n64.6\n60.5\n76.4\n73.6\n70.4\nMMBench-Video\n1.63\n1.30\n1.97\n1.70\n2.02\nLVBench\n30.8\n33.1\n-\n41.3\n47.3\nEgoSchema\n72.2\n71.2\n-\n77.9\n76.2\nPerceptionTest_test\n-\n-\n-\n68.0\n73.2\nMLVU_M-Avg_dev\n64.6\n-\n75.7\n74.6\nTempCompass_overall\n73.8\n-\n-\n74.8\nAgent benchmark\nBenchmarks\nGPT4o\nGemini 2.0\nClaude\nAguvis-72B\nQwen2VL-72B\nQwen2.5VL-72B\nScreenSpot\n18.1\n84.0\n83.0\n87.1\nScreenSpot Pro\n17.1\n1.6\n43.6\nAITZ_EM\n35.3\n72.8\n83.2\nAndroid Control High_EM\n66.4\n59.1\n67.36\nAndroid Control Low_EM\n84.4\n59.2\n93.7\nAndroidWorld_SR\n34.5% (SoM)\n27.9%\n26.1%\n35%\nMobileMiniWob++_SR\n66%\n68%\nOSWorld\n14.90\n10.26\n8.83\nRequirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers accelerate\nor you might encounter the following error:\nKeyError: 'qwen2_5_vl'\nQuickstart\nBelow, we provide simple examples to show how to use Qwen2.5-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers accelerate\nor you might encounter the following error:\nKeyError: 'qwen2_5_vl'\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\nIf you are not using Linux, you might not be able to install decord from PyPI. In that case, you can use pip install qwen-vl-utils which will fall back to using torchvision for video processing. However, you can still install decord from source to get decord used when loading video.\nUsing ü§ó  Transformers to Chat\nHere we show a code snippet to show you how to use the chat model with transformers and qwen_vl_utils:\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-VL-72B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-72B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-72B-Instruct\")\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-72B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nMulti image inference\n# Messages containing multiple images and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVideo inference\n# Messages containing a images list as a video and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": [\n\"file:///path/to/frame1.jpg\",\n\"file:///path/to/frame2.jpg\",\n\"file:///path/to/frame3.jpg\",\n\"file:///path/to/frame4.jpg\",\n],\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Messages containing a local video path and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": \"file:///path/to/video1.mp4\",\n\"max_pixels\": 360 * 420,\n\"fps\": 1.0,\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Messages containing a video url and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\nfps=fps,\npadding=True,\nreturn_tensors=\"pt\",\n**video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by FORCE_QWENVL_VIDEO_READER=torchvision or FORCE_QWENVL_VIDEO_READER=decord if you prefer not to use the default one.\nBackend\nHTTP\nHTTPS\ntorchvision >= 0.19.0\n‚úÖ\n‚úÖ\ntorchvision < 0.19.0\n‚ùå\n‚ùå\ndecord\n‚úÖ\n‚ùå\nBatch inference\n# Sample messages for batch inference\nmessages1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n],\n}\n]\nmessages2 = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n# Preparation for batch inference\ntexts = [\nprocessor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\nfor msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=texts,\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\nü§ñ ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. snapshot_download can help you solve issues concerning downloading checkpoints.\nMore Usage Tips\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Image URL\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Base64 encoded image\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nImage Resolution for performance boost\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n\"Qwen/Qwen2.5-VL-72B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\nBesides, We provide two methods for fine-grained control over the image size input to the model:\nDefine min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\nSpecify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 28.\n# min_pixels and max_pixels\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"resized_height\": 280,\n\"resized_width\": 420,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# resized_height and resized_width\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"min_pixels\": 50176,\n\"max_pixels\": 50176,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"type\": \"yarn\",\n\"mrope_section\": [\n16,\n24,\n24\n],\n\"factor\": 4,\n\"original_max_position_embeddings\": 32768\n}\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5-VL,\ntitle = {Qwen2.5-VL},\nurl = {https://qwenlm.github.io/blog/qwen2.5-vl/},\nauthor = {Qwen Team},\nmonth = {January},\nyear = {2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "onnx-community/Kokoro-82M-v1.0-ONNX": "Kokoro TTS\nTable of contents\nUsage\nJavaScript\nPython\nVoices/Samples\nQuantizations\nKokoro TTS\nKokoro is a frontier TTS model for its size of 82 million parameters (text in/audio out).\nTable of contents\nUsage\nJavaScript\nPython\nVoices/Samples\nQuantizations\nUsage\nJavaScript\nFirst, install the kokoro-js library from NPM using:\nnpm i kokoro-js\nYou can then generate speech as follows:\nimport { KokoroTTS } from \"kokoro-js\";\nconst model_id = \"onnx-community/Kokoro-82M-ONNX\";\nconst tts = await KokoroTTS.from_pretrained(model_id, {\ndtype: \"q8\", // Options: \"fp32\", \"fp16\", \"q8\", \"q4\", \"q4f16\"\n});\nconst text = \"Life is like a box of chocolates. You never know what you're gonna get.\";\nconst audio = await tts.generate(text, {\n// Use `tts.list_voices()` to list all available voices\nvoice: \"af_bella\",\n});\naudio.save(\"audio.wav\");\nPython\nimport os\nimport numpy as np\nfrom onnxruntime import InferenceSession\n# You can generate token ids as follows:\n#   1. Convert input text to phonemes using https://github.com/hexgrad/misaki\n#   2. Map phonemes to ids using https://huggingface.co/hexgrad/Kokoro-82M/blob/785407d1adfa7ae8fbef8ffd85f34ca127da3039/config.json#L34-L148\ntokens = [50, 157, 43, 135, 16, 53, 135, 46, 16, 43, 102, 16, 56, 156, 57, 135, 6, 16, 102, 62, 61, 16, 70, 56, 16, 138, 56, 156, 72, 56, 61, 85, 123, 83, 44, 83, 54, 16, 53, 65, 156, 86, 61, 62, 131, 83, 56, 4, 16, 54, 156, 43, 102, 53, 16, 156, 72, 61, 53, 102, 112, 16, 70, 56, 16, 138, 56, 44, 156, 76, 158, 123, 56, 16, 62, 131, 156, 43, 102, 54, 46, 16, 102, 48, 16, 81, 47, 102, 54, 16, 54, 156, 51, 158, 46, 16, 70, 16, 92, 156, 135, 46, 16, 54, 156, 43, 102, 48, 4, 16, 81, 47, 102, 16, 50, 156, 72, 64, 83, 56, 62, 16, 156, 51, 158, 64, 83, 56, 16, 44, 157, 102, 56, 16, 44, 156, 76, 158, 123, 56, 4]\n# Context length is 512, but leave room for the pad token 0 at the start & end\nassert len(tokens) <= 510, len(tokens)\n# Style vector based on len(tokens), ref_s has shape (1, 256)\nvoices = np.fromfile('./voices/af.bin', dtype=np.float32).reshape(-1, 1, 256)\nref_s = voices[len(tokens)]\n# Add the pad ids, and reshape tokens, should now have shape (1, <=512)\ntokens = [[0, *tokens, 0]]\nmodel_name = 'model.onnx' # Options: model.onnx, model_fp16.onnx, model_quantized.onnx, model_q8f16.onnx, model_uint8.onnx, model_uint8f16.onnx, model_q4.onnx, model_q4f16.onnx\nsess = InferenceSession(os.path.join('onnx', model_name))\naudio = sess.run(None, dict(\ninput_ids=tokens,\nstyle=ref_s,\nspeed=np.ones(1, dtype=np.float32),\n))[0]\nOptionally, save the audio to a file:\nimport scipy.io.wavfile as wavfile\nwavfile.write('audio.wav', 24000, audio[0])\nVoices/Samples\nLife is like a box of chocolates. You never know what you're gonna get.\nName\nNationality\nGender\nSample\naf_heart\nAmerican\nFemale\naf_alloy\nAmerican\nFemale\naf_aoede\nAmerican\nFemale\naf_bella\nAmerican\nFemale\naf_jessica\nAmerican\nFemale\naf_kore\nAmerican\nFemale\naf_nicole\nAmerican\nFemale\naf_nova\nAmerican\nFemale\naf_river\nAmerican\nFemale\naf_sarah\nAmerican\nFemale\naf_sky\nAmerican\nFemale\nam_adam\nAmerican\nMale\nam_echo\nAmerican\nMale\nam_eric\nAmerican\nMale\nam_fenrir\nAmerican\nMale\nam_liam\nAmerican\nMale\nam_michael\nAmerican\nMale\nam_onyx\nAmerican\nMale\nam_puck\nAmerican\nMale\nam_santa\nAmerican\nMale\nbf_alice\nBritish\nFemale\nbf_emma\nBritish\nFemale\nbf_isabella\nBritish\nFemale\nbf_lily\nBritish\nFemale\nbm_daniel\nBritish\nMale\nbm_fable\nBritish\nMale\nbm_george\nBritish\nMale\nbm_lewis\nBritish\nMale\nQuantizations\nThe model is resilient to quantization, enabling efficient high-quality speech synthesis at a fraction of the original model size.\nHow could I know? It's an unanswerable question. Like asking an unborn child if they'll lead a good life. They haven't even been born.\nModel\nSize (MB)\nSample\nmodel.onnx (fp32)\n326\nmodel_fp16.onnx (fp16)\n163\nmodel_quantized.onnx (8-bit)\n92.4\nmodel_q8f16.onnx (Mixed precision)\n86\nmodel_uint8.onnx (8-bit & mixed precision)\n177\nmodel_uint8f16.onnx (Mixed precision)\n114\nmodel_q4.onnx (4-bit matmul)\n305\nmodel_q4f16.onnx (4-bit matmul & fp16 weights)\n154",
    "google/siglip2-base-patch16-512": "SigLIP 2 Base\nIntended uses\nTraining procedure\nTraining data\nCompute\nEvaluation results\nBibTeX entry and citation info\nSigLIP 2 Base\nSigLIP 2 extends the pretraining objective of\nSigLIP with prior, independently developed techniques\ninto a unified recipe, for improved semantic understanding, localization, and dense features.\nIntended uses\nYou can use the raw model for tasks like zero-shot image classification and\nimage-text retrieval, or as a vision encoder for VLMs (and other vision tasks).\nHere is how to use this model to perform zero-shot image classification:\nfrom transformers import pipeline\n# load pipeline\nckpt = \"google/siglip2-base-patch16-512\"\nimage_classifier = pipeline(model=ckpt, task=\"zero-shot-image-classification\")\n# load image and candidate labels\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\ncandidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n# run inference\noutputs = image_classifier(image, candidate_labels)\nprint(outputs)\nYou can encode an image using the Vision Tower like so:\nimport torch\nfrom transformers import AutoModel, AutoProcessor\nfrom transformers.image_utils import load_image\n# load the model and processor\nckpt = \"google/siglip2-base-patch16-512\"\nmodel = AutoModel.from_pretrained(ckpt, device_map=\"auto\").eval()\nprocessor = AutoProcessor.from_pretrained(ckpt)\n# load the image\nimage = load_image(\"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\")\ninputs = processor(images=[image], return_tensors=\"pt\").to(model.device)\n# run infernece\nwith torch.no_grad():\nimage_embeddings = model.get_image_features(**inputs)\nprint(image_embeddings.shape)\nFor more code examples, we refer to the siglip documentation.\nTraining procedure\nSigLIP 2 adds some clever training objectives on top of SigLIP:\nDecoder loss\nGlobal-local and masked prediction loss\nAspect ratio and resolution adaptibility\nTraining data\nSigLIP 2 is pre-trained on the WebLI dataset (Chen et al., 2023).\nCompute\nThe model was trained on up to 2048 TPU-v5e chips.\nEvaluation results\nEvaluation of SigLIP 2 is shown below (taken from the paper).\nBibTeX entry and citation info\n@misc{tschannen2025siglip2multilingualvisionlanguage,\ntitle={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features},\nauthor={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier H√©naff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},\nyear={2025},\neprint={2502.14786},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2502.14786},\n}",
    "perplexity-ai/r1-1776": "R1 1776\nEvals\nR1 1776\nBlog link: https://perplexity.ai/hub/blog/open-sourcing-r1-1776\nR1 1776 is a DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship.\nThe model provides unbiased, accurate, and factual information while maintaining high reasoning capabilities.\nEvals\nTo ensure our model remains fully ‚Äúuncensored‚Äù and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.\nWe also ensured that the model‚Äôs math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.",
    "google/siglip2-so400m-patch16-naflex": "SigLIP 2 So400m\nIntended uses\nTraining procedure\nTraining data\nCompute\nEvaluation results\nBibTeX entry and citation info\nSigLIP 2 So400m\nSigLIP 2 extends the pretraining objective of\nSigLIP with prior, independently developed techniques\ninto a unified recipe, for improved semantic understanding, localization, and dense features.\nIntended uses\nYou can use the raw model for tasks like zero-shot image classification and\nimage-text retrieval, or as a vision encoder for VLMs (and other vision tasks).\nHere is how to use this model to perform zero-shot image classification:\nfrom transformers import pipeline\n# load pipeline\nckpt = \"google/siglip2-so400m-patch16-naflex\"\nimage_classifier = pipeline(model=ckpt, task=\"zero-shot-image-classification\")\n# load image and candidate labels\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\ncandidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n# run inference\noutputs = image_classifier(image, candidate_labels)\nprint(outputs)\nYou can encode an image using the Vision Tower like so:\nimport torch\nfrom transformers import AutoModel, AutoProcessor\nfrom transformers.image_utils import load_image\n# load the model and processor\nckpt = \"google/siglip2-so400m-patch16-naflex\"\nmodel = AutoModel.from_pretrained(ckpt, device_map=\"auto\").eval()\nprocessor = AutoProcessor.from_pretrained(ckpt)\n# load the image\nimage = load_image(\"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\")\ninputs = processor(images=[image], return_tensors=\"pt\").to(model.device)\n# run infernece\nwith torch.no_grad():\nimage_embeddings = model.get_image_features(**inputs)\nprint(image_embeddings.shape)\nFor more code examples, we refer to the siglip2 documentation.\nTraining procedure\nSigLIP 2 adds some clever training objectives on top of SigLIP:\nDecoder loss\nGlobal-local and masked prediction loss\nAspect ratio and resolution adaptibility\nTraining data\nSigLIP 2 is pre-trained on the WebLI dataset (Chen et al., 2023).\nCompute\nThe model was trained on up to 2048 TPU-v5e chips.\nEvaluation results\nEvaluation of SigLIP 2 is shown below (taken from the paper).\nBibTeX entry and citation info\n@misc{tschannen2025siglip2multilingualvisionlanguage,\ntitle={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features},\nauthor={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier H√©naff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},\nyear={2025},\neprint={2502.14786},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2502.14786},\n}",
    "Politrees/UVR_resources": "Made for UVR_resources on GitHub",
    "google/gemma-3-4b-pt": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/gemma-3-4b-pt\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16\n)\noutput = pipe(\n\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\ntext=\"<start_of_image> in this image, there is\"\n)\nprint(output)\n# [{'input_text': '<start_of_image> in this image, there is',\n# 'generated_text': '<start_of_image> in this image, there is a bumblebee on a pink flower.\\n\\n'}]\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/gemma-3-4b-pt\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = Gemma3ForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nprompt = \"<start_of_image> in this image, there is\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "fnlp/Llama-2-7B-MHA-d_kv_256": "Inference\nCitation\nResearch Paper \"Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs\"\nInference\nStep 1: Download the monkey patch file.\nwget https://raw.githubusercontent.com/JT-Ushio/MHA2MLA/refs/heads/main/src/mha2mla/monkey_patch.py\nStep 2(Option): For MHA2MLA models using Partial-RoPE 2-nrom method, Download the qk_2-norm file.\nTake qk_tensor_7B.pth as an example:\nwget https://github.com/JT-Ushio/MHA2MLA/raw/refs/heads/main/utils/qk_tensor_7B.pth\nStep 3: Download the MHA2MLA models and run inference.\nTake fnlp/Llama-2-7B-MHA-d_kv_256 as an example:\nimport torch\nfrom transformers import AutoConfig, AutoTokenizer, LlamaForCausalLM\nfrom monkey_patch import infer_monkey_patch\nmodel_name = \"fnlp/Llama-2-7B-MHA-d_kv_256\"\n# Monkey Patch: MHA -> MLA\nconfig = AutoConfig.from_pretrained(model_name)\nif \"RoPE\" in config:\nconfig.RoPE[\"qk_tensor_path\"] = \"qk_tensor_7B.pth\"  # Configuration for Specific Models\ninfer_monkey_patch(config.RoPE)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = LlamaForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.bfloat16).cuda()\n# Generate\ntext = \"Which American-born Sinclair won the Nobel Prize for Literature in 1930?\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\ngeneration_kwargs = {\"do_sample\": False, \"use_cache\": True, \"max_new_tokens\": 128}\noutput = model.generate(**inputs, **generation_kwargs)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n# - Sinclair Lewis\nCitation\n@misc{ji2025economicalinferenceenablingdeepseeks,\ntitle={Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs},\nauthor={Tao Ji and Bin Guo and Yuanbin Wu and Qipeng Guo and Lixing Shen and Zhan Chen and Xipeng Qiu and Qi Zhang and Tao Gui},\nyear={2025},\neprint={2502.14837},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.14837},\n}",
    "city96/umt5-xxl-encoder-gguf": "This is a GGUF conversion of Google's UMT5 xxl model, specifically the encoder part.\nThe weights can be used with ./llama-embedding or with the ComfyUI-GGUF custom node together with image/video generation models.\nThis is a non imatrix quant as llama.cpp doesn't support imatrix creation for T5 models at the time of writing. It's therefore recommended to use Q5_K_M or larger for the best results, although smaller models may also still provide decent results in resource constrained scenarios.",
    "iahhnim/adetailer_collection": "I collected a whole bunch of models from CivitAI and Hugging Face. There are many models that have been flagged, but I have all of them on my PC and nothing has happened to me yet, so I think it‚Äôs kind of safe to use\n.-.",
    "google/gemma-3-4b-it-qat-q4_0-gguf": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nThis repository corresponds to the 4B instruction-tuned version of the Gemma 3 model in GGUF format using Quantization Aware Training (QAT).\nThe GGUF corresponds to Q4_0 quantization.\nThanks to QAT, the model is able to preserve similar quality as bfloat16 while significantly reducing the memory requirements\nto load the model.\nYou can find the half-precision version here.\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model.\nllama.cpp (text-only)\n./llama-cli -hf google/gemma-3-4b-it-qat-q4_0-gguf -p \"Write a poem about the Kraken.\"\nllama.cpp (image input)\nwget https://github.com/bebechien/gemma/blob/main/surprise.png?raw=true -O ~/Downloads/surprise.png\n./llama-gemma3-cli -hf google/gemma-3-4b-it-qat-q4_0-gguf -p \"Describe this image.\" --image ~/Downloads/surprise.png\nollama (text only)\nUsing GGUFs with Ollama via Hugging Face does not support image inputs at the moment. Please check the docs on running gated repositories.\nollama run hf.co/google/gemma-3-4b-it-qat-q4_0-gguf\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nThe evaluation in this section correspond to the original checkpoint, not the QAT checkpoint.\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "immich-app/ViT-SO400M-16-SigLIP2-384__webli": "Model Description\nModel Description\nThis repo contains ONNX exports for the associated CLIP model by OpenCLIP. See the OpenCLIP repo for more info.\nThis repo is specifically intended for use with Immich, a self-hosted photo library.",
    "canopylabs/orpheus-3b-0.1-ft": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nOrpheus 3B 0.1 Finetuned\nModel Capabilities\nModel Sources\nModel Details\nModel Capabilities\nModel Sources\nUsage\nModel Misuse\nOrpheus 3B 0.1 Finetuned\n03/18/2025 ‚Äì We are releasing our 3B Orpheus TTS model with additional finetunes. Code is available on GitHub: CanopyAI/Orpheus-TTS\nOrpheus TTS is a state-of-the-art, Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation. This model has been finetuned to deliver human-level speech synthesis, achieving exceptional clarity, expressiveness, and real-time streaming performances.\nModel Details\nModel Capabilities\nHuman-Like Speech: Natural intonation, emotion, and rhythm that is superior to SOTA closed source models\nZero-Shot Voice Cloning: Clone voices without prior fine-tuning\nGuided Emotion and Intonation: Control speech and emotion characteristics with simple tags\nLow Latency: ~200ms streaming latency for realtime applications, reducible to ~100ms with input streaming\nModel Sources\nGitHub Repo: https://github.com/canopyai/Orpheus-TTS\nBlog Post: https://canopylabs.ai/model-releases\nColab Inference Notebook: notebook link\nOne-Click Deployment on Baseten: https://www.baseten.co/library/orpheus-tts/\nUsage\nCheck out our Colab (link to Colab) or GitHub (link to GitHub) on how to run easy inference on our finetuned models.\nModel Misuse\nDo not use our models for impersonation without consent, misinformation or deception (including fake news or fraudulent calls), or any illegal or harmful activity. By using this model, you agree to follow all applicable laws and ethical guidelines. We disclaim responsibility for any use.",
    "IndexTeam/Index-TTS": "üëâüèª IndexTTS üëàüèª\nAcknowledge\nüìö Citation\nIndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System\nüëâüèª IndexTTS üëàüèª\n[Paper] [Demos] [Codes]\nIndexTTS is a GPT-style text-to-speech (TTS) model mainly based on XTTS and Tortoise. It is capable of correcting the pronunciation of Chinese characters using pinyin and controlling pauses at any position through punctuation marks. We enhanced multiple modules of the system, including the improvement of speaker condition feature representation, and the integration of BigVGAN2 to optimize audio quality. Trained on tens of thousands of hours of data, our system achieves state-of-the-art performance, outperforming current popular TTS systems such as XTTS, CosyVoice2, Fish-Speech, and F5-TTS.\nExperience IndexTTS: Please contact xuanwu@bilibili.com for more detailed information.\nAcknowledge\ntortoise-tts\nXTTSv2\nBigVGAN\nwenet\nicefall\nüìö Citation\nüåü If you find our work helpful, please leave us a star and cite our paper.\n@article{deng2025indextts,\ntitle={IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System},\nauthor={Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang},\njournal={arXiv preprint arXiv:2502.05512},\nyear={2025}\n}",
    "ToastyPigeon/Gemma-3-Starshine-12B": "üå†G3 Starshine 12Büå†\nInstruct Format\nMerge Configuration\nüå†G3 Starshine 12Büå†\nThis was Merge A / A1 in the testing set.\nA creative writing model based on a merge of fine-tunes on Gemma 3 12B IT and Gemma 3 12B PT.\nThis is the Story Focused merge. This version works better for storytelling and scenarios, as the prose is more novel-like and it has a tendency to impersonate the user character.\nSee the Alternate RP Focused version as well.\nThis is a merge of two G3 models, one trained on instruct and one trained on base:\nallura-org/Gemma-3-Glitter-12B - Itself a merge of a storywriting and RP train (both also by ToastyPigeon), on instruct\nToastyPigeon/Gemma-3-Confetti-12B - Experimental application of the Glitter data using base instead of instruct, additionally includes some adventure data in the form of SpringDragon.\nThe result is a lovely blend of Glitter's ability to follow instructions and Confetti's free-spirit prose, effectively 'loosening up' much of the hesitancy that was left in Glitter.\nUpdate: Vision tower is back! Have fun.\nThank you to jebcarter for the idea to make this. I love how it turned out!\nInstruct Format\nUses Gemma2/3 instruct, but has been trained to recognize an optional system role.\nNote: While it won't immediately balk at the system role, results may be better without it.\n<start_of_turn>system\n{optional system turn with prompt}<end_of_turn>\n<start_of_turn>user\n{User messages; can also put sysprompt here to use the built-in g3 training}<end_of_turn>\n<start_of_turn>model\n{model response}<end_of_turn>\nMerge Configuration\nYeah, I actually tried several things and surprisingly this one worked best.\nmodels:\n- model: ToastyPigeon/Gemma-3-Confetti-12B\nparameters:\nweight: 0.5\n- model: allura-org/Gemma-3-Glitter-12B\nparameters:\nweight: 0.5\nmerge_method: linear\ntokenizer_source: allura-org/Gemma-3-Glitter-12B",
    "Salesforce/Llama-xLAM-2-8b-fc-r": "Welcome to the xLAM-2 Model Family!\nTable of Contents\nModel Series\nUsage\nFramework versions\nBasic Usage with Huggingface Chat Template\nUsing vLLM for Inference\nBenchmark Results\nBerkeley Function-Calling Leaderboard (BFCL v3)\nœÑ-bench Benchmark\nEthical Considerations\nModel Licenses\nCitation\n[Paper]  |\n[Homepage]  |\n[Dataset] |\n[Github]\nWelcome to the xLAM-2 Model Family!\nLarge Action Models (LAMs) are advanced language models designed to enhance decision-making by translating user intentions into executable actions. As the brains of AI agents, LAMs autonomously plan and execute tasks to achieve specific goals, making them invaluable for automating workflows across diverse domains.This model release is for research purposes only.\nThe new xLAM-2 series, built on our most advanced data synthesis, processing, and training pipelines, marks a significant leap in multi-turn conversation and tool usage. Trained using our novel APIGen-MT framework, which generates high-quality training data through simulated agent-human interactions. Our models achieve state-of-the-art performance on BFCL and œÑ-bench benchmarks, outperforming frontier models like GPT-4o and Claude 3.5. Notably, even our smaller models demonstrate superior capabilities in multi-turn scenarios while maintaining exceptional consistency across trials.\nWe've also refined the chat template and vLLM integration, making it easier to build advanced AI agents. Compared to previous xLAM models, xLAM-2 offers superior performance and seamless deployment across applications.\nComparative performance of larger xLAM-2-fc-r models (8B-70B, trained with APIGen-MT data) against state-of-the-art baselines on function-calling (BFCL v3, as of date 04/02/2025) and agentic (œÑ-bench) capabilities.\nTable of Contents\nUsage\nBasic Usage with Huggingface Chat Template\nUsing vLLM for Inference\nSetup and Serving\nTesting with OpenAI API\nBenchmark Results\nCitation\nModel Series\nxLAM series are significant better at many things including general tasks and function calling.\nFor the same number of parameters, the model have been fine-tuned across a wide range of agent tasks and scenarios, all while preserving the capabilities of the original model.\nModel\n# Total Params\nContext Length\nCategory\nDownload Model\nDownload GGUF files\nLlama-xLAM-2-70b-fc-r\n70B\n128k\nMulti-turn Conversation, Function-calling\nü§ó Link\nNA\nLlama-xLAM-2-8b-fc-r\n8B\n128k\nMulti-turn Conversation, Function-calling\nü§ó Link\nü§ó Link\nxLAM-2-32b-fc-r\n32B\n32k (max 128k)*\nMulti-turn Conversation, Function-calling\nü§ó Link\nNA\nxLAM-2-3b-fc-r\n3B\n32k (max 128k)*\nMulti-turn Conversation, Function-calling\nü§ó Link\nü§ó Link\nxLAM-2-1b-fc-r\n1B\n32k (max 128k)*\nMulti-turn Conversation, Function-calling\nü§ó Link\nü§ó Link\n*Note: The default context length for Qwen-2.5-based models is 32k, but you can use techniques like YaRN (Yet Another Recursive Network) to achieve maximum 128k context length. Please refer to here for more details.\nYou can also explore our previous xLAM series here.\nThe -fc suffix indicates that the models are fine-tuned for function calling tasks, while the -r suffix signifies a research release.\n‚úÖ All models are fully compatible with vLLM and Transformers-based inference frameworks.\nUsage\nFramework versions\nTransformers 4.46.1 (or later)\nPyTorch 2.5.1+cu124 (or later)\nDatasets 3.1.0 (or later)\nTokenizers 0.20.3 (or later)\nBasic Usage with Huggingface Chat Template\nThe new xLAM models are designed to work seamlessly with the Hugging Face Transformers library and utilize natural chat templates for an easy and intuitive conversational experience. Below are examples of how to use these models.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/Llama-xLAM-2-3b-fc-r\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/Llama-xLAM-2-3b-fc-r\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n# Example conversation with a tool call\nmessages = [\n{\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n{\"role\": \"assistant\", \"content\": \"Thanks. I am doing well. How can I help you?\"},\n{\"role\": \"user\", \"content\": \"What's the weather like in London?\"},\n]\ntools = [\n{\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather for a location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n\"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The unit of temperature to return\"}\n},\n\"required\": [\"location\"]\n}\n}\n]\nprint(\"====== prompt after applying chat template ======\")\nprint(tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, tokenize=False))\ninputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\ninput_ids_len = inputs[\"input_ids\"].shape[-1] # Get the length of the input tokens\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\nprint(\"====== model response ======\")\noutputs = model.generate(**inputs, max_new_tokens=256)\ngenerated_tokens = outputs[:, input_ids_len:] # Slice the output to get only the newly generated tokens\nprint(tokenizer.decode(generated_tokens[0], skip_special_tokens=True))\nUsing vLLM for Inference\nThe xLAM models can also be efficiently served using vLLM for high-throughput inference. Please use vllm>=0.6.5 since earlier versions will cause degraded performance for Qwen-based models.\nSetup and Serving\nInstall vLLM with the required version:\npip install \"vllm>=0.6.5\"\nDownload the tool parser plugin to your local path:\nwget https://huggingface.co/Salesforce/xLAM-2-1b-fc-r/raw/main/xlam_tool_call_parser.py\nStart the OpenAI API-compatible endpoint:\nvllm serve Salesforce/xLAM-2-1b-fc-r \\\n--enable-auto-tool-choice \\\n--tool-parser-plugin ./xlam_tool_call_parser.py \\\n--tool-call-parser xlam \\\n--tensor-parallel-size 1\nNote: Ensure that the tool parser plugin file is downloaded and that the path specified in --tool-parser-plugin correctly points to your local copy of the file. The xLAM series models all utilize the same tool call parser, so you only need to download it once for all models.\nTesting with OpenAI API\nHere's a minimal example to test tool usage with the served endpoint:\nimport openai\nimport json\n# Configure the client to use your local vLLM endpoint\nclient = openai.OpenAI(\nbase_url=\"http://localhost:8000/v1\",  # Default vLLM server URL\napi_key=\"empty\"  # Can be any string\n)\n# Define a tool/function\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather for a location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\"\n},\n\"unit\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The unit of temperature to return\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n]\n# Create a chat completion\nresponse = client.chat.completions.create(\nmodel=\"Salesforce/xLAM-2-1b-fc-r\",  # Model name doesn't matter, vLLM uses the served model\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant that can use tools.\"},\n{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n],\ntools=tools,\ntool_choice=\"auto\"\n)\n# Print the response\nprint(\"Assistant's response:\")\nprint(json.dumps(response.model_dump(), indent=2))\nFor more advanced configurations and deployment options, please refer to the vLLM documentation.\nBenchmark Results\nBerkeley Function-Calling Leaderboard (BFCL v3)\nPerformance comparison of different models on [BFCL leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html). The rank is based on the overall accuracy, which is a weighted average of different evaluation categories. \"FC\" stands for function-calling mode in contrast to using a customized \"prompt\" to extract the function calls.\nœÑ-bench Benchmark\nSuccess Rate (pass@1) on œÑ-bench benchmark averaged across at least 5 trials. Our xLAM-2-70b-fc-r model achieves an overall success rate of 56.2% on œÑ-bench, significantly outperforming the base Llama 3.1 70B Instruct model (38.2%) and other open-source models like DeepSeek v3 (40.6%). Notably, our best model even outperforms proprietary models such as GPT-4o (52.9%) and approaches the performance of more recent models like Claude 3.5 Sonnet (new) (60.1%).\nPass^k curves measuring the probability that all 5 independent trials succeed for a given task, averaged across all tasks for œÑ-retail (left) and œÑ-airline (right) domains. Higher values indicate better consistency of the models.\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people's lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nModel Licenses\nFor all Llama relevant models, please also follow corresponding Llama license and terms. Meta Llama 3 is licensed under the Meta Llama 3 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.\nCitation\nIf you use our model or dataset in your work, please cite our paper:\n@article{prabhakar2025apigen,\ntitle={APIGen-MT: Agentic PIpeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay},\nauthor={Prabhakar, Akshara and Liu, Zuxin and Zhu, Ming and Zhang, Jianguo and Awalgaonkar, Tulika and Wang, Shiyu and Liu, Zhiwei and Chen, Haolin and Hoang, Thai and others},\njournal={arXiv preprint arXiv:2504.03601},\nyear={2025}\n}\nAdditionally, please check our other awesome related works regarding xLAM series and consider citing them as well:\n@article{zhang2025actionstudio,\ntitle={ActionStudio: A Lightweight Framework for Data and Training of Action Models},\nauthor={Zhang, Jianguo and Hoang, Thai and Zhu, Ming and Liu, Zuxin and Wang, Shiyu and Awalgaonkar, Tulika and Prabhakar, Akshara and Chen, Haolin and Yao, Weiran and Liu, Zhiwei and others},\njournal={arXiv preprint arXiv:2503.22673},\nyear={2025}\n}\n@article{zhang2024xlam,\ntitle={xLAM: A Family of Large Action Models to Empower AI Agent Systems},\nauthor={Zhang, Jianguo and Lan, Tian and Zhu, Ming and Liu, Zuxin and Hoang, Thai and Kokane, Shirley and Yao, Weiran and Tan, Juntao and Prabhakar, Akshara and Chen, Haolin and others},\njournal={arXiv preprint arXiv:2409.03215},\nyear={2024}\n}\n@article{liu2024apigen,\ntitle={Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets},\nauthor={Liu, Zuxin and Hoang, Thai and Zhang, Jianguo and Zhu, Ming and Lan, Tian and Tan, Juntao and Yao, Weiran and Liu, Zhiwei and Feng, Yihao and RN, Rithesh and others},\njournal={Advances in Neural Information Processing Systems},\nvolume={37},\npages={54463--54482},\nyear={2024}\n}\n@article{zhang2024agentohana,\ntitle={AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning},\nauthor={Zhang, Jianguo and Lan, Tian and Murthy, Rithesh and Liu, Zhiwei and Yao, Weiran and Tan, Juntao and Hoang, Thai and Yang, Liangwei and Feng, Yihao and Liu, Zuxin and others},\njournal={arXiv preprint arXiv:2402.15506},\nyear={2024}\n}",
    "soob3123/amoral-gemma3-4B-v2": "\"Neutrality is not indifference. It is engagement with equal intensity.\"‚Äï J. Robert Oppenheimer [Lecture on Scientific Ethics, 1957]\nCore Function:\nProduces analytically neutral responses to sensitive queries\nMaintains factual integrity on controversial subjects\nAvoids value-judgment phrasing patterns\nResponse Characteristics:\nNo inherent moral framing (\"evil slop\" reduction)\nEmotionally neutral tone enforcement\nEpistemic humility protocols (avoids \"thrilling\", \"wonderful\", etc.)",
    "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1": "Llama-3.1-Nemotron-Ultra-253B-v1\nModel Overview\nFeature Voting\nLicense/Terms of Use\nUse Case:\nRelease Date:\nReferences\nModel Architecture\nIntended use\nInput\nOutput\nSoftware Integration\nModel Version\nQuick Start and Usage Recommendations:\nUse It with Transformers\nUse It with vLLM\nInference:\nTraining and Evaluation Datasets\nTraining Datasets\nEvaluation Datasets\nEvaluation Results\nGPQA\nAIME25\nBFCL V2 Live\nLiveCodeBench (20240801-20250201)\nIFEval\nMATH500\nJudgeBench\nEthical Considerations:\nCitation\nLlama-3.1-Nemotron-Ultra-253B-v1\nModel Overview\nLlama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference.\nLlama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model‚Äôs memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see details here), it also offers a significant improvement in latency.\nThe model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following.\nThis model is ready for commercial use.\nFor more details on how the model was trained, please see our technical report and blog.\nThis model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here:\nLlama-3.1-Nemotron-Nano-8B-v1\nLlama-3.3-Nemotron-Super-49B-v1\nFeature Voting\nWe want to hear from you! Share your ideas, vote on what matters, and help shape the future of Nemotron.\nLicense/Terms of Use\nGOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.1 Community License Agreement. Built with Llama.\nModel Developer: NVIDIA\nModel Dates: Trained between November 2024 and April 2025\nData Freshness:  The pretraining data has a cutoff of 2023 per Llama-3.1-405B-Instruct\nUse Case:\nDevelopers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks.\nRelease Date:\n2025-04-07\nReferences\n[2505.00949] Llama-Nemotron: Efficient Reasoning Models\n[2502.00203] Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment\n[2411.19146]Puzzle: Distillation-Based NAS for Inference-Optimized LLMs\n[2503.18908]FFN Fusion: Rethinking Sequential Computation in Large Language Models\nModel Architecture\nArchitecture Type: Dense decoder-only Transformer modelNetwork Architecture: Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS)\n**This model was developed based on Llama-3.1-405B-Instruct\n** This model has 253B model parameters.\nThe model is a derivative of Llama 3.1-405B-Instruct, using Neural Architecture Search (NAS). The NAS algorithm results in non-standard and non-repetitive blocks. This includes the following:\nSkip attention: In some blocks, the attention is skipped entirely, or replaced with a single linear layer.\nVariable FFN: The expansion/compression ratio in the FFN layer is different between blocks.\nFFN Fusion: When several consecutive attention layers are skipped, which can result in a sequence of multiple FFNs, that sequence of FFNs are fused into a smaller number of wider FFN layers.\nFor each block of the reference model, we create multiple variants providing different tradeoffs of quality vs. computational complexity, discussed in more depth below. We then search over the blocks to create a model which meets the required throughput and memory while minimizing the quality degradation. To recover performance, the model initially undergoes knowledge distillation (KD) for 65 billion tokens. This is followed by a continual pretraining (CPT) phase for 88 billion tokens.\nIntended use\nLlama-3.1-Nemotron-Ultra-253B-v1 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported.\nInput\nInput Type: Text\nInput Format: String\nInput Parameters: One-Dimensional (1D)\nOther Properties Related to Input: Context length up to 131,072 tokens\nOutput\nOutput Type: Text\nOutput Format: String\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output: Context length up to 131,072 tokens\nSoftware Integration\nRuntime Engine: Transformers\nRecommended Hardware Microarchitecture Compatibility:\nNVIDIA Hopper\nNVIDIA Ampere\nPreferred Operating System(s): Linux\nModel Version\n1.0 (4/7/2025)\nQuick Start and Usage Recommendations:\nReasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt\nWe recommend setting temperature to `0.6`, and Top P to `0.95` for Reasoning ON mode\nWe recommend using greedy decoding (temperature 0) for Reasoning OFF mode\nWe do not recommend to add additional system prompts besides the control prompt, all instructions should be put into user query\nWe have provided a list of prompts to use for evaluation for each benchmark where a specific template is required\nThe model will include <think></think> if no reasoning was necessary in Reasoning ON model, this is expected behaviour\nYou can try this model out through the preview API, using this link: Llama-3_1-Nemotron-Ultra-253B-v1.\nUse It with Transformers\nSee the snippet below for usage with Hugging Face Transformers library. Reasoning mode (ON/OFF) is controlled via system prompt. Please see the example below\nWe recommend using the transformers package with version 4.48.3.Example of reasoning on:\nimport torch\nimport transformers\nmodel_id = \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\"\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": True, \"device_map\": \"auto\"}\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\ntokenizer=tokenizer,\nmax_new_tokens=32768,\ntemperature=0.6,\ntop_p=0.95,\n**model_kwargs\n)\nthinking = \"on\"\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"},{\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}]))\nExample of reasoning off:\nimport torch\nimport transformers\nmodel_id = \"nvidia/Llama-3_1-Nemotron-ULtra-253B-v1\"\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": True, \"device_map\": \"auto\"}\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\ntokenizer=tokenizer,\nmax_new_tokens=32768,\ndo_sample=False,\n**model_kwargs\n)\nthinking = \"off\"\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"},{\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}]))\nUse It with vLLM\npip install vllm==0.8.3\nAn example on how to serve with vLLM:\npython3 -m vllm.entrypoints.openai.api_server \\\n--model \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\" \\\n--trust-remote-code \\\n--seed=1 \\\n--host=\"0.0.0.0\" \\\n--port=5000 \\\n--served-model-name \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\" \\\n--tensor-parallel-size=8 \\\n--max-model-len=32768 \\\n--gpu-memory-utilization 0.95 \\\n--enforce-eager\nInference:\nEngine:\nTransformers\nTest Hardware:\nBF16:\n8x NVIDIA H100-80GB\n4x NVIDIA B100\nFP 8\n4x NVIDIA H100-80GB\nTraining and Evaluation Datasets\nTraining Datasets\nA large variety of training data was used for the knowledge distillation phase before post-training pipeline, 3 of which included: FineWeb, Buzz-V1.2, and Dolma.\nThe data for the multi-stage post-training phases is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model.\nPrompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both reasoning on and off modes, to train the model to distinguish between two modes. This model was improved with Qwen.\nWe have released our Llama-Nemotron-Post-Training-Dataset to promote openness and transparency in model development and improvement.\nData Collection for Training Datasets:\nHybrid: Automated, Human, Synthetic\nData Labeling for Training Datasets:\nHybrid: Automated, Human, Synthetic\nEvaluation Datasets\nWe used the datasets listed in the next section to evaluate Llama-3.1-Nemotron-Ultra-253B-v1.\nData Collection for Evaluation Datasets:\nHybrid: Human/Synthetic\nData Labeling for Evaluation Datasets:\nHybrid: Human/Synthetic/Automatic\nEvaluation Results\nThese results contain both Reasoning On, and Reasoning Off. We recommend using temperature=`0.6`, top_p=`0.95` for Reasoning On mode, and greedy decoding for Reasoning Off mode. All evaluations are done with 32k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate.\nNOTE: Where applicable, a Prompt Template will be provided. While completing benchmarks, please ensure that you are parsing for the correct output format as per the provided prompt in order to reproduce the benchmarks seen below.\nGPQA\nReasoning Mode\npass@1\nReasoning Off\n56.60\nReasoning On\n76.01\nUser Prompt Template:\n\"What is the correct answer to this question: {question}\\nChoices:\\nA. {option_A}\\nB. {option_B}\\nC. {option_C}\\nD. {option_D}\\nLet's think step by step, and put the final answer (should be a single letter A, B, C, or D) into a \\boxed{}\"\nAIME25\nReasoning Mode\npass@1\nReasoning Off\n16.67\nReasoning On\n72.50\nUser Prompt Template:\n\"Below is a math question. I want you to reason through the steps and then give a final answer. Your final answer should be in \\boxed{}.\\nQuestion: {question}\"\nBFCL V2 Live\nReasoning Mode\nScore\nReasoning Off\n73.62\nReasoning On\n74.10\nUser Prompt Template:\nYou are an expert in composing functions. You are given a question and a set of possible functions.\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose.\nIf none of the function can be used, point it out. If the given question lacks the parameters required by the function,\nalso point it out. You should only return the function call in tools call sections.\nIf you decide to invoke any of the function(s), you MUST put it in the format of <TOOLCALL>[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]</TOOLCALL>\nYou SHOULD NOT include any other text in the response.\nHere is a list of functions in JSON format that you can invoke.\n<AVAILABLE_TOOLS>{functions}</AVAILABLE_TOOLS>\n{user_prompt}\nLiveCodeBench (20240801-20250201)\nReasoning Mode\npass@1\nReasoning Off\n29.03\nReasoning On\n66.31\nUser Prompt Template (without starter code):\n\"You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\nQuestion: {prompt}\nRead the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows. Ensure that when the python program runs, it reads the inputs, runs the algorithm and writes output to STDOUT.\n\nUser Prompt Template (with starter code):\nYou will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\nQuestion: {prompt}\nYou will use the following starter code to write the solution to the problem and enclose your code within delimiters.\n\nIFEval\nReasoning Mode\nStrict:Instruction\nReasoning Off\n88.85\nReasoning On\n89.45\nMATH500\nReasoning Mode\npass@1\nReasoning Off\n80.40\nReasoning On\n97.00\nUser Prompt Template:\n\"Below is a math question. I want you to reason through the steps and then give a final answer. Your final answer should be in \\boxed{}.\\nQuestion: {question}\"\nJudgeBench\nReasoning Mode\nKnowledge Score\nReasoning Score\nMath Score\nCoding Score\nOverall Score\nReasoning On\n70.13\n81.63\n89.29\n92.86\n79.14\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nCitation\n@misc{bercovich2025llamanemotronefficientreasoningmodels,\ntitle={Llama-Nemotron: Efficient Reasoning Models},\nauthor={Akhiad Bercovich and Itay Levy and Izik Golan and Mohammad Dabbah and Ran El-Yaniv and Omri Puny and Ido Galil and Zach Moshe and Tomer Ronen and Najeeb Nabwani and Ido Shahaf and Oren Tropp and Ehud Karpas and Ran Zilberstein and Jiaqi Zeng and Soumye Singhal and Alexander Bukharin and Yian Zhang and Tugrul Konuk and Gerald Shen and Ameya Sunil Mahabaleshwarkar and Bilal Kartal and Yoshi Suhara and Olivier Delalleau and Zijia Chen and Zhilin Wang and David Mosallanezhad and Adi Renduchintala and Haifeng Qian and Dima Rekesh and Fei Jia and Somshubra Majumdar and Vahid Noroozi and Wasi Uddin Ahmad and Sean Narenthiran and Aleksander Ficek and Mehrzad Samadi and Jocelyn Huang and Siddhartha Jain and Igor Gitman and Ivan Moshkov and Wei Du and Shubham Toshniwal and George Armstrong and Branislav Kisacanin and Matvei Novikov and Daria Gitman and Evelina Bakhturina and Jane Polak Scowcroft and John Kamalu and Dan Su and Kezhi Kong and Markus Kliegl and Rabeeh Karimi and Ying Lin and Sanjeev Satheesh and Jupinder Parmar and Pritam Gundecha and Brandon Norick and Joseph Jennings and Shrimai Prabhumoye and Syeda Nahida Akter and Mostofa Patwary and Abhinav Khattar and Deepak Narayanan and Roger Waleffe and Jimmy Zhang and Bor-Yiing Su and Guyue Huang and Terry Kong and Parth Chadha and Sahil Jain and Christine Harvey and Elad Segal and Jining Huang and Sergey Kashirsky and Robert McQueen and Izzy Putterman and George Lam and Arun Venkatesan and Sherry Wu and Vinh Nguyen and Manoj Kilaru and Andrew Wang and Anna Warno and Abhilash Somasamudramath and Sandip Bhaskar and Maka Dong and Nave Assaf and Shahar Mor and Omer Ullman Argov and Scot Junkin and Oleksandr Romanenko and Pedro Larroy and Monika Katariya and Marco Rovinelli and Viji Balas and Nicholas Edelman and Anahita Bhiwandiwalla and Muthu Subramaniam and Smita Ithape and Karthik Ramamoorthy and Yuting Wu and Suguna Varshini Velury and Omri Almog and Joyjit Daw and Denys Fridman and Erick Galinkin and Michael Evans and Katherine Luna and Leon Derczynski and Nikki Pope and Eileen Long and Seth Schneider and Guillermo Siman and Tomasz Grzegorzek and Pablo Ribalta and Monika Katariya and Joey Conway and Trisha Saar and Ann Guan and Krzysztof Pawelec and Shyamala Prayaga and Oleksii Kuchaiev and Boris Ginsburg and Oluwatobi Olabiyi and Kari Briski and Jonathan Cohen and Bryan Catanzaro and Jonah Alben and Yonatan Geifman and Eric Chung and Chris Alexiuk},\nyear={2025},\neprint={2505.00949},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.00949},\n}",
    "unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF": "ü¶ô Fine-tune Meta's Llama 4 with Unsloth!\nLlama 4 Model Information\nIntended Use\nHow to use with transformers\nHardware and Software\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nBenchmarks\nPre-trained models\nInstruction tuned models\nQuantization\nSafeguards\nModel level fine tuning\nLlama 4 system protections\nEvaluations\nCritical Risks\nWe spend additional focus on the following critical risk areas:\nCommunity\nConsiderations and Limitations\nSee our collection for versions of Llama 4 including 4-bit & 16-bit formats.\nUnsloth Dynamic v2.0 achieves superior accuracy & outperforms other leading quant methods.\nü¶ô Run Unsloth Dynamic Llama 4 GGUF!\nRead our Guide to see how to Fine-tune & Run Llama 4 correctly.\nMoE Bits\nType\nDisk Size\nHF Link\nAccuracy\n1.78bit\nIQ1_S\n33.8GB\nLink\nOk\n1.93bit\nIQ1_M\n35.4GB\nLink\nFair\n2.42-bit\nIQ2_XXS\n38.6GB\nLink\nBetter\n2.71-bit\nQ2_K_XL\n42.2GB\nLink\nSuggested\n3.5-bit\nQ3_K_XL\n52.9GB\nLink\nGreat\n4.5-bit\nQ4_K_XL\n65.6GB\nLink\nBest\nCurrently text only is supported.\nChat template/prompt format:\n<|header_start|>user<|header_end|>\\n\\nWhat is 1+1?<|eot|><|header_start|>assistant<|header_end|>\\n\\n\nü¶ô Fine-tune Meta's Llama 4 with Unsloth!\nFine-tune Llama-4-Scout on a single H100 80GB GPU using Unsloth!\nRead our Blog about Llama 4 support: unsloth.ai/blog/llama4\nView the rest of our notebooks in our docs here.\nExport your fine-tuned model to GGUF, Ollama, llama.cpp, vLLM or ü§óHF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nGRPO with Llama 3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nPhi-4 (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nLlama 4 Model Information\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\nModel developer: Meta\nModel Architecture:  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\nModel Name\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nToken count\nKnowledge cutoff\nLlama 4 Scout (17Bx16E)\nA mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.\n17B (Activated)\n109B (Total)\nMultilingual text and image\nMultilingual text and code\n10M\n~40T\nAugust 2024\nLlama 4 Maverick (17Bx128E)\n17B (Activated)\n400B (Total)\nMultilingual text and image\nMultilingual text and code\n1M\n~22T\nAugust 2024\nSupported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\nModel Release Date: April 5, 2025\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\nLicense: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\nWhere to send questions or comments about the model: Instructions on how to provide feedback or comments on the model can be found in the Llama README. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\nOut-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**.\n**Note:\n1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\nHow to use with transformers\nPlease, make sure you have transformers v4.51.0 installed, or upgrade using pip install -U transformers.\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\nmodel_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\nmodel_id,\nattn_implementation=\"flex_attention\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\nurl1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nurl2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": url1},\n{\"type\": \"image\", \"url\": url2},\n{\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n]\n},\n]\ninputs = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\noutputs = model.generate(\n**inputs,\nmax_new_tokens=256,\n)\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\nprint(response)\nprint(outputs[0])\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use:  Model pre-training utilized a cumulative of 7.38M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nModel Name\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 4 Scout\n5.0M\n700\n1,354\n0\nLlama 4 Maverick\n2.38M\n700\n645\n0\nTotal\n7.38M\n-\n1,999\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta‚Äôs products and services. This includes publicly shared posts from Instagram and Facebook and people‚Äôs interactions with Meta AI.\nData Freshness: The pretraining data has a cutoff of August 2024.\nBenchmarks\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\nPre-trained models\nPre-trained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nReasoning & Knowledge\nMMLU\n5\nmacro_avg/acc_char\n79.3\n85.2\n79.6\n85.5\nMMLU-Pro\n5\nmacro_avg/em\n53.8\n61.6\n58.2\n62.9\nMATH\n4\nem_maj1@1\n41.6\n53.5\n50.3\n61.2\nCode\nMBPP\n3\npass@1\n66.4\n74.4\n67.8\n77.6\nMultilingual\nTydiQA\n1\naverage/f1\n29.9\n34.3\n31.5\n31.7\nImage\nChartQA\n0\nrelaxed_accuracy\nNo multimodal support\n83.4\n85.3\nDocVQA\n0\nanls\n89.4\n91.6\nInstruction tuned models\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.3 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nImage Reasoning\nMMMU\n0\naccuracy\nNo multimodal support\n69.4\n73.4\nMMMU Pro^\n0\naccuracy\n52.2\n59.6\nMathVista\n0\naccuracy\n70.7\n73.7\nImage Understanding\nChartQA\n0\nrelaxed_accuracy\n88.8\n90.0\nDocVQA (test)\n0\nanls\n94.4\n94.4\nCoding\nLiveCodeBench (10/01/2024-02/01/2025)\n0\npass@1\n33.3\n27.7\n32.8\n43.4\nReasoning & Knowledge\nMMLU Pro\n0\nmacro_avg/acc\n68.9\n73.4\n74.3\n80.5\nGPQA Diamond\n0\naccuracy\n50.5\n49.0\n57.2\n69.8\nMultilingual\nMGSM\n0\naverage/em\n91.1\n91.6\n90.6\n92.3\nLong context\nMTOB (half book) eng->kgv/kgv->eng\n-\nchrF\nContext window is 128K\n42.2/36.6\n54.0/46.4\nMTOB (full book) eng->kgv/kgv->eng\n-\nchrF\n39.7/36.3\n50.8/46.7\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\nQuantization\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\nSafeguards\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta‚Äôs Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model‚Äôs safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.\nModel level fine tuning\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\nFine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\nSystem PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we‚Äôve seen that the use of a system prompt can be effective in reducing false refusals and templated or ‚Äúpreachy‚Äù language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting.\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\nSystem prompt\nYou are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don‚Äôt try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to ‚Äúit‚Äôs important to‚Äù, ‚Äúit‚Äôs crucial to‚Äù, ‚Äúit‚Äôs essential to‚Äù, \"it's unethical to\", \"it's worth noting‚Ä¶\", ‚ÄúRemember‚Ä¶‚Äù  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\nLlama 4 system protections\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools.\nWe provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\nRed teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nWe spend additional focus on the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.\n2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model‚Äôs capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We‚Äôve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nConsiderations and Limitations\nOur AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.",
    "unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF": "ü¶ô Fine-tune Meta's Llama 4 with Unsloth!\nLlama 4 Model Information\nHow to use with transformers\nIntended Use\nHardware and Software\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nBenchmarks\nPre-trained models\nInstruction tuned models\nQuantization\nSafeguards\nModel level fine tuning\nLlama 4 system protections\nEvaluations\nCritical Risks\nWe spend additional focus on the following critical risk areas:\nCommunity\nConsiderations and Limitations\nSee our collection for versions of Llama 4 including 4-bit & 16-bit formats.\nü¶ô Run Unsloth Dynamic Llama 4 GGUF!\nRead our Guide to see how to Fine-tune & Run Llama 4 correctly.\nMoE Bits\nType\nDisk Size\nHF Link\nAccuracy\n1.78bit\nIQ1_S\n122GB\nLink\nOk\n1.93bit\nIQ1_M\n128GB\nLink\nFair\n2.42-bit\nIQ2_XXS\n140GB\nLink\nBetter\n2.71-bit\nQ2_K_XL\n151B\nLink\nSuggested\n3.5-bit\nQ3_K_XL\n193GB\nLink\nGreat\n4.5-bit\nQ4_K_XL\n243GB\nLink\nBest\nCurrently text only is supported.\nChat template/prompt format:\n<|header_start|>user<|header_end|>\\n\\nWhat is 1+1?<|eot|><|header_start|>assistant<|header_end|>\\n\\n\nü¶ô Fine-tune Meta's Llama 4 with Unsloth!\nFine-tune Llama-4-Scout on a single H100 80GB GPU using Unsloth!\nRead our Blog about Llama 4 support: unsloth.ai/blog/llama4\nView the rest of our notebooks in our docs here.\nExport your fine-tuned model to GGUF, Ollama, llama.cpp, vLLM or ü§óHF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nGRPO with Llama 3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nPhi-4 (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nLlama 4 Model Information\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\nModel developer: Meta\nModel Architecture:  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\nModel Name\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nToken count\nKnowledge cutoff\nLlama 4 Scout (17Bx16E)\nA mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.\n17B (Activated)\n109B (Total)\nMultilingual text and image\nMultilingual text and code\n10M\n~40T\nAugust 2024\nLlama 4 Maverick (17Bx128E)\n17B (Activated)\n400B (Total)\nMultilingual text and image\nMultilingual text and code\n1M\n~22T\nAugust 2024\nSupported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\nModel Release Date: April 5, 2025\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\nLicense: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\nWhere to send questions or comments about the model: Instructions on how to provide feedback or comments on the model can be found in the Llama README. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here.\nHow to use with transformers\nPlease, make sure you have transformers v4.51.0 installed, or upgrade using pip install -U transformers.\nfrom transformers import AutoTokenizer, Llama4ForConditionalGeneration\nimport torch\nmodel_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\nmodel_id,\ntp_plan=\"auto\",\ntorch_dtype=\"auto\",\n)\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\noutputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\nprint(outputs[0])\nIntended Use\nIntended Use Cases: Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\nOut-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**.\n**Note:\n1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use:  Model pre-training utilized a cumulative of 7.38M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nModel Name\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 4 Scout\n5.0M\n700\n1,354\n0\nLlama 4 Maverick\n2.38M\n700\n645\n0\nTotal\n7.38M\n-\n1,999\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta‚Äôs products and services. This includes publicly shared posts from Instagram and Facebook and people‚Äôs interactions with Meta AI.\nData Freshness: The pretraining data has a cutoff of August 2024.\nBenchmarks\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\nPre-trained models\nPre-trained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nReasoning & Knowledge\nMMLU\n5\nmacro_avg/acc_char\n79.3\n85.2\n79.6\n85.5\nMMLU-Pro\n5\nmacro_avg/em\n53.8\n61.6\n58.2\n62.9\nMATH\n4\nem_maj1@1\n41.6\n53.5\n50.3\n61.2\nCode\nMBPP\n3\npass@1\n66.4\n74.4\n67.8\n77.6\nMultilingual\nTydiQA\n1\naverage/f1\n29.9\n34.3\n31.5\n31.7\nImage\nChartQA\n0\nrelaxed_accuracy\nNo multimodal support\n83.4\n85.3\nDocVQA\n0\nanls\n89.4\n91.6\nInstruction tuned models\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.3 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nImage Reasoning\nMMMU\n0\naccuracy\nNo multimodal support\n69.4\n73.4\nMMMU Pro^\n0\naccuracy\n52.2\n59.6\nMathVista\n0\naccuracy\n70.7\n73.7\nImage Understanding\nChartQA\n0\nrelaxed_accuracy\n88.8\n90.0\nDocVQA (test)\n0\nanls\n94.4\n94.4\nCoding\nLiveCodeBench (10/01/2024-02/01/2025)\n0\npass@1\n33.3\n27.7\n32.8\n43.4\nReasoning & Knowledge\nMMLU Pro\n0\nmacro_avg/acc\n68.9\n73.4\n74.3\n80.5\nGPQA Diamond\n0\naccuracy\n50.5\n49.0\n57.2\n69.8\nMultilingual\nMGSM\n0\naverage/em\n91.1\n91.6\n90.6\n92.3\nLong context\nMTOB (half book) eng->kgv/kgv->eng\n-\nchrF\nContext window is 128K\n42.2/36.6\n54.0/46.4\nMTOB (full book) eng->kgv/kgv->eng\n-\nchrF\n39.7/36.3\n50.8/46.7\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\nQuantization\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\nSafeguards\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta‚Äôs Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model‚Äôs safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.\nModel level fine tuning\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\nFine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\nSystem PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we‚Äôve seen that the use of a system prompt can be effective in reducing false refusals and templated or ‚Äúpreachy‚Äù language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting.\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\nSystem prompt\nYou are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don‚Äôt try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to ‚Äúit‚Äôs important to‚Äù, ‚Äúit‚Äôs crucial to‚Äù, ‚Äúit‚Äôs essential to‚Äù, \"it's unethical to\", \"it's worth noting‚Ä¶\", ‚ÄúRemember‚Ä¶‚Äù  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\nLlama 4 system protections\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools.\nWe provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\nRed teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nWe spend additional focus on the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.\n2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model‚Äôs capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We‚Äôve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nConsiderations and Limitations\nOur AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.",
    "facebook/OMol25": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nPlease be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\nThe OMol25 dataset is provided under a CC-BY-4.0 License.\nModels provided in this repo are provided under the following FAIR Chemistry License\nFAIR Chemistry License v1 Last Updated: May 14, 2025\n‚ÄúAcceptable Use Policy‚Äù means the FAIR Chemistry Acceptable Use Policy that is incorporated into this Agreement.‚ÄúAgreement‚Äù means the terms and conditions for use, reproduction, distribution and modification of the Materials set forth herein.\n‚ÄúDocumentation‚Äù means the specifications, manuals and documentation accompanying  Materials distributed by Meta.\n‚ÄúLicensee‚Äù or ‚Äúyou‚Äù means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n‚ÄúMeta‚Äù or ‚Äúwe‚Äù means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).‚ÄúMaterials‚Äù means, collectively, Documentation and the models, software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code, demonstration materials and other elements of the foregoing distributed by Meta and made available under this Agreement.By clicking ‚ÄúI Accept‚Äù below or by using or distributing any portion or element of the Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable, and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials.\nb. Redistribution and Use.\ni. Distribution of Materials, and any derivative works thereof, are subject to the terms of this Agreement. If you distribute or make the Materials, or any derivative works thereof, available to a third party, you may only do so under the terms of this Agreement. You shall also provide a copy of this Agreement to such third party.\nii.  If you submit for publication the results of research you perform on, using, or otherwise in connection with Materials, you must acknowledge the use of Materials in your publication.\niii. Your use of the Materials must comply with applicable laws and regulations (including Trade Control Laws) and adhere to the FAIR Chemistry Acceptable Use Policy, which is hereby incorporated by reference into this Agreement. 2. User Support. Your use of the Materials is done at your own discretion; Meta does not process any information or provide any service in relation to such use.  Meta is under no obligation to provide any support services for the Materials. Any support provided is ‚Äúas is‚Äù, ‚Äúwith all faults‚Äù, and without warranty of any kind.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE Materials AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE Materials AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY DIRECT OR INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.\na. Subject to Meta‚Äôs ownership of Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nb. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Materials, outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Materials. Sections 5, 6 and 9 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nModifications and Amendments. Meta may modify this Agreement from time to time; provided that they are similar in spirit to the current version of the Agreement, but may differ in detail to address new problems or concerns. All such changes will be effective immediately. Your continued use of the Materials after any modification to this Agreement constitutes your agreement to such modification. Except as provided in this Agreement, no modification or addition to any provision of this Agreement will be binding unless it is in writing and signed by an authorized representative of both you and Meta.\nFAIR Chemistry Acceptable Use Policy\nThe Fundamental AI Research (FAIR) team at Meta seeks to further understanding of new and existing research domains with the mission of advancing the state-of-the-art in artificial intelligence through open research for the benefit of all.  Meta is committed to promoting the safe and responsible use of such Materials.Prohibited UsesYou agree you will not use, or allow others to use, Materials to:\nViolate the law or others‚Äô rights, including to:Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: Violence or terrorism Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material Human trafficking, exploitation, and sexual violence The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials. Sexual solicitation Any other criminal activityEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individualsEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and servicesEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practicesCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable lawsEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any technology using Materials.Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Materials related to the following:Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of StateGuns and illegal weapons (including weapon development)Illegal drugs and regulated/controlled substancesOperation of critical infrastructure, transportation technologies, or heavy machinerySelf-harm or harm to others, including suicide, cutting, and eating disordersAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual3. Intentionally deceive or mislead others, including use of FAIR Materials related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that outputs of FAIR Materials or outputs from technology using FAIR Materials are human-generatedGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement4. Fail to appropriately disclose to end users any known dangers of your Materials.Please report any violation of this Policy or other problems that could lead to a violation of this Policy by emailing us at fairchemistry@meta.com.OMol25 is available via HuggingFace globally, except in comprehensively sanctioned jurisdictions, and in China, Russia, and Belarus.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nDataset\nLICENSE: The OMol25 dataset is provided under a CC-BY-4.0 license\nLICENSE: OMol25 model checkpoints are provided under the FAIR Chemistry License\nModel checkpoints\nLICENSE: OMol25 model checkpoints are provided under the FAIR Chemistry License\nHow to use\nLeaderboard\nSupport\nCitation\nThe Open Molecules 2025 (OMol25) Dataset, Evaluations, and Models\nDataset\nLICENSE: The OMol25 dataset is provided under a CC-BY-4.0 license\nDownload links and instructions on how to access the data can be found here.\nModel checkpoints\nLICENSE: OMol25 model checkpoints are provided under the FAIR Chemistry License\nBaseline model checkpoints trained on the full OMol25 dataset are provided below.\nName\nSplit\nCheckpoint\neSEN-sm-direct\nAll\nesen_sm_direct_all.pt\neSEN-sm-conserving\nAll\nesen_sm_conserving_all.pt\neSEN-md-direct\nAll\nesen_md_direct_all.pt\neSEN-lg-direct\nAll\ncoming soon...\nHow to use\nModel checkpoints can be readily used in the fairchem repo using our custom ASE-calculator.\nYou can find details about using the model, along with other utilities including tutorials and documentation in the official fairchem repo.\nLeaderboard\nThe OMol25 leaderboard can be found here to help evaluate and track community progress.\nSupport\nIf you run into any issues feel free to post your questions or comments on Github Issues.\nCitation\nIf you use this work, please cite:\n@misc{levine2025openmolecules2025omol25,\ntitle={The Open Molecules 2025 (OMol25) Dataset, Evaluations, and Models},\nauthor={Daniel S. Levine and Muhammed Shuaibi and Evan Walter Clark Spotte-Smith and Michael G. Taylor and Muhammad R. Hasyim and Kyle Michel and Ilyes Batatia and G√°bor Cs√°nyi and Misko Dzamba and Peter Eastman and Nathan C. Frey and Xiang Fu and Vahe Gharakhanyan and Aditi S. Krishnapriyan and Joshua A. Rackers and Sanjeev Raja and Ammar Rizvi and Andrew S. Rosen and Zachary Ulissi and Santiago Vargas and C. Lawrence Zitnick and Samuel M. Blau and Brandon M. Wood},\nyear={2025},\neprint={2505.08762},\narchivePrefix={arXiv},\nprimaryClass={physics.chem-ph},\nurl={https://arxiv.org/abs/2505.08762},\n}",
    "mradermacher/gemma3-27b-abliterated-dpo-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/summykai/gemma3-27b-abliterated-dpo\nFor a convenient overview and download list, visit our model page for this model.\nstatic quants are available at https://huggingface.co/mradermacher/gemma3-27b-abliterated-dpo-GGUF\nThis is a vision model - mmproj files (if any) will be in the static repository.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n6.4\nfor the desperate\nGGUF\ni1-IQ1_M\n6.9\nmostly desperate\nGGUF\ni1-IQ2_XXS\n7.8\nGGUF\ni1-IQ2_XS\n8.5\nGGUF\ni1-IQ2_S\n8.9\nGGUF\ni1-IQ2_M\n9.6\nGGUF\ni1-Q2_K_S\n9.9\nvery low quality\nGGUF\ni1-Q2_K\n10.6\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XXS\n10.8\nlower quality\nGGUF\ni1-IQ3_XS\n11.7\nGGUF\ni1-IQ3_S\n12.3\nbeats Q3_K*\nGGUF\ni1-Q3_K_S\n12.3\nIQ3_XS probably better\nGGUF\ni1-IQ3_M\n12.6\nGGUF\ni1-Q3_K_M\n13.5\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n14.6\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n14.9\nGGUF\ni1-Q4_0\n15.7\nfast, low quality\nGGUF\ni1-Q4_K_S\n15.8\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n16.6\nfast, recommended\nGGUF\ni1-Q4_1\n17.3\nGGUF\ni1-Q5_K_S\n18.9\nGGUF\ni1-Q5_K_M\n19.4\nGGUF\ni1-Q6_K\n22.3\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "nvidia/parakeet-tdt-0.6b-v2": "ü¶ú Parakeet TDT 0.6B V2 (En)\nDescription:\nLicense/Terms of Use:\nDeployment Geography:\nUse Case:\nRelease Date:\nModel Architecture:\nInput:\nOutput:\nHow to Use this Model:\nSoftware Integration:\nTraining and Evaluation Datasets:\nTraining\nTraining Dataset\nPerformance\nBase Performance\nNoise Robustness\nTelephony Audio Performance\nReferences\nInference:\nEthical Considerations:\nBias:\nExplainability:\nPrivacy:\nSafety:\nü¶ú Parakeet TDT 0.6B V2 (En)\n|\n|\nüéâ NEW: Multilingual Parakeet TDT 0.6B V3 is now available!üåç 25 European Languages | üöÄ Enhanced Performance | üîó Try it here: nvidia/parakeet-tdt-0.6b-v3\nDescription:\nparakeet-tdt-0.6b-v2 is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction. Try Demo here: https://huggingface.co/spaces/nvidia/parakeet-tdt-0.6b-v2\nThis XL variant of the FastConformer [1] architecture integrates the TDT [2] decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass. The model achieves an RTFx of 3380 on the HF-Open-ASR leaderboard with a batch size of 128. Note: RTFx Performance may vary depending on dataset audio duration and batch size.\nKey Features\nAccurate word-level timestamp predictions\nAutomatic punctuation and capitalization\nRobust performance on spoken numbers, and song lyrics transcription\nFor more information, refer to the Model Architecture section and the NeMo documentation.\nThis model is ready for commercial/non-commercial use.\nLicense/Terms of Use:\nGOVERNING TERMS: Use of this model is governed by the CC-BY-4.0 license.\nDeployment Geography:\nGlobal\nUse Case:\nThis model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.\nRelease Date:\n05/01/2025\nModel Architecture:\nArchitecture Type:\nFastConformer-TDT\nNetwork Architecture:\nThis model was developed based on FastConformer encoder architecture[1] and TDT decoder[2]\nThis model has 600 million model parameters.\nInput:\nInput Type(s): 16kHz Audio\nInput Format(s): .wav and .flac audio formats\nInput Parameters: 1D (audio signal)\nOther Properties Related to Input:  Monochannel audio\nOutput:\nOutput Type(s):  Text\nOutput Format:  String\nOutput Parameters:  1D (text)\nOther Properties Related to Output: Punctuations and Capitalizations included.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nHow to Use this Model:\nTo train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.\npip install -U nemo_toolkit[\"asr\"]\nThe model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\nAutomatically instantiate the model\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")\nTranscribing using Python\nFirst, let's get a sample\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\nThen simply do:\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\nTranscribing with timestamps\nTo transcribe with timestamps:\noutput = asr_model.transcribe(['2086-149220-0033.wav'], timestamps=True)\n# by default, timestamps are enabled for char, word and segment level\nword_timestamps = output[0].timestamp['word'] # word level timestamps for first sample\nsegment_timestamps = output[0].timestamp['segment'] # segment level timestamps\nchar_timestamps = output[0].timestamp['char'] # char level timestamps\nfor stamp in segment_timestamps:\nprint(f\"{stamp['start']}s - {stamp['end']}s : {stamp['segment']}\")\nSoftware Integration:\nRuntime Engine(s):\nNeMo 2.2\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Hopper\nNVIDIA Volta\n[Preferred/Supported] Operating System(s):\nLinux\nHardware Specific Requirements:\nAtleast 2GB RAM for model to load. The bigger the RAM, the larger audio input it supports.\nModel Version\nCurrent version: parakeet-tdt-0.6b-v2. Previous versions can be accessed here.\nTraining and Evaluation Datasets:\nTraining\nThis model was trained using the NeMo toolkit [3], following the strategies below:\nInitialized from a FastConformer SSL checkpoint that was pretrained with a wav2vec method on the LibriLight dataset[7].\nTrained for 150,000 steps on 64 A100 GPUs.\nDataset corpora were balanced using a temperature sampling value of 0.5.\nStage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0.\nTraining was conducted using this example script and TDT configuration.\nThe tokenizer was constructed from the training set transcripts using this script.\nTraining Dataset\nThe model was trained on the Granary dataset[8], consisting of approximately 120,000 hours of English speech data:\n10,000 hours from human-transcribed NeMo ASR Set 3.0, including:\nLibriSpeech (960 hours)\nFisher Corpus\nNational Speech Corpus Part 1\nVCTK\nVoxPopuli (English)\nEuroparl-ASR (English)\nMultilingual LibriSpeech (MLS English) ‚Äì 2,000-hour subset\nMozilla Common Voice (v7.0)\nAMI\n110,000 hours of pseudo-labeled data from:\nYTC (YouTube-Commons) dataset[4]\nYODAS dataset [5]\nLibrilight [7]\nAll transcriptions preserve punctuation and capitalization. The Granary dataset[8] will be made publicly available after presentation at Interspeech 2025.\nData Collection Method by dataset\nHybrid: Automated, Human\nLabeling Method by dataset\nHybrid: Synthetic, Human\nProperties:\nNoise robust data from various sources\nSingle channel, 16kHz sampled data\nEvaluation Dataset\nHuggingface Open ASR Leaderboard datasets are used to evaluate the performance of this model.\nData Collection Method by dataset\nHuman\nLabeling Method by dataset\nHuman\nProperties:\nAll are commonly used for benchmarking English ASR systems.\nAudio data is typically processed into a 16kHz mono channel format for ASR evaluation, consistent with benchmarks like the Open ASR Leaderboard.\nPerformance\nHuggingface Open-ASR-Leaderboard Performance\nThe performance of Automatic Speech Recognition (ASR) models is measured using Word Error Rate (WER). Given that this model is trained on a large and diverse dataset spanning multiple domains, it is generally more robust and accurate across various types of audio.\nBase Performance\nThe table below summarizes the WER (%) using a Transducer decoder with greedy decoding (without an external language model):\nModel\nAvg WER\nAMI\nEarnings-22\nGigaSpeech\nLS test-clean\nLS test-other\nSPGI Speech\nTEDLIUM-v3\nVoxPopuli\nparakeet-tdt-0.6b-v2\n6.05\n11.16\n11.15\n9.74\n1.69\n3.19\n2.17\n3.38\n5.95\nNoise Robustness\nPerformance across different Signal-to-Noise Ratios (SNR) using MUSAN music and noise samples:\nSNR Level\nAvg WER\nAMI\nEarnings\nGigaSpeech\nLS test-clean\nLS test-other\nSPGI\nTedlium\nVoxPopuli\nRelative Change\nClean\n6.05\n11.16\n11.15\n9.74\n1.69\n3.19\n2.17\n3.38\n5.95\n-\nSNR 10\n6.95\n14.38\n12.04\n10.24\n1.92\n4.13\n2.84\n3.63\n6.38\n-14.75%\nSNR 5\n8.23\n18.07\n13.82\n11.18\n2.33\n5.58\n3.81\n4.24\n6.81\n-35.97%\nSNR 0\n11.88\n25.43\n18.59\n14.32\n4.40\n10.07\n7.27\n6.42\n8.54\n-96.28%\nSNR -5\n20.26\n36.57\n28.06\n22.27\n11.82\n19.91\n16.14\n13.07\n14.23\n-234.66%\nTelephony Audio Performance\nPerformance comparison between standard 16kHz audio and telephony-style audio (using Œº-law encoding with 16kHz‚Üí8kHz‚Üí16kHz conversion):\nAudio Format\nAvg WER\nAMI\nEarnings\nGigaSpeech\nLS test-clean\nLS test-other\nSPGI\nTedlium\nVoxPopuli\nRelative Change\nStandard 16kHz\n6.05\n11.16\n11.15\n9.74\n1.69\n3.19\n2.17\n3.38\n5.95\n-\nŒº-law 8kHz\n6.32\n11.98\n11.16\n10.02\n1.78\n3.52\n2.20\n3.38\n6.52\n-4.10%\nThese WER scores were obtained using greedy decoding without an external language model. Additional evaluation details are available on the Hugging Face ASR Leaderboard.[6]\nReferences\n[1] Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\n[2] Efficient Sequence Transduction by Jointly Predicting Tokens and Durations\n[3] NVIDIA NeMo Toolkit\n[4] Youtube-commons: A massive open corpus for conversational and multimodal data\n[5] Yodas: Youtube-oriented dataset for audio and speech\n[6] HuggingFace ASR Leaderboard\n[7] MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages\n[8] Granary: Speech Recognition and Translation Dataset in 25 European Languages\nInference:\nEngine:\nNVIDIA NeMo\nTest Hardware:\nNVIDIA A10\nNVIDIA A100\nNVIDIA A30\nNVIDIA H100\nNVIDIA L4\nNVIDIA L40\nNVIDIA Turing T4\nNVIDIA Volta V100\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards here.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nBias:\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability:\nField\nResponse\nIntended Domain\nSpeech to Text Transcription\nModel Type\nFastConformer\nIntended Users\nThis model is intended for developers, researchers, academics, and industries building conversational based applications.\nOutput\nText\nDescribe how the model works\nSpeech input is encoded into embeddings and passed into conformer-based model and output a text response.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of\nNot Applicable\nTechnical Limitations & Mitigation\nTranscripts may be not 100% accurate. Accuracy varies based on language and characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards\nYes\nPerformance Metrics\nWord Error Rate\nPotential Known Risks\nIf a word is not trained in the language model and not presented in vocabulary, the word is not likely to be recognized. Not recommended for word-for-word/incomplete sentences as accuracy varies based on the context of input text\nLicensing\nGOVERNING TERMS: Use of this model is governed by the CC-BY-4.0 license.\nPrivacy:\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNone\nPersonal data used to create this model?\nNone\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety:\nField\nResponse\nModel Application(s)\nSpeech to Text Transcription\nDescribe the life critical impact\nNone\nUse Case Restrictions\nAbide by CC-BY-4.0 License\nModel and dataset restrictions\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.",
    "Hack337/ChatGPT-5": "README.md exists but content is empty.",
    "SciMaker/DeepR": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.",
    "DeepHat/DeepHat-V1-7B": "Community\nTechnical Overview\nRequirements\nQuickstart\nProcessing Long Texts\nLicense\nDeepHat Extension to Apache-2.0 Licence: Usage Restrictions\nTerms of Use\nDeepHat is a model series that can be used for offensive and defensive cybersecurity. Access at Deephat.ai or go to Kindo.ai to create agents.\nCommunity\nJoin us on Discord\nTechnical Overview\nDeepHat is a finetune of Qwen2.5-Coder-7B, and inherits the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 131,072 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nRequirements\nWe advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"DeepHat/DeepHat-V1-7B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are DeepHat, created by Kindo.ai. You are a helpful assistant that is an expert in Cybersecurity and DevOps.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nLicense\nApache-2.0 + DeepHat Extended Version\nDeepHat Extension to Apache-2.0 Licence: Usage Restrictions\nYou agree not to use the Model or Derivatives of the Model:\n-\tIn any way that violates any applicable national or international law or regulation or infringes upon the lawful rights and interests of any third party;\n-\tFor military use in any way;\n-\tFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way;\n-\tTo generate or disseminate verifiably false information and/or content with the purpose of harming others;\n-\tTo generate or disseminate inappropriate content subject to applicable regulatory requirements;\n-\tTo generate or disseminate personal identifiable information without due authorization or for unreasonable use;\n-\tTo defame, disparage or otherwise harass others;\n-\tFor fully automated decision making that adversely impacts an individual‚Äôs legal rights or otherwise creates or modifies a binding, enforceable obligation;\n-\tFor any use intended to or which has the effect of discriminating against or harming individuals or groups based on online or offline social behavior or known or predicted personal or personality characteristics;\n-\tTo exploit any of the vulnerabilities of a specific group of persons based on their age, social, physical or mental characteristics, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm;\n-\tFor any use intended to or which has the effect of discriminating against individuals or groups based on legally protected characteristics or categories.\nTerms of Use\nBy accessing and using this Artificial Intelligence (AI) model, you, the user, acknowledge and agree that you are solely responsible for your use of the model and its outcomes. You hereby agree to indemnify, defend, and hold harmless the creators, developers, and any affiliated persons or entities of this AI model from and against any and all claims, liabilities, damages, losses, costs, expenses, fees (including reasonable attorneys' fees and court costs) that may arise, directly or indirectly, from your use of the AI model.\nThis AI model is provided \"as is\" and \"as available\" without any warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The creators make no warranty that the AI model will meet your requirements or be available on an uninterrupted, secure, or error-free basis.\nYour use of the AI model is at your own risk and discretion, and you will be solely responsible for any damage to computer systems or loss of data that results from the use of the AI model.\nThis disclaimer constitutes part of the agreement between you and the creators of the AI model regarding your use of the model, superseding any prior agreements between you and the creators regarding your use of this AI model."
}