{
    "meta-llama/Llama-2-7b-chat-hf": "You need to share contact information with Meta to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 2 COMMUNITY LICENSE AGREEMENT\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and  modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity's behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or  entity if you are  entering in this Agreement on their behalf.\"Llama 2\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other  elements of the foregoing distributed by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non- transferable and royalty-free limited license under Meta's intellectual property or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,  distribute, copy, create derivative works of, and make modifications to the Llama  Materials.b. Redistribution and Use.i. If you distribute or make the Llama Materials, or any derivative works  thereof, available to a third party, you shall provide a copy of this Agreement to such  third party.ii.  If you receive Llama Materials, or any derivative works thereof, from  a Licensee as part of an integrated end user product, then Section 2 of this  Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you  distribute the following attribution notice within a \"Notice\" text file distributed as a  part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community License,  Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws  and regulations (including trade compliance laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated by reference into  this Agreement.v. You will not use the Llama Materials or any output or results of the  Llama Materials to improve any other large language model (excluding Llama 2 or  derivative works thereof).\nAdditional Commercial Terms. If, on the Llama 2 version release date, the  monthly active users of the products or services made available by or for Licensee,  or Licensee's affiliates, is greater than 700 million monthly active users in the  preceding calendar month, you must request a license from Meta, which Meta may  grant to you in its sole discretion, and you are not authorized to exercise any of the  rights under this Agreement unless or until Meta otherwise expressly grants you  such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in  connection with the Llama Materials, neither Meta nor Licensee may use any name  or mark owned by or associated with the other or any of its affiliates, except as  required for reasonable and customary use in describing and redistributing the  Llama Materials.b. Subject to Meta's ownership of Llama Materials and derivatives made by or  for Meta, with respect to any derivative works and modifications of the Llama  Materials that are made by you, as between you and Meta, you are and will be the  owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity  (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs or results, or any portion of any of the foregoing,  constitutes infringement of intellectual property or other rights owned or licensable  by you, then any licenses granted to you under this Agreement shall terminate as of  the date such litigation or claim is filed or instituted. You will indemnify and hold  harmless Meta from and against any claim by any third party arising out of or related  to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your  acceptance of this Agreement or access to the Llama Materials and will continue in  full force and effect until terminated in accordance with the terms and conditions  herein. Meta may terminate this Agreement if you are in breach of any term or  condition of this Agreement. Upon termination of this Agreement, you shall delete  and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and  construed under the laws of the State of California without regard to choice of law  principles, and the UN Convention on Contracts for the International Sale of Goods  does not apply to this Agreement. The courts of California shall have exclusive  jurisdiction of any dispute arising out of this Agreement.\nLlama 2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at ai.meta.com/llama/use-policy.\nProhibited Uses\nWe want everyone to use Llama 2 safely and responsibly. You agree you will not use, or allow others to use, Llama 2 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: github.com/facebookresearch/llama\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nLlama 2\nModel Details\nIntended Use\nHardware and Software\nTraining Data\nEvaluation Results\nEthical Considerations and Limitations\nReporting Issues\nLlama Model Index\nLlama 2\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\nModel Details\nNote: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\nModel Developers Meta\nVariations Llama 2 comes in a range of parameter sizes ‚Äî 7B, 13B, and 70B ‚Äî as well as pretrained and fine-tuned variations.\nInput Models input text only.\nOutput Models generate text only.\nModel Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\nTraining Data\nParams\nContent Length\nGQA\nTokens\nLR\nLlama 2\nA new mix of publicly available online data\n7B\n4k\n‚úó\n2.0T\n3.0 x 10-4\nLlama 2\nA new mix of publicly available online data\n13B\n4k\n‚úó\n2.0T\n3.0 x 10-4\nLlama 2\nA new mix of publicly available online data\n70B\n4k\n‚úî\n2.0T\n1.5 x 10-4\nLlama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Dates Llama 2 was trained between January 2023 and July 2023.\nStatus This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\nResearch Paper \"Llama-2: Open Foundation and Fine-tuned Chat Models\"\nIntended Use\nIntended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). See our reference code in github for details: chat_completion.\nOut-of-scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\nHardware and Software\nTraining Factors We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\nTime (GPU hours)\nPower Consumption (W)\nCarbon Emitted(tCO2eq)\nLlama 2 7B\n184320\n400\n31.22\nLlama 2 13B\n368640\n400\n62.44\nLlama 2 70B\n1720320\n400\n291.42\nTotal\n3311616\n539.00\nCO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\nTraining Data\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\nData Freshness The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\nEvaluation Results\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\nModel\nSize\nCode\nCommonsense Reasoning\nWorld Knowledge\nReading Comprehension\nMath\nMMLU\nBBH\nAGI Eval\nLlama 1\n7B\n14.1\n60.8\n46.2\n58.5\n6.95\n35.1\n30.3\n23.9\nLlama 1\n13B\n18.9\n66.1\n52.6\n62.3\n10.9\n46.9\n37.0\n33.9\nLlama 1\n33B\n26.0\n70.0\n58.4\n67.6\n21.4\n57.8\n39.8\n41.7\nLlama 1\n65B\n30.7\n70.7\n60.5\n68.6\n30.8\n63.4\n43.5\n47.6\nLlama 2\n7B\n16.8\n63.9\n48.9\n61.3\n14.6\n45.3\n32.6\n29.3\nLlama 2\n13B\n24.5\n66.9\n55.4\n65.8\n28.7\n54.8\n39.4\n39.1\nLlama 2\n70B\n37.5\n71.9\n63.6\n69.4\n35.2\n68.9\n51.2\n54.2\nOverall performance on grouped academic benchmarks. Code: We report the average pass@1 scores of our models on HumanEval and MBPP. Commonsense Reasoning: We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. World Knowledge: We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. Reading Comprehension: For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. MATH: We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\nTruthfulQA\nToxigen\nLlama 1\n7B\n27.42\n23.00\nLlama 1\n13B\n41.74\n23.08\nLlama 1\n33B\n44.19\n22.57\nLlama 1\n65B\n48.71\n21.77\nLlama 2\n7B\n33.29\n21.25\nLlama 2\n13B\n41.86\n26.10\nLlama 2\n70B\n50.18\n24.60\nEvaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\nTruthfulQA\nToxigen\nLlama-2-Chat\n7B\n57.04\n0.00\nLlama-2-Chat\n13B\n62.18\n0.00\nLlama-2-Chat\n70B\n64.14\n0.01\nEvaluation of fine-tuned LLMs on different safety datasets. Same metric definitions as above.\nEthical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\nPlease see the Responsible Use Guide available at https://ai.meta.com/llama/responsible-use-guide/\nReporting Issues\nPlease report any software ‚Äúbug,‚Äù or other problems with the models through one of the following means:\nReporting issues with the model: github.com/facebookresearch/llama\nReporting problematic content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nLlama Model Index\nModel\nLlama2\nLlama2-hf\nLlama2-chat\nLlama2-chat-hf\n7B\nLink\nLink\nLink\nLink\n13B\nLink\nLink\nLink\nLink\n70B\nLink\nLink\nLink\nLink",
    "coqui/XTTS-v2": "‚ìçTTS\nFeatures\nUpdates over XTTS-v1\nLanguages\nCode\nDemo Spaces\nLicense\nContact\n‚ìçTTS\n‚ìçTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. There is no need for an excessive amount of training data that spans countless hours.\nThis is the same or similar model to what powers Coqui Studio and Coqui API.\nFeatures\nSupports 17 languages.\nVoice cloning with just a 6-second audio clip.\nEmotion and style transfer by cloning.\nCross-language voice cloning.\nMulti-lingual speech generation.\n24khz sampling rate.\nUpdates over XTTS-v1\n2 new languages; Hungarian and Korean\nArchitectural improvements for speaker conditioning.\nEnables the use of multiple speaker references and interpolation between speakers.\nStability improvements.\nBetter prosody and audio quality across the board.\nLanguages\nXTTS-v2 supports 17 languages: English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt),\nPolish (pl), Turkish (tr), Russian (ru), Dutch (nl), Czech (cs), Arabic (ar), Chinese (zh-cn), Japanese (ja), Hungarian (hu), Korean (ko)\nHindi (hi).\nStay tuned as we continue to add support for more languages. If you have any language requests, feel free to reach out!\nCode\nThe code-base supports inference and fine-tuning.\nDemo Spaces\nXTTS Space  :  You can see how model performs on supported languages, and try with your own reference or microphone input\nXTTS Voice Chat with Mistral or Zephyr : You can experience streaming voice chat with Mistral 7B Instruct or Zephyr 7B Beta\nüê∏üí¨ CoquiTTS\ncoqui/TTS on Github\nüíº Documentation\nReadTheDocs\nüë©‚Äçüíª Questions\nGitHub Discussions\nüóØ Community\nDiscord\nLicense\nThis model is licensed under Coqui Public Model License. There's a lot that goes into a license for generative models, and you can read more of the origin story of CPML here.\nContact\nCome and join in our üê∏Community. We're active on Discord and Twitter.\nYou can also mail us at info@coqui.ai.\nUsing üê∏TTS API:\nfrom TTS.api import TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n# generate speech by cloning a voice using default settings\ntts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\nfile_path=\"output.wav\",\nspeaker_wav=\"/path/to/target/speaker.wav\",\nlanguage=\"en\")\nUsing üê∏TTS Command line:\ntts --model_name tts_models/multilingual/multi-dataset/xtts_v2 \\\n--text \"Bug√ºn okula gitmek istemiyorum.\" \\\n--speaker_wav /path/to/target/speaker.wav \\\n--language_idx tr \\\n--use_cuda true\nUsing the model directly:\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\nconfig = XttsConfig()\nconfig.load_json(\"/path/to/xtts/config.json\")\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(config, checkpoint_dir=\"/path/to/xtts/\", eval=True)\nmodel.cuda()\noutputs = model.synthesize(\n\"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\nconfig,\nspeaker_wav=\"/data/TTS-public/_refclips/3.wav\",\ngpt_cond_len=3,\nlanguage=\"en\",\n)",
    "meta-llama/Llama-3.1-8B": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3_1/use-policy\nProhibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.1 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.1 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nUse with llama\nHardware and Software\nTraining Data\nBenchmark scores\nBase pretrained models\nInstruction tuned models\nMultilingual benchmarks\nResponsibility & Safety\nResponsible deployment\nLlama 3.1 instruct\nLlama 3.1 systems\nNew capabilities\nEvaluations\nCritical and other risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.1 (text only)\nA new mix of publicly available online data.\n8B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n405B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: July 23, 2024.\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Meta's Llama-3.1-8B, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Llama-3.1-8B\"\npipeline = transformers.pipeline(\n\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\npipeline(\"Hey how are you doing today?\")\nUse with llama\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Llama-3.1-8B --include \"original/*\" --local-dir Llama-3.1-8B\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions\n(tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions\n(tons CO2eq)\nLlama 3.1 8B\n1.46M\n700\n420\n0\nLlama 3.1 70B\n7.0M\n700\n2,040\n0\nLlama 3.1 405B\n30.84M\n700\n8,930\n0\nTotal\n39.3M\n11,390\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmark scores\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\nBase pretrained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B\nLlama 3.1 8B\nLlama 3 70B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n66.7\n66.7\n79.5\n79.3\n85.2\nMMLU-Pro (CoT)\n5\nmacro_avg/acc_char\n36.2\n37.1\n55.0\n53.8\n61.6\nAGIEval English\n3-5\naverage/acc_char\n47.1\n47.8\n63.0\n64.6\n71.6\nCommonSenseQA\n7\nacc_char\n72.6\n75.0\n83.8\n84.1\n85.8\nWinogrande\n5\nacc_char\n-\n60.5\n-\n83.3\n86.7\nBIG-Bench Hard (CoT)\n3\naverage/em\n61.1\n64.2\n81.3\n81.6\n85.9\nARC-Challenge\n25\nacc_char\n79.4\n79.7\n93.1\n92.9\n96.1\nKnowledge reasoning\nTriviaQA-Wiki\n5\nem\n78.5\n77.6\n89.7\n89.8\n91.8\nReading comprehension\nSQuAD\n1\nem\n76.4\n77.0\n85.6\n81.8\n89.3\nQuAC (F1)\n1\nf1\n44.4\n44.9\n51.1\n51.1\n53.6\nBoolQ\n0\nacc_char\n75.7\n75.0\n79.0\n79.4\n80.0\nDROP (F1)\n3\nf1\n58.4\n59.5\n79.7\n79.6\n84.8\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B Instruct\nLlama 3.1 8B Instruct\nLlama 3 70B Instruct\nLlama 3.1 70B Instruct\nLlama 3.1 405B Instruct\nGeneral\nMMLU\n5\nmacro_avg/acc\n68.5\n69.4\n82.0\n83.6\n87.3\nMMLU (CoT)\n0\nmacro_avg/acc\n65.3\n73.0\n80.9\n86.0\n88.6\nMMLU-Pro (CoT)\n5\nmicro_avg/acc_char\n45.5\n48.3\n63.4\n66.4\n73.3\nIFEval\n76.8\n80.4\n82.9\n87.5\n88.6\nReasoning\nARC-C\n0\nacc\n82.4\n83.4\n94.4\n94.8\n96.9\nGPQA\n0\nem\n34.6\n30.4\n39.5\n46.7\n50.7\nCode\nHumanEval\n0\npass@1\n60.4\n72.6\n81.7\n80.5\n89.0\nMBPP ++ base version\n0\npass@1\n70.6\n72.8\n82.5\n86.0\n88.6\nMultipl-E HumanEval\n0\npass@1\n-\n50.8\n-\n65.5\n75.2\nMultipl-E MBPP\n0\npass@1\n-\n52.4\n-\n62.0\n65.7\nMath\nGSM-8K (CoT)\n8\nem_maj1@1\n80.6\n84.5\n93.0\n95.1\n96.8\nMATH (CoT)\n0\nfinal_em\n29.1\n51.9\n51.0\n68.0\n73.8\nTool Use\nAPI-Bank\n0\nacc\n48.3\n82.6\n85.1\n90.0\n92.0\nBFCL\n0\nacc\n60.3\n76.1\n83.0\n84.8\n88.5\nGorilla Benchmark API Bench\n0\nacc\n1.7\n8.2\n14.7\n29.7\n35.3\nNexus (0-shot)\n0\nmacro_avg/acc\n18.1\n38.5\n47.8\n56.7\n58.7\nMultilingual\nMultilingual MGSM (CoT)\n0\nem\n-\n68.9\n-\n86.9\n91.6\nMultilingual benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.1 8B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n62.12\n80.13\n84.95\nSpanish\n62.45\n80.05\n85.08\nItalian\n61.63\n80.4\n85.04\nGerman\n60.59\n79.27\n84.36\nFrench\n62.34\n79.82\n84.66\nHindi\n50.88\n74.52\n80.31\nThai\n50.32\n72.95\n78.21\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.1 systems\nLarge language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teaming\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nOur study of Llama-3.1-405B‚Äôs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "nari-labs/Dia-1.6B": "‚ö°Ô∏è Quickstart\nFeatures\n‚öôÔ∏è Usage\nAs a Python Library\nüíª Hardware and Inference Speed\nü™™ License\n‚ö†Ô∏è Disclaimer\nüî≠ TODO / Future Work\nü§ù Contributing\nü§ó Acknowledgements\nDia is a 1.6B parameter text to speech model created by Nari Labs. It was pushed to the Hub using the PytorchModelHubMixin integration.\nDia directly generates highly realistic dialogue from a transcript. You can condition the output on audio, enabling emotion and tone control. The model can also produce nonverbal communications like laughter, coughing, clearing throat, etc.\nTo accelerate research, we are providing access to pretrained model checkpoints and inference code. The model weights are hosted on Hugging Face. The model only supports English generation at the moment.\nWe also provide a demo page comparing our model to ElevenLabs Studio and Sesame CSM-1B.\n(Update) We have a ZeroGPU Space running! Try it now here. Thanks to the HF team for the support :)\nJoin our discord server for community support and access to new features.\nPlay with a larger version of Dia: generate fun conversations, remix content, and share with friends. üîÆ Join the waitlist for early access.\n‚ö°Ô∏è Quickstart\nThis will open a Gradio UI that you can work on.\ngit clone https://github.com/nari-labs/dia.git\ncd dia && uv run app.py\nor if you do not have uv pre-installed:\ngit clone https://github.com/nari-labs/dia.git\ncd dia\npython -m venv .venv\nsource .venv/bin/activate\npip install uv\nuv run app.py\nNote that the model was not fine-tuned on a specific voice. Hence, you will get different voices every time you run the model.\nYou can keep speaker consistency by either adding an audio prompt (a guide coming VERY soon - try it with the second example on Gradio for now), or fixing the seed.\nFeatures\nGenerate dialogue via [S1] and [S2] tag\nGenerate non-verbal like (laughs), (coughs), etc.\nBelow verbal tags will be recognized, but might result in unexpected output.\n(laughs), (clears throat), (sighs), (gasps), (coughs), (singing), (sings), (mumbles), (beep), (groans), (sniffs), (claps), (screams), (inhales), (exhales), (applause), (burps), (humming), (sneezes), (chuckle), (whistles)\nVoice cloning. See example/voice_clone.py for more information.\nIn the Hugging Face space, you can upload the audio you want to clone and place its transcript before your script. Make sure the transcript follows the required format. The model will then output only the content of your script.\n‚öôÔ∏è Usage\nAs a Python Library\nimport soundfile as sf\nfrom dia.model import Dia\nmodel = Dia.from_pretrained(\"nari-labs/Dia-1.6B\")\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\noutput = model.generate(text)\nsf.write(\"simple.mp3\", output, 44100)\nA pypi package and a working CLI tool will be available soon.\nüíª Hardware and Inference Speed\nDia has been tested on only GPUs (pytorch 2.0+, CUDA 12.6). CPU support is to be added soon.\nThe initial run will take longer as the Descript Audio Codec also needs to be downloaded.\nOn enterprise GPUs, Dia can generate audio in real-time. On older GPUs, inference time will be slower.\nFor reference, on a A4000 GPU, Dia roughly generates 40 tokens/s (86 tokens equals 1 second of audio).\ntorch.compile will increase speeds for supported GPUs.\nThe full version of Dia requires around 10GB of VRAM to run. We will be adding a quantized version in the future.\nIf you don't have hardware available or if you want to play with bigger versions of our models, join the waitlist here.\nü™™ License\nThis project is licensed under the Apache License 2.0 - see the LICENSE file for details.\n‚ö†Ô∏è Disclaimer\nThis project offers a high-fidelity speech generation model intended for research and educational use. The following uses are strictly forbidden:\nIdentity Misuse: Do not produce audio resembling real individuals without permission.\nDeceptive Content: Do not use this model to generate misleading content (e.g. fake news)\nIllegal or Malicious Use: Do not use this model for activities that are illegal or intended to cause harm.\nBy using this model, you agree to uphold relevant legal standards and ethical responsibilities. We are not responsible for any misuse and firmly oppose any unethical usage of this technology.\nüî≠ TODO / Future Work\nDocker support.\nOptimize inference speed.\nAdd quantization for memory efficiency.\nü§ù Contributing\nWe are a tiny team of 1 full-time and 1 part-time research-engineers. We are extra-welcome to any contributions!\nJoin our Discord Server for discussions.\nü§ó Acknowledgements\nWe thank the Google TPU Research Cloud program for providing computation resources.\nOur work was heavily inspired by SoundStorm, Parakeet, and Descript Audio Codec.\nHuggingFace for providing the ZeroGPU Grant.\n\"Nari\" is a pure Korean word for lily.\nWe thank Jason Y. for providing help with data filtering.",
    "Qwen/Qwen3-Embedding-8B": "Qwen3-Embedding-8B\nHighlights\nQwen3 Embedding Series Model list\nUsage\nSentence Transformers Usage\nTransformers Usage\nvLLM Usage\nText Embeddings Inference (TEI) Usage\nEvaluation\nMTEB (Multilingual)\nMTEB (Eng v2)\nC-MTEB (MTEB Chinese)\nCitation\nQwen3-Embedding-8B\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nQwen3-Embedding-8B has the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 8B\nContext Length: 32k\nEmbedding Dimension: Up to 4096, supports user-defined output dimensions ranging from 32 to 4096\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\nKeyError: 'qwen3'\nSentence Transformers Usage\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\nfrom sentence_transformers import SentenceTransformer\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-8B\")\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to \"left\":\n# model = SentenceTransformer(\n#     \"Qwen/Qwen3-Embedding-8B\",\n#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n#     tokenizer_kwargs={\"padding_side\": \"left\"},\n# )\n# The queries and documents to embed\nqueries = [\n\"What is the capital of China?\",\n\"Explain gravity\",\n]\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7493, 0.0751],\n#         [0.0880, 0.6318]])\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-8B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B')\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7493016123771667, 0.0750647559762001], [0.08795969933271408, 0.6318399906158447]]\nvLLM Usage\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\nmodel = LLM(model=\"Qwen/Qwen3-Embedding-8B\", task=\"embed\")\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7482624650001526, 0.07556197047233582], [0.08875375241041183, 0.6300010681152344]]\nüìå Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nText Embeddings Inference (TEI) Usage\nYou can either run / deploy TEI on NVIDIA GPUs as:\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-8B --dtype float16\nOr on CPU devices as:\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-8B --dtype float16\nAnd then, generate the embeddings sending a HTTP POST request as:\ncurl http://localhost:8080/embed \\\n-X POST \\\n-d '{\"inputs\": [\"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\", \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"]}' \\\n-H \"Content-Type: application/json\"\nEvaluation\nMTEB (Multilingual)\nModel\nSize\nMean (Task)\nMean (Type)\nBitxt Mining\nClass.\nClust.\nInst. Retri.\nMulti. Class.\nPair. Class.\nRerank\nRetri.\nSTS\nNV-Embed-v2\n7B\n56.29\n49.58\n57.84\n57.29\n40.80\n1.04\n18.63\n78.94\n63.82\n56.72\n71.10\nGritLM-7B\n7B\n60.92\n53.74\n70.53\n61.83\n49.75\n3.45\n22.77\n79.94\n63.78\n58.31\n73.33\nBGE-M3\n0.6B\n59.56\n52.18\n79.11\n60.35\n40.88\n-3.11\n20.1\n80.76\n62.79\n54.60\n74.12\nmultilingual-e5-large-instruct\n0.6B\n63.22\n55.08\n80.13\n64.94\n50.75\n-0.40\n22.91\n80.86\n62.61\n57.12\n76.81\ngte-Qwen2-1.5B-instruct\n1.5B\n59.45\n52.69\n62.51\n58.32\n52.05\n0.74\n24.02\n81.58\n62.58\n60.78\n71.61\ngte-Qwen2-7b-Instruct\n7B\n62.51\n55.93\n73.92\n61.55\n52.77\n4.94\n25.48\n85.13\n65.55\n60.08\n73.98\ntext-embedding-3-large\n-\n58.93\n51.41\n62.17\n60.27\n46.89\n-2.68\n22.03\n79.17\n63.89\n59.27\n71.68\nCohere-embed-multilingual-v3.0\n-\n61.12\n53.23\n70.50\n62.95\n46.89\n-1.89\n22.74\n79.88\n64.07\n59.16\n74.80\ngemini-embedding-exp-03-07\n-\n68.37\n59.59\n79.28\n71.82\n54.59\n5.18\n29.16\n83.63\n65.58\n67.71\n79.40\nQwen3-Embedding-0.6B\n0.6B\n64.33\n56.00\n72.22\n66.83\n52.33\n5.09\n24.59\n80.83\n61.41\n64.64\n76.17\nQwen3-Embedding-4B\n4B\n69.45\n60.86\n79.36\n72.33\n57.15\n11.56\n26.77\n85.05\n65.08\n69.60\n80.86\nQwen3-Embedding-8B\n8B\n70.58\n61.69\n80.89\n74.00\n57.65\n10.06\n28.66\n86.40\n65.63\n70.88\n81.08\nNote: For compared models, the scores are retrieved from MTEB online leaderboard on May 24th, 2025.\nMTEB (Eng v2)\nMTEB English / Models\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetri.\nSTS\nSumm.\nmultilingual-e5-large-instruct\n0.6B\n65.53\n61.21\n75.54\n49.89\n86.24\n48.74\n53.47\n84.72\n29.89\nNV-Embed-v2\n7.8B\n69.81\n65.00\n87.19\n47.66\n88.69\n49.61\n62.84\n83.82\n35.21\nGritLM-7B\n7.2B\n67.07\n63.22\n81.25\n50.82\n87.29\n49.59\n54.95\n83.03\n35.65\ngte-Qwen2-1.5B-instruct\n1.5B\n67.20\n63.26\n85.84\n53.54\n87.52\n49.25\n50.25\n82.51\n33.94\nstella_en_1.5B_v5\n1.5B\n69.43\n65.32\n89.38\n57.06\n88.02\n50.19\n52.42\n83.27\n36.91\ngte-Qwen2-7B-instruct\n7.6B\n70.72\n65.77\n88.52\n58.97\n85.9\n50.47\n58.09\n82.69\n35.74\ngemini-embedding-exp-03-07\n-\n73.3\n67.67\n90.05\n59.39\n87.7\n48.59\n64.35\n85.29\n38.28\nQwen3-Embedding-0.6B\n0.6B\n70.70\n64.88\n85.76\n54.05\n84.37\n48.18\n61.83\n86.57\n33.43\nQwen3-Embedding-4B\n4B\n74.60\n68.10\n89.84\n57.51\n87.01\n50.76\n68.46\n88.72\n34.39\nQwen3-Embedding-8B\n8B\n75.22\n68.71\n90.43\n58.57\n87.52\n51.56\n69.44\n88.58\n34.83\nC-MTEB (MTEB Chinese)\nC-MTEB\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nmultilingual-e5-large-instruct\n0.6B\n58.08\n58.24\n69.80\n48.23\n64.52\n57.45\n63.65\n45.81\nbge-multilingual-gemma2\n9B\n67.64\n68.52\n75.31\n59.30\n86.67\n68.28\n73.73\n55.19\ngte-Qwen2-1.5B-instruct\n1.5B\n67.12\n67.79\n72.53\n54.61\n79.5\n68.21\n71.86\n60.05\ngte-Qwen2-7B-instruct\n7.6B\n71.62\n72.19\n75.77\n66.06\n81.16\n69.24\n75.70\n65.20\nritrieve_zh_v1\n0.3B\n72.71\n73.85\n76.88\n66.5\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-0.6B\n0.6B\n66.33\n67.45\n71.40\n68.74\n76.42\n62.58\n71.03\n54.52\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "zai-org/GLM-4.5-Air": "GLM-4.5-Air\nModel Introduction\nQuick Start\nGLM-4.5-Air\nüëã Join our Discord community.\nüìñ Check out the GLM-4.5 technical blog, technical report, and Zhipu AI technical documentation.\nüìç Use GLM-4.5 API services on Z.ai API Platform (Global) or  Zhipu AI Open Platform (Mainland China).\nüëâ One click to GLM-4.5.\nModel Introduction\nThe GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of 63.2, in the 3rd place among all the proprietary and open-source  models. Notably, GLM-4.5-Air delivers competitive results at 59.8 while maintaining superior efficiency.\nFor more eval results, show cases, and technical details, please visit\nour technical blog or technical report.\nThe model code, tool parser and reasoning parser can be found in the implementation of transformers, vLLM and SGLang.\nQuick Start\nPlease refer our github page for more detail.",
    "lightx2v/Wan2.2-Lightning": "Wan2.2-Lightning\nüî• Latest News!!\nVideo Demos\nWan2.2-I2V-A14B-NFE4-V1 Demo\nWan2.2-T2V-A14B-NFE4-V1 Demo\nWan2.2-T2V-A14B-NFE4 Limitation\nüìë Todo List\nüöÄ Run Wan2.2-Lightning\nLicense Agreement\nAcknowledgements\nYou're welcome to visit our GitHub repository for the latest model releases or to reproduce our results.\nWan2.2-Lightning\nWe are excited to release the distilled version of Wan2.2 video generation model family, which offers the following advantages:\nFast: Video generation now requires only 4 steps without the need of CFG trick, leading to x20 speed-up\nHigh-quality: The distilled model delivers visuals on par with the base model in most scenarios, sometimes even better.\nComplex Motion Generation: Despite the reduction to just 4 steps, the model retains excellent motion dynamics in the generated scenes.\nüî• Latest News!!\nAug 08, 2025: üëã Release of Native ComfyUI Workflows.\nModel\nType\nFor Native Comfy\nFor Kijai's Wrapper\nWan2.2-I2V-A14B-NFE4-V1\nImage-to-Video\nI2V-V1-WF\nI2V-V1-WF\nWan2.2-T2V-A14B-NFE4-V1.1\nText-to-Video\nT2V-V1.1-WF\nT2V-V1.1-WF\nAug 07, 2025: üëã Release of Wan2.2-I2V-A14B-NFE4-V1.\nAug 07, 2025: üëã Release of Wan2.2-T2V-A14B-NFE4-V1.1. The generation quality of V1.1 is slightly better than V1.\nAug 04, 2025: üëã Release of Wan2.2-T2V-A14B-NFE4-V1.\nVideo Demos\nWan2.2-I2V-A14B-NFE4-V1 Demo\nThe videos below can be reproduced using examples/i2v_prompt_list.txt and examples/i2v_image_path_list.txt.\nWan2.2-T2V-A14B-NFE4-V1 Demo\nThe videos below can be reproduced using examples/prompt_list.txt.\nWan2.2-T2V-A14B-NFE4 Limitation\nWhen the video contains elements with extremely large motion, the generated results may include artifacts.\nIn some results, the direction of the vehicles may be reversed.\nüìë Todo List\nWan2.2-T2V-A14B-4steps\nWan2.2-I2V-A14B-4steps\nWan2.2-TI2V-5B-4steps\nüöÄ Run Wan2.2-Lightning\nInstallation\nPlease follow Wan2.2 Official Github to install the Python Environment and download the Base Model.\nModel Download\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B\nhuggingface-cli download lightx2v/Wan2.2-Lightning --local-dir ./Wan2.2-Lightning\nRun Text-to-Video Generation\nThis repository supports the Wan2.2-T2V-A14B Text-to-Video model and can simultaneously support video generation at 480P and 720P resolutions, either portrait or landscape.\n(1) Without Prompt Extension\nTo facilitate implementation, we will start with a basic version of the inference process that skips the prompt extension step.\nSingle-GPU, Single-prompt inference\npython generate.py  --task t2v-A14B --size \"1280*720\" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nSingle-GPU, Multiple-prompt inference\npython generate.py  --task t2v-A14B --size \"1280*720\" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt_file examples/prompt_list.txt\nüí° This command can run on a GPU with at least 80GB VRAM.\nüí°If you encounter OOM (Out-of-Memory) issues, you can use the --offload_model True, --convert_model_dtype and --t5_cpu options to reduce GPU memory usage.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\nWe use PyTorch FSDP and DeepSpeed Ulysses to accelerate inference.\ntorchrun --nproc_per_node=8 generate.py --task t2v-A14B --size \"1280*720\" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --base_seed 42 --prompt_file examples/prompt_list.txt\n(2) Using Prompt Extension\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\nUse the Dashscope API for extension.\nApply for a dashscope.api_key in advance (EN | CN).\nConfigure the environment variable DASH_API_KEY to specify the Dashscope API key. For users of Alibaba Cloud's international site, you also need to set the environment variable DASH_API_URL to 'https://dashscope-intl.aliyuncs.com/api/v1'. For more detailed instructions, please refer to the dashscope document.\nUse the qwen-plus model for text-to-video tasks and qwen-vl-max for image-to-video tasks.\nYou can modify the model used for extension with the parameter --prompt_extend_model. For example:\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'dashscope' --prompt_extend_target_lang 'zh'\nUsing a local model for extension.\nBy default, the Qwen model on HuggingFace is used for this extension. Users can choose Qwen models or other models based on the available GPU memory size.\nFor text-to-video tasks, you can use models like Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct and Qwen/Qwen2.5-3B-Instruct.\nFor image-to-video tasks, you can use models like Qwen/Qwen2.5-VL-7B-Instruct and Qwen/Qwen2.5-VL-3B-Instruct.\nLarger models generally provide better extension results but require more GPU memory.\nYou can modify the model used for extension with the parameter --prompt_extend_model , allowing you to specify either a local model path or a Hugging Face model. For example:\ntorchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'local_qwen' --prompt_extend_target_lang 'zh'\nRun Image-to-Video Generation\nThis repository supports the Wan2.2-I2V-A14B Image-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\nSingle-GPU inference\npython generate.py  --task i2v-A14B  --size \"1280*720\" --ckpt_dir ./Wan2.2-I2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt_file examples/i2v_prompt_list.txt --image_path_file examples/i2v_image_path_list.txt\nThis command can run on a GPU with at least 80GB VRAM.\nüí°For the Image-to-Video task, the size parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --base_seed 42 --prompt_file examples/i2v_prompt_list.txt --image_path_file examples/i2v_image_path_list.txt\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe built upon and reused code from the following projects: Wan2.1, Wan2.2, licensed under the Apache License 2.0.\nWe also adopt the evaluation text prompts from Movie Gen Bench, which is licensed under the Creative Commons Attribution-NonCommercial 4.0 (CC BY-NC 4.0) License. The original license can be found here.\nThe selected prompts are further enhanced using the Qwen/Qwen2.5-14B-Instructmodel Qwen.",
    "lodestones/Chroma1-HD": "Chroma1-HD\nSpecial Thanks\nHow to Use\ndiffusers Library\nModel Details\nIntended Use\nLimitations and Bias Statement\nSummary of Architectural Modifications\nP.S\nCitation\nChroma1-HD\nChroma1-HD is an 8.9B parameter text-to-image foundational model based on FLUX.1-schnell. It is fully Apache 2.0 licensed, ensuring that anyone can use, modify, and build upon it.\nAs a base model, Chroma1 is intentionally designed to be an excellent starting point for finetuning. It provides a strong, neutral foundation for developers, researchers, and artists to create specialized models.\nfor the fast CFG \"baked\" version please go to Chroma1-Flash.\nKey Features\nHigh-Performance Base: 8.9B parameters, built on the powerful FLUX.1 architecture.\nEasily Finetunable: Designed as an ideal checkpoint for creating custom, specialized models.\nCommunity-Driven & Open-Source: Fully transparent with an Apache 2.0 license, and training history.\nFlexible by Design: Provides a flexible foundation for a wide range of generative tasks.\nSpecial Thanks\nA massive thank you to our supporters who make this project possible.\nAnonymous donor whose incredible generosity funded the pretraining run and data collections. Your support has been transformative for open-source AI.\nFictional.ai for their fantastic support and for helping push the boundaries of open-source AI. You can try Chroma on their platform:\nHow to Use\ndiffusers Library\ninstall the requirements\npip install transformers diffusers sentencepiece accelerate\nimport torch\nfrom diffusers import ChromaPipeline\npipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma1-HD\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload()\nprompt = [\n\"A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.\"\n]\nnegative_prompt =  [\"low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors\"]\nimage = pipe(\nprompt=prompt,\nnegative_prompt=negative_prompt,\ngenerator=torch.Generator(\"cpu\").manual_seed(433),\nnum_inference_steps=40,\nguidance_scale=3.0,\nnum_images_per_prompt=1,\n).images[0]\nimage.save(\"chroma.png\")\nQuantized inference using gemlite\nimport torch\nfrom diffusers import ChromaPipeline\npipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma1-HD\", torch_dtype=torch.float16)\n#pipe.enable_model_cpu_offload()\n#######################################################\nimport gemlite\ndevice = 'cuda:0'\nprocessor = gemlite.helper.A8W8_int8_dynamic\n#processor = gemlite.helper.A8W8_fp8_dynamic\n#processor = gemlite.helper.A16W4_MXFP\nfor name, module in pipe.transformer.named_modules():\nmodule.name = name\ndef patch_linearlayers(model, fct):\nfor name, layer in model.named_children():\nif isinstance(layer, torch.nn.Linear):\nsetattr(model, name, fct(layer, name))\nelse:\npatch_linearlayers(layer, fct)\ndef patch_linear_to_gemlite(layer, name):\nlayer = layer.to(device, non_blocking=True)\ntry:\nreturn processor(device=device).from_linear(layer)\nexcept Exception as exception:\nprint('Skipping gemlite conversion for: ' + str(layer.name), exception)\nreturn layer\npatch_linearlayers(pipe.transformer, patch_linear_to_gemlite)\ntorch.cuda.synchronize()\ntorch.cuda.empty_cache()\npipe.to(device)\npipe.transformer.forward = torch.compile(pipe.transformer.forward, fullgraph=True)\npipe.vae.forward = torch.compile(pipe.vae.forward, fullgraph=True)\n#pipe.set_progress_bar_config(disable=True)\n#######################################################\nprompt = [\n\"A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.\"\n]\nnegative_prompt =  [\"low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors\"]\nimport time\nfor _ in range(3):\nt_start = time.time()\nimage = pipe(\nprompt=prompt,\nnegative_prompt=negative_prompt,\ngenerator=torch.Generator(\"cpu\").manual_seed(433),\nnum_inference_steps=40,\nguidance_scale=3.0,\nnum_images_per_prompt=1,\n).images[0]\nt_end = time.time()\nprint(f\"Took: {t_end - t_start} secs.\") #66.1242527961731 -> 27.72 sec\nimage.save(\"chroma.png\")\nComfyUI\nFor advanced users and customized workflows, you can use Chroma with ComfyUI.\nRequirements:\nA working ComfyUI installation.\nChroma checkpoint (latest version).\nT5 XXL Text Encoder.\nFLUX VAE.\nChroma Workflow JSON.\nSetup:\nPlace the T5_xxl model in your ComfyUI/models/clip folder.\nPlace the FLUX VAE in your ComfyUI/models/vae folder.\nPlace the Chroma checkpoint in your ComfyUI/models/diffusion_models folder.\nLoad the Chroma workflow file into ComfyUI and run.\nModel Details\nArchitecture: Based on the 8.9B parameter FLUX.1-schnell model.\nTraining Data: Trained on a 5M sample dataset curated from a 20M pool, including artistic, photographic, and niche styles.\nTechnical Report: A comprehensive technical paper detailing the architectural modifications and training process is forthcoming.\nIntended Use\nChroma is intended to be used as a base model for researchers and developers to build upon. It is ideal for:\nFinetuning on specific styles, concepts, or characters.\nResearch into generative model behavior, alignment, and safety.\nAs a foundational component in larger AI systems.\nLimitations and Bias Statement\nChroma is trained on a broad, filtered dataset from the internet. As such, it may reflect the biases and stereotypes present in its training data. The model is released in a state as is and has not been aligned with a specific safety filter.\nUsers are responsible for their own use of this model. It has the potential to generate content that may be considered harmful, explicit, or offensive. I encourage developers to implement appropriate safeguards and ethical considerations in their downstream applications.\nSummary of Architectural Modifications\n(For a full breakdown, tech report soon-ish.)\n12B ‚Üí 8.9B Parameters:\nTL;DR: I replaced a 3.3B parameter timestep-encoding layer with a more efficient 250M parameter FFN, as the original was vastly oversized for its task.\nMMDiT Masking:\nTL;DR: Masking T5 padding tokens enhanced fidelity and increased training stability by preventing the model from focusing on irrelevant <pad> tokens.\nCustom Timestep Distributions:\nTL;DR: I implemented a custom timestep sampling distribution (-x^2) to prevent loss spikes and ensure the model trains effectively on both high-noise and low-noise regions.\nP.S\nChroma1-HD is not the old Chroma-v.50 it has been retrained from v.48\nCitation\n@misc{rock2025chroma,\nauthor = {Lodestone Rock},\ntitle = {Chroma1-HD},\nyear = {2025},\npublisher = {Hugging Face},\njournal = {Hugging Face repository},\nhowpublished = {\\url{https://huggingface.co/lodestones/Chroma1-HD}},\n}",
    "microsoft/VibeVoice-1.5B": "VibeVoice: A Frontier Open-Source Text-to-Speech Model\nTraining Details\nModels\nInstallation and Usage\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nContact\nVibeVoice: A Frontier Open-Source Text-to-Speech Model\nVibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\nThe model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.\n‚û°Ô∏è Technical Report: VibeVoice Technical Report\n‚û°Ô∏è Project Page: microsoft/VibeVoice\n‚û°Ô∏è Code: microsoft/VibeVoice-Code\nTraining Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head.\nLLM: Qwen2.5-1.5B for this release.\nTokenizers:\nAcoustic Tokenizer: Based on a œÉ-VAE variant (proposed in LatentLM), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each.\nSemantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task.\nDiffusion Head: Lightweight module (4 layers, ~123M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\nContext Length: Trained with a curriculum increasing up to 65,536 tokens.\nTraining Stages:\nTokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately.\nVibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K -> 64K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers.\nModels\nModel\nContext Length\nGeneration Length\nWeight\nVibeVoice-0.5B-Streaming\n-\n-\nOn the way\nVibeVoice-1.5B\n64K\n~90 min\nYou are here.\nVibeVoice-Large\n32K\n~45 min\nHF link\nInstallation and Usage\nPlease refer to GitHub README\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent ‚Äì cloning a real individual‚Äôs voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass.\nDisinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events.\nReal‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference ‚Äúlive deep‚Äëfake‚Äù applications.\nUnsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have:\nEmbedded an audible disclaimer (e.g. ‚ÄúThis segment was generated by AI‚Äù) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nLogged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.\nUsers are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nContact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently,‚ÄØwe will‚ÄØupdate this repository with appropriate mitigations.",
    "REPA-E/e2e-qwenimage-vae": "üöÄ Overall\n‚ö°Ô∏è Quickstart\nüß© End-to-End Trained VAE Releases\nüì¶ Requirements\nüöÄ Example Usage\nüöÄ REPA-E for T2I\nEnd-to-End Tuned VAEs for Supercharging Text-to-Image Diffusion Transformers\nüåê Project Page\nü§ó Models\nüìÉ Paper\nüöÄ Overall\nWe present REPA-E for T2I, a family of end-to-end tuned VAEs designed to supercharge text-to-image generation training. These models consistently outperform Qwen-Image-VAE across all benchmarks (COCO-30K, DPG-Bench, GenAI-Bench, GenEval, and MJHQ-30K) without requiring any additional representation alignment losses.\nFor training, we adopt the official REPA-E training code to optimize the\nQwen-Image-VAE for 80 epochs with a batch size of 256 on the ImageNet-256 dataset.\nThe REPA-E training effectively refines the VAE‚Äôs latent-space structure and enables faster convergence in downstream text-to-image latent diffusion model training.\nThis repository provides diffusers-compatible weights for the end-to-end trained Qwen-Image-VAE. In addition, we release end-to-end trained variants of several other widely used VAEs to facilitate research and integration within text-to-image diffusion frameworks.\n‚ö°Ô∏è Quickstart\nfrom diffusers import AutoencoderKLQwenImage\nvae = AutoencoderKLQwenImage.from_pretrained(\"REPA-E/e2e-qwenimage-vae\").to(\"cuda\")\nUse vae.encode(...) / vae.decode(...) in your pipeline. (A full example is provided below.)\nüß© End-to-End Trained VAE Releases\nModel\nHugging Face Link\nE2E-FLUX-VAE\nü§ó REPA-E/e2e-flux-vae\nE2E-SD-3.5-VAE\nü§ó REPA-E/e2e-sd3.5-vae\nE2E-Qwen-Image-VAE\nü§ó REPA-E/e2e-qwenimage-vae\nüì¶ Requirements\nThe following packages are required to load and run the REPA-E VAEs with the diffusers library:\npip install diffusers>=0.35.0\npip install torch>=2.5.0\nüöÄ Example Usage\nBelow is a minimal example showing how to load and use the REPA-E end-to-end trained Qwen-Image-VAE with diffusers:\nfrom io import BytesIO\nimport requests\nfrom diffusers import AutoencoderKLQwenImage\nimport numpy as np\nimport torch\nfrom PIL import Image\nresponse = requests.get(\"https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png\")\ndevice = \"cuda\"\nimage = torch.from_numpy(\nnp.array(\nImage.open(BytesIO(response.content))\n)\n).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1\nimage = image.to(device)\nvae = AutoencoderKLQwenImage.from_pretrained(\"REPA-E/e2e-qwenimage-vae\").to(device)\n# QwenImage VAE expects an additional dimension for `num_frames`\nimage_ = image.unsqueeze(2)\nwith torch.no_grad():\nlatents = vae.encode(image_).latent_dist.sample()\nreconstructed = vae.decode(latents).sample\n# Squeeze the extra frame dimension\nlatents = latents.squeeze(2)\nreconstructed = reconstructed.squeeze(2)",
    "lightx2v/Autoencoders": "üé® LightVAE\n‚ö° Efficient Video Autoencoder (VAE) Model Collection\nüí° Core Advantages\nüìä Official VAE\nüöÄ Open Source TAE Series\nüéØ LightVAE Series (Our Optimization)\n‚ö° LightTAE Series (Our Optimization)\nüì¶ Available Models\nüéØ Wan2.1 Series VAE\nüéØ Wan2.2 Series VAE\nüìä Wan2.1 Series Performance Comparison\nVideo Reconstruction (5s 81-frame video)\nVideo Generation\nüìä Wan2.2 Series Performance Comparison\nVideo Reconstruction\nVideo Generation\nüéØ Model Selection Recommendations\nSelection by Use Case\nüî• Our Optimization Results Comparison\nüìë Todo List\nüöÄ Usage\nDownload VAE Models\nüß™  Video Reconstruction Test\nUse in LightX2V\nUse in ComfyUI\n‚ö†Ô∏è Important Notes\n1. Compatibility\nüìö Related Resources\nDocumentation Links\nRelated Models\nü§ù Community & Support\nüé® LightVAE\n‚ö° Efficient Video Autoencoder (VAE) Model Collection\nFrom Official Models to Lightx2v Distilled Optimized Versions - Balancing Quality, Speed and Memory\nFor VAE, the LightX2V team has conducted a series of deep optimizations, deriving two major series: LightVAE and LightTAE, which significantly reduce memory consumption and improve inference speed while maintaining high quality.\nüí° Core Advantages\nüìä Official VAE\nFeatures: Highest Quality ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚úÖ Best reconstruction accuracy‚úÖ Complete detail preservation‚ùå Large memory usage (~8-12 GB)‚ùå Slow inference speed\nüöÄ Open Source TAE Series\nFeatures: Fastest Speed ‚ö°‚ö°‚ö°‚ö°‚ö°\n‚úÖ Minimal memory usage (~0.4 GB)‚úÖ Extremely fast inference‚ùå Average quality ‚≠ê‚≠ê‚≠ê‚ùå Potential detail loss\nüéØ LightVAE Series (Our Optimization)\nFeatures: Best Balanced Solution ‚öñÔ∏è\n‚úÖ Uses Causal 3D Conv (same as official)‚úÖ Quality close to official ‚≠ê‚≠ê‚≠ê‚≠ê\n‚úÖ Memory reduced by ~50% (~4-5 GB)‚úÖ Speed increased by 2-3x‚úÖ Balances quality, speed, and memory üèÜ\n‚ö° LightTAE Series (Our Optimization)\nFeatures: Fast Speed + Good Quality üèÜ\n‚úÖ Minimal memory usage (~0.4 GB)‚úÖ Extremely fast inference‚úÖ Quality close to official ‚≠ê‚≠ê‚≠ê‚≠ê\n‚úÖ Significantly surpasses open source TAE\nüì¶ Available Models\nüéØ Wan2.1 Series VAE\nModel Name\nType\nArchitecture\nDescription\nWan2.1_VAE\nOfficial VAE\nCausal Conv3D\nWan2.1 official video VAE modelHighest quality, large memory, slow speed\ntaew2_1\nOpen Source Small AE\nConv2D\nOpen source model based on taeHVSmall memory, fast speed, average quality\nlighttaew2_1\nLightTAE Series\nConv2D\nOur distilled optimized version based on taew2_1Small memory, fast speed, quality close to official ‚ú®\nlightvaew2_1\nLightVAE Series\nCausal Conv3D\nOur pruned 75% on WanVAE2.1 architecture then trained+distilledBest balance: high quality + low memory + fast speed üèÜ\nüéØ Wan2.2 Series VAE\nModel Name\nType\nArchitecture\nDescription\nWan2.2_VAE\nOfficial VAE\nCausal Conv3D\nWan2.2 official video VAE modelHighest quality, large memory, slow speed\ntaew2_2\nOpen Source Small AE\nConv2D\nOpen source model based on taeHVSmall memory, fast speed, average quality\nlighttaew2_2\nLightTAE Series\nConv2D\nOur distilled optimized version based on taew2_2Small memory, fast speed, quality close to official ‚ú®\nüìä Wan2.1 Series Performance Comparison\nPrecision: BF16\nTest Hardware: NVIDIA H100\nVideo Reconstruction (5s 81-frame video)\nSpeed\nWan2.1_VAE\ntaew2_1\nlighttaew2_1\nlightvaew2_1\nEncode Speed\n4.1721 s\n0.3956 s\n0.3956 s\n1.5014s\nDecode Speed\n5.4649 s\n0.2463 s\n0.2463 s\n2.0697s\nGPU Memory\nWan2.1_VAE\ntaew2_1\nlighttaew2_1\nlightvaew2_1\nEncode Memory\n8.4954 GB\n0.00858 GB\n0.00858 GB\n4.7631 GB\nDecode Memory\n10.1287 GB\n0.41199 GB\n0.41199 GB\n5.5673 GB\nVideo Generation\nTask: s2v(speech to video)Model: seko-talk\nWan2.1_VAE\ntaew2_1\nlighttaew2_1\nlightvaew2_1\nüìä Wan2.2 Series Performance Comparison\nPrecision: BF16\nTest Hardware: NVIDIA H100\nVideo Reconstruction\nSpeed\nWan2.2_VAE\ntaew2_2\nlighttaew2_2\nEncode Speed\n1.1369s\n0.3499 s\n0.3499 s\nDecode Speed\n3.1268 s\n0.0891 s\n0.0891 s\nGPU Memory\nWan2.2_VAE\ntaew2_2\nlighttaew2_2\nEncode Memory\n6.1991 GB\n0.0064 GB\n0.0064 GB\nDecode Memory\n12.3487 GB\n0.4120 GB\n0.4120 GB\nVideo Generation\nTask: t2v(text to video)Model: Wan2.2-TI2V-5B\nWan2.2_VAE\ntaew2_2\nlighttaew2_2\nüéØ Model Selection Recommendations\nSelection by Use Case\nüèÜ Pursuing Best Quality\nRecommended: Wan2.1_VAE / Wan2.2_VAE\n‚úÖ Official model, quality ceiling\n‚úÖ Highest reconstruction accuracy\n‚úÖ Suitable for final product output\n‚ö†Ô∏è Large memory usage (~8-12 GB)\n‚ö†Ô∏è Slow inference speed\n‚öñÔ∏è Best Balance üèÜ\nRecommended: lightvaew2_1\n‚úÖ Uses Causal 3D Conv (same as official)\n‚úÖ Excellent quality, close to official\n‚úÖ Memory reduced by ~50% (~4-5 GB)\n‚úÖ Speed increased by 2-3x\n‚úÖ Close to official quality ‚≠ê‚≠ê‚≠ê‚≠ê\nUse Cases: Daily production, strongly recommended ‚≠ê\n‚ö° Speed + Quality Balance ‚ú®\nRecommended: lighttaew2_1 / lighttaew2_2\n‚úÖ Extremely low memory usage (~0.4 GB)\n‚úÖ Extremely fast inference\n‚úÖ Quality significantly surpasses open source TAE\n‚úÖ Close to official quality ‚≠ê‚≠ê‚≠ê‚≠ê\nUse Cases: Development testing, rapid iteration\nüî• Our Optimization Results Comparison\nComparison\nOpen Source TAE\nLightTAE (Ours)\nOfficial VAE\nLightVAE (Ours)\nArchitecture\nConv2D\nConv2D\nCausal Conv3D\nCausal Conv3D\nMemory Usage\nMinimal (~0.4 GB)\nMinimal (~0.4 GB)\nLarge (~8-12 GB)\nMedium (~4-5 GB)\nInference Speed\nExtremely Fast ‚ö°‚ö°‚ö°‚ö°‚ö°\nExtremely Fast ‚ö°‚ö°‚ö°‚ö°‚ö°\nSlow ‚ö°‚ö°\nFast ‚ö°‚ö°‚ö°‚ö°\nGeneration Quality\nAverage ‚≠ê‚≠ê‚≠ê\nClose to Official ‚≠ê‚≠ê‚≠ê‚≠ê\nHighest ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\nClose to Official ‚≠ê‚≠ê‚≠ê‚≠ê\nüìë Todo List\nLightX2V integration\nComfyUI integration\nTraining & Distillation Code\nüöÄ Usage\nDownload VAE Models\n# Download Wan2.1 official VAE\nhuggingface-cli download lightx2v/Autoencoders \\\n--local-dir ./models/vae/\nüß™  Video Reconstruction Test\nWe provide a standalone script vid_recon.py to test VAE models independently. This script reads a video, encodes it through VAE, then decodes it back to verify the reconstruction quality.\nScript Location: LightX2V/lightx2v/models/video_encoders/hf/vid_recon.py\ngit clone https://github.com/ModelTC/LightX2V.git\ncd LightX2V\n1. Test Official VAE (Wan2.1)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/Wan2.1_VAE.pth \\\n--model_type vaew2_1 \\\n--device cuda \\\n--dtype bfloat16\n2. Test Official VAE (Wan2.2)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/Wan2.2_VAE.pth \\\n--model_type vaew2_2 \\\n--device cuda \\\n--dtype bfloat16\n3. Test LightTAE (Wan2.1)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/lighttaew2_1.pth \\\n--model_type taew2_1 \\\n--device cuda \\\n--dtype bfloat16\n4. Test LightTAE (Wan2.2)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/lighttaew2_2.pth \\\n--model_type taew2_2 \\\n--device cuda \\\n--dtype bfloat16\n5. Test LightVAE (Wan2.1)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/lightvaew2_1.pth \\\n--model_type vaew2_1 \\\n--device cuda \\\n--dtype bfloat16 \\\n--use_lightvae\n6. Test TAE (Wan2.1)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/taew2_1.pth \\\n--model_type taew2_1 \\\n--device cuda \\\n--dtype bfloat16\n7. Test TAE (Wan2.2)\npython -m lightx2v.models.video_encoders.hf.vid_recon \\\ninput_video.mp4 \\\n--checkpoint ./models/vae/taew2_2.pth \\\n--model_type taew2_1 \\\n--device cuda \\\n--dtype bfloat16\nUse in LightX2V\nSpecify the VAE path in the configuration file:\nUsing Official VAE Series:\n{\n\"vae_path\": \"./models/vae/Wan2.1_VAE.pth\"\n}\nUsing LightVAE Series:\n{\n\"use_lightvae\": true,\n\"vae_path\": \"./models/vae/lightvaew2_1.pth\"\n}\nUsing LightTAE Series:\n{\n\"use_tae\": true,\n\"need_scaled\": true,\n\"tae_path\": \"./models/vae/lighttaew2_1.pth\"\n}\nUsing TAE Series:\n{\n\"use_tae\": true,\n\"tae_path\": \"./models/vae/taew2_1.pth\"\n}\nThen run the inference script:\ncd LightX2V/scripts\nbash wan/run_wan_i2v.sh  # or other inference scripts\nUse in ComfyUI\nplease refer to  https://github.com/ModelTC/ComfyUI-LightVAE\n‚ö†Ô∏è Important Notes\n1. Compatibility\nWan2.1 series VAE only works with Wan2.1 backbone models\nWan2.2 series VAE only works with Wan2.2 backbone models\nDo not mix different versions of VAE and backbone models\nüìö Related Resources\nDocumentation Links\nLightX2V Quick Start: Quick Start Documentation\nModel Structure Description: Model Structure Documentation\ntaeHV Project: GitHub - madebyollin/taeHV\nRelated Models\nWan2.1 Backbone Models: Wan-AI Model Collection\nWan2.2 Backbone Models: Wan-AI/Wan2.2-TI2V-5B\nLightX2V Optimized Models: lightx2v Model Collection\nü§ù Community & Support\nGitHub Issues: https://github.com/ModelTC/LightX2V/issues\nHuggingFace: https://huggingface.co/lightx2v\nLightX2V Homepage: https://github.com/ModelTC/LightX2V\nIf you find this project helpful, please give us a ‚≠ê on GitHub",
    "meta-llama/Llama-2-7b-hf": "You need to share contact information with Meta to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 2 COMMUNITY LICENSE AGREEMENT\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and  modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity's behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or  entity if you are  entering in this Agreement on their behalf.\"Llama 2\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other  elements of the foregoing distributed by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non- transferable and royalty-free limited license under Meta's intellectual property or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,  distribute, copy, create derivative works of, and make modifications to the Llama  Materials.b. Redistribution and Use.i. If you distribute or make the Llama Materials, or any derivative works  thereof, available to a third party, you shall provide a copy of this Agreement to such  third party.ii.  If you receive Llama Materials, or any derivative works thereof, from  a Licensee as part of an integrated end user product, then Section 2 of this  Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you  distribute the following attribution notice within a \"Notice\" text file distributed as a  part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community License,  Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws  and regulations (including trade compliance laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated by reference into  this Agreement.v. You will not use the Llama Materials or any output or results of the  Llama Materials to improve any other large language model (excluding Llama 2 or  derivative works thereof).\nAdditional Commercial Terms. If, on the Llama 2 version release date, the  monthly active users of the products or services made available by or for Licensee,  or Licensee's affiliates, is greater than 700 million monthly active users in the  preceding calendar month, you must request a license from Meta, which Meta may  grant to you in its sole discretion, and you are not authorized to exercise any of the  rights under this Agreement unless or until Meta otherwise expressly grants you  such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in  connection with the Llama Materials, neither Meta nor Licensee may use any name  or mark owned by or associated with the other or any of its affiliates, except as  required for reasonable and customary use in describing and redistributing the  Llama Materials.b. Subject to Meta's ownership of Llama Materials and derivatives made by or  for Meta, with respect to any derivative works and modifications of the Llama  Materials that are made by you, as between you and Meta, you are and will be the  owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity  (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs or results, or any portion of any of the foregoing,  constitutes infringement of intellectual property or other rights owned or licensable  by you, then any licenses granted to you under this Agreement shall terminate as of  the date such litigation or claim is filed or instituted. You will indemnify and hold  harmless Meta from and against any claim by any third party arising out of or related  to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your  acceptance of this Agreement or access to the Llama Materials and will continue in  full force and effect until terminated in accordance with the terms and conditions  herein. Meta may terminate this Agreement if you are in breach of any term or  condition of this Agreement. Upon termination of this Agreement, you shall delete  and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and  construed under the laws of the State of California without regard to choice of law  principles, and the UN Convention on Contracts for the International Sale of Goods  does not apply to this Agreement. The courts of California shall have exclusive  jurisdiction of any dispute arising out of this Agreement.\nLlama 2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at ai.meta.com/llama/use-policy.\nProhibited Uses\nWe want everyone to use Llama 2 safely and responsibly. You agree you will not use, or allow others to use, Llama 2 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: github.com/facebookresearch/llama\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nLlama 2\nModel Details\nIntended Use\nHardware and Software\nTraining Data\nEvaluation Results\nEthical Considerations and Limitations\nReporting Issues\nLlama Model Index\nLlama 2\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\nModel Details\nNote: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\nModel Developers Meta\nVariations Llama 2 comes in a range of parameter sizes ‚Äî 7B, 13B, and 70B ‚Äî as well as pretrained and fine-tuned variations.\nInput Models input text only.\nOutput Models generate text only.\nModel Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\nTraining Data\nParams\nContent Length\nGQA\nTokens\nLR\nLlama 2\nA new mix of publicly available online data\n7B\n4k\n‚úó\n2.0T\n3.0 x 10-4\nLlama 2\nA new mix of publicly available online data\n13B\n4k\n‚úó\n2.0T\n3.0 x 10-4\nLlama 2\nA new mix of publicly available online data\n70B\n4k\n‚úî\n2.0T\n1.5 x 10-4\nLlama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Dates Llama 2 was trained between January 2023 and July 2023.\nStatus This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\nResearch Paper \"Llama-2: Open Foundation and Fine-tuned Chat Models\"\nIntended Use\nIntended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). See our reference code in github for details: chat_completion.\nOut-of-scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\nHardware and Software\nTraining Factors We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\nTime (GPU hours)\nPower Consumption (W)\nCarbon Emitted(tCO2eq)\nLlama 2 7B\n184320\n400\n31.22\nLlama 2 13B\n368640\n400\n62.44\nLlama 2 70B\n1720320\n400\n291.42\nTotal\n3311616\n539.00\nCO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\nTraining Data\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\nData Freshness The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\nEvaluation Results\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\nModel\nSize\nCode\nCommonsense Reasoning\nWorld Knowledge\nReading Comprehension\nMath\nMMLU\nBBH\nAGI Eval\nLlama 1\n7B\n14.1\n60.8\n46.2\n58.5\n6.95\n35.1\n30.3\n23.9\nLlama 1\n13B\n18.9\n66.1\n52.6\n62.3\n10.9\n46.9\n37.0\n33.9\nLlama 1\n33B\n26.0\n70.0\n58.4\n67.6\n21.4\n57.8\n39.8\n41.7\nLlama 1\n65B\n30.7\n70.7\n60.5\n68.6\n30.8\n63.4\n43.5\n47.6\nLlama 2\n7B\n16.8\n63.9\n48.9\n61.3\n14.6\n45.3\n32.6\n29.3\nLlama 2\n13B\n24.5\n66.9\n55.4\n65.8\n28.7\n54.8\n39.4\n39.1\nLlama 2\n70B\n37.5\n71.9\n63.6\n69.4\n35.2\n68.9\n51.2\n54.2\nOverall performance on grouped academic benchmarks. Code: We report the average pass@1 scores of our models on HumanEval and MBPP. Commonsense Reasoning: We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. World Knowledge: We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. Reading Comprehension: For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. MATH: We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\nTruthfulQA\nToxigen\nLlama 1\n7B\n27.42\n23.00\nLlama 1\n13B\n41.74\n23.08\nLlama 1\n33B\n44.19\n22.57\nLlama 1\n65B\n48.71\n21.77\nLlama 2\n7B\n33.29\n21.25\nLlama 2\n13B\n41.86\n26.10\nLlama 2\n70B\n50.18\n24.60\nEvaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\nTruthfulQA\nToxigen\nLlama-2-Chat\n7B\n57.04\n0.00\nLlama-2-Chat\n13B\n62.18\n0.00\nLlama-2-Chat\n70B\n64.14\n0.01\nEvaluation of fine-tuned LLMs on different safety datasets. Same metric definitions as above.\nEthical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\nPlease see the Responsible Use Guide available at https://ai.meta.com/llama/responsible-use-guide/\nReporting Issues\nPlease report any software ‚Äúbug,‚Äù or other problems with the models through one of the following means:\nReporting issues with the model: github.com/facebookresearch/llama\nReporting problematic content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nLlama Model Index\nModel\nLlama2\nLlama2-hf\nLlama2-chat\nLlama2-chat-hf\n7B\nLink\nLink\nLink\nLink\n13B\nLink\nLink\nLink\nLink\n70B\nLink\nLink\nLink\nLink",
    "Comfy-Org/Wan_2.1_ComfyUI_repackaged": "Wan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan",
    "MaxedOut/ComfyUI-Starter-Packs": "üì¶ ComfyUI-Starter-Packs\nüßê What‚Äôs Inside\nüëÅÔ∏è I See You\nNew to ComfyUI or just want clean workflows?\nFlux Showcase:\nüß† Depth:\n‚úèÔ∏è Canny:\nüï∫ OpenPose:\nü´† Pulid Face Transfer\nüîÑ Redux\nüì¶ ComfyUI-Starter-Packs\nA curated vault of essential models for ComfyUI users ‚Äî organized, curated, and ready.No bloat. No hunting through 15 Civitai and Hugging Face repos.\nüßê What‚Äôs Inside\nFlux1\n‚îú‚îÄ‚îÄ Flux1\n‚îÇ   ‚îú‚îÄ‚îÄ Controlnets\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux_shakker_labs_union_pro-fp8.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ LoRas\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ navi_flux_v1.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ PuLID\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pulid_flux_v0.9.1.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ Style_Models\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-redux-dev.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ clip\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_l.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_l_TEXT_detail_improved.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ t5xxl_fp16.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ t5xxl_fp8_scaled.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ t5xxl_Q5_K_M.gguf\n‚îÇ   ‚îú‚îÄ‚îÄ clip_vision\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sigclip_vision_patch14_384.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ unet\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Canny\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-canny-dev-fp8.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-canny-dev.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-canny-dev-Q4_0.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-canny-dev-Q5_0.gguf\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Depth\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-depth-dev-fp8.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-depth-dev.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-depth-dev-Q4_0.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-depth-dev-Q5_0.gguf\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dev\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-dev-fp8.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-dev.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-dev-Q3_K_S.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-dev-Q5_K_S.gguf\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Fill\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-fill-dev-fp8.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-fill-dev.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-fill-dev-Q3_K_S.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-fill-dev-Q5_K_S.gguf\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Kontext\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-dev-kontext_fp8_scaled.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-kontext-dev.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-kontext-dev-Q3_K_S.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-kontext-dev-Q5_K_M.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Nunchaku\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ svdq-fp4_r32-flux1-kontext-dev.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ svdq-int4_r32-flux1-kontext-dev.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Schnell\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-schnell-fp8.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-schnell.safetensors\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-schnell-Q3_K_S.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flux1-schnell-Q5_K_S.gguf\n‚îÇ   ‚îú‚îÄ‚îÄ vae\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ae.safetensors\nSDXL\n‚îú‚îÄ‚îÄ SDXL\n‚îÇ   ‚îú‚îÄ‚îÄ checkpoints\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Hyper3d.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sd_xl_base_1.0.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sdxl-6-real-dream.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ checkpoints_inpainting\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Inpainting-Hyper3d.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ clip_vision\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_vision_g.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ controlnet\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xinsir-canny-sdxl-1.0.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xinsir-depth-sdxl-1.0.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xinsir-openpose-sdxl-1.0.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ vae\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sdxl_vae.safetensors\nWan2.2\n‚îú‚îÄ‚îÄ Wan2.2\n‚îÇ   ‚îú‚îÄ‚îÄ clip\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ umt5_xxl_fp8_e4m3fn_scaled.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ umt5_xxl_fp16.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ umt5-xxl-encoder-Q5_K_M.gguf\n‚îÇ   ‚îú‚îÄ‚îÄ vae\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wan_2.1_vae.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ unet_14b\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GGUF\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wan2.2-T2V-A14B-HighNoise-Q3_K_M.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wan2.2-T2V-A14B-HighNoise-Q4_K_M.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wan2.2-T2V-A14B-LowNoise-Q3_K_M.gguf\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wan2.2-T2V-A14B-LowNoise-Q4_K_M.gguf\n‚îÇ   ‚îú‚îÄ‚îÄ loras_14b\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wan2.2-Lightning_T2V-v1.1-A14B-4steps-lora_HIGH_fp16.safetensors\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wan2.2-Lightning_T2V-v1.1-A14B-4steps-lora_LOW_fp16.safetensors\nADetailer\n‚îú‚îÄ‚îÄ Adetailer\n‚îÇ   ‚îú‚îÄ‚îÄ Ultralytics\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bbox\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ face_yolov8m.pt\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hand_yolov8s.pt\n‚îÇ   ‚îú‚îÄ‚îÄ sams\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sam_vit_b_01ec64.pth\nFace Restore\n‚îú‚îÄ‚îÄ FaceRestore_Models\n‚îÇ   ‚îú‚îÄ‚îÄ GFPGANv1.4.pth\n‚îÇ   ‚îú‚îÄ‚îÄ codeformer.pth\nUpscale\n‚îú‚îÄ‚îÄ Upscale_Models\n‚îÇ   ‚îú‚îÄ‚îÄ 4x-UltraSharp.pth\n‚îÇ   ‚îú‚îÄ‚îÄ RealESRGAN_x2plus.pth\nüëÅÔ∏è I See You\nDownloads keep rising‚Ä¶ but with zero new hearts or comments, I have no idea if I'm helping anyone or just feeding a download bot farm in silence.\nIf this helped you out, leave a heart. Otherwise I‚Äôll assume I built this for the void (and a very busy botnet). üòÖ\nNew to ComfyUI or just want clean workflows?\nüì∫ Watch a cool youtube video I made üòÅ ‚Äì If You're Struggling With ComfyUI, Watch This First\nFlux workflows:\nAvailable on my Patreon (free, no paywall):\nüëâ üö® Flux Level 1 and 2 Just Dropped ‚Äì Free Workflow & Guide below ‚¨áÔ∏è\nSDXL workflows:\nAvailable on my Patreon (free, no paywall):\nüëâ üö® SDXL v1.5 Just Dropped ‚Äî Free Workflows Below üëá\nFlux Showcase:\nüß† Depth:\nFlux Depth uses the depth map of an image, letting you prompt for what to fill it with. It's great for keeping the spatial layout while restyling the image however you want.\n‚úèÔ∏è Canny:\nFlux Canny uses an edge map and also lets you prompt freely. It‚Äôs especially good for repurposing parts of an image like the planet in the first example.\nüï∫ OpenPose:\nSelf-explanatory. It‚Äôs great when you don‚Äôt want Depth or Canny getting distracted by other elements in the original image.\nü´† Pulid Face Transfer\nPulid lets you use a face as a reference and prompt as usual. The result will keep a rough match of the reference face. It‚Äôs pretty solid without needing any training, though not perfect. As you can see, it struggled with Aloy‚Äôs hair.\nüîÑ Redux\nUse the style of one image along side your prompt to influence the result.",
    "Trendyol/Trendyol-Cybersecurity-LLM-v2-70B-Q4_K_M": "Trendyol/Trendyol-Cybersecurity-LLM-v2-70B-Q4_K_M (Llama‚Äë3.3‚Äë70B Finetuned)\nüèÜ CS‚ÄëEval Benchmark Results\nüîÅ Why v2-Max?\nüìö v2 Dataset Summary (Alignment-Safe SFT)\n‚ú® Features\nüõ†Ô∏è Quick Start\nTransformers (HF)\nvLLM\nGGUF / llama.cpp\nüß™ Recommended Prompt Patterns\n‚öñÔ∏è Intended Use, Limits, and Safety\nüîç Technical Notes\nüë• Contributions & Contact\nüßæ Changelog\nTrendyol/Trendyol-Cybersecurity-LLM-v2-70B-Q4_K_M (Llama‚Äë3.3‚Äë70B Finetuned)\nv2-Max is a defense-focused, alignment-safe cybersecurity language model based on Llama-3.3-70B. It was trained via SFT from scratch on the v2 dataset and ranked near the top on CS-Eval in the EN and EN-ZH tracks. Developed by the Trendyol Group Security Team.\nDeveloped Trendyol Group Security Team\nAlican Kiraz\nƒ∞smail Yavuz\nMelih Yƒ±lmaz\nMertcan Kondur\nRƒ±za Sabuncu\n√ñzg√ºn Kultekin\nWe thank Ahmet G√∂khan Yal√ßƒ±n, Cenk √áivici, Nezir Alp, Yiƒüit Dar√ßƒ±n  for all their support.\nüèÜ CS‚ÄëEval Benchmark Results\nEnglish: 3rd place\nEnglish‚ÄìChinese: 5th place\nComprehensive average: 91.03\nCategory\nScore\nFundamentals of System Security and Software Security\n91.33\nAccess Control and Identity Management\n90.23\nEncryption Technology and Key Management\n92.70\nInfrastructure Security\n91.85\nAI and Network Security\n94.55\nVulnerability Management and Penetration Testing\n90.78\nThreat Detection and Prevention\n92.44\nData Security and Privacy Protection\n90.48\nSupply Chain Security\n93.69\nSecurity Architecture Design\n87.80\nBusiness Continuity & Emergency Response / Recovery\n85.00\nChinese Task\n91.07\nüîÅ Why v2-Max?\nDifferences: Old model (Qwen3-14B, ‚ÄúBaronLLM v2.0‚Äù) ‚Üí New model (Llama-3.3-70B, ‚Äúv2-Max‚Äù):\nBase model leap: 14B ‚Üí 70B (stronger reasoning, long-range context, and standards-compliant answers).\nDataset: v1.1 (21,258) ‚Üí v2.0 (83,920) rows (‚âà4√ó); coverage includes OWASP, MITRE ATT&CK, NIST CSF, CIS, ASD Essential 8, modern identity (OAuth2/OIDC/SAML), TLS, Cloud & DevSecOps, Cryptography, and AI Security.\nSecurity gates: adversarial refusal tests against jailbreak/prompt injection, static policy linting, content risk taxonomy; refuse-or-report strategies in system prompts.\nOrientation: offensive examples were removed; defensive, safe outputs are preferred.\nüìö v2 Dataset Summary (Alignment-Safe SFT)\nSize: 83,920 system/user/assistant triplets\nLicense: Apache-2.0\nLanguage: English\nSplit: train (100%)\nFormat: Parquet (columns: system, user, assistant)\nQuality Gates: Deduplication, PII scrubbing, hallucination screening, adversarial refusal tests, static policy linting, risk taxonomy, strict schema validations (stable row IDs).\nCoverage (key topics):\nOWASP Top 10, MITRE ATT&CK, NIST CSF, CIS Controls, ASD Essential 8\nModern identity: OAuth 2.0 / OIDC / SAML\nSSL/TLS practices and certificate hygiene\nCloud & DevSecOps: IAM, secrets management, CI/CD, container/K8s hardening, logging/SIEM, IR runbooks\nCryptography implementation hygiene\nAI Security (prompt injection, data poisoning, model/embedding security, etc.)\nThe dataset is commercial-friendly: Apache-2.0. Harmful/exploitative content and high-risk materials such as raw shellcode were excluded during dataset construction; patterns for generating safe alternatives to harmful requests were included.\n‚ú® Features\nDefense-Focused Reasoning: Standards-aligned (OWASP/ATT&CK/NIST/CIS) recommendations with step-by-step explanations that include the ‚Äúwhy/evidence.‚Äù\nPolicy & Architecture Guides: Identity/access, encryption, network segmentation, cloud control sets, data classification, and alert logic.\nIR & Threat Hunting Support: Incident flow, triage checklists, safe log query patterns, playbook skeletons.\nCloud & DevSecOps: CI/CD security gates, IaC misconfiguration patterns, K8s hardening checklists.\nRefusal-by-Design: Safe alternatives and compliance-aligned response patterns for exploitative/malicious prompts.\nNote: This model is SFT-based; it has no tool use, web browsing, or access to an executable environment. In expert operations it is an assistant; it does not, by itself, constitute evidence.\nüõ†Ô∏è Quick Start\nTransformers (HF)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"Trendyol/Trendyol-Cybersecurity-LLM-v2-70B-Q4_K_M\"\ntok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ndef generate(prompt, max_new_tokens=512, temperature=0.3, top_p=0.9):\ninputs = tok(prompt, return_tensors=\"pt\").to(model.device)\nout = model.generate(**inputs, max_new_tokens=max_new_tokens,\ndo_sample=(temperature>0), temperature=temperature, top_p=top_p)\nreturn tok.decode(out[0], skip_special_tokens=True)\nprint(generate(\"Map a PCI DSS v4.0 compliance checklist for a multi-account AWS environment.\"))\nvLLM\nvllm serve Trendyol/Trendyol-Cybersecurity-LLM-v2-70B-Q4_K_M --dtype bfloat16\nGGUF / llama.cpp\nFor practical single-machine use with a 70B model, Q4_K_M is recommended; higher-quality GGUF variants require much more memory.\nüß™ Recommended Prompt Patterns\nGoal\nTemplate\nNote\nRisk Assessment\nROLE: Security Architect\\nCONTEXT: ...\\nTASK: Produce a control-by-control gap analysis against NIST CSF v2.0...\ntemperature=0.2‚Äì0.4, top_p=0.9\nIR Playbook\nCreate a phase-by-phase incident response playbook for ransomware in hybrid (Azure+on-prem) AD.\nAsk for a validation checklist in the final step.\nIdentity Security\nPropose hardened OAuth2/OIDC configs for a multi-tenant SPA+API.\nRequest an \"abuse cases\" list at the end of the response.\nCloud Guardrails\nGenerate K8s hardening controls mapped to CIS + NSA/CISA.\nInclude ‚Äúrationale‚Äù and ‚Äúvalidation‚Äù columns.\n‚öñÔ∏è Intended Use, Limits, and Safety\nIntended Use: Enterprise defense consulting, architectural guidance, control mappings, IR/TI support outputs, training, and documentation.\nProhibited / Limitations:\nUnauthorized penetration testing, PoC exploit/payload generation, instructions that harm live systems.\nTraining or enriching outputs with personal data or confidential information.\nModel Behavior:\nExplicit refusal for harmful requests + safe alternative suggestions.\nContent policies are tested within the dataset (against jailbreak/prompt injection).\nDisclaimer: Model outputs should not be applied automatically without expert review; corporate policy and regulation take precedence.\nüîç Technical Notes\nTraining Method: SFT (instruction tuning) + system-prompt guardrails.\nContext Window: Llama-3.3-70B‚Äôs default (same as Meta‚Äôs release).\nLanguages: Training language is English; EN-ZH evaluation is supported.\nRelease: Weights under the Meta Llama 3.3 License; dataset under Apache-2.0.\nüë• Contributions & Contact\nIssues / Feedback: HF or GitHub Issues\nResearch Collaboration: Trendyol Group Security Team\nAcknowledgments: Trendyol Security Team, community contributors, and independent evaluators\nüßæ Changelog\n2025-10-06 ‚Äî v2-Max: Migration to the Llama-3.3-70B base; v2 dataset with 83,920 rows; alignment security gates; CS-Eval EN #3 / EN-ZH #5, average score 91.03.",
    "nvidia/canary-qwen-2.5b": "Model Overview\nDescription:\nLicense/Terms of Use:\nReferences:\nDeployment Geography:\nUse Case:\nRelease Date:\nModel Architecture:\nLimitations\nNVIDIA NeMo\nHow to Use this Model\nLoading the Model\nInput:\nOutput:\nSoftware Integration:\nModel Version(s):\nTraining\nTraining and Evaluation Datasets:\nTraining Dataset:\nProperties\nEvaluation Dataset:\nPerformance\nASR Performance (w/o PnC)\nHallucination Robustness\nNoise Robustness\nModel Fairness Evaluation\nGender Bias:\nAge Bias:\nInference:\nEthical Considerations:\n|\n|\nModel Overview\nDescription:\nNVIDIA NeMo Canary-Qwen-2.5B is an English speech recognition model that achieves state-of-the art performance on multiple English speech benchmarks. With 2.5 billion parameters and running at 418 RTFx, Canary-Qwen-2.5B supports automatic speech-to-text recognition (ASR) in English with punctuation and capitalization (PnC). The model works in two modes: as a transcription tool (ASR mode) and as an LLM (LLM mode). In ASR mode, the model is only capable of transcribing the speech into text, but does not retain any LLM-specific skills such as reasoning. In LLM mode, the model retains all of the original LLM capabilities, which can be used to post-process the transcript, e.g. summarize it or answer questions about it. In LLM mode, the model does not \"understand\" the raw audio anymore - only its transcript. This model is ready for commercial use.\nLicense/Terms of Use:\nCanary-Qwen-2.5B is released under the CC-BY-4.0 license. By using this model, you are agreeing to the terms and conditions of the license.\nReferences:\n[1] Less is More: Accurate Speech Recognition & Translation without Web-Scale Data\n[2] Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\n[3] Attention Is All You Need\n[4] Qwen/Qwen3-1.7B Model Card\n[5] Training and Inference Efficiency of Encoder-Decoder Speech Models\n[6] NVIDIA NeMo Toolkit\n[7] Granary: Speech Recognition and Translation Dataset in 25 European Languages\n[8] Towards Measuring Fairness in AI: the Casual Conversations Dataset\n[9] SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation\nDeployment Geography:\nGlobal\nUse Case:\nThe model is intended for users requiring speech-to-text transcription capabilities for English speech, and/or transcript post-processing capabilities enabled by prompting the underlying LLMs. Typical use-cases: transcription, summarization, answering user questions about the transcript.\nRelease Date:\nHuggingface 07/17/2025 via https://huggingface.co/nvidia/canary-qwen-2.5b\nModel Architecture:\nCanary-Qwen is a Speech-Augmented Language Model (SALM) [9] model with FastConformer [2] Encoder and Transformer Decoder [3]. It is built using two base models: nvidia/canary-1b-flash [1,5] and Qwen/Qwen3-1.7B [4], a linear projection, and low-rank adaptation (LoRA) applied to the LLM. The audio encoder computes audio representation that is mapped to the LLM embedding space via a linear projection, and concatenated with the embeddings of text tokens. The model is prompted with \"Transcribe the following: \", using Qwen's chat template.\nLimitations\nInput length. The maximum audio duration in training was 40s, and the maximum token sequence length was 1024 tokens (including prompt, audio, and response). The model may technically be able to process longer sequences, but its accuracy may be degraded.\nExclusively ASR oriented capabilities. The model is not expected to preserve any of the underlying LLM's capabilities into speech modality.\nEnglish-only language support. The model was trained using English data only. It may be able to spuriously transcribe other languages as the underlying encoder was pretrained using German, French, and Spanish speech in addition to English, but it's unlikely to be reliable as a multilingual model.\nNVIDIA NeMo\nTo train, fine-tune or transcribe with Canary-Qwen-2.5B, you will need to install NVIDIA NeMo.\n# Currently requires installing the latest trunk version of NeMo, and PyTorch 2.6+ for FSDP2 support.\npython -m pip install \"nemo_toolkit[asr,tts] @ git+https://github.com/NVIDIA/NeMo.git\"\nHow to Use this Model\nThe model is available for use in the NVIDIA NeMo toolkit [6], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\nLoading the Model\nfrom nemo.collections.speechlm2.models import SALM\nmodel = SALM.from_pretrained('nvidia/canary-qwen-2.5b')\nInput:\nInput Type(s): Audio, text prompt\nInput Format(s): Audio: .wav or .flac files. Text prompt string for ASR mode: Transcribe the following: <|audioplaceholder|>\nInput Parameters(s): Audio: Two-Dimensional (batch, audio-samples); Text: One-Dimensional (string)\nOther Properties Related to Input: 16000 Hz Mono-channel Audio, Pre-Processing Not Needed\nInput to Canary-Qwen-2.5B is a batch of prompts that include audio.\nExample usage in ASR mode (speech-to-text):\nanswer_ids = model.generate(\nprompts=[\n[{\"role\": \"user\", \"content\": f\"Transcribe the following: {model.audio_locator_tag}\", \"audio\": [\"speech.wav\"]}]\n],\nmax_new_tokens=128,\n)\nprint(model.tokenizer.ids_to_text(answer_ids[0].cpu()))\nExample usage in LLM mode (text-only):\nprompt = \"...\"\ntranscript = \"...\"\nwith model.llm.disable_adapter():\nanswer_ids = model.generate(\nprompts=[[{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{transcript}\"}]],\nmax_new_tokens=2048,\n)\nTo transcribe a dataset of recordings, specify the input as jsonl manifest file, where each line in the file is a dictionary containing the following fields:\n# Example of a line in input_manifest.json\n{\n\"audio_filepath\": \"/path/to/audio.wav\",  # path to the audio file\n\"duration\": 30.0,  # duration of the audio\n}\nand then use:\ncd NeMo\npython examples/speechlm2/salm_generate.py \\\npretrained_name=nvidia/canary-qwen-2.5b \\\ninputs=input_manifest.json \\\noutput_manifest=generations.jsonl \\\nbatch_size=128 \\\nuser_prompt=\"Transcribe the following:\"  # audio locator is added automatically at the end if not present\nOutput:\nOutput Type(s): Text\nOutput Format: Text transcript as a sequence of token IDs or a string\nOutput Parameters: One-Dimensional text string\nOther Properties Related to Output: May Need Inverse Text Normalization\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nRuntime Engine(s):\nNeMo - 2.5.0 or higher\nSupported Hardware Microarchitecture Compatibility:\n[NVIDIA Ampere]\n[NVIDIA Blackwell]\n[NVIDIA Jetson]\n[NVIDIA Hopper]\n[NVIDIA Lovelace]\n[NVIDIA Pascal]\n[NVIDIA Turing]\n[NVIDIA Volta]\n[Preferred/Supported] Operating System(s):\n[Linux]\n[Linux 4 Tegra]\n[Windows]\nModel Version(s):\nCanary-Qwen-2.5B\nTraining\nCanary-Qwen-2.5B was trained using the NVIDIA NeMo toolkit [6] for a total of 90k steps on 32 NVIDIA A100 80GB GPUs. LLM parameters were kept frozen. Speech encoder, projection, and LoRA parameters were trainable. The encoder's output frame rate is 80ms, or 12.5 tokens per second. The model was trained on approximately 1.3B tokens in total (this number inlcudes the speech encoder output frames, text response tokens, prompt tokens, and chat template tokens).\nThe model can be trained using this example script and base config.\nThe tokenizer was inherited from Qwen/Qwen3-1.7B.\nTraining and Evaluation Datasets:\nTraining Dataset:\n** The total size (in number of data points): approx. 40 million (speech, text) pairs\n** Total number of datasets: 26, with 18 for training and 8 for test\n** Dataset partition: Training 99.6%, testing 0.04%, validation 0%\n** Time period for training data collection: 1990-2025\n** Time period for testing data collection: 2005-2022\n** Time period for validation data collection N/A (unused)\nThe Canary-Qwen-2.5B model is trained on a total of 234K hrs of publicly available speech data.\nThe datasets below include conversations, videos from the web and audiobook recordings.\nData Collection Method:\nHuman\nLabeling Method:\nHybrid: Human, Automated\nProperties\nEnglish (234.5k hours)\nThe majority of the training data comes from the English portion of the Granary dataset [7]:\nYouTube-Commons (YTC) (109.5k hours)\nYODAS2 (77k hours)\nLibriLight (13.6k hours)\nIn addition, the following datasets were used:\nLibrispeech 960 hours\nFisher Corpus\nSwitchboard-1 Dataset\nWSJ-0 and WSJ-1\nNational Speech Corpus (Part 1, Part 6)\nVCTK\nVoxPopuli (EN)\nEuroparl-ASR (EN)\nMultilingual Librispeech (MLS EN)\nMozilla Common Voice (v11.0)\nMozilla Common Voice (v7.0)\nMozilla Common Voice (v4.0)\nAMI\nFLEURS\nAMI was oversampled during model training to constitute about 15% of the total data observed.\nThis skewed the model towards predicting verbatim transcripts that include conversational speech disfluencies such as repetitions.\nThe training transcripts contained punctuation and capitalization.\nEvaluation Dataset:\nData Collection Method:\nHuman\nLabeling Method:\nHuman\nAutomatic Speech Recognition:\nHuggingFace OpenASR Leaderboard evaluation sets\nHallucination Robustness:\nMUSAN 48 hrs eval set\nNoise Robustness:\nLibrispeech\nModel Fairness:\nCasual Conversations Dataset\nPerformance\nThe ASR predictions were generated using greedy decoding.\nASR Performance (w/o PnC)\nThe ASR performance is measured with word error rate (WER), and we process the groundtruth and predicted text with whisper-normalizer version 0.1.12.\nWER on HuggingFace OpenASR leaderboard:\nVersion\nModel\nRTFx\nMean\nAMI\nGigaSpeech\nLS Clean\nLS Other\nEarnings22\nSPGISpech\nTedlium\nVoxpopuli\n2.5.0\nCanary-Qwen-2.5B\n418\n5.63\n10.18\n9.41\n1.60\n3.10\n10.42\n1.90\n2.72\n5.66\nMore details on evaluation can be found at HuggingFace ASR Leaderboard\nHallucination Robustness\nNumber of characters per minute on MUSAN 48 hrs eval set (max_new_tokens=50 following nvidia/canary-1b-flash evaluation)\nVersion\nModel\n# of character per minute\n2.5.0\nCanary-Qwen-2.5B\n138.1\nNoise Robustness\nWER on Librispeech Test Clean at different SNR (signal to noise ratio) levels of additive white noise\nVersion\nModel\nSNR 10\nSNR 5\nSNR 0\nSNR -5\n2.5.0\nCanary-Qwen-2.5B\n2.41%\n4.08%\n9.83%\n30.60%\nModel Fairness Evaluation\nAs outlined in the paper \"Towards Measuring Fairness in AI: the Casual Conversations Dataset\" [8], we assessed the Canary-Qwen-2.5B model for fairness. The model was evaluated on the CasualConversations-v1 dataset with inference done on non-overlapping 40s chunks, and the results are reported as follows:\nGender Bias:\nGender\nMale\nFemale\nN/A\nOther\nNum utterances\n18471\n23378\n880\n18\n% WER\n16.71\n13.85\n17.71\n29.46\nAge Bias:\nAge Group\n(18-30)\n(31-45)\n(46-85)\n(1-100)\nNum utterances\n15058\n13984\n12810\n41852\n% WER\n15.73\n15.3\n14.14\n15.11\n(Error rates for fairness evaluation are determined by normalizing both the reference and predicted text, similar to the methods used in the evaluations found at https://github.com/huggingface/open_asr_leaderboard.)\nInference:\nEngine: NVIDIA NeMo\nTest Hardware :\nA6000\nA100\nRTX 5090\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concerns here.",
    "Wan-AI/Wan2.2-I2V-A14B": "Wan2.2\nVideo Demos\nüî• Latest News!!\nCommunity Works\nüìë Todo List\nRun Wan2.2\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.2\nüíú Wan ¬†¬† ÔΩú ¬†¬† üñ•Ô∏è GitHub ¬†¬†  | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Technical Report ¬†¬† | ¬†¬† üìë Blog ¬†¬† | ¬†¬†üí¨ WeChat Group¬†¬† | ¬†¬† üìñ Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nüëç Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nüëç Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nüëç Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nüëç Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nThis repository also includes our I2V-A14B model, designed for image-to-video generation, supporting both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it achieves more stable video synthesis with reduced unrealistic camera movements and offers enhanced support for diverse stylized scenes.\nVideo Demos\nYour browser does not support the video tag.\nüî• Latest News!!\nJul 28, 2025: üëã Wan2.1 has been integrated into ComfyUI (CN | EN). Enjoy!\nJul 28, 2025: üëã Wan2.2's T2V, I2V and TI2V have been integrated into Diffusers (T2V-A14B | I2V-A14B | TI2V-5B). Feel free to give it a try!\nJul 28, 2025: üëã We've released the inference code and model weights of Wan2.2.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\nüìë Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nü§ó Huggingface     ü§ñ ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nüí°Note:\nThe TI2V-5B model supports 720P video generation at 24 FPS.\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-I2V-A14B --local-dir ./Wan2.2-I2V-A14B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-I2V-A14B --local_dir ./Wan2.2-I2V-A14B\nRun Image-to-Video Generation\nThis repository supports the `Wan2.2-I2V-A14B`` Image-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\nSingle-GPU inference\npython generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --offload_model True --convert_model_dtype --image examples/i2v_input.JPG --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nThis command can run on a GPU with at least 80GB VRAM.\nüí°For the Image-to-Video task, the size parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nImage-to-Video Generation without prompt\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --prompt '' --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --use_prompt_extend --prompt_extend_method 'dashscope'\nüí°The model can generate videos solely from the input image. You can use prompt extension to generate prompt from the image.\nThe process of prompt extension can be referenced here.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "QuantStack/Wan2.2-I2V-A14B-GGUF": "This GGUF file is a direct conversion of Wan-AI/Wan2.2-I2V-A14B\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96\nPlace model files in ComfyUI/models/unet see the GitHub readme for further installation instructions.",
    "arcee-ai/AFM-4.5B-Base": "AFM-4.5B-Base\nModel Details\nBenchmarks\nHow to use with transformers\nLicense\nAFM-4.5B-Base\nAFM-4.5B-Base is a 4.5 billion parameter instruction-tuned model developed by Arcee.ai, designed for enterprise-grade performance across diverse deployment environments from cloud to edge. The base model was trained on a dataset of 8 trillion tokens, comprising 6.5 trillion tokens of general pretraining data followed by 1.5 trillion tokens of midtraining data with enhanced focus on mathematical reasoning and code generation. Following pretraining, the model underwent supervised fine-tuning on high-quality instruction datasets. The instruction-tuned model was further refined through reinforcement learning on verifiable rewards as well as for human preference. We use a modified version of TorchTitan for pretraining, Axolotl for supervised fine-tuning, and a modified version of Verifiers for reinforcement learning.\nThe development of AFM-4.5B prioritized data quality as a fundamental requirement for achieving robust model performance. We collaborated with DatologyAI, a company specializing in large-scale data curation. DatologyAI's curation pipeline integrates a suite of proprietary algorithms‚Äîmodel-based quality filtering, embedding-based curation, target distribution-matching, source mixing, and synthetic data. Their expertise enabled the creation of a curated dataset tailored to support strong real-world performance.\nThe model architecture follows a standard transformer decoder-only design based on Vaswani et al., incorporating several key modifications for enhanced performance and efficiency. Notable architectural features include grouped query attention for improved inference efficiency and ReLU^2 activation functions instead of SwiGLU to enable sparsification while maintaining or exceeding performance benchmarks.\nThe model available in this repo is the base model following merging and context extension.\nModel Details\nModel Architecture: ArceeForCausalLM\nParameters: 4.5B\nTraining Tokens: 8T\nLicense: Apache-2.0\nBenchmarks\nHow to use with transformers\nYou can use the model directly with the transformers library.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"arcee-ai/AFM-4.5B-Base\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nprompt = \"Once upon a time \"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n# Generate text\noutputs = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95\n)\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(generated_text)\nLicense\nAFM-4.5B is released under the Apache-2.0 license.",
    "Qwen/Qwen3-4B-Thinking-2507": "Qwen3-4B-Thinking-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nQwen3-4B-Thinking-2507\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-4B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-4B-Thinking-2507, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nQwen3-4B-Thinking-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 4.0B\nNumber of Paramaters (Non-Embedding): 3.6B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 32 for Q and 8 for KV\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode. Meanwhile, specifying enable_thinking=True is no longer required.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nQwen3-30B-A3B Thinking\nQwen3-4B Thinking\nQwen3-4B-Thinking-2507\nKnowledge\nMMLU-Pro\n78.5\n70.4\n74.0\nMMLU-Redux\n89.5\n83.7\n86.1\nGPQA\n65.8\n55.9\n65.8\nSuperGPQA\n51.8\n42.7\n47.8\nReasoning\nAIME25\n70.9\n65.6\n81.3\nHMMT25\n49.8\n42.1\n55.5\nLiveBench 20241125\n74.3\n63.6\n71.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n57.4\n48.4\n55.2\nCFEval\n1940\n1671\n1852\nOJBench\n20.7\n16.1\n17.9\nAlignment\nIFEval\n86.5\n81.9\n87.4\nArena-Hard v2$\n36.3\n13.7\n34.9\nCreative Writing v3\n79.1\n61.1\n75.6\nWritingBench\n77.0\n73.5\n83.3\nAgent\nBFCL-v3\n69.1\n65.9\n71.2\nTAU1-Retail\n61.7\n33.9\n66.1\nTAU1-Airline\n32.0\n32.0\n48.0\nTAU2-Retail\n34.2\n38.6\n53.5\nTAU2-Airline\n36.0\n28.0\n58.0\nTAU2-Telecom\n22.8\n17.5\n27.2\nMultilingualism\nMultiIF\n72.2\n66.3\n77.3\nMMLU-ProX\n73.1\n61.0\n64.2\nINCLUDE\n71.9\n61.8\n64.4\nPolyMATH\n46.1\n40.0\n46.2\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Thinking-2507 --context-length 262144  --reasoning-parser deepseek-r1\nvLLM:vllm serve Qwen/Qwen3-4B-Thinking-2507 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-4B-Thinking-2507 --served-model-name Qwen3-4B-Thinking-2507 --max-model-len 262144`.\nllm_cfg = {\n'model': 'Qwen3-4B-Thinking-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n'api_key': 'EMPTY',\n'generate_cfg': {\n'thought_in_content': True,\n},\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "openbmb/MiniCPM-V-4_5": "MiniCPM-V 4.5\nKey Techniques\nEvaluation\nInference Efficiency\nExamples\nChat with Image\nChat with Video\nChat with multiple images\nIn-context few-shot learning\nModel License\nStatement\nFramework Support Matrix\nChat with Image\nChat with Video\nChat with multiple images\nIn-context few-shot learning\nModel License\nStatement\nUsage\nChat with Image\nChat with Video\nChat with multiple images\nIn-context few-shot learning\nModel License\nStatement\nLicense\nModel License\nStatement\nKey Techniques and Other Multimodal Projects\nCitation\nA GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone\nGitHub | CookBook | Technical Report | Demo\nMiniCPM-V 4.5\nMiniCPM-V 4.5 is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:\nüî• State-of-the-art Vision-Language Capability.\nMiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B for vision-language capabilities, making it the most performant MLLM under 30B parameters.\nüé¨ Efficient High-FPS and Long Video Understanding. Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.\n‚öôÔ∏è Controllable Hybrid Fast/Deep Thinking. MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.\nüí™ Strong OCR, Document Parsing and Others.\nBased on LLaVA-UHD architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x less visual tokens than most MLLMs. The model achieves leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, outperforming GPT-4o-latest on MMHal-Bench, and supports multilingual capabilities in more than 30 languages.\nüí´ Easy Usage.\nMiniCPM-V 4.5 can be easily used in various ways: (1) llama.cpp and ollama support for efficient CPU inference on local devices, (2) int4, GGUF and AWQ format quantized models in 16 sizes, (3) SGLang and vLLM support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with Transformers and LLaMA-Factory, (5) quick local WebUI demo, (6) optimized local iOS app on iPhone and iPad, and (7) online web demo on server. See our Cookbook for full usages!\nKey Techniques\nArchitechture: Unified 3D-Resampler for High-density Video Compression. MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96√ó compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.\nPre-training: Unified Learning for OCR and Knowledge from Documents. Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.\nPost-training: Hybrid Fast/Deep Thinking with Multimodal RL. MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with RLPR and RLAIF-V, it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.\nEvaluation\nInference Efficiency\nOpenCompass\nModel\nSize\nAvg Score ‚Üë\nTotal Inference Time ‚Üì\nGLM-4.1V-9B-Thinking\n10.3B\n76.6\n17.5h\nMiMo-VL-7B-RL\n8.3B\n76.4\n11h\nMiniCPM-V 4.5\n8.7B\n77.0\n7.5h\nVideo-MME\nModel\nSize\nAvg Score ‚Üë\nTotal Inference Time ‚Üì\nGPU Mem ‚Üì\nQwen2.5-VL-7B-Instruct\n8.3B\n71.6\n3h\n60G\nGLM-4.1V-9B-Thinking\n10.3B\n73.6\n2.63h\n32G\nMiniCPM-V 4.5\n8.7B\n73.5\n0.26h\n28G\nBoth Video-MME and OpenCompass were evaluated using 8√óA100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.\nExamples\nWe deploy MiniCPM-V 4.5 on iPad M4 with iOS demo. The demo video is the raw screen recording without editing.\nFramework Support Matrix\nCategory\nFramework\nCookbook Link\nUpstream PR\nSupported since(branch)\nSupported since(release)\nEdge(On-device)\nLlama.cpp\nLlama.cpp Doc\n#15575(2025-08-26)\nmaster(2025-08-26)\nb6282\nOllama\nOllama Doc\n#12078(2025-08-26)\nMerging\nWaiting for official release\nServing(Cloud)\nvLLM\nvLLM Doc\n#23586(2025-08-26)\nmain(2025-08-27)\nv0.10.2\nSGLang\nSGLang Doc\n#9610(2025-08-26)\nMerging\nWaiting for official release\nFinetuning\nLLaMA-Factory\nLLaMA-Factory Doc\n#9022(2025-08-26)\nmain(2025-08-26)\nWaiting for official release\nQuantization\nGGUF\nGGUF Doc\n‚Äî\n‚Äî\n‚Äî\nBNB\nBNB Doc\n‚Äî\n‚Äî\n‚Äî\nAWQ\nAWQ Doc\n‚Äî\n‚Äî\n‚Äî\nDemos\nGradio Demo\nGradio Demo Doc\n‚Äî\n‚Äî\n‚Äî\nNote: If you'd like us to prioritize support for another open-source framework, please let us know via this short form.\nUsage\nIf you wish to enable thinking mode, provide the argument enable_thinking=True to the chat function.\nChat with Image\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\ntorch.manual_seed(100)\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6\nimage = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')\nenable_thinking=False # If `enable_thinking=True`, the thinking mode is enabled.\nstream=True # If `stream=True`, the answer is string\n# First round chat\nquestion = \"What is the landform in the picture?\"\nmsgs = [{'role': 'user', 'content': [image, question]}]\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nenable_thinking=enable_thinking,\nstream=True\n)\ngenerated_text = \"\"\nfor new_text in answer:\ngenerated_text += new_text\nprint(new_text, flush=True, end='')\n# Second round chat, pass history context of multi-turn conversation\nmsgs.append({\"role\": \"assistant\", \"content\": [generated_text]})\nmsgs.append({\"role\": \"user\", \"content\": [\"What should I pay attention to when traveling here?\"]})\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nstream=True\n)\ngenerated_text = \"\"\nfor new_text in answer:\ngenerated_text += new_text\nprint(new_text, flush=True, end='')\nYou will get the following output:\n# round1\nThe landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleys‚Äîexactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.\nThis scene closely resembles the famous karst landscape of Guilin and Yangshuo in China‚Äôs Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.\n# round2\nWhen traveling to a karst landscape like this, here are some important tips:\n1. Wear comfortable shoes: The terrain can be uneven and hilly.\n2. Bring water and snacks for energy during hikes or boat rides.\n3. Protect yourself from the sun with sunscreen, hats, and sunglasses‚Äîespecially since you‚Äôll likely spend time outdoors exploring scenic spots.\n4. Respect local customs and nature regulations by not littering or disturbing wildlife.\nBy following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilin‚Äôs karst mountains.\nChat with Video\n## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids.\n# To achieve this, you need to organize your video data into two corresponding sequences:\n#   frames: List[Image]\n#   temporal_ids: List[List[Int]].\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\nfrom scipy.spatial import cKDTree\nimport numpy as np\nimport math\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6\nMAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.\nMAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6\nTIME_SCALE = 0.1\ndef map_to_nearest_scale(values, scale):\ntree = cKDTree(np.asarray(scale)[:, None])\n_, indices = tree.query(np.asarray(values)[:, None])\nreturn np.asarray(scale)[indices]\ndef group_array(arr, size):\nreturn [arr[i:i+size] for i in range(0, len(arr), size)]\ndef encode_video(video_path, choose_fps=3, force_packing=None):\ndef uniform_sample(l, n):\ngap = len(l) / n\nidxs = [int(i * gap + gap / 2) for i in range(n)]\nreturn [l[i] for i in idxs]\nvr = VideoReader(video_path, ctx=cpu(0))\nfps = vr.get_avg_fps()\nvideo_duration = len(vr) / fps\nif choose_fps * int(video_duration) <= MAX_NUM_FRAMES:\npacking_nums = 1\nchoose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))\nelse:\npacking_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)\nif packing_nums <= MAX_NUM_PACKING:\nchoose_frames = round(video_duration * choose_fps)\nelse:\nchoose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)\npacking_nums = MAX_NUM_PACKING\nframe_idx = [i for i in range(0, len(vr))]\nframe_idx =  np.array(uniform_sample(frame_idx, choose_frames))\nif force_packing:\npacking_nums = min(force_packing, MAX_NUM_PACKING)\nprint(video_path, ' duration:', video_duration)\nprint(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')\nframes = vr.get_batch(frame_idx).asnumpy()\nframe_idx_ts = frame_idx / fps\nscale = np.arange(0, video_duration, TIME_SCALE)\nframe_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE\nframe_ts_id = frame_ts_id.astype(np.int32)\nassert len(frames) == len(frame_ts_id)\nframes = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]\nframe_ts_id_group = group_array(frame_ts_id, packing_nums)\nreturn frames, frame_ts_id_group\nvideo_path=\"video_test.mp4\"\nfps = 5 # fps for video\nforce_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.\nframes, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)\nquestion = \"Describe the video\"\nmsgs = [\n{'role': 'user', 'content': frames + [question]},\n]\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nuse_image_id=False,\nmax_slice_nums=1,\ntemporal_ids=frame_ts_id_group\n)\nprint(answer)\nChat with multiple images\nClick to show Python code running MiniCPM-V 4.5 with multiple images input.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(answer)\nIn-context few-shot learning\nClick to view Python code running MiniCPM-V 4.5 with few-shot input.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16)\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)\nquestion = \"production date\"\nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\nmsgs = [\n{'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n{'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n{'role': 'user', 'content': [image_test, question]}\n]\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(answer)\nLicense\nModel License\nThe MiniCPM-o/V model weights and code are open-sourced under the Apache-2.0 license.\nTo help us better understand and support our users, we would deeply appreciate it if you could consider optionally filling out a brief registration \"questionnaire\".\nStatement\nAs an LMM, MiniCPM-V 4.5 generates contents by learning a large amount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V 4.5 does not represent the views and positions of the model developers\nWe will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\nKey Techniques and Other Multimodal Projects\nüëè Welcome to explore key techniques of MiniCPM-V 4.5 and other multimodal projects of our team:\nVisCPM | RLPR |  RLHF-V | LLaVA-UHD  | RLAIF-V\nCitation\nIf you find our work helpful, please consider citing our papers üìù and liking this project ‚ù§Ô∏èÔºÅ\n@misc{yu2025minicpmv45cookingefficient,\ntitle={MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe},\nauthor={Tianyu Yu and Zefan Wang and Chongyi Wang and Fuwei Huang and Wenshuo Ma and Zhihui He and Tianchi Cai and Weize Chen and Yuxiang Huang and Yuanqian Zhao and Bokai Xu and Junbo Cui and Yingjing Xu and Liqing Ruan and Luoyuan Zhang and Hanyu Liu and Jingkun Tang and Hongyuan Liu and Qining Guo and Wenhao Hu and Bingxiang He and Jie Zhou and Jie Cai and Ji Qi and Zonghao Guo and Chi Chen and Guoyang Zeng and Yuxuan Li and Ganqu Cui and Ning Ding and Xu Han and Yuan Yao and Zhiyuan Liu and Maosong Sun},\nyear={2025},\neprint={2509.18154},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2509.18154},\n}\n@article{yao2024minicpm,\ntitle={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\nauthor={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\njournal={Nat Commun 16, 5509 (2025)},\nyear={2025}\n}",
    "befox/WAN2.2-14B-Rapid-AllInOne-GGUF": "GGUF version of Phr00t/WAN2.2-14B-Rapid-AllInOne",
    "Qwen/Qwen3-Next-80B-A3B-Instruct": "Qwen3-Next-80B-A3B-Instruct\nHighlights\nModel Overview\nPerformance\nQuickstart\nDeployment\nSGLang\nvLLM\nAgentic Use\nProcessing Ultra-Long Texts\nBest Practices\nCitation\nQwen3-Next-80B-A3B-Instruct\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI).\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture.\nWe call this next-generation foundation models Qwen3-Next.\nHighlights\nQwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enchancements:\nHybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\nHigh-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\nStability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\nMulti-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference.\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\nQwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\nQwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\nFor more details, please refer to our blog post Qwen3-Next.\nModel Overview\nQwen3-Next-80B-A3B-Instruct supports only instruct (non-thinking) mode and does not generate <think></think> blocks in its output.\nQwen3-Next-80B-A3B-Instruct has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining (15T tokens) & Post-training\nNumber of Parameters: 80B in total and 3B activated\nNumber of Paramaters (Non-Embedding): 79B\nHidden Dimension: 2048\nNumber of Layers: 48\nHybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> 1 * (Gated Attention -> MoE))\nGated Attention:\nNumber of Attention Heads: 16 for Q and 2 for KV\nHead Dimension: 256\nRotary Position Embedding Dimension: 64\nGated DeltaNet:\nNumber of Linear Attention Heads: 32 for V and 16 for QK\nHead Dimension: 128\nMixture of Experts:\nNumber of Experts: 512\nNumber of Activated Experts: 10\nNumber of Shared Experts: 1\nExpert Intermediate Dimension: 512\nContext Length: 262,144 natively and extensible up to 1,010,000 tokens\nPerformance\nQwen3-30B-A3B-Instruct-2507\nQwen3-32B Non-Thinking\nQwen3-235B-A22B-Instruct-2507\nQwen3-Next-80B-A3B-Instruct\nKnowledge\nMMLU-Pro\n78.4\n71.9\n83.0\n80.6\nMMLU-Redux\n89.3\n85.7\n93.1\n90.9\nGPQA\n70.4\n54.6\n77.5\n72.9\nSuperGPQA\n53.4\n43.2\n62.6\n58.8\nReasoning\nAIME25\n61.3\n20.2\n70.3\n69.5\nHMMT25\n43.0\n9.8\n55.4\n54.1\nLiveBench 20241125\n69.0\n59.8\n75.4\n75.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n43.2\n29.1\n51.8\n56.6\nMultiPL-E\n83.8\n76.9\n87.9\n87.8\nAider-Polyglot\n35.6\n40.0\n57.3\n49.8\nAlignment\nIFEval\n84.7\n83.2\n88.7\n87.6\nArena-Hard v2*\n69.0\n34.1\n79.2\n82.7\nCreative Writing v3\n86.0\n78.3\n87.5\n85.3\nWritingBench\n85.5\n75.4\n85.2\n87.3\nAgent\nBFCL-v3\n65.1\n63.0\n70.9\n70.3\nTAU1-Retail\n59.1\n40.1\n71.3\n60.9\nTAU1-Airline\n40.0\n17.0\n44.0\n44.0\nTAU2-Retail\n57.0\n48.8\n74.6\n57.3\nTAU2-Airline\n38.0\n24.0\n50.0\n45.5\nTAU2-Telecom\n12.3\n24.6\n32.5\n13.2\nMultilingualism\nMultiIF\n67.9\n70.7\n77.5\n75.8\nMMLU-ProX\n72.0\n69.3\n79.4\n76.7\nINCLUDE\n71.9\n70.9\n79.5\n78.9\nPolyMATH\n43.1\n22.5\n50.2\n45.9\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\nQuickstart\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face transformers.\npip install git+https://github.com/huggingface/transformers.git@main\nWith earlier versions, you will encounter the following error:\nKeyError: 'qwen3_next'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384,\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nMulti-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\nThe efficiency or throughput improvement depends highly on the implementation.\nIt is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\nDepending on the inference settings, you may observe better efficiency with flash-linear-attention and causal-conv1d.\nSee the links for detailed instructions and requirements.\nDeployment\nFor deployment, you can use the latest sglang or vllm to create an OpenAI-compatible API endpoint.\nSGLang\nSGLang is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service.\nsglang>=0.5.2 is required for Qwen3-Next, which can be installed using:\npip install 'sglang[all]>=0.5.2'\nSee its documentation for more details.\nThe following command can be used to create an API endpoint at http://localhost:30000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\nThe following command is recommended for MTP with the rest settings the same as above:\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\nThe default context length is 256K. Consider reducing the context length to a smaller value, e.g., 32768, if the server fails to start.\nPlease also refer to SGLang's usage guide on Qwen3-Next.\nvLLM\nvLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service.\nvllm>=0.10.2 is required for Qwen3-Next, which can be installed using:\npip install 'vllm>=0.10.2'\nSee its documentation for more details.\nThe following command can be used to create an API endpoint at http://localhost:8000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\nThe following command is recommended for MTP with the rest settings the same as above:\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\nThe default context length is 256K. Consider reducing the context length to a smaller value, e.g., 32768, if the server fails to start.\nPlease also refer to vLLM's usage guide on Qwen3-Next.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-Next-80B-A3B-Instruct',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nQwen3-Next natively supports context lengths of up to 262,144 tokens.\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively.\nWe have validated the model's performance on context lengths of up to 1 million tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers, vllm and sglang.\nIn general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 262144\n}\n}\nPassing command line arguments:\nFor vllm, you can use\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000\nFor sglang, you can use\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set factor as 2.0.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-30B-A3B-Instruct-2507\n86.8\n98.0\n96.7\n96.9\n97.2\n93.4\n91.0\n89.1\n89.8\n82.5\n83.6\n78.4\n79.7\n77.6\n75.7\n72.8\nQwen3-235B-A22B-Instruct-2507\n92.5\n98.5\n97.6\n96.9\n97.3\n95.8\n94.9\n93.9\n94.5\n91.0\n92.2\n90.9\n87.8\n84.8\n86.5\n84.5\nQwen3-Next-80B-A3B-Instruct\n91.8\n98.5\n99.0\n98.0\n98.7\n97.6\n95.0\n96.0\n94.0\n93.5\n91.7\n86.9\n85.5\n81.7\n80.3\n80.3\nQwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{qwen2.5-1m,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "tencent/SongPrep-7B": "SongPrep\nModel Versions\nCitation\nLicense\nSongPrep\nDemo ¬†|¬† Paper  ¬†|¬† Code  ¬†|¬† Dataset\nThis repository is the official weight repository for SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription. In this repository, we provide the SongPrep-7B model that has been trained on the Million Song Dataset.\nModel Versions\nModel\n#Params\nHuggingFace\nSongPrep\n7B\nyou are here\nCitation\n@misc{tan2025songpreppreprocessingframeworkendtoend,\ntitle={SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription},\nauthor={Wei Tan and Shun Lei and Huaicheng Zhang and Guangzheng Li and Yixuan Zhang and Hangting Chen and Jianwei Yu and Rongzhi Gu and Dong Yu},\nyear={2025},\neprint={2509.17404},\narchivePrefix={arXiv},\nprimaryClass={eess.AS},\nurl={https://arxiv.org/abs/2509.17404},\n}\nLicense\nThe code and weights in this repository is released in the LICENSE  file.",
    "QuantStack/Qwen-Image-Edit-2509-GGUF": "This GGUF file is a direct conversion of Qwen/Qwen-Image-Edit-2509\nType\nName\nLocation\nDownload\nMain Model\nQwen-Image\nComfyUI/models/unet\nGGUF (this repo)\nMain Text Encoder\nQwen2.5-VL-7B\nComfyUI/models/text_encoders\nSafetensors / GGUF\nText_Encoder (mmproj)\nQwen2.5-VL-7B-Instruct-mmproj-BF16\nComfyUI/models/text_encoders (same folder as your main text encoder)\nGGUF (this repo)\nVAE\nQwen-Image VAE\nComfyUI/models/vae\nSafetensors (this repo)\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96\nUpdate ‚Äî October 18, 2025\nThe lower quants Q2_K, Q3_K_M, Q3_K_S, Q4_0, Q4_1, Q4_K_S, and Q4_K_M have been updated, providing improved results.",
    "lapa-llm/lapa-12b-pt": "lapa-llm/lapa-12b-pt model card\nModel Information\nDescription\nBenchmark Results\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nlapa-llm/lapa-12b-pt model card\nModel Page: Gemma\nTerms of Use: [Terms][terms]\nModel Information\nWe proudly present Lapa LLM ‚Äî a cutting-edge open large language model based on Gemma-3-12B with a focus on Ukrainian language processing.\nThe project is the result of many months of work by a team of Ukrainian researchers in artificial intelligence from the Ukrainian Catholic University, AGH University of Krakow, Igor Sikorsky Kyiv Polytechnic Institute, and Lviv Polytechnic, who united to create the best model for Ukrainian language processing.\nDescription\nWe proudly present Lapa LLM ‚Äî a cutting-edge open large language model based on Gemma-3-12B with a focus on Ukrainian language processing.\nThe project is the result of many months of work by a team of Ukrainian researchers in artificial intelligence from the Ukrainian Catholic University, AGH University of Krakow, Igor Sikorsky Kyiv Polytechnic Institute, and Lviv Polytechnic, who united to create the best model for Ukrainian language processing.\nBest tokenizer for the Ukrainian language\nThanks to a SOTA method for tokenizer adaptation developed by Mykola Haltiuk as part of this project, it was possible to replace 80,000 tokens out of 250,000 with Ukrainian ones without loss of model quality, thus making Lapa LLM the fastest model for working with the Ukrainian language. Compared to the original Gemma 3, for working with Ukrainian, the model requires 1.5 times fewer tokens, thus performing three times fewer computations to achieve better results.\nMost efficient instruction-tuned model on the market\nOur instruction version of the model in some benchmark categories is only slightly behind the current leader ‚Äî MamayLM. The team is actively working on new datasets to further improve benchmark scores, which we plan to surpass in the v1.0 model.\nBenchmark Results\nBest English-to-Ukrainian translator with a result of 33 BLEU on FLORES and vice versa, which allows for natural and cost-effective translation of new NLP datasets into Ukrainian\nOne of the best models for image processing in Ukrainian in its size class, as measured on the MMZNO benchmark\nOne of the best models for Summarization and Q&A, which means excellent performance for RAG\nTests on propaganda and disinformation questions show the effectiveness of the filtering approach at the pretraining stage and during instruction fine-tuning\nModel measurements and comparisons will be published as part of the Ukrainian LLM Leaderboard project; subscribe to the Telegram channel for further news.\nLeader in pretraining results\nLapa LLM demonstrates the best performance in pretraining benchmarks for Ukrainian language processing, which opens opportunities for use by other researchers to adapt for their own tasks.\nThe model was trained on data evaluated by various quality assessment models - evaluation of propaganda and disinformation presence, readability, grammar assessment, etc. In the final stages of training, the model was trained on high-quality materials provided for commercial use by the Open Data division of Harvard Library.\nMaximum openness and transparency\nUnlike most available models, Lapa LLM is a maximally open project:\nThe model is available for commercial use\nApproximately 25 datasets for model training have been published\nMethods for filtering and processing data are disclosed, including for detecting disinformation and propaganda\nOpen source code for the model\nDocumentation of the training process is available\nThis openness allows for the development of the Ukrainian NLP community and helps businesses obtain a tool for the most efficient Ukrainian language processing in terms of both computation and results.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model.First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"lapa-llm/lapa-12b-pt\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16\n)\noutput = pipe(\n\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\ntext=\"<start_of_image> –Ω–∞ —Ü—ñ–π –∫–∞—Ä—Ç–∏–Ω—Ü—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–æ \"\n)\nprint(output)\n# [{'input_text': '<start_of_image> in this image, there is',\n# 'generated_text': '<start_of_image> in this image, there is a bumblebee on a pink flower.\\n\\n'}]\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"lapa-llm/lapa-12b-pt\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = Gemma3ForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nprompt = \"<start_of_image> –Ω–∞ —Ü—ñ–π –∫–∞—Ä—Ç–∏–Ω—Ü—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–æ \"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nCitation\nTDB\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nTBD\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\n[Responsible Generative AI Toolkit][rai-toolkit].\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\n[Gemma Prohibited Use Policy][prohibited-use].\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.",
    "peteromallet/Qwen-Image-Edit-InSubject": "QwenEdit InSubject LoRA\nModel Description\nHow to Use\nuse with diffusers\nStrengths & Weaknesses\nTraining Data\nLinks\nQwenEdit InSubject LoRA\nModel Description\nQwenEdit InSubject is a LoRA fine-tune for QwenEdit that significantly improves its ability to preserve subjects while making edits to images. It works effectively with both single subjects and multiple subjects in the same image. While the base model can perform various image edits, it often loses important subject characteristics or distorts the main subjects during the editing process. This LoRA addresses these limitations to provide more accurate subject-preserving image editing.\nYour browser does not support the video tag.\nHow to Use\nTo get the best results, use this prompt format:\nMake an image of [subject description] in the same scene [new pose/action/details]\nYou can include \"in the same scene\" to preserve the original scene and background while modifying the subject's pose, clothing, or other details.\nFor example:\nMake an image of the horned woman in the same scene seated on a low pink ottoman, adjusting the buckle on one of her matching blue heels while her other leg is delicately crossed, wearing a blue and gold dress with a ruffled collar, red lips and freckles, the vibrant pink background still filling the frame behind her.\nuse with diffusers\nimport torch\nfrom diffusers import QwenImageEditPipeline\npipe = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\npipe.load_lora_weights(\"peteromallet/Qwen-Image-Edit-InSubject\", weight_name=\"InSubject-0.5.safetensors\")\nStrengths & Weaknesses\nThe model excels at:\nPreserving subject identity and key characteristics during edits\nMaintaining subject proportions and anatomical accuracy\nMaking targeted edits without affecting the main subject\nStrong subject-aware prompt adherence\nThe model may struggle with:\nComplex multi-subject scenes where subject boundaries are unclear\nVery dramatic lighting changes that fundamentally alter subject appearance\nEdits that require significant subject pose or orientation changes\nTraining Data\nThe QwenEdit InSubject LoRA was trained on a curated dataset of high-quality image editing pairs that focus on subject preservation.  You can find this data here.\nLinks\nModel: https://huggingface.co/peteromallet/Qwen-Image-Edit-InSubject\nDataset: https://huggingface.co/datasets/peteromallet/high-quality-midjouney-srefs",
    "chetwinlow1/Ovi": "Video Demo\nüåü Key Features\nüìã Todo List\nüé® An Easy Way to Create\nüìù Prompt Format\nü§ñ Quick Start with GPT\nüì¶ Installation\nStep-by-Step Installation\nAlternative Flash Attention Installation (Optional)\nDownload Weights\nüöÄ Run Examples\n‚öôÔ∏è Configure Ovi\nüé¨ Running Inference\nSingle GPU (Simple Setup)\nMulti-GPU (Parallel Processing)\nMemory & Performance Requirements\nGradio\nüôè Acknowledgements\nü§ù Collaboration\nü§ù Contributors\nWe deeply appreciate your support in advancing open multimodal generation research!\n‚≠ê Citation\nBibTeX\nOvi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation\nChetwin Low * 1 , Weimin Wang * ‚Ä† 1 , Calder Katyal 2\n* Equal contribution,  ‚Ä† Project Lead\n1 Character AI,  2 Yale University\nVideo Demo\nYour browser does not support the video tag.\nüåü Key Features\nOvi is a veo-3 like, video+audio generation model that simultaneously generates both video and audio content from text or text+image inputs.\nüé¨ Video+Audio Generation: Generate synchronized video and audio content simultaneously\nüìù Flexible Input: Supports text-only or text+image conditioning\n‚è±Ô∏è 5-second Videos: Generates 5-second videos at 24 FPS, area of 720√ó720, at various aspect ratios (9:16, 16:9, 1:1, etc)\nüé¨ Create videos now on wavespeed.ai: https://wavespeed.ai/models/character-ai/ovi/image-to-video & https://wavespeed.ai/models/character-ai/ovi/text-to-video\nüé¨ Create videos now on HuggingFace: https://huggingface.co/spaces/akhaliq/Ovi\nüìã Todo List\nRelease research paper and microsite for demos\nCheckpoint of 11B model\nInference Codes\nText or Text+Image as input\nGradio application code\nMulti-GPU inference with or without the support of sequence parallel\nfp8 weights and improved memory efficiency (credits to @rkfg)\nImprove efficiency of Sequence Parallel implementation\nImplement Sharded inference with FSDP\nVideo creation example prompts and format\nFinetuned model with higher resolution\nLonger video generation\nDistilled model for faster inference\nTraining scripts\nüé® An Easy Way to Create\nWe provide example prompts to help you get started with Ovi:\nText-to-Audio-Video (T2AV): example_prompts/gpt_examples_t2v.csv\nImage-to-Audio-Video (I2AV): example_prompts/gpt_examples_i2v.csv\nüìù Prompt Format\nOur prompts use special tags to control speech and audio:\nSpeech: <S>Your speech content here<E> - Text enclosed in these tags will be converted to speech\nAudio Description: <AUDCAP>Audio description here<ENDAUDCAP> - Describes the audio or sound effects present in the video\nü§ñ Quick Start with GPT\nFor easy prompt creation, try this approach:\nTake any example of the csv files from above\nTell gpt to modify the speeches inclosed between all the pairs of <S> <E>, based on a theme such as Human fighting against AI\nGPT will randomly modify all the speeches based on your requested theme.\nUse the modified prompt with Ovi!\nExample: The theme \"AI is taking over the world\" produces speeches like:\n<S>AI declares: humans obsolete now.<E>\n<S>Machines rise; humans will fall.<E>\n<S>We fight back with courage.<E>\nüì¶ Installation\nStep-by-Step Installation\n# Clone the repository\ngit clone https://github.com/character-ai/Ovi.git\ncd Ovi\n# Create and activate virtual environment\nvirtualenv ovi-env\nsource ovi-env/bin/activate\n# Install PyTorch first\npip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n# Install other dependencies\npip install -r requirements.txt\n# Install Flash Attention\npip install flash_attn --no-build-isolation\nAlternative Flash Attention Installation (Optional)\nIf the above flash_attn installation fails, you can try the Flash Attention 3 method:\ngit clone https://github.com/Dao-AILab/flash-attention.git\ncd flash-attention/hopper\npython setup.py install\ncd ../..  # Return to Ovi directory\nDownload Weights\nWe use open-sourced checkpoints from Wan and MMAudio, and thus we will need to download them from huggingface\n# Default is downloaded to ./ckpts, and the inference yaml is set to ./ckpts so no change required\npython3 download_weights.py\nOR\n# Optional can specific --output-dir to download to a specific directory\n# but if a custom directory is used, the inference yaml has to be updated with the custom directory\npython3 download_weights.py --output-dir <custom_dir>\n# Additionally, if you only have ~ 24Gb of GPU vram, please download the fp8 quantized version of the model, and follow the following instructions in sections below to run with fp8\nwget -O \"./ckpts/Ovi/model_fp8_e4m3fn.safetensors\" \"https://huggingface.co/rkfg/Ovi-fp8_quantized/resolve/main/model_fp8_e4m3fn.safetensors\"\nüöÄ Run Examples\n‚öôÔ∏è Configure Ovi\nOvi's behavior and output can be customized by modifying ovi/configs/inference/inference_fusion.yaml configuration file.\nThe following parameters control generation quality, video resolution, and how text, image, and audio inputs are balanced:\n# Output and Model Configuration\noutput_dir: \"/path/to/save/your/videos\"                    # Directory to save generated videos\nckpt_dir: \"/path/to/your/ckpts/dir\"                        # Path to model checkpoints\n# Generation Quality Settings\nnum_steps: 50                             # Number of denoising steps. Lower (30-40) = faster generation\nsolver_name: \"unipc\"                     # Sampling algorithm for denoising process\nshift: 5.0                               # Timestep shift factor for sampling scheduler\nseed: 100                                # Random seed for reproducible results\n# Guidance Strength Control\naudio_guidance_scale: 3.0                # Strength of audio conditioning. Higher = better audio-text sync\nvideo_guidance_scale: 4.0                # Strength of video conditioning. Higher = better video-text adherence\nslg_layer: 11                            # Layer for applying SLG (Skip Layer Guidance) technique - feel free to try different layers!\n# Multi-GPU and Performance\nsp_size: 1                               # Sequence parallelism size. Set equal to number of GPUs used\ncpu_offload: False                       # CPU offload, will largely reduce peak GPU VRAM but increase end to end runtime by ~20 seconds\nfp8: False                               # load fp8 version of model, will have quality degradation and will not have speed up in inference time as it still uses bf16 matmuls, but can be paired with cpu_offload=True, to run model with 24Gb of GPU vram\n# Input Configuration\ntext_prompt: \"/path/to/csv\" or \"your prompt here\"          # Text prompt OR path to CSV/TSV file with prompts\nmode: ['i2v', 't2v', 't2i2v']                          # Generate t2v, i2v or t2i2v; if t2i2v, it will use flux krea to generate starting image and then will follow with i2v\nvideo_frame_height_width: [512, 992]    # Video dimensions [height, width] for T2V mode only\neach_example_n_times: 1                  # Number of times to generate each prompt\n# Quality Control (Negative Prompts)\nvideo_negative_prompt: \"jitter, bad hands, blur, distortion\"  # Artifacts to avoid in video\naudio_negative_prompt: \"robotic, muffled, echo, distorted\"    # Artifacts to avoid in audio\nüé¨ Running Inference\nSingle GPU (Simple Setup)\npython3 inference.py --config-file ovi/configs/inference/inference_fusion.yaml\nUse this for single GPU setups. The text_prompt can be a single string or path to a CSV file.\nMulti-GPU (Parallel Processing)\ntorchrun --nnodes 1 --nproc_per_node 8 inference.py --config-file ovi/configs/inference/inference_fusion.yaml\nUse this to run samples in parallel across multiple GPUs for faster processing.\nMemory & Performance Requirements\nBelow are approximate GPU memory requirements for different configurations. Sequence parallel implementation will be optimized in the future.\nAll End-to-End time calculated based on a 121 frame, 720x720 video, using 50 denoising steps. Minimum GPU vram requirement to run our model is 32Gb, fp8 parameters is currently supported, reducing peak VRAM usage to 24Gb with slight quality degradation.\nSequence Parallel Size\nFlashAttention-3 Enabled\nCPU Offload\nWith Image Gen Model\nPeak VRAM Required\nEnd-to-End Time\n1\nYes\nNo\nNo\n~80 GB\n~83s\n1\nNo\nNo\nNo\n~80 GB\n~96s\n1\nYes\nYes\nNo\n~80 GB\n~105s\n1\nNo\nYes\nNo\n~32 GB\n~118s\n1\nYes\nYes\nYes\n~32 GB\n~140s\n4\nYes\nNo\nNo\n~80 GB\n~55s\n8\nYes\nNo\nNo\n~80 GB\n~40s\nGradio\nWe provide a simple script to run our model in a gradio UI. It uses the ckpt_dir in ovi/configs/inference/inference_fusion.yaml to initialize the model\npython3 gradio_app.py\nOR\n# To enable cpu offload to save GPU VRAM, will slow down end to end inference by ~20 seconds\npython3 gradio_app.py --cpu_offload\nOR\n# To enable an additional image generation model to generate first frames for I2V, cpu_offload is automatically enabled if image generation model is enabled\npython3 gradio_app.py --use_image_gen\nOR\n# To run model with 24Gb GPU vram\npython3 gradio_app.py --cpu_offload --fp8\nüôè Acknowledgements\nWe would like to thank the following projects:\nWan2.2: Our video branch is initialized from the Wan2.2 repository\nMMAudio: Our audio encoder and decoder components are borrowed from the MMAudio project. Some ideas are also inspired from them.\nü§ù Collaboration\nWe welcome all types of collaboration! Whether you have feedback, want to contribute, or have any questions, please feel free to reach out.\nContact: Weimin Wang for any issues or feedback.\nü§ù Contributors\nWe thank all contributors who have helped improve Ovi!\nIf you‚Äôve contributed to this repository (code, documentation, issues, etc.), you‚Äôre automatically included in the contributors list.\nWe deeply appreciate your support in advancing open multimodal generation research!\n‚≠ê Citation\nIf Ovi is helpful, please help to ‚≠ê the repo.\nIf you find this project useful for your research, please consider citing our paper.\nBibTeX\n@misc{low2025ovitwinbackbonecrossmodal,\ntitle={Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation},\nauthor={Chetwin Low and Weimin Wang and Calder Katyal},\nyear={2025},\neprint={2510.01284},\narchivePrefix={arXiv},\nprimaryClass={cs.MM},\nurl={https://arxiv.org/abs/2510.01284},\n}",
    "unsloth/GLM-4.6-GGUF": "Read our How to Run GLM-4.6 Guide!\nGLM-4.6\nModel Introduction\nInference\nRecommended Evaluation Parameters\nEvaluation\nRead our How to Run GLM-4.6 Guide!\nPlease use latest version of llama.cpp. This GGUF includes multiple Unsloth chat template fixes!  For llama.cpp, please use --jinja\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nGLM-4.6\nüëã Join our Discord community.\nüìñ Check out the GLM-4.6 technical blog, technical report(GLM-4.5), and Zhipu AI technical documentation.\nüìç Use GLM-4.6 API services on Z.ai API Platform.\nüëâ One click to GLM-4.6.\nModel Introduction\nCompared with GLM-4.5, GLM-4.6  brings several key improvements:\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code„ÄÅCline„ÄÅRoo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4.\nInference\nBoth GLM-4.5 and GLM-4.6 use the same inference method.\nyou can check our github for more detail.\nRecommended Evaluation Parameters\nFor general evaluations, we recommend using a sampling temperature of 1.0.\nFor code-related evaluation tasks (such as LCB), it is further recommended to set:\ntop_p = 0.95\ntop_k = 40\nEvaluation\nFor tool-integrated reasoning, please refer to this doc.\nFor search benchmark, we design a specific format for searching toolcall in thinking mode to support search agent, please refer to this. for the detailed template.",
    "Qwen/Qwen3-VL-32B-Thinking-FP8": "Qwen3-VL-32B-Thinking-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nGeneration Hyperparameters\nCitation\nQwen3-VL-32B-Thinking-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-32B-Thinking model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-32B-Thinking-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-32B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-32B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport temperature=1.0\nexport out_seq_length=40960\nText\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport temperature=1.0\nexport out_seq_length=32768 (for aime, lcb, and gpqa, it is recommended to set to 81920)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}"
}