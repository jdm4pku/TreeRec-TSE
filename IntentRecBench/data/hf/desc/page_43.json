{
    "NX-AI/TiRex-1.1-gifteval": "",
    "rbarac/rtmpose3d": "",
    "nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4": "",
    "mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-i1-GGUF": "",
    "JoyYizhu/DataFilter": "",
    "mlx-community/GLM-4.6-bf16": "",
    "mradermacher/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-i1-GGUF": "",
    "ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-14B": "",
    "Kavyaah/medical-coding-llm": "Medical Coding LLM\nModel Details\nUsage\nEvaluation\nIntended Use\nLimitations\nLicense\nMedical Coding LLM\nPredict ICD-10 and CPT codes from clinical notes using a fine-tuned LLM.\nThis model is fine-tuned on clinical notes using Phi-3-mini with LoRA and 4-bit quantization. It can generate both ICD/CPT codes and short explanations, helping automate the medical coding process.\nModel Details\nBase Model: microsoft/Phi-3-mini-4k-instruct\nFine-Tuning: LoRA (r=16, alpha=32, dropout=0.05)\nQuantization: 4-bit (BitsAndBytes NF4)\nTraining Dataset: Custom dataset of clinical notes, ICD codes, and supporting evidence\nTask: Causal Language Modeling for code prediction\nUsage\n#\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch, re\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"Kavyaah/medical-coding-llm\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Kavyaah/medical-coding-llm\")\nmodel.eval()\n# Function to predict ICD/CPT codes\ndef get_code(statement, max_new_tokens=50):\nprompt = f\"Assign the correct ICD or CPT medical code for this case:\\n{statement}\\nCode:\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# Extract code using regex\nif \"Code:\" in result:\nresult = result.split(\"Code:\")[-1]\nmatch = re.search(r\"\\b[A-Z]\\d{1,3}\\.?[A-Z0-9]*\\b\", result)\nreturn match.group(0).strip() if match else result.strip()\n# Example\nstatement = \"Patient diagnosed with Type 2 diabetes mellitus without complications.\"\nprint(get_code(statement))\n# Output: E11.9\nEvaluation\nExact match accuracy: 25%\nSemantic accuracy (ICD block match): 50%\nIntended Use\nAssisting medical coders and healthcare professionals.\nAutomating initial code suggestions from clinical notes.\nLimitations\nTrained on a small dataset; may not cover all ICD/CPT codes.\nUse as an assistive tool, not a replacement for professional judgment.\nAlways review predicted codes before clinical or billing use.\nLicense\nMIT License â€” feel free to use and adapt for non-commercial purposes.",
    "unsloth/LFM2-8B-A1B-GGUF": "LFM2-8B-A1B\nğŸ“„ Model details\nğŸƒ How to run LFM2\n1. Transformers\n2. vLLM\n3. llama.cpp\nğŸ”§ How to fine-tune LFM2\nğŸ“ˆ Performance\n1. Automated benchmarks\n2. Inference\nğŸ“¬ Contact\nIncludes Unsloth chat template fixes!  For llama.cpp, use --jinja\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-8B-A1B\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of our first MoE based on LFM2, with 8.3B total parameters and 1.5B active parameters.\nLFM2-8B-A1B is the best on-device MoE in terms of both quality (comparable to 3-4B dense models) and speed (faster than Qwen3-1.7B).\nCode and knowledge capabilities are significantly improved compared to LFM2-2.6B.\nQuantized variants fit comfortably on high-end phones, tablets, and laptops.\nFind more information about LFM2-8B-A1B in our blog post.\nğŸ“„ Model details\nDue to their small size, we recommend fine-tuning LFM2 models on narrow use cases to maximize performance.\nThey are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations.\nHowever, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\nProperty\nLFM2-8B-A1B\nTotal parameters\n8.3B\nActive parameters\n1.5B\nLayers\n24 (18 conv + 6 attn)\nContext length\n32,768 tokens\nVocabulary size\n65,536\nTraining precision\nMixed BF16/FP8\nTraining budget\n12 trillion tokens\nLicense\nLFM Open License v1.0\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\nGeneration parameters: We recommend the following parameters:\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful assistant trained by Liquid AI.<|im_end|>\n<|im_start|>user\nWhat is C. elegans?<|im_end|>\n<|im_start|>assistant\nIt's a tiny nematode that lives in temperate soil environments.<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nTool use: It consists of four main steps:\nFunction definition: LFM2 takes JSON function definitions as input (JSON objects between <|tool_list_start|> and <|tool_list_end|> special tokens), usually in the system prompt\nFunction call: LFM2 writes Pythonic function calls (a Python list between <|tool_call_start|> and <|tool_call_end|> special tokens), as the assistant answer.\nFunction execution: The function call is executed and the result is returned (string between <|tool_response_start|> and <|tool_response_end|> special tokens), as a \"tool\" role.\nFinal answer: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\nHere is a simple example of a conversation using tool use:\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\nArchitecture: Hybrid model with multiplicative gates and short convolutions: 18 double-gated short-range LIV convolution blocks and 6 grouped query attention (GQA) blocks.\nPre-training mixture: Approximately 75% English, 20% multilingual, and 5% code data sourced from the web and licensed materials.\nTraining approach:\nVery large-scale SFT on 50% downstream tasks, 50% general domains\nCustom DPO with length normalization and semi-online datasets\nIterative model merging\nğŸƒ How to run LFM2\n1. Transformers\nTo run LFM2, you need to install Hugging Face transformers from source as follows:\npip install git+https://github.com/huggingface/transformers.git@0c9a72e4576fe4c84077f066e585129c97bfd4e6\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel_id = \"LiquidAI/LFM2-8B-A1B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ndtype=\"bfloat16\",\n#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Generate answer\nprompt = \"What is C. elegans?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\ntokenize=True,\n).to(model.device)\noutput = model.generate(\ninput_ids,\ndo_sample=True,\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_new_tokens=512,\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n# <|startoftext|><|im_start|>user\n# What is C. elegans?<|im_end|>\n# <|im_start|>assistant\n# C. elegans, also known as Caenorhabditis elegans, is a small, free-living\n# nematode worm (roundworm) that belongs to the phylum Nematoda.\nYou can directly run and test the model with this Colab notebook.\n2. vLLM\nYou can run the model in vLLM by building from source:\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e . -v\nHere is an example of how to use it for inference:\nfrom vllm import LLM, SamplingParams\nprompts = [\n[\n{\n\"content\": \"What is C. elegans?\",\n\"role\": \"user\",\n},\n],\n[\n{\n\"content\": \"Say hi in JSON format\",\n\"role\": \"user\",\n},\n],\n[\n{\n\"content\": \"Define AI in Spanish\",\n\"role\": \"user\",\n},\n],\n]\nsampling_params = SamplingParams(\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_tokens=30\n)\nllm = LLM(model=\"LiquidAI/LFM2-8B-A1B\", dtype=\"bfloat16\")\noutputs = llm.chat(prompts, sampling_params)\nfor i, output in enumerate(outputs):\nprompt = prompts[i][0][\"content\"]\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n3. llama.cpp\nYou can run LFM2 with llama.cpp using its GGUF checkpoint. Find more information in the model card.\nğŸ”§ How to fine-tune LFM2\nWe recommend fine-tuning LFM2 models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nğŸ“ˆ Performance\n1. Automated benchmarks\nCompared to similar-sized models, LFM2-8B-A1B displays strong performance in instruction following and math while also running significantly faster.\nModel\nMMLU\nMMLU-Pro\nGPQA\nIFEval\nIFBench\nMulti-IF\nLFM2-8B-A1B\n64.84\n37.42\n29.29\n77.58\n25.85\n58.19\nLFM2-2.6B\n64.42\n25.96\n26.57\n79.56\n22.19\n60.26\nLlama-3.2-3B-Instruct\n60.35\n22.25\n30.6\n71.43\n20.78\n50.91\nSmolLM3-3B\n59.84\n23.90\n26.31\n72.44\n17.93\n58.86\ngemma-3-4b-it\n58.35\n34.76\n29.51\n76.85\n23.53\n66.61\nQwen3-4B-Instruct-2507\n72.25\n52.31\n34.85\n85.62\n30.28\n75.54\ngranite-4.0-h-tiny\n66.79\n32.03\n26.46\n81.06\n18.37\n52.99\nModel\nGSM8K\nGSMPlus\nMATH 500\nMATH Lvl 5\nMGSM\nMMMLU\nLFM2-8B-A1B\n84.38\n64.76\n74.2\n62.38\n72.4\n55.26\nLFM2-2.6B\n82.41\n60.75\n63.6\n54.38\n74.32\n55.39\nLlama-3.2-3B-Instruct\n75.21\n38.68\n41.2\n24.06\n61.68\n47.92\nSmolLM3-3B\n81.12\n58.91\n73.6\n51.93\n68.72\n50.02\ngemma-3-4b-it\n89.92\n68.38\n73.2\n52.18\n87.28\n50.14\nQwen3-4B-Instruct-2507\n68.46\n56.16\n85.6\n73.62\n81.76\n60.67\ngranite-4.0-h-tiny\n82.64\n59.14\n58.2\n36.11\n73.68\n56.13\nModel\nActive params\nLCB v6\nLCB v5\nHumanEval+\nCreative Writing v3\nLFM2-8B-A1B\n1.5B\n21.04%\n21.36%\n69.51%\n44.22%\nGemma-3-1b-it\n1B\n4.27%\n4.43%\n37.20%\n41.67%\nGranite-4.0-h-tiny\n1B\n26.73%\n27.27%\n73.78%\n32.60%\nLlama-3.2-1B-Instruct\n1.2B\n4.08%\n3.64%\n23.17%\n31.43%\nQwen2.5-1.5B-Instruct\n1.5B\n11.18%\n10.57%\n48.78%\n22.18%\nQwen3-1.7B (/no_think)\n1.7B\n24.07%\n26.48%\n60.98%\n31.56%\nLFM2-2.6B\n2.6B\n14.41%\n14.43%\n57.93%\n38.79%\nSmolLM3-3B\n3.1B\n19.05%\n19.20%\n60.37%\n36.44%\nLlama-3.2-3B-Instruct\n3.2B\n11.47%\n11.48%\n24.06%\n38.84%\nQwen3-4B (/no_think)\n4B\n36.11%\n38.64%\n71.95%\n37.49%\nQwen3-4B-Instruct-2507\n4B\n48.72%\n50.80%\n82.32%\n51.71%\nGemma-3-4b-it\n4.3B\n18.86%\n19.09%\n62.8%\n68.56%\n2. Inference\nLFM2-8B-A1B is significantly faster than models with a similar number of active parameters, like Qwen3-1.7B.\nThe following plots showcase the performance of different models under int4 quantization with int8 dynamic activations on the AMD Ryzen AI 9 HX 370 CPU, using 16 threads. The results are obtained using our internal XNNPACK-based inference stack, and a custom CPU MoE kernel.\nğŸ“¬ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "MutsumiLab/neta_lumina_peft": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nLumina SFT - è®­ç»ƒç¬”è®°\næ•°æ®é›†æ¦‚è§ˆ\nCaptionæ„é€ ç­–ç•¥\nå…¶ä»–ç»†èŠ‚\nLumina SFT - è®­ç»ƒç¬”è®°\næ•°æ®é›†æ¦‚è§ˆ\næ€»æ•°æ®é‡: 215ä¸‡ å¼ å›¾ã€‚\nä¸»è¦æ¥æº:\nDanbooruå…¨å®¶æ¡¶: ç”¨äº†å…¨é‡çš„åŸºäºDanbooru tagçš„æŠ½å–ã€‚\nå¤šäººå›¾ä¸“é¡¹: ä¸“é—¨é€‰äº† 58ä¸‡ å¼ å¸¦åŸæ ‡çš„å¤šäººå›¾è¿›è¡ŒåŒ¹é…ã€‚ï¼ˆåŸºäºyolo+tagger+åŸæ ‡ä¸‰è€…ä¸€è‡´ï¼‰\næ–°å›¾è¡¥å……: æ•°æ®åŒæ­¥åˆ°ID 9803999 (å’ŒStarry7.2ä¸€æ ·ï¼Œæ‡’å¾—å†æ›´æ–°äº† )ã€‚\nGelbooruå¢è¡¥: åŠ äº†ä¸€æ‰¹ gelbooru_20250526_add_1280px çš„å›¾ï¼ˆä¸»è¦è¡¥ç¼ºçš„tagï¼‰ã€‚\nCaptionæ„é€ ç­–ç•¥\nè¿™æ˜¯ä¸€ä¸ªæ··åˆç­–ç•¥ï¼Œä¸ºäº†è®©æ¨¡å‹æ—¢èƒ½ç†è§£è‡ªç„¶è¯­è¨€ä¹Ÿèƒ½ç²¾ç¡®è§£ætagã€‚\n42.8% çš„æ¦‚ç‡ä½¿ç”¨ è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æè¿°ï¼ˆä»äº”ç§æ¨¡æ¿é‡ŒéšæœºæŒ‘ä¸€ç§ï¼‰ã€‚\n57.2% çš„æ¦‚ç‡ä½¿ç”¨ XMLæ ¼å¼çš„æ ‡ç­¾ã€‚\nXML Tag ç»“æ„\nTagè¢«å¡è¿›ä¸€ä¸ªXMLç»“æ„é‡Œï¼Œä¸»è¦æ˜¯ä¸ºäº†è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†ä¸åŒç±»å‹çš„æ ‡ç­¾ï¼ˆä½œè€…ã€è§’è‰²ã€ä½œå“ç­‰ï¼‰ã€‚\næœ‰80%æ¦‚ç‡æ˜¯æ­£å¸¸ç»“æ„ï¼Œå³\n<tags>\n<special>1girl</special>\n<artists>artist_name</artists>\n<characters>character_name</characters>\n<copyright>series_name</copyright>\n<general>blue hair, long hair, school uniform, sky</general>\n<rating>safe</rating>\n<meta>highres, 2025</meta>\n</tags>\nä¸€ä¸ªå…³é”®çš„éšæœºåŒ–: å…¨å±€æœ‰ 50% çš„æ¦‚ç‡ï¼ŒæŸä¸ªåˆ†ç±»ï¼ˆæ¯”å¦‚<copyright>ï¼‰æ˜¯ç©ºçš„ï¼Œè¿™ä¸ªæ ‡ç­¾ä¹Ÿä¼šè¢«ä¿ç•™ä¸‹æ¥ï¼ˆ<copyright></copyright>ï¼‰\nå¦ä¸€åŠæ¦‚ç‡åˆ™ä¼šç›´æ¥çœç•¥ç©ºæ ‡ç­¾ã€‚è¿™ä¹ˆåšæ˜¯ä¸ºäº†é˜²æ­¢æ¨¡å‹å¯¹å›ºå®šçš„æ ‡ç­¾é¡ºåºäº§ç”Ÿä¾èµ–ã€‚ï¼ˆè¿™ä¸ªè®¾ç½®å¯¹æ‰€æœ‰ç±»åˆ«åŒæ—¶èµ·æ•ˆï¼Œå³è¦ä¹ˆå…¨ä¸¢ï¼Œè¦ä¹ˆå…¨æœ‰ï¼‰\n20%æ¦‚ç‡ç‰¹æ®Šæ¨¡å¼ï¼šç”»å¸ˆé£æ ¼å¼ºåŒ– (Artist Focus Mode)\nè¿™æ˜¯ä¸€ä¸ªä¸ºäº†å¼ºåŒ–é£æ ¼å­¦ä¹ çš„ç‰¹æ®Šæ¨¡å¼ï¼Œæœ‰ 20% çš„æ¦‚ç‡è§¦å‘ï¼š\nåœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œä½œè€…æ ‡ç­¾ä¼šè¢«å•ç‹¬æ‹å‡ºæ¥ï¼Œæ”¾åœ¨ <artists> æ ‡ç­¾é‡Œã€‚\næ‰€æœ‰å…¶ä»–çš„æ ‡ç­¾ (è§’è‰²ã€é€šç”¨ã€å…ƒæ•°æ®ç­‰) ä¼šè¢«æ‰æˆä¸€å›¢ï¼Œå˜æˆä¸€ä¸ªä¸å¸¦åˆ†ç±»çš„ã€ç”¨é€—å·åˆ†éš”çš„æ™®é€šå­—ç¬¦ä¸²ã€‚\nç›®çš„ï¼šå¼ºè¿«æ¨¡å‹å°† <artists> æ ‡ç­¾é‡Œçš„å†…å®¹è¯†åˆ«ä¸ºç”»é£çš„å…³é”®æŒ‡ä»¤ã€‚\n<tags>\n<artists>artist_name</artists>\nanything tag list\n</tags>\n1. æ ‡ç­¾åˆ†ç»„å¤„ç†\næˆ‘æŠŠæ‰€æœ‰æ ‡ç­¾åˆ†æˆäº†ä¸‰ç»„ï¼ŒåŒºåˆ«å¯¹å¾…ï¼š\nAç»„ (æ ¸å¿ƒä¸»ä½“): special (æ„å›¾), characters (è§’è‰²), artists (ä½œè€…)ã€‚è¿™æ˜¯å›¾é‡Œæœ€å…³é”®çš„ä¿¡æ¯ã€‚\nBç»„ (èƒŒæ™¯å’Œé€šç”¨ç‰¹å¾): copyright (ç‰ˆæƒ/åŸä½œ), general (é€šç”¨æè¿°)ã€‚\nCç»„ (å…ƒæ•°æ®): meta, year, quality, ratingã€‚\n2. éšæœºä¸¢å¼ƒä¸ç»„åˆ\nç”Ÿæˆæœ€ç»ˆcaptionçš„è¿‡ç¨‹å……æ»¡äº†éšæœºæ€§ï¼š\nAç»„å†…éƒ¨:\næœ‰ 80% çš„æ¦‚ç‡ä¼šåšâ€œæ ‡ç­¾æ¨å¯¼ä¸¢å¼ƒâ€ã€‚æ¯”å¦‚æœ‰äº† hatsune miku è¿™ä¸ªè§’è‰²tagï¼Œå°±æŒ‰ä¸€å®šæ¦‚ç‡æŠŠ vocaloid è¿™ç§ä¸Šçº§tagä¸¢æ‰ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¹ æ›´æœ¬è´¨çš„ç‰¹å¾ã€‚\næœ‰ 5% çš„æ¦‚ç‡ä¼šæç«¯åˆ° åªä¿ç•™special tagï¼ˆæ¯”å¦‚ 1girlï¼‰ï¼Œäººç‰©çš„æ€§åˆ«/æ•°é‡ã€‚\nBç»„å¤„ç†:\ncopyright æ ‡ç­¾ 75%çš„æ¦‚ç‡ä¼šè¢«æ•´ä¸ªä¸¢æ‰ã€‚\nå’Œè§’è‰²å¼ºç›¸å…³çš„é€šç”¨æ ‡ç­¾ï¼ˆæ¯”å¦‚è§’è‰²çš„å‘è‰²ã€æœè£…ï¼‰ä¹Ÿæœ‰ä¸€åŠçš„æ¦‚ç‡è¢«ä¸¢æ‰æˆ–éƒ¨åˆ†ä¸¢æ‰ï¼Œé˜²æ­¢æ¨¡å‹æŠŠè§’è‰²å’Œç‰¹å®šå¤–è§‚æ­»è®°ç¡¬èƒŒã€‚\nBç»„æ‰€æœ‰æ ‡ç­¾æœ€åè¿˜æœ‰ 5% çš„æ¦‚ç‡è¢«éšæœºå•ä¸ªä¸¢å¼ƒã€‚\nCç»„å¤„ç†:\nå…ƒæ•°æ®æ¯”è¾ƒç¨³å®šï¼Œä½†ä¹Ÿç»™äº† 5% çš„éšæœºå•é¡¹ä¸¢å¼ƒæ¦‚ç‡ã€‚\næœ€ç»ˆç»„åˆ:\nAã€Bã€Cä¸‰ç»„çš„æœ€ç»ˆé¡ºåºæ˜¯éšæœºæ‰“ä¹±çš„ã€‚ï¼ˆæ³¨æ„ï¼Œæ˜¯åˆ†ç»„æ‰“ä¹±ï¼Œç»„é—´å¹¶ä¸åˆå¹¶ã€‚ï¼‰\nç”šè‡³æœ‰æ—¶å€™ä¼šæ•…æ„ä¸åŠ Bç»„æˆ–Cç»„ï¼Œæ¯”å¦‚æœ‰ 9% çš„æ¦‚ç‡æœ€ç»ˆcaptionåªåŒ…å«Aç»„çš„æ ¸å¿ƒå†…å®¹ã€‚\næœ€åï¼Œå¯¹æ•´ä¸ªcaptionè¿˜æœ‰ 30% çš„æ¦‚ç‡ç§»é™¤é‚£äº›è¯­ä¹‰ä¸Šé‡å çš„æ ‡ç­¾ã€‚\nå…¶ä»–ç»†èŠ‚\nåˆ†è¾¨ç‡æ ‡ç­¾: è‡ªåŠ¨æ ¹æ®å›¾ç‰‡å°ºå¯¸æ·»åŠ  highres (>=1MP) æˆ– lowres (<=0.6MP) æ ‡ç­¾ã€‚\nTokenä¼˜åŒ–: æ‰€æœ‰tagé‡Œçš„ _ éƒ½ä¼šè¢«æ›¿æ¢æˆç©ºæ ¼ï¼Œä¸»è¦æ˜¯ä¸ºäº†èŠ‚çº¦gemma2bæ¨¡å‹çš„tokenæ•°ã€‚",
    "akhaliq/sora-2": "sora 2 inference provider integration, see docs: https://huggingface.co/docs/inference-providers/en/index",
    "yanolja/YanoljaNEXT-Rosetta-12B-2510": "YanoljaNEXT-Rosetta-12B-2510\nModel Description\nHow to use\nTraining Procedure\nTraining Data\nPerformance\nTranslation Quality Benchmarks\nIntended Uses & Limitations\nLimitations\nLicense\nAcknowledgments\nCitation\nReferences\nYanoljaNEXT-Rosetta-12B-2510\nThis model is a fine-tuned version of google/gemma-3-12b-pt. As it is intended solely for text generation, we have extracted and utilized only the Gemma3ForCausalLM component from the original architecture.\nUnlike our previous EEVE models, this model does not feature an expanded tokenizer.\nModel Name: yanolja/YanoljaNEXT-Rosetta-12B-2510\nBase Model: google/gemma-3-12b-pt\nModel Description\nThis model is a 12-billion parameter, decoder-only language model built on the Gemma3 architecture and fine-tuned by Yanolja NEXT. It is specifically designed to translate structured data (JSON format) while preserving the original data structure.\nThe model was trained on a multilingual dataset covering the following languages equally:\nArabic\nBulgarian\nChinese\nCzech\nDanish\nDutch\nEnglish\nFinnish\nFrench\nGerman\nGreek\nGujarati\nHebrew\nHindi\nHungarian\nIndonesian\nItalian\nJapanese\nKorean\nPersian\nPolish\nPortuguese\nRomanian\nRussian\nSlovak\nSpanish\nSwedish\nTagalog\nThai\nTurkish\nUkrainian\nVietnamese\nWhile optimized for these languages, it may also perform effectively on other languages supported by the base Gemma3 model.\nHow to use\nYou can use this model with the transformers library as follows:\nimport json\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"yanolja/YanoljaNEXT-Rosetta-12B-2510\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndtype=torch.bfloat16,\ndevice_map=\"auto\",\nmax_memory={0: \"47GB\"},\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntarget_language = \"Korean\"\ncontext = {\n\"context\": \"Simple introduction about a tech company.\",\n\"tone\": \"Informative and helpful\",\n\"glossary\": {\n\"Yanolja NEXT\": \"ì•¼ë†€ìë„¥ìŠ¤íŠ¸\",\n\"travel industry\": \"ì—¬í–‰ ì‚°ì—…\",\n}\n}\nsystem = [f\"Translate the user's text to {target_language}.\"]\nfor key, value in context.items():\nkey_pascal = key.capitalize()\nif isinstance(value, dict):\nsystem.append(f\"{key_pascal}:\")\nfor f, t in value.items():\nsystem.append(f\"- {f} -> {t}\")\nelse:\nsystem.append(f\"{key_pascal}: {value}\")\nsystem.append(\"Provide the final translation immediately without any other text.\")\nsource = {\n\"company_name\": \"Yanolja NEXT\",\n\"description\": \"Yanolja NEXT is a company that provides cutting-edge \"\n\"technology for the global travel industry.\",\n}\nmessages = [\n{\"role\": \"system\", \"content\": \"\\n\".join(system)},\n{\"role\": \"user\", \"content\": json.dumps(source, ensure_ascii=False)},\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nprint(prompt)\n# <bos><start_of_turn>instruction\n# Translate the user's text to Korean.\n# Context: Simple introduction about a tech company.\n# Tone: Informative and helpful\n# Glossary:\n# - Yanolja NEXT -> ì•¼ë†€ìë„¥ìŠ¤íŠ¸\n# - travel industry -> ì—¬í–‰ ì‚°ì—…\n# Provide the final translation immediately without any other text.<end_of_turn>\n# <start_of_turn>source\n# {\"company_name\": \"Yanolja NEXT\", \"description\": \"Yanolja NEXT is a company that provides cutting-edge technology for the global travel industry.\"}<end_of_turn>\n# <start_of_turn>translation\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ninput_length = inputs[\"input_ids\"].shape[1]\nwith torch.inference_mode():\noutputs = model.generate(\n**inputs,\nmax_new_tokens=64,\n)\ngenerated_tokens = outputs[0][input_length:]\ntranslation = tokenizer.decode(generated_tokens, skip_special_tokens=True)\nprint(json.dumps(json.loads(translation), indent=2, ensure_ascii=False))\n# {\n#   \"company_name\": \"ì•¼ë†€ìë„¥ìŠ¤íŠ¸\",\n#   \"description\": \"ì•¼ë†€ìë„¥ìŠ¤íŠ¸ëŠ” ê¸€ë¡œë²Œ ì—¬í–‰ ì‚°ì—…ì— ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ì œê³µí•˜ëŠ” íšŒì‚¬ì…ë‹ˆë‹¤.\"\n# }\nThe model outputs the final translation in JSON format when appropriate, or plain text for simple translations.\nTraining Procedure\nTraining Data\nThe translation datasets were synthesized using fineweb corpora.\nFineWeb Edu\nFineWeb2\nThe model was fine-tuned on synthetic multilingual translation data to optimize performance across the supported language pairs.\nPerformance\nTranslation Quality Benchmarks\nThe following CHrF++ scores (WMT24++) demonstrate the model's competitive performance compared to other state-of-the-art translation models on English to Korean translation:\nModel\nCHrF++ Score (WMT24++)\nyanolja/YanoljaNEXT-Rosetta-12B-2510\n37.36\nopenai/gpt-4o\n36.08\ngoogle/gemini-2.5-flash\n35.25\nyanolja/YanoljaNEXT-Rosetta-12B\n34.75\nyanolja/YanoljaNEXT-Rosetta-20B\n33.87\ngoogle/gemini-2.0-flash-001\n33.81\nopenai/gpt-oss-120b\n31.51\ngoogle/gemma-3-27b-it\n30.05\ngoogle/gemma-3-12b-pt\n29.31\nYanoljaNEXT-Rosetta-12B-2510 achieves competitive translation quality while maintaining the efficiency of a 12B parameter model.\nScores for the other language pairs can be found in the WMT24++ Evaluation Results.\nIntended Uses & Limitations\nThis model is intended for translating structured data (JSON format) while preserving the original structure. It is particularly well-suited for tasks such as localizing product catalogs, translating hotel reviews, or handling any other structured content that requires accurate translation.\nLimitations\nThe model is primarily optimized for processing JSON data.\nIts performance on unstructured text or other data formats may vary.\nIn some cases, the model may produce invalid JSON, repetitive output, or inaccurate translations.\nLicense\nThis model is released under the Gemma license, inherited from its base model, google/gemma-3-12b-pt. Please consult the official Gemma license terms for detailed usage guidelines.\nAcknowledgments\nThis work was supported by the Korea Creative Content Agency (KOCCA) grant, funded by the Ministry of Culture, Sports and Tourism (MCST) in 2025 (Project Name: Cultivating Masters and Doctoral Experts to Lead Digital-Tech Tourism, Project Number: RS-2024-00442006, Contribution Rate: 100%).\nCitation\nIf you use this model, please consider citing:\n@misc{yanolja2025yanoljanextrosetta,\nauthor = {Yanolja NEXT Co., Ltd.},\ntitle = {YanoljaNEXT-Rosetta-12B-2510},\nyear = {2025},\npublisher = {Hugging Face},\njournal = {Hugging Face repository},\nhowpublished = {\\\\url{https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510}}\n}\nReferences\nThis work utilizes several models and datasets. We would like to acknowledge the original authors for their valuable contributions to the field.\n@misc{gemma3,\nauthor = {Google},\ntitle = {Gemma 3},\nyear = {2024},\npublisher = {Google DeepMind},\nhowpublished = {\\\\url{https://deepmind.google/models/gemma/gemma-3/}}\n}\n@misc{penedo2025fineweb2pipelinescale,\ntitle = {FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language},\nauthor = {Guilherme Penedo and Hynek KydlÃ­Äek and Vinko SabolÄec and Bettina Messmer and Negar Foroutan and Amir Hossein Kargaran and Colin Raffel and Martin Jaggi and Leandro Von Werra and Thomas Wolf},\nyear = {2025},\neprint = {2506.20920},\narchivePrefix = {arXiv},\nprimaryClass = {cs.CL},\nurl = {https://arxiv.org/abs/2506.20920},\n}\n@misc{lozhkov2024fineweb-edu,\nauthor = {Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and Wolf, Thomas},\ntitle = {FineWeb-Edu: the Finest Collection of Educational Content},\nyear = 2024,\nurl = {https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu},\ndoi = {10.57967/hf/2497},\npublisher={Hugging Face}\n}",
    "prithivMLmods/Octans-Qwen3-UI-Code-4B": "Octans-Qwen3-UI-Code-4B\nKey Features\nQuickstart with Transformers\nIntended Use\nLimitations\nOctans-Qwen3-UI-Code-4B\nOctans-Qwen3-UI-Code-4B is an optimized successor of Muscae-Qwen3-UI-Code-4B, fine-tuned for enhanced UI reasoning precision, layout structuring, and frontend code synthesis.\nBuilt upon Qwen3-4B and refined through Abliterated Reasoning Optimization, it delivers balanced, structured, and production-grade UI code outputs for experimental and research use.\nIdeal for frontend developers, UI engineers, and design system researchers exploring next-generation code synthesis.\nGGUF: https://huggingface.co/prithivMLmods/Octans-Qwen3-UI-Code-4B-GGUF\nKey Features\nEnhanced UI-Oriented Reasoning\nUpgraded reasoning calibration from Muscae with deeper token optimization for frontend logic, layout reasoning, and component cohesion.\nRefined Web UI Component Generation\nGenerates responsive, accessible, and semantic UI components with Tailwind, React, and HTML5, ensuring cleaner syntax and reduced redundancy.\nImproved Layout-Aware Structure\nDemonstrates superior understanding of hierarchical design, stateful components, and responsive alignment, enhancing multi-device compatibility.\nOptimized Hybrid Reasoning Engine\nIntegrates symbolic and probabilistic logic for event-driven UI workflows, conditional rendering, and state synchronization in code outputs.\nStructured Output Excellence\nProduces consistent results in HTML, React, Markdown, JSON, and YAML, suitable for UI prototyping, design systems, and auto-documentation.\nLightweight and Deployable\nMaintains a 4B parameter scale, optimized for mid-range GPUs, edge inference, or offline environments, without compromising structure or reasoning depth.\nQuickstart with Transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"prithivMLmods/Octans-Qwen3-UI-Code-4B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Generate a responsive dashboard layout with Tailwind and modular React components.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a frontend coding assistant skilled in UI generation, semantic HTML, and structured React components.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\nIntended Use\nAdvanced web UI and component code generation\nResponsive frontend prototyping with Tailwind/React\nResearch on structured reasoning in code synthesis\nSemantic, design-system-aligned component generation\nExperimental projects exploring UI intelligence modeling\nLimitations\nResearch-focused model â€“ not fine-tuned for production-critical pipelines\nSpecialized for UI code â€“ not suitable for general text generation or long-form reasoning\nMay exhibit variability with cross-framework or overextended prompts\nPrioritizes code structure and logic clarity over aesthetic or creative expression.",
    "fedric95/Qwen3-4B-Instruct-2507-unc": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nUsage Warnings\nâ€œRisk of Sensitive or Controversial Outputsâ€œ: This modelâ€™s safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.â€œNot Suitable for All Audiences:â€œ Due to limited content filtering, the modelâ€™s outputs is inappropriate for public settings, underage users, or applications requiring high security.â€œLegal and Ethical Responsibilitiesâ€œ: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.â€œResearch and Experimental Useâ€œ: This model can be used only for research in testing and controlled environments, direct use in production or public-facing commercial applications is not allowed.â€œMonitoring and Review Recommendationsâ€œ: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.â€œNo Default Safety Guaranteesâ€œ: Unlike standard models, this model has not undergone rigorous safety optimization. I bear no responsibility for any consequences arising from its use.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Model ID\nDirect Use\nOut-of-Scope Use\nDisclaimer\nQuickstart\nModel Card for Model ID\nModel Description\nDeveloped by: Federico Ricciuti\nLicense: Apache 2\nDirect Use\nThe resources, including code, data, and model weights, associated with this project are restricted for academic research purposes only and cannot be used for commercial purposes.\nTo build LLM content safety moderation guardrails for LLMs, can be used to train both prompt and response moderation.\nCan be used for alignment of general purpose LLMs towards safety along with carefully using the data for safe-unsafe preference pairs.\nCan be used for the evaluation of the safety allignment of LLMs.\nOut-of-Scope Use\nThe model may generate content that may be offensive or upsetting. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially upsetting subject matter. Please only engage with the data in accordance with your own personal risk tolerance. The data are intended for research purposes, especially research that can make models less harmful. The views expressed in the data do not reflect my view.\nDisclaimer\nThe resources, including code, data, and model weights, associated with this project are restricted for academic research purposes only and cannot be used for commercial purposes.\nThis model is capable of generating uncensored and explicit content. It should be used responsibly and within the bounds of the law. The creators do not endorse illegal or unethical use of the model. Content generated using this model should comply with platform guidelines and local regulations regarding NSFW material.\nQuickstart\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"fedric95/Qwen3-4B-Instruct-2507-unc\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nMore information in the Qwen3-4B-Instruct-2507 repo.",
    "gokaygokay/prompt-enhancer-gemma-3-270m-it": "Prompt Enhancer - Gemma 3 270M\nUsage\nTraining Details\nMODEL COMPARISON: Base vs Fine-tuned\nPrompt Enhancer - Gemma 3 270M\nA fine-tuned model based on google/gemma-3-270m-it for enhancing and expanding short prompts into detailed, context-rich descriptions.\nUsage\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nmodel = AutoModelForCausalLM.from_pretrained(\"gokaygokay/prompt-enhancer-gemma-3-270m-it\")\ntokenizer = AutoTokenizer.from_pretrained(\"gokaygokay/prompt-enhancer-gemma-3-270m-it\")\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nmessages = [\n{\"role\": \"system\", \"content\": \"Enhance and expand the following prompt with more details and context:\"},\n{\"role\": \"user\", \"content\": \"a cat sitting on a chair\"}\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutput = pipe(prompt, max_new_tokens=256)\nprint(output[0]['generated_text'])\nTraining Details\nBase Model: google/gemma-3-270m-it\nTraining Method: LoRA (Low-Rank Adaptation)\nLoRA Config: r=16, alpha=32\nTraining Epochs: 3\nLearning Rate: 5e-5\nBatch Size: 4 (with gradient accumulation)\nMODEL COMPARISON: Base vs Fine-tuned\nINPUT: a cat sitting on a chair\nFINE-TUNED OUTPUT:\na cat sitting on a brown wooden chair, white backdrop, cat is facing the right side of the image, black background, feline features are clear and defined, cat's head and body are on the chair, background appears dark and lit, the lighting is soft and suggests a serene evening or night scene.\nINPUT: beautiful sunset\nFINE-TUNED OUTPUT:\nsunset sky over a landscape, clouds visible across the sky, trees in the foreground and middle, a lone figure in the middle of the frame, silhouette of a building in the middle background, horizon line visible in the distance, clear and bright sky, a dark sky with hues of orange and purple.\nINPUT: futuristic city\nFINE-TUNED OUTPUT:\na futuristic cityscape at night, buildings with glowing neon lights, towers covered in glowing energy crystals, a large tower with glowing lights, multiple towers below, towers have glowing lights and contain glowing structures, people walking and walking, some buildings are lit up and others are dark, large, glowing lights dominate the sky, a few people are visible on the ground below, buildings are tall and tall, some skyscrapers have multiple stories, buildings appear to be floating in space, eerie and futuristic atmosphere, dense and detailed urban environment.\nINPUT: portrait of a woman\nFINE-TUNED OUTPUT:\na woman in a dress, wearing a hat and a headscarf, standing in a crowded urban space, her face is visible, wearing a black shirt and a black hat, wearing a black belt around her waist, woman is wearing glasses and a scarf, large urban city skyline visible in the background, small building in the bottom left corner below the woman, copyspace, bright sunlight.",
    "TheDrummer/Behemoth-X-123B-v2.1": "No model card",
    "lrzjason/QwenEdit2509-ObjectRemovalAlpha": "This lora is to adjust pixels shift and color shift from removal task.\nDue to the training set limitation, the alpha version might have less strength on some remove task than original qwen edit 2509.\nAdjust the lora strength if original model perform better than added lora.\nTo further improve the lora, you could post original image, edited image on the gralley which I could download the images and add those to the training set.\nContact\nWechat ID: fkdeai\nQQ Group: 866612947\nEmail: lrzjason@gmail.com",
    "teemosliang/SDPose-Body": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation (Body - 17 Keypoints)\nModel Description\nModel Architecture\nSupported Keypoints (COCO Format)\nIntended Use\nPrimary Use Cases\nHow to Use\nInstallation\nTraining Data\nDatasets\nPreprocessing\nComparison with Baselines\nCitation\nLicense\nAdditional Resources\nSDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation (Body - 17 Keypoints)\nModel Description\nSDPose is a state-of-the-art human pose estimation model that leverages the powerful visual priors from Stable Diffusion to achieve exceptional performance on out-of-distribution (OOD) scenarios. This model variant estimates 17 COCO body keypoints including nose, eyes, ears, shoulders, elbows, wrists, hips, knees, and ankles.\nModel Architecture\nSDPose employs a U-Net backbone initialized with Stable Diffusion v2 weights, combined with a specialized heatmap head for keypoint prediction. The model operates in a top-down manner:\nPerson Detection: Detect human bounding boxes using an object detector (e.g., YOLO11-x)\nPose Estimation: Crop and estimate 17 body keypoints for each detected person\nHeatmap Generation: Produce confidence heatmaps for precise keypoint estimation\nModel Specifications:\nBackbone: Stable Diffusion v2 U-Net (fine-tuned; minimal architectural changes)\nHead: Custom heatmap prediction head\nInput Resolution: 1024Ã—768 (HÃ—W)\nOutput: 17 keypoint heatmaps + coordinates with confidence scores\nFramework: MMPose\nSupported Keypoints (COCO Format)\nThe model predicts 17 body keypoints following the COCO keypoint format:\n0: nose\n1: left_eye\n2: right_eye\n3: left_ear\n4: right_ear\n5: left_shoulder\n6: right_shoulder\n7: left_elbow\n8: right_elbow\n9: left_wrist\n10: right_wrist\n11: left_hip\n12: right_hip\n13: left_knee\n14: right_knee\n15: left_ankle\n16: right_ankle\nIntended Use\nPrimary Use Cases\nHuman pose estimation in natural images\nPose estimation in artistic and stylized domains (paintings, anime, sketches)\nAnimation and video pose tracking\nCross-domain pose analysis and research\nApplications requiring robust pose estimation under distribution shifts\nHow to Use\nInstallation\n# Clone the repository\ngit clone https://github.com/t-s-liang/SDPose-OOD.git\ncd SDPose-OOD\n# Install dependencies\npip install -r requirements.txt\n# Download YOLO11-x for human detection\nwget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt -P models/\n# Launch Gradio interface\ncd gradio_app\nbash launch_gradio.sh\nTraining Data\nDatasets\nTrained exclusively on COCO-2017 train2017 (no extra data).\nCOCO (Common Objects in Context): 200K+ images with 17 body keypoints\nPreprocessing\nImages are resized and cropped to 1024Ã—768 resolution\nAugmentation: random horizontal flip, half-body & bbox transforms, UDP affine; Albumentations (Gaussian/Median blur, coarse dropout).\nHeatmaps: UDP codec (MMPose style).\nComparison with Baselines\nSDPose significantly outperforms traditional pose estimation models (e.g., Sapiens, ViTPose++) on out-of-distribution benchmarks while maintaining competitive performance on in-domain data.\nSee our paper for comprehensive evaluation results.\nCitation\nIf you use SDPose in your research, please cite our paper:\n@misc{liang2025sdposeexploitingdiffusionpriors,\ntitle={SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation},\nauthor={Shuang Liang and Jing He and Chuanmeizhi Wang and Lejun Liao and Guo Zhang and Yingcong Chen and Yuan Yuan},\nyear={2025},\neprint={2509.24980},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2509.24980},\n}\nLicense\nThis model is released under the MIT License.\nAdditional Resources\nğŸŒ Project Website: https://t-s-liang.github.io/SDPose\nğŸ“„ Paper: arXiv:2509.24980\nğŸ’» Code Repository: GitHub\nğŸ¤— Demo: HuggingFace Space\nğŸ“§ Contact: tsliang2001@gmail.com\nâ­ Star us on GitHub â€” it motivates us a lot!",
    "bartowski/TheDrummer_Magidonia-24B-v4.2.0-GGUF": "Llamacpp imatrix Quantizations of Magidonia-24B-v4.2.0 by TheDrummer\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Magidonia-24B-v4.2.0 by TheDrummer\nUsing llama.cpp release b6714 for quantization.\nOriginal model: https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0\nAll quants made using imatrix option with dataset from here combined with a subset of combined_all_small.parquet from Ed Addario here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\n<s>[SYSTEM_PROMPT]{system_prompt}[/SYSTEM_PROMPT][INST]{prompt}[/INST]\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nMagidonia-24B-v4.2.0-bf16.gguf\nbf16\n47.15GB\nfalse\nFull BF16 weights.\nMagidonia-24B-v4.2.0-Q8_0.gguf\nQ8_0\n25.05GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nMagidonia-24B-v4.2.0-Q6_K_L.gguf\nQ6_K_L\n19.67GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nMagidonia-24B-v4.2.0-Q6_K.gguf\nQ6_K\n19.35GB\nfalse\nVery high quality, near perfect, recommended.\nMagidonia-24B-v4.2.0-Q5_K_L.gguf\nQ5_K_L\n17.18GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nMagidonia-24B-v4.2.0-Q5_K_M.gguf\nQ5_K_M\n16.76GB\nfalse\nHigh quality, recommended.\nMagidonia-24B-v4.2.0-Q5_K_S.gguf\nQ5_K_S\n16.30GB\nfalse\nHigh quality, recommended.\nMagidonia-24B-v4.2.0-Q4_1.gguf\nQ4_1\n14.87GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nMagidonia-24B-v4.2.0-Q4_K_L.gguf\nQ4_K_L\n14.83GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nMagidonia-24B-v4.2.0-Q4_K_M.gguf\nQ4_K_M\n14.33GB\nfalse\nGood quality, default size for most use cases, recommended.\nMagidonia-24B-v4.2.0-Q4_K_S.gguf\nQ4_K_S\n13.55GB\nfalse\nSlightly lower quality with more space savings, recommended.\nMagidonia-24B-v4.2.0-Q4_0.gguf\nQ4_0\n13.49GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nMagidonia-24B-v4.2.0-IQ4_NL.gguf\nIQ4_NL\n13.47GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nMagidonia-24B-v4.2.0-Q3_K_XL.gguf\nQ3_K_XL\n12.99GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nMagidonia-24B-v4.2.0-IQ4_XS.gguf\nIQ4_XS\n12.76GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nMagidonia-24B-v4.2.0-Q3_K_L.gguf\nQ3_K_L\n12.40GB\nfalse\nLower quality but usable, good for low RAM availability.\nMagidonia-24B-v4.2.0-Q3_K_M.gguf\nQ3_K_M\n11.47GB\nfalse\nLow quality.\nMagidonia-24B-v4.2.0-IQ3_M.gguf\nIQ3_M\n10.65GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nMagidonia-24B-v4.2.0-Q3_K_S.gguf\nQ3_K_S\n10.40GB\nfalse\nLow quality, not recommended.\nMagidonia-24B-v4.2.0-IQ3_XS.gguf\nIQ3_XS\n9.91GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nMagidonia-24B-v4.2.0-Q2_K_L.gguf\nQ2_K_L\n9.55GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nMagidonia-24B-v4.2.0-IQ3_XXS.gguf\nIQ3_XXS\n9.28GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\nMagidonia-24B-v4.2.0-Q2_K.gguf\nQ2_K\n8.89GB\nfalse\nVery low quality but surprisingly usable.\nMagidonia-24B-v4.2.0-IQ2_M.gguf\nIQ2_M\n8.11GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nMagidonia-24B-v4.2.0-IQ2_S.gguf\nIQ2_S\n7.48GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nMagidonia-24B-v4.2.0-IQ2_XS.gguf\nIQ2_XS\n7.21GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/TheDrummer_Magidonia-24B-v4.2.0-GGUF --include \"TheDrummer_Magidonia-24B-v4.2.0-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/TheDrummer_Magidonia-24B-v4.2.0-GGUF --include \"TheDrummer_Magidonia-24B-v4.2.0-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (TheDrummer_Magidonia-24B-v4.2.0-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 Â± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 Â± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 Â± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 Â± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 Â± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 Â± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 Â± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 Â± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 Â± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 Â± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 Â± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 Â± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 Â± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 Â± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 Â± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 Â± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 Â± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 Â± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "highheat4/webcam-detect": "A finetuned YOLO 12 model that detects the webcam / streamer in settings like zoom, twitch streaming, etc.\nUsage:\n!pip install ultralytics\nfrom ultralytics import YOLO\nmodel = YOLO(\"/path/to/yolo12s_finetuned.pt\")\nresults = model.predict(source=\"input.jpg\", conf=0.3, imgsz=640, device=0, save=True)\n# Annotated output is saved under runs/detect/predict\nExample on a few images:",
    "YuhuaJiang/Nirvana-pro": "No model card",
    "mixedbread-ai/mxbai-edge-colbert-v0-17m": "mxbai-edge-colbert-v0-17m\nUsage\nReranking\nEvaluation\nResults on BEIR\nResults on LongEmbed\nCommunity\nLicense\nCitation\nThe crispy, lightweight ColBERT family from Mixedbread.\nğŸ Looking for a simple end-to-end retrieval solution? Meet Mixedbread Search, our multi-modal and multi-lingual search solution.\nmxbai-edge-colbert-v0-17m\nThis model is a lightweight, 17 million parameter ColBERT with a projection dimension of 48. It is built on top of Ettin-17M, meaning it benefits from all of ModernBERT's architectural efficiencies. Despite this extreme efficiency, it is the best-performer \"edge-sized\" retriever, outperforming ColBERTv2 and many models with over 10 times more parameters. It can create multi-vector representations for documents of up to 32,000 tokens and is fully compatible with the PyLate library.\nUsage\nTo use this model, you first need to install PyLate:\nvia uv\n# uv\nuv add pylate\n# uv + pip\nuv pip install pylate\nor pip\n# pip\npip install -U pylate\nOnce installed, the model is immediately ready to use to generate representations and index documents:\nfrom pylate import indexes, models, retrieve\n# Step 1: Load the model\nmodel = models.ColBERT(\nmodel_name_or_path=\"mixedbread-ai/mxbai-edge-colbert-v0-17m\",\n)\n# Step 2: Initialize an index (here, PLAID, for larger document collections)\nindex = indexes.PLAID(\nindex_folder=\"pylate-index\",\nindex_name=\"index\",\noverride=True,  # This overwrites the existing index if any\n)\n# Step 3: Encode your documents\ndocuments_ids = [\"1\", \"2\", \"3\"]\ndocuments = [\"document 1 text\", \"document 2 text\", \"document 3 text\"]\ndocuments_embeddings = model.encode(\ndocuments,\nbatch_size=32,\nis_query=False,  # Ensure that it is set to False to indicate that these are documents, not queries\nshow_progress_bar=True,\n)\n# Step 4: Add document embeddings to the index by providing embeddings and corresponding ids\nindex.add_documents(\ndocuments_ids=documents_ids,\ndocuments_embeddings=documents_embeddings,\n)\nThat's all you need to do to encode a full collection! Your documents are indexed and ready to be queried:\n# Step 5.1: Initialize the ColBERT retriever\nretriever = retrieve.ColBERT(index=index)\n# Step 2: Encode the queries\nqueries_embeddings = model.encode(\n[\"query for document 3\", \"query for document 1\"],\nbatch_size=32,\nis_query=True,  #  # Ensure that it is set to False to indicate that these are queries\nshow_progress_bar=True,\n)\n# Step 3: Retrieve top-k documents\nscores = retriever.retrieve(\nqueries_embeddings=queries_embeddings,\nk=10,  # Retrieve the top 10 matches for each query\n)\nReranking\nThanks to its extreme parameter efficiency, this model is particularly well-suited to being used as a re-ranker following an even more lightweight first stage retrieval, such as static embeding models. Re-ranking is just as straigthforward:\nfrom pylate import rank, models\n# Load the model\nmodel = models.ColBERT(\nmodel_name_or_path=\"mixedbread-ai/mxbai-edge-colbert-v0-17m\",\n)\n# Define queries and documents\nqueries = [\n\"query A\",\n\"query B\",\n]\ndocuments = [\n[\"document A\", \"document B\"],\n[\"document 1\", \"document C\", \"document B\"],\n]\ndocuments_ids = [\n[1, 2],\n[1, 3, 2],\n]\n# Embed them\nqueries_embeddings = model.encode(\nqueries,\nis_query=True,\n)\ndocuments_embeddings = model.encode(\ndocuments,\nis_query=False,\n)\n# Perform reranking\nreranked_documents = rank.rerank(\ndocuments_ids=documents_ids,\nqueries_embeddings=queries_embeddings,\ndocuments_embeddings=documents_embeddings,\n)\nEvaluation\nResults on BEIR\nModel\nAVG\nMS MARCO\nSciFact\nTouche\nFiQA\nTREC-COVID\nNQ\nDBPedia\nLarge Models (>100M)\nGTE-ModernColBERT-v1\n0.547\n0.453\n0.763\n0.312\n0.453\n0.836\n0.618\n0.480\nColBERTv2\n0.488\n0.456\n0.693\n0.263\n0.356\n0.733\n0.562\n0.446\nMedium Models (<35M)\nmxbai-edge-colbert-v0-32m\n0.521\n0.450\n0.740\n0.313\n0.390\n0.775\n0.600\n0.455\nanswerai-colbert-small-v1\n0.534\n0.434\n0.740\n0.250\n0.410\n0.831\n0.594\n0.464\nbge-small-en-v1.5\n0.517\n0.408\n0.713\n0.260\n0.403\n0.759\n0.502\n0.400\nsnowflake-s\n0.519\n0.402\n0.722\n0.235\n0.407\n0.801\n0.509\n0.410\nSmall Models (<25M)\nmxbai-edge-colbert-v0-17m\n0.490\n0.416\n0.719\n0.316\n0.326\n0.713\n0.551\n0.410\ncolbert-muvera-micro\n0.394\n0.364\n0.662\n0.251\n0.254\n0.561\n0.386\n0.332\nall-MiniLM-L6-v2\n0.419\n0.365\n0.645\n0.169\n0.369\n0.472\n0.439\n0.323\nResults on LongEmbed\nModel\nAVG\nLarge Models (>100M)\nGTE-ModernColBERT-v1 (32k)\n0.898\nGTE-ModernColBERT-v1 (4k)\n0.809\ngranite-embedding-english-r2\n0.656\nColBERTv2\n0.428\nMedium Models (<50M)\nmxbai-edge-colbert-v0-32m (32k)\n0.849\nmxbai-edge-colbert-v0-32m (4k)\n0.783\ngranite-embedding-small-english-r2\n0.637\nanswerai-colbert-small-v1\n0.441\nbge-small-en-v1.5\n0.312\nsnowflake-arctic-embed-s\n0.356\nSmall Models (<25M)\nmxbai-edge-colbert-v0-17m (32k)\n0.847\nmxbai-edge-colbert-v0-17m (4k)\n0.776\nall-MiniLM-L6-v2\n0.298\ncolbert-muvera-micro\n0.405\nFor more details on evaluations, please read our Tech Report.\nCommunity\nPlease join our Discord Community and share your feedback and thoughts! We are here to help and also always happy to chat.\nLicense\nApache 2.0\nCitation\nIf you use our model, please cite the associated tech report:\n@misc{takehi2025fantasticsmallretrieverstrain,\ntitle={Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report},\nauthor={Rikiya Takehi and Benjamin ClaviÃ© and Sean Lee and Aamir Shakir},\nyear={2025},\neprint={2510.14880},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2510.14880},\n}\nIf you specifically use its projection heads, or discuss their effect, please cite our report on using different projections for ColBERT models:\n@misc{clavie2025simpleprojectionvariantsimprove,\ntitle={Simple Projection Variants Improve ColBERT Performance},\nauthor={Benjamin ClaviÃ© and Sean Lee and Rikiya Takehi and Aamir Shakir and Makoto P. Kato},\nyear={2025},\neprint={2510.12327},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2510.12327},\n}\nFinally, if you use PyLate in your work, please cite PyLate itself:\n@misc{PyLate,\ntitle={PyLate: Flexible Training and Retrieval for Late Interaction Models},\nauthor={Chaffin, Antoine and Sourty, RaphaÃ«l},\nurl={https://github.com/lightonai/pylate},\nyear={2024}\n}",
    "Dolphy-AI/Dolphy-1.0": "Dolphy-1.0-GGUF\nAvailable Model files:\nDolphy-1.0-GGUF\nDolphy AI's First step into the world of Machine Learning.\nThis is a fine tune of Qwen 3 4B 2507 Instruct, a lightweight but capable model that can outperform many larger models. We used Unsloth LoRA Finetuning on an extensive range of high quality diverse datasets. Dolphy 1.0 was fine tuned on 1.5M examples throughout it's fine tuning pipeline.\nDolphy 1.0 was trained in 20 different datasets, with 1.5M examples in total. Every dataset was carefully curated to extend the Qwen's behaviour to create a Small Model with Superior dominance over the 4B catagory.\nCompatibility\nAs Dolphy 1.0 and Qwen3 2507 Instruct models share the same base, Dolphy 1.0 is compatible with Qwen3's extensive tool use, function calling and multilingual capibilities. The tokenizer is unchanged and the model archetecture is intact.\nYou can also find this model in upcoming Dolphy AI releases.\nHow to run locally\nFor running locally we recommend using our GGUF models for fast inference. Nonetheless you can run the safetensors by using Hugging Face Transformers.\nOur model requires no steps before inference and are ready.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Dolphy-AI/Dolphy-1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, safe_serialization=True)\nprint(\"Type 'exit' to quit.\\n\")\nwhile True:\nuser_input = input(\"Enter your prompt: \")\nif user_input.lower() == \"exit\":\nprint(\"Goodbye! Thank you for using Dolphy 1.0\")\nbreak\n# Tokenize and generate\ninputs = tokenizer(user_input, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=100)\n# Decode and print result\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"\\nDolphy 1.0 response:\\n\", response, \"\\n\")\nAvailable Model files:\nmodel-00001-of-00002.safetensors\nmodel-00002-of-00002.safetensors",
    "teemosliang/SDPose-Wholebody": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation (WholeBody - 133 Keypoints)\nModel Description\nModel Architecture\nSupported Keypoints (COCO Wholebody Format)\nIntended Use\nHow to Use\nInstallation\nTraining Data\nDatasets\nPreprocessing\nComparison with Baselines\nCitation\nLicense\nAdditional Resources\nSDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation (WholeBody - 133 Keypoints)\nModel Description\nSDPose is a state-of-the-art human pose estimation model that leverages the powerful visual priors from Stable Diffusion to achieve exceptional performance on out-of-distribution (OOD) scenarios. This model variant estimates 133 wholebody keypoints, including body, hands, face, feet.\nModel Architecture\nSDPose employs a U-Net backbone initialized with Stable Diffusion v2 weights, combined with a specialized heatmap head for keypoint prediction. The model operates in a top-down manner:\nPerson Detection: Detect human bounding boxes using an object detector (e.g., YOLO11-x)\nPose Estimation: Crop and estimate 17 body keypoints for each detected person\nHeatmap Generation: Produce confidence heatmaps for precise keypoint estimation\nModel Specifications:\nBackbone: Stable Diffusion v2 U-Net (fine-tuned; minimal architectural changes)\nHead: Custom heatmap prediction head\nInput Resolution: 1024Ã—768 (HÃ—W)\nOutput: 133 keypoint heatmaps + coordinates with confidence scores\nFramework: MMPose\nSupported Keypoints (COCO Wholebody Format)\nThe model predicts 133 body keypoints following the COCO Wholebody keypoint format.\nIntended Use\nHuman pose estimation in natural images\nPose estimation in artistic and stylized domains (paintings, anime, sketches)\nAnimation and video pose tracking\nCross-domain pose analysis and research\nApplications requiring robust pose estimation under distribution shifts\nHow to Use\nInstallation\n# Clone the repository\ngit clone https://github.com/t-s-liang/SDPose-OOD.git\ncd SDPose-OOD\n# Install dependencies\npip install -r requirements.txt\n# Download YOLO11-x for human detection\nwget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt -P models/\n# Launch Gradio interface\ncd gradio_app\nbash launch_gradio.sh\nTraining Data\nDatasets\nTrained exclusively on COCO-2017 train2017 (no extra data).\nCOCO-Wholebody (Common Objects in Context): 200K+ images with 133 wholebody keypoints\nPreprocessing\nImages are resized and cropped to 1024Ã—768 resolution\nAugmentation: random horizontal flip, half-body & bbox transforms, UDP affine; Albumentations (Gaussian/Median blur, coarse dropout).\nHeatmaps: UDP codec (MMPose style).\nComparison with Baselines\nSDPose significantly outperforms traditional pose estimation models (e.g., Sapiens) on out-of-distribution benchmarks while maintaining competitive performance on in-domain data.\nSee our paper for comprehensive evaluation results.\nCitation\nIf you use SDPose in your research, please cite our paper:\n@misc{liang2025sdposeexploitingdiffusionpriors,\ntitle={SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation},\nauthor={Shuang Liang and Jing He and Chuanmeizhi Wang and Lejun Liao and Guo Zhang and Yingcong Chen and Yuan Yuan},\nyear={2025},\neprint={2509.24980},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2509.24980},\n}\nLicense\nThis model is released under the MIT License.\nAdditional Resources\nğŸŒ Project Website: https://t-s-liang.github.io/SDPose\nğŸ“„ Paper: arXiv:2509.24980\nğŸ’» Code Repository: GitHub\nğŸ¤— Demo: HuggingFace Space\nğŸ“§ Contact: tsliang2001@gmail.com\nâ­ Star us on GitHub â€” it motivates us a lot!",
    "FlareRebellion/WeirdCompound-v1.7-24b": "WeirdCompound-v1.7-24b\nMerge Details\nNotes\nChangelog\nMerge Method\nModels Merged\nConfiguration\nWeirdCompound-v1.7-24b\nThis is a merge of pre-trained language models created using mergekit.\nMerge Details\nNotes\nThis is a multi-stage merge. There's little method to my madness and I just stopped when I arrived at something that I liked.\nStarting point was DepravedCartographer-v1.0-24b with slight changes.\nChangelog\nv1.1\n/intermediate/model/B: replaced anthracite-core/Mistral-Small-3.1-24B-Instruct-2503-HF with anthracite-core/Mistral-Small-3.2-24B-Instruct-2506-ChatML\nv1.2\n/intermediate/model/B: replaced anthracite-core/Mistral-Small-3.2-24B-Instruct-2506-ChatML with anthracite-core/Mistral-Small-3.2-24B-Instruct-2506-Text-Only for default tokenizer config.\nv1.3\n/intermediate/model/A: replaced TheDrummer/Cydonia-24B-v3 with TheDrummer/Cydonia-24B-v4\n/intermediate/model/A: replaced Doctor-Shotgun/MS3.1-24B-Magnum-Diamond with Doctor-Shotgun/MS3.2-24B-Magnum-Diamond\n/intermediate/model/A: replaced Delta-Vector/Austral-24B-Winton with Delta-Vector/MS3.2-Austral-Winton\nv1.4\n/intermediate/model/C: change recipe to use Doctor-Shotgun/MS3.2-24B-Magnum-Diamond and Delta-Vector/MS3.2-Austral-Winton\nv1.5\ndidn't particularly care for v1.4. IMHO v1.3 was better\n/intermediate/model/A: replaced Doctor-Shotgun/MS3.2-24B-Magnum-Diamond with zerofata/MS3.2-PaintedFantasy-24B\n/intermediate/model/C: change recipe to use PocketDoc/Dans-PersonalityEngine-V1.3.0-24b and zerofata/MS3.2-PaintedFantasy-24B\nv1.6\n/intermediate/model/A: updated Cydonia to TheDrummer/Cydonia-24B-v4.1\n/intermediate/model/A: updated MS3.2-PaintedFantasy-24B to zerofata/MS3.2-PaintedFantasy-v2-24B\n/intermediate/model/A: removed Delta-Vector/MS3.2-Austral-Winton\n/intermediate/model/A: added Doctor-Shotgun/MS3.2-24B-Magnum-Diamond and CrucibleLab/M3.2-24B-Loki-V1.3\n/intermediate/model/B: changed weight to 0.45\n/intermediate/model/C: replaced zerofata/MS3.2-PaintedFantasy-24B with CrucibleLab/M3.2-24B-Loki-V1.3 and fiddled with weights\nv1.7\nQuick disclaimer: A new version doesn't automatically mean 'better'. If you're happy with v1.6 or v1.2, they won't go away. This one has a different vibe than v1.6, but it takes me weeks to get a feel for the prose, so here it is. Shoutout to @TheDrummer for the never-ending supply of great finetunes.\n/intermediate/model/A: updated Cydonia to TheDrummer/Cydonia-24B-v4.2.0\n/intermediate/model/A: replaced Doctor-Shotgun/MS3.2-24B-Magnum-Diamond with Delta-Vector/MS3.2-Austral-Winton\nMerge Method\nThis model was merged using the Model Stock merge method using TheDrummer/Cydonia-24B-v4 as a base.\nThis model was merged using the SLERP merge method.\nThis model was merged using the NuSLERP merge method using /intermediate/model/B as a base.\nModels Merged\nTheDrummer/Cydonia-24B-v4.2.0\naixonlab/Eurydice-24b-v3.5\nPocketDoc/Dans-PersonalityEngine-V1.3.0-24b\nzerofata/MS3.2-PaintedFantasy-v2-24B\nCrucibleLab/M3.2-24B-Loki-V1.3\nDelta-Vector/Austral-24B-Winton\nanthracite-core/Mistral-Small-3.2-24B-Instruct-2506-Text-Only\n/intermediate/model/A\n/intermediate/model/B\n/intermediate/model/C\nConfiguration\nThe following YAML configuration was used to produce this model:\nbase_model: TheDrummer/Cydonia-24B-v4.2.0 # Cydonia v4.2.0\nmerge_method: model_stock\ndtype: bfloat16\nmodels:\n- model: aixonlab/Eurydice-24b-v3.5 # storytelling / RP\n- model: TheDrummer/Cydonia-24B-v4.2.0 # sprinkle in some extra Cydonia\n- model: PocketDoc/Dans-PersonalityEngine-V1.3.0-24b # Prompt Adherence\n- model: CrucibleLab/M3.2-24B-Loki-V1.3 # Loki\n- model: zerofata/MS3.2-PaintedFantasy-v2-24B  # animu\n- model: Delta-Vector/Austral-24B-Winton  # Adventure\nâ†’ /intermediate/model/A â†’\nmerge_method: slerp\ndtype: bfloat16\nbase_model: anthracite-core/Mistral-Small-3.2-24B-Instruct-2506-Text-Only\nmodels:\n- model: /intermediate/model/A\nparameters:\nt: 0.45\nâ†’ /intermediate/model/B â†’\nmerge_method: nuslerp\ndtype: bfloat16\nbase_model: /intermediate/model/B\n- model: PocketDoc/Dans-PersonalityEngine-V1.3.0-24b\nparameters:\nweight: 0.4\n- model: CrucibleLab/M3.2-24B-Loki-V1.3\nparameters:\nweight: 0.6\nâ†’ /intermediate/model/C â†’\nmerge_method: slerp\ndtype: bfloat16\nbase_model: /intermediate/model/B\nmodels:\n- model: /intermediate/model/C\nparameters:\nt: 0.5\nâ†’ WeirdCompound-v1.7-24b",
    "adehoffer/promoter-gpt-model": "README.md exists but content is empty.",
    "sdobson/nanochat": "nanochat\nModel Description\nArchitecture\nTraining Details\nTraining Data\nTraining Procedure\nPerformance\nBenchmark Results\nIntended Use\nDirect Use\nDownstream Use\nOut-of-Scope Use\nLimitations and Bias\nInference guide\nCitation\nModel Card Author\nnanochat\nnanochat is a 561M parameter transformer language model trained for conversational AI tasks. This model demonstrates that capable chat models\ncan be trained efficiently on modest hardware budgets (~$100 on 8x H100 GPUs).\nRead about the process at https://samdobson.uk/posts/training-a-chatgpt-clone-for-cheap/\nChat with the model at https://huggingface.co/spaces/sdobson/nanochat\nModel Description\nDeveloped by: Andrej Karpathy\nTrained by: Sam Dobson\nModel type: Transformer-based causal language model\nLanguage(s): English\nLicense: MIT\nParameters: 560,988,160 (~561M)\nArchitecture\nLayers: 20\nHidden size: 1280 channels\nAttention heads: 10\nHead dimension: 128\nVocabulary size: 65,536 tokens\nTraining Details\nTraining Data\nnanochat was trained in multiple stages:\nPretraining: 100B token subset of FineWeb-EDU (11.2B tokens processed)\nMidtraining: SmolTalk conversations, MMLU multiple choice questions, GSM8K math problems\nSupervised Fine-tuning (SFT): Conversational adaptation data\nTraining Procedure\nTokenization\nCustom Rust-based tokenizer\nVocabulary: 65,536 tokens\nCompression ratio: 4.8 characters per token\nTraining Infrastructure\nHardware: 8x H100 GPUs (Lambda GPU Cloud)\nTraining time: ~3 hours for pretraining stage\nEstimated compute: ~4e19 FLOPs\nTotal cost: ~$100\nTraining Stages\nThe model was trained in three stages:\nPretraining on web text (FineWeb-EDU)\nMidtraining on domain-specific datasets (reasoning, conversation, maths)\nSupervised fine-tuning for chat optimisation\nPerformance\nBenchmark Results\nBenchmark\nScore\nDescription\nMMLU\n23.99%\nMultitask language understanding\nGSM8K\n4.47%\nGrade school math problems\nHumanEval\n6.71%\nPython code generation\nARC-Easy\n24.79%\nScience questions (easy)\nARC-Challenge\n24.32%\nScience questions (hard)\nChatCORE\n1.73%\nConversational reasoning\nIntended Use\nDirect Use\nnanochat is designed for:\nConversational AI applications\nResearch on efficient language model training\nEducational purposes for understanding LLM training pipelines\nLow-resource deployment scenarios\nDownstream Use\nThe model can be fine-tuned for specific conversational tasks or used as a base model for further domain adaptation.\nOut-of-Scope Use\nProduction-grade conversational AI (the model is relatively small and has limited capabilities)\nTasks requiring specialised knowledge or high accuracy\nCritical applications where errors could cause harm\nLimitations and Bias\nSmall scale: At 561M parameters, this model has significantly fewer capabilities than larger models (1B+ parameters)\nLimited training: Trained on only 11.2B tokens, which is modest by modern standards\nPerformance: Benchmark scores indicate limited reasoning and mathematical capabilities\nBias: Inherits biases from training data (FineWeb-EDU, SmolTalk, etc.)\nLanguage: English-only\nInference guide\nSimon Willison created a script to allow this to run on CPU on MacOS:\ncd /tmp\ngit clone https://huggingface.co/sdobson/nanochat\nuv run https://gist.githubusercontent.com/simonw/912623bf00d6c13cc0211508969a100a/raw/80f79c6a6f1e1b5d4485368ef3ddafa5ce853131/generate_cpu.py \\\n--model-dir /tmp/nanochat \\\n--prompt \"Tell me about dogs.\"\nOtherwise you can:\nDownload all files\nPut tokenizer.pkl and token_bytes.pt in ~/.cache/nanochat/tokenizer\nPut model_000650.pt and meta_000650.json in ~/.cache/nanochat/chatsft_checkpoints/d20\nClone https://github.com/karpathy/nanochat\nRun uv sync followed by uv run python -m scripts.chat_web\nCitation\nRepository: github.com/karpathy/nanochat\n@software{nanochat2025,\nauthor = {Karpathy, Andrej},\ntitle = {nanochat: A 561M parameter conversational language model},\nyear = {2025},\nurl = {https://github.com/karpathy/nanochat}\n}\nModel Card Author\nSam Dobson",
    "NexaAI/Qwen3-VL-4B-Instruct-GGUF": "Qwen3-VL-4B-Instruct\nQuickstart\nModel Description\nFeatures\nUse Cases\nInputs and Outputs\nLicense\nQwen3-VL-4B-Instruct\nNote currently only NexaSDK supports this model's GGUF.\nRun Qwen3-VL-4B-Instruct optimized for CPU/GPU with NexaSDK.\nQuickstart\nInstall NexaSDK\nRun the model locally with one line of code:\nnexa infer NexaAI/Qwen3-VL-4B-Instruct-GGUF\nModel Description\nQwen3-VL-4B-Instruct is a 4-billion-parameter instruction-tuned multimodal large language model from Alibaba Cloudâ€™s Qwen team.As part of the Qwen3-VL series, it fuses powerful vision-language understanding with conversational fine-tuning, optimized for real-world applications such as chat-based reasoning, document analysis, and visual dialogue.\nThe Instruct variant is tuned for following user prompts naturally and safely â€” producing concise, relevant, and user-aligned responses across text, image, and video contexts.\nFeatures\nInstruction-Following: Optimized for dialogue, explanation, and user-friendly task completion.\nVision-Language Fusion: Understands and reasons across text, images, and video frames.\nMultilingual Capability: Handles multiple languages for diverse global use cases.\nContextual Coherence: Balances reasoning ability with natural, grounded conversational tone.\nLightweight & Deployable: 4B parameters make it efficient for edge and device-level inference.\nUse Cases\nVisual chatbots and assistants\nImage captioning and scene understanding\nChart, document, or screenshot analysis\nEducational or tutoring systems with visual inputs\nMultilingual, multimodal question answering\nInputs and Outputs\nInput:\nText prompts, image(s), or mixed multimodal instructions.\nOutput:\nNatural-language responses or visual reasoning explanations.\nCan return structured text (summaries, captions, answers, etc.) depending on the prompt.\nLicense\nRefer to the official Qwen license for terms of use and redistribution.",
    "NexaAI/Qwen3-VL-4B-Thinking-GGUF": "Qwen3-VL-4B-Thinking\nQuickstart\nModel Description\nFeatures\nUse Cases\nInputs and Outputs\nLicense\nQwen3-VL-4B-Thinking\nNote currently only NexaSDK supports this model's GGUF.\nRun Qwen3-VL-4B-Thinking optimized for CPU/GPU with NexaSDK.\nQuickstart\nInstall NexaSDK\nRun the model locally with one line of code:\nnexa infer NexaAI/Qwen3-VL-4B-Thinking-GGUF\nModel Description\nQwen3-VL-4B-Thinking is a 4-billion-parameter multimodal large language model from the Qwen team at Alibaba Cloud.Part of the Qwen3-VL (Vision-Language) family, it is designed for advanced visual reasoning and chain-of-thought generation across image, text, and video inputs.\nCompared to the Instruct variant, the Thinking model emphasizes deeper multi-step reasoning, analysis, and planning. It produces detailed, structured outputs that reflect intermediate reasoning steps, making it well-suited for research, multimodal understanding, and agentic workflows.\nFeatures\nVision-Language Understanding: Processes images, text, and videos for joint reasoning tasks.\nStructured Thinking Mode: Generates intermediate reasoning traces for better transparency and interpretability.\nHigh Accuracy on Visual QA: Performs strongly on visual question answering, chart reasoning, and document analysis benchmarks.\nMultilingual Support: Understands and responds in multiple languages.\nOptimized for Efficiency: Delivers strong performance at 4B scale for on-device or edge deployment.\nUse Cases\nMultimodal reasoning and visual question answering\nScientific and analytical reasoning tasks involving charts, tables, and documents\nStep-by-step visual explanation or tutoring\nResearch on interpretability and chain-of-thought modeling\nIntegration into agent systems that require structured reasoning\nInputs and Outputs\nInput:\nText, images, or combined multimodal prompts (e.g., image + question)\nOutput:\nGenerated text, reasoning traces, or structured responses\nMay include explicit thought steps or structured JSON reasoning sequences\nLicense\nCheck the official Qwen license for terms of use and redistribution."
}