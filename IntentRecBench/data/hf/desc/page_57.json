{
    "JungleLee/bert-toxic-comment-classification": "Model description\nHow to use\nTraining data\nEvaluation results\nModel description\nThis model is a fine-tuned version of the bert-base-uncased model to classify toxic comments.\nHow to use\nYou can use the model with the following code.\nfrom transformers import BertForSequenceClassification, BertTokenizer, TextClassificationPipeline\nmodel_path = \"JungleLee/bert-toxic-comment-classification\"\ntokenizer = BertTokenizer.from_pretrained(model_path)\nmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline(\"You're a fucking nerd.\"))\nTraining data\nThe training data comes from this Kaggle competition. We use 90% of the train.csv data to train the model.\nEvaluation results\nThe model achieves 0.95 AUC in a 1500 rows held-out test set.",
    "1-800-BAD-CODE/punctuation_fullstop_truecase_english": "Model Overview\nUsage\nModel Details\nPunctuation Tokens\nTraining Details\nTraining Framework\nTraining Data\nLimitations\nDomain\nNoisy Training Data\nAcronyms and Abbreviations\nSentence Boundary Detection Targets\nEvaluation\nTest Data and Example Generation\nResults\nFun Facts\nEmbeddings\nModel Overview\nThis model accepts as input lower-cased, unpunctuated English text and performs in one pass punctuation restoration, true-casing (capitalization), and sentence boundary detection (segmentation).\nIn contast to many similar models, this model can predict punctuated acronyms (e.g., \"U.S.\") via a special \"acronym\" class, as well as arbitarily-capitalized words (NATO, McDonald's, etc.) via multi-label true-casing predictions.\nWidget note: The text generation widget doesn't seem to respect line breaks.\nInstead, the pipeline inserts a new line token \\n in the text where the model has predicted sentence boundaries (line breaks).\nUsage\nThe easy way to use this model is to install punctuators:\npip install punctuators\nIf this package is broken, please let me know in the community tab (I update it for each model and break it a lot!).\nLet's punctuate my weekend recap, as well as few interesting sentences with acronyms and abbreviations that I made up or found on Wikipedia:\nExample Usage\nfrom typing import List\nfrom punctuators.models import PunctCapSegModelONNX\n# Instantiate this model\n# This will download the ONNX and SPE models. To clean up, delete this model from your HF cache directory.\nm = PunctCapSegModelONNX.from_pretrained(\"pcs_en\")\n# Define some input texts to punctuate\ninput_texts: List[str] = [\n# Literally my weekend\n\"i woke up at 6 am and took the dog for a hike in the metacomet mountains we like to take morning adventures on the weekends\",\n\"despite being mid march it snowed overnight and into the morning here in connecticut it was snowier up in the mountains than in the farmington valley where i live\",\n\"when i got home i trained this model on the lambda cloud on an a100 gpu with about 10 million lines of text the total budget was less than 5 dollars\",\n# Real acronyms in sentences that I made up\n\"george hw bush was the president of the us for 8 years\",\n\"i saw mr smith at the store he was shopping for a new lawn mower i suggested he get one of those new battery operated ones they're so much quieter\",\n# See how the model performs on made-up acronyms\n\"i went to the fgw store and bought a new tg optical scope\",\n# First few sentences from today's featured article summary on wikipedia\n\"it's that man again itma was a radio comedy programme that was broadcast by the bbc for twelve series from 1939 to 1949 featuring tommy handley in the central role itma was a character driven comedy whose satirical targets included officialdom and the proliferation of minor wartime regulations parts of the scripts were rewritten in the hours before the broadcast to ensure topicality\"\n]\nresults: List[List[str]] = m.infer(input_texts)\nfor input_text, output_texts in zip(input_texts, results):\nprint(f\"Input: {input_text}\")\nprint(f\"Outputs:\")\nfor text in output_texts:\nprint(f\"\\t{text}\")\nprint()\nExact output may vary based on the model version; here is the current output:\nExpected Output\nIn: i woke up at 6 am and took the dog for a hike in the metacomet mountains we like to take morning adventures on the weekends\nOut: I woke up at 6 a.m. and took the dog for a hike in the Metacomet Mountains.\nOut: We like to take morning adventures on the weekends.\nIn: despite being mid march it snowed overnight and into the morning here in connecticut it was snowier up in the mountains than in the farmington valley where i live\nOut: Despite being mid March, it snowed overnight and into the morning.\nOut: Here in Connecticut, it was snowier up in the mountains than in the Farmington Valley where I live.\nIn: when i got home i trained this model on the lambda cloud on an a100 gpu with about 10 million lines of text the total budget was less than 5 dollars\nOut: When I got home, I trained this model on the Lambda Cloud.\nOut: On an A100 GPU with about 10 million lines of text, the total budget was less than 5 dollars.\nIn: george hw bush was the president of the us for 8 years\nOut: George H.W. Bush was the president of the U.S. for 8 years.\nIn: i saw mr smith at the store he was shopping for a new lawn mower i suggested he get one of those new battery operated ones they're so much quieter\nOut: I saw Mr. Smith at the store he was shopping for a new lawn mower.\nOut: I suggested he get one of those new battery operated ones.\nOut: They're so much quieter.\nIn: i went to the fgw store and bought a new tg optical scope\nOut: I went to the FGW store and bought a new TG optical scope.\nIn: it's that man again itma was a radio comedy programme that was broadcast by the bbc for twelve series from 1939 to 1949 featuring tommy handley in the central role itma was a character driven comedy whose satirical targets included officialdom and the proliferation of minor wartime regulations parts of the scripts were rewritten in the hours before the broadcast to ensure topicality\nOut: It's that man again.\nOut: ITMA was a radio comedy programme that was broadcast by the BBC for Twelve Series from 1939 to 1949, featuring Tommy Handley.\nOut: In the central role, ITMA was a character driven comedy whose satirical targets included officialdom and the proliferation of minor wartime regulations.\nOut: Parts of the scripts were rewritten in the hours before the broadcast to ensure topicality.\nModel Details\nThis model implements the graph shown below, with brief descriptions for each step following.\nEncoding:\nThe model begins by tokenizing the text with a subword tokenizer.\nThe tokenizer used here is a SentencePiece model with a vocabulary size of 32k.\nNext, the input sequence is encoded with a base-sized Transformer, consisting of 6 layers with a model dimension of 512.\nPunctuation:\nThe encoded sequence is then fed into a feed-forward classification network to predict punctuation tokens.\nPunctation is predicted once per subword, to allow acronyms to be properly punctuated.\nAn indiret benefit of per-subword prediction is to allow the model to run in a graph generalized for continuous-script languages, e.g., Chinese.\nSentence boundary detection\nFor sentence boundary detection, we condition the model on punctuation via embeddings.\nEach punctuation prediction is used to select an embedding for that token, which is concatenated to the encoded representation.\nThe SBD head analyzes both the encoding of the un-punctuated sequence and the puncutation predictions, and predicts which tokens are sentence boundaries.\nShift and concat sentence boundaries\nIn English, the first character of each sentence should be upper-cased.\nThus, we should feed the sentence boundary information to the true-case classification network.\nSince the true-case classification network is feed-forward and has no temporal context, each time step must embed whether it is the first word of a sentence.\nTherefore, we shift the binary sentence boundary decisions to the right by one: if token N-1 is a sentence boundary, token N is the first word of a sentence.\nConcatenating this with the encoded text, each time step contains whether it is the first word of a sentence as predicted by the SBD head.\nTrue-case prediction\nArmed with the knowledge of punctation and sentence boundaries, a classification network predicts true-casing.\nSince true-casing should be done on a per-character basis, the classification network makes N predictions per token, where N is the length of the subtoken.\n(In practice, N is the longest possible subword, and the extra predictions are ignored).\nThis scheme captures acronyms, e.g., \"NATO\", as well as bi-capitalized words, e.g., \"MacDonald\".\nThe model's maximum length is 256 subtokens, due to the limit of the trained embeddings.\nHowever, the punctuators package\nas described above will transparently predict on overlapping subgsegments of long inputs and fuse the results before returning output,\nallowing inputs to be arbitrarily long.\nPunctuation Tokens\nThis model predicts the following set of punctuation tokens:\nToken\nDescription\nNULL\nPredict no punctuation\nACRONYM\nEvery character in this subword ends with a period\n.\nLatin full stop\n,\nLatin comma\n?\nLatin question mark\nTraining Details\nTraining Framework\nThis model was trained on a forked branch of the NeMo framework.\nTraining Data\nThis model was trained with News Crawl data from WMT.\nApproximately 10M lines were used from the years 2021 and 2012.\nThe latter was used to attempt to reduce bias: annual news is typically dominated by a few topics, and 2021 is dominated by COVID discussions.\nLimitations\nDomain\nThis model was trained on news data, and may not perform well on conversational or informal data.\nNoisy Training Data\nThe training data was noisy, and no manual cleaning was utilized.\nAcronyms and Abbreviations\nAcronyms and abbreviations are especially noisy; the table below shows how many variations of each token appear in the training data.\nToken\nCount\nMr\n115232\nMr.\n108212\nToken\nCount\nU.S.\n85324\nUS\n37332\nU.S\n354\nU.s\n108\nu.S.\n65\nThus, the model's acronym and abbreviation predictions may be a bit unpredictable.\nSentence Boundary Detection Targets\nAn assumption for sentence boundary detection targets is that each line of the input data is exactly one sentence.\nHowever, a non-negligible portion of the training data contains multiple sentences per line.\nThus, the SBD head may miss an obvious sentence boundary if it's similar to an error seen in the training data.\nEvaluation\nIn these metrics, keep in mind that\nThe data is noisy\nSentence boundaries and true-casing are conditioned on predicted punctuation, which is the most difficult task and sometimes incorrect.\nWhen conditioning on reference punctuation, true-casing and SBD metrics are much higher w.r.t. the reference targets.\nPunctuation can be subjective. E.g.,\nHello Frank, how's it going?\nor\nHello Frank. How's it going?\nWhen the sentences are longer and more practical, these ambiguities abound and affect all 3 analytics.\nTest Data and Example Generation\nEach test example was generated using the following procedure:\nConcatenate 10 random sentences\nLower-case the concatenated sentence\nRemove all punctuation\nThe data is a held-out portion of News Crawl, which has been deduplicated.\n3,000 lines of data was used, generating 3,000 unique examples of 10 sentences each.\nResults\nPunctuation Report\nlabel                                                precision    recall       f1           support\n<NULL> (label_id: 0)                                    98.83      98.49      98.66     446496\n<ACRONYM> (label_id: 1)                                 74.15      94.26      83.01        697\n. (label_id: 2)                                         90.64      92.99      91.80      30002\n, (label_id: 3)                                         77.19      79.13      78.15      23321\n? (label_id: 4)                                         76.58      74.56      75.56       1022\n-------------------\nmicro avg                                               97.21      97.21      97.21     501538\nmacro avg                                               83.48      87.89      85.44     501538\nweighted avg                                            97.25      97.21      97.23     501538\nTrue-casing Report\n# With predicted punctuation (not aligned with targets)\nlabel                                                precision    recall       f1           support\nLOWER (label_id: 0)                                     99.76      99.72      99.74    2020678\nUPPER (label_id: 1)                                     93.32      94.20      93.76      83873\n-------------------\nmicro avg                                               99.50      99.50      99.50    2104551\nmacro avg                                               96.54      96.96      96.75    2104551\nweighted avg                                            99.50      99.50      99.50    2104551\n# With reference punctuation (punctuation matches targets)\nlabel                                                precision    recall       f1           support\nLOWER (label_id: 0)                                     99.83      99.81      99.82    2020678\nUPPER (label_id: 1)                                     95.51      95.90      95.71      83873\n-------------------\nmicro avg                                               99.66      99.66      99.66    2104551\nmacro avg                                               97.67      97.86      97.76    2104551\nweighted avg                                            99.66      99.66      99.66    2104551\nSentence Boundary Detection report\n# With predicted punctuation (not aligned with targets)\nlabel                                                precision    recall       f1           support\nNOSTOP (label_id: 0)                                    99.59      99.45      99.52     471608\nFULLSTOP (label_id: 1)                                  91.47      93.53      92.49      29930\n-------------------\nmicro avg                                               99.09      99.09      99.09     501538\nmacro avg                                               95.53      96.49      96.00     501538\nweighted avg                                            99.10      99.09      99.10     501538\n# With reference punctuation (punctuation matches targets)\nlabel                                                precision    recall       f1           support\nNOSTOP (label_id: 0)                                   100.00      99.97      99.98     471608\nFULLSTOP (label_id: 1)                                  99.63      99.93      99.78      32923\n-------------------\nmicro avg                                               99.97      99.97      99.97     504531\nmacro avg                                               99.81      99.95      99.88     504531\nweighted avg                                            99.97      99.97      99.97     504531\nFun Facts\nSome fun facts are examined in this section.\nEmbeddings\nLet's examine the embeddings (see graph above) to see if the model meaningfully employed them.\nWe show here the cosine similarity between the embeddings of each token:\nNULL\nACRONYM\n.\n,\n?\nNULL\n1.00\nACRONYM\n-0.49\n1.00\n.\n-1.00\n0.48\n1.00\n,\n1.00\n-0.48\n-1.00\n1.00\n?\n-1.00\n0.49\n1.00\n-1.00\n1.00\nRecall that these embeddings are used to predict sentence boundaries... thus we should expect full stops to cluster.\nIndeed, we see that NULL and \",\" are exactly the same, because neither have an implication on sentence boundaries.\nNext, we see that \".\" and \"?\" are exactly the same, because w.r.t. SBD these are exactly the same: strong full stop implications.\n(Though, we may expect some difference between these tokens, given that \".\" is predicted after abbreviations, e.g., 'Mr.', that are not full stops.)\nFurther, we see that \".\" and \"?\" are exactly the opposite of NULL.\nThis is expected since these tokens typically imply sentence boundaries, whereas NULL and \",\" never do.\nLastly, we see that ACRONYM is similar to, but not the same as, the full stops \".\" and \"?\",\nand far from, but not the opposite of, NULL and \",\".\nIntuition suggests this is because acronyms can be full stops (\"I live in the northern U.S. It's cold here.\") or not (\"It's 5 a.m. and I'm tired.\").",
    "sonoisa/sentence-luke-japanese-base-lite": "‰Ωø„ÅÑÊñπ\nThis is a Japanese sentence-LUKE model.\nÊó•Êú¨Ë™ûÁî®Sentence-LUKE„É¢„Éá„É´„Åß„Åô„ÄÇ\nÊó•Êú¨Ë™ûSentence-BERT„É¢„Éá„É´„Å®Âêå‰∏Ä„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®Ë®≠ÂÆö„ÅßÂ≠¶Áøí„Åó„Åæ„Åó„Åü„ÄÇÊâãÂÖÉ„ÅÆÈùûÂÖ¨Èñã„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅØ„ÄÅÊó•Êú¨Ë™ûSentence-BERT„É¢„Éá„É´„Å®ÊØî„Åπ„Å¶ÂÆöÈáèÁöÑ„Å™Á≤æÂ∫¶„ÅåÂêåÁ≠â„Äú0.5ptÁ®ãÂ∫¶È´ò„Åè„ÄÅÂÆöÊÄßÁöÑ„Å™Á≤æÂ∫¶„ÅØÊú¨„É¢„Éá„É´„ÅÆÊñπ„ÅåÈ´ò„ÅÑÁµêÊûú„Åß„Åó„Åü„ÄÇ\n‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Å®„Åó„Å¶studio-ousia/luke-japanese-base-lite„ÇíÂà©Áî®„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åó„Åü„ÄÇ\nÊé®Ë´ñ„ÅÆÂÆüË°å„Å´„ÅØSentencePiece„ÅåÂøÖË¶Å„Åß„ÅôÔºàpip install sentencepieceÔºâ„ÄÇ\n‰Ωø„ÅÑÊñπ\nfrom transformers import MLukeTokenizer, LukeModel\nimport torch\nclass SentenceLukeJapanese:\ndef __init__(self, model_name_or_path, device=None):\nself.tokenizer = MLukeTokenizer.from_pretrained(model_name_or_path)\nself.model = LukeModel.from_pretrained(model_name_or_path)\nself.model.eval()\nif device is None:\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nself.device = torch.device(device)\nself.model.to(device)\ndef _mean_pooling(self, model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n@torch.no_grad()\ndef encode(self, sentences, batch_size=8):\nall_embeddings = []\niterator = range(0, len(sentences), batch_size)\nfor batch_idx in iterator:\nbatch = sentences[batch_idx:batch_idx + batch_size]\nencoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\ntruncation=True, return_tensors=\"pt\").to(self.device)\nmodel_output = self.model(**encoded_input)\nsentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\nall_embeddings.extend(sentence_embeddings)\nreturn torch.stack(all_embeddings)\nMODEL_NAME = \"sonoisa/sentence-luke-japanese-base-lite\"\nmodel = SentenceLukeJapanese(MODEL_NAME)\nsentences = [\"Êö¥Ëµ∞„Åó„ÅüAI\", \"Êö¥Ëµ∞„Åó„Åü‰∫∫Â∑•Áü•ËÉΩ\"]\nsentence_embeddings = model.encode(sentences, batch_size=8)\nprint(\"Sentence embeddings:\", sentence_embeddings)",
    "medicalai/ClinicalBERT": "ClinicalBERT\nPretraining Data\nModel Pretraining\nPretraining Procedures\nPretraining Hyperparameters\nHow to use the model\nCitation\nClinicalBERT\nThis model card describes the ClinicalBERT model, which was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.\nWe then utilized a large-scale corpus of EHRs from over 3 million patient records to fine tune the base language model.\nPretraining Data\nThe ClinicalBERT model was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.\nModel Pretraining\nPretraining Procedures\nThe ClinicalBERT was initialized from BERT. Then the training followed the principle of masked language model, in which given a piece of text, we randomly replace some tokens by MASKs,\nspecial tokens for masking, and then require the model to predict the original tokens via contextual text.\nPretraining Hyperparameters\nWe used a batch size of 32, a maximum sequence length of 256, and a learning rate of 5e-5 for pre-training our models.\nHow to use the model\nLoad the model via the transformers library:\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\")\nCitation\nPlease cite these articles:\nWang, G., Liu, X., Ying, Z. et al. Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial. Nat Med (2023). https://doi.org/10.1038/s41591-023-02552-9\nWang, G., Liu, X., Liu, H., Yang, G. et al. A Generalist Medical Language Model for Disease Diagnosis Assistance. Nat Med (2025). https://doi.org/10.1038/s41591-024-03416-6",
    "stabilityai/stable-diffusion-2-1-unclip": "Stable Diffusion v2-1-unclip Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEnvironmental Impact\nCitation\nStable Diffusion v2-1-unclip Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.\nThis stable-diffusion-2-1-unclip is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion UnCLIP 2-1-small in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nRunning the pipeline (if you don't swap the scheduler it will run with the default DDIM, in this example we are swapping it to DPMSolverMultistepScheduler):\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\nimport torch\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-unclip-small\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n# get image\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/tarsila_do_amaral.png\"\nimage = load_image(url)\n# run image variation\nimage = pipe(image).images[0]\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "cerebras/Cerebras-GPT-13B": "Cerebras-GPT 13B\nModel Description\nModel Details\nQuickstart\nTraining data\nTraining procedure\nEvaluations\nUses and Limitations\nIntended Use\nOut of Scope Use\nRisk, Bias, Ethical Considerations\nAcknowledgements\nCerebras-GPT 13B\nCheck out our Blog Post and arXiv paper!\nModel Description\nThe Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face.\nThe family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models.\nAll models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter) which is compute-optimal.\nThese models were trained on the Andromeda AI supercomputer comprised of 16 CS-2 wafer scale systems. Cerebras' weight streaming technology simplifies the training of LLMs by disaggregating compute from model storage. This allowed for efficient scaling of training across nodes using simple data parallelism.\nCerebras systems for pre-training and fine tuning are available in the cloud via the Cerebras Model Studio. Cerebras CS-2 compatible checkpoints are available in Cerebras Model Zoo.\nModel Details\nDeveloped by: Cerebras Systems\nLicense: Apache 2.0\nModel type: Transformer-based Language Model\nArchitecture: GPT-3 style architecture\nData set: The Pile\nTokenizer: Byte Pair Encoding\nVocabulary Size: 50257\nSequence Length: 2048\nOptimizer: AdamW, (Œ≤1, Œ≤2) = (0.9, 0.95), adam_eps = 1e‚àí8 (1e‚àí9 for larger models)\nPositional Encoding: Learned\nLanguage: English\nLearn more: Dense Scaling Laws Paper for training procedure, config files, and details on how to use.\nContact: To ask questions about Cerebras-GPT models, join the Cerebras Discord.\nThis is the standard parameterization version of Cerebras-GPT with 13B parameters\nRelated models: Cerebras-GPT Models\nModel\nParameters\nLayers\nd_model\nHeads\nd_head\nd_ffn\nLR\nBS (seq)\nBS (tokens)\nCerebras-GPT\n111M\n10\n768\n12\n64\n3072\n6.0E-04\n120\n246K\nCerebras-GPT\n256M\n14\n1088\n17\n64\n4352\n6.0E-04\n264\n541K\nCerebras-GPT\n590M\n18\n1536\n12\n128\n6144\n2.0E-04\n264\n541K\nCerebras-GPT\n1.3B\n24\n2048\n16\n128\n8192\n2.0E-04\n528\n1.08M\nCerebras-GPT\n2.7B\n32\n2560\n32\n80\n10240\n2.0E-04\n528\n1.08M\nCerebras-GPT\n6.7B\n32\n4096\n32\n128\n16384\n1.2E-04\n1040\n2.13M\nCerebras-GPT\n13B\n40\n5120\n40\n128\n20480\n1.2E-04\n720 ‚Üí 1080\n1.47M ‚Üí 2.21M\nQuickstart\nThis model can be easily loaded using the AutoModelForCausalLM functionality:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-13B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-13B\")\ntext = \"Generative AI is \"\nAnd can be used with Hugging Face Pipelines\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\ngenerated_text = pipe(text, max_length=50, do_sample=False, no_repeat_ngram_size=2)[0]\nprint(generated_text['generated_text'])\nor with model.generate()\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, num_beams=5,\nmax_new_tokens=50, early_stopping=True,\nno_repeat_ngram_size=2)\ntext_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(text_output[0])\nTraining data\nCerebras-GPT is trained using the Pile dataset from EleutherAI. See the Pile paper for a more detailed breakdown of data sources and methodology. The Pile was cleaned using the ftfy library to normalize the text, then filtered using scripts provided by Eleuther.\nWe tokenized the data using byte-pair encoding using the GPT-2 vocabulary. Our tokenized version of the Pile has 371B tokens. We include more details about the training dataset preprocessing in Appendix A.1 of our paper.\nRecent works find significant duplicate data present in the Pile. Eleuther‚Äôs Pythia applies a deduplication process to reduce replicated data, decreasing the Pile dataset size. Pythia was trained on both the standard dataset and deduplicated dataset to characterize the impact. Our models are trained on the standard Pile without deduplication, which may present an opportunity for further improvement with the deduplicated data set.\nTraining procedure\nWe use the GPT-3 style model architecture. All of our layers use full attention as opposed to the GPT-3 style sparse banded attention. The model shapes were selected to either follow aspect ratio 80 or are the same shape as GPT-3 models. Learning rate warmed up for 375M tokens (1500 steps for 111M and 256M models) and 10x cosine decayed. No dropout was used and weight decay was set to 0.1. All models are trained with MSL of 2048.\nAll models were trained to Chinchilla point: 20 tokens per model parameter. Number of steps was chosen based on optimal batch size (varied by model) and fixed sequence length (2048). See Training Table, below, for details.\nModel Params\nSequence Length\nBatch Size\nNumber of Steps\nTokens\nTokens per Parameter\nFlops\n111M\n2048\n120\n9037\n2.22E+09\n20\n2.6E+18\n256M\n2048\n264\n9468\n5.12E+09\n20\n1.3E+19\n590M\n2048\n264\n21836\n1.18E+10\n20\n6.1E+19\n1.3B\n2048\n528\n24334\n2.63E+10\n20\n2.8E+20\n2.7B\n2048\n528\n49041\n5.30E+10\n20\n1.1E+21\n6.7B\n2048\n1040\n62522\n1.33E+11\n20\n6.3E+21\n13B\n2048\n720\n174335\n2.57E+11\n20\n2.3E+22\nEvaluations\nWe trained models from smallest to largest and fit a power law as we went along. The power law was helpful for extrapolating the validation loss of the next largest model we trained and provided confidence about whether the training run was going well.\nWe performed upstream (pre-training) evaluations of text prediction cross-entropy using the Pile validation and test splits. We performed downstream evaluations of text generation accuracy on standardized tasks using the Eleuther lm-evaluation-harness. Results are compared against many publicly available large language models in Section 3 of the paper.\n0-shot Evaluation\nModel\nParams\nTraining FLOPs\nPILE test xent\nHella-Swag\nPIQA\nWino-Grande\nLambada\nARC-e\nARC-c\nOpenBookQA\nDownstream Average\nCerebras-GPT\n111M\n2.6E+18\n2.566\n0.268\n0.594\n0.488\n0.194\n0.380\n0.166\n0.118\n0.315\nCerebras-GPT\n256M\n1.3E+19\n2.299\n0.274\n0.613\n0.511\n0.293\n0.410\n0.170\n0.158\n0.347\nCerebras-GPT\n590M\n6.1E+19\n2.184\n0.291\n0.627\n0.498\n0.366\n0.464\n0.190\n0.158\n0.370\nCerebras-GPT\n1.3B\n2.8E+20\n1.996\n0.325\n0.664\n0.521\n0.462\n0.508\n0.224\n0.166\n0.410\nCerebras-GPT\n2.7B\n1.1E+21\n1.834\n0.386\n0.701\n0.559\n0.567\n0.571\n0.246\n0.206\n0.462\nCerebras-GPT\n6.7B\n6.3E+21\n1.704\n0.447\n0.739\n0.602\n0.636\n0.643\n0.282\n0.238\n0.512\nCerebras-GPT\n13B\n2.3E+22\n1.575\n0.513\n0.766\n0.646\n0.696\n0.714\n0.367\n0.286\n0.570\n5-shot Evaluation\nModel\nParams\nHella-Swag\nPIQA\nWino-Grande\nLambada\nARC-e\nARC-c\nOpenBookQA\nCerebras-GPT\n111M\n0.267\n0.588\n0.475\n0.158\n0.356\n0.166\n0.136\nCerebras-GPT\n256M\n0.278\n0.606\n0.522\n0.225\n0.422\n0.183\n0.164\nCerebras-GPT\n590M\n0.291\n0.634\n0.479\n0.281\n0.475\n0.206\n0.152\nCerebras-GPT\n1.3B\n0.326\n0.668\n0.536\n0.395\n0.529\n0.241\n0.174\nCerebras-GPT\n2.7B\n0.382\n0.697\n0.543\n0.487\n0.590\n0.267\n0.224\nCerebras-GPT\n6.7B\n0.444\n0.736\n0.590\n0.591\n0.667\n0.314\n0.270\nCerebras-GPT\n13B\n0.514\n0.768\n0.674\n0.655\n0.743\n0.398\n0.318\nUses and Limitations\nIntended Use\nThe primary intended use is to further research into large language models. These models can be used as a foundation model for NLP, applications, ethics, and alignment research. Our primary intended users are researchers who are working to improve LLMs and practitioners seeking reference implementations, training setups, hyperparameters, or pre-trained models. We release these models with a fully permissive Apache license for the community to use freely.\nYou may fine-tune and adapt Cerebras-GPT models for deployment via either Cerebras Model Studio or third-party libraries. Further safety-related testing and mitigations should be applied beore using the Cerebras-GPT model family in production downstream applications.\nDue to financial and compute budgets, Cerebras-GPT models were only trained and evaluated following the approaches described in the paper.\nOut of Scope Use\nCerebras-GPT models are trained on the Pile, with English language only, and are not suitable for machine translation tasks.\nCerebras-GPT models have not been tuned for human-facing dialog applications like chatbots and will not respond to prompts in a similar way to models that have received instruction tuning or reinforcement learning from human feedback (RLHF) like Flan-T5 or ChatGPT. Cerebras-GPT models can be tuned using those methods.\nRisk, Bias, Ethical Considerations\nData: The Pile dataset has been thoroughly analyzed from various ethical standpoints such as toxicity analysis, gender bias, pejorative content, racially sensitive content etc. Please refer to Pile dataset references.\nHuman life: The outputs from this model may or may not align with human values. The risk needs to be thoroughly investigated before deploying this model in a production environment where it can directly impact human life.\nRisks and harms: There can be distributional bias in the Pile dataset that can manifest in various forms in the downstream model deployment. There are other risks associated with large language models such as amplifying stereotypes, memorizing training data, or revealing private or secure information.\nMitigations: Only mitigations in standard Pile dataset pre-processing were employed when pre-training Cerebras-GPT.\nAcknowledgements\nWe are thankful to all Cerebras engineers, past and present, that made this work possible.",
    "facebook/fasttext-tr-vectors": "fastText (Turkish)\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nTokenization\nLicense\nEvaluation datasets\nBibTeX entry and citation info\nfastText (Turkish)\nfastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in this paper. The official website can be found here.\nModel description\nfastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.\nIt includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.\nIntended uses & limitations\nYou can use pre-trained word vectors for text classification or language identification. See the tutorials and resources on its official website to look for tasks that interest you.\nHow to use\nHere is how to load and use a pre-trained vectors\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n>>> model_path = hf_hub_download(repo_id=\"facebook/fasttext-tr-vectors\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.words\n['the', 'of', 'and', 'to', 'in', 'a', 'that', 'is', ...]\n>>> len(model.words)\n145940\n>>> model['bread']\narray([ 4.89417791e-01,  1.60882145e-01, -2.25947708e-01, -2.94273376e-01,\n-1.04577184e-01,  1.17962055e-01,  1.34821936e-01, -2.41778508e-01, ...])\nHere is how to use this model to query nearest neighbors of an English word vector:\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n>>> model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-nearest-neighbors\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.get_nearest_neighbors(\"bread\", k=5)\n[(0.5641006231307983, 'butter'),\n(0.48875734210014343, 'loaf'),\n(0.4491206705570221, 'eat'),\n(0.42444291710853577, 'food'),\n(0.4229326844215393, 'cheese')]\nHere is how to use this model to detect the language of a given text:\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n>>> model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\n(('__label__eng_Latn',), array([0.81148803]))\n>>> model.predict(\"Hello, world!\", k=5)\n(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'),\narray([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))\nLimitations and bias\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.\nCosine similarity can be used to measure the similarity between two different word vectors. If two two vectors are identical, the cosine similarity will be 1. For two completely unrelated vectors, the value will be 0. If two vectors have an opposite relationship, the value will be -1.\n>>> import numpy as np\n>>> def cosine_similarity(word1, word2):\n>>>     return np.dot(model[word1], model[word2]) / (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n>>> cosine_similarity(\"man\", \"boy\")\n0.061653383\n>>> cosine_similarity(\"man\", \"ceo\")\n0.11989131\n>>> cosine_similarity(\"woman\", \"ceo\")\n-0.08834904\nTraining data\nPre-trained word vectors for 157 languages were trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.\nTraining procedure\nTokenization\nWe used the Stanford word segmenter for Chinese, Mecab for Japanese and UETsegmenter for Vietnamese. For languages using the Latin, Cyrillic, Hebrew or Greek scripts, we used the tokenizer from the Europarl preprocessing tools. For the remaining languages, we used the ICU tokenizer.\nMore information about the training of these models can be found in the article Learning Word Vectors for 157 Languages.\nLicense\nThe word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0.\nEvaluation datasets\nThe analogy evaluation datasets described in the paper are available here: French, Hindi, Polish.\nBibTeX entry and citation info\nPlease cite [1] if using this code for learning word representations or [2] if using for text classification.\n[1] P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information\n@article{bojanowski2016enriching,\ntitle={Enriching Word Vectors with Subword Information},\nauthor={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\njournal={arXiv preprint arXiv:1607.04606},\nyear={2016}\n}\n[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification\n@article{joulin2016bag,\ntitle={Bag of Tricks for Efficient Text Classification},\nauthor={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\njournal={arXiv preprint arXiv:1607.01759},\nyear={2016}\n}\n[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J√©gou, T. Mikolov, FastText.zip: Compressing text classification models\n@article{joulin2016fasttext,\ntitle={FastText.zip: Compressing text classification models},\nauthor={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{'e}gou, H{'e}rve and Mikolov, Tomas},\njournal={arXiv preprint arXiv:1612.03651},\nyear={2016}\n}\nIf you use these word vectors, please cite the following paper:\n[4] E. Grave*, P. Bojanowski*, P. Gupta, A. Joulin, T. Mikolov, Learning Word Vectors for 157 Languages\n@inproceedings{grave2018learning,\ntitle={Learning Word Vectors for 157 Languages},\nauthor={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\nbooktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\nyear={2018}\n}\n(* These authors contributed equally.)",
    "abuchane/wav2vec2-xlsr-amharic-speech-emotion-recognition-arabic-model": "wav2vec2-xlsr-amharic-speech-emotion-recognition-arabic-model\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nFramework versions\nwav2vec2-xlsr-amharic-speech-emotion-recognition-arabic-model\nThis model is a fine-tuned version of elgeish/wav2vec2-large-xlsr-53-arabic on the None dataset.\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0001\ntrain_batch_size: 4\neval_batch_size: 4\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 8\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 1.0\nmixed_precision_training: Native AMP\nFramework versions\nTransformers 4.28.0.dev0\nPytorch 1.13.1+cu116\nDatasets 2.10.2.dev0\nTokenizers 0.13.2",
    "jamesdolezal/RetCCL": "[UNOFFICIAL]\nThis is the pretrained RetCCL model that accompanies the manuscript \"RetCCL: Clustering-Guided Contrastive Learning for Whole-Slide Image Retrieval\", published by Xiyue Wang et al in Medical Image Analysis (January 2023, DOI: https://doi.org/10.1016/j.media.2022.102645)\nThis model has been uploaded to HuggingFace for easier sharing, but has not been verified by the original authors and is in no way affiliated with the original authors.\nThe official pretrained model is available on the official GitHub repository (https://github.com/Xiyue-Wang/RetCCL) and Google Drive (https://drive.google.com/drive/folders/1AhstAFVqtTqxeS9WlBpU41BV08LYFUnL?usp=sharing). The license as included in the original repository is GPL-3.0.",
    "chkla/parlbert-german-v2": "ParlBERT v2\n‚ö†Ô∏è This version is only trained on around 5 million sentences (perplexity w/ adaption: 3.38 and w/o 13.38). The final version trained on around 30 million sentences will be available soon.\nüöÄ ParlBERT-v2 is a more general version of ParlBERT-v1 including texts from federal and state level in Germany. This first version was only trained on state level.\nParlBERT v2\nThis model is based on the German BERT (GBERT) architecture, specifically the \"deepset/gbert-base\" base model. It has been trained on over 30 million German political sentences from the \"GerParCor\" (Abrami et al. 2022) corpus for three epochs to provide a domain-adapted language model for German political texts. The German Political Texts Adapted GBERT model is designed for tasks related to German political texts. It can be used in a variety of applications.\nüìö Datasset\n\"GerParCor is a genre-specific corpus of (predominantly historical) German-language parliamentary protocols from three centuries and four countries, including state and federal level data.\" (Abrami et al. 2022)\nü§ñ Model training\nDuring the model training process, a masked language modeling approach was used with a token masking probability of 15%. The training was performed for three epochs, which means that the entire dataset was passed through the model three times during the training process.\nüë®‚Äçüíª Model Use\nfrom transformers import pipeline\nmodel = pipeline('fill-mask', model='parlbert-german-v2')\nmodel(\"Diese Themen geh√∂ren nicht ins [MASK].\")\n‚ö†Ô∏è Limitations\nThe German ParlBERT has limitations and potential biases. The GerParCor corpus only contains texts from the domain of politics, so the model may not perform well on texts from other domains. Additionally, the model may not be suitable for analyzing social media posts and many more.\nThe model's training data is derived from contemporary German political texts, which may reflect certain biases or perspectives. For instance, the corpus includes texts from specific political parties or interest groups, which may lead to overrepresentation or underrepresentation of certain viewpoints. To address these limitations and potential biases, users are encouraged to evaluate the model's performance on their specific use case and carefully consider the training data's representativeness for their target text domain.\nüê¶ Twitter: @chklamm",
    "curiousily/alpaca-bitcoin-tweets-sentiment": "No model card",
    "krea/aesthetic-controlnet": "Aesthetic ControlNet\nExamples\nMisuse and Malicious Use\nAuthors\nAesthetic ControlNet\nThis model can produce highly aesthetic results from an input image and a text prompt.\nControlNet is a method that can be used to condition diffusion models on arbitrary input features, such as image edges, segmentation maps, or human poses.\nAesthetic ControlNet is a version of this technique that uses image features extracted using a Canny edge detector and guides a text-to-image diffusion model trained on a large aesthetic dataset.\nThe base diffusion model is a fine-tuned version of Stable Diffusion 2.1 trained at a resolution of 640x640, and the control network comes from thibaud/controlnet-sd21 by @thibaudz.\nFor more information about ControlNet, please have a look at this thread or at the original work by Lvmin Zhang and Maneesh Agrawala.\nDiffusers\nInstall the following dependencies and then run the code below:\npip install opencv-python git+https://github.com/huggingface/diffusers.git\nimport cv2\nimport numpy as np\nfrom diffusers import StableDiffusionControlNetPipeline, EulerAncestralDiscreteScheduler\nfrom diffusers.utils import load_image\nimage = load_image(\"https://huggingface.co/krea/aesthetic-controlnet/resolve/main/krea.jpg\")\nimage = np.array(image)\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"krea/aesthetic-controlnet\").to(\"cuda\")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\noutput = pipe(\n\"fantasy flowers\",\ncanny_image,\nnum_inference_steps=20,\nguidance_scale=4,\nwidth=768,\nheight=768,\n)\nresult = output.images[0]\nresult.save(\"result.png\")\nExamples\nMisuse and Malicious Use\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nAuthors\nErwann Millon and Victor Perez",
    "medalpaca/medalpaca-7b": "MedAlpaca 7b\nTable of Contents\nModel Description\nArchitecture\nTraining Data\nModel Usage\nLimitations\nOpen LLM Leaderboard Evaluation Results\nMedAlpaca 7b\nTable of Contents\nModel Description\nArchitecture\nTraining DataModel UsageLimitations\nModel Description\nArchitecture\nmedalpaca-7b is a large language model specifically fine-tuned for medical domain tasks.\nIt is based on LLaMA (Large Language Model Meta AI) and contains 7 billion parameters.\nThe primary goal of this model is to improve question-answering and medical dialogue tasks.\nArchitecture\nTraining Data\nThe training data for this project was sourced from various resources.\nFirstly, we used Anki flashcards to automatically generate questions,\nfrom the front of the cards and anwers from the back of the card.\nSecondly, we generated medical question-answer pairs from Wikidoc.\nWe extracted paragraphs with relevant headings, and used Chat-GPT 3.5\nto generate questions from the headings and using the corresponding paragraphs\nas answers. This dataset is still under development and we believe\nthat approximately 70% of these question answer pairs are factual correct.\nThirdly, we used StackExchange to extract question-answer pairs, taking the\ntop-rated question from five categories: Academia, Bioinformatics, Biology,\nFitness, and Health. Additionally, we used a dataset from ChatDoctor\nconsisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.\nSource\nn items\nChatDoc large\n200000\nwikidoc\n67704\nStackexchange academia\n40865\nAnki flashcards\n33955\nStackexchange biology\n27887\nStackexchange fitness\n9833\nStackexchange health\n7721\nWikidoc patient information\n5942\nStackexchange bioinformatics\n5407\nModel Usage\nTo evaluate the performance of the model on a specific dataset, you can use the Hugging Face Transformers library's built-in evaluation scripts. Please refer to the evaluation guide for more information.\nInference\nYou can use the model for inference tasks like question-answering and medical dialogues using the Hugging Face Transformers library. Here's an example of how to use the model for a question-answering task:\nfrom transformers import pipeline\npl = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\", tokenizer=\"medalpaca/medalpaca-7b\")\nquestion = \"What are the symptoms of diabetes?\"\ncontext = \"Diabetes is a metabolic disease that causes high blood sugar. The symptoms include increased thirst, frequent urination, and unexplained weight loss.\"\nanswer = pl(f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: \")\nprint(answer)\nLimitations\nThe model may not perform effectively outside the scope of the medical domain.\nThe training data primarily targets the knowledge level of medical students,\nwhich may result in limitations when addressing the needs of board-certified physicians.\nThe model has not been tested in real-world applications, so its efficacy and accuracy are currently unknown.\nIt should never be used as a substitute for a doctor's opinion and must be treated as a research tool only.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n44.98\nARC (25-shot)\n54.1\nHellaSwag (10-shot)\n80.42\nMMLU (5-shot)\n41.47\nTruthfulQA (0-shot)\n40.46\nWinogrande (5-shot)\n71.19\nGSM8K (5-shot)\n3.03\nDROP (3-shot)\n24.21",
    "BlackKakapo/opus-mt-en-ro": "English-Romanian Translate\nHow to use\nFinetune\nBenchmarks\nEnglish-Romanian Translate\nFinetune\nThis model is a finetune of the Helsinki-NLP/opus-mt-en-ro model, on ccmatrix(full) dataset.\nHow to use\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"BlackKakapo/opus-mt-en-ro\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"BlackKakapo/opus-mt-en-ro\")\nBenchmarks\nModel\nTest Dataset\nBLEU\nTranslation Length\nReference Length\nBlackKakapo/opus-mt-en-ro\n(\"Helsinki-NLP/tatoeba_mt\", \"eng-ron\", \"test\")\n0.53118\n40516\n40352\nHelsinki-NLP/opus-mt-en-ro\n(\"Helsinki-NLP/tatoeba_mt\", \"eng-ron\", \"test\")\n0.45844\n39222\n40352\nHelsinki-NLP/opus-tatoeba-en-ro\n(\"Helsinki-NLP/tatoeba_mt\", \"eng-ron\", \"test\")\n0.46994\n39423\n40352",
    "minutillamolinara/bert-japanese_finetuned-sentiment-analysis": "bert-japanese_finetuned-sentiment-analysis\nPre-trained model\nTraining Data\nTraining hyperparameters\nUsage\nDependencies\nLicenses\nbert-japanese_finetuned-sentiment-analysis\nThis model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.\nPre-trained model\njarvisx17/japanese-sentiment-analysis\nLink : https://huggingface.co/jarvisx17/japanese-sentiment-analysis\nTraining Data\nThe model was trained on Japanese Sentiment Polarity Dictionary dataset.\nlink : https://www.cl.ecei.tohoku.ac.jp/Open_Resources-Japanese_Sentiment_Polarity_Dictionary.html\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 10\nUsage\nYou can use cURL to access this model:\nPython API:\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"minutillamolinara/bert-japanese_finetuned-sentiment-analysis\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"minutillamolinara/bert-japanese_finetuned-sentiment-analysis\")\ninputs = tokenizer(\"Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅåÈù¢ÁôΩ„ÅÑ\", return_tensors=\"pt\")\noutputs = model(**inputs)\nDependencies\n!pip install fugashi\n!pip install unidic_lite\nLicenses\nMIT",
    "timm/eva02_base_patch14_224.mim_in22k": "Model card for eva02_base_patch14_224.mim_in22k\nModel Details\nModel Usage\nImage Classification\nImage Embeddings\nModel Comparison\nCitation\nModel card for eva02_base_patch14_224.mim_in22k\nAn EVA02 feature / representation model. Pretrained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher) by paper authors.\nEVA-02 models are vision transformers with mean pooling, SwiGLU, Rotary Position Embeddings (ROPE), and extra LN in MLP (for Base & Large).\nNOTE: timm checkpoints are float32 for consistency with other models. Original checkpoints are float16 or bfloat16 in some cases, see originals if that's preferred.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 85.8\nGMACs: 23.2\nActivations (M): 36.6\nImage size: 224 x 224\nPapers:\nEVA-02: A Visual Representation for Neon Genesis: https://arxiv.org/abs/2303.11331\nEVA-CLIP: Improved Training Techniques for CLIP at Scale: https://arxiv.org/abs/2303.15389\nOriginal:\nhttps://github.com/baaivision/EVA\nhttps://huggingface.co/Yuxin-CV/EVA-02\nPretrain Dataset: ImageNet-22k\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('eva02_base_patch14_224.mim_in22k', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'eva02_base_patch14_224.mim_in22k',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 257, 768) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nmodel\ntop1\ntop5\nparam_count\nimg_size\neva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n90.054\n99.042\n305.08\n448\neva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n89.946\n99.01\n305.08\n448\neva_giant_patch14_560.m30m_ft_in22k_in1k\n89.792\n98.992\n1014.45\n560\neva02_large_patch14_448.mim_in22k_ft_in1k\n89.626\n98.954\n305.08\n448\neva02_large_patch14_448.mim_m38m_ft_in1k\n89.57\n98.918\n305.08\n448\neva_giant_patch14_336.m30m_ft_in22k_in1k\n89.56\n98.956\n1013.01\n336\neva_giant_patch14_336.clip_ft_in1k\n89.466\n98.82\n1013.01\n336\neva_large_patch14_336.in22k_ft_in22k_in1k\n89.214\n98.854\n304.53\n336\neva_giant_patch14_224.clip_ft_in1k\n88.882\n98.678\n1012.56\n224\neva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n88.692\n98.722\n87.12\n448\neva_large_patch14_336.in22k_ft_in1k\n88.652\n98.722\n304.53\n336\neva_large_patch14_196.in22k_ft_in22k_in1k\n88.592\n98.656\n304.14\n196\neva02_base_patch14_448.mim_in22k_ft_in1k\n88.23\n98.564\n87.12\n448\neva_large_patch14_196.in22k_ft_in1k\n87.934\n98.504\n304.14\n196\neva02_small_patch14_336.mim_in22k_ft_in1k\n85.74\n97.614\n22.13\n336\neva02_tiny_patch14_336.mim_in22k_ft_in1k\n80.658\n95.524\n5.76\n336\nCitation\n@article{EVA02,\ntitle={EVA-02: A Visual Representation for Neon Genesis},\nauthor={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},\njournal={arXiv preprint arXiv:2303.11331},\nyear={2023}\n}\n@article{EVA-CLIP,\ntitle={EVA-02: A Visual Representation for Neon Genesis},\nauthor={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\njournal={arXiv preprint arXiv:2303.15389},\nyear={2023}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "AIRI-Institute/gena-lm-bert-base-t2t-multi": "GENA-LM (gena-lm-bert-base-t2t-multi)\nExamples\nHow to load pre-trained model for Masked Language Modeling\nHow to load pre-trained model to fine-tune it on classification task\nModel description\nEvaluation\nCitation\nGENA-LM (gena-lm-bert-base-t2t-multi)\nGENA-LM is a Family of Open-Source Foundational Models for Long DNA Sequences.\nGENA-LM models are transformer masked language models trained on human DNA sequence.\nDifferences between GENA-LM (gena-lm-bert-base-t2t-multi) and DNABERT:\nBPE tokenization instead of k-mers;\ninput sequence size is about 4500 nucleotides (512 BPE tokens) compared to 512 nucleotides of DNABERT\npre-training on T2T + Multispecies vs. GRCh38.p13 human genome assembly.\nSource code and data: https://github.com/AIRI-Institute/GENA_LM\nPaper: https://academic.oup.com/nar/article/53/2/gkae1310/7954523\nExamples\nHow to load pre-trained model for Masked Language Modeling\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t-multi')\nmodel = AutoModel.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t-multi', trust_remote_code=True)\nHow to load pre-trained model to fine-tune it on classification task\nGet model class from GENA-LM repository:\ngit clone https://github.com/AIRI-Institute/GENA_LM.git\nfrom GENA_LM.src.gena_lm.modeling_bert import BertForSequenceClassification\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t-multi')\nmodel = BertForSequenceClassification.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t-multi')\nor you can just download modeling_bert.py and put it close to your code.\nOR you can get model class from HuggingFace AutoModel:\nfrom transformers import AutoTokenizer, AutoModel\nmodel = AutoModel.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t-multi', trust_remote_code=True)\ngena_module_name = model.__class__.__module__\nprint(gena_module_name)\nimport importlib\n# available class names:\n# - BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n# - BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,\n# - BertForQuestionAnswering\n# check https://huggingface.co/docs/transformers/model_doc/bert\ncls = getattr(importlib.import_module(gena_module_name), 'BertForSequenceClassification')\nprint(cls)\nmodel = cls.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t-multi', num_labels=2)\nModel description\nGENA-LM (gena-lm-bert-base-t2t-multi) model is trained in a masked language model (MLM) fashion, following the methods proposed in the BigBird paper by masking 15% of tokens. Model config for gena-lm-bert-base-t2t-multi is similar to the bert-base:\n512 Maximum sequence length\n12 Layers, 12 Attention heads\n768 Hidden size\n32k Vocabulary size\nWe pre-trained gena-lm-bert-base-t2t-multi using the latest T2T human genome assembly (https://www.ncbi.nlm.nih.gov/assembly/GCA_009914755.3/). The data was augmented by sampling mutations from 1000-genome SNPs (gnomAD dataset). We also add multispecies genomes from ENSEMBL release 108. The list of used species is here. Pre-training was performed for 1,925,000 iterations with batch size 256 and sequence length was equal to 512 tokens. We modified Transformer with Pre-Layer normalization, but without the final layer LayerNorm.\nEvaluation\nFor evaluation results, see our paper: https://academic.oup.com/nar/article/53/2/gkae1310/7954523\nCitation\n@article{GENA_LM,\nauthor = {Fishman, Veniamin and Kuratov, Yuri and Shmelev, Aleksei and Petrov, Maxim and Penzar, Dmitry and Shepelin, Denis and Chekanov, Nikolay and Kardymon, Olga and Burtsev, Mikhail},\ntitle = {GENA-LM: a family of open-source foundational DNA language models for long sequences},\njournal = {Nucleic Acids Research},\nvolume = {53},\nnumber = {2},\npages = {gkae1310},\nyear = {2025},\nmonth = {01},\nissn = {0305-1048},\ndoi = {10.1093/nar/gkae1310},\nurl = {https://doi.org/10.1093/nar/gkae1310},\neprint = {https://academic.oup.com/nar/article-pdf/53/2/gkae1310/61443229/gkae1310.pdf},\n}",
    "google/matcha-chart2text-pew": "Model card for MatCha - fine-tuned on Chart2text-pew\nTable of Contents\nTL;DR\nUsing the model\nConverting from T5x to huggingface\nContribution\nCitation\nModel card for MatCha - fine-tuned on Chart2text-pew\nThis model is the MatCha model, fine-tuned on Chart2text-pew dataset. This fine-tuned checkpoint might be better suited for chart summarization task.\nTable of Contents\nTL;DR\nUsing the model\nContribution\nCitation\nTL;DR\nThe abstract of the paper states that:\nVisual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models‚Äô capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.\nUsing the model\nfrom transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\nimport requests\nfrom PIL import Image\nprocessor = Pix2StructProcessor.from_pretrained('google/matcha-chart2text-pew')\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/matcha-chart2text-pew')\nurl = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/20294671002019.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\nConverting from T5x to huggingface\nYou can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:\npython convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE --is_vqa\nif you are converting a large model, run:\npython convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE --use-large --is_vqa\nOnce saved, you can push your converted model with the following snippet:\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\nmodel.push_to_hub(\"USERNAME/MODEL_NAME\")\nprocessor.push_to_hub(\"USERNAME/MODEL_NAME\")\nContribution\nThis model was originally contributed by Fangyu Liu, Francesco Piccinno et al. and added to the Hugging Face ecosystem by Younes Belkada.\nCitation\nIf you want to cite this work, please consider citing the original paper:\n@misc{liu2022matcha,\ntitle={MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering},\nauthor={Fangyu Liu and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Yasemin Altun and Nigel Collier and Julian Martin Eisenschlos},\nyear={2022},\neprint={2212.09662},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "google/deplot": "Model card for DePlot\nTable of Contents\nTL;DR\nUsing the model\nConverting from T5x to huggingface\nContribution\nCitation\nModel card for DePlot\nTable of Contents\nTL;DR\nUsing the model\nContribution\nCitation\nTL;DR\nThe abstract of the paper states that:\nVisual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.\nUsing the model\nYou can run a prediction by querying an input image together with a question as follows:\nfrom transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\nimport requests\nfrom PIL import Image\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\nurl = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\nConverting from T5x to huggingface\nYou can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:\npython convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE --is_vqa\nif you are converting a large model, run:\npython convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE --use-large --is_vqa\nOnce saved, you can push your converted model with the following snippet:\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\nmodel.push_to_hub(\"USERNAME/MODEL_NAME\")\nprocessor.push_to_hub(\"USERNAME/MODEL_NAME\")\nContribution\nThis model was originally contributed by Fangyu Liu, Julian Martin Eisenschlos et al. and added to the Hugging Face ecosystem by Younes Belkada.\nCitation\nIf you want to cite this work, please consider citing the original paper:\n@misc{liu2022deplot,\ntitle={DePlot: One-shot visual language reasoning by plot-to-table translation},\nauthor={Liu, Fangyu and Eisenschlos, Julian Martin and Piccinno, Francesco and Krichene, Syrine and Pang, Chenxi and Lee, Kenton and Joshi, Mandar and Chen, Wenhu and Collier, Nigel and Altun, Yasemin},\nyear={2022},\neprint={2212.10505},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "huggyllama/llama-7b": "This contains the weights for the LLaMA-7b model. This model is under a non-commercial license (see the LICENSE file).\nYou should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",
    "huggyllama/llama-13b": "This contains the weights for the LLaMA-13b model. This model is under a non-commercial license (see the LICENSE file).\nYou should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",
    "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224": "BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\nContents\nTraining Data\nModel Use\n1. Environment\n2.1 Load from HF hub\n2.2 Load from local files\nUse in Jupyter Notebook\nIntended Use\nReference\nLimitations\nFurther information\nBiomedCLIP-PubMedBERT_256-vit_base_patch16_224\nBiomedCLIP is a biomedical vision-language foundation model that is pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning.\nIt uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations.\nIt can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.\nBiomedCLIP establishes new state of the art in a wide range of standard datasets, and substantially outperforms prior VLP approaches:\nContents\nTraining Data\nModel Use\nReference\nLimitations\nFurther Information\nTraining Data\nWe have released BiomedCLIP Data Pipeline at https://github.com/microsoft/BiomedCLIP_data_pipeline, which automatically downloads and processes a set of articles from the PubMed Central Open Access dataset.\nBiomedCLIP builds upon the PMC-15M dataset, which is a large-scale parallel image-text dataset generated by this data pipeline for biomedical vision-language processing. It contains 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central and covers a diverse range of biomedical image types, such as microscopy, radiography, histology, and more.\nModel Use\n1. Environment\nconda create -n biomedclip python=3.10 -y\nconda activate biomedclip\npip install open_clip_torch==2.23.0 transformers==4.35.2 matplotlib\n2.1 Load from HF hub\nimport torch\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer\n# Load the model and config files from the Hugging Face Hub\nmodel, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\ntokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n# Zero-shot image classification\ntemplate = 'this is a photo of '\nlabels = [\n'adenocarcinoma histopathology',\n'brain MRI',\n'covid line chart',\n'squamous cell carcinoma histopathology',\n'immunohistochemistry histopathology',\n'bone X-ray',\n'chest X-ray',\n'pie chart',\n'hematoxylin and eosin histopathology'\n]\ndataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\ntest_imgs = [\n'squamous_cell_carcinoma_histopathology.jpeg',\n'H_and_E_histopathology.jpg',\n'bone_X-ray.jpg',\n'adenocarcinoma_histopathology.jpg',\n'covid_line_chart.png',\n'IHC_histopathology.jpg',\n'chest_X-ray.jpg',\n'brain_MRI.jpg',\n'pie_chart.png'\n]\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\nmodel.eval()\ncontext_length = 256\nimages = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\ntexts = tokenizer([template + l for l in labels], context_length=context_length).to(device)\nwith torch.no_grad():\nimage_features, text_features, logit_scale = model(images, texts)\nlogits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\nsorted_indices = torch.argsort(logits, dim=-1, descending=True)\nlogits = logits.cpu().numpy()\nsorted_indices = sorted_indices.cpu().numpy()\ntop_k = -1\nfor i, img in enumerate(test_imgs):\npred = labels[sorted_indices[i][0]]\ntop_k = len(labels) if top_k == -1 else top_k\nprint(img.split('/')[-1] + ':')\nfor j in range(top_k):\njth_index = sorted_indices[i][j]\nprint(f'{labels[jth_index]}: {logits[i][jth_index]}')\nprint('\\n')\n2.2 Load from local files\nimport json\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom open_clip import create_model_and_transforms, get_tokenizer\nfrom open_clip.factory import HF_HUB_PREFIX, _MODEL_CONFIGS\n# Download the model and config files\nhf_hub_download(\nrepo_id=\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\nfilename=\"open_clip_pytorch_model.bin\",\nlocal_dir=\"checkpoints\"\n)\nhf_hub_download(\nrepo_id=\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\nfilename=\"open_clip_config.json\",\nlocal_dir=\"checkpoints\"\n)\n# Load the model and config files\nmodel_name = \"biomedclip_local\"\nwith open(\"checkpoints/open_clip_config.json\", \"r\") as f:\nconfig = json.load(f)\nmodel_cfg = config[\"model_cfg\"]\npreprocess_cfg = config[\"preprocess_cfg\"]\nif (not model_name.startswith(HF_HUB_PREFIX)\nand model_name not in _MODEL_CONFIGS\nand config is not None):\n_MODEL_CONFIGS[model_name] = model_cfg\ntokenizer = get_tokenizer(model_name)\nmodel, _, preprocess = create_model_and_transforms(\nmodel_name=model_name,\npretrained=\"checkpoints/open_clip_pytorch_model.bin\",\n**{f\"image_{k}\": v for k, v in preprocess_cfg.items()},\n)\n# Zero-shot image classification\ntemplate = 'this is a photo of '\nlabels = [\n'adenocarcinoma histopathology',\n'brain MRI',\n'covid line chart',\n'squamous cell carcinoma histopathology',\n'immunohistochemistry histopathology',\n'bone X-ray',\n'chest X-ray',\n'pie chart',\n'hematoxylin and eosin histopathology'\n]\ndataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\ntest_imgs = [\n'squamous_cell_carcinoma_histopathology.jpeg',\n'H_and_E_histopathology.jpg',\n'bone_X-ray.jpg',\n'adenocarcinoma_histopathology.jpg',\n'covid_line_chart.png',\n'IHC_histopathology.jpg',\n'chest_X-ray.jpg',\n'brain_MRI.jpg',\n'pie_chart.png'\n]\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\nmodel.eval()\ncontext_length = 256\nimages = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\ntexts = tokenizer([template + l for l in labels], context_length=context_length).to(device)\nwith torch.no_grad():\nimage_features, text_features, logit_scale = model(images, texts)\nlogits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\nsorted_indices = torch.argsort(logits, dim=-1, descending=True)\nlogits = logits.cpu().numpy()\nsorted_indices = sorted_indices.cpu().numpy()\ntop_k = -1\nfor i, img in enumerate(test_imgs):\npred = labels[sorted_indices[i][0]]\ntop_k = len(labels) if top_k == -1 else top_k\nprint(img.split('/')[-1] + ':')\nfor j in range(top_k):\njth_index = sorted_indices[i][j]\nprint(f'{labels[jth_index]}: {logits[i][jth_index]}')\nprint('\\n')\nUse in Jupyter Notebook\nPlease refer to this example notebook.\nIntended Use\nThis model is intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported in the reference paper.\nPrimary Intended Use\nThe primary intended use is to support AI researchers building on top of this work. BiomedCLIP and its associated models should be helpful for exploring various biomedical VLP research questions, especially in the radiology domain.\nOut-of-Scope Use\nAny deployed use case of the model --- commercial or otherwise --- is currently out of scope. Although we evaluated the models using a broad set of publicly-available research benchmarks, the models and evaluations are not intended for deployed use cases. Please refer to the associated paper for more details.\nReference\n@article{zhang2024biomedclip,\ntitle={A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image‚ÄìText Pairs},\nauthor={Sheng Zhang and Yanbo Xu and Naoto Usuyama and Hanwen Xu and Jaspreet Bagga and Robert Tinn and Sam Preston and Rajesh Rao and Mu Wei and Naveen Valluri and Cliff Wong and Andrea Tupini and Yu Wang and Matt Mazzola and Swadheen Shukla and Lars Liden and Jianfeng Gao and Angela Crabtree and Brian Piening and Carlo Bifulco and Matthew P. Lungren and Tristan Naumann and Sheng Wang and Hoifung Poon},\njournal={NEJM AI},\nyear={2024},\nvolume={2},\nnumber={1},\ndoi={10.1056/AIoa2400640},\nurl={https://ai.nejm.org/doi/full/10.1056/AIoa2400640}\n}\nLimitations\nThis model was developed using English corpora, and thus can be considered English-only.\nFurther information\nPlease refer to the corresponding paper, \"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing\" for additional details on the model training and evaluation.",
    "christian-phu/bert-finetuned-japanese-sentiment": "bert-finetuned-japanese-sentiment\nModel description\nTraining and evaluation data\nTraining hyperparameters\nFramework versions\nbert-finetuned-japanese-sentiment\nThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.\nModel description\nModel Train for amazon reviews Japanese sentence sentiments.\nSentiment analysis is a common task in natural language processing. It consists of classifying the polarity of a given text at the sentence or document level. For instance, the sentence \"The food is good\" has a positive sentiment, while the sentence \"The food is bad\" has a negative sentiment.\nIn this model, we fine-tuned a BERT model on a Japanese sentiment analysis dataset. The dataset contains 20,000 sentences extracted from Amazon reviews. Each sentence is labeled as positive, neutral, or negative. The model was trained for 5 epochs with a batch size of 16.\nTraining and evaluation data\nEpochs: 6\nTraining Loss: 0.087600\nValidation Loss: 1.028876\nAccuracy: 0.813202\nPrecision: 0.712440\nRecall: 0.756031\nF1: 0.728455\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 0\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 6\nFramework versions\nTransformers 4.27.4\nPytorch 2.0.0+cu118\nTokenizers 0.13.2",
    "DeepFloyd/IF-I-XL-v1.0": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nDeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, ‚ÄúLicense‚Äù), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (‚ÄúLicensee‚Äù or ‚Äúyou‚Äù) and Stability AI Ltd.. (‚ÄúStability AI‚Äù or ‚Äúwe‚Äù) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (‚ÄúSoftware‚Äù) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (‚ÄúDocumentation‚Äù).By clicking ‚ÄúI Accept‚Äù below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the ‚ÄúSoftware Products‚Äù), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.\nLICENSE GRANT  a. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI‚Äôs copyright interests to reproduce, distribute, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign or sublicense this License or any other rights or obligations under this License without Stability AI‚Äôs prior written consent; any such assignment or sublicense will be void and will automatically and immediately terminate this License.  b. You may make a reasonable number of copies of the Documentation solely for use in connection with the license to the Software granted above.  c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.\nRESTRICTIONS  You will not, and will not permit, assist or cause any third party to: a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing; b. alter or remove copyright and other proprietary notices which appear on or in the Software Products; c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; or d. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License. e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (‚ÄúExport Laws‚Äù); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods.\nATTRIBUTION  Together with any copies of the Software Products (as well as derivative works thereof or works incorporating the Software Products) that you distribute, you must provide (i) a copy of this License, and (ii) the following attribution notice: ‚ÄúDeepFloyd is licensed under the DeepFloyd License, Copyright (c) Stability AI Ltd. All Rights Reserved.‚Äù\nDISCLAIMERS  THE SOFTWARE PRODUCTS ARE PROVIDED ‚ÄúAS IS‚Äù and ‚ÄúWITH ALL FAULTS‚Äù WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. STABILITY AIEXPRESSLY DISCLAIMS ALL REPRESENTATIONS AND WARRANTIES, EXPRESS OR IMPLIED, WHETHER BY STATUTE, CUSTOM, USAGE OR OTHERWISE AS TO ANY MATTERS RELATED TO THE SOFTWARE PRODUCTS, INCLUDING BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, SATISFACTORY QUALITY, OR NON-INFRINGEMENT. STABILITY AI MAKES NO WARRANTIES OR REPRESENTATIONS THAT THE SOFTWARE PRODUCTS WILL BE ERROR FREE OR FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS, OR PRODUCE ANY PARTICULAR RESULTS.\nLIMITATION OF LIABILITY  TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL STABILITY AI BE LIABLE TO YOU (A) UNDER ANY THEORY OF LIABILITY, WHETHER BASED IN CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, WARRANTY, OR OTHERWISE UNDER THIS LICENSE, OR (B) FOR ANY INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, PUNITIVE OR SPECIAL DAMAGES OR LOST PROFITS, EVEN IF STABILITY AI HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE SOFTWARE PRODUCTS, THEIR CONSTITUENT COMPONENTS, AND ANY OUTPUT (COLLECTIVELY, ‚ÄúSOFTWARE MATERIALS‚Äù) ARE NOT DESIGNED OR INTENDED FOR USE IN ANY APPLICATION OR SITUATION WHERE FAILURE OR FAULT OF THE SOFTWARE MATERIALS COULD REASONABLY BE ANTICIPATED TO LEAD TO SERIOUS INJURY OF ANY PERSON, INCLUDING POTENTIAL DISCRIMINATION OR VIOLATION OF AN INDIVIDUAL‚ÄôS PRIVACY RIGHTS, OR TO SEVERE PHYSICAL, PROPERTY, OR ENVIRONMENTAL DAMAGE (EACH, A ‚ÄúHIGH-RISK USE‚Äù). IF YOU ELECT TO USE ANY OF THE SOFTWARE MATERIALS FOR A HIGH-RISK USE, YOU DO SO AT YOUR OWN RISK. YOU AGREE TO DESIGN AND IMPLEMENT APPROPRIATE DECISION-MAKING AND RISK-MITIGATION PROCEDURES AND POLICIES IN CONNECTION WITH A HIGH-RISK USE SUCH THAT EVEN IF THERE IS A FAILURE OR FAULT IN ANY OF THE SOFTWARE MATERIALS, THE SAFETY OF PERSONS OR PROPERTY AFFECTED BY THE ACTIVITY STAYS AT A LEVEL THAT IS REASONABLE, APPROPRIATE, AND LAWFUL FOR THE FIELD OF THE HIGH-RISK USE.\nINDEMNIFICATION  You will indemnify, defend and hold harmless Stability AI and our subsidiaries and affiliates, and each of our respective shareholders, directors, officers, employees, agents, successors, and assigns (collectively, the ‚ÄúStability AI Parties‚Äù) from and against any losses, liabilities, damages, fines, penalties, and expenses (including reasonable attorneys‚Äô fees) incurred by any Stability AI Party in connection with any claim, demand, allegation, lawsuit, proceeding, or investigation (collectively, ‚ÄúClaims‚Äù) arising out of or related to: (a) your access to or use of the Software Products (as well as any results or data generated from such access or use), including any High-Risk Use (defined below); (b) your violation of this License; or (c) your violation, misappropriation or infringement of any rights of another (including intellectual property or other proprietary rights and privacy rights). You will promptly notify the Stability AI Parties of any such Claims, and cooperate with Stability AI Parties in defending such Claims. You will also grant the Stability AI Parties sole control of the defense or settlement, at Stability AI‚Äôs sole option, of any Claims. This indemnity is in addition to, and not in lieu of, any other indemnities or remedies set forth in a written agreement between you and Stability AI or the other Stability AI Parties.\nTERMINATION; SURVIVAL  a. This License will automatically terminate upon any breach by you of the terms of this License.‚Ä®‚Ä®b. We may terminate this License, in whole or in part, at any time upon notice (including electronic) to you.‚Ä®‚Ä®c. The following sections survive termination of this License: 2 (Restrictions), 3 (Attribution), 4 (Disclaimers), 5 (Limitation on Liability), 6 (Indemnification) 7 (Termination; Survival), 8 (Third Party Materials), 9 (Trademarks), 10 (Applicable Law; Dispute Resolution), and 11 (Miscellaneous).\nTHIRD PARTY MATERIALS  The Software Products may contain third-party software or other components (including free and open source software) (all of the foregoing, ‚ÄúThird Party Materials‚Äù), which are subject to the license terms of the respective third-party licensors. Your dealings or correspondence with third parties and your use of or interaction with any Third Party Materials are solely between you and the third party. Stability AI does not control or endorse, and makes no representations or warranties regarding, any Third Party Materials, and your access to and use of such Third Party Materials are at your own risk.\nTRADEMARKS  Licensee has not been granted any trademark license as part of this License and may not use any name or mark associated with Stability AI without the prior written permission of Stability AI, except to the extent necessary to make the reference required by the ‚ÄúATTRIBUTION‚Äù section of this Agreement.\nAPPLICABLE LAW; DISPUTE RESOLUTION  This License will be governed and construed under the laws of the State of California without regard to conflicts of law provisions. Any suit or proceeding arising out of or relating to this License will be brought in the federal or state courts, as applicable, in San Mateo County, California, and each party irrevocably submits to the jurisdiction and venue of such courts.\nMISCELLANEOUS  If any provision or part of a provision of this License is unlawful, void or unenforceable, that provision or part of the provision is deemed severed from this License, and will not affect the validity and enforceability of any remaining provisions. The failure of Stability AI to exercise or enforce any right or provision of this License will not operate as a waiver of such right or provision. This License does not confer any third-party beneficiary rights upon any other person or entity. This License, together with the Documentation, contains the entire understanding between you and Stability AI regarding the subject matter of this License, and supersedes all other written or oral agreements and understandings between you and Stability AI regarding such subject matter. No change or addition to any provision of this License will be binding unless it is in writing and signed by an authorized representative of both you and Stability AI.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nIF-I-XL-v1.0\nModel Details\nUsing with diffusers\nTraining\nEvaluation Results\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nCitation (Soon)\nIF-I-XL-v1.0\nDeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.\nInspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\nModel Details\nDeveloped by: DeepFloyd, StabilityAI\nModel type: pixel-based text-to-image cascaded diffusion model\nCascade Stage: I\nNum Parameters: 4.3B\nLanguage(s): primarily English and, to a lesser extent, other Romance languages\nLicense: DeepFloyd IF License Agreement\nModel Description: DeepFloyd-IF is modular composed of frozen text mode and three pixel cascaded diffusion modules, each designed to generate images of increasing resolution: 64x64, 256x256, and 1024x1024. All stages of the model utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention-pooling\nResources for more information: GitHub, deepfloyd.ai, All Links\nCite as (Soon): -\nUsing with diffusers\nIF is integrated with the ü§ó Hugging Face üß® diffusers library, which is optimized to run on GPUs with as little as 14 GB of VRAM.\nBefore you can use IF, you need to accept its usage conditions. To do so:\nMake sure to have a Hugging Face account and be loggin in\nAccept the license on the model card of DeepFloyd/IF-I-XL-v1.0\nMake sure to login locally. Install huggingface_hub\npip install huggingface_hub --upgrade\nrun the login function in a Python shell\nfrom huggingface_hub import login\nlogin()\nand enter your Hugging Face Hub access token.\nNext we install diffusers and dependencies:\npip install diffusers accelerate transformers safetensors sentencepiece\nAnd we can now run the model locally.\nBy default diffusers makes use of model cpu offloading to run the whole IF pipeline with as little as 14 GB of VRAM.\nIf you are using torch>=2.0.0, make sure to remove all enable_xformers_memory_efficient_attention() functions.\nLoad all stages and offload to CPU\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n# stage 1\nstage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_1.enable_model_cpu_offload()\n# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n\"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_2.enable_model_cpu_offload()\n# stage 3\nsafety_modules = {\"feature_extractor\": stage_1.feature_extractor, \"safety_checker\": stage_1.safety_checker, \"watermarker\": stage_1.watermarker}\nstage_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16)\nstage_3.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_3.enable_model_cpu_offload()\nRetrieve Text Embeddings\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\n# text embeds\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\nRun stage 1\ngenerator = torch.manual_seed(0)\nimage = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\").images\npt_to_pil(image)[0].save(\"./if_stage_I.png\")\nRun stage 2\nimage = stage_2(\nimage=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\"\n).images\npt_to_pil(image)[0].save(\"./if_stage_II.png\")\nRun stage 3\nimage = stage_3(prompt=prompt, image=image, generator=generator, noise_level=100).images\nimage[0].save(\"./if_stage_III.png\")\nThere are multiple ways to speed up the inference time and lower the memory consumption even more with diffusers. To do so, please have a look at the Diffusers docs:\nüöÄ Optimizing for inference time\n‚öôÔ∏è Optimizing for low memory during inference\nFor more in-detail information about how to use IF, please have a look at the IF blog post and the documentation üìñ.\nDiffusers dreambooth scripts also supports fine-tuning üé® IF.\nWith parameter efficient finetuning, you can add new concepts to IF with a single GPU and ~28 GB VRAM.\nTraining\nTraining Data:\n1.2B text-image pairs (based on LAION-A and few additional internal datasets)\nTest/Valid parts of datasets are not used at any cascade and stage of training. Valid part of COCO helps to demonstrate \"online\" loss behaviour during training (to catch incident and other problems), but dataset is never used for train.\nTraining Procedure: IF-I-XL-v1.0 is a pixel-based diffusion cascade which uses T5-Encoder embeddings (hidden states) to generate 64px image. During training,\nImages are cropped to square via shifted-center-crop augmentation (randomly shift from center up to 0.1 of size) and resized to 64px using Pillow==9.2.0 BICUBIC resampling with reducing_gap=None (it helps to avoid aliasing) and processed to tensor BxCxHxW\nText prompts are encoded through open-sourced frozen T5-v1_1-xxl text-encoder (that completely was trained by Google team), random 10% of texts are dropped to empty string to add ability for classifier free guidance (CFG)\nThe non-pooled output of the text encoder is fed into the projection (linear layer without activation) and is used in UNet backbone of the diffusion model via controlled hybrid self- and cross- attention\nAlso, the output of the text encode is pooled via attention-pooling (64 heads) and is used in time embed as additional features\nDiffusion process is limited by 1000 discrete steps, with cosine beta schedule of noising image\nThe loss is a reconstruction objective between the noise that was added to the image and the prediction made by the UNet\nThe training process for checkpoint IF-I-XL-v1.0 has 2_420_000 steps at resolution 64x64 on all datasets, OneCycleLR policy, few-bit backward GELU activations, optimizer AdamW8bit + DeepSpeed-Zero1, fully frozen T5-Encoder\nHardware: 64 x 8 x A100 GPUs\nOptimizer: AdamW8bit + DeepSpeed ZeRO-1\nBatch: 3072\nLearning rate: one-cycle cosine strategy, warmup 10000 steps, start_lr=2e-6, max_lr=5e-5, final_lr=5e-9\nEvaluation Results\nFID-30K: 6.66\nUses\nDirect Use\nThe model is released for research purposes. Any attempt to deploy the model in production requires not only that the LICENSE is followed but full liability over the person deploying the model.\nPossible research areas and tasks include:\nGeneration of artistic imagery and use in design and other artistic processes.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion but applies in the same way for IF.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have... (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nIF was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nIF mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nCitation (Soon)\nThis model card was written by: DeepFloyd-Team and is based on the StableDiffusion model card.",
    "lokCX/4x-Ultrasharp": "No model card",
    "youngp5/eyeglasses_detection": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nEye Glasses Detection\nEye Glasses Detection\nThe model is aimed to detect whether there is any eyeglass in an image.",
    "databricks/dolly-v2-12b": "dolly-v2-12b Model Card\nSummary\nModel Overview\nUsage\nLangChain Usage\nKnown Limitations\nPerformance Limitations\nDataset Limitations\nBenchmark Metrics\nCitation\nHappy Hacking!\ndolly-v2-12b Model Card\nSummary\nDatabricks' dolly-v2-12b, an instruction-following large language model trained on the Databricks machine learning platform\nthat is licensed for commercial use. Based on pythia-12b, Dolly is trained on ~15k instruction/response fine tuning records\ndatabricks-dolly-15k generated\nby Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,\ninformation extraction, open QA and summarization. dolly-v2-12b is not a state-of-the-art model, but does exhibit surprisingly\nhigh quality instruction following behavior not characteristic of the foundation model on which it is based.\nDolly v2 is also available in these smaller models sizes:\ndolly-v2-7b, a 6.9 billion parameter based on pythia-6.9b\ndolly-v2-3b, a 2.8 billion parameter based on pythia-2.8b\nPlease refer to the dolly GitHub repo for tips on\nrunning inference for various GPU configurations.\nOwner: Databricks, Inc.\nModel Overview\ndolly-v2-12b is a 12 billion parameter causal language model created by Databricks that is derived from\nEleutherAI's Pythia-12b and fine-tuned\non a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)\nUsage\nTo use the model with the transformers library on a machine with GPUs, first make sure you have the transformers and accelerate libraries installed.\nIn a Databricks notebook you could run:\n%pip install \"accelerate>=0.16.0,<1\" \"transformers[torch]>=4.28.1,<5\" \"torch>=1.13.1,<2\"\nThe instruction following pipeline can be loaded using the pipeline function as shown below.  This loads a custom InstructionTextGenerationPipeline\nfound in the model repo here, which is why trust_remote_code=True is required.\nIncluding torch_dtype=torch.bfloat16 is generally recommended if this type is supported in order to reduce memory usage.  It does not appear to impact output quality.\nIt is also fine to remove it if there is sufficient memory.\nimport torch\nfrom transformers import pipeline\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\nYou can then use the pipeline to answer instructions:\nres = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\nprint(res[0][\"generated_text\"])\nAlternatively, if you prefer to not use trust_remote_code=True you can download instruct_pipeline.py,\nstore it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:\nimport torch\nfrom instruct_pipeline import InstructionTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-12b\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-12b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\ngenerate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\nLangChain Usage\nTo use the pipeline with LangChain, you must set return_full_text=True, as LangChain expects the full text to be returned\nand the default for the pipeline is to only return the new text.\nimport torch\nfrom transformers import pipeline\ngenerate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\ntrust_remote_code=True, device_map=\"auto\", return_full_text=True)\nYou can create a prompt that either has only an instruction or has an instruction with context:\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n# template for an instrution with no input\nprompt = PromptTemplate(\ninput_variables=[\"instruction\"],\ntemplate=\"{instruction}\")\n# template for an instruction with input\nprompt_with_context = PromptTemplate(\ninput_variables=[\"instruction\", \"context\"],\ntemplate=\"{instruction}\\n\\nInput:\\n{context}\")\nhf_pipeline = HuggingFacePipeline(pipeline=generate_text)\nllm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\nExample predicting using a simple instruction:\nprint(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())\nExample predicting using an instruction with context:\ncontext = \"\"\"George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,\nand Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\nprint(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())\nKnown Limitations\nPerformance Limitations\ndolly-v2-12b is not a state-of-the-art generative language model and, though quantitative benchmarking is ongoing, is not designed to perform\ncompetitively with more modern model architectures or models subject to larger pretraining corpuses.\nThe Dolly model family is under active development, and so any list of shortcomings is unlikely to be exhaustive, but we include known limitations and misfires here as a means to document and share our preliminary findings with the community.In particular, dolly-v2-12b struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors,\ndates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc.\nMoreover, we find that dolly-v2-12b does not have some capabilities, such as well-formatted letter writing, present in the original model.\nDataset Limitations\nLike all language models, dolly-v2-12b reflects the content and limitations of its training corpuses.\nThe Pile: GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets,\nit contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly\nin the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit\nassociations.\ndatabricks-dolly-15k: The training data on which dolly-v2-12b is instruction tuned represents natural language instructions generated\nby Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages\nfor instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or\npersonally identifying information about non-public figures, but it may contain typos and factual errors.\nThe dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects\nthe interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.\nDatabricks is committed to ongoing research and development efforts to develop helpful, honest and harmless AI technologies that\nmaximize the potential of all individuals and organizations.\nBenchmark Metrics\nBelow you'll find various models benchmark performance on the EleutherAI LLM Evaluation Harness;\nmodel results are sorted by geometric mean to produce an intelligible ordering. As outlined above, these results demonstrate that dolly-v2-12b is not state of the art,\nand in fact underperforms dolly-v1-6b in some evaluation benchmarks. We believe this owes to the composition and size of the underlying fine tuning datasets,\nbut a robust statement as to the sources of these variations requires further study.\nmodel\nopenbookqa\narc_easy\nwinogrande\nhellaswag\narc_challenge\npiqa\nboolq\ngmean\nEleutherAI/pythia-2.8b\n0.348\n0.585859\n0.589582\n0.591217\n0.323379\n0.73395\n0.638226\n0.523431\nEleutherAI/pythia-6.9b\n0.368\n0.604798\n0.608524\n0.631548\n0.343857\n0.761153\n0.6263\n0.543567\ndatabricks/dolly-v2-3b\n0.384\n0.611532\n0.589582\n0.650767\n0.370307\n0.742655\n0.575535\n0.544886\nEleutherAI/pythia-12b\n0.364\n0.627104\n0.636148\n0.668094\n0.346416\n0.760065\n0.673394\n0.559676\nEleutherAI/gpt-j-6B\n0.382\n0.621633\n0.651144\n0.662617\n0.363481\n0.761153\n0.655963\n0.565936\ndatabricks/dolly-v2-12b\n0.408\n0.63931\n0.616417\n0.707927\n0.388225\n0.757889\n0.568196\n0.56781\ndatabricks/dolly-v2-7b\n0.392\n0.633838\n0.607735\n0.686517\n0.406997\n0.750816\n0.644037\n0.573487\ndatabricks/dolly-v1-6b\n0.41\n0.62963\n0.643252\n0.676758\n0.384812\n0.773667\n0.687768\n0.583431\nEleutherAI/gpt-neox-20b\n0.402\n0.683923\n0.656669\n0.7142\n0.408703\n0.784004\n0.695413\n0.602236\nCitation\n@online{DatabricksBlog2023DollyV2,\nauthor    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},\ntitle     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},\nyear      = {2023},\nurl       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},\nurldate   = {2023-06-30}\n}\nHappy Hacking!",
    "camenduru/SadTalker": "No model card",
    "Xipotzzz/blip2zh-chatglm-6b": "Model Card for blip2zh-chatglm-6b\nModel Details\nModel Description\nModel Sources\nUses\nLimitations\nTraining Details\nTraining Data\nTraining Procedure\nDemos\nModel Card for blip2zh-chatglm-6b\nModel Details\nModel Description\nblip2zh-chatglm-6bÊòØÂü∫‰∫éblip2ËÆ≠ÁªÉÁöÑ‰∏≠ÊñáÂ§öÊ®°ÊÄÅËÅäÂ§©Ê®°Âûã„ÄÇÂÖ∑ÊúâÂü∫Êú¨ÁöÑÂõæÂÉèÁêÜËß£ËÉΩÂäõ„ÄÇ\nÁî±‰∫éblip2ÁöÑËÆ≠ÁªÉÊñπÂºè‰∏ç‰ºöÂØπËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÂõ†Ê≠§Âú®Á∫ØÊñáÊú¨ÂØπËØù‰∏≠ÁöÑË°å‰∏∫ÂèØ‰ª•‰øùÊåÅÂíåÂéüÂßãchatglm‰∏ÄËá¥„ÄÇ\nÊ≥®ÊÑèÔºöÁî±‰∫éÁõÆÂâçÊ®°Âûã‰ªÖÁªèËøáblip2‰∏§Èò∂ÊÆµÂõæÊñáÂØπÈΩêÈ¢ÑËÆ≠ÁªÉÔºåÊ≤°ÊúâÂåÖÊã¨vqaÊàñËÄÖÊåá‰ª§ÂæÆË∞ÉÁ≠âÂÖ∑‰Ωì‰∏ãÊ∏∏‰ªªÂä°ÁöÑËÆ≠ÁªÉÔºåÂõ†Ê≠§‰æùÁÑ∂ÂÆπÊòìÁîüÊàê‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÁöÑÂÜÖÂÆπ„ÄÇ\nblip2 base model: bert-base-chinese\nVision encoder: eva-clip-vit-g\nLanguage model: chatglm-6b at commit\nModel Sources\nTraining Code: blip2ËÆ≠ÁªÉ‰ª£Á†ÅÔºåÂü∫‰∫éLAVIS\nwebui: ‰∏Ä‰∏™Áî±gradioÂÆûÁé∞ÁöÑwebui\napi: ‰∏Ä‰∏™Áî±fastapiÂÆûÁé∞ÁöÑapiÊúçÂä°ÔºåÂèØ‰ª•ÈÉ®ÁΩ≤Âú®Êú¨Âú∞ÔºåÂêåÊó∂‰πüÊîØÊåÅ‰∏Ä‰∫õÂÖ∂‰ªñÁ±ªÂûãÁöÑÊú¨Âú∞ÂèØÈÉ®ÁΩ≤ËØ≠Ë®ÄÊ®°Âûã„ÄÇ\nUses\nÊ®°ÂûãÂèÇÊï∞ÂåÖÂê´‰∫ÜÂõæÂÉèÁºñÁ†ÅÂô®Ôºåblip2Âíåchatglm-6b„ÄÇ\nÂä†ËΩΩÊ®°ÂûãÂèäÊé®ÁêÜÂèØ‰ª•ÂèÇËÄÉapiÁöÑÂÆûÁé∞\n‰∏Ä‰∫õexample\nLimitations\nÂèóÈôê‰∫é‰∏≠ÊñáÊï∞ÊçÆÈõÜÔºåÁõÆÂâçÂõæÂÉèÁêÜËß£ËÉΩÂäõ‰æùÁÑ∂ÊúâÈôêÔºå‰ºö‰∫ßÁîüÊó†ÂÖ≥ÊàñËÄÖÈîôËØØÁöÑÂÜÖÂÆπ„ÄÇ\nÁõÆÂâçÊ≤°ÊúâÂºïÂÖ•Â§öËΩÆÂØπËØùËÆ≠ÁªÉ‰ª•ÂèäÊåá‰ª§ÂæÆË∞É„ÄÇÂ§öËΩÆÂØπËØùÂèØËÉΩ‰ºöÂèóÂà∞‰∏ä‰∏ãÊñáÁöÑÂπ≤Êâ∞„ÄÇ\nÂπ∂‰∏îÂêåÊ†∑ÂèóÈôê‰∫échatglm-6bÊú¨Ë∫´ÁöÑÂØπËØùÊïàÊûú„ÄÇ\nTraining Details\nTraining Data\nlaion-2b-chinese: Êàë‰ª¨‰ªÖÈÄâÂèñ‰∫ÜÂÖ∂‰∏≠clipÂàÜÊï∞ËæÉÈ´òÁöÑ670kÂõæÊñáÂØπÂπ∂ÈááÊ†∑‰∫ÜÈÉ®ÂàÜÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇ\ncoco-zh\nflickr8k-zh\nTraining Procedure\nÂü∫‰∫éblip2ÁöÑ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊñπÊ≥ï\nDemos",
    "murodbek/uzroberta-panx-uz": "uzroberta-panx-uz\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nuzroberta-panx-uz\nThis model is a fine-tuned version of rifkat/uztext-3Gb-BPE-Roberta on the None dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.1626\nF1: 0.9175\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 5\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nF1\n0.0515\n1.0\n150\n0.1373\n0.9141\n0.0415\n2.0\n300\n0.1268\n0.9194\n0.0101\n3.0\n450\n0.1225\n0.9416\n0.0038\n4.0\n600\n0.1426\n0.9353\n0.0004\n5.0\n750\n0.1458\n0.9320\nFramework versions\nTransformers 4.27.3\nPytorch 2.0.0+cu117\nDatasets 2.11.0\nTokenizers 0.12.1"
}