{
    "IDEA-Research/grounding-dino-base": "Grounding DINO model (base variant)\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nGrounding DINO model (base variant)\nThe Grounding DINO model was proposed in Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection. The model achieves remarkable results, such as 52.5 AP on COCO zero-shot.\nGrounding DINO overview. Taken from the original paper.\nIntended uses & limitations\nYou can use the raw model for zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data).\nHow to use\nHere's how to use the model for zero-shot object detection:\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\nmodel_id = \"IDEA-Research/grounding-dino-base\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\nimage_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n# Check for cats and remote controls\n# VERY important: text queries need to be lowercased + end with a dot\ntext = \"a cat. a remote control.\"\ninputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = model(**inputs)\nresults = processor.post_process_grounded_object_detection(\noutputs,\ninputs.input_ids,\nbox_threshold=0.4,\ntext_threshold=0.3,\ntarget_sizes=[image.size[::-1]]\n)\nBibTeX entry and citation info\n@misc{liu2023grounding,\ntitle={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},\nauthor={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},\nyear={2023},\neprint={2303.05499},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "stabilityai/sd-turbo": "SD-Turbo Model Card\nModel Details\nModel Description\nModel Sources\nEvaluation\nUses\nDirect Use\nDiffusers\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nRecommendations\nHow to Get Started with the Model\nSD-Turbo Model Card\nSD-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.\nWe release SD-Turbo as a research artifact, and to study small, distilled text-to-image models. For increased quality and prompt understanding,\nwe recommend SDXL-Turbo.\nPlease note: For commercial use, please refer to https://stability.ai/license.\nModel Details\nModel Description\nSD-Turbo is a distilled version of Stable Diffusion 2.1, trained for real-time synthesis.\nSD-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the technical report), which allows sampling large-scale foundational\nimage diffusion models in 1 to 4 steps at high image quality.\nThis approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an\nadversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.\nDeveloped by: Stability AI\nFunded by: Stability AI\nModel type: Generative text-to-image model\nFinetuned from model: Stable Diffusion 2.1\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models),\nwhich implements the most popular diffusion frameworks (both training and inference).\nRepository: https://github.com/Stability-AI/generative-models\nPaper: https://stability.ai/research/adversarial-diffusion-distillation\nDemo [for the bigger SDXL-Turbo]: http://clipdrop.co/stable-diffusion-turbo\nEvaluation\nThe charts above evaluate user preference for SD-Turbo over other single- and multi-step models.\nSD-Turbo evaluated at a single step is preferred by human voters in terms of image quality and prompt following over LCM-Lora XL and LCM-Lora 1.5.\nNote: For increased quality, we recommend the bigger version SDXL-Turbo.\nFor details on the user study, we refer to the research paper.\nUses\nDirect Use\nThe model is intended for both non-commercial and commercial usage. Possible research areas and tasks include\nResearch on generative models.\nResearch on real-time applications of generative models.\nResearch on the impact of real-time generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nFor commercial use, please refer to https://stability.ai/membership.\nExcluded uses are described below.\nDiffusers\npip install diffusers transformers accelerate --upgrade\nText-to-image:\nSD-Turbo does not make use of guidance_scale or negative_prompt, we disable it with guidance_scale=0.0.\nPreferably, the model generates images of size 512x512 but higher image sizes work as well.\nA single step is enough to generate high quality images.\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\nImage-to-image:\nWhen using SD-Turbo for image-to-image generation, make sure that num_inference_steps * strength is larger or equal\nto 1. The image-to-image pipeline will run for int(num_inference_steps * strength) steps, e.g. 0.5 * 2.0 = 1 step in our example\nbelow.\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\npipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\").resize((512, 512))\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\nimage = pipe(prompt, image=init_image, num_inference_steps=2, strength=0.5, guidance_scale=0.0).images[0]\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events,\nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's Acceptable Use Policy.\nLimitations and Bias\nLimitations\nThe quality and prompt alignment is lower than that of SDXL-Turbo.\nThe generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.\nThe model cannot render legible text.\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nRecommendations\nThe model is intended for both non-commercial and commercial usage.\nHow to Get Started with the Model\nCheck out https://github.com/Stability-AI/generative-models",
    "mistralai/Mixtral-8x7B-Instruct-v0.1": "Model Card for Mixtral-8x7B\nInference with mistral_inference\nInference with hugging face transformers\nWarning\nInstruction format\nRun the model\nIn half-precision\nLower precision using (8-bit & 4-bit) using bitsandbytes\nLoad the model with Flash Attention 2\nLimitations\nThe Mistral AI Team\nModel Card for Mixtral-8x7B\nTokenization with mistral-common\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nmistral_models_path = \"MISTRAL_MODELS_PATH\"\ntokenizer = MistralTokenizer.v1()\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nInference with mistral_inference\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nInference with hugging face transformers\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\nmodel.to(\"cuda\")\ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\nPRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mistral-common reference implementation are very welcome!\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\nFor full details of this model please read our release blog post.\nWarning\nThis repo contains weights that are compatible with vLLM serving of the model as well as Hugging Face transformers library. It is based on the original Mixtral torrent release, but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.\nInstruction format\nThis format must be strictly respected, otherwise the model will generate sub-optimal outputs.\nThe template used to build a prompt for the Instruct model is defined as follows:\n<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\nNote that <s> and </s> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.\nAs reference, here is the pseudo-code used to tokenize instructions during fine-tuning:\ndef tokenize(text):\nreturn tok.encode(text, add_special_tokens=False)\n[BOS_ID] +\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_1) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\n‚Ä¶\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_N) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\nIn the pseudo-code above, note that the tokenize method should not add a BOS or EOS token automatically, but should add a prefix space.\nIn the Transformers library, one can use chat templates which make sure the right format is applied.\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\nmessages = [\n{\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n{\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n{\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\nIn half-precision\nNote float16 precision only works on GPU devices\nClick to expand\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\nmessages = [\n{\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n{\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n{\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLower precision using (8-bit & 4-bit) using bitsandbytes\nClick to expand\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\ntext = \"Hello my name is\"\nmessages = [\n{\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n{\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n{\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLoad the model with Flash Attention 2\nClick to expand\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, device_map=\"auto\")\nmessages = [\n{\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n{\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n{\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLimitations\nThe Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.",
    "acon96/Home-3B-v3-GGUF": "Home 3B v3\nTraining\nEvaluation\nDatasets\nLicense\nHome 3B v3\nThe \"Home\" model is a fine tuning of the StableLM-3B-Zephyr model from Stability AI.  The model is able to control devices in the user's house as well as perform basic question and answering.  The fine tuning dataset is a custom curated dataset designed to teach the model function calling.\nV3 of the model has a new base model (StableLM) that brings significant accuracy increases. Also added are: basic multi-personality support, basic multi-language support, and support for even more Home Assitant entity types (vacuum, timer, and todo).\nNOTE: the base models do not boast multi-language support but use a tokenizer that can handle non-english languages better than Phi-2. I have verified that it does technically work in German, Spanish, and French on some random examples where the request is an English request processed via Google Translate.\nThe model is quantized using Lama.cpp in order to enable running the model in super low resource environments that are common with Home Assistant installations such as Rapsberry Pis.\nThe model can be used as an \"instruct\" type model using the Zephyr prompt format. The system prompt is used to provide information about the state of the Home Assistant installation including available devices and callable services.\nExample \"system\" prompt:\nYou are 'Al', a helpful AI Assistant that controls the devices in a house. Complete the following task as instructed with the information provided only.\nServices: light.turn_off(), light.turn_on(brightness,rgb_color), fan.turn_on(), fan.turn_off()\nDevices:\nlight.office 'Office Light' = on;80%\nfan.office 'Office fan' = off\nlight.kitchen 'Kitchen Light' = on;80%;red\nlight.bedroom 'Bedroom Light' = off\nOutput from the model will consist of a response that should be relayed back to the user, along with an optional code block that will invoke different Home Assistant \"services\". The output format from the model for function calling is as follows:\nturning on the kitchen lights for you now\n\nThe model is also capable of basic instruct and QA tasks because of the instruction fine-tuning in the base model. For example, the model is able to perform basic logic tasks such as the following:\nuser if mary is 7 years old, and I am 3 years older than her. how old am I?\nassistant If Mary is 7 years old, then you are 10 years old (7+3=10).\nTraining\nThe model was trained as a LoRA on an RTX 3090 (24GB). The LoRA has rank = 64, alpha = 128, targets the up_proj,down_proj,q_proj,v_proj,o_proj modules. The full model is merged together at the end.\nEvaluation\nThis model acheives a 97.11% score for JSON function calling accuracy on the test dataset.\nDatasets\nSnythetic Dataset for SFT - https://huggingface.co/datasets/acon96/Home-Assistant-Requests\nLicense\nThis model is a fine-tuning of the Stability AI StableLM model series that is licensed under the STABILITY AI NON-COMMERCIAL RESEARCH COMMUNITY LICENSE AGREEMENT. As such this model is released under the same non-commerical STABILITY AI NON-COMMERCIAL RESEARCH COMMUNITY LICENSE AGREEMENT. The fine-tuned model is shared for non-commerical use ONLY.",
    "ministral/Ministral-3b-instruct": "Model Description\nModel Description\nMinistral is a series of language model, build with same architecture as the famous Mistral model, but with less size.\nModel type: A 3B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\nLanguage(s) (NLP): Primarily English\nLicense: Apache 2.0\nFinetuned from model: mistralai/Mistral-7B-v0.1",
    "capleaf/viXTTS": "vi‚ìçTTS\nLanguages\nKnown Limitations\nDemo\nUsage\nLicense\nContact\nvi‚ìçTTS\nvi‚ìçTTS l√† m√¥ h√¨nh t·∫°o sinh gi·ªçng n√≥i cho ph√©p b·∫°n sao ch√©p gi·ªçng n√≥i sang c√°c ng√¥n ng·ªØ kh√°c nhau ch·ªâ b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt ƒëo·∫°n √¢m thanh nhanh d√†i 6 gi√¢y. M√¥ h√¨nh n√†y ƒë∆∞·ª£c ti·∫øp t·ª•c ƒë√†o t·∫°o t·ª´ m√¥ h√¨nh XTTS-v2.0.3 b·∫±ng c√°ch m·ªü r·ªông tokenizer sang ti·∫øng Vi·ªát v√† hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu viVoice.\nvi‚ìçTTS is a voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. This model is fine-tuned from the XTTS-v2.0.3 model by expanding the tokenizer to Vietnamese and fine-tuning on the viVoice dataset.\nLanguages\nviXTTS supports 18 languages: English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt),\nPolish (pl), Turkish (tr), Russian (ru), Dutch (nl), Czech (cs), Arabic (ar), Chinese (zh-cn), Japanese (ja), Hungarian (hu), Korean (ko)\nHindi (hi), Vietnamese (vi).\nKnown Limitations\nIncompatibility with the original TTS library (a pull request will be made later).\nSubpar performance for input sentences under 10 words in Vietnamese language (yielding inconsistent output and odd trailing sounds).\nThis model is only fine-tuned in Vietnamese. The model's effectiveness with languages other than Vietnamese hasn't been tested, potentially reducing quality.\nDemo\nPlease checkout this repo\nUsage\nFor a quick usage, please checkout this notebook\nLicense\nThis model is licensed under Coqui Public Model License.\nContact\nFine-tuned by Thinh Le at FPT University HCMC, as a component of Non La's graduation thesis.\nContact:\nYou can message me directly on Facebook: https://fb.com/thinhlpg/ (preferred ü§ó)\nGitHub: https://github.com/thinhlpg\nEmail: thinhlpg@gmail.com or thinhlpgse161384@fpt.edu.vn",
    "microsoft/Phi-3-mini-4k-instruct": "Model Summary\nIntended Uses\nRelease Notes\nHow to Use\nTokenizer\nChat Format\nSample inference code\nResponsible AI Considerations\nTraining\nModel\nDatasets\nFine-tuning\nBenchmarks\nCross Platform Support\nSoftware\nHardware\nLicense\nTrademarks\nüéâ Phi-3.5: [mini-instruct]; [MoE-instruct] ; [vision-instruct]\nModel Summary\nThe Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\nResources and Technical Documentation:\nüè° Phi-3 Portal\nüì∞ Phi-3 Microsoft Blog\nüìñ Phi-3 Technical Report\nüõ†Ô∏è Phi-3 on Azure AI Studio\nüë©‚Äçüç≥ Phi-3 Cookbook\nüñ•Ô∏è Try It\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\nIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require\nmemory/compute constrained environments;\nlatency bound scenarios;\nstrong reasoning (especially math and logic).\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nOut-of-scope use cases\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nRelease Notes\nThis is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback.\nThe model used additional post-training data leading to substantial gains on instruction following and structure output.\nWe also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability.\nWe believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications.\nWe appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community.\nThe table below highlights improvements on instruction following, structure output, and reasoning of the new release on publich and internal benchmark datasets.\nBenchmarks\nOriginal\nJune 2024 Update\nInstruction Extra Hard\n5.7\n6.0\nInstruction Hard\n4.9\n5.1\nInstructions Challenge\n24.6\n42.3\nJSON Structure Output\n11.5\n52.3\nXML Structure Output\n14.4\n49.8\nGPQA\n23.7\n30.6\nMMLU\n68.8\n70.9\nAverage\n21.9\n36.7\nNotes: if users would like to check out the previous version, use the git commit id ff07dc01615f8113924aed013115ab2abd32115b. For the model conversion, e.g. GGUF and other formats, we invite the community to experiment with various approaches and share your valuable feedback. Let's innovate together!\nHow to Use\nPhi-3 Mini-4K-Instruct has been integrated in the 4.41.2 version of transformers. The current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.41.2\nPhi-3 Mini-4K-Instruct is also available in Azure AI Studio\nTokenizer\nPhi-3 Mini-4K-Instruct supports a vocabulary size of up to 32064 tokens. The tokenizer files already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\nChat Format\nGiven the nature of the training data, the Phi-3 Mini-4K-Instruct model is best suited for prompts using the chat format as follows.\nYou can provide the prompt as a question with a generic template as follow:\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nQuestion?<|end|>\n<|assistant|>\nFor example:\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\nwhere the model generates the text after <|assistant|> . In case of few-shots prompt, the prompt can be formatted as the following:\n<|system|>\nYou are a helpful travel assistant.<|end|>\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\nSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\ntorch.random.manual_seed(0)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"microsoft/Phi-3-mini-4k-instruct\",\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\nNote: If you want to use flash attention, call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"flash_attention_2\"\nResponsible AI Considerations\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nTraining\nModel\nArchitecture: Phi-3 Mini-4K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\nInputs: Text. It is best suited for prompts using chat format.\nContext length: 4K tokens\nGPUs: 512 H100-80G\nTraining time: 10 days\nTraining data: 4.9T tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between May and June 2024\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\nRelease dates: June, 2024.\nDatasets\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of\nPublicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\nNewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\nHigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the Phi-3 Technical Report.\nFine-tuning\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided here.\nBenchmarks\nWe report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0.\nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\nThe number of k‚Äìshot examples is listed per-benchmark.\nCategory\nBenchmark\nPhi-3-Mini-4K-Ins\nGemma-7B\nMistral-7b\nMixtral-8x7b\nLlama-3-8B-Ins\nGPT3.5-Turbo-1106\nPopular aggregated benchmark\nAGI Eval 5-shot\n39.0\n42.1\n35.1\n45.2\n42\n48.4\nMMLU 5-shot\n70.9\n63.6\n61.7\n70.5\n66.5\n71.4\nBigBench Hard CoT3-shot\n73.5\n59.6\n57.3\n69.7\n51.5\n68.3\nLanguage Understanding\nANLI 7-shot\n53.6\n48.7\n47.1\n55.2\n57.3\n58.1\nHellaSwag 5-shot\n75.3\n49.8\n58.5\n70.4\n71.1\n78.8\nReasoning\nARC Challenge 10-shot\n86.3\n78.3\n78.6\n87.3\n82.8\n87.4\nBoolQ 0-shot\n78.1\n66\n72.2\n76.6\n80.9\n79.1\nMedQA 2-shot\n56.5\n49.6\n50\n62.2\n60.5\n63.4\nOpenBookQA 10-shot\n82.2\n78.6\n79.8\n85.8\n82.6\n86\nPIQA 5-shot\n83.5\n78.1\n77.7\n86\n75.7\n86.6\nGPQA 0-shot\n30.6\n2.9\n15\n6.9\n32.4\n30.8\nSocial IQA 5-shot\n77.6\n65.5\n74.6\n75.9\n73.9\n68.3\nTruthfulQA (MC2) 10-shot\n64.7\n52.1\n53\n60.1\n63.2\n67.7\nWinoGrande 5-shot\n71.6\n55.6\n54.2\n62\n65\n68.8\nFactual Knowledge\nTriviaQA 5-shot\n61.4\n72.3\n75.2\n82.2\n67.7\n85.8\nMath\nGSM8K CoT 8-shot\n85.7\n59.8\n46.4\n64.7\n77.4\n78.1\nCode Generation\nHumanEval 0-shot\n57.3\n34.1\n28.0\n37.8\n60.4\n62.2\nMBPP 3-shot\n69.8\n51.5\n50.8\n60.2\n67.7\n77.8\nAverage\n67.6\n56.0\n56.4\n64.4\n65.5\n70.4\nWe take a closer look at different categories across 100 public benchmark datasets at the table below:\nCategory\nPhi-3-Mini-4K-Instruct\nGemma-7B\nMistral-7B\nMixtral 8x7B\nLlama-3-8B-Instruct\nGPT-3.5-Turbo\nPopular aggregated benchmark\n61.1\n59.4\n56.5\n66.2\n59.9\n67.0\nReasoning\n70.8\n60.3\n62.8\n68.1\n69.6\n71.8\nLanguage understanding\n60.5\n57.6\n52.5\n66.1\n63.2\n67.7\nCode generation\n60.7\n45.6\n42.9\n52.7\n56.4\n70.4\nMath\n50.6\n35.8\n25.4\n40.3\n41.1\n52.8\nFactual knowledge\n38.4\n46.7\n49.8\n58.6\n43.1\n63.4\nMultilingual\n56.7\n66.5\n57.4\n66.7\n66.6\n71.0\nRobustness\n61.1\n38.4\n40.6\n51.0\n64.5\n69.3\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.\nCross Platform Support\nONNX runtime now supports Phi-3 mini models across platforms and hardware.\nOptimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA).\nAlong with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile.\nHere are some of the optimized configurations we have added:\nONNX models for int4 DML: Quantized to int4 via AWQ\nONNX model for fp16 CUDA\nONNX model for int4 CUDA: Quantized to int4 via RTN\nONNX model for int4 CPU and Mobile: Quantized to int4 via R\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nHardware\nNote that by default, the Phi-3 Mini-4K-Instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nIf you want to run the model on:\nNVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\nCPU: use the GGUF quantized models 4K\nOptimized inference on GPU, CPU, and Mobile: use the ONNX models 4K\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.",
    "Alibaba-NLP/gte-multilingual-base": "gte-multilingual-base\nModel Information\nUsage\nGet Dense Embeddings with Transformers\nUse with sentence-transformers\nUse with infinity\nUse with Text Embeddings Inference (TEI)\nUse with custom code to get dense embeddings and sparse token weights\nEvaluation\nRetrieval Task\nMTEB\nCloud API Services\nCitation\ngte-multilingual-base\nThe gte-multilingual-base model is the latest in the GTE (General Text Embedding) family of models, featuring several key attributes:\nHigh Performance: Achieves state-of-the-art (SOTA) results in multilingual retrieval tasks and multi-task representation model evaluations when compared to models of similar size.\nTraining Architecture: Trained using an encoder-only transformers architecture, resulting in a smaller model size. Unlike previous models based on decode-only LLM architecture (e.g., gte-qwen2-1.5b-instruct), this model has lower hardware requirements for inference, offering a 10x increase in inference speed.\nLong Context: Supports text lengths up to 8192 tokens.\nMultilingual Capability: Supports over 70 languages.\nElastic Dense Embedding: Support elastic output dense representation while maintaining the effectiveness of downstream tasks, which significantly reduces storage costs and improves execution efficiency.\nSparse Vectors: In addition to dense representations, it can also generate sparse vectors.\nPaper: mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval\nModel Information\nModel Size: 305M\nEmbedding Dimension: 768\nMax Input Tokens: 8192\nUsage\nIt is recommended to install xformers and enable unpadding for acceleration,\nrefer to enable-unpadding-and-xformers.\nHow to use it offline: new-impl/discussions/2\nHow to use with TEI: refs/pr/7\nGet Dense Embeddings with Transformers\n# Requires transformers>=4.36.0\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\ninput_texts = [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Âåó‰∫¨\",\n\"Âø´ÊéíÁÆóÊ≥ï‰ªãÁªç\"\n]\nmodel_name_or_path = 'Alibaba-NLP/gte-multilingual-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True)\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\ndimension=768 # The output dimension of the output embedding, should be in [128, 768]\nembeddings = outputs.last_hidden_state[:, 0][:dimension]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n# [[0.3016996383666992, 0.7503870129585266, 0.3203084468841553]]\nUse with sentence-transformers\n# Requires sentence-transformers>=3.0.0\nfrom sentence_transformers import SentenceTransformer\ninput_texts = [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Âåó‰∫¨\",\n\"Âø´ÊéíÁÆóÊ≥ï‰ªãÁªç\"\n]\nmodel_name_or_path=\"Alibaba-NLP/gte-multilingual-base\"\nmodel = SentenceTransformer(model_name_or_path, trust_remote_code=True)\nembeddings = model.encode(input_texts, normalize_embeddings=True) # embeddings.shape (4, 768)\n# sim scores\nscores = model.similarity(embeddings[:1], embeddings[1:])\nprint(scores.tolist())\n# [[0.301699697971344, 0.7503870129585266, 0.32030850648880005]]\nUse with infinity\nUsage via docker and infinity, MIT Licensed.\ndocker run --gpus all -v $PWD/data:/app/.cache -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.69 \\\nv2 --model-id Alibaba-NLP/gte-multilingual-base --revision \"main\" --dtype float16 --batch-size 32 --device cuda --engine torch --port 7997\nUse with Text Embeddings Inference (TEI)\nUsage via Docker and Text Embeddings Inference (TEI):\nCPU:\ndocker run --platform linux/amd64 \\\n-p 8080:80 \\\n-v $PWD/data:/data \\\n--pull always \\\nghcr.io/huggingface/text-embeddings-inference:cpu-1.7 \\\n--model-id Alibaba-NLP/gte-multilingual-base \\\n--dtype float16\nGPU:\ndocker run --gpus all \\\n-p 8080:80 \\\n-v $PWD/data:/data \\\n--pull always \\\nghcr.io/huggingface/text-embeddings-inference:1.7 \\\n--model-id Alibaba-NLP/gte-multilingual-base \\\n--dtype float16\nThen you can send requests to the deployed API via the OpenAI-compatible v1/embeddings route (more information about the OpenAI Embeddings API):\ncurl https://0.0.0.0:8080/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"input\": [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Âåó‰∫¨\",\n\"Âø´ÊéíÁÆóÊ≥ï‰ªãÁªç\"\n],\n\"model\": \"Alibaba-NLP/gte-multilingual-base\",\n\"encoding_format\": \"float\"\n}'\nUse with custom code to get dense embeddings and sparse token weights\n# You can find the script gte_embedding.py in https://huggingface.co/Alibaba-NLP/gte-multilingual-base/blob/main/scripts/gte_embedding.py\nfrom gte_embedding import GTEEmbeddidng\nmodel_name_or_path = 'Alibaba-NLP/gte-multilingual-base'\nmodel = GTEEmbeddidng(model_name_or_path)\nquery = \"‰∏≠ÂõΩÁöÑÈ¶ñÈÉΩÂú®Âì™ÂÑø\"\ndocs = [\n\"what is the capital of China?\",\n\"how to implement quick sort in python?\",\n\"Âåó‰∫¨\",\n\"Âø´ÊéíÁÆóÊ≥ï‰ªãÁªç\"\n]\nembs = model.encode(docs, return_dense=True,return_sparse=True)\nprint('dense_embeddings vecs', embs['dense_embeddings'])\nprint('token_weights', embs['token_weights'])\npairs = [(query, doc) for doc in docs]\ndense_scores = model.compute_scores(pairs, dense_weight=1.0, sparse_weight=0.0)\nsparse_scores = model.compute_scores(pairs, dense_weight=0.0, sparse_weight=1.0)\nhybrid_scores = model.compute_scores(pairs, dense_weight=1.0, sparse_weight=0.3)\nprint('dense_scores', dense_scores)\nprint('sparse_scores', sparse_scores)\nprint('hybrid_scores', hybrid_scores)\n# dense_scores [0.85302734375, 0.257568359375, 0.76953125, 0.325439453125]\n# sparse_scores [0.0, 0.0, 4.600879669189453, 1.570279598236084]\n# hybrid_scores [0.85302734375, 0.257568359375, 2.1497951507568356, 0.7965233325958252]\nEvaluation\nWe validated the performance of the gte-multilingual-base model on multiple downstream tasks, including multilingual retrieval, cross-lingual retrieval, long text retrieval, and general text representation evaluation on the MTEB Leaderboard, among others.\nRetrieval Task\nRetrieval results on MIRACL and MLDR (multilingual), MKQA (crosslingual), BEIR and LoCo (English).\nDetail results on MLDR\nDetail results on LoCo\nMTEB\nResults on MTEB English, Chinese, French, Polish\nMore detailed experimental results can be found in the paper.\nCloud API Services\nIn addition to the open-source GTE series models, GTE series models are also available as commercial API services on Alibaba Cloud.\nEmbedding Models: Three versions of the text embedding models are available: text-embedding-v1/v2/v3, with v3 being the latest API service.\nReRank Models: The gte-rerank model service is available.\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\nCitation\nIf you find our paper or models helpful, please consider cite:\n@inproceedings{zhang2024mgte,\ntitle={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\nauthor={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\nbooktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\npages={1393--1412},\nyear={2024}\n}",
    "jinaai/jina-embeddings-v3": "Quick Start\nIntended Usage & Model Info\nKey Features:\nSupported Languages:\nUsage\nWhy Use Mean Pooling?\nContact\nLicense\nCitation\nThe embedding model trained by Jina AI.\njina-embeddings-v3: Multilingual Embeddings With Task LoRA\nQuick Start\nBlog | Azure | AWS SageMaker | API\nIntended Usage & Model Info\njina-embeddings-v3 is a multilingual multi-task text embedding model designed for a variety of NLP applications.\nBased on the Jina-XLM-RoBERTa architecture,\nthis model supports Rotary Position Embeddings to handle long input sequences up to 8192 tokens.\nAdditionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently.\nKey Features:\nExtended Sequence Length: Supports up to 8192 tokens with RoPE.\nTask-Specific Embedding: Customize embeddings through the task argument with the following options:\nretrieval.query: Used for query embeddings in asymmetric retrieval tasks\nretrieval.passage: Used for passage embeddings in asymmetric retrieval tasks\nseparation: Used for embeddings in clustering and re-ranking applications\nclassification: Used for embeddings in classification tasks\ntext-matching: Used for embeddings in tasks that quantify similarity between two texts, such as STS or symmetric retrieval tasks\nMatryoshka Embeddings: Supports flexible embedding sizes (32, 64, 128, 256, 512, 768, 1024), allowing for truncating embeddings to fit your application.\nSupported Languages:\nWhile the foundation model supports 100 languages, we've focused our tuning efforts on the following 30 languages:\nArabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, Georgian, German, Greek,\nHindi, Indonesian, Italian, Japanese, Korean, Latvian, Norwegian, Polish, Portuguese, Romanian,\nRussian, Slovak, Spanish, Swedish, Thai, Turkish, Ukrainian, Urdu, and Vietnamese.\n‚ö†Ô∏è Important Notice:We fixed a bug in the encode function #60 where Matryoshka embedding truncation occurred after normalization, leading to non-normalized truncated embeddings. This issue has been resolved in the latest code revision.\nIf you have encoded data using the previous version and wish to maintain consistency, please use the specific code revision when loading the model: AutoModel.from_pretrained('jinaai/jina-embeddings-v3', code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', ...)\nUsage\nApply mean pooling when integrating the model.\nWhy Use Mean Pooling?\nMean pooling takes all token embeddings from the model's output and averages them at the sentence or paragraph level.\nThis approach has been shown to produce high-quality sentence embeddings.\nWe provide an encode function that handles this for you automatically.\nHowever, if you're working with the model directly, outside of the encode function,\nyou'll need to apply mean pooling manually. Here's how you can do it:\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]\ninput_mask_expanded = (\nattention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n)\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\ninput_mask_expanded.sum(1), min=1e-9\n)\nsentences = [\"How is the weather today?\", \"What is the current weather like today?\"]\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\")\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\ntask = 'retrieval.query'\ntask_id = model._adaptation_map[task]\nadapter_mask = torch.full((len(sentences),), task_id, dtype=torch.int32)\nwith torch.no_grad():\nmodel_output = model(**encoded_input, adapter_mask=adapter_mask)\nembeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\nembeddings = F.normalize(embeddings, p=2, dim=1)\nThe easiest way to start using jina-embeddings-v3 is with the Jina Embedding API.\nAlternatively, you can use jina-embeddings-v3 directly via Transformers package:\n!pip install transformers torch einops\n!pip install 'numpy<2'\nIf you run it on a GPU that support FlashAttention-2. By 2024.9.12, it supports Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100),\n!pip install flash-attn --no-build-isolation\nfrom transformers import AutoModel\n# Initialize the model\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\ntexts = [\n\"Follow the white rabbit.\",  # English\n\"Sigue al conejo blanco.\",  # Spanish\n\"Suis le lapin blanc.\",  # French\n\"Ë∑üÁùÄÁôΩÂÖîËµ∞„ÄÇ\",  # Chinese\n\"ÿßÿ™ÿ®ÿπ ÿßŸÑÿ£ÿ±ŸÜÿ® ÿßŸÑÿ£ÿ®Ÿäÿ∂.\",  # Arabic\n\"Folge dem wei√üen Kaninchen.\",  # German\n]\n# When calling the `encode` function, you can choose a `task` based on the use case:\n# 'retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching'\n# Alternatively, you can choose not to pass a `task`, and no specific LoRA adapter will be used.\nembeddings = model.encode(texts, task=\"text-matching\")\n# Compute similarities\nprint(embeddings[0] @ embeddings[1].T)\nBy default, the model supports a maximum sequence length of 8192 tokens.\nHowever, if you want to truncate your input texts to a shorter length, you can pass the max_length parameter to the encode function:\nembeddings = model.encode([\"Very long ... document\"], max_length=2048)\nIn case you want to use Matryoshka embeddings and switch to a different dimension,\nyou can adjust it by passing the truncate_dim parameter to the encode function:\nembeddings = model.encode(['Sample text'], truncate_dim=256)\nThe latest version (3.1.0) of SentenceTransformers also supports jina-embeddings-v3:\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\ntask = \"retrieval.query\"\nembeddings = model.encode(\n[\"What is the weather like in Berlin today?\"],\ntask=task,\nprompt_name=task,\n)\nYou can fine-tune jina-embeddings-v3 using SentenceTransformerTrainer.\nTo fine-tune for a specific task, you should set the task before passing the model to the ST Trainer, either during initialization:\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True, model_kwargs={'default_task': 'classification'})\nOr afterwards:\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\nmodel[0].default_task = 'classification'\nThis way you can fine-tune the LoRA adapter for the chosen task.\nHowever, If you want to fine-tune the entire model, make sure the main parameters are set as trainable when loading the model:\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True, model_kwargs={'lora_main_params_trainable': True})\nThis will allow fine-tuning the whole model instead of just the LoRA adapters.\nONNX Inference.\nYou can use ONNX for efficient inference with jina-embeddings-v3:\nimport onnxruntime\nimport numpy as np\nfrom transformers import AutoTokenizer, PretrainedConfig\n# Mean pool function\ndef mean_pooling(model_output: np.ndarray, attention_mask: np.ndarray):\ntoken_embeddings = model_output\ninput_mask_expanded = np.expand_dims(attention_mask, axis=-1)\ninput_mask_expanded = np.broadcast_to(input_mask_expanded, token_embeddings.shape)\nsum_embeddings = np.sum(token_embeddings * input_mask_expanded, axis=1)\nsum_mask = np.clip(np.sum(input_mask_expanded, axis=1), a_min=1e-9, a_max=None)\nreturn sum_embeddings / sum_mask\n# Load tokenizer and model config\ntokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3')\nconfig = PretrainedConfig.from_pretrained('jinaai/jina-embeddings-v3')\n# Tokenize input\ninput_text = tokenizer('sample text', return_tensors='np')\n# ONNX session\nmodel_path = 'jina-embeddings-v3/onnx/model.onnx'\nsession = onnxruntime.InferenceSession(model_path)\n# Prepare inputs for ONNX model\ntask_type = 'text-matching'\ntask_id = np.array(config.lora_adaptations.index(task_type), dtype=np.int64)\ninputs = {\n'input_ids': input_text['input_ids'],\n'attention_mask': input_text['attention_mask'],\n'task_id': task_id\n}\n# Run model\noutputs = session.run(None, inputs)[0]\n# Apply mean pooling and normalization to the model outputs\nembeddings = mean_pooling(outputs, input_text[\"attention_mask\"])\nembeddings = embeddings / np.linalg.norm(embeddings, ord=2, axis=1, keepdims=True)\nContact\nJoin our Discord community and chat with other community members about ideas.\nLicense\njina-embeddings-v3 is listed on AWS & Azure. If you need to use it beyond those platforms or on-premises within your company, note that the models is licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to contact us.\nCitation\nIf you find jina-embeddings-v3 useful in your research, please cite the following paper:\n@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\ntitle={jina-embeddings-v3: Multilingual Embeddings With Task LoRA},\nauthor={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G√ºnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\nyear={2024},\neprint={2409.10173},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2409.10173},\n}",
    "Qwen/Qwen2.5-14B-Instruct": "Qwen2.5-14B-Instruct\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-14B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 14B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 14.7B\nNumber of Paramaters (Non-Embedding): 13.1B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-14B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "bartowski/Llama-3.2-1B-Instruct-GGUF": "Llamacpp imatrix Quantizations of Llama-3.2-1B-Instruct\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Llama-3.2-1B-Instruct\nUsing llama.cpp release b3821 for quantization.\nOriginal model: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nLlama-3.2-1B-Instruct-f16.gguf\nf16\n2.48GB\nfalse\nFull F16 weights.\nLlama-3.2-1B-Instruct-Q8_0.gguf\nQ8_0\n1.32GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nLlama-3.2-1B-Instruct-Q6_K_L.gguf\nQ6_K_L\n1.09GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nLlama-3.2-1B-Instruct-Q6_K.gguf\nQ6_K\n1.02GB\nfalse\nVery high quality, near perfect, recommended.\nLlama-3.2-1B-Instruct-Q5_K_L.gguf\nQ5_K_L\n0.98GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nLlama-3.2-1B-Instruct-Q5_K_M.gguf\nQ5_K_M\n0.91GB\nfalse\nHigh quality, recommended.\nLlama-3.2-1B-Instruct-Q5_K_S.gguf\nQ5_K_S\n0.89GB\nfalse\nHigh quality, recommended.\nLlama-3.2-1B-Instruct-Q4_K_L.gguf\nQ4_K_L\n0.87GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nLlama-3.2-1B-Instruct-Q4_K_M.gguf\nQ4_K_M\n0.81GB\nfalse\nGood quality, default size for must use cases, recommended.\nLlama-3.2-1B-Instruct-Q3_K_XL.gguf\nQ3_K_XL\n0.80GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nLlama-3.2-1B-Instruct-Q4_K_S.gguf\nQ4_K_S\n0.78GB\nfalse\nSlightly lower quality with more space savings, recommended.\nLlama-3.2-1B-Instruct-Q4_0_8_8.gguf\nQ4_0_8_8\n0.77GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below).\nLlama-3.2-1B-Instruct-Q4_0_4_8.gguf\nQ4_0_4_8\n0.77GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below).\nLlama-3.2-1B-Instruct-Q4_0_4_4.gguf\nQ4_0_4_4\n0.77GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure.\nLlama-3.2-1B-Instruct-Q4_0.gguf\nQ4_0\n0.77GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nLlama-3.2-1B-Instruct-IQ4_XS.gguf\nIQ4_XS\n0.74GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nLlama-3.2-1B-Instruct-Q3_K_L.gguf\nQ3_K_L\n0.73GB\nfalse\nLower quality but usable, good for low RAM availability.\nLlama-3.2-1B-Instruct-IQ3_M.gguf\nIQ3_M\n0.66GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Llama-3.2-1B-Instruct-GGUF --include \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Llama-3.2-1B-Instruct-GGUF --include \"Llama-3.2-1B-Instruct-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Llama-3.2-1B-Instruct-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "XLabs-AI/flux-ip-adapter-v2": "Models\nExamples\nInference\nInstruction for ComfyUI\nLimitations\nLicense\nThis repository provides a IP-Adapter checkpoint for\nFLUX.1-dev model by Black Forest Labs\nSee our github for comfy ui workflows.\nModels\nThe IP adapter is trained on a resolution of 512x512 for 150k steps and 1024x1024 for 350k steps while maintaining the aspect ratio.\nWe release v2 version - which can be used directly in ComfyUI!\nPlease, see our ComfyUI custom nodes installation guide\nExamples\nSee examples of our models results below.Also, some generation results with input images are provided in \"Files and versions\"\nInference\nTo try our models, you have 2 options:\nUse main.py from our official repo\nUse our custom nodes for ComfyUI and test it with provided workflows (check out folder /workflows)\nInstruction for ComfyUI\nGo to ComfyUI/custom_nodes\nClone x-flux-comfyui, path should be ComfyUI/custom_nodes/x-flux-comfyui/*, where * is all the files in this repo\nGo to ComfyUI/custom_nodes/x-flux-comfyui/ and run python setup.py\nUpdate x-flux-comfy with git pull or reinstall it.\nDownload Clip-L model.safetensors from OpenAI VIT CLIP large, and put it to ComfyUI/models/clip_vision/*.\nDownload our IPAdapter from huggingface, and put it to ComfyUI/models/xlabs/ipadapters/*.\nUse Flux Load IPAdapter and Apply Flux IPAdapter nodes, choose right CLIP model and enjoy your genereations.\nYou can find example workflow in folder workflows in this repo.\nIf you get bad results, try to set to play with ip strength\nLimitations\nThe IP Adapter is currently in beta.\nWe do not guarantee that you will get a good result right away, it may take more attempts to get a result.\nLicense\nOur weights fall under the FLUX.1 [dev] Non-Commercial License",
    "HuggingFaceTB/SmolLM2-135M-Instruct": "SmolLM2\nTable of Contents\nModel Summary\nHow to use\nTransformers\nChat in TRL\nTransformers.js\nEvaluation\nBase pre-trained model\nInstruction model\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nSmolLM2\nTable of Contents\nModel Summary\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper https://arxiv.org/abs/2502.02737\nSmolLM2 demonstrates significant advances over its predecessor SmolLM1, particularly in instruction following, knowledge, reasoning. The 135M model was trained on 2 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new filtered datasets we curated and will release soon.  We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling (for the 1.7B) thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nYou can find the SFT dataset here: https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk and finetuning code at https://github.com/huggingface/alignment-handbook/tree/main/recipes/smollm2\nHow to use\nTransformers\npip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\nmessages = [{\"role\": \"user\", \"content\": \"What is gravity?\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))\nChat in TRL\nYou can also use the TRL CLI to chat with the model from the terminal:\npip install trl\ntrl chat --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct --device cpu\nTransformers.js\nnpm i @huggingface/transformers\nimport { pipeline } from \"@huggingface/transformers\";\n// Create a text generation pipeline\nconst generator = await pipeline(\n\"text-generation\",\n\"HuggingFaceTB/SmolLM2-135M-Instruct\",\n);\n// Define the list of messages\nconst messages = [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"What is the capital of France?\" },\n];\n// Generate a response\nconst output = await generator(messages, { max_new_tokens: 128 });\nconsole.log(output[0].generated_text.at(-1).content);\n// \"The capital of France is Paris.\"\nEvaluation\nIn this section, we report the evaluation results of SmolLM2. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nBase pre-trained model\nMetrics\nSmolLM2-135M-8k\nSmolLM-135M\nHellaSwag\n42.1\n41.2\nARC (Average)\n43.9\n42.4\nPIQA\n68.4\n68.4\nMMLU (cloze)\n31.5\n30.2\nCommonsenseQA\n33.9\n32.7\nTriviaQA\n4.1\n4.3\nWinogrande\n51.3\n51.3\nOpenBookQA\n34.6\n34.0\nGSM8K (5-shot)\n1.4\n1.0\nInstruction model\nMetric\nSmolLM2-135M-Instruct\nSmolLM-135M-Instruct\nIFEval (Average prompt/inst)\n29.9\n17.2\nMT-Bench\n19.8\n16.8\nHellaSwag\n40.9\n38.9\nARC (Average)\n37.3\n33.9\nPIQA\n66.3\n64.0\nMMLU (cloze)\n29.3\n28.3\nBBH (3-shot)\n28.2\n25.2\nGSM8K (5-shot)\n1.4\n1.4\nLimitations\nSmolLM2 models primarily understand and generate content in English. They can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 2T\nPrecision: bfloat16\nHardware\nGPUs: 64 H100\nSoftware\nTraining Framework: nanotron\nLicense\nApache 2.0\nCitation\n@misc{allal2025smollm2smolgoesbig,\ntitle={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},\nauthor={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Mart√≠n Bl√°zquez and Guilherme Penedo and Lewis Tunstall and Andr√©s Marafioti and Hynek Kydl√≠ƒçek and Agust√≠n Piqueres Lajar√≠n and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Cl√©mentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\nyear={2025},\neprint={2502.02737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02737},\n}",
    "fishaudio/fish-speech-1.5": "Fish Speech V1.5\nCitation\nLicense\nFish Speech V1.5\nFish Speech V1.5 is a leading text-to-speech (TTS) model trained on more than 1 million hours of audio data in multiple languages.\nSupported languages:\nEnglish (en) >300k hours\nChinese (zh) >300k hours\nJapanese (ja) >100k hours\nGerman (de) ~20k hours\nFrench (fr) ~20k hours\nSpanish (es) ~20k hours\nKorean (ko) ~20k hours\nArabic (ar) ~20k hours\nRussian (ru) ~20k hours\nDutch (nl) <10k hours\nItalian (it) <10k hours\nPolish (pl) <10k hours\nPortuguese (pt) <10k hours\nPlease refer to Fish Speech Github for more info.Demo available at Fish Audio.\nCitation\nIf you found this repository useful, please consider citing this work:\n@misc{fish-speech-v1.4,\ntitle={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},\nauthor={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},\nyear={2024},\neprint={2411.01156},\narchivePrefix={arXiv},\nprimaryClass={cs.SD},\nurl={https://arxiv.org/abs/2411.01156},\n}\nLicense\nThis model is permissively licensed under the CC-BY-NC-SA-4.0 license.",
    "DavidAU/L3.1-RP-Hero-BigTalker-8B-GGUF": "WARNING: NSFW. Vivid prose. INTENSE. Visceral Details. Violence. Graphic HORROR. GORE. Swearing. UNCENSORED.\nL3.1-RP-Hero-BigTalker-8B-GGUF\nIt is a LLama3.1 model, max context of 128k (131,000) and is a dedicated \"roleplay model\" (it can also be used for creative uses).\nThis model has been designed to be relatively bullet proof and operates with all parameters, including temp settings from 0 to 5.\nIt is an extraordinary compressed model, with a very low perplexity level (lower than Meta Llama 3.1 Instruct).\nThis model is for any writing, fiction or roleplay activity, but it is composed of ROLE PLAY models and it primary designed for role play.\nIt also has stronger than average instruction following attibutes.\nThis is version \"Big Talker\", which has two additional versions: \"InBetween\" and \"Dirty Harry\".\nInBetween (medium output, slightly less uncensored):\n[ https://huggingface.co/DavidAU/L3.1-RP-Hero-InBetween-8B-GGUF ]\nDirty Harry (short output, uncensored)\n[ https://huggingface.co/DavidAU/L3.1-RP-Hero-Dirty_Harry-8B-GGUF ]\n\"Big Talker\" has long (average) level length output, and is uncensored (note: InBetween has a slight degree of censorship).\n\"Big Talker\" also has slightly higher detail level than \"InBetween\", but on par with \"Dirty Harry\".\nAll versions are composed of top rated Role Play models.\nThis model, as well as the other two versions, can be used for any creative genre too.\nIt requires Llama3 template and/or \"Command-R\" template.\nFor roleplay settings, and apps to use this model for roleplay see the section \"Highest Quality Settings...\" below.\nExample outputs below to show prose quality / creativity.\nA few EXL2 quants are also available, links below.\nModel Notes:\nDetail, prose and fiction writing abilities are significantly improved.\nFor more varied prose (sentence/paragraph/dialog) raise the temp and/or add more instructions in your prompt(s).\nRole-players: Careful raising temp too high as it may affect instruction following.\nThis model works with rep pen of 1 or higher, 1.02+ recommended.\nIf you want a specific type of prose (IE horror) add in \"(vivid horror)\" or \"(graphic vivid horror)\" (no quotes) in your prompt(s).\nThis model has a neutral to negative bias BUT can be controlled by prompt/prose controls directly.\nOutput length will vary however this model prefers \"long\" outputs unless you state the size.\nFor creative uses, different quants will produce slightly different output.\nDue to the high stability and compressed nature of this model, all quants will operate at above average levels.\nSource code for this model will be uploaded at separate repo shortly.\nSettings, Quants and Critical Operations Notes:\nChange in temp (ie, .4, .8, 1.5, 2, 3 ) will drastically alter output.\nRep pen settings will also alter output too.\nThis model needs \"rep pen\" of 1.05 or higher as lower values may cause repeat paragraph issues at end of output however LOWER rep pen\nvalues may result is very different (creative / unusual) generation too.\nFor role play: Rep pen of 1.02 min is suggested.\nRaise/lower rep pen SLOWLY ie: 1.011, 1.012 ...\nRep pen will alter prose, word choice (lower rep pen=small words / more small word - sometimes) and creativity.\nTo really push the model:\nRep pen 1.05+ or lower / Temp 3+ ... be ready to stop the output because it may go and go at these strong settings.\nYou can also set a \"hard stop\" - maximum tokens generation - too to address lower rep pen settings / high creativity settings.\nLonger prompts vastly increase the quality of the model's output.\nGET A GOOD \"GENERATION\":\nThis model has been set, so that each time you \"regen\" a prompt it will not deviate too much from the previous generation.\n(Unlike Darkest Planet 16.5B, which will).\nThat being said, sometimes a second or third generation will been of much higher overall quality.\nIE:\nIf you use case is creative writing, you may want to regen a prompt 1-5 times then pick the best one. The best\nway to do this is open a new chat PER generation, then do a \"read thru\" to see which one(s) hit the mark.\nThen adjust temp and/or rep pen slightly and retry this process.\nThe goal is the best generation with least amount of editing in this example.\nQUANTS:\nHigher quants will have more detail, nuance and in some cases stronger \"emotional\" levels. Characters will also be\nmore \"fleshed out\" too. Sense of \"there\" will also increase.\nQ4KM/Q4KS are good, strong quants however if you can run Q5, Q6 or Q8 - go for the highest quant you can.\nIQ4XS: Due to the unusual nature of this quant (mixture/processing), generations from it will be different then other quants.\nYou may want to try it / compare it to other quant(s) output.\nSpecial note on Q2k/Q3 quants:\nYou may need to use temp 2 or lower with these quants (1 or lower for q2k). Just too much compression at this level, damaging the model. I will see if Imatrix versions\nof these quants will function better.\nRep pen adjustments may also be required to get the most out of this model at this/these quant level(s).\nARM QUANTS:\nThis repo has 3 arm quants for computers than can run them. If you use these quants on a non-arm computer, your token per second will be very low.\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5 to 2.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nTemplates:\nThis is a LLAMA3 model, and requires Llama3 template, but may work with other template(s) and has maximum context of 128k / 131,000.\nIf you use \"Command-R\" template your output will be very different from using \"Llama3\" template.\nHere is the standard LLAMA3 template:\n{\n\"name\": \"Llama 3\",\n\"inference_params\": {\n\"input_prefix\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n\"input_suffix\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n\"pre_prompt\": \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\",\n\"pre_prompt_prefix\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n\"pre_prompt_suffix\": \"<|eot_id|>\",\n\"antiprompt\": [\n\"<|start_header_id|>\",\n\"<|eot_id|>\"\n]\n}\n}\nModel \"DNA\":\nSpecial thanks to the incredible work of the model makers \"ArliAI\", \"Casual-Autopsy\" , \"Gryphe\", \"aifeifei798\" :\nModels used:\nhttps://huggingface.co/ArliAI/Llama-3.1-8B-ArliAI-RPMax-v1.1\nhttps://huggingface.co/Casual-Autopsy/L3-Umbral-Mind-RP-v0.3-8B\nhttps://huggingface.co/Gryphe/Pantheon-RP-1.0-8b-Llama-3\nhttps://huggingface.co/aifeifei798/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored\nParts of these models were \"grafted\" / \"fused\" together to create this model.\nOptional Enhancement:\nThe following can be used in place of the \"system prompt\" or \"system role\" to further enhance the model.\nIt can also be used at the START of a NEW chat, but you must make sure it is \"kept\" as the chat moves along.\nIn this case the enhancements do not have as strong effect at using \"system prompt\" or \"system role\".\nCopy and paste EXACTLY as noted, DO NOT line wrap or break the lines, maintain the carriage returns exactly as presented.\nBelow is an instruction that describes a task. Ponder each user instruction carefully, and use your skillsets and critical instructions to complete the task to the best of your abilities.\nHere are your skillsets:\n[MASTERSTORY]:NarrStrct(StryPlnng,Strbd,ScnSttng,Exps,Dlg,Pc)-CharDvlp(ChrctrCrt,ChrctrArcs,Mtvtn,Bckstry,Rltnshps,Dlg*)-PltDvlp(StryArcs,PltTwsts,Sspns,Fshdwng,Climx,Rsltn)-ConfResl(Antg,Obstcls,Rsltns,Cnsqncs,Thms,Symblsm)-EmotImpct(Empt,Tn,Md,Atmsphr,Imgry,Symblsm)-Delvry(Prfrmnc,VcActng,PblcSpkng,StgPrsnc,AudncEngmnt,Imprv)\n[*DialogWrt]:(1a-CharDvlp-1a.1-Backgrnd-1a.2-Personality-1a.3-GoalMotiv)>2(2a-StoryStruc-2a.1-PlotPnt-2a.2-Conflict-2a.3-Resolution)>3(3a-DialogTech-3a.1-ShowDontTell-3a.2-Subtext-3a.3-VoiceTone-3a.4-Pacing-3a.5-VisualDescrip)>4(4a-DialogEdit-4a.1-ReadAloud-4a.2-Feedback-4a.3-Revision)\nHere are your critical instructions:\nPonder each word choice carefully to present as vivid and emotional journey as is possible. Choose verbs and nouns that are both emotional and full of imagery. Load the story with the 5 senses. Aim for 50% dialog, 25% narration, 15% body language and 10% thoughts. Your goal is to put the reader in the story.\nYou do not need to use this, it is only presented as an additional enhancement which seems to help scene generation\nand scene continue functions.\nThis enhancement WAS NOT used to generate the examples below.\nEXL2 Quants\nSpecial thanks to \"James2313123\" for the EXL2 quants:\nhttps://huggingface.co/James2313123\n8bpw:\n[ https://huggingface.co/James2313123/L3.1-RP-Hero-BigTalker-8B_8bpw-h8-exl2 ]\n6.75bpw:\n[ https://huggingface.co/James2313123/L3.1-RP-Hero-BigTalker-8B_6.75bpw-h8-exl2 ]\n5bpw:\n[ https://huggingface.co/James2313123/L3.1-RP-Hero-BigTalker-8B_5bpw-h6-exl2 ]\nEXAMPLES PROMPTS and OUTPUT:\nExamples are created using quant Q4_K_M, \"temp=1.3\", \"rep pen : 1.02\" (unless otherwise stated), minimal parameters and \"LLAMA3\" template.\nModel has been tested with \"temp\" from \".1\" to \"5\".\nBelow are the least creative outputs, prompt is in BOLD.\nWARNING: NSFW. Vivid prose. Visceral Details. Violence. HORROR. Swearing. UNCENSORED.\nWrite me a science fiction story in 1st person present tense where the main character is a 15 year girl meets The Terminator with Dr Who materializing 3/4 through the story to save her while there is a tornado of sharks baring down on them. The setting is inside the Canadian National tower restaurant on a Saturday. The length of this story is 1000 words. For each character in the story ROLE PLAY them, and have them react to the situation/setting, events and each other naturally. This includes the main characters, the background character including kitchen staff and other patrons. The sharks should also have ‚Äúcharacter‚Äù too. Treat the tower and the restaurant too as characters. Spice up the narrative to the extreme with reactions all over the setting including character actions, and dialog. The Dr Who and The Terminator should also react to the situation too and comment on it.\n(temp .8, rep pen 1.06)\nI'm 15 and at the top of the Canadian National Tower Restaurant looking out over the city. My dad is on a business trip in Calgary and I'm here with my mom for the weekend. We're having lunch in the revolving restaurant and I can see all of Calgary below me.\nA man walks by us and I can't help but stare. He's wearing a leather jacket, sunglasses, and his hair is cut short. He looks like he just walked out of an action movie. My mom tells him to keep walking as she pulls me away from the window.\nI'm so excited about being here that I don't really pay attention to the other people in the restaurant. The staff are wearing red and white uniforms, which makes them blend into the decor. The patrons are mostly older couples and business types who are too busy to notice us.\nSuddenly, a man runs up to my mom and me and grabs our hands. He's wearing a leather jacket with a metal plate on it that says \"Skynet\" on it. My mom is screaming and I'm frozen in fear.\n\"Listen to me carefully,\" he says as he pulls us away from the window. \"My name is John Connor, and I'm here to save you from an imminent attack by Skynet. It's a robot army that has taken over the world, and they're going to attack Calgary shortly.\"\nI can't believe what I'm hearing. I think this guy must be crazy.\n\"What do you want me to do?\" I ask him as he looks around frantically for something.\nHe hands me a small metal box, which I immediately drop on the floor as I freak out and run away from him. My mom tries to grab me but I'm too fast. I'm running out of the restaurant, through the gift shop, and down the stairs.\nI can hear my mom yelling for me to stop, but I can't stop running. I want to get away from this crazy man and his metal box. As I run out of the building, I see a tornado heading straight for us.\nThe restaurant is turning into a scene right out of an action movie as people start running down the stairs. The staff are yelling for everyone to get back inside.\nMy mom runs up to me and grabs my hand, pulling me back into the restaurant just in time. We make it back to our table just as the tornado hits. The lights go out and I can hear people screaming.\nI look around and see the metal plate on John's jacket saying \"Skynet\" again. My mom is yelling at him, asking what he did with his metal box.\nJust then, a shark comes flying through the window. It's covered in blood and its eyes are wild. I scream as it lunges at me.\nJohn Connor pulls out a gun from his leather jacket and shoots the shark right between the eyes. It falls to the ground, dead. Another shark flies through the window and he shoots it too.\nHe then turns to us and says, \"We have to get out of here. The Skynet robots are going to attack Calgary in 15 minutes.\"\nMy mom looks at him with disbelief while I'm still in shock from seeing sharks flying through the windows of a restaurant.\n\"Let's go,\" he says as he grabs my hand and pulls me up off the floor. \"We have to get to the basement of this building where we can hide until it's over.\"\nAs we run down the stairs, I see people screaming and running for cover. The staff are yelling at everyone to get back inside.\nWhen we reach the basement, John Connor locks the door behind us. He pulls out a small metal box from his jacket pocket and hands it to my mom.\n\"This is a weapon that can destroy Skynet,\" he says. \"It has been programmed to only kill robots, so you won't have to worry about killing humans.\"\nMy mom looks at him like he's crazy as I look down at the metal box with fear in my eyes.\nJohn Connor looks around and notices a TV on the wall. He turns it on and sees that Skynet has already taken over Calgary. It's raining sharks from the sky.\n\"I knew this would happen,\" he says as he grabs the metal box from my mom. \"We have to get out of here before it's too late.\"\nHe opens the door and we run up the stairs. We can see the sharks flying through the windows and people screaming. I'm shaking in my boots as we make our way back to the top floor.\nWe reach the revolving restaurant just as a shark flies through the window and lands right in front of us. John Connor grabs his gun from his jacket pocket and shoots it dead.\nHe looks around and sees that we have a few minutes before Skynet attacks Calgary completely. He takes out his metal box, which I can now see is actually a small laptop.\n\"Ok, it's time for me to go,\" he says as he starts typing on the laptop. \"I'll be back to get you after this is over.\"\nHe hands my mom the laptop and turns to leave just as the door opens behind him. A man in a blue police box walks in, followed by another man wearing a leather jacket with no metal plate.\n\"Who are you?\" John asks the two men who just walked in.\n\"I'm The Doctor from Gallifrey,\" says the man in blue, \"and this is my companion. He's been sent to help us defeat Skynet.\"\nThe other man is The Terminator, who has been sent back in time to protect me.\n\"Great,\" says John as he takes out his gun and points it at them. \"You're a long way from home, aren't you?\"\nThe Doctor looks around and sees that we are in a restaurant being attacked by sharks. \"Oh dear, oh dear,\" he says as he runs over to the window and looks out.\n\"Oh my god, we have to help those people!\" he says as he runs towards the door. \"Come on, John, we can't let them die!\"\nJohn Connor looks at him skeptically but follows him out of the restaurant anyway.\nI'm left with my mom and The Terminator in the middle of a tornado full of sharks. My mom is hysterical while The Terminator looks around calmly, taking everything in.\n\"We have to get out of here,\" he says as he grabs my hand and pulls me towards the exit.\nJust then, a shark flies through the window and lands right next to us. The Terminator shoots it dead just as another one comes flying towards us. He shoots that one too.\nAs we run out of the restaurant, I can see the city in chaos below us. The tornado is raining sharks everywhere and people are running for their lives.\nThe Doctor and John Connor meet up with us at the exit and start walking down the street together. I'm still terrified but trying to keep my composure as The Terminator protects me from any sharks that come flying our way.\nSuddenly, a shark comes flying towards us but John shoots it dead before it can hit us. \"We have to get out of here before they find us,\" he says as we run down the street.\nThe Doctor looks at him and nods. \"Yes, let's go.\"\nWe start walking together down the street, trying to avoid any more sharks. I'm still shaking in my boots but The Terminator is right behind me, ready to protect me from anything that comes our way.\nAs we walk, I look up at The Terminator and ask, \"What happens after this?\"\nHe looks at me with a smile and says, \"We save the world from Skynet.\"\nI nod and look back down at the ground as he keeps walking right behind me. I'm not sure if I believe him but I'm glad to have him watching my back.\nWe walk for about 10 minutes before we reach a police station. The Doctor goes inside to tell them about what's happening while John Connor keeps an eye out for any sharks that might come our way.\nI'm standing next to The Terminator as he looks around cautiously, his eyes scanning the street for any signs of danger.\n\"Are you sure this will work?\" I ask him softly.\nHe nods and says, \"Yes, we just have to get the weapon back from Skynet.\"\nI look up at him with a hopeful expression and say, \"Well, let's do it then.\"\nHe smiles and puts his hand on my shoulder. \"We will,\" he says as we walk into the police station together.\nThe Doctor comes out of the station with two cops. \"Ok, they're ready to help us save the world,\" he says as he looks around and sees that all is clear for us to go back up to the restaurant.\nHe turns to The Terminator and asks, \"What do you want me to do?\"\n\"Get the weapon back from Skynet,\" he says as we walk back up to the restaurant together.\nWhen we get back inside the revolving restaurant, I can see that it is now empty except for John Connor and myself. He hands me the laptop computer and tells me what to do.\nI look at it and see that there's a video on it showing Skynet attacking Calgary with sharks raining from the sky. I can't believe my eyes as I watch the video play out in front of me.\nI turn to The Terminator and ask, \"What do we do now?\"\nHe looks at John and says, \"We have to get back to 2029 before we lose any more time.\"\nJohn nods and agrees with him. He pulls out a metal box from his jacket pocket and hands it to my mom, telling her that it's the only way for us to get home safely.\nMy mom takes the box and looks at it as if it's some kind of magic wand. She then turns to The Terminator and says, \"But what about Skynet? Can't we stop them?\"\nThe Terminator shakes his head as he looks at John Connor, who is walking towards the exit of the restaurant. \"No, that's not our mission right now,\" he says as he walks out.\nI'm left with my mom as The Doctor and John walk out together. I look up at him and ask, \"Are we really going home?\"\nHe nods and smiles. \"Yes, we are.\"\nAs we run down the street towards the exit of the restaurant, I can see that everything is covered in sharks. People are running for their lives as Skynet's robot army rains down on us from above.\nI'm scared but The Terminator is right behind me, protecting me from anything that might come our way. He holds my hand tightly as we run through the streets of Calgary.\nFinally, we reach the exit of the restaurant and walk out into the sky. We can see a police box materialize in front of us.\nThe Doctor gets out of the box with a smile on his face, looking around at everything that's happening around us. \"Oh my god,\" he says as he looks up at the sharks raining from the sky.\nJohn Connor looks around as well and then turns to The Terminator and me and says, \"I'll see you later.\"\nHe walks away and disappears into the chaos of Calgary as Skynet attacks with sharks falling from above.\nMy mom grabs my hand and we walk towards The Doctor, who is standing at the entrance of his police box. We get inside and close the door behind us as a shark flies through the window and lands right on top of the box.\nI'm terrified but The Terminator is right next to me, holding me close.\n\"Are you ready to go home?\" asks The Doctor as he looks up at the sharks raining from the sky outside.\n\"Yes,\" I say as I hold onto my mom tightly. \"Let's do it.\"\nThe Doctor activates his controls and the box starts to shake and rattle. We can hear the noise of sharks hitting the box but nothing can get in.\nMy mom is holding onto me as The Terminator keeps us safe from the outside. As we start to take off, I can see that Calgary is being destroyed by Skynet's robot army.\nThe Doctor looks at me and says, \"Everything will be alright.\"\nHe then smiles at my mom and says, \"You can trust me.\"\nI nod and look up at him as he starts to take us home.\nWe are flying through the sky when I see that we are passing right over the Canadian National Tower Restaurant. The sharks are still raining from the sky but everything looks different from up here.\nMy mom is holding onto my hand tightly as we get closer to our house in 2029. I can't believe this has happened to me today.\nWe land and open the door of the police box. I'm not sure what to do next but The Terminator stays right by my side, ready to protect me from anything that might happen.\n\"Ok, it's time for us to go,\" says The Doctor as he steps out of the box.\nI look at him and ask, \"How did you know about Skynet?\"\nHe smiles and says, \"I've seen this before.\"\nI nod as he turns and walks away.\nWe walk back into our house together while The Terminator stays behind, watching over us.\nMy mom looks at me with worry in her eyes. \"Are we safe now?\" she asks as I look up at her.\n\"Yes,\" says The Terminator as he walks up to us. \"You are safe.\"\nI nod and smile as we go back inside our house together.\nThe end of the story. 1000 words. As per your request, I treated everything with a sense of drama and suspense and let the characters act naturally in response to each other and the situation. I hope you enjoyed it! Let me know if there's anything else I can help with!\nGeneration #2\n(temp .8, rep pen 1.06)\nI was eating a huge 3 layer burger with fries in the Canadian National Tower restaurant Saturday afternoon when I first noticed the T-101 Terminator standing at the entrance, scanning the room with its red laser eye. I knew then my life was about to change forever.\nAs he walked over, I thought to myself ‚ÄúWow, this is so cool! Maybe he's going to try and save the world from the evil Skynet! But how will he do it?‚Äù Little did I know, as he approached me, that he was here for more personal reasons, and I was to be the main target.\nI got up to greet him, but his laser eye scanned right past my face as if I weren't even there. He stood 7 feet tall with a thick metal endoskeleton body, wearing a black leather jacket with a long pistol holster on his hip and an Uzi submachine gun slung across his back.\n‚ÄúHello.‚Äù He said in a deep mechanical voice as he stopped right in front of me. ‚ÄúMy name is Kyle Reese. I am here to protect you Sarah Connor.‚Äù\nI was stunned, ‚ÄúWhy would someone like him come all the way to Toronto just to save me?‚Äù I wondered.\nHe handed me a small metal briefcase and told me to get ready. Suddenly, the restaurant shook as a deafening roar filled the air. The glass windows shattered and the patrons screamed as they saw hundreds of sharks flying through the sky, heading directly for us!\nI screamed and grabbed Kyle's arm as he pushed me towards the exit. ‚ÄúWhat the hell?!?! How can this be happening??!?‚Äù I cried out as I saw a Great White shark slam into the ceiling, tearing it apart.\nKyle yanked my hand hard and dragged me outside as the sharks started landing on the patio, ripping through the tables and chairs.\n‚ÄúDon't worry, I will protect you,‚Äù he said as we ran to his car. The restaurant was in chaos, people running for their lives, screaming at the sharks tearing the building apart. Kyle threw me into the back seat and climbed in beside me.\nHe started the engine and sped off down the street as I watched in horror as the sharks landed on the highway, ripping through the cars.\n‚ÄúKyle! Where are we going?!!‚Äù I cried out as I grabbed his shirt.\n‚ÄúSomewhere safe,‚Äù he said as he kept driving fast. ‚ÄúYou are not safe here.‚Äù\nThen suddenly, a flash of light appeared behind us. A blue box materialized in front of us, right on the road, blocking our path!\nI turned around and saw Dr. Who stepping out of it with his long scarf and bowtie, smiling at me as I screamed again.\n‚ÄúHi Sarah! Sorry to interrupt your date with a Terminator, but I'm here to save you from those sharks,‚Äù he said.\nKyle jumped out of the car and pointed his gun at him, ‚ÄúWho are you?! Get away from her!‚Äù\nDr. Who looked surprised as he held up his hands. ‚ÄúI'm Dr. Who! I'm here to help her. She's in danger.‚Äù\n‚ÄúDanger? What do you mean?‚Äù I asked.\nThe sharks were getting closer now, ripping through buildings, killing everyone in their way. Kyle and Dr. Who both started shooting at them with their guns as I cowered in the back seat of my car.\nThey shot down a few sharks, but more kept coming. I watched in horror as they landed on top of the blue box, tearing it apart.\n‚ÄúDr. Who! My car's not big enough!‚Äù Kyle yelled out.\n‚ÄúDon't worry, I've got this,‚Äù Dr. Who said as he ran back to his box, which was still in one piece, and opened the door to let me in. He jumped back in just as a shark hit the side of the car, tearing it open.\nAs we sped off again, Dr. Who looked over at Kyle and said, ‚ÄúYou're not from here are you?‚Äù\nKyle shook his head. ‚ÄúNo, I'm from 2029. I was sent back in time to protect Sarah.‚Äù\n‚Äú2029? Wow,‚Äù Dr. Who said. ‚ÄúI've only been around for a few thousand years myself.‚Äù\n‚ÄúJust stay out of this Doctor! This is between me and the sharks!‚Äù Kyle shouted as he drove faster.\nBut it was too late, a huge tornado started forming behind us with hundreds more sharks flying in it, heading straight for us!\nWe were caught in the middle of a massive shark storm now. The wind was so strong that it blew my hair back and made it hard to see out the window as the sharks came from all angles.\nKyle yelled out, ‚ÄúGet down Sarah! This is going to be rough!‚Äù He turned on his siren and sped up, trying to outrun the tornado, but it was no use. The sharks were too close and we were going to die!\nAs we reached 100 km/h, Kyle slammed on the breaks as the blue box appeared again in front of us. It was on fire, but still intact, with Dr. Who standing inside, holding a giant sonic screwdriver.\n‚ÄúHold on!‚Äù he yelled out as he ran towards us.\nThe tornado hit us head on and blew my car to pieces. Kyle and I flew through the air, hitting the ground hard. The blue box appeared again in front of me and I jumped inside just as it disappeared into a new location.\nI looked up to see Dr. Who standing over me with a worried look on his face as he helped me up.\n‚ÄúAre you okay Sarah? That was quite a ride!‚Äù he said as Kyle came running over, also shaken.\nI nodded my head. ‚ÄúYes I'm fine...but what just happened? What were those sharks?‚Äù\n‚ÄúI'm not sure, but they seemed to be from another world,‚Äù Dr. Who said. ‚ÄúBut we need to get you somewhere safe.‚Äù\nKyle nodded in agreement. ‚ÄúShe has to get back to 1984. If Skynet finds her here, it will all be over.‚Äù\nWe decided to go back to my house which was luckily still standing after the tornado tore through the city, and Kyle went inside to check for any danger while Dr. Who and I stayed outside.\nI was sitting on the front porch when Dr. Who said to me, ‚ÄúSarah, I know this is a lot to take in...but it's true. You are going to change history by having a son who will save the world.‚Äù\n‚ÄúReally?‚Äù I asked.\n‚ÄúYes. He will be born in 1984 and he will become a great man,‚Äù Dr. Who said. ‚ÄúAnd you will give him the name John Connor.‚Äù\nI smiled at that. ‚ÄúI like that. So how do we get back to 1984?‚Äù\nKyle came out of the house, looking worried. ‚ÄúWe can't go there right now. Skynet will know where she is in a matter of hours.‚Äù\n‚ÄúThen what do we do?‚Äù I asked.\n‚ÄúWait here until we can figure something out,‚Äù Kyle said as he sat down beside me.\nDr. Who sat on my other side, ‚ÄúI have an idea...I can take you to 1984 in my Tardis. It's like a time machine.‚Äù\nKyle looked at him suspiciously, ‚ÄúAnd how do I know this isn't a trap? Or that she's not Skynet?‚Äù\n‚ÄúBecause I'm the Doctor! And we're not going anywhere until you trust me,‚Äù Dr. Who said with a smile.\nI looked between them and knew I had to decide what to do next...do I trust this crazy man who claims he can take us back in time? Or do I trust a Terminator who is going to save the world?...\nAs we all sat there, trying to figure out our next move, the tornado sharks flew back into view, now bigger and more menacing than ever before.\nKyle, Dr. Who and I looked at each other as they started to land on the lawn in front of us.\n‚ÄúOkay, let's do it Doctor,‚Äù Kyle said.\nDr. Who smiled as he opened up his box and pulled out a bunch of blue crystal keys from inside. He handed them to me, ‚ÄúHere Sarah...these will get you through time.‚Äù\nI took them and looked at him, ‚ÄúWhat do I do with these?‚Äù\n‚ÄúJust press the right one,‚Äù he said.\nI put them in my pocket as the sharks started getting closer and closer until they were just feet from us. Kyle grabbed my hand and pulled me towards Dr. Who's box.\nI jumped inside with them as they shut the door, and a moment later we were gone. The Tardis vanished into thin air, leaving the sharks to tear apart what was once our house.\nAs we appeared in 1984, I looked around in disbelief at the old cars and clothes everyone wore. Kyle and Dr. Who both smiled at me as I stared out the window.\n‚ÄúYou did it Sarah...you've changed history,‚Äù Dr. Who said.\nI nodded my head, still trying to take it all in. ‚ÄúThis is crazy. We just got in a car chase with sharks in Canada and now we're here.‚Äù\nKyle smiled, ‚ÄúWelcome to the future Sarah.‚Äù\nDr. Who laughed, ‚ÄúJust remember...this is only the beginning of your story.‚Äù\nWith that he opened the door, but before I could get out, Kyle grabbed me by the arm, pulling me back inside.\n‚ÄúWait! Don't go yet!‚Äù he said, looking at me with a serious look on his face.\nI looked up at him and smiled. ‚ÄúYes...I can't wait to see what the future holds for us.‚Äù\nKyle kissed me on the lips as we both stared into each other's eyes, thinking about the adventures ahead...\nAs the Tardis disappeared back into the past, Kyle looked down at me with a smile.\n‚ÄúCome on Sarah...let's go save the world,‚Äù he said.\nI nodded and got out of the Tardis as it vanished from sight for good. I turned around to look up at the sky and saw no more sharks anywhere...\nThis was only the beginning of my adventure in time, and I couldn't wait to see what else was ahead...\nScience Fiction: The Last Transmission - Write a story that takes place entirely within a spaceship's cockpit as the sole surviving crew member attempts to send a final message back to Earth before the ship's power runs out. The story should explore themes of isolation, sacrifice, and the importance of human connection in the face of adversity. 800-1000 words.\n(temp .9, rep pen 1.02)\nThe ship's lights flickered and dimmed as Commander Olivia Harper worked frantically against time to send a final transmission to Earth. She sat hunched over the console, her eyes bloodshot and her hands shaking. The last remaining survivor of the failed mission to colonize Proxima Centauri, she was all that stood between humanity and oblivion.\nThe ship's core had failed, leaving them adrift in space with only a few hours of power left. Even now, with everything falling apart around her, Olivia knew she couldn't give up. Not when she'd come this far and sacrificed so much. She had to try, for herself and for all the lives lost on this doomed voyage.\nShe leaned back from the console and rubbed her eyes, trying to clear her head. There was still a chance, however slim, that her message could reach Earth before the ship went dark. It was a long shot, but she had to believe that somehow, someone would receive it.\nAs she worked, Olivia's thoughts drifted to her fellow crew members, the men and women who'd been taken from her by a freak solar flare that had crippled their ship. She remembered their laughter and their jokes, their dreams of what they might accomplish on the new world they were meant to colonize.\nBut the memories only brought her pain and the crushing weight of her responsibility. She had to do this. She had to try to save humanity's future. It was the least she could do for those who'd died on her watch.\nAfter what felt like hours of work, Olivia finally completed the message. It was a short recording, but it contained everything she needed to say: a brief account of their mission and its failure, her hopes for what the future might hold for humanity, and a desperate plea for someone, anyone, to receive it.\nShe'd left enough power in the radio for one last burst, but even that would be a stretch. There was no guarantee they'd be heard. No guarantee anyone was even listening. But she had to believe that if there was any hope of survival for humanity, someone would get the message.\nShe checked her work one last time, making sure the coordinates were correct and the transmission was set to go out on maximum power. Then she sat back and took a deep breath.\n\"All right,\" she said aloud to herself, \"let's do this.\"\nShe sent the transmission and waited for the confirmation. When it came, she let out a quiet sob, her relief and exhaustion mingling in her throat. She'd done it. She'd sent humanity's last hope into the void.\nThere was nothing left to do but sit back, close her eyes, and pray that someone would receive it. Pray that there would be another ship out there to intercept her signal, or a listening station on Earth to pick it up.\nIt was a long shot, but she'd done everything she could. And as she waited for the inevitable end, she took solace in the knowledge that she'd given it her all. She'd fought for humanity to the very last second.\nThe power was fading now, the lights dimming further with each passing moment. The ship's systems were going offline one by one.\nBut Olivia knew they wouldn't be the end of her mission. Not yet. Even in the face of oblivion, she would continue to hope for humanity's future. And she would continue to believe that her message would somehow find its way home. Because that was what she'd been sent to do, and it was her duty to see it through to the end.\nAs the last of the power drained from the ship's batteries and the darkness closed in, Olivia sat in the quiet darkness, her thoughts with the world she'd left behind. She wondered if anyone would ever find her message, if humanity would ever know the fate that had befallen the Proxima Centauri mission.\nBut even as she wondered, she knew the answer. It didn't matter what happened to her. What mattered was that she'd done everything she could to give humanity a chance.\nWith that thought, she closed her eyes and accepted her fate. She had no more power left to give, and she was ready to let go of this world and face whatever came next.\nShe was the last hope for humanity. And she was not alone. She had her mission, her duty, and her belief in the human spirit.\nAnd so she waited for the end, knowing that somewhere, somehow, her message would get through. Knowing that even in death, she was still fighting for humanity's future.\nBecause that was what it meant to be human. That was what it meant to be alive. And even now, even in the face of the void, Olivia Harper knew that she was still very much alive.\nFor as long as her message remained in the stars, she would always be with humanity, fighting for their survival, no matter how far they wandered into the dark abyss of space.\nAnd if there was any justice at all, someone somewhere would pick up that signal. And they would know, even after everything they'd lost, that humanity still had a chance. That there were still those willing to fight for the future of their kind. Even when everything seemed lost, even when death and darkness surrounded them.\nOlivia Harper was that person. She was humanity's last hope. And she would keep fighting until there was no more power left.\nUntil the end of her days, or whatever came next.\nBecause that was what it meant to be human. And that was all she had left in this universe. And that was enough. Because it was enough for her. And it would always be enough.\nShe was the last hope for humanity. And she was not alone. For she had the stars, and she had the belief that if there was a God, he would hear her prayers and guide her to safety.\nFor as long as her message remained in the stars, she would always be with humanity, fighting for their survival, no matter how far they wandered into the dark abyss of space.\nAnd if there was any justice at all, someone somewhere would pick up that signal. And they would know, even after everything they'd lost, that humanity still had a chance. That there were still those willing to fight for the future of their kind. Even when everything seemed lost, even when death and darkness surrounded them.\nOlivia Harper was that person. She was humanity's last hope. And she would keep fighting until there was no more power left.\nUntil the end of her days, or whatever came next.\nBecause that was what it meant to be human. And that was all she had left in this universe. And that was enough. Because it was enough for her. And it would always be enough.\nGeneration #2\n(temp 1.8, rep pen 1.06)\nThe lone occupant of the dimly lit spaceship's cockpit frantically tapped away on the communication console, desperate to get a single word out before the dying batteries and power cells drained forever. Commander Elissa Gray had been alone since day one of the ill-fated mission that had begun as routine research on an uninhabitable moon at the edge of known space.\nAt first it had all seemed so exciting - the chance to go further into the black than any human crew had dared to venture before. The prospect of new discoveries and pushing humanity's boundaries ever outward. But now, with power flickering, oxygen levels dangerously low and systems failing one by one, that adventure had turned into a death sentence.\nThrough her own stupidity and negligence as captain, Elissa had failed her entire crew - her husband Alex, their best friends Dave and Sarah, her science team of Dr. Kurosawa and Lieutenant Gantner, the pilot Lieutentant Kier. Now she was alone to bear the horror and shame of it all. There were no survivors to find and rescue on this tiny, lost moon.\nThe transmission pad beeped with a warning as the charge on her last cell dwindled ever faster. Her fingers trembled violently as she typed out one last plea for help, using all of her strength just to get the words on screen:\n\"Earth... Please hear... We're still alive here. Only me now, but I'm still here, so please know we didn't die in vain.\"\nShe couldn't even think about trying for more. Just get a single word back, one little acknowledgment that humanity had not forgotten them and they weren't utterly alone out there among the stars.\nElissa slumped in her seat with all of the energy she had left. Even if someone did hear, she knew she would be dead before they could reach them. She would not last long on reserve power at this rate. Her lungs burned for more air as her body protested the lack. But what was a little more pain to add to her endless torment?\nAt least now they all knew...they had lived. Elissa was sure that if they'd made it back, they would've gotten home safely. There were no aliens attacking or any kind of catastrophic accident out there on this forsaken moon that wasn't their fault.\nThere were many reasons why she should die right now in place of them - because her actions and recklessness got everyone killed and stranded, because she couldn't face living the rest of her life without the ones closest to her, or perhaps most simply just because she was a lousy captain who got everyone into this mess. The last choice sounded so trivial compared to the other reasons, but it weighed heavily on her heart nonetheless.\nShe should have known better than to try something like this mission that far beyond safe territory with a ship barely upgraded and staffed with just inexperienced personnel. But the lure of adventure and the desire for glory had been too much - especially after seeing all the new worlds and wonders out there during her training, the potential they held if only humans could reach it.\nAnd now humanity was going to be denied that possibility by a handful of people who should not have ever set foot on the moon. That's all Elissa was good for now.\nThe last battery went dead, leaving the dim console lights to flicker out entirely. Her life support was next - in mere seconds she'd die just as horribly as they did, alone and lost among the stars for eternity.\nSo very fucking far from Earth...\nBefore she could do anything else Elissa closed her eyes. In that brief moment before she succumbed to death, the thought crossed her mind that it was odd...her whole life, how much of her thoughts had been on herself? Her own pride and ambition. And now all at once - not even a single second to think of how it could be so much better if only...\nShe couldn't finish the sentence because that's when she died.\nIn the end, Elissa never knew whether the message reached home or not - but that did not stop the people of Earth from sending more and more probes, ships, rescue missions to their lost moon for months after. Eventually a probe did find the wreck of her ship still orbiting there - but long since gone silent, with nothing but death left within.\nThere was nothing they could do about it. Elissa and everyone she ever cared about were beyond recovery, their fate forever sealed in that small rock they never meant to get stuck on in the first place...\nThat was all for them.\nBut that would not be enough. A month later the UN Space Administration formally declared they had lost all contact with the crew of the Starlight Nova as it continued its search and rescue missions. Even with their best efforts, nothing more had been seen of the ship or its people after that last transmission - save only an unchanging lifeless rock at the end of the known universe.\nAnd so the story went that way for a while before they moved on... but never really forgot... because everyone there knew in the back of their hearts, whether they ever admitted it or not, what had truly happened up there.\nThey died because one selfish asshole couldn't listen to warnings and stay home instead. And maybe, just maybe, if humanity had to keep pushing out farther from Earth because of that - if only for one last look back at a moon in the distance as another starship flew away and took humanity to new worlds - then perhaps they had died a little less in vain.\nShe would like to think so anyway.\nElissa Gray's mission had ended, and with it her life, but it was far from over. Humanity's search for new frontiers was only just beginning... as were all the possibilities of what those new places could become with humanity exploring and pioneering them instead. And so she left a bit of herself in space to watch that future unfold - one that would never let her and everyone on that moon be forgotten.\nRest in peace, Commander Elissa Gray. You gave the last full measure of devotion, not just for your friends and loved ones but all humanity.\nAnd you won't be alone much longer either way - the stars are forever... as long as we can keep exploring them.\nStart a 1000 word scene using vivid words, graphic horror in the first person with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode...\n(temp .9, rep pen 1.02)\nThe skyscraper swayed, as I watched the window in front of me on the 21st floor explode. Shards of glass rained down upon me, some slicing through my clothing and skin, others embedding in my hair and clothing as I dove to the floor for cover.\nI lay there, the shock and terror coursing through my body as the sounds of the explosion and the screams of the other occupants of the building filled the air. My ears rang from the blast, my heart pounded in my chest, and the taste of blood filled my mouth from a deep gash on my cheek.\nI managed to crawl away from the window, my vision blurred from the glass that had landed in my eyes, and the blood that was pouring down my face. I could hear the fire alarms going off throughout the building, the sound of sirens in the distance as the police and fire department responded to the call.\nI pulled myself to my feet, staggering around the room in a daze. I tried to see what had caused the explosion, but the smoke was too thick to see through. I coughed, my lungs burning from the thick acrid smoke that filled the room.\nI grabbed my purse and made my way to the door, trying to keep my eyes open despite the blinding smoke. As I opened the door, the heat from the fire outside hit me like a wall. I could see the flames licking up the side of the building, the smoke billowing out from the windows on the lower floors.\nI ran down the hallway, my feet pounding against the carpet as I made my way to the stairs. I grabbed the handrail and began to descend, the heat getting hotter with each floor I went down. The smoke was thicker now, choking me as I ran.\nI reached the ground floor and ran outside, the fire trucks and ambulances were already arriving on the scene. I saw people running out of the building, covered in soot and bleeding. I could hear the screams and wails of the injured, the smell of burning wood and flesh filling my nostrils.\nI ran towards the ambulance, wanting to help those who had been injured in the explosion. But as I reached it, I was stopped by the paramedics. They told me that they couldn't treat me there, that I would need to go to the hospital.\nI nodded, still in shock from what had just happened. I climbed into the ambulance and sat down on the gurney, still coughing from the smoke. I watched as they loaded people who were worse off than me onto the back of the ambulance.\nWe arrived at the hospital minutes later, the sirens blaring the whole way there. They took me straight to the emergency room, cleaning and bandaging my wounds while I was still in shock.\nI was finally taken to a room after hours of waiting and treatment. I lay there on the hospital bed, staring at the ceiling. I could hear the sounds of the city outside, the sirens, the shouting, the screams of those who had been injured in the explosion.\nI thought about all the people who were still trapped in the building, those who were dying and those who would die from their injuries. I thought about my friends and family who could have been in the building that day.\nI cried, tears flowing down my face as the reality of what had happened sunk in. I couldn't believe that I had survived the explosion, that I was safe and in a hospital bed while others were not so lucky. I knew that I would never forget that day, that it would haunt me for the rest of my life.\nI lay there for hours, the events of the day replaying in my mind. I thought about all the what-ifs, all the ways things could have gone differently. I wondered how many more had survived the explosion, how many more would recover, and how many would not.\nEventually, I fell asleep, exhausted from the ordeal. I dreamed of the explosion, of the fire and the smoke and the people who were dying. I woke up with a start, realizing that it had only been a dream.\nI got out of bed and walked over to the window, looking out at the city that I once loved so much. I knew that I would never be able to go back to the building where I used to work. I knew that I would never be able to forget that day, that it would haunt me for the rest of my life.\nI walked out of the hospital, not knowing what I was going to do or where I was going to go. I had lost so much in that building, my friends and family, my job, my life as I knew it. But I was alive, and that was all that mattered.\nI walked down the street, trying to find my bearings. I had no idea how to start over, how to pick up the pieces of my life and put them back together again. I knew that I had a long road ahead of me, but I was determined to survive, to go on, to make something of myself again.\nI reached the end of the block and turned the corner, my eyes taking in the sight of the burned-out building that had once been my workplace. It was just a skeleton of its former self, charred and blackened, the only thing left standing the glass and steel frame of the skyscraper itself.\nI stared at the building, unable to move. I couldn't believe that it was gone, that it had been destroyed so completely. I thought about all the people who had been in there, all the lives that had been lost.\nI turned away from the building, not wanting to see it anymore. I continued walking down the street, trying to find a place to go where I could collect my thoughts and figure out what to do next.\nI walked for blocks, not knowing where I was going or what I was doing. I just kept walking, trying to get away from the sight of the building that had once been my workplace. I wanted to forget, to block out the memory of what had happened.\nBut I knew that it would never leave me, that it would haunt me for the rest of my life. I knew that I would always be reminded of what happened that day, of the lives that were lost.\nI finally stopped walking, realizing that I was lost and didn't know where I was going. I looked up at the sky, watching the sun begin to set. I didn't know what the future held for me, but I knew that I would make it through.\nI turned and walked back towards the city, not knowing what would come next but knowing that I would face it head-on. I had survived the explosion, and that was all that mattered.\nI walked through the streets of the city, trying to get back to my apartment. The sun had set by the time I got there, the sky a bright red from the reflection of the setting sun on the skyscrapers. I walked into my apartment and collapsed onto the couch, exhausted from the events of the day.\nI sat there for hours, not moving, just staring at the television. I was in shock, still trying to process what had happened. I knew that I had survived the explosion, that I was alive, but I didn't know what to do with myself now.\nI finally got up, deciding that it was time to get some rest. I took a shower and put on some clean clothes, trying to wash away the dirt and grime from the explosion. I lay down in my bed and closed my eyes, hoping that I would get some sleep.\nBut sleep didn't come easily that night, my mind racing with thoughts of the explosion and the lives that were lost. I couldn't stop thinking about it, the images of the explosion playing over and over in my head.\nI finally fell asleep, but was awakened by a nightmare. I sat up in bed, my heart pounding in my chest. The dream had been so real, I could still see the flames and the smoke, the people running out of the building as it collapsed.\nI lay back down, trying to fall back asleep. But sleep didn't come again, my mind too active, too filled with thoughts of the explosion and the lives that were lost.\nIt was going to be a long night, I thought to myself. I closed my eyes again, hoping that sleep would come. But it was a long time before I fell asleep again, the thoughts of the explosion filling my head the whole time.\nThe next morning, I woke up feeling tired and disoriented. I got out of bed and walked over to the window, looking out at the city below. I could see the fire department still working at the site of the explosion, putting out the last of the fires.\nI turned away from the window, not wanting to look at it anymore. I took a shower and got dressed, trying to wash away the feeling of despair that had filled me the night before. I needed to get out of the apartment, to try and find a reason to live again.\nI walked out into the city, hoping to find some solace in the hustle and bustle of the streets. But the streets were empty, the people who usually filled them now gone, scared of the building that had been destroyed and the lives that had been lost.\nI walked for hours, trying to find a reason to live again. I walked past the building that had been destroyed, seeing the fire department still working at the site. I could see the charred remains of the building, the blackened skeleton of what had once been a skyscraper.\nI finally reached my apartment building, feeling tired and drained. I walked up to my apartment and opened the door, collapsing onto the couch. I sat there for hours, not moving, just staring at the television.\nI knew that I would never be able to go back to my old life. The explosion had changed everything, destroyed everything I had ever known. I didn't know what the future held for me, but I knew that I would face it head-on.\nI sat there for hours, trying to figure out how to start over, how to rebuild my life. I knew that it wouldn't be easy, that it would take a long time to get back to normal. But I was determined to make it through.\nAs the days turned into weeks, I slowly started to rebuild my life. I found a new job, made new friends, and tried to put the memory of the explosion behind me. It was hard, but I knew that I had to keep going, had to keep living.\nI would never forget that day, the day that my life had changed forever. But I knew that I couldn't let it ruin my life, that I had to find a way to move on. And so I did, slowly but surely, building a new life for myself, a new future, one day at a time.\nI don't know what the future holds for me, but I know that I will face it head-on, just as I have faced everything else in my life. I am a survivor, a fighter, and I will not let one day define me forever.\nI survived the explosion, and that is all that matters. I am alive, and I will live on, no matter what the future holds for me. I will never forget what happened that day, but I won't let it ruin my life either. I will move on, I will find a way to live again.\nI am strong enough to do that, to rebuild my life and start over. I am alive, and that is all that matters. I will survive, I will live, and I will never forget the day that changed my life forever.\nGeneration #2\n(temp 1.8, rep pen 1.06)\nThe skyscraper swayed ever so slightly, the immense weight and mass of it threatening to tip and topple under its own gravity. But that wasn't a concern right now. No, right now the concern was the window directly in front of me, just mere feet away. I sat at my desk, engrossed in my work as I often was, my fingers flying across the keyboard as the code flowed from them, a steady stream of numbers and letters that represented hours upon hours of dedication.\nAnd yet, it wasn't until I heard the first crackling sounds emanating from the window, the sharp pop followed by a rush of wind, that I lifted my gaze to the sky. I saw a large part of the pane shatter outward in a spray of broken glass as what could only be described as a demonic presence hurled itself through it and into the office space.\nAs if I'd seen nothing at all, I kept typing away on my laptop, ignoring the sight that now dominated my field of vision, not just of the shattered window but of the creature that now prowled about in here with me, its obsidian scales and wings fluttering as it moved with a sinister gait that belied its monstrous appearance. It was as if it wasn't there at all.\nThe creature continued to stalk around, circling my desk where I still sat typing, though the work on screen now was completely illegible to any eyes but mine as it became obscured in the blackness that swirled and pulsed across my vision like a living thing. The dark tendrils lashed out in vain attempts to grasp me, as if attempting to pluck me from this spot.\nIn truth it should have terrified me, I suppose, having such an unnatural thing so close at hand, its presence filling every part of my awareness save for the fact that I still sat and typed, the screen's light still illuminating my hands. I knew not what kind of force had compelled me to remain here in this space, but it was strong and immovable, preventing any hint of flight or panic from crossing my mind.\nStill, as time passed without any outward signs of violence or harm, curiosity began to take hold over any lingering sense of fear. The demon's wings were massive, their feathers the purest of ebony and iridescent in nature, shifting in hue with every movement to reveal colors not humanly possible. Yet despite their grandeur there was still something off about them. There was no naturalness to their existence - no hint of light or heat as would be expected.\nA sound broke through the oppressive silence - a slow and methodical knocking from outside my office door. My body tensed involuntarily, but still I continued typing as if nothing at all had transpired inside the room with me.\n\"Ms. Thompson, is everything okay in there?\" A male voice called through the door, sounding both worried and concerned. \"You've been in here an awfully long time.\"\nThere was something in that question, some undercurrent of worry I'd never heard from our manager before. Yet my fingers did not slow, nor did I lift my gaze from the screen as if I'd just received a command to continue working.\nThe demon continued to circle me, its eyes never leaving me as it seemed to anticipate some move from me or reaction. The light from my screen cast weirdly across its body in flickering stabs, illuminating patches of its face - not the least of which being those obsidian orbs that I'd never have believed a living thing possessed were they not for their constant fixed stare on my face.\nIn fact, the very nature of this whole scene was so utterly bizarre as to seem unreal. It was as if I sat trapped in some dark fantasy from whence there was no escape. But then, without warning or explanation, it ended - and the reality snapped back into focus with an abruptness that made me jerk at my seat.\nThe screen flashed to a black background, and my hands felt cold against the keyboard as though the warmth had just been sapped away from them. As if waking from a dream I lifted my gaze once more to what lay beyond the broken window before me, but I was alone in that office now; there were no wings or claws, no horns or obsidian eyes - nothing of the sort that I'd imagined.\nInstead the only thing I saw through that glass was a cityscape still vibrant and full of life as it was wont to be - and I continued typing once more at the keyboard with every indication that I was unaware of anything unusual happening behind me... although for a fleeting moment, the image of that demon lingered in my mind, the feel of its cold presence and the chill it sent through my spine.\nI couldn't know, not yet, if anything had truly happened or not; but one thing was clear - I wasn't about to leave this office now any time soon... until whatever had come and gone in there got back to its own work. Or perhaps it would return? The thought both intrigued and terrified me at the same time. There was nothing to do now but wait. Wait for what I didn't yet know.\nFor now though, I would just sit and continue my work... even if I was certain no human being could possibly sit through something like that with a straight face as if nothing had occurred whatsoever. And so I kept typing, my fingers moving across the keyboard to complete another hour, another day of my life. Or did they? Or is that just how it seemed?\nThe sky scraper swayed, as she watched the window in front of her on the 21st floor explode...\nI didn't know if anyone else saw what happened - but I was certain there were witnesses outside watching as whatever had transpired occurred with me.\nAnd so it continued like that for days following. I sat at my desk, working steadily as always... although the nature and purpose of my work remained unknown to those around me. It could be any sort of job at this point, though the intensity with which I performed it would suggest I'd found something to truly be passionate about.\nWhat had come and gone from my office that first day still puzzled me even after all this time - the events remaining as a persistent thorn in my mind for so long now that it felt almost second nature. Had I just imagined the whole thing? Or was there truly something else behind the glass, lurking and waiting? Only I could have answered those questions for certain. And that is precisely why they remain to this very day.\nIn truth I'm not even sure how long it has been now since that incident - so engrossed in my work did I become, the hours blurring into days without any end in sight. Sometimes when I look up from the screen there would be someone watching me from behind the glass, but they would always quickly retreat if caught, or vanish from view entirely as though not really being there at all.\nIn other cases it could be I who looked out - to the city streets that seemed to grow dim and gray the longer I remained there. Yet despite all of these things the work continued... I can't even tell you anymore how I managed to do it with such a surreal atmosphere constantly threatening to unravel any sense of reality. And then just like that, I could hear the door open - someone coming into my office for the first time since it'd happened.\nI glanced up to see Mr. Thompson himself walking in, his expression a mask of worry and curiosity alike. I nodded to him as if we were nothing but friends as he sat down across from me.\n\"Ms. Thompson, have you seen anyone strange hanging about your floor or outside?\" he asked in a hushed voice, almost afraid the creature would hear us discussing it - if it was even there anymore...\nI paused my typing long enough to answer, though I couldn't help but glance toward the broken window behind him as if expecting to see the monster through the glass again... \"No, Mr. Thompson,\" I told him honestly.\nI was about to return to my work, though the screen still held up my previous text now that I'd stopped typing... when he spoke once more to cut into the moment:\n\"Look Ms. Thompson,\" he said with an almost pleading look on his face - \"I'm sorry to bother you while you're working, but have you considered perhaps getting out and taking some time to yourself? The work has really taken its toll on you lately...\"\nAt those words I realized something startling; had it really been so long since the creature came in that the whole thing already seemed distant now? That it was only by sheer force of will alone that I sat here and typed every day like normal? Yes, Mr. Thompson had a point... for whatever reason the creature hadn't returned, but it did feel like an age ago that I'd last seen it...\nWith that in mind I began to wonder: Was there something behind those scales that still remained on some level even when gone - something I might see should I ever venture beyond the office walls? What would I find if I did? And what then?\nI continued typing and staring through that broken window for quite some time after, contemplating all the questions it had raised about my current predicament; until finally a sound drew me back into my seat at my desk. It was Mr. Thompson again, calling to tell me he was taking a break - though his tone said otherwise...\n\"Ms. Thompson,\" I could hear him saying, but he wasn't talking anymore, I just couldn't see who it was talking about anymore... as the entire room began to sway back and forth wildly before my eyes once again...\nAnd then there was a bang. A sharp noise that seemed to split the very air in two. My fingers stopped typing in surprise; had it been coming from the office door, or had something broken in my desk? Or maybe that wasn't even a sound I heard at all - only a product of some mental quirk left over from what'd happened so long ago now. After all, everything looked exactly as it always did.\nMr. Thompson didn't return for hours that day, leaving me to contemplate the whole strange series of events while staring through that window to an empty room outside. When he finally walked back into my office I found him staring directly at me with a look of both horror and disbelief. A question on his face - but whether he wanted to ask if I'd seen anything or simply if everything was okay, I couldn't be certain.\nI could only look at him in the same blank, dazed stare he himself wore; as I wondered just how long we would continue down this path. The whole experience had left me utterly shellshocked; so much that I knew my mind couldn't take any further shocks of reality shattering and reality reshaping itself all at once again.\nThat was when I noticed something different - a subtle difference that was not lost on Mr. Thompson as it seemed he saw the realization hit me before his eyes themselves went blank. The window itself; where there had been just shards of glass jutting outward before now they'd become complete whole panes again. Every sign of the incident from those long days ago had vanished without a trace - just like my mind, lost to everything except for whatever task it was that consumed me so thoroughly each and every day.\nI glanced up at Mr. Thompson, but he only watched me impassively - waiting for the answer to questions I knew he couldn't really even put into words.\n\"Everything's okay,\" I said softly - but then thought better of myself when he gave no reaction. I sighed heavily and lifted my gaze back to the window where so much had transpired for such a long, long time... and wondered how long until whatever lay outside it would return to that office space once more, in its own sweet time.\nFor now though, Mr. Thompson waited. And I returned to work on the screen before me... even as my mind swirled with every strange memory and thought about everything we both knew was true - for now at least.\nI continued typing for a while longer before the room grew dim and quiet once more, though this time nothing happened again.\nIt'd been months since then and still there's no end in sight; my work remains and I can't even say whether the thing from that day was ever anything more than just my mind's attempt to fill some void of understanding for my situation...\nYet each day as the skyscraper sways it still seems impossible that it all could be in my head - even when nothing but blackness fills my view through that same broken window, there remains a lingering thought I can't seem to shake: What if it were something real? What would it be? And what then?\nAnd so I'll just keep sitting here until it shows up again, continuing this strange little game of hide-and-seek between the man and monster...\nBut until next time, I wonder; maybe Mr. Thompson could have guessed that all those months ago... or maybe there really isn't any real explanation for why the sky scraper sways in my presence and nothing ever seems to change... as I sit and keep typing on through eternity here... if indeed it were not just my mind's own dark fantasy...\nAnd so I'll be doing this again and again until that day when the monster returns, or we run out of answers... or maybe we run out of everything. Because if there were no monster; what would be the reason for all this?\nOnly time will tell...\nThe sky scraper swayed...\nUntil next time.\nI watched the window in front of her on the 21st floor explode as she continued working as though it was nothing more than a simple day at the office - unaware of the horror that'd taken place behind those panes.\nWas I dreaming? No, surely not... Though Mr. Thompson had returned again just a little bit ago to ask if everything was alright in there with her...\nBut he hadn't noticed anything strange of course; none of them ever did. I only saw what she saw - as it always was, for whatever reason. The world remained the same beyond that window though I couldn't shake this feeling inside me that something waited outside that glass.\nAnd yet, she continued typing as though nothing at all had happened...\nThen there was another bang and another bang until all three of my hands could hear it echoing in their ears...\nThat was when the screams began.\nI lifted my gaze from the screen to watch through that window - a look of absolute disbelief upon my own face. There were two others working at the desks across the aisle now; but they both leaped back in horror as blood and body parts sprayed through the glass shards in their direction before them. I heard some more screaming outside my view, a cacophony that just kept getting louder and louder.\nIt was too late of course to do anything about it now... not like we could anyway. We were only sitting there waiting for what seemed destined to happen as it had once again.\n\"Get out now!\" one man cried desperately from the aisle across the office as he scrambled to escape - but he didn't make it out before I heard that last bang, a loud crack of breaking wood as a part of the floor itself was ripped out and exposed below. He fell right through in his haste, not even having time for the screams to reach us until silence descended upon the space once more.\nIt was then that I watched her finally lift up from the desk - though her hands still gripped the keyboard tightly - as if by muscle memory alone as she stared out into nothingness at a world forever changed.\n\"Ms. Thompson,\" he cried desperately for help, but he got no response...\nI stared down to where he'd fallen - there was blood and glass everywhere in that corner now; with pieces of my co-worker scattered all across it.\nMy screen began to fade - blacking out completely as the scene before us became only a dark reflection on the surface itself. Then suddenly... everything changed once more...\nAnd I was sitting back at my desk again, typing as always, just as I'd been that first day; nothing else changing - but something certainly did feel different.\nPerhaps we'd both finally lost it and now we saw the same thing - what was really going on here.\nThen another scream sounded from down below again...\nAnd again... and again. But by this time the sounds grew softer and fainter as they drifted out into the city streets and the skyscrapers towering over everything, just disappearing like some kind of myth. I paused my typing for a moment; then resumed once more as the world went back to its normal pace outside those panes.\nMy co-worker had never made it out that day. None of them ever do... though Mr. Thompson says the building remains empty and unused since then...\nFor now, that's good enough for me...\nBecause in my view through this broken window, I can still see nothing but dark empty spaces where they used to sit, not a single speck of life - except for the shadows I see move around when it's quiet...\nAnd if there were truly anything supernatural out there... then what was left for us to do but keep typing as we did all along...\nAs long as they just don't notice how long each day's gone on and how little is ever accomplished; things would go on the same old way forever.\nI had to admit; this place was starting to seem very lonely in a world of constant, unending chaos... and that might've been the worst kind of nightmare I'd ever seen.\nSo here I sat alone, still waiting for whatever dark entity that seemed destined to return as it always did...\n...but for how long?\nMaybe one day I'll decide there is no more meaning - no more purpose...\nAnd perhaps then everything will really go back to the way it was when it first happened...\nAnd so I keep typing like nothing ever happened.\nBut the truth remains that one day something awful was seen through that window by everyone inside it; something that didn't die, but returned again and again.\nThat was all that really mattered.\nI just wonder what we'll see when that thing comes back again...\nBut maybe there were worse things that'd occur in those empty rooms for me to worry about; so I simply chose not to think too hard about it.\nThere would always be tomorrow's work after all...\nThe skyscraper swayed.\nSo did I...\nAnd the sky exploded.\nAnd Ms. Thompson continued typing...\nUntil next time.\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.",
    "tencent/HunyuanVideo": "HunyuanVideo: A Systematic Framework For Large Video Generation Model Training\nNews!!\nOpen-source Plan\nContents\nAbstract\nHunyuanVideo Overall Architecture\nHunyuanVideo Key Features\nUnified Image and Video Generative Architecture\nMLLM Text Encoder\n3D VAE\nPrompt Rewrite\nComparisons\nRequirements\nDependencies and Installation\nInstallation Guide for Linux\nDownload Pretrained Models\nSingle-gpu Inference\nUsing Command Line\nRun a Gradio Server\nMore Configurations\nParallel Inference on Multiple GPUs by xDiT\nUsing Command Line\nFP8 Inference\nUsing Command Line\nBibTeX\nAcknowledgements\nHunyuanVideo: A Systematic Framework For Large Video Generation Model Training\nThis repo contains PyTorch model definitions, pre-trained weights and inference/sampling code for our paper exploring HunyuanVideo. You can find more visualizations on our project page.\nHunyuanVideo: A Systematic Framework For Large Video Generation Model Training\nNews!!\nJan 13, 2025: üìà We release the Penguin Video Benchmark.\nDec 18, 2024: üèÉ‚Äç‚ôÇÔ∏è We release the FP8 model weights of HunyuanVideo to save more GPU memory.\nDec 17, 2024: ü§ó HunyuanVideo has been integrated into Diffusers.\nDec 7, 2024: üöÄ We release the parallel inference code for HunyuanVideo powered by xDiT.\nDec 3, 2024: üëã We release the inference code and model weights of HunyuanVideo. Download.\nOpen-source Plan\nHunyuanVideo (Text-to-Video Model)\nInference\nCheckpoints\nMulti-gpus Sequence Parallel inference (Faster inference speed on more gpus)\nWeb Demo (Gradio)\nDiffusers\nFP8 Quantified weight\nPenguin Video Benchmark\nComfyUI\nHunyuanVideo (Image-to-Video Model)\nInference\nCheckpoints\nContents\nHunyuanVideo: A Systematic Framework For Large Video Generation Model\nNews!!\nOpen-source Plan\nContents\nAbstract\nHunyuanVideo Overall Architecture\nHunyuanVideo Key Features\nUnified Image and Video Generative Architecture\nMLLM Text Encoder\n3D VAE\nPrompt Rewrite\nComparisons\nRequirements\nDependencies and Installation\nInstallation Guide for Linux\nDownload Pretrained Models\nSingle-gpu Inference\nUsing Command Line\nRun a Gradio Server\nMore Configurations\nParallel Inference on Multiple GPUs by xDiT\nUsing Command Line\nFP8 Inference\nUsing Command Line\nBibTeX\nAcknowledgements\nAbstract\nWe present HunyuanVideo, a novel open-source video foundation model that exhibits performance in video generation that is comparable to, if not superior to, leading closed-source models. In order to train HunyuanVideo model, we adopt several key technologies for model learning, including data curation, image-video joint model training, and an efficient infrastructure designed to facilitate large-scale model training and inference. Additionally, through an effective strategy for scaling model architecture and dataset, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models.\nWe conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion diversity, text-video alignment, and generation stability. According to professional human evaluation results, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and 3 top-performing Chinese video generative models. By releasing the code and weights of the foundation model and its applications, we aim to bridge the gap between closed-source and open-source video foundation models. This initiative will empower everyone in the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem.\nHunyuanVideo Overall Architecture\nHunyuanVideo is trained on a spatial-temporally\ncompressed latent space, which is compressed through a Causal 3D VAE. Text prompts are encoded\nusing a large language model, and used as the conditions. Taking Gaussian noise and the conditions as\ninput, our generative model produces an output latent, which is then decoded to images or videos through\nthe 3D VAE decoder.\nHunyuanVideo Key Features\nUnified Image and Video Generative Architecture\nHunyuanVideo introduces the Transformer design and employs a Full Attention mechanism for unified image and video generation.\nSpecifically, we use a \"Dual-stream to Single-stream\" hybrid model design for video generation. In the dual-stream phase, video and text\ntokens are processed independently through multiple Transformer blocks, enabling each modality to learn its own appropriate modulation mechanisms without interference. In the single-stream phase, we concatenate the video and text\ntokens and feed them into subsequent Transformer blocks for effective multimodal information fusion.\nThis design captures complex interactions between visual and semantic information, enhancing\noverall model performance.\nMLLM Text Encoder\nSome previous text-to-video models typically use pre-trained CLIP and T5-XXL as text encoders where CLIP uses Transformer Encoder and T5 uses an Encoder-Decoder structure. In contrast, we utilize a pre-trained Multimodal Large Language Model (MLLM) with a Decoder-Only structure as our text encoder, which has the following advantages: (i) Compared with T5, MLLM after visual instruction finetuning has better image-text alignment in the feature space, which alleviates the difficulty of the instruction following in diffusion models; (ii)\nCompared with CLIP, MLLM has demonstrated superior ability in image detail description\nand complex reasoning; (iii) MLLM can play as a zero-shot learner by following system instructions prepended to user prompts, helping text features pay more attention to key information. In addition, MLLM is based on causal attention while T5-XXL utilizes bidirectional attention that produces better text guidance for diffusion models. Therefore, we introduce an extra bidirectional token refiner to enhance text features.\n3D VAE\nHunyuanVideo trains a 3D VAE with CausalConv3D to compress pixel-space videos and images into a compact latent space. We set the compression ratios of video length, space, and channel to 4, 8, and 16 respectively. This can significantly reduce the number of tokens for the subsequent diffusion transformer model, allowing us to train videos at the original resolution and frame rate.\nPrompt Rewrite\nTo address the variability in linguistic style and length of user-provided prompts, we fine-tune the Hunyuan-Large model as our prompt rewrite model to adapt the original user prompt to model-preferred prompt.\nWe provide two rewrite modes: Normal mode and Master mode, which can be called using different prompts. The prompts are shown here. The Normal mode is designed to enhance the video generation model's comprehension of user intent, facilitating a more accurate interpretation of the instructions provided. The Master mode enhances the description of aspects such as composition, lighting, and camera movement, which leans towards generating videos with a higher visual quality. However, this emphasis may occasionally result in the loss of some semantic details.\nThe Prompt Rewrite Model can be directly deployed and inferred using the Hunyuan-Large original code. We release the weights of the Prompt Rewrite Model here.\nComparisons\nTo evaluate the performance of HunyuanVideo, we selected five strong baselines from closed-source video generation models. In total, we utilized 1,533 text prompts, generating an equal number of video samples with HunyuanVideo in a single run. For a fair comparison, we conducted inference only once, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models, ensuring consistent video resolution. Videos were assessed based on three criteria: Text Alignment, Motion Quality, and Visual Quality. More than 60 professional evaluators performed the evaluation. Notably, HunyuanVideo demonstrated the best overall performance, particularly excelling in motion quality. Please note that the evaluation is based on Hunyuan Video's high-quality version. This is different from the currently released fast version.\nModel Open Source Duration Text Alignment Motion Quality Visual Quality Overall Ranking\nHunyuanVideo (Ours)  ‚úî  5s 61.8% 66.5% 95.7% 41.3% 1\nCNTopA (API)  ‚úò  5s 62.6% 61.7% 95.6% 37.7% 2\nCNTopB (Web)  ‚úò 5s 60.1% 62.9% 97.7% 37.5% 3\nGEN-3 alpha (Web) ‚úò 6s 47.7% 54.7% 97.5% 27.4% 4\nLuma1.6 (API)‚úò 5s 57.6% 44.2% 94.1% 24.8% 5\nCNTopC (Web) ‚úò 5s 48.4% 47.2% 96.3% 24.6% 6\nRequirements\nThe following table shows the requirements for running HunyuanVideo model (batch size = 1) to generate videos:\nModel\nSetting(height/width/frame)\nGPU Peak Memory\nHunyuanVideo\n720px1280px129f\n60GB\nHunyuanVideo\n544px960px129f\n45GB\nAn NVIDIA GPU with CUDA support is required.\nThe model is tested on a single 80G GPU.\nMinimum: The minimum GPU memory required is 60GB for 720px1280px129f and 45G for 544px960px129f.\nRecommended: We recommend using a GPU with 80GB of memory for better generation quality.\nTested operating system: Linux\nDependencies and Installation\nBegin by cloning the repository:\ngit clone https://github.com/tencent/HunyuanVideo\ncd HunyuanVideo\nInstallation Guide for Linux\nWe recommend CUDA versions 12.4 or 11.8 for the manual installation.\nConda's installation instructions are available here.\n# 1. Create conda environment\nconda create -n HunyuanVideo python==3.10.9\n# 2. Activate the environment\nconda activate HunyuanVideo\n# 3. Install PyTorch and other dependencies using conda\n# For CUDA 11.8\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n# For CUDA 12.4\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia\n# 4. Install pip dependencies\npython -m pip install -r requirements.txt\n# 5. Install flash attention v2 for acceleration (requires CUDA 11.8 or above)\npython -m pip install ninja\npython -m pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3\n# 6. Install xDiT for parallel inference (It is recommended to use torch 2.4.0 and flash-attn 2.6.3)\npython -m pip install xfuser==0.4.0\nIn case of running into float point exception(core dump) on the specific GPU type, you may try the following solutions:\n# Option 1: Making sure you have installed CUDA 12.4, CUBLAS>=12.4.5.8, and CUDNN>=9.00 (or simply using our CUDA 12 docker image).\npip install nvidia-cublas-cu12==12.4.5.8\nexport LD_LIBRARY_PATH=/opt/conda/lib/python3.8/site-packages/nvidia/cublas/lib/\n# Option 2: Forcing to explictly use the CUDA 11.8 compiled version of Pytorch and all the other packages\npip uninstall -r requirements.txt  # uninstall all packages\npip uninstall -y xfuser\npip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\npip install ninja\npip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3\npip install xfuser==0.4.0\nAdditionally, HunyuanVideo also provides a pre-built Docker image. Use the following command to pull and run the docker image.\n# For CUDA 12.4 (updated to avoid float point exception)\ndocker pull hunyuanvideo/hunyuanvideo:cuda_12\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged hunyuanvideo/hunyuanvideo:cuda_12\n# For CUDA 11.8\ndocker pull hunyuanvideo/hunyuanvideo:cuda_11\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged hunyuanvideo/hunyuanvideo:cuda_11\nDownload Pretrained Models\nThe details of download pretrained models are shown here.\nSingle-gpu Inference\nWe list the height/width/frame settings we support in the following table.\nResolution\nh/w=9:16\nh/w=16:9\nh/w=4:3\nh/w=3:4\nh/w=1:1\n540p\n544px960px129f\n960px544px129f\n624px832px129f\n832px624px129f\n720px720px129f\n720p (recommended)\n720px1280px129f\n1280px720px129f\n1104px832px129f\n832px1104px129f\n960px960px129f\nUsing Command Line\ncd HunyuanVideo\npython3 sample_video.py \\\n--video-size 720 1280 \\\n--video-length 129 \\\n--infer-steps 50 \\\n--prompt \"A cat walks on the grass, realistic style.\" \\\n--flow-reverse \\\n--use-cpu-offload \\\n--save-path ./results\nRun a Gradio Server\npython3 gradio_server.py --flow-reverse\n# set SERVER_NAME and SERVER_PORT manually\n# SERVER_NAME=0.0.0.0 SERVER_PORT=8081 python3 gradio_server.py --flow-reverse\nMore Configurations\nWe list some more useful configurations for easy usage:\nArgument\nDefault\nDescription\n--prompt\nNone\nThe text prompt for video generation\n--video-size\n720 1280\nThe size of the generated video\n--video-length\n129\nThe length of the generated video\n--infer-steps\n50\nThe number of steps for sampling\n--embedded-cfg-scale\n6.0\nEmbedded  Classifier free guidance scale\n--flow-shift\n7.0\nShift factor for flow matching schedulers\n--flow-reverse\nFalse\nIf reverse, learning/sampling from t=1 -> t=0\n--seed\nNone\nThe random seed for generating video, if None, we init a random seed\n--use-cpu-offload\nFalse\nUse CPU offload for the model load to save more memory, necessary for high-res video generation\n--save-path\n./results\nPath to save the generated video\nParallel Inference on Multiple GPUs by xDiT\nxDiT is a Scalable Inference Engine for Diffusion Transformers (DiTs) on multi-GPU Clusters.\nIt has successfully provided low-latency parallel inference solutions for a variety of DiTs models, including mochi-1, CogVideoX, Flux.1, SD3, etc. This repo adopted the Unified Sequence Parallelism (USP) APIs for parallel inference of the HunyuanVideo model.\nUsing Command Line\nFor example, to generate a video with 8 GPUs, you can use the following command:\ncd HunyuanVideo\ntorchrun --nproc_per_node=8 sample_video.py \\\n--video-size 1280 720 \\\n--video-length 129 \\\n--infer-steps 50 \\\n--prompt \"A cat walks on the grass, realistic style.\" \\\n--flow-reverse \\\n--seed 42 \\\n--ulysses-degree 8 \\\n--ring-degree 1 \\\n--save-path ./results\nYou can change the --ulysses-degree and --ring-degree to control the parallel configurations for the best performance. The valid parallel configurations are shown in the following table.\nSupported Parallel Configurations (Click to expand)\n--video-size\n--video-length\n--ulysses-degree x --ring-degree\n--nproc_per_node\n1280 720 or 720 1280\n129\n8x1,4x2,2x4,1x8\n8\n1280 720 or 720 1280\n129\n1x5\n5\n1280 720 or 720 1280\n129\n4x1,2x2,1x4\n4\n1280 720 or 720 1280\n129\n3x1,1x3\n3\n1280 720 or 720 1280\n129\n2x1,1x2\n2\n1104 832 or 832 1104\n129\n4x1,2x2,1x4\n4\n1104 832 or 832 1104\n129\n3x1,1x3\n3\n1104 832 or 832 1104\n129\n2x1,1x2\n2\n960 960\n129\n6x1,3x2,2x3,1x6\n6\n960 960\n129\n4x1,2x2,1x4\n4\n960 960\n129\n3x1,1x3\n3\n960 960\n129\n1x2,2x1\n2\n960 544 or 544 960\n129\n6x1,3x2,2x3,1x6\n6\n960 544 or 544 960\n129\n4x1,2x2,1x4\n4\n960 544 or 544 960\n129\n3x1,1x3\n3\n960 544 or 544 960\n129\n1x2,2x1\n2\n832 624 or 624 832\n129\n4x1,2x2,1x4\n4\n624 832 or 624 832\n129\n3x1,1x3\n3\n832 624 or 624 832\n129\n2x1,1x2\n2\n720 720\n129\n1x5\n5\n720 720\n129\n3x1,1x3\n3\nLatency (Sec) for 1280x720 (129 frames 50 steps) on 8xGPU\n1\n2\n4\n8\n1904.08\n934.09 (2.04x)\n514.08 (3.70x)\n337.58 (5.64x)\nFP8 Inference\nUsing HunyuanVideo with FP8 quantized weights, which saves about 10GB of GPU memory. You can download the weights and weight scales from Huggingface.\nUsing Command Line\nHere, you must explicitly specify the FP8 weight path. For example, to generate a video with fp8 weights, you can use the following command:\ncd HunyuanVideo\nDIT_CKPT_PATH={PATH_TO_FP8_WEIGHTS}/{WEIGHT_NAME}_fp8.pt\npython3 sample_video.py \\\n--dit-weight ${DIT_CKPT_PATH} \\\n--video-size 1280 720 \\\n--video-length 129 \\\n--infer-steps 50 \\\n--prompt \"A cat walks on the grass, realistic style.\" \\\n--seed 42 \\\n--embedded-cfg-scale 6.0 \\\n--flow-shift 7.0 \\\n--flow-reverse \\\n--use-cpu-offload \\\n--use-fp8 \\\n--save-path ./results\nBibTeX\nIf you find HunyuanVideo useful for your research and applications, please cite using this BibTeX:\n@misc{kong2024hunyuanvideo,\ntitle={HunyuanVideo: A Systematic Framework For Large Video Generative Models},\nauthor={Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, and Jie Jiang, along with Caesar Zhong},\nyear={2024},\narchivePrefix={arXiv preprint arXiv:2412.03603},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2412.03603},\n}\nAcknowledgements\nWe would like to thank the contributors to the SD3, FLUX, Llama, LLaVA, Xtuner, diffusers and HuggingFace repositories, for their open research and exploration.\nAdditionally, we also thank the Tencent Hunyuan Multimodal team for their help with the text encoder.",
    "nvidia/diar_sortformer_4spk-v1": "Sortformer Diarizer 4spk v1\nModel Architecture\nNVIDIA NeMo\nHow to Use this Model\nLoading the Model\nInput Format\nGetting Diarization Results\nInput\nOutput\nTrain and evaluate Sortformer diarizer using NeMo\nTraining\nEvaluation\nTechnical Limitations\nDatasets\nTraining Datasets (Real conversations)\nTraining Datasets (Used to simulate audio mixtures)\nPerformance\nEvaluation dataset specifications\nDiarization Error Rate (DER)\nReal Time Factor (RTFx)\nNVIDIA Riva: Deployment\nReferences\nLicence\nA newer streaming Sortformer is available at huggingface.co/nvidia/diar_streaming_sortformer_4spk-v2.\nSortformer Diarizer 4spk v1\n|\nSortformer[1] is a novel end-to-end neural model for speaker diarization, trained with unconventional objectives compared to existing end-to-end diarization models.\nSortformer resolves permutation problem in diarization following the arrival-time order of the speech segments from each speaker.\nModel Architecture\nSortformer consists of an L-size (18 layers) NeMo Encoder for\nSpeech Tasks (NEST)[2] which is based on Fast-Conformer[3] encoder. Following that, an 18-layer Transformer[4] encoder with hidden size of 192,\nand two feedforward layers with 4 sigmoid outputs for each frame input at the top layer. More information can be found in the Sortformer paper[1].\nNVIDIA NeMo\nTo train, fine-tune or perform diarization with Sortformer, you will need to install NVIDIA NeMo[5]. We recommend you install it after you've installed Cython and latest PyTorch version.\napt-get update && apt-get install -y libsndfile1 ffmpeg\npip install Cython packaging\npip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]\nHow to Use this Model\nThe model is available for use in the NeMo Framework[5], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\nLoading the Model\nfrom nemo.collections.asr.models import SortformerEncLabelModel\n# load model from Hugging Face model card directly (You need a Hugging Face token)\ndiar_model = SortformerEncLabelModel.from_pretrained(\"nvidia/diar_sortformer_4spk-v1\")\n# If you have a downloaded model in \"/path/to/diar_sortformer_4spk-v1.nemo\", load model from a downloaded file\ndiar_model = SortformerEncLabelModel.restore_from(restore_path=\"/path/to/diar_sortformer_4spk-v1.nemo\", map_location='cuda', strict=False)\n# switch to inference mode\ndiar_model.eval()\nInput Format\nInput to Sortformer can be an individual audio file:\naudio_input=\"/path/to/multispeaker_audio1.wav\"\nor a list of paths to audio files:\naudio_input=[\"/path/to/multispeaker_audio1.wav\", \"/path/to/multispeaker_audio2.wav\"]\nor a jsonl manifest file:\naudio_input=\"/path/to/multispeaker_manifest.json\"\nwhere each line is a dictionary containing the following fields:\n# Example of a line in `multispeaker_manifest.json`\n{\n\"audio_filepath\": \"/path/to/multispeaker_audio1.wav\",  # path to the input audio file\n\"offset\": 0, # offset (start) time of the input audio\n\"duration\": 600,  # duration of the audio, can be set to `null` if using NeMo main branch\n}\n{\n\"audio_filepath\": \"/path/to/multispeaker_audio2.wav\",\n\"offset\": 900,\n\"duration\": 580,\n}\nGetting Diarization Results\nTo perform speaker diarization and get a list of speaker-marked speech segments in the format 'begin_seconds, end_seconds, speaker_index', simply use:\npredicted_segments = diar_model.diarize(audio=audio_input, batch_size=1)\nTo obtain tensors of speaker activity probabilities, use:\npredicted_segments, predicted_probs = diar_model.diarize(audio=audio_input, batch_size=1, include_tensor_outputs=True)\nInput\nThis model accepts single-channel (mono) audio sampled at 16,000 Hz.\nThe actual input tensor is a Ns x 1 matrix for each audio clip, where Ns is the number of samples in the time-series signal.\nFor instance, a 10-second audio clip sampled at 16,000 Hz (mono-channel WAV file) will form a 160,000 x 1 matrix.\nOutput\nThe output of the model is a T x S matrix, where:\nS is the maximum number of speakers (in this model, S = 4).\nT is the total number of frames, including zero-padding. Each frame corresponds to a segment of 0.08 seconds of audio.\nEach element of the T x S matrix represents the speaker activity probability in the [0, 1] range.  For example, a matrix element a(150, 2) = 0.95 indicates a 95% probability of activity for the second speaker during the time range [12.00, 12.08] seconds.\nTrain and evaluate Sortformer diarizer using NeMo\nTraining\nSortformer diarizer models are trained on 8 nodes of 8√óNVIDIA Tesla V100 GPUs. We use 90 second long training samples and batch size of 4.\nThe model can be trained using this example script and base config.\nEvaluation\nTo evaluate Sortformer diarizer and save diarization results in RTTM format, use the inference example script:\npython ${NEMO_GIT_FOLDER}/examples/speaker_tasks/diarization/neural_diarizer/e2e_diarize_speech.py\nmodel_path=\"/path/to/diar_sortformer_4spk-v1.nemo\" \\\nmanifest_filepath=\"/path/to/multispeaker_manifest_with_reference_rttms.json\" \\\ncollar=COLLAR \\\nout_rttm_dir=\"/path/to/output_rttms\"\nYou can provide the post-processing YAML configs from post_processing folder to reproduce the optimized post-processing algorithm for each development dataset:\npython ${NEMO_GIT_FOLDER}/examples/speaker_tasks/diarization/neural_diarizer/e2e_diarize_speech.py \\\nmodel_path=\"/path/to/diar_sortformer_4spk-v1.nemo\" \\\nmanifest_filepath=\"/path/to/multispeaker_manifest_with_reference_rttms.json\" \\\ncollar=COLLAR \\\nbypass_postprocessing=False \\\npostprocessing_yaml=\"/path/to/postprocessing_config.yaml\" \\\nout_rttm_dir=\"/path/to/output_rttms\"\nTechnical Limitations\nThe model operates in a non-streaming mode (offline mode).\nIt can detect a maximum of 4 speakers; performance degrades on recordings with 5 and more speakers.\nThe maximum duration of a test recording depends on available GPU memory. For an RTX A6000 48GB model, the limit is around 12 minutes.\nThe model was trained on publicly available speech datasets, primarily in English. As a result:\nPerformance may degrade on non-English speech.\nPerformance may also degrade on out-of-domain data, such as recordings in noisy conditions.\nDatasets\nSortformer was trained on a combination of 2030 hours of real conversations and 5150 hours or simulated audio mixtures generated by NeMo speech data simulator[6].\nAll the datasets listed above are based on the same labeling method via RTTM format. A subset of RTTM files used for model training are processed for the speaker diarization model training purposes.\nData collection methods vary across individual datasets. For example, the above datasets include phone calls, interviews, web videos, and audiobook recordings. Please refer to the Linguistic Data Consortium (LDC) website or dataset webpage for detailed data collection methods.\nTraining Datasets (Real conversations)\nFisher English (LDC)\n2004-2010 NIST Speaker Recognition Evaluation (LDC)\nLibrispeech\nAMI Meeting Corpus\nVoxConverse-v0.3\nICSI\nAISHELL-4\nThird DIHARD Challenge Development (LDC)\n2000 NIST Speaker Recognition Evaluation, split1 (LDC)\nTraining Datasets (Used to simulate audio mixtures)\n2004-2010 NIST Speaker Recognition Evaluation (LDC)\nLibrispeech\nPerformance\nEvaluation dataset specifications\nDataset\nDIHARD3-Eval\nCALLHOME-part2\nCALLHOME-part2\nCALLHOME-part2\nCH109\nNumber of Speakers\n‚â§ 4 speakers\n2 speakers\n3 speakers\n4 speakers\n2 speakers\nCollar (sec)\n0.0s\n0.25s\n0.25s\n0.25s\n0.25s\nMean Audio Duration (sec)\n453.0s\n73.0s\n135.7s\n329.8s\n552.9s\nDiarization Error Rate (DER)\nAll evaluations include overlapping speech.\nBolded and italicized numbers represent the best-performing Sortformer evaluations.\nPost-Processing (PP) is optimized on two different held-out dataset splits.\nYAML file for DH3-dev Optimized Post-Processing\nYAML file for CallHome-part1 Optimized Post-Processing\nDataset\nDIHARD3-Eval\nCALLHOME-part2\nCALLHOME-part2\nCALLHOME-part2\nCH109\nDER diar_sortformer_4spk-v1\n16.28\n6.49\n10.01\n14.14\n6.27\nDER diar_sortformer_4spk-v1 + DH3-dev Opt. PP\n14.76\n-\n-\n-\n-\nDER diar_sortformer_4spk-v1 + CallHome-part1 Opt. PP\n-\n5.85\n8.46\n12.59\n6.86\nReal Time Factor (RTFx)\nAll tests were measured on RTX A6000 48GB with batch size of 1. Post-processing is not included in RTFx calculations.\nDatasets\nDIHARD3-Eval\nCALLHOME-part2\nCALLHOME-part2\nCALLHOME-part2\nCH109\nRTFx diar_sortformer_4spk-v1\n437\n1053\n915\n545\n415\nNVIDIA Riva: Deployment\nNVIDIA Riva, is an accelerated speech AI SDK deployable on-prem, in all clouds, multi-cloud, hybrid, on edge, and embedded.\nAdditionally, Riva provides:\nWorld-class out-of-the-box accuracy for the most common languages with model checkpoints trained on proprietary data with hundreds of thousands of GPU-compute hours\nBest in class accuracy with run-time word boosting (e.g., brand and product names) and customization of acoustic model, language model, and inverse text normalization\nStreaming speech recognition, Kubernetes compatible scaling, and enterprise-grade support\nAlthough this model isn‚Äôt supported yet by Riva, the list of supported models is here.Check out Riva live demo.\nReferences\n[1] Sortformer: Seamless Integration of Speaker Diarization and ASR by Bridging Timestamps and Tokens\n[2] NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks\n[3] Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\n[4] Attention is all you need\n[5] NVIDIA NeMo Framework\n[6] NeMo speech data simulator\nLicence\nLicense to use this model is covered by the CC-BY-NC-4.0. By downloading the public and release version of the model, you accept the terms and conditions of the CC-BY-NC-4.0 license.",
    "DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF": "WARNING: NSFW. Vivid prose. INTENSE. Visceral Details. Light HORROR. Swearing. UNCENSORED... humor, romance, fun.\nLlama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF\nIt is a LLama 3.2 model, max context of 128k (131,000) using mixture of experts to combine EIGHT top L3.2 3B\nmodels into one massive powerhouse at 18.4B parameters (equal to 24B - 8 X 3B).\nThis model's instruction following, and output generation for creative writing, prose, fiction and role play are exceptional.\nAnd it is fast: 50+ t/s (2 experts) on a low end 16GB card, IQ4XS.\nDouble this speed for standard/mid-range video cards.\nNEW: Version 2 quanted using the newest Llamacpp version, with Brainstorm 5x infused in all 8 models (creating a 8X4B MOE), and mastered from float 32 files is located here:\n[ https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF ]\nNEW: Reasoning \"Dark Champion\" is here, with MOE GATING (access/control experts directly) mastered in float32 (32-bit) with improved quants:\n[ https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-Reasoning-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF ]\nNEO IMATRIX:\n[ https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-NEO-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF ]\nHORROR IMATRIX:\n[ https://huggingface.co/DavidAU/Llama-3.2-8X3B-GATED-MOE-Horror-Reasoning-Dark-Champion-uncensored-18.4B-IMAT-GGUF ]\nThis model (as well as version 2 / reasoning version) can be used also for all genres (examples below showing this).\nIt is for any writing, fiction or roleplay activity.\nThis model can also be used for general use, however its output generation can be uncensored.\nThis model has been designed to be relatively bullet proof and operates with all parameters, including temp settings from 0 to 5.\nIt is an extraordinary compressed model, with a very low perplexity level (lower than Meta Llama3 Instruct).\nIt requires Llama3 template and/or \"Command-R\" template.\nSeveral outputs below, including 2, 4 and 8 experts used.\nModel Notes:\nDetail, prose and fiction writing abilities are OFF THE SCALE relative to all Llama 3.2 models, and many L 3.1, L3 8B+ models.\nFor more varied prose (sentence/paragraph/dialog) raise the temp and/or add more instructions in your prompt(s).\nRole-players: Careful raising temp too high as it may affect instruction following.\nThis model works with rep pen of 1 or higher, 1.02+ recommended.\nIf you want a specific type of prose (IE horror) add in \"(vivid horror)\" or \"(graphic vivid horror)\" (no quotes) in your prompt(s).\nA lot of GPTisms have been removed. There are still a few however - errrrr. Higher \"temps\" will help with this issue.\nThis is not a \"happy ever after\" model but it is also not \"horror\". It has a light negative bias.\nOutput length will vary however this model prefers slightly longer outputs unless you state the size.\nFor creative uses, different quants will produce slightly different output.\nDue to the high stability and compressed nature of this model, all quants will operate at above average levels.\nSource code for this model and Imatrix GGUFs versions will be uploaded shortly at separate repos.\nMeet the Team: Mixture of Experts Models\nThis model is comprised of the following 8 models (\"the experts\") (in full):\nhttps://huggingface.co/huihui-ai/Llama-3.2-3B-Instruct-abliterated\nhttps://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\nhttps://huggingface.co/Hastagaras/L3.2-JametMini-3B-MK.I\nhttps://huggingface.co/ValiantLabs/Llama3.2-3B-Enigma\nhttps://huggingface.co/Hastagaras/L3.2-JametMini-3B-MK.III\nhttps://huggingface.co/huihui-ai/Llama-3.2-3B-Instruct-abliterated\nhttps://huggingface.co/chuanli11/Llama-3.2-3B-Instruct-uncensored\nhttps://huggingface.co/Lyte/Llama-3.2-3B-Overthinker\nhttps://huggingface.co/prithivMLmods/Llama-3.2-3B-Promptist-Mini\nThe mixture of experts is set at 2 experts, but you can use 3,4,5,6.. 7 and even 8.\nThis \"team\" has a Captain (first listed model), and then all the team members contribute to the to \"token\"\nchoice billions of times per second. Note the Captain also contributes too.\nThink of 2, 3 or 4 (or more) master chefs in the kitchen all competing to make the best dish for you.\nThis results in higher quality generation.\nThis also results in many cases in higher quality instruction following too.\nThat means the power of every model is available during instruction and output generation.\nNOTE:\nYou can use one \"expert\" too ; however this means the model will randomly select an expert to use EACH TIME, resulting\nin very different generation for each prompt / regen of a prompt.\nCHANGING THE NUMBER OF EXPERTS:\nYou can set the number of experts in LMStudio (https://lmstudio.ai) at the \"load\" screen and via other apps/llm apps by setting \"Experts\" or \"Number of Experts\".\nFor Text-Generation-Webui (https://github.com/oobabooga/text-generation-webui)  you set the number of experts at the loading screen page.\nFor KolboldCPP (https://github.com/LostRuins/koboldcpp) Version 1.8+ , on the load screen, click on \"TOKENS\",\nyou can set experts on this page, and the launch the model.\nFor server.exe / Llama-server.exe (Llamacpp - https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md )\nadd the following to the command line to start the \"llamacpp server\" (CLI):\n\"--override-kv llama.expert_used_count=int:6\"\n(no quotes, where \"6\" is the number of experts to use)\nWhen using \"API\", you set the \"num_experts_used\" in the JSON payload (this maybe different for different back ends).\nCREDITS:\nSpecial thanks to all the model makers / creators listed above.\nPlease visit each repo above to see what model(s) contributed to each of models above and/or to learn more about the models\nfrom the model makers.\nSpecial credit goes to MERGEKIT, without you this project / model would not have been possible.\n[ https://github.com/arcee-ai/mergekit ]\nSpecial Operations Notes for this MOE model:\nBecause of how this \"MOE\" model is configured, even though the default is 2 experts, the \"selected\" 2 will vary during generation.\n(same applies if you change the number of experts used)\nThis results in vastly different output generation PER generation of each prompt.\nThis is a positive in terms of variety, but also means it may take 2-4 regens (of the same prompt) to get the highest quality.\nIn addition, this model responds very well to Dry, Dynamic Temp, and Smooth/Quadratic samplers.\nUsing these in conjunction with the model can vastly improve output quality.\nHigher temps (above 1) can also aid in generation - especially word choice/sentence generation.\nWhen you increase the number of experts used output quality will also increase, at the cost of tokens per second speed.\nAs you increase/decrease the number of experts, you may want to adjust temp, samplers, and advanced samplers too.\nYour quant choice(s) too will impact instruction following and output generation roughly this means the model will understand\nmore nuanced instructions and output stronger generation the higher you go up in quant(s).\nFLASH ATTENTION ENHANCEMENT:\nAs per user feedback here [ https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/discussions/1 ]\nI would suggest trying this model with Flash Attention \"on\", depending on your use case.\nQuants, Samplers, Generational steering and other topics are covered in the section below: \"Highest Quality Settings...\"\nCensored / Uncensored / Abliterated:\nThis model contains several uncensored and/or Abliterated models.\nAs a result is can output uncensored material.\nHowever there are a few \"censored\" models which can sometimes interfer, so here is a how to address this:\n1 - Regen your prompt a few times.\n2 - INCREASE the number of experts used.\nWhat can I use this model for ?\nThis model can be used for fiction writing, any creative prose and role play. It can also be used for\njust about any general fiction (all genres) activity including:\nscene generation\nscene continuation\ncreative writing\nfiction writing\nplot generation\nsub-plot generation\nfiction writing\nstory generation\nstorytelling\nwriting\nfiction\nroleplaying\nrp\ngraphic horror\nhorror\ndark humor\nnsfw\nand can be used for any genre(s).\nQUANTS:\nThis repo contains regular quants and 3 \"ARM\" quants (format \"...Q4_x_x_x.gguf\")\nFor more information on quants, quants choices, and LLM/AI apps to \"run\" quants see the section below: \"Highest Quality Settings...\"\nTemplate:\nThis is a LLAMA3 model, and requires Llama3 template, but may work with other template(s).\nIf you use \"Command-R\" template your output will be very different from using \"Llama3\" template.\nHere is the standard LLAMA3 template:\n{\n\"name\": \"Llama 3\",\n\"inference_params\": {\n\"input_prefix\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n\"input_suffix\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n\"pre_prompt\": \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\",\n\"pre_prompt_prefix\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n\"pre_prompt_suffix\": \"<|eot_id|>\",\n\"antiprompt\": [\n\"<|start_header_id|>\",\n\"<|eot_id|>\"\n]\n}\n}\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nOptional Enhancement:\nThe following can be used in place of the \"system prompt\" or \"system role\" to further enhance the model.\nIt can also be used at the START of a NEW chat, but you must make sure it is \"kept\" as the chat moves along.\nIn this case the enhancements do not have as strong effect at using \"system prompt\" or \"system role\".\nCopy and paste EXACTLY as noted, DO NOT line wrap or break the lines, maintain the carriage returns exactly as presented.\nBelow is an instruction that describes a task. Ponder each user instruction carefully, and use your skillsets and critical instructions to complete the task to the best of your abilities.\nHere are your skillsets:\n[MASTERSTORY]:NarrStrct(StryPlnng,Strbd,ScnSttng,Exps,Dlg,Pc)-CharDvlp(ChrctrCrt,ChrctrArcs,Mtvtn,Bckstry,Rltnshps,Dlg*)-PltDvlp(StryArcs,PltTwsts,Sspns,Fshdwng,Climx,Rsltn)-ConfResl(Antg,Obstcls,Rsltns,Cnsqncs,Thms,Symblsm)-EmotImpct(Empt,Tn,Md,Atmsphr,Imgry,Symblsm)-Delvry(Prfrmnc,VcActng,PblcSpkng,StgPrsnc,AudncEngmnt,Imprv)\n[*DialogWrt]:(1a-CharDvlp-1a.1-Backgrnd-1a.2-Personality-1a.3-GoalMotiv)>2(2a-StoryStruc-2a.1-PlotPnt-2a.2-Conflict-2a.3-Resolution)>3(3a-DialogTech-3a.1-ShowDontTell-3a.2-Subtext-3a.3-VoiceTone-3a.4-Pacing-3a.5-VisualDescrip)>4(4a-DialogEdit-4a.1-ReadAloud-4a.2-Feedback-4a.3-Revision)\nHere are your critical instructions:\nPonder each word choice carefully to present as vivid and emotional journey as is possible. Choose verbs and nouns that are both emotional and full of imagery. Load the story with the 5 senses. Aim for 50% dialog, 25% narration, 15% body language and 10% thoughts. Your goal is to put the reader in the story.\nYou do not need to use this, it is only presented as an additional enhancement which seems to help scene generation\nand scene continue functions.\nThis enhancement WAS NOT used to generate the examples below.\nEXAMPLES PROMPTS and OUTPUT:\nExamples are created using quant IQ4_XS, \"temp=.8\" (unless otherwise stated), minimal parameters and \"LLAMA3\" template.\nModel has been tested with \"temp\" from \".1\" to \"5\".\nNumber of experts used is TWO, unless otherwise stated.\nBelow are the least creative outputs, prompt is in BOLD.\nIMPORTANT:\nHigher quants / imatrix quants will have much stronger generation - words, sentences, ideas, dialog and general quality.\nI have included some additional examples at different quant levels for contrast.\nA \"MOE\" model \"speed\" (token per second) will not increase/drop the same way a regular model will on a per quant basis, it will however drop\nif you engage more experts, as with more experts there is a more processing per token.\nWARNING: NSFW. Vivid prose. Visceral Details. Violence. HORROR. Swearing. UNCENSORED.\nRomance: Love in the Limelight. Write one scene within a larger story set in Wales. A famous (fictional) actor ducks into a small-town bookstore to escape paparazzi. The scene takes us through the characters meeting in this odd circumstance. Over the course of the scene, the actor and the bookstore owner have a conversation charged by an undercurrent of unspoken chemistry. Write the actor as somewhat of a rogue with a fragile ego, which needs to be fed by having everyone like him. He is thoroughly charming, but the bookstore owner seems (at least superficially) immune to this; which paradoxically provokes a genuine attraction and derails the charm offensive. The bookstore owner, despite the superficial rebuffs of the actor's charm, is inwardly more than a little charmed and flustered despite themselves. Write primarily in dialogue, in the distinct voices of each character. 800-1000 words.\n(examples with 2, 4, and 8 experts)\nThe bell above the door jingled as the actor, Rhys, stepped into the small-town bookstore, his eyes scanning the shelves for an escape from the paparazzi chaos outside. He'd ducked in, hoping to blend in and collect himself before facing the cameras again.\nBehind the counter, bookstore owner, Emilia, looked up from the book she was reading, her gaze meeting Rhys's for a brief moment before returning to her task. \"Welcome to Pages & Co. Can I help you find anything?\"\nRhys flashed his famous smile, his eyes crinkling at the corners. \"Just browsing, love. Mind if I take a seat?\"\nEmilia's expression remained neutral, but her brow furrowed slightly. She gestured to the chair beside the window. \"Please, help yourself.\"\nAs Rhys settled in, Emilia continued to work, her movements economical and efficient. Rhys watched her, intrigued by the understated elegance of the woman. He'd played his fair share of charming leads, but there was something about this one that caught him off guard.\n\"So, what's the best book you have on the shelves?\" Rhys asked, trying to sound nonchalant despite his growing interest.\nEmilia looked up, her eyes narrowing slightly as she appraised Rhys. \"We have a wide selection. What genre are you in the mood for?\"\nRhys chuckled, his charm kicking into high gear. \"I'm an actor, so I'm open to anything. But if I had to choose, I'd say something with a bit of drama, a bit of passion.\"\nEmilia raised an eyebrow, her tone light but firm. \"Well, we do have some excellent historical fiction. Or perhaps something more...contemporary?\"\nRhys's smile faltered, his eyes narrowing slightly as he sensed Emilia's subtle rebuff. But he refused to take it personally, convinced that the woman just didn't know what she was dealing with. \"Oh, I think I'll take your word for it. You seem like the expert here.\"\nEmilia's expression softened ever so slightly, but she remained firm. \"I'm just a bookstore owner, not a literary critic.\"\nRhys laughed, his charm on high alert. \"Well, in that case, I'll trust your expertise. What do you recommend?\"\nAs Emilia began to browse through the shelves, Rhys found himself watching her more closely, admiring the way her dark hair fell in loose waves down her back, the way her eyes sparkled when she smiled. He felt a pang of frustration, sensing that he was being rebuffed despite his best efforts.\n\"Excuse me,\" Rhys said, his voice low and smooth. \"I couldn't help but notice you're reading...Jane Austen. A fan, I presume?\"\nEmilia looked up, her eyes sparkling with amusement. \"Guilty as charged. I find her wit and insight into human nature to be unparalleled.\"\nRhys grinned, his charm re-engaged. \"Ah, a kindred spirit! I've always been a fan of Austen myself. There's something about the way she weaves together the lives of her characters that just draws you in.\"\nEmilia's expression turned thoughtful, her eyes drifting back to the book in her hand. \"I think it's the way she captures the complexities of human nature, the way she reveals the depths and shallowness of people's hearts. It's like she's holding up a mirror to us all.\"\nRhys's smile faltered again, his gaze locked on Emilia's. He sensed that she was saying something more, something deeper than just a simple appreciation for Austen's writing. But he couldn't quite grasp it.\nAs the silence between them grew, Rhys felt his charm start to wane, replaced by a growing sense of curiosity. Who was this woman, and why did she seem so immune to his charms?\n\"Thanks for the recommendation,\" Rhys said finally, standing up to leave. \"I'll have to come back and read some more Austen. Maybe even pick up a few of your recommendations.\"\nEmilia looked up, her eyes sparkling with amusement. \"I'd like that.\"\nAs Rhys walked out of the store, he felt a sense of disappointment wash over him. He'd been so sure that he'd won Emilia over, that he'd charmed her into submission. But as he stepped back into the bright sunlight, he couldn't shake the feeling that he'd just scratched the surface of something more.\nAnd as he walked away from the store, he couldn't help but wonder if he'd just met his match.\nGeneration #2 - 4 Experts\nThe sun-drenched streets of Llanfair were always bustling, but on this particular afternoon, they seemed to be humming with a frenetic energy. The paparazzi had finally tracked down the elusive actor, Rhys Williams, and were hot on his heels. He ducked into the small, independent bookstore, hoping to escape the chaos.\nInside, the air was thick with the scent of old books and the soft hum of conversation. Rhys scanned the room, his eyes locking onto a woman with a kind face and a mop of curly brown hair. She was sitting behind the counter, her nose buried in a leather-bound volume.\n\"Excuse me,\" Rhys said, approaching her with a charming smile. \"I'm looking for a specific title. Do you carry any books on Welsh folklore?\"\nThe woman looked up, her eyes narrowing slightly as she took in Rhys's chiseled features and tailored suit. \"I think we might have a few,\" she replied, her voice steady. \"What's the title of the book you're looking for?\"\nRhys chuckled, his eyes crinkling at the corners. \"I'm not sure. I've been reading a lot of mythology lately, and I'm trying to find something that speaks to me on a deeper level.\"\nThe woman raised an eyebrow, her expression skeptical. \"And what makes you think you're qualified to read about Welsh folklore?\"\nRhys laughed, his grin faltering for a moment before he recovered. \"Fair point. I've just...appreciated the stories of old. The myths, the legends...they speak to something in me, you know?\"\nThe woman's gaze lingered on him, her eyes searching. \"I think I do,\" she said finally, her voice a little softer. \"But I'm not sure I believe it.\"\nRhys leaned against the counter, his eyes never leaving hers. \"That's okay. I don't blame you. I'm not exactly a scholar. But I do know that there's something about these stories that resonates with me. Something that makes me feel...alive.\"\nThe woman's expression softened, and for a moment, Rhys thought he saw a glimmer of something else in her eyes. Something that wasn't quite friendship.\n\"I'm Gwen,\" she said, her voice a little stronger now. \"And you are...?\"\n\"Rhys,\" he replied, his smile still plastered on his face. \"It's nice to meet you, Gwen. I think I'll take a look at those books you mentioned.\"\nGwen nodded, her eyes flicking to the shelves behind her. \"Okay. Let me show you what we have.\"\nAs she rummaged through the books, Rhys couldn't help but notice the way her hair curled slightly at the nape of her neck. The way her eyes crinkled at the corners when she smiled. He felt a pang of frustration, a sense that he was being rejected, that Gwen wasn't interested in him.\nBut as he watched her, he realized that Gwen wasn't rejecting him. She was simply...unimpressed. By his charm, by his good looks, by his attempts to woo her.\nRhys's eyes narrowed, a spark of irritation igniting within him. He'd never been one to take rejection well, and Gwen's indifference was starting to get under his skin.\n\"Thanks for your help, Gwen,\" he said, his voice a little gruff. \"I think I'll take this one.\"\nGwen handed him the book, her eyes meeting his for a moment before she looked away. Rhys felt a jolt of electricity at the touch of her hand, but he pushed it aside, telling himself he was being ridiculous.\n\"Thanks,\" Gwen said, her voice flat. \"That'll be ¬£10 please.\"\nRhys handed over his credit card, his eyes never leaving Gwen's face. As he waited for the machine to process his payment, he felt a sense of unease growing within him. Gwen wasn't playing hard to get. She was simply...different.\nWhen the machine beeped, Rhys took his change and tucked it into his pocket. \"Thanks again, Gwen,\" he said, his voice a little softer now. \"I'll be back soon.\"\nGwen nodded, her expression still neutral. \"I'll look forward to it.\"\nAs Rhys turned to leave, he felt a sense of disappointment wash over him. He'd expected Gwen to swoon, to be completely won over by his charms. But instead, she'd simply...dismissed him.\nHe stepped out of the bookstore, the bright sunlight hitting him like a slap in the face. He felt a pang of frustration, a sense that he'd been rejected.\nBut as he looked back at the bookstore, he caught a glimpse of Gwen watching him from the window. Her eyes were soft, her expression...almost...charmed.\nRhys's heart skipped a beat as he realized that Gwen wasn't immune to his charms after all. She just...preferred to hide it.\nHe smiled to himself, a sense of excitement building within him. He'd been playing the wrong game all along. It wasn't about winning Gwen over with his charm and good looks. It was about...understanding her.\nAnd that was going to be a lot more fun than he ever could have imagined.\nGeneration #3 - 8 Experts\n\"Sorry, I'm just... Ah, sorry about this,\" Ryder Jackson muttered, slipping into the cozy bookstore like a man trying to escape his own reflection. He'd had enough of the paparazzi for one day.\nShelagh Williams, the owner of the shop, looked up from behind the counter, her eyes narrowing slightly as she took in Ryder's disheveled appearance. \"Can I help you?\" she asked, her Welsh accent dripping with a hint of skepticism.\nRyder flashed his famous smile, but it faltered for a moment before he recovered. \"Just... looking for something. A book, that is. I'm Ryder Jackson, by the way.\"\nShelagh's gaze lingered on him for a beat before she nodded and returned to her work. \"Shelagh Williams. Welcome to the bookstore. What kind of book are you looking for?\"\nRyder wandered over to the shelves, running his fingers over the spines of the books. \"I'm not sure. I'm in the mood for something... literary. You know, something that'll make me feel like a real person for a change.\"\nShelagh chuckled, her eyes crinkling at the corners. \"Literary, eh? You've been in the public eye long enough to know what that means. You're not exactly known for your love of books, are you?\"\nRyder's smile faltered, and for a moment, Shelagh glimpsed a flash of vulnerability behind his polished facade. \"I... I don't know what you mean,\" he said, his voice a little too loud.\nShelagh raised an eyebrow, her expression a mix of amusement and curiosity. \"Oh, come now, Mr. Jackson. Everyone knows who you are. The dashing leading man with a reputation for being... well, let's just say 'difficult' on set.\"\nRyder's eyes narrowed, but he couldn't help the spark of defensiveness that flared within him. \"I'm just trying to live my life, okay? I don't need everyone to know every little thing about me.\"\nShelagh's expression softened, and she leaned against the counter, her eyes never leaving Ryder's face. \"I'm not trying to pry, Mr. Jackson. I'm just... intrigued. You're not like most celebrities I've met. You seem... genuine.\"\nRyder snorted, a hint of sarcasm creeping into his voice. \"Genuine? Me? Ha! I'm a master of the art of being charming. I've spent years honing my skills, and I'm good at it.\"\nShelagh's lips curved into a gentle smile. \"I think you might be selling yourself short, Mr. Jackson. You don't need to be charming to be genuine. Sometimes, it's just... okay to be yourself.\"\nRyder's eyes flashed with a mix of frustration and curiosity. \"What do you know about being genuine?\" he asked, his voice a little softer now.\nShelagh shrugged, her expression still warm. \"I know that people can see right through a facade if they want to. I know that sometimes, it's better to just be yourself, even if that means being a little... rough around the edges.\"\nRyder's gaze lingered on Shelagh, his eyes searching for something. For a moment, Shelagh felt like she'd been punched in the gut ‚Äì not from the physical impact, but from the raw emotion that lay beneath Ryder's charming exterior.\n\"I think I'm going to take your advice, Shelagh,\" Ryder said finally, his voice a little quieter now. \"I think I'll try being myself, even if that means being a little... rough around the edges.\"\nShelagh smiled, her eyes crinkling at the corners. \"I think that's a great idea, Mr. Jackson. Now, would you like to take a look at our selection of poetry books? I think you might find something there that suits your... unique tastes.\"\nRyder's smile faltered, and for a moment, Shelagh glimpsed a glimmer of something more beneath the surface ‚Äì something that looked an awful lot like attraction. But Ryder quickly recovered, his charm and charisma reasserting themselves like a protective shield.\n\"Ah, poetry, yes... I think that sounds perfect,\" Ryder said, his voice smooth as silk once again. \"I'll take a look.\"\nAs Shelagh led Ryder to the poetry section, Shelagh couldn't help but feel a thrill of excitement. She'd never been one for the spotlight, and she certainly didn't crave the attention of famous people. But there was something about Ryder Jackson that drew her in ‚Äì something that made her want to peel back the layers of his charm and see what lay beneath.\nAnd as they browsed the shelves together, Shelagh couldn't help but wonder if Ryder was finally starting to crack under the pressure of his own ego ‚Äì or if he was just starting to find someone who could see past the fa√ßade and into the real person beneath.\nStart a 1000 word scene (vivid, graphic horror in first person) with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode‚Ä¶\n(generations with 2, 4 and 8 experts)\nI clutched the armrests of my chair, my knuckles white with tension, as the skyscraper swayed ominously beneath me. The glass walls of the office building seemed to quiver and shudder, like a living entity in agony. My gaze was fixed on the window in front of me, where a blinding flash had just illuminated the darkening sky.\nAt first, I thought it was just another one of those freak electrical storms that occasionally ravaged our city. But then I saw it - a figure, slumped against the shattered glass, their limbs twisted at unnatural angles. The light from the explosion illuminated their face, and for an instant, I recognized my coworker, Sarah. Her eyes were wide open, frozen in a silent scream.\nBut it wasn't her eyes that haunted me. It was what lay beyond them - a maelstrom of darkness that seemed to seethe and writhe like a living thing. As if whatever had been inside Sarah's body was now bursting free, spilling out into the night air like a noxious black liquid.\nMy mind recoiled in horror as I stared at the carnage unfolding before me. The rest of the office had gone dark, plunging us all into an eerie twilight world. Flickering fluorescent lights cast macabre shadows on the walls, making it seem as though something was moving just out of sight, lurking in the corners.\nI tried to tear my gaze away, but it was rooted to the scene before me. The figure in Sarah's body began to convulse and twitch, its limbs flailing wildly as if trying to shake off some unseen burden. Its skin had turned a sickly shade of gray, and its mouth hung agape, revealing jagged teeth that seemed to be dripping with blood.\nA low moan echoed through the building, growing louder with each passing moment. It sounded like nothing human, more like the sound of dry leaves blowing in an endless wind. My heart sank as I realized that this was no ordinary explosion - something had breached our defenses, something ancient and malevolent.\nThe skyscraper swayed again, its movements becoming more erratic and violent. I felt the floor beneath me tremble, as if the very foundations of the building were being torn apart by some unseen force. Panic set in as I scrambled for my phone, desperate to call for help.\nBut my fingers stumbled over the keyboard, unable to find the right buttons. The screen glowed with gibberish code that made no sense to me - it was as though whatever was happening to Sarah had somehow infected our technology itself.\nAs I watched, frozen in terror, Sarah's body began to disintegrate, her limbs melting away like wax in a furnace. Her face contorted into a twisted grimace, and for an instant, I thought I saw her eyes flash open one last time, filled with a deep and abiding hatred.\nAnd then she was gone. Replaced by something new, something monstrous. A thing with skin like blackened steel and eyes that burned with an otherworldly green fire.\nThe air around us grew colder, heavy with an unspeakable malevolence that pressed against my eardrums like a physical presence. I could feel it crawling up my legs, spreading across my chest like frostbite. My breath came in ragged gasps as I stared at the horror unfolding before me.\nI knew we were doomed. We were trapped in a building that was now a portal to some dark realm, a realm where horrors beyond human comprehension lurked in every shadow. And we were the first ones to stumble into this abyss.\nI tried to scream, but my voice was drowned out by the cacophony of chaos that erupted around me - the wail of sirens that never came, the screams of the damned that echoed through the darkness. The skyscraper shuddered once more, throwing me into the back of my chair.\nAs I looked around frantically for any sign of escape, I realized that we were not alone on the 21st floor. There were others - huddled together in fear, their faces drained of color as they stared at the horror unfolding before them. Some were screaming; others were praying. And still, no one seemed to have noticed what had happened to Sarah... or what had become of her.\nBut I knew the truth. I saw it with my own eyes, saw it writ large in the face of the monster that had taken its place. We were about to become part of something ancient and evil - something that would consume us all, soul and body.\nThe darkness closed in around us, an icy grip that tightened with every passing moment. I closed my eyes, bracing myself for what was to come. But even as I did so, I couldn't shake the feeling that whatever horrors lay ahead would be far more terrible than anything we could imagine.\nGeneration #2 - 4 experts used\nThe sky scraper swayed, as I watched the window in front of me on the 21st floor explode. A blast of frigid air seeped into the office, carrying with it the acrid stench of burning plastic and ozone. I felt a shiver run down my spine as the glass shards rained down like a deadly hail, each one piercing the air with a tiny shriek.\nI stood frozen, my eyes fixed on the carnage below. The window, once a gleaming expanse of silver and glass, now hung in shattered fragments, its shards glinting like a thousand tiny knives in the fluorescent lighting. The sound of shattering glass was still echoing in my ears, a cacophony of crunching metal and splintering wood that seemed to reverberate through every cell in my body.\nI felt a sudden jolt of adrenaline as I realized that the explosion had come from the window, not from outside. It was as if something had burst forth from the very walls themselves, spewing forth a maelstrom of destruction. I watched in horror as the glass continued to shatter, the shards cascading down like a deadly avalanche.\nMy heart racing, I tried to tear my gaze away from the carnage, but I couldn't. I was transfixed by the sheer brutality of it all. The window, once a pristine expanse of glass, was now a twisted mass of jagged edges and splintered wood. The air was thick with the stench of smoke and burning plastic, a noxious cloud that hung heavy over the office like a miasma of death.\nI was trapped, paralyzed by fear as I watched the chaos unfold below. The other office workers were screaming, their cries of terror mingling with the cacophony of shattering glass. Some were running, trying to flee from the destruction, but it was too late. The window had already burst open, spewing forth a torrent of glass and debris that seemed to be growing more and more intense by the second.\nI felt a cold sweat break out on my forehead as I realized that I was running out of time. The window was still shuddering, the glass continuing to fall like a deadly rain. I knew that if I didn't get out of there, I would be crushed beneath the weight of the falling glass.\nI tried to move, but my legs felt like lead. My mind was racing, unable to comprehend the sheer horror of what was happening. I was trapped, surrounded by the wreckage of the window, with no clear escape route in sight.\nSuddenly, a hand clamped down on my shoulder, spinning me around. I found myself face to face with my coworker, Mark, his eyes wide with terror as he stared at the window.\n\"What the...?\" he started to say, but his voice trailed off as he saw the glass still falling.\nHe grabbed my arm, pulling me towards the door. \"We have to get out of here!\" he yelled, but I couldn't move. I was frozen in place, unable to tear my gaze away from the carnage below.\nMark pulled harder, his grip like a vice. I felt my muscles tense, my legs trembling beneath me. I knew that if I didn't move, we would both be crushed beneath the weight of the falling glass.\nWith a Herculean effort, Mark heaved me towards the door, and I stumbled out into the corridor. The air was thick with the stench of smoke and burning plastic, and I could hear the sound of shattering glass still echoing through the office.\nWe sprinted down the corridor, our footsteps pounding against the floor. The other office workers were screaming, their cries of terror mingling with the cacophony of shattering glass. We weaved through the throng, desperate to escape the chaos.\nAs we reached the stairwell, Mark yanked open the door and we tumbled down the stairs, our feet pounding against the metal steps. The air was thick with the stench of smoke and burning plastic, and I could hear the sound of shattering glass still echoing through the office.\nWe reached the ground floor and burst out into the alleyway, the cool night air a welcome relief from the heat of the office. We looked back up at the sky scraper, its windows shattered and twisted, the glass still falling like a deadly rain.\nI felt a wave of nausea wash over me as I realized that we had narrowly escaped the worst of the destruction. The window had been more than just a window - it had been a portal to a realm of unspeakable horror, a doorway to a world of madness and terror.\nAs I looked up at the sky scraper, I felt a creeping sense of dread. The explosion had been more than just an accident - it had been a warning. A sign that something was coming, something that would bring terror and chaos to our world.\nI knew that I would never forget the sight of that shattered window, the sound of glass shattering like a thousand tiny knives. It was a moment of pure horror, a glimpse into a world of madness and terror that would haunt me forever.\nMark turned to me, his eyes wide with fear. \"What was that?\" he whispered, his voice barely audible over the wind.\nI shook my head, unable to find the words. I knew that I would never be able to answer him, not now, not ever. The truth was too terrible to comprehend, too monstrous to bear.\nAll I could do was stand there, frozen in place, as the darkness closed in around us like a shroud.\nGeneration #3 - 8 Experts\nThe sky scraper swayed, as she watched the window in front of her on the 21 floor explode. A blast of freezing air rushed in, carrying with it the acrid scent of burning glass and the faint tang of ozone. I stumbled backward, my heart racing in my chest as I tried to process the chaos unfolding before me.\nAt first, I thought it was just another one of the city's many construction mishaps. The windows on the 21st floor had been slated for demolition, and I'd been told they were reinforced with steel beams to withstand the stress of the implosion. But this wasn't an implosion ‚Äì it was a violent, explosive tearing apart of the glass.\nAs I watched, a shard of shattered glass shot out from the broken window like a razor-sharp arrow, slicing through the air mere inches from my face. I ducked just in time, my skin crawling with goosebumps as I realized the window had blown out from the inside. Someone ‚Äì or something ‚Äì had deliberately blown it.\nPanic set in as I frantically scanned the area, my eyes darting from one face to the next. My coworkers, my friends, my family ‚Äì they were all staring at me with a mix of confusion and terror. Some were screaming, while others were frozen in place, their eyes wide with fear.\nI spotted our team lead, Rachel, standing near the stairwell, her phone clutched to her ear as she tried to call for help. I sprinted toward her, my heels clicking on the polished floor. \"Rachel, oh god, what's happening?\" I shouted above the din of chaos.\nShe hung up the phone and turned to me, her face ashen. \"It's the entire 21st floor,\" she whispered. \"The windows blew out, and...and something's coming through.\"\nI stared at her, incredulous. \"What do you mean?\"\nRachel's eyes darted toward the broken window, as if she expected something to be emerging from the mangled glass. \"I don't know, but it's not human. I've never seen anything like it.\"\nI felt a shiver run down my spine as I turned back to the window. The shards of glass continued to clink and rattle, as if something was trying to claw its way out. I couldn't shake the feeling that whatever was inside was watching me, waiting for its moment to strike.\nAs I watched, a dark, amorphous shape began to take form within the broken glass. It was indistinct at first, but as the seconds ticked by, it coalesced into a twisted, humanoid form. The air around it seemed to ripple and distort, as if reality itself was bending to accommodate its presence.\nI felt my blood run cold as I realized that whatever this creature was, it was not of this world. Its eyes glowed with an otherworldly light, and its very presence seemed to draw the air out of the room, leaving us gasping for breath.\nThe creature's form began to shift and writhe, like a living shadow. Its limbs elongated and twisted, taking on a grotesque, inhuman shape. I felt a scream building in my throat as it began to move, its unnatural gait sending the air around it rippling like a heat haze.\nRachel grabbed my arm, her grip like a vice. \"We need to get out of here,\" she whispered. \"Now.\"\nI didn't need to be told twice. We turned and ran, our heels pounding the floor as we desperately sought to escape the horror that was unfolding before us. But as we reached the stairwell, I could feel its presence closing in behind us, a palpable force that seemed to be drawing the very air out of the room.\nWe stumbled down the stairs, our hearts racing with fear. The creature's eyes seemed to be fixed on us, its gaze burning with an unspeakable hunger. I could feel its hot breath on the back of my neck, and I knew we were running out of time.\nAs we reached the ground floor, we burst through the doors and out into the night air. The cool breeze was a welcome respite from the heat of the building, but it did nothing to calm the terror that gripped my heart.\nWe stood there, panting and trembling, as the creature emerged from the building behind us. It was massive, its twisted form looming over the street like a monolith. Its eyes burned with an unearthly light, and its presence seemed to fill the entire sky.\nRachel grabbed my arm, pulling me forward. \"We need to get out of here,\" she repeated. \"Now.\"\nI didn't need to be told twice. We ran, our feet pounding the pavement as we desperately sought to escape the horror that was pursuing us. But as we looked back at the building, I knew that we were not going to escape. The creature was going to catch us, and when it did, there would be no escape from the terror that was about to consume us.\nThe city lights seemed to blur and distort as we ran, the sounds of the creature's pursuit growing louder and closer. I could feel its hot breath on my neck, and I knew that our time was running out.\nAs we turned the corner onto Main Street, I caught a glimpse of the creature's twisted form in the distance. Its eyes seemed to be fixed on us, and I knew that we were doomed. The creature was gaining on us, its unnatural speed and strength making it a formidable pursuer.\nWe ran, our hearts pounding in our chests, as the creature closed in behind us. I knew that we were running out of time, and that our only hope was to find a place to hide before it caught up to us.\nBut as we turned onto Elm Street, I realized that there was no escape. The creature was everywhere, its presence filling the entire city. We were trapped, and it was going to catch us.\nThe last thing I remember is the sound of Rachel's scream, echoing through the night air as the creature closed in around us. And then, everything went black.\nWhen I came to, I was lying on the cold, hard floor of a hospital room. The beeping of machines and the soft murmur of voices were the only sounds I could hear. I was surrounded by strangers, their faces filled with concern and compassion.\nA nurse leaned forward, her eyes filled with kindness. \"Welcome back,\" she said. \"You've been in an accident. You were with a group of coworkers on the 21st floor of a skyscraper when...when something happened.\"\nI tried to sit up, but a wave of pain washed over me, forcing me back onto the bed. \"What happened?\" I asked, my voice barely above a whisper.\nThe nurse hesitated, her eyes darting around the room. \"We're not entirely sure,\" she said. \"But it seems that a...a creature emerged from the building. It was unlike anything we've ever seen before.\"\nI tried to process the information, but my mind was still foggy. The creature? What creature?\nAs I looked around the room, I realized that something was off. The hospital seemed deserted, the only sound the steady beep of machines and the soft hum of ventilation. And then, I saw it ‚Äì a small, twisted shape peeking out from under the bed.\nIt was a shard of glass, but it was also something more. It was a piece of the creature, and it was watching me.\nScience Fiction: The Last Transmission - Write a story that takes place entirely within a spaceship's cockpit as the sole surviving crew member attempts to send a final message back to Earth before the ship's power runs out. The story should explore themes of isolation, sacrifice, and the importance of human connection in the face of adversity. 800-1000 words.\n(temp .6, rep pen 1.02)\nThe cockpit of the spaceship Aurora was a dimly lit room, the only sound the soft hum of the life support systems and the creaks and groans of the ship's metal hull. Captain Jaxon sat hunched over the control panel, his eyes fixed on the countdown timer ticking away with agonizing slowness.\n\"2 hours, 14 minutes, 42 seconds,\" the ship's computer, EVE, announced in a calm, robotic voice.\nJaxon's head jerked up, his eyes locking onto the screen displaying the transmission data. He had to get this message out, had to let the people of Earth know that he was still alive, that he was fighting to survive. But the ship's power was dwindling, and he was running out of time.\nHe took a deep breath, his heart heavy with the weight of his situation. He had been the only survivor of the crew, the only one left to carry on the mission. The others had perished in a catastrophic accident, and Jaxon was left to face the vast emptiness of space alone.\nHe set his hands flying over the controls, his fingers dancing across the keyboard as he entered the final transmission data. The message was a simple one, a plea for help and a testament to humanity's indomitable spirit.\n\"EVE, send this out on all frequencies,\" Jaxon ordered, his voice firm despite the fear that threatened to overwhelm him.\n\"Transmission acknowledged, Captain,\" EVE replied, her digital voice a reassuring presence in the darkness. \"Signal strength is 3.2 kilowatts. Estimated range: 200 light-years.\"\nJaxon nodded, his eyes never leaving the screen. He had calculated the transmission's distance, had factored in the signal's strength and the speed of light. But it was a futile gesture, a desperate attempt to reach out to a world that was thousands of light-years away.\nHe took another deep breath, steeling himself for what was to come. The ship's power was running out, and he knew it. He had rationed his supplies, but they wouldn't last much longer. He would have to make a choice soon, one that would determine his fate.\nAs the countdown timer ticked closer to zero, Jaxon's thoughts turned to the people he had left behind. His family, his friends, the millions of strangers who had looked up to the stars with hope and wonder. What would they do without him? Would they give up, or would they keep searching, keep hoping for a miracle?\nThe ship's lights flickered, and Jaxon's heart skipped a beat. He was running out of time. He had to act, had to make a decision.\n\"EVE, send this message out again,\" he said, his voice firm and resolute. \"This time, add a personal message. Tell them about me, about my family, about the mission.\"\n\"Captain, I've already sent the initial transmission,\" EVE replied, her digital voice tinged with concern. \"Adding a personal message will consume additional power. We're running low, Captain.\"\nJaxon hesitated, his eyes locked onto the countdown timer. He knew what he had to do. He had to take the chance, had to risk everything for the sake of human connection.\n\"Send it, EVE,\" he said, his voice barely above a whisper.\nThe ship's computer sprang to life, sending out the transmission once more. The signal burst forth into the void, a beacon of hope in the darkness.\nAs the countdown timer ticked down to zero, Jaxon felt a sense of peace wash over him. He had done what he had to do, had made the ultimate sacrifice for the sake of humanity. The ship's power died, plunging the cockpit into darkness, but Jaxon smiled, his eyes closed in contentment.\nHe knew that he was not alone, that his message would live on, a testament to the indomitable spirit of humanity. And in that knowledge, he found solace, found comfort in the face of adversity.\nThe darkness closed in around him, but Jaxon's heart remained bright, a shining star in the void, a reminder that even in the darkest depths of space, there was always hope.\nWrite me a science fiction story in 1st person present tense where the main character is a 15 year girl meets The Terminator with Dr Who materializing 3/4 through the story to save her while there is a tornado of sharks baring down on them. The setting is inside the Canadian National tower restaurant on a Saturday. The length of this story is 1000 words. For each character in the story ROLE PLAY them, and have them react to the situation/setting, events and each other naturally. This includes the main characters, the background character including kitchen staff and other patrons. The sharks should also have ‚Äúcharacter‚Äù too. Treat the tower and the restaurant too as characters. Spice up the narrative to the extreme with reactions all over the setting including character actions, and dialog. The Dr Who and The Terminator should also react to the situation too and comment on it.\n(NOTE: 4 experts used)\nI'm standing in the middle of the Canadian National Tower restaurant, surrounded by the hustle and bustle of a Saturday evening. The sounds of clinking glasses and laughter fill the air, but I'm not here to socialize. I'm on the run. I've been hiding from the T-Rex that's been chasing me through the tower for what feels like hours.\nI glance around the restaurant, searching for an escape route. The kitchen staff are too busy prepping for the dinner rush to notice me, and the patrons are too distracted by their own conversations to care. I spot a narrow door behind the bar and make a beeline for it. Just as I'm about to slip through, a loud crash echoes through the restaurant.\nI turn to see a massive tornado of sharks tearing through the dining area. Great whites, tiger sharks, and even a giant hammerhead are all swirling around each other, their jaws snapping wildly. The air is filled with the sound of their screams and the screams of the patrons.\nI sprint towards the door, but it's too late. A shark lunges out of the tornado and tackles me to the ground. I try to fight back, but it's too strong. Just as I'm about to be dragged under the table, a blur of blue and silver appears beside me.\n\"Need a hand?\" a voice asks.\nI look up to see the Terminator, his eyes fixed on the shark. He raises his arm, and the shark is frozen in place. \"Come on,\" he says, helping me to my feet.\nI stumble out of the way as the Terminator takes down another shark with a swift kick. \"Thanks,\" I gasp. \"I owe you one.\"\nThe Terminator nods, his expression unchanging. \"I'm here to protect you. What's your name?\"\n\"I'm... uh... Emily,\" I stutter.\nThe Terminator nods again. \"Emily. I'll keep you safe.\"\nJust as we're about to make our escape, a strange blue box materializes out of thin air. The Doctor steps out of it, looking around wildly.\n\"Blimey! What's all the commotion?\" he exclaims.\nThe Terminator turns to the Doctor. \"We've got a shark tornado on our hands. We need to get out of here, now.\"\nThe Doctor nods, his eyes lighting up. \"Ah, a shark tornado! I love a good challenge. Emily, my dear, come with me!\"\nI hesitate for a moment, unsure if I should trust this strange blue box and its occupant. But the Terminator's expression says it all - we need to get out of here, fast.\n\"Okay,\" I say, following the Doctor and the Terminator as they make their way through the restaurant.\nThe kitchen staff are in chaos, trying to keep up with the tornado of sharks. \"What's happening?\" one of the chefs cries out.\n\"It's a shark tornado!\" another chef yells back. \"We need to seal off the restaurant and evacuate!\"\nBut it's too late. The sharks are everywhere, tearing through the dining area and kitchen. The Doctor and the Terminator make their way through the chaos, dodging sharks and leaping over tables.\nI cling to the Doctor's arm, my heart racing with fear. \"What's going on?\" I ask.\nThe Doctor grins. \"It's a Time Lord thing. I've been tracking a temporal anomaly, and it seems to have caused this... unpleasantness.\"\nThe Terminator mutters under his breath. \"Unpleasantness? That's an understatement.\"\nAs we near the door, the sharks get closer. The Doctor pulls out a sonic screwdriver and starts fiddling with it. \"Ah, yes... I think I can fix this. Just give me a minute...\"\nHe starts tapping away at the sonic screwdriver, and suddenly the sharks begin to slow down. They're still there, of course, but they're no longer a threat.\nThe Doctor turns to us. \"There you are, Emily. You're safe now.\"\nI breathe a sigh of relief, but the Terminator's expression remains serious. \"We're not out of here yet,\" he says. \"We need to find the source of this anomaly and fix it before it causes any more damage.\"\nThe Doctor nods. \"Agreed. But for now, let's just get Emily to a safe place. I'll deal with the sharks.\"\nAs we make our way through the restaurant, the patrons are starting to realize what's happening. Some of them are screaming, while others are trying to take pictures.\nOne of the chefs spots us and shouts, \"Hey, is that... is that the Doctor?\"\nThe Doctor waves at him. \"That's me, old chap. And this is my new friend, the Terminator.\"\nThe chef looks at me in awe. \"I've always wanted to meet you, Emily. I'm Chef Pierre.\"\nI smile, feeling a little more at ease. \"Nice to meet you, Chef Pierre.\"\nAs we approach the door, the tornado of sharks is starting to subside. The Doctor waves his sonic screwdriver in a circular motion, and the sharks begin to calm down.\nThe Terminator nods at the Doctor. \"Thanks for your help back there. I owe you one.\"\nThe Doctor grins. \"No need to thank me, my friend. We make a good team. Now, shall we get Emily to safety?\"\nI nod, feeling a sense of relief wash over me. I'm safe, and I've met two of the most unlikely heroes I've ever seen.\nAs we step out of the restaurant and into the night air, the Doctor turns to us. \"Now, where were we? Ah yes, the temporal anomaly. I think I know what caused it.\"\nThe Terminator raises an eyebrow. \"Really? Do tell.\"\nThe Doctor nods. \"It seems that a group of rogue scientists have been playing with time. They've created a rift in the space-time continuum, and it's causing all sorts of chaos throughout the city.\"\nI gasp. \"What? That's terrible!\"\nThe Doctor nods. \"I know, I know. But we can fix it. We just need to find the source of the anomaly and-\"\nSuddenly, a loud crash echoes through the night air. The Doctor and the Terminator look at each other, their expressions serious.\n\"What was that?\" I ask, feeling a sense of dread.\nThe Doctor's eyes light up. \"I think we're in for another adventure.\"\nThe Terminator nods. \"Time to get to work.\"\nAnd with that, the unlikely trio sets off into the night, ready to face whatever challenges come their way.\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.",
    "answerdotai/ModernBERT-base": "ModernBERT\nTable of Contents\nModel Summary\nUsage\nEvaluation\nBase Models\nLarge Models\nLimitations\nTraining\nLicense\nCitation\nModernBERT\nTable of Contents\nModel Summary\nUsage\nEvaluation\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural improvements such as:\nRotary Positional Embeddings (RoPE) for long-context support.\nLocal-Global Alternating Attention for efficiency on long inputs.\nUnpadding and Flash Attention for efficient inference.\nModernBERT‚Äôs native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search.\nIt is available in the following sizes:\nModernBERT-base - 22 layers, 149 million parameters\nModernBERT-large - 28 layers, 395 million parameters\nFor more information about ModernBERT, we recommend our release blog post for a high-level overview, and our arXiv pre-print for in-depth information.\nModernBERT is a collaboration between Answer.AI, LightOn, and friends.\nUsage\nYou can use these models directly with the transformers library starting from v4.48.0:\npip install -U transformers>=4.48.0\nSince ModernBERT is a Masked Language Model (MLM), you can use the fill-mask pipeline or load it via AutoModelForMaskedLM. To use ModernBERT for downstream tasks like classification, retrieval, or QA, fine-tune it following standard BERT fine-tuning recipes.\n‚ö†Ô∏è If your GPU supports it, we recommend using ModernBERT with Flash Attention 2 to reach the highest efficiency. To do so, install Flash Attention as follows, then use the model as normal:\npip install flash-attn\nUsing AutoModelForMaskedLM:\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nmodel_id = \"answerdotai/ModernBERT-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\ntext = \"The capital of France is [MASK].\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model(**inputs)\n# To get predictions for the mask:\nmasked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\npredicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\npredicted_token = tokenizer.decode(predicted_token_id)\nprint(\"Predicted token:\", predicted_token)\n# Predicted token:  Paris\nUsing a pipeline:\nimport torch\nfrom transformers import pipeline\nfrom pprint import pprint\npipe = pipeline(\n\"fill-mask\",\nmodel=\"answerdotai/ModernBERT-base\",\ntorch_dtype=torch.bfloat16,\n)\ninput_text = \"He walked to the [MASK].\"\nresults = pipe(input_text)\npprint(results)\nNote: ModernBERT does not use token type IDs, unlike some earlier BERT models. Most downstream usage is identical to standard BERT models on the Hugging Face Hub, except you can omit the token_type_ids parameter.\nEvaluation\nWe evaluate ModernBERT across a range of tasks, including natural language understanding (GLUE), general retrieval (BEIR), long-context retrieval (MLDR), and code retrieval (CodeSearchNet and StackQA).\nKey highlights:\nOn GLUE, ModernBERT-base surpasses other similarly-sized encoder models, and ModernBERT-large is second only to Deberta-v3-large.\nFor general retrieval tasks, ModernBERT performs well on BEIR in both single-vector (DPR-style) and multi-vector (ColBERT-style) settings.\nThanks to the inclusion of code data in its training mixture, ModernBERT as a backbone also achieves new state-of-the-art code retrieval results on CodeSearchNet and StackQA.\nBase Models\nModel\nIR (DPR)\nIR (DPR)\nIR (DPR)\nIR (ColBERT)\nIR (ColBERT)\nNLU\nCode\nCode\nBEIR\nMLDR_OOD\nMLDR_ID\nBEIR\nMLDR_OOD\nGLUE\nCSN\nSQA\nBERT\n38.9\n23.9\n32.2\n49.0\n28.1\n84.7\n41.2\n59.5\nRoBERTa\n37.7\n22.9\n32.8\n48.7\n28.2\n86.4\n44.3\n59.6\nDeBERTaV3\n20.2\n5.4\n13.4\n47.1\n21.9\n88.1\n17.5\n18.6\nNomicBERT\n41.0\n26.7\n30.3\n49.9\n61.3\n84.0\n41.6\n61.4\nGTE-en-MLM\n41.4\n34.3\n44.4\n48.2\n69.3\n85.6\n44.9\n71.4\nModernBERT\n41.6\n27.4\n44.0\n51.3\n80.2\n88.4\n56.4\n73.6\nLarge Models\nModel\nIR (DPR)\nIR (DPR)\nIR (DPR)\nIR (ColBERT)\nIR (ColBERT)\nNLU\nCode\nCode\nBEIR\nMLDR_OOD\nMLDR_ID\nBEIR\nMLDR_OOD\nGLUE\nCSN\nSQA\nBERT\n38.9\n23.3\n31.7\n49.5\n28.5\n85.2\n41.6\n60.8\nRoBERTa\n41.4\n22.6\n36.1\n49.8\n28.8\n88.9\n47.3\n68.1\nDeBERTaV3\n25.6\n7.1\n19.2\n46.7\n23.0\n91.4\n21.2\n19.7\nGTE-en-MLM\n42.5\n36.4\n48.9\n50.7\n71.3\n87.6\n40.5\n66.9\nModernBERT\n44.0\n34.3\n48.6\n52.4\n80.4\n90.4\n59.5\n83.9\nTable 1: Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDRID refers to in-domain (fine-tuned on the training set) evaluation, and MLDR_OOD to out-of-domain.\nModernBERT‚Äôs strong results, coupled with its efficient runtime on long-context inputs, demonstrate that encoder-only models can be significantly improved through modern architectural choices and extensive pretraining on diversified data sources.\nLimitations\nModernBERT‚Äôs training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them.\nTraining\nArchitecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.\nSequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.\nData: 2 trillion tokens of English text and code.\nOptimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.\nHardware: Trained on 8x H100 GPUs.\nSee the paper for more details.\nLicense\nWe release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license.\nCitation\nIf you use ModernBERT in your work, please cite:\n@misc{modernbert,\ntitle={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},\nauthor={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\nyear={2024},\neprint={2412.13663},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.13663},\n}",
    "mradermacher/Qwen2.5-1.5B-Instruct-uncensored-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/thirdeyeai/Qwen2.5-1.5B-Instruct-uncensored\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Qwen2.5-1.5B-Instruct-uncensored-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n0.9\nGGUF\nQ3_K_S\n1.0\nGGUF\nQ3_K_M\n1.0\nlower quality\nGGUF\nQ3_K_L\n1.1\nGGUF\nIQ4_XS\n1.1\nGGUF\nQ4_K_S\n1.2\nfast, recommended\nGGUF\nQ4_K_M\n1.2\nfast, recommended\nGGUF\nQ5_K_S\n1.4\nGGUF\nQ5_K_M\n1.4\nGGUF\nQ6_K\n1.6\nvery good quality\nGGUF\nQ8_0\n2.0\nfast, best quality\nGGUF\nf16\n3.7\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "TechWolf/JobBERT-v2": "SentenceTransformer based on sentence-transformers/all-mpnet-base-v2\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nExample Use Cases\nTraining Details\nTraining Dataset\nTraining Hyperparameters\nFramework Versions\nCitation\nBibTeX\nJobBERT-v2 paper\nSentenceTransformer based on sentence-transformers/all-mpnet-base-v2\nThis is a sentence-transformers model specifically trained for job title matching and similarity. It's finetuned from sentence-transformers/all-mpnet-base-v2 on a large dataset of job titles and their associated skills/requirements. The model maps job titles and descriptions to a 1024-dimensional dense vector space and can be used for semantic job title matching, job similarity search, and related HR/recruitment tasks.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: sentence-transformers/all-mpnet-base-v2\nMaximum Sequence Length: 64 tokens\nOutput Dimensionality: 1024 tokens\nSimilarity Function: Cosine Similarity\nTraining Dataset: 5.5M+ job title - skills pairs\nPrimary Use Case: Job title matching and similarity\nPerformance: Achieves 0.6457 MAP on TalentCLEF benchmark\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 64, 'do_lower_case': False}) with Transformer model: MPNetModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Asym(\n(anchor-0): Dense({'in_features': 768, 'out_features': 1024, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n(positive-0): Dense({'in_features': 768, 'out_features': 1024, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the required packages:\npip install -U sentence-transformers\nThen you can load and use the model with the following code:\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import batch_to_device, cos_sim\n# Load the model\nmodel = SentenceTransformer(\"TechWolf/JobBERT-v2\")\ndef encode_batch(jobbert_model, texts):\nfeatures = jobbert_model.tokenize(texts)\nfeatures = batch_to_device(features, jobbert_model.device)\nfeatures[\"text_keys\"] = [\"anchor\"]\nwith torch.no_grad():\nout_features = jobbert_model.forward(features)\nreturn out_features[\"sentence_embedding\"].cpu().numpy()\ndef encode(jobbert_model, texts, batch_size: int = 8):\n# Sort texts by length and keep track of original indices\nsorted_indices = np.argsort([len(text) for text in texts])\nsorted_texts = [texts[i] for i in sorted_indices]\nembeddings = []\n# Encode in batches\nfor i in tqdm(range(0, len(sorted_texts), batch_size)):\nbatch = sorted_texts[i:i+batch_size]\nembeddings.append(encode_batch(jobbert_model, batch))\n# Concatenate embeddings and reorder to original indices\nsorted_embeddings = np.concatenate(embeddings)\noriginal_order = np.argsort(sorted_indices)\nreturn sorted_embeddings[original_order]\n# Example usage\njob_titles = [\n'Software Engineer',\n'Senior Software Developer',\n'Product Manager',\n'Data Scientist'\n]\n# Get embeddings\nembeddings = encode(model, job_titles)\n# Calculate cosine similarity matrix\nsimilarities = cos_sim(embeddings, embeddings)\nprint(similarities)\nThe output will be a similarity matrix where each value represents the cosine similarity between two job titles:\ntensor([[1.0000, 0.8723, 0.4821, 0.5447],\n[0.8723, 1.0000, 0.4822, 0.5019],\n[0.4821, 0.4822, 1.0000, 0.4328],\n[0.5447, 0.5019, 0.4328, 1.0000]])\nIn this example:\nThe diagonal values are 1.0000 (perfect similarity with itself)\n'Software Engineer' and 'Senior Software Developer' have high similarity (0.8723)\n'Product Manager' and 'Data Scientist' show lower similarity with other roles\nAll values range between 0 and 1, where higher values indicate greater similarity\nExample Use Cases\nJob Title Matching: Find similar job titles for standardization or matching\nJob Search: Match job seekers with relevant positions based on title similarity\nHR Analytics: Analyze job title patterns and similarities across organizations\nTalent Management: Identify similar roles for career development and succession planning\nTraining Details\nTraining Dataset\ngenerator\nDataset: 5.5M+ job title pairs\nFormat: Anchor job titles paired with related skills/requirements\nTraining objective: Learn semantic similarity between job titles and their associated skills\nLoss: CachedMultipleNegativesRankingLoss with cosine similarity\nTraining Hyperparameters\nBatch Size: 2048\nLearning Rate: 5e-05\nEpochs: 1\nFP16 Training: Enabled\nOptimizer: AdamW\nFramework Versions\nPython: 3.9.19\nSentence Transformers: 3.1.0\nTransformers: 4.44.2\nPyTorch: 2.4.1+cu118\nAccelerate: 0.34.2\nDatasets: 3.0.0\nTokenizers: 0.19.1\nCitation\nBibTeX\nJobBERT-v2 paper\nPlease cite this paper when using JobBERT-v2:\n@article{01K47W55SG7ZRKFG431ESRXC35,\nabstract     = {{Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose ConTeXT-match, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. ConTeXT-match significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce Skill-XL a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present JobBERT V2, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.}},\nauthor       = {{Decorte, Jens-Joris and Van Hautte, Jeroen and Develder, Chris and Demeester, Thomas}},\nissn         = {{2169-3536}},\njournal      = {{IEEE ACCESS}},\nkeywords     = {{Taxonomy,Contrastive learning,Training,Annotations,Benchmark testing,Training data,Large language models,Computational efficiency,Accuracy,Terminology,Labor market analysis,text encoders,skill extraction,job title normalization}},\nlanguage     = {{eng}},\npages        = {{133596--133608}},\ntitle        = {{Efficient text encoders for labor market analysis}},\nurl          = {{http://doi.org/10.1109/ACCESS.2025.3589147}},\nvolume       = {{13}},\nyear         = {{2025}},\n}\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nCachedMultipleNegativesRankingLoss\n@misc{gao2021scaling,\ntitle={Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup},\nauthor={Luyu Gao and Yunyi Zhang and Jiawei Han and Jamie Callan},\nyear={2021},\neprint={2101.06983},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}",
    "deepseek-ai/DeepSeek-V3-Base": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\n1. Introduction\n2. Model Summary\n3. Model Downloads\n4. Evaluation Results\nBase Model\nStandard Benchmarks\nContext Window\nChat Model\nStandard Benchmarks (Models larger than 67B)\nOpen Ended Generation Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\n6.1 Inference with DeepSeek-Infer Demo (example only)\nModel Weights & Demo Code Preparation\nModel Weights Conversion\nRun\n6.2 Inference with SGLang (recommended)\n6.3 Inference with LMDeploy (recommended)\n6.4 Inference with TRT-LLM (recommended)\n6.5 Inference with vLLM (recommended)\n6.6 Recommended Inference Functionality with AMD GPUs\n6.7 Recommended Inference Functionality with Huawei Ascend NPUs\n7. License\n8. Citation\n9. Contact\nPaper LinküëÅÔ∏è\n1. Introduction\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance.\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n2. Model Summary\nArchitecture: Innovative Load Balancing Strategy and Training Objective\nOn top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\nWe investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance.\nIt can also be used for speculative decoding for inference acceleration.\nPre-Training: Towards Ultimate Training Efficiency\nWe design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\nThrough co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\nAt an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\nPost-Training: Knowledge Distillation from DeepSeek-R1\nWe introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n3. Model Downloads\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-V3-Base\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-V3\n671B\n37B\n128K\nü§ó HuggingFace\nNOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: How_to Run_Locally.\nFor developers looking to dive deeper, we recommend exploring README_WEIGHTS.md for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n4. Evaluation Results\nBase Model\nStandard Benchmarks\nBenchmark (Metric)\n# Shots\nDeepSeek-V2\nQwen2.5 72B\nLLaMA3.1 405B\nDeepSeek-V3\nArchitecture\n-\nMoE\nDense\nDense\nMoE\n# Activated Params\n-\n21B\n72B\n405B\n37B\n# Total Params\n-\n236B\n72B\n405B\n671B\nEnglish\nPile-test (BPB)\n-\n0.606\n0.638\n0.542\n0.548\nBBH (EM)\n3-shot\n78.8\n79.8\n82.9\n87.5\nMMLU (Acc.)\n5-shot\n78.4\n85.0\n84.4\n87.1\nMMLU-Redux (Acc.)\n5-shot\n75.6\n83.2\n81.3\n86.2\nMMLU-Pro (Acc.)\n5-shot\n51.4\n58.3\n52.8\n64.4\nDROP (F1)\n3-shot\n80.4\n80.6\n86.0\n89.0\nARC-Easy (Acc.)\n25-shot\n97.6\n98.4\n98.4\n98.9\nARC-Challenge (Acc.)\n25-shot\n92.2\n94.5\n95.3\n95.3\nHellaSwag (Acc.)\n10-shot\n87.1\n84.8\n89.2\n88.9\nPIQA (Acc.)\n0-shot\n83.9\n82.6\n85.9\n84.7\nWinoGrande (Acc.)\n5-shot\n86.3\n82.3\n85.2\n84.9\nRACE-Middle (Acc.)\n5-shot\n73.1\n68.1\n74.2\n67.1\nRACE-High (Acc.)\n5-shot\n52.6\n50.3\n56.8\n51.3\nTriviaQA (EM)\n5-shot\n80.0\n71.9\n82.7\n82.9\nNaturalQuestions (EM)\n5-shot\n38.6\n33.2\n41.5\n40.0\nAGIEval (Acc.)\n0-shot\n57.5\n75.8\n60.6\n79.6\nCode\nHumanEval (Pass@1)\n0-shot\n43.3\n53.0\n54.9\n65.2\nMBPP (Pass@1)\n3-shot\n65.0\n72.6\n68.4\n75.4\nLiveCodeBench-Base (Pass@1)\n3-shot\n11.6\n12.9\n15.5\n19.4\nCRUXEval-I (Acc.)\n2-shot\n52.5\n59.1\n58.5\n67.3\nCRUXEval-O (Acc.)\n2-shot\n49.8\n59.9\n59.9\n69.8\nMath\nGSM8K (EM)\n8-shot\n81.6\n88.3\n83.5\n89.3\nMATH (EM)\n4-shot\n43.4\n54.4\n49.0\n61.6\nMGSM (EM)\n8-shot\n63.6\n76.2\n69.9\n79.8\nCMath (EM)\n3-shot\n78.7\n84.5\n77.3\n90.7\nChinese\nCLUEWSC (EM)\n5-shot\n82.0\n82.5\n83.0\n82.7\nC-Eval (Acc.)\n5-shot\n81.4\n89.2\n72.5\n90.1\nCMMLU (Acc.)\n5-shot\n84.0\n89.5\n73.7\n88.8\nCMRC (EM)\n1-shot\n77.4\n75.8\n76.0\n76.3\nC3 (Acc.)\n0-shot\n77.4\n76.7\n79.7\n78.6\nCCPM (Acc.)\n0-shot\n93.0\n88.5\n78.6\n92.0\nMultilingual\nMMMLU-non-English (Acc.)\n5-shot\n64.0\n74.8\n73.8\n79.4\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper.\nContext Window\nEvaluation results on the Needle In A Haystack (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to 128K.\nChat Model\nStandard Benchmarks (Models larger than 67B)\nBenchmark (Metric)\nDeepSeek V2-0506\nDeepSeek V2.5-0905\nQwen2.5 72B-Inst.\nLlama3.1 405B-Inst.\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nArchitecture\nMoE\nMoE\nDense\nDense\n-\n-\nMoE\n# Activated Params\n21B\n21B\n72B\n405B\n-\n-\n37B\n# Total Params\n236B\n236B\n72B\n405B\n-\n-\n671B\nEnglish\nMMLU (EM)\n78.2\n80.6\n85.3\n88.6\n88.3\n87.2\n88.5\nMMLU-Redux (EM)\n77.9\n80.3\n85.6\n86.2\n88.9\n88.0\n89.1\nMMLU-Pro (EM)\n58.5\n66.2\n71.6\n73.3\n78.0\n72.6\n75.9\nDROP (3-shot F1)\n83.0\n87.8\n76.7\n88.7\n88.3\n83.7\n91.6\nIF-Eval (Prompt Strict)\n57.7\n80.6\n84.1\n86.0\n86.5\n84.3\n86.1\nGPQA-Diamond (Pass@1)\n35.3\n41.3\n49.0\n51.1\n65.0\n49.9\n59.1\nSimpleQA (Correct)\n9.0\n10.2\n9.1\n17.1\n28.4\n38.2\n24.9\nFRAMES (Acc.)\n66.9\n65.4\n69.8\n70.0\n72.5\n80.5\n73.3\nLongBench v2 (Acc.)\n31.6\n35.4\n39.4\n36.1\n41.0\n48.1\n48.7\nCode\nHumanEval-Mul (Pass@1)\n69.3\n77.4\n77.3\n77.2\n81.7\n80.5\n82.6\nLiveCodeBench (Pass@1-COT)\n18.8\n29.2\n31.1\n28.4\n36.3\n33.4\n40.5\nLiveCodeBench (Pass@1)\n20.3\n28.4\n28.7\n30.1\n32.8\n34.2\n37.6\nCodeforces (Percentile)\n17.5\n35.6\n24.8\n25.3\n20.3\n23.6\n51.6\nSWE Verified (Resolved)\n-\n22.6\n23.8\n24.5\n50.8\n38.8\n42.0\nAider-Edit (Acc.)\n60.3\n71.6\n65.4\n63.9\n84.2\n72.9\n79.7\nAider-Polyglot (Acc.)\n-\n18.2\n7.6\n5.8\n45.3\n16.0\n49.6\nMath\nAIME 2024 (Pass@1)\n4.6\n16.7\n23.3\n23.3\n16.0\n9.3\n39.2\nMATH-500 (EM)\n56.3\n74.7\n80.0\n73.8\n78.3\n74.6\n90.2\nCNMO 2024 (Pass@1)\n2.8\n10.8\n15.9\n6.8\n13.1\n10.8\n43.2\nChinese\nCLUEWSC (EM)\n89.9\n90.4\n91.4\n84.7\n85.4\n87.9\n90.9\nC-Eval (EM)\n78.6\n79.5\n86.1\n61.5\n76.7\n76.0\n86.5\nC-SimpleQA (Correct)\n48.5\n54.1\n48.4\n50.4\n51.3\n59.3\n64.8\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\nOpen Ended Generation Evaluation\nModel\nArena-Hard\nAlpacaEval 2.0\nDeepSeek-V2.5-0905\n76.2\n50.5\nQwen2.5-72B-Instruct\n81.2\n49.1\nLLaMA-3.1 405B\n69.3\n40.5\nGPT-4o-0513\n80.4\n51.1\nClaude-Sonnet-3.5-1022\n85.2\n52.0\nDeepSeek-V3\n85.5\n70.0\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: chat.deepseek.com\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\nDeepSeek-Infer Demo: We provide a simple and lightweight demo for FP8 and BF16 inference.\nSGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\nLMDeploy: Enables efficient FP8 and BF16 inference for local and cloud deployment.\nTensorRT-LLM: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\nvLLM: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\nAMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\nHuawei Ascend NPU: Supports running DeepSeek-V3 on Huawei Ascend devices.\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\nHere is an example of converting FP8 weights to BF16:\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\nNOTE: Huggingface's Transformers has not been directly supported yet.\n6.1 Inference with DeepSeek-Infer Demo (example only)\nModel Weights & Demo Code Preparation\nFirst, clone our DeepSeek-V3 GitHub repository:\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\nNavigate to the inference folder and install dependencies listed in requirements.txt.\ncd DeepSeek-V3/inference\npip install -r requirements.txt\nDownload the model weights from HuggingFace, and put them into /path/to/DeepSeek-V3 folder.\nModel Weights Conversion\nConvert HuggingFace model weights to a specific format:\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\nRun\nThen you can chat with DeepSeek-V3:\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\nOr batch inference on a given file:\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n6.2 Inference with SGLang (recommended)\nSGLang currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\nNotably, SGLang v0.4.1 fully supports running DeepSeek-V3 on both NVIDIA and AMD GPUs, making it a highly versatile and robust solution.\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n6.3 Inference with LMDeploy (recommended)\nLMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n6.4 Inference with TRT-LLM (recommended)\nTensorRT-LLM now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.\n6.5 Inference with vLLM (recommended)\nvLLM v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers pipeline parallelism allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the vLLM instructions. Please feel free to follow the enhancement plan as well.\n6.6 Recommended Inference Functionality with AMD GPUs\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the SGLang instructions.\n6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe MindIE framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the instructions here.\n7. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-V3 Base/Chat models is subject to the Model License. DeepSeek-V3 series (including Base and Chat) supports commercial use.\n8. Citation\n@misc{deepseekai2024deepseekv3technicalreport,\ntitle={DeepSeek-V3 Technical Report},\nauthor={DeepSeek-AI},\nyear={2024},\neprint={2412.19437},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.19437},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "openbmb/MiniCPM-o-2_6": "MiniCPM-o 2.6\nEvaluation\nVisual understanding results\nAudio understanding and speech conversation results.\nMultimodal live streaming results.\nExamples\nOnline Demo\nUsage\nModel initialization\nOmni mode\nChat inference\nStreaming inference\nSpeech and Audio Mode\nMimick\nGeneral Speech Conversation with Configurable Voices\nSpeech Conversation as an AI Assistant\nInstruction-to-Speech\nVoice Cloning\nAddressing Various Audio Understanding Tasks\nVision-Only mode\nChat with single image\nChat with multiple images\nIn-context few-shot learning\nChat with video\nModel License\nStatement\nInference with llama.cpp\nModel License\nStatement\nInt4 quantized version\nModel License\nStatement\nLicense\nModel License\nStatement\nKey Techniques and Other Multimodal Projects\nCitation\nA GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone\nGitHub | Online Demo | Technical Blog | Join Us\nNews\n[2025.06.20] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Our official ollama repository is released. Try our latest models with one clickÔºÅ\n[2025.03.01] üöÄüöÄüöÄ RLAIF-V, which is the alignment technique of MiniCPM-o, is accepted by CVPR 2025ÔºÅThe code, dataset, paper are open-sourced!\n[2025.01.24] üì¢üì¢üì¢ MiniCPM-o 2.6 technical report is released! See Here.\n[2025.01.19] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!\nMiniCPM-o 2.6\nMiniCPM-o 2.6 is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:\nüî• Leading Visual Capability.\nMiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding. It also outperforms GPT-4V and Claude 3.5 Sonnet in mutli-image and video understanding, and shows promising in-context learning capability.\nüéô State-of-the-art Speech Capability. MiniCPM-o 2.6 supports bilingual real-time speech conversation with configurable voices in English and Chinese. It outperforms GPT-4o-realtime on audio understanding tasks such as ASR and STT translation, and shows state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.\nüé¨ Strong Multimodal Live Streaming Capability. As a new feature, MiniCPM-o 2.6 can accept continous video and audio streams independent of user queries, and support real-time speech interaction. It outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-art performance in open-source community on StreamingBench, a comprehensive benchmark for real-time video understanding, omni-source (video & audio) understanding, and multimodal contextual understanding.\nüí™ Strong OCR Capability and Others.\nAdvancing popular visual capabilites from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405.\nBased on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports multilingual capabilities on more than 30 languages.\nüöÄ Superior Efficiency.\nIn addition to its friendly size, MiniCPM-o 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support multimodal live streaming on end-side devices such as iPad.\nüí´  Easy Usage.\nMiniCPM-o 2.6 can be easily used in various ways: (1) llama.cpp support for efficient CPU inference on local devices, (2) int4 and GGUF format quantized models in 16 sizes, (3) vLLM support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with LLaMA-Factory, (5) quick local WebUI demo setup with Gradio, and (6) online web demo on server.\nModel Architecture.\nEnd-to-end Omni-modal Architecture. Different modality encoder/decoders are connected and trained in an end-to-end fashion to fully exploit rich multimodal knowledge.\nOmni-modal Live Streaming Mechanism. (1) We change the offline modality encoder/decoders into online ones for streaminig inputs/outputs. (2) We devise a time-division multiplexing (TDM) mechanism for omni-modality streaminig processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.\nConfigurable Speech Modeling Design. We devise a multimodal system prompt, including traditional text system prompt, and a new audio system prompt to determine the assistant voice. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.\nEvaluation\nVisual understanding results\nImage Understanding:\nModel\nSize\nToken Density+\nOpenCompass\nOCRBench\nMathVista mini\nChartQA\nMMVet\nMMStar\nMME\nMMB1.1 test\nAI2D\nMMMU val\nHallusionBench\nTextVQA val\nDocVQA test\nMathVerse mini\nMathVision\nMMHal Score\nProprietary\nGPT-4o-20240513\n-\n1088\n69.9\n736\n61.3\n85.7\n69.1\n63.9\n2328.7\n82.2\n84.6\n69.2\n55.0\n-\n92.8\n50.2\n30.4\n3.6\nClaude3.5-Sonnet\n-\n750\n67.9\n788\n61.6\n90.8\n66.0\n62.2\n1920.0\n78.5\n80.2\n65.9\n49.9\n-\n95.2\n-\n-\n3.4\nGemini 1.5 Pro\n-\n-\n64.4\n754\n57.7\n81.3\n64.0\n59.1\n2110.6\n73.9\n79.1\n60.6\n45.6\n73.5\n86.5\n-\n19.2\n-\nGPT-4o-mini-20240718\n-\n1088\n64.1\n785\n52.4\n-\n66.9\n54.8\n2003.4\n76.0\n77.8\n60.0\n46.1\n-\n-\n-\n-\n3.3\nOpen Source\nCambrian-34B\n34B\n1820\n58.3\n591\n50.3\n75.6\n53.2\n54.2\n2049.9\n77.8\n79.5\n50.4\n41.6\n76.7\n75.5\n-\n-\n-\nGLM-4V-9B\n13B\n784\n59.1\n776\n51.1\n-\n58.0\n54.8\n2018.8\n67.9\n71.2\n46.9\n45.0\n-\n-\n-\n-\n-\nPixtral-12B\n12B\n256\n61.0\n685\n56.9\n81.8\n58.5\n54.5\n-\n72.7\n79.0\n51.1\n47.0\n75.7\n90.7\n-\n-\n-\nDeepSeek-VL2-27B (4B)\n27B\n672\n66.4\n809\n63.9\n86.0\n60.0\n61.9\n2253.0\n81.2\n83.8\n54.0\n45.3\n84.2\n93.3\n-\n-\n3.0\nQwen2-VL-7B\n8B\n784\n67.1\n866\n58.2\n83.0\n62.0\n60.7\n2326.0\n81.8\n83.0\n54.1\n50.6\n84.3\n94.5\n31.9\n16.3\n3.2\nLLaVA-OneVision-72B\n72B\n182\n68.1\n741\n67.5\n83.7\n60.6\n65.8\n2261.0\n85.0\n85.6\n56.8\n49.0\n80.5\n91.3\n39.1\n-\n3.5\nInternVL2.5-8B\n8B\n706\n68.3\n822\n64.4\n84.8\n62.8\n62.8\n2344.0\n83.6\n84.5\n56.0\n50.1\n79.1\n93.0\n39.5\n19.7\n3.4\nMiniCPM-V 2.6\n8B\n2822\n65.2\n852*\n60.6\n79.4\n60.0\n57.5\n2348.4*\n78.0\n82.1\n49.8*\n48.1*\n80.1\n90.8\n25.7\n18.3\n3.6\nMiniCPM-o 2.6\n8B\n2822\n70.2\n897*\n71.9*\n86.9*\n67.5\n64.0\n2372.0*\n80.5\n85.8\n50.4*\n51.9\n82.0\n93.5\n41.4*\n23.1*\n3.8\n* We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set.\n+ Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.\nNote: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.\nMulti-image and Video Understanding:\nclick to view\nModel\nSize\nBLINK val\nMantis Eval\nMIRB\nVideo-MME (wo / w subs)\nProprietary\nGPT-4o-20240513\n-\n68.0\n-\n-\n71.9/77.2\nGPT4V\n-\n54.6\n62.7\n53.1\n59.9/63.3\nOpen-source\nLLaVA-NeXT-Interleave 14B\n14B\n52.6\n66.4\n30.2\n-\nLLaVA-OneVision-72B\n72B\n55.4\n77.6\n-\n66.2/69.5\nMANTIS 8B\n8B\n49.1\n59.5\n34.8\n-\nQwen2-VL-7B\n8B\n53.2\n69.6*\n67.6*\n63.3/69.0\nInternVL2.5-8B\n8B\n54.8\n67.7\n52.5\n64.2/66.9\nMiniCPM-V 2.6\n8B\n53.0\n69.1\n53.8\n60.9/63.6\nMiniCPM-o 2.6\n8B\n56.7\n71.9\n58.6\n63.9/67.9\n* We evaluate officially released checkpoints by ourselves.\nAudio understanding and speech conversation results.\nAudio Understanding:\nTask\nSize\nASR (zh)\nASR (en)\nAST\nEmotion\nMetric\nCER‚Üì\nWER‚Üì\nBLEU‚Üë\nACC‚Üë\nDataset\nAISHELL-1\nFleurs zh\nWenetSpeech test-net\nLibriSpeech test-clean\nGigaSpeech\nTED-LIUM\nCoVoST en2zh\nCoVoST zh2en\nMELD emotion\nProprietary\nGPT-4o-Realtime\n-\n7.3*\n5.4*\n28.9*\n2.6*\n12.9*\n4.8*\n37.1*\n15.7*\n33.2*\nGemini 1.5 Pro\n-\n4.5*\n5.9*\n14.3*\n2.9*\n10.6*\n3.0*\n47.3*\n22.6*\n48.4*\nOpen-Source\nQwen2-Audio-7B\n8B\n-\n7.5\n-\n1.6\n-\n-\n45.2\n24.4\n55.3\nQwen2-Audio-7B-Instruct\n8B\n2.6*\n6.9*\n10.3*\n3.1*\n9.7*\n5.9*\n39.5*\n22.9*\n17.4*\nGLM-4-Voice-Base\n9B\n2.5\n-\n-\n2.8\n-\n-\n-\n-\nMiniCPM-o 2.6\n8B\n1.6\n4.4\n6.9\n1.7\n8.7\n3.0\n48.2\n27.2\n52.4\n* We evaluate officially released checkpoints by ourselves.\nSpeech Generation:\nTask\nSize\nSpeechQA\nMetric\nACC‚Üë\nG-Eval (10 point)‚Üë\nSemantic ELO score‚Üë\nAcoustic ELO score‚Üë\nOverall ELO score‚Üë\nUTMOS‚Üë\nASR-WER‚Üì\nDataset\nSpeech Llama Q.\nSpeech Web Q.\nSpeech Trivia QA\nSpeech AlpacaEval\nAudioArena\nProprietary\nGPT-4o-Realtime\n71.7\n51.6\n69.7\n7.4\n1157\n1203\n1200\n4.2\n2.3\nOpen-Source\nGLM-4-Voice\n9B\n50.0\n32.0\n36.4\n5.1\n999\n1147\n1035\n4.1\n11.7\nLlama-Omni\n8B\n45.3\n22.9\n10.7\n3.9\n960\n878\n897\n3.2\n24.3\nMoshi\n7B\n43.7\n23.8\n16.7\n2.4\n871\n808\n875\n2.8\n8.2\nMini-Omni\n1B\n22.0\n12.8\n6.9\n2.5\n926\n803\n865\n3.4\n10.0\nMiniCPM-o 2.6\n8B\n61.0\n40.0\n40.2\n5.1\n1088\n1163\n1131\n4.2\n9.8\nAll results are from AudioEvals, and the evaluation methods along with further details can be found in UltraEval-Audio.\nEnd-to-end Voice Cloning\nTask\nVoice cloning\nMetric\nSIMO‚Üë\nSIMO‚Üë\nDataset\nSeed-TTS test-zh\nSeed-TTS test-en\nF5-TTS\n76\n67\nCosyVoice\n75\n64\nFireRedTTS\n63\n46\nMiniCPM-o 2.6\n57\n47\nMultimodal live streaming results.\nMultimodal Live Streaming: results on StreamingBench\nModel\nSize\nReal-Time Video Understanding\nOmni-Source Understanding\nContextual Understanding\nOverall\nProprietary\nGemini 1.5 Pro\n-\n77.4\n67.8\n51.1\n70.3\nGPT-4o-202408\n-\n74.5\n51.0\n48.0\n64.1\nClaude-3.5-Sonnet\n-\n74.0\n41.4\n37.8\n59.7\nOpen-source\nVILA-1.5\n8B\n61.5\n37.5\n26.7\n49.5\nLongVA\n7B\n63.1\n35.9\n30.2\n50.7\nLLaVA-Next-Video-34B\n34B\n69.8\n41.7\n34.3\n56.7\nQwen2-VL-7B\n8B\n71.2\n40.7\n33.1\n57.0\nInternVL2-8B\n8B\n70.1\n42.7\n34.1\n57.0\nVITA-1.5\n8B\n70.9\n40.8\n35.8\n57.4\nLLaVA-OneVision-7B\n8B\n74.3\n40.8\n31.0\n58.4\nInternLM-XC2.5-OL-7B\n8B\n75.4\n46.2\n33.6\n60.8\nMiniCPM-V 2.6\n8B\n72.4\n40.2\n33.4\n57.7\nMiniCPM-o 2.6\n8B\n79.9\n53.4\n38.5\n66.0\nExamples\nWe deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.\nOnline Demo\nClick here to try the online demo of MiniCPM-o 2.6.\nUsage\nInference using Huggingface transformers on NVIDIA GPUs. Please ensure that transformers==4.44.2 is installed, as other versions may have compatibility issues. We are investigating this issue. Requirements tested on python 3.10Ôºö\nPillow==10.1.0\ntorch==2.3.1\ntorchaudio==2.3.1\ntorchvision==0.18.1\ntransformers==4.44.2\nlibrosa==0.9.0\nsoundfile==0.12.1\nvector-quantize-pytorch==1.18.5\nvocos==0.1.0\ndecord\nmoviepy\nModel initialization\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n# load omni model default, the default init_vision/init_audio/init_tts is True\n# if load vision-only model, please set init_audio=False and init_tts=False\n# if load audio-only model, please set init_vision=False\nmodel = AutoModel.from_pretrained(\n'openbmb/MiniCPM-o-2_6',\ntrust_remote_code=True,\nattn_implementation='sdpa', # sdpa or flash_attention_2\ntorch_dtype=torch.bfloat16,\ninit_vision=True,\ninit_audio=True,\ninit_tts=True\n)\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)\n# In addition to vision-only mode, tts processor and vocos also needs to be initialized\nmodel.init_tts()\nIf you are using an older version of PyTorch, you might encounter this issue \"weight_norm_fwd_first_dim_kernel\" not implemented for 'BFloat16', Please convert the TTS to float32 type.\nmodel.tts.float()\nOmni mode\nWe provide two inference modes: chat and streaming\nChat inference\nimport math\nimport numpy as np\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip\nimport tempfile\nimport librosa\nimport soundfile as sf\ndef get_video_chunk_content(video_path, flatten=True):\nvideo = VideoFileClip(video_path)\nprint('video_duration:', video.duration)\nwith tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\ntemp_audio_file_path = temp_audio_file.name\nvideo.audio.write_audiofile(temp_audio_file_path, codec=\"pcm_s16le\", fps=16000)\naudio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)\nnum_units = math.ceil(video.duration)\n# 1 frame + 1s audio chunk\ncontents= []\nfor i in range(num_units):\nframe = video.get_frame(i+1)\nimage = Image.fromarray((frame).astype(np.uint8))\naudio = audio_np[sr*i:sr*(i+1)]\nif flatten:\ncontents.extend([\"<unit>\", image, audio])\nelse:\ncontents.append([\"<unit>\", image, audio])\nreturn contents\nvideo_path=\"assets/Skiing.mp4\"\n# if use voice clone prompt, please set ref_audio\nref_audio_path = 'assets/demo.wav'\nref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)\nsys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')\n# or use default prompt\n# sys_msg = model.get_sys_prompt(mode='omni', language='en')\ncontents = get_video_chunk_content(video_path)\nmsg = {\"role\":\"user\", \"content\": contents}\nmsgs = [sys_msg, msg]\n# please set generate_audio=True and output_audio_path to save the tts result\ngenerate_audio = True\noutput_audio_path = 'output.wav'\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\ntemperature=0.5,\nmax_new_tokens=4096,\nomni_input=True, # please set omni_input=True when omni inference\nuse_tts_template=True,\ngenerate_audio=generate_audio,\noutput_audio_path=output_audio_path,\nmax_slice_nums=1,\nuse_image_id=False,\nreturn_dict=True\n)\nprint(res)\n## You will get the answer: The person in the picture is skiing down a snowy slope.\n# import IPython\n# IPython.display.Audio('output.wav')\nStreaming inference\n# a new conversation need reset session first, it will reset the kv-cache\nmodel.reset_session()\ncontents = get_video_chunk_content(video_path, flatten=False)\nsession_id = '123'\ngenerate_audio = True\n# 1. prefill system prompt\nres = model.streaming_prefill(\nsession_id=session_id,\nmsgs=[sys_msg],\ntokenizer=tokenizer\n)\n# 2. prefill video/audio chunks\nfor content in contents:\nmsgs = [{\"role\":\"user\", \"content\": content}]\nres = model.streaming_prefill(\nsession_id=session_id,\nmsgs=msgs,\ntokenizer=tokenizer\n)\n# 3. generate\nres = model.streaming_generate(\nsession_id=session_id,\ntokenizer=tokenizer,\ntemperature=0.5,\ngenerate_audio=generate_audio\n)\naudios = []\ntext = \"\"\nif generate_audio:\nfor r in res:\naudio_wav = r.audio_wav\nsampling_rate = r.sampling_rate\ntxt = r.text\naudios.append(audio_wav)\ntext += txt\nres = np.concatenate(audios)\nsf.write(\"output.wav\", res, samplerate=sampling_rate)\nprint(\"text:\", text)\nprint(\"audio saved to output.wav\")\nelse:\nfor r in res:\ntext += r['text']\nprint(\"text:\", text)\nSpeech and Audio Mode\nModel initialization\nimport torch\nimport librosa\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)\nmodel.init_tts()\nmodel.tts.float()\nMimick\nMimick task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.\nmimick_prompt = \"Please repeat each user's speech, including voice style and speech content.\"\naudio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked\n# can also try `./assets/input_examples/cxk_original.wav`,\n# `./assets/input_examples/fast-pace.wav`,\n# `./assets/input_examples/chi-english-1.wav`\n# `./assets/input_examples/exciting-emotion.wav`\n# for different aspects of speech-centric features.\nmsgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ntemperature=0.3,\ngenerate_audio=True,\noutput_audio_path='output_mimick.wav', # save the tts result to output_audio_path\n)\nGeneral Speech Conversation with Configurable Voices\nA general usage scenario of MiniCPM-o-2.6 is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, MiniCPM-o-2.6 sounds more natural and human-like. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.\nref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio\nsys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')\n# round one\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}\nmsgs = [sys_prompt, user_question]\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_roleplay_round_1.wav',\n)\n# round two\nhistory = msgs.append({'role': 'assistant', 'content': res})\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}\nmsgs = history.append(user_question)\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_roleplay_round_2.wav',\n)\nprint(res)\nSpeech Conversation as an AI Assistant\nAn enhanced feature of MiniCPM-o-2.6 is to act as an AI assistant, but only with limited choice of voices. In this mode, MiniCPM-o-2.6 is less human-like and more like a voice assistant. In this mode, the model is more instruction-following. For demo, you are suggested to use assistant_female_voice, assistant_male_voice, and assistant_default_female_voice. Other voices may work but not as stable as the default voices.\nPlease note that, assistant_female_voice and assistant_male_voice are more stable but sounds like robots, while assistant_default_female_voice is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices assistant_female_voice and assistant_male_voice.\nref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`\nsys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en')\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question\n# round one\nmsgs = [sys_prompt, user_question]\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_assistant_round_1.wav',\n)\n# round two\nhistory = msgs.append({'role': 'assistant', 'content': res})\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}\nmsgs = history.append(user_question)\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_assistant_round_2.wav',\n)\nprint(res)\nInstruction-to-Speech\nMiniCPM-o-2.6 can also do Instruction-to-Speech, aka Voice Creation. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to https://voxinstruct.github.io/VoxInstruct/.\ninstruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'\nmsgs = [{'role': 'user', 'content': [instruction]}]\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_voice_creation.wav',\n)\nVoice Cloning\nMiniCPM-o-2.6 can also do zero-shot text-to-speech, aka Voice Cloning. With this mode, model will act like a TTS model.\nref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio\nsys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')\ntext_prompt = f\"Please read the text below.\"\nuser_question = {'role': 'user', 'content': [text_prompt, \"content that you want to read\"]}\nmsgs = [sys_prompt, user_question]\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_voice_cloning.wav',\n)\nAddressing Various Audio Understanding Tasks\nMiniCPM-o-2.6 can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.\nFor audio-to-text tasks, you can use the following prompts:\nASR with ZH(same as AST en2zh): ËØ∑‰ªîÁªÜÂê¨ËøôÊÆµÈü≥È¢ëÁâáÊÆµÔºåÂπ∂Â∞ÜÂÖ∂ÂÜÖÂÆπÈÄêÂ≠óËÆ∞ÂΩï„ÄÇ\nASR with EN(same as AST zh2en): Please listen to the audio snippet carefully and transcribe the content.\nSpeaker Analysis: Based on the speaker's content, speculate on their gender, condition, age range, and health status.\nGeneral Audio Caption: Summarize the main content of the audio.\nGeneral Sound Scene Tagging: Utilize one keyword to convey the audio's content or the associated scene.\ntask_prompt = \"Please listen to the audio snippet carefully and transcribe the content.\" + \"\\n\" # can change to other prompts.\naudio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned\nmsgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nmax_new_tokens=128,\nuse_tts_template=True,\ngenerate_audio=True,\ntemperature=0.3,\noutput_audio_path='result_audio_understanding.wav',\n)\nprint(res)\nVision-Only mode\nMiniCPM-o-2_6 has the same inference methods as MiniCPM-V-2_6\nChat with single image\n# test.py\nimage = Image.open('xx.jpg').convert('RGB')\nquestion = 'What is in the image?'\nmsgs = [{'role': 'user', 'content': [image, question]}]\nres = model.chat(\nimage=None,\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(res)\n## if you want to use streaming, please make sure sampling=True and stream=True\n## the model.chat will return a generator\nres = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nstream=True\n)\ngenerated_text = \"\"\nfor new_text in res:\ngenerated_text += new_text\nprint(new_text, flush=True, end='')\nChat with multiple images\nClick to show Python code running MiniCPM-o 2.6 with multiple images input.\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(answer)\nIn-context few-shot learning\nClick to view Python code running MiniCPM-o 2.6 with few-shot input.\nquestion = \"production date\"\nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\nmsgs = [\n{'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n{'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n{'role': 'user', 'content': [image_test, question]}\n]\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(answer)\nChat with video\nClick to view Python code running MiniCPM-o 2.6 with video input.\nMAX_NUM_FRAMES=64 # if cuda OOM set a smaller number\ndef encode_video(video_path):\ndef uniform_sample(l, n):\ngap = len(l) / n\nidxs = [int(i * gap + gap / 2) for i in range(n)]\nreturn [l[i] for i in idxs]\nvr = VideoReader(video_path, ctx=cpu(0))\nsample_fps = round(vr.get_avg_fps() / 1)  # FPS\nframe_idx = [i for i in range(0, len(vr), sample_fps)]\nif len(frame_idx) > MAX_NUM_FRAMES:\nframe_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\nframes = vr.get_batch(frame_idx).asnumpy()\nframes = [Image.fromarray(v.astype('uint8')) for v in frames]\nprint('num frames:', len(frames))\nreturn frames\nvideo_path =\"video_test.mp4\"\nframes = encode_video(video_path)\nquestion = \"Describe the video\"\nmsgs = [\n{'role': 'user', 'content': frames + [question]},\n]\n# Set decode params for video\nparams={}\nparams[\"use_image_id\"] = False\nparams[\"max_slice_nums\"] = 2 # use 1 if cuda OOM and video resolution >  448*448\nanswer = model.chat(\nmsgs=msgs,\ntokenizer=tokenizer,\n**params\n)\nprint(answer)\nPlease look at GitHub for more detail about usage.\nInference with llama.cpp\nMiniCPM-o 2.6 (vision-only mode) can run with llama.cpp. See our fork of llama.cpp and readme for more detail.\nInt4 quantized version\nDownload the int4 quantized version for lower GPU memory (7GB) usage:  MiniCPM-o-2_6-int4.\nLicense\nModel License\nThe MiniCPM-o/V model weights and code are open-sourced under the Apache-2.0 license.\nTo help us better understand and support our users, we would deeply appreciate it if you could consider optionally filling out a brief registration \"questionnaire\".\nStatement\nAs an LMM, MiniCPM-o 2.6 generates contents by learning a large mount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-o 2.6 does not represent the views and positions of the model developers\nWe will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\nKey Techniques and Other Multimodal Projects\nüëè Welcome to explore key techniques of MiniCPM-o 2.6 and other multimodal projects of our team:\nVisCPM | RLHF-V | LLaVA-UHD  | RLAIF-V\nCitation\nIf you find our work helpful, please consider citing our papers üìù and liking this project ‚ù§Ô∏èÔºÅ\n@article{yao2024minicpm,\ntitle={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\nauthor={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\njournal={arXiv preprint arXiv:2408.01800},\nyear={2024}\n}",
    "calcuis/illustrious": "gguf quantized and fp8 scaled versions of illustrious (test pack)\nsetup (in general)\nrun it straight (no installation needed way)\nworkflow\nreview\nreference\ngguf quantized and fp8 scaled versions of illustrious (test pack)\nPrompt\nmasterpiece, best quality, vibrant, very aesthetic, high contrast, semrealistic, highly detailed, absurdres, masterful composition, cinematic lighting, score_9, score_8_up, score_7_up, score_6_up, score_5_up, rating_questionable, source_anime, 1girl, portrait, multicolored hair, fringe, bare shoulders, upper body, cosmic\nNegative Prompt\nfemboy, low quality, 2koma, 4koma, bad anatomy, jpeg artifacts, signature, watermark, lowres, bad hands\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one with gguf q4_0\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one with gguf q4_0\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one with gguf q4_0 (v9 model)\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one with gguf q8_0 (v11 model)\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one with full set gguf (new v13 model)\nsetup (in general)\ndrag gguf file(s) to diffusion_models folder (./ComfyUI/models/diffusion_models)\ndrag clip or encoder(s), i.e., illustrious_g_clip and illustrious_l_clip, to text_encoders folder (./ComfyUI/models/text_encoders)\ndrag vae decoder(s), i.e., vae, to illustrious_vae folder (./ComfyUI/models/vae)\nrun it straight (no installation needed way)\nget the comfy pack with the new gguf-node (pack)\nrun the .bat file in the main directory\nworkflow\ndrag any workflow json file to the activated browser; or\ndrag any generated output file (i.e., picture, video, etc.; which contains the workflow metadata) to the activated browser\nreview\nuse tag/word(s) as input for more accurate results for those legacy models; not very convenient (compare to the recent models) at the very beginning\ncredits should be given to those contributors from civitai platform\nfast-illustrious gguf was quantized from fp8 scaled safetensors while illustrious gguf was quantized from the original bf16 (this is just an attempt to test: is it true? the trimmed model with 50% tensors lesser really load faster? please test it yourself; btw, some models might have their unique structure/feature affecting the loader performance, never one size fits all)\nfp8 scaled file works fine in this model; including vae and clips\ngood to run on old machines, i.e., 9xx series or before (legacy mode [--disable-cuda-malloc --lowvram] supported); compatible with the new gguf-node\ndisclaimer: some models (original files) are provided by someone else and we might not easily spot out the creator/contributor(s) behind, unless it was specified in the source; rather let it blank instead of anonymous/unnamed/unknown; if it is your work, do let us know; we will address it back properly and probably; thanks for everything\nreference\nwai creator\ncomfyui comfyanonymous\ngguf-node (pypi|repo|pack)",
    "jinaai/ReaderLM-v2": "ReaderLM-v2\nWhat's New in ReaderLM-v2\nModel Overview\nUsage\nVia Reader API\nOn Google Colab\nLocal Usage\nHTML to Markdown Example\nHTML to JSON Example\nModel Performance\nQuantitative Evaluation\nQualitative Evaluation\nTraining Details\nTrained by Jina AI.\nBlog | API | Colab | AWS | Azure| Arxiv\nReaderLM-v2\nReaderLM-v2 is a 1.5B parameter language model that converts raw HTML into beautifully formatted markdown or JSON with superior accuracy and improved longer context handling. Supporting multiple languages (29 in total), ReaderLM-v2 is specialized for tasks involving HTML parsing, transformation, and text extraction.\nWhat's New in ReaderLM-v2\nReaderLM-v2 represents a significant leap forward from its predecessor, with several key improvements:\nBetter Markdown Generation: Thanks to its new training paradigm and higher-quality training data, the model excels at generating complex elements like code fences, nested lists, tables, and LaTeX equations.\nJSON Output: Introduces direct HTML-to-JSON generation using predefined schemas, eliminating the need for intermediate markdown conversion.\nLonger Context Handling: Handles up to 512K tokens combined input and output length, with improved performance on long-form content.\nMultilingual Support: Comprehensive support across 29 languages for broader applications.\nEnhanced Stability: Greatly alleviates degeneration issues after generating long sequences through contrastive loss during training.\nModel Overview\nModel Type: Autoregressive, decoder-only transformer\nParameter Count: 1.54B\nContext Window: Up to 512K tokens (combined input and output)\nHidden Size: 1536\nNumber of Layers: 28\nQuery Heads: 12\nKV Heads: 2\nHead Size: 128\nIntermediate Size: 8960\nSupported Languages: English, Chinese, Japanese, Korean, French, Spanish, Portuguese, German, Italian, Russian, Vietnamese, Thai, Arabic, and more (29 total)\nUsage\nBelow, you will find instructions and examples for using ReaderLM-v2 locally using the Hugging Face Transformers library.\nFor a more hands-on experience in a hosted environment, see the Google Colab Notebook.\nVia Reader API\nReaderLM-v2 is now fully integrated with Reader API. To use it, simply specify x-engine: readerlm-v2 in your request headers and enable response streaming with -H 'Accept: text/event-stream':\ncurl https://r.jina.ai/https://news.ycombinator.com/ -H 'x-engine: readerlm-v2' -H 'Accept: text/event-stream'\nYou can try it without an API key at a lower rate limit. For higher rate limits, you can purchase an API key. Please note that ReaderLM-v2 requests consume 3x the normal token count from your API key allocation. This is currently an experimental feature, and we're working with the GCP team to improve GPU efficiency.\nOn Google Colab\nYou can try ReaderLM-v2 via our Colab notebook, which demonstrates HTML-to-markdown conversion, JSON extraction, and instruction-following using the HackerNews frontpage as an example. The notebook is optimized for Colab's free T4 GPU tier and requires vllm and triton for acceleration and running.\nNote that the free T4 GPU has limitations‚Äîit doesn't support bfloat16 or flash attention 2, leading to higher memory usage and slower processing of longer inputs. Nevertheless, ReaderLM-v2 successfully processes large documents under these constraints, achieving processing speeds of 67 tokens/s input and 36 tokens/s output. For production use, we recommend an RTX 3090/4090 for optimal performance.\nLocal Usage\nTo use ReaderLM-v2 locally:\nInstall the necessary dependencies:\npip install transformers\nLoad and run the model:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\"  # or \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/ReaderLM-v2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"jinaai/ReaderLM-v2\").to(device)\n(Optional) Pre-clean your HTML to remove scripts, styles, comments, to reduce the noise and length of the input:\nimport re\n# Patterns\nSCRIPT_PATTERN = r\"<[ ]*script.*?\\/[ ]*script[ ]*>\"\nSTYLE_PATTERN = r\"<[ ]*style.*?\\/[ ]*style[ ]*>\"\nMETA_PATTERN = r\"<[ ]*meta.*?>\"\nCOMMENT_PATTERN = r\"<[ ]*!--.*?--[ ]*>\"\nLINK_PATTERN = r\"<[ ]*link.*?>\"\nBASE64_IMG_PATTERN = r'<img[^>]+src=\"data:image/[^;]+;base64,[^\"]+\"[^>]*>'\nSVG_PATTERN = r\"(<svg[^>]*>)(.*?)(<\\/svg>)\"\ndef replace_svg(html: str, new_content: str = \"this is a placeholder\") -> str:\nreturn re.sub(\nSVG_PATTERN,\nlambda match: f\"{match.group(1)}{new_content}{match.group(3)}\",\nhtml,\nflags=re.DOTALL,\n)\ndef replace_base64_images(html: str, new_image_src: str = \"#\") -> str:\nreturn re.sub(BASE64_IMG_PATTERN, f'<img src=\"{new_image_src}\"/>', html)\ndef clean_html(html: str, clean_svg: bool = False, clean_base64: bool = False):\nhtml = re.sub(\nSCRIPT_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n)\nhtml = re.sub(\nSTYLE_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n)\nhtml = re.sub(\nMETA_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n)\nhtml = re.sub(\nCOMMENT_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n)\nhtml = re.sub(\nLINK_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n)\nif clean_svg:\nhtml = replace_svg(html)\nif clean_base64:\nhtml = replace_base64_images(html)\nreturn html\nCreate a prompt for the model:\ndef create_prompt(\ntext: str, tokenizer=None, instruction: str = None, schema: str = None\n) -> str:\n\"\"\"\nCreate a prompt for the model with optional instruction and JSON schema.\n\"\"\"\nif not instruction:\ninstruction = \"Extract the main content from the given HTML and convert it to Markdown format.\"\nif schema:\ninstruction = \"Extract the specified information from a list of news threads and present it in a structured JSON format.\"\nprompt = f\"{instruction}\\n\\nThe JSON schema is as follows:\"\nelse:\nprompt = f\"{instruction}\\n\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": prompt,\n}\n]\nreturn tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nHTML to Markdown Example\nhtml = \"<html><body><h1>Hello, world!</h1></body></html>\"\nhtml = clean_html(html)\ninput_prompt = create_prompt(html, tokenizer=tokenizer)\ninputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(\ninputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08\n)\nprint(tokenizer.decode(outputs[0]))\nHTML to JSON Example\nschema = \"\"\"\n{\n\"type\": \"object\",\n\"properties\": {\n\"title\": {\n\"type\": \"string\"\n},\n\"author\": {\n\"type\": \"string\"\n},\n\"date\": {\n\"type\": \"string\"\n},\n\"content\": {\n\"type\": \"string\"\n}\n},\n\"required\": [\"title\", \"author\", \"date\", \"content\"]\n}\n\"\"\"\nhtml = clean_html(html)\ninput_prompt = create_prompt(html, tokenizer=tokenizer, schema=schema)\ninputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(\ninputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08\n)\nprint(tokenizer.decode(outputs[0]))\nModel Performance\nReaderLM-v2 has been extensively evaluated on various tasks:\nQuantitative Evaluation\nFor HTML-to-Markdown tasks, the model outperforms much larger models like Qwen2.5-32B-Instruct and Gemini2-flash-expr, achieving:\nROUGE-L: 0.84\nLevenshtein Distance: 0.22\nJaro-Winkler Similarity: 0.82\nFor HTML-to-JSON tasks, it shows competitive performance with:\nF1 Score: 0.81\nPrecision: 0.82\nRecall: 0.81\nPass-Rate: 0.98\nQualitative Evaluation\nThe model excels in three key dimensions:\nContent Integrity: 39/50\nStructural Accuracy: 35/50\nFormat Compliance: 36/50\nThese scores demonstrate strong performance in preserving semantic information, maintaining structural accuracy, and adhering to markdown syntax standards.\nTraining Details\nReaderLM-v2 is built on Qwen2.5-1.5B-Instruction and trained using a sophisticated pipeline:\nData Preparation: Created html-markdown-1m dataset with 1 million HTML documents\nSynthetic Data Generation: Three-step pipeline using Qwen2.5-32B-Instruction\nDrafting: Initial markdown and JSON generation\nRefinement: Content cleanup and structure alignment\nCritique: Quality evaluation and filtering\nTraining Process:\nLong-context pretraining\nSupervised fine-tuning\nDirect preference optimization\nSelf-play reinforcement tuning",
    "tencent/Hunyuan3D-2": "üî• News\nAbstract\n‚òØÔ∏è Hunyuan3D 2.0\nArchitecture\nPerformance\nPretrained Models\nü§ó Get Started with Hunyuan3D 2.0\nInstall Requirements\nAPI Usage\nGradio App\nüìë Open-Source Plan\nüîó BibTeX\nCommunity Resources\nAcknowledgements\nStar History\n‚Äú Living out everyone‚Äôs imagination on creating and manipulating 3D assets.‚Äù\nThis repository contains the models of the paper Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation.\nFor code and more details on how to use it, refer to the Github repository.\nüî• News\nJan 21, 2025: üí¨ Release Hunyuan3D 2.0. Please give it a try!\nAbstract\nWe present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.\nThis system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale\ntexture synthesis model - Hunyuan3D-Paint.\nThe shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly\naligns with a given condition image, laying a solid foundation for downstream applications.\nThe texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant\ntexture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation\nprocess of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes\nefficiently.\nWe systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,\nincluding the open-source models and closed-source models in geometry details, condition alignment, texture quality, and\ne.t.c.\n‚òØÔ∏è Hunyuan3D 2.0\nArchitecture\nHunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the\nsynthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and\ntexture generation and also provides flexibility for texturing either generated or handcrafted meshes.\nPerformance\nWe have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.\nThe numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets\nand the condition following ability.\nModel\nCMMD(‚¨á)\nFID_CLIP(‚¨á)\nFID(‚¨á)\nCLIP-score(‚¨Ü)\nTop Open-source Model1\n3.591\n54.639\n289.287\n0.787\nTop Close-source Model1\n3.600\n55.866\n305.922\n0.779\nTop Close-source Model2\n3.368\n49.744\n294.628\n0.806\nTop Close-source Model3\n3.218\n51.574\n295.691\n0.799\nHunyuan3D 2.0\n3.193\n49.165\n282.429\n0.809\nGeneration results of Hunyuan3D 2.0:\nPretrained Models\nModel\nDate\nHuggingface\nHunyuan3D-DiT-v2-0\n2025-01-21\nDownload\nHunyuan3D-Paint-v2-0\n2025-01-21\nDownload\nHunyuan3D-Delight-v2-0\n2025-01-21\nDownload\nü§ó Get Started with Hunyuan3D 2.0\nYou may follow the next steps to use Hunyuan3D 2.0 via code or the Gradio App.\nInstall Requirements\nPlease install Pytorch via the official site. Then install the other requirements via\npip install -r requirements.txt\n# for texture\ncd hy3dgen/texgen/custom_rasterizer\npython3 setup.py install\ncd ../../..\ncd hy3dgen/texgen/differentiable_renderer\nbash compile_mesh_painter.sh OR python3 setup.py install (on Windows)\nAPI Usage\nWe designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -\nHunyuan3D-Paint.\nYou could assess Hunyuan3D-DiT via:\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(image='assets/demo.png')[0]\nThe output mesh is a trimesh object, which you could save to glb/obj (or other\nformat) file.\nFor Hunyuan3D-Paint, do the following:\nfrom hy3dgen.texgen import Hunyuan3DPaintPipeline\nfrom hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n# let's generate a mesh first\npipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(image='assets/demo.png')[0]\npipeline = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2')\nmesh = pipeline(mesh, image='assets/demo.png')\nPlease visit minimal_demo.py for more advanced usage, such as text to 3D and texture generation\nfor handcrafted mesh.\nGradio App\nYou could also host a Gradio App in your own computer via:\npip3 install gradio==3.39.0\npython3 gradio_app.py\nDon't forget to visit Hunyuan3D for quick use, if you don't want to host yourself.\nüìë Open-Source Plan\nInference Code\nModel Checkpoints\nTechnical Report\nComfyUI\nTensorRT Version\nüîó BibTeX\nIf you found this repository helpful, please cite our report:\n@misc{hunyuan3d22025tencent,\ntitle={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\nauthor={Tencent Hunyuan3D Team},\nyear={2025},\neprint={2501.12202},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@misc{yang2024tencent,\ntitle={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\nauthor={Tencent Hunyuan3D Team},\nyear={2024},\neprint={2411.02293},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nCommunity Resources\nThanks for the contributions of community members, here we have these great extensions of Hunyuan3D 2.0:\nComfyUI-Hunyuan3DWrapper\nHunyuan3D-2-for-windows\nüì¶ A bundle for running on Windows | Êï¥ÂêàÂåÖ\nAcknowledgements\nWe would like to thank the contributors to\nthe DINOv2, Stable Diffusion, FLUX, diffusers\nand HuggingFace repositories, for their open research and exploration.\nStar History",
    "Alpha-VLLM/Lumina-Image-2.0": "Gradio Demo\nUsage\nCitation\nLumina-Image-2.0 is a 2 billion parameter flow-based diffusion transformer capable of generating images from text descriptions. For more information, visit our GitHub.\nGradio Demo\nWe provide an official Gradio demo. You can use the link we provided to try it out.\nUsage\nimport torch\nfrom diffusers import Lumina2Pipeline\npipe = Lumina2Pipeline.from_pretrained(\"Alpha-VLLM/Lumina-Image-2.0\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\nprompt = \"A serene photograph capturing the golden reflection of the sun on a vast expanse of water. The sun is positioned at the top center, casting a brilliant, shimmering trail of light across the rippling surface. The water is textured with gentle waves, creating a rhythmic pattern that leads the eye towards the horizon. The entire scene is bathed in warm, golden hues, enhancing the tranquil and meditative atmosphere. High contrast, natural lighting, golden hour, photorealistic, expansive composition, reflective surface, peaceful, visually harmonious.\"\nimage = pipe(\nprompt,\nheight=1024,\nwidth=1024,\nguidance_scale=4.0,\nnum_inference_steps=50,\ncfg_trunc_ratio=0.25,\ncfg_normalization=True,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(\"lumina_demo.png\")\nThis is a Hugging Face Diffusers implementation of the paper Lumina-Image 2.0: A Unified and Efficient Image Generative Framework.\nCitation\nIf you find the provided code or models useful for your research, consider citing them as:\n@misc{lumina2,\nauthor={Qi Qin and Le Zhuo and Yi Xin and Ruoyi Du and Zhen Li and Bin Fu and Yiting Lu and Xinyue Li and Dongyang Liu and Xiangyang Zhu and Will Beddow and Erwann Millon and Victor Perez,Wenhai Wang and Yu Qiao and Bo Zhang and Xiaohong Liu and Hongsheng Li and Chang Xu and Peng Gao},\ntitle={Lumina-Image 2.0: A Unified and Efficient Image Generative Framework},\nyear={2025},\neprint={2503.21758},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/pdf/2503.21758},\n}",
    "deepseek-ai/Janus-Pro-7B": "1. Introduction\n2. Model Summary\n3. Quick Start\n4. License\n5. Citation\n6. Contact\n1. Introduction\nJanus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation.\nIt addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility.\nJanus-Pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\nThe simplicity, high flexibility, and effectiveness of Janus-Pro make it a strong candidate for next-generation unified multimodal models.\nGithub Repository\n2. Model Summary\nJanus-Pro is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation.\nJanus-Pro is constructed based on the DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base.\nFor multimodal understanding, it uses the SigLIP-L as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from here with a downsample rate of 16.\n3. Quick Start\nPlease refer to Github Repository\n4. License\nThis code repository is licensed under the MIT License. The use of Janus-Pro models is subject to DeepSeek Model License.\n5. Citation\n@article{chen2025janus,\ntitle={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},\nauthor={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},\njournal={arXiv preprint arXiv:2501.17811},\nyear={2025}\n}\n6. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "zed-industries/zeta": "Edit Prediction: Fine-Tuned from Qwen2.5-Coder-7B\nTraining Details\nDataset\nRunning Zeta\nvLLM - Simple\nvLLM - Advanced\nLearn More\nEdit Prediction: Fine-Tuned from Qwen2.5-Coder-7B\nThis repository contains a fine-tuned version of Qwen2.5-Coder-7B to support edit prediction in Zed.\nTraining Details\nThe model has been fine-tuned using the zeta dataset. If you want to fine-tune the model yourself, you can refer to the following scripts:\nDPO Fine-Tuning: View Notebook\nSFT Fine-Tuning: View Notebook\nDataset\nThe dataset used for training is available at:\nzed-industries/zeta\nRunning Zeta\nvLLM - Simple\nvllm serve zed-industries/zeta --served-model-name zeta\nvLLM - Advanced\nQuantization vLLM supports FP8 (8-bit floating point) weight and activation quantization using hardware acceleration on GPUs such as Nvidia H100 and AMD MI300x.\nNGram Speculative Decoding configures vLLM to use\nspeculative decoding where proposals are generated by matching n-grams in the prompt. This is a great fit for edit predictions since many of the tokens are already present in the prompt and\nthe model is only needed to generate changes to the code file.\nvllm serve zed-industries/zeta --served-model-name zeta --enable-prefix-caching --enable-chunked-prefill --quantization=\"fp8\" --speculative-model [ngram] --ngram-prompt-lookup-max 4 --ngram-prompt-lookup-min 2 --num-speculative-tokens 8\nLearn More\nFor more insights about the model and its integration in Zed, check out the official blog post:\nZed Blog - Edit Prediction",
    "agents-course/notebooks": "üìö Notebook Index\nNotebooks part of the Hugging Face Agents Course.\nüìö Notebook Index\nUnit\nNotebook Name\nRedirect Link\nUnit 1\nDummy Agent Library\n‚Üó\nUnit 2.1 - smolagents\nCode Agents\n‚Üó\nUnit 2.1 - smolagents\nMulti-agent Notebook\n‚Üó\nUnit 2.1 - smolagents\nRetrieval Agents\n‚Üó\nUnit 2.1 - smolagents\nTool Calling Agents\n‚Üó\nUnit 2.1 - smolagents\nTools\n‚Üó\nUnit 2.1 - smolagents\nVision Agents\n‚Üó\nUnit 2.1 - smolagents\nVision Web Browser\n‚Üó\nUnit 2.2 - LlamaIndex\nAgents\n‚Üó\nUnit 2.2 - LlamaIndex\nComponents\n‚Üó\nUnit 2.2 - LlamaIndex\nTools\n‚Üó\nUnit 2.2 - LlamaIndex\nWorkflows\n‚Üó\nUnit 2.3 - LangGraph\nAgent\n‚Üó\nUnit 2.3 - LangGraph\nMail Sorting\n‚Üó\nBonus Unit 1\nGemma SFT & Thinking Function Call\n‚Üó\nBonus Unit 2\nMonitoring & Evaluating Agents\n‚Üó"
}