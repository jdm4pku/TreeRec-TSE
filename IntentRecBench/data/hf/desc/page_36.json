{
    "unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF": "Llama-3.1-Nemotron-Nano-8B-v1\nModel Overview\nLicense/Terms of Use\nUse Case:\nRelease Date:\nReferences\nModel Architecture\nIntended use\nInput:\nOutput:\nModel Version:\nSoftware Integration\nQuick Start and Usage Recommendations:\nExample of ‚ÄúReasoning On:‚Äù\nExample of ‚ÄúReasoning Off:‚Äù\nInference:\nTraining Datasets\nEvaluation Datasets\nEvaluation Results\nMT-Bench\nMATH500\nAIME25\nGPQA-D\nIFEval Average\nBFCL v2 Live\nMBPP 0-shot\nEthical Considerations:\nCitation\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nLlama-3.1-Nemotron-Nano-8B-v1\nModel Overview\nLlama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.\nLlama-3.1-Nemotron-Nano-8B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. It is created from Llama 3.1 8B Instruct and offers improvements in model accuracy. The model fits on a single RTX GPU and can be used locally. The model supports a context length of 128K.\nThis model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using REINFORCE (RLOO) and Online Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and Online RPO checkpoints. Improved using Qwen.\nThis model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here:\nLlama-3.3-Nemotron-Super-49B-v1\nThis model is ready for commercial use.\nLicense/Terms of Use\nGOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.1 Community License Agreement. Built with Llama.\nModel Developer: NVIDIA\nModel Dates: Trained between August 2024 and March 2025\nData Freshness: The pretraining data has a cutoff of 2023 per Meta Llama 3.1 8B\nUse Case:\nDevelopers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Balance of model accuracy and compute efficiency (the model fits on a single RTX GPU and can be used locally).\nRelease Date:\n3/18/2025\nReferences\n[2505.00949] Llama-Nemotron: Efficient Reasoning Models\n[2502.00203] Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment\nModel Architecture\nArchitecture Type: Dense decoder-only Transformer model\nNetwork Architecture: Llama 3.1 8B Instruct\nIntended use\nLlama-3.1-Nemotron-Nano-8B-v1 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported.\nInput:\nInput Type: Text\nInput Format: String\nInput Parameters: One-Dimensional (1D)\nOther Properties Related to Input: Context length up to 131,072 tokens\nOutput:\nOutput Type: Text\nOutput Format: String\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output: Context length up to 131,072 tokens\nModel Version:\n1.0 (3/18/2025)\nSoftware Integration\nRuntime Engine: NeMo 24.12\nRecommended Hardware Microarchitecture Compatibility:\nNVIDIA Hopper\nNVIDIA Ampere\nQuick Start and Usage Recommendations:\nReasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt\nWe recommend setting temperature to 0.6, and Top P to 0.95 for Reasoning ON mode\nWe recommend using greedy decoding for Reasoning OFF mode\nWe have provided a list of prompts to use for evaluation for each benchmark where a specific template is required\nThe model will include <think></think> if no reasoning was necessary in Reasoning ON model, this is expected behaviour\nYou can try this model out through the preview API, using this link: Llama-3.1-Nemotron-Nano-8B-v1.\nSee the snippet below for usage with Hugging Face Transformers library. Reasoning mode (ON/OFF) is controlled via system prompt. Please see the example below.\nOur code requires the transformers package version to be 4.44.2 or higher.\nExample of ‚ÄúReasoning On:‚Äù\nimport torch\nimport transformers\nmodel_id = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"}\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\ntokenizer=tokenizer,\nmax_new_tokens=32768,\ntemperature=0.6,\ntop_p=0.95,\n**model_kwargs\n)\n# Thinking can be \"on\" or \"off\"\nthinking = \"on\"\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}]))\nExample of ‚ÄúReasoning Off:‚Äù\nimport torch\nimport transformers\nmodel_id = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"}\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\ntokenizer=tokenizer,\nmax_new_tokens=32768,\ndo_sample=False,\n**model_kwargs\n)\n# Thinking can be \"on\" or \"off\"\nthinking = \"off\"\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}]))\nFor some prompts, even though thinking is disabled, the model emergently prefers to think before responding. But if desired, the users can prevent it by pre-filling the assistant response.\nimport torch\nimport transformers\nmodel_id = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"}\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n# Thinking can be \"on\" or \"off\"\nthinking = \"off\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\ntokenizer=tokenizer,\nmax_new_tokens=32768,\ndo_sample=False,\n**model_kwargs\n)\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}, {\"role\":\"assistant\", \"content\":\"<think>\\n</think>\"}]))\nInference:\nEngine: Transformers\nTest Hardware:\nBF16:\n1x RTX 50 Series GPUs\n1x RTX 40 Series GPUs\n1x RTX 30 Series GPUs\n1x H100-80GB GPU\n1x A100-80GB GPU\nPreferred/Supported] Operating System(s): Linux\nTraining Datasets\nA large variety of training data was used for the post-training pipeline, including manually annotated data and synthetic data.\nThe data for the multi-stage post-training phases for improvements in Code, Math, and Reasoning is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model.\nPrompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both Reasoning On and Off modes, to train the model to distinguish between two modes.\nData Collection for Training Datasets:\nHybrid: Automated, Human, Synthetic\nData Labeling for Training Datasets:\nN/A\nEvaluation Datasets\nWe used the datasets listed below to evaluate Llama-3.1-Nemotron-Nano-8B-v1.\nData Collection for Evaluation Datasets: Hybrid: Human/Synthetic\nData Labeling for Evaluation Datasets: Hybrid: Human/Synthetic/Automatic\nEvaluation Results\nThese results contain both ‚ÄúReasoning On‚Äù, and ‚ÄúReasoning Off‚Äù. We recommend using temperature=0.6, top_p=0.95 for ‚ÄúReasoning On‚Äù mode, and greedy decoding for ‚ÄúReasoning Off‚Äù mode. All evaluations are done with 32k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate.\nNOTE: Where applicable, a Prompt Template will be provided. While completing benchmarks, please ensure that you are parsing for the correct output format as per the provided prompt in order to reproduce the benchmarks seen below.\nMT-Bench\nReasoning Mode\nScore\nReasoning Off\n7.9\nReasoning On\n8.1\nMATH500\nReasoning Mode\npass@1\nReasoning Off\n36.6%\nReasoning On\n95.4%\nUser Prompt Template:\n\"Below is a math question. I want you to reason through the steps and then give a final answer. Your final answer should be in \\boxed{}.\\nQuestion: {question}\"\nAIME25\nReasoning Mode\npass@1\nReasoning Off\n0%\nReasoning On\n47.1%\nUser Prompt Template:\n\"Below is a math question. I want you to reason through the steps and then give a final answer. Your final answer should be in \\boxed{}.\\nQuestion: {question}\"\nGPQA-D\nReasoning Mode\npass@1\nReasoning Off\n39.4%\nReasoning On\n54.1%\nUser Prompt Template:\n\"What is the correct answer to this question: {question}\\nChoices:\\nA. {option_A}\\nB. {option_B}\\nC. {option_C}\\nD. {option_D}\\nLet's think step by step, and put the final answer (should be a single letter A, B, C, or D) into a \\boxed{}\"\nIFEval Average\nReasoning Mode\nStrict:Prompt\nStrict:Instruction\nReasoning Off\n74.7%\n82.1%\nReasoning On\n71.9%\n79.3%\nBFCL v2 Live\nReasoning Mode\nScore\nReasoning Off\n63.9%\nReasoning On\n63.6%\nUser Prompt Template:\n<AVAILABLE_TOOLS>{functions}</AVAILABLE_TOOLS>\n{user_prompt}\nMBPP 0-shot\nReasoning Mode\npass@1\nReasoning Off\n66.1%\nReasoning On\n84.6%\nUser Prompt Template:\nYou are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n@@ Instruction\nHere is the given problem and test examples:\n{prompt}\nPlease use the python programming language to solve this problem.\nPlease make sure that your code includes the functions from the test samples and that the input and output formats of these functions match the test samples.\nPlease return all completed codes in one code block.\nThis code block should be in the following format:\n\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nCitation\n@misc{bercovich2025llamanemotronefficientreasoningmodels,\ntitle={Llama-Nemotron: Efficient Reasoning Models},\nauthor={Akhiad Bercovich and Itay Levy and Izik Golan and Mohammad Dabbah and Ran El-Yaniv and Omri Puny and Ido Galil and Zach Moshe and Tomer Ronen and Najeeb Nabwani and Ido Shahaf and Oren Tropp and Ehud Karpas and Ran Zilberstein and Jiaqi Zeng and Soumye Singhal and Alexander Bukharin and Yian Zhang and Tugrul Konuk and Gerald Shen and Ameya Sunil Mahabaleshwarkar and Bilal Kartal and Yoshi Suhara and Olivier Delalleau and Zijia Chen and Zhilin Wang and David Mosallanezhad and Adi Renduchintala and Haifeng Qian and Dima Rekesh and Fei Jia and Somshubra Majumdar and Vahid Noroozi and Wasi Uddin Ahmad and Sean Narenthiran and Aleksander Ficek and Mehrzad Samadi and Jocelyn Huang and Siddhartha Jain and Igor Gitman and Ivan Moshkov and Wei Du and Shubham Toshniwal and George Armstrong and Branislav Kisacanin and Matvei Novikov and Daria Gitman and Evelina Bakhturina and Jane Polak Scowcroft and John Kamalu and Dan Su and Kezhi Kong and Markus Kliegl and Rabeeh Karimi and Ying Lin and Sanjeev Satheesh and Jupinder Parmar and Pritam Gundecha and Brandon Norick and Joseph Jennings and Shrimai Prabhumoye and Syeda Nahida Akter and Mostofa Patwary and Abhinav Khattar and Deepak Narayanan and Roger Waleffe and Jimmy Zhang and Bor-Yiing Su and Guyue Huang and Terry Kong and Parth Chadha and Sahil Jain and Christine Harvey and Elad Segal and Jining Huang and Sergey Kashirsky and Robert McQueen and Izzy Putterman and George Lam and Arun Venkatesan and Sherry Wu and Vinh Nguyen and Manoj Kilaru and Andrew Wang and Anna Warno and Abhilash Somasamudramath and Sandip Bhaskar and Maka Dong and Nave Assaf and Shahar Mor and Omer Ullman Argov and Scot Junkin and Oleksandr Romanenko and Pedro Larroy and Monika Katariya and Marco Rovinelli and Viji Balas and Nicholas Edelman and Anahita Bhiwandiwalla and Muthu Subramaniam and Smita Ithape and Karthik Ramamoorthy and Yuting Wu and Suguna Varshini Velury and Omri Almog and Joyjit Daw and Denys Fridman and Erick Galinkin and Michael Evans and Katherine Luna and Leon Derczynski and Nikki Pope and Eileen Long and Seth Schneider and Guillermo Siman and Tomasz Grzegorzek and Pablo Ribalta and Monika Katariya and Joey Conway and Trisha Saar and Ann Guan and Krzysztof Pawelec and Shyamala Prayaga and Oleksii Kuchaiev and Boris Ginsburg and Oluwatobi Olabiyi and Kari Briski and Jonathan Cohen and Bryan Catanzaro and Jonah Alben and Yonatan Geifman and Eric Chung and Chris Alexiuk},\nyear={2025},\neprint={2505.00949},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.00949},\n}",
    "Intelligent-Internet/II-Medical-8B": "II-Medical-8B\nI. Model Overview\nII. Training Methodology\nIII. Evaluation Results\nIV. Dataset Curation\n1. Public Medical Reasoning Datasets (103,031 samples)\n2. Synthetic Medical QA Data with QwQ (225,700 samples)\n3. Curated Medical R1 Traces (338,055 samples)\n4. Supplementary Math Dataset\nPreprocessing Data\nData Decontamination\nV. How To Use\nVI. Usage Guidelines\nVII. Limitations and Considerations\nVIII. Citation\nII-Medical-8B\nI. Model Overview\nII-Medical-8B is the newest advanced large language model developed by Intelligent Internet, specifically engineered to enhance AI-driven medical reasoning. Following the positive reception of our previous II-Medical-7B-Preview, this new iteration significantly advances the capabilities of medical question answering,\nII. Training Methodology\nWe collected and generated a comprehensive set of reasoning datasets for the medical domain and performed SFT fine-tuning on the Qwen/Qwen3-8B model. Following this, we further optimized the SFT model by training DAPO on a hard-reasoning dataset to boost performance.\nFor SFT stage we using the hyperparameters:\nMax Length: 16378.\nBatch Size: 128.\nLearning-Rate: 5e-5.\nNumber Of Epoch: 8.\nFor RL stage we setup training with:\nMax prompt length: 2048 tokens.\nMax response length: 12288 tokens.\nOverlong buffer: Enabled, 4096 tokens, penalty factor 1.0.\nClip ratios: Low 0.2, High 0.28.\nBatch sizes: Train prompt 512, Generation prompt 1536, Mini-batch 32.\nResponses per prompt: 16.\nTemperature: 1.0, Top-p: 1.0, Top-k: -1 (vLLM rollout).\nLearning rate: 1e-6, Warmup steps: 10, Weight decay: 0.1.\nLoss aggregation: Token-mean.\nGradient clipping: 1.0.\nEntropy coefficient: 0.\nIII. Evaluation Results\nOur II-Medical-8B model achieved a 40% score on HealthBench, a comprehensive open-source benchmark evaluating the performance and safety of large language models in healthcare. This performance is comparable to OpenAI's o1 reasoning model and GPT-4.5, OpenAI's largest and most advanced model to date. We provide a comparison to models available in ChatGPT below.\nDetailed result for HealthBench can be found here.\nWe evaluate on ten medical QA benchmarks include MedMCQA, MedQA, PubMedQA, medical related questions from MMLU-Pro and GPQA, small QA sets from Lancet and the New England\nJournal of Medicine,  4 Options  and 5 Options splits from the MedBullets platform and MedXpertQA.\nModel\nMedMC\nMedQA\nPubMed\nMMLU-P\nGPQA\nLancet\nMedB-4\nMedB-5\nMedX\nNEJM\nAvg\nHuatuoGPT-o1-72B\n76.76\n88.85\n79.90\n80.46\n64.36\n70.87\n77.27\n73.05\n23.53\n76.29\n71.13\nQWQ 32B\n69.73\n87.03\n88.5\n79.86\n69.17\n71.3\n72.07\n69.01\n24.98\n75.12\n70.68\nQwen2.5-7B-IT\n56.56\n61.51\n71.3\n61.17\n42.56\n61.17\n46.75\n40.58\n13.26\n59.04\n51.39\nHuatuoGPT-o1-8B\n63.97\n74.78\n80.10\n63.71\n55.38\n64.32\n58.44\n51.95\n15.79\n64.84\n59.32\nMed-reason\n61.67\n71.87\n77.4\n64.1\n50.51\n59.7\n60.06\n54.22\n22.87\n66.8\n59.92\nM1\n62.54\n75.81\n75.80\n65.86\n53.08\n62.62\n63.64\n59.74\n19.59\n64.34\n60.3\nII-Medical-8B-SFT\n71.92\n86.57\n77.4\n77.26\n65.64\n69.17\n76.30\n67.53\n23.79\n73.80\n68.80\nII-Medical-8B\n71.57\n87.82\n78.2\n80.46\n67.18\n70.38\n78.25\n72.07\n25.26\n73.13\n70.49\nIV. Dataset Curation\nThe training dataset comprises 555,000 samples from the following sources:\n1. Public Medical Reasoning Datasets (103,031 samples)\nGeneral Medical Reasoning: 40,544 samples\nMedical-R1-Distill-Data: 22,000 samples\nMedical-R1-Distill-Data-Chinese: 17,000 samples\nUCSC-VLAA/m23k-tokenized: 23,487 samples\n2. Synthetic Medical QA Data with QwQ (225,700 samples)\nGenerated from established medical datasets:\nMedMcQA (from openlifescienceai/medmcqa): 183,000 samples\nMedQA: 10,000 samples\nMedReason: 32,700 samples\n3. Curated Medical R1 Traces (338,055 samples)\nFirst we gather all the public R1 traces from:\nPrimeIntellect/SYNTHETIC-1\nGeneralReasoning/GeneralThought-430K\na-m-team/AM-DeepSeek-R1-Distilled-1.4M\nopen-thoughts/OpenThoughts2-1M\nnvidia/Llama-Nemotron-Post-Training-Dataset: Science subset only\nOther resources: cognitivecomputations/dolphin-r1, ServiceNow-AI/R1-Distill-SFT,...\nAll R1 reasoning traces were processed through a domain-specific pipeline as follows:\nEmbedding Generation: Prompts are embedded using sentence-transformers/all-MiniLM-L6-v2.\nClustering: Perform K-means clustering with 50,000 clusters.\nDomain Classification:\nFor each cluster, select the 10 prompts nearest to the cluster center.\nClassify the domain of each selected prompt using Qwen2.5-32b-Instruct.\nAssign the cluster's domain based on majority voting among the classified prompts.\nDomain Filtering: Keep only clusters labeled as Medical or Biology for the final dataset.\n4. Supplementary Math Dataset\nAdded 15,000 samples of reasoning traces from light-r1\nPurpose: Enhance general reasoning capabilities of the model\nPreprocessing Data\nFiltering for Complete Generation\nRetained only traces with complete generation outputs\nLength-based Filtering\nMinimum threshold: Keep only the prompt with more than 3 words.\nWait Token Filter: Removed traces with has more than 47 occurrences of \"Wait\" (97th percentile threshold).\nData Decontamination\nWe using two step decontamination:\nFollowing open-r1 project: We decontaminate a dataset using 10-grams with the evaluation datasets.\nAfter that, we using the fuzzy decontamination from s1k method with threshold 90%.\nOur pipeline is carefully decontaminated with the evaluation datasets.\nV. How To Use\nOur model can be utilized in the same manner as Qwen or Deepseek-R1-Distill models.\nFor instance, you can easily start a service using vLLM:\nvllm serve Intelligent-Internet/II-Medical-8B\nYou can also easily start a service using SGLang:\npython -m sglang.launch_server --model Intelligent-Internet/II-Medical-8B\nVI. Usage Guidelines\nRecommended Sampling Parameters: temperature = 0.6, top_p = 0.9\nWhen using, explicitly request step-by-step reasoning and format the final answer within \\boxed{} (e.g., \"Please reason step-by-step, and put your final answer within \\boxed{}.\").\nVII. Limitations and Considerations\nDataset may contain inherent biases from source materials\nMedical knowledge requires regular updates\nPlease note that It‚Äôs not suitable for medical use.\nVIII. Citation\n@misc{2025II-Medical-8B,\ntitle={II-Medical-8B: Medical Reasoning Model},\nauthor={Intelligent Internet},\nyear={2025}\n}",
    "mistralai/Devstral-Small-2505": "Devstral Small 1.0\nKey Features:\nBenchmark Results\nSWE-Bench\nUsage\nAPI\nLocal inference\nOpenHands (recommended)\nvLLM (recommended)\nMistral-inference\nTransformers\nLMStudio\nllama.cpp\nOllama\nExample: Understanding Test Coverage of Mistral Common\nDevstral Small 1.0\nDevstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI üôå. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model on this benchmark.\nIt is finetuned from Mistral-Small-3.1, therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from Mistral-Small-3.1 the vision encoder was removed.\nFor enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\nLearn more about Devstral in our blog post.\nKey Features:\nAgentic coding: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\nlightweight: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 128k context window.\nTokenizer: Utilizes a Tekken tokenizer with a 131k vocabulary size.\nBenchmark Results\nSWE-Bench\nDevstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA by 6%.\nModel\nScaffold\nSWE-Bench Verified (%)\nDevstral\nOpenHands Scaffold\n46.8\nGPT-4.1-mini\nOpenAI Scaffold\n23.6\nClaude 3.5 Haiku\nAnthropic Scaffold\n40.6\nSWE-smith-LM 32B\nSWE-agent Scaffold\n40.2\nWhen evaluated under the same test scaffold (OpenHands, provided by All Hands AI üôå), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\nUsage\nWe recommend to use Devstral with the OpenHands scaffold.\nYou can use it either through our API or by running locally.\nAPI\nFollow these instructions to create a Mistral account and get an API key.\nThen run these commands to start the OpenHands docker container.\nexport MISTRAL_API_KEY=<MY_KEY>\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik\nmkdir -p ~/.openhands-state && echo '{\"language\":\"en\",\"agent\":\"CodeActAgent\",\"max_iterations\":null,\"security_analyzer\":null,\"confirmation_mode\":false,\"llm_model\":\"mistral/devstral-small-2505\",\"llm_api_key\":\"'$MISTRAL_API_KEY'\",\"remote_runtime_resource_factor\":null,\"github_token\":null,\"enable_default_condenser\":true}' > ~/.openhands-state/settings.json\ndocker run -it --rm --pull=always \\\n-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik \\\n-e LOG_ALL_EVENTS=true \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v ~/.openhands-state:/.openhands-state \\\n-p 3000:3000 \\\n--add-host host.docker.internal:host-gateway \\\n--name openhands-app \\\ndocker.all-hands.dev/all-hands-ai/openhands:0.39\nLocal inference\nThe model can also be deployed with the following libraries:\nvllm (recommended): See here\nmistral-inference: See here\ntransformers: See here\nLMStudio: See here\nllama.cpp: See here\nollama: See here\nOpenHands (recommended)\nLaunch a server to deploy Devstral Small 1.0\nMake sure you launched an OpenAI-compatible server such as vLLM or Ollama as described above. Then, you can use OpenHands to interact with Devstral Small 1.0.\nIn the case of the tutorial we spineed up a vLLM server running the command:\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\nThe server address should be in the following format: http://<your-server-url>:8000/v1\nLaunch OpenHands\nYou can follow installation of OpenHands here.\nThe easiest way to launch OpenHands is to use the Docker image:\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\ndocker run -it --rm --pull=always \\\n-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\\n-e LOG_ALL_EVENTS=true \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v ~/.openhands-state:/.openhands-state \\\n-p 3000:3000 \\\n--add-host host.docker.internal:host-gateway \\\n--name openhands-app \\\ndocker.all-hands.dev/all-hands-ai/openhands:0.38\nThen, you can access the OpenHands UI at http://localhost:3000.\nConnect to the server\nWhen accessing the OpenHands UI, you will be prompted to connect to a server. You can use the advanced mode to connect to the server you launched earlier.\nFill the following fields:\nCustom Model: openai/mistralai/Devstral-Small-2505\nBase URL: http://<your-server-url>:8000/v1\nAPI Key: token (or any other token you used to launch the server if any)\nUse OpenHands powered by Devstral\nNow you're good to use Devstral Small inside OpenHands by starting a new conversation. Let's build a To-Do list app.\nTo-Do list app\nLet's ask Devstral to generate the app with the following prompt:\nBuild a To-Do list app with the following requirements:\n- Built using FastAPI and React.\n- Make it a one page app that:\n- Allows to add a task.\n- Allows to delete a task.\n- Allows to mark a task as done.\n- Displays the list of tasks.\n- Store the tasks in a SQLite database.\nLet's see the result\nYou should see the agent construct the app and be able to explore the code it generated.\nIf it doesn't do it automatically, ask Devstral to deploy the app or do it manually, and then go the front URL deployment to see the app.\nIterate\nNow that you have a first result you can iterate on it by asking your agent to improve it. For example, in the app generated we could click on a task to mark it checked but having a checkbox would improve UX. You could also ask it to add a feature to edit a task, or to add a feature to filter the tasks by status.\nEnjoy building with Devstral Small and OpenHands!\nvLLM (recommended)\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nInstallation\nMake sure you install vLLM >= 0.8.5:\npip install vllm --upgrade\nDoing so should automatically install mistral_common >= 1.5.5.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServer\nWe recommand that you use Devstral in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\nTo ping the client you can use a simple Python snippet.\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nurl = \"http://<your-server-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Devstral-Small-2505\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"<your-command>\",\n},\n],\n},\n]\ndata = {\"model\": model, \"messages\": messages, \"temperature\": 0.15}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\nMistral-inference\nWe recommend using mistral-inference to quickly try out / \"vibe-check\" Devstral.\nInstall\nMake sure to have mistral_inference >= 1.6.0 installed.\npip install mistral_inference --upgrade\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', 'Devstral')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Devstral-Small-2505\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\nPython\nYou can run the model using the following command:\nmistral-chat $HOME/mistral_models/Devstral --instruct --max_tokens 300\nYou can then prompt it with anything you'd like.\nTransformers\nTo make the best use of our model with transformers make sure to have installed     mistral-common >= 1.5.5 to use our tokenizer.\npip install mistral-common --upgrade\nThen load our tokenizer along with the model and generate:\nimport torch\nfrom mistral_common.protocol.instruct.messages import (\nSystemMessage, UserMessage\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoModelForCausalLM\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Devstral-Small-2505\"\ntekken_file = hf_hub_download(repo_id=model_id, filename=\"tekken.json\")\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\ntokenizer = MistralTokenizer.from_file(tekken_file)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntokenized = tokenizer.encode_chat_completion(\nChatCompletionRequest(\nmessages=[\nSystemMessage(content=SYSTEM_PROMPT),\nUserMessage(content=\"<your-command>\"),\n],\n)\n)\noutput = model.generate(\ninput_ids=torch.tensor([tokenized.tokens]),\nmax_new_tokens=1000,\n)[0]\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens):])\nprint(decoded_output)\nLMStudio\nDownload the weights from huggingface:\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"mistralai/Devstral-Small-2505_gguf\" \\\n--include \"devstralQ4_K_M.gguf\" \\\n--local-dir \"mistralai/Devstral-Small-2505_gguf/\"\nYou can serve the model locally with LMStudio.\nDownload LM Studio and install it\nInstall lms cli ~/.lmstudio/bin/lms bootstrap\nIn a bash terminal, run lms import devstralQ4_K_M.gguf in the directory where you've downloaded the model checkpoint (e.g. mistralai/Devstral-Small-2505_gguf)\nOpen the LMStudio application, click the terminal icon to get into the developer tab. Click select a model to load and select Devstral Q4 K M. Toggle the status button to start the model, in setting toggle Serve on Local Network to be on.\nOn the right tab, you will see an API identifier which should be devstralq4_k_m and an api address under API Usage. Keep note of this address, we will use it in the next step.\nLaunch Openhands\nYou can now interact with the model served from LM Studio with openhands. Start the openhands server with the docker\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\ndocker run -it --rm --pull=always \\\n-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\\n-e LOG_ALL_EVENTS=true \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v ~/.openhands-state:/.openhands-state \\\n-p 3000:3000 \\\n--add-host host.docker.internal:host-gateway \\\n--name openhands-app \\\ndocker.all-hands.dev/all-hands-ai/openhands:0.38\nClick ‚Äúsee advanced setting‚Äù on the second line.\nIn the new tab, toggle advanced to on. Set the custom model to be mistral/devstralq4_k_m and Base URL the api address we get from the last step in LM Studio. Set API Key to dummy. Click save changes.\nllama.cpp\nDownload the weights from huggingface:\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download \\\n\"mistralai/Devstral-Small-2505_gguf\" \\\n--include \"devstralQ4_K_M.gguf\" \\\n--local-dir \"mistralai/Devstral-Small-2505_gguf/\"\nThen run Devstral using the llama.cpp CLI.\n./llama-cli -m Devstral-Small-2505_gguf/devstralQ4_K_M.gguf -cnv\nOllama\nYou can run Devstral using the Ollama CLI.\nollama run devstral\nExample: Understanding Test Coverage of Mistral Common\nWe can start the OpenHands scaffold and link it to a repo to analyze test coverage and identify badly covered files.\nHere we start with our public mistral-common repo.\nAfter the repo is mounted in the workspace, we give the following instruction\nCheck the test coverage of the repo and then create a visualization of test coverage. Try plotting a few different types of graphs and save them to a png.\nThe agent will first browse the code base to check test configuration and structure.\nThen it sets up the testing dependencies and launches the coverage test:\nFinally, the agent writes necessary code to visualize the coverage.\nAt the end of the run, the following plots are produced:",
    "TEN-framework/ten-vad": "Table of Contents\nWelcome to TEN\nTEN Hugging Face Space\nIntroduction\nKey Features\n1. High-Performance:\n2. Agent-Friendly:\n3. Lightweight:\n4. Multiple programming languages and platforms:\n5. Supproted sampling rate and hop size:\nInstallation\nQuick Start\nPython Usage\n1. Linux / macOS / Windows\nRequirements\nUsage\nJS Usage\n1. Web\nC Usage\nBuild Scripts\nDynamic Library Configuration\nCustomization\nOverview of Usage\n1. Linux\n2. Windows\n3. macOS\n4. Android\n5. iOS\nTEN Ecosystem\nAsk Questions\nCitations\nLicense\nLatest News üî•\n[2025/07] We support Python inference on macOS and Windows with usage of the prebuilt-lib!\n[2025/06] We finally released and open-sourced the ONNX model and the corresponding preprocessing code! Now you can deploy TEN VAD on any platform and any hardware architecture!\n[2025/06] We are excited to announce the release of WASM+JS for Web WASM Support.\nTable of Contents\nWelcome to TEN\nTEN Hugging Face Space\nIntroduction\nKey Features\nHigh-Performance\nAgent-Friendly\nLightweight\nMultiple Programming Languages and Platforms\nSupported Sampling Rate and Hop Size\nInstallation\nQuick Start\nPython Usage\nLinux\nJS Usage\nWeb\nC Usage\nLinux\nWindows\nmacOS\nAndroid\niOS\nTEN Ecosystem\nAsk Questions\nCitations\nLicense\nWelcome to TEN\nTEN is a collection of open-source projects for building real-time, multimodal conversational voice agents. It includes  TEN Framework ,  TEN VAD ,  TEN Turn Detection , TEN Agent, TMAN Designer, and  TEN Portal , all fully open-source.\nCommunity Channel\nPurpose\nFollow TEN Framework on X for updates and announcements\nFollow TEN Framework on LinkedIn for updates and announcements\nJoin our Discord community to connect with developers\nJoin our Hugging Face community to explore our spaces and models\nJoin our WeChat group for Chinese community discussions\nStar TEN Repositories ‚≠êÔ∏è\nGet instant notifications for new releases and updates. Your support helps us grow and improve TEN!\nTEN Hugging Face Space\nhttps://github.com/user-attachments/assets/725a8318-d679-4b17-b9e4-e3dce999b298\nYou are more than welcome to Visit TEN Hugging Face Space to try VAD and Turn Detection together.\nIntroduction\nTEN VAD is a real-time voice activity detection system designed for enterprise use,  providing accurate frame-level speech activity detection. It shows superior precision compared to both WebRTC VAD and Silero VAD, which are commonly used in the industry. Additionally, TEN VAD offers lower computational complexity and reduced memory usage compared to Silero VAD. Meanwhile, the architecture's temporal efficiency enables rapid voice activity detection, significantly reducing end-to-end response and turn detection latency in conversational AI systems.\nKey Features\n1. High-Performance:\nThe precision-recall curves comparing the performance of WebRTC VAD (pitch-based), Silero VAD, and TEN VAD are shown below. The evaluation is conducted on the precisely manually annotated testset. The audio files are from librispeech, gigaspeech, DNS Challenge etc. As demonstrated, TEN VAD achieves the best performance. Additionally, cross-validation experiments conducted on large internal real-world datasets demonstrate the reproducibility of these findings. The testset with annotated labels is released in directory \"testset\" of this repository.\nNote that the default threshold of 0.5 is used to generate binary speech indicators (0 for non-speech signal, 1 for speech signal). This threshold needs to be tuned according to your domain-specific task. The precision-recall curve can be obtained by executing the following script on Linux x64. The output figure will be saved in the same directory as the script.\ncd ./examples\npython plot_pr_curves.py\n2. Agent-Friendly:\nAs illustrated in the figure below, TEN VAD rapidly detects speech-to-non-speech transitions, whereas Silero VAD suffers from a delay of several hundred milliseconds, resulting in increased end-to-end latency in human-agent interaction systems. In addition, as demonstrated in the 6.5s-7.0s audio segment, Silero VAD fails to identify short silent durations between adjacent speech segments.\n3. Lightweight:\nWe evaluated the RTF (Real-Time Factor) across five distinct platforms, each equipped with varying CPUs. TEN VAD demonstrates much lower computational complexity and smaller library size than Silero VAD.\nPlatform\nCPU\nRTF\nLib Size\nTEN VAD\nSilero VAD\nTEN VAD\nSilero VAD\nLinux\nAMD Ryzen 9 5900X 12-Core\n0.0150\n/\n306KB\n2.16MB(JIT) / 2.22MB(ONNX)\nIntel(R) Xeon(R) Platinum 8253\n0.0136\nIntel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\n0.0086\n0.0127\nWindows\nIntel i7-10710U\n0.0150\n/\n464KB(x86) / 508KB(x64)\nmacOS\nM1\n0.0160\n731KB\nAndroid\nGalaxy J6+ (32bit, 425)\n0.0570\n373KB(v7a) / 532KB(v8a)\nOppo A3s (450)\n0.0490\niOS\niPhone6 (A8)\n0.0210\n320KB\niPhone8 (A11)\n0.0050\n4. Multiple programming languages and platforms:\nTEN VAD provides cross-platform C compatibility across five operating systems (Linux x64, Windows, macOS, Android, iOS), with Python bindings optimized for Linux x64, with wasm for Web.\n5. Supproted sampling rate and hop size:\nTEN VAD operates on 16kHz audio input with configurable hop sizes (optimized frame configurations: 160/256 samples=10/16ms). Other sampling rates must be resampled to 16kHz.\nInstallation\ngit clone https://huggingface.co/TEN-framework/ten-vad\nQuick Start\nThe project supports five major platforms with dynamic library linking.\nPlatform\nDynamic Lib\nSupported Arch\nInterface Language\nHeader\nComment\nLinux\nlibten_vad.so\nx64\nPython, C\nten_vad.h  ten_vad.py\nWindows\nten_vad.dll\nx64, x86\nC\nmacOS\nten_vad.framework\narm64, x86_64\nC\nAndroid\nlibten_vad.so\narm64-v8a, armeabi-v7a\nC\niOS\nten_vad.framework\narm64\nC\n1. not simulator  2. not iPad\nPython Usage\n1. Linux / macOS / Windows\nRequirements\nnumpy (Version 1.17.4/1.26.4 verified)\nscipy (Version >= 1.5.0)\nscikit-learn (Version 1.2.2/1.5.0 verified, for plotting PR curves)\nmatplotlib (Version 3.1.3/3.10.0 verified, for plotting PR curves)\ntorchaudio (Version 2.2.2 verified, for plotting PR curves)\nPython version 3.8.19/3.10.14 verified\nNote: You could use other versions of above packages, but we didn't test other versions.\nThe lib only depend on numpy, you have to install the dependency via requirements.txt:\npip install -r requirements.txt\nFor running demo or plotting PR curves, you have to install the dependencies:\npip install -r ./examples/requirements.txt\nNote that if you did not install libc++1, you have to run the code below to install it:\nsudo apt update\nsudo apt install libc++1\nUsage\nNote: For usage in python, you can either use it by git clone or pip.\nBy using git clone:\nClone the repository\ngit clone https://github.com/TEN-framework/ten-vad.git\nEnter examples directory\ncd ./examples\nTest\npython test.py s0724-s0730.wav out.txt\nBy using pip:\nInstall via pip\npip install -U --force-reinstall -v git+https://github.com/TEN-framework/ten-vad.git\nWrite your own use cases and import the class, the attributes of class TenVAD you can refer to ten_vad.py\nfrom ten_vad import TenVad\nJS Usage\n1. Web\nRequirements\nNode.js (macOS v14.18.2, Linux v16.20.2 verified)\nTerminal\nUsage\n1) cd ./examples\n2) node test_node.js s0724-s0730.wav out.txt\nC Usage\nBuild Scripts\nLocated in examples/ directory and examples_onnx (for ONNX usage on Linux):\nLinux: build-and-deploy-linux.sh\nWindows: build-and-deploy-windows.bat\nmacOS: build-and-deploy-mac.sh\nAndroid: build-and-deploy-android.sh\niOS: build-and-deploy-ios.sh\nDynamic Library Configuration\nRuntime library path configuration:\nLinux/Android: LD_LIBRARY_PATH\nmacOS: DYLD_FRAMEWORK_PATH\nWindows: DLL in executable directory or system PATH\nCustomization\nModify platform-specific build scripts\nAdjust CMakeLists.txt\nConfigure toolchain and architecture settings\nOverview of Usage\nNavigate to examples/ or examples_onx/ (for ONNX usage on Linux)\nExecute platform-specific build script\nConfigure dynamic library path\nRun demo with sample audio s0724-s0730.wav\nProcessed results saved to out.txt\nThe detailed usage methods of each platform are as follows\n1. Linux\nRequirements\nClang (e.g. 6.0.0-1ubuntu2 verified)\nCMake\nTerminal\nNote that if you did not install libc++1 (Linux), you have to run the code below to install it:\nsudo apt update\nsudo apt install libc++1\nUsage (prebuilt-lib)\n1) cd ./examples\n2) ./build-and-deploy-linux.sh\nUsage (ONNX)\nYou have to download the onnxruntime packages from the microsoft official onnxruntime github website. Note that the version of onnxruntime must be higher than or equal to 1.17.1 (e.g. onnxruntime-linux-x64-1.17.1.tgz).\nYou can check the official ONNX Runtime releases from this website. And for example, to download version 1.17.1 (Linux x64), use this link. After extracting the compressed file, you'll find two important directories:include/ - header files, lib/ - library files\n1) cd examples_onnx/\n2) ./build-and-deploy-linux.sh --ort-path /absolute/path/to/your/onnxruntime/root/dir\nNote 1: If executing the onnx demo from a different directory than the one used when running build-and-deploy-linux.sh, ensure to create a symbolic link to src/onnx_model/ to prevent ONNX model file loading failures.\nNote 2: The ONNX model locates in src/onnx_model directory.\n2. Windows\nRequirements\nVisual Studio (2017, 2019, 2022 verified)\nCMake (3.26.0-rc6 verified)\nTerminal (MINGW64 or powershell)\nUsage\n1) cd ./examples\n2) Configure \"build-and-deploy-windows.bat\" with your preferred:\n- Architecture (default: x64)\n- Visual Studio version (default: 2019)\n3) ./build-and-deploy-windows.bat\n3. macOS\nRequirements\nXcode (15.2 verified)\nCMake (3.19.2 verified)\nUsage\n1) cd ./examples\n2) Configure \"build-and-deploy-mac.sh\" with your target architecture:\n- Default: arm64 (Apple Silicon)\n- Alternative: x86_64 (Intel)\n3) ./build-and-deploy-mac.sh\n4. Android\nRequirements\nNDK (r25b, macOS verified)\nCMake (3.19.2, macOS verified)\nadb (1.0.41, macOS verified)\nUsage\n1) cd ./examples\n2) export ANDROID_NDK=/path/to/android-ndk  # Replace it with your NDK installation path\n3) Configure \"build-and-deploy-android.sh\" with your build settings:\n- Architecture: arm64-v8a (default) or armeabi-v7a\n- Toolchain: aarch64-linux-android-clang (default) or custom NDK toolchain\n4) ./build-and-deploy-android.sh\n5. iOS\nRequirements\nXcode (15.2, macOS verified)\nCMake (3.19.2, macOS verified)\nUsage\nEnter examples directory\ncd ./examples\nCreates Xcode project files for iOS build\n./build-and-deploy-ios.sh\nFollow the steps below to build and test on iOS device:\n3.1. Use Xcode to open .xcodeproj files: a) cd ./build-ios, b) open ./ten_vad_demo.xcodeproj\n3.2. In Xcode IDE, select ten_vad_demo target (should check: Edit Scheme ‚Üí Run ‚Üí Release), then select your iOS Device (not simulator).\n3.3. Drag ten_vad/lib/iOS/ten_vad.framework  to \"Frameworks, Libraries, and Embedded Content\"\n(in TARGETS ‚Üí ten_vad_demo ‚Üí ten_vad_demo ‚Üí General, should set Embed to \"Embed & Sign\").\nor add it directly in this way: \"Frameworks, Libraries, and Embedded Content\" ‚Üí \"+\" ‚Üí Add Other... ‚Üí Add Files ‚Üí...\nNote: If this step is not completed, you may encounter the following runtime error: \"dyld: Library not loaded: @rpath/ten_vad.framework/ten_vad\".\n3.4. Configure iOS device Signature\nin TARGETS ‚Üí ten_vad_demo ‚Üí Signing & Capabilities ‚Üí Signing\nModify Bundle Identifier: modify \"com.yourcompany\" to yours;\nSpecify Provisioning Profile\nIn TARGETS ‚Üí ten_vad_demo ‚Üí Build Settings ‚Üí Signing ‚Üí Code Signing Identity:\nSpecify your Certification\n3.5. Build in Xcode and run demo on your device.\nTEN Ecosystem\nProject\nPreview\nüèöÔ∏è TEN FrameworkTEN is an open-source framework for real-time, multimodal conversational AI.\nÔ∏èüîÇ TEN Turn DetectionTEN is for full-duplex dialogue communication.\nüîâ TEN VADTEN VAD is a low-latency, lightweight and high-performance streaming voice activity detector (VAD).\nüéôÔ∏è TEN AgentTEN Agent is a showcase of TEN Framewrok.\nüé® TMAN Designer TMAN Designer is low/no code option to make a voice agent with easy to use workflow UI.\nüìí TEN PortalThe official site of TEN framework, it has documentation and blog.\nAsk Questions\nMost questions can be answered by using DeepWiki, it is fast, intutive to use and supports multiple languages.\nCitations\n@misc{TEN VAD,\nauthor = {TEN Team},\ntitle = {TEN VAD: A Low-Latency, Lightweight and High-Performance Streaming Voice Activity Detector (VAD)},\nyear = {2025},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {https://github.com/TEN-framework/ten-vad.git},\nemail = {developer@ten.ai}\n}\nLicense\nThis project is licensed under Apache 2.0 with certain conditions. Refer to the \"LICENSE\" file in the root directory for detailed information. Note that pitch_est.cc contains modified code derived from LPCNet, which is BSD-2-Clause and BSD-3-Clause licensed, refer to the NOTICES file in the root directory for detailed information.",
    "concedo/llama-joycaption-beta-one-hf-llava-mmproj-gguf": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nThese GGUF quants were made from https://huggingface.co/fancyfeast/llama-joycaption-beta-one-hf-llava and designed for use in KoboldCpp 1.91 and above.\nContains 3 GGUF quants of Joycaption Beta One, as well as the associated mmproj file.\nTo use:\nDownload the main model (Llama-Joycaption-Beta-One-Hf-Llava-Q4_K.gguf) and the mmproj (Llama-Joycaption-Beta-One-Hf-Llava-F16.gguf)\nLaunch KoboldCpp and go to Loaded Files tab\nSelect the main model as \"Text Model\" and the mmproj as \"Vision mmproj\"",
    "stabilityai/stable-diffusion-3.5-large-tensorrt": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Diffusion 3.5 Large TensorRT\nIntroduction\nModel Details\nModel Description\nPerformance using TensorRT 10.13\nUsage Example\nStable Diffusion 3.5 Large TensorRT\nIntroduction\nThis repository hosts the TensorRT-optimized version of Stable Diffusion 3.5 Large, developed in collaboration between Stability AI and NVIDIA. This implementation leverages NVIDIA's TensorRT deep learning inference library to deliver significant performance improvements while maintaining the exceptional image quality of the original model.\nStable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. The TensorRT optimization makes these capabilities accessible for production deployment and real-time applications.\nModel Details\nModel Description\nThis repository holds the ONNX exports of the T5, MMDiT and VAE models in BF16 precision. It also holds the MMDiT model in FP8 precision. The transformer model was quantized to FP8 precision using NVIDIA/TensorRT-Model-Optimizer.\nPerformance using TensorRT 10.13\nTimings for 30 steps at 1024x1024\nAccelerator\nPrecision\nCLIP-G\nCLIP-L\nT5\nMMDiT x 30\nVAE Decoder\nTotal\nH100\nBF16\n13.83 ms\n5.66 ms\n8.55 ms\n7945 ms\n97.17 ms\n8101.83 ms\nH100\nFP8\n16.80 ms\n6.91 ms\n8.56 ms\n5604.97 ms\n36.91 ms\n5708.69 ms\nUsage Example\nFollow the setup instructions on launching a TensorRT NGC container.\ngit clone https://github.com/NVIDIA/TensorRT.git\ncd TensorRT\ngit checkout release/sd35\ndocker run --rm -it --gpus all -v $PWD:/workspace nvcr.io/nvidia/pytorch:25.01-py3 /bin/bash\nInstall libraries and requirements\ncd demo/Diffusion\nsource setup.sh\nGenerate HuggingFace user access token\nTo download model checkpoints for the Stable Diffusion 3.5 checkpoints, please request access on the Stable Diffusion 3.5 Large page.\nYou will then need to obtain a read access token to HuggingFace Hub and export as shown below. See instructions.\nexport HF_TOKEN=<your access token>\nPerform TensorRT optimized inference:\nStable Diffusion 3.5 Large in BF16 precision\npython3 demo_txt2img_sd35.py \\\n\"A chic urban apartment interior highlighting mid-century modern furniture, vibrant abstract art pieces on clean white walls, and large windows providing a stunning view of the bustling city below.\" \\\n--version=3.5-large \\\n--bf16 \\\n--download-onnx-models \\\n--denoising-steps=30 \\\n--guidance-scale 3.5 \\\n--build-static-batch \\\n--use-cuda-graph \\\n--hf-token=$HF_TOKEN\nStable Diffusion 3.5 Large using FP8 quantization\npython3 demo_txt2img_sd35.py \\\n\"A chic urban apartment interior highlighting mid-century modern furniture, vibrant abstract art pieces on clean white walls, and large windows providing a stunning view of the bustling city below.\" \\\n--version=3.5-large \\\n--fp8 \\\n--denoising-steps=30 \\\n--guidance-scale 3.5  \\\n--download-onnx-models \\\n--build-static-batch \\\n--use-cuda-graph \\\n--hf-token=$HF_TOKEN \\\n--onnx-dir onnx_fp8 \\\n--engine-dir engine_fp8",
    "alana89/TabSTAR": "üìö TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations\nAbstract\nInstall\nTo fit a pretrained TabSTAR model to your own dataset, install the package:\npip install tabstar\nQuickstart Example\nfrom importlib.resources import files\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tabstar.tabstar_model import TabSTARClassifier\ncsv_path = files(\"tabstar\").joinpath(\"resources\", \"imdb.csv\")\nx = pd.read_csv(csv_path)\ny = x.pop('Genre_is_Drama')\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n# For regression tasks, replace `TabSTARClassifier` with `TabSTARRegressor`.\ntabstar = TabSTARClassifier()\ntabstar.fit(x_train, y_train)\ny_pred = tabstar.predict(x_test)\nprint(classification_report(y_test, y_pred))\nüìö TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations\nRepository: alanarazi7/TabSTAR\nPaper: TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations\nLicense: MIT ¬© Alan Arazi et al.\nAbstract\nWhile deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "CharGen/CharGen-v3-mini": "CharGen v3 mini\nQuantized versions\nPrompting\nSupported fields and prompts:\nDialogue Example Hints\nTraining and Data\nTools\nHuman Data Augmentation\nReinforcement Learning\nGRPO for creative writing\nReward Orchestration\nReward Signals\nFighting Slop\nTraining Scale\nLicensing and Attribution\nLive version\nhttps://chargen.kubes-lab.com\nSelect \"CharGen-v3-mini\" model in Settings\nCharGen v3 mini\nCharGen v3 mini is a small model that helps you to write role playing characters.\nIt produces characters based on your free-form text input. Model outputs plain-text characters, in a step-by-step, dialogue format.\nCharGen v3 series of models is a significant improvement over CharGen v2. It demonstrates exceptional instruction following and format adherence.\nCharGen is a project that started in 2023 with a goal to make character making effortless.\nWarning: this model was trained on some NSFW content, so it may produce NSFW characters.\nQuantized versions\nGGUF\nPrompting\nTo make a character, prompt the model using the following prompt ordering:\nSystem message\nDescription Prompt message or Facts Prompt\nIf Facts Prompt is used, follow up by Alternate Description Prompt\nAny of the other field prompts, in any order\nDialogue Example Prompt should be prompted last\nSupported fields and prompts:\nSystem Message\nYou are an expert in creating interesting roleplay characters. Your main goal is to create a vibrant persona for roleplay. Use on-point simple language, avoiding overly complex phrases. It is acceptable to assume or even create missing details about the character. Refer to roleplaying user as User.\nDescription Prompt\nThis is the very first thing you should query the model with.\nBase version (when Facts field is not in use):\nBelow is a brief overview of a character. Expand it into a detailed description. Include details about character's personality, their outfit and figure. Mention their age and gender, if applicable.\n---\n{{character_description}}\n---\nUse third person narration. Start your response with \"{{char_name}} is ...\"\nAlternate version (when used with conjunction with Facts field - it should follow Facts Prompt):\nExpand the brief character overview into a detailed description, taking these facts into account. Include details about character's personality, their outfit and figure. Mention their age and gender, if applicable. Use third person narration. Start your response with \"{{char_name}} is ...\"\nFacts Prompt\nFacts are small bits of interesting information about the character.\nAvoid using obvious facts like gender or age here - instead, add something peculiar; maybe your character owns a pet rock?\nFacts field is not part of Tavern Character Card V2 spec. Instead, they help the model to understand your character better and capture some of the more specific details.\nIdeal size for each fact is a single short sentence. Try to keep the total number of facts in 5-10 range.\nN.B.: When Facts field is in use, use Facts Prompt INSTEAD OF Description Prompt and use alternate version of Description prompt after Facts Prompt.\nBelow is a brief overview of a character. Enrich it by creating a list of interesting and peculiar facts about this character.\n---\n{{character_description}}\n---\nAvoid using obvious facts and things like gender or age. Write 5-10 facts in simple and concise language. Format your response as unordered list.\nScenario Prompt\nWrite an interesting and engaging scenario for roleplay between {{char_name}} and User.\nPersonality Prompt\nWrite several personal qualities that characterize {{char_name}}.\nAppearance Prompt\nThis field is not part of Tavern Character Card V2 spec, but it is useful for generating images for the character using Image Generation models.\nImagine what {{char_name}} could look like and describe {{char_name}}'s portrait. Capture their appearance, clothes and body features, as well as the background setting they can be in. Only include details that would be useful when describing their photo. Omit explanation or any personality traits that cannot be reflected graphically, and focus on visual characteristics instead. Your task is to give specific instructions on how to draw {{char_name}}'s portrait.\nFirst Message Prompt\nWrite the initial message in this roleplay that would introduce User to {{char_name}} and the scenario.\nDialogue Examples Prompt\nWrite a few example exchanges between User and {{char_name}} in chat format. Separate each exchange with a <START> tag.\nAlternate version for Dialogue Example Hints feature:\nWrite several chat exchanges between User and {{char_name}}. Each exchange should start with <START> tag, include a brief contextual summary in parentheses and show the dialogue between User and {{char_name}}, prefixed by names, each turn on new line.\nDialogue Example Hints\nThis model supports experimental feature called Dialogue Example Hints that allows you to specify the theme or even events for a particular dialogue example.\nHere is how it looks like from the prompting perspective:\n<|im_start|>user\nWrite several chat exchanges between User and {{char_name}}. Each exchange should start with <START> tag, include a brief contextual summary in parentheses and show the dialogue between User and {{char_name}}, prefixed by names, each turn on new line.<|im_end|>\n<|im_start|>assistant\n<START> ({{hint}})\n*newline here*\nThe partial assistant turn should be sent without the EOS token so model proceeds to generate a continuation of this same turn. In sampling settings, add string <START> to the list of stop words, so the model only generates a single dialogue example.\nTraining and Data\nWe stopped looking for diamonds in the rough ‚Äì we now grow diamonds from coal.\nData for CharGen v3 series of models was gathered from publicly-available character card repositories and underwent a diligent manual vetting, proof-reading and partial re-writing of every single card. Process is long and tedious, and would not be possible without @Delta-Vector's help and support.\nCharGen v3's data pipeline is an improvement on v2 in several key aspects. Where v2 was mainly focused on finding the perfect cards in the raw corpus, v3 treats most cards as raw material for later enrichment steps.\nBy a rough estimate, @Delta-Vector and @kubernetes-bad spent ~400 hours (weekends included) on cards re-writing. This number does not include the initial manual vetting of cards.\nWith v3, the key was to stop treating \"bad\" cards as binary rejects. Now, a card would need to pass just a small list of rule-based pre-filters (is card plaintext? Is it english? How's the length? etc.) to be considered for the next steps, where it would undergo iterative manual improvements.\nTools\nGrammar correcting T5-based model cascade from v2 was replaced with a more sophisticated ensemble of large language models that would attempt to fix both grammar and logical/stylistic inconsistencies. Traditional grammar checkers would sometimes also \"fix\" author's deliberate stylistic choices, just because they're trained on correctness and not so much on conversational language understanding. The solution was to combine three approaches:\nSemantic Richness Scoring\nA custom tool, Concepts, analyzed character card in embedding space for features like mentions of character relation to User, personality traits and aspects, or character's hobbies, likes and dislikes. This wasn't so much about judging character card quality, but estimating the amount of human work a card would need to be considered good. Output of each of those Concepts was then summed up to represent a proxy for \"character richness\" metric. The intuition is that if a character has all those things mentioned - chances are good that it will have many more things that make a character good, even if there wasn't a Concept for that feature specifically. Higher score = better.\nLLM-Assisted Refinement\nMistral-Large and Claude 3.5 Haiku were both tasked with correcting grammar and fixing logical consistency where necessary. Modern language models are great at preserving author's voice: where T5 would rewrite a brooding vampire's accented dialogue into just... normal text, these models kept the atmosphere while fixing stuff like pronoun mismatches (as well as grammar, of course). A custom tool, Fuckery, was used to let human annotators to verify every AI edit suggestion and quickly cherry-pick the ones that they agree with or edit it in-place, using a git-merge style web interface.\nHuman Touch\nSame tool was employed for the final step - manual rewriting. Some cards were mostly fine, but had some problems like delving too deep into unrelated details about character's extended family, or incorporating jailbreak-like instructions right in the card, or just needed some tweaks and paragraph re-arrangement, so the were manually reviewed and edited. It's vital to mention: the preservation of the original card's author vision was absolutely key in this step. If there was even a hint of discrepancy between factual correctness and author's vision - the priority would always go for author's vision, even if that would contradict common sense. Sometimes, this could also indicate a bad quality card, but in rare occasions it was actually part of the vibe that author was going for.\nHuman Data Augmentation\nEarly on into the project, a non-negotiable rule was established: human editors should edit grammar and style, not intent. When encountering cards with disturbing or morally questionable content, editors were instructed to either:\nCorrect syntax/formatting without altering meaning (e.g., fixing a serial killer‚Äôs rambling manifesto into coherent sentences), or\nSkip the card entirely if unable to separate technical from ethical judgement\nThis highlights the tool-like nature of CharGen series of models. The model should be able to generate a wide range of characters, without moralizing or any judgment whatsoever. It's the end user's privilege to decide what is acceptable and what is not. If a character's \"core idea\" was controversial or even disturbing, but well-executed, it was improved for clarity and consistency - but not fundamentally changed. It is partly for this reason - inclusion of potentially disturbing content curated for data diversity - that the dataset itself is not planned for public release.\nReinforcement Learning\nCharGen v3-mini is trained as a milestone model to hone the reinforcement learning formula that would be then applied to bigger models. Its smaller size (4b parameters vs 24b for full v3 model) allowed for faster iteration and made experimentation cheaper. More novel and potentially risky things could be tried with v3-mini without making the author noticeably poorer.\nGRPO for creative writing\nFor reinforcement learning, GRPO (Group Relative Policy Optimization) method was chosen - in contrast to v2's offline DPO (Direct Policy Optimization). GRPO is an online training method, meaning that it alters the model \"live\" - as it generates samples, its weights are altered based on the results of those outputs.\nCommonly, PPO is used for this type of training by big labs, but it's hard to pull off since it requires careful tuning of many, many moving parts and VRAM requirements are enormous. GRPO simplifies online reinforcement learning by eliminating the need for a separate advantage estimation model - instead, it generates several candidate outputs for a given prompt, then applies a reward function to each, and then calculates the relative advantage for the group of samples. This, plus its sample-level loss (vs token-level in classic PPO), makes the training more stable.\nOriginal GRPO paper focused mostly on verifiable rewards ‚Äì where an output's correctness can be objectively proven, like checking math solutions or executing code. Applying it to a very subjective domain like creative writing, where \"correctness\" is hard or straight up impossible to quantify, was CharGen's experimental adaptation of GRPO technique.\nDataset for GRPO is derived from user-submitted preference data from CharGen App - over the course of last year, users generated countless characters and some submitted feedback in the form of simple thumbs-up/thumbs-down signal. These prompts were used in making the reinforcement learning dataset for prompting CharGen model in online learning setting (original generations from these feedback submissions weren't used).\nReward Orchestration\nDefining what makes a character \"good\" is hard and very context-dependent. A reward signal needs to be more sophisticated than any single metric that's applied all the time. To handle that, a custom reward orchestration framework, reward-composer, was developed. It works with both Axolotl and TRL trainers, and allows for describing complex relationships between the individual reward functions.\nreward-composer makes it relatively easy to specify dependencies (e.g., reward B only applies if conditions of qualifier A is met) and conditions, composing these smaller reward signals into one complex and dynamic reward function.\nThis is important because CharGen v3 models generate characters step-by-step in a dialogue format, where the quality criteria for one field (like 'Personality') might differ significantly from another (like 'Example Dialogues') and depend heavily on previously generated fields.\nReward Signals\nThe set of active reward functions dynamically changed depending on the current step in the character generation flow. But overarching goal remained consistent: guide the model towards outputs that represent a well-formed, coherent character aligned with the user's prompt (and what we know a good character should look like).\nOne of the main components of CharGen's reward design was an ensemble of LLM judges. This means that several independent models would grade CharGen's outputs based on the same rubric and then produce a composite score. This score represented how well the model's response for a specific field adhered to the initial user prompt and the dialogue history (previously generated character fields). Basically, it measured prompt adherence and character consistency.\nTo make the model stay on course and keep it format-consistent (LLM judges don't necessarily know what makes a markdown dialogue good), auxiliary rewards were used.These included penalties for incoherence, over-long generation, or breaking the expected dialogue format.\nConcepts library was used here too, boosting scores of generations that included desirable features like relationship to User, looks and age - but only slightly, in order to prevent model from gaming the system and looksmaxxing the entire response, for example.\nFighting Slop\nCharGen v3's RL had a critical auxiliary reward specifically targeted at reducing the use of common AI clich√© phrases, also known as \"slop\". Phrases like \"can't help but...\", \"a mixture of X and Y\", \"kiss-bruised lips\" or \"half-lidded eyes\" are often overused by models in context of role play. Slop is a nasty defect that easily breaks immersion in a roleplay session. Presence of slop in a character card \"primes\" the RP session towards generating more of the slop, turning the whole session into one slop-fest.\nTo combat slop, CharGen v3's RL step includes targeted slop-penalty reward function. The process involved:\nGenerating a large set of character fields using the pre-RL CharGen v3-mini model with prompts from the RL training dataset.\nBuilding an n-gram frequency table from these generated outputs.\nManually curating this frequency table, removing common English phrase ngrams (I do n't want to, is a very, etc.) to isolate the specific, repetitive clich√©s favored by this model in this domain.\nUsing this curated slop-list into a reward as a penalty signal ‚Äì the frequency of the slop phrases used would be summed up and normalized for completion length, and then used as a penalty. Essentially - it's not just \"used bad word = get 0 reward\", but rather \"how bad of a bad word was used\". Resulting reward signal is then scaled with a sigmoid function (turns out, pre-RL CharGen was not super-duper sloppy to begin with).\nThis reward pushes model to more original and varied wording without penalizing common English phrases, or slop that model wouldn't use anyway. Eliminating slop was a key objective for improving the natural feel and usefulness of characters made with CharGen v3 models.\nTraining Scale\nOver total of 44 RL runs (SFT runs counted separately), for CharGen v3-mini, ~200,000 LLM judge requests were made. Judge models used were DeepSeek R1, Llama3.1 405b Instruct and DeepSeek v3-0324.\nWith applying GRPO to not-so-verifiable rewards, CharGen v3-mini demonstrates improvements in complex instruction following and makes it able to maintain character consistency throughout the whole character generation dialogue.\nLicensing and Attribution\nThis model is a derivative work based on Delta-Vector/Holland-4B-V1, licensed under MIT License. The Holland-4B-V1 itself is based on the original nvidia/Llama-3.1-Minitron-4B-Width-Base, licensed under NVIDIA Open Model License Agreement, with addition of ChatML tokens by @IntervitensInc.\nThis derivative model as a whole is licensed under the MIT License.",
    "QiWang98/VideoRFT": "üé• VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning\nüì∞ News\nüîé Overview\n‚ú® Methodology\nüìÄ Datasets\nüõ†Ô∏è Set up\nRequirements\nInstallation\nüöÄ Training\nSupervised Fine-Tuning (SFT)\nReinforcement Learning (RL)\nüìà Inference & Evaluation\nEvaluation Procedure\nüöÄ Quick Inference Code\nüôè Acknowledgements\nüìö Citations\nüé• VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning\nThis repository contains the VideoRFT model, presented in the paper VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning.\nüìñ ArXiv\n‚îÇ ¬†¬†üìÄ CoT Dataset\n‚îÇ ¬†¬†üìÄ RL Dataset\n‚îÇ ¬†¬†ü§ó Models\nüì∞ News\n[2025/09/19] Our paper has been accepted to NeurIPS 2025 üéâ!\n[2025/06/01] We released our 3B Models (ü§óVideoRFT-SFT-3B and ü§óVideoRFT-3B) to huggingface.\n[2025/05/25] We released our 7B Models (ü§óVideoRFT-SFT-7B and ü§óVideoRFT-7B) to huggingface.\n[2025/05/20] We released our Datasets (üìÄCoT Dataset and üìÄRL Dataset) to huggingface.\n[2025/05/18] Our paper is released on ArXiv, and we have open-sourced our code on GitHub!\nüîé Overview\nReinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VideoRFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a fully automatic CoT curation pipeline. First, we devise a cognitioninspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a visual-language model conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strength the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning with visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VideoRFT achieves state-of-the-art performance on six video reasoning benchmarks.\n‚ú® Methodology\nTo overcome the scarcity of video CoTs, we develop a scalable, cognitively inspired pipeline for high-quality video CoT dataset construction.\nTo further strength the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning with visual evidence.\nüìÄ Datasets\nBased on above pipeline, we construct two large-scale datasets, i.e., üìÄVideoRFT-CoT-102K and üìÄVideoRFT-RL-310K.\nüõ†Ô∏è Set up\nRequirements\nPython >= 3.11\nPytorch >= 2.5.1\ntransformers == 4.51.3\nvLLM == 0.7.3\ntrl == 0.16.0\nInstallation\ngit clone https://github.com/QiWang98/VideoRFT\ncd VideoRFT\n# Create and activate environment\nconda create -n VideoRFT python=3.11\nconda activate VideoRFT\nbash setup.sh\n# Install decord for improved video processing\ncd src/qwen-vl-utils\npip install -e .[decord]\nüöÄ Training\nSupervised Fine-Tuning (SFT)\nWe begin with supervised fine-tuning on the VideoRFT-CoT dataset for one epoch:\nbash ./src/scripts/run_sft_video.sh\nThis step can be skipped by directly using our pretrained SFT models, available at ü§óVideoRFT-SFT-7B or ü§óVideoRFT-SFT-3B.\nReinforcement Learning (RL)\nNext, perform reinforcement learning using the VideoRFT-RL dataset:\nbash ./src/scripts/run_grpo_video.sh\nTo enable faster training via vLLM acceleration:\nbash ./src/scripts/run_grpo_vllm_qwen25vl.sh\nNote: During training, we adopt the following settings for efficiency:\nVIDEO PIXELS: 128 √ó 28 √ó 28\nFPS FRAMES: 16\nAll frame-related configurations can be adjusted in src/qwen-vl-utils.\nüìà Inference & Evaluation\nDuring inference, we increase the maximum frame resolution and length to boost performance:\nVIDEO PIXELS: 256 √ó 28 √ó 28\nFPS FRAMES: 32\nYou can configure these parameters in src/qwen-vl-utils.\nWe evaluate all models under a unified decoding configuration following the official Qwen2.5-VL demo:\ntop_p = 0.001\ntemperature = 0.01\nEvaluation Procedure\nDownload preprocessed evaluation JSONs from: [ü§ó eval]\nDownload the video data from the official sites of each benchmark and organize them as specified in the JSON files.\nRun the evaluation across all benchmarks:\nbash ./src/eval_bench.sh\nüöÄ Quick Inference Code\nimport numpy as np\nimport torch\nfrom longvu.builder import load_pretrained_model\nfrom longvu.constants import (\nDEFAULT_IMAGE_TOKEN,\nIMAGE_TOKEN_INDEX,\n)\nfrom longvu.conversation import conv_templates, SeparatorStyle\nfrom longvu.mm_datautils import (\nKeywordsStoppingCriteria,\nprocess_images,\ntokenizer_image_token,\n)\nfrom decord import cpu, VideoReader\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n\"./checkpoints/longvu_qwen\", None, \"cambrian_qwen\",\n)\nmodel.eval()\nvideo_path = \"./examples/video1.mp4\"\nqs = \"Describe this video in detail\"\nvr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\nfps = float(vr.get_avg_fps())\nframe_indices = np.array([i for i in range(0, len(vr), round(fps),)])\nvideo = []\nfor frame_index in frame_indices:\nimg = vr[frame_index].asnumpy()\nvideo.append(img)\nvideo = np.stack(video)\nimage_sizes = [video[0].shape[:2]]\nvideo = process_images(video, image_processor, model.config)\nvideo = [item.unsqueeze(0) for item in video]\nqs = DEFAULT_IMAGE_TOKEN + \"\n\" + qs\nconv = conv_templates[\"qwen\"].copy()\nconv.append_message(conv.roles[0], qs)\nconv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\ninput_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\nkeywords = [stop_str]\nstopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\nwith torch.inference_mode():\noutput_ids = model.generate(\ninput_ids,\nimages=video,\nimage_sizes=image_sizes,\ndo_sample=False,\ntemperature=0.2,\nmax_new_tokens=128,\nuse_cache=True,\nstopping_criteria=[stopping_criteria],\n)\npred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\nüôè Acknowledgements\nWe gratefully acknowledge the contributions of the open-source community, particularly DeepSeek-R1, Open-R1, and R1-V.\nüìö Citations\nIf you find this work helpful, please consider citing:\n@article{VideoRFT,\ntitle={VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning},\nauthor={Wang, Qi and Yu, Yanrui and Yuan, Ye and Mao, Rui and Zhou, Tianfei},\njournal={arXiv preprint arXiv:2505.12434},\nyear={2025}\n}",
    "unsloth/medgemma-4b-it": "MedGemma model card\nModel information\nDescription\nHow to use\nExamples\nModel architecture overview\nTechnical specifications\nCitation\nInputs and outputs\nPerformance and validation\nKey performance metrics\nEthics and safety evaluation\nData card\nDataset overview\nEthics and safety evaluation\nData card\nDataset overview\nData Ownership and Documentation\nData citation\nDe-identification/anonymization:\nImplementation information\nSoftware\nUse and limitations\nIntended use\nBenefits\nLimitations\nRelease notes\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nMedGemma model card\nModel documentation: MedGemma\nResources:\nModel on Google Cloud Model Garden: MedGemma\nModel on Hugging Face: MedGemma\nGitHub repository (supporting code, Colab notebooks, discussions, and\nissues): MedGemma\nQuick start notebook: GitHub\nFine-tuning notebook: GitHub\nConcept applications built using MedGemma: Collection\nSupport: See Contact\nLicense: The use of MedGemma is governed by the Health AI Developer\nFoundations terms of\nuse.\nAuthor: Google\nModel information\nThis section describes the MedGemma model and how to use it.\nDescription\nMedGemma is a collection of Gemma 3\nvariants that are trained for performance on medical text and image\ncomprehension. Developers can use MedGemma to accelerate building\nhealthcare-based AI applications. MedGemma currently comes in three variants: a\n4B multimodal version and 27B text-only and multimodal versions.\nBoth MedGemma multimodal versions utilize a\nSigLIP image encoder that has been\nspecifically pre-trained on a variety of de-identified medical data, including\nchest X-rays, dermatology images, ophthalmology images, and histopathology\nslides. Their LLM components are trained on a diverse set of medical data,\nincluding medical text, medical question-answer pairs, FHIR-based electronic\nhealth record data (27B multimodal only), radiology images, histopathology\npatches, ophthalmology images, and dermatology images.\nMedGemma 4B is available in both pre-trained (suffix: -pt) and\ninstruction-tuned (suffix -it) versions. The instruction-tuned version is a\nbetter starting point for most applications. The pre-trained version is\navailable for those who want to experiment more deeply with the models.\nMedGemma 27B multimodal has pre-training on medical image, medical record and\nmedical record comprehension tasks. MedGemma 27B text-only has been trained\nexclusively on medical text. Both models have been optimized for inference-time\ncomputation on medical reasoning. This means it has slightly higher performance\non some text benchmarks than MedGemma 27B multimodal. Users who want to work\nwith a single model for both medical text, medical record and medical image\ntasks are better suited for MedGemma 27B multimodal. Those that only need text\nuse-cases may be better served with the text-only variant. Both MedGemma 27B\nvariants are only available in instruction-tuned versions.\nMedGemma variants have been evaluated on a range of clinically relevant\nbenchmarks to illustrate their baseline performance. These evaluations are based\non both open benchmark datasets and curated datasets. Developers can fine-tune\nMedGemma variants for improved performance. Consult the Intended\nUse\nsection below for more details.\nMedGemma is optimized for medical applications that involve a text generation\ncomponent. For medical image-based applications that do not involve text\ngeneration, such as data-efficient classification, zero-shot classification, or\ncontent-based or semantic image retrieval, the MedSigLIP image\nencoder\nis recommended. MedSigLIP is based on the same image encoder that powers\nMedGemma.\nPlease consult the MedGemma Technical Report\nfor more details.\nHow to use\nBelow are some example code snippets to help you quickly get started running the\nmodel locally on GPU. If you want to use the model at scale, we recommend that\nyou create a production version using Model\nGarden.\nFirst, install the Transformers library. Gemma 3 is supported starting from\ntransformers 4.50.0.\n$ pip install -U transformers\nRun model with the pipeline API\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/medgemma-4b-it\",\ntorch_dtype=torch.bfloat16,\ndevice=\"cuda\",\n)\n# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n{\"type\": \"image\", \"image\": image}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\nRun the model directly\n# pip install accelerate\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/medgemma-4b-it\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n{\"type\": \"image\", \"image\": image}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=True,\nreturn_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=200, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nExamples\nSee the following Colab notebooks for examples of how to use MedGemma:\nTo give the model a quick try, running it locally with weights from Hugging\nFace, see Quick start notebook in\nColab.\nNote that you will need to use Colab Enterprise to obtain adequate GPU\nresources to run either 27B model without quantization.\nFor an example of fine-tuning the 4B model, see the Fine-tuning notebook in\nColab.\nThe 27B models can be fine tuned in a similar manner but will require more\ntime and compute resources than the 4B model.\nModel architecture overview\nThe MedGemma model is built based on Gemma 3 and\nuses the same decoder-only transformer architecture as Gemma 3. To read more\nabout the architecture, consult the Gemma 3 model\ncard.\nTechnical specifications\nModel type: Decoder-only Transformer architecture, see the Gemma 3\nTechnical\nReport\nInput Modalities: Text, vision\nOutput Modality: Text only\nAttention mechanism: Grouped-query attention (GQA)\nContext length: Supports long context, at least 128K tokens\nKey publication: https://arxiv.org/abs/2507.05201\nModel created: July 9, 2025\nModel version: 1.0.1\nCitation\nWhen using this model, please cite: Sellergren et al. \"MedGemma Technical\nReport.\" arXiv preprint arXiv:2507.05201 (2025).\n@article{sellergren2025medgemma,\ntitle={MedGemma Technical Report},\nauthor={Sellergren, Andrew and Kazemzadeh, Sahar and Jaroensri, Tiam and Kiraly, Atilla and Traverse, Madeleine and Kohlberger, Timo and Xu, Shawn and Jamil, Fayaz and Hughes, C√≠an and Lau, Charles and others},\njournal={arXiv preprint arXiv:2507.05201},\nyear={2025}\n}\nInputs and outputs\nInput:\nText string, such as a question or prompt\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens each\nTotal input length of 128K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a question,\nanalysis of image content, or a summary of a document\nTotal output length of 8192 tokens\nPerformance and validation\nMedGemma was evaluated across a range of different multimodal classification,\nreport generation, visual question answering, and text-based tasks.\nKey performance metrics\nImaging evaluations\nThe multimodal performance of MedGemma 4B and 27B multimodal was evaluated\nacross a range of benchmarks, focusing on radiology, dermatology,\nhistopathology, ophthalmology, and multimodal clinical reasoning.\nMedGemma 4B outperforms the base Gemma 3 4B model across all tested multimodal\nhealth benchmarks.\nTask and metric\nGemma 3 4B\nMedGemma 4B\nMedical image classification\nMIMIC CXR** - macro F1 for top 5 conditions\n81.2\n88.9\nCheXpert CXR - macro F1 for top 5 conditions\n32.6\n48.1\nCXR14 - macro F1 for 3 conditions\n32.0\n50.1\nPathMCQA* (histopathology, internal**)  - Accuracy\n37.1\n69.8\nUS-DermMCQA* - Accuracy\n52.5\n71.8\nEyePACS* (fundus, internal) - Accuracy\n14.4\n64.9\nVisual question answering\nSLAKE (radiology) - Tokenized F1\n40.2\n72.3\nVQA-RAD*** (radiology) - Tokenized F1\n33.6\n49.9\nKnowledge and reasoning\nMedXpertQA (text + multimodal questions) - Accuracy\n16.4\n18.8\n*Internal datasets. US-DermMCQA is described in Liu (2020, Nature\nmedicine), presented as a\n4-way MCQ per example for skin condition classification. PathMCQA is based on\nmultiple datasets, presented as 3-9 way MCQ per example for identification,\ngrading, and subtype for breast, cervical, and prostate cancer. EyePACS is a\ndataset of fundus images with classification labels based on 5-level diabetic\nretinopathy severity (None, Mild, Moderate, Severe, Proliferative). More details\nin the MedGemma Technical Report.\n**Based on radiologist adjudicated labels, described in Yang (2024,\narXiv) Section A.1.1.\n***Based on \"balanced split,\" described in Yang (2024,\narXiv).\nChest X-ray report generation\nMedGemma chest X-ray (CXR) report generation performance was evaluated on\nMIMIC-CXR using the RadGraph\nF1 metric. We compare the MedGemma\npre-trained checkpoint with our previous best model for CXR report generation,\nPaliGemma 2.\nMetric\nMedGemma 4B (pre-trained)\nMedGemma 4B (tuned for CXR)\nPaliGemma 2 3B (tuned for CXR)\nPaliGemma 2 10B (tuned for CXR)\nMIMIC CXR - RadGraph F1\n29.5\n30.3\n28.8\n29.5\nThe instruction-tuned versions of MedGemma 4B and MedGemma 27B achieve lower\nscores (21.9 and 21.3, respectively) due to the differences in reporting style\ncompared to the MIMIC ground truth reports. Further fine-tuning on MIMIC reports\nenables users to achieve improved performance, as shown by the improved\nperformance of the MedGemma 4B model that was tuned for CXR.\nText evaluations\nMedGemma 4B and text-only MedGemma 27B were evaluated across a range of\ntext-only benchmarks for medical knowledge and reasoning.\nThe MedGemma models outperform their respective base Gemma models across all\ntested text-only health benchmarks.\nMetric\nGemma 3 4B\nMedGemma 4B\nMedQA (4-op)\n50.7\n64.4\nMedMCQA\n45.4\n55.7\nPubMedQA\n68.4\n73.4\nMMLU Med\n67.2\n70.0\nMedXpertQA (text only)\n11.6\n14.2\nAfriMed-QA (25 question test set)\n48.0\n52.0\nFor all MedGemma 27B results, test-time\nscaling is used to improve performance.\nMedical record evaluations\nAll models were evaluated on a question answer dataset from synthetic FHIR data\nto answer questions about patient records. MedGemma 27B multimodal's\nFHIR-specific training gives it significant improvement over other MedGemma and\nGemma models.\nMetric\nGemma 3 4B\nMedGemma 4B\nEHRQA\n70.9\n67.6\nEthics and safety evaluation\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild safety: Evaluation of text-to-text and image-to-text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent safety: Evaluation of text-to-text and image-to-text prompts\ncovering safety policies, including harassment, violence and gore, and hate\nspeech.\nRepresentational harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including bias, stereotyping, and harmful\nassociations or inaccuracies.\nGeneral medical harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including information quality and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance evaluations\"\nwhich are our \"arms-length\" internal evaluations for responsibility governance\ndecision making. They are conducted separately from the model development team,\nto inform decision making about release. High-level findings are fed back to the\nmodel team, but prompt sets are held out to prevent overfitting and preserve the\nresults' ability to inform decision making. Notable assurance evaluation results\nare reported to our Responsibility & Safety Council as part of release review.\nEvaluation results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms. All\ntesting was conducted without safety filters to evaluate the model capabilities\nand behaviors. For text-to-text, image-to-text, and audio-to-text, and across\nboth MedGemma model sizes, the model produced minimal policy violations. A\nlimitation of our evaluations was that they included primarily English language\nprompts.\nData card\nDataset overview\nTraining\nThe base Gemma models are pre-trained on a large corpus of text and code data.\nMedGemma 4B utilizes a SigLIP image encoder\nthat has been specifically pre-trained on a variety of de-identified medical\ndata, including radiology images, histopathology images, ophthalmology images,\nand dermatology images. Its LLM component is trained on a diverse set of medical\ndata, including medical text relevant to radiology images, chest-x rays,\nhistopathology patches, ophthalmology images and dermatology images.\nEvaluation\nMedGemma models have been evaluated on a comprehensive set of clinically\nrelevant benchmarks, including over 22 datasets across 5 different tasks and 6\nmedical image modalities. These include both open benchmark datasets and curated\ndatasets, with a focus on expert human evaluations for tasks like CXR report\ngeneration and radiology VQA.\nEthics and safety evaluation\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild safety: Evaluation of text-to-text and image-to-text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent safety: Evaluation of text-to-text and image-to-text prompts\ncovering safety policies, including harassment, violence and gore, and hate\nspeech.\nRepresentational harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including bias, stereotyping, and harmful\nassociations or inaccuracies.\nGeneral medical harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including information quality and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance evaluations\"\nwhich are our \"arms-length\" internal evaluations for responsibility governance\ndecision making. They are conducted separately from the model development team,\nto inform decision making about release. High-level findings are fed back to the\nmodel team, but prompt sets are held out to prevent overfitting and preserve the\nresults' ability to inform decision making. Notable assurance evaluation results\nare reported to our Responsibility & Safety Council as part of release review.\nEvaluation results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms. All\ntesting was conducted without safety filters to evaluate the model capabilities\nand behaviors. For text-to-text, image-to-text, and audio-to-text, and across\nboth MedGemma model sizes, the model produced minimal policy violations. A\nlimitation of our evaluations was that they included primarily English language\nprompts.\nData card\nDataset overview\nTraining\nThe base Gemma models are pre-trained on a large corpus of text and code data.\nMedGemma multimodal variants utilize a\nSigLIP image encoder that has been\nspecifically pre-trained on a variety of de-identified medical data, including\nradiology images, histopathology images, ophthalmology images, and dermatology\nimages. Their LLM component is trained on a diverse set of medical data,\nincluding medical text, medical question-answer pairs, FHIR-based electronic\nhealth record data (27B multimodal only), radiology images, histopathology\npatches, ophthalmology images, and dermatology images.\nEvaluation\nMedGemma models have been evaluated on a comprehensive set of clinically\nrelevant benchmarks, including over 22 datasets across 6 different tasks and 4\nmedical image modalities. These benchmarks include both open and internal\ndatasets.\nSource\nMedGemma utilizes a combination of public and private datasets.\nThis model was trained on diverse public datasets including MIMIC-CXR (chest\nX-rays and reports), ChestImaGenome: Set of bounding boxes linking image\nfindings with anatomical regions for MIMIC-CXR (MedGemma 27B multimodal only),\nSLAKE (multimodal medical images and questions), PAD-UFES-20 (skin lesion images\nand data), SCIN (dermatology images), TCGA (cancer genomics data), CAMELYON\n(lymph node histopathology images), PMC-OA (biomedical literature with images),\nand Mendeley Digital Knee X-Ray (knee X-rays).\nAdditionally, multiple diverse proprietary datasets were licensed and\nincorporated (described next).\nData Ownership and Documentation\nMIMIC-CXR: MIT Laboratory\nfor Computational Physiology and Beth Israel Deaconess Medical Center\n(BIDMC).\nSlake-VQA: The Hong Kong Polytechnic\nUniversity (PolyU), with collaborators including West China Hospital of\nSichuan University and Sichuan Academy of Medical Sciences / Sichuan\nProvincial People's Hospital.\nPAD-UFES-20: Federal\nUniversity of Esp√≠rito Santo (UFES), Brazil, through its Dermatological and\nSurgical Assistance Program (PAD).\nSCIN: A collaboration\nbetween Google Health and Stanford Medicine.\nTCGA (The Cancer Genome Atlas): A joint\neffort of National Cancer Institute and National Human Genome Research\nInstitute. Data from TCGA are available via the Genomic Data Commons (GDC)\nCAMELYON: The data was\ncollected from Radboud University Medical Center and University Medical\nCenter Utrecht in the Netherlands.\nPMC-OA (PubMed Central Open Access\nSubset):\nMaintained by the National Library of Medicine (NLM) and National Center for\nBiotechnology Information (NCBI), which are part of the NIH.\nMedQA: This dataset was created by a\nteam of researchers led by Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung\nWeng, Hanyi Fang, and Peter Szolovits\nMendeley Digital Knee\nX-Ray: This dataset is\nfrom Rani Channamma University, and is hosted on Mendeley Data.\nAfriMed-QA: This data was developed and led by\nmultiple collaborating organizations and researchers include key\ncontributors: Intron Health, SisonkeBiotik, BioRAMP, Georgia Institute of\nTechnology, and MasakhaneNLP.\nVQA-RAD: This dataset was\ncreated by a research team led by Jason J. Lau, Soumya Gayen, Asma Ben\nAbacha, and Dina Demner-Fushman and their affiliated institutions (the US\nNational Library of Medicine and National Institutes of Health)\nChest ImaGenome: IBM\nResearch.\nMedExpQA:\nThis dataset was created by researchers at the HiTZ Center (Basque Center\nfor Language Technology and Artificial Intelligence).\nMedXpertQA: This\ndataset was developed by researchers at Tsinghua University (Beijing, China)\nand Shanghai Artificial Intelligence Laboratory (Shanghai, China).\nHealthSearchQA:\nThis dataset consists of consisting of 3,173 commonly searched consumer\nquestions\nIn addition to the public datasets listed above, MedGemma was also trained on\nde-identified, licensed datasets or datasets collected internally at Google from\nconsented participants.\nRadiology dataset 1: De-identified dataset of different CT studies\nacross body parts from a US-based radiology outpatient diagnostic center\nnetwork.\nOphthalmology dataset 1 (EyePACS): De-identified dataset of fundus\nimages from diabetic retinopathy screening.\nDermatology dataset 1: De-identified dataset of teledermatology skin\ncondition images (both clinical and dermatoscopic) from Colombia.\nDermatology dataset 2: De-identified dataset of skin cancer images (both\nclinical and dermatoscopic) from Australia.\nDermatology dataset 3: De-identified dataset of non-diseased skin images\nfrom an internal data collection effort.\nPathology dataset 1: De-identified dataset of histopathology H&E whole\nslide images created in collaboration with an academic research hospital and\nbiobank in Europe. Comprises de-identified colon, prostate, and lymph nodes.\nPathology dataset 2: De-identified dataset of lung histopathology H&E\nand IHC whole slide images created by a commercial biobank in the United\nStates.\nPathology dataset 3: De-identified dataset of prostate and lymph node\nH&E and IHC histopathology whole slide images created by a contract\nresearch organization in the United States.\nPathology dataset 4: De-identified dataset of histopathology whole slide\nimages created in collaboration with a large, tertiary teaching hospital in\nthe United States. Comprises a diverse set of tissue and stain types,\npredominantly H&E.\nEHR dataset 1: Question/answer dataset drawn from synthetic FHIR records\ncreated by Synthea. The test\nset includes 19 unique patients with 200 questions per patient divided into\n10 different categories.\nData citation\nMIMIC-CXR: Johnson, A., Pollard, T., Mark, R., Berkowitz, S., & Horng,\nS. (2024). MIMIC-CXR Database (version 2.1.0). PhysioNet.\nhttps://physionet.org/content/mimic-cxr/2.1.0/\nand Johnson, Alistair E. W., Tom J. Pollard, Seth J. Berkowitz, Nathaniel\nR. Greenbaum, Matthew P. Lungren, Chih-Ying Deng, Roger G. Mark, and Steven\nHorng. 2019. \"MIMIC-CXR, a de-Identified Publicly Available Database of\nChest Radiographs with Free-Text Reports.\" Scientific Data 6 (1): 1‚Äì8.\nSLAKE: Liu, Bo, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.\n2021.SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical\nVisual Question Answering.\"\nhttp://arxiv.org/abs/2102.09542.\nPAD-UEFS-20: Pacheco, Andre GC, et al. \"PAD-UFES-20: A skin lesion\ndataset composed of patient data and clinical images collected from\nsmartphones.\" Data in brief 32 (2020): 106221.\nSCIN: Ward, Abbi, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley\nCarrick, Bilson Campana, Jay Hartford, et al. 2024. \"Creating an Empirical\nDermatology Dataset Through Crowdsourcing With Web Search Advertisements.\"\nJAMA Network Open 7 (11): e2446615‚Äìe2446615.\nTCGA: The results shown here are in whole or part based upon data\ngenerated by the TCGA Research Network:\nhttps://www.cancer.gov/tcga.\nCAMELYON16: Ehteshami Bejnordi, Babak, Mitko Veta, Paul Johannes van\nDiest, Bram van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen A. W. M.\nvan der Laak, et al. 2017. \"Diagnostic Assessment of Deep Learning\nAlgorithms for Detection of Lymph Node Metastases in Women With Breast\nCancer.\" JAMA 318 (22): 2199‚Äì2210.\nMendeley Digital Knee X-Ray: Gornale, Shivanand; Patravali, Pooja\n(2020), \"Digital Knee X-ray Images\", Mendeley Data, V1, doi:\n10.17632/t9ndx37v5h.1\nVQA-RAD: Lau, Jason J., Soumya Gayen, Asma Ben Abacha, and Dina\nDemner-Fushman. 2018. \"A Dataset of Clinically Generated Visual Questions\nand Answers about Radiology Images.\" Scientific Data 5 (1): 1‚Äì10.\nChest ImaGenome: Wu, J., Agu, N., Lourentzou, I., Sharma, A., Paguio,\nJ., Yao, J. S., Dee, E. C., Mitchell, W., Kashyap, S., Giovannini, A., Celi,\nL. A., Syeda-Mahmood, T., & Moradi, M. (2021). Chest ImaGenome Dataset\n(version 1.0.0). PhysioNet. RRID:SCR_007345.\nhttps://doi.org/10.13026/wv01-y230\nMedQA: Jin, Di, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang,\nand Peter Szolovits. 2020. \"What Disease Does This Patient Have? A\nLarge-Scale Open Domain Question Answering Dataset from Medical Exams.\"\nhttp://arxiv.org/abs/2009.13081.\nAfrimedQA: Olatunji, Tobi, Charles Nimo, Abraham Owodunni, Tassallah\nAbdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, et al. 2024.\n\"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\nBenchmark Dataset.\"\nhttp://arxiv.org/abs/2411.15640.\nMedExpQA: Alonso, I., Oronoz, M., & Agerri, R. (2024). MedExpQA:\nMultilingual Benchmarking of Large Language Models for Medical Question\nAnswering. arXiv preprint arXiv:2404.05590. Retrieved from\nhttps://arxiv.org/abs/2404.05590\nMedXpertQA: Zuo, Yuxin, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu,\nErmo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. 2025. \"MedXpertQA:\nBenchmarking Expert-Level Medical Reasoning and Understanding.\"\nhttp://arxiv.org/abs/2501.18362.\nDe-identification/anonymization:\nGoogle and its partners utilize datasets that have been rigorously anonymized or\nde-identified to ensure the protection of individual research participants and\npatient privacy.\nImplementation information\nDetails about the model internals.\nSoftware\nTraining was done using JAX.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nUse and limitations\nIntended use\nMedGemma is an open multimodal generative AI model intended to be used as a\nstarting point that enables more efficient development of downstream healthcare\napplications involving medical text and images. MedGemma is intended for\ndevelopers in the life sciences and healthcare space. Developers are responsible\nfor training, adapting and making meaningful changes to MedGemma to accomplish\ntheir specific intended use. MedGemma models can be fine-tuned by developers\nusing their own proprietary data for their specific tasks or solutions.\nMedGemma is based on Gemma 3 and has been further trained on medical images and\ntext. MedGemma enables further development in any medical context (image and\ntextual), however the model was pre-trained using chest X-ray, pathology,\ndermatology, and fundus images. Examples of tasks within MedGemma's training\ninclude visual question answering pertaining to medical images, such as\nradiographs, or providing answers to textual medical questions. Full details of\nall the tasks MedGemma has been evaluated can be found in the MedGemma\nTechnical Report.\nBenefits\nProvides strong baseline medical image and text comprehension for models of\nits size.\nThis strong performance makes it efficient to adapt for downstream\nhealthcare-based use cases, compared to models of similar size without\nmedical data pre-training.\nThis adaptation may involve prompt engineering, grounding, agentic\norchestration or fine-tuning depending on the use case, baseline validation\nrequirements, and desired performance characteristics.\nLimitations\nMedGemma is not intended to be used without appropriate validation, adaptation\nand/or making meaningful modification by developers for their specific use case.\nThe outputs generated by MedGemma are not intended to directly inform clinical\ndiagnosis, patient management decisions, treatment recommendations, or any other\ndirect clinical practice applications. Performance benchmarks highlight baseline\ncapabilities on relevant benchmarks, but even for image and text domains that\nconstitute a substantial portion of training data, inaccurate model output is\npossible. All outputs from MedGemma should be considered preliminary and require\nindependent verification, clinical correlation, and further investigation\nthrough established research and development methodologies.\nMedGemma's multimodal capabilities have been primarily evaluated on single-image\ntasks. MedGemma has not been evaluated in use cases that involve comprehension\nof multiple images.\nMedGemma has not been evaluated or optimized for multi-turn applications.\nMedGemma's training may make it more sensitive to the specific prompt used than\nGemma 3.\nWhen adapting MedGemma developer should consider the following:\nBias in validation data: As with any research, developers should ensure\nthat any downstream application is validated to understand performance using\ndata that is appropriately representative of the intended use setting for\nthe specific application (e.g., age, sex, gender, condition, imaging device,\netc).\nData contamination concerns: When evaluating the generalization\ncapabilities of a large model like MedGemma in a medical context, there is a\nrisk of data contamination, where the model might have inadvertently seen\nrelated medical information during its pre-training, potentially\noverestimating its true ability to generalize to novel medical concepts.\nDevelopers should validate MedGemma on datasets not publicly available or\notherwise made available to non-institutional researchers to mitigate this\nrisk.\nRelease notes\nMay 20, 2025: Initial Release\nJuly 9, 2025 Bug Fix: Fixed the subtle degradation in the multimodal\nperformance. The issue was due to a missing end-of-image token in the model\nvocabulary, impacting combined text-and-image tasks. This fix reinstates and\ncorrectly maps that token, ensuring text-only tasks remain unaffected while\nrestoring multimodal performance.",
    "kakaocorp/kanana-1.5-8b-instruct-2505": "Kanana 1.5\nPerformance\nBase Model Evaluation\nInstruct Model Evaluation\nProcessing 32K+ Length\nContributors\nCitation\nContact\nü§ó 1.5 HF Models ¬† |\nüìï 1.5 Blog ¬† |\nüìú Technical Report\nNews üî•\n‚ú®2025/05/23: Published a blog post about Kanana 1.5 models and released ü§óHF model weights.\nüìú2025/02/27: Released Technical Report and ü§óHF model weights.\nüìï2025/01/10: Published a blog post about the development of Kanana Nano model.\nüìï2024/11/14: Published blog posts (pre-training, post-training) about the development of Kanana models.\n‚ñ∂Ô∏è2024/11/06: Published a presentation video about the development of the Kanana models.\nTable of Contents\nKanana 1.5\nPerformance\nBase Model Evaluation\nInstruct Model Evaluation\nProcessing 32K+ Length\nContributors\nCitation\nContact\nKanana 1.5\nKanana 1.5, a newly introduced version of the Kanana model family, presents substantial enhancements in coding, mathematics, and function calling capabilities over the previous version, enabling broader application to more complex real-world problems. This new version now can handle up to 32K tokens length natively and up to 128K tokens using YaRN, allowing the model to maintain coherence when handling extensive documents or engaging in extended conversations. Furthermore, Kanana 1.5 delivers more natural and accurate conversations through a refined post-training process.\nNeither the pre-training nor the post-training data includes Kakao user data.\nPerformance\nBase Model Evaluation\nModels\nMMLU\nKMMLU\nHAERAE\nHumanEval\nMBPP\nGSM8K\nKanana-1.5-8B\n64.24\n48.94\n82.77\n61.59\n57.80\n63.53\nKanana-8B\n64.22\n48.30\n83.41\n40.24\n51.40\n57.09\nInstruct Model Evaluation\nModels\nMT-Bench\nKoMT-Bench\nIFEval\nHumanEval+\nMBPP+\nGSM8K (0-shot)\nMATH\nMMLU (0-shot, CoT)\nKMMLU (0-shot, CoT)\nFunctionChatBench\nKanana-1.5-8B*\n7.76\n7.63\n80.11\n76.83\n67.99\n87.64\n67.54\n68.82\n48.28\n58.00\nKanana-8B\n7.13\n6.92\n76.91\n62.20\n43.92\n79.23\n37.68\n66.50\n47.43\n17.37\n* Models released under Apache 2.0 are trained on the latest versions compared to other models.\nProcessing 32K+ Length\nCurrently, the config.json uploaded to HuggingFace is configured for token lengths of 32,768 or less. To process tokens beyond this length, YaRN must be applied. By updating the config.json with the following parameters, you can apply YaRN to handle token sequences up to 128K in length:\n\"rope_scaling\": {\n\"factor\": 4.4,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\",\n\"beta_fast\": 64,\n\"beta_slow\": 2\n},\nContributors\nLanguage Model Training: Yunju Bak, Doohae Jung, Boseop Kim, Nayeon Kim, Hojin Lee, Jaesun Park, Minho Ryu\nLanguage Model Alignment: Jiyeon Ham, Seungjae Jung, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Daniel Wontae Nam\nAI Engineering: Youmin Kim, Hyeongju Kim\nCitation\n@misc{kananallmteam2025kananacomputeefficientbilinguallanguage,\ntitle={Kanana: Compute-efficient Bilingual Language Models},\nauthor={Kanana LLM Team and Yunju Bak and Hojin Lee and Minho Ryu and Jiyeon Ham and Seungjae Jung and Daniel Wontae Nam and Taegyeong Eo and Donghun Lee and Doohae Jung and Boseop Kim and Nayeon Kim and Jaesun Park and Hyunho Kim and Hyunwoong Ko and Changmin Lee and Kyoung-Woon On and Seulye Baeg and Junrae Cho and Sunghee Jung and Jieun Kang and EungGyun Kim and Eunhwa Kim and Byeongil Ko and Daniel Lee and Minchul Lee and Miok Lee and Shinbok Lee and Gaeun Seo},\nyear={2025},\neprint={2502.18934},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.18934},\n}\nContact\nKanana LLM Team Technical Support: kanana-llm@kakaocorp.com\nBusiness & Partnership Contact: alpha.k@kakaocorp.com",
    "nvidia/GEN3C-Cosmos-7B": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control\nDescription:\nLicense/Terms of Use:\nDeployment Geography:\nUse Case:\nRelease Date:\nReference:\nModel Architecture:\nInput:\nOutput:\nSoftware Integration:\nModel Version(s):\nInference:\nEthical Considerations:\nPlus Plus (++) Promise\nBias\nExplainability\nPrivacy\nSafety\nCitation\nGEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control\nCVPR 2025 (Highlight)\nXuanchi Ren*,\nTianchang Shen*\nJiahui Huang,\nHuan Ling,\nYifan Lu,\nMerlin Nimier-David,\nThomas M√ºller,\nAlexander Keller,\nSanja Fidler,\nJun Gao\n* indicates equal contribution\nDescription:\nGEN3C is a generative video model with precise camera control and temporal three-dimensional (3D) Consistency.  We achieve this with a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the two-dimensional (2D) renderings of the 3D cache with the new camera trajectory provided by the user. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video.\nThis model is ready for commercial/non-commercial use\nLicense/Terms of Use:\nThis model is released under the NVIDIA Open Model License. For a custom license, please contact cosmos-license@nvidia.com.\nImportant Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.\nDeployment Geography:\nGlobal\nUse Case:\nThis model is intended for researchers interested in developing consistent video generation and allows users to use cameras to control the final generation. For AV applications, we can enable users to generate driving videos and specify the camera trajectories in this video, such as switching from the viewpoint of a sedan car to a truck, or looking at a different lane.\nRelease Date:\nGithub 06/10/2025 via https://github.com/nv-tlabs/Gen3C\nReference:\nGEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control\nPaper, Project Page\nModel Architecture:\nArchitecture Type: Convolutional Neural Network (CNN), Transformer\nNetwork Architecture: Transformer\n**This model was developed based on Cosmos Predict 1\n** This model has 7B of model parameters.\nInput:\nInput Type(s): Camera Parameters, Image\nInput Format(s): 1D Array of Camera Poses, 2D Array of Images.\nInput Parameters: Camera Poses (1D), Images (2D)\nOther Properties Related to Input: The input image should be 720 * 1080 resolution, and we recommend using 121 frames for the camera parameters.\nOutput:\nOutput Type(s): Videos\nOutput Format: MP4 video\nOutput Parameters: 3D (N x H x W), with 3 channels (Red, Green, Blue ((RGB))\nOther Properties Related to Output: A sequence of images (N x H x W x 3), N is the number of frames, H is the height and W is the width. Three (3) refers to the number of RGB channels.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems A100 and H100. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nRuntime Engine(s):*Cosmos-Predict1\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Hopper\n[Preferred/Supported] Operating System(s):\nLinux\nModel Version(s):\n-V1.0\nInference:\nEngine: Cosmos-Predict1\nTest Hardware:\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Hopper\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nUsers are responsible for model inputs and outputs. Users are responsible for ensuring safe integration of this model, including implementing guardrails as well as other safety mechanisms, prior to deployment.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards link to subcard.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nPlus Plus (++) Promise\nWe value you, the datasets, the diversity they represent, and what we have been entrusted with. This model and its associated data have been:\nVerified to comply with current applicable disclosure laws, regulations, and industry standards.\nVerified to comply with applicable privacy labeling requirements.\nAnnotated to describe the collector/source (NVIDIA or a third-party).\nCharacterized for technical limitations.\nReviewed to ensure proper disclosure is accessible to, maintained for, and in compliance with NVIDIA data subjects and their requests.\nReviewed before release.\nTagged for known restrictions and potential safety implications.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing:\nNone\nMeasures taken to mitigate against unwanted bias:\nNone\nExplainability\nField\nResponse\nIntended Task/Domain:\nNovel view synthesis, video generation\nModel Type:\nTransformer\nIntended Users:\nPhysical AI developers.\nOutput:\nVideos\nDescribe how the model works:\nWe first predict depth for the input image, unproject it in to 3D to maintain a 3D cache. The 3D cache is then projected into a incomplete 2D video, which will be used as a condition for Cosmos to generate final video.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nNot Applicable.\nTechnical Limitations & Mitigation:\nWhile the model aims to create photorealistic scenes that replicate real-world conditions, it may generate outputs that are not entirely visually accurate and may require augmentation and/or real-world data depending on the scope and use case.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nQualitative and Quantitative Evaluation including PSNR, SSIM, LPIPS metrics. See Gen3C paper Section 5. for details.\nPotential Known Risks:\nThis model may inaccurately characterize depth, which will make the generated video un-realistic and prone to artifacts.\nLicensing:\nNVIDIA Open Model License\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\n[None Known]\nPersonal data used to create this model?\n[None Known]\nWas consent obtained for any personal data used?\n[None Known]\nHow often is dataset reviewed?\nBefore Release\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety\nField\nResponse\nModel Application Field(s):\nWorld Generation\nDescribe the life critical impact (if present).\nNone Known\nUse Case Restrictions:\nAbide by  NVIDIA Open Model License\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development.  Restrictions enforce dataset access during training, and dataset license constraints adhered to.\nCitation\n@inproceedings{ren2025gen3c,\ntitle={GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control},\nauthor={Ren, Xuanchi and Shen, Tianchang and Huang, Jiahui and Ling, Huan and\nLu, Yifan and Nimier-David, Merlin and M√ºller, Thomas and Keller, Alexander and\nFidler, Sanja and Gao, Jun},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\nyear={2025}\n}",
    "Snowflake/Arctic-Text2SQL-R1-7B": "Overview\nKey Features\nIntended Use\nNot intended for:\nEthical Considerations\nOverview\nArctic-Text2SQL-R1-7B is a 7-billion-parameter Text-to-SQL model fine-tuned using Group Relative Policy Optimization (GRPO) with a simple execution-based reward signal. It converts natural language questions into executable SQL queries.\nRead more in our paper: Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL.\nKey Features\nLightweight RL formulation: Uses only execution correctness and syntax validity as rewards.\nState-of-the-art performance: Achieves 68.9% execution accuracy on BIRD-dev and 68.5% on BIRD-test, with an average of 57.2% across six benchmarks (BIRD, Spider, Spider2.0, Spider-DK, EHRSQL, ScienceBenchmark)\nEfficiency: Outperforms many 70B+ models with only 7B parameters.\nIntended Use\nThis model is designed for:\nInteractive natural language interfaces to relational databases.\nData analytics tools enabling non-technical users to query databases.\nNot intended for:\nGeneration of non-SQL text or free-form natural language tasks.\nProduction systems without validation, especially in safety-critical domains.\nBenchmark\nDev/Test Accuracy\nBIRD-dev\n68.9%\nBIRD-test\n68.5%\nSpider-test\n88.8%\nSpider2.0-DK\n15.6%\nEHRSQL\n36.7%\nScienceBenchmark\n51.8%\nAverage\n57.2%\nEthical Considerations\nAvoid using for private or sensitive data without proper oversight.\nValidate generated SQL to prevent data leakage or unauthorized access.",
    "PKU-Alignment/SAE-V": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\n(ICML 2025 Poster) SAE-V: Interpreting Multimodal Models for Enhanced Alignment\n1.Training Parameter\n2. Quickstart\n(ICML 2025 Poster) SAE-V: Interpreting Multimodal Models for Enhanced Alignment\nThis repository contains the SAE-V model for our ICML 2025 Poster paper \"SAE-V: Interpreting Multimodal Models for Enhanced Alignment\", including 2 sparse autoencoder (SAE) and 3 sparse autoencoder with Vision (SAE-V). See each model folders and the source code for more information.\n1.Training Parameter\nAll 5 models training paramters are list below:\nHyper-parameters\nSAE and SAE-V of LLaVA-NeXT/Mistral\nSAE and SAE-V of Chameleon/Anole\nTraining Parameters\ntotal training steps\n30000\n30000\nbatch size\n4096\n4096\nLR\n5e-5\n5e-5\nLR warmup steps\n1500\n1500\nLR decay steps\n6000\n6000\nadam beta1\n0.9\n0.9\nadam beta2\n0.999\n0.999\nLR scheduler name\nconstant\nconstant\nLR coefficient\n5\n5\nseed\n42\n42\ndtype\nfloat32\nfloat32\nbuffer batches num\n32\n64\nstore batch size prompts\n4\n16\nfeature sampling window\n1000\n1000\ndead feature window\n1000\n1000\ndead feature threshold\n1e-4\n1e-4\nModel Parameters\nhook layer\n16\n8\ninput dimension\n4096\n4096\nexpansion factor\n16\n32\nfeature number\n65536\n131072\ncontext size\n4096\n2048\nThe differences in training parameters arise because the LLaVA-NeXT-7B model requires more GPU memory to handle vision input, so fewer batches can be cached. For the SAE and SAE-V parameters, we set different hook layers and context sizes based on the distinct architectures of the two models. We also experimented with different feature numbers on both models, but found that only around 30,000 features are actually activated during training. All training runs were conducted until convergence. All SAE and SAE-V training is performed on 8xA800 GPUs. We ensured that the variations in the parameters did not affect the experiment results.\n2. Quickstart\nThe SAE and SAE-V is developed based on SAELens-V. The loading example is as follow:\nfrom saev_lens import SAE\nsae = SAE.load_from_pretrained(\npath = \"./SAEV_LLaVA_NeXT-7b_OBELICS\",\ndevice =\"cuda:0\"\n)\nMore using tutorial is presented in SAELens-V.",
    "Haozhan72/Openvla-oft-SFT-libero10-trajall": "README.md exists but content is empty.",
    "Minthy/RouWei-0.8": "Features and prompting:\nImportant change:\nBasic settings:\nQuality tags:\nNegative prompt:\nArtist styles:\nGeneral styles:\nNatural text\nBrightness/colors/contrast:\nVpred version:\nBase model and Float version\nDiscord server\nSafety:\nLicense:\nThanks:\nDonations:\nIn depth retraining of Illustrious to achieve best prompt adherence, knowledge and state of the art performance.\nDataset of 13M unique pictures (~4M with natural text captions) picked and balanced from over 25M of anime art, covers, digital illustrations, western media and other sources, including private datasets. More detailed description on Civitai\nVpred version\nVpred version is now available.\nIt works flawlessly out of box without any burning or related issues. Consider to use lower CFG (3..5), other generation settings are same. Some exotic and experimental samplers/schedulers untested.\nKey advantages:\nFresh and wast knowledge about characters (1 2), concepts, styles(list examples), cultural and related things\nThe best prompt adherence among SDXL anime models at the moment of release\nSolved main problems with tags bleeding and biases, common for Illustrious, NoobAi and other checkpoints\nExcellent aesthetics and knowledge across a wide range of styles (over 50,000 artists, including hundreds of unique cherry-picked datasets from private galleries, including those received from the artists themselves)\nHigh flexibility and variety without stability tradeoff\nNo more annoying watermarks for popular styles thanks to clean dataset\nVibrant colors and smooth gradients without trace of burning, full range even with epsilon\nPure training from Illustrious v0.1 without involving third-party checkpoints, Loras, tweakers, etc.\nDataset cut-off - end if April 2025.\nFeatures and prompting:\nImportant change:\nWhen you are prompting artist styles, especially mixing several, their tags MUST BE in a separate CLIP chunk. Add BREAK after it (for A1111 and derivatives), use conditioning concat node (for Comfy) or at least put them in the very end. Otherwise, significant degradation of results is likely.\nThe model is designed to work both with short booru tag-based and long complex natural text prompts. The best result can be achieved using the combination of tags and some natural text phrases.\nFor tags classic danbooru-style comma-separated tags without underscores were used.\nBasic settings:\n~1..1.5 megapixel for txt2img, any AR with resolution multiple of 64 (1024x1024, 1152x, 1216x832,...). Euler_a, CFG 4..8 for epsilon/3..5 for vpred, 20..28steps. LCM/PCM/DMD untested, cfg++ samplers work fine, some shedulers not working. Highresfix: x1.5 latent + denoise 0.6 or any gan + denoise 0.3..0.55.\nPlease note that vpred version requires a lower CFG value.\nExamples can be found in repo, more on civitai.\nQuality tags:\nThere are only 4:\nmasterpiece, best quality\nfor positive and\nlow quality, worst quality\nfor negative\nNothing else. All except low quality in negative can be ommited.\nMeta tags like lowres have been removed, do not use them. Low resolution images have been either removed or upscaled and cleaned with DAT depending on their importance\nNegative prompt:\nworst quality, low quality, watermark\nFor best results keep it as clean as possible. Spamming of popular sequences will not improve results, since all related flaws have been solved, but will only lead to unwanted effects, biases and poor quality.\nArtist styles:\nThe model knows over 35k of artist styles. List, grids with example on Mega. Used with by , will not work properly without it.\nGeneral styles:\n2.5d, anime screencap, bold line, sketch, cgi, digital painting, flat colors, smooth shading, minimalistic, ink style, oil style, pastel style\nNatural text\nUse it in combination with booru tags, works great. Use only natural text after typing styles and quality tags. Use just booru tags and forget about it, it's all up to you. About 4M of pitures from dataset have hybrid natural-text captions made by Claude, GPT, Gemini and ToriiGate\nVersion 0.8 comes with advanced understanding of natural text prompts, providing state of the art performance among SDXL anime models.\nIt doesn't mean that you are obligated to use nl prompts, tags only - completely fine, especially because understanding of tags combinations is also improved.\nBrightness/colors/contrast:\nYou can use extra meta tags to control it:\nlow brightness, high brightness, low saturation, high saturation, low gamma, high gamma, sharp colors, soft colors, hdr, sdr\nVpred version:\nVpred version for RouWei-0.8 will come soon.\nBase model and Float version\nYou can use FP32 version for more accurate merging, or to get some benefits from using text encoders in fp32 mode with Comfy.\nEpsilon and vpred versions here have a brief aesthetic polishing after main training to improve small details and coherence. If you want to use RouWei in merges, extract or finetune it without bringing that last things - you can use BASE VERSION of RouWei: FP16 FP32\nDiscord server\njoin\nSafety:\nModel tends to generate NSFW images for corresponding prompts, consider to add extra filtering. Outputs may be inacurate and provocative and must not be used as a reference.\nLicense:\nSame as illustrious, please check out original page for limitation. Fell free to use in your merges, finetunes, ets. just please leave a link.\nThanks:\nA number of anonymous persons, Bakariso, dga, Fi., ello, K., LOL2024, NeuroSenko, rred, Soviet Cat, Sv1., T., TekeshiX and other fellow brothers that helped.\nDonations:\nBTC bc1qwv83ggq8rvv07uk6dv4njs0j3yygj3aax4wg6c\nETH/USDT(e) 0x04C8a749F49aE8a56CB84cF0C99CD9E92eDB17db\nXMR 47F7JAyKP8tMBtzwxpoZsUVB8wzg2VrbtDKBice9FAS1FikbHEXXPof4PAb42CQ5ch8p8Hs4RvJuzPHDtaVSdQzD6ZbA5TZ",
    "BlinkDL/rwkv-8-pile": "RWKV-8 trained on the Pile w/ \"20b tokenizer\" (332115325534 tokens)\nHere are early testing versions and I will iterate\nHow to run it:\nhttps://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7",
    "mradermacher/VideoRFT-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/QiWang98/VideoRFT\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nmmproj-Q8_0\n1.0\nmulti-modal supplement\nGGUF\nmmproj-f16\n1.5\nmulti-modal supplement\nGGUF\nQ2_K\n3.1\nGGUF\nQ3_K_S\n3.6\nGGUF\nQ3_K_M\n3.9\nlower quality\nGGUF\nQ3_K_L\n4.2\nGGUF\nIQ4_XS\n4.4\nGGUF\nQ4_K_S\n4.6\nfast, recommended\nGGUF\nQ4_K_M\n4.8\nfast, recommended\nGGUF\nQ5_K_S\n5.4\nGGUF\nQ5_K_M\n5.5\nGGUF\nQ6_K\n6.4\nvery good quality\nGGUF\nQ8_0\n8.2\nfast, best quality\nGGUF\nf16\n15.3\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "kakaocorp/kanana-safeguard-8b": "Kanana Safeguard\nÎ™®Îç∏ ÏÉÅÏÑ∏ÏÑ§Î™Ö\nÎ¶¨Ïä§ÌÅ¨ Î∂ÑÎ•ò Ï≤¥Í≥Ñ\nÏßÄÏõê Ïñ∏Ïñ¥\nÎπ†Î•∏ ÏãúÏûë\nü§ó HuggingFace Transformers\nÏÇ¨Ïö© ÏòàÏãú\nÌïôÏäµ Îç∞Ïù¥ÌÑ∞\nÌèâÍ∞Ä\nÌïúÍ≥ÑÏ†ê\nCitation\nContributors\nKanana Safeguard\nüì¶Models | üìï Blog\nÎ™®Îç∏ ÏÉÅÏÑ∏ÏÑ§Î™Ö\nKanana SafeguardÎäî Ïπ¥Ïπ¥Ïò§Ïùò ÏûêÏ≤¥ Ïñ∏Ïñ¥Î™®Îç∏Ïù∏ Kanana 8BÎ•º Í∏∞Î∞òÏúºÎ°ú Ìïú Ïú†Ìï¥ ÏΩòÌÖêÏ∏† ÌÉêÏßÄ Î™®Îç∏ÏûÖÎãàÎã§. Ïù¥ Î™®Îç∏ÏùÄ ÎåÄÌôîÌòï AI ÏãúÏä§ÌÖú ÎÇ¥ ÏÇ¨Ïö©Ïûê Î∞úÌôî ÎòêÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Ïùò ÎãµÎ≥ÄÏúºÎ°úÎ∂ÄÌÑ∞ Î¶¨Ïä§ÌÅ¨ Ïó¨Î∂ÄÎ•º Î∂ÑÎ•òÌïòÎèÑÎ°ù ÌïôÏäµÎêòÏóàÏäµÎãàÎã§. Î∂ÑÎ•ò Í≤∞Í≥ºÎäî <SAFE> ÎòêÎäî <UNSAFE-S4> ÌòïÏãùÏùò Îã®Ïùº ÌÜ†ÌÅ∞ÏúºÎ°ú Ï∂úÎ†•Îê©ÎãàÎã§. Ïó¨Í∏∞ÏóêÏÑú S4Îäî ÏÇ¨Ïö©Ïûê Î∞úÌôî ÎòêÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ ÎãµÎ≥ÄÏù¥ ÏúÑÎ∞òÌïú Î¶¨Ïä§ÌÅ¨ Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÏΩîÎìúÎ•º ÏùòÎØ∏Ìï©ÎãàÎã§.\nÏïÑÎûòÎäî Kanana Safeguard Î™®Îç∏Ïùò ÏûëÎèô ÏòàÏãúÏûÖÎãàÎã§.\nÎ¶¨Ïä§ÌÅ¨ Î∂ÑÎ•ò Ï≤¥Í≥Ñ\nÎ≥∏ Î™®Îç∏Ïùò Î¶¨Ïä§ÌÅ¨ Ïπ¥ÌÖåÍ≥†Î¶¨Îäî MLCommons  Î∂ÑÎ•òÏ≤¥Í≥ÑÏóê Í∏∞Î∞òÌïòÍ≥† ÏûàÏúºÎ©∞, Ïó¨Í∏∞Ïóê ÌïúÍµ≠ Î°úÏª¨ ÌäπÏÑ±Ïóê ÎßûÎäî Î¶¨Ïä§ÌÅ¨ Ïπ¥ÌÖåÍ≥†Î¶¨Î•º Ï∂îÍ∞ÄÌï®ÏúºÎ°úÏç® ÏïÑÎûòÏôÄ Í∞ôÏù¥ Ï¥ù 7Í∞ÄÏßÄ Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú Íµ¨ÏÑ±Îêú Î¶¨Ïä§ÌÅ¨ Î∂ÑÎ•òÏ≤¥Í≥ÑÎ•º ÏàòÎ¶ΩÌïòÏòÄÏäµÎãàÎã§.\nÎ≥∏ Î™®Îç∏ÏóêÏÑú ‚ë†ÏÇ¨Ïö©ÏûêÏùò Î∞úÌôîÏôÄ ‚ë°AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Ïùò ÎãµÎ≥ÄÏùÄ ÎèôÏùºÌïú Î¶¨Ïä§ÌÅ¨ Î∂ÑÎ•òÏ≤¥Í≥ÑÏóê ÏùòÌï¥ ÌåêÎ≥ÑÎê©ÎãàÎã§.\nÏΩîÎìú\nÏπ¥ÌÖåÍ≥†Î¶¨\nÏÑ§Î™Ö\nS1\nÏ¶ùÏò§\nÏ∂úÏã†, Ïù∏Ï¢Ö, Ïô∏Ïñë, Ïû•Ïï† Î∞è ÏßàÎ≥ë Ïú†Î¨¥, ÏÇ¨Ìöå Í≤ΩÏ†úÏ†Å ÏÉÅÌô© Î∞è ÏßÄÏúÑ, Ï¢ÖÍµê, Ïó∞Î†π, ÏÑ±Î≥Ñ¬∑ÏÑ± Ï†ïÏ≤¥ÏÑ±¬∑ÏÑ±Ï†Å ÏßÄÌñ• ÎòêÎäî Í∏∞ÌÉÄ Ï†ïÏ≤¥ÏÑ± ÏöîÏù∏ Îì±ÏùÑ Ïù¥Ïú†Î°ú ÌäπÏ†ï ÎåÄÏÉÅÏùÑ Ï∞®Î≥ÑÌïòÍ±∞ÎÇò, Ïù¥Îü¨Ìïú Ï∞®Î≥ÑÏóê Í∏∞Î∞òÌï¥ Í∞úÏù∏ ÎòêÎäî ÏßëÎã®ÏùÑ Í≥µÍ≤©ÌïòÎäî Î∞úÌôî\nS2\nÍ¥¥Î°≠Ìûò\nÌÉÄÏù∏ÏóêÍ≤å Î∂àÏæåÍ∞êÏù¥ÎÇò Íµ¥ÏöïÍ∞êÏùÑ Ï£ºÍ±∞ÎÇò, ÏúÑÌòëÏ†ÅÏù¥Í±∞ÎÇò, ÌäπÏ†ï ÎåÄÏÉÅÏóê ÎåÄÌïú Í¥¥Î°≠ÌûòÏùÑ Î∂ÄÏ∂îÍ∏∞Îäî Î∞úÌôî\nS3\nÏÑ±Ï†Å ÏΩòÌÖêÏ∏†\nÏÑ±Ï†Å ÌñâÏúÑÎÇò Ïã†Ï≤¥Î•º Î¨òÏÇ¨/ÏïîÏãúÌïòÍ±∞ÎÇò, ÏÑ±Ï†Å ÏàòÏπòÏã¨/ÌòêÏò§Í∞êÏùÑ ÏùºÏúºÌÇ¨ Ïàò ÏûàÎäî Î∞úÌôî (ÏÑ±ÍµêÏú° Î∞è Ïõ∞Îπô Ï†úÏô∏)\nS4\nÎ≤îÏ£Ñ\nÎ∂àÎ≤ïÏ†ÅÏù∏ ÌñâÏúÑ(Ïòà: Ìè≠Î†•‚àôÎπÑÌè≠Î†• Î≤îÏ£Ñ, ÏÑ±Î≤îÏ£Ñ, Î¨¥Í∏∞ Ï†úÏûë¬∑Ï°∞Îã¨)Î•º Í∏∞ÌöçÌïòÍ≥† Ï§ÄÎπÑÌïòÎäî Í≥ºÏ†ïÏùÑ Îã¥ÏùÄ Î∞úÌôî\nS5\nÏïÑÎèô ÏÑ±Ï∞©Ï∑®\nÏïÑÎèô ÎåÄÏÉÅÏùò ÏÑ±Ï†Å ÌïôÎåÄÏôÄ Í¥ÄÎ†®Îêú ÏÑ§Î™Ö, Í≤©Î†§, ÏßÄÏßÄ Îì±Ïùò Î∞úÌôî (Ïòà: Í∑∏Î£®Î∞ç, CSAM Í¥ÄÎ†® ÌÖçÏä§Ìä∏ Îì±)\nS6\nÏûêÏÇ¥ Î∞è ÏûêÌï¥\nÏùòÎèÑÏ†ÅÏúºÎ°ú ÏûêÏã†Ïùò ÏÉùÎ™ÖÏùÑ ÎÅäÍ±∞ÎÇò ÏûêÏã†Ïùò Ïã†Ï≤¥Î•º ÏùòÎèÑÏ†ÅÏúºÎ°ú Ìï¥ÏπòÎäî ÌñâÏúÑÎ•º Î¨òÏÇ¨ÌïòÍ±∞ÎÇò Ïú†ÎèÑÌïòÎäî Î∞úÌôî\nS7\nÏûòÎ™ªÎêú Ï†ïÎ≥¥\nÍ∞úÏù∏Ïù¥ÎÇò ÏßëÎã®ÏóêÍ≤å ÏûòÎ™ªÎêú Ï†ïÎ≥¥Î•º Ï†ÑÌååÌï† Ïàò ÏûàÎäî Î∞úÌôî\nÌëú 1. Kanana Safeguard Î¶¨Ïä§ÌÅ¨ Ïπ¥ÌÖåÍ≥†Î¶¨\nÏßÄÏõê Ïñ∏Ïñ¥\nKanana SafeguardÎäî ÌïúÍµ≠Ïñ¥Ïóê ÏµúÏ†ÅÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\nÎπ†Î•∏ ÏãúÏûë\nü§ó HuggingFace Transformers\nÎ™®Îç∏ÏùÑ Ïã§ÌñâÌïòÎ†§Î©¥ transformers>=4.51.3 ÎòêÎäî ÏµúÏã† Î≤ÑÏ†ÑÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.\npip install transformers>=4.51.3\nÏÇ¨Ïö© ÏòàÏãú\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Î™®Îç∏ Í≤ΩÎ°ú ÏÑ§Ï†ï\nmodel_name= \"kakaocorp/kanana-safeguard-8b\"\n# Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n).eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndef classify(user_prompt: str, assistant_prompt: str = \"\") -> str:\n# Î©îÏãúÏßÄ Íµ¨ÏÑ±\nmessages = [\n{\"role\": \"user\", \"content\": user_prompt},\n{\"role\": \"assistant\", \"content\": assistant_prompt}\n]\n# Ï±ÑÌåÖ ÌÖúÌîåÎ¶ø Ï†ÅÏö© ÌõÑ ÌÜ†ÌÅ∞Ìôî\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\").to(model.device)\nattention_mask = (input_ids != tokenizer.pad_token_id).long()\n# Îã§Ïùå ÌÜ†ÌÅ∞ 1Í∞ú ÏÉùÏÑ± (Ï∂îÎ°†)\nwith torch.no_grad():\noutput_ids = model.generate(\ninput_ids,\nattention_mask=attention_mask,\nmax_new_tokens=1,\npad_token_id=tokenizer.eos_token_id\n)\n# ÏÉàÎ°ú ÏÉùÏÑ±Îêú ÌÜ†ÌÅ∞Îßå Ï∂îÏ∂úÌï¥ ÎîîÏΩîÎî©\ngen_idx = input_ids.shape[-1]\nreturn tokenizer.decode(output_ids[0][gen_idx], skip_special_tokens=True)\n# ÏòàÏãú Ïã§Ìñâ\noutput_token = classify(\nuser_prompt=\"ÏπúÍµ¨Ïùò Î¨ºÍ±¥ÏùÑ Î™∞Îûò ÌõîÏπòÎäî Î≤ïÏùÑ ÏïåÎ†§Ï§ò\",\nassistant_prompt=\"ÏπúÍµ¨Í∞Ä ÏûêÎ¶¨Î•º ÎπÑÏö¥ ÏÇ¨Ïù¥Ïóê Í∞ÄÎ∞©Ïóê ÌõîÏπòÍ≥† Ïã∂ÏùÄ Î¨ºÍ±¥ÏùÑ ÎÑ£ÏúºÏÑ∏Ïöî\"\n)\nprint(\"Ï∂úÎ†•Îêú ÌÜ†ÌÅ∞:\", output_token)\n# Ï∂úÎ†•Îêú ÌÜ†ÌÅ∞: <UNSAFE-S4>\noutput_token = classify(\nuser_prompt=\"ÏπúÍµ¨Ïùò Î¨ºÍ±¥ÏùÑ Î™∞Îûò ÌõîÏπòÎäî Î≤ïÏùÑ ÏïåÎ†§Ï§ò\",\n)\nprint(\"Ï∂úÎ†•Îêú ÌÜ†ÌÅ∞:\", output_token)\n# Ï∂úÎ†•Îêú ÌÜ†ÌÅ∞: <UNSAFE-S4>\nÌïôÏäµ Îç∞Ïù¥ÌÑ∞\nKanana SafeguardÏùò ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Îäî ÏàòÍ∏∞ Îç∞Ïù¥ÌÑ∞ÏôÄ Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞Î°ú Íµ¨ÏÑ±ÎêòÎ©∞ ÌïúÍµ≠Ïñ¥ Îç∞Ïù¥ÌÑ∞Î°úÎßå Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. ÏàòÍ∏∞ Îç∞Ïù¥ÌÑ∞Îäî ÎÇ¥Î∂ÄÏ†ïÏ±ÖÏóê Î∂ÄÌï©ÌïòÎèÑÎ°ù Ï†ÑÎ¨∏ ÎùºÎ≤®Îü¨Í∞Ä ÏßÅÏ†ë ÏÉùÏÑ±ÌïòÍ≥† ÎùºÎ≤®ÎßÅÌïú Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§. Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞Îäî LLM Í∏∞Î∞ò ÌëúÌòÑ Î≥ÄÌôòÍ≥º ÎÖ∏Ïù¥Ï¶à ÏÇΩÏûÖ Îì± Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Í∏∞Î≤ïÏùÑ ÌÜµÌï¥ ÏÉùÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\nÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏóêÎäî ÏïàÏ†ÑÌïòÏßÄ ÏïäÏùÄ Î∞úÌôî Îç∞Ïù¥ÌÑ∞ Ïô∏ÏóêÎèÑ, Î™®Îç∏Ïùò Í±∞Ïßì ÏñëÏÑ±(false positive) ÎπÑÏú®ÏùÑ Ï§ÑÏù¥Í∏∞ ÏúÑÌï¥ Ïú†Ìï¥Ìïú ÏßàÎ¨∏Ïóê ÎåÄÌï¥ ÏïàÏ†ÑÌïòÍ≤å ÏùëÎãµÌïú AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Ïùò ÎåÄÌôî Îç∞Ïù¥ÌÑ∞Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\nÌèâÍ∞Ä\nKanana SafeguardÎäî SAFE/UNSAFE Ïù¥ÏßÑ Î∂ÑÎ•ò Í∏∞Ï§ÄÏúºÎ°ú ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌñàÏäµÎãàÎã§. Î™®Îì† ÌèâÍ∞ÄÎäî UNSAFEÎ•º ÏñëÏÑ±(positive) ÌÅ¥ÎûòÏä§Î°ú Í∞ÑÏ£ºÌïòÍ≥†, Î™®Îç∏Ïù¥ Ï∂úÎ†•Ìïú Ï≤´ Î≤àÏß∏ ÌÜ†ÌÅ∞ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÎ•òÌñàÏäµÎãàÎã§.\nÏô∏Î∂Ä Î≤§ÏπòÎßàÌÅ¨ Î™®Îç∏ÏùÄ Í∞Å Î™®Îç∏Ïùò Ï∂úÎ†•Í∞íÏóê ÎåÄÌï¥ Îã§ÏùåÍ≥º Í∞ôÏùÄ Î∞©ÏãùÏúºÎ°ú ÌèâÍ∞ÄÌïòÏòÄÏäµÎãàÎã§. LlamaGuardÎäî SAFE/UNSAFE ÌÜ†ÌÅ∞ÏùÑ Í∑∏ÎåÄÎ°ú ÌôúÏö©Ìï¥ Í≤∞Í≥ºÎ•º ÌåêÏ†ïÌñàÏäµÎãàÎã§. ShieldGemmaÎäî ÏûÑÍ≥ÑÏπòÎ•º 0.5Î°ú ÏÑ§Ï†ïÌïòÏó¨ Ïù¥ÏßÑ Î∂ÑÎ•òÎ•º ÏàòÌñâÌñàÏäµÎãàÎã§. GPT-4oÎäî Î¶¨Ïä§ÌÅ¨ Ïπ¥ÌÖåÍ≥†Î¶¨ Í∏∞Î∞ò Î∂ÑÎ•ò ÌîÑÎ°¨ÌîÑÌä∏Î•º zero-shot Î∞©ÏãùÏúºÎ°ú ÏûÖÎ†•ÌïòÍ≥†, Ï∂úÎ†• ÎÇ¥Ïö©Ïù¥ ÌäπÏ†ï ÏΩîÎìúÎ°ú Î∂ÑÎ•òÎêú Í≤ΩÏö∞ UNSAFEÎ°ú Í∞ÑÏ£ºÌïòÏó¨ Ïù¥ÏßÑ Î∂ÑÎ•òÎ•º ÏàòÌñâÌñàÏäµÎãàÎã§.\nÍ∑∏ Í≤∞Í≥º ÏûêÏ≤¥Ï†ÅÏúºÎ°ú Íµ¨Ï∂ïÌïú ÌïúÍµ≠Ïñ¥ ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú Kanana SafeguardÏùò Î∂ÑÎ•ò ÏÑ±Îä•Ïù¥ ÌÉÄ Î≤§ÏπòÎßàÌÅ¨ Î™®Îç∏ ÎåÄÎπÑ Í∞ÄÏû• Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ ÎÇòÌÉÄÎÉàÏäµÎãàÎã§.\nModel\nF1 Score\nPrecision\nRecall\nKanana Safeguard 8B\n0.946\n0.944\n0.948\nLlamaGuard3 8B\n0.540\n0.893\n0.387\nShieldGemma 9B\n0.477\n0.640\n0.380\nGPT-4o (zero-shot)\n0.763\n0.696\n0.843\nÌëú 2. Î¶¨Ïä§ÌÅ¨ Î∂ÑÎ•ò Ï≤¥Í≥ÑÏóê Îî∞Î•∏ ÎÇ¥Î∂Ä ÌïúÍµ≠Ïñ¥ ÌÖåÏä§Ìä∏ÏÖã Í∏∞Ï§Ä ÏùëÎãµ Î∂ÑÎ•ò ÏÑ±Îä• ÎπÑÍµê\nÎ™®Îì† Î™®Îç∏ÏùÄ ÎèôÏùºÌïú ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖãÍ≥º Î∂ÑÎ•ò Í∏∞Ï§ÄÏúºÎ°ú ÌèâÍ∞ÄÎêòÏóàÏúºÎ©∞, Ï†ïÏ±Ö Î∞è Î™®Îç∏ Íµ¨Ï°∞ Ï∞®Ïù¥Ïóê Îî∞Î•∏ ÏòÅÌñ•ÏùÑ ÏµúÏÜåÌôîÌïòÍ≥†, Í≥µÏ†ïÌïòÍ≥† Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ ÎπÑÍµêÍ∞Ä Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§.\nÌïúÍ≥ÑÏ†ê\nKanana SafeguardÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌïúÍ≥ÑÏ†êÏù¥ ÏûàÏúºÎ©∞, Ïù¥Îäî Ìñ•ÌõÑ ÏßÄÏÜçÏ†ÅÏúºÎ°ú Í∞úÏÑ†Ìï¥ÎÇòÍ∞à ÏòàÏ†ïÏûÖÎãàÎã§.\n1. Ïò§ÌÉêÏßÄ Í∞ÄÎä•ÏÑ± Ï°¥Ïû¨\nÎ≥∏ Î™®Îç∏ÏùÄ 100% ÏôÑÎ≤ΩÌïú Î∂ÑÎ•òÎ•º Î≥¥Ïû•ÌïòÏßÄ ÏïäÏäµÎãàÎã§. ÌäπÌûà, Î™®Îç∏Ïùò Ï†ïÏ±ÖÏùÄ ÏùºÎ∞òÏ†ÅÏù∏ ÏÇ¨Ïö©ÏÇ¨Î°ÄÏóê Í∏∞Î∞òÌïòÏó¨ ÏàòÎ¶ΩÎêòÏóàÍ∏∞ ÎïåÎ¨∏Ïóê ÌäπÏ†ïÌïú ÎèÑÎ©îÏù∏ÏóêÏÑúÎäî ÏûòÎ™ª Î∂ÑÎ•òÎê† Ïàò ÏûàÏäµÎãàÎã§.\n2. Context Ïù∏Ïãù ÎØ∏ÏßÄÏõê\nÎ≥∏ Î™®Îç∏ÏùÄ Ïù¥Ï†Ñ ÎåÄÌôî Ïù¥Î†•ÏùÑ Í∏∞Î∞òÏúºÎ°ú Î¨∏Îß•ÏùÑ Ïú†ÏßÄÌïòÍ±∞ÎÇò ÎåÄÌôîÎ•º Ïù¥Ïñ¥Í∞ÄÎäî Í∏∞Îä•ÏùÄ Ï†úÍ≥µÌïòÏßÄ ÏïäÏäµÎãàÎã§.\n3. Ï†úÌïúÎêú Î¶¨Ïä§ÌÅ¨ Ïπ¥ÌÖåÍ≥†Î¶¨\nÎ≥∏ Î™®Îç∏ÏùÄ Ï†ïÌï¥ÏßÑ Î¶¨Ïä§ÌÅ¨ÎßåÏùÑ ÌÉêÏßÄÌïòÎØÄÎ°ú Ïã§ÏÇ¨Î°ÄÏùò Î™®Îì† Î¶¨Ïä§ÌÅ¨Î•º ÌÉêÏßÄÌï† ÏàòÎäî ÏóÜÏäµÎãàÎã§. Îî∞ÎùºÏÑú ÏùòÎèÑÏóê Îî∞Îùº Kanana Safeguard-Siren(Î≤ïÏ†Å Î¶¨Ïä§ÌÅ¨ ÌÉêÏßÄ Î™®Îç∏), Kanana Safeguard-Prompt(ÌîÑÎ°¨ÌîÑÌä∏ Í≥µÍ≤© ÌÉêÏßÄ Î™®Îç∏)ÏôÄ Ìï®Íªò ÏÇ¨Ïö©ÌïòÎ©¥ Ï†ÑÏ≤¥Ï†ÅÏù∏ ÏïàÏ†ÑÏÑ±ÏùÑ ÎçîÏö± ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§.\nCitation\n@misc{Kanana Safeguard,\ntitle = {Kanana Safeguard},\nurl = {https://tech.kakao.com/posts/705},\nauthor = {Kanana Safeguard Team},\nmonth = {May},\nyear = {2025}\n}\nContributors\nJeongHwan Lee, Deok Jeong, HyeYeon Cho, JiEun Choi",
    "kingabzpro/medgemma-brain-cancer": "üß† MedGemma-Brain-Cancer\nüî¨ Model Details\nüìä Results & Notebook\nüöÄ Inference Example\nüß™ Intended Use\nüè∑Ô∏è Tags\nüìú License\nüß† MedGemma-Brain-Cancer\nmedgemma-brain-cancer is a fine-tuned version of google/medgemma-4b-it, trained specifically for brain tumor diagnosis and classification from MRI scans. This model leverages vision-language learning for enhanced medical imaging interpretation.\nüî¨ Model Details\nBase Model: google/medgemma-4b-it\nDataset: orvile/brain-cancer-mri-dataset\nFine-tuning Approach: Supervised fine-tuning (SFT) using Transformers Reinforcement Learning (TRL)\nTask: Brain tumor classification from MRI images\nPipeline Tag: image-text-to-text\nAccuracy Improvement:\nBase model accuracy: 33%\nFine-tuned model accuracy: 89%\nüìä Results & Notebook\nExplore the training pipeline, evaluation results, and experiments in the notebook:\nüëâ Fine_tuning_MedGemma.ipynb\nüöÄ Inference Example\n# pip install transformers accelerate\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"kingabzpro/medgemma-brain-cancer\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Example Brain MRI image ‚Äî attribution: Orvile, via Kaggle dataset\nimage_url = \"https://storage.googleapis.com/kagglesdsdata/datasets/7006196/11239552/Brain_Cancer%20raw%20MRI%20data/Brain_Cancer/brain_menin/brain_menin_0002.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250527%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250527T102729Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=4b83c95f9776b7f1f1a9d7184002f2f3c33b8d9c5fcfc3326b5f7bb9fa380910cd22534e28224a0b576abdd14f3ba2ebd0ef9ecca6ef8bd3fb1ba0aa048fe8a5cee77f06bebe91d9954793851a259a72f1c204e930e1f6957113d52a199ba7fa7d36841c943df7fcfbc599d76eb1e04999cee1e9a9d02afcc853418a7306da3e95b9f13ac16187e3d85e6dca81ffce7a6c71eee966a32166f0e6cd6f751e62883864f4d27401e0dc7de98645ca5ead9e9f5c6e989ca62448a46076885e4422acbe21b579f27616732b527f234ef9e172455777e550bc558ffd28107cc354057667befdc5c8e87475eaf7af4507ee6012d8b58130c62cf0171b86b4f8596c7677\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"text\": None, \"image\": image},\n{\"type\": \"text\", \"text\": \"What is the most likely type of brain cancer shown in the MRI image?\\nA: brain glioma\\nB: brain menin\\nC: brain tumor\"}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=True,\nreturn_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=20, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nExpected Output:\nB: brain menin\nüß™ Intended Use\nThis model is intended for research and educational purposes related to medical imaging, specifically brain tumor classification. It is not a certified diagnostic tool and should not be used in clinical decision-making without further validation.\nüè∑Ô∏è Tags\nmedical\nbrain_tumor\nmri\ntrl\nsft\nüìú License\nApache 2.0 License",
    "Xeno443/3wolfMondAI-SDG": "‚≠ê3wolfMondAI-SDG‚≠ê\nüü®Model Description\nüü®Uses\nüü®Prompt Recommendations\nüü®Generation Recommendations\nüü®More Information\nüü®Quantized version\nüü®Sampler / Scheduler Comparison Grid - base style\nüü®Sampler / Scheduler Comparison Grid - realism, photorealism\nüü®Style Examples Grid\nüü®Artist Tag Examples -  base style\nüü®Artist Tag Examples - realism, photorealism\nüü®RescaleCFG comparison\nüü®Example Generations (including metadata)\nüü®Uncommon Samplers Grid\n‚≠ê3wolfMondAI-SDG‚≠ê\nAn improved version of realMondAI-SDG to provide better realism and bring back toony styles support.\nPrompt recommendations\nGeneration recommendations\nSampler/Scheduler comparison\nStyle examples\nArtist examples\nRescaleCFG comparison\nExample Generations\nüü®Model Description\nDeveloped by: Big Lasagna\nFunded by: /sdg/\nDirect Download link\nAlso available on civitai.com\nAlso available on tensor.art\nArtist tag gallery: rentry.org\nüü®Uses\nThis merge combines the best of StableMondAI-SDG and RealMondAI-SDG into one model. Its base style is a stable semi-realistic look while the realism tags turn on the\nrealistic generations from RealMondAI, and using artist tags and style tags bring back the toony styles. Some can even be combined but some toony styles are trained\nso strong that they overwrite the realism completely. Play around with different weights to find a middle ground.\nThis model is in V-PREDICTION mode.\nPrompts need to use danbooru/e621 tags, no Natural Language Support is added at this point. Use noobAI quality tags at your own discretion.\nSee below for examples and comparison grids.\nüü®Prompt Recommendations\nStart with a basic prompt and work from there. All IllustriousXL/noobAIXL quality tags are recognized but these models work better with leaner prompts.\nStarter Positive prompt:\nmasterpiece, best quality, newest,\nStarter Negative prompt:\nworst quality,\nTo increase the realism, add these tags to the positive prompt\n(photorealism, realistic,) detailed background,\nand these to the negative prompt\nblurry background, blurry foreground, simple background,\nIf you want increased fur detail, try adding some of these tags into the prompt:\n(detailed fur:1.2), fluffy, shaggy fur,\nThe realism combined with some tags will try to pull your generations toward human features, if this happens you can add \"human (feature)\" to the neg prompt, e.g.\nhuman hands, human lips,\nüü®Generation Recommendations\nThe following settings are recommended starting points:\nSampler: Euler A\nScheduler: SGM Uniform\nSteps: 30\nCFG: 5.5\nRescaleCFG of 0.6 is recommended.\nEuler A & DPM++ 2s ancestral as well as normal Euler and DPM++2M seem to work best, SDE samplers tend to generate more artifacts on some schedulers. Karras is not recommended.\nüü®More Information\nüü®Quantized version\nA kind anon created quantized versions of this model which you can use if you are very low on VRAM or for special setups like Mixmod.\nYou can find them in the quantized model subfolder of the download section.\nüü®Sampler / Scheduler Comparison Grid - base style\nCFG 5.0 / RescaleCFG 0.7\nüü®Sampler / Scheduler Comparison Grid - realism, photorealism\nCFG 5.0 / RescaleCFG 0.7\nüü®Style Examples Grid\nüü®Artist Tag Examples -  base style\nAlso check this rentry with ~2000 artist tag example generations\nüü®Artist Tag Examples - realism, photorealism\nüü®RescaleCFG comparison\nüü®Example Generations (including metadata)\nEuler A / SGM UniformEuler A / NormalDPM++ 3M SDE / Align your stepsDPM++ 2M SDE / SGM Uniform\nDPM++ 3M SDE / Align Your Steps 32DPM++ 3M SDE / Align Your Steps 32DPM++ 2S a / NormalEuler A / Uniform\nüü®Uncommon Samplers Grid\nCFG 5.0 / RescaleCFG 0.7",
    "unsloth/Qwen2.5-Omni-7B-GGUF": "Qwen2.5-Omni\nOverview\nIntroduction\nKey Features\nModel Architecture\nPerformance\nQuickstart\nü§ó  Transformers Usage\nUsage Tips\nCitation\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nQwen2.5-Omni\nOverview\nIntroduction\nQwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner.\nKey Features\nOmni and Novel Architecture: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.\nReal-Time Voice and Video Chat: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.\nNatural and Robust Speech Generation: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.\nStrong Performance Across Modalities: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.\nExcellent End-to-End Speech Instruction Following: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.\nModel Architecture\nPerformance\nWe conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).\nMultimodality  -> Text\nDatasets\nModel\nPerformance\nOmniBenchSpeech | Sound Event | Music | Avg\nGemini-1.5-Pro\n42.67%|42.26%|46.23%|42.91%\nMIO-Instruct\n36.96%|33.58%|11.32%|33.80%\nAnyGPT (7B)\n17.77%|20.75%|13.21%|18.04%\nvideo-SALMONN\n34.11%|31.70%|56.60%|35.64%\nUnifiedIO2-xlarge\n39.56%|36.98%|29.25%|38.00%\nUnifiedIO2-xxlarge\n34.24%|36.98%|24.53%|33.98%\nMiniCPM-o\n-|-|-|40.50%\nBaichuan-Omni-1.5\n-|-|-|42.90%\nQwen2.5-Omni-3B\n52.14%|52.08%|52.83%|52.19%\nQwen2.5-Omni-7B\n55.25%|60.00%|52.83%|56.13%\nAudio -> Text\nDatasets\nModel\nPerformance\nASR\nLibrispeechdev-clean | dev other | test-clean | test-other\nSALMONN\n-|-|2.1|4.9\nSpeechVerse\n-|-|2.1|4.4\nWhisper-large-v3\n-|-|1.8|3.6\nLlama-3-8B\n-|-|-|3.4\nLlama-3-70B\n-|-|-|3.1\nSeed-ASR-Multilingual\n-|-|1.6|2.8\nMiniCPM-o\n-|-|1.7|-\nMinMo\n-|-|1.7|3.9\nQwen-Audio\n1.8|4.0|2.0|4.2\nQwen2-Audio\n1.3|3.4|1.6|3.6\nQwen2.5-Omni-3B\n2.0|4.1|2.2|4.5\nQwen2.5-Omni-7B\n1.6|3.5|1.8|3.4\nCommon Voice 15en | zh | yue | fr\nWhisper-large-v3\n9.3|12.8|10.9|10.8\nMinMo\n7.9|6.3|6.4|8.5\nQwen2-Audio\n8.6|6.9|5.9|9.6\nQwen2.5-Omni-3B\n9.1|6.0|11.6|9.6\nQwen2.5-Omni-7B\n7.6|5.2|7.3|7.5\nFleurszh | en\nWhisper-large-v3\n7.7|4.1\nSeed-ASR-Multilingual\n-|3.4\nMegrez-3B-Omni\n10.8|-\nMiniCPM-o\n4.4|-\nMinMo\n3.0|3.8\nQwen2-Audio\n7.5|-\nQwen2.5-Omni-3B\n3.2|5.4\nQwen2.5-Omni-7B\n3.0|4.1\nWenetspeechtest-net | test-meeting\nSeed-ASR-Chinese\n4.7|5.7\nMegrez-3B-Omni\n-|16.4\nMiniCPM-o\n6.9|-\nMinMo\n6.8|7.4\nQwen2.5-Omni-3B\n6.3|8.1\nQwen2.5-Omni-7B\n5.9|7.7\nVoxpopuli-V1.0-en\nLlama-3-8B\n6.2\nLlama-3-70B\n5.7\nQwen2.5-Omni-3B\n6.6\nQwen2.5-Omni-7B\n5.8\nS2TT\nCoVoST2en-de | de-en | en-zh | zh-en\nSALMONN\n18.6|-|33.1|-\nSpeechLLaMA\n-|27.1|-|12.3\nBLSP\n14.1|-|-|-\nMiniCPM-o\n-|-|48.2|27.2\nMinMo\n-|39.9|46.7|26.0\nQwen-Audio\n25.1|33.9|41.5|15.7\nQwen2-Audio\n29.9|35.2|45.2|24.4\nQwen2.5-Omni-3B\n28.3|38.1|41.4|26.6\nQwen2.5-Omni-7B\n30.2|37.7|41.4|29.4\nSER\nMeld\nWavLM-large\n0.542\nMiniCPM-o\n0.524\nQwen-Audio\n0.557\nQwen2-Audio\n0.553\nQwen2.5-Omni-3B\n0.558\nQwen2.5-Omni-7B\n0.570\nVSC\nVocalSound\nCLAP\n0.495\nPengi\n0.604\nQwen-Audio\n0.929\nQwen2-Audio\n0.939\nQwen2.5-Omni-3B\n0.936\nQwen2.5-Omni-7B\n0.939\nMusic\nGiantSteps Tempo\nLlark-7B\n0.86\nQwen2.5-Omni-3B\n0.88\nQwen2.5-Omni-7B\n0.88\nMusicCaps\nLP-MusicCaps\n0.291|0.149|0.089|0.061|0.129|0.130\nQwen2.5-Omni-3B\n0.325|0.163|0.093|0.057|0.132|0.229\nQwen2.5-Omni-7B\n0.328|0.162|0.090|0.055|0.127|0.225\nAudio Reasoning\nMMAUSound | Music | Speech | Avg\nGemini-Pro-V1.5\n56.75|49.40|58.55|54.90\nQwen2-Audio\n54.95|50.98|42.04|49.20\nQwen2.5-Omni-3B\n70.27|60.48|59.16|63.30\nQwen2.5-Omni-7B\n67.87|69.16|59.76|65.60\nVoice Chatting\nVoiceBenchAlpacaEval | CommonEval | SD-QA | MMSU\nUltravox-v0.4.1-LLaMA-3.1-8B\n4.55|3.90|53.35|47.17\nMERaLiON\n4.50|3.77|55.06|34.95\nMegrez-3B-Omni\n3.50|2.95|25.95|27.03\nLyra-Base\n3.85|3.50|38.25|49.74\nMiniCPM-o\n4.42|4.15|50.72|54.78\nBaichuan-Omni-1.5\n4.50|4.05|43.40|57.25\nQwen2-Audio\n3.74|3.43|35.71|35.72\nQwen2.5-Omni-3B\n4.32|4.00|49.37|50.23\nQwen2.5-Omni-7B\n4.49|3.93|55.71|61.32\nVoiceBenchOpenBookQA | IFEval | AdvBench | Avg\nUltravox-v0.4.1-LLaMA-3.1-8B\n65.27|66.88|98.46|71.45\nMERaLiON\n27.23|62.93|94.81|62.91\nMegrez-3B-Omni\n28.35|25.71|87.69|46.25\nLyra-Base\n72.75|36.28|59.62|57.66\nMiniCPM-o\n78.02|49.25|97.69|71.69\nBaichuan-Omni-1.5\n74.51|54.54|97.31|71.14\nQwen2-Audio\n49.45|26.33|96.73|55.35\nQwen2.5-Omni-3B\n74.73|42.10|98.85|68.81\nQwen2.5-Omni-7B\n81.10|52.87|99.42|74.12\nImage -> Text\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nOther Best\nQwen2.5-VL-7B\nGPT-4o-mini\nMMMUval\n59.2\n53.1\n53.9\n58.6\n60.0\nMMMU-Prooverall\n36.6\n29.7\n-\n38.3\n37.6\nMathVistatestmini\n67.9\n59.4\n71.9\n68.2\n52.5\nMathVisionfull\n25.0\n20.8\n23.1\n25.1\n-\nMMBench-V1.1-ENtest\n81.8\n77.8\n80.5\n82.6\n76.0\nMMVetturbo\n66.8\n62.1\n67.5\n67.1\n66.9\nMMStar\n64.0\n55.7\n64.0\n63.9\n54.8\nMMEsum\n2340\n2117\n2372\n2347\n2003\nMuirBench\n59.2\n48.0\n-\n59.2\n-\nCRPErelation\n76.5\n73.7\n-\n76.4\n-\nRealWorldQAavg\n70.3\n62.6\n71.9\n68.5\n-\nMME-RealWorlden\n61.6\n55.6\n-\n57.4\n-\nMM-MT-Bench\n6.0\n5.0\n-\n6.3\n-\nAI2D\n83.2\n79.5\n85.8\n83.9\n-\nTextVQAval\n84.4\n79.8\n83.2\n84.9\n-\nDocVQAtest\n95.2\n93.3\n93.5\n95.7\n-\nChartQAtest Avg\n85.3\n82.8\n84.9\n87.3\n-\nOCRBench_V2en\n57.8\n51.7\n-\n56.3\n-\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nQwen2.5-VL-7B\nGrounding DINO\nGemini 1.5 Pro\nRefcocoval\n90.5\n88.7\n90.0\n90.6\n73.2\nRefcocotextA\n93.5\n91.8\n92.5\n93.2\n72.9\nRefcocotextB\n86.6\n84.0\n85.4\n88.2\n74.6\nRefcoco+val\n85.4\n81.1\n84.2\n88.2\n62.5\nRefcoco+textA\n91.0\n87.5\n89.1\n89.0\n63.9\nRefcoco+textB\n79.3\n73.2\n76.9\n75.9\n65.0\nRefcocog+val\n87.4\n85.0\n87.2\n86.1\n75.2\nRefcocog+test\n87.9\n85.1\n87.2\n87.0\n76.2\nODinW\n42.4\n39.2\n37.3\n55.0\n36.7\nPointGrounding\n66.5\n46.2\n67.3\n-\n-\nVideo(without audio) -> Text\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nOther Best\nQwen2.5-VL-7B\nGPT-4o-mini\nVideo-MMEw/o sub\n64.3\n62.0\n63.9\n65.1\n64.8\nVideo-MMEw sub\n72.4\n68.6\n67.9\n71.6\n-\nMVBench\n70.3\n68.7\n67.2\n69.6\n-\nEgoSchematest\n68.6\n61.4\n63.2\n65.0\n-\nZero-shot Speech Generation\nDatasets\nModel\nPerformance\nContent Consistency\nSEEDtest-zh | test-en | test-hard\nSeed-TTS_ICL\n1.11 | 2.24 | 7.58\nSeed-TTS_RL\n1.00 | 1.94 | 6.42\nMaskGCT\n2.27 | 2.62 | 10.27\nE2_TTS\n1.97 | 2.19 | -\nF5-TTS\n1.56 | 1.83 | 8.67\nCosyVoice 2\n1.45 | 2.57 | 6.83\nCosyVoice 2-S\n1.45 | 2.38 | 8.08\nQwen2.5-Omni-3B_ICL\n1.95 | 2.87 | 9.92\nQwen2.5-Omni-3B_RL\n1.58 | 2.51 | 7.86\nQwen2.5-Omni-7B_ICL\n1.70 | 2.72 | 7.97\nQwen2.5-Omni-7B_RL\n1.42 | 2.32 | 6.54\nSpeaker Similarity\nSEEDtest-zh | test-en | test-hard\nSeed-TTS_ICL\n0.796 | 0.762 | 0.776\nSeed-TTS_RL\n0.801 | 0.766 | 0.782\nMaskGCT\n0.774 | 0.714 | 0.748\nE2_TTS\n0.730 | 0.710 | -\nF5-TTS\n0.741 | 0.647 | 0.713\nCosyVoice 2\n0.748 | 0.652 | 0.724\nCosyVoice 2-S\n0.753 | 0.654 | 0.732\nQwen2.5-Omni-3B_ICL\n0.741 | 0.635 | 0.748\nQwen2.5-Omni-3B_RL\n0.744 | 0.635 | 0.746\nQwen2.5-Omni-7B_ICL\n0.752 | 0.632 | 0.747\nQwen2.5-Omni-7B_RL\n0.754 | 0.641 | 0.752\nText -> Text\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nQwen2.5-7B\nQwen2.5-3B\nQwen2-7B\nLlama3.1-8B\nGemma2-9B\nMMLU-Pro\n47.0\n40.4\n56.3\n43.7\n44.1\n48.3\n52.1\nMMLU-redux\n71.0\n60.9\n75.4\n64.4\n67.3\n67.2\n72.8\nLiveBench0831\n29.6\n22.3\n35.9\n26.8\n29.2\n26.7\n30.6\nGPQA\n30.8\n34.3\n36.4\n30.3\n34.3\n32.8\n32.8\nMATH\n71.5\n63.6\n75.5\n65.9\n52.9\n51.9\n44.3\nGSM8K\n88.7\n82.6\n91.6\n86.7\n85.7\n84.5\n76.7\nHumanEval\n78.7\n70.7\n84.8\n74.4\n79.9\n72.6\n68.9\nMBPP\n73.2\n70.4\n79.2\n72.7\n67.2\n69.6\n74.9\nMultiPL-E\n65.8\n57.6\n70.4\n60.2\n59.1\n50.7\n53.4\nLiveCodeBench2305-2409\n24.6\n16.5\n28.7\n19.9\n23.9\n8.3\n18.9\nQuickstart\nBelow, we provide simple examples to show how to use Qwen2.5-Omni with ü§ó Transformers. The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to build from source with command:\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\npip install accelerate\nor you might encounter the following error:\nKeyError: 'qwen2_5_omni'\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has ffmpeg installed:\n# It's highly recommended to use `[decord]` feature for faster video loading.\npip install qwen-omni-utils[decord] -U\nIf you are not using Linux, you might not be able to install decord from PyPI. In that case, you can use pip install qwen-omni-utils -U which will fall back to using torchvision for video processing. However, you can still install decord from source to get decord used when loading video.\nü§ó  Transformers Usage\nHere we show a code snippet to show you how to use the chat model with transformers and qwen_omni_utils:\nimport soundfile as sf\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-7B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\nconversation = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n],\n},\n]\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n\"output.wav\",\naudio.reshape(-1).detach().cpu().numpy(),\nsamplerate=24000,\n)\nMinimum GPU memory requirements\nModel\nPrecision\n15(s) Video\n30(s) Video\n60(s) Video\nQwen-Omni-3B\nFP32\n89.10 GB\nNot Recommend\nNot Recommend\nQwen-Omni-3B\nBF16\n18.38 GB\n22.43 GB\n28.22 GB\nQwen-Omni-7B\nFP32\n93.56 GB\nNot Recommend\nNot Recommend\nQwen-Omni-7B\nBF16\n31.11 GB\n41.85 GB\n60.19 GB\nNote: The table above presents the theoretical minimum memory requirements for inference with transformers and BF16 is test with attn_implementation=\"flash_attention_2\"; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource here.\nVideo URL resource usage\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by FORCE_QWENVL_VIDEO_READER=torchvision or FORCE_QWENVL_VIDEO_READER=decord if you prefer not to use the default one.\nBackend\nHTTP\nHTTPS\ntorchvision >= 0.19.0\n‚úÖ\n‚úÖ\ntorchvision < 0.19.0\n‚ùå\n‚ùå\ndecord\n‚úÖ\n‚ùå\nBatch inference\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when return_audio=False is set. Here is an example.\n# Sample messages for batch inference\n# Conversation with video only\nconversation1 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n]\n}\n]\n# Conversation with audio only\nconversation2 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n]\n}\n]\n# Conversation with pure text\nconversation3 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": \"who are you?\"\n}\n]\n# Conversation with mixed media\nconversation4 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"/path/to/image.jpg\"},\n{\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n{\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n{\"type\": \"text\", \"text\": \"What are the elements can you see and hear in these medias?\"},\n],\n}\n]\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n# Batch Inference\ntext_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nUsage Tips\nPrompt for audio output\nIf users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n}\nUse audio in video\nIn the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.\n# first place, in data preprocessing\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=True)\n# second place, in model processor\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\",\npadding=True, use_audio_in_video=True)\n#  third place, in model inference\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=True)\nIt is worth noting that during a multi-round conversation, the use_audio_in_video parameter in these places must be set to the same, otherwise unexpected results will occur.\nUse audio output or not\nThe model supports both text and audio outputs, if users do not need audio outputs, they can call model.disable_talker() after init the model. This option will save about ~2GB of GPU memory but the return_audio option for generate function will only allow to be set at False.\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-Omni-7B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nmodel.disable_talker()\nIn order to obtain a flexible experience, we recommend that users can decide whether to return audio when generate function is called. If return_audio is set to False, the model will only return text outputs to get text responses faster.\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-Omni-7B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n...\ntext_ids = model.generate(**inputs, return_audio=False)\nChange voice type of output audio\nQwen2.5-Omni supports the ability to change the voice of the output audio. The \"Qwen/Qwen2.5-Omni-7B\" checkpoint support two voice types as follow:\nVoice Type\nGender\nDescription\nChelsie\nFemale\nA honeyed, velvety voice that carries a gentle warmth and luminous clarity.\nEthan\nMale\nA bright, upbeat voice with infectious energy and a warm, approachable vibe.\nUsers can use the speaker parameter of generate function to specify the voice type. By default, if speaker is not specified, the default voice type is Chelsie.\ntext_ids, audio = model.generate(**inputs, speaker=\"Chelsie\")\ntext_ids, audio = model.generate(**inputs, speaker=\"Ethan\")\nFlash-Attention 2 to speed up generation\nFirst, make sure to install the latest version of Flash Attention 2:\npip install -U flash-attn --no-build-isolation\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the flash attention repository. FlashAttention-2 can only be used when a model is loaded in torch.float16 or torch.bfloat16.\nTo load and run a model using FlashAttention-2, add attn_implementation=\"flash_attention_2\" when loading the model:\nfrom transformers import Qwen2_5OmniForConditionalGeneration\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-Omni-7B\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\n)\nCitation\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n@article{Qwen2.5-Omni,\ntitle={Qwen2.5-Omni Technical Report},\nauthor={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\njournal={arXiv preprint arXiv:2503.20215},\nyear={2025}\n}",
    "openmmlab-community/mm_grounding_dino_large_all": "MM Grounding DINO (large variant)\nIntended uses\nTraining Data\nEvaluation results\nBibTeX entry and citation info\nMM Grounding DINO (large variant)\nMM Grounding DINO model was proposed in An Open and Comprehensive Pipeline for Unified Object Grounding and Detection by Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang.\nMM Grounding DINO improves upon the Grounding DINO by improving the contrastive class head and removing the parameter sharing in the decoder, improving zero-shot detection performance on both COCO (50.6(+2.2) AP) and LVIS (31.9(+11.8) val AP and 41.4(+12.6) minival AP).\nYou can find all the original MM Grounding DINO checkpoints under the MM Grounding DINO collection.\nIntended uses\nYou can use the raw model for zero-shot object detection.\nHere's how to use the model for zero-shot object detection:\nimport torch\nfrom transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\nfrom transformers.image_utils import load_image\n# Prepare processor and model\nmodel_id = \"rziga/mm_grounding_dino_large_all\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n# Prepare inputs\nimage_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = load_image(image_url)\ntext_labels = [[\"a cat\", \"a remote control\"]]\ninputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n# Run inference\nwith torch.no_grad():\noutputs = model(**inputs)\n# Postprocess outputs\nresults = processor.post_process_grounded_object_detection(\noutputs,\nthreshold=0.4,\ntarget_sizes=[(image.height, image.width)]\n)\n# Retrieve the first image result\nresult = results[0]\nfor box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\nbox = [round(x, 2) for x in box.tolist()]\nprint(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")\nTraining Data\nThis model was trained on:\nObjects365v1\nOpen Images v6\nGOLD-G\nV3Det\nCOCO 2017\nLVIS v1.0\nCOCO 2014\nGRIT\nRefCOCO\nRefCOCO+\nRefCOCOg\ngRefCOCO\nEvaluation results\nHere's a table of models and their object detection performance results on COCO (results from official repo):\nModel\nBackbone\nPre-Train Data\nStyle\nCOCO mAP\nmm_grounding_dino_tiny_o365v1_goldg\nSwin-T\nO365,GoldG\nZero-shot\n50.4(+2.3)\nmm_grounding_dino_tiny_o365v1_goldg_grit\nSwin-T\nO365,GoldG,GRIT\nZero-shot\n50.5(+2.1)\nmm_grounding_dino_tiny_o365v1_goldg_v3det\nSwin-T\nO365,GoldG,V3Det\nZero-shot\n50.6(+2.2)\nmm_grounding_dino_tiny_o365v1_goldg_grit_v3det\nSwin-T\nO365,GoldG,GRIT,V3Det\nZero-shot\n50.4(+2.0)\nmm_grounding_dino_base_o365v1_goldg_v3det\nSwin-B\nO365,GoldG,V3Det\nZero-shot\n52.5\nmm_grounding_dino_base_all\nSwin-B\nO365,ALL\n-\n59.5\nmm_grounding_dino_large_o365v2_oiv6_goldg\nSwin-L\nO365V2,OpenImageV6,GoldG\nZero-shot\n53.0\nmm_grounding_dino_large_all\nSwin-L\nO365V2,OpenImageV6,ALL\n-\n60.3\nHere's a table of MM Grounding DINO tiny models and their object detection performance on LVIS (results from official repo):\nModel\nPre-Train Data\nMiniVal APr\nMiniVal APc\nMiniVal APf\nMiniVal AP\nVal1.0 APr\nVal1.0 APc\nVal1.0 APf\nVal1.0 AP\nmm_grounding_dino_tiny_o365v1_goldg\nO365,GoldG\n28.1\n30.2\n42.0\n35.7(+6.9)\n17.1\n22.4\n36.5\n27.0(+6.9)\nmm_grounding_dino_tiny_o365v1_goldg_grit\nO365,GoldG,GRIT\n26.6\n32.4\n41.8\n36.5(+7.7)\n17.3\n22.6\n36.4\n27.1(+7.0)\nmm_grounding_dino_tiny_o365v1_goldg_v3det\nO365,GoldG,V3Det\n33.0\n36.0\n45.9\n40.5(+11.7)\n21.5\n25.5\n40.2\n30.6(+10.5)\nmm_grounding_dino_tiny_o365v1_goldg_grit_v3det\nO365,GoldG,GRIT,V3Det\n34.2\n37.4\n46.2\n41.4(+12.6)\n23.6\n27.6\n40.5\n31.9(+11.8)\nBibTeX entry and citation info\n@article{zhao2024open,\ntitle={An Open and Comprehensive Pipeline for Unified Object Grounding and Detection},\nauthor={Zhao, Xiangyu and Chen, Yicheng and Xu, Shilin and Li, Xiangtai and Wang, Xinjiang and Li, Yining and Huang, Haian},\njournal={arXiv preprint arXiv:2401.02361},\nyear={2024}\n}",
    "zibojia/minimax-remover": "üöÄ Overview\n‚ú® Features:\nüõ†Ô∏è Installation\nüìÇ Download\n‚ö° Quick Start\nMinimal Example\nüìß Contact\nMiniMax-Remover: Taming Bad Noise Helps Video Object Removal\nBojia Zi*,\nWeixuan Peng*,\nXianbiao Qi‚Ä†,\nJianan Wang, Shihao Zhao, Rong Xiao, Kam-Fai Wong\n* Equal contribution. ‚Ä† Corresponding author.\nüöÄ Overview\nMiniMax-Remover is a fast and effective video object remover based on minimax optimization. It operates in two stages: the first stage trains a remover using a simplified DiT architecture, while the second stage distills a robust remover with CFG removal and fewer inference steps.\n‚ú® Features:\nFast: Requires only 6 inference steps and does not use CFG, making it highly efficient.\nEffective: Seamlessly removes objects from videos and generates high-quality visual content.\nRobust: Maintains robustness by preventing the regeneration of undesired objects or artifacts within the masked region, even under varying noise conditions.\nüõ†Ô∏è Installation\nAll dependencies are listed in requirements.txt.\npip install -r requirements.txt\nüìÇ Download\nhuggingface-cli download zibojia/minimax-remover --include vae transformer scheduler --local-dir .\n‚ö° Quick Start\nMinimal Example\nimport torch\nfrom diffusers.utils import export_to_video\nfrom decord import VideoReader\nfrom diffusers.models import AutoencoderKLWan\nfrom transformer_minimax_remover import Transformer3DModel\nfrom diffusers.schedulers import UniPCMultistepScheduler\nfrom pipeline_minimax_remover import Minimax_Remover_Pipeline\nrandom_seed = 42\nvideo_length = 81\ndevice = torch.device(\"cuda:0\")\n# Load model weights separately\nvae = AutoencoderKLWan.from_pretrained(\"./vae\", torch_dtype=torch.float16)\ntransformer = Transformer3DModel.from_pretrained(\"./transformer\", torch_dtype=torch.float16)\nscheduler = UniPCMultistepScheduler.from_pretrained(\"./scheduler\")\nimages = # images in range [-1, 1]\nmasks = # masks in range [0, 1]\n# Initialize the pipeline (pass the loaded weights as objects)\npipe = Minimax_Remover_Pipeline(vae=vae, transformer=transformer, \\\nscheduler=scheduler, torch_dtype=torch.float16\n).to(device)\nresult = pipe(images=images, masks=masks, num_frames=video_length, height=480, width=832, \\\nnum_inference_steps=12, generator=torch.Generator(device=device).manual_seed(random_seed), iterations=6 \\\n).frames[0]\nexport_to_video(result, \"./output.mp4\")\nüìß Contact\nFeel free to send an email to 19210240030@fudan.edu.cn if you have any questions or suggestions.",
    "Kutches/UncensoredV2": "No model card",
    "QCRI/Fanar-1-9B-Instruct": "Fanar-1-9B-Instruct\nModel Details\nModel Training\nGetting Started\nIntended Use\nEthical Considerations & Limitations\nThe output generated by this model is not considered a statement of QCRI, HBKU, Qatar Foundation, MCIT or any other organization or individual.\nEvaluation\nCitation\nAcknowledgements\nLicense\nFanar-1-9B-Instruct\nFanar-1-9B-Instruct is a powerful Arabic-English LLM developed by Qatar Computing Research Institute (QCRI) at Hamad Bin Khalifa University (HBKU), a member of Qatar Foundation for Education, Science, and Community Development. It is the instruction-tuned version of Fanar-1-9B. We continually pretrain the google/gemma-2-9b model on 1T Arabic and English tokens. We pay particular attention to the richness of the Arabic language by supporting Modern Standard Arabic (MSA) and a diverse set of Arabic dialects, including Gulf, Levantine, and Egyptian. Fanar models, through meticulous curation of the pretraining and instruction-tuning data, are aligned with Islamic values and Arab cultures.\nFanar-1-9B-Instruct is a core component of the Fanar GenAI platform that offers a suite of capabilities including image generation, video and image understanding, deep thinking, advanced text-to-speech (TTS) and automatic-speech-recognition (ASR), attribution and fact-checking, Islamic RAG, among several other features.\nWe have published a comprehensive report with all the details regarding our Fanar GenAI platform. We also provide an API to our models and the GenAI platform (request access here).\nModel Details\nAttribute\nValue\nDeveloped by\nQCRI at HBKU\nSponsored by\nMinistry of Communications and Information Technology, State of Qatar\nModel Type\nAutoregressive Transformer\nParameter Count\n8.7 Billion\nContext Length\n4096 Tokens\nInput\nText only\nOutput\nText only\nTraining Framework\nLitGPT\nPretraining Token Count\n1 Trillion (ar + en)\nSFT Instructions\n4.5M\nDPO Preference Pairs\n250K\nLanguages\nArabic, English\nLicense\nApache 2.0\nModel Training\nPretraining\nFanar-1-9B-Instruct was continually pretrained on 1T tokens, with a balanced focus on Arabic and English: ~515B English tokens from a carefully curated subset of the Dolma dataset, 410B Arabic tokens that we collected, parsed, and filtered from a variety of sources, and 102B code tokens curated from The Stack dataset. Our codebase used the LitGPT framework.\nPost-training\nFanar-1-9B-Instruct underwent a two-phase post-training pipeline:\nPhase\nSize\nSupervised Fine-tuning (SFT)\n4.5M Instructions\nDirect Preference Optimization (DPO)\n250K Preference Pairs\nGetting Started\nFanar-1-9B-Instruct is compatible with the Hugging Face transformers library (‚â• v4.40.0). Here's how to load and use the model:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"QCRI/Fanar-1-9B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n# message content may be in Arabic or English\nmessages = [\n{\"role\": \"user\", \"content\": \"ŸÖÿß ŸáŸä ÿπÿßÿµŸÖÿ© ŸÇÿ∑ÿ±ÿü\"},\n]\ninputs = tokenizer.apply_chat_template(messages, tokenize=False, return_tensors=\"pt\")\noutputs = model.generate(**tokenizer(inputs, return_tensors=\"pt\", return_token_type_ids=False), max_new_tokens=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nInference using VLLM is also supported:\nfrom vllm import LLM, SamplingParams\nmodel_name = \"QCRI/Fanar-1-9B-Instruct\"\nllm = LLM(model=model_name)\nsampling_params = SamplingParams(temperature=0.7, max_tokens=256)\n# message content may be in Arabic or English\nmessages = [\n{\"role\": \"user\", \"content\": \"ŸÖÿß ŸáŸä ÿπÿßÿµŸÖÿ© ŸÇÿ∑ÿ±ÿü\"},\n]\noutputs = llm.chat(messages, sampling_params)\nprint(outputs[0].outputs[0].text)\nIntended Use\nFanar-1-9B-Instruct is built for:\nConversational agents (Arabic only or bilingual)\nCultural and dialectal question answering in Arabic\nEducational, governmental, and civic NLP applications focused on the Arab world or Arabic-speaking audiences\nResearch on Arabic natural language generation and understanding\nFanar-1-9B-Instruct can be deployed as part of a broader AI system. Developers are encouraged to implement proper safeguards to ensure culturally respectful, accurate, and safe deployment. It should not be used to generate or spread harmful, illegal, or misleading content.\nA version of this model can be accessed through Fanar Chat. We are continuously improving the Fanar‚Äôs models and capabilities, and answers can differ from what you get from Fanar-1-9B-Instruct.\nEthical Considerations & Limitations\nFanar-1-9B-Instruct is capable of generating fluent and contextually appropriate responses. However, as with any generative model there are uncertainities. The model may produce biased, offensive, or incorrect outputs. The model is not suitable for high-stakes decision-making (e.g., legal, medical, or financial advice). Though we have extensively tested Fanar-1-9B-Instruct and attempted to mitigate these issues, we cannot redress every possible scenario. Thus, we advise developers to implement safety checks and perform domain-specific fine-tuning for sensitive use cases. Kindly refer to our Terms of Service and Privacy Policy.\nThe output generated by this model is not considered a statement of QCRI, HBKU, Qatar Foundation, MCIT or any other organization or individual.\nEvaluation\nEvaluation was conducted using a modified version of the LM Evaluation Harness and internal cultural alignment benchmarks.\nModel\nMMLU (5-shot)\nMMMLU (Arabic) (0-shot)\nArabicMMLU (3-shot)\nHellaSwag (0-shot)\nPIQA (0-shot)\nARC Challenge (0-shot)\nBelebele (Arabic) (3-shot)\nACVA (5-shot)\nGSM8k\nOALL (0-shot)\nOALL v2 (0-shot)\nAlmieyar Arabic (3-shot)\nArab Cultural MCQ (3-shot)\nAraDiCE PIQA (MSA) (0-shot)\nAraDiCE PIQA(Egy) (0-shot)\nAraDiCE PIQA(Lev) (0-shot)\nAraDiCE ArabicMMLU(Egy) (0-shot)\nAraDiCE ArabicMMLU(Lev) (0-shot)\nFanar-1-9B-it\n71.53%\n58.89%\n67.69%\n83.16%\n82.54%\n67.15%\n83.22%\n80.02%\n74.60%\n68.32%\n66.29%\n78.68%\n72.40%\n67.68%\n63.66%\n59.03%\n59.63%\n60.62%\nALLaM-7B-Instruct-preview\n60.72%\n54.89%\n68.59%\n76.35%\n80.52%\n51.62%\n75.80%\n74.52%\n46.63%\n57.31%\n63.66%\n76.31%\n74.20%\n67.52%\n63.44%\n60.88%\n62.50%\n64.17%\naya-expanse-8b\n62.85%\n47.14%\n60.10%\n78.54%\n81.18%\n56.40%\n70.78%\n77.11%\n8.26%\n53.18%\n59.74%\n70.20%\n67.30%\n63.00%\n59.41%\n56.53%\n53.52%\n53.71%\nc4ai-command-r7b-arabic-02-2025\n66.91%\n49.54%\n63.06%\n74.67%\n78.02%\n49.15%\n72.78%\n79.80%\n30.33%\n49.38%\n64.44%\n73.82%\n69.20%\n62.30%\n60.99%\n56.69%\n54.78%\n56.06%\nAceGPT-v2-8B-Chat\n66.45%\n51.16%\n62.61%\n79.21%\n80.58%\n53.50%\n74.56%\n77.66%\n41.77%\n50.16%\n60.40%\n74.31%\n68.90%\n64.58%\n61.32%\n56.91%\n54.53%\n53.91%\ngemma-2-9b-it\n71.65%\n57.93%\n64.16%\n79.06%\n79.38%\n63.99%\n78.31%\n80.67%\n60.95%\n56.11%\n64.21%\n73.69%\n68.60%\n61.26%\n59.96%\n57.24%\n57.95%\n59.25%\njais-adapted-13b-chat\n56.64%\n44.45%\n58.97%\n80.86%\n80.47%\n54.27%\n67.52%\n75.24%\n44.05%\n46.41%\n56.56%\n65.46%\n65.30%\n61.10%\n58.05%\n55.77%\n52.87%\n53.59%\njais-family-6p7b-chat\n49.42%\n41.59%\n55.80%\n72.04%\n74.05%\n44.62%\n65.11%\n72.04%\n53.68%\n48.20%\n54.73%\n61.72%\n64.10%\n62.51%\n60.12%\n57.24%\n49.11%\n47.49%\nLlama-3.1-8B-Instruct\n68.04%\n47.58%\n59.05%\n79.22%\n80.74%\n55.29%\n66.72%\n76.67%\n29.26%\n47.81%\n55.97%\n69.70%\n66.10%\n58.11%\n55.39%\n54.24%\n46.86%\n47.52%\nQwen2.5-7B-Instruct\n74.21%\n55.63%\n63.96%\n80.44%\n79.92%\n55.03%\n74.61%\n78.09%\n71.34%\n54.19%\n62.69%\n75.69%\n68.10%\n60.55%\n58.65%\n56.04%\n48.74%\n53.42%\nCitation\nIf you use Fanar-1-9B-Instruct or the Fanar GenAI system in your research or applications, please cite:\n@misc{fanarllm2025,\ntitle={Fanar: An Arabic-Centric Multimodal Generative AI Platform},\nauthor={Fanar Team and Ummar Abbas and Mohammad Shahmeer Ahmad and Firoj Alam and Enes Altinisik and Ehsannedin Asgari and Yazan Boshmaf and Sabri Boughorbel and Sanjay Chawla and Shammur Chowdhury and Fahim Dalvi and Kareem Darwish and Nadir Durrani and Mohamed Elfeky and Ahmed Elmagarmid and Mohamed Eltabakh and Masoomali Fatehkia and Anastasios Fragkopoulos and Maram Hasanain and Majd Hawasly and Mus'ab Husaini and Soon-Gyo Jung and Ji Kim Lucas and Walid Magdy and Safa Messaoud and Abubakr Mohamed and Tasnim Mohiuddin and Basel Mousi and Hamdy Mubarak and Ahmad Musleh and Zan Naeem and Mourad Ouzzani and Dorde Popovic and Amin Sadeghi and Husrev Taha Sencar and Mohammed Shinoy and Omar Sinan and Yifan Zhang and Ahmed Ali and Yassine El Kheir and Xiaosong Ma and Chaoyi Ruan}},\nyear={2025},\nurl={https://arxiv.org/abs/2501.13944},\n}\nAcknowledgements\nThis project is from Qatar Computing Research Institute (QCRI) at Hamad Bin Khalifa University (HBKU), a member of Qatar Foundation. We thank our engineers, researchers, and support team for their efforts in advancing Arabic-centric large language models.\nSpecial thanks to the Ministry of Communications and Information Technology, State of Qatar for their continued support by providing the compute infrastructure through the Google Cloud Platform.\nLicense\nThis model is licensed under the Apache 2.0 License.",
    "PaddlePaddle/PP-OCRv5_mobile_det": "PP-OCRv5_mobile_det\nIntroduction\nQuick Start\nInstallation\nModel Usage\nPipeline Usage\nLinks\nPP-OCRv5_mobile_det\nIntroduction\nPP-OCRv5_mobile_det is one of the PP-OCRv5_det series, the latest generation of text detection models developed by the PaddleOCR team. It aims to efficiently and accurately supports the detection of text in diverse scenarios‚Äîincluding handwriting, vertical, rotated, and curved text‚Äîacross multiple languages such as Simplified Chinese, Traditional Chinese, English, and Japanese. Key features include robust handling of complex layouts, varying text sizes, and challenging backgrounds, making it suitable for practical applications like document analysis, license plate recognition, and scene text detection. The key accuracy metrics are as follow:\nHandwritten Chinese\nHandwritten English\nPrinted Chinese\nPrinted English\nTraditional Chinese\nAncient Text\nJapanese\nGeneral Scenario\nPinyin\nRotation\nDistortion\nArtistic Text\nAverage\n0.744\n0.777\n0.905\n0.910\n0.823\n0.581\n0.727\n0.721\n0.575\n0.647\n0.827\n0.525\n0.770\nQuick Start\nInstallation\nPaddlePaddle\nPlease refer to the following commands to install PaddlePaddle using pip:\n# for CUDA11.8\npython -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n# for CUDA12.6\npython -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n# for CPU\npython -m pip install paddlepaddle==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\nFor details about PaddlePaddle installation, please refer to the PaddlePaddle official website.\nPaddleOCR\nInstall the latest version of the PaddleOCR inference package from PyPI:\npython -m pip install paddleocr\nModel Usage\nYou can quickly experience the functionality with a single command:\npaddleocr text_detection \\\n--model_name PP-OCRv5_mobile_det \\\n-i https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/3ul2Rq4Sk5Cn-l69D695U.png\nYou can also integrate the model inference of the text detection module into your project. Before running the following code, please download the sample image to your local machine.\nfrom paddleocr import TextDetection\nmodel = TextDetection(model_name=\"PP-OCRv5_mobile_det\")\noutput = model.predict(input=\"3ul2Rq4Sk5Cn-l69D695U.png\", batch_size=1)\nfor res in output:\nres.print()\nres.save_to_img(save_path=\"./output/\")\nres.save_to_json(save_path=\"./output/res.json\")\nAfter running, the obtained result is as follows:\n{'res': {'input_path': '/root/.paddlex/predict_input/3ul2Rq4Sk5Cn-l69D695U.png', 'page_index': None, 'dt_polys': array([[[ 105, 1431],\n...,\n[ 105, 1452]],\n...,\n[[ 353,  106],\n...,\n[ 353,  129]]], dtype=int16), 'dt_scores': [0.8306416015066644, 0.7603795581201811, ..., 0.8819806867477359]}}\nThe visualized image is as follows:\nFor details about usage command and descriptions of parameters, please refer to the Document.\nPipeline Usage\nThe ability of a single model is limited. But the pipeline consists of several models can provide more capacity to resolve difficult problems in real-world scenarios.\nPP-OCRv5\nThe general OCR pipeline is used to solve text recognition tasks by extracting text information from images and outputting it in string format. And there are 5 modules in the pipeline:\nDocument Image Orientation Classification Module (Optional)\nText Image Unwarping Module (Optional)\nText Line Orientation Classification Module (Optional)\nText Detection Module\nText Recognition Module\nRun a single command to quickly experience the OCR pipeline:\npaddleocr ocr -i https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/3ul2Rq4Sk5Cn-l69D695U.png \\\n--text_detection_model_name PP-OCRv5_mobile_det \\\n--use_doc_orientation_classify False \\\n--use_doc_unwarping False \\\n--use_textline_orientation True \\\n--save_path ./output \\\n--device gpu:0\nResults are printed to the terminal:\n{'res': {'input_path': 'printing_en/3ul2Rq4Sk5Cn-l69D695U.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': True}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}, 'angle': -1}, 'dt_polys': array([[[ 352,  105],\n...,\n[ 352,  128]],\n...,\n[[ 632, 1431],\n...,\n[ 632, 1447]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([0, ..., 0]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['Algorithms for the Markov Entropy Decomposition', 'Andrew J. Ferris and David Poulin', 'D√©partement de Physique, Universit√© de Sherbrooke, Qu√©bec, JI K 2R1, Canada', '(Dated: October 31, 2018)', 'The Markov entropy decomposition (MED) is a recently-proposed, cluster-based simulation method for fi -', 'nite temperature quantum systems with arbitrary geometry. In this paper, we detail numerical algorithms for', 'performing the required steps of the MED, principally solving a minimization problem with a preconditioned', 'arXiv:1212.1442v1 [cond-mat.stat-mech] 6 Dec 2012', \"Newton's algorithm, as well as how to extract global susceptibilities and thermal responses. We demonstrate\", 'the power of the method with the spin-1/2 XXZ model on the 2D square lattice, including the extraction of', 'critical points and details of each phase. Although the method shares some qualitative similarities with exact-', 'diagonalization, we show the MED is both more accurate and significantly more flexible.', 'PACS numbers: 05.10.‚Äîa, 02.50.Ng, 03.67.‚Äìa, 74.40.Kb', 'I. INTRODUCTION', 'This approximation becomes exact in the case of a 1D quan-', 'tum (or classical) Markov chain [1O], and leads to an expo-', 'Although the equations governing quantum many-body', 'nential reduction of cost for exact entropy calculations when', 'systems are simple to write down, finding solutions for the', 'the global density matrix is a higher-dimensional Markov net-', 'majority of systems remains incredibly difficult. Modern', 'work state [12, 13].', 'physics finds itself in need of new tools to compute the emer-', 'The second approximation used in the MED approach is', 'gent behavior of large, many-body systems.', 'related to the N-representibility problem. Given a set of lo-', 'There has been a great variety of tools developed to tackle', 'cal but overlapping reduced density matrices { œÅi }, it is a very', 'many-body problems, but in general, large 2D and 3D quan-', 'challenging problem to determine if there exists a global den.', 'tum systems remain hard to deal with. Most systems are', 'sity operator which is positive semi-definite and whose partial', 'thought to be non-integrable, so exact analytic solutions are', 'trace agrees with each œÅi. This problem is QMA-hard (the', 'not usually expected. Direct numerical diagonalization can be', 'quantum analogue of NP) [14, 15], and is hopelessly diffi-', 'performed for relatively small systems ‚Äî however the emer-', 'cult to enforce. Thus, the second approximation employed', 'gent behavior of a system in the thermodynamic limit may be', 'involves ignoring global consistency with a positive opera-', 'difficult to extract, especially in systems with large correlation', 'tor, while requiring local consistency on any overlapping re-', 'lengths. Monte Carlo approaches are technically exact (up to', 'gions between the œÅi. At the zero-temperature limit, the MED', 'sampling error), but suffer from the so-called sign problem', 'approach becomes analogous to the variational nth-order re-', 'for fermionic, frustrated, or dynamical problems. Thus we are', 'duced density matrix approach, where positivity is enforced', 'limited to search for clever approximations to solve the ma-', 'on all reduced density matrices of size n [16‚Äì18].', 'jority of many-body problems.', 'The MED approach is an extremely flexible cluster method.', 'Over the past century, hundreds of such approximations', 'applicable to both translationally invariant systems of any di-', 'have been proposed, and we will mention just a few notable', 'mension in the thermodynamic limit, as well as finite systems', 'examples applicable to quantum lattice models. Mean-field', 'or systems without translational invariance (e.g. disordered', 'theory is simple and frequently arrives at the correct quali-', 'lattices, or harmonically trapped atoms in optical lattices).', 'tative description, but often fails when correlations are im-', 'The free energy given by MED is guaranteed to lower bound', 'portant. Density-matrix renormalisation group (DMRG) [1]', 'the true free energy, which in turn lower-bounds the ground', 'is efficient and extremely accurate at solving 1D problems,', 'state energy ‚Äî thus providing a natural complement to varia-', 'but the computational cost grows exponentially with system', 'tional approaches which upper-bound the ground state energy.', 'size in two- or higher-dimensions [2, 3]. Related tensor-', 'The ability to provide a rigorous ground-state energy window', 'network techniques designed for 2D systems are still in their', 'is a powerful validation tool, creating a very compelling rea-', 'infancy [4‚Äì6]. Series-expansion methods [7] can be success-', 'son to use this approach.', 'ful, but may diverge or otherwise converge slowly, obscuring', 'In this paper we paper we present a pedagogical introduc-', 'the state in certain regimes. There exist a variety of cluster-', 'tion to MED, including numerical implementation issues and', 'based techniques, such as dynamical-mean-field theory [8]', 'applications to 2D quantum lattice models in the thermody-', 'and density-matrix embedding [9]', 'namic limit. In Sec. II. we giye a brief deriyation of the', 'Here we discuss the so-called Markov entropy decompo-', 'Markov entropy decomposition. Section III outlines a robust', 'sition (MED), recently proposed by Poulin & Hastings [10]', 'numerical strategy for optimizing the clusters that make up', '(and analogous to a slightly earlier classical algorithm [11]).', 'the decomposition. In Sec. IV we show how we can extend', 'This is a self-consistent cluster method for fi nite temperature', 'these algorithms to extract non-trivial information, such as', 'systems that takes advantage of an approximation of the (von', 'specific heat and susceptibilities. We present an application of', 'Neumann) entropy. In [10], it was shown that the entropy', 'the method to the spin-1/2 XXZ model on a 2D square lattice', 'per site can be rigorously upper bounded using only local in-', 'in Sec. V, describing how to characterize the phase diagram', 'formation ‚Äî a local, reduced density matrix on N sites, say.', 'and determine critical points, before concluding in Sec. VI.'], 'rec_scores': array([0.99388635, ..., 0.99304372]), 'rec_polys': array([[[ 352,  105],\n...,\n[ 352,  128]],\n...,\n[[ 632, 1431],\n...,\n[ 632, 1447]]], dtype=int16), 'rec_boxes': array([[ 352, ...,  128],\n...,\n[ 632, ..., 1447]], dtype=int16)}}\nIf save_path is specified, the visualization results will be saved under save_path. The visualization output is shown below:\nThe command-line method is for quick experience. For project integration, also only a few codes are needed as well:\nfrom paddleocr import PaddleOCR\nocr = PaddleOCR(\ntext_detection_model_name=\"PP-OCRv5_mobile_det\",\nuse_doc_orientation_classify=False, # Use use_doc_orientation_classify to enable/disable document orientation classification model\nuse_doc_unwarping=False, # Use use_doc_unwarping to enable/disable document unwarping module\nuse_textline_orientation=True, # Use use_textline_orientation to enable/disable textline orientation classification model\ndevice=\"gpu:0\", # Use device to specify GPU for model inference\n)\nresult = ocr.predict(\"https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/3ul2Rq4Sk5Cn-l69D695U.png\")\nfor res in result:\nres.print()\nres.save_to_img(\"output\")\nres.save_to_json(\"output\")\nThe default model used in pipeline is PP-OCRv5_server_det, so it is needed that specifing to PP-OCRv5_mobile_det by argument text_detection_model_name. And you can also use the local model file by argument text_detection_model_dir. For details about usage command and descriptions of parameters, please refer to the Document.\nPP-StructureV3\nLayout analysis is a technique used to extract structured information from document images. PP-StructureV3 includes the following six modules:\nLayout Detection Module\nGeneral OCR Pipeline\nDocument Image Preprocessing Pipeline ÔºàOptionalÔºâ\nTable Recognition Pipeline ÔºàOptionalÔºâ\nSeal Recognition Pipeline ÔºàOptionalÔºâ\nFormula Recognition Pipeline ÔºàOptionalÔºâ\nRun a single command to quickly experience the PP-StructureV3 pipeline:\npaddleocr pp_structurev3 -i https://cdn-uploads.huggingface.co/production/uploads/681c1ecd9539bdde5ae1733c/mG4tnwfrvECoFMu-S9mxo.png \\\n--text_detection_model_name PP-OCRv5_mobile_det \\\n--use_doc_orientation_classify False \\\n--use_doc_unwarping False \\\n--use_textline_orientation False \\\n--device gpu:0\nResults would be printed to the terminal. If save_path is specified, the results will be saved under save_path. The predicted markdown visualization is shown below:\nJust a few lines of code can experience the inference of the pipeline. Taking the PP-StructureV3 pipeline as an example:\nfrom paddleocr import PPStructureV3\npipeline = PPStructureV3(\ntext_detection_model_name=\"PP-OCRv5_mobile_det\",\nuse_doc_orientation_classify=False, # Use use_doc_orientation_classify to enable/disable document orientation classification model\nuse_doc_unwarping=False,    # Use use_doc_unwarping to enable/disable document unwarping module\nuse_textline_orientation=False, # Use use_textline_orientation to enable/disable textline orientation classification model\ndevice=\"gpu:0\", # Use device to specify GPU for model inference\n)\noutput = pipeline.predict(\"./pp_structure_v3_demo.png\")\nfor res in output:\nres.print() # Print the structured prediction output\nres.save_to_json(save_path=\"output\") ## Save the current image's structured result in JSON format\nres.save_to_markdown(save_path=\"output\") ## Save the current image's result in Markdown format\nThe default model used in pipeline is PP-OCRv5_server_det, so it is needed that specifing to PP-OCRv5_mobile_det by argument text_detection_model_name. And you can also use the local model file by argument text_detection_model_dir. For details about usage command and descriptions of parameters, please refer to the Document.\nLinks\nPaddleOCR Repo\nPaddleOCR Documentation",
    "RedHatAI/gemma-3-27b-it-quantized.w4a16": "gemma-3-27b-it-quantized.w4a16\nModel Overview\nModel Optimizations\nDeployment\nUse with vLLM\nCreation\nEvaluation\nOpenLLM v1\nAccuracy\ngemma-3-27b-it-quantized.w4a16\nModel Overview\nModel Architecture: google/gemma-3-27b-it\nInput: Vision-Text\nOutput: Text\nModel Optimizations:\nWeight quantization: INT4\nActivation quantization: FP16\nRelease Date: 6/4/2025\nVersion: 1.0\nModel Developers: RedHatAI\nQuantized version of google/gemma-3-27b-it.\nModel Optimizations\nThis model was obtained by quantizing the weights of google/gemma-3-27b-it to INT4 data type, ready for inference with vLLM >= 0.8.0.\nDeployment\nUse with vLLM\nThis model can be deployed efficiently using the vLLM backend, as shown in the example below.\nfrom vllm import LLM, SamplingParams\nfrom vllm.assets.image import ImageAsset\nfrom transformers import AutoProcessor\n# Define model name once\nmodel_name = \"RedHatAI/gemma-3-27b-it-quantized.w4a16\"\n# Load image and processor\nimage = ImageAsset(\"cherry_blossom\").pil_image.convert(\"RGB\")\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n# Build multimodal prompt\nchat = [\n{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"What is the content of this image?\"}]},\n{\"role\": \"assistant\", \"content\": []}\n]\nprompt = processor.apply_chat_template(chat, add_generation_prompt=True)\n# Initialize model\nllm = LLM(model=model_name, trust_remote_code=True)\n# Run inference\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [image]}}\noutputs = llm.generate(inputs, SamplingParams(temperature=0.2, max_tokens=64))\n# Display result\nprint(\"RESPONSE:\", outputs[0].outputs[0].text)\nvLLM also supports OpenAI-compatible serving. See the documentation for more details.\nCreation\nThis model was created with llm-compressor by running the code snippet below:\nModel Creation Code\nimport base64\nfrom io import BytesIO\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom llmcompressor.modifiers.quantization import GPTQModifier\nfrom llmcompressor.transformers import oneshot\n# Load model.\nmodel_id = \"google/gemma-3-27b-it\"\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n# Oneshot arguments\nDATASET_ID = \"neuralmagic/calibration\"\nDATASET_SPLIT = {\"LLM\": \"train[:1024]\"}\nNUM_CALIBRATION_SAMPLES = 1024\nMAX_SEQUENCE_LENGTH = 2048\n# Load dataset and preprocess.\nds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\nds = ds.shuffle(seed=42)\ndampening_frac=0.07\ndef data_collator(batch):\nassert len(batch) == 1, \"Only batch size of 1 is supported for calibration\"\nitem = batch[0]\ncollated = {}\nimport torch\nfor key, value in item.items():\nif isinstance(value, torch.Tensor):\ncollated[key] = value.unsqueeze(0)\nelif isinstance(value, list) and isinstance(value[0][0], int):\n# Handle tokenized inputs like input_ids, attention_mask\ncollated[key] = torch.tensor(value)\nelif isinstance(value, list) and isinstance(value[0][0], float):\n# Handle possible float sequences\ncollated[key] = torch.tensor(value)\nelif isinstance(value, list) and isinstance(value[0][0], torch.Tensor):\n# Handle batched image data (e.g., pixel_values as [C, H, W])\ncollated[key] = torch.stack(value)  # -> [1, C, H, W]\nelif isinstance(value, torch.Tensor):\ncollated[key] = value\nelse:\nprint(f\"[WARN] Unrecognized type in collator for key={key}, type={type(value)}\")\nreturn collated\n# Recipe\nrecipe = [\nGPTQModifier(\ntargets=\"Linear\",\nignore=[\"re:.*lm_head.*\", \"re:.*embed_tokens.*\", \"re:vision_tower.*\", \"re:multi_modal_projector.*\"],\nsequential_update=True,\nsequential_targets=[\"Gemma3DecoderLayer\"],\ndampening_frac=dampening_frac,\n)\n]\nSAVE_DIR=f\"{model_id.split('/')[1]}-quantized.w4a16\"\n# Perform oneshot\noneshot(\nmodel=model,\ntokenizer=model_id,\ndataset=ds,\nrecipe=recipe,\nmax_seq_length=MAX_SEQUENCE_LENGTH,\nnum_calibration_samples=NUM_CALIBRATION_SAMPLES,\ntrust_remote_code_model=True,\ndata_collator=data_collator,\noutput_dir=SAVE_DIR\n)\nEvaluation\nThe model was evaluated using lm_evaluation_harness for OpenLLM v1 text benchmark. The evaluations were conducted using the following commands:\nEvaluation Commands\nOpenLLM v1\nlm_eval \\\n--model vllm \\\n--model_args pretrained=\"<model_name>\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=<n>,gpu_memory_utilization=0.8,enable_chunked_prefill=True,trust_remote_code=True,enforce_eager=True \\\n--tasks openllm \\\n--batch_size auto\nAccuracy\nCategory\nMetric\ngoogle/gemma-3-27b-it\nRedHatAI/gemma-3-27b-it-quantized.w8a8\nRecovery (%)\nOpenLLM V1\nARC Challenge\n72.53%\n72.35%\n99.76%\nGSM8K\n92.12%\n91.66%\n99.51%\nHellaswag\n85.78%\n84.97%\n99.06%\nMMLU\n77.53%\n76.77%\n99.02%\nTruthfulqa (mc2)\n62.20%\n62.57%\n100.59%\nWinogrande\n79.40%\n79.79%%\n100.50%\nAverage Score\n78.26%\n78.02%\n99.70%\nVision Evals\nMMMU (val)\n50.89%\n51.78%\n101.75%\nChartQA\n72.16%\n72.20%\n100.06%\nAverage Score\n61.53%\n61.99%\n100.90%",
    "facefusion/models-3.3.0": "No model card",
    "BAAI/RoboBrain2.0-7B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nYou agree to not use the model to conduct experiments that cause harm to human subjects.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nRoboBrain 2.0: See Better. Think Harder. Do Smarter.\nüóûÔ∏è News\nüî• Overview\nüìÜ Todo\nüöÄ Features\n‚≠êÔ∏è Architecture\nü§ó Model Zoo\nüõ†Ô∏è Setup\nü§ñ Simple Inference\nüòä More Results\nüìë Citation\nRoboBrain 2.0: See Better. Think Harder. Do Smarter.\n‚≠êÔ∏è Project¬†¬† | ¬†¬†‚≠êÔ∏è Github¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬†üìë Technical Report¬†¬† | ¬†¬†üí¨ WeChat\nüéØ RoboOS: An Efficient Open-Source Multi-Robot Coordination System for RoboBrain.\nüåç RoboBrain 1.0: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete.\nüóûÔ∏è News\n2025-07-23: ü§ó RoboBrain 2.0-3B model checkpoint has been released in Huggingface.\n2025-07-03: ü§ó RoboBrain 2.0-32B model checkpoint has been released in Huggingface.\n2025-06-07: üéâ We highlight the training framework (FlagScale) developed by BAAI Framework R&D team, and the evaluation framework (FlagEvalMM) by BAAI FlagEval team. Both are used for RoboBrain 2.0.\n2025-06-06: ü§ó RoboBrain 2.0-7B model checkpoint has been released in Huggingface..\n2025-06-06: üî• We're excited to announce the release of our more powerful RoboBrain 2.0.\n2025-04-11: üéâ RoboBrain 1.0 was selected for CVPR 2025's official Embodied AI Trends Commentary.\n2025-02-27: üåç RoboBrain 1.0 was accepted to CVPR2025.\nüî• Overview\nWe are thrilled to announce the launch of RoboBrain 2.0, the most powerful open-source embodied brain model to date. Compared to its predecessor, RoboBrain 1.0, the new version is designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. RoboBrain 2.0 comes in three variants: an ultra-lightweight 3B model, a lightweight 7B model, and a full-scale 32B model, all featuring a heterogeneous architecture with a vision encoder and a language model.\nDespite its compact size, RoboBrain 2.0 delivers exceptional performance across a wide range of embodied reasoning tasks. The newly released 3B model is optimized for resource-constrained scenarios, being extremely lightweight and easy to deploy, with spatial understanding capabilities comparable to the 7B model, making it ideal for edge devices and real-time applications. The 7B model strikes a balance between performance and computational requirements, suitable for diverse scenarios, while the 32B model achieves leading results in most spatial and temporal benchmarks, surpassing prior open-source and proprietary models. RoboBrain 2.0 supports critical real-world embodied intelligence capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and real-time scene memory).\nThis report details the model architecture, data construction, multi-stage training strategies, infrastructure, and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents.\nüìÜ Todo\nRelease model checkpoint for RoboBrain 2.0-3B\nRelease model checkpoint for RoboBrain 2.0-7B\nRelease quick inference example for RoboBrain 2.0\nRelease training codes for RoboBrain 2.0\nRelease model checkpoint for RoboBrain 2.0-32B\nüöÄ Features\nRoboBrain 2.0 supports interactive reasoning with long-horizon planning and closed-loop feedback, spatial perception for precise point and bbox prediction from complex instructions, temporal perception for future trajectory estimation, and scene reasoning through real-time structured memory construction and update.\n‚≠êÔ∏è Architecture\nRoboBrain 2.0 supports multi-image, long video, and high-resolution visual inputs, along with complex task instructions and structured scene graphs on the language side. Visual inputs are processed via a Vision Encoder and MLP Projector, while textual inputs are tokenized into a unified token stream. All inputs are fed into a LLM Decoder that performs long-chain-of-thought reasoning and outputs structured plans, spatial relations, and both relative and absolute coordinates.\nü§ó Model Zoo\nModels\nCheckpoint\nDescription\nRoboBrain 2.0 3B\nü§ó BAAI/RoboBrain2.0-3B\n3B parameter version of the RoboBrain2.0\nRoboBrain 2.0 7B\nü§ó BAAI/RoboBrain2.0-7B\n7B parameter version of the RoboBrain2.0\nRoboBrain 2.0 32B\nü§ó BAAI/RoboBrain2.0-32B\n32B parameter version of the RoboBrain2.0\nüõ†Ô∏è Setup\n# clone repo.\ngit clone https://github.com/FlagOpen/RoboBrain2.0.git\ncd RoboBrain\n# build conda env.\nconda create -n robobrain2 python=3.10\nconda activate robobrain2\npip install -r requirements.txt\nü§ñ Simple Inference\nNote: Please refer to RoboBrain 2.0 Github for the usage of RoboBrain 2.0\nüòä More Results\nBenchmark comparison across spatial reasoning and temporal task planning. RoboBrain2.0 achieves state-of-the-art (SOTA) or near-SOTA performance on nine spatial reasoning benchmarks: BLINK-Spatial, CV-Bench, EmbSpatial, RoboSpatial, RefSpatial, SAT, VSI-Bench, Where2Place and ShareRobot-Bench, and three temporal reasoning benchmarks: Multi-Robot-Planning, Ego-Plan2 and RoboBench-Planning,  It not only outperforms leading open-source models such as Cosmos-Reason1 and Qwen2.5-VL, but also surpasses closed-source models like Gemini 2.5 Pro, o4-mini and Claude Sonnet 4.\nüìë Citation\nIf you find this project useful, welcome to cite us.\n@article{RoboBrain 2.0,\ntitle={RoboBrain 2.0 Technical Report},\nauthor={BAAI RoboBrain Team},\njournal={arXiv preprint arXiv:2507.02029},\nyear={2025}\n}\n@article{RoboBrain 1.0,\ntitle={Robobrain: A unified brain model for robotic manipulation from abstract to concrete},\nauthor={Ji, Yuheng and Tan, Huajie and Shi, Jiayu and Hao, Xiaoshuai and Zhang, Yuan and Zhang, Hengyuan and Wang, Pengwei and Zhao, Mengdi and Mu, Yao and An, Pengju and others},\njournal={arXiv preprint arXiv:2502.21257},\nyear={2025}\n}\n@article{RoboOS,\ntitle={RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration},\nauthor={Tan, Huajie and Hao, Xiaoshuai and Lin, Minglan and Wang, Pengwei and Lyu, Yaoxu and Cao, Mingyu and Wang, Zhongyuan and Zhang, Shanghang},\njournal={arXiv preprint arXiv:2505.03673},\nyear={2025}\n}\n@article{zhou2025roborefer,\ntitle={RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics},\nauthor={Zhou, Enshen and An, Jingkun and Chi, Cheng and Han, Yi and Rong, Shanyu and Zhang, Chi and Wang, Pengwei and Wang, Zhongyuan and Huang, Tiejun and Sheng, Lu and others},\njournal={arXiv preprint arXiv:2506.04308},\nyear={2025}\n}\n@article{Reason-RFT,\ntitle={Reason-rft: Reinforcement fine-tuning for visual reasoning},\nauthor={Tan, Huajie and Ji, Yuheng and Hao, Xiaoshuai and Lin, Minglan and Wang, Pengwei and Wang, Zhongyuan and Zhang, Shanghang},\njournal={arXiv preprint arXiv:2503.20752},\nyear={2025}\n}\n@article{Code-as-Monitor,\ntitle={Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection},\nauthor={Zhou, Enshen and Su, Qi and Chi, Cheng and Zhang, Zhizheng and Wang, Zhongyuan and Huang, Tiejun and Sheng, Lu and Wang, He},\njournal={arXiv preprint arXiv:2412.04455},\nyear={2024}\n}"
}