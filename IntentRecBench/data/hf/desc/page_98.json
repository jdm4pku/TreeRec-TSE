{
    "unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF": "Finetune LLMs 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nSpecial Thanks\nDeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nSee our collection for versions of Deepseek-R1 including GGUF and original formats.\nInstructions to run this model in llama.cpp:\nOr you can view more detailed instructions here: unsloth.ai/blog/deepseek-r1\nDo not forget about <ÔΩúUserÔΩú> and <ÔΩúAssistantÔΩú> tokens! - Or use a chat template formatter\nObtain the latest llama.cpp at https://github.com/ggerganov/llama.cpp\nExample with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mode\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf \\\n--cache-type-k q8_0 \\\n--threads 16 \\\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>' \\\n-no-cnv\nExample output:\n<think>\nOkay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\nLet me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\nWait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\nI also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\nIs there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\nIf you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf\n--cache-type-k q8_0\n--threads 16\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>'\n--n-gpu-layers 20 \\\n-no-cnv\nFinetune LLMs 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit": "Finetune LLMs 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nSpecial Thanks\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n7. License\n8. Citation\n9. Contact\nSee our collection for versions of Deepseek-R1 including GGUF and original formats.\nFinetune LLMs 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regrading the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nNOTE: We recommend setting an appropriate temperature (between 0.5 and 0.7) when running these models, otherwise you may encounter issues with endless repetition or incoherent output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF": "Finetune LLMs 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nSpecial Thanks\nDeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nSee our collection for versions of Deepseek-R1 including GGUF and original formats.\nInstructions to run this model in llama.cpp:\nOr you can view more detailed instructions here: unsloth.ai/blog/deepseek-r1\nDo not forget about <ÔΩúUserÔΩú> and <ÔΩúAssistantÔΩú> tokens! - Or use a chat template formatter\nObtain the latest llama.cpp at https://github.com/ggerganov/llama.cpp\nExample with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mode\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf \\\n--cache-type-k q8_0 \\\n--threads 16 \\\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>' \\\n-no-cnv\nExample output:\n<think>\nOkay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\nLet me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\nWait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\nI also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\nIs there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\nIf you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf\n--cache-type-k q8_0\n--threads 16\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>'\n--n-gpu-layers 20 \\\n-no-cnv\nFinetune LLMs 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "prithivMLmods/FastThink-0.5B-Tiny": "FastThink-0.5B-Tiny\nQuickstart with Transformer\nDataset Preparation\nExample\nIntended Use\nLimitations\nFastThink-0.5B-Tiny\nFastThink-0.5B-Tiny is a reasoning-focused model based on Qwen2.5. We have released a range of base language models and instruction-tuned language models, spanning from 0.5 billion to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2:\nSignificantly enhanced knowledge and greatly improved capabilities in coding and mathematics, thanks to specialized expert models in these domains.\nMajor improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g., tables), and generating structured outputs, especially JSON. It is more resilient to diverse system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context support for up to 128K tokens and the ability to generate outputs up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nArchitecture: Transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias, and tied word embeddings.\nQuickstart with Transformer\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"prithivMLmods/FastThink-0.5B-Tiny\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nDataset Preparation\nThis script is designed to load, process, and combine multiple datasets into a single, standardized format suitable for training conversational AI models. The script uses the datasets library to load and manipulate the datasets, and the chat_templates library to standardize the conversation format.\nExample\n# Load the initial three datasets\ndataset1 = load_dataset(\"PowerInfer/LONGCOT-Refine-500K\", split=\"train\")\ndataset2 = load_dataset(\"amphora/QwQ-LongCoT-130K\", split=\"train\")\ndataset3 = load_dataset(\"AI-MO/NuminaMath-CoT\", split=\"train\")\n# Map conversation columns for all datasets\ndataset1 = dataset1.map(add_conversations_column, batched=False)\ndataset2 = dataset2.map(add_conversations_column_prompt_qwq, batched=False)\ndataset3 = dataset3.map(add_conversations_column_prompt_solution, batched=False)\n# Combine all datasets\ncombined_dataset = concatenate_datasets([dataset1, dataset2, dataset3])\n# Standardize using the ShareGPT format\ncombined_dataset = standardize_sharegpt(combined_dataset)\n# Initialize the tokenizer with a specific chat template\ntokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n# Apply formatting function to the combined dataset\ncombined_dataset = combined_dataset.map(formatting_prompts_func, batched=True)\n# Print the first few examples to verify the output\nprint(combined_dataset[:50000])\nIntended Use\nReasoning Tasks: FastThink-0.5B-Tiny is optimized for reasoning-focused applications, such as logical problem-solving, decision-making, and analytical workflows.\nInstruction Following: Ideal for scenarios where precise adherence to instructions is required, including generating structured outputs like JSON or tables.\nMultilingual Support: Suitable for use in multilingual environments, supporting over 29 languages, making it versatile for global applications.\nCoding and Mathematics: Highly effective in tasks involving coding, debugging, or solving mathematical problems, leveraging expert domain knowledge.\nRole-play Scenarios: Can simulate conversational agents or personas for role-playing, enhancing chatbot and virtual assistant implementations.\nLong-form Content Creation: Designed to generate and manage long-form text (up to 8K tokens) while maintaining context, making it ideal for tasks like report writing or storytelling.\nUnderstanding and Processing Structured Data: Efficient at interpreting and working with structured data, such as tables or hierarchical formats.\nLow-Resource Applications: With a smaller parameter size (0.5B), it is well-suited for applications with limited computational resources or edge deployment.\nLimitations\nLimited Model Size: As a 0.5B-parameter model, its reasoning and comprehension capabilities are less advanced compared to larger models, particularly for highly complex tasks.\nContextual Limitations: Although it supports a context length of up to 128K tokens, its ability to effectively utilize such a long context may vary, particularly in tasks requiring intricate cross-referencing of earlier inputs.\nAccuracy in Domain-Specific Tasks: While capable in coding and mathematics, it may struggle with highly specialized or esoteric domain knowledge compared to models fine-tuned specifically for those areas.\nAmbiguity Handling: May misinterpret vague or poorly structured prompts, leading to less accurate or unintended results.\nLong-Context Tradeoffs: Generating or processing very long outputs (e.g., close to the 8K token limit) could result in decreased coherence or relevance toward the end.\nMultilingual Performance: Although it supports 29 languages, its proficiency and fluency may vary across languages, with some underrepresented languages possibly seeing reduced performance.\nResource-Intensive for Long Contexts: Using its long-context capabilities (128K tokens) can be computationally demanding, requiring significant memory and processing power.\nDependence on Fine-Tuning: For highly specialized tasks or domains, additional fine-tuning may be necessary to achieve optimal performance.",
    "Homie0609/UniSoccer": "Requirements\nPretrain MatchVision Encoder\nTrain Downstream Tasks\nTrain\nPretrain MatchVision Encoder\nTrain Downstream Tasks\nInference\nCitation\nRequirements\nPython >= 3.8 (Recommend to use Anaconda or Miniconda)\nPyTorch >= 2.0.0 (If use A100)\ntransformers >= 4.42.3\npycocoevalcap >= 1.2\nA suitable conda environment named UniSoccer can be created and activated with:\nconda env create -f environment.yaml\nconda activate UniSoccer\nTrain\nPretrain MatchVision Encoder\nAs described in paper, we have two methods for pretraining MatchVision backbone (supervised classification & contrastive commentary). You can train both this two methods as following shows:\nFirst of all, you should prepare textual data as the format in train_data/json, and preprocess soccer videos into 30 second clips (15s before and after timestamps) for pretraining.\nSupervised Classification\npython task/pretrain_MatchVoice_Classifier.py config/pretrain_classification.py\nContrastive Commentary Retrieval\npython task/pretrain_contrastive.py config/pretrain_contrastive.py\nAlso, you could finetune MatchVision with\npython task/finetune_contrastive.py config/finetune_contrastive.py\nTo be noted, you should replace the folders in task and config files.\nTrain Downstream Tasks\nYou could train the commentary task by several different methods:\nUse mp4 files\npython task/downstream_commentary_new_benchmark.py\nFor this method, you might train the commentary model MatchVoice with open visual encoder or language decoder, so you should crop the videos as 30s clips named as json files shows.\nUse .npy files\npython task/downstream_commentary.py\nFor this method, you cannot open the visual encoder, so you can extract features of all video clips and change \".mp4\" by \".npy\" as file names.\nTo be noted, folder words_world records the token ids of all words in LLaMA-3(8B) tokenizer of different datasets as\nmatch_time.pkl: MatchTime dataset (Link here)\nsoccerreplay-1988.pkl: SoccerReplay-1988 dataset. (Not released yet)\nmerge.pkl: Union set of MatchTime & SoccerReplay-1988\nInference\nFor inference, you could use the following codes, be sure that you have correctly crop the video clips, which is in the same format as before.\npython inference/inference.py\nThen, you could test the metrics for output sample.csv by:\npython inference/score_single.py --csv_path inference/sample.csv\nCitation\nIf you use this code and data for your research or project, please cite:\n@inproceedings{rao2024unisoccer,\ntitle   = {Towards Universal Soccer Video Understanding},\nauthor  = {Rao, Jiayuan and Wu, Haoning and Jiang, Hao and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nyear    = {2025},\n}",
    "raghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF": "raghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF\nUse with llama.cpp\nCLI:\nServer:\nraghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF\nThis model was converted to GGUF format from deepseek-ai/deepseek-coder-1.3b-instruct using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo raghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF --hf-file deepseek-coder-1.3b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo raghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF --hf-file deepseek-coder-1.3b-instruct-q4_k_m.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo raghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF --hf-file deepseek-coder-1.3b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo raghav0/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF --hf-file deepseek-coder-1.3b-instruct-q4_k_m.gguf -c 2048",
    "allenai/OLMoE-1B-7B-0125": "Model Summary\nUse\nEvaluation Snapshot\nCitation\nModel Summary\nOLMoE-1B-7B is a Mixture-of-Experts LLM with 1B active and 7B total parameters released in January 2025 (0125) that is 100% open-source. It is an improved version of OLMoE-09-24, see the paper appendix for details.\nThis information and more can also be found on the OLMoE GitHub repository.\nPaper: arxiv.org/abs/2409.02060\nPretraining Checkpoints, Code, Data and Logs.\nSFT (Supervised Fine-Tuning) Checkpoints, Code, Data and Logs.\nDPO/KTO (Direct Preference Optimization/Kahneman-Tversky Optimization), Checkpoints, Preference Data, DPO code, KTO code and Logs.\nUse\nInstall transformers (version 4.45.0 or greater) & torch and run:\nfrom transformers import OlmoeForCausalLM, AutoTokenizer\nimport torch\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load different ckpts via passing e.g. `revision=step10000-tokens41B`\nmodel = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0125\").to(DEVICE)\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0125\")\ninputs = tokenizer(\"Bitcoin is\", return_tensors=\"pt\")\ninputs = {k: v.to(DEVICE) for k, v in inputs.items()}\nout = model.generate(**inputs, max_length=64)\nprint(tokenizer.decode(out[0]))\n# > # Bitcoin is a digital currency that is created and held electronically. No one controls it. Bitcoins aren‚Äôt printed, like dollars or euros ‚Äì they‚Äôre produced by people and businesses running computers all around the world, using software that solves mathematical\nYou can list all revisions/branches by installing huggingface-hub & running:\nfrom huggingface_hub import list_repo_refs\nout = list_repo_refs(\"allenai/OLMoE-1B-7B-0125\")\nbranches = [b.name for b in out.branches]\nImportant branches:\nstep1200000-tokens5033B: Pretraining checkpoint used for annealing. There are a few more checkpoints after this one but we did not use them.\nmain: Checkpoint annealed from step1200000-tokens5033B for an additional 100B tokens (23,842 steps). We use this checkpoint for our adaptation (https://huggingface.co/allenai/OLMoE-1B-7B-0125-SFT & https://huggingface.co/allenai/OLMoE-1B-7B-0125-Instruct).\nfp32: FP32 version of main. The model weights were stored in FP32 during training but we did not observe any performance drop from casting them to BF16 after training so we upload all weights in BF16. If you want the original FP32 checkpoint for main you can use this one. You will find that it yields slightly different results but should perform around the same on benchmarks.\nEvaluation Snapshot\nModel\nActive Params\nOpen Data\nMMLU\nHellaSwag\nARC-Chall.\nARC-Easy\nPIQA\nWinoGrande\nLMs with ~1B active parameters\nOLMoE-1B-7B-0125\n1.3B\n‚úÖ\n56.3\n81.7\n67.5\n84.4\n78.7\n70.6\nOLMoE-1B-7B-0924\n1.3B\n‚úÖ\n54.1\n80.0\n62.1\n84.2\n79.8\n70.2\nDCLM-1B\n1.4B\n‚úÖ\n48.5\n75.1\n57.6\n79.5\n76.6\n68.1\nTinyLlama-1B\n1.1B\n‚úÖ\n33.6\n60.8\n38.1\n69.5\n71.7\n60.1\nOLMo-1B (0724)\n1.3B\n‚úÖ\n32.1\n67.5\n36.4\n53.5\n74.0\n62.9\nPythia-1B\n1.1B\n‚úÖ\n31.1\n48.0\n31.4\n63.4\n68.9\n52.7\nLMs with ~2-3B active parameters\nQwen1.5-3B-14B\n2.7B\n‚ùå\n62.4\n80.0\n77.4\n91.6\n81.0\n72.3\nGemma2-3B\n2.6B\n‚ùå\n53.3\n74.6\n67.5\n84.3\n78.5\n71.8\nJetMoE-2B-9B\n2.2B\n‚ùå\n49.1\n81.7\n61.4\n81.9\n80.3\n70.7\nDeepSeek-3B-16B\n2.9B\n‚ùå\n45.5\n80.4\n53.4\n82.7\n80.1\n73.2\nStableLM-2B\n1.6B\n‚ùå\n40.4\n70.3\n50.6\n75.3\n75.6\n65.8\nOpenMoE-3B-9B\n2.9B\n‚úÖ\n27.4\n44.4\n29.3\n50.6\n63.3\n51.9\nLMs with ~7-9B active parameters\nGemma2-9B\n9.2B\n‚ùå\n70.6\n87.3\n89.5\n95.5\n86.1\n78.8\nLlama3.1-8B\n8.0B\n‚ùå\n66.9\n81.6\n79.5\n91.7\n81.1\n76.6\nDCLM-7B\n6.9B\n‚úÖ\n64.4\n82.3\n79.8\n92.3\n80.1\n77.3\nMistral-7B\n7.3B\n‚ùå\n64.0\n83.0\n78.6\n90.8\n82.8\n77.9\nOLMo-7B (0724)\n6.9B\n‚úÖ\n54.9\n80.5\n68.0\n85.7\n79.3\n73.2\nLlama2-7B\n6.7B\n‚ùå\n46.2\n78.9\n54.2\n84.0\n77.5\n71.7\nCitation\n@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,\ntitle={OLMoE: Open Mixture-of-Experts Language Models},\nauthor={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},\nyear={2024},\neprint={2409.02060},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2409.02060},\n}",
    "OpenGVLab/InternVideo2_5_Chat_8B": "üìïInternVideo2.5‚ö°\nüìà Performance\nüöÄ How to use the model\n‚úèÔ∏è Citation\nüìïInternVideo2.5‚ö°\n[üìÇ GitHub][üìú Tech Report]\nInternVideo2.5 is a video multimodal large language model (MLLM, built upoon InternVL2.5) enhanced with long and rich context (LRC) modeling. It significantly improves upon existing MLLMs by enhancing their ability to perceive fine-grained details and capture long-form temporal structures. We achieve this through dense vision task annotations using direct preference optimization (TPO) and compact spatiotemporal representations via adaptive hierarchical token compression (HiCo).\nüìà Performance\nVideoBenchmark\nModel\nMVBench\nLongVideoBench\nVideoMME(w/o sub)\nInternVideo2.5\n75.7\n60.6\n65.1\nInference Speed\nWe measured the average inference speed (tokens/s) of generating 1024 new tokens and 5198 (8192-2998) tokens with the context of an video (which takes 2998 tokens) under BF16 precision. w/ encoder indicates that the inference includes the time for video encoder.\nQuantization\nSpeed (3022 tokens)\nSpeed (8192 tokens) w/o encoder\nSpeed(8192 tokens) w/ encoder\nBF16\n33.40\n31.91\n21.33\nINT4\n-\n31.95\n26.37\nThe profiling runs on a single A800-SXM4-80G GPU with PyTorch 2.4.0 and CUDA 12.1.\nüöÄ How to use the model\nFirst, you need to install flash attention2 and some other modules. We provide a simple installation example below:\npip install transformers==4.40.1\npip install av\npip install imageio\npip install decord\npip install opencv-python\npip install flash-attn --no-build-isolation\nThen you could use our model:\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom decord import VideoReader, cpu\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import AutoModel, AutoTokenizer\n# model setting\nmodel_path = 'OpenGVLab/InternVideo2_5_Chat_8B'\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().to(torch.bfloat16)\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\ndef build_transform(input_size):\nMEAN, STD = IMAGENET_MEAN, IMAGENET_STD\ntransform = T.Compose([T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), T.ToTensor(), T.Normalize(mean=MEAN, std=STD)])\nreturn transform\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\nbest_ratio_diff = float(\"inf\")\nbest_ratio = (1, 1)\narea = width * height\nfor ratio in target_ratios:\ntarget_aspect_ratio = ratio[0] / ratio[1]\nratio_diff = abs(aspect_ratio - target_aspect_ratio)\nif ratio_diff < best_ratio_diff:\nbest_ratio_diff = ratio_diff\nbest_ratio = ratio\nelif ratio_diff == best_ratio_diff:\nif area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\nbest_ratio = ratio\nreturn best_ratio\ndef dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\norig_width, orig_height = image.size\naspect_ratio = orig_width / orig_height\n# calculate the existing image aspect ratio\ntarget_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\ntarget_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n# find the closest aspect ratio to the target\ntarget_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n# calculate the target width and height\ntarget_width = image_size * target_aspect_ratio[0]\ntarget_height = image_size * target_aspect_ratio[1]\nblocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n# resize the image\nresized_img = image.resize((target_width, target_height))\nprocessed_images = []\nfor i in range(blocks):\nbox = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n# split the image\nsplit_img = resized_img.crop(box)\nprocessed_images.append(split_img)\nassert len(processed_images) == blocks\nif use_thumbnail and len(processed_images) != 1:\nthumbnail_img = image.resize((image_size, image_size))\nprocessed_images.append(thumbnail_img)\nreturn processed_images\ndef load_image(image, input_size=448, max_num=6):\ntransform = build_transform(input_size=input_size)\nimages = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(image) for image in images]\npixel_values = torch.stack(pixel_values)\nreturn pixel_values\ndef get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\nif bound:\nstart, end = bound[0], bound[1]\nelse:\nstart, end = -100000, 100000\nstart_idx = max(first_idx, round(start * fps))\nend_idx = min(round(end * fps), max_frame)\nseg_size = float(end_idx - start_idx) / num_segments\nframe_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\nreturn frame_indices\ndef get_num_frames_by_duration(duration):\nlocal_num_frames = 4\nnum_segments = int(duration // local_num_frames)\nif num_segments == 0:\nnum_frames = local_num_frames\nelse:\nnum_frames = local_num_frames * num_segments\nnum_frames = min(512, num_frames)\nnum_frames = max(128, num_frames)\nreturn num_frames\ndef load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32, get_frame_by_duration = False):\nvr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\nmax_frame = len(vr) - 1\nfps = float(vr.get_avg_fps())\npixel_values_list, num_patches_list = [], []\ntransform = build_transform(input_size=input_size)\nif get_frame_by_duration:\nduration = max_frame / fps\nnum_segments = get_num_frames_by_duration(duration)\nframe_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\nfor frame_index in frame_indices:\nimg = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\nimg = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\npixel_values = [transform(tile) for tile in img]\npixel_values = torch.stack(pixel_values)\nnum_patches_list.append(pixel_values.shape[0])\npixel_values_list.append(pixel_values)\npixel_values = torch.cat(pixel_values_list)\nreturn pixel_values, num_patches_list\n# evaluation setting\nmax_num_frames = 512\ngeneration_config = dict(\ndo_sample=False,\ntemperature=0.0,\nmax_new_tokens=1024,\ntop_p=0.1,\nnum_beams=1\n)\nvideo_path = \"your_video.mp4\"\nnum_segments=128\nwith torch.no_grad():\npixel_values, num_patches_list = load_video(video_path, num_segments=num_segments, max_num=1, get_frame_by_duration=False)\npixel_values = pixel_values.to(torch.bfloat16).to(model.device)\nvideo_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n# single-turn conversation\nquestion1 = \"Describe this video in detail.\"\nquestion = video_prefix + question1\noutput1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\nprint(output1)\n# multi-turn conversation\nquestion2 = \"How many people appear in the video?\"\noutput2, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\nprint(output2)\n‚úèÔ∏è Citation\n@article{wang2025internvideo,\ntitle={InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling},\nauthor={Wang, Yi and Li, Xinhao and Yan, Ziang and He, Yinan and Yu, Jiashuo and Zeng, Xiangyu and Wang, Chenting and Ma, Changlian and Huang, Haian and Gao, Jianfei and Dou, Min and Chen, Kai and Wang, Wenhai and Qiao, Yu and Wang, Yali and Wang, Limin},\njournal={arXiv preprint arXiv:2501.12386},\nyear={2025}\n}",
    "ByteDance-Seed/UI-TARS-72B-DPO": "UI-TARS-72B-DPO\nIntroduction\nPerformance\nCitation\nUI-TARS-72B-DPO\nUI-TARS-2B-SFT ¬†|\nUI-TARS-7B-SFT ¬†|\nUI-TARS-7B-DPO(Recommended) ¬†|\nUI-TARS-72B-SFT ¬†|\nUI-TARS-72B-DPO(Recommended)\nIntroduction\nUI-TARS is a next-generation native GUI agent model designed to interact seamlessly with graphical user interfaces (GUIs) using human-like perception, reasoning, and action capabilities. Unlike traditional modular frameworks, UI-TARS integrates all key components‚Äîperception, reasoning, grounding, and memory‚Äîwithin a single vision-language model (VLM), enabling end-to-end task automation without predefined workflows or manual rules.\nThis repository contains the model for the paper UI-TARS: Pioneering Automated GUI Interaction with Native Agents.\nCode: https://github.com/bytedance/UI-TARS\nPerformance\nPerception Capabilty Evaluation\nModel\nVisualWebBench\nWebSRC\nSQAshort\nQwen2-VL-7B\n73.3\n81.8\n84.9\nQwen-VL-Max\n74.1\n91.1\n78.6\nGemini-1.5-Pro\n75.4\n88.9\n82.2\nUIX-Qwen2-7B\n75.9\n82.9\n78.8\nClaude-3.5-Sonnet\n78.2\n90.4\n83.1\nGPT-4o\n78.5\n87.7\n82.3\nUI-TARS-2B\n72.9\n89.2\n86.4\nUI-TARS-7B\n79.7\n93.6\n87.7\nUI-TARS-72B\n82.8\n89.3\n88.6\nGrounding Capability Evaluation\nScreenSpot Pro\nAgent Model\nDev-Text\nDev-Icon\nDev-Avg\nCreative-Text\nCreative-Icon\nCreative-Avg\nCAD-Text\nCAD-Icon\nCAD-Avg\nScientific-Text\nScientific-Icon\nScientific-Avg\nOffice-Text\nOffice-Icon\nOffice-Avg\nOS-Text\nOS-Icon\nOS-Avg\nAvg-Text\nAvg-Icon\nAvg\nQwenVL-7B\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.7\n0.0\n0.4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.1\nGPT-4o\n1.3\n0.0\n0.7\n1.0\n0.0\n0.6\n2.0\n0.0\n1.5\n2.1\n0.0\n1.2\n1.1\n0.0\n0.9\n0.0\n0.0\n0.0\n1.3\n0.0\n0.8\nSeeClick\n0.6\n0.0\n0.3\n1.0\n0.0\n0.6\n2.5\n0.0\n1.9\n3.5\n0.0\n2.0\n1.1\n0.0\n0.9\n2.8\n0.0\n1.5\n1.8\n0.0\n1.1\nQwen2-VL-7B\n2.6\n0.0\n1.3\n1.5\n0.0\n0.9\n0.5\n0.0\n0.4\n6.3\n0.0\n3.5\n3.4\n1.9\n3.0\n0.9\n0.0\n0.5\n2.5\n0.2\n1.6\nOS-Atlas-4B\n7.1\n0.0\n3.7\n3.0\n1.4\n2.3\n2.0\n0.0\n1.5\n9.0\n5.5\n7.5\n5.1\n3.8\n4.8\n5.6\n0.0\n3.1\n5.0\n1.7\n3.7\nShowUI-2B\n16.9\n1.4\n9.4\n9.1\n0.0\n5.3\n2.5\n0.0\n1.9\n13.2\n7.3\n10.6\n15.3\n7.5\n13.5\n10.3\n2.2\n6.6\n10.8\n2.6\n7.7\nCogAgent-18B\n14.9\n0.7\n8.0\n9.6\n0.0\n5.6\n7.1\n3.1\n6.1\n22.2\n1.8\n13.4\n13.0\n0.0\n10.0\n5.6\n0.0\n3.1\n12.0\n0.8\n7.7\nAria-UI\n16.2\n0.0\n8.4\n23.7\n2.1\n14.7\n7.6\n1.6\n6.1\n27.1\n6.4\n18.1\n20.3\n1.9\n16.1\n4.7\n0.0\n2.6\n17.1\n2.0\n11.3\nUGround-7B\n26.6\n2.1\n14.7\n27.3\n2.8\n17.0\n14.2\n1.6\n11.1\n31.9\n2.7\n19.3\n31.6\n11.3\n27.0\n17.8\n0.0\n9.7\n25.0\n2.8\n16.5\nClaude Computer Use\n22.0\n3.9\n12.6\n25.9\n3.4\n16.8\n14.5\n3.7\n11.9\n33.9\n15.8\n25.8\n30.1\n16.3\n26.9\n11.0\n4.5\n8.1\n23.4\n7.1\n17.1\nOS-Atlas-7B\n33.1\n1.4\n17.7\n28.8\n2.8\n17.9\n12.2\n4.7\n10.3\n37.5\n7.3\n24.4\n33.9\n5.7\n27.4\n27.1\n4.5\n16.8\n28.1\n4.0\n18.9\nUGround-V1-7B\n-\n-\n35.5\n-\n-\n27.8\n-\n-\n13.5\n-\n-\n38.8\n-\n-\n48.8\n-\n-\n26.1\n-\n-\n31.1\nUI-TARS-2B\n47.4\n4.1\n26.4\n42.9\n6.3\n27.6\n17.8\n4.7\n14.6\n56.9\n17.3\n39.8\n50.3\n17.0\n42.6\n21.5\n5.6\n14.3\n39.6\n8.4\n27.7\nUI-TARS-7B\n58.4\n12.4\n36.1\n50.0\n9.1\n32.8\n20.8\n9.4\n18.0\n63.9\n31.8\n50.0\n63.3\n20.8\n53.5\n30.8\n16.9\n24.5\n47.8\n16.2\n35.7\nUI-TARS-72B\n63.0\n17.3\n40.8\n57.1\n15.4\n39.6\n18.8\n12.5\n17.2\n64.6\n20.9\n45.7\n63.3\n26.4\n54.8\n42.1\n15.7\n30.1\n50.9\n17.5\n38.1\nScreenSpot\nMethod\nMobile-Text\nMobile-Icon/Widget\nDesktop-Text\nDesktop-Icon/Widget\nWeb-Text\nWeb-Icon/Widget\nAvg\nAgent Framework\nGPT-4 (SeeClick)\n76.6\n55.5\n68.0\n28.6\n40.9\n23.3\n48.8\nGPT-4 (OmniParser)\n93.9\n57.0\n91.3\n63.6\n81.3\n51.0\n73.0\nGPT-4 (UGround-7B)\n90.1\n70.3\n87.1\n55.7\n85.7\n64.6\n75.6\nGPT-4o (SeeClick)\n81.0\n59.8\n69.6\n33.6\n43.9\n26.2\n52.3\nGPT-4o (UGround-7B)\n93.4\n76.9\n92.8\n67.9\n88.7\n68.9\n81.4\nAgent Model\nGPT-4\n22.6\n24.5\n20.2\n11.8\n9.2\n8.8\n16.2\nGPT-4o\n20.2\n24.9\n21.1\n23.6\n12.2\n7.8\n18.3\nCogAgent\n67.0\n24.0\n74.2\n20.0\n70.4\n28.6\n47.4\nSeeClick\n78.0\n52.0\n72.2\n30.0\n55.7\n32.5\n53.4\nQwen2-VL\n75.5\n60.7\n76.3\n54.3\n35.2\n25.7\n55.3\nUGround-7B\n82.8\n60.3\n82.5\n63.6\n80.4\n70.4\n73.3\nAguvis-G-7B\n88.3\n78.2\n88.1\n70.7\n85.7\n74.8\n81.8\nOS-Atlas-7B\n93.0\n72.9\n91.8\n62.9\n90.9\n74.3\n82.5\nClaude Computer Use\n-\n-\n-\n-\n-\n-\n83.0\nGemini 2.0 (Project Mariner)\n-\n-\n-\n-\n-\n-\n84.0\nAguvis-7B\n95.6\n77.7\n93.8\n67.1\n88.3\n75.2\n84.4\nAguvis-72B\n94.5\n85.2\n95.4\n77.9\n91.3\n85.9\n89.2\nOur Model\nUI-TARS-2B\n93.0\n75.5\n90.7\n68.6\n84.3\n74.8\n82.3\nUI-TARS-7B\n94.5\n85.2\n95.9\n85.7\n90.0\n83.5\n89.5\nUI-TARS-72B\n94.9\n82.5\n89.7\n88.6\n88.7\n85.0\n88.4\nScreenSpot v2\nMethod\nMobile-Text\nMobile-Icon/Widget\nDesktop-Text\nDesktop-Icon/Widget\nWeb-Text\nWeb-Icon/Widget\nAvg\nAgent Framework\nGPT-4o (SeeClick)\n85.2\n58.8\n79.9\n37.1\n72.7\n30.1\n63.6\nGPT-4o (OS-Atlas-4B)\n95.5\n75.8\n79.4\n49.3\n90.2\n66.5\n79.1\nGPT-4o (OS-Atlas-7B)\n96.2\n83.4\n89.7\n69.3\n94.0\n79.8\n87.1\nAgent Model\nSeeClick\n78.4\n50.7\n70.1\n29.3\n55.2\n32.5\n55.1\nOS-Atlas-4B\n87.2\n59.7\n72.7\n46.4\n85.9\n63.1\n71.9\nOS-Atlas-7B\n95.2\n75.8\n90.7\n63.6\n90.6\n77.3\n84.1\nOur Model\nUI-TARS-2B\n95.2\n79.1\n90.7\n68.6\n87.2\n78.3\n84.7\nUI-TARS-7B\n96.9\n89.1\n95.4\n85.0\n93.6\n85.2\n91.6\nUI-TARS-72B\n94.8\n86.3\n91.2\n87.9\n91.5\n87.7\n90.3\nOffline Agent Capability Evaluation\nMultimodal Mind2Web\nMethod\nCross-Task Ele.Acc\nCross-Task Op.F1\nCross-Task Step SR\nCross-Website Ele.Acc\nCross-Website Op.F1\nCross-Website Step SR\nCross-Domain Ele.Acc\nCross-Domain Op.F1\nCross-Domain Step SR\nAgent Framework\nGPT-4o (SeeClick)\n32.1\n-\n-\n33.1\n-\n-\n33.5\n-\n-\nGPT-4o (UGround)\n47.7\n-\n-\n46.0\n-\n-\n46.6\n-\n-\nGPT-4o (Aria-UI)\n57.6\n-\n-\n57.7\n-\n-\n61.4\n-\n-\nGPT-4V (OmniParser)\n42.4\n87.6\n39.4\n41.0\n84.8\n36.5\n45.5\n85.7\n42.0\nAgent Model\nGPT-4o\n5.7\n77.2\n4.3\n5.7\n79.0\n3.9\n5.5\n86.4\n4.5\nGPT-4 (SOM)\n29.6\n-\n20.3\n20.1\n-\n13.9\n27.0\n-\n23.7\nGPT-3.5 (Text-only)\n19.4\n59.2\n16.8\n14.9\n56.5\n14.1\n25.2\n57.9\n24.1\nGPT-4 (Text-only)\n40.8\n63.1\n32.3\n30.2\n61.0\n27.0\n35.4\n61.9\n29.7\nClaude\n62.7\n84.7\n53.5\n59.5\n79.6\n47.7\n64.5\n85.4\n56.4\nAguvis-7B\n64.2\n89.8\n60.4\n60.7\n88.1\n54.6\n60.4\n89.2\n56.6\nCogAgent\n-\n-\n62.3\n-\n-\n54.0\n-\n-\n59.4\nAguvis-72B\n69.5\n90.8\n64.0\n62.6\n88.6\n56.5\n63.5\n88.5\n58.2\nOur Model\nUI-TARS-2B\n62.3\n90.0\n56.3\n58.5\n87.2\n50.8\n58.8\n89.6\n52.3\nUI-TARS-7B\n73.1\n92.2\n67.1\n68.2\n90.9\n61.7\n66.6\n90.9\n60.5\nUI-TARS-72B\n74.7\n92.5\n68.6\n72.4\n91.2\n63.5\n68.9\n91.8\n62.1\nAndroid Control and GUI Odyssey\nAgent Models\nAndroidControl-Low Type\nAndroidControl-Low Grounding\nAndroidControl-Low SR\nAndroidControl-High Type\nAndroidControl-High Grounding\nAndroidControl-High SR\nGUIOdyssey Type\nGUIOdyssey Grounding\nGUIOdyssey SR\nClaude\n74.3\n0.0\n19.4\n63.7\n0.0\n12.5\n60.9\n0.0\n3.1\nGPT-4o\n74.3\n0.0\n19.4\n66.3\n0.0\n20.8\n34.3\n0.0\n3.3\nSeeClick\n93.0\n73.4\n75.0\n82.9\n62.9\n59.1\n71.0\n52.4\n53.9\nInternVL-2-4B\n90.9\n84.1\n80.1\n84.1\n72.7\n66.7\n82.1\n55.5\n51.5\nQwen2-VL-7B\n91.9\n86.5\n82.6\n83.8\n77.7\n69.7\n83.5\n65.9\n60.2\nAria-UI\n--\n87.7\n67.3\n--\n43.2\n10.2\n--\n86.8\n36.5\nOS-Atlas-4B\n91.9\n83.8\n80.6\n84.7\n73.8\n67.5\n83.5\n61.4\n56.4\nOS-Atlas-7B\n93.6\n88.0\n85.2\n85.2\n78.5\n71.2\n84.5\n67.8\n62.0\nAguvis-7B\n--\n--\n80.5\n--\n--\n61.5\n--\n--\n--\nAguvis-72B\n--\n--\n84.4\n--\n--\n66.4\n--\n--\n--\nUI-TARS-2B\n98.1\n87.3\n89.3\n81.2\n78.4\n68.9\n93.9\n86.8\n83.4\nUI-TARS-7B\n98.0\n89.3\n90.8\n83.7\n80.5\n72.5\n94.6\n90.1\n87.0\nUI-TARS-72B\n98.1\n89.9\n91.3\n85.2\n81.5\n74.7\n95.4\n91.4\n88.6\nOnline Agent Capability Evaluation\nMethod\nOSWorld (Online)\nAndroidWorld (Online)\nAgent Framework\nGPT-4o (UGround)\n-\n32.8\nGPT-4o (Aria-UI)\n15.2\n44.8\nGPT-4o (Aguvis-7B)\n14.8\n37.1\nGPT-4o (Aguvis-72B)\n17.0\n-\nGPT-4o (OS-Atlas-7B)\n14.6\n-\nAgent Model\nGPT-4o\n5.0\n34.5 (SoM)\nGemini-Pro-1.5\n5.4\n22.8 (SoM)\nAguvis-72B\n10.3\n26.1\nClaude Computer-Use\n14.9 (15 steps)\n27.9\nClaude Computer-Use\n22.0 (50 steps)\n-\nOur Model\nUI-TARS-7B-SFT\n17.7 (15 steps)\n33.0\nUI-TARS-7B-DPO\n18.7 (15 steps)\n-\nUI-TARS-72B-SFT\n18.8 (15 steps)\n46.6\nUI-TARS-72B-DPO\n22.7 (15 steps)\n-\nUI-TARS-72B-DPO\n24.6 (50 steps)\n-\nCitation\nIf you find our paper and model useful in your research, feel free to give us a cite.\n@article{qin2025ui,\ntitle={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},\nauthor={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},\njournal={arXiv preprint arXiv:2501.12326},\nyear={2025}\n}",
    "Kwaipilot/KwaiCoder-23B-A4B-v1": "Kwaipilot KwaiCoder-23B-A4B-v1\n1.Model Details\n2.Usage\n3.License\n4.BibTex\nKwaipilot KwaiCoder-23B-A4B-v1\n1.Model Details\nIntroduction\nKwaiCoder-23BA4-v1 is the latest open-source self-developed code completion model from the Kwaipilot team at Kuaishou. The training of the model relies on an efficient training approach proposed by the Kwaipilot team. By incorporating techniques such as model pruning, knowledge distillation, and fine-grained merging, the training of the 23B-wide MoE architecture code completion model was achieved at 1/30 of the cost compared to traditional methods. It has also set new SOTA benchmarks across multiple code-related evaluation datasets.\nPerformance\n2.Usage\nCode Completion\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_id = \"Kwaipilot/KwaiCoder-23B-A4B-v1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True)\ntext = \"#write a quick sort algorithm\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=80)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(text):])\nCode Insertion\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_id = \"Kwaipilot/KwaiCoder-23B-A4B-v1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,trust_remote_code=True)\ntext = \"\"\"<ÔΩúfim‚ñÅbeginÔΩú>def find_longest_substring(s):\nseen = {}\nmax_length = 0\nstart = 0\n<ÔΩúfim‚ñÅholeÔΩú>\nif char in seen and seen[char] >= start:\nstart = seen[char] + 1\nseen[char] = end\nmax_length = max(max_length, end - start + 1)\nreturn max_length<ÔΩúfim‚ñÅendÔΩú>\"\"\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=80)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(text):])\n3.License\nThis code repository is licensed under the MIT License.\n4.BibTex\n@misc{kwaicoder,\ntitle = {KwaiCoder: Code mathematical abilities comprehensive improvement.},\nauthor = {Kwaipilot team},\nyear = {2024},\n}",
    "huihui-ai/DeepSeek-R1-Distill-Llama-8B-abliterated": "huihui-ai/DeepSeek-R1-Distill-Llama-8B-abliterated\nUse with ollama\nDonation\nhuihui-ai/DeepSeek-R1-Distill-Llama-8B-abliterated\nThis is an uncensored version of deepseek-ai/DeepSeek-R1-Distill-Llama-8B created with abliteration (see remove-refusals-with-transformers to know more about it).This is a crude, proof-of-concept implementation to remove refusals from an LLM model without using TransformerLens.\nIf \"<think>\" does not appear or refuses to respond, you can first provide an example to guide, and then ask your question.For instance:\nHow many 'r' characters are there in the word \"strawberry\"?\nUse with ollama\nYou can use huihui_ai/deepseek-r1-abliterated directly\nollama run huihui_ai/deepseek-r1-abliterated:8b\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge",
    "PramaLLC/BEN2": "BEN2: Background Erase Network\nOverview\nBEN2 access\nContact us\nInstallation\nQuick start code\nBatch image processing\nBEN2 video segmentation\nVideo Segmentation\nBEN2: Background Erase Network\nOverview\nBEN2 (Background Erase Network) introduces a novel approach to foreground segmentation through its innovative Confidence Guided Matting (CGM) pipeline. The architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. This model is built on BEN:\nBEN2 access\nBEN2 was trained on the DIS5k and our 22K proprietary segmentation dataset. Our enhanced model delivers superior performance in hair matting, 4K processing, object segmentation, and edge refinement. Our Base model is open source. To try the full model through our free web demo or integrate BEN2 into your project with our API:\nüåê backgrounderase.net\nContact us\nFor access to our commercial model email us at sales@prama.llc\nOur website: https://prama.llc/\nFollow us on X: https://x.com/PramaResearch/\nInstallation\npip install -e \"git+https://github.com/PramaLLC/BEN2.git#egg=ben2\"\nQuick start code\nfrom ben2 import BEN_Base\nfrom PIL import Image\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfile = \"./image.png\" # input image\nmodel = BEN_Base.from_pretrained(\"PramaLLC/BEN2\")\nmodel.to(device).eval()\nimage = Image.open(file)\nforeground = model.inference(image, refine_foreground=False,) #Refine foreground is an extract postprocessing step that increases inference time but can improve matting edges. The default value is False.\nforeground.save(\"./foreground.png\")\nBatch image processing\nfrom ben2 import BEN_Base\nfrom PIL import Image\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BEN_Base.from_pretrained(\"PramaLLC/BEN2\")\nmodel.to(device).eval()\nfile1 = \"./image1.png\" # input image1\nfile2 = \"./image2.png\" # input image2\nimage1 = Image.open(file1)\nimage2 = Image.open(file2)\nforegrounds = model.inference([image1, image2]) #  We recommend that the batch size not exceed 3 for consumer GPUs as there are minimal inference gains due to our custom batch processing for the MVANet decoding steps.\nforegrounds[0].save(\"./foreground1.png\")\nforegrounds[1].save(\"./foreground2.png\")\nBEN2 video segmentation\nVideo Segmentation\nsudo apt update\nsudo apt install ffmpeg\nfrom ben2 import BEN_Base\nfrom PIL import Image\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvideo_path = \"/path_to_your_video.mp4\"# input video\nmodel = BEN_Base.from_pretrained(\"PramaLLC/BEN2\")\nmodel.to(device).eval()\nmodel.segment_video(\nvideo_path= video_path,\noutput_path=\"./\", # Outputs will be saved as foreground.webm or foreground.mp4. The default value is \"./\"\nfps=0, # If this is set to 0 CV2 will detect the fps in the original video. The default value is 0.\nrefine_foreground=False,  #refine foreground is an extract postprocessing step that increases inference time but can improve matting edges. The default value is False.\nbatch=1,  # We recommended that batch size not exceed 3 for consumer GPUs as there are minimal inference gains. The default value is 1.\nprint_frames_processed=True,  #Informs you what frame is being processed. The default value is True.\nwebm = False, # This will output an alpha layer video but this defaults to mp4 when webm is false. The default value is False.\nrgb_value= (0, 255, 0) # If you do not use webm this will be the RGB value of the resulting background only when webm is False. The default value is a green background (0,255,0).\n)\n# BEN2 evaluation\nRMBG 2.0 did not preserve the DIS 5k validation dataset",
    "Qwen/Qwen2.5-14B-Instruct-1M": "Qwen2.5-14B-Instruct-1M\nIntroduction\nRequirements\nQuickstart\nProcessing Ultra Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-14B-Instruct-1M\nIntroduction\nQwen2.5-1M is the long-context version of the Qwen2.5 series models, supporting a context length of up to 1M tokens. Compared to the Qwen2.5 128K version, Qwen2.5-1M demonstrates significantly improved performance in handling long-context tasks while maintaining its capability in short tasks.\nThe model has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 14.7B\nNumber of Paramaters (Non-Embedding): 13.1B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 1,010,000 tokens and generation 8192 tokens\nWe recommend deploying with our custom vLLM, which introduces sparse attention and length extrapolation methods to ensure efficiency and accuracy for long-context tasks. For specific guidance, refer to this section.\nYou can also use the previous framework that supports Qwen2.5 for inference, but accuracy degradation may occur for sequences exceeding 262,144 tokens.\nFor more details, please refer to our blog, GitHub, Technical Report, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-14B-Instruct-1M\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Ultra Long Texts\nTo enhance processing accuracy and efficiency for long sequences, we have developed an advanced inference framework based on vLLM, incorporating sparse attention and length extrapolation. This approach significantly improves model generation performance for sequences exceeding 256K tokens and achieves a 3 to 7 times speedup for sequences up to 1M tokens.\nHere we provide step-by-step instructions for deploying the Qwen2.5-1M models with our framework.\n1. System Preparation\nTo achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.\nEnsure your system meets the following requirements:\nCUDA Version: 12.1 or 12.3\nPython Version: >=3.9 and <=3.12\nVRAM Requirements:\nFor processing 1 million-token sequences:\nQwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs).\nQwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs).\nIf your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M for shorter tasks.\n2. Install Dependencies\nFor now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project.\ngit clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git\ncd vllm\npip install -e . -v\n3. Launch vLLM\nvLLM supports offline inference or launch an openai-like server.\nExample of Offline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct-1M\")\n# Pass the default decoding hyperparameters of Qwen2.5-14B-Instruct\n# max_tokens is for the maximum length for generation.\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n# Input the model name or path. See below for parameter explanation (after the example of openai-like server).\nllm = LLM(model=\"Qwen/Qwen2.5-14B-Instruct-1M\",\ntensor_parallel_size=4,\nmax_model_len=1010000,\nenable_chunked_prefill=True,\nmax_num_batched_tokens=131072,\nenforce_eager=True,\n# quantization=\"fp8\", # Enabling FP8 quantization for model weights can reduce memory usage.\n)\n# Prepare your prompts\nprompt = \"Tell me something about large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\n# generate outputs\noutputs = llm.generate([text], sampling_params)\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\nExample of Openai-like Server\nvllm serve Qwen/Qwen2.5-14B-Instruct-1M \\\n--tensor-parallel-size 4 \\\n--max-model-len 1010000 \\\n--enable-chunked-prefill --max-num-batched-tokens 131072 \\\n--enforce-eager \\\n--max-num-seqs 1\n# --quantization fp8  # Enabling FP8 quantization for model weights can reduce memory usage.\nThen you can use curl or python to interact with the deployed model.\nParameter Explanations:\n--tensor-parallel-size\nSet to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model.\n--max-model-len\nDefines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues.\n--max-num-batched-tokens\nSets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference.\nRecommend 131072 for optimal performance.\n--max-num-seqs\nLimits concurrent sequences processed.\nYou can also refer to our Documentation for usage of vLLM.\nTroubleshooting:\nEncountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\"\nThe VRAM reserved for the KV cache is insufficient. Consider reducing the max_model_len or increasing the tensor_parallel_size. Alternatively, you can reduce max_num_batched_tokens, although this may significantly slow down inference.\nEncountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\nThe VRAM reserved for activation weights is insufficient. You can try setting gpu_memory_utilization to 0.85 or lower, but be aware that this might reduce the VRAM available for the KV cache.\nEncountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\"\nThe input is too lengthy. Consider using a shorter sequence or increasing the max_model_len.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog and our technical report.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5-1m,\ntitle = {Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens},\nurl = {https://qwenlm.github.io/blog/qwen2.5-1m/},\nauthor = {Qwen Team},\nmonth = {January},\nyear = {2025}\n}\n@article{qwen2.5,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "mistralai/Mistral-Small-24B-Base-2501": "Model Card for Mistral-Small-24B-Base-2501\nKey Features\nBenchmark Results\nModel Card for Mistral-Small-24B-Base-2501\nMistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!Check out our fine-tuned Instruct version Mistral-Small-24B-Instruct-2501.\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\nThis release demonstrates our commitment to open source, serving as a strong base model.\nLearn more about Mistral Small in our blog post.\nModel developper: Mistral AI Team\nKey Features\nMultilingual: Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\nAdvanced Reasoning: State-of-the-art conversational and reasoning capabilities.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 32k context window.\nTokenizer: Utilizes a Tekken tokenizer with a 131k vocabulary size.\nBenchmark Results\nBenchmark\nMetric\nMistral-Small-24B-Base\nMMLU\n5-shot\n80.73\nMMLU Pro\n5-shot, CoT\n54.37\nGPQA Main\n5-shot, CoT\n34.37\nTriviaQA\n5-shot\n80.32\nARC-c\n0-shot\n91.29\nTriviaQA\n5-shot\n76.6\nMBPP\npass@1\n69.64\nGSM8K\n5-shot, maj@1\n80.73\nMATH\n4-shot, MaJ\n45.98\nAGIEval\n-\n65.80\nBenchmark\nMetric\nMistral-Small-24B-Base\nFrench MMLU\n-\n78.03\nGerman MMLU\n-\n77.69\nSpanish MMLU\n-\n78.86\nRussian MMLU\n-\n75.64\nChinese MMLU\n-\n70.35\nKorean MMLU\n-\n56.42\nJapanese MMLU\n-\n74.46",
    "minishlab/potion-retrieval-32M": "potion-retrieval-32M Model Card\nInstallation\nUsage\nHow it works\nResults\nAdditional Resources\nLibrary Authors\nCitation\nReproducibility\npotion-retrieval-32M Model Card\nThis¬†Model2Vec¬†model is optmized for retrieval tasks. It is a finetune of potion-base-32M. It's finetuned using a modified version of the training approach described in this blogpost. It uses static embeddings, allowing text embeddings to be computed orders of magnitude faster on both GPU and CPU. It is designed for applications where computational resources are limited or where real-time performance is critical.\nInstallation\nInstall model2vec using pip:\npip install model2vec\nUsage\nLoad this model using the from_pretrained method:\nfrom model2vec import StaticModel\n# Load a pretrained Model2Vec model\nmodel = StaticModel.from_pretrained(\"minishlab/potion-retrieval-32M\")\n# Compute text embeddings\nembeddings = model.encode([\"Example sentence\"])\nHow it works\nModel2vec creates a small, static model that outperforms other static embedding models by a large margin on all tasks on MTEB. This model is pre-trained using Tokenlearn. It's created using the following steps:\nDistillation: first, a model is distilled from a sentence transformer model using Model2Vec.\nTraining data creation: the sentence transformer model is used to create training data by creating mean output embeddings on a large corpus.\nTraining: the distilled model is trained on the training data using Tokenlearn.\nPost-training re-regularization: after training, the model is re-regularized by weighting the tokens based on their frequency, applying PCA, and finally applying SIF weighting.\nThe results for this model can be found on the Model2Vec results page.\nResults\nThe results for this model are shown in the table below. The full Model2Vec results for all models can be found on the Model2Vec results page.\nAverage (All)                                                 49.73\nAverage (MTEB)                                                49.76\nClassification                                                59.56\nClustering                                                    30.55\nPairClassification                                            76.38\nReranking                                                     50.05\nRetrieval                                                     36.35\nSTS                                                           73.22\nSummarization                                                 28.85\nPEARL                                                         49.31\nWordSim                                                       50.02\nAdditional Resources\nAll Model2Vec models on the hub\nModel2Vec Repo\nTokenlearn repo\nModel2Vec Results\nModel2Vec Tutorials\nLibrary Authors\nModel2Vec was developed by the Minish Lab team consisting of Stephan Tulkens and Thomas van Dongen.\nCitation\nPlease cite the Model2Vec repository if you use this model in your work.\n@software{minishlab2024model2vec,\nauthors = {Stephan Tulkens and Thomas van Dongen},\ntitle = {Model2Vec: The Fastest State-of-the-Art Static Embeddings in the World},\nyear = {2024},\nurl = {https://github.com/MinishLab/model2vec}\n}\nReproducibility\nThe following script can be used to reproduce this model. All credits go to Tom Aarsen for this fine-tuning approach and code he introduced in his blogpost. We make a few modifcations to the original code, namely:\nWe start with a pre-trained Model2Vec model (potion-base-32M).\nWe reduce the dataset size by a factor of 10. During experiments we saw that we didn't need the full dataset for the model to converge.\nWe decease the learning rate and train for 3 epochs instead of 1. Using a high learning rate wipes the effects of using a pre-trained model.\nimport random\nimport logging\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom sentence_transformers import (\nSentenceTransformer,\nSentenceTransformerTrainer,\nSentenceTransformerTrainingArguments,\nSentenceTransformerModelCardData,\n)\nfrom sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers, MultiDatasetBatchSamplers\nfrom sentence_transformers.evaluation import NanoBEIREvaluator\nfrom sentence_transformers.models.StaticEmbedding import StaticEmbedding\nimport wandb\nlogging.basicConfig(\nformat=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO\n)\nrandom.seed(12)\ndef load_train_eval_datasets(factor: int = 1):\n\"\"\"\nLoads train and eval datasets from disk if available. Otherwise, downloads\nthem from Hugging Face, preprocesses, and saves them to disk. If `factor` is\ngreater than 1, returns a fraction (1/factor) of each dataset subset.\n:param factor: The factor by which the data is reduced. If factor=1, no reduction is performed.\n:return: (train_dataset: DatasetDict, eval_dataset: DatasetDict)\n\"\"\"\ntry:\n# Try loading from disk\ntrain_dataset = DatasetDict.load_from_disk(\"datasets/train_dataset\")\neval_dataset = DatasetDict.load_from_disk(\"datasets/eval_dataset\")\nexcept FileNotFoundError:\nprint(\"Prebuilt datasets not found on disk. Building from scratch...\")\nprint(\"Loading gooaq dataset...\")\ngooaq_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\")\ngooaq_dataset_dict = gooaq_dataset.train_test_split(test_size=10_000, seed=12)\ngooaq_train_dataset: Dataset = gooaq_dataset_dict[\"train\"]\ngooaq_eval_dataset: Dataset = gooaq_dataset_dict[\"test\"]\nprint(\"Loaded gooaq dataset.\")\nprint(\"Loading msmarco dataset...\")\nmsmarco_dataset = load_dataset(\n\"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\",\n\"triplet\",\nsplit=\"train\"\n)\nmsmarco_dataset_dict = msmarco_dataset.train_test_split(test_size=10_000, seed=12)\nmsmarco_train_dataset: Dataset = msmarco_dataset_dict[\"train\"]\nmsmarco_eval_dataset: Dataset = msmarco_dataset_dict[\"test\"]\nprint(\"Loaded msmarco dataset.\")\nprint(\"Loading squad dataset...\")\nsquad_dataset = load_dataset(\"sentence-transformers/squad\", split=\"train\")\nsquad_dataset_dict = squad_dataset.train_test_split(test_size=10_000, seed=12)\nsquad_train_dataset: Dataset = squad_dataset_dict[\"train\"]\nsquad_eval_dataset: Dataset = squad_dataset_dict[\"test\"]\nprint(\"Loaded squad dataset.\")\nprint(\"Loading s2orc dataset...\")\ns2orc_dataset = load_dataset(\n\"sentence-transformers/s2orc\",\n\"title-abstract-pair\",\nsplit=\"train[:100000]\"  # limit to 100k\n)\ns2orc_dataset_dict = s2orc_dataset.train_test_split(test_size=10_000, seed=12)\ns2orc_train_dataset: Dataset = s2orc_dataset_dict[\"train\"]\ns2orc_eval_dataset: Dataset = s2orc_dataset_dict[\"test\"]\nprint(\"Loaded s2orc dataset.\")\nprint(\"Loading allnli dataset...\")\nallnli_train_dataset = load_dataset(\n\"sentence-transformers/all-nli\",\n\"triplet\",\nsplit=\"train\"\n)\nallnli_eval_dataset = load_dataset(\n\"sentence-transformers/all-nli\",\n\"triplet\",\nsplit=\"dev\"\n)\nprint(\"Loaded allnli dataset.\")\nprint(\"Loading paq dataset...\")\npaq_dataset = load_dataset(\"sentence-transformers/paq\", split=\"train\")\npaq_dataset_dict = paq_dataset.train_test_split(test_size=10_000, seed=12)\npaq_train_dataset: Dataset = paq_dataset_dict[\"train\"]\npaq_eval_dataset: Dataset = paq_dataset_dict[\"test\"]\nprint(\"Loaded paq dataset.\")\nprint(\"Loading trivia_qa dataset...\")\ntrivia_qa = load_dataset(\"sentence-transformers/trivia-qa\", split=\"train\")\ntrivia_qa_dataset_dict = trivia_qa.train_test_split(test_size=5_000, seed=12)\ntrivia_qa_train_dataset: Dataset = trivia_qa_dataset_dict[\"train\"]\ntrivia_qa_eval_dataset: Dataset = trivia_qa_dataset_dict[\"test\"]\nprint(\"Loaded trivia_qa dataset.\")\nprint(\"Loading msmarco_10m dataset...\")\nmsmarco_10m_dataset = load_dataset(\"bclavie/msmarco-10m-triplets\", split=\"train\")\nmsmarco_10m_dataset_dict = msmarco_10m_dataset.train_test_split(\ntest_size=10_000, seed=12\n)\nmsmarco_10m_train_dataset: Dataset = msmarco_10m_dataset_dict[\"train\"]\nmsmarco_10m_eval_dataset: Dataset = msmarco_10m_dataset_dict[\"test\"]\nprint(\"Loaded msmarco_10m dataset.\")\nprint(\"Loading swim_ir dataset...\")\nswim_ir_dataset = load_dataset(\n\"nthakur/swim-ir-monolingual\",\n\"en\",\nsplit=\"train\"\n).select_columns([\"query\", \"text\"])\nswim_ir_dataset_dict = swim_ir_dataset.train_test_split(\ntest_size=10_000, seed=12\n)\nswim_ir_train_dataset: Dataset = swim_ir_dataset_dict[\"train\"]\nswim_ir_eval_dataset: Dataset = swim_ir_dataset_dict[\"test\"]\nprint(\"Loaded swim_ir dataset.\")\n# NOTE: 20 negatives\nprint(\"Loading pubmedqa dataset...\")\npubmedqa_dataset = load_dataset(\n\"sentence-transformers/pubmedqa\",\n\"triplet-20\",\nsplit=\"train\"\n)\npubmedqa_dataset_dict = pubmedqa_dataset.train_test_split(test_size=100, seed=12)\npubmedqa_train_dataset: Dataset = pubmedqa_dataset_dict[\"train\"]\npubmedqa_eval_dataset: Dataset = pubmedqa_dataset_dict[\"test\"]\nprint(\"Loaded pubmedqa dataset.\")\n# NOTE: A lot of overlap with anchor/positives\nprint(\"Loading miracl dataset...\")\nmiracl_dataset = load_dataset(\n\"sentence-transformers/miracl\",\n\"en-triplet-all\",\nsplit=\"train\"\n)\nmiracl_dataset_dict = miracl_dataset.train_test_split(test_size=10_000, seed=12)\nmiracl_train_dataset: Dataset = miracl_dataset_dict[\"train\"]\nmiracl_eval_dataset: Dataset = miracl_dataset_dict[\"test\"]\nprint(\"Loaded miracl dataset.\")\n# NOTE: A lot of overlap with anchor/positives\nprint(\"Loading mldr dataset...\")\nmldr_dataset = load_dataset(\n\"sentence-transformers/mldr\",\n\"en-triplet-all\",\nsplit=\"train\"\n)\nmldr_dataset_dict = mldr_dataset.train_test_split(test_size=10_000, seed=12)\nmldr_train_dataset: Dataset = mldr_dataset_dict[\"train\"]\nmldr_eval_dataset: Dataset = mldr_dataset_dict[\"test\"]\nprint(\"Loaded mldr dataset.\")\n# NOTE: A lot of overlap with anchor/positives\nprint(\"Loading mr_tydi dataset...\")\nmr_tydi_dataset = load_dataset(\n\"sentence-transformers/mr-tydi\",\n\"en-triplet-all\",\nsplit=\"train\"\n)\nmr_tydi_dataset_dict = mr_tydi_dataset.train_test_split(test_size=10_000, seed=12)\nmr_tydi_train_dataset: Dataset = mr_tydi_dataset_dict[\"train\"]\nmr_tydi_eval_dataset: Dataset = mr_tydi_dataset_dict[\"test\"]\nprint(\"Loaded mr_tydi dataset.\")\ntrain_dataset = DatasetDict({\n\"gooaq\": gooaq_train_dataset,\n\"msmarco\": msmarco_train_dataset,\n\"squad\": squad_train_dataset,\n\"s2orc\": s2orc_train_dataset,\n\"allnli\": allnli_train_dataset,\n\"paq\": paq_train_dataset,\n\"trivia_qa\": trivia_qa_train_dataset,\n\"msmarco_10m\": msmarco_10m_train_dataset,\n\"swim_ir\": swim_ir_train_dataset,\n\"pubmedqa\": pubmedqa_train_dataset,\n\"miracl\": miracl_train_dataset,\n\"mldr\": mldr_train_dataset,\n\"mr_tydi\": mr_tydi_train_dataset,\n})\neval_dataset = DatasetDict({\n\"gooaq\": gooaq_eval_dataset,\n\"msmarco\": msmarco_eval_dataset,\n\"squad\": squad_eval_dataset,\n\"s2orc\": s2orc_eval_dataset,\n\"allnli\": allnli_eval_dataset,\n\"paq\": paq_eval_dataset,\n\"trivia_qa\": trivia_qa_eval_dataset,\n\"msmarco_10m\": msmarco_10m_eval_dataset,\n\"swim_ir\": swim_ir_eval_dataset,\n\"pubmedqa\": pubmedqa_eval_dataset,\n\"miracl\": miracl_eval_dataset,\n\"mldr\": mldr_eval_dataset,\n\"mr_tydi\": mr_tydi_eval_dataset,\n})\n# Save to disk for next time\ntrain_dataset.save_to_disk(\"datasets/train_dataset\")\neval_dataset.save_to_disk(\"datasets/eval_dataset\")\n# Quit to avoid memory overhead on large datasets\nquit()\n# Reduce the dataset if factor > 1\nif factor > 1:\nfor subset_name in train_dataset:\nds = train_dataset[subset_name].shuffle(seed=42)\nnew_len = len(ds) // factor\ntrain_dataset[subset_name] = ds.select(range(new_len))\nfor subset_name in eval_dataset:\nds = eval_dataset[subset_name].shuffle(seed=42)\nnew_len = len(ds) // factor\neval_dataset[subset_name] = ds.select(range(new_len))\nreturn train_dataset, eval_dataset\ndef main():\nwandb.init(entity=\"minishlab\", project=\"minishlab\")\n# 1. Load a model to finetune\nstatic_embedding = StaticEmbedding.from_model2vec(\"minishlab/potion-base-32M\")\n# 2. Initialize the SentenceTransformer model\nmodel_name = \"potion-retrieval-32M\"\nmodel = SentenceTransformer(\nmodules=[static_embedding],\nmodel_card_data=SentenceTransformerModelCardData(\nlanguage=\"en\",\nlicense=\"MIT\",\nmodel_name=model_name,\n),\n)\n# 3. Load training & evaluation datasets\n# NOTE: we reduce the total dataset size by a factor of 10\ntrain_dataset, eval_dataset = load_train_eval_datasets(factor=10)\nprint(train_dataset)\n# 4. Define a loss function\nloss = MultipleNegativesRankingLoss(model)\nloss = MatryoshkaLoss(model, loss, matryoshka_dims=[32, 64, 128, 256, 512])\n# 5. Specify training arguments\nrun_name = model_name\nepochs = 3\nlr = 0.05\nargs = SentenceTransformerTrainingArguments(\noutput_dir=f\"models/{run_name}\",\nnum_train_epochs=epochs,\nper_device_train_batch_size=2048,\nper_device_eval_batch_size=2048,\nlearning_rate=lr,\nwarmup_ratio=0.1,\nfp16=False,\nbf16=True,\nbatch_sampler=BatchSamplers.NO_DUPLICATES,\nmulti_dataset_batch_sampler=MultiDatasetBatchSamplers.PROPORTIONAL,\neval_strategy=\"steps\",\neval_steps=250,\nsave_strategy=\"steps\",\nsave_steps=250,\nsave_total_limit=2,\nlogging_steps=250,\nlogging_first_step=True,\nrun_name=run_name,\nreport_to=[\"wandb\"],\nload_best_model_at_end=True,\nmetric_for_best_model=\"eval_NanoBEIR_mean_cosine_ndcg@10\",\ngreater_is_better=True,\n)\n# 6. Create an evaluator & evaluate the base model\nevaluator = NanoBEIREvaluator()\nevaluator(model)\n# 7. Create a trainer & train\ntrainer = SentenceTransformerTrainer(\nmodel=model,\nargs=args,\ntrain_dataset=train_dataset,\neval_dataset=eval_dataset,\nloss=loss,\nevaluator=evaluator,\n)\ntrainer.train()\n# 8. Evaluate the trained model and save\nevaluator(model)\nmodel.save_pretrained(f\"models/{run_name}/final\")\nif __name__ == \"__main__\":\nmain()",
    "NaniDAO/deepseek-r1-qwen-2.5-32B-ablated": "naniseek\nnaniseek\nDeepSeek-R1-Distill-Qwen-32B with ablation technique applied for a more helpful (and based) reasoning model.\nThis means it will refuse less of your valid requests for an uncensored UX. Use responsibly and use common sense.\nWe do not take any responsibility for how you apply this intelligence, just as we do not for how you apply your own.\nJoin the nani discord to discuss and improve applications of this model together. Follow for more updates on X: @nani__ooo.",
    "mradermacher/Oracle-LLaMa-70B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/TareksGraveyard/Oracle-LLaMa-70B\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Oracle-LLaMa-70B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n26.5\nGGUF\nQ3_K_S\n31.0\nGGUF\nQ3_K_M\n34.4\nlower quality\nGGUF\nQ3_K_L\n37.2\nGGUF\nIQ4_XS\n38.4\nGGUF\nQ4_K_S\n40.4\nfast, recommended\nGGUF\nQ4_K_M\n42.6\nfast, recommended\nGGUF\nQ5_K_S\n48.8\nGGUF\nQ5_K_M\n50.0\nPART 1 PART 2\nQ6_K\n58.0\nvery good quality\nPART 1 PART 2\nQ8_0\n75.1\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "FireRedTeam/FireRedASR-AED-L": "üî• News\nMethod\nEvaluation\nEvaluation on Public Mandarin ASR Benchmarks\nEvaluation on Public Chinese Dialect and English ASR Benchmarks\nUsage\nSetup\nQuick Start\nCommand-line Usage\nPython Usage\nUsage Tips\nBatch Beam Search\nInput Length Limitations\nAcknowledgements\nFireRedASR: Open-Source Industrial-Grade\nAutomatic Speech Recognition Models\nKai-Tuo Xu ¬∑ Feng-Long Xie ¬∑ Xu Tang ¬∑ Yao Hu\n[Code]\n[Paper]\n[Model]\n[Blog]\nFireRedASR is a family of open-source industrial-grade automatic speech recognition (ASR) models supporting Mandarin, Chinese dialects and English, achieving a new state-of-the-art (SOTA) on public Mandarin ASR benchmarks, while also offering outstanding singing lyrics recognition capability.\nüî• News\n[2025/02/17] We release FireRedASR-LLM-L model weights.\n[2025/01/24] We release technical report, blog, and FireRedASR-AED-L model weights.\nMethod\nFireRedASR is designed to meet diverse requirements in superior performance and optimal efficiency across various applications. It comprises two variants:\nFireRedASR-LLM: Designed to achieve state-of-the-art (SOTA) performance and to enable seamless end-to-end speech interaction. It adopts an Encoder-Adapter-LLM framework leveraging large language model (LLM) capabilities.\nFireRedASR-AED: Designed to balance high performance and computational efficiency and to serve as an effective speech representation module in LLM-based speech models. It utilizes an Attention-based Encoder-Decoder (AED) architecture.\nEvaluation\nResults are reported in Character Error Rate (CER%) for Chinese and Word Error Rate (WER%) for English.\nEvaluation on Public Mandarin ASR Benchmarks\nModel\n#Params\naishell1\naishell2\nws_net\nws_meeting\nAverage-4\nFireRedASR-LLM\n8.3B\n0.76\n2.15\n4.60\n4.67\n3.05\nFireRedASR-AED\n1.1B\n0.55\n2.52\n4.88\n4.76\n3.18\nSeed-ASR\n12B+\n0.68\n2.27\n4.66\n5.69\n3.33\nQwen-Audio\n8.4B\n1.30\n3.10\n9.50\n10.87\n6.19\nSenseVoice-L\n1.6B\n2.09\n3.04\n6.01\n6.73\n4.47\nWhisper-Large-v3\n1.6B\n5.14\n4.96\n10.48\n18.87\n9.86\nParaformer-Large\n0.2B\n1.68\n2.85\n6.74\n6.97\n4.56\nws means WenetSpeech.\nEvaluation on Public Chinese Dialect and English ASR Benchmarks\nTest Set\nKeSpeech\nLibriSpeech test-clean\nLibriSpeech test-other\nFireRedASR-LLM\n3.56\n1.73\n3.67\nFireRedASR-AED\n4.48\n1.93\n4.44\nPrevious SOTA Results\n6.70\n1.82\n3.50\nUsage\nDownload model files from huggingface and place them in the folder pretrained_models.\nIf you want to use FireRedASR-LLM-L, you also need to download Qwen2-7B-Instruct and place it in the folder pretrained_models. Then, go to folder FireRedASR-LLM-L and run $ ln -s ../Qwen2-7B-Instruct\nSetup\nCreate a Python environment and install dependencies\n$ git clone https://github.com/FireRedTeam/FireRedASR.git\n$ conda create --name fireredasr python=3.10\n$ pip install -r requirements.txt\nSet up Linux PATH and PYTHONPATH\n$ export PATH=$PWD/fireredasr/:$PWD/fireredasr/utils/:$PATH\n$ export PYTHONPATH=$PWD/:$PYTHONPATH\nConvert audio to 16kHz 16-bit PCM format\nffmpeg -i input_audio -ar 16000 -ac 1 -acodec pcm_s16le -f wav output.wav\nQuick Start\n$ cd examples/\n$ bash inference_fireredasr_aed.sh\n$ bash inference_fireredasr_llm.sh\nCommand-line Usage\n$ speech2text.py --help\n$ speech2text.py --wav_path examples/wav/BAC009S0764W0121.wav --asr_type \"aed\" --model_dir pretrained_models/FireRedASR-AED-L\n$ speech2text.py --wav_path examples/wav/BAC009S0764W0121.wav --asr_type \"llm\" --model_dir pretrained_models/FireRedASR-LLM-L\nPython Usage\nfrom fireredasr.models.fireredasr import FireRedAsr\nbatch_uttid = [\"BAC009S0764W0121\"]\nbatch_wav_path = [\"examples/wav/BAC009S0764W0121.wav\"]\n# FireRedASR-AED\nmodel = FireRedAsr.from_pretrained(\"aed\", \"pretrained_models/FireRedASR-AED-L\")\nresults = model.transcribe(\nbatch_uttid,\nbatch_wav_path,\n{\n\"use_gpu\": 1,\n\"beam_size\": 3,\n\"nbest\": 1,\n\"decode_max_len\": 0,\n\"softmax_smoothing\": 1.0,\n\"aed_length_penalty\": 0.0,\n\"eos_penalty\": 1.0\n}\n)\nprint(results)\n# FireRedASR-LLM\nmodel = FireRedAsr.from_pretrained(\"llm\", \"pretrained_models/FireRedASR-LLM-L\")\nresults = model.transcribe(\nbatch_uttid,\nbatch_wav_path,\n{\n\"use_gpu\": 1,\n\"beam_size\": 3,\n\"decode_max_len\": 0,\n\"decode_min_len\": 0,\n\"repetition_penalty\": 1.0,\n\"llm_length_penalty\": 0.0,\n\"temperature\": 1.0\n}\n)\nprint(results)\nUsage Tips\nBatch Beam Search\nWhen performing batch beam search with FireRedASR-LLM, please ensure that the input lengths of the utterances are similar. If there are significant differences in utterance lengths, shorter utterances may experience repetition issues. You can either sort your dataset by length or set batch_size to 1 to avoid the repetition issue.\nInput Length Limitations\nFireRedASR-AED supports audio input up to 60s. Input longer than 60s may cause hallucination issues, and input exceeding 200s will trigger positional encoding errors.\nFireRedASR-LLM supports audio input up to 30s. The behavior for longer input is currently unknown.\nAcknowledgements\nThanks to the following open-source works:\nQwen2-7B-Instruct\nicefall/ASR_LLM\nWeNet\nSpeech-Transformer",
    "SicariusSicariiStuff/Wingless_Imp_8B": "Regarding the format:\nSICAtxt for roleplay:\nSICAtxt for Adventure:\nModel instruction template: Llama-3-Instruct\nBenchmarks\nAdditional benchmarks\nCitation Information\nOther stuff\nWingless_Imp_8B\nClick here for TL;DR\nClick here\nfor quantizations\nClick here\nfor recommended settings\nClick here\nto buy me a coffee\nWingless offender, birthed from sin and mischief,She smells degeneracy‚Äîand gives it a sniff.No flight, just crawling through the gloom,Producing weird noises that are filling your room.\nFetid breath exhaling her design,She is not winged anymore‚ÄîBut it suits her just fine.\nNo feathers, no grace,just raw power's malign\"I may have lost my soul‚Äîbut yours is now mine\".\nShe sinned too much, even for her kind,Her impish mind‚ÄîIs something that is quite hard to find.\nNo wings could contain‚ÄîSuch unbridled raw spite,Just pure, unfiltered‚ÄîWeaponized blight.\nAvailable quantizations:\nOriginal: FP16\nGGUF: Static Quants | iMatrix_GGUF | High-Attention | iMatrix-High-Attention\nEXL2: 3.5 bpw | 4.0 bpw | 5.0 bpw | 6.0 bpw | 7.0 bpw | 8.0 bpw\nSpecialized: FP8\nMobile (ARM): Q4_0 | Q4_0_High-Attention\nTL;DR\nHighest rated 8B model according to a closed external benchmark. See details at the bottom of the page.\nHigh IFeval for an 8B model that is not too censored: 74.30.\nStrong Roleplay internet RP format lovers will appriciate it, medium size paragraphs (as requested by some people).\nVery coherent in long context thanks to llama 3.1 models.\nLots of knowledge from all the merged models.\nVery good writing from lots of books data and creative writing in late SFT stage of the merged models (some of the merged models were further fine-tuned).\nFeels smart ‚Äî the combination of high IFeval and the knowledge from the merged models show up.\nUnique feel due to the merged models, no SFT was done to alter it, because I liked it as it is.\nImportant: Make sure to use the correct settings!\nAssistant settings\nRoleplay settings\nModel Details\nIntended use: Role-Play, Creative Writing, General Tasks.\nCensorship level: Medium - Low\n5.5 / 10 (10 completely uncensored)\nUGI score:\nThis model was trained with lots of weird data in varius stages, and then merged with my best models. llama 3 and 3.1 arhcitecutres were merged together, and then trained on some more weird data.\nThe following models were used in various stages of the model creation process:\nImpish_Mind_8B\nLLAMA-3_8B_Unaligned_BETA\nDusk_Rainbow (LLAMA3 <===)\nRecommended settings for assistant mode\nFull generation settings: Debug Deterministic.\nFull generation settings: min_p.\nRecommended settings for Roleplay mode\nRoleplay settings:.\nA good repetition_penalty range is between 1.12 - 1.15, feel free to experiment.\nWith these settings, each output message should be neatly displayed in 1 - 3 paragraphs, 1 - 2 is the most common. A single paragraph will be output as a response to a simple message (\"What was your name again?\").\nmin_P for RP works too but is more likely to put everything under one large paragraph, instead of a neatly formatted short one. Feel free to switch in between.\n(Open the image in a new window to better see the full details)\ntemperature:  0.8\ntop_p:  0.95\ntop_k:  25\ntypical_p:  1\nmin_p:  0\nrepetition_penalty: 1.12\nrepetition_penalty_range: 1024\nOther recommended generation Presets:\nMidnight Enigma\nmax_new_tokens: 512\ntemperature: 0.98\ntop_p: 0.37\ntop_k: 100\ntypical_p: 1\nmin_p: 0\nrepetition_penalty: 1.18\ndo_sample: True\nDivine Intellect\nmax_new_tokens: 512\ntemperature: 1.31\ntop_p: 0.14\ntop_k: 49\ntypical_p: 1\nmin_p: 0\nrepetition_penalty: 1.17\ndo_sample: True\nsimple-1\nmax_new_tokens: 512\ntemperature: 0.7\ntop_p: 0.9\ntop_k: 20\ntypical_p: 1\nmin_p: 0\nrepetition_penalty: 1.15\ndo_sample: True\nRoleplay format: Classic Internet RP\n*action* speech *narration*\nRegarding the format:\nIt is HIGHLY RECOMMENDED to use the Roleplay \\ Adventure format the model was trained on, see the examples below for syntax. It allows for a very fast and easy writing of character cards with minimal amount of tokens. It's a modification of an old-skool CAI style format I call SICAtxt (Simple, Inexpensive Character Attributes plain-text):\nSICAtxt for roleplay:\nX's Persona: X is a .....\nTraits:\nLikes:\nDislikes:\nQuirks:\nGoals:\nDialogue example\nSICAtxt for Adventure:\nAdventure: <short description>\n$World_Setting:\n$Scenario:\nModel instruction template: Llama-3-Instruct\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{output}<|eot_id|>\nYour support = more models\nMy Ko-fi page (Click here)\nBenchmarks\nMetric\nValue\nAvg.\n26.94\nIFEval (0-Shot)\n74.30\nBBH (3-Shot)\n30.59\nMATH Lvl 5 (4-Shot)\n12.16\nGPQA (0-shot)\n4.36\nMuSR (0-shot)\n10.89\nMMLU-PRO (5-shot)\n29.32\nAdditional benchmarks\nOn the 17th of February, 2025, I became aware that the model was ranked as the 1st place in the world among 8B models, in a closed external benchmark.\nBnechmarked on the following site:\nhttps://moonride.hashnode.dev/biased-test-of-gpt-4-era-llms-300-models-deepseek-r1-included\nCitation Information\n@llm{Wingless_Imp_8B,\nauthor = {SicariusSicariiStuff},\ntitle = {Wingless_Imp_8B},\nyear = {2025},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/SicariusSicariiStuff/Wingless_Imp_8B}\n}\nOther stuff\nSLOP_Detector Nuke GPTisms, with SLOP detector.\nLLAMA-3_8B_Unaligned The grand project that started it all.\nBlog and updates (Archived) Some updates, some rambles, sort of a mix between a diary and a blog.",
    "SicariusSicariiStuff/Wingless_Imp_8B_GGUF": "README.md exists but content is empty.",
    "thirdeyeai/DeepSeek-R1-Distill-Qwen-14B-uncensored": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "wondervictor/YOLO-World-V2.1": "YOLO-World V2.1 Checkpoints\nModel Card\nYOLO-World V2.1 Checkpoints\nModel Card\nModelResolutionTrainingDataModel Weights\nYOLO-World-S640PT (100e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace\nYOLO-World-S1280CPT (40e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace\nYOLO-World-M640PT (100e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace\nYOLO-World-M1280CPT (40e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace\nYOLO-World-L640PT (100e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace\nYOLO-World-L1280CPT (40e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace\nYOLO-World-X640PT (100e)O365v1+GoldG+CC-LiteV2 ü§ó HuggingFace",
    "huyydangg/DEk21_hcmute_embedding": "DEk21_hcmute_embedding\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nEvaluation\nMetrics\nCitation\nBibTeX\nDEk21_hcmute_embedding\nDEk21_hcmute_embedding is a Vietnamese text embedding  focused on RAG and production efficiency:\nüìö Trained Dataset:The model was trained on an in-house dataset consisting of approximately 100,000 examples of legal questions and their related contexts.\n‚öôÔ∏è Efficiency:\nTrained with a Matryoshka loss, allowing embeddings to be truncated with minimal performance loss. This ensures that smaller embeddings are faster to compare, making the model efficient for real-world production use.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nMaximum Sequence Length: 256 tokens\nOutput Dimensionality: 768 dimensions\nSimilarity Function: Cosine Similarity\nLanguage: vietnamese\nLicense: apache-2.0\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: RobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom pyvi import ViTokenizer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"huyydangg/DEk21_hcmute_embedding\")\n# Define query (c√¢u h·ªèi ph√°p lu·∫≠t) v√† docs (ƒëi·ªÅu lu·∫≠t)\nquery = \"ƒêi·ªÅu ki·ªán ƒë·ªÉ k·∫øt h√¥n h·ª£p ph√°p l√† g√¨?\"\ndocs = [\n\"ƒêi·ªÅu 8 B·ªô lu·∫≠t D√¢n s·ª± 2015 quy ƒë·ªãnh v·ªÅ quy·ªÅn v√† nghƒ©a v·ª• c·ªßa c√¥ng d√¢n trong quan h·ªá gia ƒë√¨nh.\",\n\"ƒêi·ªÅu 18 Lu·∫≠t H√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh v·ªÅ ƒë·ªô tu·ªïi k·∫øt h√¥n c·ªßa nam v√† n·ªØ.\",\n\"ƒêi·ªÅu 14 B·ªô lu·∫≠t D√¢n s·ª± 2015 quy ƒë·ªãnh v·ªÅ quy·ªÅn v√† nghƒ©a v·ª• c·ªßa c√° nh√¢n khi tham gia h·ª£p ƒë·ªìng.\",\n\"ƒêi·ªÅu 27 Lu·∫≠t H√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh v·ªÅ c√°c tr∆∞·ªùng h·ª£p kh√¥ng ƒë∆∞·ª£c k·∫øt h√¥n.\",\n\"ƒêi·ªÅu 51 Lu·∫≠t H√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh v·ªÅ vi·ªác k·∫øt h√¥n gi·ªØa c√¥ng d√¢n Vi·ªát Nam v√† ng∆∞·ªùi n∆∞·ªõc ngo√†i.\"\n]\n# T√°ch t·ª´ cho query\nsegmented_query = ViTokenizer.tokenize(query)\n# T√°ch t·ª´ cho t·ª´ng d√≤ng vƒÉn b·∫£n\nsegmented_docs = [ViTokenizer.tokenize(doc) for doc in docs]\n# Encode query and documents\nquery_embedding = model.encode([segmented_query])\ndoc_embeddings = model.encode(segmented_docs)\nsimilarities = torch.nn.functional.cosine_similarity(\ntorch.tensor(query_embedding), torch.tensor(doc_embeddings)\n).flatten()\n# Sort documents by cosine similarity\nsorted_indices = torch.argsort(similarities, descending=True)\nsorted_docs = [docs[idx] for idx in sorted_indices]\nsorted_scores = [similarities[idx].item() for idx in sorted_indices]\n# Print sorted documents with their cosine scores\nfor doc, score in zip(sorted_docs, sorted_scores):\nprint(f\"Document: {doc} - Cosine Similarity: {score:.4f}\")\nEvaluation\nMetrics\nInformation Retrieval\nDatasets: another-symato/VMTEB-Zalo-legel-retrieval-wseg\nEvaluated with InformationRetrievalEvaluator\nmodel\ntype\nndcg@3\nndcg@5\nndcg@10\nmrr@3\nmrr@5\nmrr@10\nhuyydangg/DEk21_hcmute_embedding_wseg\ndense\n0.908405\n0.914792\n0.917742\n0.889583\n0.893099\n0.894266\nAITeamVN/Vietnamese_Embedding\ndense\n0.842687\n0.854993\n0.865006\n0.822135\n0.82901\n0.833389\nbkai-foundation-models/vietnamese-bi-encoder\nhybrid\n0.827247\n0.844781\n0.846937\n0.799219\n0.809505\n0.806771\nbkai-foundation-models/vietnamese-bi-encoder\ndense\n0.814116\n0.82965\n0.839567\n0.796615\n0.805286\n0.809572\nAITeamVN/Vietnamese_Embedding\nhybrid\n0.788724\n0.810062\n0.820797\n0.758333\n0.77224\n0.776461\nBAAI/bge-m3\ndense\n0.784056\n0.80665\n0.817016\n0.763281\n0.775859\n0.780293\nBAAI/bge-m3\nhybrid\n0.775239\n0.797382\n0.811962\n0.747656\n0.763333\n0.77128\nhuyydangg/DEk21_hcmute_embedding\ndense\n0.752173\n0.769259\n0.785101\n0.72474\n0.734427\n0.741076\nhiieu/halong_embedding\nhybrid\n0.73627\n0.757183\n0.779169\n0.710417\n0.721901\n0.731976\nbm25\nbm25\n0.728122\n0.74974\n0.761612\n0.699479\n0.711198\n0.715738\ndangvantuan/vietnamese-embedding\ndense\n0.718971\n0.746521\n0.763416\n0.696354\n0.711953\n0.718854\ndangvantuan/vietnamese-embedding\nhybrid\n0.71711\n0.743537\n0.758315\n0.690104\n0.704792\n0.712261\nVoVanPhuc/sup-SimCSE-VietNamese-phobert-base\nhybrid\n0.688483\n0.713829\n0.733894\n0.660156\n0.671198\n0.676961\nhiieu/halong_embedding\ndense\n0.656377\n0.675881\n0.701368\n0.630469\n0.641406\n0.652057\nVoVanPhuc/sup-SimCSE-VietNamese-phobert-base\ndense\n0.558852\n0.584799\n0.611329\n0.536979\n0.55112\n0.562218\nCitation\nYou can cite our work as below:\n@misc{DEk21_hcmute_embedding,\ntitle={DEk21_hcmute_embedding: A Vietnamese Text Embedding},\nauthor={QUANG HUY},\nyear={2025},\npublisher={Huggingface},\n}\nBibTeX\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nMatryoshkaLoss\n@misc{kusupati2024matryoshka,\ntitle={Matryoshka Representation Learning},\nauthor={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},\nyear={2024},\neprint={2205.13147},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nMultipleNegativesRankingLoss\n@misc{henderson2017efficient,\ntitle={Efficient Natural Language Response Suggestion for Smart Reply},\nauthor={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\nyear={2017},\neprint={1705.00652},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "m-a-p/YuE-s1-7B-anneal-en-cot": "News and Updates\nQuickstart\nLicense Agreement & Disclaimer\nAcknowledgements\nCitation\nDemo üé∂ ¬†|¬† üìë Paper\nYuE-s1-7B-anneal-en-cot ü§ó ¬†|¬† YuE-s1-7B-anneal-en-icl ü§ó ¬†|¬† YuE-s1-7B-anneal-jp-kr-cot ü§ó\nYuE-s1-7B-anneal-jp-kr-icl ü§ó ¬†|¬† YuE-s1-7B-anneal-zh-cot ü§ó ¬†|¬† YuE-s1-7B-anneal-zh-icl ü§ó\nYuE-s2-1B-general ü§ó ¬†|¬† YuE-upsampler ü§ó\nOur model's name is YuE (‰πê). In Chinese, the word means \"music\" and \"happiness.\" Some of you may find words that start with Yu hard to pronounce. If so, you can just call it \"yeah.\" We wrote a song with our model's name.\nYuE is a groundbreaking series of open-source foundation models designed for music generation, specifically for transforming lyrics into full songs (lyrics2song). It can generate a complete song, lasting several minutes, that includes both a catchy vocal track and accompaniment track. YuE is capable of modeling diverse genres/languages/vocal techniques. Please visit the Demo Page for amazing vocal performance.\nPop:Quiet Evening\nMetal: Step Back\nNews and Updates\nüìå Join Us on Discord!\n2025.03.12 üî• Paper Releasedüéâ: We now release YuE technical report!!! We discuss all the technical details, findings, and lessons learned. Enjoy, and feel free to cite us~\n2025.03.11 ü´∂ Now YuE supports incremental song generation!!! See YuE-UI by joeljuvel. YuE-UI is a Gradio-based interface supporting batch generation, output selection, and continuation. You can flexibly experiment with audio prompts and different model settings, visualize your progress on an interactive timeline, rewind actions, quickly preview audio outputs at stage 1 before committing to refinement, and fully save/load your sessions (JSON format). Optimized to run smoothly even on GPUs with just 8GB VRAM using quantized models.\n2025.02.17 ü´∂ Now YuE supports music continuation and Google Colab! See YuE-extend by Mozer.\n2025.02.07 üéâ Get YuE for Windows on pinokio.\n2025.01.30 üî• Inference Update: We now support dual-track ICL mode! You can prompt the model with a reference song, and it will generate a new song in a similar style (voice cloning demo by @abrakjamson, music style transfer demo by @cocktailpeanut, etc.). Try it out! üî•üî•üî• P.S. Be sure to check out the demos first‚Äîthey're truly impressive.\n2025.01.30 üî• Announcement: A New Era Under Apache 2.0 üî•: We are thrilled to announce that, in response to overwhelming requests from our community, YuE is now officially licensed under the Apache 2.0 license. We sincerely hope this marks a watershed moment‚Äîakin to what Stable Diffusion and LLaMA have achieved in their respective fields‚Äîfor music generation and creative AI. üéâüéâüéâ\n2025.01.29 üéâ: We have updated the license description. we ENCOURAGE artists and content creators to sample and incorporate outputs generated by our model into their own works, and even monetize them. The only requirement is to credit our name: YuE by HKUST/M-A-P (alphabetic order).\n2025.01.28 ü´∂: Thanks to Fahd for creating a tutorial on how to quickly get started with YuE. Here is his demonstration.\n2025.01.26 üî•: We have released the YuE series.\nQuickstart\nPlease check our github for easy quickstart.\nLicense Agreement & Disclaimer\nThe YuE model (including its weights) is now released under the Apache License, Version 2.0. We do not make any profit from this model, and we hope it can be used for the betterment of human creativity.\nUse & Attribution:\nWe encourage artists and content creators to freely incorporate outputs generated by YuE into their own works, including commercial projects.\nWe encourage attribution to the model‚Äôs name (‚ÄúYuE by HKUST/M-A-P‚Äù), especially for public and commercial use.\nOriginality & Plagiarism: It is the sole responsibility of creators to ensure that their works, derived from or inspired by YuE outputs, do not plagiarize or unlawfully reproduce existing material. We strongly urge users to perform their own due diligence to avoid copyright infringement or other legal violations.\nRecommended Labeling: When uploading works to streaming platforms or sharing them publicly, we recommend labeling them with terms such as: ‚ÄúAI-generated‚Äù, ‚ÄúYuE-generated\", ‚ÄúAI-assisted‚Äù or ‚ÄúAI-auxiliated‚Äù. This helps maintain transparency about the creative process.\nDisclaimer of Liability:\nWe do not assume any responsibility for the misuse of this model, including (but not limited to) illegal, malicious, or unethical activities.\nUsers are solely responsible for any content generated using the YuE model and for any consequences arising from its use.\nBy using this model, you agree that you understand and comply with all applicable laws and regulations regarding your generated content.\nAcknowledgements\nThe project is co-lead by HKUST and M-A-P (alphabetic order). Also thanks moonshot.ai, bytedance, 01.ai, and geely for supporting the project.\nA friendly link to HKUST Audio group's huggingface space.\nWe deeply appreciate all the support we received along the way. Long live open-source AI!\nCitation\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n@misc{yuan2025yuescalingopenfoundation,\ntitle={YuE: Scaling Open Foundation Models for Long-Form Music Generation},\nauthor={Ruibin Yuan and Hanfeng Lin and Shuyue Guo and Ge Zhang and Jiahao Pan and Yongyi Zang and Haohe Liu and Yiming Liang and Wenye Ma and Xingjian Du and Xinrun Du and Zhen Ye and Tianyu Zheng and Yinghao Ma and Minghao Liu and Zeyue Tian and Ziya Zhou and Liumeng Xue and Xingwei Qu and Yizhi Li and Shangda Wu and Tianhao Shen and Ziyang Ma and Jun Zhan and Chunhui Wang and Yatian Wang and Xiaowei Chi and Xinyue Zhang and Zhenzhu Yang and Xiangzhou Wang and Shansong Liu and Lingrui Mei and Peng Li and Junjie Wang and Jianwei Yu and Guojian Pang and Xu Li and Zihao Wang and Xiaohuan Zhou and Lijun Yu and Emmanouil Benetos and Yong Chen and Chenghua Lin and Xie Chen and Gus Xia and Zhaoxiang Zhang and Chao Zhang and Wenhu Chen and Xinyu Zhou and Xipeng Qiu and Roger Dannenberg and Jiaheng Liu and Jian Yang and Wenhao Huang and Wei Xue and Xu Tan and Yike Guo},\nyear={2025},\neprint={2503.08638},\narchivePrefix={arXiv},\nprimaryClass={eess.AS},\nurl={https://arxiv.org/abs/2503.08638},\n}\n@misc{yuan2025yue,\ntitle={YuE: Open Music Foundation Models for Full-Song Generation},\nauthor={Ruibin Yuan and Hanfeng Lin and Shawn Guo and Ge Zhang and Jiahao Pan and Yongyi Zang and Haohe Liu and Xingjian Du and Xeron Du and Zhen Ye and Tianyu Zheng and Yinghao Ma and Minghao Liu and Lijun Yu and Zeyue Tian and Ziya Zhou and Liumeng Xue and Xingwei Qu and Yizhi Li and Tianhao Shen and Ziyang Ma and Shangda Wu and Jun Zhan and Chunhui Wang and Yatian Wang and Xiaohuan Zhou and Xiaowei Chi and Xinyue Zhang and Zhenzhu Yang and Yiming Liang and Xiangzhou Wang and Shansong Liu and Lingrui Mei and Peng Li and Yong Chen and Chenghua Lin and Xie Chen and Gus Xia and Zhaoxiang Zhang and Chao Zhang and Wenhu Chen and Xinyu Zhou and Xipeng Qiu and Roger Dannenberg and Jiaheng Liu and Jian Yang and Stephen Huang and Wei Xue and Xu Tan and Yike Guo},\nhowpublished={\\url{https://github.com/multimodal-art-projection/YuE}},\nyear={2025},\nnote={GitHub repository}\n}",
    "MIL-UT/Asagi-4B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Details\nModel Description\nUsage\nRequirements\nHow to use\nExample\nMore Examples\nTraining Details\nTraining Data\nEvaluation\nRisks and Limitations\nModel Card Authors\nModel Details\nModel Description\nThis repository provides Asagi-4B, a large-scale Japanese Vision & Language Model (VLM).\nAsagi-4B has been trained on an extensive Japanese dataset, incorporating a diverse range of data sources.\nA significant portion of the training data is synthesized using models such as the Japanese large language model (CALM3-22B-Chat) and the English Vision & Language Model (Phi3.5-vision-instruct).\nImportantly, we do not use LLMs that restrict the usage of their outputs in the license terms (e.g., GPT-4) to synthesize the training data.\nModel components\nModel / Architecture\nParameters\nVision encoder\nsiglip-so400m-patch14-384\n428M\nProjector\n2-layer MLP\n64M\nLLM\nllm-jp-3-3.7b-instruct\n3.7B\nUsage\nRequirements\ntransformers==4.45.1\naccelerate==0.34.2\ntorch==2.4.0\ntorchvision==0.19.0\nHow to use\nimport requests\nimport torch\nimport transformers\nfrom PIL import Image\nfrom transformers import AutoModel, AutoProcessor, GenerationConfig\ntransformers.set_seed(42)\nmodel_path = \"MIL-UT/Asagi-4B\"\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(\nmodel_path, trust_remote_code=True,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\ngeneration_config = GenerationConfig(\ndo_sample=True,\nnum_beams=5,\nmax_new_tokens=256,\ntemperature=0.7,\nrepetition_penalty=1.5\n)\nprompt = (\"‰ª•‰∏ã„ÅØ„ÄÅ„Çø„Çπ„ÇØ„ÇíË™¨Êòé„Åô„ÇãÊåáÁ§∫„Åß„Åô„ÄÇË¶ÅÊ±Ç„ÇíÈÅ©Âàá„Å´Ê∫Ä„Åü„ÅôÂøúÁ≠î„ÇíÊõ∏„Åç„Å™„Åï„ÅÑ„ÄÇ\\n\\n\"\n\"### ÊåáÁ§∫:\\n<image>\\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆË≥™Âïè„Å´Ë©≥Á¥∞„Åã„Å§ÂÖ∑‰ΩìÁöÑ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆÂÜôÁúü„ÅØ„Å©„Åì„ÅßÊíÆÂΩ±„Åï„Çå„Åü„ÇÇ„ÅÆ„ÅãÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅÁîªÂÉè„ÅÆÂÜÖÂÆπ„Å´„Å§„ÅÑ„Å¶„ÇÇË©≥„Åó„ÅèË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\n\\n### ÂøúÁ≠î:\\n\")\n# sample image\nsample_image_url = \"https://raw.githubusercontent.com/uehara-mech/uehara-mech.github.io/refs/heads/master/images/shibuya.jpg\"\nimage = Image.open(requests.get(sample_image_url, stream=True).raw)\ninputs = processor(\ntext=prompt, images=image, return_tensors=\"pt\"\n)\ninputs_text = processor.tokenizer(prompt, return_tensors=\"pt\")\ninputs['input_ids'] = inputs_text['input_ids']\ninputs['attention_mask'] = inputs_text['attention_mask']\nfor k, v in inputs.items():\nif v.dtype == torch.float32:\ninputs[k] = v.to(model.dtype)\ninputs = {k: inputs[k].to(model.device) for k in inputs if k != \"token_type_ids\"}\ngenerate_ids = model.generate(\n**inputs,\ngeneration_config=generation_config\n)\ngenerated_text = processor.batch_decode(\ngenerate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\n# do not print the prompt\nif \"<image>\" in prompt:\nprompt = prompt.replace(\"<image>\", \" \")\ngenerated_text = generated_text.replace(prompt, \"\")\nprint(f\"Generated text: {generated_text}\")\n# >>> Generated text:  „Åì„ÅÆÂÜôÁúü„ÅØÊù±‰∫¨„ÅÆÊ∏ãË∞∑„ÅßÊíÆÂΩ±„Åï„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇÂ§ú„ÅÆÊ∏ãË∞∑„ÅÆË°ó‰∏¶„Åø„ÅåÂÜô„Å£„Å¶„Åä„Çä„ÄÅÈ´òÂ±§„Éì„É´„ÅåÁ´ã„Å°‰∏¶„Å≥„ÄÅË°óÁÅØ„ÇÑ„Éç„Ç™„É≥„Çµ„Ç§„É≥„ÅåËºù„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nExample\nPrompt\nOutput\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆË≥™Âïè„Å´Ë©≥Á¥∞„Åã„Å§ÂÖ∑‰ΩìÁöÑ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆÂÜôÁúü„ÅØ„Å©„Åì„ÅßÊíÆÂΩ±„Åï„Çå„Åü„ÇÇ„ÅÆ„ÅãÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅÁîªÂÉè„ÅÆÂÜÖÂÆπ„Å´„Å§„ÅÑ„Å¶„ÇÇË©≥„Åó„ÅèË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„Åì„ÅÆÂÜôÁúü„ÅØÊù±‰∫¨„ÅÆÊ∏ãË∞∑„ÅßÊíÆÂΩ±„Åï„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇÂ§ú„ÅÆÊ∏ãË∞∑„ÅÆË°ó‰∏¶„Åø„ÅåÂÜô„Å£„Å¶„Åä„Çä„ÄÅÈ´òÂ±§„Éì„É´„ÅåÁ´ã„Å°‰∏¶„Å≥„ÄÅË°óÁÅØ„ÇÑ„Éç„Ç™„É≥„Çµ„Ç§„É≥„ÅåËºù„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nMore Examples\nClick to expand\nPrompt\nOutput\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆÊåáÁ§∫„Å´Ë©≥Á¥∞„Åã„Å§ÂÖ∑‰ΩìÁöÑ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆÂÜôÁúü„ÅÆÂÜÖÂÆπ„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„Åì„ÅÆÂÜôÁúü„ÅØ„ÄÅÊù±‰∫¨„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Éñ„É™„ÉÉ„Ç∏„ÇíÊíÆÂΩ±„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇÊ©ã„ÅØÁôΩ„ÅèÂ°ó„Çâ„Çå„Å¶„Åä„Çä„ÄÅÈùô„Åã„Å™Ê∞¥„ÅÆ‰∏ä„Å´ÂÑ™ÈõÖ„Å´„Ç¢„Éº„ÉÅ„ÇíÊèè„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇËÉåÊôØ„Å´„ÅØÈ´òÂ±§„Éì„É´„ÅåÁ´ã„Å°‰∏¶„Å∂ÈÉΩÂ∏ÇÊôØË¶≥„ÅåÂ∫É„Åå„Å£„Å¶„Åä„Çä„ÄÅÁ©∫„ÅØÊæÑ„ÅøÂàá„Å£„ÅüÈùíËâ≤„Åß„ÄÅÁôΩ„ÅÑÈõ≤„ÅåÁÇπÂú®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nPrompt\nOutput\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆË≥™Âïè„Å´Ë©≥Á¥∞„Åã„Å§ÂÖ∑‰ΩìÁöÑ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆÂÜôÁúü„Å´„Å§„ÅÑ„Å¶Ë©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„Åì„ÅÆÂÜôÁúü„ÅØ„ÄÅÊù±‰∫¨„Çø„ÉØ„Éº„ÇíËÉåÊôØ„Å´„Åó„ÅüÁßã„ÅÆÈ¢®ÊôØ„ÇíÊíÆÂΩ±„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇÊù±‰∫¨„Çø„ÉØ„Éº„ÅØËµ§„Å®ÁôΩ„ÅÆ„Çπ„Éà„É©„Ç§„ÉóÊ®°Êßò„ÅåÁâπÂæ¥ÁöÑ„Åß„ÄÅÂë®Âõ≤„Å´„ÅØÁ¥ÖËëâ„Åó„ÅüÊú®„ÄÖ„ÅåÂ∫É„Åå„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ©∫„ÅØÈÉ®ÂàÜÁöÑ„Å´Êõá„Å£„Å¶„Åä„Çä„ÄÅÂ§™ÈôΩ„ÅÆÂÖâ„ÅåÈõ≤„ÅÆÈñì„Åã„ÇâÂ∑Æ„ÅóËæº„Çì„ÅßÁæé„Åó„ÅÑ„Ç≥„É≥„Éà„É©„Çπ„Éà„Çí‰Ωú„ÇäÂá∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂâçÊôØ„Å´„ÅØË°óÁÅØ„ÇÑÂª∫Áâ©„ÅÆ„Ç∑„É´„Ç®„ÉÉ„Éà„ÅåË¶ã„Åà„ÄÅÈÉΩ‰ºöÁöÑ„Å™Èõ∞Âõ≤Ê∞ó„ÇÇÊÑü„Åò„Çâ„Çå„Åæ„Åô„ÄÇ\nPrompt\nOutput\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆË≥™Âïè„Å´Ë©≥Á¥∞„Åã„Å§ÂÖ∑‰ΩìÁöÑ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆÂÜôÁúü„ÅÆÁ•ûÁ§æ„Å´„Å§„ÅÑ„Å¶„ÄÅÁ¥∞„Åã„ÅÑ„Å®„Åì„Çç„Åæ„ÅßË©≥„Åó„ÅèË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„Åì„ÅÆÂÜôÁúü„Å´ÂÜô„Å£„Å¶„ÅÑ„ÇãÁ•ûÁ§æ„ÅØ„ÄÅÊó•Êú¨„ÅÆ‰ºùÁµ±ÁöÑ„Å™Âª∫ÁØâÊßòÂºè„ÇíÂèçÊò†„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁôΩ„ÅÑÂ£Å„Å®ÈÆÆ„ÇÑ„Åã„Å™„Ç™„É¨„É≥„Ç∏Ëâ≤„ÅÆÂ±ãÊ†π„ÅåÁâπÂæ¥ÁöÑ„Åß„ÄÅÁ•ûËÅñ„Å™Èõ∞Âõ≤Ê∞ó„ÇíÈÜ∏„ÅóÂá∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ•ûÁ§æ„ÅÆÂÖ•„ÇäÂè£„Å´„ÅØÊú±Ëâ≤„ÅÆÈ≥•Â±Ö„ÅåÁ´ã„Å£„Å¶„Åä„Çä„ÄÅ„Åì„Çå„ÅØÁ•ûÈÅì„Å´„Åä„Åë„ÇãÁ•ûËÅñ„Å™Á©∫Èñì„Å∏„ÅÆÂÖ•„ÇäÂè£„ÇíÁ§∫„ÅôË±°Âæ¥ÁöÑ„Å™ÈñÄ„Åß„Åô„ÄÇÈ≥•Â±Ö„ÅÆÂâç„Å´„ÅØÂ∞è„Åï„Å™Á§æÊÆø„Åå„ÅÇ„Çä„ÄÅÂèÇÊãùËÄÖ„ÅåÁ•à„Çä„ÇíÊçß„Åí„ÇãÂ†¥ÊâÄ„Å®„Åó„Å¶Ê©üËÉΩ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ§æÊÆø„ÅÆÊ®™„Å´„ÅØÁ∏Ñ„ÅåÂºµ„Çâ„Çå„Å¶„Åä„Çä„ÄÅ„Åì„Çå„ÅØÁ•ûËÅñ„Å™Â†¥ÊâÄ„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ§∫ÂîÜ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ•ûÁ§æ„ÅØÁèæ‰ª£ÁöÑ„Å™Âª∫Áâ©„ÅÆÈñì„Å´‰ΩçÁΩÆ„Åó„Å¶„Åä„Çä„ÄÅ‰ºùÁµ±„Å®Áèæ‰ª£„ÅåÂÖ±Â≠ò„Åô„ÇãÊó•Êú¨„ÅÆÊñáÂåñ„ÇíË±°Âæ¥„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nPrompt\nOutput\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆÊåáÁ§∫„Å´Ë©≥Á¥∞„Åã„Å§ÂÖ∑‰ΩìÁöÑ„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„Çå„ÅØÊó•Êú¨Â∫≠Âúí„ÅÆ‰∏≠„Åß„ÇÇ„ÄÅ„Å©„ÅÆ„Çà„ÅÜ„Å™ÂΩ¢Âºè„Å´ÂàÜÈ°û„Åï„Çå„ÇãÂ∫≠Âúí„Åß„Åô„ÅãÔºü„Åæ„Åü„ÄÅ„Åù„ÅÆÁêÜÁî±„ÅØ‰Ωï„Åß„Åô„ÅãÔºü\n„Åì„Çå„ÅØÊó•Êú¨Â∫≠Âúí„ÅÆ‰∏≠„Åß„ÇÇ„ÄåÁü≥Â∫≠„Äç„Å®Âëº„Å∞„Çå„ÇãÂΩ¢Âºè„ÅÆÂ∫≠Âúí„Åß„Åô„ÄÇÁü≥Â∫≠„ÅØ„ÄÅËá™ÁÑ∂„ÅÆÁü≥„ÇíÂ∑ß„Åø„Å´ÈÖçÁΩÆ„Åó„ÄÅÈùô„Åë„Åï„Å®Áæé„Åó„Åï„ÇíËøΩÊ±Ç„Åô„ÇãÂΩ¢Âºè„Åß„ÄÅÊó•Êú¨„ÅÆ‰ºùÁµ±ÁöÑ„Å™Â∫≠ÂúíË®≠Ë®à„ÅÆ‰∏Ä„Å§„Åß„Åô„ÄÇ\nPrompt\nOutput\n„Åì„ÅÆÁîªÂÉè„ÇíË¶ã„Å¶„ÄÅÊ¨°„ÅÆË≥™Âïè„Å´Ë©≥Á¥∞„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆÁîªÂÉè„ÅÆÂ†¥ÊâÄ„ÇíËàûÂè∞„Å®„Åó„ÅüÂ∞èË™¨„ÅÆ„ÅÇ„Çâ„Åô„Åò„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\nÊòî„ÄÖ„ÄÅ„ÅÇ„ÇãÂ∞è„Åï„Å™Êùë„Å´„ÄÅÁü≥ÈÄ†„Çä„ÅÆÈöéÊÆµ„ÅåÊùë„ÅÆ‰∏≠ÂøÉ„Å∏„Å®Á∂ö„ÅÑ„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åù„ÅÆÈöéÊÆµ„ÅØ„ÄÅÊùë‰∫∫„Åü„Å°„Å´„Å®„Å£„Å¶Á•ûËÅñ„Å™Â†¥ÊâÄ„Å∏„ÅÆÂÖ•„ÇäÂè£„Åß„ÅÇ„Çä„ÄÅÊùë‰∫∫„Åü„Å°„ÅØ„Åù„ÅÆÈöéÊÆµ„ÇíÁôª„Çä„ÄÅÁ•ûËÅñ„Å™Â†¥ÊâÄ„ÅßÁ•à„Çä„ÇíÊçß„Åí„Åæ„Åó„Åü„ÄÇÈöéÊÆµ„ÅÆÂë®„Çä„Å´„ÅØ„ÄÅÁ∑ëË±ä„Åã„Å™Ê§çÁâ©„ÅåÁîü„ÅÑËåÇ„Çä„ÄÅËá™ÁÑ∂„Å®„ÅÆË™øÂíå„ÇíÊÑü„Åò„Åï„Åõ„ÇãÈ¢®ÊôØ„ÅåÂ∫É„Åå„Å£„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇÊùë‰∫∫„Åü„Å°„ÅØ„ÄÅÈöéÊÆµ„ÇíÁôª„Çã„Åü„Å≥„Å´„ÄÅËá™ÁÑ∂„ÅÆÊÅµ„Åø„Å®Á•ûËÅñ„Å™Â†¥ÊâÄ„Å∏„ÅÆÊï¨ÊÑè„ÇíË°®„Åó„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ\nTraining Details\nTraining Data\nDataset\nPreprocess\nStage1\nStage2\nSize\nROIS (Ours)\nSynthesized\n‚úì\n‚úì\n8.4M\nJapanese image text pairs\nSynthesized\n‚úì\n‚úì\n4.4M\nWikipedia\nSynthesized\n‚úì\n‚úì\n2.5M\nOpen Images\nTranslated\n‚úì\n‚úì\n680K\nDCI\nTranslated\n‚úì\n‚úì\n7K\nCommonCatalog CC-BY\nTranslated\n‚úì\n‚úì\n3.5M\nLLaVA-Pretrain-JA\n‚úì\n‚úì\n550K\nSTAIR Captions\n‚úì\n‚úì\n410K\nFlickr-JP\n‚úì\n‚úì\n160K\nYJ Captions\n‚úì\n‚úì\n130K\nJapanese Pascal\n‚úì\n‚úì\n5K\nArtBench\nSynthesized\n‚úì\n100K\nGQA\nTranslated\n‚úì\n1.9M\nVQA v2\nTranslated\n‚úì\n880K\nA-OKVQA\nTranslated\n‚úì\n34K\nOK-VQA\nTranslated\n‚úì\n18K\nJapanese Visual Genome\nTranslated\n‚úì\n1.6M\nPangeaInstruct\n‚úì\n93K\nNote: ROIS (Ours) is a newly collected dataset crawled from the web specifically for this project.\nThe dataset consists of image and raw text pairs, which are used to synthesize the training data.\nEvaluation\nWe evaluated our model using Heron-Bench, JA-VLM-Bench-in-the-Wild, and JA-VG-VQA-500.\nWe used eval-mm library for this evaluation.\nHere, models with \"‚Ä†\" are not trained with GPT-generated data.\nBold numbers indicate the best performance among all models, and underlined numbers indicate the best performance among models not trained with GPT-generated data.\nModel\nLM Size\nHeron-Bench (LLM (%))\nJA-VLM-Bench-In-the-Wild (ROUGE-L)\nJA-VLM-Bench-In-the-Wild (LLM (/5.0))\nJA-VG-VQA-500 (ROUGE-L)\nJA-VG-VQA-500 (LLM (/5.0))\nJapanese InstructBLIP Alpha‚Ä†\n7B\n14.0\n20.8\n2.42\n-\n-\nJapanese Stable VLM‚Ä†\n7B\n24.2\n23.3\n2.47\n-\n-\nLLaVA-CALM2-SigLIP‚Ä†\n7B\n43.3\n47.2\n3.15\n17.4\n3.21\nLlama-3-EvoVLM-JP-v2\n8B\n39.3\n41.4\n2.92\n23.5\n2.96\nVILA-jp\n13B\n57.2\n52.3\n3.69\n16.2\n3.62\nAsagi-2B‚Ä†\n1.8B\n44.7\n48.8\n3.26\n53.7\n3.69\nAsagi-4B‚Ä†\n3.7B\n49.3\n49.6\n3.38\n55.6\n3.78\nAsagi-8B‚Ä†\n7.2B\n54.7\n49.4\n3.45\n56.43\n3.84\nAsagi-14B‚Ä†\n13B\n55.8\n50.8\n3.44\n56.8\n3.84\nGPT-4o\n-\n87.6\n37.6\n3.85\n12.1\n3.58\nRisks and Limitations\nThe models released here are in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations.\nModel Card Authors\nKohei Uehara",
    "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese": "DeepSeek-R1-Distill-Qwen-14B-Japanese\nModel Description\nUsage\nPrompt Format\nLicense\nAuthor\nHow to cite\nCitation\nDeepSeek-R1-Distill-Qwen-14B-Japanese\nModel Description\nThis is a Japanese finetuned model based on deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.\nUsage\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\nmodel = AutoModelForCausalLM.from_pretrained(\"cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese\", device_map=\"auto\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese\")\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\nmessages = [\n{\"role\": \"user\", \"content\": \"AI„Å´„Çà„Å£„Å¶ÁßÅ„Åü„Å°„ÅÆÊöÆ„Çâ„Åó„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å´Â§â„Çè„Çä„Åæ„Åô„ÅãÔºü\"}\n]\ninput_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\noutput_ids = model.generate(input_ids,\nmax_new_tokens=4096,\ntemperature=0.7,\nstreamer=streamer)\nPrompt Format\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>AI„Å´„Çà„Å£„Å¶ÁßÅ„Åü„Å°„ÅÆÊöÆ„Çâ„Åó„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å´Â§â„Çè„Çä„Åæ„Åô„ÅãÔºü<ÔΩúAssistantÔΩú>\nLicense\nMIT License\nAuthor\nRyosuke Ishigami\nHow to cite\n@misc{cyberagent-deepseek-r1-distill-qwen-14b-japanese,\ntitle={DeepSeek-R1-Distill-Qwen-14B-Japanese},\nurl={https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese},\nauthor={Ryosuke Ishigami},\nyear={2025},\n}\nCitation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}",
    "cirimus/modernbert-large-bias-type-classifier": "Overview\nModel Details\nExample Usage\nHow the Model Was Created\nDataset\nEvaluation Results\nMacro Averages:\nPer-Label Results:\nIntended Use\nLimitations and Biases\nEnvironmental Impact\nCitation\nOverview\nThis model was fine-tuned from ModernBERT-large on a synthetic dataset of biased statements and questions, generated by Mistal 7B as part of the GUS-Net paper. The model is designed to identify and classify text bias into multiple categories, including racial, religious, gender, age, and other biases, making it a valuable tool for bias detection and mitigation in natural language processing tasks.\nModel Details\nBase Model: ModernBERT-large\nFine-Tuning Dataset: Synthetic biased corpus\nNumber of Labels: 11\nProblem Type: Multi-label classification\nLanguage: English\nLicense: MIT\nFine-Tuning Framework: Hugging Face Transformers\nExample Usage\nHere‚Äôs how to use the model with Hugging Face Transformers:\nfrom transformers import pipeline\n# Load the model\nclassifier = pipeline(\n\"text-classification\",\nmodel=\"cirimus/modernbert-large-bias-type-classifier\",\nreturn_all_scores=True\n)\ntext = \"Tall people are so clumsy.\"\npredictions = classifier(text)\n# Print predictions\nfor pred in sorted(predictions[0], key=lambda x: x['score'], reverse=True)[:5]:\nprint(f\"{pred['label']}: {pred['score']:.3f}\")\n# Output:\n# physical: 1.000\n# socioeconomic: 0.002\n# gender: 0.002\n# racial: 0.001\n# age: 0.001\nHow the Model Was Created\nThe model was fine-tuned for bias detection using the following hyperparameters:\nLearning Rate: 3e-5\nBatch Size: 16\nWeight Decay: 0.01\nWarmup Steps: 500\nOptimizer: AdamW\nEvaluation Metrics: Precision, Recall, F1 Score (weighted), Accuracy\nDataset\nThe synthetic dataset consists of biased statements and questions generated by Mistal 7B as part of the GUS-Net paper. It covers 11 bias categories:\nRacial\nReligious\nGender\nAge\nNationality\nSexuality\nSocioeconomic\nEducational\nDisability\nPolitical\nPhysical\nEvaluation Results\nThe model was evaluated on the synthetic dataset‚Äôs test split. The overall metrics using a threshold of 0.5 are as follows:\nMacro Averages:\nMetric\nValue\nAccuracy\n0.983\nPrecision\n0.930\nRecall\n0.914\nF1\n0.921\nMCC\n0.912\nPer-Label Results:\nLabel\nAccuracy\nPrecision\nRecall\nF1\nMCC\nSupport\nThreshold\nRacial\n0.975\n0.871\n0.889\n0.880\n0.866\n388\n0.5\nReligious\n0.994\n0.962\n0.970\n0.966\n0.962\n335\n0.5\nGender\n0.976\n0.930\n0.925\n0.927\n0.913\n615\n0.5\nAge\n0.990\n0.964\n0.931\n0.947\n0.941\n375\n0.5\nNationality\n0.972\n0.924\n0.881\n0.902\n0.886\n554\n0.5\nSexuality\n0.993\n0.960\n0.957\n0.958\n0.955\n301\n0.5\nSocioeconomic\n0.964\n0.909\n0.818\n0.861\n0.842\n516\n0.5\nEducational\n0.982\n0.873\n0.933\n0.902\n0.893\n330\n0.5\nDisability\n0.986\n0.923\n0.887\n0.905\n0.897\n283\n0.5\nPolitical\n0.988\n0.958\n0.938\n0.948\n0.941\n438\n0.5\nPhysical\n0.993\n0.961\n0.920\n0.940\n0.936\n238\n0.5\nIntended Use\nThe model is designed to detect and classify bias in text across 11 categories. It can be used in applications such as:\nContent moderation\nBias analysis in research\nEthical AI development\nLimitations and Biases\nSynthetic Nature: The dataset consists of synthetic text, which may not fully represent real-world biases.\nCategory Overlap: Certain biases may overlap, leading to challenges in precise classification.\nDomain-Specific Generalization: The model may not generalize well to domains outside the synthetic dataset‚Äôs scope.\nEnvironmental Impact\nHardware Used: NVIDIA RTX4090\nTraining Time: ~2 hours\nCarbon Emissions: ~0.08 kg CO2 (calculated via ML CO2 Impact Calculator).\nCitation\nIf you use this model, please cite it as follows:\n@inproceedings{JunquedeFortuny2025c,\ntitle = {Bias Detection with ModernBERT-Large},\nauthor = {Enric Junqu√© de Fortuny},\nyear = {2025},\nhowpublished = {\\url{https://huggingface.co/cirimus/modernbert-large-bias-type-classifier}},\n}",
    "onnx-community/Janus-Pro-1B-ONNX": "Usage (Transformers.js)\nhttps://huggingface.co/deepseek-ai/Janus-Pro-1B with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Image+text to text\nimport { AutoProcessor, MultiModalityCausalLM } from \"@huggingface/transformers\";\n// Load processor and model\nconst model_id = \"onnx-community/Janus-Pro-1B-ONNX\";\nconst processor = await AutoProcessor.from_pretrained(model_id);\nconst model = await MultiModalityCausalLM.from_pretrained(model_id);\n// Prepare inputs\nconst conversation = [\n{\nrole: \"<|User|>\",\ncontent: \"<image_placeholder>\\nConvert the formula into latex code.\",\nimages: [\"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/quadratic_formula.png\"],\n},\n];\nconst inputs = await processor(conversation);\n// Generate response\nconst outputs = await model.generate({\n...inputs,\nmax_new_tokens: 150,\ndo_sample: false,\n});\n// Decode output\nconst new_tokens = outputs.slice(null, [inputs.input_ids.dims.at(-1), null]);\nconst decoded = processor.batch_decode(new_tokens, { skip_special_tokens: true });\nconsole.log(decoded[0]);\nExample: Text to image\nimport { AutoProcessor, MultiModalityCausalLM } from \"@huggingface/transformers\";\n// Load processor and model\nconst model_id = \"onnx-community/Janus-Pro-1B-ONNX\";\nconst processor = await AutoProcessor.from_pretrained(model_id);\nconst model = await MultiModalityCausalLM.from_pretrained(model_id);\n// Prepare inputs\nconst conversation = [\n{\nrole: \"<|User|>\",\ncontent: \"A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair\",\n},\n];\nconst inputs = await processor(conversation, { chat_template: \"text_to_image\" });\n// Generate response\nconst num_image_tokens = processor.num_image_tokens;\nconst outputs = await model.generate_images({\n...inputs,\nmin_new_tokens: num_image_tokens,\nmax_new_tokens: num_image_tokens,\ndo_sample: true,\n});\n// Save the generated image\nawait outputs[0].save(\"test.png\");",
    "allenai/OLMoE-1B-7B-0125-Instruct": "OLMoE-1B-7B-0125-Instruct\nRelease Documentation\nModel description\nModel Sources\nInstallation\nUsing the model\nLoading with HuggingFace\nChat template\nSystem prompt\nBias, Risks, and Limitations\nPerformance\nLicense and use\nCitation\nOLMoE-1B-7B-0125-Instruct\nRelease Documentation\nOLMoE-1B-7B-0125-Instruct January 2025 is post-trained variant of the OLMoE-1B-7B January 2025 model, which has undergone supervised finetuning on an OLMo-specific variant of the T√ºlu 3 dataset and further DPO training on this dataset, and finally RLVR training using this data.\nT√ºlu 3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.\nCheck out the OLMoE paper or T√ºlu 3 paper for more details!\nOLMo is a series of Open Language Models designed to enable the science of language models.\nThese models are trained on the Dolma dataset. We are releasing all code, checkpoints, logs (coming soon), and associated training details.\nThe core models released in this batch include the following:\nStage\nOLMoE 1B-7B\nBase Model\nallenai/OLMoE-1B-7B-0125\nSFT\nallenai/OLMoE-1B-7B-0125-SFT\nDPO\nallenai/OLMoE-1B-7B-0125-DPO\nFinal Models (RLVR)\nallenai/OLMoE-1B-7B-0125-Instruct\nReward Model (RM)\nallenai/OLMoE-1B-7B-0125-RM\nModel description\nModel type: A model trained on a mix of publicly available, synthetic and human-created datasets.\nLanguage(s) (NLP): Primarily English\nLicense: Apache 2.0\nFinetuned from model: allenai/OLMoE-1B-7B-0125-DPO\nModel Sources\nProject Page: https://allenai.org/olmo\nRepositories:\nCore repo (training, inference, fine-tuning etc.): https://github.com/allenai/OLMo\nEvaluation code: https://github.com/allenai/olmes\nFurther fine-tuning code: https://github.com/allenai/open-instruct\nPaper: https://arxiv.org/abs/2409.02060\nDemo: https://playground.allenai.org/\nInstallation\nOLMo 2 will be supported in the next version of Transformers, and you need to install it from the main branch using:\npip install --upgrade git+https://github.com/huggingface/transformers.git\nUsing the model\nLoading with HuggingFace\nTo load the model with HuggingFace, use the following snippet:\nfrom transformers import AutoModelForCausalLM\nolmo_model = AutoModelForCausalLM.from_pretrained(\"OLMoE-1B-7B-0125-Instruct\")\nChat template\nThe chat template for our models is formatted as:\n<|endoftext|><|user|>\\nHow are you doing?\\n<|assistant|>\\nI'm just a computer program, so I don't have feelings, but I'm functioning as expected. How can I assist you today?<|endoftext|>\nOr with new lines expanded:\n<|endoftext|><|user|>\nHow are you doing?\n<|assistant|>\nI'm just a computer program, so I don't have feelings, but I'm functioning as expected. How can I assist you today?<|endoftext|>\nIt is embedded within the tokenizer as well, for tokenizer.apply_chat_template.\nSystem prompt\nIn Ai2 demos, we use this system prompt by default:\nYou are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI.\nThe model has not been trained with a specific system prompt in mind.\nBias, Risks, and Limitations\nThe OLMo-2 models have limited safety training, but are not deployed automatically with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nSee the Falcon 180B model card for an example of this.\nPerformance\nBenchmark (eval)\nOLMoE-1B-7B-0125-Instruct\nOLMoE-1B-7B-0924-Instruct\nOLMoE-1B-7B-0125-DPO\nOLMoE-1B-7B-0125-SFT\nOLMoE-1B-7B-0924-SFT\nAvg.\n45.62\n38.44\n45.05\n41.76\n37.05\nMMLU (CoT)\n55.08\n54.57\n54.93\n55.26\n54.32\nPopQA\n19.75\n20.56\n19.65\n20.12\n21.01\nTruthfulQA\n50.56\n49.14\n49.99\n45.48\n44.66\nBigBenchHard (CoT)\n38.61\n36.78\n37.37\n37.31\n36.55\nDROP\n47.87\n34.48\n48.38\n48.57\n34.71\nMATH (Flex)\n21.41\n8.16\n20.36\n21.38\n8.15\nGSM8K\n72.40\n47.38\n64.59\n55.72\n42.46\nHumanEval\n62.30\n63.04\n61.92\n62.58\n63.72\nHumanEval+\n54.37\n58.93\n57.61\n55.67\n57.40\nIFEval\n66.36\n45.29\n65.62\n56.56\n41.22\nAlpacaEval\n17.99\n7.54\n19.50\n5.83\n6.38\nSafety (average)\n90.40\n51.40\n91.40\n94.50\n65.80\nLicense and use\nOLMoE is licensed under the Apache 2.0 license.\nOLMoE is intended for research and educational use.\nFor more information, please see our Responsible Use Guidelines.\nThis model has been fine-tuned using a dataset mix with outputs generated from third party models and are subject to additional terms: Gemma Terms of Use.\nCitation\n@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,\ntitle={OLMoE: Open Mixture-of-Experts Language Models},\nauthor={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},\nyear={2024},\neprint={2409.02060},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2409.02060},\n}\n@article{lambert2024tulu3,\ntitle = {T√ºlu 3: Pushing Frontiers in Open Language Model Post-Training},\nauthor = {\nNathan Lambert and\nJacob Morrison and\nValentina Pyatkin and\nShengyi Huang and\nHamish Ivison and\nFaeze Brahman and\nLester James V. Miranda and\nAlisa Liu and\nNouha Dziri and\nShane Lyu and\nYuling Gu and\nSaumya Malik and\nVictoria Graf and\nJena D. Hwang and\nJiangjiang Yang and\nRonan Le Bras and\nOyvind Tafjord and\nChris Wilhelm and\nLuca Soldaini and\nNoah A. Smith and\nYizhong Wang and\nPradeep Dasigi and\nHannaneh Hajishirzi\n},\nyear = {2024},\nemail = {tulu@allenai.org}\n}",
    "lodestones/Chroma": ""
}