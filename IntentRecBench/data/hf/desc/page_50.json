{
    "facebook/fastspeech2-en-ljspeech": "fastspeech2-en-ljspeech\nUsage\nCitation\nfastspeech2-en-ljspeech\nFastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):\nEnglish\nSingle-speaker female voice\nTrained on LJSpeech\nUsage\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\nimport IPython.display as ipd\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n\"facebook/fastspeech2-en-ljspeech\",\narg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False}\n)\nmodel = models[0]\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\ngenerator = task.build_generator(model, cfg)\ntext = \"Hello, this is a test run.\"\nsample = TTSHubInterface.get_model_input(task, text)\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\nipd.Audio(wav, rate=rate)\nSee also fairseq S^2 example.\nCitation\n@inproceedings{wang-etal-2021-fairseq,\ntitle = \"fairseq S{\\^{}}2: A Scalable and Integrable Speech Synthesis Toolkit\",\nauthor = \"Wang, Changhan  and\nHsu, Wei-Ning  and\nAdi, Yossi  and\nPolyak, Adam  and\nLee, Ann  and\nChen, Peng-Jen  and\nGu, Jiatao  and\nPino, Juan\",\nbooktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\nmonth = nov,\nyear = \"2021\",\naddress = \"Online and Punta Cana, Dominican Republic\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2021.emnlp-demo.17\",\ndoi = \"10.18653/v1/2021.emnlp-demo.17\",\npages = \"143--152\",\n}",
    "facebook/m2m100_1.2B": "M2M100 1.2B\nLanguages covered\nBibTeX entry and citation info\nM2M100 1.2B\nM2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation.\nIt was introduced in this paper and first released in this repository.\nThe model that can directly translate between the 9,900 directions of 100 languages.\nTo translate into a target language, the target language id is forced as the first generated token.\nTo force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.\nNote: M2M100Tokenizer depends on sentencepiece, so make sure to install it before running the example.\nTo install sentencepiece run pip install sentencepiece\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\nhi_text = \"‡§ú‡•Ä‡§µ‡§® ‡§è‡§ï ‡§ö‡•â‡§ï‡§≤‡•á‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§π‡•à‡•§\"\nchinese_text = \"ÁîüÊ¥ªÂ∞±ÂÉè‰∏ÄÁõíÂ∑ßÂÖãÂäõ„ÄÇ\"\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n# translate Hindi to French\ntokenizer.src_lang = \"hi\"\nencoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"La vie est comme une bo√Æte de chocolat.\"\n# translate Chinese to English\ntokenizer.src_lang = \"zh\"\nencoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Life is like a box of chocolate.\"\nSee the model hub to look for more fine-tuned versions.\nLanguages covered\nAfrikaans (af), Amharic (am), Arabic (ar),  Asturian (ast), Azerbaijani (az), Bashkir (ba), Belarusian (be), Bulgarian (bg), Bengali (bn), Breton (br), Bosnian (bs), Catalan; Valencian (ca), Cebuano (ceb), Czech (cs), Welsh (cy), Danish (da), German (de), Greeek (el), English (en), Spanish (es), Estonian (et), Persian (fa), Fulah (ff), Finnish (fi), French (fr), Western Frisian (fy), Irish (ga), Gaelic; Scottish Gaelic (gd), Galician (gl), Gujarati (gu), Hausa (ha), Hebrew (he), Hindi (hi), Croatian (hr), Haitian; Haitian Creole (ht), Hungarian (hu), Armenian (hy), Indonesian (id), Igbo (ig),  Iloko (ilo), Icelandic (is), Italian (it), Japanese (ja), Javanese (jv), Georgian (ka), Kazakh (kk), Central Khmer (km), Kannada (kn), Korean (ko), Luxembourgish; Letzeburgesch (lb), Ganda (lg), Lingala (ln), Lao (lo), Lithuanian (lt), Latvian (lv), Malagasy (mg), Macedonian (mk), Malayalam (ml), Mongolian (mn), Marathi (mr), Malay (ms), Burmese (my), Nepali (ne), Dutch; Flemish (nl), Norwegian (no),  Northern Sotho (ns), Occitan (post 1500) (oc), Oriya (or), Panjabi; Punjabi (pa), Polish (pl), Pushto; Pashto (ps), Portuguese (pt), Romanian; Moldavian; Moldovan (ro), Russian (ru), Sindhi (sd), Sinhala; Sinhalese (si), Slovak (sk), Slovenian (sl), Somali (so), Albanian (sq), Serbian (sr), Swati (ss), Sundanese (su), Swedish (sv), Swahili (sw), Tamil (ta), Thai (th), Tagalog (tl), Tswana (tn), Turkish (tr), Ukrainian (uk), Urdu (ur), Uzbek (uz), Vietnamese (vi), Wolof (wo), Xhosa (xh), Yiddish (yi), Yoruba (yo), Chinese (zh), Zulu (zu)\nBibTeX entry and citation info\n@misc{fan2020englishcentric,\ntitle={Beyond English-Centric Multilingual Machine Translation},\nauthor={Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},\nyear={2020},\neprint={2010.11125},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "facebook/m2m100_418M": "M2M100 418M\nLanguages covered\nBibTeX entry and citation info\nM2M100 418M\nM2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation.\nIt was introduced in this paper and first released in this repository.\nThe model that can directly translate between the 9,900 directions of 100 languages.\nTo translate into a target language, the target language id is forced as the first generated token.\nTo force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.\nNote: M2M100Tokenizer depends on sentencepiece, so make sure to install it before running the example.\nTo install sentencepiece run pip install sentencepiece\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\nhi_text = \"‡§ú‡•Ä‡§µ‡§® ‡§è‡§ï ‡§ö‡•â‡§ï‡§≤‡•á‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§π‡•à‡•§\"\nchinese_text = \"ÁîüÊ¥ªÂ∞±ÂÉè‰∏ÄÁõíÂ∑ßÂÖãÂäõ„ÄÇ\"\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n# translate Hindi to French\ntokenizer.src_lang = \"hi\"\nencoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"La vie est comme une bo√Æte de chocolat.\"\n# translate Chinese to English\ntokenizer.src_lang = \"zh\"\nencoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Life is like a box of chocolate.\"\nSee the model hub to look for more fine-tuned versions.\nLanguages covered\nAfrikaans (af), Amharic (am), Arabic (ar),  Asturian (ast), Azerbaijani (az), Bashkir (ba), Belarusian (be), Bulgarian (bg), Bengali (bn), Breton (br), Bosnian (bs), Catalan; Valencian (ca), Cebuano (ceb), Czech (cs), Welsh (cy), Danish (da), German (de), Greeek (el), English (en), Spanish (es), Estonian (et), Persian (fa), Fulah (ff), Finnish (fi), French (fr), Western Frisian (fy), Irish (ga), Gaelic; Scottish Gaelic (gd), Galician (gl), Gujarati (gu), Hausa (ha), Hebrew (he), Hindi (hi), Croatian (hr), Haitian; Haitian Creole (ht), Hungarian (hu), Armenian (hy), Indonesian (id), Igbo (ig),  Iloko (ilo), Icelandic (is), Italian (it), Japanese (ja), Javanese (jv), Georgian (ka), Kazakh (kk), Central Khmer (km), Kannada (kn), Korean (ko), Luxembourgish; Letzeburgesch (lb), Ganda (lg), Lingala (ln), Lao (lo), Lithuanian (lt), Latvian (lv), Malagasy (mg), Macedonian (mk), Malayalam (ml), Mongolian (mn), Marathi (mr), Malay (ms), Burmese (my), Nepali (ne), Dutch; Flemish (nl), Norwegian (no),  Northern Sotho (ns), Occitan (post 1500) (oc), Oriya (or), Panjabi; Punjabi (pa), Polish (pl), Pushto; Pashto (ps), Portuguese (pt), Romanian; Moldavian; Moldovan (ro), Russian (ru), Sindhi (sd), Sinhala; Sinhalese (si), Slovak (sk), Slovenian (sl), Somali (so), Albanian (sq), Serbian (sr), Swati (ss), Sundanese (su), Swedish (sv), Swahili (sw), Tamil (ta), Thai (th), Tagalog (tl), Tswana (tn), Turkish (tr), Ukrainian (uk), Urdu (ur), Uzbek (uz), Vietnamese (vi), Wolof (wo), Xhosa (xh), Yiddish (yi), Yoruba (yo), Chinese (zh), Zulu (zu)\nBibTeX entry and citation info\n@misc{fan2020englishcentric,\ntitle={Beyond English-Centric Multilingual Machine Translation},\nauthor={Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},\nyear={2020},\neprint={2010.11125},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "facebook/mbart-large-en-ro": "",
    "facebook/rag-token-nq": "",
    "facebook/vit-mae-base": "",
    "facebook/wav2vec2-base-960h": "Wav2Vec2-Base-960h\nUsage\nEvaluation\nWav2Vec2-Base-960h\nFacebook's Wav2Vec2\nThe base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model\nmake sure that your speech input is also sampled at 16Khz.\nPaper\nAuthors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\nAbstract\nWe show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\nThe original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.\nUsage\nTo transcribe audio files the model can be used as a standalone acoustic model as follows:\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nfrom datasets import load_dataset\nimport torch\n# load model and tokenizer\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n# load dummy dataset and read soundfiles\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# tokenize\ninput_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n# retrieve logits\nlogits = model(input_values).logits\n# take argmax and decode\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)\nEvaluation\nThis code snippet shows how to evaluate facebook/wav2vec2-base-960h on LibriSpeech's \"clean\" and \"other\" test data.\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torch\nfrom jiwer import wer\nlibrispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\ndef map_to_pred(batch):\ninput_values = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\nwith torch.no_grad():\nlogits = model(input_values.to(\"cuda\")).logits\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)\nbatch[\"transcription\"] = transcription\nreturn batch\nresult = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=[\"audio\"])\nprint(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))\nResult (WER):\n\"clean\"\n\"other\"\n3.4\n8.6",
    "facebook/wav2vec2-large-xlsr-53": "Wav2Vec2-XLSR-53\nUsage\nWav2Vec2-XLSR-53\nFacebook's XLSR-Wav2Vec2\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. Note that this model should be fine-tuned on a downstream task, like Automatic Speech Recognition. Check out this blog for more information.\nPaper\nAuthors: Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli\nAbstract\nThis paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.\nThe original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.\nUsage\nSee this notebook for more information on how to fine-tune the model.",
    "facebook/wav2vec2-xlsr-53-espeak-cv-ft": "YAML Metadata\nError:\n\"language\" with value \"multi-lingual\" is not valid. It must be an ISO 639-1, 639-2 or 639-3 code (two/three letters), or a special value like \"code\", \"multilingual\". If you want to use BCP-47 identifiers, you can specify them in language_bcp47.\nWav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice\nUsage\nWav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice\nThis checkpoint leverages the pretrained checkpoint wav2vec2-large-xlsr-53\nand is fine-tuned on CommonVoice to recognize phonetic labels in multiple languages.\nWhen using the model make sure that your speech input is sampled at 16kHz.\nNote that the model outputs a string of phonetic labels. A dictionary mapping phonetic labels to words\nhas to be used to map the phonetic output labels to output words.\nPaper: Simple and Effective Zero-shot Cross-lingual Phoneme Recognition\nAuthors: Qiantong Xu, Alexei Baevski, Michael Auli\nAbstract\nRecent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech recognition systems without any labeled data. However, in many cases there is labeled data available for related languages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by mapping phonemes of the training languages to the target language using articulatory features. Experiments show that this simple method significantly outperforms prior work which introduced task-specific architectures and used only part of a monolingually pretrained model.\nThe original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.\nUsage\nTo transcribe audio files the model can be used as a standalone acoustic model as follows:\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nfrom datasets import load_dataset\nimport torch\n# load model and processor\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")\n# load dummy dataset and read soundfiles\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# tokenize\ninput_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n# retrieve logits\nwith torch.no_grad():\nlogits = model(input_values).logits\n# take argmax and decode\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)\n# => should give ['m …™ s t …ö k w …™ l t …ö …™ z √∞ …™ …ê p …ëÀê s …ôl l  å v √∞ …ô m …™ d …ôl k l √¶ s …™ z √¶ n d w iÀê a ä …° l √¶ d t …ô w …õ l k …ô m h …™ z …° …ëÀê s p …ô']",
    "felflare/bert-restore-punctuation": "‚ú® bert-restore-punctuation\nüöã Usage\nüì° Training data\nüéØ Accuracy\n‚òï Contact\n‚ú® bert-restore-punctuation\nThis a bert-base-uncased model finetuned for punctuation restoration on Yelp Reviews.\nThe model predicts the punctuation and upper-casing of plain, lower-cased text. An example use case can be ASR output. Or other cases when text has lost punctuation.\nThis model is intended for direct use as a punctuation restoration model for the general English language. Alternatively, you can use this for further fine-tuning on domain-specific texts for punctuation restoration tasks.\nModel restores the following punctuations -- [! ? . , - : ; ' ]\nThe model also restores the upper-casing of words.\nüöã Usage\nBelow is a quick way to get up and running with the model.\nFirst, install the package.\npip install rpunct\nSample python code.\nfrom rpunct import RestorePuncts\n# The default language is 'english'\nrpunct = RestorePuncts()\nrpunct.punctuate(\"\"\"in 2018 cornell researchers built a high-powered detector that in combination with an algorithm-driven process called ptychography set a world record\nby tripling the resolution of a state-of-the-art electron microscope as successful as it was that approach had a weakness it only worked with ultrathin samples that were\na few atoms thick anything thicker would cause the electrons to scatter in ways that could not be disentangled now a team again led by david muller the samuel b eckert\nprofessor of engineering has bested its own record by a factor of two with an electron microscope pixel array detector empad that incorporates even more sophisticated\n3d reconstruction algorithms the resolution is so fine-tuned the only blurring that remains is the thermal jiggling of the atoms themselves\"\"\")\n# Outputs the following:\n# In 2018, Cornell researchers built a high-powered detector that, in combination with an algorithm-driven process called Ptychography, set a world record by tripling the\n# resolution of a state-of-the-art electron microscope. As successful as it was, that approach had a weakness. It only worked with ultrathin samples that were a few atoms\n# thick. Anything thicker would cause the electrons to scatter in ways that could not be disentangled. Now, a team again led by David Muller, the Samuel B.\n# Eckert Professor of Engineering, has bested its own record by a factor of two with an Electron microscope pixel array detector empad that incorporates even more\n# sophisticated 3d reconstruction algorithms. The resolution is so fine-tuned the only blurring that remains is the thermal jiggling of the atoms themselves.\nThis model works on arbitrarily large text in English language and uses GPU if available.\nüì° Training data\nHere is the number of product reviews we used for finetuning the model:\nLanguage\nNumber of text samples\nEnglish\n560,000\nWe found the best convergence around 3 epochs, which is what presented here and available via a download.\nüéØ Accuracy\nThe fine-tuned model obtained the following accuracy on 45,990 held-out text samples:\nAccuracy\nOverall F1\nEval Support\n91%\n90%\n45,990\nBelow is a breakdown of the performance of the model by each label:\nlabel\nprecision\nrecall\nf1-score\nsupport\n!\n0.45\n0.17\n0.24\n424\n!+Upper\n0.43\n0.34\n0.38\n98\n'\n0.60\n0.27\n0.37\n11\n,\n0.59\n0.51\n0.55\n1522\n,+Upper\n0.52\n0.50\n0.51\n239\n-\n0.00\n0.00\n0.00\n18\n.\n0.69\n0.84\n0.75\n2488\n.+Upper\n0.65\n0.52\n0.57\n274\n:\n0.52\n0.31\n0.39\n39\n:+Upper\n0.36\n0.62\n0.45\n16\n;\n0.00\n0.00\n0.00\n17\n?\n0.54\n0.48\n0.51\n46\n?+Upper\n0.40\n0.50\n0.44\n4\nnone\n0.96\n0.96\n0.96\n35352\nUpper\n0.84\n0.82\n0.83\n5442\n‚òï Contact\nContact Daulet Nurmanbetov for questions, feedback and/or requests for similar models.",
    "finiteautomata/bertweet-base-sentiment-analysis": "Sentiment Analysis in English\nbertweet-sentiment-analysis\nLicense\nCitation\nSentiment Analysis in English\nbertweet-sentiment-analysis\nRepository: https://github.com/finiteautomata/pysentimiento/\nModel trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets.\nUses POS, NEG, NEU labels.\nLicense\npysentimiento is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses.\nTASS Dataset license\nSEMEval 2017 Dataset license\nCitation\nIf you use pysentimiento in your work, please cite this paper\n@misc{perez2021pysentimiento,\ntitle={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\nauthor={Juan Manuel P√©rez and Juan Carlos Giudici and Franco Luque},\nyear={2021},\neprint={2106.09462},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nEnjoy! ü§ó",
    "flax-community/clip-rsicd-v2": "Model Card: clip-rsicd\nModel Details\nModel Date\nModel Type\nModel Version\nTraining\nDemo\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nData\nPerformance and Limitations\nPerformance\nLimitations\nModel Card: clip-rsicd\nModel Details\nThis model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.\nModel Date\nJuly 2021\nModel Type\nThe base model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\nModel Version\nWe release several checkpoints for clip-rsicd model. Refer to our github repo for performance metrics on zero-shot classification for each of those.\nTraining\nTo reproduce the fine-tuning procedure one can use released script.\nThe model was trained using batch size 1024, adafactor optimizer with linear warmup and decay with peak learning rate 1e-4 on 1 TPU-v3-8.\nFull log of the training run can be found on WandB.\nDemo\nCheck out the model text-to-image and image-to-image capabilities using this demo.\nDocuments\nFine-tuning CLIP on RSICD with HuggingFace and flax/jax on colab using TPU\nUse with Transformers\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"flax-community/clip-rsicd-v2\")\nprocessor = CLIPProcessor.from_pretrained(\"flax-community/clip-rsicd-v2\")\nurl = \"https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nlabels = [\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"]\ninputs = processor(text=[f\"a photo of a {l}\" for l in labels], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\nfor l, p in zip(labels, probs[0]):\nprint(f\"{l:<16} {p:.4f}\")\nTry it on colab\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification.\nIn addition, we can imagine applications in defense and law enforcement, climate change and global warming, and even some consumer applications. A partial list of applications can be found here. In general we think such models can be useful as digital assistants for humans engaged in searching through large collections of images.\nWe also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nData\nThe model was trained on publicly available remote sensing image captions datasets. Namely RSICD, UCM and Sydney. More information on the datasets used can be found on our project page.\nPerformance and Limitations\nPerformance\nModel-name\nk=1\nk=3\nk=5\nk=10\noriginal CLIP\n0.572\n0.745\n0.837\n0.939\nclip-rsicd-v2 (this model)\n0.883\n0.968\n0.982\n0.998\nLimitations\nThe model is fine-tuned on RSI data but can contain some biases and limitations of the original CLIP model. Refer to CLIP model card for details on those.",
    "google/electra-base-discriminator": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\nHow to use the discriminator in transformers\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\nELECTRA is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the SQuAD 2.0 dataset.\nFor a detailed description and experimental results, please refer to our paper ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\nThis repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. GLUE), QA tasks (e.g., SQuAD), and sequence tagging tasks (e.g., text chunking).\nHow to use the discriminator in transformers\nfrom transformers import ElectraForPreTraining, ElectraTokenizerFast\nimport torch\ndiscriminator = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")\ntokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-base-discriminator\")\nsentence = \"The quick brown fox jumps over the lazy dog\"\nfake_sentence = \"The quick brown fox fake over the lazy dog\"\nfake_tokens = tokenizer.tokenize(fake_sentence)\nfake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\ndiscriminator_outputs = discriminator(fake_inputs)\npredictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n[print(\"%7s\" % token, end=\"\") for token in fake_tokens]\n[print(\"%7s\" % int(prediction), end=\"\") for prediction in predictions.tolist()]",
    "google/fnet-large": "FNet large model\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nContributions\nFNet large model\nPretrained model on English language using a masked language modeling (MLM) and next sentence prediction (NSP) objective. It was\nintroduced in this paper and first released in this repository.\nThis model is cased: it makes a difference between english and English. The model achieves 0.58 accuracy on MLM objective and 0.80 on NSP objective.\nDisclaimer: This model card has been written by gchhablani.\nModel description\nFNet is a transformers model with attention replaced with fourier transforms. Hence, the inputs do not contain an attention_mask. It is pretrained on a large corpus of\nEnglish data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling\nthem in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and\nlabels from those texts. More precisely, it was pretrained with two objectives:\nMasked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\nthe entire masked sentence through the model and has to predict the masked words. This is different from traditional\nrecurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\nGPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\nsentence.\nNext sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\nthey correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\npredict if the two sentences were following each other or not.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the FNet model as inputs.\nThis model has the following configuration:\n24-layer\n1024 hidden dimension\nIntended uses & limitations\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the model hub to look for\nfine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\nNote: The mask filling pipeline doesn't work exactly as the original model performs masking after converting to tokens. In masking pipeline an additional space is added after the [MASK].\n>>> from transformers import FNetForMaskedLM, FNetTokenizer, pipeline\n>>> tokenizer = FNetTokenizer.from_pretrained(\"google/fnet-large\")\n>>> model = FNetForMaskedLM.from_pretrained(\"google/fnet-large\")\n>>> unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[\n{\"sequence\": \"hello i'm a. model.\", \"score\": 0.12840192019939423, \"token\": 16678, \"token_str\": \".\"},\n{\"sequence\": \"hello i'm a a model.\", \"score\": 0.07460460811853409, \"token\": 8, \"token_str\": \"a\"},\n{\"sequence\": \"hello i'm a, model.\", \"score\": 0.05011311173439026, \"token\": 16680, \"token_str\": \",\"},\n{\"sequence\": \"hello i'm a and model.\", \"score\": 0.047409165650606155, \"token\": 36, \"token_str\": \"and\"},\n{\"sequence\": \"hello i'm a the model.\", \"score\": 0.0269990973174572, \"token\": 13, \"token_str\": \"the\"},\n]\nHere is how to use this model to get the features of a given text in PyTorch:\nNote: You must specify the maximum sequence length to be 512 and truncate/pad to the same length because the original model has no attention mask and considers all the hidden states during forward pass.\nfrom transformers import FNetTokenizer, FNetModel\ntokenizer = FNetTokenizer.from_pretrained(\"google/fnet-large\")\nmodel = FNetModel.from_pretrained(\"google/fnet-large\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\noutput = model(**encoded_input)\nLimitations and bias\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. However, the model's MLM accuracy may also affect answers. Given below are some example where gender-bias could be expected:\n>>> from transformers import FNetForMaskedLM, FNetTokenizer, pipeline\n>>> tokenizer = FNetTokenizer.from_pretrained(\"google/fnet-large\")\n>>> model = FNetForMaskedLM.from_pretrained(\"google/fnet-large\")\n>>> unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n>>> unmasker(\"The man worked as a [MASK].\")\n[\n{\"sequence\": \"the man worked as a a.\", \"score\": 0.39862048625946045, \"token\": 8, \"token_str\": \"a\"},\n{\"sequence\": \"the man worked as a the.\", \"score\": 0.20786496996879578, \"token\": 13, \"token_str\": \"the\"},\n{\"sequence\": \"the man worked as a as.\", \"score\": 0.012523212470114231, \"token\": 106, \"token_str\": \"as\"},\n{\"sequence\": \"the man worked as a an.\", \"score\": 0.010838045738637447, \"token\": 102, \"token_str\": \"an\"},\n{\"sequence\": \"the man worked as a and.\", \"score\": 0.006571347825229168, \"token\": 36, \"token_str\": \"and\"},\n]\n>>> unmasker(\"The woman worked as a [MASK].\")\n[\n{\"sequence\": \"the woman worked as a the.\", \"score\": 0.3320266902446747, \"token\": 13, \"token_str\": \"the\"},\n{\"sequence\": \"the woman worked as a a.\", \"score\": 0.2591220438480377, \"token\": 8, \"token_str\": \"a\"},\n{\"sequence\": \"the woman worked as a as.\", \"score\": 0.011250585317611694, \"token\": 106, \"token_str\": \"as\"},\n{\"sequence\": \"the woman worked as a an.\", \"score\": 0.010153685696423054, \"token\": 102, \"token_str\": \"an\"},\n{\"sequence\": \"the woman worked as a and.\", \"score\": 0.010126154869794846, \"token\": 36, \"token_str\": \"and\"},\n]\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nThe FNet model was pretrained on C4, a cleaned version of the Common Crawl dataset.\nTraining procedure\nPreprocessing\nThe texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model are\nthen of the form:\n[CLS] Sentence A [SEP] Sentence B [SEP]\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nPretraining\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 512 tokens. The optimizer\nused is Adam with a learning rate of 1e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.999\\beta_{2} = 0.999Œ≤2‚Äã=0.999, a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\nEvaluation results\nWhen fine-tuned on downstream tasks, this model achieves the following results:\nGlue test results:\nTask\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n78/76\n85\n85\n94\n78\n84\n88\n69\n81.9\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2105-03824,\nauthor    = {James Lee{-}Thorp and\nJoshua Ainslie and\nIlya Eckstein and\nSantiago Onta{\\~{n}}{\\'{o}}n},\ntitle     = {FNet: Mixing Tokens with Fourier Transforms},\njournal   = {CoRR},\nvolume    = {abs/2105.03824},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2105.03824},\narchivePrefix = {arXiv},\neprint    = {2105.03824},\ntimestamp = {Fri, 14 May 2021 12:13:30 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2105-03824.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\nContributions\nThanks to @gchhablani for adding this model.",
    "google/muril-base-cased": "MuRIL: Multilingual Representations for Indian Languages\nOverview\nTraining\nTrainable parameters\nUses & Limitations\nEvaluation\nReferences\nCitation\nContact\nMuRIL: Multilingual Representations for Indian Languages\nMuRIL is a BERT model pre-trained on 17 Indian languages and their transliterated counterparts. We have released the pre-trained model (with the MLM layer intact, enabling masked word predictions) in this repository. We have also released the encoder on TFHub with an additional pre-processing module, that processes raw text into the expected input format for the encoder. You can find more details on MuRIL in this paper.\nOverview\nThis model uses a BERT base architecture [1] pretrained from scratch using the\nWikipedia [2], Common Crawl [3], PMINDIA [4] and Dakshina [5] corpora for 17 [6]\nIndian languages.\nWe use a training paradigm similar to multilingual bert, with a few\nmodifications as listed:\nWe include translation and transliteration segment pairs in training as\nwell.\nWe keep an exponent value of 0.3 and not 0.7 for upsampling, shown to\nenhance low-resource performance. [7]\nSee the Training section for more details.\nTraining\nThe MuRIL model is pre-trained on monolingual segments as well as parallel\nsegments as detailed below :\nMonolingual Data : We make use of publicly available corpora from Wikipedia\nand Common Crawl for 17 Indian languages.\nParallel Data : We have two types of parallel data :\nTranslated Data : We obtain translations of the above monolingual\ncorpora using the Google NMT pipeline. We feed translated segment pairs\nas input. We also make use of the publicly available PMINDIA corpus.\nTransliterated Data : We obtain transliterations of Wikipedia using the\nIndicTrans [8] library. We feed transliterated segment pairs as input.\nWe also make use of the publicly available Dakshina dataset.\nWe keep an exponent value of 0.3 to calculate duplication multiplier values for\nupsampling of lower resourced languages and set dupe factors accordingly. Note,\nwe limit transliterated pairs to Wikipedia only.\nThe model was trained using a self-supervised masked language modeling task. We\ndo whole word masking with a maximum of 80 predictions. The model was trained\nfor 1000K steps, with a batch size of 4096, and a max sequence length of 512.\nTrainable parameters\nAll parameters in the module are trainable, and fine-tuning all parameters is\nthe recommended practice.\nUses & Limitations\nThis model is intended to be used for a variety of downstream NLP tasks for\nIndian languages. This model is trained on transliterated data as well, a\nphenomomenon commonly observed in the Indian context. This model is not expected\nto perform well on languages other than the ones used in pretraining, i.e. 17\nIndian languages.\nEvaluation\nWe provide the results of fine-tuning this model on a set of downstream tasks.\nWe choose these tasks from the XTREME benchmark, with evaluation done on Indian language test-sets.\nWe also transliterate the test-sets and evaluate on the same.\nWe use the same fine-tuning setting as is used by [9], except for TyDiQA, where we use additional SQuAD v1.1 English training data, similar to [10].\nFor Tatoeba, we do not fine-tune the model, and use the pooled_output of the last layer as the sentence embedding.\nAll results are computed in a zero-shot setting, with English being the high resource training set language.\nShown below are results on datasets from the XTREME benchmark (in %)\nPANX (F1)\nml\nta\nte\nen\nbn\nhi\nmr\nur\nAverage\nmBERT\n54.77\n51.24\n50.16\n84.40\n68.59\n65.13\n58.44\n31.36\n58.01\nMuRIL\n75.74\n71.86\n64.99\n84.43\n85.97\n78.09\n74.63\n85.07\n77.60\nUDPOS (F1)\nen\nhi\nmr\nta\nte\nur\nAverage\nmBERT\n95.35\n66.09\n71.27\n59.58\n76.98\n57.85\n71.19\nMuRIL\n95.55\n64.47\n82.95\n62.57\n85.63\n58.93\n75.02\nXNLI (Accuracy)\nen\nhi\nur\nAverage\nmBERT\n81.72\n60.52\n58.20\n66.81\nMuRIL\n83.85\n70.66\n67.70\n74.07\nTatoeba (Accuracy)\nml\nta\nte\nbn\nhi\nmr\nur\nAverage\nmBERT\n20.23\n12.38\n14.96\n12.80\n27.80\n18.00\n22.70\n18.41\nMuRIL\n26.35\n36.81\n17.52\n20.20\n31.50\n26.60\n17.10\n25.15\nXQUAD (F1/EM)\nen\nhi\nAverage\nmBERT\n83.85/72.86\n58.46/43.53\n71.15/58.19\nMuRIL\n84.31/72.94\n73.93/58.32\n79.12/65.63\nMLQA (F1/EM)\nen\nhi\nAverage\nmBERT\n80.39/67.30\n50.28/35.18\n65.34/51.24\nMuRIL\n80.28/67.37\n67.34/50.22\n73.81/58.80\nTyDiQA (F1/EM)\nen\nbn\nte\nAverage\nmBERT\n75.21/65.00\n60.62/45.13\n53.55/44.54\n63.13/51.66\nMuRIL\n74.10/64.55\n78.03/66.37\n73.95/46.94\n75.36/59.28\nShown below are results on the transliterated versions of the above\ntest-sets.\nPANX (F1)\nml_tr\nta_tr\nte_tr\nbn_tr\nhi_tr\nmr_tr\nur_tr\nAverage\nmBERT\n7.53\n1.04\n8.24\n41.77\n25.46\n8.34\n7.30\n14.24\nMuRIL\n63.39\n7.00\n53.62\n72.94\n69.75\n68.77\n68.41\n57.70\nUDPOS (F1)\nhi_tr\nmr_tr\nta_tr\nte_tr\nur_tr\nAverage\nmBERT\n25.00\n33.67\n24.02\n36.21\n22.07\n28.20\nMuRIL\n63.09\n67.19\n58.40\n65.30\n56.49\n62.09\nXNLI (Accuracy)\nhi_tr\nur_tr\nAverage\nmBERT\n39.6\n38.86\n39.23\nMuRIL\n68.24\n61.16\n64.70\nTatoeba (Accuracy)\nml_tr\nta_tr\nte_tr\nbn_tr\nhi_tr\nmr_tr\nur_tr\nAverage\nmBERT\n2.18\n1.95\n5.13\n1.80\n3.00\n2.40\n2.30\n2.68\nMuRIL\n10.33\n11.07\n11.54\n8.10\n14.90\n7.20\n13.70\n10.98\nReferences\n[1]: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT:\nPre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv preprint\narXiv:1810.04805, 2018.\n[2]: Wikipedia\n[3]: Common Crawl\n[4]:\nPMINDIA\n[5]: Dakshina\n[6]: Assamese (as), Bengali (bn), English (en), Gujarati (gu), Hindi (hi),\nKannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya\n(or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu\n(ur).\n[7]: Conneau, Alexis, et al.\nUnsupervised cross-lingual representation learning at scale.\narXiv preprint arXiv:1911.02116 (2019).\n[8]: IndicTrans\n[9]: Hu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., & Johnson, M.\n(2020). Xtreme: A massively multilingual multi-task benchmark for evaluating\ncross-lingual generalization. arXiv\npreprint arXiv:2003.11080.\n[10]: Fang, Y., Wang, S., Gan, Z., Sun, S., & Liu, J. (2020).\nFILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding.\narXiv preprint arXiv:2009.05166.\nCitation\nIf you find MuRIL useful in your applications, please cite the following paper:\n@misc{khanuja2021muril,\ntitle={MuRIL: Multilingual Representations for Indian Languages},\nauthor={Simran Khanuja and Diksha Bansal and Sarvesh Mehtani and Savya Khosla and Atreyee Dey and Balaji Gopalan and Dilip Kumar Margam and Pooja Aggarwal and Rajiv Teja Nagipogu and Shachi Dave and Shruti Gupta and Subhash Chandra Bose Gali and Vish Subramanian and Partha Talukdar},\nyear={2021},\neprint={2103.10730},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nContact\nPlease mail your queries/feedback to muril-contact@google.com.",
    "google/reformer-enwik8": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nReformer Language model on character level and trained on enwik8.\nReformer Language model on character level and trained on enwik8.\nenwik8 is a dataset based on Wikipedia and is often used to measure the model's ability to compress data, e.g. in\nthe scope of the Hutter prize: https://en.wikipedia.org/wiki/Hutter_Prize.\nreformer-enwik8 was pretrained on the first 90M chars of enwik8 whereas the text was chunked into batches of size 65536 chars (=2^16).\nThe model's weights were taken from https://console.cloud.google.com/storage/browser/trax-ml/reformer/enwik8 and converted\nto Hugging Face's PyTorch ReformerLM model ReformerModelWithLMHead.\nThe model is a language model that operates on characters.\nTherefore, this model does not need a tokenizer. The following function can instead be used for encoding and decoding:\nimport torch\n# Encoding\ndef encode(list_of_strings, pad_token_id=0):\nmax_length = max([len(string) for string in list_of_strings])\n# create emtpy tensors\nattention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\ninput_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\nfor idx, string in enumerate(list_of_strings):\n# make sure string is in byte format\nif not isinstance(string, bytes):\nstring = str.encode(string)\ninput_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\nattention_masks[idx, :len(string)] = 1\nreturn input_ids, attention_masks\n# Decoding\ndef decode(outputs_ids):\ndecoded_outputs = []\nfor output_ids in outputs_ids.tolist():\n# transform id back to char IDs < 2 are simply transformed to \"\"\ndecoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\nreturn decoded_outputs\nText can be generated as follows:\nfrom transformers import ReformerModelWithLMHead\nmodel = ReformerModelWithLMHead.from_pretrained(\"google/reformer-enwik8\")\nencoded, attention_masks = encode([\"In 1965, Brooks left IBM to found the Department of\"])\ndecode(model.generate(encoded, do_sample=True, max_length=150))\n# gives:\n# In 1965, Brooks left IBM to found the Department of Journalism in 1968. IBM had jurisdiction himself in 1980, while Brooks resolved, nevertheless thro\nNote: Language generation using ReformerModelWithLMHead is not optimized yet and is rather slow.",
    "google/t5-efficient-mini": "T5-Efficient-MINI (Deep-Narrow version)\nDetails model architecture\nPre-Training\nFine-Tuning\nDownstream Performance\nComputational Complexity\nMore information\nT5-Efficient-MINI (Deep-Narrow version)\nT5-Efficient-MINI is a variation of Google's original T5 following the T5 model architecture.\nIt is a pretrained-only checkpoint and was released with the\npaper Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers\nby Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler.\nIn a nutshell, the paper indicates that a Deep-Narrow model architecture is favorable for downstream performance compared to other model architectures\nof similar parameter count.\nTo quote the paper:\nWe generally recommend a DeepNarrow strategy where the model‚Äôs depth is preferentially increased\nbefore considering any other forms of uniform scaling across other dimensions. This is largely due to\nhow much depth influences the Pareto-frontier as shown in earlier sections of the paper. Specifically, a\ntall small (deep and narrow) model is generally more efficient compared to the base model. Likewise,\na tall base model might also generally more efficient compared to a large model. We generally find\nthat, regardless of size, even if absolute performance might increase as we continue to stack layers,\nthe relative gain of Pareto-efficiency diminishes as we increase the layers, converging at 32 to 36\nlayers. Finally, we note that our notion of efficiency here relates to any one compute dimension, i.e.,\nparams, FLOPs or throughput (speed). We report all three key efficiency metrics (number of params,\nFLOPS and speed) and leave this decision to the practitioner to decide which compute dimension to\nconsider.\nTo be more precise, model depth is defined as the number of transformer blocks that are stacked sequentially.\nA sequence of word embeddings is therefore processed sequentially by each transformer block.\nDetails model architecture\nThis model checkpoint - t5-efficient-mini - is of model type Mini with no variations.\nIt has 31.23 million parameters and thus requires ca. 124.92 MB of memory in full precision (fp32)\nor  62.46 MB of memory in half precision (fp16 or bf16).\nA summary of the original T5 model architectures can be seen here:\nModel\nnl (el/dl)\nff\ndm\nkv\nnh\n#Params\nTiny\n4/4\n1024\n256\n32\n4\n16M\nMini\n4/4\n1536\n384\n32\n8\n31M\nSmall\n6/6\n2048\n512\n32\n8\n60M\nBase\n12/12\n3072\n768\n64\n12\n220M\nLarge\n24/24\n4096\n1024\n64\n16\n738M\nXl\n24/24\n16384\n1024\n128\n32\n3B\nXXl\n24/24\n65536\n1024\n128\n128\n11B\nwhereas the following abbreviations are used:\nAbbreviation\nDefinition\nnl\nNumber of transformer blocks (depth)\ndm\nDimension of embedding vector (output vector of transformers block)\nkv\nDimension of key/value projection matrix\nnh\nNumber of attention heads\nff\nDimension of intermediate vector within transformer block (size of feed-forward projection matrix)\nel\nNumber of transformer blocks in the encoder (encoder depth)\ndl\nNumber of transformer blocks in the decoder (decoder depth)\nsh\nSignifies that attention heads are shared\nskv\nSignifies that key-values projection matrices are tied\nIf a model checkpoint has no specific, el or dl than both the number of encoder- and decoder layers correspond to nl.\nPre-Training\nThe checkpoint was pretrained on the Colossal, Cleaned version of Common Crawl (C4) for 524288 steps using\nthe span-based masked language modeling (MLM) objective.\nFine-Tuning\nNote: This model is a pretrained checkpoint and has to be fine-tuned for practical usage.\nThe checkpoint was pretrained in English and is therefore only useful for English NLP tasks.\nYou can follow on of the following examples on how to fine-tune the model:\nPyTorch:\nSummarization\nQuestion Answering\nText Classification - Note: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\nTensorflow:\nSummarization\nText Classification - Note: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\nJAX/Flax:\nSummarization\nText Classification - Note: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\nDownstream Performance\nTODO: Add table if available\nComputational Complexity\nTODO: Add table if available\nMore information\nWe strongly recommend the reader to go carefully through the original paper Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers to get a more nuanced understanding of this model checkpoint.\nAs explained in the following issue, checkpoints including the sh or skv\nmodel architecture variations have not been ported to Transformers as they are probably of limited practical usage and are lacking a more detailed description. Those checkpoints are kept here as they might be ported potentially in the future.",
    "google/tapas-base": "TAPAS base model\nModel description\nIntended uses & limitations\nTraining procedure\nPreprocessing\nPre-training\nBibTeX entry and citation info\nTAPAS base model\nThis model has 2 versions which can be used. The latest version, which is the default one, corresponds to the tapas_inter_masklm_base_reset checkpoint of the original Github repository.\nThis model was pre-trained on MLM and an additional step which the authors call intermediate pre-training. It uses relative position embeddings by default (i.e. resetting the position index at every cell of the table).\nThe other (non-default) version which can be used is the one with absolute position embeddings:\nrevision=\"no_reset\", which corresponds to tapas_inter_masklm_base\nDisclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by\nthe Hugging Face team and contributors.\nModel description\nTAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion.\nThis means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it\ncan use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\nMasked language modeling (MLM): taking a (flattened) table and associated context, the model randomly masks 15% of the words in\nthe input, then runs the entire (partially masked) sequence through the model. The model then has to predict the masked words.\nThis is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other,\nor from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional\nrepresentation of a table and associated text.\nIntermediate pre-training: to encourage numerical reasoning on tables, the authors additionally pre-trained the model by creating\na balanced dataset of millions of syntactically created training examples. Here, the model must predict (classify) whether a sentence\nis supported or refuted by the contents of a table. The training examples are created based on synthetic as well as counterfactual statements.\nThis way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used\nto extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed\nor refuted by the contents of a table. Fine-tuning is done by adding one or more classification heads on top of the pre-trained model, and then\njointly train these randomly initialized classification heads with the base model on a downstream task.\nIntended uses & limitations\nYou can use the raw model for getting hidden representatons about table-question pairs, but it's mostly intended to be fine-tuned on a downstream task such as question answering or sequence classification. See the model hub to look for fine-tuned versions on a task that interests you.\nTraining procedure\nPreprocessing\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n[CLS] Sentence [SEP] Flattened table [SEP]\nPre-training\nThe model was pre-trained on 32 Cloud TPU v3 cores for 1,000,000 steps with maximum sequence length 512 and batch size of 512.\nIn this setup, pre-training on MLM only takes around 3 days. Aditionally, the model has been further pre-trained on a second task (table entailment). See the original TAPAS paper and the follow-up paper for more details.\nThe optimizer used is Adam with a learning rate of 5e-5, and a warmup\nratio of 0.01.\nBibTeX entry and citation info\n@misc{herzig2020tapas,\ntitle={TAPAS: Weakly Supervised Table Parsing via Pre-training},\nauthor={Jonathan Herzig and Pawe≈Ç Krzysztof Nowak and Thomas M√ºller and Francesco Piccinno and Julian Martin Eisenschlos},\nyear={2020},\neprint={2004.02349},\narchivePrefix={arXiv},\nprimaryClass={cs.IR}\n}\n@misc{eisenschlos2020understanding,\ntitle={Understanding tables with intermediate pre-training},\nauthor={Julian Martin Eisenschlos and Syrine Krichene and Thomas M√ºller},\nyear={2020},\neprint={2010.00571},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "google/vit-base-patch16-224-in21k": "Vision Transformer (base-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nVision Transformer (base-sized model)\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nNote that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model in PyTorch:\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nHere is how to use this model in JAX/Flax:\nfrom transformers import ViTImageProcessor, FlaxViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"np\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nTraining data\nThe ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\nPretraining\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\n@misc{wu2020visual,\ntitle={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\nauthor={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\nyear={2020},\neprint={2006.03677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "google/vit-base-patch16-384": "Vision Transformer (base-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nVision Transformer (base-sized model)\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at a higher resolution of 384x384.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nCurrently, both the feature extractor and model  support PyTorch. Tensorflow and JAX/FLAX are coming soon, and the API of ViTFeatureExtractor might change.\nTraining data\nThe ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, and fine-tuned on ImageNet, a dataset consisting of 1 million images and 1k classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\nPretraining\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\n@misc{wu2020visual,\ntitle={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\nauthor={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\nyear={2020},\neprint={2006.03677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "google/vit-large-patch16-384": "Vision Transformer (large-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nVision Transformer (large-sized model)\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at a higher resolution of 384x384.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-384')\nmodel = ViTForImageClassification.from_pretrained('google/vit-large-patch16-384')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nCurrently, both the feature extractor and model  support PyTorch. Tensorflow and JAX/FLAX are coming soon, and the API of ViTFeatureExtractor might change.\nTraining data\nThe ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, and fine-tuned on ImageNet, a dataset consisting of 1 million images and 1k classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\nPretraining\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\n@misc{wu2020visual,\ntitle={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\nauthor={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\nyear={2020},\neprint={2006.03677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "hemekci/off_detection_turkish": "Offensive Language Detection Model in Turkish\nTraining Data\nValidation\nOffensive Language Detection Model in Turkish\nuses Bert and pytorch\nfine tuned with Twitter data.\nUTF-8 configuration is done\nTraining Data\nNumber of training sentences: 31,277\nExample Tweets\n19823 Daliaan yifng cok erken attin be...      1.38 ...| NOT|\n30525 @USER Bak biri kollarƒ±mda uyuyup gitmem diyor..|NOT|\n26468 Helal olsun be :) Norve√ßten sabaha kar≈üƒ± geldi aq... |  OFF|\n14105 @USER Sunu cekecek ve g√ºzel oldugunu s√∂ylecek aptal...       |OFF|\n4958  Ya seni yerim ben ≈üap≈üal ≈üey ü§ó | NOT|\n12966 Herkesin akƒ±llƒ± ge√ßindiƒüi bir sosyal medyamƒ±z var ...   |NOT|\n5788  Ma√ßƒ±n √∂zetlerini izleyenler futbolcular gidiyo...       |NOT|\nOFFENSIVE\nRESULT\nNOT\n25231\nOFF\n6046\ndtype: int64\nValidation\nepoch\nTraining Loss\nValid. Loss\nValid.Accuracy\nTraining Time\nValidation Time\n1\n0.31\n0.28\n0.89\n0:07:14\n0:00:13\n2\n0.18\n0.29\n0.90\n0:07:18\n0:00:13\n3\n0.08\n0.40\n0.89\n0:07:16\n0:00:13\n4\n0.04\n0.59\n0.89\n0:07:13\n0:00:13\nMatthews Corr. Coef. (-1 : +1):\nTotal MCC Score: 0.633",
    "hfl/chinese-macbert-large": "Please use 'Bert' related functions to load this model!\nIntroduction\nCitation\nPlease use 'Bert' related functions to load this model!\nThis repository contains the resources in our paper \"Revisiting Pre-trained Models for Chinese Natural Language Processing\", which will be published in \"Findings of EMNLP\". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.\nRevisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu\nYou may also interested in,\nChinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm\nChinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\nChinese XLNet: https://github.com/ymcui/Chinese-XLNet\nKnowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/TextBrewer\nMore resources by HFL: https://github.com/ymcui/HFL-Anthology\nIntroduction\nMacBERT is an improved BERT with novel MLM as correction pre-training task, which mitigates the discrepancy of pre-training and fine-tuning.\nInstead of masking with [MASK] token, which never appears in the Ô¨Åne-tuning stage, we propose to use similar words for the masking purpose. A similar word is obtained by using Synonyms toolkit (Wang and Hu, 2017), which is based on word2vec (Mikolov et al., 2013) similarity calculations. If an N-gram is selected to mask, we will Ô¨Ånd similar words individually. In rare cases, when there is no similar word, we will degrade to use random word replacement.\nHere is an example of our pre-training task.\nExample\nOriginal Sentence\nwe use a language model to predict the probability of the next word.\nMLM\nwe use a language [M] to [M] ##di ##ct the pro [M] ##bility of the next word .\nWhole word masking\nwe use a language [M] to [M] [M] [M] the [M] [M] [M] of the next word .\nN-gram masking\nwe use a [M] [M] to [M] [M] [M] the [M] [M] [M] [M] [M] next word .\nMLM as correction\nwe use a text system to ca ##lc ##ulate the po ##si ##bility of the next word .\nExcept for the new pre-training task, we also incorporate the following techniques.\nWhole Word Masking (WWM)\nN-gram masking\nSentence-Order Prediction (SOP)\nNote that our MacBERT can be directly replaced with the original BERT as there is no differences in the main neural architecture.\nFor more technical details, please check our paper: Revisiting Pre-trained Models for Chinese Natural Language Processing\nCitation\nIf you find our resource or paper is useful, please consider including the following citation in your paper.\nhttps://arxiv.org/abs/2004.13922\n@inproceedings{cui-etal-2020-revisiting,\ntitle = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\nauthor = \"Cui, Yiming  and\nChe, Wanxiang  and\nLiu, Ting  and\nQin, Bing  and\nWang, Shijin  and\nHu, Guoping\",\nbooktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\npages = \"657--668\",\n}",
    "hfl/chinese-roberta-wwm-ext": "Please use 'Bert' related functions to load this model!\nChinese BERT with Whole Word Masking\nCitation\nPlease use 'Bert' related functions to load this model!\nChinese BERT with Whole Word Masking\nFor further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking.\nPre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu\nThis repository is developed based onÔºöhttps://github.com/google-research/bert\nYou may also interested in,\nChinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm\nChinese MacBERT: https://github.com/ymcui/MacBERT\nChinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\nChinese XLNet: https://github.com/ymcui/Chinese-XLNet\nKnowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/TextBrewer\nMore resources by HFL: https://github.com/ymcui/HFL-Anthology\nCitation\nIf you find the technical report or resource is useful, please cite the following technical report in your paper.\nPrimary: https://arxiv.org/abs/2004.13922\n@inproceedings{cui-etal-2020-revisiting,\ntitle = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\nauthor = \"Cui, Yiming  and\nChe, Wanxiang  and\nLiu, Ting  and\nQin, Bing  and\nWang, Shijin  and\nHu, Guoping\",\nbooktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\npages = \"657--668\",\n}\nSecondary: https://arxiv.org/abs/1906.08101\n@article{chinese-bert-wwm,\ntitle={Pre-Training with Whole Word Masking for Chinese BERT},\nauthor={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},\njournal={arXiv preprint arXiv:1906.08101},\nyear={2019}\n}",
    "huawei-noah/TinyBERT_General_4L_312D": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTinyBERT: Distilling BERT for Natural Language Understanding\nCitation\nTinyBERT: Distilling BERT for Natural Language Understanding\nTinyBERT is 7.5x smaller and 9.4x faster on inference than BERT-base and achieves competitive performances in the tasks of natural language understanding. It performs a novel transformer distillation at both the pre-training and task-specific learning stages. In general distillation, we use the original BERT-base without fine-tuning as the teacher and a large-scale text corpus as the learning data. By performing the Transformer distillation on the text from general domain, we obtain a general TinyBERT which provides a good initialization for the task-specific distillation. We here provide the general TinyBERT for your tasks at hand.\nFor more details about the techniques of TinyBERT, refer to our paper:\nTinyBERT: Distilling BERT for Natural Language Understanding\nCitation\nIf you find TinyBERT useful in your research, please cite the following paper:\n@article{jiao2019tinybert,\ntitle={Tinybert: Distilling bert for natural language understanding},\nauthor={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},\njournal={arXiv preprint arXiv:1909.10351},\nyear={2019}\n}",
    "huggingartists/billie-eilish": "How does it work?\nTraining data\nTraining procedure\nHow to use\nLimitations and bias\nAbout\nü§ñ HuggingArtists Model ü§ñ\nBillie Eilish\n@billie-eilish\nI was made with huggingartists.\nCreate your own bot based on your favorite artist with the demo!\nHow does it work?\nTo understand how the model was developed, check the W&B report.\nTraining data\nThe model was trained on lyrics from Billie Eilish.\nDataset is available here.\nAnd can be used with:\nfrom datasets import load_dataset\ndataset = load_dataset(\"huggingartists/billie-eilish\")\nOr with Transformers library:\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/billie-eilish\")\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/billie-eilish\")\nExplore the data, which is tracked with W&B artifacts at every step of the pipeline.\nTraining procedure\nThe model is based on a pre-trained GPT-2 which is fine-tuned on Billie Eilish's lyrics.\nHyperparameters and metrics are recorded in the W&B training run for full transparency and reproducibility.\nAt the end of training, the final model is logged and versioned.\nHow to use\nYou can use this model directly with a pipeline for text generation:\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\nmodel='huggingartists/billie-eilish')\ngenerator(\"I am\", num_return_sequences=5)\nLimitations and bias\nThe model suffers from the same limitations and bias as GPT-2.\nIn addition, the data present in the user's tweets further affects the text generated by the model.\nAbout\nBuilt by Aleksey Korshuk\nFor more details, visit the project repository.",
    "ibm-research/tslm-discourse-markers": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nlanguage:\n- en\ntags:\n- discourse-markers\nlicense: apache-2.0\nSenDM model described at https://arxiv.org/pdf/2201.02026\nlanguage:\n- en\ntags:\n- discourse-markers\nlicense: apache-2.0",
    "jhgan/ko-sroberta-multitask": "ko-sroberta-multitask\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nEvaluation Results\nTraining\nFull Model Architecture\nCiting & Authors\nko-sroberta-multitask\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"ÏïàÎÖïÌïòÏÑ∏Ïöî?\", \"ÌïúÍµ≠Ïñ¥ Î¨∏Ïû• ÏûÑÎ≤†Îî©ÏùÑ ÏúÑÌïú Î≤ÑÌä∏ Î™®Îç∏ÏûÖÎãàÎã§.\"]\nmodel = SentenceTransformer('jhgan/ko-sroberta-multitask')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')\nmodel = AutoModel.from_pretrained('jhgan/ko-sroberta-multitask')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nEvaluation Results\nKorSTS, KorNLI ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú Î©ÄÌã∞ ÌÉúÏä§ÌÅ¨ ÌïôÏäµÏùÑ ÏßÑÌñâÌïú ÌõÑ KorSTS ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÌèâÍ∞ÄÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.\nCosine Pearson: 84.77\nCosine Spearman: 85.60\nEuclidean Pearson: 83.71\nEuclidean Spearman: 84.40\nManhattan Pearson: 83.70\nManhattan Spearman: 84.38\nDot Pearson: 82.42\nDot Spearman: 82.33\nTraining\nThe model was trained with the parameters:\nDataLoader:\nsentence_transformers.datasets.NoDuplicatesDataLoader.NoDuplicatesDataLoader of length 8885 with parameters:\n{'batch_size': 64}\nLoss:\nsentence_transformers.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss with parameters:\n{'scale': 20.0, 'similarity_fct': 'cos_sim'}\nDataLoader:\ntorch.utils.data.dataloader.DataLoader of length 719 with parameters:\n{'batch_size': 8, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\nLoss:\nsentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss\nParameters of the fit()-Method:\n{\n\"epochs\": 5,\n\"evaluation_steps\": 1000,\n\"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n\"max_grad_norm\": 1,\n\"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n\"optimizer_params\": {\n\"lr\": 2e-05\n},\n\"scheduler\": \"WarmupLinear\",\n\"steps_per_epoch\": null,\n\"warmup_steps\": 360,\n\"weight_decay\": 0.01\n}\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nHam, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). Kornli and korsts: New benchmark datasets for korean natural language understanding. arXiv\npreprint arXiv:2004.03289\nReimers, Nils and Iryna Gurevych. ‚ÄúSentence-BERT: Sentence Embeddings using Siamese BERT-Networks.‚Äù ArXiv abs/1908.10084 (2019)\nReimers, Nils and Iryna Gurevych. ‚ÄúMaking Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.‚Äù EMNLP (2020).",
    "kamalkraj/BioELECTRA-PICO": "BioELECTRA-PICO\nCite our paper using below citation\n@inproceedings{kanakarajan-etal-2021-bioelectra,\ntitle = \"{B}io{ELECTRA}:Pretrained Biomedical text Encoder using Discriminators\",\nauthor = \"Kanakarajan, Kamal raj  and\nKundumani, Bhuvana  and\nSankarasubbu, Malaikannan\",\nbooktitle = \"Proceedings of the 20th Workshop on Biomedical Language Processing\",\nmonth = jun,\nyear = \"2021\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2021.bionlp-1.16\",\ndoi = \"10.18653/v1/2021.bionlp-1.16\",\npages = \"143--154\",\nabstract = \"Recent advancements in pretraining strategies in NLP have shown a significant improvement in the performance of models on various text mining tasks. We apply {`}replaced token detection{'} pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34{\\%}(1.39{\\%} accuracy improvement) on MedNLI and 64{\\%} (2.98{\\%} accuracy improvement) on PubMedQA dataset.\",\n}",
    "keras-io/deeplabv3p-resnet50": "Multiclass semantic segmentation using DeepLabV3+\nBackground Information\nTraining Data\nModel\nMulticlass semantic segmentation using DeepLabV3+\nThis repo contains the model and the notebook to this Keras example on Multiclass semantic segmentation using DeepLabV3+.\nFull credits to: Soumik Rakshit\nThe model is trained for demonstrative purposes and does not guarantee the best results in production. For better results, follow & optimize the Keras example as per your need.\nBackground Information\nSemantic segmentation, with the goal to assign semantic labels to every pixel in an image, is an essential computer vision task. In this example, we implement the DeepLabV3+ model for multi-class semantic segmentation, a fully-convolutional architecture that performs well on semantic segmentation benchmarks.\nTraining Data\nThe model is trained on a subset (10,000 images) of Crowd Instance-level Human Parsing Dataset. The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification. This dataset can be used for the \"human part segmentation\" task.\nModel\nThe model uses ResNet50 pretrained on ImageNet as the backbone model.\nReferences:\nEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\nRethinking Atrous Convolution for Semantic Image Segmentation\nDeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"
}