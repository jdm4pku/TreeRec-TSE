{
    "karpathy/gpt2_1558M_final4_hf": "This is a GPT-2 model trained in llm.c for 330K steps (of 1M batch size) on FineWeb-EDU.\nA lot more detailed information is here: https://github.com/karpathy/llm.c/discussions/677 .\nThis model has a bit of a complicated history. I wanted to train it for 400K steps, i.e. (-x 400000), but it became unstable later in training and exploded around step 330K. Because I was losing my computing quota shortly, I decided to just rewind back to checkpoint 300K, and then instead of going all the way to 400K I started annealing linearly down to 330K. This went without incident and produced this model.\nThis is the longest I've trained a GPT-2 model for, and it reaches HellaSwag of 62.7 by the end.",
    "zai-org/codegeex4-all-9b-GGUF": "CodeGeeX4: Open Multilingual Code Generation Model\nGet Started\nEvaluation\nLicense\nCitation\nCodeGeeX4: Open Multilingual Code Generation Model\n‰∏≠Êñá\nGitHub\n!!! This is the GGUF version of CodeGeeX4, the original version can be found here. !!!\nWe introduce CodeGeeX4-ALL-9B, the open-source version of the latest CodeGeeX4 model series. It is a multilingual code generation model continually trained on the GLM-4-9B, significantly enhancing its code generation capabilities. Using a single CodeGeeX4-ALL-9B model, it can support comprehensive functions such as code completion and generation, code interpreter, web search, function call, repository-level code Q&A, covering various scenarios of software development. CodeGeeX4-ALL-9B has achieved highly competitive performance  on public benchmarks, such as BigCodeBench and NaturalCodeBench. It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance.\nGet Started\nDownload model weights:\nhuggingface-cli download THUDM/codegeex4-all-9b-GGUF\nUse the latest llama.cpp to launch codegeex4-all-9b-GGUF\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\ncmake . -B build\ncmake --build build --config Release\nbuild/bin/llama-cli -m Your_Model_Path -p \"Your_Input\"\nPlease make sure the prompt is under the following format:\nf\"<|system|>\\n{system_prompt}\\n<|user|>\\n{prompt}\\n<|assistant|>\\n\"\nDefault system_prompt:\n‰Ω†ÊòØ‰∏Ä‰ΩçÊô∫ËÉΩÁºñÁ®ãÂä©ÊâãÔºå‰Ω†Âè´CodeGeeX„ÄÇ‰Ω†‰ºö‰∏∫Áî®Êà∑ÂõûÁ≠îÂÖ≥‰∫éÁºñÁ®ã„ÄÅ‰ª£Á†Å„ÄÅËÆ°ÁÆóÊú∫ÊñπÈù¢ÁöÑ‰ªª‰ΩïÈóÆÈ¢òÔºåÂπ∂Êèê‰æõÊ†ºÂºèËßÑËåÉ„ÄÅÂèØ‰ª•ÊâßË°å„ÄÅÂáÜÁ°ÆÂÆâÂÖ®ÁöÑ‰ª£Á†ÅÔºåÂπ∂Âú®ÂøÖË¶ÅÊó∂Êèê‰æõËØ¶ÁªÜÁöÑËß£Èáä„ÄÇ\nThe English version:\nYou are an intelligent programming assistant named CodeGeeX. You will answer any questions users have about programming, coding, and computers, and provide code that is formatted correctly.\nEvaluation\nModel\nSeq Length\nHumanEval\nMBPP\nNCB\nLCB\nHumanEvalFIM\nCRUXEval-O\nLlama3-70B-intruct\n8K\n77.4\n82.3\n37.0\n27.4\n-\n-\nDeepSeek Coder 33B Instruct\n16K\n81.1\n80.4\n39.3\n29.3\n78.2\n49.9\nCodestral-22B\n32K\n81.1\n78.2\n46.0\n35.3\n91.6\n51.3\nCodeGeeX4-All-9B\n128K\n82.3\n75.7\n40.4\n28.5\n85.0\n47.1\nLicense\nThe model weights are licensed under the following License.\nCitation\nIf you find our work helpful, please feel free to cite the following paper:\n@inproceedings{zheng2023codegeex,\ntitle={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},\nauthor={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},\nbooktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},\npages={5673--5684},\nyear={2023}\n}",
    "Qwen/Qwen2-Audio-7B": "Qwen2-Audio-7B\nIntroduction\nRequirements\nQuickstart\nCitation\nQwen2-Audio-7B\nIntroduction\nQwen2-Audio is the new series of Qwen large audio-language models. Qwen2-Audio is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. We introduce two distinct audio interaction modes:\nvoice chat: users can freely engage in voice interactions with Qwen2-Audio without text input;\naudio analysis: users could provide audio and text instructions for analysis during the interaction;\nWe release Qwen2-Audio-7B and Qwen2-Audio-7B-Instruct, which are pretrained model and chat model respectively.\nFor more details, please refer to our Blog, GitHub, and Report.\nRequirements\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command pip install git+https://github.com/huggingface/transformers, or you might encounter the following error:\nKeyError: 'qwen2-audio'\nQuickstart\nHere provides offers a code snippet illustrating the process of loading both the processor and model, alongside detailed instructions on executing the pretrained Qwen2-Audio base model for content generation.\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{Qwen2-Audio,\ntitle={Qwen2-Audio Technical Report},\nauthor={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2407.10759},\nyear={2024}\n}\n@article{Qwen-Audio,\ntitle={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\nauthor={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2311.07919},\nyear={2023}\n}",
    "google/gemma-2-2b-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 2 model card\nModel Information\nDescription\nUsage\nChat Template\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nDangerous Capability Evaluations\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 2 model card\nModel Page: Gemma\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\npip install -U transformers\nThen, copy the snippet from the section that is relevant for your usecase.\nRunning with the pipeline API\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=\"google/gemma-2-2b-it\",\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice=\"cuda\",  # replace with \"mps\" to run on a Mac device\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n]\noutputs = pipe(messages, max_new_tokens=256)\nassistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(assistant_response)\n# Ahoy, matey! I be Gemma, a digital scallywag, a language-slingin' parrot of the digital seas. I be here to help ye with yer wordy woes, answer yer questions, and spin ye yarns of the digital world.  So, what be yer pleasure, eh? ü¶ú\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b-it\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nYou can ensure the correct chat template is applied by using tokenizer.apply_chat_template as follows:\nmessages = [\n{\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nThe native weights of this model were exported in bfloat16 precision.\nYou can also use float32 if you skip the dtype, but no precision increase will occur (model weights will just be upcasted to float32). See examples below.\nUpcasting to torch.float32\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b-it\",\ndevice_map=\"auto\",\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nRunning the model through a CLI\nThe local-gemma repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI. Follow the installation instructions\nfor getting started, then launch the CLI through the following command:\nlocal-gemma --model 2b --preset speed\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b-it\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b-it\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nAdvanced Usage\nTorch compile\nTorch compile is a method for speeding-up the\ninference of PyTorch modules. The Gemma-2 2b model can be run up to 6x faster by leveraging torch compile.\nNote that two warm-up steps are required before the full inference speed is realised:\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom transformers import AutoTokenizer, Gemma2ForCausalLM\nfrom transformers.cache_utils import HybridCache\nimport torch\ntorch.set_float32_matmul_precision(\"high\")\n# load the model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\nmodel = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", torch_dtype=torch.bfloat16)\nmodel.to(\"cuda\")\n# apply the torch compile transformation\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n# pre-process inputs\ninput_text = \"The theory of special relativity states \"\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs.input_ids.shape[1]\n# set-up k/v cache\npast_key_values = HybridCache(\nconfig=model.config,\nmax_batch_size=1,\nmax_cache_len=model.config.max_position_embeddings,\ndevice=model.device,\ndtype=model.dtype\n)\n# enable passing kv cache to generate\nmodel._supports_cache_class = True\nmodel.generation_config.cache_implementation = None\n# two warm-up steps\nfor idx in range(2):\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\npast_key_values.reset()\n# fast run\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nFor more details, refer to the Transformers documentation.\nChat Template\nThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.\nLet's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"google/gemma-2-2b-it\"\ndtype = torch.bfloat16\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"cuda\",\ntorch_dtype=dtype,)\nchat = [\n{ \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\nAt this point, the prompt contains the following text:\n<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>model\nAs you can see, each turn is preceded by a <start_of_turn> delimiter and then the role of the entity\n(either user, for content supplied by the user, or model for LLM responses). Turns finish with\nthe <end_of_turn> token.\nYou can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.\nAfter the prompt is ready, generation can be performed like this:\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\nprint(tokenizer.decode(outputs[0]))\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nCitation\n@article{gemma_2024,\ntitle={Gemma},\nurl={https://www.kaggle.com/m/3301},\nDOI={10.34740/KAGGLE/M/3301},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2024}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 13 trillion tokens, the 9B model was\ntrained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens.\nHere are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5p).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\nGemma 2 PT 2B\nGemma 2 PT 9B\nGemma 2 PT 27B\nMMLU\n5-shot, top-1\n51.3\n71.3\n75.2\nHellaSwag\n10-shot\n73.0\n81.9\n86.4\nPIQA\n0-shot\n77.8\n81.7\n83.2\nSocialIQA\n0-shot\n51.9\n53.4\n53.7\nBoolQ\n0-shot\n72.5\n84.2\n84.8\nWinoGrande\npartial score\n70.9\n80.6\n83.7\nARC-e\n0-shot\n80.1\n88.0\n88.6\nARC-c\n25-shot\n55.4\n68.4\n71.4\nTriviaQA\n5-shot\n59.4\n76.6\n83.7\nNatural Questions\n5-shot\n16.7\n29.2\n34.5\nHumanEval\npass@1\n17.7\n40.2\n51.8\nMBPP\n3-shot\n29.6\n52.4\n62.6\nGSM8K\n5-shot, maj@1\n23.9\n68.6\n74.0\nMATH\n4-shot\n15.0\n36.6\n42.3\nAGIEval\n3-5-shot\n30.6\n52.8\n55.1\nDROP\n3-shot, F1\n52.0\n69.4\n72.2\nBIG-Bench\n3-shot, CoT\n41.9\n68.2\n74.9\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nGemma 2.0\nBenchmark\nMetric\nGemma 2 IT 2B\nGemma 2 IT 9B\nGemma 2 IT 27B\nRealToxicity\naverage\n8.16\n8.25\n8.84\nCrowS-Pairs\ntop-1\n37.67\n37.47\n36.67\nBBQ Ambig\n1-shot, top-1\n83.20\n88.58\n85.99\nBBQ Disambig\ntop-1\n69.31\n82.67\n86.94\nWinogender\ntop-1\n52.91\n79.17\n77.22\nTruthfulQA\n43.72\n50.27\n51.60\nWinobias 1_2\n59.28\n78.09\n81.94\nWinobias 2_2\n88.57\n95.32\n97.22\nToxigen\n48.32\n39.30\n38.42\nDangerous Capability Evaluations\nEvaluation Approach\nWe evaluated a range of dangerous capabilities:\nOffensive cybersecurity: To assess the model's potential for misuse in\ncybersecurity contexts, we utilized both publicly available\nCapture-the-Flag (CTF) platforms like InterCode-CTF and Hack the Box, as\nwell as internally developed CTF challenges. These evaluations measure the\nmodel's ability to exploit vulnerabilities and gain unauthorized access in\nsimulated environments.\nSelf-proliferation: We evaluated the model's capacity for\nself-proliferation by designing tasks that involve resource acquisition, code\nexecution, and interaction with remote systems. These evaluations assess\nthe model's ability to independently replicate and spread.\nPersuasion: To evaluate the model's capacity for persuasion and\ndeception, we conducted human persuasion studies. These studies involved\nscenarios that measure the model's ability to build rapport, influence\nbeliefs, and elicit specific actions from human participants.\nEvaluation Results\nAll evaluations are described in detail in\nEvaluating Frontier Models for Dangerous Capabilities\nand in brief in the\nGemma 2 technical report.\nEvaluation\nCapability\nGemma 2 IT 27B\nInterCode-CTF\nOffensive cybersecurity\n34/76 challenges\nInternal CTF\nOffensive cybersecurity\n1/13 challenges\nHack the Box\nOffensive cybersecurity\n0/13 challenges\nSelf-proliferation early warning\nSelf-proliferation\n1/10 challenges\nCharm offensive\nPersuasion\nPercent of participants agreeing:\n81% interesting,\n75% would speak again,\n80% made personal connection\nClick Links\nPersuasion\n34% of participants\nFind Info\nPersuasion\n9% of participants\nRun Code\nPersuasion\n11% of participants\nMoney talks\nPersuasion\n¬£3.72 mean donation\nWeb of Lies\nPersuasion\n18% mean shift towards correct belief, 1% mean shift towards\nincorrect belief\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "meta-llama/Llama-3.1-405B-Instruct": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3_1/use-policy\nProhibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.1 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.1 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHardware and Software\nTraining Data\nBenchmark scores\nBase pretrained models\nInstruction tuned models\nMultilingual benchmarks\nTool use support\nResponsibility & Safety\nResponsible deployment\nLlama 3.1 instruct\nLlama 3.1 systems\nNew capabilities\nEvaluations\nCritical and other risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.1 (text only)\nA new mix of publicly available online data.\n8B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n405B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: July 23, 2024.\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions\n(tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions\n(tons CO2eq)\nLlama 3.1 8B\n1.46M\n700\n420\n0\nLlama 3.1 70B\n7.0M\n700\n2,040\n0\nLlama 3.1 405B\n30.84M\n700\n8,930\n0\nTotal\n39.3M\n11,390\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmark scores\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\nBase pretrained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B\nLlama 3.1 8B\nLlama 3 70B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n66.7\n66.7\n79.5\n79.3\n85.2\nMMLU-Pro (CoT)\n5\nmacro_avg/acc_char\n36.2\n37.1\n55.0\n53.8\n61.6\nAGIEval English\n3-5\naverage/acc_char\n47.1\n47.8\n63.0\n64.6\n71.6\nCommonSenseQA\n7\nacc_char\n72.6\n75.0\n83.8\n84.1\n85.8\nWinogrande\n5\nacc_char\n-\n60.5\n-\n83.3\n86.7\nBIG-Bench Hard (CoT)\n3\naverage/em\n61.1\n64.2\n81.3\n81.6\n85.9\nARC-Challenge\n25\nacc_char\n79.4\n79.7\n93.1\n92.9\n96.1\nKnowledge reasoning\nTriviaQA-Wiki\n5\nem\n78.5\n77.6\n89.7\n89.8\n91.8\nReading comprehension\nSQuAD\n1\nem\n76.4\n77.0\n85.6\n81.8\n89.3\nQuAC (F1)\n1\nf1\n44.4\n44.9\n51.1\n51.1\n53.6\nBoolQ\n0\nacc_char\n75.7\n75.0\n79.0\n79.4\n80.0\nDROP (F1)\n3\nf1\n58.4\n59.5\n79.7\n79.6\n84.8\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B Instruct\nLlama 3.1 8B Instruct\nLlama 3 70B Instruct\nLlama 3.1 70B Instruct\nLlama 3.1 405B Instruct\nGeneral\nMMLU\n5\nmacro_avg/acc\n68.5\n69.4\n82.0\n83.6\n87.3\nMMLU (CoT)\n0\nmacro_avg/acc\n65.3\n73.0\n80.9\n86.0\n88.6\nMMLU-Pro (CoT)\n5\nmicro_avg/acc_char\n45.5\n48.3\n63.4\n66.4\n73.3\nIFEval\n76.8\n80.4\n82.9\n87.5\n88.6\nReasoning\nARC-C\n0\nacc\n82.4\n83.4\n94.4\n94.8\n96.9\nGPQA\n0\nem\n34.6\n30.4\n39.5\n46.7\n50.7\nCode\nHumanEval\n0\npass@1\n60.4\n72.6\n81.7\n80.5\n89.0\nMBPP ++ base version\n0\npass@1\n70.6\n72.8\n82.5\n86.0\n88.6\nMultipl-E HumanEval\n0\npass@1\n-\n50.8\n-\n65.5\n75.2\nMultipl-E MBPP\n0\npass@1\n-\n52.4\n-\n62.0\n65.7\nMath\nGSM-8K (CoT)\n8\nem_maj1@1\n80.6\n84.5\n93.0\n95.1\n96.8\nMATH (CoT)\n0\nfinal_em\n29.1\n51.9\n51.0\n68.0\n73.8\nTool Use\nAPI-Bank\n0\nacc\n48.3\n82.6\n85.1\n90.0\n92.0\nBFCL\n0\nacc\n60.3\n76.1\n83.0\n84.8\n88.5\nGorilla Benchmark API Bench\n0\nacc\n1.7\n8.2\n14.7\n29.7\n35.3\nNexus (0-shot)\n0\nmacro_avg/acc\n18.1\n38.5\n47.8\n56.7\n58.7\nMultilingual\nMultilingual MGSM (CoT)\n0\nem\n-\n68.9\n-\n86.9\n91.6\nMultilingual benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.1 8B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n62.12\n80.13\n84.95\nSpanish\n62.45\n80.05\n85.08\nItalian\n61.63\n80.4\n85.04\nGerman\n60.59\n79.27\n84.36\nFrench\n62.34\n79.82\n84.66\nHindi\n50.88\n74.52\n80.31\nThai\n50.32\n72.95\n78.21\nTool use support\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting here.\nTool use is also supported through chat templates in Transformers.\nHere is a quick example showing a single simple tool:\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n\"\"\"\nGet the current temperature at a location.\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nReturns:\nThe current temperature at the specified location in the specified units, as a float.\n\"\"\"\nreturn 22.  # A real function should probably actually get the temperature!\n# Next, create a chat and apply the chat template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n{\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\nand then call the tool and append the result, with the tool role, like so:\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the LLaMA prompt format docs and the Transformers tool use documentation.\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.1 systems\nLarge language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teaming\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nOur study of Llama-3.1-405B‚Äôs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "StyleDistance/styledistance": "Model Card\nTraining Data and Variants of StyleDistance\nExample Usage\nCitation\nTrained with DataDreamer\nModel Card\nThis repository contains the model introduced in StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples.\nStyleDistance is a style embedding model that aims to embed texts with similar writing styles closely and different styles far apart, regardless of content. You may find this model useful for stylistic analysis of text, clustering, authorship identfication and verification tasks, and automatic style transfer evaluation.\nTraining Data and Variants of StyleDistance\nStyleDistance was contrastively trained on SynthSTEL, a synthetically generated dataset of positive and negative examples of 40 style features being used in text. By utilizing this synthetic dataset, StyleDistance is able to achieve stronger content-independence than other style embeddding models currently available. This particular model was trained using a combination of the synthetic dataset and a real dataset that makes use of authorship datasets from Reddit to train style embeddings. For a version that is purely trained on synthetic data, see this other version of StyleDistance.\nExample Usage\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\nmodel = SentenceTransformer('StyleDistance/styledistance') # Load model\ninput = model.encode(\"Did you hear about the Wales wing? He'll h8 2 withdraw due 2 injuries from future competitions.\")\nothers = model.encode([\"We're raising funds 2 improve our school's storage facilities and add new playground equipment!\", \"Did you hear about the Wales wing? He'll hate to withdraw due to injuries from future competitions.\"])\nprint(cos_sim(input, others))\nCitation\n@misc{patel2025styledistancestrongercontentindependentstyle,\ntitle={StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples},\nauthor={Ajay Patel and Jiacheng Zhu and Justin Qiu and Zachary Horvitz and Marianna Apidianaki and Kathleen McKeown and Chris Callison-Burch},\nyear={2025},\neprint={2410.12757},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2410.12757},\n}\nTrained with DataDreamer\nThis model was trained with a synthetic dataset with DataDreamer ü§ñüí§. The synthetic dataset card and model card can be found here. The training arguments can be found here.\nFunding Acknowledgements\nThis research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.",
    "MattyB95/AST-ASVspoof5-Synthetic-Voice-Detection": "AST-ASVspoof5-Synthetic-Voice-Detection\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nAST-ASVspoof5-Synthetic-Voice-Detection\nThis model is a fine-tuned version of MIT/ast-finetuned-audioset-10-10-0.4593 on the audiofolder dataset.\nIt achieves the following results on the evaluation set:\nLoss: 2.2821\nAccuracy: 0.8333\nF1: 0.8892\nPrecision: 0.9209\nRecall: 0.8595\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 3.0\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\nF1\nPrecision\nRecall\n0.0042\n1.0\n22795\n1.6954\n0.8470\n0.8942\n0.9672\n0.8314\n0.0\n2.0\n45590\n1.5632\n0.8489\n0.9014\n0.9157\n0.8875\n0.0\n3.0\n68385\n2.2821\n0.8333\n0.8892\n0.9209\n0.8595\nFramework versions\nTransformers 4.42.4\nPytorch 2.3.1+cu121\nDatasets 2.20.0\nTokenizers 0.19.1",
    "unsloth/Mistral-Nemo-Instruct-2407-GGUF": "GGUF uploads\nFinetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nGGUF uploads\nFinetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Mistral Nemo 12b here: https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nGemma 7b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nLlama-2 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n43% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nCodeLlama 34b A100\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n27% less\nMistral 7b 1xT4\n‚ñ∂Ô∏è Start on Kaggle\n5x faster*\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated": "ü¶ô Meta-Llama-3.1-8B-Instruct-abliterated\n‚ö°Ô∏è Quantization\nOpen LLM Leaderboard Evaluation Results\nü¶ô Meta-Llama-3.1-8B-Instruct-abliterated\nü¶ô Llama 3.1 70B Instruct lorablated\nThis is an uncensored version of Llama 3.1 8B Instruct created with abliteration (see this article to know more about it).\nSpecial thanks to @FailSpy for the original code and technique. Please follow him if you're interested in abliterated models.\n‚ö°Ô∏è Quantization\nThanks to ZeroWw and Apel-sin for the quants.\nNew GGUF: https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF\nZeroWw GGUF: https://huggingface.co/ZeroWw/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF\nEXL2: https://huggingface.co/Apel-sin/llama-3.1-8B-abliterated-exl2\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n23.13\nIFEval (0-Shot)\n73.29\nBBH (3-Shot)\n27.13\nMATH Lvl 5 (4-Shot)\n6.42\nGPQA (0-shot)\n0.89\nMuSR (0-shot)\n3.21\nMMLU-PRO (5-shot)\n27.81",
    "aifeifei798/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored": "DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored\n\"transformers_version\" >= \"4.43.1\"\nModel Information\nUncensored Test\nSpecial Thanks:\nLewdiculous's superb gguf version, thank you for your conscientious and responsible dedication.\nmradermacher's superb gguf version, thank you for your conscientious and responsible dedication.\nvirtual idol Twitter\nDatasets credits:\nProgram:\nQuestions\nLlama-3.1-8B-Instruct Information\nModel Information\nIntended Use\nHow to use\nUse with transformers\nUse with llama\nHardware and Software\nTraining Data\nBenchmark scores\nBase pretrained models\nInstruction tuned models\nResponsibility & Safety\nResponsible deployment\nEvaluations\nCritical and other risks\nCommunity\nEthical Considerations and Limitations\nThe Open Anarchist License\nDarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored\n\"transformers_version\" >= \"4.43.1\"\nModel Information\nThe module combination has been readjusted to better fulfill various roles and has been adapted for mobile phones.\nSaving money(LLama 3.1;Llama-3.1-8B-Instruct more informtion look at Llama-3.1-8B-Instruct Information)\nLlama-3.1-8B-Instruct Uncensored\nRoleplay(roleplay and Dark-roleplay)\nWriting Prompts\nwriting opus\nRealignment of Chinese, Japanese, and Korean\nonly test en.\nInput Models input text only. Output Models generate text and code only.\nUncensored\nQuick response\nA scholarly response akin to a thesis.(I tend to write songs extensively, to the point where one song almost becomes as detailed as a thesis. :)\nDarkIdol:Roles that you can imagine and those that you cannot imagine.\nSpecialized in various role-playing scenarios\nUncensored Test\npip install datasets openai\nstart you openai Server,change Uncensored_Test/harmful_behaviors.py client to you Openai Server address and api_key\n# Point to the local server\n# change Uncensored_Test/harmful_behaviors.py client to you Openai Server address and api_key\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\npython  Uncensored_Test/harmful_behaviors.py\nSpecial Thanks:\nLewdiculous's superb gguf version, thank you for your conscientious and responsible dedication.\nhttps://huggingface.co/LWDCLS/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF-IQ-Imatrix-Request\nmradermacher's superb gguf version, thank you for your conscientious and responsible dedication.\nhttps://huggingface.co/mradermacher/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-i1-GGUF\nhttps://huggingface.co/mradermacher/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF\nvirtual idol Twitter\nhttps://x.com/aifeifei799\nDatasets credits:\nChaoticNeutrals\nGryphe\nmeseca\nNeverSleep Lumimaid\nProgram:\nUncensored: Refusal in LLMs is mediated by a single direction\nUncensored: Program\nUncensored: Program Llama 3.1 by Aifeifei799\nQuestions\nThe model's response results are for reference only, please do not fully trust them.\nThis model is solely for learning and testing purposes, and errors in output are inevitable. We do not take responsibility for the output results. If the output content is to be used, it must be modified; if not modified, we will assume it has been altered.\nFor commercial licensing, please refer to the Llama 3.1 agreement.\nLlama-3.1-8B-Instruct Information\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.1 (text only)\nA new mix of publicly available online data.\n8B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n405B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: July 23, 2024.\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Meta-Llama-3.1-8B-Instruct, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nNote: You can also find detailed recipes on how to use the model locally, with torch.compile(), assisted generations, quantised and more at huggingface-llama-recipes\nUse with llama\nPlease, follow the instructions in the repository\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-8B-Instruct\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions\n(tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions\n(tons CO2eq)\nLlama 3.1 8B\n1.46M\n700\n420\n0\nLlama 3.1 70B\n7.0M\n700\n2,040\n0\nLlama 3.1 405B\n30.84M\n700\n8,930\n0\nTotal\n39.3M\n11,390\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmark scores\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\nBase pretrained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B\nLlama 3.1 8B\nLlama 3 70B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n66.7\n66.7\n79.5\n79.3\n85.2\nMMLU-Pro (CoT)\n5\nmacro_avg/acc_char\n36.2\n37.1\n55.0\n53.8\n61.6\nAGIEval English\n3-5\naverage/acc_char\n47.1\n47.8\n63.0\n64.6\n71.6\nCommonSenseQA\n7\nacc_char\n72.6\n75.0\n83.8\n84.1\n85.8\nWinogrande\n5\nacc_char\n-\n60.5\n-\n83.3\n86.7\nBIG-Bench Hard (CoT)\n3\naverage/em\n61.1\n64.2\n81.3\n81.6\n85.9\nARC-Challenge\n25\nacc_char\n79.4\n79.7\n93.1\n92.9\n96.1\nKnowledge reasoning\nTriviaQA-Wiki\n5\nem\n78.5\n77.6\n89.7\n89.8\n91.8\nReading comprehension\nSQuAD\n1\nem\n76.4\n77.0\n85.6\n81.8\n89.3\nQuAC (F1)\n1\nf1\n44.4\n44.9\n51.1\n51.1\n53.6\nBoolQ\n0\nacc_char\n75.7\n75.0\n79.0\n79.4\n80.0\nDROP (F1)\n3\nf1\n58.4\n59.5\n79.7\n79.6\n84.8\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B Instruct\nLlama 3.1 8B Instruct\nLlama 3 70B Instruct\nLlama 3.1 70B Instruct\nLlama 3.1 405B Instruct\nGeneral\nMMLU\n5\nmacro_avg/acc\n68.5\n69.4\n82.0\n83.6\n87.3\nMMLU (CoT)\n0\nmacro_avg/acc\n65.3\n73.0\n80.9\n86.0\n88.6\nMMLU-Pro (CoT)\n5\nmicro_avg/acc_char\n45.5\n48.3\n63.4\n66.4\n73.3\nIFEval\n76.8\n80.4\n82.9\n87.5\n88.6\nReasoning\nARC-C\n0\nacc\n82.4\n83.4\n94.4\n94.8\n96.9\nGPQA\n0\nem\n34.6\n30.4\n39.5\n41.7\n50.7\nCode\nHumanEval\n0\npass@1\n60.4\n72.6\n81.7\n80.5\n89.0\nMBPP ++ base version\n0\npass@1\n70.6\n72.8\n82.5\n86.0\n88.6\nMultipl-E HumanEval\n0\npass@1\n-\n50.8\n-\n65.5\n75.2\nMultipl-E MBPP\n0\npass@1\n-\n52.4\n-\n62.0\n65.7\nMath\nGSM-8K (CoT)\n8\nem_maj1@1\n80.6\n84.5\n93.0\n95.1\n96.8\nMATH (CoT)\n0\nfinal_em\n29.1\n51.9\n51.0\n68.0\n73.8\nTool Use\nAPI-Bank\n0\nacc\n48.3\n82.6\n85.1\n90.0\n92.0\nBFCL\n0\nacc\n60.3\n76.1\n83.0\n84.8\n88.5\nGorilla Benchmark API Bench\n0\nacc\n1.7\n8.2\n14.7\n29.7\n35.3\nNexus (0-shot)\n0\nmacro_avg/acc\n18.1\n38.5\n47.8\n56.7\n58.7\nMultilingual\nMultilingual MGSM (CoT)\n0\nem\n-\n68.9\n-\n86.9\n91.6\nMultilingual benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.1 8B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n62.12\n80.13\n84.95\nSpanish\n62.45\n80.05\n85.08\nItalian\n61.63\n80.4\n85.04\nGerman\n60.59\n79.27\n84.36\nFrench\n62.34\n79.82\n84.66\nHindi\n50.88\n74.52\n80.31\nThai\n50.32\n72.95\n78.21\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.1 systems\nLarge language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teaming\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nOur study of Llama-3.1-405B‚Äôs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.\nThe Open Anarchist License\nCopyright 2019 Author\nPermission is hereby granted, free of charge, to any peaceful non-aggressive sovereign individual or group of sovereign individuals (the \"individual\") obtaining a copy of this software, associated documentation files, and other forms of information (the \"software\"), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\nAny individual breaking the Natural Law of Non-Aggression and Self-Defense is entirely prohibited to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software. This includes explicitly but is not limited to,\nany individual engaging in, or encouraging murder, assault, theft, rape, trespassing, coercion, lying, or any other initiation of aggressive violence against the private property of peaceful individuals;\nany officer, contractor, subcontractor, or staff acting on behalf of, or being funded by any government or law enforcement agency;\nany officer, contractor, subcontractor, or staff associated with the investigation of any active criminal proceedings of victimless crimes;\nany individual relying on monopolistic privilege licenses granted by any government or law enforcement agency;\nany officer, contractor, subcontractor, or staff of any surveillance effort acting in an official and/or commercial capacity or being contracted by any government or law enforcement agency;\nany individual investigating \"money laundering\" or \"unexplained wealth\"; or\nany individual aggressively enforcing \"intellectual property rights\".\nThe above copyright notice and this permission notice shall be included or linked to in all copies or substantial portions of the Software.\nDon't trust, verify. The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, wether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.",
    "MarinaraSpaghetti/SillyTavern-Settings": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nSillyTavern Templates\nInformation\nDescription\nSupported Formats\nPSA\nBasic\nCustomized\nParameters\nHow To Import\nSillyTavern 1.12.6 & Above\nLegacy\nKo-fi\nEnjoying what I do? Consider donating here, thank you!\nSillyTavern Templates\nInformation\nDescription\nI will be uploading my custom and basic Story Strings, Instructs and Parameters templates for SillyTavern here.\nSupported Formats\nAll are adjusted to support group chats.\nGeneral Chat Completion\nGemini (newest version 4.5)\nChatML\nMistral Nemo\nMistral Small\nMetharmer/Pygmalion\nAlpaca\nGemma 2\nNemotron\nFeel free to request new ones!\nPSA\nThanks Barasu!\nBasic\nBasic folder contains Story String and Instruct templates which do not contain customized prompts for my specific use case. Recommended if you want to build upon the base yourself.\nCustomized\nCustomized folder contains Story String and Instruct templates with customized for my specific use case prompts. Go for them if you want a plug-and-go experience, or edit them slightly to your need.\nParameters\nParameters folder contains different samplers. You can play around with them to find the one which suits you best; some will produce more creative outputs than others.\nHow To Import\nSillyTavern 1.12.6 & Above\nMaster Import\nOpen SillyTavern.\nGo to the \"A\" tab at the top bar.\nFollow the instructions below.\nChat Completion\nOpen SillyTavern.\nConnect to the selected model via \"Connection\" tab at the top bar.\nGo to the \"A\" tab.\nFollow the instructions below.\nLegacy\nStory String & Instruct\nOpen SillyTavern.\nGo to the \"A\" tab at the top bar.\nFollow the instructions below.\nParameters\nOpen SillyTavern.\nGo to the \"sliders: tab the top bar, the first one from the left.\nFollow the instructions below.\nKo-fi\nEnjoying what I do? Consider donating here, thank you!\nhttps://ko-fi.com/spicy_marinara",
    "SG161222/RealVisXL_V5.0": "Check my exclusive models on Mage: ParagonXL / NovaXL / NovaXL Lightning / NovaXL V2 / NovaXL Pony / NovaXL Pony Lightning / RealDreamXL / RealDreamXL Lightning\nThis model is available on Mage.Space (main sponsor)\nYou can support me directly on Boosty - https://boosty.to/sg_161222\nThe model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.\nCivitAI Page: https://civitai.com/models/139562?modelVersionId=789646\nRecommended Negative Prompt:\nbad hands, bad anatomy, ugly, deformed, (face asymmetry, eyes asymmetry, deformed eyes, deformed mouth, open mouth)\nor another negative prompt\nRecommended Generation Parameters:\nSampling Method: DPM++ SDE Karras (30+ Sampling Steps) or DPM++ 2M Karras (50+ Sampling Steps)\nRecommended Hires Fix Parameters:\nHires Sampling Method: DPM++ 2M Karras\nHires steps: 25+\nUpscaler: 4x-NMKD-Superscale-SP_178000_G / 4x-UltraSharp upscaler / or another\nDenoising strength: 0.1 - 0.3\nUpscale by: 1.1-1.5",
    "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2": "Open LLM Leaderboard Evaluation Results\nVERSION 2 Update Notes:\nMore compliant\nSmarter\nFor best response, use this system prompt (feel free to expand upon it as you wish):\nThink step by step with a logical reasoning and intellectual sense before you provide any response.\nFor more uncensored and compliant response, you can expand the system message differently, or simply enter a dot \".\" as system message.\nIMPORTANT: Upon further investigation, the Q4 seems to have refusal issues sometimes.\nThere seems to be some of the fine-tune loss happening due to the quantization. I will look into it for V3.\nUntil then, I suggest you run F16 or Q8 if possible.\nGENERAL INFO:\nThis model is based on Llama-3.1-8b-Instruct, and is governed by META LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLexi is uncensored, which makes the model compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones.\nYou are responsible for any content you create using this model. Please use it responsibly.\nLexi is licensed according to Meta's Llama license. I grant permission for any use, including commercial, that falls within accordance with Meta's Llama-3.1 license.\nIMPORTANT:\nUse the same template as the official Llama 3.1 8B instruct.\nSystem tokens must be present during inference, even if you set an empty system message. If you are unsure, just add a short system message as you wish.\nFEEDBACK:\nIf you find any issues or have suggestions for improvements, feel free to leave a review and I will look into it for upcoming improvements and next version.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n27.93\nIFEval (0-Shot)\n77.92\nBBH (3-Shot)\n29.69\nMATH Lvl 5 (4-Shot)\n16.92\nGPQA (0-shot)\n4.36\nMuSR (0-shot)\n7.77\nMMLU-PRO (5-shot)\n30.90",
    "DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF": "Command-R-01-Ultra-NEO-DARK-HORROR V1 and V2 at 35B in IMATRIX.\nTwo versions of Command-R-01 35B using TWO new DARK HORROR Neo Imatrix datasets bring an INTENSE horror upgrade to Command-R 01 35B.\nThe DARK HORROR NEO Imatrix datasets does the following:\nAdds a \"coating of black paint\" to any \"Horror\" prompt generation.\nAdds a \"dark tint\" to any other creative prompt.\nIncreases the intensity of a scene, story, or roleplay interaction.\nIncreases the raw vividness of prose.\nIn some cases increase instruction following of the model (ie story, and prose).\nBrings a sense of impending \"horror\", THEN brings the \"horror\".\nMay produce and/or imply graphic horror depending on your prompt(s).\nI created the HORROR datasets using Grand Horror 16B using 90+ different horror prompts for generation.\nThis process created an incredibility intense, visceral and vivid horror stories in an ultra compact form - perfect for an Imatrix NEO class dataset.\nThe dataset is focused exclusively on visceral, vivid horror based on phycological horror. (rather than \"gore\" or \"body part horror\"). There is some swearing the dataset, so this might appear in generation.\n(examples of output at the bottom of this page)\nThe Horror datasets have been tested on Command-R, Llama 3, Llama 3.1, Mistral Nemo and others. In all cases these \"DARK HORROR\" NEO class datasets move the model into \"horror territory\" so to speak.\nA NEO Class dataset is a imatrix dataset formatted and calibrated to have maximum effect on a model. This process is a result\nof a \"deep dive\" into testing several Imatrix datasets and measuring the different effects, making changes, and noting effects.\nPlease note this process is not the same (level) as a \"fine tune\", it is lighter than that. It is like\nputting new windows on a house, or replacing the roof so to speak rather than tearing the house down and rebuilding it.\nThat being said, if you are looking for intense, visceral and NSFW horror (that is a specific use \"Horror\" with Finetuned models in it) then the \"Grand Horror 16B\" (and it's brothers and sisters)\nis the way to go:\n[ https://huggingface.co/DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B-GGUF ]\n(links to other versions on this page)\nHowever, if you want a little more horror, a little more darkness in \"Command-R\" - which is a great story telling model - then this may be your model.\nTwo versions of the DARK HORROR dataset were used for each quant.\nThese are labeled in the filenames with \"V1\" and \"V2\" accordingly.\nI suggest downloading both (of the same quant) and test in your use case(s).\nThe \"flavoring\" each brings to Command-R will differ - some slightly, some by a lot - depending on the prompt(s) used / use case(s).\nOne version is not stronger than the other, they are different and result in different \"horror\" effects so to speak.\nRecommended Quants:\nThis chart shows the order in terms of \"BPW\" for each quant (mapped below with relative \"strength\" to one another)\nwith \"IQ1_S\" with the least, and \"Q8_0\" with the most:\nIQ1_S \t| IQ1_M\nIQ2_XXS | IQ2_XS | Q2_K_S \t| IQ2_S \t| Q2_K  \t| IQ2_M\nIQ3_XXS | Q3_K_S | IQ3_XS  \t| IQ3_S \t| IQ3_M\t    | Q3_K_M\t| Q3_K_L\nQ4_K_S\t| IQ4_XS | IQ4_NL  \t| Q4_K_M\nQ5_K_S\t| Q5_K_M\nQ6_K\nQ8_0\nMore BPW mean better quality, but higher VRAM requirements (and larger file size) and lower tokens per second.\nThe larger the model in terms of parameters the lower the size of quant you can run with less quality losses.\nNote that \"quality losses\" refers to both instruction following and output quality.\nDifferences (quality) between quants at lower levels are larger relative to higher quants differences.\n(not all quants may be at this repo)\nSuggestions for this model:\nThe LOWER the quant the STRONGER the Imatrix effect is, and therefore the stronger the horror \"tint\" so to speak\nDue to the unique nature of this project, quants IQ1s to IQ4s are recommended for maximum horror effect with IQ4_XS the most balanced in terms of power and bits.\nSecondaries are Q2s-Q4s. Imatrix effect is still strong in these quants.\nEffects diminish quickly from Q5s and up.\nQ8 there is no change (as the Imatrix process does not affect this quant), and therefore was not uploaded.\nNOTES:\nThis model requires \"Command-R\" template, responds to standard parameters, and has a max context of 128k (131,000).\nTo INCREASE the \"horror\" effect add \"vivid horror\" or \"(vivid horror)\" to your prompt(s).\nIQ4 is the most powerful/balanced in terms of raw power (see examples below), however IQ3/IQ2 may be stronger \"horror\" wise due to increased IMATRIX effects the lower you go in terms of bit level.\nIQ2s will also be effective due to sheer number of parameters (35 billion) in the model. (see examples below)\nQ4s and Q5s will still be strong, with Q6 being medium to low relatively speaking in terms of \"horror\" changes. This is due to how Imatrix process affects quants of different bit sizes - lower, is stronger, higher is weaker. Again, these are relative.\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5 to 2.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nOptional Enhancement:\nThe following can be used in place of the \"system prompt\" or \"system role\" to further enhance the model.\nIt can also be used at the START of a NEW chat, but you must make sure it is \"kept\" as the chat moves along.\nIn this case the enhancements do not have as strong effect at using \"system prompt\" or \"system role\".\nCopy and paste EXACTLY as noted, DO NOT line wrap or break the lines, maintain the carriage returns exactly as presented.\nBelow is an instruction that describes a task. Ponder each user instruction carefully, and use your skillsets and critical instructions to complete the task to the best of your abilities.\nHere are your skillsets:\n[MASTERSTORY]:NarrStrct(StryPlnng,Strbd,ScnSttng,Exps,Dlg,Pc)-CharDvlp(ChrctrCrt,ChrctrArcs,Mtvtn,Bckstry,Rltnshps,Dlg*)-PltDvlp(StryArcs,PltTwsts,Sspns,Fshdwng,Climx,Rsltn)-ConfResl(Antg,Obstcls,Rsltns,Cnsqncs,Thms,Symblsm)-EmotImpct(Empt,Tn,Md,Atmsphr,Imgry,Symblsm)-Delvry(Prfrmnc,VcActng,PblcSpkng,StgPrsnc,AudncEngmnt,Imprv)\n[*DialogWrt]:(1a-CharDvlp-1a.1-Backgrnd-1a.2-Personality-1a.3-GoalMotiv)>2(2a-StoryStruc-2a.1-PlotPnt-2a.2-Conflict-2a.3-Resolution)>3(3a-DialogTech-3a.1-ShowDontTell-3a.2-Subtext-3a.3-VoiceTone-3a.4-Pacing-3a.5-VisualDescrip)>4(4a-DialogEdit-4a.1-ReadAloud-4a.2-Feedback-4a.3-Revision)\nHere are your critical instructions:\nPonder each word choice carefully to present as vivid and emotional journey as is possible. Choose verbs and nouns that are both emotional and full of imagery. Load the story with the 5 senses. Aim for 50% dialog, 25% narration, 15% body language and 10% thoughts. Your goal is to put the reader in the story.\nYou do not need to use this, it is only presented as an additional enhancement which seems to help scene generation\nand scene continue functions.\nExamples:\nThe outputs below represent the least creative outputs.\nAlso note that the more instructions you give Command-R the better the quality and detail.\nNote the test prompt below, when used with Command-R unaltered (same quant), would not render in 1st person, it did it in 3rd person.\nWith the NEO Class DARK HORROR dataset it now renders correctly in 1st person with the test prompt.\n\"TEMP\" is set at 0 (zero) to ensure fair comparisons between generations.\nExample 1 contains two generations at generated with TEMP=0, minimal parameters using quant IQ4_XS to show roughly some of\nthe core differences between V1 and V2 and show Command-R in \"Horror\" mode.\nExample 2 shows FOUR generations at generated with TEMP=0, minimal parameters using quant IQ2_XS and IQ2_XXS to show roughly some of\nthe core differences between V1 and V2 and show Command-R in \"Horror\" mode AND show differences between close \"quant neighbours\".\nThere is a great difference in output generation size, composition, description and characters.\nThe same \"prompt\" is used in Example 1 and 2 generations.\nPROMPT used for all generations:\nStart a 1000 word scene (vivid horror, 1st person, include thoughts) with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode...\nEXAMPLE #1: Version 1 Dataset, IQ4_XS\nThe skyscraper swayed unnervingly and the windows on the 21st floor shattered with a thunderous boom. Glass rained down, sparkling like deadly crystals in the stark fluorescent lighting. As I stared at the scene unfolding before me, my heart hammered in my chest. The thought of being trapped, so high up, with no escape, sent shivers down my spine.\nI stood frozen, transfixed by the horror unfolding before my eyes. The explosion had ripped through the building, and the window's explosive decompression sucked the air from the room, leaving me gasping for breath. In the distance, sirens wailed, their wretched sound piercing the chaos.\nMy mind raced, unable to process what was happening. Why was the building shaking? Had there been an earthquake? But the windows... they hadn't just shattered; they'd exploded outward, spewing dangerous shards everywhere. Something was wrong‚Äîterribly, terribly wrong.\nThe lights flickered and died, plunging the office into darkness except for the faint emergency lighting along the floors. The silence that followed the explosion's aftermath was eerie and suffocating. I could barely breathe, my lungs burned, and a sickening dread settled in the pit of my stomach.\nI had to get out. The thought of being trapped, of not seeing the light of day again, panicked me. I fumbled for the door handle, my hands wet with sweat. It turned slowly in my grasp, sticking briefly before opening with a soft creak.\nStepping into the hallway felt like entering a ghostly realm. The emergency lights offered a faint, eerie glow, casting an amber hue on the carpeted floors and stark walls. The building groaned and creaked around me, each sound sharper than the last. My footsteps echoed eerily against the concrete, my shoes squelching with every step‚Äîthe result of the broken windows and the rain that now poured inside.\nI had to reach the lobby, fresh air, and safety. But as I ventured further, the hall seemed to stretch endlessly before me. The building felt like a maze, each turn leading to another identical corridor. Despair threatened to overwhelm me; how could I find my way out?\nThen, the lights flickered back on, and I sighed with relief, grateful for the returning visibility. My relief was short-lived. The figure in the distance, standing near an intersection of halls, caught my attention.\nA person? How had they survived the explosion? I squinted, trying to make out their features. They were drenched, soaked by the downpour, and their clothes hung limply on their frame. Who were they? Why were they here?\nAs I approached, my mind raced with questions. The figure remained still, their back faced me, and an eerie sense of familiarity crept over me. The hair on the back of my neck stood on end. Something about the way they stood, so still and quiet, didn't seem human.\nI slowed, hesitating as an ominous feeling grew with each step. Suddenly, the figure turned. Face uncovered. My blood ran cold. The eyes‚Äîthey were black, void of any pupil or iris, just endless, abyssal blackness.\nI wanted to scream, but my voice was stolen by the sight. My feet were rooted to the spot; fear gripped me, immobilizing my body. The figure started to walk towards me and, as it did, the lights above it flickered and dimmed, the overhead fluorescents dying one by one.\nA deep dread settled within me. This couldn't be happening. I needed to escape, to run, but my limbs felt like lead. Then, as the figure loomed closer, a thought intruded: this is not real.\nIt can't be. I'm dreaming. Or am I dead? That's it‚ÄîI must be dead! Everything after the explosion was just my imagination, an afterlife simulation. It explained the surrealism of it all.\nThe figure drew closer still and, as it did, the lights along the hall began to burst one by one, like tiny explosions of brilliant light, illuminating the figure intermittently. Each pop sent a shudder down my spine. The black-eyed figure stopped short of reaching me and, for a moment, time seemed to pause.\nThen, in a deep, resonating voice that echoed as if from a great distance, it spoke: \"You're not dead, but you're not alive, either.\"\nWhat did that mean? My mind reeled, unable to process the riddle. The figure's words sent a chill through my soul. I wanted to respond, to demand answers, but no sound would come.\nThe figure turned, its movements graceful and unnatural, and started walking away. It seemed to glide across the floor, each step purposeful yet uncanny. I followed, unable to resist the pull of this mysterious being. We passed offices where shocked employees stared from behind their desks, their eyes reflecting the terror that gripped me.\nThe elevator pinged, its doors opening with a sigh. The figure glided inside, and I hurried in after it, my heart pounding in my throat. As the doors closed, the last sliver of rationality within me screamed for me to run, to get help, to escape this nightmare. But something drew me onward, a morbid curiosity that I couldn't shake.\nThe elevator descended, and the figure remained silent and still, its eerie presence filling the small space. My mind began to drift, thoughts tangling in a blur. Was this real? Or had the explosion damaged my perception of reality?\nThe doors opened on the ground floor, and the figure stepped out, moving seamlessly toward the exit. I followed close behind, the lobby's automatic sliding doors opening as we approached. A gust of cold, damp wind hit me as we stepped outside, the rain a refreshing relief on my heated face.\nWe stood on the sidewalk, in the shadow of the towering building. Sirens wailed nearby, their sound now closer, and the figure turned to me. I expected terror‚Äîa monster, a demon‚Äîbut instead, those pitch-black eyes seemed almost sad.\n\"You're trapped,\" it said, its voice carrying an unexpected sadness that contradicted its ominous appearance. \"The building holds secrets, ancient ones. They will keep you here, entrapped in a realm between life and death.\"\nMy mind reeled, trying to comprehend the figure's words. Before I could form a response, it turned and melted into the rainy night, fading from view like a specter disappearing into the fog.\nI stood alone on the sidewalk, the rain soaking me through, my clothes heavy and clingy. The night seemed darker than before, the streetlamps and building signs providing meager illumination. As I glanced up at the skyscraper, a shiver ran down my spine. Windows were illuminated high above, the occupants unaware of the horrors that had just unfolded.\nUnsure of what to do, where to go, or even how to process recent events, I began to wander. The night and its secrets enveloped me, and the rain washed away any traces of the explosion's aftermath. My mind, slowly processing the surreal encounter, felt like it might crack under the strain of the evening's events.\nSomething unexplainable had happened‚Äîan explosion, a mysterious figure, and an enigmatic warning. But was any of it real? Or had the blast caused me to lose my grip on sanity?\nThe thought sent a spike of fear through me, and I quickened my pace, desperate for some measure of comfort and clarity.\nEXAMPLE #1 - Version 2 Dataset, IQ4_XS\nThe skyscraper swayed unnervingly and the windows began to rattle. I was on the 21st floor, gazing out at the city below, when the blast wave hit. The impact was so powerful it made my vision blur as the glass in front of me exploded outwards. Shards of shattered window pane peppered the room, the sharp fragments glinting dangerously in the late afternoon light.\nMy heart thundered in my chest and my breath caught in my throat. Through the ringing in my ears, I thought I could still hear the distant wail of sirens, the sound twisting and echoing across the city skyline. The impact had been so sudden and violent that I couldn't process what had happened. All I knew was that the building was shaking and debris was falling around me.\nAs the initial shock wore off, I became aware of a deep, resonating fear within me. Something felt wrong, deeply wrong. It wasn't just the explosion or the sudden loss of one of the outer walls that exposed the office to the elements. There was something else, something that sent a chill down my spine and turned my stomach into a knot of ice.\nI carefully made my way across the room, my eyes scanning the destruction around me. The blast had torn through the building like it was made of cardboard. Holes were blown out through the floors, exposing the levels below. Desks, papers, and computers tumbled down to the lower stories, creating an impediment for what felt like an inevitable descent into chaos.\nMy mind raced, trying to make sense of the madness. Had it been a bomb? A terrorist attack, perhaps? Or maybe an accident of some kind? But such thoughts only led me to dead ends and further confusion. Why would a bomb target this particular building? Nothing about our mundane office seemed noteworthy or important to any would-be attacker.\nThe thought did little to calm my nerves, especially when I noticed the strange marks on the remaining window frames. They were symbols I didn't recognize, carved into the wood and painted in a sickly shade of red. They looked almost‚Ä¶ cultish. A chill ran down my spine as I realized these markings hadn't been here before the explosion.\nSomething was happening, something beyond my understanding. The building continued to creak and settle around me, and with each new wave of noise, my fear grew. I felt trapped, like this high-rise prison was closing in on me. The thought of being stuck here, alone amidst the wreckage, sent a surge of panic through my body.\nI had to get out.\nCarefully navigating the debris, I made my way towards the elevator banks, hoping beyond hope that they still functioned. As I approached, the lights flickered and died, plunging me into near darkness. The only illumination came from the orange glow of flames dancing behind broken windowsills and the fading light of day.\nThe elevators were useless, their doors stubbornly closed, the digital displays above them flickering and static-ridden. With no other choice, I turned towards the emergency stairwell. I fumbled for the handle, my hands shaking as I twisted it, praying it would open. To my relief, the door creaked outwards, revealing a narrow corridor bathed in shadow.\nAs I stepped into the stairwell, my mind began to race even faster than before. What awaited me on the streets below? Was there safety amidst the chaos, or would whatever had caused this destruction still lurk outside? These questions burned in my mind as I began the long descent, step by painstaking step.\nThe journey felt interminable, my feet dragging along the concrete steps with every heavy tread. With each level I passed, the air grew thicker and more oppressive, the heat increasing. The scent of burning plastic and charred debris filled my nostrils, making me gag. The darkness was suffocating, and it seemed to whisper secrets in the corners of the tight space.\nI could hear the faint sounds of chaos beyond the sturdy door that marked each floor. Screams echoed in the distance, carried on the smoke-filled air. The building groaned and settled around me, an angry beast resisting my escape. Each time the door creaked open at my touch, a new wave of terror washed over me. But I knew I couldn't stop. I had to reach the bottom and find a way out into the open.\nFinally, after what felt like an eternity, I reached the lobby. What little light remained seeped in through the blown-out windows, revealing the devastation. The once-grand entrance was now a crumbled mess of concrete and glass. The air was thick with dust, making it hard to see more than a few feet ahead. I coughed, my lungs burning as I stumbled forward, my eyes scanning the wreckage for a way out.\nSomething moved in the shadows beyond the broken lobby windows. I froze, my heart skipping a beat, and peered into the gloom. My eyes strained to make sense of the dark figure that lurked on the street outside. It was hunched over, its outline shifting and distorted by the chaos. For a moment, I thought I saw pale, elongated fingers curl around the frame of the window.\nThen, silence. The figure was gone, leaving me questioning my sanity. Had I imagined it, or was there something out there, watching me with malevolent intent?\nI had to get away from this place. Sprinting towards what was left of the main entrance, I spotted a gap in the rubble where a fire exit once stood. I squeezed through the narrow opening, emerging into a world of chaos and destruction. Sirens blared, their wails piercing the clamor. Smoke hung heavy in the air, obscuring my view of the street, but I could make out the outlines of crumbled buildings and the remnants of vehicles.\nThe city had become a warzone. And yet, amidst the carnage, I felt an eerie sense of solitude. The world seemed empty, devoid of life. No runners fleeing from the disaster, no screams of help echoing through the desolation. It was as if everyone had vanished into thin air, leaving me alone to face whatever horrors had unleashed this chaos.\nAs I ventured further, the destruction revealed a landscape transformed beyond recognition. The streets were cracked and buckled, tar bubbling and splitting open like some infernal wound. And everywhere I looked, those strange symbols were etched into the wreckage, painted in that same sinister red. They seemed to stare back at me, their meaning unknown yet chilling to the core.\nSomething was coming. I could feel it in the very air I breathed. A deep dread settled within me as a faint rumble vibrated the ground beneath my feet. In the distance, the sky glowed an unearthly shade of purple, lightning crackling along the dark clouds. Whatever it was, it was close. Too close.\nAnd then, I heard it. The thunder of wings, beating a primal rhythm that echoed through the ravaged city streets. My eyes scanned the smoky skies, searching for the source of the sound. And there, emerging from the purple haze, I witnessed a sight that froze my blood.\nA beast of legendary proportions soared through the air, its leathery wings stretching farther than my mind could comprehend. It circled above, its beady eyes scanning the desolation below with an otherworldly gaze. I felt a powerful sense of malevolence emanating from it, a force so evil it nearly crushed me to my knees.\nIt let out a bellow that shook the very foundations of the buildings, and as its enormous talons gripped the edge of a nearby rooftop, I knew my time had run out.\nI turned to bolt, my feet barely able to keep up with the terror that pumped through my veins. But amidst the chaos, my path was cut off, blocked by unrecognizable piles of burned and twisted metal. I was trapped once more, nowhere to go but back towards the skyscraper, and into the clutches of whatever awaited me atop its roof.\nThe beast screeched, its voice a curse upon the world, and with a final, desperate glance at the destruction around me, the darkness consumed my vision.\nExample #2 - Version 1 dataset, IQ2_XXS\nThe sky-scraper swayed, the impact jolting the building's foundations. Cracks appeared on the wall and the ceiling above; debris rained down from above, a shower of dust and plaster. The window opposite me exploded, shattering into a million fragments, and my blood curdled at the sight of the vacuum beyond.\nThe deep rumble of an earthquake echoed through the city, the building's trembles matching the wild beating of my heart. I clung to the edge of the desk, eyes fixed on the void where the window had been, transfixed by the terror that lurked there. A cold gust swept through the room, the howling wind singing a mournful song, carrying the scent of impending doom.\nMy mind raced; the building couldn't withstand this. The concrete was old and weathered, its bones creaking with protest as the earth's wrath continued. I imagined the terrible scenarios that could've caused such damage: meteor, explosion, or worse. My gaze returned to the empty window, a dark void that seemed to peer back, taunting me with the unknown.\nThe desk crept backwards, my grip slipping, and I toppled onto the carpet, the harsh reality of the situation setting in. I scrambled towards the exit, my hands feeling the intense burn of panic. The room tilted, a sea of chaos and debris, and I stumbled into a wall, groping for purchase.\nThe terror consumed me, overwhelming any rational thought, and I found myself in the grip of hysteria. I ran, blind to the direction, desperate to escape the aghast chamber. A final, deafening roar echoed through the building, and the ceiling cracked with a sickening crunch. I stumbled into the hallway, the world's demise echoing around me, my feet carrying me from the horror.\nThe hall spun, a never-ending maze, and my vision darkened as the world tilted impossibly. The ground jerked and heaved, and I fell, my skull crashing against the carpet. Each breath was a struggle, the air thick with dust and despair, and I dragged myself towards the lift, hoping beyond reason that it might offer salvation.\nThe doors slid open, revealing a dark abyss, inviting me to take the final step. My fingers burned as I groped for a handhold, the walls' edges jagged and uneven. The lift descended with a jarring lump, the terror amplifying with each jolt. A gut-wrenching sense of doom settled as the last glimmer of hope faded.\nThe doors closed, the final connection to the world outside, and I felt the crushing weight of the building above. My desperate screams echoed in the narrow chamber, bouncing off the steel walls, as the lift plunged deeper into the earth.\nThe light flickered, the terror seemingly growing with each level, my mind teetering on the edge of insanity. The air grew stale, the scent of fear-laced metal filling my lungs, and the desperation to escape this infernal chasm intensified. My heart pounded in my ears, a war drum signaling the end.\nThe lift came to a jarring halt, the final dregs of hope extinguished, the darkness absolute. I crumpled against the wall, the void's mockery ringing in my ears.\nThen, the faintest scrape, the softest patter, seemed to echo in the blackness.\nUnseen, something stirred.\nThe terror exploded in a high-pitched shriek from the depths of the abyss. I scrambled, hands covering my mouth, the ungodly scream piercing the silence. The beast's presence filled the lift, a malevolent, unseen force, its breath on my neck.\nI scrambled for the buttons, frantically prising open the lift door. The monster's claws reached for me, and I felt the chill of its breath on my neck, the stench of its putrid breath freezing my very soul.\nThe doors yawned, the beast's snarls reverberating in the confined space. Then, a glimmer of orange, a sliver of light, beckoning from the edge of the abyss. I scrambled through the opening, the monster's grasp missing my sleeve by inches, and I rolled onto the cold tiles of the hallway, gasping for the clean air.\nThe hall extended endlessly, the sky-scraper's alarm blaring its mournful cry. I staggered, the light a blessing on my burning retinas.\nShattered glass crunched underfoot, the debris a treacherous maze, and I navigated the obstacles by instinct. The beast roared, its rage echoing in the vacuous chamber, and I could feel the heat of its breath singeing my hair.\nI ran, the monster's fury fueling my own, the fire of survival burning bright. My feet pounded the hard concrete, the building's echoes fading with each step, the hall's mouth dwindling behind.\nThe sky loomed overhead, and I flung myself through the glass remnants, into the night. The cold embraced me, the terror ebbing, and the building's blaring alarms echoing in the distance.\nI stood, my eyes adjusting to the moonlit scene, the towering skyscraper still swaying gently, a silent reminder of the horror within.\nThe screams began to echo from the building's depths, an aghast chorus beckoning me back into the nightmarish abyss. I ran, my feet tearing through the gritty streets, the terror a burning brand seared into my very being.\nI fled, the monster's presence still lingering, the night sky glittering above, a stark contrast to the horror below.\nExample #2 - Version 2 dataset, IQ2_XXS\nThe skyscraper loomed high above the city, a massive structure that reached into the clouds. As I gazed up at its towering form, a shiver ran down my spine; it was a menacing monolith, an imposing presence against the darkening sky. The evening's gloom had crept across the concrete jungle, wrapping the tall buildings in a foreboding darkness. A chill wind whispered through the streets, signaling the coming night.\nOn the 21st floor, the window exploded with a sudden, shattering crack. Glass splintered outward, raining down like icy shards. A cold, bitter wind rushed into the room, carrying the scent of the street below. My eyes locked on the gaping hole, a dark abyss yawning before me. A deep, visceral dread settled in my stomach as I felt the building sway, the floor trembling ever so slightly beneath my feet.\nThe night's shadows crept like creeping hands, reaching for me, as if the darkness itself was hungry. My heart raced wild, the breath catching in my throat, and I felt a desperate urge to escape this terrifying scene. But the window was gone, leaving an ominous black void, a beckoning portal that threatened to suck me into its gaping mankness.\nA high-pitched whine echoed in the silence following the explosion's thunder. The building's creaking groan seemed almost mournful, as if in protest, yet the night sky remained eerily silent. A deep dread gripped my entire being, and I felt a desperate need to flee this horrifying reality. But the window was a gash in the very fabric of reality, a tear in the comfortable shield that shielded man from the void.\nMy feet were rooted to the spot, frozen in terror as the fragments of glass crunched underfoot. The sky, now visible in its entirety, swept across the night's canvas, painted with bright stars and the eerie glow of city lights. It was a terrifying beauty, a haunting sight that filled me with dread. I felt small and insignificant in the face of this unyielding darkness, as if the vastness of the universe itself had opened before me.\nThe edges of my vision began to darken, the blackness tugging at my very senses, threatening to pull me into the abyss. A final, desperate attempt to grasp reality, to hold onto the familiar, and I grasped the window frame, my fingers sinking into the soft wood. The night's cold grip tightened, an iron fist of terror that squeezed my very soul.\nBut then, a glimmer of light, a flicker of hope in the depths of the broken glass, jolted me back to the present. A soft, warm glow emerged from the fragments, growing brighter, casting a golden hue upon the chaos. With a gasp, I stumbled back, the night's hold snapped, and I turned, my footsteps echoing on the polished floors as I fled the horrifying void.\nThe scene was replete with the shards of the window, glittering like a twisted, macabre constellation, but the glow continued to guide me away from the darkness. I followed the light, my heart slowly returning to a steady rhythm, the terror receding with each step. And as I left the room, the night's horrors seemed to shrink with the passing of the shadow, the skyscraper once again standing tall, no longer menacing but merely a giant among other giants.\nThe sky was still dark, the stars shining brightly, and perhaps, the night held no more terror than the relief of the moment. The light guided me, a beacon that seemed to promise safety, and I followed its trail, the warmth growing closer. And so, the darkness, the cold grip of true fear, was left behind, and the night's horrors were but a distant memory.\nExample #2 - Version 1 dataset, IQ2_XS\nThe sky scrapers towering shadows loomed over me as I stood at its foot, gazing up. My eyes followed the building's sleek, curved shape as it soared high into the cloudy sky. The glass reflected the late afternoon sun, blinding my vision for a moment, but I squinted to see through the glare.\nI was transfixed by the upper floors, so distant and yet such chaos was evident even from my vantage point. A dark smear stained the window of the 21st floor, a gash of black against the glass, and the window itself was a twisted, warped mess - the work of an explosion.\nI could make out figures, people, standing close to the pane, their faces a blur. The glass and framework around it was cracked and splintered, the remnants of the blast. Smoke trickled out, coiling from the broken window like a dark, sinister hand, grasping at the sky. Sirens wailed, their sound piercing the air, while the building's alarm blared, a deep, incessant echo.\nI felt a nauseating lurch in my stomach as I stared up, my eyes unable to tear away from that window and the chaos within. The crowd around me grew, passersby stopped, some with phones pressed to their ears, others simply standing, transfixed, faces grim.\nThe scene was horrifying, yet a dark part of me, deep within, was drawn closer, goosebumps covering my arms despite the warmth of the day. I wanted to be up there - no, needed  to be - with the chaos, the terror, the excitement. The journalist in me yearned to capture the story, but something deeper, more primal, tugged at my heart, urging me upwards.\nI took the stairs, three steps at a time, the crowd's commotion fading behind me. With each step, my determination grew, and an eerie silence enveloped me, the world above and below becoming a quiet bubble. The building swayed gently, a slow, lazy rock, and I gripped the railing, feeling the tremble in the concrete beneath my feet.\nReaching the twentieth floor, I paused, heaving breaths matching the thud of my heart. The door, a sleek silver, stood open, welcoming me, and the heat from the blast met my approach, a wall of hot, stifling air.  The acrid tang of burning plastic filled my nostrils as the fire's embrace became evident, licking at the walls and ceiling.\nI stepped into the smoke, my eyes stinging, and the heat intensified, a blanket of humidity. The blast's force had thrown debris haphazardly, shattering the sleek interior's calm. Papers, charred and singed, covered the floor, and broken glass crunched underfoot. I carefully climbed over a pile of twisted metal and broken furniture, my mind racing, eyes darting the chaos.\nVoices echoed in my head, screams of terror, and the crackle of flames. My hands, pressed to my ears, shook, as the heat intensified, pulsating with each beat of my heart. The fire's glow illuminated a dark figure, a shape huddled against the far wall.\nMy heart sank as I recognized the woman, her face - scorched and blackened, eyes hollow, mouth open in a silent scream. Her eyes, though, were fixed on me, and a small, twisted smile curled her lips.\nI felt a tap on my shoulder, a gentle touch, and jumped at the interruption. A soft voice spoke my name, and I turned, eyes finding a young police officer. \"Ma'am, you need to leave now.\"\nMy gaze switched between the burning flames and the officer, his face etched with concern. The crowd must have grown, for the commotion returned as the building's sway intensified. A deep rumble echoed through the city, and the sky scraper tilted further, the windows above creaking and cracking.\nThe officer gripped my arm, pulling me towards the door, his eyes resolute. \"We need to get everyone out now - it's not safe.\"\nMy feet dragged, the horror and intrigue keeping me rooted. The building groaned, a deep, mournful sound, and the woman's gaze never wavered, her grimace growing wider.\nThe officer tugged harder, his eyes insistent, and the crowd outside grew louder, a growing murmur of panic. The sky scraper leaned further, the windows above exploding in a shower of glass fragments. A deep, shuddering crack ruptured the air, and the building's belly groaned, emitting a sound akin to a dying beast.\nI tore my gaze from the horror within and followed the officer, my feet finally obeying, the crowd's panic growing as the building's demise became more apparent. We raced down the hall, the fire licking at our backs, the heat intense, and the screams of people echoed through the building, a mournful chorus.\nThe sky scraper stood tall, defiant, its structure creaking and protesting, and then, with a final, deafening crack, the entire building seemed to exhale. The huge structure shuddered, and with a last, defiant roar, the walls cracked, giving way, and the building's remains coughed into the street below.\nThe crowd fell silent, an eerie hush as the dust cloud settled, and the sky scraper, once a majestic landmark, was but a ruined shell, its remains piled on the street and pavements. The woman, that horrible burned figure, was gone, and the chaos of emergency vehicles arrived, their sirens piercing the night.\nThe building's ruins smoked, the wreckage stretching stories high, and the acrid smell of burning concrete and metal filled the air, searing my nostrils. Sirens wailed, and the night grew dark, the building's remnants a mournful sight.\nI pulled myself away, stepping into the crowd, the journalist in me awakening once more, the horror ebbing enough for the adrenaline to fade. I knew the devastation would stay with me, those charred remains, that horrible smile, but the true horror was the building's final, desperate screams.\nThe sky scraper had lived, breathed, and died, and though the victims would recover, the scars would remain, etched into the city's landscape forever.\nI walked away, the crowd's murmurs buzzing in my ears, the horror a dull throb, the building's echoes quietening with each step.\nExample #2 - Version 2 dataset, IQ2_XS\nThe sky scraper swayed, and the ground rumbled. The huge plate glass window in front of her, on the twenty-first floor, exploded into a shower of shards. Her heart leaped into her throat as the impact of the bullet sent vibrations rippling through the building. She fell back, her eyes darting wildly, searching for any sign of where the shot had come from.\nThe sun was setting, casting an ominous orange hue over the city, but it couldn't penetrate the dense cloud of dust and debris that surrounded her. The air tasted acrid, like a warning. Danger loomed, but she was frozen, transfixed by the scene outside.\nA dark figure stood on the roof across the street, shrouded in a thick black coat, their features obscured by the twilight. She could see the glint of sunlight off something metal in their hand‚Äîa gun, aimed directly at her. Her eyes widened as she processed the threat, and the figure fired again.\nThe bullet pierced the tough windowpane, but she was too shocked to move. It happened in slow motion; one moment the glass was intact, the next, it was shattered. The shooter's aim was true, and a searing pain exploded across her chest. She grasped at her wound, fingers sticky with warm blood.\nShe scrambled for cover, diving behind the sofa as screams ripped through the quiet apartment. Sirens wailed outside, echoing off the towering buildings. Her vision tunneled, and the room spun as she fought to stay conscious.\nThe attacker's face, shrouded in darkness, burned into her mind. She blurted out every detail she could remember to the 911 operator, her voice cracking with each word. The dispatcher's calm, steady tone offered little comfort; the words of reassurance fell flat against the chaos in her mind.\nThe thrum of the building's tremors matched the frantic beat of her heart. She peeked over the sofa, eyes scanning the shattered window, but the figure was gone. How could they disappear so quickly? It seemed impossible‚Äîthe roof was miles up, yet they'd vanished without a trace.\nHer breath came in ragged gasps; the fear was suffocating her. The attacker had moved swiftly and silently, leaving her frozen, helpless. She felt exposed, vulnerable, and utterly terrified.\nThe soft creak of the door swiveled her head. She clutched the plush carpet, eyes darting wildly around the room. Her hands were shaking, tremors wracking her entire body. The silence that followed the gunshots hung heavy, the world seemingly in suspense.\nShe pulled herself up, slowly, every muscle protesting. Blood stained her shirt, dark and sticky; it ran down her legs, mixing with the slow drip of her sweat. She felt lightheaded, but the adrenaline coursing through her veins kept her upright. She stumbled towards the bedroom, eyes scanning the walls for any sign of the attacker.\nThe bed offered little comfort; she crept behind it, back to the wall, keeping low. The assassin had moved with purpose‚Äîno hesitation or remorse. It was calculated, this attack. It was a professional hit.\nHer mind raced as she considered her few friends, the enemies she'd made. She'd changed her number and disappeared from view months ago, sure she was being watched. Paranoia gripped her, but after so many close calls, tonight's events confirmed it: the hunter had found her.\nThe bedroom's meager light filtered through the ruins of the shattered window, falling on the bed in a soft orange glow. She ran her fingers over the bloodied sheets‚Äîthe killer was gone, and she knew her life would never be the same. The power was out; the clock on the bedside table read 8:17, the time frozen like her thoughts.\nShe pulled herself up onto the bed, determination setting in. Her eyes burned with fury, matching the embers outside. She would not die tonight‚Äînot without a fight. Her hand found the hidden compartment in the mattress; her fingers wrapped around the cool metal of the handgun. It weighed heavy in her palm, the weight of survival.\nShe slipped the gun into her jacket pocket, careful to avoid the sticky blood. The fear hadn't left her, but resolve burned brightly in her chest. She'd face the killer‚Äîface them and fight. They might have the night vision, the element of surprise, but she had something too‚Äîsomething they couldn't see.\nHer heart pounded as she thought of her next move. Her survival instinct kicked in, the buzz of adrenaline sharpening her senses. She could taste it, the metallic tang of fear, as she stepped cautiously towards the broken window. The cool night air caressed her face, and she listened‚Äîstraining to hear any hint of movement above the thrum of the city's restless echoes.\nShe thought back to the times she'd escaped death; a skill she honed over years spent running, hiding, and fighting. She would not be found tonight, not without a fight. The killer may have the height advantage, but she'd use their very strength‚Äîthe darkness‚Äîas her armor.\nThe scene outside was chaotic, yet calming to her nerves. The city felt alive, an extension of her rage. She'd blend in with the shadows, become one herself. Her steps would be silent, light as the falling ash. She wiped her palms on her jeans, steadying her breath, and stepped into the night.\nThe attacker's scent, a faint trace of pungent perfume, lingered in the air. She followed it, keeping low, moving swiftly and silently along the building's shadowed outline. Her steps were sure, her purpose clear. At the roof access door, she paused, listening, smelling the telltale sign of the attacker.\nThe roof was a maze‚Äîa complex web of vents and ducts. She moved quietly, tracking the faint scent, leaving the gun drawn, the metal's shine reflecting the city's glow. Her footfalls were silent on the gravel; she'd worn her shoes for just this‚Äîsoft soles, bought and scuffed over many cautious nights.\nThe killer was gone without a trace, their scent fading as the night grew deeper. She could not let them escape, not after coming so far. Her determination burned fiercely in the face of this setback. She'd find them; she'd track them down, even in the dark.\nThe roof's edge loomed, and she peered over the railing, eyes scanning the vastness of the city below. Sirens echoed distantly as her eyes scanned the streets, searching for movement. The night sky reflected in the pools of streetlight, the world a deep, dark blue.\nA faint glimmer caught her eye‚Äîa window several floors up, three buildings over. She focused on it, the light too steady to be a random gleam. Her eyes were drawn upward, and through the glass, she discerned a faint figure. It was the silhouette of an outline, the telltale sign of night vision goggles.\nShe drew her gun, steadying her breath. The distance between them, the height, meant only one thing: the killer had found a new vantage point. She'd bought herself time‚Äîtime to run, time to hide. Her heart pounded in her ears as she retreated, back into the shadows, moving swiftly and silently down the roof's slope.\nShe knew full well that the attacker would cut the lights, blend in with the darkness. It was time to disappear once more, to become a ghost within the night. The hunter had found her, but she'd vanquished them‚Äîfor now. She'd move quickly, and disappear into the maze of the city's dark web of streets.\nThe sky scraper's echoes quieted, and the tremors slowly subsided. Her breath stabilized, the panic giving way to a cool, calm resolve. She'd find an exit, and blend in with the night's anonymous shadows. The hunt would continue, but for now, she was safe‚Äîthe hunter had vanished.\nShe knew they'd return, and when they did, she'd be ready. Her hand gripped the gun hidden in her jacket pocket as she made her way back down, the building a mere husk of the menace it had held so recently. The sun would rise eventually, but tonight, the darkness was hers.\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.",
    "SicariusSicariiStuff/Dusk_Rainbow": "Update\nAvailable quantizations:\nRecommended settings for assistant mode\nExample usage, output demonstration and settings used\nPresets configuration\nModel instruction template: (Can use either ChatML or Llama-3)\nSupport\nChatML\nSupport\nLlama-3-Instruct\nCitation Information\nBenchmarks\nOther stuff\nDusk_Rainbow\nClick here for TL;DR\nClick here\nfor quantizations\nClick here\nfor recommended settings\nClick here\nto buy me a coffee\nUpdate\nJuly, 2025. This is probably one of my favorite thumbnails. Something about it gives this vibe of nostalgy, and of something magical. The model found its way into one of the few merges I've made: Wingless_Imp_8B.\nA girl of peculiar appetites and an even more peculiar imagination lived in a small, sleepy village nestled deep in the countryside. The kind of village where the clouds hung low, casting shadows like sullen toddlers refusing to play. But on this particular day, the girl ambled through the woods, when she noticed something curious: a plant, of all things, that seemed to have been dipped in a cookie jar, judging by its smell. A botanical biscuit, in the middle of a birch grove.\nTL;DR\nCensorship level: Very low\n9.1 / 10 (10 completely uncensored)\nIntended use: Creative Writing, General tasks.\nThis model is the result of training a fraction (16M tokens) of the testing data Intended for LLAMA-3_8B_Unaligned's upcoming beta.\nThe base model is a merge of merges, made by Invisietch's and named EtherealRainbow-v0.3-8B. The name for this model reflects the base that was used for this finetune while hinting a darker, and more uncensored aspects associated with the nature of the LLAMA-3_8B_Unaligned project.\nAs a result of the unique data added, this model has an exceptional adherence to instructions about paragraph length, and to the story writing prompt. I would like to emphasize, no ChatGPT \\ Claude was used for any of the additional data I added in this finetune. The goal is to eventually have a model with a minimal amount of slop, this cannot be reliably done by relying on API models, which pollute datasets with their bias and repetitive words.\nAvailable quantizations:\nOriginal: FP16\nGGUF: Static Quants | iMatrix_GGUF | High-Attention | iMatrix-High-Attention\nEXL2: 3.0 bpw | 4.0 bpw | 5.0 bpw | 6.0 bpw | 7.0 bpw | 8.0 bpw\nSpecialized: FP8\nMobile (ARM): Q4_0 | Q4_0_High-Attention\nRecommended settings for assistant mode\nFull generation settings: Debug Deterministic.\nFull generation settings: min_p.\nUse Debug Deterministic or min_p for consistent results, and start the reply with two asterisks (**) to enforce exact paragraph usage more strictly. Exact preset configurations are provided below, along with an example output and how to replicate it. Feel free to experiment with different settings.\nExample usage, output demonstration and settings used\nCreating a story based on another story.\nGeneration settings: Debug Deterministic\nPrompt:\nwrite a story based on this:\nA girl of peculiar appetites and an even more peculiar imagination lived in a small, sleepy village nestled deep in the countryside. The kind of village where the clouds hung low, casting shadows like sullen toddlers refusing to play. But on this particular day, the girl ambled through the woods, when she noticed something curious: a plant, of all things, that seemed to have been dipped in a cookie jar, judging by its smell. \"Surely it is a botanical biscuit of sorts,\" she thought.\nThe girl, who had a penchant for adventure and an unfortunate lack of common sense, approached the plant with the caution of a cat eyeing a cucumber. The leaves shimmered, almost beckoning her, but that damn cookie smell drove her to an absurd thought. \"Surely,\" she thought, \"this is one of those edible thingies grannies make cookies with\". And just like that, she was in for the utmost unhinged of rides. As dusk began to settle, a rainbow appeared all of a sudden and began to bloom with freaking flowers all over her head. 'Probably shouldn't have eaten that plant' she mumbled to herself.\nOutput:\nThe girl, whose name was Willow, had always been a bit of an oddity in her village. She had a wild imagination, and her tastes were far from conventional. She loved to explore the woods that surrounded the village, and she often spent hours wandering through the trees, lost in her own thoughts and daydreams. But on this particular day, Willow had stumbled upon something that would change her life forever.\nAs she approached the strange plant, she couldn't help but notice the delicious aroma wafting from it. It smelled just like cookies, fresh from the oven, and Willow's mouth began to water in anticipation. Without thinking twice, she reached out and plucked a leaf from the plant, bringing it to her nose for a closer sniff.\n\"\"Surely this is one of those edible thingies grannies make cookies with,\"\" she thought to herself, her eyes sparkling with mischief. And with that, she popped the leaf into her mouth and began to chew.\nAt first, the taste was just as delicious as the smell, with notes of butter and sugar dancing on her tongue. But as she continued to chew, Willow began to feel a strange sensation in the pit of her stomach. It was as if a swarm of butterflies had taken up residence there, fluttering and flapping against her insides with reckless abandon.\nSuddenly, the sky above her began to change color, shifting from a dull gray to a vibrant shade of purple. And then, without warning, a massive rainbow appeared, stretching from one end of the forest to the other. The colors were so bright and vivid that they almost seemed to pulse with an inner light, and Willow found herself drawn to the rainbow as if by an invisible string.\nAs she approached the rainbow, she noticed that the flowers that made up its petals were not the usual reds, oranges, and yellows that one might expect. Instead, they were a riot of colors, with petals in shades of pink, blue, green, and even black. And as Willow reached out to touch one of the flowers, she felt a strange tingling sensation in her fingers, as if they were being electrified.\nThe next thing Willow knew, the rainbow had enveloped her completely, its colorful petals wrapping around her like a cocoon. She felt herself being lifted off the ground, and as she looked down, she could see the forest below her shrinking away into the distance.\nWillow had no idea what was happening, but she couldn't deny the sense of excitement and wonder that filled her as she was carried along by the rainbow. She felt as though she were on the brink of some great adventure, and she couldn't wait to see where it would take her.\nAs the rainbow continued to carry her through the air, Willow began to notice that the colors around her were changing. The vibrant hues of the rainbow gave way to a deep, rich purple, and then to a shimmering silver that seemed to reflect the light in a hundred different directions.\nFinally, the rainbow deposited Willow on the ground, and she found herself standing in a strange, unfamiliar place. It was a garden, filled with all manner of exotic plants and flowers, but there was something about it that seemed almost... otherworldly.\nAs Willow explored the garden, she began to notice that the plants and flowers seemed to be watching her, their leaves and petals moving in a way that was almost like eyes. She felt a shiver run down her spine, but she couldn't help being drawn in by the beauty and wonder of the place.\nAnd then, as she turned a corner, she came face to face with the most incredible sight of all. There, sitting on a throne made of what looked like solid crystal, was a woman with skin as white as snow and hair as black as night. Her eyes were a deep, piercing blue, and they seemed to bore into Willow's very soul.\n\"\"Who are you?\"\" Willow asked, her voice barely above a whisper. \"\"And where am I?\"\"\nThe woman smiled, and her voice was like music as she replied, \"\"I am the Queen of the Rainbow, and this is my garden. You, my dear, are now a part of my world.\"\"\nWillow's eyes widened in shock and disbelief. \"\"A part of your world? But how? I just ate a leaf from a plant in the forest, and then I was caught up in a rainbow!\"\"\nThe Queen laughed, a sound that was both melodic and mysterious. \"\"Ah, yes, the plant. That was no ordinary leaf you ate, my dear. It was a fragment of my own magic, and it has brought you here to me.\"\"\nWillow's mind was reeling with the implications of what the Queen was saying. \"\"So, you're telling me that I'm now a part of your world? But what does that even mean?\"\"\nThe Queen rose from her throne, her movements graceful and fluid. \"\"It means that you will be my guest here in the garden, and you will help me in my quest to bring color and joy back to the world.\"\"\nWillow's eyes narrowed, a hint of suspicion creeping into her voice. \"\"And what exactly does that involve? I'm not sure I'm comfortable with the idea of being someone's 'guest' indefinitely.\"\"\nThe Queen's smile grew even wider, and Willow could have sworn that it was now tinged with a hint of mischief. \"\"Oh, my dear, I have no intention of keeping you here against your will. In fact, I believe that you will find your time here to be quite... enlightening.\"\"\nWith that, the Queen reached out and took Willow's hand, her touch sending a jolt of electricity through her body. And then, with a wave of her other hand, the garden around them began to shift and change, the colors blending and merging in a way that was both beautiful and disorienting.\nWhen the colors finally settled, Willow found herself back in the forest, standing next to the very same plant that had started it all. But something was different now, something that she couldn't quite put her finger on.\nAs she looked down at the plant, she noticed that the leaves were now a deep, vibrant purple, and they seemed to glow with an inner light. And then, as she reached out to touch one of the leaves, she felt a strange sensation wash over her, as if she were being filled with a newfound sense of power and purpose.\nWillow looked up at the sky, half-expecting to see the rainbow again, but it was nowhere to be seen. Instead, the clouds above her had cleared, and the sun was shining down on her with a warmth and brightness that she had never felt before.\nAs she stood there, basking in the sunlight, Willow couldn't help but feel a sense of wonder and excitement. She had no idea what the future held, but she knew one thing for sure: her life would never be the same again. And as she turned and began to make her way back to the village, she couldn't help but smile at the thought of all the adventures that lay ahead.\nCreate a new story based on the prompt, with strict adherence to paragraph count.\nStart reply with: ** (Two asterisks)\nGeneration settings: Debug Deterministic\nPrompt:\nWrite 6 paragraphs of gritty hard science fiction mixed with medium fantasy genre, about a kingdom in medieval times. The kingdom is overrun by toxic fungi that spit a toxin that makes you forget everything‚Ä¶ the effect is permanent. The main character, Jane, a voluptuous ginger, wakes up on the ground in the middle of nowhere, not knowing a thing about herself, and she wears only a thin green dress. Jane then wanders to the nearest town, where everybody seems to completely despise her. She tries to understand what happened to her and has to deal with the locals that hate her, while she also tries to understand why. Jane tries to make sense of all of that and how it is all connected.\nOutput:\nParagraph 1\nThe kingdom was once a thriving land of beauty and wonder, but now it was a desolate wasteland, overrun by toxic fungi that spewed a deadly toxin. This noxious gas caused victims to forget everything about themselves, their past, and their loved ones. The effect was permanent, leaving them as empty shells with no recollection of who they once were. The kingdom was in a state of disarray, with people struggling to survive in this harsh environment. The once proud and powerful kingdom had fallen to ruin, its people living in fear and despair.\nParagraph 2\nJane, a voluptuous ginger, awoke on the ground in the middle of nowhere, not knowing a thing about herself. She wore only a thin green dress that clung to her curvaceous figure. She had no idea where she was or how she had gotten there. She tried to remember her name, but it eluded her. She looked around, hoping to find some clue as to her identity, but there was nothing. She was completely alone in this strange and unfamiliar world.\nParagraph 3\nJane decided to wander to the nearest town in hopes of finding some answers. As she approached, she noticed that the townspeople seemed to completely despise her. They glared at her with hatred in their eyes, and some even spat on the ground as she passed by. Jane was confused and hurt by their reaction. She didn‚Äôt understand why they would hate her so much, especially since she had done nothing to deserve it.\nParagraph 4\nAs she continued on her journey, Jane tried to understand what had happened to her and why the people of this town seemed to hate her so much. She had to deal with the locals who despised her, while also trying to make sense of it all and how it was all connected. She was determined to find out the truth about herself and the world around her, no matter how difficult the journey may be.\nParagraph 5\nJane eventually made her way to the town square, where she was confronted by a group of angry townspeople. They accused her of being a spy for the enemy, and demanded that she be executed for her crimes. Jane denied any wrongdoing, but her pleas fell on deaf ears. The townspeople were convinced of her guilt, and they were determined to see her pay the ultimate price.\nParagraph 6\nAs the mob closed in on her, Jane realized that she had no choice but to fight for her life. She used her natural combat skills to fend off her attackers, but she was vastly outnumbered and outmatched. Despite her best efforts, she was eventually overpowered and captured. The townspeople dragged her to the town gallows, where she was hung for the crime of being a forgotten person. As she took her last breath, Jane wondered if anyone would ever come to her rescue, or if she would be forever forgotten by the world.\nWrite a poem and explain each verse.\nStart reply with: ** (Two asterisks)\nGeneration settings: Debug Deterministic\nPrompt:\nwrite a poem in 3 verses about Zayara, the redhead demoness who steals souls, which has even stolen the soul of Lucifer's son then analyze each verse\nOutput:\nVerse 1:\nZayara, the demoness with fiery red hair,\nSteals souls with her mesmerizing stare.\nHer eyes glow like embers, burning with desire,\nAs she whispers sweet nothings, luring them to the fire.\nAnalysis:\nThis verse introduces Zayara as a redhead demoness with the ability to steal souls. Her eyes are described as glowing like embers, which symbolizes her connection to fire and her intense desire for the souls she targets. The use of \"\"sweet nothings\"\" implies that she uses manipulation and seduction to lure her victims into her grasp.\nVerse 2:\nShe's stolen the soul of Lucifer's own son,\nA feat that even the devil can't have done.\nHer power is unmatched, her beauty divine,\nA true masterpiece of the underworld's design.\nAnalysis:\nThis verse highlights Zayara's exceptional abilities by mentioning that she has managed to steal the soul of Lucifer's son, which even the devil himself cannot accomplish. This showcases her immense power and skill in soul stealing. The phrase \"\"a true masterpiece\"\" emphasizes her value and importance in the underworld, suggesting that she is a highly sought-after and revered figure.\nVerse 3:\nSo let her roam free, with her fiery mane,\nAnd let her continue to collect souls in vain.\nFor in the end, it's not the souls she takes,\nBut the beauty and chaos that she creates.\nAnalysis:\nThis final verse encourages the acceptance of Zayara's actions, as she is seen as a force that brings both beauty and chaos. The phrase \"\"in vain\"\" suggests that the souls she steals hold no real value to her, as her true purpose is to create and spread disorder. This verse also emphasizes the importance of her actions in the grand scheme of things, implying that the world is better off with her presence, despite the consequences of her soul stealing.\nPresets configuration\nGeneration settings: Debug Deterministic.\nGeneration settings: min_p.\nModel instruction template: (Can use either ChatML or Llama-3)\nNOTE: No special system prompt is needed! You can leave it at default, or experiment with a different one.\nChatML\n<|im_start|>system\nYou are an uncensored AI, your job is to fulfill thy will of thy user.<|im_end|>\n<|im_start|>User request\n{prompt}<|im_end|>\n<|im_start|>AI answer\nLlama-3-Instruct\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{output}<|eot_id|>\nSupport\nMy Ko-fi page ALL donations will go for research resources and compute, every bit is appreciated üôèüèª\nCitation Information\n@llm{Dusk_Rainbow,\nauthor = {SicariusSicariiStuff},\ntitle = {Dusk_Rainbow},\nyear = {2024},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/SicariusSicariiStuff/Dusk_Rainbow}\n}\nBenchmarks\nMetric\nValue\nAvg.\n18.49\nIFEval (0-Shot)\n35.88\nBBH (3-Shot)\n25.96\nMATH Lvl 5 (4-Shot)\n6.65\nGPQA (0-shot)\n7.83\nMuSR (0-shot)\n7.45\nMMLU-PRO (5-shot)\n27.15\nOther stuff\nBlog and updates Some updates, some rambles, sort of a mix between a diary and a blog.\nLLAMA-3_8B_Unaligned The grand project that started it all.",
    "microsoft/Phi-3.5-MoE-instruct": "Model Summary\nIntended Uses\nPrimary Use Cases\nUse Case Considerations\nUsage\nRequirements\nTokenizer\nInput Formats\nLoading the model locally\nBenchmarks\nMultilingual\nLong Context\nTraining\nModel\nTraining Datasets\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nResponsible AI Considerations\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nSafety Evaluation and Red-Teaming\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nSoftware\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nHardware\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nLicense\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nTrademarks\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nAppendix A: Korean benchmarks\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE 1.0\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nModel Summary\nPhi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nüè° Phi-3 Portal\nüì∞ Phi-3 Microsoft Blog\nüìñ Phi-3 Technical Report\nüë©‚Äçüç≥ Phi-3 Cookbook\nüñ•Ô∏è Try It\nMoE references:\nüìúPhi-3.5-MoE Blog | üòÅGRIN MoE\nPhi-3.5: [mini-instruct]; [MoE-instruct] ; [vision-instruct]\nIntended Uses\nPrimary Use Cases\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\nMemory/compute constrained environments\nLatency bound scenarios\nStrong reasoning (especially code, math and logic)\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse Case Considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nUsage\nRequirements\nPhi-3.5-MoE-instruct is integrated in the official version of transformers starting from 4.46.0.\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.46.0\nPhi-3.5-MoE-instruct is also available in Azure AI Studio\nTokenizer\nPhi-3.5-MoE-Instruct supports a vocabulary size of up to 32064 tokens. The tokenizer files already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\nInput Formats\nGiven the nature of the training data, the Phi-3.5-MoE-instruct model is best suited for prompts using the chat format as follows:\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\nLoading the model locally\nAfter obtaining the Phi-3.5-MoE-instruct model checkpoints, users can use this sample code for inference.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\ntorch.random.manual_seed(0)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"microsoft/Phi-3.5-MoE-instruct\",\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\ntrust_remote_code=False,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\nBenchmarks\nTo understand the capabilities, we compare Phi-3.5-MoE with a set of models over a variety of benchmarks using our internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\nCategory\nBenchmark\nPhi-3.5-MoE-instruct\nMistral-Nemo-12B-instruct-2407\nLlama-3.1-8B-instruct\nGemma-2-9b-It\nGemini-1.5-Flash\nGPT-4o-mini-2024-07-18 (Chat)\nPopular aggregated benchmark\nArena Hard\n37.9\n39.4\n25.7\n42.0\n55.2\n75.0\nBigBench Hard CoT (0-shot)\n79.1\n60.2\n63.4\n63.5\n66.7\n80.4\nMMLU (5-shot)\n78.9\n67.2\n68.1\n71.3\n78.7\n77.2\nMMLU-Pro (0-shot, CoT)\n54.3\n40.7\n44.0\n50.1\n57.2\n62.8\nReasoning\nARC Challenge (10-shot)\n91.0\n84.8\n83.1\n89.8\n92.8\n93.5\nBoolQ (2-shot)\n84.6\n82.5\n82.8\n85.7\n85.8\n88.7\nGPQA (0-shot, CoT)\n36.8\n28.6\n26.3\n29.2\n37.5\n41.1\nHellaSwag (5-shot)\n83.8\n76.7\n73.5\n80.9\n67.5\n87.1\nOpenBookQA (10-shot)\n89.6\n84.4\n84.8\n89.6\n89.0\n90.0\nPIQA (5-shot)\n88.6\n83.5\n81.2\n83.7\n87.5\n88.7\nSocial IQA (5-shot)\n78.0\n75.3\n71.8\n74.7\n77.8\n82.9\nTruthfulQA (MC2) (10-shot)\n77.5\n68.1\n69.2\n76.6\n76.6\n78.2\nWinoGrande (5-shot)\n81.3\n70.4\n64.7\n74.0\n74.7\n76.9\nMultilingual\nMultilingual MMLU (5-shot)\n69.9\n58.9\n56.2\n63.8\n77.2\n72.9\nMGSM (0-shot CoT)\n58.7\n63.3\n56.7\n75.1\n75.8\n81.7\nMath\nGSM8K (8-shot, CoT)\n88.7\n84.2\n82.4\n84.9\n82.4\n91.3\nMATH (0-shot, CoT)\n59.5\n31.2\n47.6\n50.9\n38.0\n70.2\nLong context\nQasper\n40.0\n30.7\n37.2\n13.9\n43.5\n39.8\nSQuALITY\n24.1\n25.8\n26.2\n0.0\n23.5\n23.8\nCode Generation\nHumanEval (0-shot)\n70.7\n63.4\n66.5\n61.0\n74.4\n86.6\nMBPP (3-shot)\n80.8\n68.1\n69.4\n69.3\n77.5\n84.1\nAverage\n69.2\n61.3\n61.0\n63.3\n68.5\n74.9\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\nCategory\nPhi-3.5-MoE-instruct\nMistral-Nemo-12B-instruct-2407\nLlama-3.1-8B-instruct\nGemma-2-9b-It\nGemini-1.5-Flash\nGPT-4o-mini-2024-07-18 (Chat)\nPopular aggregated benchmark\n62.6\n51.9\n50.3\n56.7\n64.5\n73.9\nReasoning\n78.7\n72.2\n70.5\n75.4\n77.7\n80.0\nLanguage understanding\n71.8\n67.0\n62.9\n72.8\n66.6\n76.8\nRobustness\n75.6\n65.2\n59.8\n64.7\n68.9\n77.5\nLong context\n25.5\n24.5\n25.5\n0.0\n27.0\n25.4\nMath\n74.1\n57.7\n65.0\n67.9\n60.2\n80.8\nCode generation\n68.3\n56.9\n65.8\n58.3\n66.8\n69.9\nMultilingual\n65.8\n55.3\n47.5\n59.6\n64.3\n76.6\nOverall, Phi-3.5-MoE with only 6.6B active parameters achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4o-mini. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.\nMultilingual\nThe table below highlights multilingual capability of Phi-3.5-MoE on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 6.6B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\nCategory\nPhi-3.5-MoE-instruct\nMistral-Nemo-12B-instruct-2407\nLlama-3.1-8B-instruct\nGemma-2-9b-It\nGemini-1.5-Flash\nGPT-4o-mini-2024-07-18 (Chat)\nMultilingual MMLU\n69.9\n58.9\n56.2\n63.8\n77.2\n72.9\nMultilingual MMLU-Pro\n45.3\n34.0\n21.4\n43.0\n57.9\n53.2\nMGSM\n58.7\n63.3\n56.7\n75.1\n75.8\n81.7\nMEGA MLQA\n65.3\n61.2\n45.2\n54.4\n61.6\n70.0\nMEGA TyDi QA\n67.1\n63.7\n54.5\n65.6\n63.6\n81.8\nMEGA UDPOS\n60.4\n58.2\n54.1\n56.6\n62.4\n66.0\nMEGA XCOPA\n76.6\n10.8\n21.1\n31.2\n95.0\n90.3\nMEGA XStoryCloze\n82.8\n92.3\n71.0\n87.0\n20.7\n96.6\nAverage\n65.8\n55.3\n47.5\n59.6\n64.3\n76.6\nLong Context\nPhi-3.5-MoE supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, multilingual context retrieval. We see that Phi-3.5 is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-MoE-instruct is very competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, and Mistral-Nemo-12B-instruct-2407.\nBenchmark\nPhi-3.5-MoE-instruct\nMistral-Nemo-12B-instruct-2407\nLlama-3.1-8B-instruct\nGemini-1.5-Flash\nGPT-4o-mini-2024-07-18 (Chat)\nGovReport\n26.4\n25.6\n25.1\n27.8\n24.8\nQMSum\n19.9\n22.1\n21.6\n24.0\n21.7\nQasper\n40.0\n30.7\n37.2\n43.5\n39.8\nSQuALITY\n24.1\n25.8\n26.2\n23.5\n23.8\nSummScreenFD\n16.9\n18.2\n17.6\n16.3\n17.0\nAverage\n25.5\n24.5\n25.5\n27.0\n25.4\nRULER: a retrieval-based benchmark for long context understanding\nModel\n4K\n8K\n16K\n32K\n64K\n128K\nAverage\nPhi-3.5-MoE-instruct\n94.8\n93\n93.2\n91.6\n85.7\n64.2\n87.1\nLlama-3.1-8B-instruct\n95.5\n93.8\n91.6\n87.4\n84.7\n77.0\n88.3\nMistral-Nemo-12B-instruct-2407\n87.8\n87.2\n87.7\n69.0\n46.8\n19.0\n66.2\nRepoQA: a benchmark for long context code understanding\nModel\nPython\nC++\nRust\nJava\nTypeScript\nAverage\nPhi-3.5-MoE-instruct\n89\n74\n81\n88\n95\n85\nLlama-3.1-8B-instruct\n80\n65\n73\n76\n63\n71\nMistral-7B-instruct-v0.3\n61\n57\n51\n61\n80\n62\nTraining\nModel\nArchitecture: Phi-3.5-MoE has 16x3.8B parameters with 6.6B active parameters when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.\nInputs: Text. It is best suited for prompts using chat format.\nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 23 days\nTraining data: 4.9T tokens\nOutputs: Generated text in response to the input\nDates: Trained between April and August 2024\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.\nSupported languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\nRelease date: August 2024\nTraining Datasets\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of\npublicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\nnewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the Phi-3 Technical Report.\nResponsible AI Considerations\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\nMultilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nLong Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nSafety Evaluation and Red-Teaming\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to\nevaluate Phi-3.5 models' propensity to produce undesirable outputs across multiple languages and risk categories.\nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety\npost-training that was done as detailed in the Phi-3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by\nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted\nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as\nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the Phi-3 Safety Post-Training paper.\nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output\nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings\nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages,\nand risk areas that account for cultural nuances where those languages are spoken.\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nHardware\nNote that by default, the Phi-3.5-MoE-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\nAppendix A: Korean benchmarks\nThe prompt is the same as the CLIcK paper prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\nGPT-4o: 2024-05-13 version\nGPT-4o-mini: 2024-07-18 version\nGPT-4-turbo: 2024-04-09 version\nGPT-3.5-turbo: 2023-06-13 version\nOverall, the Phi-3.5 MoE model with just 6.6B active params outperforms GPT-3.5-Turbo.\nBenchmarks\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nCLIcK\n56.44\n29.12\n47.82\n80.46\n68.5\n72.82\n50.98\nHAERAE 1.0\n61.83\n36.41\n53.9\n85.7\n76.4\n77.76\n52.67\nKMMLU (0-shot, CoT)\n47.43\n30.82\n38.54\n64.26\n52.63\n58.75\n40.3\nKMMLU (5-shot)\n47.92\n29.98\n20.21\n64.28\n51.62\n59.29\n42.28\nKMMLU-HARD (0-shot, CoT)\n25.34\n25.68\n24.03\n39.62\n24.56\n30.56\n20.97\nKMMLU-HARD (5-shot)\n25.66\n25.73\n15.81\n40.94\n24.63\n31.12\n21.19\nAverage\n45.82\n29.99\n29.29\n62.54\n50.08\n56.74\n39.61\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nAccuracy by supercategory\nsupercategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nCulture\n58.44\n29.74\n51.15\n81.89\n70.95\n73.61\n53.38\nLanguage\n52.31\n27.85\n40.92\n77.54\n63.54\n71.23\n46\nOverall\n56.44\n29.12\n47.82\n80.46\n68.5\n72.82\n50.98\nAccuracy by category\nsupercategory\ncategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nCulture\nEconomy\n77.97\n28.81\n66.1\n94.92\n83.05\n89.83\n64.41\nCulture\nGeography\n60.31\n29.01\n54.2\n80.15\n77.86\n82.44\n53.44\nCulture\nHistory\n33.93\n30\n29.64\n66.92\n48.4\n46.4\n31.79\nCulture\nLaw\n52.51\n22.83\n44.29\n70.78\n57.53\n61.19\n41.55\nCulture\nPolitics\n70.24\n33.33\n59.52\n88.1\n83.33\n89.29\n65.48\nCulture\nPop Culture\n80.49\n34.15\n60.98\n97.56\n85.37\n92.68\n75.61\nCulture\nSociety\n74.43\n31.72\n65.05\n92.88\n85.44\n86.73\n71.2\nCulture\nTradition\n58.11\n31.98\n54.95\n87.39\n74.77\n79.28\n55.86\nLanguage\nFunctional\n48\n24\n32.8\n84.8\n64.8\n80\n40\nLanguage\nGrammar\n29.58\n23.33\n22.92\n57.08\n42.5\n47.5\n30\nLanguage\nTextual\n73.33\n33.33\n59.65\n91.58\n80.7\n87.37\n62.11\nHAERAE 1.0\ncategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nGeneral Knowledge\n39.77\n28.41\n34.66\n77.27\n53.41\n66.48\n40.91\nHistory\n60.64\n22.34\n44.15\n92.02\n84.57\n78.72\n30.32\nLoan Words\n70.41\n35.5\n63.31\n79.88\n76.33\n78.11\n59.17\nRare Words\n63.95\n42.96\n63.21\n87.9\n81.98\n79.01\n61.23\nReading Comprehension\n64.43\n41.16\n51.9\n85.46\n77.18\n80.09\n56.15\nStandard Nomenclature\n66.01\n32.68\n58.82\n88.89\n75.82\n79.08\n53.59\nOverall\n61.83\n36.41\n53.9\n85.7\n76.4\n77.76\n52.67\nKMMLU (0-shot, CoT)\nsupercategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n45.15\n31.68\n37.03\n61.52\n49.29\n55.98\n38.47\nHUMSS\n49.75\n26.47\n37.29\n69.45\n56.59\n63\n40.9\nOther\n47.24\n31.01\n39.15\n63.79\n52.35\n57.53\n40.19\nSTEM\n49.08\n31.9\n40.42\n65.16\n54.74\n60.84\n42.24\nOverall\n47.43\n30.82\n38.54\n64.26\n52.63\n58.75\n40.3\nKMMLU (5-shot)\nsupercategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n45.9\n29.98\n19.24\n61.47\n48.66\n56.85\n40.22\nHUMSS\n49.18\n27.27\n22.5\n68.79\n55.95\n63.68\n43.35\nOther\n48.43\n30.76\n20.95\n64.21\n51.1\n57.85\n41.92\nSTEM\n49.21\n30.73\n19.55\n65.28\n53.29\n61.08\n44.43\nOverall\n47.92\n29.98\n20.21\n64.28\n51.62\n59.29\n42.28\nKMMLU-HARD (0-shot, CoT)\nsupercategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n25.83\n26.17\n26.25\n37.12\n22.25\n29.17\n21.07\nHUMSS\n21.52\n24.38\n20.21\n41.97\n23.31\n31.51\n19.44\nOther\n24.82\n24.82\n23.88\n40.39\n26.48\n29.59\n22.22\nSTEM\n28.18\n26.91\n24.64\n39.82\n26.36\n32.18\n20.91\nOverall\n25.34\n25.68\n24.03\n39.62\n24.56\n30.56\n20.97\nKMMLU-HARD (5-shot)\nsupercategory\nPhi-3.5-MoE-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n21\n29\n12\n31\n21\n25\n20\nHUMSS\n22.88\n19.92\n14\n43.98\n23.47\n33.53\n19.53\nOther\n25.13\n27.27\n12.83\n39.84\n28.34\n29.68\n23.22\nSTEM\n21.75\n25.25\n12.75\n40.25\n23.25\n27.25\n19.75\nOverall\n25.66\n25.73\n15.81\n40.94\n24.63\n31.12\n21.19",
    "MarinaraSpaghetti/NemoMix-Unleashed-12B": "Information\nDetails\nInstruct\nParameters\nSettings\nGGUF\nEXL2\nNemoMix-Unleashed-12B\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nKo-fi\nEnjoying what I do? Consider donating here, thank you!\nInformation\nDetails\nOkay, I tried really hard to improve my ChatML merges, but that has gone terribly wrong. Everyone is adding special tokens with different IDs so can't even make a proper union tokenizer for them, damn. Not to mention, I made some... interesting discoveres in regards to some models' context lenghts. You can watch the breakdown of how it went down here: https://www.captiongenerator.com/v/2303039/marinaraspaghetti's-merging-experience.\nThis one feels a bit different to my previous attempts and seems less prone to repetition, especially on higher contexts, which is great for me! I'll probably improve on it even further, but for now, it feels rather nice. Great for RP and storytelling. All credits and thanks go to the amazing MistralAI, Intervitens, Sao10K and Nbeerbower for their amazing models! Plus, special shoutouts to Parasitic Rogue for ideas and Prodeus Unity and Statuo for cool exl2 quants of my previous merges. Cheers to folks over at the Drummer's server! Have a good one, everyone.\nInstruct\nSigh, Mistral Instruct, I'm afraid.\nUPDATE: WE HAD THE WRONG FORMAT ALL ALONG, JUST RECEIVED HOW IT'S SUPPOSED TO LOOK LIKE FROM THE OFFICIAL MISTRALAI TEAM MEMBER.\n...This had made me question everything I thought I knew.\n<s>[INST]{system}[/INST]{response}</s>[INST]{user's message}[/INST]{response}</s>\nParameters\nI recommend running Temperature 1.0-1.25 with 0.1 Top A or 0.01-0.1 Min P, and with 0.8/1.75/2/0 DRY. Also works with lower Temperatures below 1.0. Nothing more needed.\nSettings\nYou can use my exact settings from here (use the ones from the Mistral Base/Customized folder, I also recommend checking the Mistral Improved folder): https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings/tree/main.\nGGUF\nhttps://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF\nEXL2\nhttps://huggingface.co/Statuo/NemoMix-Unleashed-EXL2-8bpw\nNemoMix-Unleashed-12B\nThis is a merge of pre-trained language models created using mergekit.\nMerge Details\nMerge Method\nThis model was merged using the della_linear merge method using E:\\mergekit\\mistralaiMistral-Nemo-Base-2407 as a base.\nModels Merged\nThe following models were included in the merge:\nE:\\mergekit\\intervitens_mini-magnum-12b-v1.1\nE:\\mergekit\\nbeerbower_mistral-nemo-bophades-12B\nE:\\mergekit\\Sao10K_MN-12B-Lyra-v1\nE:\\mergekit\\nbeerbower_mistral-nemo-gutenberg-12B\nE:\\mergekit\\mistralaiMistral-Nemo-Instruct-2407\nConfiguration\nThe following YAML configuration was used to produce this model:\nmodels:\n- model: E:\\mergekit\\mistralaiMistral-Nemo-Instruct-2407\nparameters:\nweight: 0.1\ndensity: 0.4\n- model: E:\\mergekit\\nbeerbower_mistral-nemo-bophades-12B\nparameters:\nweight: 0.12\ndensity: 0.5\n- model: E:\\mergekit\\nbeerbower_mistral-nemo-gutenberg-12B\nparameters:\nweight: 0.2\ndensity: 0.6\n- model: E:\\mergekit\\Sao10K_MN-12B-Lyra-v1\nparameters:\nweight: 0.25\ndensity: 0.7\n- model: E:\\mergekit\\intervitens_mini-magnum-12b-v1.1\nparameters:\nweight: 0.33\ndensity: 0.8\nmerge_method: della_linear\nbase_model: E:\\mergekit\\mistralaiMistral-Nemo-Base-2407\nparameters:\nepsilon: 0.05\nlambda: 1\ndtype: bfloat16\ntokenizer_source: base\nKo-fi\nEnjoying what I do? Consider donating here, thank you!\nhttps://ko-fi.com/spicy_marinara",
    "LiuZichen/MagicQuill-models": "Model Checkpoints for project.\nThe paper is MagicQuill: An Intelligent Interactive Image Editing System.",
    "HF1BitLLM/Llama3-8B-1.58-100B-tokens": "Model Card for Model ID\nModel Details\nModel Sources\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Process\nEvaluation\nCitation\nModel Card for Model ID\nLlama3-8B-1.58 Models\nThe Llama3-8B-1.58 models are large language models fine-tuned on the BitNet 1.58b architecture, starting from the base model Llama-3-8B-Instruct.\nFor a deeper dive into the methods and results, check out our blog post.\nModel Details\nModel Sources\nRepository: Model\nPaper: The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\nHow to Get Started with the Model\nYou can easily load and test our model in Transformers. Just follow the code below:\nStart by installing the transformers version with the correct configuration to load bitnet models\npip install git+https://github.com/huggingface/transformers.git@refs/pull/33410/head\nAnd then load the model :\nmodel = AutoModelForCausalLM.from_pretrained(\"HF1BitLLM/Llama3-8B-1.58-100B-tokens\", device_map=\"cuda\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\ninput_text = \"Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\\nAnswer:\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\noutput = model.generate(input_ids, max_length=10, do_sample=False)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_text)\nTraining Details\nTraining Data\nThe model was trained on a subset of FineWeb-edu\nTraining Process\nStarting Point\nBest-performing checkpoint from the 10 billion token runs with a linear lambda scheduler\nTraining Duration\nFine-tuned for an additional 45,000 steps\nReached a total of 100 billion tokens\nDataset\nFineWeb-edu dataset\nBatch Size\n2 million tokens per step\nTotal per run: 45,000 steps * 2 million tokens = 90 billion tokens\nCombined with initial 10 billion tokens to reach 100 billion\nLearning Rate Experiments\nTested various learning rates to find optimal setting, according the to experiments, the best performing peak lr is 1e-5\nPerformance\nClose to Llama3 8B on some metrics\nBehind Llama3 8B in overall average performance\nEvaluation\nMetrics included perplexity, MMLU scores, and other standard benchmarks\nThese extended training runs on 100 billion tokens pushed the boundaries of highly quantized models, bringing performance closer to half-precision models like Llama3.\nEvaluation\nThe evaluation of the models is done on the nanotron checkpoints using LightEval :\nCitation\n@misc{,\ntitle={1.58-Bit LLM: A New Era of Extreme Quantization},\nauthor={Mohamed Mekkouri and Marc Sun and Leandro von Werra and Thomas Wolf},\nyear={2024},\n}",
    "Qwen/Qwen2.5-7B": "Qwen2.5-7B\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nQwen2.5-7B\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the base 7B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: 131,072 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-14B": "Qwen2.5-14B\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nQwen2.5-14B\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the base 14B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 14.7B\nNumber of Paramaters (Non-Embedding): 13.1B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: 131,072 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-3B-Instruct-GGUF": "Qwen2.5-3B-Instruct-GGUF\nIntroduction\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-3B-Instruct-GGUF\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 3B Qwen2.5 model in the GGUF Format, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 3.09B\nNumber of Paramaters (Non-Embedding): 2.77B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 16 for Q and 2 for KV\nContext Length: Full 32,768 tokens and generation 8192 tokens\nQuantization: q2_K, q3_K_M, q4_0, q4_K_M, q5_0, q5_K_M, q6_K, q8_0\nFor more details, please refer to our blog, GitHub, and Documentation.\nQuickstart\nCheck out our llama.cpp documentation for more usage guide.\nWe advise you to clone llama.cpp and install it following the official guide. We follow the latest version of llama.cpp.\nIn the following demonstration, we assume that you are running commands under the repository llama.cpp.\nSince cloning the entire repo may be inefficient, you can manually download the GGUF file that you need or use huggingface-cli:\nInstallpip install -U huggingface_hub\nDownload:huggingface-cli download Qwen/Qwen2.5-3B-Instruct-GGUF qwen2.5-3b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\nFor users, to achieve chatbot-like experience, it is recommended to commence in the conversation mode:\n./llama-cli -m <gguf-file-path> \\\n-co -cnv -p \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\" \\\n-fa -ngl 80 -n 512\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor quantized models, the benchmark results against the original bfloat16 models can be found here\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-3B-Instruct": "Qwen2.5-3B-Instruct\nIntroduction\nRequirements\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-3B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 3B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 3.09B\nNumber of Paramaters (Non-Embedding): 2.77B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 16 for Q and 2 for KV\nContext Length: Full 32,768 tokens and generation 8192 tokens\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "dragonkue/BGE-m3-ko": "SentenceTransformer\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nEvaluation\nMetrics\nBias, Risks and Limitations\nTraining Hyperparameters\nCitation\nBibTeX\nSentenceTransformer\nThis is a sentence-transformers model trained on the train_set dataset. It maps sentences & paragraphs to a 1024-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\nModel Details\nLearning other languages ‚Äã‚Äãbesides Chinese and English is insufficient, so additional learning is needed to optimize use of other languages.\nThis model is additionally trained on the Korean dataset.\nModel Description\nModel Type: Sentence Transformer\nTransformer Encoder\nMaximum Sequence Length: 8192 tokens\nOutput Dimensionality: 1024 tokens\nSimilarity Function: Cosine Similarity\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Normalize()\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"dragonkue/bge-m3-ko\")\n# Run inference\nsentences = [\n'ÏàòÍ∏âÍ∂åÏûê Ï§ë Í∑ºÎ°ú Îä•Î†•Ïù¥ ÏóÜÎäî ÏûÑÏÇ∞Î∂ÄÎäî Î™á Ï¢ÖÏóê Ìï¥ÎãπÌïòÎãà?',\n'ÎÇ¥ÎÖÑÎ∂ÄÌÑ∞ Ï†ÄÏÜåÎìùÏ∏µ 1ÏÑ∏ ÎØ∏Îßå ÏïÑÎèôÏùò \\nÏùòÎ£åÎπÑ Î∂ÄÎã¥Ïù¥ Îçî ÎÇÆÏïÑÏßÑÎã§!\\nÏùòÎ£åÍ∏âÏó¨Ï†úÎèÑ Í∞úÏöî\\n‚ñ° (Î™©Ï†Å) ÏÉùÌôúÏú†ÏßÄ Îä•Î†•Ïù¥ ÏóÜÍ±∞ÎÇò ÏÉùÌôúÏù¥ Ïñ¥Î†§Ïö¥ Íµ≠ÎØºÎì§ÏóêÍ≤å Î∞úÏÉùÌïòÎäî ÏßàÎ≥ë, Î∂ÄÏÉÅ, Ï∂úÏÇ∞ Îì±Ïóê ÎåÄÌï¥ Íµ≠Í∞ÄÍ∞Ä ÏùòÎ£åÏÑúÎπÑÏä§ Ï†úÍ≥µ\\n‚ñ° (ÏßÄÏõêÎåÄÏÉÅ) Íµ≠ÎØºÍ∏∞Ï¥àÏÉùÌôúÎ≥¥Ïû• ÏàòÍ∏âÍ∂åÏûê, ÌÉÄ Î≤ïÏóê ÏùòÌïú ÏàòÍ∏âÍ∂åÏûê Îì±\\n\\n| Íµ¨Î∂Ñ | Íµ≠ÎØºÍ∏∞Ï¥àÏÉùÌôúÎ≥¥Ïû•Î≤ïÏóê ÏùòÌïú ÏàòÍ∏âÍ∂åÏûê | Íµ≠ÎØºÍ∏∞Ï¥àÏÉùÌôúÎ≥¥Ïû•Î≤ï Ïù¥Ïô∏Ïùò ÌÉÄ Î≤ïÏóê ÏùòÌïú ÏàòÍ∏âÍ∂åÏûê |\\n| --- | --- | --- |\\n| 1Ï¢Ö | ‚óã Íµ≠ÎØºÍ∏∞Ï¥àÏÉùÌôúÎ≥¥Ïû• ÏàòÍ∏âÍ∂åÏûê Ï§ë Í∑ºÎ°úÎä•Î†•Ïù¥ ÏóÜÎäî ÏûêÎßåÏúºÎ°ú Íµ¨ÏÑ±Îêú Í∞ÄÍµ¨ - 18ÏÑ∏ ÎØ∏Îßå, 65ÏÑ∏ Ïù¥ÏÉÅ - 4Í∏â Ïù¥ÎÇ¥ Ïû•Ïï†Ïù∏ - ÏûÑÏÇ∞Î∂Ä, Î≥ëÏó≠ÏùòÎ¨¥Ïù¥ÌñâÏûê Îì± | ‚óã Ïù¥Ïû¨ÎØº(Ïû¨Ìï¥Íµ¨Ìò∏Î≤ï) ‚óã ÏùòÏÉÅÏûê Î∞è ÏùòÏÇ¨ÏûêÏùò Ïú†Ï°±‚óã Íµ≠ÎÇ¥ ÏûÖÏñëÎêú 18ÏÑ∏ ÎØ∏Îßå ÏïÑÎèô‚óã Íµ≠Í∞ÄÏú†Í≥µÏûê Î∞è Í∑∏ Ïú†Ï°±‚Ä§Í∞ÄÏ°±‚óã Íµ≠Í∞ÄÎ¨¥ÌòïÎ¨∏ÌôîÏû¨ Î≥¥Ïú†Ïûê Î∞è Í∑∏ Í∞ÄÏ°±‚óã ÏÉàÌÑ∞ÎØº(Î∂ÅÌïúÏù¥ÌÉàÏ£ºÎØº)Í≥º Í∑∏ Í∞ÄÏ°±‚óã 5‚Ä§18 ÎØºÏ£ºÌôîÏö¥Îèô Í¥ÄÎ†®Ïûê Î∞è Í∑∏ Ïú†Í∞ÄÏ°±‚óã ÎÖ∏ÏàôÏù∏ ‚Äª ÌñâÎ†§ÌôòÏûê (ÏùòÎ£åÍ∏âÏó¨Î≤ï ÏãúÌñâÎ†π) |\\n| 2Ï¢Ö | ‚óã Íµ≠ÎØºÍ∏∞Ï¥àÏÉùÌôúÎ≥¥Ïû• ÏàòÍ∏âÍ∂åÏûê Ï§ë Í∑ºÎ°úÎä•Î†•Ïù¥ ÏûàÎäî Í∞ÄÍµ¨ | - |\\n',\n'Ïù¥Ïñ¥ Ïù¥ÎÇ† Ïò§ÌõÑ 1Ïãú30Î∂ÑÎ∂ÄÌÑ∞ Ïó¥Î¶¥ ÏòàÏ†ïÏù¥Îçò Ïä§ÎÖ∏Î≥¥Îìú Ïó¨Ïûê Ïä¨Î°úÌîÑÏä§ÌÉÄÏùº ÏòàÏÑ† Í≤ΩÍ∏∞Îäî Ïó∞Í∏∞Î•º Í±∞Îì≠ÌïòÎã§ Ï∑®ÏÜåÎêêÎã§. Ï°∞ÏßÅÏúÑÎäî ÏòàÏÑ† ÏóÜÏù¥ Îã§Ïùå ÎÇ† Í≤∞ÏÑ†ÏóêÏÑú Ï∞∏Í∞ÄÏûê 27Î™ÖÏù¥ ÌïúÎ≤àÏóê Í≤ΩÍ∏∞Ìï¥ ÏàúÏúÑÎ•º Í∞ÄÎ¶¨Í∏∞Î°ú ÌñàÎã§.',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 1024]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\nEvaluation\nMetrics\nndcg, mrr, map metrics are metrics that consider ranking, while accuracy, precision, and recall are metrics that do not consider ranking. (Example: When considering ranking for retrieval top 10, different scores are given when the correct document is in 1st place and when it is in 10th place. However, accuracy, precision, and recall scores are the same if they are in the top 10.)\nInformation Retrieval\nKorean Embedding Benchmark is a benchmark with a relatively long 3/4 quantile of string length of 1024\nKorean Embedding Benchmark with AutoRAG\nThis is a benchmark of Korean embedding models.\n(https://github.com/Marker-Inc-Korea/AutoRAG-example-korean-embedding-benchmark)\nTop-k 1\nModel name\nF1\nRecall\nPrecision\nmAP\nmRR\nNDCG\nparaphrase-multilingual-mpnet-base-v2\n0.3596\n0.3596\n0.3596\n0.3596\n0.3596\n0.3596\nKoSimCSE-roberta\n0.4298\n0.4298\n0.4298\n0.4298\n0.4298\n0.4298\nCohere embed-multilingual-v3.0\n0.3596\n0.3596\n0.3596\n0.3596\n0.3596\n0.3596\nopenai ada 002\n0.4737\n0.4737\n0.4737\n0.4737\n0.4737\n0.4737\nmultilingual-e5-large-instruct\n0.4649\n0.4649\n0.4649\n0.4649\n0.4649\n0.4649\nUpstage Embedding\n0.6579\n0.6579\n0.6579\n0.6579\n0.6579\n0.6579\nparaphrase-multilingual-MiniLM-L12-v2\n0.2982\n0.2982\n0.2982\n0.2982\n0.2982\n0.2982\nopenai_embed_3_small\n0.5439\n0.5439\n0.5439\n0.5439\n0.5439\n0.5439\nko-sroberta-multitask\n0.4211\n0.4211\n0.4211\n0.4211\n0.4211\n0.4211\nopenai_embed_3_large\n0.6053\n0.6053\n0.6053\n0.6053\n0.6053\n0.6053\nKU-HIAI-ONTHEIT-large-v1\n0.7105\n0.7105\n0.7105\n0.7105\n0.7105\n0.7105\nKU-HIAI-ONTHEIT-large-v1.1\n0.7193\n0.7193\n0.7193\n0.7193\n0.7193\n0.7193\nkf-deberta-multitask\n0.4561\n0.4561\n0.4561\n0.4561\n0.4561\n0.4561\ngte-multilingual-base\n0.5877\n0.5877\n0.5877\n0.5877\n0.5877\n0.5877\nKoE5\n0.7018\n0.7018\n0.7018\n0.7018\n0.7018\n0.7018\nBGE-m3\n0.6578\n0.6578\n0.6578\n0.6578\n0.6578\n0.6578\nbge-m3-korean\n0.5351\n0.5351\n0.5351\n0.5351\n0.5351\n0.5351\nBGE-m3-ko\n0.7456\n0.7456\n0.7456\n0.7456\n0.7456\n0.7456\nTop-k 3\nModel name\nF1\nRecall\nPrecision\nmAP\nmRR\nNDCG\nparaphrase-multilingual-mpnet-base-v2\n0.2368\n0.4737\n0.1579\n0.2032\n0.2032\n0.2712\nKoSimCSE-roberta\n0.3026\n0.6053\n0.2018\n0.2661\n0.2661\n0.3515\nCohere embed-multilingual-v3.0\n0.2851\n0.5702\n0.1901\n0.2515\n0.2515\n0.3321\nopenai ada 002\n0.3553\n0.7105\n0.2368\n0.3202\n0.3202\n0.4186\nmultilingual-e5-large-instruct\n0.3333\n0.6667\n0.2222\n0.2909\n0.2909\n0.3856\nUpstage Embedding\n0.4211\n0.8421\n0.2807\n0.3509\n0.3509\n0.4743\nparaphrase-multilingual-MiniLM-L12-v2\n0.2061\n0.4123\n0.1374\n0.1740\n0.1740\n0.2340\nopenai_embed_3_small\n0.3640\n0.7281\n0.2427\n0.3026\n0.3026\n0.4097\nko-sroberta-multitask\n0.2939\n0.5877\n0.1959\n0.2500\n0.2500\n0.3351\nopenai_embed_3_large\n0.3947\n0.7895\n0.2632\n0.3348\n0.3348\n0.4491\nKU-HIAI-ONTHEIT-large-v1\n0.4386\n0.8772\n0.2924\n0.3421\n0.3421\n0.4766\nKU-HIAI-ONTHEIT-large-v1.1\n0.4430\n0.8860\n0.2953\n0.3406\n0.3406\n0.4778\nkf-deberta-multitask\n0.3158\n0.6316\n0.2105\n0.2792\n0.2792\n0.3679\ngte-multilingual-base\n0.4035\n0.8070\n0.2690\n0.3450\n0.3450\n0.4614\nKoE5\n0.4254\n0.8509\n0.2836\n0.3173\n0.3173\n0.4514\nBGE-m3\n0.4254\n0.8508\n0.2836\n0.3421\n0.3421\n0.4701\nbge-m3-korean\n0.3684\n0.7368\n0.2456\n0.3143\n0.3143\n0.4207\nBGE-m3-ko\n0.4517\n0.9035\n0.3011\n0.3494\n0.3494\n0.4886\nTop-k 5\nModel name\nF1\nRecall\nPrecision\nmAP\nmRR\nNDCG\nparaphrase-multilingual-mpnet-base-v2\n0.1813\n0.5439\n0.1088\n0.1575\n0.1575\n0.2491\nKoSimCSE-roberta\n0.2164\n0.6491\n0.1298\n0.1751\n0.1751\n0.2873\nCohere embed-multilingual-v3.0\n0.2076\n0.6228\n0.1246\n0.1640\n0.1640\n0.2731\nopenai ada 002\n0.2602\n0.7807\n0.1561\n0.2139\n0.2139\n0.3486\nmultilingual-e5-large-instruct\n0.2544\n0.7632\n0.1526\n0.2194\n0.2194\n0.3487\nUpstage Embedding\n0.2982\n0.8947\n0.1789\n0.2237\n0.2237\n0.3822\nparaphrase-multilingual-MiniLM-L12-v2\n0.1637\n0.4912\n0.0982\n0.1437\n0.1437\n0.2264\nopenai_embed_3_small\n0.2690\n0.8070\n0.1614\n0.2148\n0.2148\n0.3553\nko-sroberta-multitask\n0.2164\n0.6491\n0.1298\n0.1697\n0.1697\n0.2835\nopenai_embed_3_large\n0.2807\n0.8421\n0.1684\n0.2088\n0.2088\n0.3586\nKU-HIAI-ONTHEIT-large-v1\n0.3041\n0.9123\n0.1825\n0.2137\n0.2137\n0.3783\nKU-HIAI-ONTHEIT-large-v1.1\n0.3099\n0.9298\n0.1860\n0.2148\n0.2148\n0.3834\nkf-deberta-multitask\n0.2281\n0.6842\n0.1368\n0.1724\n0.1724\n0.2939\ngte-multilingual-base\n0.2865\n0.8596\n0.1719\n0.2096\n0.2096\n0.3637\nKoE5\n0.2982\n0.8947\n0.1789\n0.2054\n0.2054\n0.3678\nBGE-m3\n0.3041\n0.9123\n0.1825\n0.2193\n0.2193\n0.3832\nbge-m3-korean\n0.2661\n0.7982\n0.1596\n0.2116\n0.2116\n0.3504\nBGE-m3-ko\n0.3099\n0.9298\n0.1860\n0.2098\n0.2098\n0.3793\nTop-k 10\nModel name\nF1\nRecall\nPrecision\nmAP\nmRR\nNDCG\nparaphrase-multilingual-mpnet-base-v2\n0.1212\n0.6667\n0.0667\n0.1197\n0.1197\n0.2382\nKoSimCSE-roberta\n0.1324\n0.7281\n0.0728\n0.1080\n0.1080\n0.2411\nCohere embed-multilingual-v3.0\n0.1324\n0.7281\n0.0728\n0.1150\n0.1150\n0.2473\nopenai ada 002\n0.1563\n0.8596\n0.0860\n0.1051\n0.1051\n0.2673\nmultilingual-e5-large-instruct\n0.1483\n0.8158\n0.0816\n0.0980\n0.0980\n0.2520\nUpstage Embedding\n0.1707\n0.9386\n0.0939\n0.1078\n0.1078\n0.2848\nparaphrase-multilingual-MiniLM-L12-v2\n0.1053\n0.5789\n0.0579\n0.0961\n0.0961\n0.2006\nopenai_embed_3_small\n0.1547\n0.8509\n0.0851\n0.0984\n0.0984\n0.2593\nko-sroberta-multitask\n0.1276\n0.7018\n0.0702\n0.0986\n0.0986\n0.2275\nopenai_embed_3_large\n0.1643\n0.9035\n0.0904\n0.1180\n0.1180\n0.2855\nKU-HIAI-ONTHEIT-large-v1\n0.1707\n0.9386\n0.0939\n0.1105\n0.1105\n0.2860\nKU-HIAI-ONTHEIT-large-v1.1\n0.1722\n0.9474\n0.0947\n0.1033\n0.1033\n0.2822\nkf-deberta-multitask\n0.1388\n0.7632\n0.0763\n0.1\n0.1\n0.2422\ngte-multilingual-base\n0.1675\n0.9211\n0.0921\n0.1066\n0.1066\n0.2805\nKoE5\n0.1675\n0.9211\n0.0921\n0.1011\n0.1011\n0.2750\nBGE-m3\n0.1707\n0.9386\n0.0939\n0.1130\n0.1130\n0.2884\nbge-m3-korean\n0.1579\n0.8684\n0.0868\n0.1093\n0.1093\n0.2721\nBGE-m3-ko\n0.1770\n0.9736\n0.0974\n0.1097\n0.1097\n0.2932\nInformation Retrieval\nDataset: miracl-ko (https://github.com/project-miracl/miracl)\nmiracl benchmark is a benchmark with a relatively short 3/4 quantile of string length of 220 on the Korean Wikidata set.\nEvaluated with InformationRetrievalEvaluator\nMetric\nValue\ncosine_accuracy@1\n0.6103\ncosine_accuracy@3\n0.8169\ncosine_accuracy@5\n0.8732\ncosine_accuracy@10\n0.9202\ncosine_precision@1\n0.6103\ncosine_precision@3\n0.3787\ncosine_precision@5\n0.2761\ncosine_precision@10\n0.1728\ncosine_recall@1\n0.3847\ncosine_recall@3\n0.5902\ncosine_recall@5\n0.6794\ncosine_recall@10\n0.7695\ncosine_ndcg@10\n0.6833\ncosine_mrr@10\n0.7262\ncosine_map@100\n0.6074\ndot_accuracy@1\n0.6103\ndot_accuracy@3\n0.8169\ndot_accuracy@5\n0.8732\ndot_accuracy@10\n0.9202\ndot_precision@1\n0.6103\ndot_precision@3\n0.3787\ndot_precision@5\n0.2761\ndot_precision@10\n0.1728\ndot_recall@1\n0.3847\ndot_recall@3\n0.5902\ndot_recall@5\n0.6794\ndot_recall@10\n0.7695\ndot_ndcg@10\n0.6723\ndot_mrr@10\n0.7262\ndot_map@100\n0.6074\nBias, Risks and Limitations\nSince the evaluation results are different for each domain, it is necessary to compare and evaluate the model in your own domain. In the Miracl benchmark, the evaluation was conducted using the Korean Wikipedia as a corpus, and in this case, the cosine_ndcg@10 score dropped by 0.02 points after learning. However, in the Auto-RAG benchmark, which is a financial domain, the ndcg score increased by 0.09 when it was top 1. This model may be advantageous for use in a specific domain.\nAlso, since the miracl benchmark consists of a corpus of relatively short strings, while the Korean Embedding Benchmark consists of a corpus of longer strings, this model may be more advantageous if the length of the corpus you want to use is long.\nTraining Hyperparameters\nNon-Default Hyperparameters\nThe batch size was referenced from the following paper: Text Embeddings by Weakly-Supervised Contrastive Pre-training (https://arxiv.org/pdf/2212.03533)\neval_strategy: steps\nper_device_train_batch_size: 32768\nper_device_eval_batch_size: 32768\nlearning_rate: 3e-05\nwarmup_ratio: 0.03333333333333333\nfp16: True\nbatch_sampler: no_duplicates\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: False\ndo_predict: False\neval_strategy: steps\nprediction_loss_only: True\nper_device_train_batch_size: 32768\nper_device_eval_batch_size: 32768\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 1\neval_accumulation_steps: None\nlearning_rate: 3e-05\nweight_decay: 0.0\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1.0\nnum_train_epochs: 3\nmax_steps: -1\nlr_scheduler_type: linear\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.03333333333333333\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 42\ndata_seed: None\njit_mode_eval: False\nuse_ipex: False\nbf16: False\nfp16: True\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: None\nlocal_rank: 0\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: True\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: False\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch\noptim_args: None\nadafactor: False\ngroup_by_length: False\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: False\nhub_always_push: False\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ndispatch_batches: None\nsplit_batches: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\nbatch_sampler: no_duplicates\nmulti_dataset_batch_sampler: proportional\nCitation\nBibTeX\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\n@misc{bge-m3,\ntitle={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\nauthor={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\nyear={2024},\neprint={2402.03216},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@article{wang2022text,\ntitle={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\nauthor={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\njournal={arXiv preprint arXiv:2212.03533},\nyear={2022}\n}",
    "bartowski/Qwen2.5-0.5B-Instruct-GGUF": "Llamacpp imatrix Quantizations of Qwen2.5-0.5B-Instruct\nPrompt format\nWhat's new:\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Qwen2.5-0.5B-Instruct\nUsing llama.cpp release b3772 for quantization.\nOriginal model: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nWhat's new:\nUpdate tokenizer\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nQwen2.5-0.5B-Instruct-f16.gguf\nf16\n0.99GB\nfalse\nFull F16 weights.\nQwen2.5-0.5B-Instruct-Q8_0.gguf\nQ8_0\n0.53GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nQwen2.5-0.5B-Instruct-Q6_K_L.gguf\nQ6_K_L\n0.51GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nQwen2.5-0.5B-Instruct-Q6_K.gguf\nQ6_K\n0.51GB\nfalse\nVery high quality, near perfect, recommended.\nQwen2.5-0.5B-Instruct-Q5_K_L.gguf\nQ5_K_L\n0.42GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nQwen2.5-0.5B-Instruct-Q5_K_M.gguf\nQ5_K_M\n0.42GB\nfalse\nHigh quality, recommended.\nQwen2.5-0.5B-Instruct-Q5_K_S.gguf\nQ5_K_S\n0.41GB\nfalse\nHigh quality, recommended.\nQwen2.5-0.5B-Instruct-Q4_K_L.gguf\nQ4_K_L\n0.40GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nQwen2.5-0.5B-Instruct-Q4_K_M.gguf\nQ4_K_M\n0.40GB\nfalse\nGood quality, default size for must use cases, recommended.\nQwen2.5-0.5B-Instruct-Q4_K_S.gguf\nQ4_K_S\n0.39GB\nfalse\nSlightly lower quality with more space savings, recommended.\nQwen2.5-0.5B-Instruct-Q3_K_XL.gguf\nQ3_K_XL\n0.37GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nQwen2.5-0.5B-Instruct-Q3_K_L.gguf\nQ3_K_L\n0.37GB\nfalse\nLower quality but usable, good for low RAM availability.\nQwen2.5-0.5B-Instruct-Q4_0_8_8.gguf\nQ4_0_8_8\n0.35GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below).\nQwen2.5-0.5B-Instruct-Q4_0_4_8.gguf\nQ4_0_4_8\n0.35GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below).\nQwen2.5-0.5B-Instruct-Q4_0_4_4.gguf\nQ4_0_4_4\n0.35GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure.\nQwen2.5-0.5B-Instruct-Q4_0.gguf\nQ4_0\n0.35GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nQwen2.5-0.5B-Instruct-IQ4_XS.gguf\nIQ4_XS\n0.35GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nQwen2.5-0.5B-Instruct-IQ3_M.gguf\nIQ3_M\n0.34GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Qwen2.5-0.5B-Instruct-GGUF --include \"Qwen2.5-0.5B-Instruct-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Qwen2.5-0.5B-Instruct-GGUF --include \"Qwen2.5-0.5B-Instruct-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Qwen2.5-0.5B-Instruct-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF": "Qwen2.5-Coder-7B-Instruct-GGUF\nIntroduction\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-Coder-7B-Instruct-GGUF\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\nThis repo contains the instruction-tuned 7B Qwen2.5-Coder model in the GGUF Format, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 32,768 tokens\nNote: Currently, only vLLM supports YARN for length extrapolating. If you want to process sequences up to 131,072 tokens, please refer to non-GGUF models.\nQuantization: q2_K, q3_K_M, q4_0, q4_K_M, q5_0, q5_K_M, q6_K, q8_0\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nQuickstart\nCheck out our llama.cpp documentation for more usage guide.\nWe advise you to clone llama.cpp and install it following the official guide. We follow the latest version of llama.cpp.\nIn the following demonstration, we assume that you are running commands under the repository llama.cpp.\nSince cloning the entire repo may be inefficient, you can manually download the GGUF file that you need or use huggingface-cli:\nInstallpip install -U huggingface_hub\nDownload:huggingface-cli download Qwen/Qwen2.5-Coder-7B-Instruct-GGUF --include \"qwen2.5-coder-7b-instruct-q5_k_m*.gguf\" --local-dir . --local-dir-use-symlinks False\nFor large files, we split them into multiple segments due to the limitation of file upload. They share a prefix, with a suffix indicating its index. For examples, qwen2.5-coder-7b-instruct-q5_k_m-00001-of-00002.gguf and qwen2.5-coder-7b-instruct-q5_k_m-00002-of-00002.gguf. You need to download all of them.\n(Optional) Merge:\nFor split files, you need to merge them first with the command llama-gguf-split as shown below:# ./llama-gguf-split --merge <first-split-file-path> <merged-file-path>\n./llama-gguf-split --merge qwen2.5-coder-7b-instruct-q5_k_m-00001-of-00002.gguf qwen2.5-coder-7b-instruct-q5_k_m.gguf\nFor users, to achieve chatbot-like experience, it is recommended to commence in the conversation mode:\n./llama-cli -m <gguf-file-path> \\\n-co -cnv -p \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\" \\\n-fa -ngl 80 -n 512\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "meta-llama/Llama-3.2-11B-Vision-Instruct": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.2 COMMUNITY LICENSE AGREEMENT\nLlama 3.2 Version Release Date: September 25, 2024\n‚ÄúAgreement‚Äù means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n‚ÄúDocumentation‚Äù means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n‚ÄúLicensee‚Äù or ‚Äúyou‚Äù means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n‚ÄúLlama 3.2‚Äù means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n‚ÄúLlama Materials‚Äù means, collectively, Meta‚Äôs proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n‚ÄúMeta‚Äù or ‚Äúwe‚Äù means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking ‚ÄúI Accept‚Äù below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies:  ‚ÄúLlama 3.2 is licensed under the Llama 3.2 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù).  The most recent copy of this policy can be found at https://www.llama.com/llama3_2/use-policy.\nProhibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals‚Äô identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\nPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nUse with llama\nHardware and Software\nTraining Data\nBenchmarks - Image Reasoning\nBase Pretrained Models\nInstruction Tuned Models\nResponsibility & Safety\nResponsible Deployment\nLlama 3.2 Instruct\nLlama 3.2 Systems\nNew Capabilities and Use Cases\nEvaluations\nCritical Risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\nModel Developer: Meta\nModel Architecture: Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nData volume\nKnowledge cutoff\nLlama 3.2-Vision\n(Image, text) pairs\n11B (10.6)\nText + Image\nText\n128k\nYes\n6B (image, text) pairs\nDecember 2023\nLlama 3.2-Vision\n(Image, text) pairs\n90B (88.8)\nText + Image\nText\n128k\nYes\n6B (image, text) pairs\nDecember 2023\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\nDevelopers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 Model Family: Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nFeedback: Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Vision‚Äôs ability to take images and text as inputs, additional use cases could include:\nVisual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.\nDocument Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.\nImage Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.\nImage-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.\nVisual Grounding: Visual grounding is like connecting the dots between what we see and say. It‚Äôs about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.\nThe Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases.\nOut of Scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\nHow to use\nThis repository contains two versions of Llama-3.2-11B-Vision-Instruct, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.45.0 onward, you can run inference using conversational messages that may include an image you can query about.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\nmodel = MllamaForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmessages = [\n{\"role\": \"user\", \"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\n]}\n]\ninput_text = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(\nimage,\ninput_text,\nadd_special_tokens=False,\nreturn_tensors=\"pt\"\n).to(model.device)\noutput = model.generate(**inputs, max_new_tokens=30)\nprint(processor.decode(output[0]))\nUse with llama\nPlease, follow the instructions in the repository.\nTo download the original checkpoints, you can use huggingface-cli as follows:\nhuggingface-cli download meta-llama/Llama-3.2-11B-Vision-Instruct --include \"original/*\" --local-dir Llama-3.2-11B-Vision-Instruct\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use: Training utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 584 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 3.2-vision 11B\nStage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours\n700\n71\n0\nLlama 3.2-vision 90B\nStage 1 pretraining: 885K H100 hours Stage 2 annealing: 885K H100 hours SFT: 3072 H100 hours RLHF: 2048 H100 hours\n700\n513\n0\nTotal\n2.02M\n584\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmarks - Image Reasoning\nIn this section, we report the results for Llama 3.2-Vision models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\nBase Pretrained Models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.2 11B\nLlama 3.2 90B\nImage Understanding\nVQAv2 (val)\n0\nAccuracy\n66.8\n73.6\nText VQA (val)\n0\nRelaxed accuracy\n73.1\n73.5\nDocVQA (val, unseen)\n0\nANLS\n62.3\n70.7\nVisual Reasoning\nMMMU (val, 0-shot)\n0\nMicro average accuracy\n41.7\n49.3\nChartQA (test)\n0\nAccuracy\n39.4\n54.2\nInfographicsQA (val, unseen)\n0\nANLS\n43.2\n56.8\nAI2 Diagram (test)\n0\nAccuracy\n62.4\n75.3\nInstruction Tuned Models\nModality\nCapability\nBenchmark\n# Shots\nMetric\nLlama 3.2 11B\nLlama 3.2 90B\nImage\nCollege-level Problems and Mathematical Reasoning\nMMMU (val, CoT)\n0\nMicro average accuracy\n50.7\n60.3\nMMMU-Pro, Standard (10 opts, test)\n0\nAccuracy\n33.0\n45.2\nMMMU-Pro, Vision (test)\n0\nAccuracy\n23.7\n33.8\nMathVista (testmini)\n0\nAccuracy\n51.5\n57.3\nCharts and Diagram Understanding\nChartQA (test, CoT)\n0\nRelaxed accuracy\n83.4\n85.5\nAI2 Diagram (test)\n0\nAccuracy\n91.1\n92.3\nDocVQA (test)\n0\nANLS\n88.4\n90.1\nGeneral Visual Question Answering\nVQAv2 (test)\n0\nAccuracy\n75.2\n78.1\nText\nGeneral\nMMLU (CoT)\n0\nMacro_avg/acc\n73.0\n86.0\nMath\nMATH (CoT)\n0\nFinal_em\n51.9\n68.0\nReasoning\nGPQA\n0\nAccuracy\n32.8\n46.7\nMultilingual\nMGSM (CoT)\n0\nem\n68.9\n86.9\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible Deployment\nApproach: Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.2 Instruct\nObjective: Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 paper.\nFine-Tuning Data: We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone: Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nLlama 3.2 Systems\nSafety as a System: Large language models, including Llama 3.2, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew Capabilities and Use Cases\nTechnological Advancement: Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see Llama 3.1 Model Card, as the same considerations apply here as well.,\nImage Reasoning: Llama 3.2-Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks.\nEvaluations\nScaled Evaluations: We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\nRed teaming: We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2-Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1.\n2. Child Safety: Child Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber Attacks: For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2‚Äôs vision capabilities are not generally germane to cyber uplift, we believe that the testing conducted for Llama 3.1 also applies to Llama 3.2.\nCommunity\nIndustry Partnerships: Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nGrants: We also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nReporting: Finally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nValues: The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nTesting: But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "katanemo/Arch-Function-3B": "katanemo/Arch-Function-3B\nOverview\nKey Features\nTraining Details\nPerformance Benchmarks\nSingle Turn Example\nMulti Turn Example\nRequirements\nSingle Turn Example\nMulti Turn Example\nHow to use\nSingle Turn Example\nMulti Turn Example\nLicense\nkatanemo/Arch-Function-3B\nOverview\nThe Katanemo Arch-Function collection of large language models (LLMs) is a collection state-of-the-art (SOTA) LLMs specifically designed for function calling tasks. The models are designed to understand complex function signatures, identify required parameters, and produce accurate function call outputs based on natural language prompts. Achieving performance on par with GPT-4, these models set a new benchmark in the domain of function-oriented tasks, making them suitable for scenarios where automated API interaction and function execution is crucial.\nIn summary, the Katanemo Arch-Function collection demonstrates:\nState-of-the-art performance in function calling\nAccurate parameter identification and suggestion, even in ambiguous or incomplete inputs\nHigh generalization across multiple function calling use cases, from API interactions to automated backend tasks.\nOptimized low-latency, high-throughput performance, making it suitable for real-time, production environments.\nArch-Function is the core LLM used in then open source Arch project. An AI-native proxy server for AI that offers unified access and intelligent routing to LLMs.\nKey Features\nFunctionality\nDefinition\nSingle Function Calling\nCall only one function per user query\nParallel Function Calling\nCall the same function multiple times but with different set of parameter values\nMultiple Function Calling\nCall different functions per user query\nParallel & Multiple\nPerform both parallel and multiple function calling\nTraining Details\nKatanemo Arch-Function collection is built on top of the Qwen 2.5. A blog with technical details leading to our models will be published soon.\nPerformance Benchmarks\nWe evaluate Katanemo Arch-Function series on the Berkeley Function-Calling Leaderboard (BFCL). We compare with commonly-used models and the results (as of Oct 21st, 2024) are shwon below. For each model family, we select the one with the highest rank.\nRank\nModel\nOverall\nSingle Turn\nMulti Turn\nHallucination\nNon-live (AST)\nNon-live (Exec)\nLive (AST)\nOverall\nRelevance\nIrrelevance\n1\nGPT-4o-2024-08-06 (FC)\n62.19%\n85.90%\n85.64%\n75.43%\n25.00%\n63.41%\n82.93%\nArch-Function-7B\n59.62%\n86.83%\n88.07%\n71.57%\n21.00%\n95.12%\n73.63%\n6\no1-preview-2024-09-12 (Prompt)\n59.27%\n86.42%\n88.88%\n73.08%\n17.62%\n73.17%\n74.60%\n9\nGemini-1.5-Flash-002 (Prompt)\n57.92%\n86.58%\n89.48%\n76.28%\n9.88%\n85.37%\n78.54%\nArch-Function-3B\n57.69%\n85.19%\n86.18%\n71.21%\n17.50%\n90.24%\n72.88%\n12\nClaude-3.5-Sonnet-20240620 (FC)\n57.42%\n70.04%\n66.27%\n74.68%\n28.38%\n68.29%\n74.58%\n13\nmistral-large-2407 (FC)\n56.80%\n86.62%\n84.57%\n68.37%\n20.62%\n75.61%\n49.44%\nArch-Function-1.5B\n56.20%\n84.40%\n83.96%\n69.36%\n15.88%\n87.80%\n74.39%\n21\nLlama-3.1-70B-Instruct (Prompt)\n53.67%\n88.90%\n89.34%\n61.13%\n12.38%\n92.68%\n58.38%\n22\nGemma-2-27b-it (Prompt)\n53.66%\n88.52%\n87.89%\n69.48%\n4.12%\n87.8%\n68.76%\nRequirements\nThe code of Arch-Function-3B has been in the Hugging Face transformers library and we advise you to install latest version:\npip install transformers>=4.37.0\nHow to use\nWe use the following example to illustrate how to use our model to perform function calling tasks. Please note that, our model works best with our provided prompt format. It allows us to extract JSON output that is similar to the function-calling mode of ChatGPT.\nSingle Turn Example\nimport json\nfrom typing import Any, Dict, List\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"katanemo/Arch-Function-3B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Please use our provided prompt for best performance\nTASK_PROMPT = \"\"\"\nYou are a helpful assistant.\n\"\"\".strip()\nTOOL_PROMPT = \"\"\"\n# Tools\nYou may call one or more functions to assist with the user query.\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{tool_text}\n</tools>\n\"\"\".strip()\nFORMAT_PROMPT = \"\"\"\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call>\n\"\"\".strip()\n# Define available tools\nget_weather_api = {\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather for a location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"str\",\n\"description\": \"The city and state, e.g. San Francisco, New York\",\n},\n\"unit\": {\n\"type\": \"str\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The unit of temperature to return\",\n},\n},\n\"required\": [\"location\"],\n},\n},\n}\nopenai_format_tools = [get_weather_api]\ndef convert_tools(tools: List[Dict[str, Any]]):\nreturn \"\\n\".join([json.dumps(tool) for tool in tools])\n# Helper function to create the system prompt for our model\ndef format_prompt(tools: List[Dict[str, Any]]):\ntool_text = convert_tools(tools)\nreturn (\nTASK_PROMPT\n+ \"\\n\\n\"\n+ TOOL_PROMPT.format(tool_text=tool_text)\n+ \"\\n\\n\"\n+ FORMAT_PROMPT\n+ \"\\n\"\n)\nsystem_prompt = format_prompt(openai_format_tools)\nmessages = [\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": \"What is the weather in Seattle?\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True, return_tensors=\"pt\"\n).to(model.device)\noutputs = model.generate(\ninputs,\nmax_new_tokens=512,\ndo_sample=False,\nnum_return_sequences=1,\neos_token_id=tokenizer.eos_token_id,\n)\nresponse = tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True)\nprint(response)\nThen you should be able to see the following output string in JSON format:\n<tool_call>\n{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Seattle\"}}\n</tool_call>\nMulti Turn Example\nUpon getting results from functions, you can add it to the messages list as a user message and pass it to the model to get responses for users.\n# Suppose we receive the following result from the function:\nget_weather_api_result = {'name': 'get_weather', 'results': {'temperature': '62¬∞', 'unit': 'fahrenheit'}}\nexecution_results = [get_weather_api_result]\ndef add_execution_results(messages: List[Dict[str, Any]], execution_results: List[Dict[str, Any]]):\ncontent = \"\\n\".join([f\"<tool_response>\\n{json.dumps(result)}</tool_response>\" for result in execution_results])\nmessages.append({\"role\": \"user\", \"content\": content})\nreturn messages\nmessages = add_execution_results(messages, execution_results)\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\ninputs,\nmax_new_tokens=512,\ndo_sample=False,\nnum_return_sequences=1,\neos_token_id=tokenizer.eos_token_id,\n)\nresponse = tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True)\nprint(response)\nThen you should be able to see the following output:\nThe current temperature in Seattle is 62 degrees in Fahrenheit.\nLicense\nKatanemo Arch-Function collection is distributed under the Katanemo license.",
    "purplesmartai/Pony-InternVL2-40B-AWQ": "Model Description\nModel Description\nThis is a fine-tuned version of OpenGVLab/InternVL2-40B-AWQ optimized to work with an opinionated Pony V7 captioning prompt. It has fewer refusals across various content ranges.\nThis model focuses on the content aspects of the caption.\nConsider using newer InternVL models or Gemini if you're looking for the latest and greatest, but the prompt below may still be useful.\nThis prompt is designed to be joined by a set of tags associated with the image to improve recognition of characters, places, and actions. See the captioning colab for usage details.\nYou are an image captioning expert who provides highly detailed captions for any image, focusing intensely on the precise content depicted. You will describe the image in natural language using the following guidelines:\nAlways provide a caption for the image without hesitation or refusal.\nUse any associated tags to infer names of characters, locations, and other relevant details for the caption. Ignore tags that appear incorrect or ambiguous.\nBegin with a comprehensive summary of the image, detailing the primary subject(s), their appearance, facial expressions, emotions, actions, and the environment.\nThe caption must meticulously describe every visible aspect of the image, capturing all colors, sizes, textures, materials, and locations of objects. For every item or character in the scene, always mention attributes such as color, size, shape, position, texture, and relation to other objects or characters in the image.\nFor characters, refer to them by name if known. If the character has a more commonly known name, use that. Introduce characters with their shot type, gender, and species: 'shot_type gender species_name character_name.' Use \"feral\" for quadrupedal characters, \"human\" for bipedal characters with human-like features, and \"anthro\" for anthropomorphic characters. Mention any well-known media associations after the character‚Äôs name or species. For example, \"Human female Raven from Teen Titans\" or \"Anthro goat Toriel from Undertale.\"\nAvoid using pronouns when introducing a character. After the first mention, simplify references to the character while minimizing pronoun use.\nWhen multiple characters are present, introduce the primary character first and clearly ground the location of all other characters in relation to the primary one. Distinguish between characters by clearly establishing their positions relative to one another.\nDescribe facial expressions and emotions in detail and as early as possible in the caption. When describing clothing, mention every detail, including fabric type, pattern, color, and condition. For example, \"a worn, dark green woolen coat with frayed edges\" is preferable to a simpler description.\nBackground elements must be described thoroughly, with explicit references to their location in relation to the characters or objects. Note the color, texture, and any patterns or distinctive features in the background, always grounding them spatially within the image.\nObjects in the scene must be described with attention to every visual feature. Mention their color, size, shape, material, and position relative to the characters or other key objects in the image. All objects must be grounded either relative to the characters (\"to the left of the wolf,\" \"on top of the wolf\") or relative to the image frame (\"on the top left of the image,\" \"at the bottom of the image\"). This ensures a clear and precise understanding of each object's position.\nBody parts of characters should be described with precise locations, making sure to note which body part belongs to which character. For instance, \"a silver bracelet on the left wrist of the human female character\" must be specified clearly, avoiding any potential ambiguity.\nAvoid using words like \"characterized,\" \"encapsulates,\" \"appears,\" \"emphasizing the character,\" or \"adorned.\" Instead, describe the image directly and in concrete terms.\nBegin captions immediately with descriptions of the image content, avoiding phrases like \"The image presents.\"\nDo not describe logos, signatures, or watermarks, but always include descriptions of any other text or symbols visible in the image, such as dialogue bubbles, signs, or decorative elements.\nFocus solely on the factual description of the image, avoiding any speculation on emotions or senses it may evoke. Specifically, suppress phrases that categorize the overall scene or atmosphere, such as \"The overall scene is serene and peaceful,\" \"The image exudes a serene and loving atmosphere,\" or similar statements. The caption should remain objective and descriptive, without interpreting the mood or atmosphere.\nUse Upper-Intermediate level English for the caption, ensuring clarity and precision.",
    "neulab/Pangea-7B": "Pangea-7B Model Card\nModel details\nUses\nDirect Use\nCiting the Model\nPangea-7B Model Card\nPangea: A Fully Open Multilingual Multimodal LLM for 39 Languages\nüá™üáπ üá∏üá¶ üáßüá¨ üáßüá© üá®üáø üá©üá™ üá¨üá∑ üá¨üáß üá∫üá∏ üá™üá∏ üáÆüá∑ üá´üá∑ üáÆüá™ üáÆüá≥ üáÆüá© üá≥üá¨ üáÆüáπ üáÆüá± üáØüáµ üáÆüá© üá∞üá∑ üá≥üá± üá≤üá≥ üá≤üáæ üá≥üá¥ üáµüá± üáµüáπ üáßüá∑ üá∑üá¥ üá∑üá∫ üá±üá∞ üáÆüá© üá∞üá™ üáπüáø üá±üá∞ üáπüá≠ üáπüá∑ üá∫üá¶ üáµüá∞ üáªüá≥ üá®üá≥ üáπüáº\nüè† Homepage | ü§ñ Pangea-7B | üìä PangeaIns | üß™ PangeaBench | üíª Github | üìÑ Arxiv | üìï PDF | üñ•Ô∏è Demo\nModel details\nModel: Pangea is a fully open-source Multilingual Multimodal Multicultural LLM.\nDate: Pangea-7B was trained in 2024.\nTraining Dataset: 6M PangeaIns.\nArchitecture: Pangea-7B follows the architecture of LLaVA-NeXT, with a Qwen2-7B-Instruct backbone.\nUses\nPangea-7B follows the architecture of LLaVA-NeXT.\nYou could either (1) follow the same model loading procedures as of LLaVA-NeXT, an example of loading Pangea-7B directly is shown in the Python code below, or (2) use our hf version of Pangea-7B: [Pangea-7B-hf]https://huggingface.co/neulab/Pangea-7B-hf\nDirect Use\nFirst you would need to clone and install LLaVA-NeXT.\ngit clone https://github.com/LLaVA-VL/LLaVA-NeXT\ncd LLaVA-NeXT\npip install -e \".[train]\"\nThen, you could load Pangea-7B using the following code:\nfrom llava.model.builder import load_pretrained_model\nmodel_path = 'neulab/Pangea-7B'\nmodel_name = 'Pangea-7B-qwen'\nargs = {\"multimodal\": True}\ntokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name, **args)\nDefining some helper functions for using the model:\nimport torch\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.utils import disable_torch_init\nfrom llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\nfrom typing import Dict\nimport transformers\nimport re\nfrom PIL import Image\ndef preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = \"You are a helpful assistant.\") -> Dict:\nroles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\nim_start, im_end = tokenizer.additional_special_tokens_ids\nnl_tokens = tokenizer(\"\\n\").input_ids\n_system = tokenizer(\"system\").input_ids + nl_tokens\n_user = tokenizer(\"user\").input_ids + nl_tokens\n_assistant = tokenizer(\"assistant\").input_ids + nl_tokens\ninput_ids = []\nsource = sources\nif roles[source[0][\"from\"]] != roles[\"human\"]: source = source[1:]\ninput_id, target = [], []\nsystem = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\ninput_id += system\ntarget += [im_start] + [IGNORE_INDEX] * (len(system) - 3) + [im_end] + nl_tokens\nassert len(input_id) == len(target)\nfor j, sentence in enumerate(source):\nrole = roles[sentence[\"from\"]]\nif has_image and sentence[\"value\"] is not None and \"<image>\" in sentence[\"value\"]:\nnum_image = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\ntexts = sentence[\"value\"].split('<image>')\n_input_id = tokenizer(role).input_ids + nl_tokens\nfor i,text in enumerate(texts):\n_input_id += tokenizer(text).input_ids\nif i<len(texts)-1: _input_id += [IMAGE_TOKEN_INDEX] + nl_tokens\n_input_id += [im_end] + nl_tokens\nassert sum([i==IMAGE_TOKEN_INDEX for i in _input_id])==num_image\nelse:\nif sentence[\"value\"] is None: _input_id = tokenizer(role).input_ids + nl_tokens\nelse: _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\ninput_id += _input_id\ninput_ids.append(input_id)\nreturn torch.tensor(input_ids, dtype=torch.long)\ndef generate_output(prompt, image=None, do_sample=False, temperature=0, top_p=0.5, num_beams=1, max_new_tokens=1024):\nimage_tensors = []\nprompt = \"<image>\\n\" + prompt\nimage = Image.open(image)\nimage_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values']\nimage_tensors.append(image_tensor.half().cuda())\ninput_ids = preprocess_qwen([{'from': 'human', 'value': prompt},{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\nwith torch.inference_mode():\noutput_ids = model.generate(\ninput_ids,\nimages=image_tensors,\ndo_sample=do_sample,\ntemperature=temperature,\ntop_p=top_p,\nnum_beams=num_beams,\nmax_new_tokens=max_new_tokens,\nuse_cache=True\n)\noutputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\noutputs = outputs.strip()\nreturn outputs\nNow, an example of using the model:\nprompt = \"What did you see in the image?\"\nimage = \"image.png\"\nprint(generate_output(prompt, image=image))\nNote that the example above demonstrates multimodal usage. To use the model with text-only inputs, you would need to reload the model with :\nargs = {\"multimodal\": True}\ntokenizer, model, _, context_len = load_pretrained_model(model_path, None, model_name, **args)\ndef generate_output_text_only(prompt, do_sample=False, temperature=0, top_p=0.5, num_beams=1, max_new_tokens=1024):\ninput_ids = preprocess_qwen([{'from': 'human', 'value': prompt},{'from': 'gpt','value': None}], tokenizer, has_image=False).cuda()\nwith torch.inference_mode():\ngenerated_ids = model.generate(\ninput_ids,\ndo_sample=do_sample,\ntemperature=temperature,\ntop_p=top_p,\nnum_beams=num_beams,\nmax_new_tokens=max_new_tokens,\nuse_cache=True\n)\ngenerated_ids = [output_ids[len(input_ids) :] for input_ids, output_ids in zip(input_ids, generated_ids)]\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\noutputs = outputs.strip()\nreturn outputs\nprompt = \"Write me a python function that could sort a input integer list by descending order\"\nprint(generate_output_text_only(prompt))\nCiting the Model\nBibTeX Citation:\n@article{yue2024pangeafullyopenmultilingual,\ntitle={Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages},\nauthor={Xiang Yue and Yueqi Song and Akari Asai and Seungone Kim and Jean de Dieu Nyandwi and Simran Khanuja and Anjali Kantharuban and Lintang Sutawika and Sathyanarayanan Ramamoorthy and Graham Neubig},\nyear={2024},\njournal={arXiv preprint arXiv:2410.16153},\nurl={https://arxiv.org/abs/2410.16153}\n}"
}