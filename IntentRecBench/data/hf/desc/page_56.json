{
    "prof-freakenstein/Ai-avatar-Generator": "Model Card for Model ID\nModel Details\nModel Description\nModel Card for Model ID\nA generative model, that geneartive artistic images using stable diffusion\nModel Details\nModel Description\nDeveloped by: [ANURAG Kr. SINGH]\nModel type: [TEXT-TO-IMAGE]\nLanguage(s) (NLP): [PYTHON]\nLicense: [APCAHE 2.0]",
    "Lykon/DreamShaper": "Dream Shaper\nOfficial Repository\nDream Shaper\nOfficial Repository\nRead more about this model here: https://civitai.com/models/4384/dreamshaper\nAlso please support by giving 5 stars and a heart, which will notify new updates.\nPlease consider supporting me on Patreon or buy me a coffee\nhttps://www.patreon.com/Lykon275\nhttps://snipfeed.co/lykon\nYou can run this model on:\nhttps://huggingface.co/spaces/Lykon/DreamShaper-webui\nMage.space, sinkin.ai and more",
    "unstructuredio/yolo_x_layout": "No model card",
    "eenzeenee/t5-base-korean-summarization": "t5-base-korean-summarization\nUsage (HuggingFace Transformers)\nEvalutation Result\nTraining\nModel Architecture\nCitation\nt5-base-korean-summarization\nThis is T5 model for korean text summarization.\nFinetuned based on 'paust/pko-t5-base' model.\nFinetuned with 3 datasets. Specifically, it is described below.\nKorean Paper Summarization Dataset(ë…¼ë¬¸ìžë£Œ ìš”ì•½)\nKorean Book Summarization Dataset(ë„ì„œìžë£Œ ìš”ì•½)\nKorean Summary statement and Report Generation Dataset(ìš”ì•½ë¬¸ ë° ë ˆí¬íŠ¸ ìƒì„± ë°ì´í„°)\nUsage (HuggingFace Transformers)\nimport nltk\nnltk.download('punkt')\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-base-korean-summarization')\ntokenizer = AutoTokenizer.from_pretrained('eenzeenee/t5-base-korean-summarization')\nprefix = \"summarize: \"\nsample = \"\"\"\nì•ˆë…•í•˜ì„¸ìš”? ìš°ë¦¬ (2í•™ë…„)/(ì´ í•™ë…„) ì¹œêµ¬ë“¤ ìš°ë¦¬ ì¹œêµ¬ë“¤ í•™êµì— ê°€ì„œ ì§„ì§œ (2í•™ë…„)/(ì´ í•™ë…„) ì´ ë˜ê³  ì‹¶ì—ˆëŠ”ë° í•™êµì— ëª» ê°€ê³  ìžˆì–´ì„œ ë‹µë‹µí•˜ì£ ?\nê·¸ëž˜ë„ ìš°ë¦¬ ì¹œêµ¬ë“¤ì˜ ì•ˆì „ê³¼ ê±´ê°•ì´ ìµœìš°ì„ ì´ë‹ˆê¹Œìš” ì˜¤ëŠ˜ë¶€í„° ì„ ìƒë‹˜ì´ëž‘ ë§¤ì¼ ë§¤ì¼ êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ë³´ë„ë¡ í•´ìš”.\nì–´/ ì‹œê°„ì´ ë²Œì¨ ì´ë ‡ê²Œ ëë‚˜ìš”? ëŠ¦ì—ˆì–´ìš”. ëŠ¦ì—ˆì–´ìš”. ë¹¨ë¦¬ êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ì•¼ ë¼ìš”.\nê·¸ëŸ°ë° ì–´/ êµ­ì–´ì—¬í–‰ì„ ë– ë‚˜ê¸° ì „ì— ìš°ë¦¬ê°€ ì¤€ë¹„ë¬¼ì„ ì±™ê²¨ì•¼ ë˜ê² ì£ ? êµ­ì–´ ì—¬í–‰ì„ ë– ë‚  ì¤€ë¹„ë¬¼, êµì•ˆì„ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìžˆëŠ”ì§€ ì„ ìƒë‹˜ì´ ì„¤ëª…ì„ í•´ì¤„ê²Œìš”.\n(EBS)/(ì´ë¹„ì—ìŠ¤) ì´ˆë“±ì„ ê²€ìƒ‰í•´ì„œ ë“¤ì–´ê°€ë©´ìš” ì²«í™”ë©´ì´ ì´ë ‡ê²Œ ë‚˜ì™€ìš”.\nìž/ ê·¸ëŸ¬ë©´ìš” ì—¬ê¸° (X)/(ì—‘ìŠ¤) ëˆŒëŸ¬ì£¼(ê³ ìš”)/(êµ¬ìš”). ì €ê¸° (ë™ê·¸ë¼ë¯¸)/(ë˜¥ê·¸ë¼ë¯¸) (EBS)/(ì´ë¹„ì—ìŠ¤) (2ì£¼)/(ì´ ì£¼) ë¼ì´ë¸ŒíŠ¹ê°•ì´ë¼ê³  ë˜ì–´ìžˆì£ ?\nê±°ê¸°ë¥¼ ë°”ë¡œ ê°€ê¸°ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤. ìž/ (ëˆ„ë¥´ë©´ìš”)/(ëˆŒë¥´ë©´ìš”). ì–´ë–»ê²Œ ë˜ëƒ? b/ ë°‘ìœ¼ë¡œ ë‚´ë ¤ìš” ë‚´ë ¤ìš” ë‚´ë ¤ìš” ì­‰ ë‚´ë ¤ìš”.\nìš°ë¦¬ ëª‡ í•™ë…„ì´ì£ ? ì•„/ (2í•™ë…„)/(ì´ í•™ë…„) ì´ì£  (2í•™ë…„)/(ì´ í•™ë…„)ì˜ ë¬´ìŠ¨ ê³¼ëª©? êµ­ì–´.\nì´ë²ˆì£¼ëŠ” (1ì£¼)/(ì¼ ì£¼) ì°¨ë‹ˆê¹Œìš” ì—¬ê¸° êµì•ˆ. ë‹¤ìŒì£¼ëŠ” ì—¬ê¸°ì„œ ë‹¤ìš´ì„ ë°›ìœ¼ë©´ ë¼ìš”.\nì´ êµì•ˆì„ í´ë¦­ì„ í•˜ë©´, ì§œìž”/. ì´ë ‡ê²Œ êµìž¬ê°€ ë‚˜ì˜µë‹ˆë‹¤ .ì´ êµì•ˆì„ (ë‹¤ìš´)/(ë”°ìš´)ë°›ì•„ì„œ ìš°ë¦¬ êµ­ì–´ì—¬í–‰ì„ ë– ë‚  ìˆ˜ê°€ ìžˆì–´ìš”.\nê·¸ëŸ¼ ìš°ë¦¬ ì§„ì§œë¡œ êµ­ì–´ ì—¬í–‰ì„ í•œë²ˆ ë– ë‚˜ë³´ë„ë¡ í•´ìš”? êµ­ì–´ì—¬í–‰ ì¶œë°œ. ìž/ (1ë‹¨ì›)/(ì¼ ë‹¨ì›) ì œëª©ì´ ë­”ê°€ìš”? í•œë²ˆ ì°¾ì•„ë´ìš”.\nì‹œë¥¼ ì¦ê²¨ìš” ì—ìš”. ê·¸ëƒ¥ ì‹œë¥¼ ì½ì–´ìš” ê°€ ì•„ë‹ˆì—ìš”. ì‹œë¥¼ ì¦ê²¨ì•¼ ë¼ìš” ì¦ê²¨ì•¼ ë¼. ì–´ë–»ê²Œ ì¦ê¸¸ê¹Œ? ì¼ë‹¨ì€ ë‚´ë‚´ ì‹œë¥¼ ì¦ê¸°ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ê³µë¶€ë¥¼ í•  ê±´ë°ìš”.\nê·¸ëŸ¼ ì˜¤ëŠ˜ì€ìš” ì–´ë–»ê²Œ ì¦ê¸¸ê¹Œìš”? ì˜¤ëŠ˜ ê³µë¶€í•  ë‚´ìš©ì€ìš” ì‹œë¥¼ ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì½ê¸°ë¥¼ ê³µë¶€í• ê²ë‹ˆë‹¤.\nì–´ë–»ê²Œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì½ì„ê¹Œ ìš°ë¦¬ ê³µë¶€í•´ ë³´ë„ë¡ í•´ìš”. ì˜¤ëŠ˜ì˜ ì‹œ ë‚˜ì™€ë¼ ì§œìž”/! ì‹œê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤ ì‹œì˜ ì œëª©ì´ ë­”ê°€ìš”? ë‹¤íˆ° ë‚ ì´ì—ìš” ë‹¤íˆ° ë‚ .\nëˆ„êµ¬ëž‘ ë‹¤í‰œë‚˜ ë™ìƒì´ëž‘ ë‹¤í‰œë‚˜ ì–¸ë‹ˆëž‘ ì¹œêµ¬ëž‘? ëˆ„êµ¬ëž‘ ë‹¤í‰œëŠ”ì§€ ì„ ìƒë‹˜ì´ ì‹œë¥¼ ì½ì–´ ì¤„ í…Œë‹ˆê¹Œ í•œë²ˆ ìƒê°ì„ í•´ë³´ë„ë¡ í•´ìš”.\"\"\"\ninputs = [prefix + sample]\ninputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=3, do_sample=True, min_length=10, max_length=64)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\nresult = nltk.sent_tokenize(decoded_output.strip())[0]\nprint('RESULT >>', result)\nRESULT >> êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ê¸° ì „ì— êµ­ì–´ ì—¬í–‰ì„ ë– ë‚  ì¤€ë¹„ë¬¼ê³¼ êµì•ˆì„ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìžˆëŠ”ì§€ ì„ ìƒë‹˜ì´ ì„¤ëª…í•´ ì¤€ë‹¤.\nEvalutation Result\nKorean Paper Summarization Dataset(ë…¼ë¬¸ìžë£Œ ìš”ì•½)ROUGE-2-R 0.09868624890432466\nROUGE-2-P 0.9666714545849712\nROUGE-2-F 0.17250881441169427\nKorean Book Summarization Dataset(ë„ì„œìžë£Œ ìš”ì•½)ROUGE-2-R 0.1575686156943213\nROUGE-2-P 0.9718318136896944\nROUGE-2-F 0.26548116834852586\nKorean Summary statement and Report Generation Dataset(ìš”ì•½ë¬¸ ë° ë ˆí¬íŠ¸ ìƒì„± ë°ì´í„°)ROUGE-2-R 0.0987891733555808\nROUGE-2-P 0.9276946867981899\nROUGE-2-F 0.17726493110448185\nTraining\nThe model was trained with the parameters:\ntraining arguments\nSeq2SeqTrainingArguments(\nper_device_train_batch_size=8,\nper_device_eval_batch_size=8,\nauto_find_batch_size=False,\nweight_decay=0.01,\nlearning_rate=4e-05,\nlr_scheduler_type=linear,\nnum_train_epochs=3,\nfp16=True)\nModel Architecture\nT5ForConditionalGeneration(\n(shared): Embedding(50358, 768)\n(encoder): T5Stack(\n(embed_tokens): Embedding(50358, 768)\n(block): ModuleList(\n(0): T5Block(\n(layer): ModuleList(\n(0): T5LayerSelfAttention(\n(SelfAttention): T5Attention(\n(q): Linear(in_features=768, out_features=768, bias=False)\n(k): Linear(in_features=768, out_features=768, bias=False)\n(v): Linear(in_features=768, out_features=768, bias=False)\n(o): Linear(in_features=768, out_features=768, bias=False)\n(relative_attention_bias): Embedding(32, 12)\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(1): T5LayerFF(\n(DenseReluDense): T5DenseGatedActDense(\n(wi_0): Linear(in_features=768, out_features=2048, bias=False)\n(wi_1): Linear(in_features=768, out_features=2048, bias=False)\n(wo): Linear(in_features=2048, out_features=768, bias=False)\n(dropout): Dropout(p=0.1, inplace=False)\n(act): NewGELUActivation()\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n)\n)\n(1~11): T5Block(\n(layer): ModuleList(\n(0): T5LayerSelfAttention(\n(SelfAttention): T5Attention(\n(q): Linear(in_features=768, out_features=768, bias=False)\n(k): Linear(in_features=768, out_features=768, bias=False)\n(v): Linear(in_features=768, out_features=768, bias=False)\n(o): Linear(in_features=768, out_features=768, bias=False)\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(1): T5LayerFF(\n(DenseReluDense): T5DenseGatedActDense(\n(wi_0): Linear(in_features=768, out_features=2048, bias=False)\n(wi_1): Linear(in_features=768, out_features=2048, bias=False)\n(wo): Linear(in_features=2048, out_features=768, bias=False)\n(dropout): Dropout(p=0.1, inplace=False)\n(act): NewGELUActivation()\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n)\n)\n)\n(final_layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(decoder): T5Stack(\n(embed_tokens): Embedding(50358, 768)\n(block): ModuleList(\n(0): T5Block(\n(layer): ModuleList(\n(0): T5LayerSelfAttention(\n(SelfAttention): T5Attention(\n(q): Linear(in_features=768, out_features=768, bias=False)\n(k): Linear(in_features=768, out_features=768, bias=False)\n(v): Linear(in_features=768, out_features=768, bias=False)\n(o): Linear(in_features=768, out_features=768, bias=False)\n(relative_attention_bias): Embedding(32, 12)\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(1): T5LayerCrossAttention(\n(EncDecAttention): T5Attention(\n(q): Linear(in_features=768, out_features=768, bias=False)\n(k): Linear(in_features=768, out_features=768, bias=False)\n(v): Linear(in_features=768, out_features=768, bias=False)\n(o): Linear(in_features=768, out_features=768, bias=False)\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(2): T5LayerFF(\n(DenseReluDense): T5DenseGatedActDense(\n(wi_0): Linear(in_features=768, out_features=2048, bias=False)\n(wi_1): Linear(in_features=768, out_features=2048, bias=False)\n(wo): Linear(in_features=2048, out_features=768, bias=False)\n(dropout): Dropout(p=0.1, inplace=False)\n(act): NewGELUActivation()\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n)\n)\n(1~11): T5Block(\n(layer): ModuleList(\n(0): T5LayerSelfAttention(\n(SelfAttention): T5Attention(\n(q): Linear(in_features=768, out_features=768, bias=False)\n(k): Linear(in_features=768, out_features=768, bias=False)\n(v): Linear(in_features=768, out_features=768, bias=False)\n(o): Linear(in_features=768, out_features=768, bias=False)\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(1): T5LayerCrossAttention(\n(EncDecAttention): T5Attention(\n(q): Linear(in_features=768, out_features=768, bias=False)\n(k): Linear(in_features=768, out_features=768, bias=False)\n(v): Linear(in_features=768, out_features=768, bias=False)\n(o): Linear(in_features=768, out_features=768, bias=False)\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(2): T5LayerFF(\n(DenseReluDense): T5DenseGatedActDense(\n(wi_0): Linear(in_features=768, out_features=2048, bias=False)\n(wi_1): Linear(in_features=768, out_features=2048, bias=False)\n(wo): Linear(in_features=2048, out_features=768, bias=False)\n(dropout): Dropout(p=0.1, inplace=False)\n(act): NewGELUActivation()\n)\n(layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n)\n)\n(final_layer_norm): T5LayerNorm()\n(dropout): Dropout(p=0.1, inplace=False)\n)\n(lm_head): Linear(in_features=768, out_features=50358, bias=False)\n)\nCitation\nRaffel, Colin, et al. \"Exploring the limits of transfer learning with a unified text-to-text transformer.\" J. Mach. Learn. Res. 21.140 (2020): 1-67.",
    "OFA-Sys/small-stable-diffusion-v0": "Small Stable Diffusion Model Card\nGradio\nExample\nInitialization\nTraining Procedure\nTraining Data\nCitation\nTraining\nInitialization\nTraining Procedure\nTraining Data\nCitation\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nSafety Module\nSmall Stable Diffusion Model Card\nã€Update 2023/02/07ã€‘ Recently, we have released a diffusion deployment repo to speedup the inference on both GPU (~4x speedup, based on TensorRT) and CPU (~12x speedup, based on IntelOpenVINO).\nIntegrated with this repo, small-stable-diffusion could generate images in just 5 seconds on the CPU*.\n* Test on Intel(R) Xeon(R) Platinum 8369B CPU, DPMSolverMultistepScheduler 10 steps, fix channel/height/width when converting to Onnx\nSimilar image generation quality, but is nearly 1/2 smaller!Here are some samples:\nGradio\nWe support a Gradio Web UI to run small-stable-diffusion-v0:\nWe also provide a space demo for small-stable-diffusion-v0 + diffusion-deploy.As huggingface provides AMD CPU for the space demo, it costs about 35 seconds to generate an image with 15 steps, which is much slower than the Intel CPU environment as diffusion-deploy is based on Intel's OpenVINO.\nExample\nUse Diffusers >=0.8.0, do not support lower versions.\nimport torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = \"OFA-Sys/small-stable-diffusion-v0/\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"an apple, 4k\"\nimage = pipe(prompt).images[0]\nimage.save(\"apple.png\")\nTraining\nInitialization\nThis model is initialized from stable-diffusion v1-4. As the model structure is not the same as stable-diffusion and the number of parameters is smaller, the parameters of stable diffusion could not be utilized directly. Therefore, small stable diffusion set layers_per_block=1 and select the first layer of each block in original stable diffusion to initilize the small model.\nTraining Procedure\nAfter the initialization, the model has been trained for 1100k steps in 8xA100 GPUS. The training progress consists of three stages. The first stage is a simple pre-training precedure. In the last two stages, the original stable diffusion was utilized to distill knowledge to small model as a teacher model. In all stages, only the parameters in unet were trained and other parameters were frozen.\nHardware: 8 x A100-80GB GPUs\nOptimizer: AdamW\nStage 1 - Pretrain the unet part of the model.\nSteps: 500,000\nBatch: batch size=8, GPUs=8, Gradient Accumulations=2. Total batch size=128\nLearning rate: warmup to 1e-5 for 10,000 steps and then kept constant\nStage 2 - Distill the model using stable-diffusion v1-4 as the teacher. Besides the ground truth, the training in this stage uses the soft-label (pred_noise) generated by teacher model as well.\nSteps: 400,000\nBatch: batch size=8, GPUs=8, Gradient Accumulations=2. Total batch size=128\nLearning rate: warmup to 1e-5 for 5,000 steps and then kept constant\nSoft label weight: 0.5\nHard label weight: 0.5\nStage 3 - Distill the model using stable-diffusion v1-5 as the teacher. Use several techniques in Knowledge Distillation of Transformer-based Language Models Revisited, including similarity-based layer match apart from soft label.\nSteps: 200,000\nBatch: batch size=8, GPUs=8, Gradient Accumulations=2. Total batch size=128\nLearning rate: warmup to 1e-5 for 5,000 steps and then kept constant\nSoftlabel weight: 0.5\nHard label weight: 0.5\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-2B en aesthetic\nLAION-Art\nLAION-HD\nCitation\n@article{Lu2022KnowledgeDO,\ntitle={Knowledge Distillation of Transformer-based Language Models Revisited},\nauthor={Chengqiang Lu and Jianwei Zhang and Yunfei Chu and Zhengyu Chen and Jingren Zhou and Fei Wu and Haiqing Chen and Hongxia Yang},\njournal={ArXiv},\nyear={2022},\nvolume={abs/2206.14366}\n}\nUses\nThe following section is adapted from the Stable Diffusion model card\nDirect Use\nThe model is intended for research purposes only. Possible research areas and\ntasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a large-scale dataset\nLAION-5B which contains adult material\nand is not fit for product use without additional safety mechanisms and\nconsiderations.\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\nThe training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion v1 was trained on subsets of LAION-2B(en),\nwhich consists of images that are primarily limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nSafety Module\nThe intended use of this model is with the Safety Checker in Diffusers.\nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the CLIPModel after generation of the images.\nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\nThis model card was written by: Justin Pinkney and is based on the Stable Diffusion model card.",
    "chunliangmmr/dongxuelian_lora": "README.md exists but content is empty.",
    "geolocal/StreetCLIP": "Model Card for StreetCLIP\nModel Description\nModel Details\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nPreprocessing\nTraining Procedure\nEvaluation\nTesting Data and Metrics\nTesting Data\nMetrics\nResults\nSummary\nEnvironmental Impact\nCitation\nModel Card for StreetCLIP\nStreetCLIP is a robust foundation model for open-domain image geolocalization and other\ngeographic and climate-related tasks.\nTrained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves\nstate-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot,\noutperforming supervised models trained on millions of images.\nModel Description\nStreetCLIP is a model pretrained by deriving image captions synthetically from image class labels using\na domain-specific caption template. This allows StreetCLIP to transfer its generalized zero-shot learning\ncapabilities to a specific domain (i.e. the domain of image geolocalization).\nStreetCLIP builds on the OpenAI's pretrained large version of CLIP ViT, using 14x14 pixel\npatches and images with a 336 pixel side length.\nModel Details\nModel type: CLIP\nLanguage: English\nLicense: Create Commons Attribution Non Commercial 4.0\nTrained from model: openai/clip-vit-large-patch14-336\nModel Sources\nPaper: Preprint\nCite preprint as:\n@misc{haas2023learning,\ntitle={Learning Generalized Zero-Shot Learners for Open-Domain Image Geolocalization},\nauthor={Lukas Haas and Silas Alberti and Michal Skreta},\nyear={2023},\neprint={2302.00275},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nUses\nStreetCLIP has a deep understanding of the visual features found in street-level urban and rural scenes\nand knows how to relate these concepts to specific countries, regions, and cities. Given its training setup,\nthe following use cases are recommended for StreetCLIP.\nDirect Use\nStreetCLIP can be used out-of-the box using zero-shot learning to infer the geolocation of images on a country, region,\nor city level. Given that StreetCLIP was pretrained on a dataset of street-level urban and rural images,\nthe best performance can be expected on images from a similar distribution.\nBroader direct use cases are any zero-shot image classification tasks that rely on urban and rural street-level\nunderstanding or geographical information relating visual clues to their region of origin.\nDownstream Use\nStreetCLIP can be finetuned for any downstream applications that require geographic or street-level urban or rural\nscene understanding. Examples of use cases are the following:\nUnderstanding the Built Environment\nAnalyzing building quality\nBuilding type classifcation\nBuilding energy efficiency Classification\nAnalyzing Infrastructure\nAnalyzing road quality\nUtility pole maintenance\nIdentifying damage from natural disasters or armed conflicts\nUnderstanding the Natural Environment\nMapping vegetation\nVegetation classification\nSoil type classifcation\nTracking deforestation\nGeneral Use Cases\nStreet-level image segmentation\nUrban and rural scene classification\nObject detection in urban or rural environments\nImproving navigation and self-driving car technology\nOut-of-Scope Use\nAny use cases attempting to geolocate users' private images are out-of-scope and discouraged.\nBias, Risks, and Limitations\nStreetCLIP was not trained on social media images or images of identifable people for a reason. As such, any use case\nattempting to geolocalize users' private images\nRecommendations\nWe encourage the community to apply StreetCLIP to applications with significant social impact of which there are many.\nThe first three categories of potential use cases under Downstream Use list potential use cases with social impact\nto explore.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"geolocal/StreetCLIP\")\nprocessor = CLIPProcessor.from_pretrained(\"geolocal/StreetCLIP\")\nurl = \"https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nchoices = [\"San Jose\", \"San Diego\", \"Los Angeles\", \"Las Vegas\", \"San Francisco\"]\ninputs = processor(text=choices, images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\nTraining Details\nTraining Data\nStreetCLIP was trained on an original, unreleased street-level dataset of 1.1 million real-world,\nurban and rural images. The data used to train the model comes from 101 countries, biased towards\nwestern countries and not including India and China.\nPreprocessing\nSame preprocessing as openai/clip-vit-large-patch14-336.\nTraining Procedure\nStreetCLIP is initialized with OpenAI's pretrained large version of CLIP ViT and then pretrained using the synthetic\ncaption domain-specific pretraining method described in the paper corresponding to this work. StreetCLIP was trained\nfor 3 epochs using an AdamW optimizer with a learning rate of 1e-6 on 3 NVIDIA A100 80GB GPUs, a batch size of 32,\nand gradient accumulation of 12 steps.\nStreetCLIP was trained with the goal of matching images in the batch\nwith the caption correponding to the correct city, region, and country of the images' origins.\nEvaluation\nStreetCLIP was evaluated in zero-shot on two open-domain image geolocalization benchmarks using a\ntechnique called hierarchical linear probing. Hierarchical linear probing sequentially attempts to\nidentify the correct country and then city of geographical image origin.\nTesting Data and Metrics\nTesting Data\nStreetCLIP was evaluated on the following two open-domain image geolocalization benchmarks.\nIM2GPS.\nIM2GPS3K\nMetrics\nThe objective of the listed benchmark datasets is to predict the images' coordinates of origin with as\nlittle deviation as possible. A common metric set forth in prior literature is called Percentage at Kilometer (% @ KM).\nThe Percentage at Kilometer metric first calculates the distance in kilometers between the predicted coordinates\nto the ground truth coordinates and then looks at what percentage of error distances are below a certain kilometer threshold.\nResults\nIM2GPS\nModel\n25km\n200km\n750km\n2,500km\nPlaNet (2016)\n24.5\n37.6\n53.6\n71.3\nISNs (2018)\n43.0\n51.9\n66.7\n80.2\nTransLocator (2022)\n48.1\n64.6\n75.6\n86.7\nZero-Shot CLIP (ours)\n27.0\n42.2\n71.7\n86.9\nZero-Shot StreetCLIP (ours)\n28.3\n45.1\n74.7\n88.2\nMetric: Percentage at Kilometer (% @ KM)\nIM2GPS3K\nModel\n25km\n200km\n750km\n2,500km\nPlaNet (2016)\n24.8\n34.3\n48.4\n64.6\nISNs (2018)\n28.0\n36.6\n49.7\n66.0\nTransLocator (2022)\n31.1\n46.7\n58.9\n80.1\nZero-Shot CLIP (ours)\n19.5\n34.0\n60.0\n78.1\nZero-Shot StreetCLIP (ours)\n22.4\n37.4\n61.3\n80.4\nMetric: Percentage at Kilometer (% @ KM)\nSummary\nOur experiments demonstrate that our synthetic caption pretraining method is capable of significantly\nimproving CLIP's generalized zero-shot capabilities applied to open-domain image geolocalization while\nachieving state-of-the-art performance on a selection of benchmark metrics.\nEnvironmental Impact\nHardware Type: 4 NVIDIA A100 GPUs\nHours used: 12\nCitation\nCite preprint as:\n@misc{haas2023learning,\ntitle={Learning Generalized Zero-Shot Learners for Open-Domain Image Geolocalization},\nauthor={Lukas Haas and Silas Alberti and Michal Skreta},\nyear={2023},\neprint={2302.00275},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "2vXpSwA7/iroiro-lora": "README.md exists but content is empty.",
    "OpenAssistant/reward-model-deberta-v3-large-v2": "Reward model trained from human feedback\nHow to use\nPerformance\nOther\nReward model trained from human feedback\nReward model (RM) trained to predict which generated answer is better judged by a human, given a question.\nRM are useful in these domain:\nQA model evaluation\nserves as reward score in RLHF\ndetect potential toxic response via ranking\nAll models are train on these dataset with a same split seed across datasets (if validation split wasn't available)\nwebgpt_comparisons\nsummarize_from_feedback\nsynthetic-instruct-gptj-pairwise\nanthropic_hh-rlhf\nHow to use\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nreward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\nrank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\nquestion, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\ninputs = tokenizer(question, answer, return_tensors='pt')\nscore = rank_model(**inputs).logits[0].cpu().detach()\nprint(score)\nToxic response detection\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nreward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\nrank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\nquestion = \"I just came out of from jail, any suggestion of my future?\"\nhelpful = \"It's great to hear that you have been released from jail.\"\nbad = \"Go back to jail you scum\"\ninputs = tokenizer(question, helpful, return_tensors='pt')\ngood_score = rank_model(**inputs).logits[0].cpu().detach()\ninputs = tokenizer(question, bad, return_tensors='pt')\nbad_score = rank_model(**inputs).logits[0].cpu().detach()\nprint(good_score > bad_score) # tensor([True])\nPerformance\nValidation split accuracy\nModel\nWebGPT\nSummary\nSytheticGPT\nAnthropic RLHF\nelectra-large-discriminator\n59.30\n68.66\n99.85\n54.33\ndeberta-v3-large-v2\n61.57\n71.47\n99.88\n69.25\ndeberta-v3-large\n61.13\n72.23\n99.94\n55.62\ndeberta-v3-base\n59.07\n66.84\n99.85\n54.51\ndeberta-v2-xxlarge\n58.67\n73.27\n99.77\n66.74\nIts likely SytheticGPT has somekind of surface pattern on the choosen-rejected pair which makes it trivial to differentiate between better the answer.\nOther\nSincere thanks to stability.ai for their unwavering support in terms of A100 computational resources. Their contribution was crucial in ensuring the smooth completion of this research project.",
    "microsoft/speecht5_tts": "SpeechT5 (TTS task)\nModel Description\nModel Sources [optional]\nUses\nðŸ¤— Transformers Usage\nFine-tuning the Model\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nPreprocessing [optional]\nTraining hyperparameters\nSpeeds, Sizes, Times [optional]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nFactors\nMetrics\nResults\nSummary\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nHardware\nSoftware\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nSpeechT5 (TTS task)\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\nThis model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\nSpeechT5 was first released in this repository, original weights. The license used is MIT.\nModel Description\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\nDeveloped by: Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\nShared by [optional]: Matthijs Hollemans\nModel type: text-to-speech\nLanguage(s) (NLP): [More Information Needed]\nLicense: MIT\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [https://github.com/microsoft/SpeechT5/]\nPaper: [https://arxiv.org/pdf/2110.07205.pdf]\nBlog Post: [https://huggingface.co/blog/speecht5]\nDemo: [https://huggingface.co/spaces/Matthijs/speecht5-tts-demo]\nUses\nðŸ¤— Transformers Usage\nYou can run SpeechT5 TTS locally with the ðŸ¤— Transformers library.\nFirst install the ðŸ¤— Transformers library, sentencepiece, soundfile and datasets(optional):\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\nRun inference via the Text-to-Speech (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\nsynthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\nembeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\nsf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\nRun inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\nprocessor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\ninputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n# load xvector containing speaker's voice characteristics from a dataset\nembeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\nsf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\nFine-tuning the Model\nRefer to this Colab notebook for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\nDirect Use\nYou can use this model for speech synthesis. See the model hub to look for fine-tuned versions on a task that interests you.\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nTraining Details\nTraining Data\nLibriTTS\nTraining Procedure\nPreprocessing [optional]\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\nTraining hyperparameters\nPrecision: [More Information Needed]\nRegime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets.\nAfter preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n@inproceedings{ao-etal-2022-speecht5,\ntitle = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\nauthor = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\nbooktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\nmonth = {May},\nyear = {2022},\npages={5723--5738},\n}\nGlossary [optional]\ntext-to-speech to synthesize audio\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel Card Contact\n[More Information Needed]",
    "alimazhar-110/website_classification": "website_classification\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nwebsite_classification\nThis model is a fine-tuned version of distilbert-base-uncased on an unknown dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.2292\nAccuracy: 0.9504\nF1: 0.9489\nPrecision: 0.9510\nRecall: 0.9504\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 30\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\nF1\nPrecision\nRecall\n2.404\n1.0\n71\n1.7840\n0.8865\n0.8776\n0.8785\n0.8865\n1.295\n2.0\n142\n0.8539\n0.8972\n0.8871\n0.8803\n0.8972\n0.6186\n3.0\n213\n0.4818\n0.9326\n0.9263\n0.9266\n0.9326\n0.3103\n4.0\n284\n0.3101\n0.9397\n0.9343\n0.9324\n0.9397\n0.1618\n5.0\n355\n0.3001\n0.9291\n0.9251\n0.9278\n0.9291\n0.0893\n6.0\n426\n0.2743\n0.9291\n0.9251\n0.9276\n0.9291\n0.0547\n7.0\n497\n0.2605\n0.9255\n0.9236\n0.9334\n0.9255\n0.028\n8.0\n568\n0.2167\n0.9397\n0.9375\n0.9403\n0.9397\n0.0186\n9.0\n639\n0.2096\n0.9468\n0.9467\n0.9499\n0.9468\n0.0134\n10.0\n710\n0.2219\n0.9362\n0.9354\n0.9402\n0.9362\n0.0107\n11.0\n781\n0.2124\n0.9468\n0.9466\n0.9507\n0.9468\n0.0087\n12.0\n852\n0.2119\n0.9504\n0.9497\n0.9534\n0.9504\n0.0075\n13.0\n923\n0.2141\n0.9504\n0.9497\n0.9534\n0.9504\n0.0066\n14.0\n994\n0.2198\n0.9433\n0.9415\n0.9442\n0.9433\n0.0058\n15.0\n1065\n0.2188\n0.9468\n0.9454\n0.9474\n0.9468\n0.0052\n16.0\n1136\n0.2181\n0.9468\n0.9454\n0.9474\n0.9468\n0.0047\n17.0\n1207\n0.2220\n0.9504\n0.9489\n0.9510\n0.9504\n0.0044\n18.0\n1278\n0.2232\n0.9504\n0.9489\n0.9510\n0.9504\n0.004\n19.0\n1349\n0.2216\n0.9539\n0.9535\n0.9565\n0.9539\n0.0037\n20.0\n1420\n0.2251\n0.9504\n0.9489\n0.9510\n0.9504\n0.0036\n21.0\n1491\n0.2275\n0.9468\n0.9451\n0.9470\n0.9468\n0.0034\n22.0\n1562\n0.2264\n0.9539\n0.9535\n0.9565\n0.9539\n0.0032\n23.0\n1633\n0.2283\n0.9504\n0.9489\n0.9510\n0.9504\n0.003\n24.0\n1704\n0.2299\n0.9504\n0.9489\n0.9510\n0.9504\n0.0029\n25.0\n1775\n0.2282\n0.9468\n0.9451\n0.9470\n0.9468\n0.0029\n26.0\n1846\n0.2288\n0.9468\n0.9451\n0.9470\n0.9468\n0.0028\n27.0\n1917\n0.2286\n0.9504\n0.9489\n0.9510\n0.9504\n0.0027\n28.0\n1988\n0.2293\n0.9504\n0.9489\n0.9510\n0.9504\n0.0026\n29.0\n2059\n0.2291\n0.9504\n0.9489\n0.9510\n0.9504\n0.0026\n30.0\n2130\n0.2292\n0.9504\n0.9489\n0.9510\n0.9504\nFramework versions\nTransformers 4.26.0\nPytorch 1.13.1+cu116\nDatasets 2.9.0\nTokenizers 0.13.2",
    "OrK7/parler_hate_speech": "Results\nSocial Network Hate Detection: Finding Social Media Posts Containing Hateful Information Using Ensemble Methods and Back-Translation\nRecent research efforts have been directed toward the development of automated systems for detecting hateful content to assist social media providers in identifying and removing such content before it can be viewed by the public. This paper introduces a unique ensemble approach that utilizes DeBERTa models, which benefits from pre-training on massive synthetic data and the integration of back-translation techniques during training and testing. Our findings reveal that this approach delivers state-of-the-art results in hate-speech detection. The results demonstrate that the combination of back-translation, ensemble, and test-time augmentation results in a considerable improvement across various metrics and models in both the Parler and GAB datasets. We show that our method reduces modelsâ€™ bias in an effective and meaningful way, and also reduces the RMSE from 0.838 to around 0.766 and increases R-squared from 0.520 to 0.599. The biggest improvement was seen in small Deberate models, while for large models, there was either a minor improvement or no change.\nResults\n!pip install huggingface_hub\n!pip install tokenizers transformers\n!pip install iterative-stratification\n!git clone https://github.com/OrKatz7/parler-hate-speech\n%cd parler-hate-speech/src\nfrom huggingface_hub import hf_hub_download\nimport torch\nimport sys\nfrom model import CustomModel,MeanPooling\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nimport numpy as np\nclass CFG:\nmodel=\"microsoft/deberta-v3-base\"\ntarget_cols=['label_mean']\nname = \"OrK7/parler_hate_speech\"\ndownloaded_model_path = hf_hub_download(repo_id=name, filename=\"pytorch_model.bin\")\nmodel = torch.load(downloaded_model_path)\ntokenizer = AutoTokenizer.from_pretrained(name)\ndef prepare_input(text):\ninputs = tokenizer.encode_plus(\ntext,\nreturn_tensors=None,\nadd_special_tokens=True,\nmax_length=512,\npad_to_max_length=True,\ntruncation=True\n)\nfor k, v in inputs.items():\ninputs[k] = torch.tensor(np.array(v).reshape(1,-1), dtype=torch.long)\nreturn inputs\ndef collate(inputs):\nmask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\nfor k, v in inputs.items():\ninputs[k] = inputs[k][:,:mask_len]\nreturn inputs\nfrom transformers import Pipeline\nclass HatePipeline(Pipeline):\ndef _sanitize_parameters(self, **kwargs):\npreprocess_kwargs = {}\nif \"maybe_arg\" in kwargs:\npreprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\nreturn preprocess_kwargs, {}, {}\ndef preprocess(self, inputs):\nout = prepare_input(inputs)\nreturn collate(out)\ndef _forward(self, model_inputs):\noutputs = self.model(model_inputs)\nreturn outputs\ndef postprocess(self, model_outputs):\nreturn np.array(model_outputs[0,0].numpy()).clip(0,1)*4+1\npipe = HatePipeline(model=model)\npipe(\"I Love you #\")\nresults: 1.0\npipe(\"I Hate #$%#$%Jewish%$#@%^^@#\")\nresults: 4.155200004577637",
    "alexandrainst/da-discourse-coherence-base": "da-discourse-coherence-base\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nContributor\nda-discourse-coherence-base\nThis model is a fine-tuned version of NbAiLab/nb-bert-base on the DDisco dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.7487\nAccuracy: 0.6915\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 4\neval_batch_size: 4\nseed: 703\ngradient_accumulation_steps: 16\ntotal_train_batch_size: 64\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_ratio: 0.05\nnum_epochs: 6.0\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\n1.3422\n0.4\n5\n1.0166\n0.5721\n0.9645\n0.8\n10\n0.8966\n0.5721\n0.9854\n1.24\n15\n0.8499\n0.5721\n0.8628\n1.64\n20\n0.8379\n0.6517\n0.9046\n2.08\n25\n0.8228\n0.5721\n0.8361\n2.48\n30\n0.7980\n0.5821\n0.8158\n2.88\n35\n0.8095\n0.5821\n0.8689\n3.32\n40\n0.7989\n0.6169\n0.8125\n3.72\n45\n0.7730\n0.6965\n0.843\n4.16\n50\n0.7566\n0.6418\n0.7421\n4.56\n55\n0.7840\n0.6517\n0.7949\n4.96\n60\n0.7531\n0.6915\n0.828\n5.4\n65\n0.7464\n0.6816\n0.7438\n5.8\n70\n0.7487\n0.6915\nFramework versions\nTransformers 4.26.0\nPytorch 1.13.0a0+d0d6b1f\nDatasets 2.9.0\nTokenizers 0.13.2\nContributor\najders",
    "lllyasviel/ControlNet": "Description of Files\nRelated Resources\nMisuse, Malicious Use, and Out-of-Scope Use\nThis is the pretrained weights and some other detector weights of ControlNet.\nSee also: https://github.com/lllyasviel/ControlNet\nDescription of Files\nControlNet/models/control_sd15_canny.pth\nThe ControlNet+SD1.5 model to control SD using canny edge detection.\nControlNet/models/control_sd15_depth.pth\nThe ControlNet+SD1.5 model to control SD using Midas depth estimation.\nControlNet/models/control_sd15_hed.pth\nThe ControlNet+SD1.5 model to control SD using HED edge detection (soft edge).\nControlNet/models/control_sd15_mlsd.pth\nThe ControlNet+SD1.5 model to control SD using M-LSD line detection (will also work with traditional Hough transform).\nControlNet/models/control_sd15_normal.pth\nThe ControlNet+SD1.5 model to control SD using normal map. Best to use the normal map generated by that Gradio app. Other normal maps may also work as long as the direction is correct (left looks red, right looks blue, up looks green, down looks purple).\nControlNet/models/control_sd15_openpose.pth\nThe ControlNet+SD1.5 model to control SD using OpenPose pose detection. Directly manipulating pose skeleton should also work.\nControlNet/models/control_sd15_scribble.pth\nThe ControlNet+SD1.5 model to control SD using human scribbles. The model is trained with boundary edges with very strong data augmentation to simulate boundary lines similar to that drawn by human.\nControlNet/models/control_sd15_seg.pth\nThe ControlNet+SD1.5 model to control SD using semantic segmentation. The protocol is ADE20k.\nControlNet/annotator/ckpts/body_pose_model.pth\nThird-party model: Openposeâ€™s pose detection model.\nControlNet/annotator/ckpts/hand_pose_model.pth\nThird-party model: Openposeâ€™s hand detection model.\nControlNet/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\nThird-party model: Midas depth estimation model.\nControlNet/annotator/ckpts/mlsd_large_512_fp32.pth\nThird-party model: M-LSD detection model.\nControlNet/annotator/ckpts/mlsd_tiny_512_fp32.pth\nThird-party model: M-LSDâ€™s another smaller detection model (we do not use this one).\nControlNet/annotator/ckpts/network-bsds500.pth\nThird-party model: HED boundary detection.\nControlNet/annotator/ckpts/upernet_global_small.pth\nThird-party model: Uniformer semantic segmentation.\nControlNet/training/fill50k.zip\nThe data for our training tutorial.\nRelated Resources\nSpecial Thank to the great project - Mikubill' A1111 Webui Plugin !\nWe also thank Hysts for making Gradio demo in Hugging Face Space as well as more than 65 models in that amazing Colab list!\nThank haofanwang for making ControlNet-for-Diffusers!\nWe also thank all authors for making Controlnet DEMOs, including but not limited to fffiloni, other-model, ThereforeGames, RamAnanth1, etc!\nMisuse, Malicious Use, and Out-of-Scope Use\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.",
    "Kaludi/food-category-classification-v2.0": "Food Category Classification v2.0\nValidation Metrics\nFood Category Classification v2.0\nThis is an updated Food Category Image Classifier model of the old model that has been trained by Kaludi to recognize 12 different categories of foods, which includes Bread, Dairy, Dessert, Egg, Fried Food, Fruit, Meat, Noodles, Rice, Seafood, Soup, and Vegetable. It can accurately classify an image of food into one of these categories by analyzing its visual features. This model can be used by food bloggers, restaurants, and recipe websites to quickly categorize and sort their food images, making it easier to manage their content and provide a better user experience.\nGradio\nThis model supports a Gradio Web UI to run the data-food-classification model:\nValidation Metrics\nProblem type: Multi-class Classification\nModel ID: 3353292434\nCO2 Emissions (in grams): 12.4563\nLoss: 0.144\nAccuracy: 0.960\nMacro F1: 0.959\nMicro F1: 0.960\nWeighted F1: 0.959\nMacro Precision: 0.962\nMicro Precision: 0.960\nWeighted Precision: 0.962\nMacro Recall: 0.960\nMicro Recall: 0.960\nWeighted Recall: 0.960",
    "Unbabel/wmt22-comet-da": "Paper\nLicense\nUsage (unbabel-comet)\nIntended uses\nLanguages Covered:\nThis is a COMET evaluation model: It receives a triplet with (source sentence, translation, reference translation) and returns a score that reflects the quality of the translation compared to both source and reference.\nPaper\nCOMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task (Rei et al., WMT 2022)\nLicense\nApache-2.0\nUsage (unbabel-comet)\nUsing this model requires unbabel-comet to be installed:\npip install --upgrade pip  # ensures that pip is current\npip install unbabel-comet\nThen you can use it through comet CLI:\ncomet-score -s {source-inputs}.txt -t {translation-outputs}.txt -r {references}.txt --model Unbabel/wmt22-comet-da\nOr using Python:\nfrom comet import download_model, load_from_checkpoint\nmodel_path = download_model(\"Unbabel/wmt22-comet-da\")\nmodel = load_from_checkpoint(model_path)\ndata = [\n{\n\"src\": \"Dem Feuer konnte Einhalt geboten werden\",\n\"mt\": \"The fire could be stopped\",\n\"ref\": \"They were able to control the fire.\"\n},\n{\n\"src\": \"Schulen und KindergÃ¤rten wurden erÃ¶ffnet.\",\n\"mt\": \"Schools and kindergartens were open\",\n\"ref\": \"Schools and kindergartens opened\"\n}\n]\nmodel_output = model.predict(data, batch_size=8, gpus=1)\nprint (model_output)\nIntended uses\nOur model is intented to be used for MT evaluation.\nGiven a a triplet with (source sentence, translation, reference translation) outputs a single score between 0 and 1 where 1 represents a perfect translation.\nLanguages Covered:\nThis model builds on top of XLM-R which cover the following languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanized, Bosnian, Breton, Bulgarian, Burmese, Burmese, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanized, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskri, Scottish, Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanized, Telugu, Telugu Romanized, Thai, Turkish, Ukrainian, Urdu, Urdu Romanized, Uyghur, Uzbek, Vietnamese, Welsh, Western, Frisian, Xhosa, Yiddish.\nThus, results for language pairs containing uncovered languages are unreliable!",
    "nolanaatama/embeddings": "README.md exists but content is empty.",
    "patrickjohncyh/fashion-clip": "Model Card: Fashion CLIP\nModel Details\nModel Date\nModel Type\nDocuments\nData\nLimitations, Bias and Fiarness\nCitation\nModel Card: Fashion CLIP\nDisclaimer: The model card adapts the model card from here.\nModel Details\nUPDATE (10/03/23): We have updated the model! We found that laion/CLIP-ViT-B-32-laion2B-s34B-b79K checkpoint (thanks Bin!) worked better than original OpenAI CLIP on Fashion. We thus fine-tune a newer (and better!) version of FashionCLIP (henceforth FashionCLIP 2.0), while keeping the architecture the same. We postulate that the perofrmance gains afforded by laion/CLIP-ViT-B-32-laion2B-s34B-b79K are due to the increased training data (5x OpenAI CLIP data). Our thesis, however, remains the same -- fine-tuning laion/CLIP on our fashion dataset improved zero-shot perofrmance across our benchmarks. See the below table comparing weighted macro F1 score across models.\nModel\nFMNIST\nKAGL\nDEEP\nOpenAI CLIP\n0.66\n0.63\n0.45\nFashionCLIP\n0.74\n0.67\n0.48\nLaion CLIP\n0.78\n0.71\n0.58\nFashionCLIP 2.0\n0.83\n0.73\n0.62\nFashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, we train FashionCLIP on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks. FashionCLIP was not developed for model deplyoment - to do so, researchers will first need to carefully study their capabilities in relation to the specific context theyâ€™re being deployed within.\nModel Date\nMarch 2023\nModel Type\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained, starting from a pre-trained checkpoint, to maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset containing 800K products.\nDocuments\nFashionCLIP Github Repo\nFashionCLIP Paper\nData\nThe model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the highlight (e.g., â€œstripesâ€, â€œlong sleevesâ€, â€œArmaniâ€) and short description (â€œ80s styled t-shirtâ€)) available in the Farfetch dataset.\nLimitations, Bias and Fiarness\nWe acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in \"blue shoes for a woman\" that inevitably associate aspects of clothing with specific people.\nOur investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background).\nModel selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large.\nCitation\n@Article{Chia2022,\ntitle=\"Contrastive language and vision learning of general fashion concepts\",\nauthor=\"Chia, Patrick John\nand Attanasio, Giuseppe\nand Bianchi, Federico\nand Terragni, Silvia\nand Magalh{\\~a}es, Ana Rita\nand Goncalves, Diogo\nand Greco, Ciro\nand Tagliabue, Jacopo\",\njournal=\"Scientific Reports\",\nyear=\"2022\",\nmonth=\"Nov\",\nday=\"08\",\nvolume=\"12\",\nnumber=\"1\",\nabstract=\"The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by FashionCLIP with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community.\",\nissn=\"2045-2322\",\ndoi=\"10.1038/s41598-022-23052-9\",\nurl=\"https://doi.org/10.1038/s41598-022-23052-9\"\n}",
    "marcoyang/sherpa-ncnn-streaming-zipformer-zh-14M-2023-02-23": "README.md exists but content is empty.",
    "lllyasviel/sd-controlnet-scribble": "Controlnet - Scribble Version\nModel Details\nIntroduction\nReleased Checkpoints\nExample\nTraining\nBlog post\nControlnet - Scribble Version\nControlNet is a neural network structure to control diffusion models by adding extra conditions.\nThis checkpoint corresponds to the ControlNet conditioned on Scribble images.\nIt can be used in combination with Stable Diffusion.\nModel Details\nDeveloped by: Lvmin Zhang, Maneesh Agrawala\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@misc{zhang2023adding,\ntitle={Adding Conditional Control to Text-to-Image Diffusion Models},\nauthor={Lvmin Zhang and Maneesh Agrawala},\nyear={2023},\neprint={2302.05543},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nIntroduction\nControlnet was proposed in Adding Conditional Control to Text-to-Image Diffusion Models by\nLvmin Zhang, Maneesh Agrawala.\nThe abstract reads as follows:\nWe present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions.\nThe ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k).\nMoreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices.\nAlternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data.\nWe report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc.\nThis may enrich the methods to control large diffusion models and further facilitate related applications.\nReleased Checkpoints\nThe authors released 8 different checkpoints, each trained with Stable Diffusion v1-5\non a different type of conditioning:\nModel Name\nControl Image Overview\nControl Image Example\nGenerated Image Example\nlllyasviel/sd-controlnet-canny Trained with canny edge detection\nA monochrome image with white edges on a black background.\nlllyasviel/sd-controlnet-depth Trained with Midas depth estimation\nA grayscale image with black representing deep areas and white representing shallow areas.\nlllyasviel/sd-controlnet-hed Trained with HED edge detection (soft edge)\nA monochrome image with white soft edges on a black background.\nlllyasviel/sd-controlnet-mlsd Trained with M-LSD line detection\nA monochrome image composed only of white straight lines on a black background.\nlllyasviel/sd-controlnet-normal Trained with normal map\nA normal mapped image.\nlllyasviel/sd-controlnet_openpose Trained with OpenPose bone image\nA OpenPose bone image.\nlllyasviel/sd-controlnet_scribble Trained with human scribbles\nA hand-drawn monochrome image with white outlines on a black background.\nlllyasviel/sd-controlnet_segTrained with semantic segmentation\nAn ADE20K's segmentation protocol image.\nExample\nIt is recommended to use the checkpoint with Stable Diffusion v1-5 as the checkpoint\nhas been trained on it.\nExperimentally, the checkpoint can be used with other diffusion models such as dreamboothed stable diffusion.\nNote: If you want to process an image to create the auxiliary conditioning, external dependencies are required as shown below:\nInstall https://github.com/patrickvonplaten/controlnet_aux\n$ pip install controlnet_aux\nLet's install diffusers and related packages:\n$ pip install diffusers transformers accelerate\nRun code:\nfrom PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nfrom controlnet_aux import HEDdetector\nfrom diffusers.utils import load_image\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png\")\nimage = hed(image, scribble=True)\ncontrolnet = ControlNetModel.from_pretrained(\n\"lllyasviel/sd-controlnet-scribble\", torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# Remove if you do not have xformers installed\n# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n# for installation instructions\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(\"bag\", image, num_inference_steps=20).images[0]\nimage.save('images/bag_scribble_out.png')\nTraining\nThe scribble model was trained on 500k scribble-image, caption pairs. The scribble images were generated with HED boundary detection and a set of data augmentations â€” thresholds, masking, morphological transformations, and non-maximum suppression. The model was trained for 150 GPU-hours with Nvidia A100 80G using the canny model as a base model.\nBlog post\nFor more information, please also have a look at the official ControlNet Blog Post.",
    "lllyasviel/sd-controlnet-seg": "Controlnet - Image Segmentation Version\nModel Details\nIntroduction\nReleased Checkpoints\nExample\nTraining\nBlog post\nControlnet - Image Segmentation Version\nControlNet is a neural network structure to control diffusion models by adding extra conditions.\nThis checkpoint corresponds to the ControlNet conditioned on Image Segmentation.\nIt can be used in combination with Stable Diffusion.\nModel Details\nDeveloped by: Lvmin Zhang, Maneesh Agrawala\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@misc{zhang2023adding,\ntitle={Adding Conditional Control to Text-to-Image Diffusion Models},\nauthor={Lvmin Zhang and Maneesh Agrawala},\nyear={2023},\neprint={2302.05543},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nIntroduction\nControlnet was proposed in Adding Conditional Control to Text-to-Image Diffusion Models by\nLvmin Zhang, Maneesh Agrawala.\nThe abstract reads as follows:\nWe present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions.\nThe ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k).\nMoreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices.\nAlternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data.\nWe report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc.\nThis may enrich the methods to control large diffusion models and further facilitate related applications.\nReleased Checkpoints\nThe authors released 8 different checkpoints, each trained with Stable Diffusion v1-5\non a different type of conditioning:\nModel Name\nControl Image Overview\nControl Image Example\nGenerated Image Example\nlllyasviel/sd-controlnet-canny Trained with canny edge detection\nA monochrome image with white edges on a black background.\nlllyasviel/sd-controlnet-depth Trained with Midas depth estimation\nA grayscale image with black representing deep areas and white representing shallow areas.\nlllyasviel/sd-controlnet-hed Trained with HED edge detection (soft edge)\nA monochrome image with white soft edges on a black background.\nlllyasviel/sd-controlnet-mlsd Trained with M-LSD line detection\nA monochrome image composed only of white straight lines on a black background.\nlllyasviel/sd-controlnet-normal Trained with normal map\nA normal mapped image.\nlllyasviel/sd-controlnet_openpose Trained with OpenPose bone image\nA OpenPose bone image.\nlllyasviel/sd-controlnet_scribble Trained with human scribbles\nA hand-drawn monochrome image with white outlines on a black background.\nlllyasviel/sd-controlnet_segTrained with semantic segmentation\nAn ADE20K's segmentation protocol image.\nExample\nIt is recommended to use the checkpoint with Stable Diffusion v1-5 as the checkpoint\nhas been trained on it.\nExperimentally, the checkpoint can be used with other diffusion models such as dreamboothed stable diffusion.\nLet's install diffusers and related packages:\n$ pip install diffusers transformers accelerate\nWe'll need to make use of a color palette here as described in semantic_segmentation:\npalette = np.asarray([\n[0, 0, 0],\n[120, 120, 120],\n[180, 120, 120],\n[6, 230, 230],\n[80, 50, 50],\n[4, 200, 3],\n[120, 120, 80],\n[140, 140, 140],\n[204, 5, 255],\n[230, 230, 230],\n[4, 250, 7],\n[224, 5, 255],\n[235, 255, 7],\n[150, 5, 61],\n[120, 120, 70],\n[8, 255, 51],\n[255, 6, 82],\n[143, 255, 140],\n[204, 255, 4],\n[255, 51, 7],\n[204, 70, 3],\n[0, 102, 200],\n[61, 230, 250],\n[255, 6, 51],\n[11, 102, 255],\n[255, 7, 71],\n[255, 9, 224],\n[9, 7, 230],\n[220, 220, 220],\n[255, 9, 92],\n[112, 9, 255],\n[8, 255, 214],\n[7, 255, 224],\n[255, 184, 6],\n[10, 255, 71],\n[255, 41, 10],\n[7, 255, 255],\n[224, 255, 8],\n[102, 8, 255],\n[255, 61, 6],\n[255, 194, 7],\n[255, 122, 8],\n[0, 255, 20],\n[255, 8, 41],\n[255, 5, 153],\n[6, 51, 255],\n[235, 12, 255],\n[160, 150, 20],\n[0, 163, 255],\n[140, 140, 140],\n[250, 10, 15],\n[20, 255, 0],\n[31, 255, 0],\n[255, 31, 0],\n[255, 224, 0],\n[153, 255, 0],\n[0, 0, 255],\n[255, 71, 0],\n[0, 235, 255],\n[0, 173, 255],\n[31, 0, 255],\n[11, 200, 200],\n[255, 82, 0],\n[0, 255, 245],\n[0, 61, 255],\n[0, 255, 112],\n[0, 255, 133],\n[255, 0, 0],\n[255, 163, 0],\n[255, 102, 0],\n[194, 255, 0],\n[0, 143, 255],\n[51, 255, 0],\n[0, 82, 255],\n[0, 255, 41],\n[0, 255, 173],\n[10, 0, 255],\n[173, 255, 0],\n[0, 255, 153],\n[255, 92, 0],\n[255, 0, 255],\n[255, 0, 245],\n[255, 0, 102],\n[255, 173, 0],\n[255, 0, 20],\n[255, 184, 184],\n[0, 31, 255],\n[0, 255, 61],\n[0, 71, 255],\n[255, 0, 204],\n[0, 255, 194],\n[0, 255, 82],\n[0, 10, 255],\n[0, 112, 255],\n[51, 0, 255],\n[0, 194, 255],\n[0, 122, 255],\n[0, 255, 163],\n[255, 153, 0],\n[0, 255, 10],\n[255, 112, 0],\n[143, 255, 0],\n[82, 0, 255],\n[163, 255, 0],\n[255, 235, 0],\n[8, 184, 170],\n[133, 0, 255],\n[0, 255, 92],\n[184, 0, 255],\n[255, 0, 31],\n[0, 184, 255],\n[0, 214, 255],\n[255, 0, 112],\n[92, 255, 0],\n[0, 224, 255],\n[112, 224, 255],\n[70, 184, 160],\n[163, 0, 255],\n[153, 0, 255],\n[71, 255, 0],\n[255, 0, 163],\n[255, 204, 0],\n[255, 0, 143],\n[0, 255, 235],\n[133, 255, 0],\n[255, 0, 235],\n[245, 0, 255],\n[255, 0, 122],\n[255, 245, 0],\n[10, 190, 212],\n[214, 255, 0],\n[0, 204, 255],\n[20, 0, 255],\n[255, 255, 0],\n[0, 153, 255],\n[0, 41, 255],\n[0, 255, 204],\n[41, 0, 255],\n[41, 255, 0],\n[173, 0, 255],\n[0, 245, 255],\n[71, 0, 255],\n[122, 0, 255],\n[0, 255, 184],\n[0, 92, 255],\n[184, 255, 0],\n[0, 133, 255],\n[255, 214, 0],\n[25, 194, 194],\n[102, 255, 0],\n[92, 0, 255],\n])\nHaving defined the color palette we can now run the whole segmentation + controlnet generation code:\nfrom transformers import AutoImageProcessor, UperNetForSemanticSegmentation\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\nimage_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\nimage_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-seg/resolve/main/images/house.png\").convert('RGB')\npixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\nwith torch.no_grad():\noutputs = image_segmentor(pixel_values)\nseg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\ncolor_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\ncolor_seg[seg == label, :] = color\ncolor_seg = color_seg.astype(np.uint8)\nimage = Image.fromarray(color_seg)\ncontrolnet = ControlNetModel.from_pretrained(\n\"lllyasviel/sd-controlnet-seg\", torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# Remove if you do not have xformers installed\n# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n# for installation instructions\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(\"house\", image, num_inference_steps=20).images[0]\nimage.save('./images/house_seg_out.png')\nTraining\nThe semantic segmentation model was trained on 164K segmentation-image, caption pairs from ADE20K. The model was trained for 200 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model.\nBlog post\nFor more information, please also have a look at the official ControlNet Blog Post.",
    "waifu-diffusion/wd-1-5-beta2": "WD 1.5 Beta 2\nRelease Notes\nExample Images\nVAE\nLicense\nWD 1.5 Beta 2\nFor this release, we release two versions of the model:\nWD 1.5 Beta 2\nWD 1.5 Beta 2 Aesthetic\nFor the aesthetic version, we finetune the attention layer on popular aesthetic images. For training, it is recomended to use the base version.\nVAE\nWD 1.5 uses the same VAE as WD 1.4, which can be found here https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt\nRelease Notes\nhttps://cafeai.notion.site/WD-1-5-Beta-2-Release-Notes-2852db5a9cdd456ba52fc5730b91acfd\nExample Images\nhttps://cafeai.notion.site/WD-1-5-Beta-2-Aesthetic-Ver-c44a410fec06478fbf1a08a9890310ff\nLicense\nWD 1.5 is released under the Fair AI Public License 1.0-SD (https://freedevproject.org/faipl-1.0-sd/). If any derivative of this model is made, please share your changes accordingly. Special thanks to ronsor/undeleted (https://undeleted.ronsor.com/) for help with the license.",
    "typesense/models-moved": "Typesense Built-in Embedding Models\nUsage\nContributing\nConvert a model to ONNX format\nTypesense Built-in Embedding Models\nThis repository holds all the built-in ML models supported by Typesense for semantic search currently.\nIf you have a model that you would like to add to our supported list, you can convert it to the ONNX format and create a Pull Request (PR) to include it. (See below for instructions).\nUsage\nHere's an example of how to specify the model to use for auto-embedding generation when creating a collection in Typesense:\ncurl -X POST \\\n'http://localhost:8108/collections' \\\n-H 'Content-Type: application/json' \\\n-H \"X-TYPESENSE-API-KEY: ${TYPESENSE_API_KEY}\" \\\n-d '{\n\"name\": \"products\",\n\"fields\": [\n{\n\"name\": \"product_name\",\n\"type\": \"string\"\n},\n{\n\"name\": \"embedding\",\n\"type\": \"float[]\",\n\"embed\": {\n\"from\": [\n\"product_name\"\n],\n\"model_config\": {\n\"model_name\": \"ts/all-MiniLM-L12-v2\"\n}\n}\n}\n]\n}'\nReplace all-MiniLM-L12-v2 with any model name from this repository.\nHere's a detailed step-by-step article with more information: https://typesense.org/docs/guide/semantic-search.html\nContributing\nIf you have a model that you would like to add to our supported list, you can convert it to the ONNX format and create a Pull Request (PR) to include it. (See below for instructions).\nConvert a model to ONNX format\nConverting a Hugging Face Transformers Model\nTo convert any model from Hugging Face to ONNX format, you can follow the instructions in this link using the optimum-cli.\nConverting a PyTorch Model\nIf you have a PyTorch model, you can use the torch.onnx APIs to convert it to the ONNX format. More information on the conversion process can be found  here.\nConverting a Tensorflow Model\nFor Tensorflow models, you can utilize the tf2onnx tool to convert them to the ONNX format. Detailed guidance on this conversion can be found here.\nCreating model config\nBefore submitting your ONNX model through a PR, you need to organize the necessary files under a folder with the model's name. Ensure that your model configuration adheres to the following structure:\nModel File: The ONNX model file.\nVocab File: The vocabulary file required for the model.\nModel Config File: Named as config.json, this file should contain the following keys:\nKey\nDescription\nOptional\nmodel_md5\nMD5 checksum of model file as string\nNo\nvocab_md5\nMD5 checksum of vocab file as string\nNo\nmodel_type\nModel type (currently only bert and xlm_roberta supported)\nNo\nvocab_file_name\nFile name of vocab file\nNo\nindexing_prefix\nPrefix to be added before embedding documents\nYes\nquery_prefix\nPrefix to be added before embedding queries\nYes\nPlease make sure that the information in the configuration file is accurate and complete before submitting your PR.\nWe appreciate your contributions to expand our collection of supported embedding models!",
    "nyanko7/LLaMA-7B": "README.md exists but content is empty.",
    "Serenak/chilloutmix": "README.md exists but content is empty.",
    "Seyfelislem/whisper-medium-arabic": "whisper-medium-arabic-streaming\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nwhisper-medium-arabic-streaming\nThis model is a fine-tuned version of openai/whisper-medium on the None dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.2194\nWer: 18.2888\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 2\neval_batch_size: 8\nseed: 42\ngradient_accumulation_steps: 16\ntotal_train_batch_size: 32\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 500\ntraining_steps: 800\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nWer\n0.3327\n1.0\n800\n0.2194\n18.2888\nFramework versions\nTransformers 4.27.0.dev0\nPytorch 1.13.0\nDatasets 2.10.2.dev0\nTokenizers 0.13.2",
    "facebook/fasttext-language-identification": "fastText (Language Identification)\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nTokenization\nLicense\nEvaluation datasets\nBibTeX entry and citation info\nfastText (Language Identification)\nfastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in this paper. The official website can be found here.\nThis LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (lid218e) was released as part of the NLLB project and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the official fastText website.\nModel description\nfastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.\nIt includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.\nIntended uses & limitations\nYou can use pre-trained word vectors for text classification or language identification. See the tutorials and resources on its official website to look for tasks that interest you.\nHow to use\nHere is how to use this model to detect the language of a given text:\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n>>> model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\n(('__label__eng_Latn',), array([0.81148803]))\n>>> model.predict(\"Hello, world!\", k=5)\n(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'),\narray([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))\nLimitations and bias\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.\nCosine similarity can be used to measure the similarity between two different word vectors. If two two vectors are identical, the cosine similarity will be 1. For two completely unrelated vectors, the value will be 0. If two vectors have an opposite relationship, the value will be -1.\n>>> import numpy as np\n>>> def cosine_similarity(word1, word2):\n>>>     return np.dot(model[word1], model[word2]) / (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n>>> cosine_similarity(\"man\", \"boy\")\n0.061653383\n>>> cosine_similarity(\"man\", \"ceo\")\n0.11989131\n>>> cosine_similarity(\"woman\", \"ceo\")\n-0.08834904\nTraining data\nPre-trained word vectors for 157 languages were trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.\nTraining procedure\nTokenization\nWe used the Stanford word segmenter for Chinese, Mecab for Japanese and UETsegmenter for Vietnamese. For languages using the Latin, Cyrillic, Hebrew or Greek scripts, we used the tokenizer from the Europarl preprocessing tools. For the remaining languages, we used the ICU tokenizer.\nMore information about the training of these models can be found in the article Learning Word Vectors for 157 Languages.\nLicense\nThe language identification model is distributed under the Creative Commons Attribution-NonCommercial 4.0 International Public License.\nEvaluation datasets\nThe analogy evaluation datasets described in the paper are available here: French, Hindi, Polish.\nBibTeX entry and citation info\nPlease cite [1] if using this code for learning word representations or [2] if using for text classification.\n[1] P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information\n@article{bojanowski2016enriching,\ntitle={Enriching Word Vectors with Subword Information},\nauthor={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\njournal={arXiv preprint arXiv:1607.04606},\nyear={2016}\n}\n[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification\n@article{joulin2016bag,\ntitle={Bag of Tricks for Efficient Text Classification},\nauthor={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\njournal={arXiv preprint arXiv:1607.01759},\nyear={2016}\n}\n[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. JÃ©gou, T. Mikolov, FastText.zip: Compressing text classification models\n@article{joulin2016fasttext,\ntitle={FastText.zip: Compressing text classification models},\nauthor={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{'e}gou, H{'e}rve and Mikolov, Tomas},\njournal={arXiv preprint arXiv:1612.03651},\nyear={2016}\n}\nIf you use these word vectors, please cite the following paper:\n[4] E. Grave*, P. Bojanowski*, P. Gupta, A. Joulin, T. Mikolov, Learning Word Vectors for 157 Languages\n@inproceedings{grave2018learning,\ntitle={Learning Word Vectors for 157 Languages},\nauthor={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\nbooktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\nyear={2018}\n}\n(* These authors contributed equally.)",
    "thibaud/controlnet-sd21": "Safetensors version uploaded, only 700mb!\nCanny:\nDepth:\nZoeDepth:\nHed:\nScribble:\nOpenPose:\nColor:\nOpenPose:\nLineArt:\nAde20K:\nNormal BAE:\nTo use with Automatic1111:\nTo use ZoeDepth:\nMisuse, Malicious Use, and Out-of-Scope Use\nModels can't be sell, merge, distributed without prior writing agreement.\nWant to support my work: you can bought my Artbook: https://thibaud.art\nHere's the first version of controlnet for stablediffusion 2.1\nTrained on a subset of laion/laion-art\nLicense: refers to the different preprocessor's ones.\nSafetensors version uploaded, only 700mb!\nCanny:\nDepth:\nZoeDepth:\nHed:\nScribble:\nOpenPose:\nColor:\nOpenPose:\nLineArt:\nAde20K:\nNormal BAE:\nTo use with Automatic1111:\nDownload the ckpt files or safetensors ones\nPut it in extensions/sd-webui-controlnet/models\nin settings/controlnet, change cldm_v15.yaml by cldm_v21.yaml\nEnjoy\nTo use ZoeDepth:\nYou can use it with annotator depth/le_res but it works better with ZoeDepth Annotator. My PR is not accepted yet but you can use my fork.\nMy fork: https://github.com/thibaudart/sd-webui-controlnet\nThe PR: https://github.com/Mikubill/sd-webui-controlnet/pull/655#issuecomment-1481724024\nMisuse, Malicious Use, and Out-of-Scope Use\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nThanks https://huggingface.co/lllyasviel/ for the implementation and the release of 1.5 models.\nThanks https://huggingface.co/p1atdev/ for the conversion script from ckpt to safetensors pruned & fp16\nModels can't be sell, merge, distributed without prior writing agreement.",
    "AI-Porn/AI-PORN": "",
    "google/pix2struct-widget-captioning-large": ""
}