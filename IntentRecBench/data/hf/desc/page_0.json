{
    "deepseek-ai/DeepSeek-OCR": "Usage\nvLLM\nVisualizations\nAcknowledgement\nCitation\nüåü Github |\nüì• Model Download |\nüìÑ Paper Link |\nüìÑ Arxiv Paper Link |\nDeepSeek-OCR: Contexts Optical Compression\nExplore the boundaries of visual-text compression.\nUsage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8Ôºö\ntorch==2.6.0\ntransformers==4.46.3\ntokenizers==0.20.3\neinops\naddict\neasydict\npip install flash-attn==2.7.3 --no-build-isolation\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR'\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\nvLLM\nRefer to üåüGitHub for guidance on model inference acceleration and PDF processing, etc.\n[2025/10/23] üöÄüöÄüöÄ DeepSeek-OCR is now officially supported in upstream vLLM.\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n# Create model instance\nllm = LLM(\nmodel=\"deepseek-ai/DeepSeek-OCR\",\nenable_prefix_caching=False,\nmm_processor_cache_gb=0,\nlogits_processors=[NGramPerReqLogitsProcessor]\n)\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\nmodel_input = [\n{\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": image_1}\n},\n{\n\"prompt\": prompt,\n\"multi_modal_data\": {\"image\": image_2}\n}\n]\nsampling_param = SamplingParams(\ntemperature=0.0,\nmax_tokens=8192,\n# ngram logit processor args\nextra_args=dict(\nngram_size=30,\nwindow_size=90,\nwhitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n),\nskip_special_tokens=False,\n)\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n# Print output\nfor output in model_outputs:\nprint(output.outputs[0].text)\nVisualizations\nAcknowledgement\nWe would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.\nWe also appreciate the benchmarks: Fox, OminiDocBench.\nCitation\n@article{wei2025deepseek,\ntitle={DeepSeek-OCR: Contexts Optical Compression},\nauthor={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\njournal={arXiv preprint arXiv:2510.18234},\nyear={2025}\n}",
    "MiniMaxAI/MiniMax-M2": "Meet MiniMax-M2\nHighlights\nCoding & Agentic Benchmarks\nIntelligence Benchmarks\nWhy activation size matters\nAt a glance\nHow to Use\nLocal Deployment Guide\nSGLang\nvLLM\nInference Parameters\nTool Calling Guide\nCommunity Showcases\nContact Us\nMeet MiniMax-M2\nToday, we release and open source MiniMax-M2, a Mini model built for Max coding & agentic workflows.\nMiniMax-M2 redefines efficiency for agents. It's a compact, fast, and cost-effective MoE model (230 billion total parameters with 10 billion active parameters) built for elite performance in coding and agentic tasks, all while maintaining powerful general intelligence. With just 10 billion activated parameters, MiniMax-M2 provides the sophisticated, end-to-end tool use performance expected from today's leading models, but in a streamlined form factor that makes deployment and scaling easier than ever.\nHighlights\nSuperior Intelligence. According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use. Its composite score ranks #1 among open-source models globally.\nAdvanced Coding. Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench‚Äìstyle tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages.\nAgent Performance. MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps.\nEfficient Design. With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling‚Äîperfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks.\nCoding & Agentic Benchmarks\nThese comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI.\nBenchmark\nMiniMax-M2\nClaude Sonnet 4\nClaude Sonnet 4.5\nGemini 2.5 Pro\nGPT-5 (thinking)\nGLM-4.6\nKimi K2 0905\nDeepSeek-V3.2\nSWE-bench Verified\n69.4\n72.7 *\n77.2 *\n63.8 *\n74.9 *\n68 *\n69.2 *\n67.8 *\nMulti-SWE-Bench\n36.2\n35.7 *\n44.3\n/\n/\n30\n33.5\n30.6\nSWE-bench Multilingual\n56.5\n56.9 *\n68\n/\n/\n53.8\n55.9 *\n57.9 *\nTerminal-Bench\n46.3\n36.4 *\n50 *\n25.3 *\n43.8 *\n40.5 *\n44.5 *\n37.7 *\nArtifactsBench\n66.8\n57.3*\n61.5\n57.7*\n73*\n59.8\n54.2\n55.8\nBrowseComp\n44\n12.2\n19.6\n9.9\n54.9*\n45.1*\n14.1\n40.1*\nBrowseComp-zh\n48.5\n29.1\n40.8\n32.2\n65\n49.5\n28.8\n47.9*\nGAIA (text only)\n75.7\n68.3\n71.2\n60.2\n76.4\n71.9\n60.2\n63.5\nxbench-DeepSearch\n72\n64.6\n66\n56\n77.8\n70\n61\n71\nHLE (w/ tools)\n31.8\n20.3\n24.5\n28.4 *\n35.2 *\n30.4 *\n26.9 *\n27.2 *\nœÑ¬≤-Bench\n77.2\n65.5*\n84.7*\n59.2\n80.1*\n75.9*\n70.3\n66.7\nFinSearchComp-global\n65.5\n42\n60.8\n42.6*\n63.9*\n29.2\n29.5*\n26.2\nAgentCompany\n36\n37\n41\n39.3*\n/\n35\n30\n34\nNotes: Data points marked with an asterisk (*) are taken directly from the model's official tech report or blog. All other metrics were obtained using the evaluation methods described below.\nSWE-bench Verified:  We use the same scaffold as R2E-Gym (Jain et al. 2025) on top of OpenHands to test with agents on SWE tasks. All scores are validated on our internal infrastructure with 128k context length, 100 max steps, and no test-time scaling. All git-related content is removed to ensure agent sees only the code at the issue point.\nMulti-SWE-Bench & SWE-bench Multilingual: All scores are averaged across 8 runs using the claude-code CLI (300 max steps) as the evaluation scaffold.\nTerminal-Bench: All scores are evaluated with the official claude-code from the original Terminal-Bench repository(commit 94bf692), averaged over 8 runs to report the mean pass rate.\nArtifactsBench: All Scores are computed by averaging three runs with the official implementation of ArtifactsBench, using the stable Gemini-2.5-Pro as the judge model.\nBrowseComp & BrowseComp-zh & GAIA (text only) & xbench-DeepSearch: All scores reported use the same agent framework as WebExplorer (Liu et al. 2025), with minor tools description adjustment. We use the 103-sample text-only GAIA validation subset following WebExplorer (Liu et al. 2025).\nHLE (w/ tools): All reported scores are obtained using search tools and a Python tool. The search tools employ the same agent framework as WebExplorer (Liu et al. 2025), and the Python tool runs in a Jupyter environment. We use the text-only HLE subset.\nœÑ¬≤-Bench: All scores reported use \"extended thinking with tool use\", and employ GPT-4.1 as the user simulator.\nFinSearchComp-global: Official results are reported for GPT-5-Thinking, Gemini 2.5 Pro, and Kimi-K2. Other models are evaluated using the open-source FinSearchComp (Hu et al. 2025) framework using both  search and Python tools, launched simultaneously for consistency.\nAgentCompany: All scores reported use OpenHands 0.42 agent framework.\nIntelligence Benchmarks\nWe align with Artificial Analysis, which aggregates challenging benchmarks using a consistent methodology to reflect a model‚Äôs broader intelligence profile across math, science, instruction following, coding, and agentic tool use.\nMetric (AA)\nMiniMax-M2\nClaude Sonnet 4\nClaude Sonnet 4.5\nGemini 2.5 Pro\nGPT-5 (thinking)\nGLM-4.6\nKimi K2 0905\nDeepSeek-V3.2\nAIME25\n78\n74\n88\n88\n94\n86\n57\n88\nMMLU-Pro\n82\n84\n88\n86\n87\n83\n82\n85\nGPQA-Diamond\n78\n78\n83\n84\n85\n78\n77\n80\nHLE (w/o tools)\n12.5\n9.6\n17.3\n21.1\n26.5\n13.3\n6.3\n13.8\nLiveCodeBench (LCB)\n83\n66\n71\n80\n85\n70\n61\n79\nSciCode\n36\n40\n45\n43\n43\n38\n31\n38\nIFBench\n72\n55\n57\n49\n73\n43\n42\n54\nAA-LCR\n61\n65\n66\n66\n76\n54\n52\n69\nœÑ¬≤-Bench-Telecom\n87\n65\n78\n54\n85\n71\n73\n34\nTerminal-Bench-Hard\n24\n30\n33\n25\n31\n23\n23\n29\nAA Intelligence\n61\n57\n63\n60\n69\n56\n50\n57\nAA: All scores of MiniMax-M2 aligned with Artificial Analysis Intelligence Benchmarking Methodology (https://artificialanalysis.ai/methodology/intelligence-benchmarking). All scores of other models reported from https://artificialanalysis.ai/.\nWhy activation size matters\nBy maintaining activations around 10B , the plan ‚Üí act ‚Üí verify loop in the agentic workflow is streamlined, improving responsiveness and reducing compute overhead:\nFaster feedback cycles in compile-run-test and browse-retrieve-cite chains.\nMore concurrent runs on the same budget for regression suites and multi-seed explorations.\nSimpler capacity planning with smaller per-request memory and steadier tail latency.\nIn short: 10B activations = responsive agent loops + better unit economics.\nAt a glance\nIf you need frontier-style coding and agents without frontier-scale costs, MiniMax-M2 hits the sweet spot: fast inference speeds, robust tool-use capabilities, and a deployment-friendly footprint.\nWe look forward to your feedback and to collaborating with developers and researchers to bring the future of intelligent collaboration one step closer.\nHow to Use\nOur product MiniMax Agent, built on MiniMax-M2, is now publicly available and free for a limited time: https://agent.minimax.io/\nThe MiniMax-M2 API is now live on the MiniMax Open Platform and is free for a limited time: https://platform.minimax.io/docs/guides/text-generation\nThe MiniMax-M2 model weights are now open-source, allowing for local deployment and use: https://huggingface.co/MiniMaxAI/MiniMax-M2.\nLocal Deployment Guide\nDownload the model from HuggingFace repository: https://huggingface.co/MiniMaxAI/MiniMax-M2. We recommend using the following inference frameworks (listed alphabetically) to serve the model:\nSGLang\nWe recommend using SGLang to serve MiniMax-M2. SGLang provides solid day-0 support for MiniMax-M2 model. Please refer to our SGLang Deployment Guide for more details, and thanks so much for our collaboration with the SGLang team.\nvLLM\nWe recommend using vLLM to serve MiniMax-M2. vLLM provides efficient day-0 support of MiniMax-M2 model, check https://docs.vllm.ai/projects/recipes/en/latest/MiniMax/MiniMax-M2.html for latest deployment guide. We also provide our vLLM Deployment Guide.\nInference Parameters\nWe recommend using the following parameters for best performance: temperature=1.0, top_p = 0.95, top_k = 40.\nIMPORTANT: MiniMax-M2 is an interleaved thinking model. Therefore, when using it, it is important to retain the thinking content from the assistant's turns within the historical messages. In the model's output content, we use the <think>...</think> format to wrap the assistant's thinking content. When using the model, you must ensure that the historical content is passed back in its original format. Do not remove the <think>...</think> part, otherwise, the model's performance will be negatively affected.\nTool Calling Guide\nPlease refer to our Tool Calling Guide.\nCommunity Showcases\nThe projects below are built and maintained by the community/partners. They are not official MiniMax products, and results may vary.\nAnyCoder ‚Äî a web IDE‚Äìstyle coding assistant Space on Hugging Face, uses MiniMax-M2 as the default model: https://huggingface.co/spaces/akhaliq/anycoderMaintainer: @akhaliq (Hugging Face)\nContact Us\nContact us at model@minimax.io.",
    "tencent/HunyuanWorld-Mirror": "‚òØÔ∏è HunyuanWorld-Mirror Introduction\nArchitecture\nüîó BibTeX\nAcknowledgements\nHunyuanWorld-Mirror is a versatile feed-forward model for comprehensive 3D geometric prediction. It integrates diverse geometric priors (camera poses, calibrated intrinsics, depth maps) and simultaneously generates various 3D representations (point clouds, multi-view depths, camera parameters, surface normals, 3D Gaussians) in a single forward pass.\n‚òØÔ∏è HunyuanWorld-Mirror Introduction\nArchitecture\nHunyuanWorld-Mirror consists of two key components:\n(1) Multi-Modal Prior Prompting: A mechanism that embeds diverse prior modalities,\nincluding calibrated intrinsics, camera pose, and depth, into the feed-forward model. Given any subset of the available priors, we utilize several lightweight encoding layers to convert each modality into structured tokens.\n(2) Universal Geometric Prediction: A unified architecture capable of handling\nthe full spectrum of 3D reconstruction tasks from camera and depth estimation to point map regression, surface normal estimation, and novel view synthesis.\nüîó BibTeX\nIf you find HunyuanWorld-Mirror useful for your research and applications, please cite using this BibTeX:\n@article{liu2025worldmirror,\ntitle={WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting},\nauthor={Liu, Yifan and Min, Zhiyuan and Wang, Zhenwei and Wu, Junta and Wang, Tengfei and Yuan, Yixuan and Luo, Yawei and Guo, Chunchao},\njournal={arXiv preprint arXiv:2510.10726},\nyear={2025}\n}\nAcknowledgements\nWe would like to thank HunyuanWorld. We also sincerely thank the authors and contributors of VGGT, Fast3R, CUT3R, and DUSt3R for their outstanding open-source work and pioneering research.",
    "PaddlePaddle/PaddleOCR-VL": "Introduction\nCore Features\nModel Architecture\nNews\nUsage\nInstall Dependencies\nBasic Usage\nAccelerate VLM Inference via Optimized Inference Servers\nPerformance\nPage-Level Document Parsing\n1. OmniDocBench v1.5\n2. OmniDocBench v1.0\nElement-level Recognition\n1. Text\n2. Table\n3. Formula\n4. Chart\nVisualization\nComprehensive Document Parsing\nText\nTable\nFormula\nChart\nAcknowledgments\nCitation\nPaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model\nüî• Official Demo: Baidu AI Studio |\nüìù arXiv: Technical Report\nIntroduction\nPaddleOCR-VL is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.\nCore Features\nCompact yet Powerful VLM Architecture: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the model‚Äôs recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.\nSOTA Performance on Document Parsing: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.\nMultilingual Support: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.\nModel Architecture\nNews\n2025.10.16 üöÄ We release PaddleOCR-VL, ‚Äî a multilingual documents parsing via a 0.9B Ultra-Compact Vision-Language Model with SOTA performance.\nUsage\nInstall Dependencies\nInstall PaddlePaddle and PaddleOCR:\npython -m pip install paddlepaddle-gpu==3.2.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\npython -m pip install -U \"paddleocr[doc-parser]\"\npython -m pip install https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl\nFor Windows users, please use WSL or a Docker container.\nBasic Usage\nCLI usage:\npaddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png\nPython API usage:\nfrom paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png\")\nfor res in output:\nres.print()\nres.save_to_json(save_path=\"output\")\nres.save_to_markdown(save_path=\"output\")\nAccelerate VLM Inference via Optimized Inference Servers\nStart the VLM inference server (the default port is 8080):\ndocker run \\\n--rm \\\n--gpus all \\\n--network host \\\nccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddlex-genai-vllm-server\nCall the PaddleOCR CLI or Python API:\npaddleocr doc_parser \\\n-i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png \\\n--vl_rec_backend vllm-server \\\n--vl_rec_server_url http://127.0.0.1:8080/v1\nfrom paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL(vl_rec_backend=\"vllm-server\", vl_rec_server_url=\"http://127.0.0.1:8080/v1\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png\")\nfor res in output:\nres.print()\nres.save_to_json(save_path=\"output\")\nres.save_to_markdown(save_path=\"output\")\nFor more usage details and parameter explanations, see the documentation.\nPerformance\nPage-Level Document Parsing\n1. OmniDocBench v1.5\nPaddleOCR-VL achieves SOTA performance for overall, text, formula, tables and reading order on OmniDocBench v1.5\n2. OmniDocBench v1.0\nPaddleOCR-VL achieves SOTA performance for almost all metrics of overall, text, formula, tables and reading order on OmniDocBench v1.0\nNotes:\nThe metrics are from MinerU, OmniDocBench, and our own internal evaluations.\nElement-level Recognition\n1. Text\nComparison of OmniDocBench-OCR-block Performance\nPaddleOCR-VL‚Äôs robust and versatile capability in handling diverse document types, establishing it as the leading method in the OmniDocBench-OCR-block performance evaluation.\nComparison of In-house-OCR Performance\nIn-house-OCR provides a evaluation of performance across multiple languages and text types. Our model demonstrates outstanding accuracy with the lowest edit distances in all evaluated scripts.\n2. Table\nComparison of In-house-Table Performance\nOur self-built evaluation set contains diverse types of table images, such as Chinese, English, mixed Chinese-English, and tables with various characteristics like full, partial, or no borders, book/manual formats, lists, academic papers, merged cells, as well as low-quality, watermarked, etc. PaddleOCR-VL achieves remarkable performance across all categories.\n3. Formula\nComparison of In-house-Formula Performance\nIn-house-Formula evaluation set contains simple prints, complex prints, camera scans, and handwritten formulas. PaddleOCR-VL demonstrates the best performance in every category.\n4. Chart\nComparison of In-house-Chart Performance\nThe evaluation set is broadly categorized into 11 chart categories, including bar-line hybrid, pie, 100% stacked bar, area, bar, bubble, histogram, line, scatterplot, stacked area, and stacked bar. PaddleOCR-VL not only outperforms expert OCR VLMs but also surpasses some 72B-level multimodal language models.\nVisualization\nComprehensive Document Parsing\nText\nTable\nFormula\nChart\nAcknowledgments\nWe would like to thank ERNIE, Keye, MinerU, OmniDocBench for providing valuable code, model weights and benchmarks. We also appreciate everyone's contribution to this open-source project!\nCitation\nIf you find PaddleOCR-VL helpful, feel free to give us a star and citation.\n@misc{cui2025paddleocrvlboostingmultilingualdocument,\ntitle={PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model},\nauthor={Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},\nyear={2025},\neprint={2510.14528},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.14528},\n}",
    "meituan-longcat/LongCat-Video": "LongCat-Video\nModel Introduction\nKey Features\nQuick Start\nInstallation\nModel Download\nRun Text-to-Video\nRun Image-to-Video\nRun Video-Continuation\nRun Long-Video Generation\nRun Streamlit\nEvaluation Results\nText-to-Video\nImage-to-Video\nLicense Agreement\nUsage Considerations\nCitation\nAcknowledgements\nContact\nLongCat-Video\nModel Introduction\nWe introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across Text-to-Video, Image-to-Video, and Video-Continuation generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models.\nKey Features\nüåü Unified architecture for multiple tasks: LongCat-Video unifies Text-to-Video, Image-to-Video, and Video-Continuation tasks within a single video generation framework. It natively supports all these tasks with a single model and consistently delivers strong performance across each individual task.\nüåü Long video generation: LongCat-Video is natively pretrained on Video-Continuation tasks, enabling it to produce minutes-long videos without color drifting or quality degradation.\nüåü Efficient inference: LongCat-Video generates $720p$, $30fps$ videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions\nüåü Strong performance with multi-reward RLHF: Powered by multi-reward Group Relative Policy Optimization (GRPO), comprehensive evaluations on both internal and public benchmarks demonstrate that LongCat-Video achieves performance comparable to leading open-source video generation models as well as the latest commercial solutions.\nFor more detail, please refer to the comprehensive LongCat-Video Technical Report.\nQuick Start\nInstallation\nClone the repo:\ngit clone https://github.com/meituan-longcat/LongCat-Video\ncd LongCat-Video\nInstall dependencies:\n# create conda environment\nconda create -n longcat-video python=3.10\nconda activate longcat-video\n# install torch (configure according to your CUDA version)\npip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n# install flash-attn-2\npip install ninja\npip install psutil\npip install packaging\npip install flash_attn==2.7.4.post1\n# install other requirements\npip install -r requirements.txt\nFlashAttention-2 is enabled in the model config by default; you can also change the model config to use FlashAttention-3 or xformers.\nModel Download\nModels\nDownload Link\nLongCat-Video\nü§ó Huggingface\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video\nRun Text-to-Video\n# Single-GPU inference\ntorchrun run_demo_text_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_text_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\nRun Image-to-Video\n# Single-GPU inference\ntorchrun run_demo_image_to_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_image_to_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\nRun Video-Continuation\n# Single-GPU inference\ntorchrun run_demo_video_continuation.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_video_continuation.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\nRun Long-Video Generation\n# Single-GPU inference\ntorchrun run_demo_long_video.py --checkpoint_dir=./weights/LongCat-Video --enable_compile\n# Multi-GPU inference\ntorchrun --nproc_per_node=2 run_demo_long_video.py --context_parallel_size=2 --checkpoint_dir=./weights/LongCat-Video --enable_compile\nRun Streamlit\n# Single-GPU inference\nstreamlit run ./run_streamlit.py --server.fileWatcherType none --server.headless=false\nEvaluation Results\nText-to-Video\nThe Text-to-Video MOS evaluation results on our internal benchmark.\nMOS score\nVeo3\nPixVerse-V5\nWan 2.2-T2V-A14B\nLongCat-Video\nAccessibility\nProprietary\nProprietary\nOpen Source\nOpen Source\nArchitecture\n-\n-\nMoE\nDense\n# Total Params\n-\n-\n28B\n13.6B\n# Activated Params\n-\n-\n14B\n13.6B\nText-Alignment‚Üë\n3.99\n3.81\n3.70\n3.76\nVisual Quality‚Üë\n3.23\n3.13\n3.26\n3.25\nMotion Quality‚Üë\n3.86\n3.81\n3.78\n3.74\nOverall Quality‚Üë\n3.48\n3.36\n3.35\n3.38\nImage-to-Video\nThe Image-to-Video MOS evaluation results on our internal benchmark.\nMOS score\nSeedance 1.0\nHailuo-02\nWan 2.2-I2V-A14B\nLongCat-Video\nAccessibility\nProprietary\nProprietary\nOpen Source\nOpen Source\nArchitecture\n-\n-\nMoE\nDense\n# Total Params\n-\n-\n28B\n13.6B\n# Activated Params\n-\n-\n14B\n13.6B\nImage-Alignment‚Üë\n4.12\n4.18\n4.18\n4.04\nText-Alignment‚Üë\n3.70\n3.85\n3.33\n3.49\nVisual Quality‚Üë\n3.22\n3.18\n3.23\n3.27\nMotion Quality‚Üë\n3.77\n3.80\n3.79\n3.59\nOverall Quality‚Üë\n3.35\n3.27\n3.26\n3.17\nLicense Agreement\nThe model weights are released under the MIT License.\nAny contributions to this repository are licensed under the MIT License, unless otherwise stated. This license does not grant any rights to use Meituan trademarks or patents.\nSee the LICENSE file for the full license text.\nUsage Considerations\nThis model has not been specifically designed or comprehensively evaluated for every possible downstream application.\nDevelopers should take into account the known limitations of large language models, including performance variations across different languages, and carefully assess accuracy, safety, and fairness before deploying the model in sensitive or high-risk scenarios.\nIt is the responsibility of developers and downstream users to understand and comply with all applicable laws and regulations relevant to their use case, including but not limited to data protection, privacy, and content safety requirements.\nNothing in this Model Card should be interpreted as altering or restricting the terms of the MIT License under which the model is released.\nCitation\nWe kindly encourage citation of our work if you find it useful.\n@misc{meituan2025longcatvideotechnicalreport,\ntitle={LongCat-Video Technical Report},\nauthor={Meituan LongCat Team},\nyear={2025},\neprint={xxx},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/xxx},\n}\nAcknowledgements\nWe would like to thank the contributors to the Wan, UMT5-XXL, Diffusers and HuggingFace repositories, for their open research.\nContact\nPlease contact us at longcat-team@meituan.com or join our WeChat Group if you have any questions.",
    "krea/krea-realtime-video": "Video To Video\nText To Video\nUse it with our inference code\nUse it with üß® diffusers\nKrea Realtime 14B is distilled from the Wan 2.1 14B text-to-video model using Self-Forcing, a technique for converting regular video diffusion models into autoregressive models. It achieves a text-to-video inference speed of 11fps using 4 inference steps on a single NVIDIA B200 GPU. For more details on our training methodology and sampling innovations, refer to our technical blog post.\nInference code can be found here.\nYour browser does not support the video tag.\nOur model is over 10x larger than existing realtime video models\nWe introduce novel techniques for mitigating error accumulation, including KV Cache Recomputation and KV Cache Attention Bias\nWe develop memory optimizations specific to autoregressive video diffusion models that facilitate training large autoregressive models\nOur model enables realtime interactive capabilities: Users can modify prompts mid-generation, restyle videos on-the-fly, and see first frames within 1 second\nVideo To Video\nKrea realtime allows users to stream real videos, webcam inputs, or canvas primitives into the model, unlocking controllable video synthesis and editing\nYour browser does not support the video tag.\nYour browser does not support the video tag.\nYour browser does not support the video tag.\nYour browser does not support the video tag.\nText To Video\nKrea realtime allows users to generate videos in a streaming fashion with ~1s time to first frame.\nYour browser does not support the video tag.\nYour browser does not support the video tag.\nYour browser does not support the video tag.\nYour browser does not support the video tag.\nUse it with our inference code\nSet up\nsudo apt install ffmpeg # install if you haven't already\ngit clone https://github.com/krea-ai/realtime-video\ncd realtime-video\nuv sync\nuv pip install flash_attn --no-build-isolation\nhuggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir-use-symlinks False --local-dir wan_models/Wan2.1-T2V-1.3B\nhuggingface-cli download krea/krea-realtime-video krea-realtime-video-14b.safetensors --local-dir-use-symlinks False --local-dir checkpoints/krea-realtime-video-14b.safetensors\nRun\nexport MODEL_FOLDER=Wan-AI\nexport CUDA_VISIBLE_DEVICES=0 # pick the GPU you want to serve on\nexport DO_COMPILE=true\nuvicorn release_server:app --host 0.0.0.0 --port 8000\nAnd use the web app at http://localhost:8000/ in your browser\n(for more advanced use-cases and custom pipeline check out our GitHub repository: https://github.com/krea-ai/realtime-video)\nUse it with üß® diffusers\nKrea Realtime 14B can be used with the diffusers library utilizing the new Modular Diffusers structure (for now supporting text-to-video, video-to-video coming soon)\n# Install diffusers from main\npip install git+github.com/huggingface/diffusers.git\nimport torch\nfrom collections import deque\nfrom diffusers.utils import export_to_video\nfrom diffusers import ModularPipelineBlocks\nfrom diffusers.modular_pipelines import PipelineState, WanModularPipeline\nrepo_id = \"krea/krea-realtime-video\"\nblocks = ModularPipelineBlocks.from_pretrained(repo_id, trust_remote_code=True)\npipe = WanModularPipeline(blocks, repo_id)\npipe.load_components(\ntrust_remote_code=True,\ndevice_map=\"cuda\",\ntorch_dtype={\"default\": torch.bfloat16, \"vae\": torch.float16},\n)\nnum_frames_per_block = 3\nnum_blocks = 9\nframes = []\nstate = PipelineState()\nstate.set(\"frame_cache_context\", deque(maxlen=pipe.config.frame_cache_len))\nprompt = [\"a cat sitting on a boat\"]\nfor block in pipe.transformer.blocks:\nblock.self_attn.fuse_projections()\nfor block_idx in range(num_blocks):\nstate = pipe(\nstate,\nprompt=prompt,\nnum_inference_steps=6,\nnum_blocks=num_blocks,\nnum_frames_per_block=num_frames_per_block,\nblock_idx=block_idx,\ngenerator=torch.Generator(\"cuda\").manual_seed(42),\n)\nframes.extend(state.values[\"videos\"][0])\nexport_to_video(frames, \"output.mp4\", fps=16)",
    "Qwen/Qwen3-VL-2B-Instruct": "Qwen3-VL-2B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-2B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-2B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-2B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-2B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-2B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "datalab-to/chandra": "Chandra\nFeatures\nQuickstart\nBenchmarks\nExamples\nUsage\nInstallation\nFrom code\nWith transformers\nCredits\nChandra\nChandra is an OCR model that outputs markdown, HTML, and JSON.  It is highly accurate at extracting text from images and PDFs, while preserving layout information.\nYou can try Chandra in the free playground here, or at a hosted API here.\nFeatures\nConvert documents to markdown, html, or json with detailed layout information\nGood handwriting support\nReconstructs forms accurately, including checkboxes\nGood support for tables, math, and complex layouts\nExtracts images and diagrams, with captions and structured data\nSupport for 40+ languages\nQuickstart\nThe easiest way to start is with the CLI tools:\npip install chandra-ocr\n# With VLLM\nchandra_vllm\nchandra input.pdf ./output\n# With HuggingFace\nchandra input.pdf ./output --method hf\n# Interactive streamlit app\nchandra_app\nBenchmarks\nWe used the olmocr benchmark, which seems to be the most reliable current OCR benchmark in our testing.\nModel\nArXiv\nOld Scans Math\nTables\nOld Scans\nHeaders and Footers\nMulti column\nLong tiny text\nBase\nOverall\nSource\nDatalab Chandra v0.1.0\n82.2\n80.3\n88.0\n50.4\n90.8\n81.2\n92.3\n99.9\n83.1 ¬± 0.9\nOwn benchmarks\nDatalab Marker v1.10.0\n83.8\n69.7\n74.8\n32.3\n86.6\n79.4\n85.7\n99.6\n76.5 ¬± 1.0\nOwn benchmarks\nMistral OCR API\n77.2\n67.5\n60.6\n29.3\n93.6\n71.3\n77.1\n99.4\n72.0 ¬± 1.1\nolmocr repo\nDeepseek OCR\n75.2\n72.3\n79.7\n33.3\n96.1\n66.7\n80.1\n99.7\n75.4 ¬± 1.0\nOwn benchmarks\nGPT-4o (Anchored)\n53.5\n74.5\n70.0\n40.7\n93.8\n69.3\n60.6\n96.8\n69.9 ¬± 1.1\nolmocr repo\nGemini Flash 2 (Anchored)\n54.5\n56.1\n72.1\n34.2\n64.7\n61.5\n71.5\n95.6\n63.8 ¬± 1.2\nolmocr repo\nQwen 3 VL\n70.2\n75.1\n45.6\n37.5\n89.1\n62.1\n43.0\n94.3\n64.6 ¬± 1.1\nOwn benchmarks\nolmOCR v0.3.0\n78.6\n79.9\n72.9\n43.9\n95.1\n77.3\n81.2\n98.9\n78.5 ¬± 1.1\nolmocr repo\ndots.ocr\n82.1\n64.2\n88.3\n40.9\n94.1\n82.4\n81.2\n99.5\n79.1 ¬± 1.0\ndots.ocr repo\nExamples\nType\nName\nLink\nTables\nWater Damage Form\nView\nTables\n10K Filing\nView\nForms\nHandwritten Form\nView\nForms\nLease Agreement\nView\nHandwriting\nDoctor Note\nView\nHandwriting\nMath Homework\nView\nBooks\nGeography Textbook\nView\nBooks\nExercise Problems\nView\nMath\nAttention Diagram\nView\nMath\nWorksheet\nView\nMath\nEGA Page\nView\nNewspapers\nNew York Times\nView\nNewspapers\nLA Times\nView\nOther\nTranscript\nView\nOther\nFlowchart\nView\nUsage\nInstallation\npip install chandra-ocr\nFrom code\nfrom chandra.model import InferenceManager\nfrom chandra.model.schema import BatchInputItem\n# Run chandra_vllm to start a vLLM server first if you pass vllm, else pass hf\n# you can also start your own vllm server with the datalab-to/chandra model\nmanager = InferenceManager(method=\"vllm\")\nbatch = [\nBatchInputItem(\nimage=PIL_IMAGE,\nprompt_type=\"ocr_layout\"\n)\n]\nresult = manager.generate(batch)[0]\nprint(result.markdown)\nWith transformers\nfrom transformers import AutoModel, AutoProcessor\nfrom chandra.model.hf import generate_hf\nfrom chandra.model.schema import BatchInputItem\nfrom chandra.output import parse_markdown\nmodel = AutoModel.from_pretrained(\"datalab-to/chandra\").cuda()\nmodel.processor = AutoProcessor.from_pretrained(\"datalab-to/chandra\")\nbatch = [\nBatchInputItem(\nimage=PIL_IMAGE,\nprompt_type=\"ocr_layout\"\n)\n]\nresult = generate_hf(batch, model)[0]\nmarkdown = parse_markdown(result.raw)\nCredits\nThank you to the following open source projects:\nHuggingface Transformers\nVLLM\nolmocr\nQwen 3 VL",
    "allenai/olmOCR-2-7B-1025-FP8": "olmOCR-2-7B-1025-FP8\nolmOCR-Bench Scores\nUsage\nManual Prompting\nLicense and use\nolmOCR-2-7B-1025-FP8\nQuantized to FP8 Version of olmOCR-2-7B-1025, using llmcompressor.\nThis is a release of the olmOCR model that's fine tuned from Qwen2.5-VL-7B-Instruct using the\nolmOCR-mix-1025 dataset. It has been additionally\nfine tuned using GRPO RL training to boost its performance at math equations, tables, and other tricky OCR cases.\nQuick links:\nüìÉ Paper\nü§ó SFT Dataset\nü§ó RL Dataset\nüõ†Ô∏è Code\nüéÆ Demo\nThe best way to use this model is via the olmOCR toolkit.\nThe toolkit comes with an efficient inference setup via VLLM that can handle millions of documents\nat scale.\nolmOCR-Bench Scores\nThis model scores the following scores on olmOCR-bench when used with the\nolmOCR toolkit toolkit which automatically renders, rotates, and retries pages as needed.\nModel\nArXiv\nOld Scans Math\nTables\nOld Scans\nHeaders and Footers\nMulti column\nLong tiny text\nBase\nOverall\nolmOCR pipeline v0.4.0 with olmOCR-2-7B-1025\n82.9\n82.1\n84.3\n48.3\n95.7\n84.3\n81.4\n99.7\n82.3 ¬± 1.1\nolmOCR pipeline v0.4.0 with olmOCR-2-7B-1025-FP8\n83.0\n82.3\n84.9\n47.7\n96.1\n83.7\n81.9\n99.7\n82.4 ¬± 1.1\nUsage\nThis model expects as input a single document image, rendered such that the longest dimension is 1288 pixels.\nThe prompt must then contain the additional metadata from the document, and the easiest way to generate this\nis to use the methods provided by the olmOCR toolkit.\nManual Prompting\nIf you want to prompt this model manually instead of using the olmOCR toolkit, please see the code below.\nIn normal usage, the olmOCR toolkit builds the prompt by rendering the PDF page, and\nextracting relevant text blocks and image metadata. To duplicate that you will need to\npip install olmocr>=0.4.0\nand then run the following sample code.\nimport torch\nimport base64\nimport urllib.request\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\nfrom olmocr.data.renderpdf import render_pdf_to_base64png\nfrom olmocr.prompts import build_no_anchoring_v4_yaml_prompt\n# Initialize the model\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-2-7B-1025\", torch_dtype=torch.bfloat16).eval()\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Grab a sample PDF\nurllib.request.urlretrieve(\"https://olmocr.allenai.org/papers/olmocr.pdf\", \"./paper.pdf\")\n# Render page 1 to an image\nimage_base64 = render_pdf_to_base64png(\"./paper.pdf\", 1, target_longest_image_dim=1288)\n# Build the full prompt\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": build_no_anchoring_v4_yaml_prompt()},\n{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n],\n}\n]\n# Apply the chat template and processor\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmain_image = Image.open(BytesIO(base64.b64decode(image_base64)))\ninputs = processor(\ntext=[text],\nimages=[main_image],\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = {key: value.to(device) for (key, value) in inputs.items()}\n# Generate the output\noutput = model.generate(\n**inputs,\ntemperature=0.1,\nmax_new_tokens=50,\nnum_return_sequences=1,\ndo_sample=True,\n)\n# Decode the output\nprompt_length = inputs[\"input_ids\"].shape[1]\nnew_tokens = output[:, prompt_length:]\ntext_output = processor.tokenizer.batch_decode(\nnew_tokens, skip_special_tokens=True\n)\nprint(text_output)\n# ['---\\nprimary_language: en\\nis_rotation_valid: True\\nrotation_correction: 0\\nis_table: False\\nis_diagram: False\\n---\\nolmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models\\n\\nJake Poz']\nLicense and use\nThis model is licensed under Apache 2.0. It is intended for research and educational use in accordance with Ai2's Responsible Use Guidelines.",
    "Qwen/Qwen3-VL-8B-Instruct": "Qwen3-VL-8B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-8B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-8B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-8B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-8B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "lovis93/next-scene-qwen-image-lora-2509": "üé• next-scene-qwen-image-lora-2509\nüéâ ‚ú® UPDATE - Version 2 Now Available! (21 Oct 2025) ‚ú® üéâ\nV2 Demo Examples:\nüì¶ Version 1 (Legacy)\nV1 Demo Examples:\nüß† What This Model Does\nExamples of Cinematic Logic:\n‚öôÔ∏è Usage Instructions\nBasic Setup:\nExample Prompt:\nPro Tips:\nüé¨ Design Philosophy\nIdeal Applications:\n‚ö†Ô∏è Important Limitations\nüß± Technical Specifications\nüìÑ License\nüåê Creator\nüê¶ Share This Model\nüé• next-scene-qwen-image-lora-2509\nüéâ ‚ú® UPDATE - Version 2 Now Available! (21 Oct 2025) ‚ú® üéâ\nüöÄ New Model: next-scene_lora-v2-3000.safetensors\nWhat's New in V2:\nüéØ Trained on higher quality data for significantly improved results\nüí™ Better command responsiveness - the model follows your prompts more accurately\nüñºÔ∏è Fixed black bar artifacts - no more unwanted black borders on generated images\n‚ö° Overall enhanced performance - smoother transitions and better cinematic flow\nRecommended: Use V2 for all new projects.\nüì• ComfyUI Workflow: workflow-comfyui-basic-next-scene-v2.json\nV2 Demo Examples:\nnext-scene-qwen-image-lora-2509 is a LoRA adapter fine-tuned on Qwen-Image-Edit (build 2509), purpose-built to generate cinematic image sequences with natural visual progression from frame to frame.\nThis model enables Qwen Image Edit to think like a film director‚Äîunderstanding camera dynamics, visual composition, and narrative continuity to create shots that flow seamlessly into one another.\nüì¶ Version 1 (Legacy)\nModel File: next-scene_lora_v1-3000.safetensorsComfyUI Workflow: workflow-comfyui-basic-next-scene.json\nV1 Demo Examples:\nüß† What This Model Does\nThis LoRA brings cinematic storytelling continuity into AI image generation workflows.\nEach output frame functions as the \"Next Scene\" in an evolving visual narrative, maintaining compositional coherence while introducing organic transitions such as:\nCamera movement: Dolly shots, push-ins, pull-backs, and tracking moves\nFraming evolution: Wide to close-up transitions, angle shifts, reframing\nEnvironmental reveals: New characters entering frame, expanded scenery, spatial progression\nAtmospheric shifts: Lighting changes, weather evolution, time-of-day transitions\nExamples of Cinematic Logic:\n\"Next Scene: The camera pulls back from a tight close-up on the airship to a sweeping aerial view, revealing an entire fleet of vessels soaring through a fantasy landscape.\"\n\"Next Scene: The camera tracks forward and tilts down, bringing the sun and helicopters closer into frame as a strong lens flare intensifies.\"\n\"Next Scene: The camera pans right, removing the dragon and rider from view while revealing more of the floating mountain range in the distance.\"\n‚öôÔ∏è Usage Instructions\nBasic Setup:\nLoad Qwen-Image-Edit 2509 as your base model\nAdd a LoRA Loader node and select:\nV2 (Recommended): next-scene_lora-v2-3000.safetensors\nV1 (Legacy): next-scene_lora_v1-3000.safetensors\nSet LoRA strength: 0.7 ‚Äì 0.8 (recommended)\nStructure your prompts with \"Next Scene:\" prefix for optimal results\nExample Prompt:\nNext Scene: The camera moves slightly forward as sunlight breaks through the clouds, casting a soft glow around the character's silhouette in the mist. Realistic cinematic style, atmospheric depth.\nPro Tips:\nBegin prompts with camera direction for stronger continuity\nSpecify lighting and atmospheric changes for mood consistency\nChain multiple generations to create sequential storyboards\nWorks particularly well with landscape and establishing shots\nüé¨ Design Philosophy\nTrained on an extensive, curated cinematic dataset (proprietary), this model has learned to think directionally rather than just visually.\nIt doesn't simply modify an image‚Äîit advances the story, preserving spatial relationships, lighting consistency, and emotional resonance across sequential frames.\nIdeal Applications:\nStoryboard generation for film and animation pre-production\nCinematic AI video pipelines requiring frame-to-frame coherence\nSequential narrative workflows in ComfyUI and similar tools\nConcept art evolution showing scene progression\nVisual storytelling for creative projects and presentations\n‚ö†Ô∏è Important Limitations\nNot optimized for: Static portraits, single-image illustration tasks, or non-sequential edits\nBest suited for: Multi-frame workflows with narrative progression\nDesign priority: Storytelling flow and continuity over isolated image perfection\nRecommended use case: Scene-to-scene transitions rather than detailed object manipulation\nüß± Technical Specifications\nBase Model: Qwen-Image-Edit (build 2509)\nArchitecture: Low-Rank Adaptation (LoRA)\nTraining Objective: Scene continuity and cinematic shot coherence\nDataset: Large-scale proprietary cinematic imagery\nRecommended Strength: 0.7‚Äì0.8\nCompatible Platforms: ComfyUI, Automatic1111 (with Qwen support), custom pipelines\nüìÑ License\nMIT License ‚Äî Free for research, educational, and creative use.\nCommercial applications require independent testing and proper attribution. See LICENSE file for full terms.\nüåê Creator\nDeveloped by @lovis93\nPushing the boundaries of AI-directed visual storytelling and cinematic image generation.\nüê¶ Share This Model\nüé• Introducing next-scene-qwen-image-lora-2509\nA LoRA fine-tuned for Qwen-Image-Edit 2509 that thinks like a film director.\nIt evolves each frame naturally‚Äînew angles, new lighting, same coherent world.\nPerfect for cinematic storyboards, sequential edits, and \"Next Scene\" workflows.\nüëâ https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509\n#AIart #ComfyUI #Qwen #LoRA #GenerativeAI #AIcinema #ImageEditing",
    "Qwen/Qwen3-VL-32B-Instruct": "Qwen3-VL-32B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-32B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-32B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-32B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-32B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-32B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "Phr00t/Qwen-Image-Edit-Rapid-AIO": "Merge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support.\nUse a \"Load Checkpoint\" node. 1 CFG, 4 step. Use the \"TextEncodeQwenImageEditPlus\" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision.\nBoth NSFW and SFW models are available! v4 and older combine both NSFW and SFW uses in one model, but performance is subpar. v5+ separates out a NSFW and SFW version, so please pick which model for your use case.\nHaving problems with scaling, cropping or zooming? Scaling images in the TextEncoderQwenEditPlus node is the problem. There are many workarounds, but I prefer just fixing the node and I've supplied my version in the Files area. It also supports up to 4 input images. Just set the \"target_size\" to a little less than your output's largest size (like 896 if making a 1024x1024 image). I find this improves quality over skipping scaling entirely, as input images better match output resolutions.\nV1: Uses Qwen-Image-Edit-2509 & 4-step Lightning v2.0. Includes a touch of NSFW LORAs, so it should be a very versatile model for both SFW and NSFW use. sa_solver/beta recommended, but euler_a/beta and er_sde/beta can give decent results too.\nV2: Now uses a mix of Qwen-Image-Edit accelerators, mixing both 8 and 4 steps in one. Also significantly tweaked the NSFW LORAs for better all-around SFW and NSFW use. sa_solver/simple strongly recommended.\nV3: Uses new Qwen-Image-Edit lightning LORAs for much better results. Also significantly adjusted NSFW LORA mix, removing poor ones and increasing quality ones. sa_solver/beta highly recommended.\nV4: Mix of many Qwen Edit and base Qwen accelerators, which I think gives better results. Added a touch of a skin correction LORA. 4-5 steps: use sa_solver/simple, lcm/beta or euler_a/beta and 6-8 steps: use lcm/beta or euler_a/beta only.\nV5: NSFW and SFW use cases interfered with eachother too much, so I separated them to specialize in their use cases. Updated \"snofs\" and \"qwen4play\" NSFW LORAs + Meta4 for v5.2, then added \"Qwen Image NSFW Adv.\" by fok3827 for v5.3. SFW: lcm/beta or er_sde/beta generally recommended and NSFW: lcm/normal recommended. Prompting \"Professional digital photography\" helps reduce the plastic look.",
    "lightonai/LightOnOCR-1B-1025": "LightOnOCR-1B-1025\nModel Overview\nBenchmarks\nInstallation\nStart Server\nPDF Inference\nRendering and Preprocessing Tips\nVariants\nFine-tuning\nData\nLicense\nCitation\nLightOnOCR-1B-1025\nFull BF16 version of the model. We recommend this variant for further fine-tuning or research use.\nLightOnOCR-1B is a compact, end-to-end vision‚Äìlanguage model for Optical Character Recognition (OCR) and document understanding. It achieves state-of-the-art accuracy in its weight class while being several times faster and cheaper than larger general-purpose VLMs.\nüìù Read the full blog post | üöÄ Try the demo\nHighlights\n‚ö° Speed: 5√ó faster than dots.ocr, 2√ó faster than PaddleOCR-VL-0.9B, 1.73√ó faster than DeepSeekOCR\nüí∏ Efficiency: Processes 5.71 pages/s on a single H100 (~493k pages/day) for <$0.01 per 1,000 pages\nüß† End-to-End: Fully differentiable, no external OCR pipeline\nüßæ Versatile: Handles tables, receipts, forms, multi-column layouts, and math notation\nüåç Compact variants: 32k and 16k vocab options for European languages\nModel Overview\nLightOnOCR combines a Vision Transformer encoder(Pixtral-based) with a lightweight text decoder(Qwen3-based) distilled from high-quality open VLMs.\nIt is optimized for document parsing tasks, producing accurate, layout-aware text extraction from high-resolution pages.\nBenchmarks\nModel\nArXiv\nOld Scans\nMath\nTables\nMulti-Column\nTiny Text\nBase\nOverall\nLightOnOCR-1B-1025 (151k vocab)\n81.4\n71.6\n76.4\n35.2\n80.0\n88.7\n99.5\n76.1\nLightOnOCR-1B-32k (32k vocab)\n80.6\n66.2\n73.5\n33.5\n71.2\n87.6\n99.5\n73.1\nLightOnOCR-1B-16k (16k vocab)\n82.3\n72.9\n75.3\n33.5\n78.6\n85.1\n99.8\n75.4\nAll benchmarks evaluated using vLLM on the Olmo-Bench.\nInstallation\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\nuv pip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly \\\n--prerelease=allow\n# if this fails try adding triton-kernels package\n'triton-kernels @ git+https://github.com/triton-lang/triton.git@v3.5.0#subdirectory=python/triton_kernels'\nuv pip install pypdfium2 pillow requests\nStart Server\nvllm serve lightonai/LightOnOCR-1B-1025 \\\n--limit-mm-per-prompt '{\"image\": 1}' \\\n--async-scheduling\nPDF Inference\nimport base64\nimport requests\nimport pypdfium2 as pdfium\nimport io\nENDPOINT = \"http://localhost:8000/v1/chat/completions\"\nMODEL = \"lightonai/LightOnOCR-1B-1025\"\n# Download PDF from arXiv\npdf_url = \"https://arxiv.org/pdf/2412.13663\"\npdf_data = requests.get(pdf_url).content\n# Open PDF and convert first page to image\npdf = pdfium.PdfDocument(pdf_data)\npage = pdf[0]\n# Render at 200 DPI (scale factor = 200/72 ‚âà 2.77)\npil_image = page.render(scale=2.77).to_pil()\n# Convert to base64\nbuffer = io.BytesIO()\npil_image.save(buffer, format=\"PNG\")\nimage_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n# Make request\npayload = {\n\"model\": MODEL,\n\"messages\": [{\n\"role\": \"user\",\n\"content\": [{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}\n}]\n}],\n\"max_tokens\": 4096,\n\"temperature\": 0.2,\n\"top_p\": 0.9,\n}\nresponse = requests.post(ENDPOINT, json=payload)\ntext = response.json()['choices'][0]['message']['content']\nprint(text)\nRendering and Preprocessing Tips\nRender PDFs to PNG or JPEG at a target longest dimension of 1540px\nMaintain aspect ratio to preserve text geometry\nUse one image per page; batching supported by vLLM\nVariants\nVariant\nDescription\nLightOnOCR-1B-1025\nFull multilingual model (default)\nLightOnOCR-1B-32k\nFastest pruned-vocabulary version (32k tokens) optimized for European languages\nLightOnOCR-1B-16k\nMost compact variant with smallest vocabulary\nFine-tuning\nTransformers integration is coming soon for training and inference.\nLightOnOCR is fully differentiable and supports:\nLoRA fine-tuning\nDomain adaptation (receipts, scientific articles, forms, etc.)\nMultilingual fine-tuning with task-specific corpora\nExample fine-tuning configurations will be released alongside the dataset.\nData\nTrained on a diverse large-scale PDF corpus covering:\nScientific papers, books, receipts, invoices, tables, forms, and handwritten text\nMultiple languages (Latin alphabet dominant)\nReal and synthetic document scans\nThe dataset will be released under an open license.\nLicense\nApache License 2.0\nCitation\n@misc{lightonocr2025,\ntitle        = {LightOnOCR-1B: End-to-End and Efficient Domain-Specific Vision-Language Models for OCR},\nauthor       = {Said Taghadouini and Baptiste Aubertin and Adrien Cavaill√®s},\nyear         = {2025},\nhowpublished = {\\url{https://huggingface.co/blog/lightonai/lightonocr}}\n}",
    "PokeeAI/pokee_research_7b": "Model Card for PokeeResearch\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nCitation\nGlossary\nMore Information\nModel Card Authors\nModel Card Contact\nModel Card for PokeeResearch\nModel Details\nModel Description\nPokeeResearch-7B is a 7-billion-parameter deep research agent developed by Pokee AI to advance reliable, aligned, and scalable research-grade reasoning in tool-augmented LLMs.The model integrates Reinforcement Learning from AI Feedback (RLAIF) with a robust reasoning scaffold, enabling it to conduct complex, multi-step research workflows that include self-correction, verification, and synthesis across multiple independent research threads.\nDeveloped by: Pokee AI\nModel type: Tool-augmented large language model (LLM) research agent\nLanguage(s): English, Chinese and many more\nLicense: Apache 2.0\nFinetuned from model: Qwen2.5-7B-Instruct\nModel Sources\nRepository: https://github.com/Pokee-AI/PokeeResearchOSS\nPaper: PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold, Pokee AI, October 2025\nProject Page: https://pokee.ai/deepresearch-preview\nUses\nDirect Use\nPokeeResearch-7B is designed for deep research automation, where the model autonomously:\nDecomposes complex user queries\nRetrieves and reads from external sources\nSynthesizes factual, verifiable, and grounded answers\nIt can be used as a standalone research assistant or integrated into multi-agent systems to support academic, enterprise, or product-level research tasks.\nDownstream Use\nPokeeResearch-7B can be fine-tuned or extended for:\nDomain-specific scientific discovery\nAutonomous document retrieval and synthesis\nMulti-source verification and summarization pipelines\nIntegration into reinforcement learning research agents (RLHF/RLAIF frameworks)\nOut-of-Scope Use\nThe model should not be used for:\nGenerating unverified or speculative claims\nAutomated decision-making in high-stakes domains (medical, legal, or financial)\nApplications requiring strict factual precision without external verification\nGenerating content without citation or evidence tracing\nBias, Risks, and Limitations\nPokeeResearch-7B is optimized for factual grounding and robustness, but limitations include:\nDependence on external data quality and retrieval accuracy\nPotential semantic bias introduced by AI-based feedback signals\nLimited coverage for non-English or multi-modal reasoning tasks\nRisk of hallucinated synthesis when sources conflict or lack clarity\nRecommendations\nUsers should:\nCross-verify answers, especially in multi-hop reasoning cases\nMonitor output for citation accuracy and alignment with source data\nRefrain from using outputs as sole evidence in decision-critical contexts\nHow to Get Started with the Model\nplease refer to the following codebase for how to use PokeeResearch-7B\nhttps://github.com/Pokee-AI/PokeeResearchOSS/blob/main/README.md\nTraining Details\nTraining Data\nDataset: MiroRL-GenQA dataset (MiroMind AI, 2025)\nData characteristics: Complex, multi-turn question‚Äìanswer pairs requiring multi-step reasoning\nData filtering: No benchmark data used for testing; the model was trained only on open-domain text Q&A samples\nTraining Procedure\nPreprocessing\nNormalization and tokenization aligned with Qwen2.5 tokenizer\nStructured prompt‚Äìresponse pairs in research/verification format (<tool_call>, <answer>, <verification>)\nTraining Hyperparameters\nAlgorithm: RLOO (REINFORCE Leave-One-Out)\nBatch size: 64\nResearch threads per prompt: 8\nLearning rate: 3e-6\nContext limit: 32,768 tokens\nSteps: 140 fine-tuning iterations\nRegularization: None (no entropy or KL regularization)\nPrecision regime: bf16 mixed precision\nReward Design\nCombined reward signal from:\nAI feedback (semantic equivalence via external LLM judge)\nFormat adherence reward (ensures correct agent behavior)\nSpeeds, Sizes, Times\nModel size: 7 billion parameters\nTraining duration: ~5 days on 8 √ó A100 80G GPUs\nCheckpoint size: ~13 GB\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n10 open-domain research and QA benchmarks:\nNQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle, GAIA, BrowseComp, Humanity‚Äôs Last Exam\nFactors\nBenchmarks differ by reasoning depth, retrieval dependence, and factual precision requirements.\nEvaluations disaggregate by dataset difficulty and task type (single-hop vs multi-hop).\nMetrics\nMean accuracy (mean@4 across independent research threads) based on\nResults\nPokeeResearch-7B (RTS variant) and PokeeResearch-7B outperforms all baselines at 7B scale across 10 benchmarks.Highlights (mean@4 accuracy):\nMethod\nHLE\nGAIA\nBrowseComp\nBAMB\n2WIKI\nTQ\nNQ\nPOPQA\nMUSIQUE\nHOTPOTQA\nR1searcher\n5.4\n8.3\n1.0\n63.2\n61.4\n77.2\n59.6\n51.8\n35.8\n62.4\nSearchR1\n13.0\n18.7\n0.4\n67.8\n62.8\n81.0\n67.6\n59.6\n33.2\n63.2\nZeroSearch\n8.6\n9.9\n1.4\n51.4\n33.6\n61.6\n48.2\n38.0\n19.0\n32.4\nASearcher\n13.8\n22.1\n3.2\n68.8\n69.2\n85.2\n71.2\n58.2\n35.8\n71.0\nDeepResearcher\n6.0\n24.03\n1.8\n71.0\n58.8\n82.2\n60.2\n55.2\n26.8\n56.6\nPR\n15.2\n36.9\n5.4\n74.5\n74.0\n91.3\n75.1\n59.8\n39.8\n71.2\nPR+\n17.6\n41.3\n8.4\n75.0\n75.0\n91.8\n75.0\n60.0\n41.4\n71.6\nSummary\nPokeeResearch-7B variants achieves state-of-the-art performance among 7B-scale open deep research agents, validating RLAIF and reasoning scaffold design for robust, verifiable research workflows.\nTechnical Specifications\nModel Architecture and Objective\nBase Architecture: Transformer decoder (Qwen2.5-7B-Instruct backbone)\nObjective: Reinforcement learning with AI feedback to maximize semantic correctness and alignment with human-style reasoning\nCompute Infrastructure\nHardware\nNVIDIA A100 80GB GPUs √ó8 for training and x1 for inference\nCitation\nBibTeX:\n@article{pokee2025deepresearch,\ntitle={PokeeResearch: Effective Deep Research via\nReinforcement Learning from AI Feedback and Robust Reasoning Scaffold},\nauthor={Yi Wan* and Jiuqi Wang* and Liam Li\nand Jinsong Liu and Ruihao Zhu and Zheqing Zhu},\njournal={Pokee AI Technical Report},\nyear={2025},\nurl={https://arxiv.org/pdf/2510.15862}\n}\nAPA:\nWan, Y., Wang, J., Li, L., Liu, J., Zhu, R., & Zhu, Z. (2025). PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold. Pokee AI.\nGlossary\nRLAIF: Reinforcement Learning from AI Feedback ‚Äì optimization using LLM-based reward signals.\nRLOO: REINFORCE Leave-One-Out ‚Äì unbiased policy gradient variant for on-policy learning.\nRTS: Research Threads Synthesis ‚Äì synthesis of multiple independent reasoning threads at inference time.\nMore Information\nFor technical details, visit: https://github.com/Pokee-AI/PokeeResearchOSSFor inquiries, contact: hello@pokee.ai\nModel Card Authors\nYi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, and Zheqing Zhu ‚Äî Pokee AI Research Team\nModel Card Contact\nPokee AI Team ‚Äî hello@pokee.ai",
    "nvidia/omnivinci": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM\nIntroduction\nQuickstart\nEnvironment Setup\nü§ó Transformers Usage\nLicense / Terms of Use\nCitation\nOmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM\nIntroduction\nOmniVinci is an NVIDIA research project focused on exploring omni-modal LLMs that can not only see and read but also listen, speak, and reason.\nWe are among the best omni-modality understanding models. Check out our performance on some of the most popular omni-modality, audio, and vision benchmarks:\nQuickstart\nBelow, we provide simple examples to show how to use our model with Transformers.\nEnvironment Setup\nDownload and navigate to the HuggingFace repository:\nhuggingface-cli download nvidia/omnivinci --local-dir ./omnivinci --local-dir-use-symlinks False\ncd ./omnivinci\nInstall Python environment (based on NVILA codebase):\nbash ./environment_setup.sh omnivinci\nü§ó Transformers Usage\nVideo (with Audio) Inference Example\nfrom transformers import AutoProcessor, AutoModel, AutoConfig,AutoModelForCausalLM\nimport torch\nimport os\n# default: Load the model on the available device(s)\nmodel_path = \"./\"\nvideo_path = \"xxx.mp4\"\ngeneration_kwargs = {\"max_new_tokens\": 1024, \"max_length\": 99999999}\nload_audio_in_video = True\nnum_video_frames = 128\naudio_length = \"max_3600\"\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_path,\ntrust_remote_code=True,\ntorch_dtype=\"torch.float16\",\ndevice_map=\"auto\")\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\ngeneration_config = model.default_generation_config\ngeneration_config.update(**generation_kwargs)\nmodel.config.load_audio_in_video = load_audio_in_video\nprocessor.config.load_audio_in_video = load_audio_in_video\nif num_video_frames > 0:\nmodel.config.num_video_frames = num_video_frames\nprocessor.config.num_video_frames = num_video_frames\nif audio_length != -1:\nmodel.config.audio_chunk_length = audio_length\nprocessor.config.audio_chunk_length = audio_length\nconversation = [{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"video\":video_path},\n{\"type\": \"text\", \"text\": \"Assess the video, followed by a detailed description of its video and audio contents.\"}\n]\n}]\ntext = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\ninputs = processor([text])\noutput_ids = model.generate(\ninput_ids=inputs.input_ids,\nmedia=getattr(inputs, 'media', None),\nmedia_config=getattr(inputs, 'media_config', None),\ngeneration_config=generation_config,\n)\nprint(processor.tokenizer.batch_decode(output_ids, skip_special_tokens=True))\nFor audio and image inference examples, please refer to example_mini_audio.py and example_mini_image.py.\nLicense / Terms of Use\nThe model is released under the NVIDIA OneWay Noncommercial License.\nCitation\nPlease consider to cite our paper and this framework, if they are helpful in your research.\n@article{omnivinci2025,\ntitle={OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM},\nauthor={Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov},\njournal={arXiv},\nyear={2025},\n}",
    "zai-org/GLM-4.6": "GLM-4.6\nModel Introduction\nInference\nRecommended Evaluation Parameters\nEvaluation\nGLM-4.6\nüëã Join our Discord community.\nüìñ Check out the GLM-4.6 technical blog, technical report(GLM-4.5), and Zhipu AI technical documentation.\nüìç Use GLM-4.6 API services on Z.ai API Platform.\nüëâ One click to GLM-4.6.\nModel Introduction\nCompared with GLM-4.5, GLM-4.6  brings several key improvements:\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code„ÄÅCline„ÄÅRoo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4.\nInference\nBoth GLM-4.5 and GLM-4.6 use the same inference method.\nyou can check our github for more detail.\nRecommended Evaluation Parameters\nFor general evaluations, we recommend using a sampling temperature of 1.0.\nFor code-related evaluation tasks (such as LCB), it is further recommended to set:\ntop_p = 0.95\ntop_k = 40\nEvaluation\nFor tool-integrated reasoning, please refer to this doc.\nFor search benchmark, we design a specific format for searching toolcall in thinking mode to support search agent, please refer to this. for the detailed template.",
    "valiantcat/Qwen-Image-Edit-MeiTu": "üåà Qwen-Image-Edit-MeiTu\n‚ú® Key Improvements\nüñºÔ∏è Showcase\nüí¨ Recommended Prompts\nüß© Integration with ComfyUI\nüì• Download Model\nüß† Training\nüìú License\nüíº Join Us\nüåà Qwen-Image-Edit-MeiTu\nThis model ‚Äî Qwen-Image-Edit-MeiTu ‚Äî is an improved variant of Qwen/Qwen-Image-Edit, built with DiT-based architecture fine-tuning to enhance visual consistency, aesthetic quality, and structural alignment in complex edits.\nDeveloped by Valiant Cat AI Lab, this version aims to further close the gap between high-fidelity semantic editing and coherent artistic rendering, achieving a more natural and professional output across a wide range of prompts and subjects.\n‚ú® Key Improvements\nEnhanced Consistency:Utilizes DiT (Diffusion Transformer) fine-tuning to ensure structural stability between input and edited regions, maintaining global spatial coherence.\nAesthetic Optimization:Trained with aesthetic discriminators and curated aesthetic score datasets, producing more pleasing colors, contrast, and light balance.\nBetter Detail Preservation:Improved low-level reconstruction for fine details such as textures, faces, and typography.\nBroader Scene Adaptability:Performs well on portraits, environments, product photos, and illustrations, supporting both semantic and appearance-based editing.\nüñºÔ∏è Showcase\nBelow are examples of consistency and aesthetic improvement in complex editing scenarios:\nInput & Output\nüí¨ Recommended Prompts\nTry these prompts to explore the model‚Äôs strengths:\n‚Äúmake the lighting soft and cinematic with better balance‚Äù\n‚Äúenhance the photo‚Äôs composition and maintain realism‚Äù\n‚Äúrefine skin tone and texture consistency‚Äù\n‚Äúimprove the global color tone and aesthetic harmony‚Äù\n‚Äúincrease photo realism and clarity without changing content‚Äù\nüß© Integration with ComfyUI\nThis model works seamlessly with a modified ComfyUI Qwen-Image-Edit workflow.Just use this model in the Unet node to workflow for edit image.\nüì• Download Model\nWeights available in Safetensors format:\nüëâ Download Qwen-Image-Edit-MeiTu\nüß† Training\nThis model was trained and optimized by theAI Laboratory of Chongqing Valiant Cat Technology Co., LTD.Visit https://vvicat.com/ for business collaborations or research partnerships.\nüìú License\nLicensed under Apache 2.0.\nüíº Join Us\nWe are hiring research engineers and creative ML practitioners atChongqing Valiant Cat Technology Co., LTD ‚Äî reach out viaüìß tommy@vvicat.com",
    "Phr00t/WAN2.2-14B-Rapid-AllInOne": "These are mixtures of WAN 2.2 and other WAN-like models and accelerators (with CLIP and VAE also included) to provide a fast, \"all in one\" solution for making videos as easily and quickly as possible. FP8 precision. Generally the latest version available for each type of model (image to video or text to video) is recommended.\nMEGA Merge: This is the \"one model to rule them all\" version which pretty much does everything. It can handle text to video, image to video, and first frame to last frame and last frame only (because it includes VACE). There is a specific workflow to use these merges included in the mega-v3/ folder, as it is slightly more complicated (but shouldn't be slower) due to its flexibility. See below for a screenshot of \"mega\" being used.\nNSFW Merges: Degenerates should steer clear of these merges, as they are only for the most civilized people of culture or scientific researchers. These merge various spicy WAN 2.1+2.2 LORAs at generally low strengths to provide a \"jack of all trades, master of none\" all in one despicable solution. If you are not getting the results you want, add more LORAs or just use the non-NSFW versions with hand-picked LORAs.\nYou just need to use the basic ComfyUI \"Load Checkpoint\" node with these, as you can take the VAE, CLIP and Model all from one AIO safetensors (saved in your 'checkpoints' folder). All models are intended to use 1 CFG and 4 steps. See sampler recommendations for each version below.\nWAN 2.1 LORA compatibility is generally still good, along with \"low noise\" WAN 2.2 LORA compatibility (do not use \"high noise\" LORAs). You might need to adjust LORA strengths (up or down) to get results you want, though.\nMEGA version workflow screenshot (you can use VideoCombine instead of Preview Image):\nMEGA I2V: Just bypass the \"end frame\" so the \"start frame\" will be your I2V starting frame. Keep everything else the same.\nMEGA T2V: Bypass \"end frame\", \"start frame\" and the \"VACEFirstToLastFrame\" node. Set strength to 0 for WanVaceToVideo.\nMEGA Last Frame: Just bypass the \"start frame\" and keep \"end frame\". Keep everything else the same as in the picture.\nMEGA First->Last Frame: Use it like shown in the picture above.\nOlder non-MEGA workflows (v10 and below):\nSeems to work even on 8GB VRAM:\nCHANGELOG/VERSIONS:\nbase: This is the first attempt and very \"stable\", but mostly WAN 2.1 with few WAN 2.2 features. sa_solver recommended.\nV2: This is a more dynamic mixture with more WAN 2.2 features. sa_solver OR euler_a sampler recommended. Suffers from minor color shifts and noise in I2V, typically just at the start.\nV3: This is a mixture of SkyReels and WAN 2.2, which should improve prompt adherence and quality. euler_a sampler recommended, beta scheduler. Suffers from minor color shifts and noise in I2V, typically just at the start.\nV4: WAN 2.2 Lightning in the mix! euler_a/beta recommended. I2V noise and color shifting generally improved, but motion is a bit overexaggerated.\nV5: Improved overexaggeration of I2V model. euler_a/beta recommended.\nV6: New merging structure and overall significantly improved quality. I2V noise for the first 1-2 frames still exists, but it clears up much better than previous versions. Some WAN 2.1 LORAs at heavy strengths may cause up to 5 poor early frames with T2V, where discarding (or lowering strengths) may help. sa_solver/beta recommended. I2V rarely suffers from some dramatic scene shifts.\nV7: I2V scene shifting should be fixed, but some I2V noise persists (generally for just the first 1-2 frames). No changes needed for the T2V model, so that remains at V6. sa_solver/beta recommended.\nV8: T2V is now based entirely off of WAN 2.2 \"low\" (with PUSA, SkyReels and Lightning accelerators mixed in), which should resolve noise problems with it (8.1 adds more SkyReels). I2V scaled back some of the WAN 2.2 mix, which was contributing to noise problems. There still is some minor I2V noise, but more of a delicate balance of WAN 2.2 + SkyReels to keep decent motion and flexibility. Euler_a/beta recommended.\nV9: Removed PUSA and SkyReels from the WAN 2.2-side of I2V (and completely from T2V). as I think PUSA/SkyReels wasn't consistently helping (and sometimes hurting) when applied to WAN 2.2. This should provide a more reliable base to work from. euler_a/beta recommended, but feel free to experiment with sa_solver/beta or others!\nV10: Fixes wrong accelerators being used (now WAN 2.2 Lightning in I2V and an an adaptive rank Lightx2v along with WAN 2.2 lightning in T2V). I2V now has a tendency to zoom into whatever is going on in your prompt, which I believe comes from increased camera movement from Wan 2.2 Lightning and being less tied to your initial image as the video progresses (so, prompt accordingly). Euler_a/beta still seems good.\nMEGA v1: This is likely how I will continue making models, as I don't need separate I2V and T2V versions. No noise problems with I2V anymore! MEGA v1 is based off of WAN 2.2 \"low T2V\", then adds VACE Fun, SkyReels, FunReward and the usual accelerator/CLIP/VAE mix. Use the included workflow. ipndm/sgm_uniform sampler/scheduler recommended.\nMEGA v2: Removed the FunReward LORA, which was causing faces to shift. I did notice some minor face shifting in the NSFW merge remaining, which I think is due to the LORA mixture, but it has been improved. Also reduced some of the SkyReels LORA a bit. ipndm/beta recommended.\nMEGA v3: Very different merging method using SkyReels 2.1 33% base and WAN 2.2 66% on top. I now also match accelerators for each version (2.1 and 2.2), then merge. I think this gets a better result by basing \"mega\" on models designed for 1 sampler (2.1) but then bringing in most of WAN 2.2 to lay on top. Camera control and prompt following is better, but keeping facial features still struggles compared to v10 I2V (might be a VACE limitation). ipndm/beta recommended. euler_a/beta seems to work better with the NSFW v3.1 merge, though.\nMEGA v4: Uses the WAN 2.2 finetune from https://huggingface.co/eddy1111111/WAN22.XX_Palingenesis (also slight tweaks to accelerator strengths)\nMEGA v5: New merging method with very experimental accelerator mix! I include small amounts of many I2V and T2V accelerators on top of WAN22.XX_Palingenesis and SkyReels 720p, plus VACE. The goal is to improve I2V consistency without hurting T2V. I think quality, detail and consistency has improved, but I do wish camera control was better. euler_a/beta recommended.\nMEGA v6: Adjusted accelerators, bringing in more of the older Lightx2v as relying too much on the newest WAN 2.2 Lightning was hurting motion. I'm seeing better camera movement and prompt adherence in my testing than v5. NSFW v6.1 version has newer LORAs included and tweaked parameters. sa_solver/beta recommended.\nMEGA v7: Now uses 3 different accelerators mixed together: lightx2v, WAN 2.2 Lightning (250928) and rCM. Motion seems to be improved further. euler_a/beta seems to work pretty good.\nMEGA v8: Updated rCM 720p accelerator, which is now the biggest accelerator in the mix, reducing lightx2v and WAN 2.2 Lightning. Updated NSFW LORAs a bit. euler_a/beta still recommended.\nMEGA v9: Removed SkyReels 2.1 720p completely. This is now based completely on WAN22.XX_Palingenesis T2V + VACE, using mostly rCM 720p for acceleration. Updated MysticXXX v2 for the NSFW merge among other tweaks. Motion should be better, hopefully. euler_a/beta recommended.\nMEGA v10: Packed the models a bit differently, tweaked acclerators and NSFW LORAs some more. I tried to test this version a bit more and was getting better results. euler_a/beta recommended.\nMEGA v11: Mostly the same as v10, but pulled in the latest WAN 2.1 distill from lightx2v. euler_a/beta recommended.\nLooking for GGUFs? Check the sidebar for quants.\nLooking for FP16 precision? TekeshiX has been helping me build variants in FP16 format (but they are kinda outdated):\nhttps://huggingface.co/TekeshiX/RAPID-AIO-FP16/tree/main\nDISCLAIMER: As you may expect, some compromises had to be made to reach this level of speed and simplicity. If you want more complex workflows and longer generation times to run \"full WAN 2.2\"'s pair of models (which will give higher quality results), or control over accelerator LORAs included in this merge, there are many resources elsewhere to do that.",
    "Qwen/Qwen3-VL-2B-Thinking": "Qwen3-VL-2B-Thinking\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-2B-Thinking\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-2B-Thinking.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show you how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-2B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-2B-Thinking\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-2B-Thinking\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport temperature=1.0\nexport out_seq_length=40960\nText\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport temperature=1.0\nexport out_seq_length=32768 (for aime, lcb, and gpqa, it is recommended to set to 81920)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "LiquidAI/LFM2-VL-3B": "LFM2‚ÄëVL\nüìÑ Model details\nüèÉ How to run LFM2-VL\nüîß How to fine-tune\nüìà Performance\nüì¨ Contact\nLFM2‚ÄëVL\nLFM2-VL-3B is the newest and most capable model in Liquid AI's multimodal LFM2-VL series, designed to process text and images with variable resolutions.Built on the LFM2 backbone, it extends the architecture for higher-capacity reasoning and stronger visual understanding while retaining efficiency.\nWe are releasing the weights of the new 3B checkpoint‚Äîoffering higher performance across benchmarks while remaining optimized for scalable deployment.\nCompetitive multimodal performance among lightweight open models.\nEnhanced visual understanding and reasoning, particularly on fine-grained perception tasks\nRetains efficient inference with the same flexible architecture and user-tunable speed-quality tradeoffs\nProcesses native resolutions up to 512√ó512 with intelligent patch-based handling for larger inputs\nFor more details, see the LFM2-VL-3B post and the LFM2 blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2-VL models on narrow use cases to maximize performance.\nThey were trained for instruction following and lightweight agentic flows.\nNot intended for safety‚Äëcritical decisions.\nProperty\nLFM2-VL-450M\nLFM2-VL-1.6B\nLFM2-VL-3B\nParameters (LM only)\n350M\n1.2B\n2.6B\nVision encoder\nSigLIP2 NaFlex base (86M)\nSigLIP2 NaFlex shape-optimized (400M)\nSigLIP2 NaFlex large (400M)\nBackbone layers\nhybrid conv+attention\nhybrid conv+attention\nhybrid conv+attention\nContext (text)\n32,768 tokens\n32,768 tokens\n32,768 tokens\nImage tokens\ndynamic, user-tunable\ndynamic, user-tunable\ndynamic, user-tunable\nVocab size\n65,536\n65,536\n65,536\nPrecision\nbfloat16\nbfloat16\nbfloat16\nLicense\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nSupported languages: English\nGeneration parameters: We recommend the following parameters:\nText: temperature=0.1, min_p=0.15, repetition_penalty=1.05\nVision: min_image_tokens=64 max_image_tokens=256, do_image_splitting=True\nChat template: LFM2-VL uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful multimodal assistant by Liquid AI.<|im_end|>\n<|im_start|>user\n<image>Describe this image.<|im_end|>\n<|im_start|>assistant\nThis image shows a Caenorhabditis elegans (C. elegans) nematode.<|im_end|>\nImages are referenced with a sentinel (<image>), which is automatically replaced with the image tokens by the processor.\nYou can apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nArchitecture\nHybrid backbone: Language model tower (LFM2-2.6B) paired with SigLIP2 NaFlex vision encoders (400M shape-optimized)\nNative resolution processing: Handles images up to 512√ó512 pixels without upscaling and preserves non-standard aspect ratios without distortion\nTiling strategy: Splits large images into non-overlapping 512√ó512 patches and includes thumbnail encoding for global context\nEfficient token mapping: 2-layer MLP connector with pixel unshuffle reduces image tokens (e.g., 256√ó384 image ‚Üí 96 tokens, 1000√ó3000 ‚Üí 1,020 tokens)\nInference-time flexibility: User-tunable maximum image tokens and patch count for speed/quality tradeoff without retraining\nTraining approach\nBuilds on the LFM2 base model with joint mid-training that fuses vision and language capabilities using a gradually adjusted text-to-image ratio\nApplies joint SFT with emphasis on image understanding and vision tasks\nLeverages large-scale open-source datasets combined with in-house synthetic vision data, selected for balanced task coverage\nFollows a progressive training strategy: base model ‚Üí joint mid-training ‚Üí supervised fine-tuning\nüèÉ How to run LFM2-VL\nYou can run LFM2-VL with Hugging Face transformers via installing Transformers from source as follows:\npip install git+https://github.com/huggingface/transformers.git@87be5595081364ef99393feeaa60d71db3652679 pillow\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom transformers.image_utils import load_image\n# Load model and processor\nmodel_id = \"LiquidAI/LFM2-VL-3B\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ndtype=\"bfloat16\"\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Load image and create conversation\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = load_image(url)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": image},\n{\"type\": \"text\", \"text\": \"What is in this image?\"},\n],\n},\n]\n# Generate Answer\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\ntokenize=True,\n).to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=64)\nprocessor.batch_decode(outputs, skip_special_tokens=True)[0]\n# This image captures a vibrant street scene in a Chinatown area. The focal point is a large red Chinese archway with gold and black accents, adorned with Chinese characters. Flanking the archway are two white stone lion statues, which are traditional guardians in Chinese culture.\nYou can directly run and test the model with this Colab notebook.\nüîß How to fine-tune\nWe recommend fine-tuning LFM2-VL models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nüìà Performance\nModel\nAverage\nMMStar\nRealWorldQA\nMM-IFEval\nBLINK\nMMBench (dev en)\nOCRBench\nPOPE\nInternVL3_5-2B\n66.50\n57.67\n60.78\n47.31\n50.97\n78.18\n834.00\n87.17\nQwen2.5-VL-3B\n65.42\n56.13\n65.23\n38.62\n48.97\n80.41\n824.00\n86.17\nInternVL3-2B\n67.44\n61.10\n65.10\n38.49\n53.10\n81.10\n831.00\n90.10\nSmolVLM2-2.2B\n56.01\n46.00\n57.50\n19.42\n42.30\n69.24\n725.00\n85.10\nLFM2-VL-3B\n69.00\n57.73\n71.37\n51.83\n51.03\n79.81\n822.00\n89.01\nMore benchmark scores are reported in our LFM2-VL-3B post. We obtained the scores for competitive models using VLMEvalKit. Qwen3-VL-2B is not listed in the results table, as its release occurred the day before.\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "openai/gpt-oss-20b": "Highlights\nInference examples\nTransformers\nvLLM\nPyTorch / Triton\nOllama\nDownload the model\nReasoning levels\nTool use\nFine-tuning\nCitation\nTry gpt-oss ¬∑\nGuides ¬∑\nModel card ¬∑\nOpenAI blog\nWelcome to the gpt-oss series, OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.\nWe‚Äôre releasing two flavors of these open models:\ngpt-oss-120b ‚Äî for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\ngpt-oss-20b ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\nBoth models were trained on our harmony response format and should only be used with the harmony format as it will not work correctly otherwise.\nThis model card is dedicated to the smaller gpt-oss-20b model. Check out gpt-oss-120b for the larger model.\nHighlights\nPermissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.\nConfigurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\nFull chain-of-thought: Gain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs. It‚Äôs not intended to be shown to end users.\nFine-tunable: Fully customize models to your specific use case through parameter fine-tuning.\nAgentic capabilities: Use the models‚Äô native capabilities for function calling, web browsing, Python code execution, and Structured Outputs.\nMXFP4 quantization: The models were post-trained with MXFP4 quantization of the MoE weights, making gpt-oss-120b run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the gpt-oss-20b model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\nInference examples\nTransformers\nYou can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format. If you use model.generate directly, you need to apply the harmony format manually using the chat template or use our openai-harmony package.\nTo get started, install the necessary dependencies to setup your environment:\npip install -U transformers kernels torch\nOnce, setup you can proceed to run the model by running the snippet below:\nfrom transformers import pipeline\nimport torch\nmodel_id = \"openai/gpt-oss-20b\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nAlternatively, you can run the model via Transformers Serve to spin up a OpenAI-compatible webserver:\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\nLearn more about how to use gpt-oss with Transformers.\nvLLM\nvLLM recommends using uv for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\nuv pip install --pre vllm==0.10.1+gptoss \\\n--extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n--extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n--index-strategy unsafe-best-match\nvllm serve openai/gpt-oss-20b\nLearn more about how to use gpt-oss with vLLM.\nPyTorch / Triton\nTo learn about how to use this model with PyTorch and Triton, check out our reference implementations in the gpt-oss repository.\nOllama\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after installing Ollama.\n# gpt-oss-20b\nollama pull gpt-oss:20b\nollama run gpt-oss:20b\nLearn more about how to use gpt-oss with Ollama.\nLM Studio\nIf you are using LM Studio you can use the following commands to download.\n# gpt-oss-20b\nlms get openai/gpt-oss-20b\nCheck out our awesome list for a broader collection of gpt-oss resources and inference partners.\nDownload the model\nYou can download the model weights from the Hugging Face Hub directly from Hugging Face CLI:\n# gpt-oss-20b\nhuggingface-cli download openai/gpt-oss-20b --include \"original/*\" --local-dir gpt-oss-20b/\npip install gpt-oss\npython -m gpt_oss.chat model/\nReasoning levels\nYou can adjust the reasoning level that suits your task across three levels:\nLow: Fast responses for general dialogue.\nMedium: Balanced speed and detail.\nHigh: Deep and detailed analysis.\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\nTool use\nThe gpt-oss models are excellent for:\nWeb browsing (using built-in browsing tools)\nFunction calling with defined schemas\nAgentic operations like browser tasks\nFine-tuning\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\nThis smaller model gpt-oss-20b can be fine-tuned on consumer hardware, whereas the larger gpt-oss-120b can be fine-tuned on a single H100 node.\nCitation\n@misc{openai2025gptoss120bgptoss20bmodel,\ntitle={gpt-oss-120b & gpt-oss-20b Model Card},\nauthor={OpenAI},\nyear={2025},\neprint={2508.10925},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.10925},\n}",
    "nanonets/Nanonets-OCR2-3B": "Nanonets-OCR2 Family\nUsage\nUsing transformers\nUsing vLLM\nUsing Docstrange\nEvaluation\nMarkdown Evaluations\nNanonets OCR2 Plus\nNanonets OCR2 3B\nVisual Question Answering (VQA) Evaluations\nTips to improve accuracy\nBibTex\nNanonets-OCR2: A model for transforming documents into structured markdown with intelligent content recognition and semantic tagging\nüñ•Ô∏è Live Demo |\nüì¢ Blog |\n‚å®Ô∏è GitHub\nüìñ Cookbooks\nNanonets-OCR2 by Nanonets is a family of powerful, state-of-the-art image-to-markdown OCR models that go far beyond traditional text extraction. It transforms documents into structured markdown with intelligent content recognition and semantic tagging, making it ideal for downstream processing by Large Language Models (LLMs).\nNanonets-OCR2 is packed with features designed to handle complex documents with ease:\nLaTeX Equation Recognition: Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline ($...$) and display ($$...$$) equations.\nIntelligent Image Description: Describes images within documents using structured <img> tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.\nSignature Detection & Isolation: Identifies and isolates signatures from other text, outputting them within a <signature> tag. This is crucial for processing legal and business documents.\nWatermark Extraction: Detects and extracts watermark text from documents, placing it within a <watermark> tag.\nSmart Checkbox Handling: Converts form checkboxes and radio buttons into standardized Unicode symbols (‚òê, ‚òë, ‚òí) for consistent and reliable processing.\nComplex Table Extraction: Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.\nFlow charts & Organisational charts: Extracts flow charts and organisational as mermaid code.\nHandwritten Documents: The model is trained on handwritten documents across multiple languages.\nMultilingual: Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.\nVisual Question Answering (VQA): The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with \"Not mentioned.\"\nNanonets-OCR2 Family\nModel\nAccess Link\nNanonets-OCR2-Plus\nDocstrange link\nNanonets-OCR2-3B\nü§ó link\nNanonets-OCR2-1.5B-exp\nü§ó link\nUsage\nUsing transformers\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\nmodel_path = \"nanonets/Nanonets-OCR2-3B\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_path,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\nattn_implementation=\"flash_attention_2\"\n)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprocessor = AutoProcessor.from_pretrained(model_path)\ndef ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):\nprompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ‚òê and ‚òë for check boxes.\"\"\"\nimage = Image.open(image_path)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n{\"type\": \"text\", \"text\": prompt},\n]},\n]\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\ninputs = inputs.to(model.device)\noutput_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\ngenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nreturn output_text[0]\nimage_path = \"/path/to/your/document.jpg\"\nresult = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)\nprint(result)\nUsing vLLM\nStart the vLLM server.\nvllm serve nanonets/Nanonets-OCR2-3B\nPredict with the model\nfrom openai import OpenAI\nimport base64\nclient = OpenAI(api_key=\"123\", base_url=\"http://localhost:8000/v1\")\nmodel = \"nanonets/Nanonets-OCR2-3B\"\ndef encode_image(image_path):\nwith open(image_path, \"rb\") as image_file:\nreturn base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef ocr_page_with_nanonets_s(img_base64):\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"},\n},\n{\n\"type\": \"text\",\n\"text\": \"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ‚òê and ‚òë for check boxes.\",\n},\n],\n}\n],\ntemperature=0.0,\nmax_tokens=15000\n)\nreturn response.choices[0].message.content\ntest_img_path = \"/path/to/your/document.jpg\"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_nanonets_s(img_base64))\nUsing Docstrange\nimport requests\nurl = \"https://extraction-api.nanonets.com/extract\"\nheaders = {\"Authorization\": <API KEY>}\nfiles = {\"file\": open(\"/path/to/your/file\", \"rb\")}\ndata = {\"output_type\": \"markdown\"}\ndata[\"model\"] = \"nanonets\"\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\nCheck out Docstrange for more details.\nEvaluation\nMarkdown Evaluations\nNanonets OCR2 Plus\nModel\nWin Rate vs Nanonets OCR2 Plus (%)\nLose Rate vs Nanonets OCR2 Plus (%)\nBoth Correct (%)\nGemini 2.5 flash (No Thinking)\n34.35\n57.60\n8.06\nNanonets OCR2 3B\n29.37\n54.58\n16.04\nNanonets-OCR-s\n24.86\n66.12\n9.02\nNanonets OCR2 1.5B exp\n13.00\n81.20\n5.79\nGPT-5 (Thinking: low)\n23.53\n74.86\n1.60\nNanonets OCR2 3B\nModel\nWin Rate vs Nanonets OCR2 3B (%)\nLose Rate vs Nanonets OCR2 3B (%)\nBoth Correct (%)\nGemini 2.5 flash (No Thinking)\n39.98\n52.43\n7.58\nNanonets-OCR-s\n30.61\n58.28\n11.12\nNanonets OCR2 1.5B exp\n14.78\n79.18\n6.04\nGPT-5\n25.00\n72.87\n2.13\nVisual Question Answering (VQA) Evaluations\nDataset\nNanonets OCR2 Plus\nNanonets OCR2 3B\nQwen2.5-VL-72B-Instruct\nGemini 2.5 Flash\nChartQA (IDP-Leaderboard)\n79.20\n78.56\n76.20\n84.82\nDocVQA (IDP-Leaderboard)\n85.15\n89.43\n84.00\n85.51\nTips to improve accuracy\nIncreasing the image resolution will improve model's performance.\nFor complex tables (eg. Financial documents) using repetition_penalty=1 gives better results. You can try this prompt also, which generally works better for finantial documents.\nuser_prompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in HTML format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ‚òê and ‚òë for check boxes. Only return HTML table within <table></table>.\"\"\"\nThis is already implemented in Docstrange, please use the Markdown (Financial Docs) option for processing table heavy financial documents.\nimport requests\nurl = \"https://extraction-api.nanonets.com/extract\"\nheaders = {\"Authorization\": <API KEY>}\nfiles = {\"file\": open(\"/path/to/your/file\", \"rb\")}\ndata = {\"output_type\": \"markdown-financial-docs\"}\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\nModel might work best on certain resolution for specific document types. Please check the cookbooks for details.\nBibTex\n@misc{Nanonets-OCR2,\ntitle={Nanonets-OCR2: A model for transforming documents into structured markdown with intelligent content recognition and semantic tagging},\nauthor={Souvik Mandal and Ashish Talewar and Siddhant Thakuria and Paras Ahuja and Prathamesh Juvatkar},\nyear={2025},\n}",
    "allenai/olmOCR-2-7B-1025": "olmOCR-2-7B-1025\nolmOCR-Bench Scores\nUsage\nManual Prompting\nLicense and use\nolmOCR-2-7B-1025\nFull BF16 version of olmOCR-2-7B-1025-FP8.\nWe recommend using the FP8 version for all practical purposes except further fine tuning.\nThis is a release of the olmOCR model that's fine tuned from Qwen2.5-VL-7B-Instruct using the\nolmOCR-mix-1025 dataset. It has been additionally\nfine tuned using GRPO RL training to boost its performance at math equations, tables, and other tricky OCR cases.\nQuick links:\nüìÉ Paper\nü§ó SFT Dataset\nü§ó RL Dataset\nüõ†Ô∏è Code\nüéÆ Demo\nThe best way to use this model is via the olmOCR toolkit.\nThe toolkit comes with an efficient inference setup via VLLM that can handle millions of documents\nat scale.\nolmOCR-Bench Scores\nThis model scores the following scores on olmOCR-bench when used with the\nolmOCR toolkit toolkit which automatically renders, rotates, and retries pages as needed.\nModel\nArXiv\nOld Scans Math\nTables\nOld Scans\nHeaders and Footers\nMulti column\nLong tiny text\nBase\nOverall\nolmOCR pipeline v0.4.0 with olmOCR-2-7B-1025\n82.9\n82.1\n84.3\n48.3\n95.7\n84.3\n81.4\n99.7\n82.3 ¬± 1.1\nolmOCR pipeline v0.4.0 with olmOCR-2-7B-1025-FP8\n83.0\n82.3\n84.9\n47.7\n96.1\n83.7\n81.9\n99.7\n82.4 ¬± 1.1\nUsage\nThis model expects as input a single document image, rendered such that the longest dimension is 1288 pixels.\nThe prompt must then contain the additional metadata from the document, and the easiest way to generate this\nis to use the methods provided by the olmOCR toolkit.\nManual Prompting\nIf you want to prompt this model manually instead of using the olmOCR toolkit, please see the code below.\nIn normal usage, the olmOCR toolkit builds the prompt by rendering the PDF page, and\nextracting relevant text blocks and image metadata. To duplicate that you will need to\npip install olmocr>=0.4.0\nand then run the following sample code.\nimport torch\nimport base64\nimport urllib.request\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\nfrom olmocr.data.renderpdf import render_pdf_to_base64png\nfrom olmocr.prompts import build_no_anchoring_v4_yaml_prompt\n# Initialize the model\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-2-7B-1025\", torch_dtype=torch.bfloat16).eval()\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Grab a sample PDF\nurllib.request.urlretrieve(\"https://olmocr.allenai.org/papers/olmocr.pdf\", \"./paper.pdf\")\n# Render page 1 to an image\nimage_base64 = render_pdf_to_base64png(\"./paper.pdf\", 1, target_longest_image_dim=1288)\n# Build the full prompt\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": build_no_anchoring_v4_yaml_prompt()},\n{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n],\n}\n]\n# Apply the chat template and processor\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmain_image = Image.open(BytesIO(base64.b64decode(image_base64)))\ninputs = processor(\ntext=[text],\nimages=[main_image],\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = {key: value.to(device) for (key, value) in inputs.items()}\n# Generate the output\noutput = model.generate(\n**inputs,\ntemperature=0.1,\nmax_new_tokens=50,\nnum_return_sequences=1,\ndo_sample=True,\n)\n# Decode the output\nprompt_length = inputs[\"input_ids\"].shape[1]\nnew_tokens = output[:, prompt_length:]\ntext_output = processor.tokenizer.batch_decode(\nnew_tokens, skip_special_tokens=True\n)\nprint(text_output)\n# ['---\\nprimary_language: en\\nis_rotation_valid: True\\nrotation_correction: 0\\nis_table: False\\nis_diagram: False\\n---\\nolmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models\\n\\nJake Poz']\nLicense and use\nThis model is licensed under Apache 2.0. It is intended for research and educational use in accordance with Ai2's Responsible Use Guidelines.",
    "dx8152/Relight": "Welcome everyone to use Lora of Qwen-Edit-2509 of Image Fusion, his performance is very amazingÔºÅ\nThere are two ways to use it: 1. Use the prompt words in the dataset to change the image directly. 2. After installing Lora, The trigger word is: ÈáçÊñ∞ÁÖßÊòé„ÄÇ ÔºàFor example:ÈáçÊñ∞ÁÖßÊòé,‰ΩøÁî®Á™óÂ∏òÈÄèÂÖâÔºàÊüîÂíåÊº´Â∞ÑÔºâÁöÑÂÖâÁ∫øÂØπÂõæÁâáËøõË°åÈáçÊñ∞ÁÖßÊòéÔºâ Meaning: Relighting, using the soft, diffuse light from the curtains to relight the image.\nÔºàI also uploaded the re-lit Lora by Kontext downloaded from the Internet. If anyone knows who the original author is, I will add the source.Ôºâ\nInstructions: Download the lora fileÔºàQwen-Edit-Relight.safetensorsÔºâ to the models/loras folder.\nYou also need this lora to use together: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main\nThis is the effect used by Lora: https://youtu.be/GmDZmzoseFk\nFor communication/cooperation, you can join the discord group to communicateÔºö https://discord.gg/Qbq3VdjK",
    "meta-llama/Llama-3.1-8B-Instruct": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3_1/use-policy\nProhibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.1 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.1 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nTool use with transformers\nUse with llama\nHardware and Software\nTraining Data\nBenchmark scores\nBase pretrained models\nInstruction tuned models\nMultilingual benchmarks\nResponsibility & Safety\nResponsible deployment\nLlama 3.1 instruct\nLlama 3.1 systems\nNew capabilities\nEvaluations\nCritical and other risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.1 (text only)\nA new mix of publicly available online data.\n8B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n405B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: July 23, 2024.\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Meta-Llama-3.1-8B-Instruct, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nNote: You can also find detailed recipes on how to use the model locally, with torch.compile(), assisted generations, quantised and more at huggingface-llama-recipes\nTool use with transformers\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting here.\nTool use is also supported through chat templates in Transformers.\nHere is a quick example showing a single simple tool:\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n\"\"\"\nGet the current temperature at a location.\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nReturns:\nThe current temperature at the specified location in the specified units, as a float.\n\"\"\"\nreturn 22.  # A real function should probably actually get the temperature!\n# Next, create a chat and apply the chat template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n{\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\nand then call the tool and append the result, with the tool role, like so:\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the LLaMA prompt format docs and the Transformers tool use documentation.\nUse with llama\nPlease, follow the instructions in the repository\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-8B-Instruct\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions\n(tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions\n(tons CO2eq)\nLlama 3.1 8B\n1.46M\n700\n420\n0\nLlama 3.1 70B\n7.0M\n700\n2,040\n0\nLlama 3.1 405B\n30.84M\n700\n8,930\n0\nTotal\n39.3M\n11,390\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmark scores\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library.\nBase pretrained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B\nLlama 3.1 8B\nLlama 3 70B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n66.7\n66.7\n79.5\n79.3\n85.2\nMMLU-Pro (CoT)\n5\nmacro_avg/acc_char\n36.2\n37.1\n55.0\n53.8\n61.6\nAGIEval English\n3-5\naverage/acc_char\n47.1\n47.8\n63.0\n64.6\n71.6\nCommonSenseQA\n7\nacc_char\n72.6\n75.0\n83.8\n84.1\n85.8\nWinogrande\n5\nacc_char\n-\n60.5\n-\n83.3\n86.7\nBIG-Bench Hard (CoT)\n3\naverage/em\n61.1\n64.2\n81.3\n81.6\n85.9\nARC-Challenge\n25\nacc_char\n79.4\n79.7\n93.1\n92.9\n96.1\nKnowledge reasoning\nTriviaQA-Wiki\n5\nem\n78.5\n77.6\n89.7\n89.8\n91.8\nReading comprehension\nSQuAD\n1\nem\n76.4\n77.0\n85.6\n81.8\n89.3\nQuAC (F1)\n1\nf1\n44.4\n44.9\n51.1\n51.1\n53.6\nBoolQ\n0\nacc_char\n75.7\n75.0\n79.0\n79.4\n80.0\nDROP (F1)\n3\nf1\n58.4\n59.5\n79.7\n79.6\n84.8\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3 8B Instruct\nLlama 3.1 8B Instruct\nLlama 3 70B Instruct\nLlama 3.1 70B Instruct\nLlama 3.1 405B Instruct\nGeneral\nMMLU\n5\nmacro_avg/acc\n68.5\n69.4\n82.0\n83.6\n87.3\nMMLU (CoT)\n0\nmacro_avg/acc\n65.3\n73.0\n80.9\n86.0\n88.6\nMMLU-Pro (CoT)\n5\nmicro_avg/acc_char\n45.5\n48.3\n63.4\n66.4\n73.3\nIFEval\n76.8\n80.4\n82.9\n87.5\n88.6\nReasoning\nARC-C\n0\nacc\n82.4\n83.4\n94.4\n94.8\n96.9\nGPQA\n0\nem\n34.6\n30.4\n39.5\n46.7\n50.7\nCode\nHumanEval\n0\npass@1\n60.4\n72.6\n81.7\n80.5\n89.0\nMBPP ++ base version\n0\npass@1\n70.6\n72.8\n82.5\n86.0\n88.6\nMultipl-E HumanEval\n0\npass@1\n-\n50.8\n-\n65.5\n75.2\nMultipl-E MBPP\n0\npass@1\n-\n52.4\n-\n62.0\n65.7\nMath\nGSM-8K (CoT)\n8\nem_maj1@1\n80.6\n84.5\n93.0\n95.1\n96.8\nMATH (CoT)\n0\nfinal_em\n29.1\n51.9\n51.0\n68.0\n73.8\nTool Use\nAPI-Bank\n0\nacc\n48.3\n82.6\n85.1\n90.0\n92.0\nBFCL\n0\nacc\n60.3\n76.1\n83.0\n84.8\n88.5\nGorilla Benchmark API Bench\n0\nacc\n1.7\n8.2\n14.7\n29.7\n35.3\nNexus (0-shot)\n0\nmacro_avg/acc\n18.1\n38.5\n47.8\n56.7\n58.7\nMultilingual\nMultilingual MGSM (CoT)\n0\nem\n-\n68.9\n-\n86.9\n91.6\nMultilingual benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.1 8B\nLlama 3.1 70B\nLlama 3.1 405B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n62.12\n80.13\n84.95\nSpanish\n62.45\n80.05\n85.08\nItalian\n61.63\n80.4\n85.04\nGerman\n60.59\n79.27\n84.36\nFrench\n62.34\n79.82\n84.66\nHindi\n50.88\n74.52\n80.31\nThai\n50.32\n72.95\n78.21\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.1 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.1 systems\nLarge language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teaming\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nOur study of Llama-3.1-405B‚Äôs social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "JunhaoZhuang/FlashVSR": "‚ö° FlashVSR\nüåü Abstract\nüì∞ News\nüìã TODO\nüöÄ Getting Started\nüõ†Ô∏è Method\nü§ó Feedback & Support\nüìÑ Acknowledgments\nüìû Contact\nüìú Citation\n‚ö° FlashVSR\nTowards Real-Time Diffusion-Based Streaming Video Super-Resolution\nAuthors: Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue\nYour star means a lot for us to develop this project! :star:\nüåü Abstract\nDiffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at ‚àº17 FPS for 768 √ó 1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train‚Äìtest resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to ‚àº12√ó speedup over prior one-step diffusion VSR models.\nüì∞ News\nRelease Date: October 2025 ‚Äî Inference code and model weights are available now! üéâ\nComing Soon: Dataset release (VSR-120K) for large-scale training.\nüìã TODO\n‚úÖ Release inference code and model weights\n‚¨ú Release dataset (VSR-120K)\nüöÄ Getting Started\nFollow these steps to set up and run FlashVSR on your local machine:\n1Ô∏è‚É£ Clone the Repository\ngit clone https://github.com/OpenImagingLab/FlashVSR\ncd FlashVSR\n2Ô∏è‚É£ Set Up the Python Environment\nCreate and activate the environment (Python 3.11.13):\nconda create -n flashvsr python=3.11.13\nconda activate flashvsr\nInstall project dependencies:\npip install -e .\npip install -r requirements.txt\n3Ô∏è‚É£ Install Block-Sparse Attention (Required)\nFlashVSR relies on the Block-Sparse Attention backend to enable flexible and dynamic attention masking for efficient inference.\ngit clone https://github.com/mit-han-lab/Block-Sparse-Attention\ncd Block-Sparse-Attention\npip install packaging\npip install ninja\npython setup.py install\n‚ö†Ô∏è Note: The Block-Sparse Attention backend currently achieves ideal acceleration only on NVIDIA A100 or A800 GPUs (Ampere architecture). On H100/H800 (Hopper) GPUs, due to differences in hardware scheduling and sparse kernel behavior, the expected speedup may not be realized, and in some cases performance can even be slower than dense attention.\n4Ô∏è‚É£ Download Model Weights from Hugging Face\nWeights are hosted on Hugging Face via Git LFS. Please install Git LFS first:\n# From the repo root\ncd examples/WanVSR\n# Install Git LFS (once per machine)\ngit lfs install\n# Clone the model repository into examples/WanVSR\ngit lfs clone https://huggingface.co/JunhaoZhuang/FlashVSR\nAfter cloning, you should have:\n./examples/WanVSR/FlashVSR/\n‚îÇ\n‚îú‚îÄ‚îÄ LQ_proj_in.ckpt\n‚îú‚îÄ‚îÄ TCDecoder.ckpt\n‚îú‚îÄ‚îÄ Wan2.1_VAE.pth\n‚îú‚îÄ‚îÄ diffusion_pytorch_model_streaming_dmd.safetensors\n‚îî‚îÄ‚îÄ README.md\nThe inference scripts will load weights from ./examples/WanVSR/FlashVSR/ by default.\n5Ô∏è‚É£ Run Inference\n# From the repo root\ncd examples/WanVSR\npython infer_flashvsr_full.py      # Full model\n# or\npython infer_flashvsr_tiny.py      # Tiny model\nüõ†Ô∏è Method\nThe overview of FlashVSR. This framework features:\nThree-Stage Distillation Pipeline for streaming VSR training.\nLocality-Constrained Sparse Attention to cut redundant computation and bridge the train‚Äìtest resolution gap.\nTiny Conditional Decoder for efficient, high-quality reconstruction.\nVSR-120K Dataset consisting of 120k videos and 180k images, supports joint training on both images and videos.\nü§ó Feedback & Support\nWe welcome feedback and issues. Thank you for trying FlashVSR!\nüìÑ Acknowledgments\nWe gratefully acknowledge the following open-source projects:\nDiffSynth Studio ‚Äî https://github.com/modelscope/DiffSynth-Studio\nBlock-Sparse-Attention ‚Äî https://github.com/mit-han-lab/Block-Sparse-Attention\ntaehv ‚Äî https://github.com/madebyollin/taehv\nüìû Contact\nJunhao Zhuang\nEmail: zhuangjh23@mails.tsinghua.edu.cn\nüìú Citation\n@misc{zhuang2025flashvsrrealtimediffusionbasedstreaming,\ntitle={FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution},\nauthor={Junhao Zhuang and Shi Guo and Xin Cai and Xiaohui Li and Yihao Liu and Chun Yuan and Tianfan Xue},\nyear={2025},\neprint={2510.12747},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.12747},\n}",
    "lightx2v/Wan2.2-Distill-Loras": "üé¨ Wan2.2 Distilled LoRA Models\nüåü What's Special?\n‚ö° Flexible Deployment\nüéØ Dual Noise Control\nüíæ Storage Efficient\nüöÄ 4-Step Inference\nüì¶ LoRA Model Catalog\nüé• Available LoRA Models\nüöÄ Usage\nPrerequisites\nMethod 1: LightX2V - Offline LoRA Merging (Recommended ‚≠ê)\nMethod 2: LightX2V - Online LoRA Loading\nMethod 3: ComfyUI\n‚ö†Ô∏è Important Notes\nüìö Related Resources\nDocumentation Links\nRelated Models\nü§ù Community & Support\nüé¨ Wan2.2 Distilled LoRA Models\n‚ö° High-Performance Video Generation with 4-Step Inference Using LoRA\nLoRA weights extracted from Wan2.2 distilled models - Flexible deployment with excellent generation quality\nüåü What's Special?\n‚ö° Flexible Deployment\nBase Model + LoRA: Can be combined with base models\nOffline Merging: Pre-merge LoRA into models\nOnline Loading: Dynamically load LoRA during inference\nMultiple Frameworks: Supports LightX2V and ComfyUI\nüéØ Dual Noise Control\nHigh Noise: More creative, diverse outputs\nLow Noise: More faithful to input, stable outputs\nRank 64 LoRA, compact size\nüíæ Storage Efficient\nSmall LoRA Size: Significantly smaller than full models\nFlexible Combination: Can be combined with quantization\nEasy Sharing: Convenient for model weight distribution\nüöÄ 4-Step Inference\nUltra-Fast Generation: Generate high-quality videos in just 4 steps\nDistillation Acceleration: Inherits advantages of distilled models\nQuality Assurance: Maintains excellent generation quality\nüì¶ LoRA Model Catalog\nüé• Available LoRA Models\nTask Type\nNoise Level\nModel File\nRank\nPurpose\nI2V\nHigh Noise\nwan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors\n64\nMore creative image-to-video\nI2V\nLow Noise\nwan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors\n64\nMore stable image-to-video\nüí° Note:\nxxx in filenames represents version number or timestamp, please check HuggingFace repository for the latest version\nThese LoRAs must be used with Wan2.2 base models\nüöÄ Usage\nPrerequisites\nBase Model: You need to prepare Wan2.2 I2V base model (original model without distillation)\nDownload base model (choose one):\nMethod 1: From LightX2V Official Repository (Recommended)\n# Download high noise base model\nhuggingface-cli download lightx2v/Wan2.2-Official-Models \\\nwan2.2_i2v_A14b_high_noise_lightx2v.safetensors \\\n--local-dir ./models/Wan2.2-Official-Models\n# Download low noise base model\nhuggingface-cli download lightx2v/Wan2.2-Official-Models \\\nwan2.2_i2v_A14b_low_noise_lightx2v.safetensors \\\n--local-dir ./models/Wan2.2-Official-Models\nMethod 2: From Wan-AI Official Repository\nhuggingface-cli download Wan-AI/Wan2.2-I2V-A14B \\\n--local-dir ./models/Wan2.2-I2V-A14B\nüí° Note: lightx2v/Wan2.2-Official-Models provides separate high noise and low noise base models, download as needed\nMethod 1: LightX2V - Offline LoRA Merging (Recommended ‚≠ê)\nOffline LoRA merging provides best performance and supports quantization simultaneously.\n1.1 Download LoRA Models\n# Download both LoRAs (high noise and low noise)\n# Note: xxx represents version number, please check HuggingFace for actual filename\nhuggingface-cli download lightx2v/Wan2.2-Distill-Loras \\\nwan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\nwan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--local-dir ./loras/\n1.2 Merge LoRA (Basic Merging)\nMerge LoRA:\ncd LightX2V/tools/convert\n# For directory-based base model: --source /path/to/Wan2.2-I2V-A14B/high_noise_model/\npython converter.py \\\n--source ./models/Wan2.2-Official-Models/wan2.2_i2v_A14b_high_noise_lightx2v.safetensors \\\n--output /path/to/output/ \\\n--output_ext .safetensors \\\n--output_name wan2.2_i2v_A14b_high_noise_lightx2v_4step \\\n--model_type wan_dit \\\n--lora_path /path/to/loras/wan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--lora_strength 1.0 \\\n--single_file\n# For directory-based base model: --source /path/to/Wan2.2-I2V-A14B/low_noise_model/\npython converter.py \\\n--source ./models/Wan2.2-Official-Models/wan2.2_i2v_A14b_low_noise_lightx2v.safetensors \\\n--output /path/to/output/ \\\n--output_ext .safetensors \\\n--output_name wan2.2_i2v_A14b_low_noise_lightx2v_4step \\\n--model_type wan_dit \\\n--lora_path /path/to/loras/wan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--lora_strength 1.0 \\\n--single_file\n1.3 Merge LoRA + Quantization (Recommended)\nMerge LoRA + FP8 Quantization:\ncd LightX2V/tools/convert\n# For directory-based base model: --source /path/to/Wan2.2-I2V-A14B/high_noise_model/\npython converter.py \\\n--source ./models/Wan2.2-Official-Models/wan2.2_i2v_A14b_high_noise_lightx2v.safetensors \\\n--output /path/to/output/ \\\n--output_ext .safetensors \\\n--output_name wan2.2_i2v_A14b_high_noise_scaled_fp8_e4m3_lightx2v_4step \\\n--model_type wan_dit \\\n--lora_path /path/to/loras/wan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--lora_strength 1.0 \\\n--quantized \\\n--linear_dtype torch.float8_e4m3fn \\\n--non_linear_dtype torch.bfloat16 \\\n--single_file\n# For directory-based base model: --source /path/to/Wan2.2-I2V-A14B/low_noise_model/\npython converter.py \\\n--source ./models/Wan2.2-Official-Models/wan2.2_i2v_A14b_low_noise_lightx2v.safetensors \\\n--output /path/to/output/ \\\n--output_ext .safetensors \\\n--output_name wan2.2_i2v_A14b_low_noise_scaled_fp8_e4m3_lightx2v_4step \\\n--model_type wan_dit \\\n--lora_path /path/to/loras/wan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--lora_strength 1.0 \\\n--quantized \\\n--linear_dtype torch.float8_e4m3fn \\\n--non_linear_dtype torch.bfloat16 \\\n--single_file\nMerge LoRA + ComfyUI FP8 Format:\ncd LightX2V/tools/convert\n# For directory-based base model: --source /path/to/Wan2.2-I2V-A14B/high_noise_model/\npython converter.py \\\n--source ./models/Wan2.2-Official-Models/wan2.2_i2v_A14b_high_noise_lightx2v.safetensors \\\n--output /path/to/output/ \\\n--output_ext .safetensors \\\n--output_name wan2.2_i2v_A14b_high_noise_scaled_fp8_e4m3_lightx2v_4step_comfyui \\\n--model_type wan_dit \\\n--lora_path /path/to/loras/wan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--lora_strength 1.0 \\\n--quantized \\\n--linear_dtype torch.float8_e4m3fn \\\n--non_linear_dtype torch.bfloat16 \\\n--single_file \\\n--comfyui_mode\n# For directory-based base model: --source /path/to/Wan2.2-I2V-A14B/low_noise_model/\npython converter.py \\\n--source ./models/Wan2.2-Official-Models/wan2.2_i2v_A14b_low_noise_lightx2v.safetensors \\\n--output /path/to/output/ \\\n--output_ext .safetensors \\\n--output_name wan2.2_i2v_A14b_low_noise_scaled_fp8_e4m3_lightx2v_4step_comfyui \\\n--model_type wan_dit \\\n--lora_path /path/to/loras/wan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--lora_strength 1.0 \\\n--quantized \\\n--linear_dtype torch.float8_e4m3fn \\\n--non_linear_dtype torch.bfloat16 \\\n--single_file \\\n--comfyui_mode\nüìù Reference Documentation: For more merging options, see LightX2V Model Conversion Documentation\nMethod 2: LightX2V - Online LoRA Loading\nOnline LoRA loading requires no pre-merging, loads dynamically during inference, more flexible.\n2.1 Download LoRA Models\n# Download both LoRAs (high noise and low noise)\n# Note: xxx represents version number, please check HuggingFace for actual filename\nhuggingface-cli download lightx2v/Wan2.2-Distill-Loras \\\nwan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\nwan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors \\\n--local-dir ./loras/\n2.2 Use Configuration File\nReference configuration file: wan_moe_i2v_distil_with_lora.json\nLoRA configuration example in config file:\n{\n\"lora_configs\": [\n{\n\"name\": \"high_noise_model\",\n\"path\": \"/path/to/loras/wan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_xxx.safetensors\",\n\"strength\": 1.0\n},\n{\n\"name\": \"low_noise_model\",\n\"path\": \"/path/to/loras/wan2.2_i2v_A14b_low_noise_lora_rank64_lightx2v_4step_xxx.safetensors\",\n\"strength\": 1.0\n}\n]\n}\nüí° Tip: Replace xxx with actual version number (e.g., 1022). Check HuggingFace repository for the latest version\n2.3 Run Inference\nUsing I2V as example:\ncd scripts\nbash wan22/run_wan22_moe_i2v_distill.sh\nMethod 3: ComfyUI\nPlease refer to workflow\n‚ö†Ô∏è Important Notes\nBase Model Requirement: These LoRAs must be used with Wan2.2-I2V-A14B base model, cannot be used standalone\nOther Components: In addition to DIT model and LoRA, the following are also required at runtime:\nT5 text encoder\nCLIP vision encoder\nVAE encoder/decoder\nTokenizer\nPlease refer to LightX2V Documentation for how to organize complete model directory\nInference Configuration: When using 4-step inference, configure correct denoising_step_list, recommended: [1000, 750, 500, 250]\nüìö Related Resources\nDocumentation Links\nLightX2V Quick Start: Quick Start Documentation\nModel Conversion Tool: Conversion Tool Documentation\nOnline LoRA Loading: Configuration File Example\nQuantization Guide: Quantization Documentation\nModel Structure: Model Structure Documentation\nRelated Models\nDistilled Full Models: Wan2.2-Distill-Models\nWan2.2 Official Models: Wan2.2-Official-Models - Contains high noise and low noise base models\nBase Model (Wan-AI): Wan2.2-I2V-A14B\nü§ù Community & Support\nGitHub Issues: https://github.com/ModelTC/LightX2V/issues\nHuggingFace: https://huggingface.co/lightx2v/Wan2.2-Distill-Loras\nLightX2V Homepage: https://github.com/ModelTC/LightX2V\nIf you find this project helpful, please give us a ‚≠ê on GitHub",
    "nvidia/audio-flamingo-3-hf": "Model Overview\nDescription:\nResults:\nModel Architecture:\nLicense / Terms of Use\nDeployment Geography\nUse Case\nRelease Date\nReferences:\nModel Architecture:\nInput:\nOutput:\nSoftware Integration:\nModel Version:\nTraining and Testing Datasets:\nTraining Dataset:\nTesting Dataset:\nInference:\nEthical Considerations:\nAcknowledgements\nModel Overview\nAudio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio-Language Models\nDescription:\nAudio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:\nUnified audio representation learning (speech, sound, music)\nFlexible, on-demand chain-of-thought reasoning\nLong-context audio comprehension (up to 10 minutes)\nMulti-turn, multi-audio conversational dialogue (AF3-Chat)\nVoice-to-voice interaction (AF3-Chat)\nExtensive evaluations confirm AF3‚Äôs effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.\nThis model is for non-commercial research purposes only.\nResults:\nModel Architecture:\nAudio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.\nLicense / Terms of Use\nThe model is released under the NVIDIA OneWay Noncommercial License. Portions of the dataset generation are also subject to the Qwen Research License and OpenAI‚Äôs Terms of Use.\nDeployment Geography\nGlobal.\nUse Case\nIntended for researchers and developers to explore:\nAudio question answering and reasoning\nLong-context audio comprehension\nInteractive sound/music design assistants\nMulti-turn (voice) chat\nRelease Date\nGithub (07/10/2025) via https://github.com/NVIDIA/audio-flamingo\nHuggingFace (07/10/2025) via https://huggingface.co/nvidia/audio-flamingo-3\nReferences:\nAudio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio-Language Models\nProject Page\nDemo Website\nHugging Face\nModel Architecture:\nArchitecture Type: TransformerNetwork Architecture: Audio Flamingo 3\nAF3 uses:\nAF-Whisper unified audio encoder\nMLP-based audio adaptor\nDecoder-only LLM backbone (Qwen2.5-7B)\nStreaming TTS module (AF3-Chat)\n**This model was developed based on NVILA and Qwen-2.5-7B\nInput:\nInput Type: Audio, Text\nInput Format: WAV/MP3/FLAC, UTF-8 text\nInput Parameters: Audio is Two-Dimensional (2D) and Text is One-Dimensional (1D)\nOther Properties Related to Input:\nMax Audio Length: 10 Minutes\nMax Text Length: 16000 tokens\nOutput:\nOutput Type: Text (and optional speech)\nText Format: UTF-8 string\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output:\nMax Text Length: 1024 tokens\nSpeech Format: streaming TTS (text-to-speech) waveform\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems (A100/H100). By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nRuntime Engine: PyTorch / HuggingFace Transformers\nSupported Hardware:\nNVIDIA Ampere (A100)\nNVIDIA Hopper (H100)\nSupported OS:\nLinux\nModel Version:\nv3.0\nTraining and Testing Datasets:\nTraining Dataset:\nAF3 is trained entirely on open-source audio data, organized into four novel, large-scale collections. For each dataset, we mention whether the dataset annotations are collected by Human or they are Automated i.e. generated using AI models.\nThe data collection method noted below applies for all datasets used for training and testing:\nData Collection Method: Human\nLabeling Collection Method: Please see below:\nGeneral Sound:\nWavCaps (Automated)\nMACS (Human)\nSoundDescs (Human)\nClotho-v2 (Human)\nWavText5K (Human)\nClotho-AQA (Human)\nOpen-AQA  (Automated)\nCompA-R  (Automated)\nSalmonn AQA  (Automated)\nAudio Entailment(Automated)\nCompA  (Automated)\nAudioSet  (Human)\nYouTube-8M  (Human)\nFSD50k  (Human)\nCochlScene  (Human)\nNonSpeech7K  (Human)\nChime-Home  (Human)\nSonyc-UST  (Human)\nMusic:\nLP-MusicCaps  (Automated)\nMusicQA  (Automated)\nMusicAVQA  (Human)\nMusicBench  (Automated)\nMu-LLAMA  (Automated)\nNSynth  (Human)\nFMA  (Human)\nMusDB-HQ  (Human)\nMusic4All  (Human)\nMillion Song Dataset  (Human)\nSpeech:\nMSP-Podcast  (Human)\nJL-Corpus  (Human)\nMELD  (Human)\nTess  (Human)\nOMGEmotion  (Human)\nEmov-DB  (Human)\nLibriSpeech  (Human)\nSPGISpeech  (Human)\nTEDLIUM  (Human)\nGigaSpeech  (Human)\nCommon Voice 15  (Human)\nVoxPopuli  (Human)\nVoxCeleb2  (Human)\nSwitchboard  (Human)\nAMI  (Human)\nVoice:\nVoiceAssistant-400K  (Automated)\nMixed:\nAudioSkills-XL (ours) (Automated)\nLongAudio-XL (ours) (Automated)\nAF-Think (ours) (Automated)\nAF-Chat (ours) (Automated)\nTesting Dataset:\nAudio Flamingo 3 is evaluated on the test split of the following datasets.\nData Collection Method: Human (for all datasets noted below)\nLabeling Method: See below\nClothoAQA  (Human)\nMusicAVQA  (Human)\nClotho-v2  (Human)\nCochlScene  (Human)\nNonSpeech7K  (Human)\nNSynth  (Human)\nAudioCaps  (Human)\nUS8K  (Human)\nGTZAN  (Human)\nMMAU  (Human)\nMMAR  (Human)\nAudio Entailment(Automated)\nCompA-R-test  (Automated)\nMuchoMusic  (Automated)\nOpen-AQA(Automated)\nMusicInstruct  (Automated)\nMusicQA  (Automated)\nCMM Hallucination  (Human)\nIEMOCAP  (Human)\nVoiceBench  (Human)\nOpenAudioBench (Human)\nSEED  (Human)\nLibriSpeech  (Human)\nSPGISpeech  (Human)\nTEDLIUM  (Human)\nGigaSpeech  (Human)\nCommon Voice 15  (Human)\nVoxPopuli  (Human)\nLongAudioBench (ours)  (Automated)\nAF-Chat-test (ours)  (Human)\nInference:\nEngine: HuggingFace TransformersTest Hardware: NVIDIA A100 80 GB\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nAcknowledgements\nBuilt with Qwen, NVILA and the open audio-ML community.",
    "neuphonic/neutts-air": "NeuTTS Air ‚òÅÔ∏è\nKey Features\nModel Details\nGet Started\nBasic Example\nSimple One-Code Block Usage\nExample Reference Files\nGuidelines for Best Results\nTips\nExample Reference Files\nGuidelines for Best Results\nResponsibility\nDisclaimer\nNeuTTS Air ‚òÅÔ∏è\nüöÄ Spaces Demo, üîß Github\nQ8 GGUF version, Q4 GGUF version\nCreated by Neuphonic - building faster, smaller, on-device voice AI\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\nKey Features\nüó£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\nüì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\nüë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio\nüöÑSimple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\nWebsites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.\nWe are on neuphonic.com only. Please be careful out there! üôè\nModel Details\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\nAudio Codec: NeuCodec - our proprietary neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\nFormat: Available in GGML format for efficient on-device inference\nResponsibility: Watermarked outputs\nInference Speed: Real-time generation on mid-range devices\nPower Consumption: Optimised for mobile and embedded devices\nGet Started\nClone the Git Repo\ngit clone https://github.com/neuphonic/neutts-air.git\ncd neuttsair\nInstall¬†espeak¬†(required dependency)\nPlease refer to the following link for instructions on how to install¬†espeak:\nhttps://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n# Mac OS\nbrew install espeak\n# Ubuntu/Debian\nsudo apt install espeak\n# Arch Linux\nparu -S aur/espeak\nInstall Python dependencies\nThe requirements file includes the dependencies needed to run the model with PyTorch. When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\nThe inference is compatible and tested on¬†python>=3.11.\npip install -r requirements.txt\nBasic Example\nRun the basic example script to synthesize speech:\npython -m examples.basic_example \\\n--input_text \"My name is Dave, and um, I'm from London\" \\\n--ref_audio samples/dave.wav \\\n--ref_text samples/dave.txt\nTo specify a particular model repo for the backbone or codec, add the¬†--backbone¬†argument. Available backbones are listed in¬†NeuTTS-Air huggingface collection.\nSeveral examples are available, including a Jupyter notebook in the¬†examples¬†folder.\nSimple One-Code Block Usage\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\ntts = NeuTTSAir( backbone_repo=\"neuphonic/neutts-air-q4-gguf\", backbone_device=\"cpu\", codec_repo=\"neuphonic/neucodec\", codec_device=\"cpu\")\ninput_text = \"My name is Dave, and um, I'm from London.\"\nref_text = \"samples/dave.txt\"\nref_audio_path = \"samples/dave.wav\"\nref_text = open(ref_text, \"r\").read().strip()\nref_codes = tts.encode_reference(ref_audio_path)\nwav = tts.infer(input_text, ref_codes, ref_text)\nsf.write(\"test.wav\", wav, 24000)\nTips\nNeuTTS Air requires two inputs:\nA reference audio sample (.wav file)\nA text string\nThe model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS Air‚Äôs instant voice cloning capability.\nExample Reference Files\nYou can find some ready-to-use samples in the examples folder:\nsamples/dave.wav\nsamples/jo.wav\nGuidelines for Best Results\nFor optimal performance, reference audio samples should be:\nMono channel\n16-44 kHz sample rate\n3‚Äì15 seconds in length\nSaved as a .wav file\nClean ‚Äî minimal to no background noise\nNatural, continuous speech ‚Äî like a monologue or conversation, with few pauses, so the model can capture tone effectively\nResponsibility\nEvery audio file generated by NeuTTS Air includes **Perth (Perceptual Threshold) Watermarker.**\nDisclaimer\nDon't use this model to do bad things‚Ä¶ please."
}