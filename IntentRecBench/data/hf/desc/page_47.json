{
    "uva-cv-lab/FrameINO_Wan2.2_5B_Stage2_MotionINO_v1.5": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nREADME.md exists but content is empty.",
    "mradermacher/Luminous-Shadow-12B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Vortex5/Luminous-Shadow-12B\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Luminous-Shadow-12B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n4.9\nGGUF\nQ3_K_S\n5.6\nGGUF\nQ3_K_M\n6.2\nlower quality\nGGUF\nQ3_K_L\n6.7\nGGUF\nIQ4_XS\n6.9\nGGUF\nQ4_K_S\n7.2\nfast, recommended\nGGUF\nQ4_K_M\n7.6\nfast, recommended\nGGUF\nQ5_K_S\n8.6\nGGUF\nQ5_K_M\n8.8\nGGUF\nQ6_K\n10.2\nvery good quality\nGGUF\nQ8_0\n13.1\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "Alibaba-NLP/E2Rank-8B": "üìå Introduction\nüöÄ Quick Start\nModel List\nUsage\nEmbedding Model\nReranking\nEnd-to-end search\nüìä Evaluation\nReranking Benchmark\nBEIR\nEmbedding Benchmark\nMTEB (Eng, v1)\nüö© Citation\nE2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker\nü§ñ Website |\nüìÑ Arxiv Paper |\nü§ó Huggingface Collection |\nüö© Citation\nüìå Introduction\nWe introduce E2Rank,\nmeaning Efficient Embedding-based Ranking\n(also meaning Embedding-to-Rank),\nwhich extends a single text embedding model\nto perform both high-quality retrieval and listwise reranking,\nthereby achieving strong effectiveness with remarkable efficiency.\nBy applying cosine similarity between the query and\ndocument embeddings as a unified ranking function, the listwise ranking prompt,\nwhich is constructed from the original query and its candidate documents, serves\nas an enhanced query enriched with signals from the top-K documents, akin to\npseudo-relevance feedback (PRF) in traditional retrieval models. This design\npreserves the efficiency and representational quality of the base embedding model\nwhile significantly improving its reranking performance.\nEmpirically, E2Rank achieves state-of-the-art results on the BEIR reranking benchmark\nand demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark,\nwith very low reranking latency. We also show that the ranking training process\nimproves embedding performance on the MTEB benchmark.\nOur findings indicate that a single embedding model can effectively unify retrieval and reranking,\noffering both computational efficiency and competitive ranking accuracy.\nOur work highlights the potential of single embedding models to serve as unified retrieval-reranking engines, offering a practical, efficient, and accurate alternative to complex multi-stage ranking systems.\nüöÄ Quick Start\nModel List\nSupported Task\nModel Name\nSize\nLayers\nSequence Length\nEmbedding Dimension\nInstruction Aware\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-0.6B\n0.6B\n28\n32K\n1024\nYes\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-4B\n4B\n36\n32K\n2560\nYes\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-8B\n8B\n36\n32K\n4096\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n0.6B\n28\n32K\n1024\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n4B\n36\n32K\n2560\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n8B\n36\n32K\n4096\nYes\nNote:\nEmbedding Only indicates that the model is trained only with the constrative learning and support embedding tasks, while Embedding + Reranking indicates the full E2Rank model trained with both embedding and reranking objectives (for more detals, please refer to the paper).\nInstruction Aware notes whether the model supports customizing the input instruction according to different tasks.\nUsage\nEmbedding Model\nThe usage of E2Rank as an embedding model is similar to Qwen3-Embedding. The only difference is that Qwen3-Embedding will automatically append an EOS token, while E2Rank requires users to manully append the special token <|endoftext|> at the end of each input text.\nvLLM Usage (recommended)\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ninput_texts = [t + \"<|endoftext|>\" for t in input_texts]\nmodel = LLM(\nmodel=\"Alibaba-NLP/E2Rank-8B\",\ntask=\"embed\",\noverride_pooler_config=PoolerConfig(pooling_type=\"LAST\", normalize=True)\n)\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ninput_texts = [t + \"<|endoftext|>\" for t in input_texts]\ntokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/E2Rank-8B', padding_side='left')\nmodel = AutoModel.from_pretrained('Alibaba-NLP/E2Rank-8B')\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nwith torch.no_grad():\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nReranking\nFor using E2Rank as a reranker, you only need to perform additional processing on the query by adding (part of) the docs that needs to be reranked to the listwise prompt, while the rest is the same as using the embedding model.\nvLLM Usage (recommended)\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\nmodel = LLM(\nmodel=\"./checkpoints/E2Rank-8B\",\ntask=\"embed\",\noverride_pooler_config=PoolerConfig(pooling_type=\"LAST\", normalize=True)\n)\ntokenizer = model.get_tokenizer()\ndef get_listwise_prompt(task_description: str, query: str, documents: list[str], num_input_docs: int = 20) -> str:\ninput_docs = documents[:num_input_docs]\ninput_docs = \"\\n\".join([f\"[{i}] {doc}\" for i, doc in enumerate(input_docs, start=1)])\nmessages = [{\n\"role\": \"user\",\n\"content\": f'{task_description}\\nDocuments:\\n{input_docs}Search Query:{query}'\n}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False,\n)\nreturn text\ntask = 'Given a web search query and some relevant documents, rerank the documents that answer the query:'\nqueries = [\n'What is the capital of China?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ndocuments = [doc + \"<|endoftext|>\" for doc in documents]\npseudo_queries = [\nget_listwise_prompt(task, queries[0], documents),\nget_listwise_prompt(task, queries[1], documents)\n]  # no need to add the EOS token here\ninput_texts = pseudo_queries + documents\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('./checkpoints/E2Rank-8B', padding_side='left')\nmodel = AutoModel.from_pretrained('./checkpoints/E2Rank-8B')\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_listwise_prompt(task_description: str, query: str, documents: list[str], num_input_docs: int = 20) -> str:\ninput_docs = documents[:num_input_docs]\ninput_docs = \"\\n\".join([f\"[{i}] {doc}\" for i, doc in enumerate(input_docs, start=1)])\nmessages = [{\n\"role\": \"user\",\n\"content\": f'{task_description}\\nDocuments:\\n{input_docs}Search Query:{query}'\n}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False,\n)\nreturn text\ntask = 'Given a web search query and some relevant documents, rerank the documents that answer the query:'\nqueries = [\n'What is the capital of China?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ndocuments = [doc + \"<|endoftext|>\" for doc in documents]\npseudo_queries = [\nget_listwise_prompt(task, queries[0], documents),\nget_listwise_prompt(task, queries[1], documents)\n]  # no need to add the EOS token here\ninput_texts = pseudo_queries + documents\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nwith torch.no_grad():\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nEnd-to-end search\nSince E2Rank extends a single text embedding model to perform both high-quality retrieval and listwise reranking, you can directly use it to build an end-to-end search system. By reusing the embeddings computed during the retrieval stage, E2Rank only need to compute the pseudo query's embedding and can efficiently rerank the retrieved documents with minimal additional computational overhead.\nExample code is coming soon.\nüìä Evaluation\nReranking Benchmark\nBEIR\nCovid\nNFCorpus\nTouche\nDBPedia\nSciFact\nSignal\nNews\nRobust\nAvg.\nBM25\n59.47\n30.75\n44.22\n31.80\n67.89\n33.05\n39.52\n40.70\n43.43\nZero-shot Listwise Reranker\nRankGPT-4o\n83.41\n39.67\n32.26\n45.56\n77.41\n34.20\n51.92\n60.25\n53.09\nRankGPT-4o-mini\n80.03\n38.73\n30.91\n44.54\n73.14\n33.64\n50.91\n57.41\n51.16\nRankQwen3-14B\n84.45\n38.94\n38.30\n44.52\n78.64\n33.58\n51.24\n59.66\n53.67\nRankQwen3-32B\n83.48\n39.22\n37.13\n45.00\n78.22\n32.12\n51.08\n60.74\n53.37\nFine-tuned Listwise Reranker based on Qwen3\nRankQwen3-0.6B\n78.35\n36.41\n37.54\n39.19\n71.01\n30.96\n44.43\n46.31\n48.03\nRankQwen3-4B\n83.91\n39.88\n32.66\n43.91\n76.37\n32.15\n50.81\n59.36\n52.38\nRankQwen3-8B\n85.37\n40.05\n31.73\n45.44\n78.96\n32.48\n52.36\n60.72\n53.39\nOurs\nE2Rank-0.6B\n79.17\n38.60\n41.91\n41.96\n73.43\n35.26\n52.75\n53.67\n52.09\nE2Rank-4B\n83.30\n39.20\n43.16\n42.95\n77.19\n34.48\n52.71\n60.16\n54.14\nE2Rank-8B\n84.09\n39.08\n42.06\n43.44\n77.49\n34.01\n54.25\n60.34\n54.35\nEmbedding Benchmark\nMTEB (Eng, v1)\nModels\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\nInstructor-xl\n49.26\n57.29\n44.74\n86.62\n73.12\n83.06\n32.32\n61.79\nBGE-large-en-v1.5\n54.29\n60.03\n46.08\n87.12\n75.97\n83.11\n31.61\n64.23\nGritLM-7B\n53.10\n61.30\n48.90\n86.90\n77.00\n82.80\n29.40\n64.70\nE5-Mistral-7b-v1\n52.78\n60.38\n47.78\n88.47\n76.80\n83.77\n31.90\n64.56\nEcho-Mistral-7b-v1\n55.52\n58.14\n46.32\n87.34\n77.43\n82.56\n30.73\n64.68\nLLM2Vec-Mistral-7B\n55.99\n58.42\n45.54\n87.99\n76.63\n84.09\n29.96\n64.80\nLLM2Vec-Meta-LLaMA-3-8B\n56.63\n59.68\n46.45\n87.80\n75.92\n83.58\n30.94\n65.01\nE2Rank-0.6B\n51.74\n55.97\n40.85\n83.93\n73.66\n81.41\n30.90\n61.25\nE2Rank-4B\n55.33\n59.10\n44.27\n87.14\n77.08\n84.03\n30.06\n64.47\nE2Rank-8B\n56.89\n59.58\n44.75\n86.96\n76.81\n84.52\n30.23\n65.03\nNote: For baselines, we only compared with models that are trained using public datasets.\nüö© Citation\nIf this work is helpful, please kindly cite as:\n@misc{liu2025e2rank,\ntitle={E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker},\nauthor={Qi Liu and Yanzhao Zhang and Mingxin Li and Dingkun Long and Pengjun Xie and Jiaxin Mao},\nyear={2025},\neprint={2510.22733},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2510.22733},\n}\nIf you have any questions, feel free to contact us via qiliu6777[AT]gmail.com or create an issue.",
    "Alibaba-NLP/E2Rank-4B": "üìå Introduction\nüöÄ Quick Start\nModel List\nUsage\nEmbedding Model\nReranking\nEnd-to-end search\nüìä Evaluation\nReranking Benchmark\nBEIR\nEmbedding Benchmark\nMTEB (Eng, v1)\nüö© Citation\nE2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker\nü§ñ Website |\nüìÑ Arxiv Paper |\nü§ó Huggingface Collection |\nüö© Citation\nüìå Introduction\nWe introduce E2Rank,\nmeaning Efficient Embedding-based Ranking\n(also meaning Embedding-to-Rank),\nwhich extends a single text embedding model\nto perform both high-quality retrieval and listwise reranking,\nthereby achieving strong effectiveness with remarkable efficiency.\nBy applying cosine similarity between the query and\ndocument embeddings as a unified ranking function, the listwise ranking prompt,\nwhich is constructed from the original query and its candidate documents, serves\nas an enhanced query enriched with signals from the top-K documents, akin to\npseudo-relevance feedback (PRF) in traditional retrieval models. This design\npreserves the efficiency and representational quality of the base embedding model\nwhile significantly improving its reranking performance.\nEmpirically, E2Rank achieves state-of-the-art results on the BEIR reranking benchmark\nand demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark,\nwith very low reranking latency. We also show that the ranking training process\nimproves embedding performance on the MTEB benchmark.\nOur findings indicate that a single embedding model can effectively unify retrieval and reranking,\noffering both computational efficiency and competitive ranking accuracy.\nOur work highlights the potential of single embedding models to serve as unified retrieval-reranking engines, offering a practical, efficient, and accurate alternative to complex multi-stage ranking systems.\nüöÄ Quick Start\nModel List\nSupported Task\nModel Name\nSize\nLayers\nSequence Length\nEmbedding Dimension\nInstruction Aware\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-0.6B\n0.6B\n28\n32K\n1024\nYes\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-4B\n4B\n36\n32K\n2560\nYes\nEmbedding + Reranking\nAlibaba-NLP/E2Rank-8B\n8B\n36\n32K\n4096\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n0.6B\n28\n32K\n1024\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n4B\n36\n32K\n2560\nYes\nEmbedding Only\nAlibaba-NLP/E2Rank-0.6B-Embedding-Only\n8B\n36\n32K\n4096\nYes\nNote:\nEmbedding Only indicates that the model is trained only with the constrative learning and support embedding tasks, while Embedding + Reranking indicates the full E2Rank model trained with both embedding and reranking objectives (for more detals, please refer to the paper).\nInstruction Aware notes whether the model supports customizing the input instruction according to different tasks.\nUsage\nEmbedding Model\nThe usage of E2Rank as an embedding model is similar to Qwen3-Embedding. The only difference is that Qwen3-Embedding will automatically append an EOS token, while E2Rank requires users to manully append the special token <|endoftext|> at the end of each input text.\nvLLM Usage (recommended)\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ninput_texts = [t + \"<|endoftext|>\" for t in input_texts]\nmodel = LLM(\nmodel=\"Alibaba-NLP/E2Rank-4B\",\ntask=\"embed\",\noverride_pooler_config=PoolerConfig(pooling_type=\"LAST\", normalize=True)\n)\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ninput_texts = [t + \"<|endoftext|>\" for t in input_texts]\ntokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/E2Rank-4B', padding_side='left')\nmodel = AutoModel.from_pretrained('Alibaba-NLP/E2Rank-4B')\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nwith torch.no_grad():\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nReranking\nFor using E2Rank as a reranker, you only need to perform additional processing on the query by adding (part of) the docs that needs to be reranked to the listwise prompt, while the rest is the same as using the embedding model.\nvLLM Usage (recommended)\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\nmodel = LLM(\nmodel=\"./checkpoints/E2Rank-4B\",\ntask=\"embed\",\noverride_pooler_config=PoolerConfig(pooling_type=\"LAST\", normalize=True)\n)\ntokenizer = model.get_tokenizer()\ndef get_listwise_prompt(task_description: str, query: str, documents: list[str], num_input_docs: int = 20) -> str:\ninput_docs = documents[:num_input_docs]\ninput_docs = \"\\n\".join([f\"[{i}] {doc}\" for i, doc in enumerate(input_docs, start=1)])\nmessages = [{\n\"role\": \"user\",\n\"content\": f'{task_description}\\nDocuments:\\n{input_docs}Search Query:{query}'\n}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False,\n)\nreturn text\ntask = 'Given a web search query and some relevant documents, rerank the documents that answer the query:'\nqueries = [\n'What is the capital of China?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ndocuments = [doc + \"<|endoftext|>\" for doc in documents]\npseudo_queries = [\nget_listwise_prompt(task, queries[0], documents),\nget_listwise_prompt(task, queries[1], documents)\n]  # no need to add the EOS token here\ninput_texts = pseudo_queries + documents\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('./checkpoints/E2Rank-4B', padding_side='left')\nmodel = AutoModel.from_pretrained('./checkpoints/E2Rank-4B')\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_listwise_prompt(task_description: str, query: str, documents: list[str], num_input_docs: int = 20) -> str:\ninput_docs = documents[:num_input_docs]\ninput_docs = \"\\n\".join([f\"[{i}] {doc}\" for i, doc in enumerate(input_docs, start=1)])\nmessages = [{\n\"role\": \"user\",\n\"content\": f'{task_description}\\nDocuments:\\n{input_docs}Search Query:{query}'\n}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False,\n)\nreturn text\ntask = 'Given a web search query and some relevant documents, rerank the documents that answer the query:'\nqueries = [\n'What is the capital of China?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ndocuments = [doc + \"<|endoftext|>\" for doc in documents]\npseudo_queries = [\nget_listwise_prompt(task, queries[0], documents),\nget_listwise_prompt(task, queries[1], documents)\n]  # no need to add the EOS token here\ninput_texts = pseudo_queries + documents\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nwith torch.no_grad():\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\nEnd-to-end search\nSince E2Rank extends a single text embedding model to perform both high-quality retrieval and listwise reranking, you can directly use it to build an end-to-end search system. By reusing the embeddings computed during the retrieval stage, E2Rank only need to compute the pseudo query's embedding and can efficiently rerank the retrieved documents with minimal additional computational overhead.\nExample code is coming soon.\nüìä Evaluation\nReranking Benchmark\nBEIR\nCovid\nNFCorpus\nTouche\nDBPedia\nSciFact\nSignal\nNews\nRobust\nAvg.\nBM25\n59.47\n30.75\n44.22\n31.80\n67.89\n33.05\n39.52\n40.70\n43.43\nZero-shot Listwise Reranker\nRankGPT-4o\n83.41\n39.67\n32.26\n45.56\n77.41\n34.20\n51.92\n60.25\n53.09\nRankGPT-4o-mini\n80.03\n38.73\n30.91\n44.54\n73.14\n33.64\n50.91\n57.41\n51.16\nRankQwen3-14B\n84.45\n38.94\n38.30\n44.52\n78.64\n33.58\n51.24\n59.66\n53.67\nRankQwen3-32B\n83.48\n39.22\n37.13\n45.00\n78.22\n32.12\n51.08\n60.74\n53.37\nFine-tuned Listwise Reranker based on Qwen3\nRankQwen3-0.6B\n78.35\n36.41\n37.54\n39.19\n71.01\n30.96\n44.43\n46.31\n48.03\nRankQwen3-4B\n83.91\n39.88\n32.66\n43.91\n76.37\n32.15\n50.81\n59.36\n52.38\nRankQwen3-8B\n85.37\n40.05\n31.73\n45.44\n78.96\n32.48\n52.36\n60.72\n53.39\nOurs\nE2Rank-0.6B\n79.17\n38.60\n41.91\n41.96\n73.43\n35.26\n52.75\n53.67\n52.09\nE2Rank-4B\n83.30\n39.20\n43.16\n42.95\n77.19\n34.48\n52.71\n60.16\n54.14\nE2Rank-8B\n84.09\n39.08\n42.06\n43.44\n77.49\n34.01\n54.25\n60.34\n54.35\nEmbedding Benchmark\nMTEB (Eng, v1)\nModels\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\nInstructor-xl\n49.26\n57.29\n44.74\n86.62\n73.12\n83.06\n32.32\n61.79\nBGE-large-en-v1.5\n54.29\n60.03\n46.08\n87.12\n75.97\n83.11\n31.61\n64.23\nGritLM-7B\n53.10\n61.30\n48.90\n86.90\n77.00\n82.80\n29.40\n64.70\nE5-Mistral-7b-v1\n52.78\n60.38\n47.78\n88.47\n76.80\n83.77\n31.90\n64.56\nEcho-Mistral-7b-v1\n55.52\n58.14\n46.32\n87.34\n77.43\n82.56\n30.73\n64.68\nLLM2Vec-Mistral-7B\n55.99\n58.42\n45.54\n87.99\n76.63\n84.09\n29.96\n64.80\nLLM2Vec-Meta-LLaMA-3-8B\n56.63\n59.68\n46.45\n87.80\n75.92\n83.58\n30.94\n65.01\nE2Rank-0.6B\n51.74\n55.97\n40.85\n83.93\n73.66\n81.41\n30.90\n61.25\nE2Rank-4B\n55.33\n59.10\n44.27\n87.14\n77.08\n84.03\n30.06\n64.47\nE2Rank-8B\n56.89\n59.58\n44.75\n86.96\n76.81\n84.52\n30.23\n65.03\nNote: For baselines, we only compared with models that are trained using public datasets.\nüö© Citation\nIf this work is helpful, please kindly cite as:\n@misc{liu2025e2rank,\ntitle={E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker},\nauthor={Qi Liu and Yanzhao Zhang and Mingxin Li and Dingkun Long and Pengjun Xie and Jiaxin Mao},\nyear={2025},\neprint={2510.22733},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2510.22733},\n}\nIf you have any questions, feel free to contact us via qiliu6777[AT]gmail.com or create an issue.",
    "Chinastark/DatA-SQL-3B": "Performance on the BIRD Development Set\nPerformance on the BIRD Development Set\nWe further evaluate DatA-SQL-3B on the BIRD development set using different self-consistency voting sizes.Under Vote@8, our model attains an execution accuracy (EX) of 61.05 %.\nWhen the voting size increases to Vote@32, the EX further improves to 62.58 %.\nThese results confirm that larger voting ensembles enhance semantic robustness and execution stability while maintaining nearly the same inference cost due to our lightweight multi-agent design.Overall, DatA-SQL achieves competitive or superior accuracy compared with GPT-based pipelines at only a fraction of their computational expense.",
    "Frywind/GREAM": "üß† GREAM: Generative Reasoning Recommendation Model\nüß© Model Summary\nKey Features\nüß† Model Architecture\nüìö Training Data\nüìä Evaluation\nDatasets\nCitation\nüß† GREAM: Generative Reasoning Recommendation Model\nPaper: Generative Reasoning Recommendation via LLMs, 2025.Authors: Minjie Hong*, Zetong Zhou*, Zirun Guo, Ziang Zhang, Ruofan Hu, Weinan Gan, Jieming Zhu, Zhou Zhao‚Ä†Repository: https://github.com/Indolent-Kawhi/GRRMHF Papers Link: https://huggingface.co/papers/2510.20815\nüß© Model Summary\nGREAM (Generative Reasoning Recommendation Model) is a large language model (LLM)-based generative reasoning recommender designed to unify understanding, reasoning, and prediction for recommendation tasks.It introduces a reasoning-enhanced, verifiable reinforcement learning framework that allows both high-throughput direct recommendations and interpretable reasoning-based outputs.\nKey Features\nCollaborative‚ÄìSemantic Alignment: Fuses textual (titles, descriptions, reviews) and behavioral signals to align linguistic and collaborative semantics.\nReasoning Curriculum Activation: Builds synthetic Chain-of-Thought (CoT) data and trains via curriculum to develop causal reasoning for recommendations.\nSparse-Regularized Group Policy Optimization (SRPO): Enables stable RL fine-tuning using Residual-Sensitive Verifiable Rewards and Bonus-Calibrated Group Advantage Estimation for sparse feedback.\nüß† Model Architecture\nComponent\nDescription\nBackbone\nQwen3-4B-Instruct\nIndexing\nResidual Quantization (RQ-KMeans, 5 levels, 256 values per level)\nTraining Phases\n‚ë† Collaborative‚ÄìSemantic Alignment ‚Üí ‚ë° Reasoning Curriculum Activation ‚Üí ‚ë¢ SRPO Reinforcement Learning\nInference Modes\n- Direct Sequence Recommendation: low-latency item generation - Sequential Reasoning Recommendation: interpretable CoT reasoning chains\nRL Framework\nVerl + SGLang backend\nüìö Training Data\nData Type\nSource\nDescription\nDalign\nAmazon Review Datasets (Beauty, Sports, Instruments)\nSequential, semantic reconstruction, and preference understanding tasks\nDreason\nSynthetic CoT data generated via GPT-5 / Qwen3-30B / Llama-3.1\nMulti-step reasoning sequences with <think>...</think> and <answer>...</answer> supervision\nText Sources\nItem titles, descriptions, and high-quality reviews\nCombined and rewritten to form dense item semantics\nüìä Evaluation\nDatasets\nAmazon-Beauty\nAmazon-Sports & Outdoors\nAmazon-Musical Instruments\nCitation\n@misc{hong2025generativereasoningrecommendationllms,\ntitle={Generative Reasoning Recommendation via LLMs},\nauthor={Minjie Hong and Zetong Zhou and Zirun Guo and Ziang Zhang and Ruofan Hu and Weinan Gan and Jieming Zhu and Zhou Zhao},\nyear={2025},\neprint={2510.20815},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2510.20815},\n}",
    "sm54/GLM-4.6-REAP-218B-A32B-MXFP4_MOE": "README.md exists but content is empty.",
    "Wwayu/GLM-4.6-REAP-218B-A32B-mlx-3Bit": "Wwayu/GLM-4.6-REAP-218B-A32B-mlx-3Bit\nUse with mlx\nWwayu/GLM-4.6-REAP-218B-A32B-mlx-3Bit\nThe Model Wwayu/GLM-4.6-REAP-218B-A32B-mlx-3Bit was converted to MLX format from cerebras/GLM-4.6-REAP-218B-A32B using mlx-lm version 0.26.4.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"Wwayu/GLM-4.6-REAP-218B-A32B-mlx-3Bit\")\nprompt=\"hello\"\nif hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "electron271/graig-code-turbo-fast-slow-4.5-mini": "DISCLAIMER: DO NOT USE THIS IN PUBLIC DEPLOYMENTS WE ARE NOT RESPONSIBLE FOR WHAT THIS MODEL IN PARTICULAR SAYS\nTHIS IS AN EXPERIMENT\ngraig-code-turbo-fast-slow-4.5-mini\nRecommended Settings\nDISCLAIMER: DO NOT USE THIS IN PUBLIC DEPLOYMENTS WE ARE NOT RESPONSIBLE FOR WHAT THIS MODEL IN PARTICULAR SAYS\nTHIS IS AN EXPERIMENT\ngraig-code-turbo-fast-slow-4.5-mini\nthe latest state of the art model in the field of accuracy\nother companies may be trying to reach artificial general intelligence, but we are trying to reach artificial grain intelligence. with the help of our team of the best grain farmers in the world, we are making huge strides in the field. fine tuned fully locally using a RX 9070 XT using unsloth.\nollama run hf.co/electron271/graig-code-turbo-fast-slow-4.5-mini:F16\nRecommended Settings\ntemperature = 0.6\ntop_k = 20\nmin_p = 0.00 (llama.cpp's default is 0.1)\ntop_p = 0.95\npresence_penalty = 0.0 to 2.0 (llama.cpp default turns it off, but to reduce repetitions, you can use this) Try 1.0 for example.\nSupports up to 131,072 context natively but you can set it to 32,768 tokens for less RAM use\nyou can also use /no_think for extra chaoticness",
    "marin-community/marin-32b-base": "Model Card for Marin 32B\nDatasets\nDatasets used in Marin 32B Base\nCheckpoints\nBase Model Checkpoints\nInstallation\nInference\nModel Description\nModel Sources\nEvaluation\nModel Details\nArchitecture Details\nTokenizer Details\nTraining Phases\nBias, Risks, and Limitations\nModel Card Contact\nAcknowledgements\nModel Card for Marin 32B\nThis is the model card for the Marin 32B base model. The Marin Project is a collaborative effort to develop open-source foundation models.\nDatasets\nDatasets used in Marin 32B Base\nMarin 32B Base was trained in multiple phases that reused our 8B recipe and introduced new high-quality cooldown data:\nNemotron-CC (medium, medium_low, medium_high, low_actual, low_synth, hq_actual, hq_synth)\nStarcoder Data\nProofpile 2\nFineMath 3+\nDolma components accessed via Dolmino, including:\npeS2o\nFLAN\nStackExchange mixtures\nWikipedia slices\nMarin Markdownified StackExchange\nMarin Markdownified Wikipedia\nMarin Markdownified Ar5iv (No Problem)\nMarin Markdownified Ar5iv (Warnings)\nMarin Datashop Science QA\nMegaMath (web, text_code_block, web_pro, translated_code, QA splits)\nCommon Pile Stack V2 EDU (filtered Python)\nThe Markdownified datasets are licensed under the original licenses of the individual documents.\nPlease refer to StackExchange,\nWikipedia,\nand arXiv for more information.\nThe Datashop Science QA dataset is licensed under CC-BY-SA 4.0.\nCheckpoints\nWe release multiple checkpoints covering the major training phases.\nBase Model Checkpoints\nMain Page: marin-community/marin-32b-base\nmain currently refers to the mantis revision.\nInstallation\nMarin 32B follows a Llama-style transformer architecture with QK-Norm attention (matching the Qwen3 32B backbone) and works out-of-the-box with the Hugging Face Transformers library and other libraries that support Llama/Qwen-style causal language models.\nWe use the stanford-crfm/marin-tokenizer tokenizer.\nInference\nYou can use Marin 32B with the standard Hugging Face Transformers library:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmarin = AutoModelForCausalLM.from_pretrained(\"marin-community/marin-32b-base\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"marin-community/marin-32b-base\")\nmessage = [\"The Marin wind is\"]\ninputs = tokenizer(message, return_tensors=\"pt\", return_token_type_ids=False)\nresponse = marin.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\nModel Description\nDeveloped by: The Marin team at Stanford CRFM.\nModel type: Transformer-style autoregressive language model.\nKnowledge Cutoff: To the best of our knowledge, the base model has no data from later than July 2024.\nLanguage(s) (NLP): English\nLicense: The code and model are released under Apache 2.0.\nContact: dlwh at stanford.edu\nModel Sources\nProject Page: https://marin.community\nRepositories:\nCore repo (data and experiment management): https://github.com/marin-community/marin\nTraining code: https://github.com/stanford-crfm/levanter\nRetrospective: https://marin.readthedocs.io/en/latest/reports/marin-32b-retro.html\nW&B Logs: Marin 32B\nEvaluation\nWe evaluate with EleutherAI's LM Eval Harness defaults across a standard suite. Numbers may differ from model cards or other evaluation harnesses due to prompt/format differences. ‚ÄúAverage‚Äù is a simple mean over the shown tasks.\nModel\nAverage\nAGI Eval LSAT-AR\nARC Easy\nARC Challenge\nBoolQ\nCommonSense QA\nCOPA\nHellaSwag\nlambada_openai\nOpenBookQA\nPIQA\nWinoGrande\nWSC\nMMLU\nGPQA\nBBH\nMMLU Pro\nHumanEval\nGSM8K\nMATH\nMarin 32B (Mantis)\n65.2\n24.8\n88.0\n65.7\n89.4\n82.8\n93.0\n86.9\n77.2\n46.4\n85.9\n79.3\n79.5\n74.7\n34.0\n59.6\n45.1\n42.7\n69.1\n15.3\nMarin 32B (Bison)\n63.0\n23.4\n87.8\n65.8\n88.9\n82.3\n94.0\n86.6\n77.4\n46.6\n86.1\n78.6\n82.4\n72.9\n32.1\n55.2\n41.9\n29.3\n54.7\n10.4\nOLMo 2 32B Base\n63.2\n22.6\n85.9\n61.9\n83.0\n78.6\n93.0\n85.9\n78.3\n47.2\n83.1\n78.9\n86.8\n71.9\n32.2\n56.1\n42.0\n23.8\n76.4\n12.7\nQwen 2.5 32B Base\n68.1\n30.4\n80.8\n55.9\n87.7\n88.5\n87.0\n84.1\n77.6\n44.4\n82.4\n75.7\n81.0\n80.8\n39.0\n67.4\n57.9\n48.8\n89.3\n36.3\nGemma 3 27B PT\n65.1\n22.2\n88.2\n65.4\n87.1\n73.4\n93.0\n83.0\n78.1\n45.0\n84.1\n79.0\n91.9\n75.3\n35.7\n61.4\n49.4\n17.6\n82.0\n25.8\nNVIDIA Nemotron Nano 12B v2 Base\n68.6\n28.7\n83.6\n60.6\n84.8\n76.1\n85.0\n81.4\n72.9\n45.8\n82.8\n74.4\n85.4\n77.9\n36.6\n62.0\n53.1\n59.2\n84.1\n68.3\nThe Mantis cooldown improves coding (HumanEval) and math (GSM8K, MATH) performance dramatically compared with the earlier Bison cooldown while maintaining competitive accuracy across general-language benchmarks.\nModel Details\nPlease see our technical retrospective for more details on the pretraining process.\nArchitecture Details\nArchitecture: Qwen3-style 32B with QK-Norm attention\nHidden size: 5120\nFeedforward size: 27648\nNumber of layers: 64\nNumber of attention heads: 40\nNumber of KV heads: 8\nSequence length: 4096\nTokenizer Details\nMarin 32B uses the stanford-crfm/marin-tokenizer. It has the same vocabulary as Llama 3 but bundles a chat template into the base tokenizer for convenience.\nTraining Phases\nPhase 1 ‚Äî Llama Backbone: exp1295_32b trained ‚âà2.679T tokens on Nemotron-CC, Starcoder, and Proofpile 2 using a Llama-3-style 32B configuration on TPU v5p/v4 hardware. Loss spikes around 70k‚Äì80k steps triggered tighter gradient clipping, update-norm clipping, and skip-step logic.\nPhase 2 ‚Äî Recovery Attempts: Diagnostic restarts (exp1390_32b_necro, exp1388, exp1380) rebuilt optimizer state and tried NAdamW and Muon optimizers, but loss spikes recurred.\nPhase 3 ‚Äî QK-Norm Switch: exp1395_qwen3_32b warm-started the Qwen3 32B backbone with QK-Norm attention from the 80k checkpoint and trained ‚âà2.684T tokens, eliminating instability.\nPhase 4 ‚Äî Cooldowns: The Bison cooldown (‚âà1.074T tokens) mixed Nemotron PT with Dolmino HQ data but suffered GSM8K contamination and shuffle artifacts. The Mantis cooldown restarted from 160k steps, introduced a Feistel shuffle, swapped in MegaMath and Common Pile Stack V2 EDU, and trained another ‚âà1.074T tokens to deliver the final release.\nAll released checkpoints except the early Llama phases use exponential moving averages of the model weights.\nBias, Risks, and Limitations\nLike any base language model or fine-tuned model without safety filtering, these models can be prompted to generate harmful or sensitive content. Such content may also be produced unintentionally, especially in cases involving bias, so users should consider the risks when applying this technology. Additionally, many statements from Marin or any LLM can be inaccurate, so responses should be verified.\nMarin 32B has not undergone safety tuning or evaluation. We strongly recommend using this model with caution and considering the risks when applying this technology.\nIn particular, this model is not intended for fully autonomous use.\nModel Card Contact\nFor errors in this model card, please open an issue in this repository. For technical inquiries, please contact dlwh at stanford.edu.\nAcknowledgements\nThe compute for this model was generously provided by Google's TPU Research Cloud.",
    "rafiaa/terraform-cloud-codellama-7b": "terraform-cloud-codellama-7b\nModel Description\nKey Features\nModel Details\nTechnical Specifications\nUses\nDirect Use\nExample Use Cases\nHow to Get Started\nInstallation\nLoading the Model\nUsage Example\nAdvanced Usage\nTraining Details\nTraining Data\nTraining Procedure\nTraining Hyperparameters\nPerformance Comparison\nLimitations and Bias\nKnown Limitations\nRecommendations\nEnvironmental Impact\nCitation\nRelated Models\nModel Card Contact\nAcknowledgments\nterraform-cloud-codellama-7b\nRECOMMENDED MODEL - An advanced LoRA fine-tuned model for comprehensive Terraform infrastructure-as-code generation, supporting multiple cloud providers (AWS, Azure, GCP). This model generates Terraform configurations, HCL code, and multi-cloud infrastructure automation scripts.\nModel Description\nThis is the enhanced model - an advanced version of terraform-codellama-7b that has been additionally trained on AWS, Azure, and GCP public documentation. It provides superior performance for multi-cloud Terraform development with deep understanding of cloud provider-specific resources and best practices.\nKey Features\nMulti-Cloud Support: Trained on AWS, Azure, and GCP documentation\nEnhanced Performance: Superior to the base terraform-codellama-7b model\nProduction Ready: Optimized for real-world multi-cloud infrastructure development\nComprehensive Coverage: Handles complex cloud provider-specific configurations\nEfficient Training: Uses QLoRA (4-bit quantization + LoRA) for memory efficiency\nModel Details\nDeveloped by: Rafi Al Attrach, Patrick Schmitt, Nan Wu, Helena Schneider, Stefania Saju (TUM + IBM Research Project)\nModel type: LoRA fine-tuned CodeLlama (Enhanced)\nLanguage(s): English\nLicense: Apache 2.0\nFinetuned from: codellama/CodeLlama-7b-Instruct-hf\nTraining method: QLoRA (4-bit quantization + LoRA)\nBase Model: Built on rafiaa/terraform-codellama-7b\nTechnical Specifications\nBase Model: CodeLlama-7b-Instruct-hf\nLoRA Rank: 64\nLoRA Alpha: 16\nTarget Modules: q_proj, v_proj\nTraining Epochs: 3 (Stage 1) + Additional training (Stage 2)\nMax Sequence Length: 512\nQuantization: 4-bit (fp4)\nUses\nDirect Use\nThis model is designed for:\nMulti-cloud Terraform development\nAWS resource configuration (EC2, S3, RDS, Lambda, etc.)\nAzure resource management (Virtual Machines, Storage Accounts, App Services, etc.)\nGCP resource deployment (Compute Engine, Cloud Storage, Cloud SQL, etc.)\nComplex infrastructure orchestration\nCloud provider-specific best practices\nExample Use Cases\n# Generate AWS multi-service infrastructure\nprompt = \"Create a Terraform configuration for an AWS application with VPC, EC2, RDS, and S3\"\n# Generate Azure App Service with database\nprompt = \"Create a Terraform configuration for an Azure App Service with PostgreSQL database\"\n# Generate GCP Kubernetes cluster\nprompt = \"Create a Terraform configuration for a GCP GKE cluster with node pools\"\n# Generate multi-cloud setup\nprompt = \"Create a Terraform configuration for a hybrid cloud setup using AWS and Azure\"\nHow to Get Started\nInstallation\npip install transformers torch peft accelerate bitsandbytes\nLoading the Model\nGPU Usage (Recommended)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n# Load base model with 4-bit quantization (GPU)\nbase_model = \"codellama/CodeLlama-7b-Instruct-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\nbase_model,\nload_in_4bit=True,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\"\n)\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(model, \"rafiaa/terraform-cloud-codellama-7b\")\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Set pad token\nif tokenizer.pad_token is None:\ntokenizer.pad_token = tokenizer.eos_token\nCPU Usage (Alternative)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n# Load base model (CPU compatible)\nbase_model = \"codellama/CodeLlama-7b-Instruct-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\nbase_model,\ntorch_dtype=torch.float32,\ndevice_map=\"cpu\"\n)\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(model, \"rafiaa/terraform-cloud-codellama-7b\")\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Set pad token\nif tokenizer.pad_token is None:\ntokenizer.pad_token = tokenizer.eos_token\nUsage Example\ndef generate_terraform(prompt, max_length=512):\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model.generate(\n**inputs,\nmax_length=max_length,\ntemperature=0.7,\ndo_sample=True,\npad_token_id=tokenizer.eos_token_id\n)\nreturn tokenizer.decode(outputs[0], skip_special_tokens=True)\n# Example: Multi-cloud infrastructure\nprompt = \"\"\"\nCreate a Terraform configuration for a multi-cloud setup:\n- AWS: VPC with public/private subnets, EC2 instances\n- Azure: Storage account and App Service\n- GCP: Cloud SQL database\n\"\"\"\nresult = generate_terraform(prompt)\nprint(result)\nAdvanced Usage\n# Cloud-specific prompts\naws_prompt = \"Create a Terraform configuration for AWS EKS cluster with managed node groups\"\nazure_prompt = \"Create a Terraform configuration for Azure Kubernetes Service (AKS)\"\ngcp_prompt = \"Create a Terraform configuration for GCP Cloud Run service\"\n# Generate configurations\naws_config = generate_terraform(aws_prompt)\nazure_config = generate_terraform(azure_prompt)\ngcp_config = generate_terraform(gcp_prompt)\nTraining Details\nTraining Data\nStage 1: Public Terraform Registry documentation\nStage 2: Additional training on:\nAWS Documentation: EC2, S3, RDS, Lambda, VPC, IAM, etc.\nAzure Documentation: Virtual Machines, Storage Accounts, App Services, Key Vault, etc.\nGCP Documentation: Compute Engine, Cloud Storage, Cloud SQL, GKE, etc.\nTraining Procedure\nMethod: QLoRA (4-bit quantization + LoRA)\nTwo-Stage Training:\nTerraform Registry documentation\nCloud provider documentation (AWS, Azure, GCP)\nLoRA Rank: 64\nLoRA Alpha: 16\nTarget Modules: q_proj, v_proj\nTraining Epochs: 3 (Stage 1) + Additional training (Stage 2)\nMax Sequence Length: 512\nQuantization: 4-bit (fp4)\nTraining Hyperparameters\nTraining regime: 4-bit mixed precision\nLoRA Dropout: 0.0\nLearning Rate: Optimized for QLoRA training\nBatch Size: Optimized for memory efficiency\nPerformance Comparison\nModel\nTerraform Knowledge\nAWS Support\nAzure Support\nGCP Support\nMulti-Cloud Capability\nterraform-codellama-7b\nExcellent\nLimited\nLimited\nLimited\nBasic\nterraform-cloud-codellama-7b\nExcellent\nExcellent\nExcellent\nExcellent\nAdvanced\nLimitations and Bias\nKnown Limitations\nContext Length: Limited to 512 tokens due to training configuration\nDomain Specificity: Optimized for Terraform and cloud infrastructure\nBase Model Limitations: Inherits limitations from CodeLlama-7b-Instruct-hf\nCloud Provider Updates: May not include the latest cloud provider features\nRecommendations\nUse for Terraform and cloud infrastructure tasks\nValidate generated configurations before deployment\nConsider the 512-token context limit for complex configurations\nFor production use, always review and test generated code\nStay updated with cloud provider documentation for latest features\nEnvironmental Impact\nTraining Method: QLoRA reduces computational requirements significantly\nHardware: Trained using efficient 4-bit quantization\nCarbon Footprint: Reduced compared to full fine-tuning due to QLoRA efficiency\nTwo-Stage Approach: Efficient incremental training\nCitation\nIf you use this model in your research, please cite:\n@misc{terraform-cloud-codellama-7b,\ntitle={terraform-cloud-codellama-7b: A Multi-Cloud LoRA Fine-tuned Model for Terraform Code Generation},\nauthor={Rafi Al Attrach and Patrick Schmitt and Nan Wu and Helena Schneider and Stefania Saju},\nyear={2024},\nurl={https://huggingface.co/rafiaa/terraform-cloud-codellama-7b}\n}\nRelated Models\nBase Model: codellama/CodeLlama-7b-Instruct-hf\nStage 1 Model: rafiaa/terraform-codellama-7b\nThis Model: rafiaa/terraform-cloud-codellama-7b (Recommended)\nModel Card Contact\nAuthor: rafiaa\nModel Repository: HuggingFace Model\nIssues: Please report issues through the HuggingFace model page\nAcknowledgments\nResearch Project: Early 2024 research project at TUM + IBM\nTraining Data: Public documentation from Terraform Registry, AWS, Azure, and GCP\nBase Model: Meta's CodeLlama-7b-Instruct-hf\nTraining Method: QLoRA for efficient fine-tuning\nThis model represents the culmination of a two-stage fine-tuning approach, combining Terraform expertise with comprehensive cloud provider knowledge for superior infrastructure-as-code generation.",
    "mlx-community/LLaDA2.0-flash-preview-4bit": "mlx-community/LLaDA2.0-flash-preview-4bit\nUse with mlx\nmlx-community/LLaDA2.0-flash-preview-4bit\nThis model mlx-community/LLaDA2.0-flash-preview-4bit was\nconverted to MLX format from inclusionAI/LLaDA2.0-flash-preview\nusing mlx-lm version 0.28.4.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/LLaDA2.0-flash-preview-4bit\")\nprompt = \"hello\"\nif tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "armand0e/gpt-oss-20b-glm-4.6-distill": "Uploaded finetuned  model\nUploaded finetuned  model\nDeveloped by: armand0e\nLicense: apache-2.0\nFinetuned from model : unsloth/gpt-oss-20b-unsloth-bnb-4bit\nThis gpt_oss model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "BeaverAI/Cydonia-24B-v4y-GGUF": "No model card",
    "boltzgen/boltzgen-1": "BoltzGen\nBoltzGen\nBoltzGen-1 is a generative model for designing proteins and peptides that bind arbitrary biomolecules.",
    "CodeGoat24/UnifiedReward-Edit-qwen-32b": "UnifiedReward-Edit-qwen-32B\nCitation\nUnifiedReward-Edit-qwen-32B\n[2025/10/23] üî•üî•üî• We release UnifiedReward-Edit-32b, a unified reward model for both Text-to-Image and Image-to-Image generation!!\nFor image editing reward task, our models support:\nPairwise Rank ‚Äî directly judge which of two edited images is better.\nPairwise Score ‚Äî assign a separate score to each image in a pair.\nPointwise Score ‚Äî rate a single image on two axes: instruction-following and overall image quality.\nüöÄ The image editing reward inference code is available at UnifiedReward-Edit/ directory, while T2I inference code is unchanged from previous models. The editing training data is preprocessed from EditScore and EditReward and will be released soon. We sincerely appreciate all contributors!!\nFor further details, please refer to the following resources:\nüì∞ Paper: https://arxiv.org/pdf/2503.05236\nü™ê Project Page: https://codegoat24.github.io/UnifiedReward/\nü§ó Model Collections: https://huggingface.co/collections/CodeGoat24/unifiedreward-models-67c3008148c3a380d15ac63a\nü§ó Dataset Collections: https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede\nüëã Point of Contact: Yibin Wang\nCitation\n@article{unifiedreward,\ntitle={Unified reward model for multimodal understanding and generation},\nauthor={Wang, Yibin and Zang, Yuhang and Li, Hao and Jin, Cheng and Wang, Jiaqi},\njournal={arXiv preprint arXiv:2503.05236},\nyear={2025}\n}",
    "Neural-Hacker/NEET_BioBERT": "DistilBERT NEET Biology MCQ Classifier (NEET_BioBERT)\nThis model is a fine-tuned version of DistilBERT (base uncased) specifically trained to classify the correct option for NEET-style multiple-choice biology questions. It selects the best answer among four choices (A, B, C, D).\nTraining Data\nSource: sweatSmile / NEET Biology QA Dataset\nDomain: NEET (Undergraduate Medical Entrance Exam) ‚Äì Biology\nFormat: Each question has 4 options with one correct answer\nDataset Size: 793 questions\nSplit: 80% train / 20% validation\nTraining Configuration\nBase Model:\tdistilbert-base-uncased\nEpochs:\t10\nBatch Size:\t4\nLearning Rate:\t5e-5\nWeight Decay:\t0.01\nTask Type:\tMultiple Choice Classification\nResults\nValidation Accuracy\t72.96% (~73%)\nFinal Training Loss\t~0.35\nLimitations\nTrained on a relatively small dataset (793 questions).\nLimited to NEET-level biology content; not suitable for physics or chemistry.\nDoes not support:\nAssertion-reasoning questions\nDiagram-based questions\nParagraph/Case study type questions\nIntended Use\nEducational Research\nAI-powered NEET Biology assistants\nMCQ practice evaluation\nBaseline model for future fine-tuning with larger datasets\nNOTE:\nNot recommended as a final exam-ready solution without further fine-tuning and validation.\nLicense: MIT",
    "prithivMLmods/Gliese-OCR-7B-Post2.0-final": "Gliese-OCR-7B-Post2.0-final\nKey Enhancements\nQuick Start with Transformers\nIntended Use\nLimitations\nGliese-OCR-7B-Post2.0-final\nThe Gliese-OCR-7B-Post2.0-final model is a refined and optimized version of Gliese-OCR-7B-Post1.0, built upon the Qwen2.5-VL architecture. It represents the final iteration in the Gliese-OCR series, offering enhanced efficiency, precision, and visualization capabilities for document OCR, visual analysis, and information extraction.\nFine-tuned with extended document visualization data and OCR-focused objectives, this model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.\nKey Enhancements\nOptimized Document Visualization and OCR Pipeline: Significantly improved recognition of text, layout, and embedded visuals for structured document understanding.\nContext-Aware Multimodal Linking: Enhanced understanding of document context with stronger alignment between text, images, and layout components.\nRefined Document Retrieval: Improved retrieval accuracy from complex layouts and multi-page documents.\nHigh-Fidelity Content Extraction: Precise extraction of structured, semi-structured, and unstructured information with advanced text normalization.\nAnalytical Recognition: Superior reasoning over charts, graphs, tables, and mathematical equations.\nImproved Visual Reasoning and Layout Awareness: Trained on document visualization datasets for advanced spatial and semantic comprehension.\nState-of-the-Art Performance Across Resolutions: Achieves top results on benchmarks such as DocVQA, InfographicVQA, MathVista, and RealWorldQA.\nExtended Multimodal Duration Support: Handles long document sequences and extended videos (20+ minutes).\nFinal Release Stability: Consolidates all prior improvements for stable and reliable performance.\nQuick Start with Transformers\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Gliese-OCR-7B-Post2.0-final\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Gliese-OCR-7B-Post2.0-final\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"},\n{\"type\": \"text\", \"text\": \"Describe the document structure and extract key text content.\"},\n],\n}\n]\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n).to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=256)\ngenerated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\noutput_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(output_text)\nIntended Use\nDocument visualization and OCR extraction tasks.\nContext-aware document retrieval and multimodal linking.\nExtraction and LaTeX formatting of equations and structured content.\nAnalytical document interpretation (charts, tables, graphs, and figures).\nMultilingual OCR for enterprise, academic, and research use cases.\nSummarization, question answering, and cross-modal reasoning over long documents.\nIntelligent robotic or mobile automation guided by visual document input.\nLimitations\nReduced accuracy on heavily degraded or occluded documents.\nHigh computational requirements for large-scale or real-time applications.\nLimited optimization for low-resource or edge devices.\nOccasional misalignment in text layout or minor hallucinations in outputs.\nPerformance may vary depending on visual token configuration and context length settings.",
    "tarun7r/vibevoice-hindi-lora": "VibeVoice-Hindi-LoRA\nModel Description\nBase Model\nFine-tuning Details\nDemo\nSample Output:\nWhat's Included\nUsage\nDemo and Inference Code\nHindi Inference\nUsing with VibeVoice Inference Pipeline\nHindi Inference with Gradio Demo\nLaunch the Gradio Demo:\nUsing the Web Interface:\nModel Capabilities\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nLicense & Redistribution Notice\nAcknowledgments\nContact\nOther Key Projects\nVibeVoice-Hindi-LoRA\nModel Description\nThis repository contains LoRA (Low-Rank Adaptation) weights for fine-tuning the VibeVoice-7B model for Hindi text-to-speech synthesis. These adapter weights enable efficient fine-tuning of the base VibeVoice model to generate high-quality, natural-sounding Hindi speech without requiring a full model retrain.\nLoRA is a parameter-efficient fine-tuning technique that adds trainable rank decomposition matrices to the model while keeping the original pre-trained weights frozen. This results in significantly smaller model sizes and faster training times.\nBase Model\nBase Model: vibevoice/VibeVoice-7B\nArchitecture: Qwen2.5-7B + Acoustic/Semantic Tokenizers + Diffusion Head\nOriginal Training: English and Chinese speech synthesis\nFine-tuning Details\nTarget Language: Hindi\nMethod: LoRA (Low-Rank Adaptation)\nFine-tuned Components:\nLLM backbone (LoRA adapters)\nDiffusion head (full fine-tuning)\nAcoustic and Semantic connectors\nDemo\nSample Output:\nWhat's Included\nThis repository contains:\nadapter_config.json - LoRA configuration file\nadapter_model.safetensors - LoRA adapter weights for the LLM backbone\ndiffusion_head/ - Full diffusion head weights fine-tuned for Hindi\nacoustic_connector/ - Acoustic connector weights\nsemantic_connector/ - Semantic connector weights\nUsage\nDemo and Inference Code\nFor complete inference examples and demos, please refer to:\nCommunity Repository: vibevoice-community/VibeVoice\nComfyUI Integration: Enemyx-net/VibeVoice-ComfyUI\nHindi Inference\nUsing with VibeVoice Inference Pipeline\n# Clone the community repository\ngit clone https://github.com/vibevoice-community/VibeVoice.git\ncd VibeVoice\n# Install dependencies\nuv pip install -e .\nWith voice cloning (Recommended for Hindi models):\npython demo/inference_from_file.py \\\n--checkpoint_path \"./vibevoice-hindi-lora\" \\\n--txt_path \"./example_hindi_script.txt\" \\\n--model_path \"vibevoice/VibeVoice-7B\" \\\n--speaker_names hi-Priya_woman \\\n--seed 42\nWith multiple speakers:\npython demo/inference_from_file.py \\\n--checkpoint_path \"./vibevoice-hindi-lora\" \\\n--txt_path \"./example_hindi_script.txt\" \\\n--model_path \"vibevoice/VibeVoice-7B\" \\\n--speaker_names \"Speaker1\" \"Speaker2\" \\\n--cfg_scale 1.3\nNote: For voice cloning, ensure you have corresponding voice files in demo/voices/ directory. The script will automatically map speaker names to voice files.\nKey points for Hindi inference:\nWith voice cloning: Specify --speaker_names to map speakers to voice files\nUse --model_path \"vibevoice/VibeVoice-7B\" to match your checkpoint size\nThe model will use the provided voice samples for generation\nVoice samples are loaded and used for voice cloning\nVoice Cloning Setup:\nPlace voice sample files in demo/voices/ directory\nRequired file: hi-Priya_woman.wav - Hindi female voice sample\nUse descriptive filenames like hindi-speaker1.wav, hindi-speaker2.wav\nThe script will automatically map speaker names to voice files\nVoice cloning works best with high-quality, clear voice samples\nModel Architecture Compatibility:\nEnsure your checkpoint matches the model size (7B checkpoint requires --model_path \"vibevoice/VibeVoice-7B\")\nHindi Inference with Gradio Demo\nFor interactive Hindi text generation with a model:\nLaunch the Gradio Demo:\npython demo/gradio_demo.py \\\n--model_path \"vibevoice/VibeVoice-7B\" \\\n--checkpoint_path \"./vibevoice-hindi-lora\" \\\n--device cuda\nUsing the Web Interface:\nEnter your Hindi script in the text area\nSelect speakers (use hi-Priya_woman for Hindi voice)\nClick \"üöÄ Generate Podcast\"\nKey points:\nVoice samples are loaded and used for voice cloning\nThe model will use the provided voice samples for generation\nReal-time streaming audio generation is supported\nWorks with both 1.5B and 7B models (ensure checkpoint matches model size)\nMake sure hi-Priya_woman.wav is in the demo/voices/ directory\nImportant Note: The quality of the generated audio depends heavily on the reference voice file you provide in the demo/voices/ directory. For best results:\nUse high-quality, clear voice samples\nEnsure the reference voice matches the desired speaking style\nLonger reference samples (10-30 seconds) generally produce better results\nThe voice characteristics of the reference sample will be transferred to the generated speech\nModel Capabilities\nText-to-Speech: Convert Hindi text to natural-sounding speech\nMulti-speaker Support: Generate speech with multiple distinct speakers\nLong-form Audio: Synthesize extended audio sequences\nExpressive Speech: Maintain natural prosody and intonation for Hindi\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent ‚Äì cloning a real individual's voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass.\nDisinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events.\nReal‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference \"live deep‚Äëfake\" applications.\nUnsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model. Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content. English and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs. Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects. Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have: Embedded an audible disclaimer (e.g. \"This segment was generated by AI\") automatically into every synthesized audio file. Added an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card. Logged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly. Users are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nLicense & Redistribution Notice\nThis model is released under the MIT License, consistent with the base VibeVoice model.\nRedistribution Notice:\nThis repository contains model weights derived from microsoft/VibeVoice-Large, which is licensed under the MIT License. The MIT License permits redistribution and derivative works.\nMy understanding of the MIT License, which is consistent with the broader open-source community's consensus, is that it grants the right to distribute copies of the software and its derivatives. Therefore, I am lawfully exercising the right to redistribute this model.\nIf you are a rights holder and believe this understanding of the license is incorrect, please submit a DMCA complaint to Hugging Face at dmca@huggingface.co\nAcknowledgments\nBase Model: Microsoft Research for the original VibeVoice model\nFine-tuning Code: vibevoice-community/VibeVoice for the training framework\nTraining Infrastructure: Nebius H100 GPU cluster\nCommunity: Hugging Face and the open-source AI community\nFramework: Built on Qwen2.5, Transformers, and PEFT libraries\nContact\nActively seeking opportunities as an ML Engineer II / Data Scientist II\nFor questions, issues, or collaboration:\nGitHub: tarun7r\nHugging Face: tarun7r\nBase model contact: VibeVoice@microsoft.com\nOther Key Projects\nSpeechAlgo - Comprehensive Speech Processing Algorithms Library\nVocal-Agent - Cascading voice assistant with real-time speech recognition\nFinance-Llama-8B - Financial domain fine-tuned Llama model\nNote: This is a research model. Please use responsibly and in compliance with applicable laws and ethical guidelines.",
    "tarun7r/vibevoice-hindi-7b": "VibeVoice-Hindi-7B\nModel Description\nBase Model\nFine-tuning Details\nDemo\nSample Output:\nUsage\nDemo and Inference Code\nHindi Inference\nUsing with VibeVoice Inference Pipeline\nUsing the dedicated Hindi inference script (Recommended):\nHindi Inference with Gradio Demo\nLaunch the Gradio Demo:\nUsing the Web Interface:\nModel Capabilities\nTraining Details\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nLicense & Redistribution Notice\nAcknowledgments\nContact\nKey Projects\nVibeVoice-Hindi-7B\nModel Description\nVibeVoice-Hindi-7B is a frontier text-to-speech model specifically fine-tuned for Hindi language synthesis. This model is built upon the VibeVoice-7B architecture and has been adapted to generate high-quality, natural, and expressive Hindi speech from text input.\nVibeVoice represents a breakthrough in TTS technology, capable of generating long-form, multi-speaker conversational audio such as podcasts and dialogues. This Hindi variant extends these capabilities to one of the world's most widely spoken languages.\nBase Model\nBase Architecture: vibevoice/VibeVoice-7B\nLLM Backbone: Qwen2.5-7B\nTokenizers: Acoustic (œÉ-VAE) + Semantic tokenizers @ 7.5 Hz\nDiffusion Head: ~600M parameters for high-fidelity acoustic generation\nContext Length: 32K tokens (~45 minutes of audio)\nFine-tuning Details\nTarget Language: Hindi\nMethod: LoRA adapters on LLM + full fine-tuning of diffusion head\nTraining Strategy: Curriculum learning with increasing sequence lengths\nDemo\nSample Output:\nUsage\nDemo and Inference Code\nFor complete inference examples and demos, please refer to:\nCommunity Repository: vibevoice-community/VibeVoice\nComfyUI Integration: Enemyx-net/VibeVoice-ComfyUI\nHindi Inference\nUsing with VibeVoice Inference Pipeline\n# Clone the community repository\ngit clone https://github.com/vibevoice-community/VibeVoice.git\ncd VibeVoice\n# Install dependencies\nuv pip install -e .\nWith voice cloning (Recommended for Hindi models):\npython demo/inference_from_file.py \\\n--model_path \"tarun7r/vibevoice-hindi-7b\" \\\n--txt_path \"./example_hindi_script.txt\" \\\n--speaker_names hi-Priya_woman \\\n--seed 42\nWith multiple speakers:\npython demo/inference_from_file.py \\\n--model_path \"tarun7r/vibevoice-hindi-7b\" \\\n--txt_path \"./example_hindi_script.txt\" \\\n--speaker_names \"Speaker1\" \"Speaker2\" \\\n--cfg_scale 1.3\nNote: For voice cloning, ensure you have corresponding voice files in demo/voices/ directory. The script will automatically map speaker names to voice files.\nUsing the dedicated Hindi inference script (Recommended):\npython inference_hindi_vibevoice.py \\\n--model_path \"tarun7r/vibevoice-hindi-7b\" \\\n--txt_path \"./example_hindi_script.txt\"\nKey points for Hindi inference:\nWith voice cloning: Specify --speaker_names to map speakers to voice files\nUse --model_path \"tarun7r/vibevoice-hindi-7b\" to match your model\nThe model will use the provided voice samples for generation\nVoice samples are loaded and used for voice cloning\nVoice Cloning Setup:\nPlace voice sample files in demo/voices/ directory\nRequired file: hi-Priya_woman.wav - Hindi female voice sample\nUse descriptive filenames like hindi-speaker1.wav, hindi-speaker2.wav\nThe script will automatically map speaker names to voice files\nVoice cloning works best with high-quality, clear voice samples\nModel Architecture Compatibility:\nEnsure your model matches the size (7B model requires --model_path \"tarun7r/vibevoice-hindi-7b\")\nHindi Inference with Gradio Demo\nFor interactive Hindi text generation with a model:\nLaunch the Gradio Demo:\npython demo/gradio_demo.py \\\n--model_path \"tarun7r/vibevoice-hindi-7b\" \\\n--device cuda\nUsing the Web Interface:\nEnter your Hindi script in the text area\nSelect speakers (use hi-Priya_woman for Hindi voice)\nClick \"üöÄ Generate Podcast\"\nKey points:\nVoice samples are loaded and used for voice cloning\nThe model will use the provided voice samples for generation\nReal-time streaming audio generation is supported\nWorks with both 1.5B and 7B models (ensure model matches size)\nMake sure hi-Priya_woman.wav is in the demo/voices/ directory\nImportant Note: The quality of the generated audio depends heavily on the reference voice file you provide in the demo/voices/ directory. For best results:\nUse high-quality, clear voice samples\nEnsure the reference voice matches the desired speaking style\nLonger reference samples (10-30 seconds) generally produce better results\nThe voice characteristics of the reference sample will be transferred to the generated speech\nModel Capabilities\nText-to-Speech: Convert Hindi text to natural-sounding speech\nMulti-speaker Support: Generate speech with multiple distinct speakers\nLong-form Audio: Synthesize extended audio sequences\nExpressive Speech: Maintain natural prosody and intonation for Hindi\nTraining Details\nThis model was fine-tuned using:\nTechnique: LoRA with rank decomposition\nComponents Trained:\nLoRA adapters on the LLM backbone\nFull fine-tuning of diffusion head\nConnector modules for acoustic and semantic features\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent ‚Äì cloning a real individual's voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass.\nDisinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events.\nReal‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference \"live deep‚Äëfake\" applications.\nUnsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model. Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content. English and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs. Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects. Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have: Embedded an audible disclaimer (e.g. \"This segment was generated by AI\") automatically into every synthesized audio file. Added an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card. Logged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly. Users are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nLicense & Redistribution Notice\nThis model is released under the MIT License, consistent with the base VibeVoice model.\nRedistribution Notice:\nThis repository contains model weights derived from microsoft/VibeVoice-Large, which is licensed under the MIT License. The MIT License permits redistribution and derivative works.\nMy understanding of the MIT License, which is consistent with the broader open-source community's consensus, is that it grants the right to distribute copies of the software and its derivatives. Therefore, I am lawfully exercising the right to redistribute this model.\nIf you are a rights holder and believe this understanding of the license is incorrect, please submit a DMCA complaint to Hugging Face at dmca@huggingface.co\nAcknowledgments\nBase Model: Microsoft Research for the original VibeVoice model\nFine-tuning Code: vibevoice-community/VibeVoice for the training framework\nTraining Infrastructure: Nebius H100 GPU cluster\nCommunity: Hugging Face and the open-source AI community\nFramework: Built on Qwen2.5, Transformers, and PEFT libraries\nContact\nActively seeking opportunities as an ML Engineer II / Data Scientist II\nFor questions, issues, or collaboration:\nGitHub: tarun7r\nHugging Face: tarun7r\nBase model contact: VibeVoice@microsoft.com\nKey Projects\nSpeechAlgo - Comprehensive Speech Processing Algorithms Library\nVocal-Agent - Cascading voice assistant with real-time speech recognition\nFinance-Llama-8B - Financial domain fine-tuned Llama model\nNote: This is a research model. Please use responsibly and in compliance with applicable laws and ethical guidelines.",
    "uva-cv-lab/FrameINO_Wan2.2_5B_Stage1_Motion_v1.5": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nREADME.md exists but content is empty.",
    "mrfakename/EmoAct-MiMo": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nAudio samples (thanks to Christoph from LAION for generating them):\nVery early checkpoint, but it's a good model/finetune.\nBase model: MiMo Audio\nPrompt format:\nEmotion: <emotion>\nText: <text>\nExample:\nEmotion: intense anger, rage, fury, hatred, and annoyance, speaking without any accent\nText: You know what? I'm done. I'm done with your excuses. (sharp exhale) Every single time, it's the same, and I actually believed you'd change. (voice cracks slightly) God, I'm such an idiot for trusting you again.\nTraining code (private): https://github.com/fakerybakery/mimo2\nVoice cloning is not supported yet",
    "Tesslate/UIGEN-FX-4B-RL-Preview": "@@@ ALERT @@@ ‚Äî Research Preview: some generations may not be ready for production. Use repeat_penalty or frequency_penalty at >= 1.1\nTesslate ‚Ä¢ UIGEN Series\nUIGEN-FX-4B-RL-Preview\nFX = ‚ÄúFrontend Engineer.‚Äù This upgrade in the UIGEN line focuses on better visual polish,\nfunctional structure, and web-ready markup to ship cleaner, more complete websites from a single prompt.\nThis model was trained on A11y, Axe, and Lighthouse scores to improve SEO, Accesibility, and reproducibility for users.\nOpen weights\nWeb-only bias\nMobile-first output\nMinimal JS by default\nModel Repo\nTry in Designer\nDemos\nWebsite\nDiscord\nWhat is FX?\nUIGEN-FX-4B-Preview is a 4B parameter UI generation model tuned to behave like a\nfrontend engineer across 22 Frameworks.\nWhy 4B?\nSmall enough for laptops and fast iteration, while keeping strong structure and visual consistency.\nFX emphasizes layout rhythm, spacing, and component composition to reduce cleanup work.\nQuickstart\nTransformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_id = \"Tesslate/UIGEN-FX-4B-Preview\"\ntok = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nprompt = \"\"\"Make a single-file landing page for 'LatticeDB'.\nStyle: modern, generous whitespace, Tailwind, rounded-xl, soft gradients.\nSections: navbar, hero (headline + 2 CTAs), features grid, pricing (3 tiers),\nFAQ accordion, footer. Constraints: semantic HTML, no external JS.\"\"\"\ninputs = tok(prompt, return_tensors=\"pt\").to(model.device)\nout = model.generate(**inputs, max_new_tokens=2200, temperature=0.6, top_p=0.9)\nprint(tok.decode(out[0], skip_special_tokens=True))\nvLLM\nvllm serve Tesslate/UIGEN-FX-4B-Preview \\\n--host 0.0.0.0 --port 8000 \\\n--max-model-len 65536 \\\n--gpu-memory-utilization 0.92\nsglang\npython -m sglang.launch_server \\\n--model-path Tesslate/UIGEN-FX-4B-Preview \\\n--host 0.0.0.0 --port 5000 \\\n--mem-fraction-static 0.94 \\\n--attention-backend flashinfer \\\n--served-model-name UIGEN-FX-4B-Preview\nTip: Lower temperature (0.4‚Äì0.6) yields stricter, cleaner markup; raise it for more visual variety.\nSuggested Inference Settings\nParamValueNotes\ntemperature0.6Balance creativity & consistency (lower if quantized)\ntop_p0.9Nucleus sampling\ntop_k40Optional vocab restriction\nmax_new_tokens1200‚Äì2500Single-file sites often fit < 1800\nrepetition_penalty1.08‚Äì1.15Reduces repetitive classes/markup\nPrompts that work well\nStarter\nMake a single-file landing page for \"RasterFlow\" (GPU video pipeline).\nStyle: modern tech, muted palette, Tailwind, rounded-xl, subtle gradients.\nSections: navbar, hero (big headline + 2 CTAs), logos row, features (3x cards),\ncode block (copyable), pricing (3 tiers), FAQ accordion, footer.\nConstraints: semantic HTML, no external JS. Return ONLY the HTML code.\nDesign-strict\nUse an 8pt spacing system. Palette: slate with indigo accents.\nTypography scale: 14/16/18/24/36/56. Max width: 1200px.\nAvoid shadows > md; prefer borders/dividers; keep line-length ~68ch.\nQuantization & VRAM (example)\nFormatFootprintNotes\nBF16~8.1 GBFastest, best fidelity\nGGUF Q5_K_M~2.9 GBGreat quality/size trade-off\nGGUF Q4_K_M~2.5 GBLaptop-friendly\nIntended Use & Scope\nPrimary:React, Tailwind, Javascript, Static Site generators, Python Frontend WebUI\nSecondary: Component blocks (hero, pricing, FAQ) for manual composition\nLimitations\nAccessibility: headings/labels are encouraged; ARIA coverage may need review.\nComplex widgets: JS kept minimal unless requested; consider post-edit for heavy interactivity.\nEthical Considerations\nUse rights-cleared assets when adding logos/images.\nCurate prompts responsibly; review outputs before production use.\nTraining Summary (research preview)\nBase: Qwen/Qwen3-4B-Instruct-2507\nObjective: Web-only bias; reward semantic structure, spacing rhythm, responsive blocks; improved visual polish vs earlier UIGEN releases.\nData: Curated HTML/CSS/Tailwind snippets, component libraries, synthetic page specs & layout constraints.\nRecipe: SFT with format constraints ‚Üí instruction tuning ‚Üí preference optimization on style/structure.\nContext: effective ~64k; default generations sized for practical single-file pages.\nCommunity\nExamples: uigenoutput.tesslate.com\nDiscord: discord.gg/EcCpcTv93U\nWebsite: tesslate.com\n‚ÄúFX aims to ship what a frontend engineer would: clean structure first, pretty pixels second.‚Äù ‚Äî Tesslate Team\nCitation\n@misc{tesslate_uigen_fx_4b_2025,\ntitle   = {UIGEN-FX-4B-Preview: Frontend Engineer-tuned web generation (Research Preview)},\nauthor  = {Tesslate Team},\nyear    = {2025},\nurl     = {https://huggingface.co/Tesslate/UIGEN-FX-4B-Preview}\n}",
    "mradermacher/Impish-LongPen-12B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Retreatcost/Impish-LongPen-12B\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Impish-LongPen-12B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n4.9\nGGUF\nQ3_K_S\n5.6\nGGUF\nQ3_K_M\n6.2\nlower quality\nGGUF\nQ3_K_L\n6.7\nGGUF\nIQ4_XS\n6.9\nGGUF\nQ4_K_S\n7.2\nfast, recommended\nGGUF\nQ4_K_M\n7.6\nfast, recommended\nGGUF\nQ5_K_S\n8.6\nGGUF\nQ5_K_M\n8.8\nGGUF\nQ6_K\n10.2\nvery good quality\nGGUF\nQ8_0\n13.1\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "Crilias/Kaleido-14B-S2V": "KALEIDO: OPEN-SOURCED MULTI-SUBJECT REFERENCE VIDEO GENERATION MODEL\nUpdate and News\nCheckpoints Download\nCitation\nKALEIDO: OPEN-SOURCED MULTI-SUBJECT REFERENCE VIDEO GENERATION MODEL\nThis repository contains the official implementation of Kaleido, proposed in our paper:\nUpdate and News\n2025.10.28: üî• We release the checkpoints of Kaleido-14B-S2V.\n2025.10.22: üî• We propose Kaleido, a novel multi-subject reference video generation model.\nCheckpoints Download\nckpts\nDownload Link\nNotes\nKaleido-14B\nü§ó Huggingface\nSupports  512P\nUse the following commands to download the model weights\n(We have integrated both Wan VAE and T5 modules into this checkpoint for convenience).\n# Download the repository (skip automatic LFS file downloads)\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Crilias/Kaleido-14B-S2V\n# Enter the repository folder\ncd Kaleido-14B-S2V\n# Merge the checkpoint files\npython merge_kaleido.py\nArrange the model files into the following structure:\n.\n‚îú‚îÄ‚îÄ Kaleido-14B-S2V\n‚îÇ   ‚îú‚îÄ‚îÄ model\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ....\n‚îÇ   ‚îú‚îÄ‚îÄ Wan2.1_VAE.pth\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ umt5-xxl\n‚îÇ       ‚îî‚îÄ‚îÄ ....\n‚îú‚îÄ‚îÄ configs\n‚îú‚îÄ‚îÄ sat\n‚îî‚îÄ‚îÄ sgm\nCitation\nIf you find our work helpful, please cite our paper:\n@misc{zhang2025kaleidoopensourcedmultisubjectreference,\ntitle={Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model},\nauthor={Zhenxing Zhang and Jiayan Teng and Zhuoyi Yang and Tiankun Cao and Cheng Wang and Xiaotao Gu and Jie Tang and Dan Guo and Meng Wang},\nyear={2025},\neprint={2510.18573},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.18573},\n}",
    "manuelaschrittwieser/phi-3-mini-sql-assistant-adapter": "QLoRA Adapter for Phi-3-mini: A Technical SQL Assistant\nModel Description\nHow to Use\nTraining Procedure\nEvaluation\nLimitations and Bias\nQLoRA Adapter for Phi-3-mini: A Technical SQL Assistant\nThis repository contains a QLoRA (Parameter-Efficient Fine-Tuning) adapter for the microsoft/Phi-3-mini-4k-instruct model. The adapter was fine-tuned to act as a technical assistant that generates SQL queries from natural language questions, based on a provided database schema.\nThis project was developed as an educational exercise to demonstrate a complete, end-to-end fine-tuning pipeline, from data preparation to model evaluation.\nModel Description\nBase Model: microsoft/Phi-3-mini-4k-instruct\nFine-tuning Method: QLoRA (4-bit NormalFloat quantization)\nTask: Text-to-SQL Generation\nThe model is designed to receive a database schema (as a CREATE TABLE statement) and a user's question in natural language. It then generates the appropriate SQL query to answer that question.\nHow to Use\nFirst, ensure you have the necessary libraries installed:\npip install -U transformers peft accelerate bitsandbytes torch\nNext, use the following Python code.The AutoPeftModelForCausalLM class will automatically load the base model (Phi-3-mini) and\napply this fine-tuned adapter on top.\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\nimport torch\n# Your adapter's ID on the Hugging Face Hub\nadapter_id = \"manuelaschrittwieser/phi-3-mini-sql-assistant-adapter\"\nprint(\"Loading model and adapter...\")\n# Load the fine-tuned model and adapter in one step\nmodel = AutoPeftModelForCausalLM.from_pretrained(\nadapter_id,\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\n)\n# The tokenizer is also loaded from the adapter's repository\ntokenizer = AutoTokenizer.from_pretrained(adapter_id, trust_remote_code=True)\nprint(\"Model loaded successfully!\")\n# --- Define your schema and question ---\ncontext = \"CREATE TABLE employees (name VARCHAR, department VARCHAR, salary INTEGER)\"\nquestion = \"What are the names of employees in the 'Engineering' department with a salary over 80000?\"\n# --- Format the prompt using the Phi-3 chat template ---\nprompt = f\"\"\"<|user|>\nGiven the database schema:\n{context}\nGenerate the SQL query for the following request:\n{question}<|end|>\n<|assistant|>\n\"\"\"\n# --- Generate the response ---\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=False)\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n# --- Safely extract the generated SQL ---\n# This handles cases where the model might forget the <|assistant|> token\nassistant_token = \"<|assistant|>\"\nif assistant_token in generated_text:\ngenerated_sql = generated_text.split(assistant_token)[1].strip()\nelse:\n# Fallback for when the model doesn't follow the template perfectly\n# It removes the original prompt from the generated text\nprompt_for_splitting = prompt.replace(assistant_token, \"\")\ngenerated_sql = generated_text.split(prompt_for_splitting)[-1].strip()\nprint(f\"\\nGenerated SQL: {generated_sql}\")\n# Expected output: SELECT name FROM employees WHERE department = 'Engineering' AND salary > 80000\nTraining Procedure\nDataset\nThe model was fine-tuned on a 10,000-sample subset of the b-mc2/sql-create-context dataset. This dataset provides pairs of database schemas (context), natural language questions (question), and their corresponding SQL queries (answer). The subset was split into 9,000 examples for training and 1,000 for testing.\nFine-tuning Configuration (QLoRA)\nQuantization: 4-bit NormalFloat (NF4) with bfloat16 compute dtype.\nLoRA Rank (r): 8\nLoRA Alpha (lora_alpha): 16\nTarget Modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\nTraining Hyperparameters\nLearning Rate: 2e-4\nEpochs: 1\nBatch Size: 2 (with 4 gradient accumulation steps for an effective batch size of 8)\nOptimizer: Paged AdamW (32-bit)\nLR Scheduler: Cosine\nEvaluation\nThe model's primary goal was to generate syntactically correct and logically sound SQL queries. A qualitative evaluation on the held-out test set showed that the model successfully learned this task, often producing the exact ground truth SQL.\nHowever, after only one epoch of training, the model sometimes fails to perfectly adhere to the chat template format (e.g., omitting the <|assistant|> token), even when the generated SQL itself is correct. Further training would likely improve this formatting consistency.\nLimitations and Bias\nThis model is a proof-of-concept and was trained on a limited dataset for a short duration. It may not generalize well to complex, unseen database schemas or highly intricate SQL queries.\nThe model is not designed to be a chatbot and has no knowledge outside of the Text-to-SQL domain it was trained on.\nThe training data may contain biases, and the model's outputs should be reviewed before execution in a production environment, especially to prevent potential SQL injection vulnerabilities.",
    "manglu3935/img2time": "README.md exists but content is empty.",
    "lefromage/Qwen3-Next-80B-A3B-Thinking-GGUF": "Update:\nQuantized Models:\nHow to build and run for MacOS\nRun examples\nExample prompt and output\nto be used with llama.cpp PR 16095\nUpdate:\nI have tested some of these smaller models on NVIDIA with default CUDA compile\nwith the excellent release from @cturan on NVIDIA L40S GPU.\nSince L40S GPU is 48GB VRAM, I was able to run Q2_K, Q3_K_M, Q4_K_S, Q4_0 and Q4_MXFP4_MOE:\nbut Q4_K_M was too big.\nAlthough it works if using -ngl 45\nbut it slowed down quite a bit.\nThere may be a better way but did not have time to test.\nWas able to get a good speed of 53 tokens per second in the generation\nand 800 tokens per second in the prompt reading.\nwget https://github.com/cturan/llama.cpp/archive/refs/tags/test.tar.gz\ntar xf test.tar.gz\ncd llama.cpp-test\n# export PATH=/usr/local/cuda/bin:$PATH\ntime cmake -B build -DGGML_CUDA=ON\ntime cmake --build build --config Release --parallel $(nproc --all)\nYou may need to add /usr/local/cuda/bin to your PATH\nto find nvcc (Nvidia CUDA compiler)\nBuilding from source took about 7 minutes.\nFor more detail on CUDA build see:\nhttps://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cuda\nQuantized Models:\nThese quantized models were generated using the excellent pull request from @pwilkin\n#16095\non 2025-10-19 with commit 2fdbf16eb.\nNOTE: currently they only work with the llama.cpp 16095 pull request which is still in development.\nSpeed and quality should improve over time.\nHow to build and run for MacOS\nPR=16095\ngit clone https://github.com/ggml-org/llama.cpp llama.cpp-PR-$PR\ncd llama.cpp-PR-$PR\ngit fetch origin pull/$PR/head:pr-$PR\ngit checkout pr-$PR\ntime cmake -B build\ntime cmake --build build --config Release --parallel $(nproc --all)\nRun examples\nRun with Hugging Face model:\nbuild/bin/llama-cli -hf lefromage/Qwen3-Next-80B-A3B-Thinking-GGUF --prompt 'What is the capital of France?' --no-mmap -st\nby default will download lefromage/Qwen3-Next-80B-A3B-Thinking-GGUF:Q4_K_M\nTo download:\nwget https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Thinking-GGUF/resolve/main/Qwen__Qwen3-Next-80B-A3B-Thinking-Q4_0.gguf\nor\npip install hf_transfer 'huggingface_hub[cli]'\nhf download lefromage/Qwen3-Next-80B-A3B-Thinking-GGUF Qwen__Qwen3-Next-80B-A3B-Thinking-Q4_0.gguf\nRun with local model file:\nbuild/bin/llama-cli -m Qwen__Qwen3-Next-80B-A3B-Thinking-Q4_0.gguf --prompt 'Write a paragraph about quantum computing' --no-mmap -st\nbuild/bin/llama-cli -ngl 100 -m Qwen__Qwen3-Next-80B-A3B-Thinking-Q2_K.gguf --no-mmap --prompt 'what is the capital of france' -st\nExample prompt and output\nUser prompt:\nwhat is the capital of france\nAssistant output:\n<think>\nOkay, the user asked, \"what is the capital of France?\" Hmm, that's a pretty basic geography question. Let me think. First, I know the capital of France is Paris. But wait, maybe I should double-check to be absolutely sure. Yeah, definitely Paris. It's one of those common facts that's easy to remember.\nWait, but why would someone ask this? Maybe they're a student doing homework, or maybe a traveler planning a trip. Or perhaps they're just curious. Either way, the answer is straightforward. No need for complex analysis here.\nI recall that Paris has been the capital for a long time, since the Middle Ages. It's not like some countries where the capital changed. France has always had Paris as its capital. Yeah, that's right.\nShould I mention anything else? Like maybe that it's the largest city in France, or that it's known for landmarks like the Eiffel Tower? But the question is just about the capital, so maybe keep it simple. Just confirm Paris.\nWait, is there any chance of confusion? Like, sometimes people think of Marseille or Lyon, but no, those are other major cities but not the capital. Definitely Paris.\nI think the best answer is to state clearly that the capital is Paris. Maybe add a bit about it being a major global city. But keep it short and accurate.\nYeah, I think that's it. No need to overcomplicate. Just give the answer directly. So, \"The capital of France is Paris.\" Done.\n</think>\nThe capital of France is **Paris**.\nParis has been the capital since the 6th century and is France's largest city, as well as a major global center for culture, commerce, and tourism. It is renowned for landmarks like the Eiffel Tower, the Louvre, and Notre-Dame Cathedral. [end of text]\nllama_perf_sampler_print:    sampling time =      33.98 ms /   403 runs   (    0.08 ms per token, 11858.87 tokens per second)\nllama_perf_context_print:        load time =   10380.46 ms\nllama_perf_context_print: prompt eval time =    5709.11 ms /    14 tokens (  407.79 ms per token,     2.45 tokens per second)\nllama_perf_context_print:        eval time =   85045.12 ms /   388 runs   (  219.19 ms per token,     4.56 tokens per second)\nllama_perf_context_print:       total time =   90917.58 ms /   402 tokens\nllama_perf_context_print:    graphs reused =          0\nllama_memory_breakdown_print: | memory breakdown [MiB]   | total    free     self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Metal (Apple M4 Max) | 98304 = 69920 + (28151 = 27675 +     171 +     304) +         232 |\nllama_memory_breakdown_print: |   - Host                 |                    167 =    97 +       0 +      70                |\nggml_metal_free: deallocating\nreal\t1m41.530s",
    "Retreatcost/Darkstar-12B": "Darkstar-12B\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nDarkstar-12B\nI've combined Violet-Lyra-Gutenberg-v2, mini-magnum-12b-v1.1 and MN-Dark-Planet-TITAN-12B using karcher.\nThen I used Dark-Desires-12B-v1.0 as a base and merged it with Darkness-Incarnate-12B-Nemo-v2.2 using arcee_fusion.\nThese intermediate models were combined using nearswap.\nResulting model is pretty NSFW-heavy with graphic descriptions, foul language and tends to create spooky, horror-infused scenarios.\nSometimes shows refusals.\nTried adding abliterated LoRa, while it totally worked, it also watered down the model a lot, so I decided to keep it as is.\nUses Mistral V7 Tekken.\nThis is a merge of pre-trained language models created using mergekit.\nMerge Details\nMerge Method\nThis model was merged using the NearSwap merge method using carnal_desires as a base.\nModels Merged\nThe following models were included in the merge:\ndarkness\nConfiguration\nThe following YAML configuration was used to produce this model:\nmerge_method: nearswap\nbase_model: carnal_desires\nmodels:\n- model: darkness\nparameters:\nt: [0.001, 0.0015, 0.0025, 0.0015, 0.001]\ndtype: bfloat16\ntokenizer_source: carnal_desires",
    "Bedovyy/smoothMixWan22-I2V-GGUF": "README.md exists but content is empty."
}