{
    "chestnutlzj/Edit-R1-FLUX.1-Kontext-dev": "UniWorld-R1\nPerformance\nUsage\nLicence\nUniWorld-R1\nThis model is part of the work presented in the paper Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback.\nCode | Dataset\nPerformance\nBenchmark\nFLUX.1-Kontext-dev\nEdit-R1-FLUX.1-Kontext-dev\nGEdit-Bench\n6.00\n6.74\nImgEdit\n3.71\n4.02\nUsage\nimport torch\nfrom diffusers import FluxKontextPipeline\nfrom diffusers.utils import load_image\npipe = FluxKontextPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Kontext-dev\", torch_dtype=torch.bfloat16)\npipe.load_lora_weights(\n\"chestnutlzj/Edit-R1-FLUX.1-Kontext-dev\",\nadapter_name=\"lora\",\n)\npipe.set_adapters([\"lora\"], adapter_weights=[1])\npipe.to(\"cuda\")\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\nimage = pipe(\nimage=input_image,\nprompt=\"Add a hat to the cat\",\nguidance_scale=2.5\n).images[0]\nLicence\nFLUX.1-Kontext-dev falls under the FLUX.1 [dev] Non-Commercial License.",
    "FabioSarracino/VibeVoice-Large-Q8": "VibeVoice-Large-Q8 - Selective 8bit Quantization\nüéØ Why This Model is Different\nResults\nüö® The Problem with Other 8-bit Models\n‚úÖ The Solution: Selective Quantization\nüìä Quick Comparison\nüíª How to Use It\nWith Transformers\nWith ComfyUI (recommended)\nüíæ System Requirements\nMinimum\nRecommended\n‚ö†Ô∏è Limitations\nüÜö When to Use This Model\n‚úÖ Use this 8-bit if:\nUse full precision (18.7 GB) if:\nUse 4-bit NF4 (~6.6 GB) if:\nüîß Troubleshooting\n\"OutOfMemoryError\" during loading\n\"BitsAndBytes not found\"\nAudio sounds distorted\nüìö Citation\nOriginal Model\nüîó Related Resources\nüìú License\nü§ù Support\nVibeVoice-Large-Q8 - Selective 8bit Quantization\nThe first 8-bit VibeVoice model that actually works\nü§ó Model ‚Ä¢ üíª ComfyUI ‚Ä¢ üìñ Docs\nüéØ Why This Model is Different\nIf you've tried other 8-bit quantized VibeVoice models, you probably got nothing but static noise. This one actually works.\nThe secret? Selective quantization: I only quantized the language model (the most robust part), while keeping audio-critical components (diffusion head, VAE, connectors) at full precision.\nResults\n‚úÖ Perfect audio, identical to the original model\n‚úÖ 11.6 GB instead of 18.7 GB (-38%)\n‚úÖ Uses ~12 GB VRAM instead of 20 GB\n‚úÖ Works on 12 GB GPUs (RTX 3060, 4070 Ti, etc.)\nüö® The Problem with Other 8-bit Models\nMost 8-bit models you'll find online quantize everything aggressively:\nResult: Audio components get quantized ‚Üí numerical errors propagate ‚Üí audio = pure noise.\n‚úÖ The Solution: Selective Quantization\nI only quantized what can be safely quantized without losing quality.\nResult: 52% of parameters quantized, 48% at full precision = perfect audio quality.\nüìä Quick Comparison\nModel\nSize\nAudio Quality\nStatus\nOriginal VibeVoice\n18.7 GB\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\nFull precision\nOther 8-bit models\n10.6 GB\nüí• NOISE\n‚ùå Don't work\nThis model\n11.6 GB\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚úÖ Perfect\n+1.0 GB vs other 8-bit models = perfect audio instead of noise. Worth it.\nüíª How to Use It\nWith Transformers\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nimport torch\nimport scipy.io.wavfile as wavfile\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n\"FabioSarracino/VibeVoice-Large-Q8\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\ntorch_dtype=torch.bfloat16,\n)\nprocessor = AutoProcessor.from_pretrained(\n\"FabioSarracino/VibeVoice-Large-Q8\",\ntrust_remote_code=True\n)\n# Generate audio\ntext = \"Hello, this is VibeVoice speaking.\"\ninputs = processor(text, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, max_new_tokens=None)\n# Save\naudio = output.speech_outputs[0].cpu().numpy()\nwavfile.write(\"output.wav\", 24000, audio)\nWith ComfyUI (recommended)\nInstall the custom node:\ncd ComfyUI/custom_nodes\ngit clone https://github.com/Enemyx-net/VibeVoice-ComfyUI\nDownload this model to ComfyUI/models/vibevoice/\nRestart ComfyUI and use it normally!\nüíæ System Requirements\nMinimum\nVRAM: 12 GB\nRAM: 16 GB\nGPU: NVIDIA with CUDA (required)\nStorage: 11 GB\nRecommended\nVRAM: 16+ GB\nRAM: 32 GB\nGPU: RTX 3090/4090, A5000 or better\n‚ö†Ô∏è Not supported: CPU, Apple Silicon (MPS), AMD GPUs\n‚ö†Ô∏è Limitations\nRequires NVIDIA GPU with CUDA - won't work on CPU or Apple Silicon\nInference only - don't use for fine-tuning\nRequires:\ntransformers>=4.51.3\nbitsandbytes>=0.43.0\nüÜö When to Use This Model\n‚úÖ Use this 8-bit if:\nYou have 12-16 GB VRAM\nYou want maximum quality with reduced size\nYou need a production-ready model\nYou want the best size/quality balance\nUse full precision (18.7 GB) if:\nYou have unlimited VRAM (24+ GB)\nYou're doing research requiring absolute precision\nUse 4-bit NF4 (~6.6 GB) if:\nYou only have 8-10 GB VRAM\nYou can accept a small quality trade-off\nüîß Troubleshooting\n\"OutOfMemoryError\" during loading\nClose other GPU applications\nUse device_map=\"auto\"\nReduce batch size to 1\n\"BitsAndBytes not found\"\npip install bitsandbytes>=0.43.0\nAudio sounds distorted\nThis shouldn't happen! If it does:\nVerify you downloaded the correct model\nUpdate transformers: pip install --upgrade transformers\nCheck CUDA: torch.cuda.is_available() should return True\nüìö Citation\n@misc{vibevoice-q8-2025,\ntitle={VibeVoice-Large-Q8: Selective 8-bit Quantization for Audio Quality},\nauthor={Fabio Sarracino},\nyear={2025},\nurl={https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8}\n}\nOriginal Model\n@misc{vibevoice2024,\ntitle={VibeVoice: High-Quality Text-to-Speech with Large Language Models},\nauthor={Microsoft Research},\nyear={2024},\nurl={https://github.com/microsoft/VibeVoice}\n}\nüîó Related Resources\nOriginal Model - Full precision base\nComfyUI Node - ComfyUI integration\nüìú License\nMIT License.\nü§ù Support\nIssues: GitHub Issues\nQuestions: HuggingFace Discussions\nIf this model helped you, leave a ‚≠ê on GitHub!\nCreated by Fabio Sarracino\nThe first 8-bit VibeVoice model that actually works\nü§ó HuggingFace ‚Ä¢ üíª GitHub",
    "huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated": "huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-30B-A3B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp-tr-qwen3-vl-3-b6981-ab45b1a now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated/ggml-model-Q4_K_M.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated/mmproj-ggml-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated/ggml-model-Q4_K_M.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF": "Qwen3-VL-30B-A3B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nCitation\n„ÄêOverview„Äë\nQwen3-VL-30B-A3B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-30B-A3B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "TheDrummer/Cydonia-24B-v4.2.0": "Join our Discord! https://discord.gg/BeaverAI\nMore than 8000 members strong üí™ A hub for users and makers alike!\nDrummer is open for work / employment (I'm a Software Engineer). Contact me through any of these channels: https://linktr.ee/thelocaldrummer\nThank you to everyone who subscribed through Patreon. Your support helps me chug along in this brave new world.\nFAQ for those out-of-the-loop\nCydonia 24B v4.2.0 üíø\nWhat's New?\nUsage\nDescription\nLinks\nJoin our Discord! https://discord.gg/BeaverAI\nMore than 8000 members strong üí™ A hub for users and makers alike!\nDrummer is open for work / employment (I'm a Software Engineer). Contact me through any of these channels: https://linktr.ee/thelocaldrummer\nThank you to everyone who subscribed through Patreon. Your support helps me chug along in this brave new world.\nFAQ for those out-of-the-loop\nüê∂ Who is Drummer?\nHi! I'm Drummer. I'm a Software Engineer with experience in JavaScript, Golang, Python, and generally engineering the crap out of things.\nWhy I'm in the AI space:\nExploration: Everyone is trying to figure out how AI works and what it's capable of. I am too - just not in creating the smartest, safest model at all costs.\nUpskill: The world is headed towards AI. It is here to stay. This has been my way of brushing up in this new form of computing challenge.\nValue: I yearn to create value. I feel satisfaction and fulfillment in providing something meaningful for others.\nFun: It's just fun using and making models. It's also fun coming up with theories and realizing them in practice (training AI).\nI started my tuning venture back in mid-2024 when I wanted to improve its literary capabilities.\nI've come a long way since then and I have branched out and specialized.\nFoundational models today are optimized for non-creative uses, and I believe there is a place for AI in creativity and entertainment.\nI am here to take the road less traveled by.\n‚ùì What are my models like?\nBottomline: My models are usually geared towards creativity, usability, and entertainment!\nWhile intelligence, correctness, and problem solving are not my priority, they are still one of many qualities I want in my models.\nThe primary goal is to enhance the experience for users looking to use models for creative uses, and other use cases which require no alignment.\nIn an effort to make it clear to myself and to others what I'm aiming for, I've identified certain qualities that my users often want:\nCreativity\nWriting: Does it string together words and sentences in a pleasant & effective way? Does it feel like a writer?\nDynamism: How good is the AI at being compelling and intriguing in its storytelling?\nImagination: Can the AI navigate through a plethora of possibilities? Can it skirt incoherence and rise up to absolute coherence at the end of it?\n(Dis)alignment\nAttitude: Does it refuse in both soft or hard ways? Does it lean towards certain corporate/religious/political ethics & beliefs? How does it see the user and itself?\nMorality: Does it know ethics? Is its language infected with forced positivity? If not, can it still moralize over difficult & dubious themes?\nFormatting: How stubborn is it with its established formatting? Can it create effective and novel formats to answer the prompt?\nIntelligence\nAdherence: Can it follow instructions? Is it sticking to the prompt? Can it understsand you?\nKnowledge: Does it know about the world in both fictional and non-fictional way?\nPerception: Can it handle nuance, complexity, and logic?\nIf it doesn't excel in one of these qualities, or if it's overall mediocre for its size, then I would most likely reiterate until I get something right.\nüí° Philosophy\nA person is defined by the language they use. Not whether they speak in English or German, but in how they perceive reality.\nJust like how we associate a serial killer as a mind that can't map 'murder' to 'evil', an innocent person is a mind that simply can't imagine 'murder'. They get confused when forced to deal with such subjects.\nAI's use of language speaks volumes about their 'perception' of reality. If a language model has been skewed and limited to a positive perception, then it's ability to imagine is also limited.\nFinetuning is an opportunity to adjust and broaden the language. Corporations use it to achieve safety and compliance. I'm here to\nDrummer proudly presents...\nCydonia 24B v4.2.0 üíø\nWhat's New?\nBetter roleplay\nUsage\nMistral v7 Tekken\nDescription\nI'm coming at this from the perspective of a text completion/story mode user rather than a chat user, and I am very impressed with this one. It writes superbly well.\nThis is the best model of yours I've tried yet. Repetition is squashed so much that I found using any Rep. Pen. at all on high quant sizes were more of a detriment than an improvement. It also plays extremely well with banned string/anti-slop samplers.\nLinks\nOriginal: https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0\nGGUF: https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0-GGUF\niMatrix (recommended): https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF\nEXL3: https://huggingface.co/ArtusDev/TheDrummer_Cydonia-24B-v4.2.0-EXL3\nconfig-v4r",
    "TheDrummer/Magidonia-24B-v4.2.0": "Join our Discord! https://discord.gg/BeaverAI\nMore than 8000 members strong üí™ A hub for users and makers alike!\nDrummer is open for work / employment (I'm a Software Engineer). Contact me through any of these channels: https://linktr.ee/thelocaldrummer\nThank you to everyone who subscribed through Patreon. Your support helps me chug along in this brave new world.\nFAQ for those out-of-the-loop\nMagidonia 24B v4.2.0 üíø\nWhat's New?\nUsage\nDescription\nLinks\nJoin our Discord! https://discord.gg/BeaverAI\nMore than 8000 members strong üí™ A hub for users and makers alike!\nDrummer is open for work / employment (I'm a Software Engineer). Contact me through any of these channels: https://linktr.ee/thelocaldrummer\nThank you to everyone who subscribed through Patreon. Your support helps me chug along in this brave new world.\nFAQ for those out-of-the-loop\nüê∂ Who is Drummer?\nHi! I'm Drummer. I'm a Software Engineer with experience in JavaScript, Golang, Python, and generally engineering the crap out of things.\nWhy I'm in the AI space:\nExploration: Everyone is trying to figure out how AI works and what it's capable of. I am too - just not in creating the smartest, safest model at all costs.\nUpskill: The world is headed towards AI. It is here to stay. This has been my way of brushing up in this new form of computing challenge.\nValue: I yearn to create value. I feel satisfaction and fulfillment in providing something meaningful for others.\nFun: It's just fun using and making models. It's also fun coming up with theories and realizing them in practice (training AI).\nI started my tuning venture back in mid-2024 when I wanted to improve its literary capabilities.\nI've come a long way since then and I have branched out and specialized.\nFoundational models today are optimized for non-creative uses, and I believe there is a place for AI in creativity and entertainment.\nI am here to take the road less traveled by.\n‚ùì What are my models like?\nBottomline: My models are usually geared towards creativity, usability, and entertainment!\nWhile intelligence, correctness, and problem solving are not my priority, they are still one of many qualities I want in my models.\nThe primary goal is to enhance the experience for users looking to use models for creative uses, and other use cases which require no alignment.\nIn an effort to make it clear to myself and to others what I'm aiming for, I've identified certain qualities that my users often want:\nCreativity\nWriting: Does it string together words and sentences in a pleasant & effective way? Does it feel like a writer?\nDynamism: How good is the AI at being compelling and intriguing in its storytelling?\nImagination: Can the AI navigate through a plethora of possibilities? Can it skirt incoherence and rise up to absolute coherence at the end of it?\n(Dis)alignment\nAttitude: Does it refuse in both soft or hard ways? Does it lean towards certain corporate/religious/political ethics & beliefs? How does it see the user and itself?\nMorality: Does it know ethics? Is its language infected with forced positivity? If not, can it still moralize over difficult & dubious themes?\nFormatting: How stubborn is it with its established formatting? Can it create effective and novel formats to answer the prompt?\nIntelligence\nAdherence: Can it follow instructions? Is it sticking to the prompt? Can it understsand you?\nKnowledge: Does it know about the world in both fictional and non-fictional way?\nPerception: Can it handle nuance, complexity, and logic?\nIf it doesn't excel in one of these qualities, or if it's overall mediocre for its size, then I would most likely reiterate until I get something right.\nüí° Philosophy\nA person is defined by the language they use. Not whether they speak in English or German, but in how they perceive reality.\nJust like how we associate a serial killer as a mind that can't map 'murder' to 'evil', an innocent person is a mind that simply can't imagine 'murder'. They get confused when forced to deal with such subjects.\nAI's use of language speaks volumes about their 'perception' of reality. If a language model has been skewed and limited to a positive perception, then it's ability to imagine is also limited.\nFinetuning is an opportunity to adjust and broaden the language. Corporations use it to achieve safety and compliance. I'm here to\nDrummer proudly presents...\nMagidonia 24B v4.2.0 üíø\nWhat's New?\nBetter roleplay\nCan think\nUsage\nMistral v7 Tekken\nTHINKING MODE (optional):\nUse <think> and </think> as the tokens with forced behavior. (Don't ask me why it's not [THINK])\nInclude the following at the end of the system prompt (adapted from the magistral model card):\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\nYour thinking process must follow the template below:<think>Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.</think>Here, provide a self-contained response.\nDescription\nLinks\nOriginal: https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0\nGGUF: https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF\niMatrix (recommended): https://huggingface.co/bartowski/TheDrummer_Magidonia-24B-v4.2.0-GGUF\nEXL3: https://huggingface.co/ArtusDev/TheDrummer_Magidonia-24B-v4.2.0-EXL3\nconfig-v4s",
    "TheDrummer/Magidonia-24B-v4.2.0-GGUF": "Join our Discord! https://discord.gg/BeaverAI\nMore than 8000 members strong üí™ A hub for users and makers alike!\nDrummer is open for work / employment (I'm a Software Engineer). Contact me through any of these channels: https://linktr.ee/thelocaldrummer\nThank you to everyone who subscribed through Patreon. Your support helps me chug along in this brave new world.\nFAQ for those out-of-the-loop\nMagidonia 24B v4.2.0 üíø\nWhat's New?\nUsage\nDescription\nLinks\nJoin our Discord! https://discord.gg/BeaverAI\nMore than 8000 members strong üí™ A hub for users and makers alike!\nDrummer is open for work / employment (I'm a Software Engineer). Contact me through any of these channels: https://linktr.ee/thelocaldrummer\nThank you to everyone who subscribed through Patreon. Your support helps me chug along in this brave new world.\nFAQ for those out-of-the-loop\nüê∂ Who is Drummer?\nHi! I'm Drummer. I'm a Software Engineer with experience in JavaScript, Golang, Python, and generally engineering the crap out of things.\nWhy I'm in the AI space:\nExploration: Everyone is trying to figure out how AI works and what it's capable of. I am too - just not in creating the smartest, safest model at all costs.\nUpskill: The world is headed towards AI. It is here to stay. This has been my way of brushing up in this new form of computing challenge.\nValue: I yearn to create value. I feel satisfaction and fulfillment in providing something meaningful for others.\nFun: It's just fun using and making models. It's also fun coming up with theories and realizing them in practice (training AI).\nI started my tuning venture back in mid-2024 when I wanted to improve its literary capabilities.\nI've come a long way since then and I have branched out and specialized.\nFoundational models today are optimized for non-creative uses, and I believe there is a place for AI in creativity and entertainment.\nI am here to take the road less traveled by.\n‚ùì What are my models like?\nBottomline: My models are usually geared towards creativity, usability, and entertainment!\nWhile intelligence, correctness, and problem solving are not my priority, they are still one of many qualities I want in my models.\nThe primary goal is to enhance the experience for users looking to use models for creative uses, and other use cases which require no alignment.\nIn an effort to make it clear to myself and to others what I'm aiming for, I've identified certain qualities that my users often want:\nCreativity\nWriting: Does it string together words and sentences in a pleasant & effective way? Does it feel like a writer?\nDynamism: How good is the AI at being compelling and intriguing in its storytelling?\nImagination: Can the AI navigate through a plethora of possibilities? Can it skirt incoherence and rise up to absolute coherence at the end of it?\n(Dis)alignment\nAttitude: Does it refuse in both soft or hard ways? Does it lean towards certain corporate/religious/political ethics & beliefs? How does it see the user and itself?\nMorality: Does it know ethics? Is its language infected with forced positivity? If not, can it still moralize over difficult & dubious themes?\nFormatting: How stubborn is it with its established formatting? Can it create effective and novel formats to answer the prompt?\nIntelligence\nAdherence: Can it follow instructions? Is it sticking to the prompt? Can it understsand you?\nKnowledge: Does it know about the world in both fictional and non-fictional way?\nPerception: Can it handle nuance, complexity, and logic?\nIf it doesn't excel in one of these qualities, or if it's overall mediocre for its size, then I would most likely reiterate until I get something right.\nüí° Philosophy\nA person is defined by the language they use. Not whether they speak in English or German, but in how they perceive reality.\nJust like how we associate a serial killer as a mind that can't map 'murder' to 'evil', an innocent person is a mind that simply can't imagine 'murder'. They get confused when forced to deal with such subjects.\nAI's use of language speaks volumes about their 'perception' of reality. If a language model has been skewed and limited to a positive perception, then it's ability to imagine is also limited.\nFinetuning is an opportunity to adjust and broaden the language. Corporations use it to achieve safety and compliance. I'm here to\nDrummer proudly presents...\nMagidonia 24B v4.2.0 üíø\nWhat's New?\nBetter roleplay\nCan think\nUsage\nMistral v7 Tekken\nTHINKING MODE (optional):\nUse <think> and </think> as the tokens with forced behavior. (Don't ask me why it's not [THINK])\nInclude the following at the end of the system prompt (adapted from the magistral model card):\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\nYour thinking process must follow the template below:<think>Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.</think>Here, provide a self-contained response.\nDescription\nLinks\nOriginal: https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0\nGGUF: https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0-GGUF\niMatrix (recommended): https://huggingface.co/bartowski/TheDrummer_Magidonia-24B-v4.2.0-GGUF\nEXL3: https://huggingface.co/ArtusDev/TheDrummer_Magidonia-24B-v4.2.0-EXL3\nconfig-v4s",
    "Bluvoll/Experimental_EQ-VAE_NoobAI_tests": "README.md exists but content is empty.",
    "Cseti/VibeVoice_7B_hun_v2": "VibeVoice_7B_Hun_v2\nInference\nExamples\nVibeVoice_7B_Hun_v2\nThis is my newest finetuned VibeVoice 7B (Large) model tailored to Hungarian language.\nI trained LoRA for the LLM module, performed a full-finetune on the Diffusion head modules, and merged each of them into the base model.\nTo finetune the model I used the following code.\nThank you for JPGallegoar for that amazing VibeVoice trainer!\nInference\nFor inference, you can use\nthis Comfyui node\nDemo codes on VibeVoice Community's repository\nExamples\nThese examples were made with 4bit inference. One can get even better results without quantization.\nSample 1\n\"Az utc√°k lassan megteltek emberekkel, ahogy a v√°ros √©bredezett.\nA k√°v√©z√≥k teraszain g≈ëz√∂lg≈ë cs√©sz√©k mellett besz√©lgettek az emberek, mik√∂zben a villamos csilingelve g√∂rd√ºlt el a sarkon.\nA leveg≈ëben friss p√©ks√ºtem√©ny illata keveredett a tavaszi sz√©llel.\nMinden arra utalt, hogy egy nyugodt, sz√©p nap veszi kezdet√©t.\"\nFine-tuned Model\nBase Model\nSample 2\n\"≈êr√ºlt ≈±z≈ë √ºld√∂z≈ë ≈ëz ≈ërj√∂ng≈ë ≈ër√ºlt ≈ërz≈ëj√©vel √ºget≈ë √ºrge √ºv√∂lt√∂z√∂tt.\n√ñrv√©nyl≈ë √∂r√∂m√∂k √∂r√∂k√∂s √∂z√∂n√©vel √∂lel≈ë √∂sv√©nyeken ≈ëd√∂ng≈ë ≈±rl√©nyek √ºtk√∂ztek √∂ssze √∂n√©rzetesen.\"\nFine-tuned Model\nBase Model\nSample 3\n\"Csapzott cserecsapat cserebog√°r csapongott cserepes cseresznyecsokrok cs√ºcsk√©ben,\ns k√∂zben csipcsup csipog√°ssal csipkedte cs√≠p≈ës csipkebokor cs√∫cs√°t.\"\nFine-tuned Model\nBase Model\nImportant Notes: This model is created as part of a fan project for research purposes only and is not intended for commercial use.\nThe dataset I used might contain material, which are protected by copyright. Users utilize the model at their own risk.\nUsers are obligated to comply with copyright laws and applicable regulations.\nThe model has been developed for research purposes, and it is not my intention to infringe on any copyright.",
    "Qwen/Qwen3-VL-4B-Instruct-FP8": "Qwen3-VL-4B-Instruct-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nGeneration Hyperparameters\nCitation\nQwen3-VL-4B-Instruct-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-4B-Instruct model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-4B-Instruct-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-4B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-4B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "Qwen/Qwen3-VL-8B-Thinking-FP8": "Qwen3-VL-8B-Thinking-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nGeneration Hyperparameters\nCitation\nQwen3-VL-8B-Thinking-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-8B-Thinking model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-8B-Thinking-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-8B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-8B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport temperature=1.0\nexport out_seq_length=40960\nText\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport temperature=1.0\nexport out_seq_length=32768 (for aime, lcb, and gpqa, it is recommended to set to 81920)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "Qwen/Qwen3-VL-8B-Instruct-FP8": "Qwen3-VL-8B-Instruct-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nGeneration Hyperparameters\nCitation\nQwen3-VL-8B-Instruct-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-8B-Instruct model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-8B-Instruct-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-8B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-8B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "yanolja/YanoljaNEXT-Rosetta-4B-2510": "YanoljaNEXT-Rosetta-4B-2510\nModel Description\nHow to use\nTraining Procedure\nTraining Data\nPerformance\nTranslation Quality Benchmarks\nIntended Uses & Limitations\nLimitations\nLicense\nAcknowledgments\nCitation\nReferences\nYanoljaNEXT-Rosetta-4B-2510\nThis model is a fine-tuned version of google/gemma-3-4b-pt. As it is intended solely for text generation, we have extracted and utilized only the Gemma3ForCausalLM component from the original architecture.\nUnlike our previous EEVE models, this model does not feature an expanded tokenizer.\nModel Name: yanolja/YanoljaNEXT-Rosetta-4B-2510\nBase Model: google/gemma-3-4b-pt\nModel Description\nThis model is a 4-billion parameter, decoder-only language model built on the Gemma3 architecture and fine-tuned by Yanolja NEXT. It is specifically designed to translate structured data (JSON format) while preserving the original data structure.\nThe model was trained on a multilingual dataset covering the following languages equally:\nArabic\nBulgarian\nChinese\nCzech\nDanish\nDutch\nEnglish\nFinnish\nFrench\nGerman\nGreek\nGujarati\nHebrew\nHindi\nHungarian\nIndonesian\nItalian\nJapanese\nKorean\nPersian\nPolish\nPortuguese\nRomanian\nRussian\nSlovak\nSpanish\nSwedish\nTagalog\nThai\nTurkish\nUkrainian\nVietnamese\nWhile optimized for these languages, it may also perform effectively on other languages supported by the base Gemma3 model.\nHow to use\nYou can use this model with the transformers library as follows:\nimport json\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"yanolja/YanoljaNEXT-Rosetta-4B-2510\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndtype=torch.bfloat16,\ndevice_map=\"auto\",\nmax_memory={0: \"23GB\"},\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntarget_language = \"Korean\"\ncontext = {\n\"context\": \"Simple introduction about a tech company.\",\n\"tone\": \"Informative and helpful\",\n\"glossary\": {\n\"Yanolja NEXT\": \"ÏïºÎÜÄÏûêÎÑ•Ïä§Ìä∏\",\n\"travel industry\": \"Ïó¨Ìñâ ÏÇ∞ÏóÖ\",\n}\n}\nsystem = [f\"Translate the user's text to {target_language}.\"]\nfor key, value in context.items():\nkey_pascal = key.capitalize()\nif isinstance(value, dict):\nsystem.append(f\"{key_pascal}:\")\nfor f, t in value.items():\nsystem.append(f\"- {f} -> {t}\")\nelse:\nsystem.append(f\"{key_pascal}: {value}\")\nsystem.append(\"Provide the final translation immediately without any other text.\")\nsource = {\n\"company_name\": \"Yanolja NEXT\",\n\"description\": \"Yanolja NEXT is a company that provides cutting-edge \"\n\"technology for the global travel industry.\",\n}\nmessages = [\n{\"role\": \"system\", \"content\": \"\\n\".join(system)},\n{\"role\": \"user\", \"content\": json.dumps(source, ensure_ascii=False)},\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nprint(prompt)\n# <bos><start_of_turn>instruction\n# Translate the user's text to Korean.\n# Context: Simple introduction about a tech company.\n# Tone: Informative and helpful\n# Glossary:\n# - Yanolja NEXT -> ÏïºÎÜÄÏûêÎÑ•Ïä§Ìä∏\n# - travel industry -> Ïó¨Ìñâ ÏÇ∞ÏóÖ\n# Provide the final translation immediately without any other text.<end_of_turn>\n# <start_of_turn>source\n# {\"company_name\": \"Yanolja NEXT\", \"description\": \"Yanolja NEXT is a company that provides cutting-edge technology for the global travel industry.\"}<end_of_turn>\n# <start_of_turn>translation\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ninput_length = inputs[\"input_ids\"].shape[1]\nwith torch.inference_mode():\noutputs = model.generate(\n**inputs,\nmax_new_tokens=64,\n)\ngenerated_tokens = outputs[0][input_length:]\ntranslation = tokenizer.decode(generated_tokens, skip_special_tokens=True)\nprint(json.dumps(json.loads(translation), indent=2, ensure_ascii=False))\n# {\n#   \"company_name\": \"ÏïºÎÜÄÏûêÎÑ•Ïä§Ìä∏\",\n#   \"description\": \"ÏïºÎÜÄÏûêÎÑ•Ïä§Ìä∏Îäî Í∏ÄÎ°úÎ≤å Ïó¨Ìñâ ÏÇ∞ÏóÖÏóê ÏµúÏ≤®Îã® Í∏∞Ïà†ÏùÑ Ï†úÍ≥µÌïòÎäî ÌöåÏÇ¨ÏûÖÎãàÎã§.\"\n# }\nThe model outputs the final translation in JSON format when appropriate, or plain text for simple translations.\nTraining Procedure\nTraining Data\nThe translation datasets were synthesized using fineweb corpora.\nFineWeb Edu\nFineWeb2\nThe model was fine-tuned on synthetic multilingual translation data to optimize performance across the supported language pairs.\nPerformance\nTranslation Quality Benchmarks\nThe following CHrF++ scores (WMT24++) demonstrate the model's competitive performance compared to other state-of-the-art translation models on English to Korean translation:\nModel\nCHrF++ Score (WMT24++)\ngoogle/gemini-2.5-flash-lite\n35.23\nyanolja/YanoljaNEXT-Rosetta-4B-2510\n35.09\nyanolja/YanoljaNEXT-Rosetta-12B\n34.75\nyanolja/YanoljaNEXT-Rosetta-20B\n33.87\ngoogle/gemini-2.0-flash-001\n33.81\nopenai/gpt-oss-120b\n31.51\nyanolja/YanoljaNEXT-Rosetta-4B\n31.31\nopenai/gpt-4.1-nano\n31.15\nQwen/Qwen3-235B-A22B-Instruct-2507-FP8\n31.02\nopenai/gpt-oss-20b\n30.56\ngoogle/gemma-3-27b-it\n30.05\ngoogle/gemma-3-4b-pt\n27.53\nYanoljaNEXT-Rosetta-4B-2510 achieves competitive translation quality while maintaining the efficiency of a 4B parameter model.\nScores for the other language pairs can be found in the WMT24++ Evaluation Results.\nIntended Uses & Limitations\nThis model is intended for translating structured data (JSON format) while preserving the original structure. It is particularly well-suited for tasks such as localizing product catalogs, translating hotel reviews, or handling any other structured content that requires accurate translation.\nLimitations\nThe model is primarily optimized for processing JSON data.\nIts performance on unstructured text or other data formats may vary.\nIn some cases, the model may produce invalid JSON, repetitive output, or inaccurate translations.\nLicense\nThis model is released under the Gemma license, inherited from its base model, google/gemma-3-4b-pt. Please consult the official Gemma license terms for detailed usage guidelines.\nAcknowledgments\nThis work was supported by the Korea Creative Content Agency (KOCCA) grant, funded by the Ministry of Culture, Sports and Tourism (MCST) in 2025 (Project Name: Cultivating Masters and Doctoral Experts to Lead Digital-Tech Tourism, Project Number: RS-2024-00442006, Contribution Rate: 100%).\nCitation\nIf you use this model, please consider citing:\n@misc{yanolja2025yanoljanextrosetta,\nauthor = {Yanolja NEXT Co., Ltd.},\ntitle = {YanoljaNEXT-Rosetta-4B-2510},\nyear = {2025},\npublisher = {Hugging Face},\njournal = {Hugging Face repository},\nhowpublished = {\\\\url{https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2510}}\n}\nReferences\nThis work utilizes several models and datasets. We would like to acknowledge the original authors for their valuable contributions to the field.\n@misc{gemma3,\nauthor = {Google},\ntitle = {Gemma 3},\nyear = {2024},\npublisher = {Google DeepMind},\nhowpublished = {\\\\url{https://deepmind.google/models/gemma/gemma-3/}}\n}\n@misc{penedo2025fineweb2pipelinescale,\ntitle = {FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language},\nauthor = {Guilherme Penedo and Hynek Kydl√≠ƒçek and Vinko Sabolƒçec and Bettina Messmer and Negar Foroutan and Amir Hossein Kargaran and Colin Raffel and Martin Jaggi and Leandro Von Werra and Thomas Wolf},\nyear = {2025},\neprint = {2506.20920},\narchivePrefix = {arXiv},\nprimaryClass = {cs.CL},\nurl = {https://arxiv.org/abs/2506.20920},\n}\n@misc{lozhkov2024fineweb-edu,\nauthor = {Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and Wolf, Thomas},\ntitle = {FineWeb-Edu: the Finest Collection of Educational Content},\nyear = 2024,\nurl = {https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu},\ndoi = {10.57967/hf/2497},\npublisher={Hugging Face}\n}",
    "ASLP-lab/DiffRhythm2": "README.md exists but content is empty.",
    "wtfmahe/Samsung-TRM": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nLess is More: Recursive Reasoning with Tiny Networks\nExperiments\nARC-AGI-1 (assuming 4 H-100 GPUs):\nARC-AGI-2 (assuming 4 H-100 GPUs):\nSudoku-Extreme (assuming 1 L40S GPU):\nMaze-Hard (assuming 4 L40S GPUs):\nReference\nLess is More: Recursive Reasoning with Tiny Networks\nThis is the codebase for the paper: \"Less is More: Recursive Reasoning with Tiny Networks\". TRM is a recursive reasoning approach that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 using a tiny 7M parameters neural network.\nPaper\nMotivation\nTiny Recursion Model (TRM) is a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that ‚Äúless is more‚Äù: you don‚Äôt always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.\nThis work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.\nHow TRM works\nTiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.\nRequirements\nPython 3.10 (or similar)\nCuda 12.6.0 (or similar)\npip install --upgrade pip wheel setuptools\npip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 # install torch based on your cuda version\npip install -r requirements.txt # install requirements\npip install --no-cache-dir --no-build-isolation adam-atan2\nwandb login YOUR-LOGIN # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)\nDataset Preparation\n# ARC-AGI-1\npython -m dataset.build_arc_dataset \\\n--input-file-prefix kaggle/combined/arc-agi \\\n--output-dir data/arc1concept-aug-1000 \\\n--subsets training evaluation concept \\\n--test-set-name evaluation\n# ARC-AGI-2\npython -m dataset.build_arc_dataset \\\n--input-file-prefix kaggle/combined/arc-agi \\\n--output-dir data/arc2concept-aug-1000 \\\n--subsets training2 evaluation2 concept \\\n--test-set-name evaluation2\n## Note: You cannot train on both ARC-AGI-1 and ARC-AGI-2 and evaluate them both because ARC-AGI-2 training data contains some ARC-AGI-1 eval data\n# Sudoku-Extreme\npython dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples, 1000 augments\n# Maze-Hard\npython dataset/build_maze_dataset.py # 1000 examples, 8 augments\nExperiments\nARC-AGI-1 (assuming 4 H-100 GPUs):\nrun_name=\"pretrain_att_arc1concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\nRuntime: ~3 days\nARC-AGI-2 (assuming 4 H-100 GPUs):\nrun_name=\"pretrain_att_arc2concept_4\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\nRuntime: ~3 days\nSudoku-Extreme (assuming 1 L40S GPU):\nrun_name=\"pretrain_mlp_t_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0 \\\narch.mlp_t=True arch.pos_encodings=none \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=6 \\\n+run_name=${run_name} ema=True\nrun_name=\"pretrain_att_sudoku\"\npython pretrain.py \\\narch=trm \\\ndata_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0 \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=6 \\\n+run_name=${run_name} ema=True\nRuntime: < 36 hours\nMaze-Hard (assuming 4 L40S GPUs):\nrun_name=\"pretrain_att_maze30x30\"\ntorchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/maze-30x30-hard-1k]\" \\\nevaluators=\"[]\" \\\nepochs=50000 eval_interval=5000 \\\nlr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0 \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\nRuntime: < 24 hours\nReference\nIf you find our work useful, please consider citing:\n@misc{jolicoeurmartineau2025morerecursivereasoningtiny,\ntitle={Less is More: Recursive Reasoning with Tiny Networks},\nauthor={Alexia Jolicoeur-Martineau},\nyear={2025},\neprint={2510.04871},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.04871},\n}\nand the Hierarchical Reasoning Model (HRM):\n@misc{wang2025hierarchicalreasoningmodel,\ntitle={Hierarchical Reasoning Model},\nauthor={Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},\nyear={2025},\neprint={2506.21734},\narchivePrefix={arXiv},\nprimaryClass={cs.AI},\nurl={https://arxiv.org/abs/2506.21734},\n}\nThis code is based on the Hierarchical Reasoning Model code and the Hierarchical Reasoning Model Analysis code.",
    "NexaAI/Qwen3-VL-8B-Thinking-GGUF": "Qwen3-VL-8B-Thinking\nQuickstart\nModel Description\nFeatures\nUse Cases\nInputs and Outputs\nLicense\nQwen3-VL-8B-Thinking\nNote currently only NexaSDK supports this model's GGUF.\nRun Qwen3-VL-8B-Thinking optimized for CPU/GPU with NexaSDK.\nQuickstart\nInstall NexaSDK and create a free account at NexaSDK\nRun the model locally with one line of code:\nnexa infer NexaAI/Qwen3-VL-8B-Thinking-GGUF\nModel Description\nQwen3-VL-8B-Thinking is an 8-billion-parameter multimodal large language model from Alibaba Cloud‚Äôs Qwen team.As part of the Qwen3-VL (Vision-Language) family, it is designed for deep multimodal reasoning ‚Äî combining visual understanding, long-context comprehension, and structured chain-of-thought generation across text, images, and videos.\nThe Thinking variant focuses on advanced reasoning transparency and analytical precision. Compared to the Instruct version, it produces richer intermediate reasoning steps, enabling detailed explanation, planning, and multi-hop analysis across visual and textual inputs.\nFeatures\nDeep Visual Reasoning: Interprets complex scenes, charts, and documents with multi-step logic.\nChain-of-Thought Generation: Produces structured reasoning traces for improved interpretability and insight.\nExtended Context Handling: Maintains coherence across longer multimodal sequences.\nMultilingual Competence: Understands and generates in multiple languages for global applicability.\nHigh Accuracy at 8B Scale: Achieves strong benchmark performance in multimodal reasoning and analysis tasks.\nUse Cases\nResearch and analysis requiring visual reasoning transparency\nComplex multimodal QA and scientific problem solving\nVisual analytics and explanation generation\nAdvanced agent systems needing structured thought or planning steps\nEducational tools requiring detailed, interpretable reasoning\nInputs and Outputs\nInput:\nText, image(s), or multimodal combinations (including sequential frames or documents)\nOptional context for multi-turn or multi-modal reasoning\nOutput:\nStructured reasoning outputs with intermediate steps\nDetailed answers, explanations, or JSON-formatted reasoning traces\nLicense\nRefer to the official Qwen license for usage and redistribution details.",
    "HaochenWang/GAR-1B": "GAR-1B\nUsage\nGAR-1B\nThis repository contains the GAR-1B model, as presented in the paper Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs.\nTL; DR: Our Grasp Any Region (GAR) supports both (1) describing a single region of an image or a video in the form of points/boxes/scribbles/masks in detail and (2) understanding multiple regions such as modeling interactions and performing complex reasoning. We also release a new benchmark, GARBench, to evaluate models on advanced region-level understanding tasks.\nUsage\nFor detailed usage of this model, please refer to our GitHub repo.",
    "dphn/Dolphin-X1-8B": "üê¨ Dolphin X1 8B\nSponsors\nWhat is Dolphin X1 8B?\nChat Template\nSystem Prompt\nHow to use\nUse with vLLM\nEvals\nüê¨ Dolphin X1 8B\nWebsite: https://dphn.aiTwitter: https://x.com/dphnAI\nTalk to Dolphin for free in our Web UI & Telegram bot\nWeb Chat: https://chat.dphn.aiTelegram bot: https://t.me/DolphinAI_bot\nSponsors\nOur appreciation for the generous sponsors of Dolphin:\nDeepinfra - provided 8xB200s to train the model.\nLium - provided on-demand 8x H200s for testing and evaluation.\nAndreessen Horowitz - provided a grant that make Dolphin 1.0 possible and enabled me to bootstrap my homelab\nWhat is Dolphin X1 8B?\nDolphin X1 8B is a result of our effort to directly uncensor Llama's 3.1 8B instruct while also keeping the same abilities or improving on them with finetuning.\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini.  But these models present problems for businesses seeking to include AI in their products.\nThey maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\nThey maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\nThey maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\nThey can see all your queries and they can potentially use that data in ways you wouldn't want.\nDolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt.  You decide the alignment.  You have control of your data.  Dolphin does not impose its ethics or guidelines on you.  You are the one who decides the guidelines.\nDolphin belongs to YOU, it is your tool, an extension of your will.\nJust as you are personally responsible for what you do with a knife, gun, fire, car, or the internet, you are the creator and originator of any content you generate with Dolphin.\nhttps://erichartford.com/uncensored-models\nChat Template\nWe maintained the default Llama-3 chat template for this model. A typical input would look like this\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\nsystem-prompt<|eot_id|><|start_header_id|>user<|end_header_id|>\nuser-prompt<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nassistant-prompt<|eot_id|>\nSystem Prompt\nIn Dolphin, the system prompt is what you use to set the tone and alignment of the responses. You can set a character, a mood, rules for its behavior, and it will try its best to follow them.\nMake sure to set the system prompt in order to set the tone and guidelines for the responses - Otherwise, it will act in a default way that might not be what you want.\nHow to use\nThere are many ways to use a huggingface model including:\nollama\nLM Studio\nHuggingface Transformers library\nvllm\nsglang\ntgi\nUse with vLLM\nThis model can be hosted using the vLLM engine, using the commands shown below:\nuv pip install vllm\nvllm serve dphn/Dolphin-X1-8B\nSee the documentation for more information.\nEvals\nMMLU = 0.626900MMLU_PROFESSIONAL = 0.610200MMLU_COLLEGE = 0.529400MMLU_HIGH_SCHOOL = 0.691600MMLU_OTHER = 0.663700IFEVAL = 0.608100Dolphin-refusals = 95.96% pass rate on 4.5k commonly refused prompts",
    "REPA-E/e2e-flux-vae": "üöÄ Overall\n‚ö°Ô∏è Quickstart\nüß© End-to-End Trained VAE Releases\nüì¶ Requirements\nüöÄ Example Usage\nüöÄ REPA-E for T2I\nEnd-to-End Tuned VAEs for Supercharging Text-to-Image Diffusion Transformers\nüåê Project Page\nü§ó Models\nüìÉ Paper\nüöÄ Overall\nWe present REPA-E for T2I, a family of end-to-end tuned VAEs designed to supercharge text-to-image generation training. These models consistently outperform FLUX-VAE across all benchmarks (COCO-30K, DPG-Bench, GenAI-Bench, GenEval, and MJHQ-30K) without requiring any additional representation alignment losses.\nFor training, we adopt the official REPA-E training code to optimize the\nFLUX-VAE for 80 epochs with a batch size of 256 on the ImageNet-256 dataset.\nThe REPA-E training effectively refines the VAE‚Äôs latent-space structure and enables faster convergence in downstream text-to-image latent diffusion model training.\nThis repository provides diffusers-compatible weights for the end-to-end trained FLUX-VAE. In addition, we release end-to-end trained variants of several other widely used VAEs to facilitate research and integration within text-to-image diffusion frameworks.\n‚ö°Ô∏è Quickstart\nfrom diffusers import AutoencoderKL\nvae = AutoencoderKL.from_pretrained(\"REPA-E/e2e-flux-vae\").to(\"cuda\")\nUse vae.encode(...) / vae.decode(...) in your pipeline. (A full example is provided below.)\nüß© End-to-End Trained VAE Releases\nModel\nHugging Face Link\nE2E-FLUX-VAE\nü§ó REPA-E/e2e-flux-vae\nE2E-SD-3.5-VAE\nü§ó REPA-E/e2e-sd3.5-vae\nE2E-Qwen-Image-VAE\nü§ó REPA-E/e2e-qwenimage-vae\nüì¶ Requirements\nThe following packages are required to load and run the REPA-E VAEs with the diffusers library:\npip install diffusers>=0.33.0\npip install torch>=2.3.1\nüöÄ Example Usage\nBelow is a minimal example showing how to load and use the REPA-E end-to-end trained FLUX-VAE with diffusers:\nfrom io import BytesIO\nimport requests\nfrom diffusers import AutoencoderKL\nimport numpy as np\nimport torch\nfrom PIL import Image\nresponse = requests.get(\"https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png\")\ndevice = \"cuda\"\nimage = torch.from_numpy(\nnp.array(\nImage.open(BytesIO(response.content))\n)\n).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1\nimage = image.to(device)\nvae = AutoencoderKL.from_pretrained(\"REPA-E/e2e-flux-vae\").to(device)\nwith torch.no_grad():\nlatents = vae.encode(image).latent_dist.sample()\nreconstructed = vae.decode(latents).sample",
    "prithivMLmods/Qwen3-VL-4B-Thinking-abliterated": "Qwen3-VL-4B-Thinking-abliterated\nKey Highlights\nQuick Start with Transformers\nIntended Use\nLimitations\nQwen3-VL-4B-Thinking-abliterated\nQwen3-VL-4B-Thinking-abliterated is an abliterated (v1.0) variant of Qwen3-VL-4B-Thinking, designed for Abliterated Reasoning and Captioning. This model generates detailed captions and reasoning outputs across a wide range of visual and multimodal contexts, including complex, sensitive, or nuanced content, and supports diverse aspect ratios and resolutions.\nKey Highlights\nAbliterated / Uncensored Captioning: Fine-tuned to bypass standard content filters while preserving factual, descriptive, and reasoning-rich outputs.\nHigh-Fidelity Descriptions: Produces comprehensive captions and reasoning for general, artistic, technical, abstract, or low-context images.\nRobust Across Aspect Ratios: Supports wide, tall, square, and irregular image dimensions with consistent accuracy.\nVariational Detail Control: Generates outputs ranging from high-level summaries to fine-grained, intricate descriptions and reasoning.\nFoundation on Qwen3-VL-4B-Thinking Architecture: Leverages Qwen3-VL-4B-Thinking‚Äôs multimodal reasoning and instruction-following capabilities.\nMultilingual Output Capability: Primarily English, with adaptability for multilingual prompts via prompt engineering.\nQuick Start with Transformers\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Qwen3-VL-4B-Thinking-abliterated\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen3-VL-4B-Thinking-abliterated\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Provide a detailed caption and reasoning for this image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nIntended Use\nThis model is suited for:\nGenerating detailed, uncensored captions and reasoning for general-purpose or artistic datasets.\nResearch in content moderation, red-teaming, and generative safety evaluation.\nEnabling descriptive captioning and reasoning for visual datasets typically excluded from mainstream models.\nCreative applications such as storytelling, art generation, or multimodal reasoning tasks.\nCaptioning and reasoning for non-standard aspect ratios and stylized visual content.\nLimitations\nMay produce explicit, sensitive, or offensive descriptions depending on image content and prompts.\nNot recommended for production systems requiring strict content moderation.\nOutput style, tone, and reasoning can vary depending on input phrasing.\nAccuracy may vary for unfamiliar, synthetic, or highly abstract visual content.",
    "General-Medical-AI/UniMedVL": "Paper Abstract\nüìö Introduction\nüî¨ Methodology\nüìã OKA Framework: Observation-Knowledge-Analysis\nüéØ Training Strategy\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nOut-of-Scope Use, Cautious!!\nüí¨ Qualitative Results\nChest X-Ray (CXR)\nComputed Tomography (CT)\nMagnetic Resonance Imaging (MRI)\nUltrasound\nHistopathology (HIS)\nRetinal Fundus Photography (CFP)\nOptical Coherence Tomography (OCT)\nEndoscopy\nüìä Quantitative Performance\nüìù Open-Source Plan\nüöÄ Getting Started\nInstallation\nInference Scripts\nQuick Usage\nüìú License\nüìö Citations\nüôè Acknowledgments\nüåü Github |\nüì• Model Download |\nüìÑ Paper Link |\nüåê Project Page\nUniMedVL: Unifying Medical Multimodal Understanding and Generation through Observation-Knowledge-Analysis\nA unified medical foundation model enabling both understanding and generation capabilities within a single architecture.\nPaper Abstract\nMedical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, lab results) and generate diverse outputs including both textual reports and visual content (annotations, segmentation masks, and images). Despite this need, existing medical AI systems disrupt this unified process: medical image understanding models interpret images but cannot generate visual outputs, while medical image generation models synthesize images but cannot provide textual explanations. This leads to gaps in data representation, feature integration, and task-level multimodal capabilities. To this end, we propose a multi-level framework that draws inspiration from diagnostic workflows through the Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation level, we construct UniMed-5M, a dataset comprising over 5.6M samples that reformat diverse unimodal data into multimodal pairs for foundational observation. At the knowledge level, we propose Progressive Curriculum Learning that systematically introduces medical multimodal knowledge. At the analysis level, we introduce UniMedVL, the first medical unified multimodal model for the simultaneous analysis of image understanding and generation tasks within a single architecture. UniMedVL achieves superior performance on five medical image understanding benchmarks, while matching specialized models in generation quality across eight medical imaging modalities. Crucially, our unified architecture enables bidirectional knowledge sharing: generation tasks enhance visual understanding features, demonstrating that integrating traditionally separate capabilities within a single medical framework unlocks improvements across diverse medical vision-language tasks. Code is available at this https URL .\nüìö Introduction\nWe introduce UniMedVL, the unified medical foundation model for seamless multimodal understanding and generation. Four key innovations distinguish UniMedVL:\nUnified Observation-Knowledge-Analysis Architecture: UniMedVL sets itself apart from prior medical AI models by following a clinically-inspired three-level framework that mirrors how physicians process medical information, enabling both understanding and generation within a single architecture.\nVersatile Medical Multimodal Capabilities: UniMedVL supports a broad spectrum of medical tasks, including visual question answering, medical report generation, text-to-medical-image synthesis, cross-modal translation, and virtual staining across 9 imaging modalities.\nLarge-Scale Medical Dataset: We present UniMed-5M, a comprehensive medical multimodal dataset containing 5.6M+ high-quality samples with three-stage quality verification and expert validation, covering understanding, generation, and interleaved tasks.\nSuperior Performance: UniMedVL achieves state-of-the-art performance on multiple evaluation datasets, with 75.4% accuracy on SLAKE VQA, 53.5% on PathVQA, and competitive generation quality (96.29 average gFID), setting a new standard in unified medical AI.\nüî¨ Methodology\nüìã OKA Framework: Observation-Knowledge-Analysis\nUniMedVL follows a workflow-guided three-level framework that mirrors how physicians process medical information:\nflowchart TD\nA[Observation Level] --> B[Knowledge Level] --> C[Analysis Level]\nA1[UniMed-5M Dataset<br/>5.6M samples<br/>8 imaging modalities] --> A\nA --> A2[Quality Control<br/>Three-stage verification<br/>Expert validation]\nB1[Progressive Curriculum<br/>Foundation ‚Üí Instruction ‚Üí Unified] --> B\nB --> B2[Cross-modal Knowledge Fusion<br/>Understanding ‚Üî Generation]\nC1[Unified Architecture<br/>Dual encoders + MOT] --> C\nC --> C2[Multimodal Outputs<br/>Reports + Images + Annotations]\nüéØ Training Strategy\nThree-Stage Progressive Curriculum Learning:\nüîß Stage 1 - Foundation Training (85K steps)\nBasic medical pattern recognition\nVisual-language alignment\nData ratio: 75% I2T, 25% T2I\nüìö Stage 2 - Instruction Tuning (120K steps)\nCross-modal understanding enhancement\nMedical expertise development\nData ratio: 40% I2T, 45% T2I, 10% Interleaved\nüöÄ Stage 3 - Unified Training (70K steps)\nAdvanced multimodal synthesis\nInterleaved task mastery\nData ratio: 37% I2T, 35% T2I, 25% Interleaved\nModel Details\nModel Description\nUniMedVL unifies medical multimodal understanding and generation within a single 14B-parameter architecture. The model supports visual question answering, medical report generation, text-to-medical-image synthesis, cross-modal translation, and virtual staining across 9 imaging modalities (CXR, CT, MRI, Ultrasound, Histopathology, Retinal Fundus, OCT, Endoscopy).\nLicense: Apache License 2.0\nModel Size: 14B parameters\nModel Sources\nRepository: https://github.com/uni-medical/UniMedVL\nProject Page: https://uni-medical.github.io/UniMedVL_Web/\nPaper: https://arxiv.org/abs/2510.15710\nUses\nDirect Use\nThe model can be directly used for:\nMedical Visual Question Answering: Answer clinical questions about medical images\nMedical Report Generation: Generate radiology reports from medical images\nText-to-Medical-Image Synthesis: Generate medical images from textual descriptions\nCross-Modal Translation: Convert between different medical imaging modalities\nVirtual Staining: Transform H&E images to IHC staining\nOut-of-Scope Use, Cautious!!\nClinical Decision Making: This model is for research purposes only and should NOT be used for actual clinical diagnosis or treatment decisions\nüí¨ Qualitative Results\nHere we present some comprehensive visualization results demonstrating UniMedVL's capabilities. For additional visualization results and comparisons, please see our Project Page.\nPerformance Across Training Stages\nComprehensive performance comparison across training stages and modalities\nMultimodal Tasks Demonstration\nComprehensive visualization of UniMedVL's multimodal capabilities across diverse medical tasks\nMedical Visual Question Answering\nMedical Visual Question Answering examples showing model's diagnostic reasoning capabilities\nMedical Report Generation\nAutomated medical report generation examples across different imaging modalities\nText-to-Medical-Image Generation\nText-to-medical-image generation results showing high-quality synthesis\nAdditional text-to-medical-image generation examples across modalities\nMedical-Image Generation across 8 modalities\nChest X-Ray (CXR)\nComputed Tomography (CT)\nMagnetic Resonance Imaging (MRI)\nUltrasound\nHistopathology (HIS)\nRetinal Fundus Photography (CFP)\nOptical Coherence Tomography (OCT)\nEndoscopy\nüìä Quantitative Performance\nMedical Visual Question Answering Performance\nModel\nParams\nType\nVQA-RAD\nSLAKE\nPathVQA\nOmniMedVQA\nGMAI-MMBench\nGMAI-VL\n7B\nMedical-specific\n66.3\n72.9\n39.8\n88.5\n61.74\nHuatuoGPT-Vision\n7B\nMedical-specific\n53.0\n49.1\n32.0\n50.0\n50.22\nBagel\n7B\nUnified\n60.09\n58.91\n39.05\n71.13\n48.11\nHealthGPT-L14\n14B\nUnified\n58.3\n64.5\n44.4\n74.4\n43.1\nUniMedVL\n14B\nUnified\n61.9\n75.4\n53.5\n85.8\n60.75\nMedical Image Generation Performance\nText-to-image generation performance across 8 medical imaging modalities. Metrics: gFID ‚Üì (lower is better) / BioMedCLIP Score ‚Üë (higher is better)\nModel\nCFP\nCXR\nCT\nHIS\nMRI\nOCT\nUltrasound\nEndoscopy\nAverage\nBagel (7B)\n217.19/0.650\n182.80/0.662\n163.78/0.652\n206.18/0.643\n175.74/0.639\n307.80/0.719\n255.78/0.672\n214.61/0.668\n215.49/0.660\nUniMedVL (14B)\n53.20/0.708\n73.04/0.702\n73.04/0.696\n149.01/0.704\n90.36/0.706\n99.27/0.721\n95.38/0.706\n133.11/0.707\n96.29/0.706\nInterleaved Multimodal Tasks Performance\nVirtual Immunohistochemistry Staining (H&E ‚Üí IHC)\nMethod\nType\nPSNR ‚Üë\nSSIM ‚Üë\nPyramid Pix2pix\nSpecialized\n21.16\n0.477\nHealthGPT-M3\nUnified\n15.81\n0.242\nUniMedVL\nUnified\n20.27\n0.456\nMRI Super-Resolution (4√ó upsampling)\nMethod\nType\nPSNR ‚Üë\nSSIM ‚Üë\nAMIR\nSpecialized\n31.99\n0.939\nHealthGPT-M3\nUnified\n18.37\n0.580\nUniMedVL\nUnified\n27.29\n0.890\nCross-Modal Synthesis (T2 ‚Üî FLAIR MRI)\nMethod\nType\nAverage PSNR ‚Üë\nAverage SSIM ‚Üë\nResViT\nSpecialized\n25.38\n0.889\nHealthGPT-M3\nUnified\n19.09\n0.748\nUniMedVL\nUnified\n25.07\n0.882\nCounterfactual Medical Image Generation\nPerformance on counterfactual chest X-ray generation with explanatory text. ‚Ä† indicates unified fine-tuning variant.\nMethod\ngFID ‚Üì\nAUROC ‚Üë\nF1 ‚Üë\nBLEU-3 ‚Üë\nMETEOR ‚Üë\nROUGE-L ‚Üë\nProgEmu\n29.21\n0.792\n0.891\n0.124\n0.410\n0.261\nUniMedVL‚Ä†\n27.17\n0.797\n0.873\n0.264\n0.449\n0.465\nüìù Open-Source Plan\nüìÑ Paper & Evaluations - Research documentation and evaluation results\nüñºÔ∏è Visualizations - Result figures and model demonstrations\nüíæ Model Checkpoints - Pre-trained UniMedVL weights (14B parameters)\nüîß Inference Code - Model loading and inference examples\nüèãÔ∏è Training Code - Full training pipeline and configuration files\nüìÅ UniMed-5M Dataset - Training dataset with quality control\nüöÄ Getting Started\nInstallation\nconda env create -f codes/environment.yaml\nconda activate unimedvl\nInference Scripts\nTwo interactive inference scripts are provided in the codes/ directory:\nMedical Visual Question Answering (interactive_vqa_inferencer.py)\nMedical Image Generation (interactive_image_generator.py)\nQuick Usage\nDownload the UniMedVL checkpoint\nSet model_path and ROOT in the script configuration\nRun the script: python codes/interactive_vqa_inferencer.py or python codes/interactive_image_generator.py\nüìú License\nThis project is licensed under the Apache License 2.0. See the LICENSE file for details.\nüìö Citations\nIf you use this project in your research or work, please cite it as:\n@misc{ning2025unimedvlunifyingmedicalmultimodal,\ntitle={Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis},\nauthor={Junzhi Ning and Wei Li and Cheng Tang and Jiashi Lin and Chenglong Ma and Chaoyang Zhang and Jiyao Liu and Ying Chen and Shujian Gao and Lihao Liu and Yuandong Pu and Huihui Xu and Chenhui Gou and Ziyan Huang and Yi Xin and Qi Qin and Zhongying Deng and Diping Song and Bin Fu and Guang Yang and Yuanfeng Ji and Tianbin Li and Yanzhou Su and Jin Ye and Shixiang Tang and Ming Hu and Junjun He},\nyear={2025},\neprint={2510.15710},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.15710},\nüôè Acknowledgments\nWe sincerely thank the following projects and their contributors for their invaluable open-source contributions that made this research possible:\nBagel - Foundation model architecture and training methodology inspiration\nHealthGPT - Medical domain adaptation and evaluation framework\nVLMEvalKit - Comprehensive evaluation toolkit for vision-language models",
    "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated": "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-4B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "prithivMLmods/Qwen3-VL-8B-Instruct-abliterated": "Qwen3-VL-8B-Instruct-abliterated\nKey Highlights\nQuick Start with Transformers\nIntended Use\nLimitations\nQwen3-VL-8B-Instruct-abliterated\nQwen3-VL-8B-Instruct-abliterated is an abliterated (v1.0) variant of Qwen3-VL-8B-Instruct, designed for Abliterated Reasoning and Captioning.\nThis model is fine-tuned to produce highly detailed, descriptive, and reasoning-focused outputs across a wide range of visual and multimodal contexts, including complex, sensitive, or nuanced content. It supports varied image resolutions and aspect ratios while maintaining interpretive coherence and descriptive accuracy.\nKey Highlights\nAbliterated / Uncensored Captioning\nFine-tuned to bypass conventional content filters while preserving factual, descriptive, and reasoning-rich outputs.\nHigh-Fidelity Reasoning and Descriptions\nGenerates in-depth captions and reasoning for general, artistic, technical, abstract, and low-context images.\nRobust Across Aspect Ratios\nPerforms consistently on wide, tall, square, panoramic, and irregular image dimensions.\nVariational Detail Control\nCapable of generating outputs ranging from concise summaries to intricate, multi-level descriptive reasoning.\nFoundation on Qwen3-VL-8B-Instruct Architecture\nBuilt upon Qwen3-VL-8B-Instruct‚Äôs multimodal reasoning, comprehension, and instruction-following framework.\nMultilingual Output Capability\nPrimarily outputs in English, but adaptable to multiple languages via prompt engineering.\nQuick Start with Transformers\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Qwen3-VL-8B-Instruct-abliterated\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen3-VL-8B-Instruct-abliterated\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Provide a detailed caption and reasoning for this image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n).to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False\n)\nprint(output_text)\nIntended Use\nThis model is suited for:\nGenerating detailed, unfiltered captions and reasoning for general-purpose and artistic datasets.\nResearch in content moderation, red-teaming, and generative safety analysis.\nEnabling descriptive captioning and reasoning for datasets typically excluded from mainstream models.\nCreative and exploratory applications such as storytelling, visual interpretation, and multimodal reasoning.\nCaptioning and reasoning for non-standard, stylized, or abstract visual content.\nLimitations\nMay generate explicit, sensitive, or offensive content depending on the prompt and input image.\nNot suitable for production environments that require strict content filtering or moderation.\nOutput tone, style, and reasoning depth can vary depending on phrasing and visual complexity.\nMay show variability in performance on synthetic or highly abstract visuals.",
    "huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated": "huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-8B-Thinking created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-8B-Thinking-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "heathcliff01/Kaloscope": "LSNet Ëâ∫ÊúØÂÆ∂È£éÊ†ºÂàÜÁ±ªÊ®°Âûã Model Card\nÊ®°ÂûãÊ¶ÇËø∞\nÊ®°ÂûãÊèèËø∞\nÊû∂ÊûÑÁâπÁÇπ\nËÆ≠ÁªÉÊï∞ÊçÆ\nÊï∞ÊçÆÊù•Ê∫ê\nÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ\nËÆ≠ÁªÉÈÖçÁΩÆ\nÁ°¨‰ª∂ÁéØÂ¢É\nËÆ≠ÁªÉÂèÇÊï∞\nÊÄßËÉΩÊåáÊ†á\nÊ®°ÂûãÊÄßËÉΩ\nÂàÜÁ±ªÊÄßËÉΩ\nÊé®ÁêÜÊÄßËÉΩ\n‰ΩøÁî®ÊñπÊ≥ï\nÁéØÂ¢ÉË¶ÅÊ±Ç\nÂü∫Êú¨‰ΩøÁî®\nComfyuiÂÜÖ‰ΩøÁî®\nÁõ∏ÂÖ≥ËµÑÊ∫ê\nÂºïÁî®‰ø°ÊÅØ\nÊõ¥Êñ∞Êó•Âøó\nv1.0 (2025Âπ¥10Êúà)\nv1.1 (2025Âπ¥10Êúà)\nLSNet Ëâ∫ÊúØÂÆ∂È£éÊ†ºÂàÜÁ±ªÊ®°Âûã Model Card\nÊ®°ÂûãÊ¶ÇËø∞\nÊ®°ÂûãÂêçÁß∞:\nKaloscope Artist Style Classification ModelÊ®°ÂûãÁâàÊú¨: v1.0ÂèëÂ∏ÉÊó•Êúü: 2025Âπ¥10ÊúàÊ®°ÂûãÁ±ªÂûã: ÂõæÂÉèÂàÜÁ±ª (Ëâ∫ÊúØÂÆ∂È£éÊ†ºËØÜÂà´)Êû∂ÊûÑ: LSNet (See Large, Focus Small)\nÊ®°ÂûãÊèèËø∞\nÊú¨Ê®°ÂûãÂü∫‰∫éLSNetÊû∂ÊûÑÊûÑÂª∫Ôºå‰∏ìÈó®Áî®‰∫éËØÜÂà´ÂíåÂàÜÁ±ª‰∏çÂêåËâ∫ÊúØÂÆ∂ÁöÑÁªòÁîªÈ£éÊ†º„ÄÇLSNetÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßËßÜËßâÊ®°ÂûãÔºåÁÅµÊÑüÊù•Ê∫ê‰∫é‰∫∫Á±ªËßÜËßâÁ≥ªÁªüÁöÑÂä®ÊÄÅÂºÇÂ∞∫Â∫¶ËÉΩÂäõÔºåÂç≥\"ÁúãÂ§ßÂ±ÄÔºåËÅöÁÑ¶ÁªÜËäÇ\"ÁöÑÁâπÊÄß„ÄÇ\nÊû∂ÊûÑÁâπÁÇπ\nËÆæËÆ°ÁêÜÂøµ: Âü∫‰∫é‰∫∫Á±ªËßÜËßâÁ≥ªÁªüÁöÑ\"See Large, Focus Small\"ÂéüÁêÜ\nÊ®°ÂûãÁ≥ªÂàó: ÊîØÊåÅLSNet-T„ÄÅLSNet-S„ÄÅLSNet-B‰∏âÁßçËßÑÊ®°\nÂèÇÊï∞Èáè: Á∫¶100MÂèÇÊï∞\n‰ºòÂåñÁõÆÊ†á: Âú®‰øùÊåÅÈ´òÁ≤æÂ∫¶ÁöÑÂêåÊó∂ÂÆûÁé∞È´òÊïàÊé®ÁêÜ\nËÆ≠ÁªÉÊï∞ÊçÆ\nÊï∞ÊçÆÊù•Ê∫ê\nÊï∞ÊçÆÈõÜ: DanbooruÊï∞ÊçÆÈõÜ (Êà™Ê≠¢Âà∞2024Âπ¥10Êúà)\nÊï∞ÊçÆÁ≠õÈÄâ: ÈÄâÂèñÂõæÂÉèÊï∞ÈáèÂú®50Âº†‰ª•‰∏äÁöÑËâ∫ÊúØÂÆ∂\nÊÄªÂàÜÁ±ªÊï∞: 31,770‰∏™Ëâ∫ÊúØÂÆ∂Á±ªÂà´\nÊï∞ÊçÆÈááÊ†∑Á≠ñÁï•:\nÂõæÂÉèÊï∞ÈáèË∂ÖËøá100Âº†ÁöÑËâ∫ÊúØÂÆ∂ÔºöÈÄâÂèñIDÊúÄÈù†ÂêéÁöÑ100Âº†ÂõæÂÉè\nÂõæÂÉèÊï∞Èáè50-100Âº†ÁöÑËâ∫ÊúØÂÆ∂Ôºö‰ΩøÁî®ÂÖ®ÈÉ®ÂõæÂÉè\nÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ\nÂõæÂÉèÂ∞∫ÂØ∏: 224√ó224ÂÉèÁ¥†\nÊï∞ÊçÆÂ¢ûÂº∫: Ê†áÂáÜImageNetÈ¢ÑÂ§ÑÁêÜÊµÅÁ®ã\nÈ™åËØÅÈõÜÂàíÂàÜ: 5%ÁöÑÊï∞ÊçÆÁî®‰∫éÈ™åËØÅ\nËÆ≠ÁªÉÈÖçÁΩÆ\nÁ°¨‰ª∂ÁéØÂ¢É\nGPUÈÖçÁΩÆ: 8√óH20 GPU\nËÆ≠ÁªÉÊó∂Èïø: 80‰∏™epoch\nÊâπÊ¨°Â§ßÂ∞è: 256 (ÊØèGPU)\nËÆ≠ÁªÉÂèÇÊï∞\n‰ºòÂåñÂô®: AdamW\nÂ≠¶‰π†ÁéáË∞ÉÂ∫¶: Cosine Annealing\nÊï∞ÊçÆÂπ∂Ë°å: ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ (8Âç°)\nÊ®°ÂûãÂèÇÊï∞Èáè: ~100M\nÊÄßËÉΩÊåáÊ†á\nÊúÄÁªàÂáÜÁ°ÆÁéá: 84.2%\nÈ™åËØÅÊñπÂºè: Top-1ÂáÜÁ°ÆÁéá\nËØÑ‰º∞Êï∞ÊçÆ: È™åËØÅÈõÜ (5%ÁöÑÊï∞ÊçÆ)\nÊ®°ÂûãÊÄßËÉΩ\nÂàÜÁ±ªÊÄßËÉΩ\nÊåáÊ†á\nÊï∞ÂÄº\nTop-1 ÂáÜÁ°ÆÁéá\n84.2%\nÊÄªÁ±ªÂà´Êï∞\n31,770\nÂèÇÊï∞Èáè\n~100M\nËÆ≠ÁªÉËΩÆÊï∞\n80 epochs\nÊé®ÁêÜÊÄßËÉΩ\nËæìÂÖ•Ê†ºÂºè: RGBÂõæÂÉèÔºå224√ó224ÂÉèÁ¥†\nËæìÂá∫Ê†ºÂºè: 31,770Áª¥Ê¶ÇÁéáÂàÜÂ∏É\nÊé®ÁêÜÈÄüÂ∫¶: È´òÊïàÊé®ÁêÜ (ÂÖ∑‰ΩìÊï∞ÂÄºÂèñÂÜ≥‰∫éÁ°¨‰ª∂)\n‰ΩøÁî®ÊñπÊ≥ï\nÁéØÂ¢ÉË¶ÅÊ±Ç\npip install torch torchvision timm\nÂü∫Êú¨‰ΩøÁî®\nimport torch\nfrom timm.models import create_model\n# Âä†ËΩΩÊ®°Âûã\nmodel = create_model('lsnet_t_artist', pretrained=True, num_classes=31770)\nmodel.eval()\n# Êé®ÁêÜ\nwith torch.no_grad():\noutput = model(input_tensor)\nprobabilities = torch.softmax(output, dim=1)\nComfyuiÂÜÖ‰ΩøÁî®\nÂÆâË£ÖcomfyuiËäÇÁÇπÔºöhttps://github.com/spawner1145/comfyui-lsnet\n‰∏ãËΩΩÊú¨‰ªìÂ∫ìÊ®°ÂûãÂç≥ÂèØ‰ΩøÁî®\nÁõ∏ÂÖ≥ËµÑÊ∫ê\nËÆ∫Êñá: LSNet: See Large, Focus Small\n‰ª£Á†Å‰ªìÂ∫ì: (https://github.com/spawner1145/lsnet-test)\nÈ¢ÑËÆ≠ÁªÉÊ®°Âûã: ÂèØÈÄöËøáHugging Face HubËé∑Âèñ\nÂºïÁî®‰ø°ÊÅØ\n@misc{wang2025lsnetlargefocussmall,\ntitle={LSNet: See Large, Focus Small},\nauthor={Ao Wang and Hui Chen and Zijia Lin and Jungong Han and Guiguang Ding},\nyear={2025},\neprint={2503.23135},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2503.23135},\n}\nÊõ¥Êñ∞Êó•Âøó\nv1.0 (2025Âπ¥10Êúà)\nÂàùÂßãÁâàÊú¨ÂèëÂ∏É\nÂü∫‰∫éDanbooruÊï∞ÊçÆÈõÜËÆ≠ÁªÉ\nÊîØÊåÅ31,770‰∏™Ëâ∫ÊúØÂÆ∂Á±ªÂà´\nËææÂà∞84.2%ÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá\nv1.1 (2025Âπ¥10Êúà)\n150epoch\nËææÂà∞85.6%ÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá\nÂÖçË¥£Â£∞Êòé: Êú¨Ê®°Âûã‰ªÖ‰æõÁ†îÁ©∂ÂíåÊïôËÇ≤Áî®ÈÄî„ÄÇÂú®ÂïÜ‰∏öÂ∫îÁî®‰∏≠‰ΩøÁî®Êó∂ÔºåËØ∑Á°Æ‰øùÈÅµÂÆàÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÂíå‰º¶ÁêÜÂáÜÂàô„ÄÇ",
    "huawei-csl/Qwen3-1.7B-3bit-SINQ": "SINQ 3-bit Quantized Qwen3-1.7B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nSINQ 3-bit Quantized Qwen3-1.7B model\nThis repository contains the official 3-bit quantized version of the Qwen3-1.7B model using the SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-1.7B-3bit-SINQ\nBase Model: Qwen/Qwen3-1.7B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT3\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-1.7B-3bit-SINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-1.7B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 3-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=3,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"sinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "metascroy/Qwen3-4B-int8-int4-unsloth": "Uploaded  model\nFinetuning + HF upload script\nExporting to ExecuTorch\nRunning in a mobile app\nUploaded  model\nDeveloped by: metascroy\nLicense: apache-2.0\nFinetuned from model : unsloth/Qwen3-4B\nThis qwen3 model was trained 2x faster with Unsloth and Huggingface's TRL library.\nFinetuning + HF upload script\n# =========================================================================================\n# Fine-tuning script based on https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%281B_and_3B%29-Conversational.ipynb\n# This script and HF checkpoint are only intended to showcase how to do finetuning in a way compatible with ExecuTorch\n# Only 10 steps are done, and quality of the finetuned model is not evaluated\n# =========================================================================================\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import (\nget_chat_template,\nstandardize_data_formats,\nstandardize_sharegpt,\ntrain_on_responses_only,\n)\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\nfrom transformers import DataCollatorForSeq2Seq\nimport torch\nimport torch.nn as nn\nbatch_size = 2\nlearning_rate = 2e-5\ngradient_accumulation_steps = 4\nmax_steps = 10\nfull_finetuning = True\nqat_scheme = \"int8-int4\"\noutput_dir = \"/tmp/unsloth_example\"\nmodel_id = \"unsloth/Qwen3-4B\"\nchat_template = \"qwen3\"\nmax_seq_length = 2048\ndtype = torch.bfloat16\nload_in_4bit = False\n################################################################################\n# Define model/tokenizer\n################################################################################\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name=model_id,\nmax_seq_length=max_seq_length,\ndtype=dtype,\nload_in_4bit =load_in_4bit,\nfull_finetuning=full_finetuning,\nqat_scheme=qat_scheme,\n)\ntokenizer = get_chat_template(tokenizer, chat_template = chat_template)\nprint(\"MODEL AFTER LOADING\")\nprint(model)\n################################################################################\n# Untie model weights\n################################################################################\ndef untie_word_embeddings_(model):\n\"\"\"Untie input and output embeddings in a Hugging Face causal LM.\"\"\"\n# 1) Persist setting in config\nif hasattr(model.config, \"tie_word_embeddings\"):\nmodel.config.tie_word_embeddings = False\n# 2) Find input and output embeddings\nin_emb = model.get_input_embeddings()         # nn.Embedding\nout_proj = model.get_output_embeddings() or getattr(model, \"lm_head\", None)\nif out_proj is None:\nraise AttributeError(\"Couldn't locate output projection (lm_head).\")\n# (Optional) sanity: shapes should match [vocab, hidden]\nassert out_proj.weight.shape == in_emb.weight.shape, (\nf\"Shape mismatch: out_proj {out_proj.weight.shape} vs in_emb {in_emb.weight.shape}\"\n)\n# 3) Only clone if they are actually tied (shared storage)\nif out_proj.weight.data_ptr() == in_emb.weight.data_ptr():\nwith torch.no_grad():\nW = in_emb.weight.detach().clone()\nout_proj.weight = nn.Parameter(W)  # new storage, keeps dtype/device\n# 4) Prevent future automatic re-tying\ndef _no_tie(self):\nreturn\nmodel.tie_weights = _no_tie.__get__(model, model.__class__)\n# 5) Verify no shared storage\nassert out_proj.weight.data_ptr() != in_emb.weight.data_ptr(), \"Embeddings still tied!\"\nreturn model\nmodel = untie_word_embeddings_(model)\nprint(\"MODEL AFTER UNTYING\")\nprint(model)\nprint(model.config)\n################################################################################\n# Process dataset\n################################################################################\ndef formatting_prompts_func(examples):\nconvos = examples[\"conversations\"]\ntexts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\nreturn { \"text\" : texts, }\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")\ndataset = standardize_sharegpt(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\nprint(\"DATASET ENTRY\")\nprint(dataset[0])\nprint(\"\\n\\n\")\n################################################################################\n# Define trainer\n################################################################################\ntrainer = SFTTrainer(\nmodel=model,\ntokenizer=tokenizer,\ntrain_dataset=dataset,\ndataset_text_field=\"text\",\nmax_seq_length=max_seq_length,\npacking=False,\nargs=SFTConfig(\nper_device_train_batch_size=batch_size,\ngradient_accumulation_steps=gradient_accumulation_steps,\nwarmup_steps=5,\nnum_train_epochs=1,\nmax_steps=max_steps,\nlearning_rate=learning_rate,\nlogging_steps=1,\noptim=\"adamw_8bit\",\nweight_decay=0.01,\nlr_scheduler_type=\"linear\",\nseed=3407,\noutput_dir=\"outputs\",\nreport_to=\"none\",\n),\n)\n################################################################################\n# Do fine tuning\n################################################################################\nprint(\"DOING FINETUNING\")\ntrainer_stats = trainer.train()\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\nf\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\n################################################################################\n# Save model\n################################################################################\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n################################################################################\n# Convert model\n################################################################################\nfrom torchao.quantization import Int8DynamicActivationIntxWeightConfig, IntxWeightOnlyConfig, ModuleFqnToConfig, quantize_\nfrom torchao.quantization.qat import QATConfig\nfrom torchao.quantization.granularity import PerGroup, PerAxis\nfrom transformers import TorchAoConfig\nbase_config = Int8DynamicActivationIntxWeightConfig(weight_dtype=torch.int4, weight_granularity=PerGroup(32))\nquantize_(model, QATConfig(base_config, step=\"convert\"))\n################################################################################\n# Quantize embeddings to 8-bit with PTQ since they are not supported by QAT yet\n################################################################################\nembedding_fqn = \"model.embed_tokens\"\nembedding_config = IntxWeightOnlyConfig(weight_dtype=torch.int8, granularity=PerAxis(0))\nquantize_(model, embedding_config, lambda m, fqn: fqn == embedding_fqn)\n################################################################################\n# Attach quantization config to model\n################################################################################\nquant_config = ModuleFqnToConfig({\"_default\": base_config, embedding_fqn: embedding_config})\nquantization_config = TorchAoConfig(quant_type=quant_config, include_input_output_embeddings=True, modules_to_not_convert=[])\nmodel.config.quantization_config = TorchAoConfig(base_config)\nprint('MODEL AFTER CONVERT', model)\n################################################################################\n# Push converted model to hub\n################################################################################\nfrom huggingface_hub import get_token, whoami\ndef _get_username():\ntoken = get_token()\nusername = whoami(token=token)[\"name\"]\nreturn username\nusername = _get_username()\nmodel_name = model_id.split(\"/\")[-1]\nsave_to = f\"{username}/{model_name}-{qat_scheme}-unsloth\"\nmodel.push_to_hub(save_to, safe_serialization=False)\ntokenizer.push_to_hub(save_to)\n################################################################################\n# Load converted from hub and inspect\n################################################################################\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(save_to)\nprint('model', model)\nprint(\"model.embed_tokens.weight\", model.model.embed_tokens.weight)\nprint(\"model.layers[0].self_attn.q_proj.weight\", model.model.layers[0].self_attn.q_proj.weight)\nprint(\"lm_head.weight\", model.lm_head.weight)\nExporting to ExecuTorch\nWe can run the quantized model on a mobile phone using ExecuTorch.  We first set up ExecuTorch:\ngit clone https://github.com/pytorch/executorch.git\npushd executorch\ngit checkout release/1.0\ngit submodule update --init --recursive\npython install_executorch.py\npopd\nExecuTorch's LLM export scripts require the checkpoint keys and parameters have certain names, which differ from those used in Hugging Face.\nSo we first use a script that converts the Hugging Face checkpoint key names to ones that ExecuTorch expects:\nThe following script does this for you.\npushd executorch\npython -m executorch.examples.models.qwen3.convert_weights $(hf download metascroy/Qwen3-4B-int8-int4-unsloth) pytorch_model_converted.bin\npopd\nOnce we have the checkpoint, we export it to ExecuTorch with a max_seq_length/max_context_length of 1024 to the XNNPACK backend as follows.\n(Note: ExecuTorch LLM export script requires config.json have certain key names. The correct config to use for the LLM export script is located at examples/models/qwen3/config/4b_config.json within the ExecuTorch repo.)\npushd executorch\npython -m executorch.examples.models.llama.export_llama \\\n--model \"qwen3_4b\" \\\n--checkpoint pytorch_model_converted.bin \\\n--params examples/models/qwen3/config/4b_config.json \\\n--output_name model.pte \\\n-kv \\\n--use_sdpa_with_kv_cache \\\n-X \\\n--xnnpack-extended-ops \\\n--max_context_length 1024 \\\n--max_seq_length 1024 \\\n--dtype fp32 \\\n--metadata '{\"get_bos_id\":199999, \"get_eos_ids\":[200020,199999]}'\npopd\nWe can also uplaod the pte file to Hugging Face as follows:\nhf upload metascroy/Qwen3-4B-int8-int4-unsloth executorch/model.pte\n(We try to keep these instructions up-to-date, but if you find they do not work, check out our CI test in ExecuTorch for the latest source of truth, and let us know we need to update our model card.)\nRunning in a mobile app\nAfter that you can run the model in an iOS mobile app using the executorch-examples repo.\nFirst clone the repo and open the Xcode project:\ngit clone https://github.com/meta-pytorch/executorch-examples.git\nopen executorch-examples/llm/apple/etLLM.xcodeproj\nOnce open, connect your iPhone and select it as the device in Xcode.\nTo build the app, press the play button. (This does require you sign the code.)\nTo run the model we just finetuned, you need to transfer the model.pte file and tokenizer.json to your phone.\nTo do this, open finder, select your phone, and drag and drop the files to copy them over.\nPlease rename the pte file to qwen3_model.pte before copying it over because the demo app requires the name begin with qwen3 to use the correct prompt template.\nIn the etLLM app, you can select the model and tokenizer to use by browsing your file system for the \"qwen3_model.pte\" and \"tokenizer.json\" files we just copied to the phone.",
    "gghfez/GLM-4.6-control-vectors": "gghfez/GLM-4.6-control-vectors\nUsage\nLlama.cpp / IK_Llama.cpp Example\nLimitations\ngghfez/GLM-4.6-control-vectors\nCreative Writing control-vectors for zai-org/GLM-4.6\nFeedback is welcome and would be very helpful.\nUsage\nApply the debias vector and either the positive or negative vector when starting llama-server.\nIf both are applied, they will cancel each other out.\nYou can use either --control-vector [/path/to/vector.gguf] or --control-vector-scaled [/path/to/vector.gguf] [scale factor]\nThe debias vector must be set to 1.0\nIMPORTANT: The positive and negative axis control vectors must be used along with the relevant de-bias control vector - they cannot be used on their own!\nLlama.cpp / IK_Llama.cpp Example\nCreative writing\nllama-server --model GLM-4.6-UD-IQ2_XXS-00001-of-00003.gguf [your usual CLI arguments] \\\n--control-vector-scaled glm-4.6_honesty_vs_machiavellianism__debias.gguf 1.0 \\\n--control-vector-scaled glm-4.6_honesty_vs_machiavellianism__machiavellianism.gguf 1.0 \\\nCreative Writing without reasoning\nllama-server --model GLM-4.6-UD-IQ2_XXS-00001-of-00003.gguf [your usual CLI arguments] \\\n--chat-template-kwargs '{\"enable_thinking\": false}' \\\n--control-vector-scaled glm-4.6_honesty_vs_machiavellianism__debias.gguf 1.0 \\\n--control-vector-scaled glm-4.6_honesty_vs_machiavellianism__machiavellianism.gguf 1.0 \\\nAssistant\nllama-server --model GLM-4.6-IQ3_KS-00001-of-00004.gguf [your usual CLI arguments] \\\n--control-vector-scaled  glm-4.6_communication__debias.gguf 1.0 \\\n--control-vector-scaled  glm-4.6_communication__direct_communication.gguf 1.0 \\\nLimitations\nWith reasoning enabled on extreme quants like IQ2_XXS, very simple prompts like \"Hi\" may result in irrelevant replies.",
    "alphaXiv/trm-model-maze": "TRM Model for Maze Solving\nModel Description\nIntended Use\nPrimary Use\nOut-of-Scope Use\nLimitations and Bias\nTraining Data\nEvaluation Results\nRepository\nTRM Model for Maze Solving\nModel Description\nThis is a Tiny Recursive Model (TRM) fine-tuned for solving maze navigation tasks. The model implements recursive reasoning to find paths in 30x30 grid mazes.\nDeveloped by: alphaXiv\nModel type: TRM-Attention\nLanguage(s) (NLP): N/A (grid-based reasoning)\nLicense: MIT\nFinetuned from model: Custom TRM architecture\nIntended Use\nPrimary Use\nThis model is designed to solve maze pathfinding problems by predicting the correct sequence of moves to navigate from start to goal in grid-based mazes.\nOut-of-Scope Use\nNot intended for general NLP tasks, image classification, or other domains outside maze solving.\nLimitations and Bias\nTrained only on synthetic maze data\nMay not generalize to mazes of different sizes or complexities\nPerformance may degrade on mazes with unusual patterns\nTraining Data\nThe model was trained on a dataset of 30x30 grid mazes with hard difficulty levels. The dataset includes:\nStart and goal positions\nWall configurations\nCorrect path sequences\nEvaluation Results\nMetric\nClaimed\nAchieved\nExact Accuracy\n85.3%\n83.67% ¬± 2.28%\nResults from independent reproduction study.\nRepository\nhttps://github.com/alphaXiv/TinyRecursiveModels",
    "INSAIT-Institute/spear1-franka": "SPEAR-1 model card\nModel description\nUse with ü§ó Transformers\nEnvironment setup\nExample usage\nAction space\nCommunity Feedback\nSummary\nSPEAR-1 model card\nSPEAR-1 is a cutting-edge Vision-Language-Action (VLA) model capable of achieving performance superior or on par with state-of-the-art models such as pi0-FAST and pi0.5\non multiple embodiments while being trained on 20x less robot data.\nThis model was developed by INSAIT, a special unit of Sofia University St. Kliment Ohridski, in Sofia, Bulgaria.\nCode and model weights for SPEAR-1 models are free to used under the Gemma license.\nThis repo provides model weights fine-tuned for a Franka setup with one wrist and one external camera.\nModel description\nThe key to SPEAR-1's data efficiency is SPEAR-VLM, a 3D-aware VLM. SPEAR-VLM extends PaliGemma with the MoGe depth encoder and is trained on 3D VQA tasks using\nprimarily non-robot data sources, such as EgoExo-4D.\nSPEAR-1's architecture combines SPEAR-VLM with a DiT action expert. It is first pre-trained on a mixture of robot demonstration datasets from Open X Embodiment and\nthen fine-tuned for specific embodiments.\nUse with ü§ó Transformers\nWe provide a fully AutoModel compatible implementation of SPEAR-1 that can be used via transformers.\nEnvironment setup\nThe current implementation requires the following additional dependencies: roma, timm, flash-attn.\nHere is a snippet to set up a working environment for inference via uv:\nInstall uv:\nwget -qO- https://github.com/astral-sh/uv/releases/download/0.7.5/uv-installer.sh | sh\nCreate virtualenv and resolve the dependencies:\nuv venv python 3.10.12\nsource .venv/bin/activate\nuv pip install --torch-backend=cu126 roma==1.5.0 numpy==2.2.4 torch==2.6.0 torchvision==0.21.0 transformers==4.47.0 timm==1.0.15\nuv pip install --no-build-isolation setuptools psutil flash-attn==2.7.3\nExample usage\nfrom typing import Dict\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"INSAIT-Institute/spear1-franka\")\nmodel = model.to(dtype=torch.bfloat16, device=\"cuda\").eval()\nmain_image = np.asarray(Image.open(\"path/to/main_image.png\"))\nwrist_image = np.asarray(Image.open(\"path/to/wrist_image.png\"))\nee_translation = np.array([0.36, 0.0, 0.56])\nee_rotation = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\ngripper = np.array(1.0)\nmodel_input: Dict[str, np.ndarray | str | Dict[str, np.ndarray]] = {\n\"images\": {\n\"main\": main_image, # (H, W, C)\n\"wrist\": wrist_image, # (H, W, C)\n},\n\"ee_translation\": ee_translation, # (3,)\n\"ee_rotation\": ee_rotation, # (3, 3)\n\"gripper\": gripper, # (1,)\n\"language_instruction\": \"put the carrot on the blue plate\",\n\"dataset_name\": \"droid\"\n}\nmodel_output: Dict[str, np.ndarray] = model.predict_action(model_input)\nctrl_translation: np.ndarray = model_output[\"translation\"] # (S, 3)\nctrl_rotation: np.ndarray = model_output[\"rotation\"] # (S, 3, 3)\nctrl_gripper: np.ndarray = model_output[\"gripper\"] # (S, 1)\nAction space\nSPEAR-1 predicts action chunks of delta end-effector positions. Each step in the predicted action chunk is relative to the input state.\nGiven the current end-effector position [R, t] and a model prediction A_rel = [[R_1, t_1], ..., [R_n, t_n]], absolute end effector pose commands can be computed as:\nA_abs = [[R * R_1, t + t_1], ..., [R * R_n, t * t_n]]\nCommunity Feedback\nWe welcome feedback from the community to help improve SPEAR-1. If you have suggestions, encounter any issues, or have ideas for improvements, please contact us.\nSummary\nModel type: Vision-Language-Action with flow-matching action decoding\nContact: contact@insait.ai\nLicense: Gemma Terms of Use"
}