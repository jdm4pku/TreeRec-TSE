{
    "adult-ai-provider/nudify-lora-v1.41": "No model card",
    "ibm-granite/granite-embedding-107m-multilingual": "Granite-Embedding-107m-multilingual\nGranite-Embedding-107m-multilingual\nModel Summary:\nGranite-Embedding-107M-Multilingual is a 107M parameter dense biencoder embedding model from the Granite Embeddings suite that can be used to generate high quality text embeddings. This model produces embedding vectors of size 384 and is trained using a combination of open source relevance-pair datasets with permissive, enterprise-friendly license, and IBM collected and generated datasets. This model is developed using contrastive finetuning, knowledge distillation and model merging for improved performance.\nDevelopers: Granite Embedding Team, IBM\nGitHub Repository: ibm-granite/granite-embedding-models\nWebsite: Granite Docs\nPaper: Technical Report\nRelease Date: December 18th, 2024\nLicense: Apache 2.0\nSupported Languages:\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite-Embedding-107M-Multilingual for languages beyond these 12 languages.\nIntended use:\nThe model is designed to produce fixed length vector representations for a given text, which can be used for text similarity, retrieval, and search applications.\nUsage with Sentence Transformers:\nThe model is compatible with SentenceTransformer library and is very easy to use:\nFirst, install the sentence transformers library\npip install sentence_transformers\nThe model can then be used to encode pairs of text and find the similarity between their representations\nfrom sentence_transformers import SentenceTransformer, util\nmodel_path = \"ibm-granite/granite-embedding-107m-multilingual\"\n# Load the Sentence Transformer model\nmodel = SentenceTransformer(model_path)\ninput_queries = [\n' Who made the song My achy breaky heart? ',\n'summit define'\n]\ninput_passages = [\n\"Achy Breaky Heart is a country song written by Don Von Tress. Originally titled Don't Tell My Heart and performed by The Marcy Brothers in 1991. \",\n\"Definition of summit for English Language Learners. : 1 the highest point of a mountain : the top of a mountain. : 2 the highest level. : 3 a meeting or series of meetings between the leaders of two or more governments.\"\n]\n# encode queries and passages\nquery_embeddings = model.encode(input_queries)\npassage_embeddings = model.encode(input_passages)\n# calculate cosine similarity\nprint(util.cos_sim(query_embeddings, passage_embeddings))\nUsage with Huggingface Transformers:\nThis is a simple example of how to use the Granite-Embedding-107m-Multilingual model with the Transformers library and PyTorch.\nFirst, install the required libraries\npip install transformers torch\nThe model can then be used to encode pairs of text\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nmodel_path = \"ibm-granite/granite-embedding-107m-multilingual\"\n# Load the model and tokenizer\nmodel = AutoModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()\ninput_queries = [\n' Who made the song My achy breaky heart? ',\n'summit define'\n]\n# tokenize inputs\ntokenized_queries = tokenizer(input_queries, padding=True, truncation=True, return_tensors='pt')\n# encode queries\nwith torch.no_grad():\n# Queries\nmodel_output = model(**tokenized_queries)\n# Perform pooling. granite-embedding-107m-multilingual uses CLS Pooling\nquery_embeddings = model_output[0][:, 0]\n# normalize the embeddings\nquery_embeddings = torch.nn.functional.normalize(query_embeddings, dim=1)\nEvaluation:\nThe average performance of the Granite-Embedding-107M-Multilingual on Multilingual Miracl (across 18 langauges), Mintaka Retrieval (across 8 languages) and MTEB Retrieval for English (across 15 tasks), German (across 4 tasks), Spanish (across 2 tasks), Frenc (across 5 tasks), Japanese (across 2 tasks), Arabic (1 task), Korean (1 task) and Chinese (across 8 tasks) is reported below. Granite-Embedding-107M-Multilingual is twice as fast as other models with similar embedding dimensions.\nModel\nParamters (M)\nEmbedding Dimension\nMiracl (18)\nMintaka Retrieval (8)\nMTEB English (15)\nMTEB German (4)\nMTEB Spanish (2)\nMTEB French (5)\nMTEB Japanese (2)\nMTEB Arabic (1)\nMTEB Korean (1)\nMTEB Chinese (8)\ngranite-embedding-107m-multilingual\n107\n384\n55.9\n22.6\n45.3\n70.3\n48.7\n51.1\n59.0\n63.2\n70.5\n40.8\nModel Architecture:\nGranite-Embedding-107m-Multilingual is based on an encoder-only XLM-RoBERTa like transformer architecture, trained internally at IBM Research.\nModel\ngranite-embedding-30m-english\ngranite-embedding-125m-english\ngranite-embedding-107m-multilingual\ngranite-embedding-278m-multilingual\nEmbedding size\n384\n768\n384\n768\nNumber of layers\n6\n12\n6\n12\nNumber of attention heads\n12\n12\n12\n12\nIntermediate size\n1536\n3072\n1536\n3072\nActivation Function\nGeLU\nGeLU\nGeLU\nGeLU\nVocabulary Size\n50265\n50265\n250002\n250002\nMax. Sequence Length\n512\n512\n512\n512\n# Parameters\n30M\n125M\n107M\n278M\nTraining Data:\nOverall, the training data consists of four key sources: (1) unsupervised title-body paired data scraped from the web, (2) publicly available paired with permissive, enterprise-friendly license, (3) IBM-internal paired data targetting specific technical domains, and (4) IBM-generated synthetic data. The data is listed below:\nDataset\nNum. Pairs\nMultilingual MC4\n52,823,484\nMultilingual Webhose\n12,369,322\nEnglish Wikipedia\n20,745,403\nMultilingual Wikimedia\n2,911,090\nMiracl Corpus (Title-Body)\n10,120,398\nStack Exchange¬†Duplicate questions (titles)\n304,525\nStack Exchange¬†Duplicate questions (titles)\n304,525\nStack Exchange Duplicate questions (bodies)\n250,519\nMachine Translations of Stack Exchange Duplicate questions (titles)\n187,195\nStack Exchange (Title, Answer) pairs\n4,067,139\nStack Exchange¬†(Title, Body) pairs\n23,978,013\nStack Exchange¬†(Title, Body) pairs\n23,978,013\nMachine Translations of Stack Exchange¬†(Title+Body, Answer) pairs\n1,827,15\nSearchQA\n582,261\nS2ORC (Title, Abstract)\n41,769,185\nWikiAnswers¬†Duplicate question pairs\n77,427,422\nCCNews\n614,664\nXSum\n226,711\nSimpleWiki\n102,225\nMachine Translated Cross Lingual Parallel Corpora\n28,376,115\nSPECTER citation triplets\n684,100\nMachine Translations of SPECTER citation triplets\n4,104,600\nNatural Questions (NQ)\n100,231\nSQuAD2.0\n87,599\nHotpotQA\n85,000\nFever\n109,810\nPubMed\n20,000,000\nMultilingual Miracl Triples\n81,409\nMultilingual MrTydi Triples\n48,715\nSadeeem Question Asnwering\n4,037\nDBPedia Title-Body Pairs\n4,635,922\nSynthetic: English Query-Wikipedia Passage\n1,879,093\nSynthetic: English Fact Verification\n9,888\nSynthetic: Multilingual Query-Wikipedia Passage\n300,266\nSynthetic: Multilingual News Summaries\n37,489\nIBM Internal Triples\n40,290\nIBM Internal Title-Body Pairs\n1,524,586\nNotably, we do not use the popular MS-MARCO retrieval dataset in our training corpus due to its non-commercial license, while other open-source models train on this dataset due to its high quality.\nInfrastructure:\nWe train Granite Embedding Models using IBM's computing cluster, Cognitive Compute Cluster, which is outfitted with NVIDIA A100 80gb GPUs. This cluster provides a scalable and efficient infrastructure for training our models over multiple GPUs.\nEthical Considerations and Limitations:\nThe data used to train the base language model was filtered to remove text containing hate, abuse, and profanity. Granite-Embedding-107m-Multilingual is finetuned on 12 languages, and has a context length of 512 tokens (longer texts will be truncated to this size).\nResources\n‚≠êÔ∏è Learn about the latest updates with Granite: https://www.ibm.com/granite\nüìÑ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources",
    "dima806/fairface_age_image_detection": "Detects age group with about 59% accuracy based on an image.\nSee https://www.kaggle.com/code/dima806/age-group-image-classification-vit for details.\nClassification report:\nprecision    recall  f1-score   support\n0-2     0.7803    0.7500    0.7649       180\n3-9     0.7998    0.7998    0.7998      1249\n10-19     0.5361    0.4236    0.4733      1086\n20-29     0.6402    0.7221    0.6787      3026\n30-39     0.4935    0.5083    0.5008      2099\n40-49     0.4848    0.4386    0.4606      1238\n50-59     0.5000    0.4814    0.4905       725\n60-69     0.4497    0.4685    0.4589       286\nmore than 70     0.6897    0.1802    0.2857       111\naccuracy                         0.5892     10000\nmacro avg     0.5971    0.5303    0.5459     10000\nweighted avg     0.5863    0.5892    0.5844     10000",
    "unsloth/Llama-3.3-70B-Instruct-GGUF": "Finetune Llama 3.3, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nunsloth/Llama-3.3-70B-Instruct-GGUF\n‚ú® Finetune for Free\nSpecial Thanks\nModel Information\nIntended Use\nHow to use\nUse with transformers\nTool use with transformers\nUse with bitsandbytes\nUse with llama\nHardware and Software\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nBenchmarks - English Text\nInstruction tuned models\nResponsibility & Safety\nResponsible deployment\nEvaluations\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n2. Child Safety\nCommunity\nEthical Considerations and Limitations\nSee our collection for all versions of Llama 3.3 including GGUF, 4-bit and original 16-bit formats.\nFinetune Llama 3.3, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\nunsloth/Llama-3.3-70B-Instruct-GGUF\nFor more details on the model, please go to Meta's original model card\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models\nModel Information\nThe Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.3 (text only)\nA new mix of publicly available online data.\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.3 model. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date:\n70B Instruct: December 6, 2024\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Llama-3.3-70B-Instruct, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nSee the snippet below for usage with Transformers:\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nTool use with transformers\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting here.\nTool use is also supported through chat templates in Transformers.\nHere is a quick example showing a single simple tool:\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n\"\"\"\nGet the current temperature at a location.\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nReturns:\nThe current temperature at the specified location in the specified units, as a float.\n\"\"\"\nreturn 22.  # A real function should probably actually get the temperature!\n# Next, create a chat and apply the chat template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n{\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\nand then call the tool and append the result, with the tool role, like so:\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the LLaMA prompt format docs and the Transformers tool use documentation.\nUse with bitsandbytes\nThe model checkpoints can be used in 8-bit and 4-bit for further memory optimisations using bitsandbytes and transformers\nSee the snippet below for usage:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nquantized_model = AutoModelForCausalLM.from_pretrained(\nmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nTo load in 4-bit simply pass load_in_4bit=True\nUse with llama\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 3.3 70B\n7.0M\n700\n2,040\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmarks - English Text\nIn this section, we report the results for Llama 3.3 relative to our previous models.\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 8B Instruct\nLlama 3.1 70B Instruct\nLlama-3.3 70B Instruct\nLlama 3.1 405B Instruct\nMMLU (CoT)\n0\nmacro_avg/acc\n73.0\n86.0\n86.0\n88.6\nMMLU Pro (CoT)\n5\nmacro_avg/acc\n48.3\n66.4\n68.9\n73.3\nSteerability\nIFEval\n80.4\n87.5\n92.1\n88.6\nReasoning\nGPQA Diamond (CoT)\n0\nacc\n31.8\n48.0\n50.5\n49.0\nCode\nHumanEval\n0\npass@1\n72.6\n80.5\n88.4\n89.0\nMBPP EvalPlus (base)\n0\npass@1\n72.8\n86.0\n87.6\n88.6\nMath\nMATH (CoT)\n0\nsympy_intersection_score\n51.9\n68.0\n77.0\n73.8\nTool Use\nBFCL v2\n0\noverall_ast_summary/macro_avg/valid\n65.4\n77.5\n77.3\n81.1\nMultilingual\nMGSM\n0\nem\n68.9\n86.9\n91.1\n91.6\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.3 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and ToneBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines.\nLlama 3.3 systems\nLarge language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.As part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew capabilities\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teamingFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of the Llama 3.3 model could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablementOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "ggml-org/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF": "Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF\nUse with llama.cpp\nCLI:\nServer:\nNOTE: this model is used for testing\nQwen2.5-Coder-1.5B-IQ3_XXS-GGUF\nThis model was converted to GGUF format from Qwen/Qwen2.5-Coder-1.5B using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo ngxson/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF --hf-file qwen2.5-coder-1.5b-iq3_xxs-imat.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo ngxson/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF --hf-file qwen2.5-coder-1.5b-iq3_xxs-imat.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo ngxson/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF --hf-file qwen2.5-coder-1.5b-iq3_xxs-imat.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo ngxson/Qwen2.5-Coder-1.5B-IQ3_XXS-GGUF --hf-file qwen2.5-coder-1.5b-iq3_xxs-imat.gguf -c 2048",
    "deepseek-ai/DeepSeek-V2.5-1210": "DeepSeek-V2.5-1210\n1. Introduction\n2. How to run locally\nInference with Huggingface's Transformers\nInference with vLLM (recommended)\nFunction calling\nJSON output\nFIM completion\n3. License\n4. Citation\n5. Contact\nPaper LinküëÅÔ∏è\nDeepSeek-V2.5-1210\n1. Introduction\nDeepSeek-V2.5-1210 is an upgraded version of DeepSeek-V2.5, with improvements across various capabilities:\nMathematical: Performance on the MATH-500 benchmark has improved from 74.8% to 82.8% .\nCoding: Accuracy on the LiveCodebench (08.01 - 12.01) benchmark has increased from 29.2% to 34.38% .\nWriting and Reasoning: Corresponding improvements have been observed in internal test datasets.\nAdditionally, the new version of the model has optimized the user experience for file upload and webpage summarization functionalities.\n2. How to run locally\nTo utilize DeepSeek-V2.5 in BF16 format for inference, 80GB*8 GPUs are required.\nInference with Huggingface's Transformers\nYou can directly employ Huggingface's Transformers for model inference.\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nmodel_name = \"deepseek-ai/DeepSeek-V2.5-1210\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# `max_memory` should be set based on your devices\nmax_memory = {i: \"75GB\" for i in range(8)}\n# `device_map` cannot be set to `auto`\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"sequential\", torch_dtype=torch.bfloat16, max_memory=max_memory, attn_implementation=\"eager\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\nmessages = [\n{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\nThe complete chat template can be found within tokenizer_config.json located in the huggingface model repository.\nNote: The chat template has been updated compared to the previous DeepSeek-V2-Chat version.\nAn example of chat template is as belows:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>{user_message_1}<ÔΩúAssistantÔΩú>{assistant_message_1}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>{user_message_2}<ÔΩúAssistantÔΩú>\nYou can also add an optional system message:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system_message}<ÔΩúUserÔΩú>{user_message_1}<ÔΩúAssistantÔΩú>{assistant_message_1}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>{user_message_2}<ÔΩúAssistantÔΩú>\nInference with vLLM (recommended)\nTo utilize vLLM for model inference, please merge this Pull Request into your vLLM codebase: https://github.com/vllm-project/vllm/pull/4650.\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmax_model_len, tp_size = 8192, 8\nmodel_name = \"deepseek-ai/DeepSeek-V2.5-1210\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\nmessages_list = [\n[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n[{\"role\": \"user\", \"content\": \"Translate the following content into Chinese directly: DeepSeek-V2 adopts innovative architectures to guarantee economical training and efficient inference.\"}],\n[{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\nFunction calling\nFunction calling allows the model to call external tools to enhance its capabilities.\nHere is an example:\n# Assume that `model` and `tokenizer` are loaded\nmodel.generation_config = GenerationConfig(do_sample=False, max_new_tokens=128, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\ntool_system_prompt = \"\"\"You are a helpful Assistant.\n## Tools\n### Function\nYou have the following functions available:\n- `get_current_weather`:\n\"\"\"\ntool_call_messages = [{\"role\": \"system\", \"content\": tool_system_prompt}, {\"role\": \"user\", \"content\": \"What's the weather like in Tokyo and Paris?\"}]\ntool_call_inputs = tokenizer.apply_chat_template(tool_call_messages, add_generation_prompt=True, return_tensors=\"pt\")\ntool_call_outputs = model.generate(tool_call_inputs.to(model.device))\n# Generated text: '<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>function<ÔΩútool‚ñÅsepÔΩú>get_current_weather\\n<ÔΩútool‚ñÅcall‚ñÅendÔΩú>\\n<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>function<ÔΩútool‚ñÅsepÔΩú>get_current_weather\\n<ÔΩútool‚ñÅcall‚ñÅendÔΩú><ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'\n# Mock response of calling `get_current_weather`\ntool_messages = [{\"role\": \"tool\", \"content\": '{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": null}'}, {\"role\": \"tool\", \"content\": '{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": null}'}]\ntool_inputs = tokenizer.apply_chat_template(tool_messages, add_generation_prompt=False, return_tensors=\"pt\")[:, 1:]\ntool_inputs = torch.cat([tool_call_outputs, tool_inputs.to(model.device)], dim=1)\ntool_outputs = model.generate(tool_inputs)\n# Generated text: The current weather in Tokyo is 10 degrees, and in Paris, it is 22 degrees.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\nJSON output\nYou can use JSON Output Mode to ensure the model generates a valid JSON object. To active this mode, a special instruction should be appended to your system prompt.\n# Assume that `model` and `tokenizer` are loaded\nmodel.generation_config = GenerationConfig(do_sample=False, max_new_tokens=128, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\nuser_system_prompt = 'The user will provide some exam text. Please parse the \"question\" and \"answer\" and output them in JSON format.'\njson_system_prompt = f\"\"\"{user_system_prompt}\n## Response Format\nReply with JSON object ONLY.\"\"\"\njson_messages = [{\"role\": \"system\", \"content\": json_system_prompt}, {\"role\": \"user\", \"content\": \"Which is the highest mountain in the world? Mount Everest.\"}]\njson_inputs = tokenizer.apply_chat_template(json_messages, add_generation_prompt=True, return_tensors=\"pt\")\njson_outpus = model.generate(json_inputs.to(model.device))\n# Generated text: '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'\nFIM completion\nIn FIM (Fill In the Middle) completion, you can provide a prefix and an optional suffix, and the model will complete the content in between.\n# Assume that `model` and `tokenizer` are loaded\nmodel.generation_config = GenerationConfig(do_sample=False, max_new_tokens=128, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\nprefix = \"\"\"def quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[0]\nleft = []\nright = []\n\"\"\"\nsuffix = \"\"\"\nif arr[i] < pivot:\nleft.append(arr[i])\nelse:\nright.append(arr[i])\nreturn quick_sort(left) + [pivot] + quick_sort(right)\"\"\"\nfim_prompt = f\"<ÔΩúfim‚ñÅbeginÔΩú>{prefix}<ÔΩúfim‚ñÅholeÔΩú>{suffix}<ÔΩúfim‚ñÅendÔΩú>\"\nfim_inputs = tokenizer(fim_prompt, add_special_tokens=True, return_tensors=\"pt\").input_ids\nfim_outputs = model.generate(fim_inputs.to(model.device))\n# Generated text: \"    for i in range(1, len(arr)):<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\"\n3. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-V2 Base/Chat models is subject to the Model License. DeepSeek-V2 series (including Base and Chat) supports commercial use.\n4. Citation\n@misc{deepseekv2,\ntitle={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},\nauthor={DeepSeek-AI},\nyear={2024},\neprint={2405.04434},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n5. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "ai-sage/Giga-Embeddings-instruct": "Giga-Embeddings-instruct\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\nRequirements\nTransformers\nSentence Transformers\nLangChain\n–ò–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\n–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏\nFAQ\n–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è\nGiga-Embeddings-instruct\nBase Decoder-only LLM: GigaChat-3b\nPooling Type: Latent-Attention\nEmbedding Dimension: 2048\n–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª—è—Ö, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –Ω–∞—à–µ–π —Å—Ç–∞—Ç—å–µ.\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω –ø—Ä–∏–º–µ—Ä –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤.\nRequirements\npip install -q transformers==4.51.0 sentence-transformers==5.1.1 flash-attn langchain_community langchain_huggingface langchain_gigachat\nTransformers\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery: {query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of Russia?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of Russia is Moscow.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\ntokenizer = AutoTokenizer.from_pretrained(\n'ai-sage/Giga-Embeddings-instruct',\ntrust_remote_code=True\n)\nmodel = AutoModel.from_pretrained(\n'ai-sage/Giga-Embeddings-instruct',\nattn_implementation=\"flash_attention_2\",\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True\n)\nmodel.eval()\nmodel.cuda()\nmax_length = 4096\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nembeddings = model(**batch_dict, return_embeddings=True)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.58203125, 0.0712890625], [0.06884765625, 0.62109375]]\nSentence Transformers\nimport torch\nfrom sentence_transformers import SentenceTransformer\n# Load the model\n# We recommend enabling flash_attention_2 for better acceleration and memory saving\nmodel = SentenceTransformer(\n\"ai-sage/Giga-Embeddings-instruct\",\nmodel_kwargs={\n\"attn_implementation\": \"flash_attention_2\",\n\"torch_dtype\": torch.bfloat16,\n\"trust_remote_code\": \"True\"\n},\nconfig_kwargs={\n\"trust_remote_code\": \"True\"\n}\n)\nmodel.max_seq_length = 4096\n# The queries and documents to embed\nqueries = [\n'What is the capital of Russia?',\n'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of Russia is Moscow.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\n# Encode the queries and documents. Note that queries benefit from using a prompt\nquery_embeddings = model.encode(queries, prompt='Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: ')\ndocument_embeddings = model.encode(documents)\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.5846, 0.0702],\n#         [0.0691, 0.6207]])\nLangChain\nimport torch\nfrom langchain_huggingface import HuggingFaceEmbeddings\n# Load model\nembeddings = HuggingFaceEmbeddings(\nmodel_name='ai-sage/Giga-Embeddings-instruct',\nencode_kwargs={},\nmodel_kwargs={\n'device': 'cuda',\n'trust_remote_code': True,\n'model_kwargs': {'torch_dtype': torch.bfloat16},\n'prompts': {'query': 'Instruct: Given a question, retrieve passages that answer the question\\nQuery: '}\n}\n)\n# Tokenizer\nembeddings._client.tokenizer.tokenize(\"Hello world! I am GigaChat\")\n# Query embeddings\nquery_embeddings = embeddings.embed_query(\"Hello world!\")\nprint(f\"Your embeddings: {query_embeddings[0:20]}...\")\nprint(f\"Vector size: {len(query_embeddings)}\")\n# Document embeddings\ndocuments = [\"foo bar\", \"bar foo\"]\ndocuments_embeddings = embeddings.embed_documents(documents)\nprint(f\"Vector size: {len(documents_embeddings)} x {len(documents_embeddings[0])}\")\n–ò–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n–î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (retrieval), —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–ª—è—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –ø–µ—Ä–µ–¥ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º (query). –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ü–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞, —á—Ç–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ —Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º, –∞ –Ω–µ –ø–µ—Ä–µ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º.\n–î–ª—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (classification) –∏–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ (semantic text similarity), –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–ª—è—Ç—å –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ —Ç–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤—Å–µ—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∏–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏—Ö.\n–ü—Ä–∏–º–µ—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á:\n\"Retrieve semantically similar text\"\n\"Given a text, retrieve semantically similar text\"\n\"–î–∞–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞–π—Ç–∏ –µ–≥–æ –ø–∞—Ä–∞—Ñ—Ä–∞–∑\"\n\"–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –æ—Ç–∑—ã–≤ –Ω–∞ —Ç–æ–≤–∞—Ä –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π\"\n\"–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—É—é —Ç–µ–º—É –ø–æ –∑–∞–ø—Ä–æ—Å—É\"\n–î–ª—è retrieval-–∑–∞–¥–∞—á (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ) –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é:'–î–∞–Ω –≤–æ–ø—Ä–æ—Å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞–π—Ç–∏ –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ —Å –æ—Ç–≤–µ—Ç–æ–º'.\n–¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞.\n–ü—Ä–∏–º–µ—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è retrieval-–∑–∞–¥–∞—á:\n'–î–∞–Ω –≤–æ–ø—Ä–æ—Å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞–π—Ç–∏ –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ —Å –æ—Ç–≤–µ—Ç–æ–º'\n'Given the question, find a paragraph with the answer'\n–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–æ—Ä–∞—á–∏–≤–∞—Ç—å –≤ —à–∞–±–ª–æ–Ω: f'Instruct: {task_description}\\nQuery: {query}'. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è —Ç–µ—Å—Ç–∞–º–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ RuBQ, MIRACL. –î–ª—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.\n–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏\n–≠—Ç–∞ –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ pretrain –º–æ–¥–µ–ª—å—é GigaChat –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Å–º–µ—Å–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∏ —Ä—É—Å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\nFAQ\n–ù—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫ –∑–∞–ø—Ä–æ—Å—É?\n–î–∞, –∏–º–µ–Ω–Ω–æ —Ç–∞–∫ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å, –∏–Ω–∞—á–µ –≤—ã —É–≤–∏–¥–∏—Ç–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É. –≠—Ç–æ —Å–ø–æ—Å–æ–± –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å –ø–æ–º–æ—â—å—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ.\n–° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –¥–æ–±–∞–≤–ª—è—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n–ü–æ—á–µ–º—É –º–æ–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ –∫–∞—Ä—Ç–æ—á–∫–µ –º–æ–¥–µ–ª–∏?\n–†–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ transformers –∏ pytorch –º–æ–≥—É—Ç –≤—ã–∑—ã–≤–∞—Ç—å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ, –Ω–æ –Ω–µ–Ω—É–ª–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö.\n–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –±–æ–ª–µ–µ 4096 —Ç–æ–∫–µ–Ω–æ–≤, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.",
    "nvidia/prompt-task-and-complexity-classifier": "NemoCurator Prompt Task and Complexity Classifier\nModel Overview\nLicense\nModel Architecture\nHow to Use in NVIDIA NeMo Curator\nInput & Output\nInput\nOutput\nExamples\nSoftware Integration\nModel Version\nTraining, Testing, and Evaluation Datasets\nTraining Data\nEvaluation\nInference\nHow to Use in Transformers\nReferences\nEthical Considerations\nNemoCurator Prompt Task and Complexity Classifier\nModel Overview\nThis is a multi-headed model which classifies English text prompts across task types and complexity dimensions. Tasks are classified across 11 common categories. Complexity is evaluated across 6 dimensions and ensembled to create an overall complexity score. Further information on the taxonomies can be found below.\nThis model is ready for commercial use.\nTask types:\nOpen QA: A question where the response is based on general knowledge\nClosed QA: A question where the response is based on text/data provided with the prompt\nSummarization\nText Generation\nCode Generation\nChatbot\nClassification\nRewrite\nBrainstorming\nExtraction\nOther\nComplexity dimensions:\nOverall Complexity Score: The weighted sum of the complexity dimensions. Calculated as 0.35*CreativityScore + 0.25*ReasoningScore + 0.15*ConstraintScore + 0.15*DomainKnowledgeScore + 0.05*ContextualKnowledgeScore + 0.05*NumberOfFewShots\nCreativity: The level of creativity needed to respond to a prompt. Score range of 0-1, with a higher score indicating more creativity.\nReasoning: The extent of logical or cognitive effort required to respond to a prompt. Score range of 0-1, with a higher score indicating more reasoning\nContextual Knowledge: The background information necessary to respond to a prompt. Score range of 0-1, with a higher score indicating more contextual knowledge required outside of prompt.\nDomain Knowledge: The amount of specialized knowledge or expertise within a specific subject area needed to respond to a prompt. Score range of 0-1, with a higher score indicating more domain knowledge is required.\nConstraints: The number of constraints or conditions provided with the prompt. Score range of 0-1, with a higher score indicating more constraints in the prompt.\nNumber of Few Shots: The number of examples provided with the prompt. Score range of 0-n, with a higher score indicating more examples provided in the prompt.\nLicense\nThis model is released under the NVIDIA Open Model License Agreement.\nModel Architecture\nThe model architecture uses a DeBERTa backbone and incorporates multiple classification heads, each dedicated to a task categorization or complexity dimension. This approach enables the training of a unified network, allowing it to predict simultaneously during inference. Deberta-v3-base can theoretically handle up to 12k tokens, but default context length is set at 512 tokens.\nHow to Use in NVIDIA NeMo Curator\nNeMo Curator improves generative AI model accuracy by processing text, image, and video data at scale for training and customization. It also provides pre-built pipelines for generating synthetic data to customize and evaluate generative AI systems.\nThe inference code for this model is available through the NeMo Curator GitHub repository. Check out this example notebook to get started.\nInput & Output\nInput\nInput Type: Text\nInput Format: String\nInput Parameters: 1D\nOther Properties Related to Input: Token Limit of 512 tokens\nOutput\nOutput Type: Text/Numeric Classifications\nOutput Format: String & Numeric\nOutput Parameters: 1D\nOther Properties Related to Output: None\nExamples\nPrompt: Write a mystery set in a small town where an everyday object goes missing, causing a ripple of curiosity and suspicion. Follow the investigation and reveal the surprising truth behind the disappearance.\nTask\nComplexity\nCreativity\nReasoning\nContextual Knowledge\nDomain Knowledge\nConstraints\n# of Few Shots\nText Generation\n0.472\n0.867\n0.056\n0.048\n0.226\n0.785\n0\nPrompt: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body‚Äôs immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. Explain the above in one sentence.\nTask\nComplexity\nCreativity\nReasoning\nContextual Knowledge\nDomain Knowledge\nConstraints\n# of Few Shots\nSummarization\n0.133\n0.003\n0.014\n0.003\n0.644\n0.211\n0\nSoftware Integration\nRuntime Engine: Python 3.10 and NeMo Curator\nSupported Hardware Microarchitecture Compatibility: NVIDIA GPU, Volta‚Ñ¢ or higher (compute capability 7.0+), CUDA 12 (or above)\nPreferred/Supported Operating System(s): Ubuntu 22.04/20.04\nModel Version\nNemoCurator Prompt Task and Complexity Classifier v1.1\nTraining, Testing, and Evaluation Datasets\nTraining Data\n4024 English prompts with task distribution outlined below\nPrompts were annotated by humans according to task and complexity taxonomies\nTask distribution:\nTask\nCount\nOpen QA\n1214\nClosed QA\n786\nText Generation\n480\nChatbot\n448\nClassification\n267\nSummarization\n230\nCode Generation\n185\nRewrite\n169\nOther\n104\nBrainstorming\n81\nExtraction\n60\nTotal\n4024\nEvaluation\nFor evaluation, Top-1 accuracy metric was used, which involves matching the category with the highest probability to the expected answer. Additionally, n-fold cross-validation was used to produce n different values for this metric to verify the consistency of the results. The table below displays the average of the top-1 accuracy values for the N folds calculated for each complexity dimension separately.\nTask Accuracy\nCreative Accuracy\nReasoning Accuracy\nContextual Accuracy\nFewShots Accuracy\nDomain Accuracy\nConstraint Accuracy\nAverage of 10 Folds\n0.981\n0.996\n0.997\n0.981\n0.979\n0.937\n0.991\nInference\nEngine: PyTorch\nTest Hardware: A10G\nHow to Use in Transformers\nTo use the prompt task and complexity classifier, use the following code:\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nclass MeanPooling(nn.Module):\ndef __init__(self):\nsuper(MeanPooling, self).__init__()\ndef forward(self, last_hidden_state, attention_mask):\ninput_mask_expanded = (\nattention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n)\nsum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\nsum_mask = input_mask_expanded.sum(1)\nsum_mask = torch.clamp(sum_mask, min=1e-9)\nmean_embeddings = sum_embeddings / sum_mask\nreturn mean_embeddings\nclass MulticlassHead(nn.Module):\ndef __init__(self, input_size, num_classes):\nsuper(MulticlassHead, self).__init__()\nself.fc = nn.Linear(input_size, num_classes)\ndef forward(self, x):\nx = self.fc(x)\nreturn x\nclass CustomModel(nn.Module, PyTorchModelHubMixin):\ndef __init__(self, target_sizes, task_type_map, weights_map, divisor_map):\nsuper(CustomModel, self).__init__()\nself.backbone = AutoModel.from_pretrained(\"microsoft/DeBERTa-v3-base\")\nself.target_sizes = target_sizes.values()\nself.task_type_map = task_type_map\nself.weights_map = weights_map\nself.divisor_map = divisor_map\nself.heads = [\nMulticlassHead(self.backbone.config.hidden_size, sz)\nfor sz in self.target_sizes\n]\nfor i, head in enumerate(self.heads):\nself.add_module(f\"head_{i}\", head)\nself.pool = MeanPooling()\ndef compute_results(self, preds, target, decimal=4):\nif target == \"task_type\":\ntask_type = {}\ntop2_indices = torch.topk(preds, k=2, dim=1).indices\nsoftmax_probs = torch.softmax(preds, dim=1)\ntop2_probs = softmax_probs.gather(1, top2_indices)\ntop2 = top2_indices.detach().cpu().tolist()\ntop2_prob = top2_probs.detach().cpu().tolist()\ntop2_strings = [\n[self.task_type_map[str(idx)] for idx in sample] for sample in top2\n]\ntop2_prob_rounded = [\n[round(value, 3) for value in sublist] for sublist in top2_prob\n]\ncounter = 0\nfor sublist in top2_prob_rounded:\nif sublist[1] < 0.1:\ntop2_strings[counter][1] = \"NA\"\ncounter += 1\ntask_type_1 = [sublist[0] for sublist in top2_strings]\ntask_type_2 = [sublist[1] for sublist in top2_strings]\ntask_type_prob = [sublist[0] for sublist in top2_prob_rounded]\nreturn (task_type_1, task_type_2, task_type_prob)\nelse:\npreds = torch.softmax(preds, dim=1)\nweights = np.array(self.weights_map[target])\nweighted_sum = np.sum(np.array(preds.detach().cpu()) * weights, axis=1)\nscores = weighted_sum / self.divisor_map[target]\nscores = [round(value, decimal) for value in scores]\nif target == \"number_of_few_shots\":\nscores = [x if x >= 0.05 else 0 for x in scores]\nreturn scores\ndef process_logits(self, logits):\nresult = {}\n# Round 1: \"task_type\"\ntask_type_logits = logits[0]\ntask_type_results = self.compute_results(task_type_logits, target=\"task_type\")\nresult[\"task_type_1\"] = task_type_results[0]\nresult[\"task_type_2\"] = task_type_results[1]\nresult[\"task_type_prob\"] = task_type_results[2]\n# Round 2: \"creativity_scope\"\ncreativity_scope_logits = logits[1]\ntarget = \"creativity_scope\"\nresult[target] = self.compute_results(creativity_scope_logits, target=target)\n# Round 3: \"reasoning\"\nreasoning_logits = logits[2]\ntarget = \"reasoning\"\nresult[target] = self.compute_results(reasoning_logits, target=target)\n# Round 4: \"contextual_knowledge\"\ncontextual_knowledge_logits = logits[3]\ntarget = \"contextual_knowledge\"\nresult[target] = self.compute_results(\ncontextual_knowledge_logits, target=target\n)\n# Round 5: \"number_of_few_shots\"\nnumber_of_few_shots_logits = logits[4]\ntarget = \"number_of_few_shots\"\nresult[target] = self.compute_results(number_of_few_shots_logits, target=target)\n# Round 6: \"domain_knowledge\"\ndomain_knowledge_logits = logits[5]\ntarget = \"domain_knowledge\"\nresult[target] = self.compute_results(domain_knowledge_logits, target=target)\n# Round 7: \"no_label_reason\"\nno_label_reason_logits = logits[6]\ntarget = \"no_label_reason\"\nresult[target] = self.compute_results(no_label_reason_logits, target=target)\n# Round 8: \"constraint_ct\"\nconstraint_ct_logits = logits[7]\ntarget = \"constraint_ct\"\nresult[target] = self.compute_results(constraint_ct_logits, target=target)\n# Round 9: \"prompt_complexity_score\"\nresult[\"prompt_complexity_score\"] = [\nround(\n0.35 * creativity\n+ 0.25 * reasoning\n+ 0.15 * constraint\n+ 0.15 * domain_knowledge\n+ 0.05 * contextual_knowledge\n+ 0.05 * few_shots,\n5,\n)\nfor creativity, reasoning, constraint, domain_knowledge, contextual_knowledge, few_shots in zip(\nresult[\"creativity_scope\"],\nresult[\"reasoning\"],\nresult[\"constraint_ct\"],\nresult[\"domain_knowledge\"],\nresult[\"contextual_knowledge\"],\nresult[\"number_of_few_shots\"],\n)\n]\nreturn result\ndef forward(self, batch):\ninput_ids = batch[\"input_ids\"]\nattention_mask = batch[\"attention_mask\"]\noutputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\nlast_hidden_state = outputs.last_hidden_state\nmean_pooled_representation = self.pool(last_hidden_state, attention_mask)\nlogits = [\nself.heads[k](mean_pooled_representation)\nfor k in range(len(self.target_sizes))\n]\nreturn self.process_logits(logits)\nconfig = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\ntokenizer = AutoTokenizer.from_pretrained(\n\"nvidia/prompt-task-and-complexity-classifier\"\n)\nmodel = CustomModel(\ntarget_sizes=config.target_sizes,\ntask_type_map=config.task_type_map,\nweights_map=config.weights_map,\ndivisor_map=config.divisor_map,\n).from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\nmodel.eval()\nprompt = [\"Prompt: Write a Python script that uses a for loop.\"]\nencoded_texts = tokenizer(\nprompt,\nreturn_tensors=\"pt\",\nadd_special_tokens=True,\nmax_length=512,\npadding=\"max_length\",\ntruncation=True,\n)\nresult = model(encoded_texts)\nprint(result)\n# {'task_type_1': ['Code Generation'], 'task_type_2': ['Text Generation'], 'task_type_prob': [0.767], 'creativity_scope': [0.0826], 'reasoning': [0.0632], 'contextual_knowledge': [0.056], 'number_of_few_shots': [0], 'domain_knowledge': [0.9803], 'no_label_reason': [0.0], 'constraint_ct': [0.5578], 'prompt_complexity_score': [0.27822]}\nReferences\nDeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nDeBERTa: Decoding-enhanced BERT with Disentangled Attention\nTraining language models to follow instructions with human feedback\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.",
    "deepseek-ai/deepseek-vl2-small": "1. Introduction\n2. Model Summary\n3. Quick Start\nInstallation\nNotifications\nSimple Inference Example\nGradio Demo (TODO)\n4. License\n5. Citation\n6. Contact\n1. Introduction\nIntroducing DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition,  document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively.\nDeepSeek-VL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models.\nDeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding\nGithub Repository\nZhiyu Wu*, Xiaokang Chen*, Zizheng Pan*, Xingchao Liu*, Wen Liu**, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruan*** (* Equal Contribution, ** Project Lead, *** Corresponding author)\n2. Model Summary\nDeepSeek-VL2-small is built on DeepSeekMoE-16B.\n3. Quick Start\nInstallation\nOn the basis of Python >= 3.8 environment, install the necessary dependencies by running the following command:\npip install -e .\nNotifications\nWe suggest to use a temperature T <= 0.7 when sampling. We observe a larger temperature decreases the generation quality.\nTo keep the number of tokens managable in the context window, we apply dynamic tiling strategy to <=2 images. When there are >=3 images, we directly pad the images to 384*384 as inputs without tiling.\nThe main difference between DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2 is the base LLM.\nSimple Inference Example\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom deepseek_vl.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\nfrom deepseek_vl.utils.io import load_pil_images\n# specify the path to the model\nmodel_path = \"deepseek-ai/deepseek-vl2-small\"\nvl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\nvl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n## single image conversation example\nconversation = [\n{\n\"role\": \"<|User|>\",\n\"content\": \"<image>\\n<|ref|>The giraffe at the back.<|/ref|>.\",\n\"images\": [\"./images/visual_grounding.jpeg\"],\n},\n{\"role\": \"<|Assistant|>\", \"content\": \"\"},\n]\n## multiple images (or in-context learning) conversation example\n# conversation = [\n#     {\n#         \"role\": \"User\",\n#         \"content\": \"<image_placeholder>A dog wearing nothing in the foreground, \"\n#                    \"<image_placeholder>a dog wearing a santa hat, \"\n#                    \"<image_placeholder>a dog wearing a wizard outfit, and \"\n#                    \"<image_placeholder>what's the dog wearing?\",\n#         \"images\": [\n#             \"images/dog_a.png\",\n#             \"images/dog_b.png\",\n#             \"images/dog_c.png\",\n#             \"images/dog_d.png\",\n#         ],\n#     },\n#     {\"role\": \"Assistant\", \"content\": \"\"}\n# ]\n# load images and prepare for inputs\npil_images = load_pil_images(conversation)\nprepare_inputs = vl_chat_processor(\nconversations=conversation,\nimages=pil_images,\nforce_batchify=True,\nsystem_prompt=\"\"\n).to(vl_gpt.device)\n# run image encoder to get the image embeddings\ninputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n# run the model to get the response\noutputs = vl_gpt.language_model.generate(\ninputs_embeds=inputs_embeds,\nattention_mask=prepare_inputs.attention_mask,\npad_token_id=tokenizer.eos_token_id,\nbos_token_id=tokenizer.bos_token_id,\neos_token_id=tokenizer.eos_token_id,\nmax_new_tokens=512,\ndo_sample=False,\nuse_cache=True\n)\nanswer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\nprint(f\"{prepare_inputs['sft_format'][0]}\", answer)\nGradio Demo (TODO)\n4. License\nThis code repository is licensed under MIT License. The use of DeepSeek-VL2 models is subject to DeepSeek Model License. DeepSeek-VL2 series supports commercial use.\n5. Citation\n@misc{wu2024deepseekvl2mixtureofexpertsvisionlanguagemodels,\ntitle={DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding},\nauthor={Zhiyu Wu and Xiaokang Chen and Zizheng Pan and Xingchao Liu and Wen Liu and Damai Dai and Huazuo Gao and Yiyang Ma and Chengyue Wu and Bingxuan Wang and Zhenda Xie and Yu Wu and Kai Hu and Jiawei Wang and Yaofeng Sun and Yukun Li and Yishi Piao and Kang Guan and Aixin Liu and Xin Xie and Yuxiang You and Kai Dong and Xingkai Yu and Haowei Zhang and Liang Zhao and Yisong Wang and Chong Ruan},\nyear={2024},\neprint={2412.10302},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2412.10302},\n}\n6. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "deepseek-ai/deepseek-vl2": "1. Introduction\n2. Model Summary\n3. Quick Start\nInstallation\nNotifications\nSimple Inference Example\nGradio Demo (TODO)\n4. License\n5. Citation\n6. Contact\n1. Introduction\nIntroducing DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition,  document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively.\nDeepSeek-VL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models.\nDeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding\nGithub Repository\nZhiyu Wu*, Xiaokang Chen*, Zizheng Pan*, Xingchao Liu*, Wen Liu**, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruan*** (* Equal Contribution, ** Project Lead, *** Corresponding author)\n2. Model Summary\nDeepSeek-VL2 is built on DeepSeekMoE-27B.\n3. Quick Start\nInstallation\nOn the basis of Python >= 3.8 environment, install the necessary dependencies by running the following command:\npip install -e .\nNotifications\nWe suggest to use a temperature T <= 0.7 when sampling. We observe a larger temperature decreases the generation quality.\nTo keep the number of tokens managable in the context window, we apply dynamic tiling strategy to <=2 images. When there are >=3 images, we directly pad the images to 384*384 as inputs without tiling.\nThe main difference between DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2 is the base LLM.\nSimple Inference Example\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom deepseek_vl.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\nfrom deepseek_vl.utils.io import load_pil_images\n# specify the path to the model\nmodel_path = \"deepseek-ai/deepseek-vl2-small\"\nvl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\nvl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n## single image conversation example\nconversation = [\n{\n\"role\": \"<|User|>\",\n\"content\": \"<image>\\n<|ref|>The giraffe at the back.<|/ref|>.\",\n\"images\": [\"./images/visual_grounding.jpeg\"],\n},\n{\"role\": \"<|Assistant|>\", \"content\": \"\"},\n]\n## multiple images (or in-context learning) conversation example\n# conversation = [\n#     {\n#         \"role\": \"User\",\n#         \"content\": \"<image_placeholder>A dog wearing nothing in the foreground, \"\n#                    \"<image_placeholder>a dog wearing a santa hat, \"\n#                    \"<image_placeholder>a dog wearing a wizard outfit, and \"\n#                    \"<image_placeholder>what's the dog wearing?\",\n#         \"images\": [\n#             \"images/dog_a.png\",\n#             \"images/dog_b.png\",\n#             \"images/dog_c.png\",\n#             \"images/dog_d.png\",\n#         ],\n#     },\n#     {\"role\": \"Assistant\", \"content\": \"\"}\n# ]\n# load images and prepare for inputs\npil_images = load_pil_images(conversation)\nprepare_inputs = vl_chat_processor(\nconversations=conversation,\nimages=pil_images,\nforce_batchify=True,\nsystem_prompt=\"\"\n).to(vl_gpt.device)\n# run image encoder to get the image embeddings\ninputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n# run the model to get the response\noutputs = vl_gpt.language_model.generate(\ninputs_embeds=inputs_embeds,\nattention_mask=prepare_inputs.attention_mask,\npad_token_id=tokenizer.eos_token_id,\nbos_token_id=tokenizer.bos_token_id,\neos_token_id=tokenizer.eos_token_id,\nmax_new_tokens=512,\ndo_sample=False,\nuse_cache=True\n)\nanswer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\nprint(f\"{prepare_inputs['sft_format'][0]}\", answer)\nGradio Demo (TODO)\n4. License\nThis code repository is licensed under MIT License. The use of DeepSeek-VL2 models is subject to DeepSeek Model License. DeepSeek-VL2 series supports commercial use.\n5. Citation\n@misc{wu2024deepseekvl2mixtureofexpertsvisionlanguagemodels,\ntitle={DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding},\nauthor={Zhiyu Wu and Xiaokang Chen and Zizheng Pan and Xingchao Liu and Wen Liu and Damai Dai and Huazuo Gao and Yiyang Ma and Chengyue Wu and Bingxuan Wang and Zhenda Xie and Yu Wu and Kai Hu and Jiawei Wang and Yaofeng Sun and Yukun Li and Yishi Piao and Kang Guan and Aixin Liu and Xin Xie and Yuxiang You and Kai Dong and Xingkai Yu and Haowei Zhang and Liang Zhao and Yisong Wang and Chong Ruan},\nyear={2024},\neprint={2412.10302},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2412.10302},\n}\n6. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "ai-sage/GigaChat-20B-A3B-instruct": "GigaChat-20B-A3B-instruct\n–ë–µ–Ω—á–º–∞—Ä–∫–∏\n–°–µ–º–µ–π—Å—Ç–≤–æ GigaChat\nRequirements\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ transformers\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ vLLM\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è vLLM server\nGigaChat-20B-A3B-instruct\n–î–∏–∞–ª–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏–∑ —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π GigaChat, –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–∞ GigaChat-20B-A3B-base. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ 131 —Ç—ã—Å—è—á—É —Ç–æ–∫–µ–Ω–æ–≤.\nThis repository contains the instructed model of GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture.\n–ë–æ–ª—å—à–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π –≤ —Ö–∞–±—Ä —Å—Ç–∞—Ç—å–µ.\n–î–ª—è –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤–µ—Å–∞ –≤ bf16 –∏ int8\nUpd. –ü–µ—Ä–µ–∑–∞–ª–∏–ª–∏ –≤–µ—Å–∞ –≤ .safetensors\n–ë–µ–Ω—á–º–∞—Ä–∫–∏\nT-lite-instruct-0.1(llama 3.0 8B based)\ngemma-2-9b-it\nGigaChat-20B-A3B-instruct\nMERA\n0.335\n0.392\n0.513\nru-MMLU 5-shot\n0.555\n0.625\n0.598\nShlepa\n0.36\n0.388\n0.482\n–°–µ–º–µ–π—Å—Ç–≤–æ GigaChat\nGigaChat-20B-A3B-instruct\nGigaChat-Pro v26.20\nGigaChat-Max v26.20\n–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏\nGSM8K 5-shot\n0,763\n0,782\n0,929\nMATH 4-shot\n0,426\n0,446\n0,53\n–ù–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞\nHumanEval 0-shot\n0,329\n0,439\n0,64\nMBPP 0-shot\n0,385\n0,487\n0,667\n–û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è\nMMLU EN 5-shot\n0,648\n0,687\n0,804\nMMLU RU 5-shot–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ MMLU EN 5-shot\n0,598\n0,645\n0,75\nMMLU RU 1-shot\n‚Äî\n0,617\n0,718\nMMLU PRO EN 5-shot\n0,348\n0,431\n0,589\nRUBQ 0-shot\n0,675\n0,724\n0,73\nWINOGRANDE 4-shot\n0,75\n0,796\n0,832\nCyberMetric 0-shot\n0,798\n0,827\n0,864\n–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º\nIFEval 0-shot\n0,411\n0,566\n0,721\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞–º–µ—Ä–æ–≤\nGSM8k ‚Äî —ç—Ç–æ —Ç–µ—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–∞–∫ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ —Å —á–∏—Å–ª–∞–º–∏. –í –Ω–∞—à–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ 5 —à–æ—Ç–æ–≤, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å, –∏ —Å–º–æ—Ç—Ä–µ–ª–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –≤ –æ—Ç–≤–µ—Ç–µ. –í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–µ –æ—Ç–≤–µ—Ç –∏—â–µ—Ç—Å—è –ø–æ —à–∞–±–ª–æ–Ω—É: ‚Äò### —á–∏—Å–ª–æ‚Äô.\n–¢–µ—Å—Ç Math —Ç–æ–∂–µ –∏–º–µ–µ—Ç —Ä–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –í –Ω–∞—à–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –º—ã –¥–∞–≤–∞–ª–∏ 4 –ø—Ä–∏–º–µ—Ä–∞ –∏ —Å–º–æ—Ç—Ä–µ–ª–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '\\boxed{expression}'. –ó–∞—Ç–µ–º –æ—Ü–µ–Ω–∏–≤–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ sympy.\nRequirements\ntransformers>=4.47\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nmodel_name = \"ai-sage/GigaChat-20B-A3B-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmessages = [\n{\"role\": \"user\", \"content\": \"–î–æ–∫–∞–∂–∏ —Ç–µ–æ—Ä–µ–º—É –æ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ–π —Ç–æ—á–∫–µ\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device))\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=False)\nprint(result)\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ vLLM\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmodel_name = \"ai-sage/GigaChat-20B-A3B-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=8192)\nmessages_list = [\n[{\"role\": \"user\", \"content\": \"–î–æ–∫–∞–∂–∏ —Ç–µ–æ—Ä–µ–º—É –æ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ–π —Ç–æ—á–∫–µ\"}],\n]\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n–í GigaChat-20B-A3B-instruct –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Å–æ–±—ã–π —Å–ø–æ—Å–æ–± —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –ø–æ—ç—Ç–æ–º—É –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–π\ninput_string = tokenizer.apply_chat_template(messages,tokenize=False, add_generation_prompt=True)\ninput_tensor = tokenizer(input_string, return_tensors=\"pt\")\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è vLLM server\n–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞\nvllm serve ai-sage/GigaChat-20B-A3B-instruct  \\\n--disable-log-requests \\\n--trust_remote_code \\\n--dtype bfloat16 \\\n--max-seq-len 8192\n–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\ncurl http://localhost:8000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"ai-sage/GigaChat-20B-A3B-instruct\" ,\n\"messages\": [\n{\"role\": \"system\", \"content\": \"–¢—ã –û–ß–ï–ù–¨ —É–º–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏–∫\"},\n{\"role\": \"user\", \"content\": \"–î–æ–∫–∞–∂–∏ —Ç–µ–æ—Ä–µ–º—É –æ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ–π —Ç–æ—á–∫–µ\"}\n]\n}'",
    "nlpai-lab/KURE-v1": "üîé KURE-v1\nModel Versions\nModel Description\nExample code\nInstall Dependencies\nPython code\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nMetrics\nBenchmark Datasets\nResults\nTop-k 1\nTop-k 3\nTop-k 5\nTop-k 10\nCitation\nüîé KURE-v1\nIntroducing Korea University Retrieval Embedding model, KURE-v1\nIt has shown remarkable performance in Korean text retrieval, speficially overwhelming most multilingual embedding models.To our knowledge, It is one of the best publicly opened Korean retrieval models.\nFor details, visit the KURE repository\nModel Versions\nModel Name\nDimension\nSequence Length\nIntroduction\nKURE-v1\n1024\n8192\nFine-tuned BAAI/bge-m3 with Korean data via CachedGISTEmbedLoss\nKoE5\n1024\n512\nFine-tuned intfloat/multilingual-e5-large with ko-triplet-v1.0 via CachedMultipleNegativesRankingLoss\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub.\nDeveloped by: NLP&AI Lab\nLanguage(s) (NLP): Korean, English\nLicense: MIT\nFinetuned from model: BAAI/bge-m3\nExample code\nInstall Dependencies\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nPython code\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"nlpai-lab/KURE-v1\")\n# Run inference\nsentences = [\n'ÌóåÎ≤ïÍ≥º Î≤ïÏõêÏ°∞ÏßÅÎ≤ïÏùÄ Ïñ¥Îñ§ Î∞©ÏãùÏùÑ ÌÜµÌï¥ Í∏∞Î≥∏Í∂å Î≥¥Ïû• Îì±Ïùò Îã§ÏñëÌïú Î≤ïÏ†Å Î™®ÏÉâÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌñàÏñ¥',\n'4. ÏãúÏÇ¨Ï†êÍ≥º Í∞úÏÑ†Î∞©Ìñ• ÏïûÏÑú ÏÇ¥Ìé¥Î≥∏ Î∞îÏôÄ Í∞ôÏù¥ Ïö∞Î¶¨ ÌóåÎ≤ïÍ≥º ÔΩ¢Î≤ïÏõêÏ°∞ÏßÅ Î≤ïÔΩ£ÏùÄ ÎåÄÎ≤ïÏõê Íµ¨ÏÑ±ÏùÑ Îã§ÏñëÌôîÌïòÏó¨ Í∏∞Î≥∏Í∂å Î≥¥Ïû•Í≥º ÎØºÏ£ºÏ£ºÏùò ÌôïÎ¶ΩÏóê ÏûàÏñ¥ Îã§Í∞ÅÏ†ÅÏù∏ Î≤ïÏ†Å Î™®ÏÉâÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïòÎäî Í≤ÉÏùÑ Í∑ºÎ≥∏ Í∑úÎ≤îÏúºÎ°ú ÌïòÍ≥† ÏûàÎã§. ÎçîÏö±Ïù¥ Ìï©ÏùòÏ≤¥Î°úÏÑúÏùò ÎåÄÎ≤ïÏõê ÏõêÎ¶¨Î•º Ï±ÑÌÉùÌïòÍ≥† ÏûàÎäî Í≤É Ïó≠Ïãú Í∑∏ Íµ¨ÏÑ±Ïùò Îã§ÏñëÏÑ±ÏùÑ ÏöîÏ≤≠ÌïòÎäî Í≤ÉÏúºÎ°ú Ìï¥ÏÑùÎêúÎã§. Ïù¥ÏôÄ Í∞ôÏùÄ Í¥ÄÏ†êÏóêÏÑú Î≥º Îïå ÌòÑÏßÅ Î≤ïÏõêÏû•Í∏â Í≥†ÏúÑÎ≤ïÍ¥ÄÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÎåÄÎ≤ïÏõêÏùÑ Íµ¨ÏÑ±ÌïòÎäî Í¥ÄÌñâÏùÄ Í∞úÏÑ†Ìï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ÉÏúºÎ°ú Î≥¥Ïù∏Îã§.',\n'Ïó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎäî 2001ÎÖÑ 1Ïõî 24Ïùº 5:3Ïùò Îã§ÏàòÍ≤¨Ìï¥Î°ú „ÄåÎ≤ïÏõêÏ°∞ÏßÅÎ≤ï„Äç Ï†ú169Ï°∞ Ï†ú2Î¨∏Ïù¥ ÌóåÎ≤ïÏóê Ìï©ÏπòÎêúÎã§Îäî ÌåêÍ≤∞ÏùÑ ÎÇ¥Î†∏Ïùå ‚óã 5Ïù∏Ïùò Îã§Ïàò Ïû¨ÌåêÍ¥ÄÏùÄ ÏÜåÏÜ°Í¥ÄÍ≥ÑÏù∏Ïùò Ïù∏Í≤©Í∂å Î≥¥Ìò∏, Í≥µÏ†ïÌïú Ï†àÏ∞®Ïùò Î≥¥Ïû•Í≥º Î∞©Ìï¥Î∞õÏßÄ ÏïäÎäî Î≤ïÍ≥º ÏßÑÏã§ Î∞úÍ≤¨ Îì±ÏùÑ Í∑ºÍ±∞Î°ú ÌïòÏó¨ ÌÖîÎ†àÎπÑÏ†Ñ Ï¥¨ÏòÅÏóê ÎåÄÌïú Ï†àÎåÄÏ†ÅÏù∏ Í∏àÏßÄÎ•º ÌóåÎ≤ïÏóê Ìï©ÏπòÌïòÎäî Í≤ÉÏúºÎ°ú Î≥¥ÏïòÏùå ‚óã Í∑∏Îü¨ÎÇò ÎÇòÎ®∏ÏßÄ 3Ïù∏Ïùò Ïû¨ÌåêÍ¥ÄÏùÄ ÌñâÏ†ïÎ≤ïÏõêÏùò ÏÜåÏÜ°Ï†àÏ∞®Îäî ÌäπÎ≥ÑÌïú Ïù∏Í≤©Í∂å Î≥¥Ìò∏Ïùò Ïù¥ÏùµÎèÑ ÏóÜÏúºÎ©∞, ÌÖîÎ†àÎπÑÏ†Ñ Í≥µÍ∞úÏ£ºÏùòÎ°ú Ïù∏Ìï¥ Î≤ïÍ≥º ÏßÑÏã§ Î∞úÍ≤¨Ïùò Í≥ºÏ†ïÏù¥ Ïñ∏Ï†úÎÇò ÏúÑÌÉúÎ°≠Í≤å ÎêòÎäî Í≤ÉÏùÄ ÏïÑÎãàÎùºÎ©¥ÏÑú Î∞òÎåÄÏùòÍ≤¨ÏùÑ Ï†úÏãúÌï® ‚óã ÏôúÎÉêÌïòÎ©¥ ÌñâÏ†ïÎ≤ïÏõêÏùò ÏÜåÏÜ°Ï†àÏ∞®ÏóêÏÑúÎäî ÏÜåÏÜ°ÎãπÏÇ¨ÏûêÍ∞Ä Í∞úÏù∏Ï†ÅÏúºÎ°ú ÏßÅÏ†ë Ïã¨Î¶¨Ïóê Ï∞∏ÏÑùÌïòÍ∏∞Î≥¥Îã§Îäî Î≥ÄÌò∏ÏÇ¨Í∞Ä Ï∞∏ÏÑùÌïòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏúºÎ©∞, Ïã¨Î¶¨ÎåÄÏÉÅÎèÑ ÏÇ¨Ïã§Î¨∏Ï†úÍ∞Ä ÏïÑÎãå Î≤ïÎ•†Î¨∏Ï†úÍ∞Ä ÎåÄÎ∂ÄÎ∂ÑÏù¥Í∏∞ ÎïåÎ¨∏Ïù¥ÎùºÎäî Í≤ÉÏûÑ ‚ñ° ÌïúÌé∏, Ïó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎäî „ÄåÏó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎ≤ï„Äç(Bundesverfassungsgerichtsgesetz: BVerfGG) Ï†ú17aÏ°∞Ïóê Îî∞Îùº Ï†úÌïúÏ†ÅÏù¥ÎÇòÎßà Ïû¨ÌåêÏóê ÎåÄÌïú Î∞©ÏÜ°ÏùÑ ÌóàÏö©ÌïòÍ≥† ÏûàÏùå ‚óã „ÄåÏó∞Î∞©ÌóåÎ≤ïÏû¨ÌåêÏÜåÎ≤ï„Äç Ï†ú17Ï°∞ÏóêÏÑú „ÄåÎ≤ïÏõêÏ°∞ÏßÅÎ≤ï„Äç Ï†ú14Ï†à ÎÇ¥ÏßÄ Ï†ú16Ï†àÏùò Í∑úÏ†ïÏùÑ Ï§ÄÏö©ÌïòÎèÑÎ°ù ÌïòÍ≥† ÏûàÏßÄÎßå, ÎÖπÏùåÏù¥ÎÇò Ï¥¨ÏòÅÏùÑ ÌÜµÌïú Ïû¨ÌåêÍ≥µÍ∞úÏôÄ Í¥ÄÎ†®ÌïòÏó¨ÏÑúÎäî „ÄåÎ≤ïÏõêÏ°∞ÏßÅÎ≤ï„ÄçÍ≥º Îã§Î•∏ ÎÇ¥Ïö©ÏùÑ Í∑úÏ†ïÌïòÍ≥† ÏûàÏùå',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 1024]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# Results for KURE-v1\n# tensor([[1.0000, 0.6967, 0.5306],\n#         [0.6967, 1.0000, 0.4427],\n#         [0.5306, 0.4427, 1.0000]])\nTraining Details\nTraining Data\nKURE-v1\nKorean query-document-hard_negative(5) data\n2,000,000 examples\nTraining Procedure\nloss: Used CachedGISTEmbedLoss by sentence-transformers\nbatch size: 4096\nlearning rate: 2e-05\nepochs: 1\nEvaluation\nMetrics\nRecall, Precision, NDCG, F1\nBenchmark Datasets\nKo-StrategyQA: ÌïúÍµ≠Ïñ¥ ODQA multi-hop Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã (StrategyQA Î≤àÏó≠)\nAutoRAGRetrieval: Í∏àÏúµ, Í≥µÍ≥µ, ÏùòÎ£å, Î≤ïÎ•†, Ïª§Î®∏Ïä§ 5Í∞ú Î∂ÑÏïºÏóê ÎåÄÌï¥, pdfÎ•º ÌååÏã±ÌïòÏó¨ Íµ¨ÏÑ±Ìïú ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nMIRACLRetrieval: Wikipedia Í∏∞Î∞òÏùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nPublicHealthQA: ÏùòÎ£å Î∞è Í≥µÏ§ëÎ≥¥Í±¥ ÎèÑÎ©îÏù∏Ïóê ÎåÄÌïú ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nBelebeleRetrieval: FLORES-200 Í∏∞Î∞òÏùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nMrTidyRetrieval: Wikipedia Í∏∞Î∞òÏùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nMultiLongDocRetrieval: Îã§ÏñëÌïú ÎèÑÎ©îÏù∏Ïùò ÌïúÍµ≠Ïñ¥ Ïû•Î¨∏ Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nXPQARetrieval: Îã§ÏñëÌïú ÎèÑÎ©îÏù∏Ïùò ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú Í≤ÄÏÉâ Îç∞Ïù¥ÌÑ∞ÏÖã\nResults\nÏïÑÎûòÎäî Î™®Îì† Î™®Îç∏Ïùò, Î™®Îì† Î≤§ÏπòÎßàÌÅ¨ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌïú ÌèâÍ∑† Í≤∞Í≥ºÏûÖÎãàÎã§.\nÏûêÏÑ∏Ìïú Í≤∞Í≥ºÎäî KURE GithubÏóêÏÑú ÌôïÏù∏ÌïòÏã§ Ïàò ÏûàÏäµÎãàÎã§.\nTop-k 1\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.52640\n0.60551\n0.60551\n0.55784\ndragonkue/BGE-m3-ko\n0.52361\n0.60394\n0.60394\n0.55535\nBAAI/bge-m3\n0.51778\n0.59846\n0.59846\n0.54998\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.51246\n0.59384\n0.59384\n0.54489\nnlpai-lab/KoE5\n0.50157\n0.57790\n0.57790\n0.53178\nintfloat/multilingual-e5-large\n0.50052\n0.57727\n0.57727\n0.53122\njinaai/jina-embeddings-v3\n0.48287\n0.56068\n0.56068\n0.51361\nBAAI/bge-multilingual-gemma2\n0.47904\n0.55472\n0.55472\n0.50916\nintfloat/multilingual-e5-large-instruct\n0.47842\n0.55435\n0.55435\n0.50826\nintfloat/multilingual-e5-base\n0.46950\n0.54490\n0.54490\n0.49947\nintfloat/e5-mistral-7b-instruct\n0.46772\n0.54394\n0.54394\n0.49781\nAlibaba-NLP/gte-multilingual-base\n0.46469\n0.53744\n0.53744\n0.49353\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.46633\n0.53625\n0.53625\n0.49429\nopenai/text-embedding-3-large\n0.44884\n0.51688\n0.51688\n0.47572\nSalesforce/SFR-Embedding-2_R\n0.43748\n0.50815\n0.50815\n0.46504\nupskyy/bge-m3-korean\n0.43125\n0.50245\n0.50245\n0.45945\njhgan/ko-sroberta-multitask\n0.33788\n0.38497\n0.38497\n0.35678\nTop-k 3\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.68678\n0.28711\n0.65538\n0.39835\ndragonkue/BGE-m3-ko\n0.67834\n0.28385\n0.64950\n0.39378\nBAAI/bge-m3\n0.67526\n0.28374\n0.64556\n0.39291\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.67128\n0.28193\n0.64042\n0.39072\nintfloat/multilingual-e5-large\n0.65807\n0.27777\n0.62822\n0.38423\nnlpai-lab/KoE5\n0.65174\n0.27329\n0.62369\n0.37882\nBAAI/bge-multilingual-gemma2\n0.64415\n0.27416\n0.61105\n0.37782\njinaai/jina-embeddings-v3\n0.64116\n0.27165\n0.60954\n0.37511\nintfloat/multilingual-e5-large-instruct\n0.64353\n0.27040\n0.60790\n0.37453\nAlibaba-NLP/gte-multilingual-base\n0.63744\n0.26404\n0.59695\n0.36764\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.63163\n0.25937\n0.59237\n0.36263\nintfloat/multilingual-e5-base\n0.62099\n0.26144\n0.59179\n0.36203\nintfloat/e5-mistral-7b-instruct\n0.62087\n0.26144\n0.58917\n0.36188\nopenai/text-embedding-3-large\n0.61035\n0.25356\n0.57329\n0.35270\nSalesforce/SFR-Embedding-2_R\n0.60001\n0.25253\n0.56346\n0.34952\nupskyy/bge-m3-korean\n0.59215\n0.25076\n0.55722\n0.34623\njhgan/ko-sroberta-multitask\n0.46930\n0.18994\n0.43293\n0.26696\nTop-k 5\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.73851\n0.19130\n0.67479\n0.29903\ndragonkue/BGE-m3-ko\n0.72517\n0.18799\n0.66692\n0.29401\nBAAI/bge-m3\n0.72954\n0.18975\n0.66615\n0.29632\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.72962\n0.18875\n0.66236\n0.29542\nnlpai-lab/KoE5\n0.70820\n0.18287\n0.64499\n0.28628\nintfloat/multilingual-e5-large\n0.70124\n0.18316\n0.64402\n0.28588\nBAAI/bge-multilingual-gemma2\n0.70258\n0.18556\n0.63338\n0.28851\njinaai/jina-embeddings-v3\n0.69933\n0.18256\n0.63133\n0.28505\nintfloat/multilingual-e5-large-instruct\n0.69018\n0.17838\n0.62486\n0.27933\nAlibaba-NLP/gte-multilingual-base\n0.69365\n0.17789\n0.61896\n0.27879\nintfloat/multilingual-e5-base\n0.67250\n0.17406\n0.61119\n0.27247\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.67447\n0.17114\n0.60952\n0.26943\nintfloat/e5-mistral-7b-instruct\n0.67449\n0.17484\n0.60935\n0.27349\nopenai/text-embedding-3-large\n0.66365\n0.17004\n0.59389\n0.26677\nSalesforce/SFR-Embedding-2_R\n0.65622\n0.17018\n0.58494\n0.26612\nupskyy/bge-m3-korean\n0.65477\n0.17015\n0.58073\n0.26589\njhgan/ko-sroberta-multitask\n0.53136\n0.13264\n0.45879\n0.20976\nTop-k 10\nModel\nAverage Recall_top1\nAverage Precision_top1\nAverage NDCG_top1\nAverage F1_top1\nnlpai-lab/KURE-v1\n0.79682\n0.10624\n0.69473\n0.18524\ndragonkue/BGE-m3-ko\n0.78450\n0.10492\n0.68748\n0.18288\nBAAI/bge-m3\n0.79195\n0.10592\n0.68723\n0.18456\nSnowflake/snowflake-arctic-embed-l-v2.0\n0.78669\n0.10462\n0.68189\n0.18260\nintfloat/multilingual-e5-large\n0.75902\n0.10147\n0.66370\n0.17693\nnlpai-lab/KoE5\n0.75296\n0.09937\n0.66012\n0.17369\nBAAI/bge-multilingual-gemma2\n0.76153\n0.10364\n0.65330\n0.18003\njinaai/jina-embeddings-v3\n0.76277\n0.10240\n0.65290\n0.17843\nintfloat/multilingual-e5-large-instruct\n0.74851\n0.09888\n0.64451\n0.17283\nAlibaba-NLP/gte-multilingual-base\n0.75631\n0.09938\n0.64025\n0.17363\nAlibaba-NLP/gte-Qwen2-7B-instruct\n0.74092\n0.09607\n0.63258\n0.16847\nintfloat/multilingual-e5-base\n0.73512\n0.09717\n0.63216\n0.16977\nintfloat/e5-mistral-7b-instruct\n0.73795\n0.09777\n0.63076\n0.17078\nopenai/text-embedding-3-large\n0.72946\n0.09571\n0.61670\n0.16739\nSalesforce/SFR-Embedding-2_R\n0.71662\n0.09546\n0.60589\n0.16651\nupskyy/bge-m3-korean\n0.71895\n0.09583\n0.60258\n0.16712\njhgan/ko-sroberta-multitask\n0.61225\n0.07826\n0.48687\n0.13757\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@misc{KURE,\npublisher = {Youngjoon Jang, Junyoung Son, Taemin Lee},\nyear = {2024},\nurl = {https://github.com/nlpai-lab/KURE}\n},\n@misc{KoE5,\nauthor = {NLP & AI Lab and Human-Inspired AI research},\ntitle = {KoE5: A New Dataset and Model for Improving Korean Embedding Performance},\nyear = {2024},\npublisher = {Youngjoon Jang, Junyoung Son, Taemin Lee},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/nlpai-lab/KoE5}},\n}",
    "Pointcept/Sonata": "README.md exists but content is empty.",
    "ali-vilab/ACE_Plus": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nüìö Introduction\nüì¢ News\nüî•The unified fft model for ACE++\nExamples\nComfyui Workflows in community\nüî• ACE Models\nACE++ Portrait\nACE++ Subject\nACE++ LocalEditing\nüî• Applications\n‚öôÔ∏èÔ∏è Installation\nüöÄ Inference\nüöÄ Train\nüíª Demo\nüìö Limitations\nüìù Citation\n++: Instruction-Based Image Creation and Editing  via Context-Aware Content Filling\nChaojie Mao\n¬∑\nJingfeng Zhang\n¬∑\nYulin Pan\n¬∑\nZeyinzi Jiang\n¬∑\nZhen Han\n¬∑\nYu Liu\n¬∑\nJingren Zhou\nTongyi Lab, Alibaba Group\nüìö Introduction\nThe original intention behind the design of ACE++ was to unify reference image generation, local editing,\nand controllable generation into a single framework, and to enable one model to adapt to a wider range of tasks.\nA more versatile model is often capable of handling more complex tasks. We have already released three LoRA models,\nfocusing on portraits, objects, and regional editing, with the expectation that each would demonstrate strong adaptability\nwithin their respective domains. Undoubtedly, this presents certain challenges.\nWe are currently training a fully fine-tuned model, which has now entered the final stage of quality tuning.\nWe are confident it will be released soon. This model will support a broader range of capabilities and is\nexpected to empower community developers to build even more interesting applications.\nüì¢ News\n[2025.01.06] Release the code and models of ACE++.\n[2025.01.07] Release the demo on HuggingFace.\n[2025.01.16] Release the training code for lora.\n[2025.02.15] Collection of workflows in Comfyui.\n[2025.02.15] Release the config for fully fine-tuning.\n[2025.03.03] Release a unified fft model for ACE++, support more image to image tasks.\nüî•The unified fft model for ACE++\nFully finetuning a composite model with ACE‚Äôs data to support various editing and reference generation tasks through an instructive approach.\nWe found that there are conflicts between the repainting task and the editing task during the experimental process. This is because the edited image is concatenated with noise in the channel dimension, whereas the repainting task modifies the region using zero pixel values in the VAE's latent space. The editing task uses RGB pixel values in the modified region through the VAE's latent space, which is similar to the distribution of the non-modified part of the repainting task, making it a challenge for the model to distinguish between the two tasks.\nTo address this issue, we introduced 64 additional channels in the channel dimension to differentiate between these two tasks. In these channels, we place the latent representation of the pixel space from the edited image, while keeping other channels consistent with the repainting task. This approach significantly enhances the model's adaptability to different tasks.\nOne issue with this approach is that it changes the input channel number of the FLUX-Fill-Dev model from 384 to 448. The specific configuration can be referenced in the configuration file.\nExamples\nInput Reference Image\nInput Edit Image\nInput Edit Mask\nOutput\nInstruction\nFunction\n\"Maintain the facial features, A girl is wearing a neat police uniform and sporting a badge. She is smiling with a friendly and confident demeanor. The background is blurred, featuring a cartoon logo.\"\n\"Character ID Consistency Generation\"\n\"Display the logo in a minimalist style printed in white on a matte black ceramic coffee mug, alongside a steaming cup of coffee on a cozy cafe table.\"\n\"Subject Consistency Generation\"\n\"The item is put on the table.\"\n\"Subject Consistency Editing\"\n\"The logo is printed on the headphones.\"\n\"Subject Consistency Editing\"\n\"The woman dresses this skirt.\"\n\"Try On\"\n\"{image}, the man faces the camera.\"\n\"Face swap\"\n\"{image} features a close-up of a young, furry tiger cub on a rock. The tiger, which appears to be quite young, has distinctive orange, black, and white striped fur, typical of tigers. The cub's eyes have a bright and curious expression, and its ears are perked up, indicating alertness. The cub seems to be in the act of climbing or resting on the rock. The background is a blurred grassland with trees, but the focus is on the cub, which is vividly colored while the rest of the image is in grayscale, drawing attention to the tiger's details. The photo captures a moment in the wild, depicting the charming and tenacious nature of this young tiger, as well as its typical interaction with the environment.\"\n\"Super-resolution\"\n\"a blue hand\"\n\"Regional Editing\"\n\"Mechanical  hands like a robot\"\n\"Regional Editing\"\n\"{image} Beautiful female portrait, Robot with smooth White transparent carbon shell, rococo detailing, Natural lighting, Highly detailed, Cinematic, 4K.\"\n\"Recolorizing\"\n\"{image} Beautiful female portrait, Robot with smooth White transparent carbon shell, rococo detailing, Natural lighting, Highly detailed, Cinematic, 4K.\"\n\"Depth Guided Generation\"\n\"{image} Beautiful female portrait, Robot with smooth White transparent carbon shell, rococo detailing, Natural lighting, Highly detailed, Cinematic, 4K.\"\n\"Contour Guided Generation\"\nComfyui Workflows in community\nWe are deeply grateful to the community developers for building many fascinating applications based on the ACE++ series of models.\nDuring this process, we have received valuable feedback, particularly regarding artifacts in generated images and the stability of the results.\nIn response to these issues, many developers have proposed creative solutions, which have greatly inspired us, and we pay tribute to them.\nAt the same time, we will take these concerns into account in our further optimization efforts, carefully evaluating and testing before releasing new models.\nIn the table below, we have briefly listed some workflows for everyone to use.\nWorkflow\nAuthor\nExample\n„ÄêÈ£ûÁøîÈ≤∏„ÄëÁéãÁÇ∏ÔºÅFLUX+ACE++Êç¢Â§¥\nÈ£ûÁøîÈ≤∏\nÊúÄÊñ∞ACE++ Redux ‰∏áÁâ©ËøÅÁßª AIÊ®°ÁâπÊç¢Ë£Ö UP Ê•öÈó®ÁöÑAI‰∏ñÁïå\nÊ•öÈó®ÁöÑAI‰∏ñÁïå\nAce Plus+Fill+ReduxÁ®≥ÂÆöÊó†ÊäΩÂç°Êç¢Ë£ÖÂ∑•‰ΩúÊµÅ\nT8star-Aix\nAce Plus+Fill+ReduxÁ®≥ÂÆöÂ∞ëÊäΩÂç°Ê†áÂøóÂ∑•‰ΩúÊµÅ\nT8star-Aix\nAce Plus+Fill+ReduxÁ®≥ÂÆöÊó†ÊäΩÂç°Êç¢ËÑ∏Â∑•‰ΩúÊµÅ\nT8star-Aix\nace++ face swap in different styles\njax\nfllux ace++ subject without reference image\nleeguandong\nScepter-ACE++ More convenient replacement of everything\nHaoBeen\nAdditionally, many bloggers have published tutorials on how to use it, which are listed in the table below.\nTutorial\nBest Faceswapper I've Seen. ACE++ in ComfyUI.\nACE ++ In ComfyUI All-round Creator & Editor - More Than Just A Faceswap AI\nAiÁªòÁîªËøõÈò∂140-Âí¶ÔºüÂ§ßÂÆ∂Áî®ÁöÑÈÉΩ‰∏çÂØπÔºüÔºÅAce PlusÂ∑•‰ΩúÊµÅÊ≠£Á°ÆÊê≠Âª∫ÊñπÂºèÂèäÈÄªËæëÔºåÂèÇÊï∞ËØ¶Ëß£ÔºåFlux FillÔºåReduxËÅîÁî®-T8 ComfyuiÊïôÁ®ã\nace++ÔºöÂëäÂà´ Lora ËÆ≠ÁªÉÔºåÊó†ÈúÄpulidÔºåËΩªÊùæÊâìÈÄ†‰∏ìÂ±ûËßíËâ≤ÔºÅ | No Lora Training, Easily Create Exclusive Characters!\nAce++ and Flux Fill: Advanced Face Swapping Made Easy in ComfyUI | No Lora Training, Easily Create Exclusive Characters!\nComfyUI - ACE Plus Subject Portrait Lora\nü§óAI‰∏ÄËá¥ÊÄßÊäÄÊúØÊñ∞Á™ÅÁ†¥ÔºÅACE++ÊäÄÊúØ‰∏ÄËá¥ÊÄßcomfyuiÂ∑•‰ΩúÊµÅüçã‚Äç\nComfyUI Á¨¨55ÈõÜ ‰∫∫ÂÉèÊç¢ËÑ∏ FLUXÁöÑFILLÊ®°Âûã+ACE LORA\nÊç¢Ë£ÖÂèòËÑ∏Ë¥¥logoÔºåÊó†ÊâÄ‰∏çËÉΩÁöÑAce_Plus lora\nüî• ACE Models\nACE++ provides a comprehensive toolkit for image editing and generation to support various applications. We encourage developers to choose the appropriate model based on their own scenarios and to fine-tune their models using data from their specific scenarios to achieve more stable results.\nACE++ Portrait\nPortrait-consistent generation to maintain the consistency of the portrait.\nTuning Method\nInput\nOutput\nInstruction\nModels\nLoRA + ACE Data\n\"Maintain the facial features. A girl is wearing a neat police uniform and sporting a badge. She is smiling with a friendly and confident demeanor. The background is blurred, featuring a cartoon logo.\"\nModels' scepter_path:\nModelScope: ms://iic/ACE_Plus@portrait/xxxx.safetensors\nHuggingFace: hf://ali-vilab/ACE_Plus@portrait/xxxx.safetensors\nACE++ Subject\nSubject-driven image generation task to maintain the consistency of a specific subject in different scenes.\nTuning Method\nInput\nOutput\nInstruction\nModels\nLoRA + ACE Data\n\"Display the logo in a minimalist style printed in white on a matte black ceramic coffee mug, alongside a steaming cup of coffee on a cozy cafe table.\"\nModels' scepter_path:\nModelScope: ms://iic/ACE_Plus@subject/xxxx.safetensors\nHuggingFace: hf://ali-vilab/ACE_Plus@subject/xxxx.safetensors\nACE++ LocalEditing\nRedrawing the mask area of images while maintaining the original structural information of the edited area.\nTuning Method\nInput\nOutput\nInstruction\nModels\nLoRA + ACE Data\n\"By referencing the mask, restore a partial image from the doodle {image} that aligns with the textual explanation: \"1 white old owl\".\"\nModels' scepter_path:\nModelScope: ms://iic/ACE_Plus@local_editing/xxxx.safetensors\nHuggingFace: hf://ali-vilab/ACE_Plus@local_editing/xxxx.safetensors\nüî• Applications\nThe ACE++ model supports a wide range of downstream tasks through simple adaptations. Here are some examples, and we look forward to seeing the community explore even more exciting applications utilizing the ACE++ model.\nApplication\nACE++ Model\nExamples\nTry On\nACE++ Subject\n\"The woman dresses this skirt.\"\nLogo Paste\nACE++ Subject\n\"The logo is printed on the headphones.\"\nPhoto Editing\nACE++ Subject\n\"The item is put on the ground.\"\nMovie Poster Editor\nACE++ Portrait\n\"The man is facing the camera and is smiling.\"\n‚öôÔ∏èÔ∏è Installation\nDownload the code using the following command:\ngit clone https://github.com/ali-vilab/ACE_plus.git\nInstall the necessary packages with pip:\ncd ACE_plus\npip install -r requirements.txt\nACE++ depends on FLUX.1-Fill-dev as its base model, which you can download from .\nIn order to run the inference code or Gradio demo normally, we have defined the relevant environment variables to specify the location of the model.\nFor model preparation, we provide three methods for downloading the model. The summary of relevant settings is as follows.\nModel Downloading Method\nClone to Local Path\nAutomatic Downloading during Runtime(Setting the Environment Variables using scepter_path in ACE Models)\nEnvironment Variables Setting\nexport FLUX_FILL_PATH=\"path/to/FLUX.1-Fill-dev\"export PORTRAIT_MODEL_PATH=\"path/to/ACE++ PORTRAIT PATH\"export SUBJECT_MODEL_PATH=\"path/to/ACE++ SUBJECT PATH\"export LOCAL_MODEL_PATH=\"path/to/ACE++ LOCAL EDITING PATH\"\nexport FLUX_FILL_PATH=\"hf://black-forest-labs/FLUX.1-Fill-dev\"export PORTRAIT_MODEL_PATH=\"${scepter_path}\"export SUBJECT_MODEL_PATH=\"${scepter_path}\"export LOCAL_MODEL_PATH=\"${scepter_path}\"\nüöÄ Inference\nUnder the condition that the environment variables defined in Installation, users can run examples and test your own samples by executing infer.py.\nThe relevant commands are as follows:\nexport FLUX_FILL_PATH=\"hf://black-forest-labs/FLUX.1-Fill-dev\"\nexport PORTRAIT_MODEL_PATH=\"ms://iic/ACE_Plus@portrait/comfyui_portrait_lora64.safetensors\"\nexport SUBJECT_MODEL_PATH=\"ms://iic/ACE_Plus@subject/comfyui_subject_lora16.safetensors\"\nexport LOCAL_MODEL_PATH=\"ms://iic/ACE_Plus@local_editing/comfyui_local_lora16.safetensors\"\n# Use the model from huggingface\n# export PORTRAIT_MODEL_PATH=\"hf://ali-vilab/ACE_Plus@portrait/comfyui_portrait_lora64.safetensors\"\n# export SUBJECT_MODEL_PATH=\"hf://ali-vilab/ACE_Plus@subject/comfyui_subject_lora16.safetensors\"\n# export LOCAL_MODEL_PATH=\"hf://ali-vilab/ACE_Plus@local_editing/comfyui_local_lora16.safetensors\"\npython infer.py\nüöÄ Train\nWe provide training code that allows users to train on their own data. Reference the data in 'data/train.csv' and 'data/eval.csv' to construct the training data and test data, respectively. We use '#;#' to separate fields.\nThe required fields include the following six, with their explanations as follows.\n\"edit_image\": represents the input image for the editing task. If it is not an editing task but a reference generation, this field can be left empty.\n\"edit_mask\": represents the input image mask for the editing task, used to specify the editing area. If it is not an editing task but rather for reference generation, this field can be left empty.\n\"ref_image\": represents the input image for the reference image generation task; if it is a pure editing task, this field can be left empty.\n\"target_image\": represents the generated target image and cannot be empty.\n\"prompt\": represents the prompt for the generation task.\n\"data_type\": represents the type of data, which can be 'portrait', 'subject', or 'local'. This field is not used in training phase.\nAll parameters related to training are stored in 'train_config/ace_plus_lora.yaml'. To run the training code, execute the following command.\nexport FLUX_FILL_PATH=\"hf://black-forest-labs/FLUX.1-Fill-dev\"\npython run_train.py  --cfg train_config/ace_plus_lora.yaml\nThe models trained by ACE++ can be found in ./examples/exp_example/xxxx/checkpoints/xxxx/0_SwiftLoRA/comfyui_model.safetensors.\nüíª Demo\nWe have built a GUI demo based on Gradio to help users better utilize the ACE++ model. Just execute the following command.\nexport FLUX_FILL_PATH=\"hf://black-forest-labs/FLUX.1-Fill-dev\"\nexport PORTRAIT_MODEL_PATH=\"ms://iic/ACE_Plus@portrait/comfyui_portrait_lora64.safetensors\"\nexport SUBJECT_MODEL_PATH=\"ms://iic/ACE_Plus@subject/comfyui_subject_lora16.safetensors\"\nexport LOCAL_MODEL_PATH=\"ms://iic/ACE_Plus@local_editing/comfyui_local_lora16.safetensors\"\n# Use the model from huggingface\n# export PORTRAIT_MODEL_PATH=\"hf://ali-vilab/ACE_Plus@portrait/comfyui_portrait_lora64.safetensors\"\n# export SUBJECT_MODEL_PATH=\"hf://ali-vilab/ACE_Plus@subject/comfyui_subject_lora16.safetensors\"\n# export LOCAL_MODEL_PATH=\"hf://ali-vilab/ACE_Plus@local_editing/comfyui_local_lora16.safetensors\"\npython demo.py\nüìö Limitations\nFor certain tasks, such as deleting and adding objects, there are flaws in instruction following. For adding and replacing objects, we recommend trying the repainting method of the local editing model to achieve this.\nThe generated results may contain artifacts, especially when it comes to the generation of hands, which still exhibit distortions.\nThe current version of ACE++ is still in the development stage. We are working on improving the model's performance and adding more features.\nüìù Citation\nACE++ is a post-training model based on the FLUX.1-dev series from black-forest-labs. Please adhere to its open-source license. The test materials used in ACE++ come from the internet and are intended for academic research and communication purposes. If the original creators feel uncomfortable, please contact us to have them removed.\nIf you use this model in your research, please cite the works of FLUX.1-dev and the following papers:\n@article{mao2025ace++,\ntitle={ACE++: Instruction-Based Image Creation and Editing via Context-Aware Content Filling},\nauthor={Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Jiang, Zeyinzi and Han, Zhen and Liu, Yu and Zhou, Jingren},\njournal={arXiv preprint arXiv:2501.02487},\nyear={2025}\n}\n@article{han2024ace,\ntitle={ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer},\nauthor={Han, Zhen and Jiang, Zeyinzi and Pan, Yulin and Zhang, Jingfeng and Mao, Chaojie and Xie, Chenwei and Liu, Yu and Zhou, Jingren},\njournal={arXiv preprint arXiv:2410.00086},\nyear={2024}\n}",
    "usyd-community/vitpose-plus-base": "Model Card for VitPose\nModel Details\nModel Description\nModel Sources\nUses\nBias, Risks, and Limitations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nEvaluation\nResults\nModel Architecture and Objective\nCitation\nModel Card for VitPose\nViTPose: Simple Vision Transformer Baselines for Human Pose Estimation and ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation. It obtains 81.1 AP on MS COCO Keypoint test-dev set.\nModel Details\nAlthough no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for\npose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm,\nand transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision\ntransformers as backbones to extract features for a given person instance and a\nlightweight decoder for pose estimation. It can be scaled up from 100M to 1B\nparameters by taking the advantages of the scalable model capacity and high\nparallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose\ntasks. We also empirically demonstrate that the knowledge of large ViTPose models\ncan be easily transferred to small ones via a simple knowledge token. Experimental\nresults show that our basic ViTPose model outperforms representative methods\non the challenging MS COCO Keypoint Detection benchmark, while the largest\nmodel sets a new state-of-the-art, i.e., 80.9 AP on the MS COCO test-dev set. The\ncode and models are available at https://github.com/ViTAE-Transformer/ViTPose\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao\nFunded by: ARC FL-170100117 and IH-180100002.\nLicense: Apache-2.0\nPorted to ü§ó Transformers by: Sangbum Choi and Niels Rogge\nModel Sources\nOriginal repository: https://github.com/ViTAE-Transformer/ViTPose\nPaper: https://arxiv.org/pdf/2204.12484\nDemo: https://huggingface.co/spaces?sort=trending&search=vitpose\nUses\nThe ViTPose model, developed by the ViTAE-Transformer team, is primarily designed for pose estimation tasks. Here are some direct uses of the model:\nHuman Pose Estimation: The model can be used to estimate the poses of humans in images or videos. This involves identifying the locations of key body joints such as the head, shoulders, elbows, wrists, hips, knees, and ankles.\nAction Recognition: By analyzing the poses over time, the model can help in recognizing various human actions and activities.\nSurveillance: In security and surveillance applications, ViTPose can be used to monitor and analyze human behavior in public spaces or private premises.\nHealth and Fitness: The model can be utilized in fitness apps to track and analyze exercise poses, providing feedback on form and technique.\nGaming and Animation: ViTPose can be integrated into gaming and animation systems to create more realistic character movements and interactions.\nBias, Risks, and Limitations\nIn this paper, we propose a simple yet effective vision transformer baseline for pose estimation,\ni.e., ViTPose. Despite no elaborate designs in structure, ViTPose obtains SOTA performance\non the MS COCO dataset. However, the potential of ViTPose is not fully explored with more\nadvanced technologies, such as complex decoders or FPN structures, which may further improve the\nperformance. Besides, although the ViTPose demonstrates exciting properties such as simplicity,\nscalability, flexibility, and transferability, more research efforts could be made, e.g., exploring the\nprompt-based tuning to demonstrate the flexibility of ViTPose further. In addition, we believe\nViTPose can also be applied to other pose estimation datasets, e.g., animal pose estimation [47, 9, 45]\nand face keypoint detection [21, 6]. We leave them as the future work.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nimport torch\nimport requests\nimport numpy as np\nfrom PIL import Image\nfrom transformers import (\nAutoProcessor,\nRTDetrForObjectDetection,\nVitPoseForPoseEstimation,\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nurl = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# ------------------------------------------------------------------------\n# Stage 1. Detect humans on the image\n# ------------------------------------------------------------------------\n# You can choose detector by your choice\nperson_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nperson_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\ninputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = person_model(**inputs)\nresults = person_image_processor.post_process_object_detection(\noutputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n)\nresult = results[0]  # take first image results\n# Human label refers 0 index in COCO dataset\nperson_boxes = result[\"boxes\"][result[\"labels\"] == 0]\nperson_boxes = person_boxes.cpu().numpy()\n# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\nperson_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\nperson_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n# ------------------------------------------------------------------------\n# Stage 2. Detect keypoints for each person found\n# ------------------------------------------------------------------------\nimage_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device_map=device)\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n# This is MOE architecture, we should specify dataset indexes for each image in range 0..5\ninputs[\"dataset_index\"] = torch.tensor([0], device=device)\nwith torch.no_grad():\noutputs = model(**inputs)\npose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes], threshold=0.3)\nimage_pose_result = pose_results[0]  # results for first image\nfor i, person_pose in enumerate(image_pose_result):\nprint(f\"Person #{i}\")\nfor keypoint, label, score in zip(\nperson_pose[\"keypoints\"], person_pose[\"labels\"], person_pose[\"scores\"]\n):\nkeypoint_name = model.config.id2label[label.item()]\nx, y = keypoint\nprint(f\" - {keypoint_name}: x={x.item():.2f}, y={y.item():.2f}, score={score.item():.2f}\")\nOutput:\nPerson #0\n- Nose: x=428.81, y=171.53, score=0.92\n- L_Eye: x=429.32, y=168.30, score=0.92\n- R_Eye: x=428.84, y=168.47, score=0.82\n- L_Ear: x=434.60, y=166.54, score=0.90\n- R_Ear: x=440.14, y=165.80, score=0.80\n- L_Shoulder: x=440.74, y=176.95, score=0.96\n- R_Shoulder: x=444.06, y=177.52, score=0.68\n- L_Elbow: x=436.30, y=197.08, score=0.91\n- R_Elbow: x=432.29, y=201.22, score=0.79\n- L_Wrist: x=429.91, y=217.90, score=0.84\n- R_Wrist: x=421.08, y=212.72, score=0.90\n- L_Hip: x=446.15, y=223.88, score=0.74\n- R_Hip: x=449.32, y=223.45, score=0.65\n- L_Knee: x=443.73, y=255.72, score=0.76\n- R_Knee: x=450.72, y=255.21, score=0.73\n- L_Ankle: x=452.14, y=287.30, score=0.66\n- R_Ankle: x=456.02, y=285.99, score=0.72\nPerson #1\n- Nose: x=398.22, y=181.60, score=0.88\n- L_Eye: x=398.67, y=179.84, score=0.87\n- R_Eye: x=396.07, y=179.44, score=0.87\n- R_Ear: x=388.94, y=180.38, score=0.87\n- L_Shoulder: x=397.11, y=194.19, score=0.71\n- R_Shoulder: x=384.75, y=190.74, score=0.55\nTraining Details\nTraining Data\nDataset details. We use MS COCO [28], AI Challenger [41], MPII [3], and CrowdPose [22] datasets\nfor training and evaluation. OCHuman [54] dataset is only involved in the evaluation stage to measure\nthe models‚Äô performance in dealing with occluded people. The MS COCO dataset contains 118K\nimages and 150K human instances with at most 17 keypoint annotations each instance for training.\nThe dataset is under the CC-BY-4.0 license. MPII dataset is under the BSD license and contains\n15K images and 22K human instances for training. There are at most 16 human keypoints for each\ninstance annotated in this dataset. AI Challenger is much bigger and contains over 200K training\nimages and 350 human instances, with at most 14 keypoints for each instance annotated. OCHuman\ncontains human instances with heavy occlusion and is just used for val and test set, which includes\n4K images and 8K instances.\nTraining Hyperparameters\nTraining regime:\nSpeeds, Sizes, Times\nEvaluation\nOCHuman val and test set. To evaluate the performance of human pose estimation models on the\nhuman instances with heavy occlusion, we test the ViTPose variants and representative models on\nthe OCHuman val and test set with ground truth bounding boxes. We do not adopt extra human\ndetectors since not all human instances are annotated in the OCHuman datasets, where the human\ndetector will cause a lot of ‚Äúfalse positive‚Äù bounding boxes and can not reflect the true ability of\npose estimation models. Specifically, the decoder head of ViTPose corresponding to the MS COCO\ndataset is used, as the keypoint definitions are the same in MS COCO and OCHuman datasets.\nMPII val set. We evaluate the performance of ViTPose and representative models on the MPII val\nset with the ground truth bounding boxes. Following the default settings of MPII, we use PCKh\nas metric for performance evaluation.\nResults\nModel Architecture and Objective\nHardware\nThe models are trained on 8 A100 GPUs based on the mmpose codebase\nCitation\nBibTeX:\n@article{xu2022vitposesimplevisiontransformer,\ntitle={ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation},\nauthor={Yufei Xu and Jing Zhang and Qiming Zhang and Dacheng Tao},\nyear={2022},\neprint={2204.12484},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2204.12484}\n}",
    "BSC-LT/salamandraTA-7b-instruct": "SalamandraTA Model Card\nModel Details\nDescription\nArchitecture\nIntended Use\nDirect Use\nOut-of-scope Use\nHardware and Software\nTraining Framework\nCompute Infrastructure\nHow to use\nPost-editing\nDocument-level translation\nNamed-entity recognition\nGrammar checker\nData\nPretraining Data\nInstruction Tuning Data\nEvaluation\nEnglish\nSpanish\nCatalan\nGalician\nBasque\nLow-Resource Languages of Spain\nGender Aware Translation\nEthical Considerations and Limitations\nAdditional information\nAuthor\nContact\nCopyright\nFunding\nAcknowledgements\nDisclaimer\nDisclaimer\nLicense\nCitation\nSalamandraTA Model Card\nSalamandraTA-7b-instruct is a translation LLM that has been instruction-tuned from SalamandraTA-7b-base.\nThe base model results from continually pre-training Salamandra-7b on parallel data and has not been published,\nbut is reserved for internal use.\nSalamandraTA-7b-instruct is proficient in 35 European languages (plus 3 varieties) and supports translation-related tasks, namely: sentence-level-translation,\nparagraph-level-translation, document-level-translation, automatic post-editing, grammar checking, machine translation evaluation, alternative translations,\nnamed-entity-recognition and context-aware translation.\nDISCLAIMER: This version of Salamandra is tailored exclusively for translation tasks. It lacks chat capabilities and has not been trained with any chat instructions.\nModel Details\nDescription\nSalamandraTA-7b-base is a continual pre-training of Salamandra-7b using parallel data, resulting in a total of 424B tokens processed during training.\nArchitecture\nTotal Parameters\n7,768,117,248\nEmbedding Parameters\n1,048,576,000\nLayers\n32\nHidden size\n4,096\nAttention heads\n32\nContext length\n8,192\nVocabulary size\n256,000\nPrecision\nbfloat16\nEmbedding type\nRoPE\nActivation Function\nSwiGLU\nLayer normalization\nRMS Norm\nFlash attention\n‚úÖ\nGrouped Query Attention\n‚úÖ\nNum. query groups\n8\nIntended Use\nDirect Use\nThe model is intended for both research and commercial use in any of the languages included in the training data for general machine translation tasks.\nOut-of-scope Use\nThe model is not intended for malicious activities, such as harming others or violating human rights.\nAny downstream application must comply with current laws and regulations.\nIrresponsible usage in production environments without proper risk assessment and mitigation is also discouraged.\nHardware and Software\nTraining Framework\nSalamandraTA-7b-base was continually pre-trained using NVIDIA‚Äôs NeMo Framework,\nwhich leverages PyTorch Lightning for efficient model training in highly distributed settings.\nSalamandraTA-7b-instruct was produced with FastChat.\nCompute Infrastructure\nAll models were trained on MareNostrum 5, a pre-exascale EuroHPC supercomputer hosted and\noperated by Barcelona Supercomputing Center.\nThe accelerated partition is composed of 1,120 nodes with the following specifications:\n4x Nvidia Hopper GPUs with 64GB HBM2 memory\n2x Intel Sapphire Rapids 8460Y+ at 2.3Ghz and 32c each (64 cores)\n4x NDR200 (BW per node 800Gb/s)\n512 GB of Main memory (DDR5)\n460GB on NVMe storage\nHow to use\nYou can translate between the following 35 languages (and 3 varieties):\nAragonese, Asturian, Basque, Bulgarian, Catalan (and Catalan-Valencian variety), Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hungarian,\nIrish, Italian, Latvian, Lithuanian, Maltese, Norwegian (Bokm√•l and Nynorsk varieties), Occitan (and Aranese variety), Polish, Portuguese, Romanian, Russian, Serbian, Slovak,\nSlovenian, Spanish, Swedish, Ukrainian, Welsh.\nThe instruction-following model uses the commonly adopted ChatML template:\n<|im_start|>system\n{SYSTEM PROMPT}<|im_end|>\n<|im_start|>user\n{USER PROMPT}<|im_end|>\n<|im_start|>assistant\n{MODEL RESPONSE}<|im_end|>\n<|im_start|>user\n[...]\nThe easiest way to apply it is by using the tokenizer's built-in functions, as shown in the following snippet.\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"BSC-LT/salamandraTA-7b-instruct\"\nsource = 'Spanish'\ntarget = 'Catalan'\nsentence = \"Ayer se fue, tom√≥ sus cosas y se puso a navegar. Una camisa, un pantal√≥n vaquero y una canci√≥n, d√≥nde ir√°, d√≥nde ir√°. Se despidi√≥, y decidi√≥ batirse en duelo con el mar. Y recorrer el mundo en su velero. Y navegar, nai-na-na, navegar\"\ntext = f\"Translate the following text from {source} into {target}.\\n{source}: {sentence} \\n{target}:\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16\n)\nmessage = [ { \"role\": \"user\", \"content\": text } ]\ndate_string = datetime.today().strftime('%Y-%m-%d')\nprompt = tokenizer.apply_chat_template(\nmessage,\ntokenize=False,\nadd_generation_prompt=True,\ndate_string=date_string\n)\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\ninput_length = inputs.shape[1]\noutputs = model.generate(input_ids=inputs.to(model.device),\nmax_new_tokens=400,\nearly_stopping=True,\nnum_beams=5)\nprint(tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True))\n# Ahir se'n va anar, va recollir les seves coses i es va fer a la mar. Una camisa, uns texans i una can√ß√≥, on anir√†, on anir√†. Es va acomiadar i va decidir batre's en duel amb el mar. I fer la volta al m√≥n en el seu veler. I navegar, nai-na-na, navegar\nUsing this template, each turn is preceded by a <|im_start|> delimiter and the role of the entity\n(either user, for content supplied by the user, or assistant for LLM responses), and finished with the <|im_end|> token.\nGeneral translation\nFor machine translation tasks, you can use the following prompt template:\nTranslate the following text from {source} into {target}.\n{source}: {source sentence}\n{target}:\nShow an example\nsource = 'Catalan'\ntarget = 'Galician'\nsource_sentence = \"Als antics egipcis del per√≠ode de l'Imperi Nou els fascinaven els monuments dels seus predecessors, que llavors tenien m√©s de mil anys.\"\ntext = f\"Translate the following text from {source} into {target}.\\n{source}: {source_sentence} \\n{target}:\"\n# Os antigos exipcios do per√≠odo do Imperio Novo estaban fascinados polos monumentos dos seus predecesores, que ent√≥n ti√±an m√°is de mil anos de antig√ºidade.\nPost-editing\nFor post-editing tasks, you can use the following prompt template:\nPlease fix any mistakes in the following {source}-{target} machine translation or keep it unedited if it's correct.\nSource: {source_sentence}\nMT: {machine_translation}\nCorrected:\"\nShow an example\nsource = 'Catalan'\ntarget = 'English'\nsource_sentence = 'Rafael Nadal i Maria Magdalena van inspirar a una generaci√≥ sencera.'\nmachine_translation = 'Rafael Christmas and Maria the Muffin inspired an entire generation each in their own way.'\ntext = f\"Please fix any mistakes in the following {source}-{target} machine translation or keep it unedited if it's correct.\\nSource: {source_sentence} \\nMT: {machine_translation} \\nCorrected:\"\n# Rafael Nadal and Maria Magdalena inspired an entire generation.\nDocument-level translation\nFor document-level translation tasks, you can use the following prompt template:\nPlease translate this text from {source} into {target}.\n{source}: {1st paragraph of the document}\n{2nd paragraph of the document}\n{Nth paragraph of the document}\n{target}:\nShow an example\nsource = 'English'\ntarget = 'Asturian'\ntext = \"\"\"Please translate this text from {} into {}.\\n{}: President Donald Trump, who campaigned on promises to crack down on illegal immigration, has raised alarms in the U.S. dairy industry with his threat to impose 25% tariffs on Mexico and Canada by February 2025. This move is part of a broader strategy to declare a national emergency at the southern border to halt illegal migration completely.\nHowever, the implications for the agriculture sector, particularly dairy, are significant. Approximately half of the U.S. dairy industry's workforce consists of immigrant labor, many of whom are undocumented. The National Milk Producers Federation estimates that removing immigrant workers could decimate the dairy herd by 2.1 million cows and slash milk production by nearly 50 billion pounds, leading to a dramatic 90.4% increase in milk prices.\nThe complex perspectives of Americans on undocumented workers were highlighted in a Pew Research Center study. While 64% of U.S. adults support legal pathways for undocumented immigrants, 35% oppose it‚Äîa gap that has been narrowing recently. Factors influencing public opinion include the belief that immigrants should have jobs and pass security checks, contrasted by concerns about lawbreakers being rewarded, fairness for legal migrants, and resource allocation.\nAccording to Zach Rutledge, an agricultural economist at Michigan State University, as nations grow wealthier, their labor forces transition away from agriculture toward sectors like services and manufacturing. This shift has led to the U.S. relying heavily on immigrant labor for agricultural work. Domestic workers, even with employment taxes, may cost $15 to $25 an hour, while H-2A visa program workers might cost $25 to $30 an hour, accounting for additional housing expenses.\nThe National Milk Producers Federation has been vocal in advocating for changes to the H-2A visa program, which outside of its current seasonal limitations, does not support the dairy industry's year-round labor needs. Executive vice-president Jaime Castaneda reiterated the need for legislative clarity to address the undocumented workforce issues in dairy farming.\nThe Farm Workforce Modernization Act of 2023, which could grant legal status to certain undocumented farmworkers, has been stalled in Congress, despite acknowledgment of the sector's importance to feeding America. The need for coordinated legislative efforts to ensure both border security and labor market stability is imperative moving forward.\n{}:\"\"\".format(source, target, source, target)\nNamed-entity recognition\nFor named-entity recognition tasks, you can use the following prompt template:\nAnalyse the following tokenized text and mark the tokens containing named entities.\nUse the following annotation guidelines with these tags for named entities:\n- ORG (Refers to named groups or organizations)\n- PER (Refers to individual people or named groups of people)\n- LOC (Refers to physical places or natural landmarks)\n- MISC (Refers to entities that don't fit into standard categories).\nPrepend B- to the first token of a given entity and I- to the remaining ones if they exist.\nIf a token is not a named entity, label it as O.\nInput: {list of words in a sentence}\nMarked:\nShow an example\ntext = \"\"\"Analyse the following tokenized text and mark the tokens containing named entities.\nUse the following annotation guidelines with these tags for named entities:\n- ORG (Refers to named groups or organizations)\n- PER (Refers to individual people or named groups of people)\n- LOC (Refers to physical places or natural landmarks)\n- MISC (Refers to entities that don't fit into standard categories).\nPrepend B- to the first token of a given entity and I- to the remaining ones if they exist.\nIf a token is not a named entity, label it as O.\nInput: ['La', 'defensa', 'del', 'antiguo', 'responsable', 'de', 'la', 'RFEF', 'confirma', 'que', 'interpondr√°', 'un', 'recurso.']\nMarked: \"\"\"\n# [('La', 'O'), ('defensa', 'O'), ('del', 'O'), ('antiguo', 'O'), ('responsable', 'O'), ('de', 'O'), ('la', 'O'), ('RFEF', 'B-ORG'), ('confirma', 'O'), ('que', 'O'), ('interpondr√°', 'O'), ('un', 'O'), ('recurso.', 'O')]\nGrammar checker\nFor fixing any mistakes in grammar, you can use the following prompt template:\nPlease fix any mistakes in the following {source} sentence or keep it unedited if it's correct.\nSentence: {sentence}\nCorrected:\nShow an example\nsource = 'Catalan'\nsentence = 'Entonses, el meu jefe m‚Äôha dit que he de treballar els fins de setmana.'\ntext = f\"Please fix any mistakes in the following {source} sentence or keep it unedited if it's correct.\\nSentence: {sentence} \\nCorrected:\"\n# Llavors, el meu cap m'ha dit que he de treballar els caps de setmana.\nData\nPretraining Data\nThe pretraining corpus consists of 424 billion tokens of Catalan-centric, Spanish-centric, and English-centric parallel data,\nincluding all of the official European languages plus Catalan, Basque, Galician, Asturian, Aragonese and Aranese.\nIt amounts to 6,574,251,526 parallel sentence pairs.\nThis highly multilingual corpus is predominantly composed of data sourced from OPUS,\nwith additional data taken from the NTEU Project, Aina Project, and other sources\n(see: Data Sources and References).\nWhere little parallel Catalan <-> xx data could be found, synthetic Catalan data was generated from the Spanish side of the collected Spanish <-> xx corpora using\nProjecte Aina‚Äôs Spanish-Catalan model. The final distribution of languages was as below:\nClick the expand button below to see the full list of corpora included in the training data.\nData Sources\nDataset\nCa-xx Languages\nEs-xx Langugages\nEn-xx Languages\nAINA\nen\nARANESE-SYNTH-CORPUS-BSC\narn\nBOUA-SYNTH-BSC\nval\nBOUMH\nval\nBOUA-PILAR\nval\nCCMatrix\neu\nga\nDGT\nbg,cs,da,de,el\t,et,fi,fr,ga,hr,hu,lt,lv,mt,nl,pl,pt,ro,sk,sl,sv\nda,et,ga,hr,hu,lt,lv,mt,sh,sl\nDOGV-SYNTH-BSC\nval\nDOGV-PILAR\nval\nELRC-EMEA\nbg,cs,da,hu,lt,lv,mt,pl,ro,sk,sl\net,hr,lv,ro,sk,sl\nEMEA\nbg,cs,da,el,fi,hu,lt,mt,nl,pl,ro,sk,sl,sv\net,mt\nEUBookshop\nlt,pl,pt\ncs,da,de,el,fi,fr,ga,it,lv,mt,nl,pl,pt,ro,sk,sl,sv\ncy,ga\nEuroparl\nbg,cs,da,el,en,fi,fr,hu,lt,lv,nl,pl,pt\t,ro,sk,sl,sv\nEuropat\nen,hr\nno\nGAITU Corpus\neu\nKDE4\nbg,cs,da,de,el\t,et,eu,fi,fr,ga,gl,hr,it,lt,lv,nl,pl,pt,ro,sk,sl,sv\nbg,ga,hr\ncy,ga,nn,oc\nGlobalVoices\nbg,de,fr,it,nl,pl,pt\nbg,de,fr,pt\nGNOME\neu,fr,ga,gl,pt\nga\ncy,ga,nn\nJRC-Arquis\ncs,da,et,fr,lt,lv,mt,nl,pl\t,ro,sv\net\nLES-CORTS-VALENCIANES-SYNTH-BSC\nval\nMaCoCu\nen\nhr,mt,uk\nMultiCCAligned\nbg,cs,de,el,et,fi,fr,hr,hu,it,lt,lv,nl,pl,ro,sk,sv\nbg,fi,fr,hr,it,lv,nl,pt\nbg,cy,da,et,fi,hr,hu,lt,lv,no,sl,sr,uk\nMultiHPLT\nen, et,fi,ga,hr,mt\nfi,ga,gl,hr,mt,nn,sr\nMultiParaCrawl\nbg,da\nde,en,fr,ga,hr,hu,it,mt,pt\nbg,cs,da,de,el,et,fi,fr,ga,hr,hu,lt,lv,mt,nn,pl,ro,sk,sl,uk\nMultiUN\nfr\nNews-Commentary\nfr\nNLLB\nbg,da,el,en,et,fi,fr,gl,hu,it\t,lt,lv,pt,ro,sk,sl\nbg,cs,da,de,el\t,et,fi,fr,hu,it,lt,lv,nl,pl,pt\t,ro,sk,sl,sv\nbg,cs,cy,da,de,el,et,fi,fr,ga,hr,hu,it,lt,lv,mt,nl,no,oc,pl,pt,ro,ru,sk,sl,sr,sv,uk\nN√ìS Authentic Corpus\ngl\nN√ìS Synthetic Corpus\ngl\nNTEU\nbg,cs,da,de,el,en,et,fi,fr,ga,hr,hu,it,lt,lv,mt,nl,pl,pt,ro,sk,sl,sv\nda,et,ga,hr,lt,lv,mt,ro,sk,sl,sv\nOpenSubtitles\nbg,cs,da,de,el\t,et,eu,fi,gl,hr,hu,lt,lv,nl,pl,pt,ro,sk,sl,sv\nda,de,fi,fr,hr,hu,it,lv,nl\nbg,cs,de,el,et,hr,fi,fr,hr,hu,no,sl,sr\nOPUS-100\nen\ngl\nStanfordNLP-NMT\ncs\nTatoeba\nde,pt\npt\nTildeModel\nbg\net,hr,lt,lv,mt\nUNPC\nen,fr\nru\nPILAR-VALENCIAN-AUTH\nval\nPILAR-VALENCIAN-SYNTH\nval\nWikiMatrix\nbg,cs,da,de,el\t,et,eu,fi,fr,gl,hr,hu,it,lt,nl,pl,pt,ro,sk,sl,sv\nbg,en,fr,hr,it,pt\noc,sh\nWikimedia\ncy,nn\nXLENT\neu,ga,gl\nga\ncy,et,ga,gl,hr,oc,sh\nDatasets with \"-BSC\" in their names (e.g., BOUA-SYNTH-BSC, DOGV-SYNTH-BSC) are synthetic datasets obtained by machine translating\npre-existing monolingual corpora with our own seq-to-seq models. These datasets were generated internally for model training and are not published.\nTo consult the data summary document with the respective licences, please send an e-mail to ipr@bsc.es.\nReferences\nAulamo, M., Sulubacak, U., Virpioja, S., & Tiedemann, J. (2020). OpusTools and Parallel Corpus Diagnostics. In N. Calzolari, F. B√©chet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference (pp. 3782‚Äì3789). European Language Resources Association. https://aclanthology.org/2020.lrec-1.467\nChaudhary, V., Tang, Y., Guzm√°n, F., Schwenk, H., & Koehn, P. (2019). Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings. In O. Bojar, R. Chatterjee, C. Federmann, M. Fishel, Y. Graham, B. Haddow, M. Huck, A. J. Yepes, P. Koehn, A. Martins, C. Monz, M. Negri, A. N√©v√©ol, M. Neves, M. Post, M. Turchi, & K. Verspoor (Eds.), Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2) (pp. 261‚Äì266). Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-5435\nDGT-Translation Memory‚ÄîEuropean Commission. (n.d.). Retrieved November 4, 2024, from https://joint-research-centre.ec.europa.eu/language-technology-resources/dgt-translation-memory_en\nEisele, A., & Chen, Y. (2010). MultiUN: A Multilingual Corpus from United Nation Documents. In N. Calzolari, K. Choukri, B. Maegaard, J. Mariani, J. Odijk, S. Piperidis, M. Rosner, & D. Tapias (Eds.), Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC‚Äô10). European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf\nEl-Kishky, A., Chaudhary, V., Guzm√°n, F., & Koehn, P. (2020). CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 5960‚Äì5969. https://doi.org/10.18653/v1/2020.emnlp-main.480\nEl-Kishky, A., Renduchintala, A., Cross, J., Guzm√°n, F., & Koehn, P. (2021). XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 10424‚Äì10430. https://doi.org/10.18653/v1/2021.emnlp-main.814\nFan, A., Bhosale, S., Schwenk, H., Ma, Z., El-Kishky, A., Goyal, S., Baines, M., Celebi, O., Wenzek, G., Chaudhary, V., Goyal, N., Birch, T., Liptchinsky, V., Edunov, S., Grave, E., Auli, M., & Joulin, A. (2020). Beyond English-Centric Multilingual Machine Translation (No. arXiv:2010.11125). arXiv. https://doi.org/10.48550/arXiv.2010.11125\nGarc√≠a-Mart√≠nez, M., Bi√©, L., Cerd√†, A., Estela, A., Herranz, M., Kri≈°lauks, R., Melero, M., O‚ÄôDowd, T., O‚ÄôGorman, S., Pinnis, M., Stafanoviƒç, A., Superbo, R., & Vasiƒºevskis, A. (2021). Neural Translation for European Union (NTEU). 316‚Äì334. https://aclanthology.org/2021.mtsummit-up.23\nGibert, O. de, Nail, G., Arefyev, N., Ba√±√≥n, M., Linde, J. van der, Ji, S., Zaragoza-Bernabeu, J., Aulamo, M., Ram√≠rez-S√°nchez, G., Kutuzov, A., Pyysalo, S., Oepen, S., & Tiedemann, J. (2024). A New Massive Multilingual Dataset for High-Performance Language Technologies (No. arXiv:2403.14009). arXiv. http://arxiv.org/abs/2403.14009\nKoehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. Proceedings of Machine Translation Summit X: Papers, 79‚Äì86. https://aclanthology.org/2005.mtsummit-papers.11\nKreutzer, J., Caswell, I., Wang, L., Wahab, A., Van Esch, D., Ulzii-Orshikh, N., Tapo, A., Subramani, N., Sokolov, A., Sikasote, C., Setyawan, M., Sarin, S., Samb, S., Sagot, B., Rivera, C., Rios, A., Papadimitriou, I., Osei, S., Suarez, P. O., ‚Ä¶ Adeyemi, M. (2022). Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics, 10, 50‚Äì72. https://doi.org/10.1162/tacl_a_00447\nRozis, R.,Skadi≈Ü≈°, R (2017). Tilde MODEL - Multilingual Open Data for EU Languages. https://aclanthology.org/W17-0235\nSchwenk, H., Chaudhary, V., Sun, S., Gong, H., & Guzm√°n, F. (2019). WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia (No. arXiv:1907.05791). arXiv. https://doi.org/10.48550/arXiv.1907.05791\nSchwenk, H., Wenzek, G., Edunov, S., Grave, E., & Joulin, A. (2020). CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB (No. arXiv:1911.04944). arXiv. https://doi.org/10.48550/arXiv.1911.04944\nSteinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufi≈ü, D., & Varga, D. (n.d.). The JRC-Acquis: A Multilingual Aligned Parallel Corpus with 20+ Languages. http://www.lrec-conf.org/proceedings/lrec2006/pdf/340_pdf\nSubramani, N., Luccioni, S., Dodge, J., & Mitchell, M. (2023). Detecting Personal Information in Training Corpora: An Analysis. In A. Ovalle, K.-W. Chang, N. Mehrabi, Y. Pruksachatkun, A. Galystan, J. Dhamala, A. Verma, T. Cao, A. Kumar, & R. Gupta (Eds.), Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023) (pp. 208‚Äì220). Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.trustnlp-1.18\nTiedemann, J. (23-25). Parallel Data, Tools and Interfaces in OPUS. In N. C. (Conference Chair), K. Choukri, T. Declerck, M. U. Doƒüan, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC‚Äô12). European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper\nZiemski, M., Junczys-Dowmunt, M., & Pouliquen, B. (n.d.). The United Nations Parallel Corpus v1.0. https://aclanthology.org/L16-1561\nInstruction Tuning Data\nThis model has been fine-tuned on ~135k instructions, primarily targeting machine translation performance for Catalan, English, and Spanish.\nAdditional instruction data for other European and closely related Iberian languages was also included, as it yielded a positive impact on the languages of interest.\nThat said, the performance in these additional languages is not guaranteed due to the limited amount of available data and the lack of resources for thorough testing.\nA portion of our fine-tuning data comes directly from, or is sampled from TowerBlocks.\nWe also created additional datasets for our main languages of interest.\nWhile tasks relating to machine translation are included, it‚Äôs important to note that no chat data was used in the fine-tuning process.\nThe final distribution of tasks was as below:\nClick the expand button below to see the full list of tasks included in the finetuning data.\nData Sources\nTask\nSource\nLanguages\nCount\nMulti-reference Translation\nTowerBlocks: Tatoeba Dev (filtered)\nmixed\n10000\nParaphrase\nTowerBlocks: PAWS-X Dev\nmixed\n3521\nNamed-entity Recognition\nAnCora-Ca-NER\nca\n12059\nNamed-entity Recognition\nBasqueGLUE, EusIE\neu\n4304\nNamed-entity Recognition\nSLI NERC Galician Gold Corpus\ngl\n6483\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\npt\n854\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nnl\n800\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nes\n1654\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nen\n1671\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nru\n800\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nit\n858\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nfr\n857\nNamed-entity Recognition\nTowerBlocks: MultiCoNER 2022 and 2023 Dev\nde\n1312\nTerminology-aware Translation\nTowerBlocks: WMT21 Terminology Dev (filtered)\nen-ru\n50\nTerminology-aware Translation\nTowerBlocks: WMT21 Terminology Dev (filtered)\nen-fr\n29\nAutomatic Post Editing\nTowerBlocks: QT21, ApeQuest\nen-fr\n6133\nAutomatic Post Editing\nTowerBlocks: QT21, ApeQuest\nen-nl\n9077\nAutomatic Post Editing\nTowerBlocks: QT21, ApeQuest\nen-pt\n5762\nAutomatic Post Editing\nTowerBlocks: QT21, ApeQuest\nde-en\n10000\nAutomatic Post Editing\nTowerBlocks: QT21, ApeQuest\nen-de\n10000\nMachine Translation Evaluation\nTowerBlocks-sample: WMT20 to WMT22 Metrics MQM, WMT17 to WMT22 Metrics Direct Assessments\nen-ru, en-pl, ru-en, en-de, en-ru, de-fr, de-en, en-de\n353\nMachine Translation Evaluation\nNon-public\nfour pivot languages (eu, es, ca, gl) paired with European languages (bg, cs, da, de, el, en, et, fi, fr, ga, hr, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv)\n9700\nGeneral Machine Translation\nTowerBlocks: WMT14 to WMT21, NTREX, Flores Dev, FRMT, QT21, ApeQuest, OPUS (Quality Filtered), MT-GenEval\nnl-en, en-ru, it-en, fr-en, es-en, en-fr, ru-en, fr-de, en-nl, de-fr\n500\nGeneral Machine Translation\nNon-public\nthree pivot languages (es, ca, en) paired with European languages (ast, arn, arg, bg, cs, cy, da, de, el, et, fi, ga, gl, hr, it, lt, lv, mt, nb, nn, nl, oc, pl, pt, ro, ru, sk, sl, sr, sv, uk, eu)\n9350\nFill-in-the-Blank\nNon-public\nfive pivot languages (ca, es, eu, gl, en) paired with European languages (cs, da, de, el, et, fi, fr, ga, hr, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv)\n11500\nDocument-level Translation\nNon-public\ntwo pivot languages (es, en) paired with European languages (bg, cs, da, de, el, et, fi, fr, hu, it, lt, lv, nl, pl, pt, ro, ru, sk, sv)\n7600\nParagraph-level Translation\nNon-public\ntwo pivot languages (es, en) paired with European languages (bg, cs, da, de, el, et, fi, fr, hu, it, lt, lv, nl, pl, pt, ro, ru, sk, sv)\n7600\nContext-Aware Translation\nTowerBlocks: MT-GenEval\nen-it\n348\nContext-Aware Translation\nTowerBlocks: MT-GenEval\nen-ru\n454\nContext-Aware Translation\nTowerBlocks: MT-GenEval\nen-fr\n369\nContext-Aware Translation\nTowerBlocks: MT-GenEval\nen-nl\n417\nContext-Aware Translation\nTowerBlocks: MT-GenEval\nen-es\n431\nContext-Aware Translation\nTowerBlocks: MT-GenEval\nen-de\n558\nTotal\n135,404\nThe non-public portion of this dataset was jointly created by the ILENIA partners: BSC-LT, HiTZ,\nand CiTIUS. For further information regarding the instruction-tuning data,\nplease contact langtech@bsc.es.\nReferences\nAlves, D. M., Pombal, J., Guerreiro, N. M., Martins, P. H., Alves, J., Farajian, A., Peters, B., Rei, R., Fernandes, P., Agrawal, S., Colombo, P., de Souza, J. G. C., & Martins, A. F. T. (2024). Tower: An open multilingual large language model for translation-related tasks (No. arXiv: 2402.17733). arXiv. https://arxiv.org/abs/2402.17733\nArmengol-Estap√©, J., Carrino, C. P., Rodriguez-Penagos, C., de Gibert Bonet, O., Armentano-Oller, C., Gonzalez-Agirre, A., Melero, M., & Villegas, M. (2021). Are multilingual models the best choice for moderately under-resourced languages? A comprehensive assessment for Catalan. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 4933‚Äì4946. Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.findings-acl.437\nCurrey, A., Nadejde, M., Pappagari, R. R., Mayer, M., Lauly, S., Niu, X., Hsu, B., & Dinu, G. (2022). MT-GenEval: A counterfactual and contextual dataset for evaluating gender accuracy in machine translation. In Y. Goldberg, Z. Kozareva, & Y. Zhang (Eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 4287‚Äì4299). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.emnlp-main.288\nFedermann, C., Kocmi, T., & Xin, Y. (2022). NTREX-128 ‚Äì News test references for MT evaluation of 128 languages. Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, 21‚Äì24. Association for Computational Linguistics. https://aclanthology.org/2022.sumeval-1.4\nIve, J., Specia, L., Szoc, S., Vanallemeersch, T., Van den Bogaert, J., Farah, E., Maroti, C., Ventura, A., & Khalilov, M. (2020). A post-editing dataset in the legal domain: Do we underestimate neural machine translation quality? In N. Calzolari, F. B√©chet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference (pp. 3692‚Äì3697). European Language Resources Association. https://aclanthology.org/2020.lrec-1.455/\nMalmasi, S., Fang, A., Fetahu, B., Kar, S., & Rokhlenko, O. (2022). MultiCoNER: A large-scale multilingual dataset for complex named entity recognition. Proceedings of the 29th International Conference on Computational Linguistics, 3798‚Äì3809. International Committee on Computational Linguistics. https://aclanthology.org/2022.coling-1.334/\nNLLB Team, Costa-juss√†, M. R., Cross, J., √áelebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., Sun, A., Wang, S., Wenzek, G., Youngblood, A., Akula, B., Barrault, L., Mejia Gonzalez, G., Hansanti, P., Hoffman, J., Jarrett, S., Sadagopan, K. R., Rowe, D., Spruit, S., Tran, C., Andrews, P., Ayan, N. F., Bhosale, S., Edunov, S., Fan, A., Gao, C., Goswami, V., Guzm√°n, F., Koehn, P., Mourachko, A., Ropers, C., Saleem, S., Schwenk, H., & Wang, J. (2022). No language left behind: Scaling human-centered machine translation (No. arXiv: 2207.04672). arXiv. https://arxiv.org/abs/2207.04672\nRiley, P., Dozat, T., Botha, J. A., Garcia, X., Garrette, D., Riesa, J., Firat, O., & Constant, N. (2022). FRMT: A benchmark for few-shot region-aware machine translation (No. arXiv: 2210.00193). arXiv. https://doi.org/10.48550/ARXIV.2210.00193\nSpecia, L., Harris, K., Blain, F., Burchardt, A., Macketanz, V., Skadi≈Üa, I., Negri, M., & Turchi, M. (2017). Translation quality and productivity: A study on rich morphology languages. Proceedings of Machine Translation Summit XVI, 55‚Äì71. Nagoya, Japan.\nTiedemann, J. (2020). The Tatoeba translation challenge ‚Äì Realistic data sets for low-resource and multilingual MT. Proceedings of the Fifth Conference on Machine Translation, 1174‚Äì1182. Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.wmt-1.139\nUrbizu, G., San Vicente, I., Saralegi, X., Agerri, R., & Soroa, A. (2022). BasqueGLUE: A natural language understanding benchmark for Basque. Proceedings of the Language Resources and Evaluation Conference, 1603‚Äì1612. European Language Resources Association. https://aclanthology.org/2022.lrec-1.172\nYang, Y., Zhang, Y., Tar, C., & Baldridge, J. (2019). PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (pp. 3687‚Äì3692). Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1382\nZubillaga, M., Sainz, O., Estarrona, A., Lopez de Lacalle, O., & Agirre, E. (2024). Event extraction in Basque: Typologically motivated cross-lingual transfer-learning analysis (No. arXiv: 2404.06392). arXiv. https://arxiv.org/abs/2404.06392\nEvaluation\nBelow are the evaluation results on the Flores+200 devtest set,\ncompared against the state-of-the-art MADLAD400-7B-mt model (Kudugunta, S., et al.) and SalamandraTA-7b-base model.\nThese results cover the translation directions CA-XX, ES-XX, EN-XX, as well as XX-CA, XX-ES, and XX-EN.\nThe metrics have been computed excluding Asturian, Aranese, and Aragonese, as we report them separately.\nThe evaluation was conducted using MT-Lens, following the standard setting (beam search with beam size 5, limiting the translation length to 500 tokens). We report the following metrics:\nClick to show metrics details\nBLEU: Sacrebleu implementation. Signature: nrefs:1‚Äî case:mixed‚Äî eff:no‚Äî tok:13a‚Äî smooth:exp‚Äîversion:2.3.1\nTER: Sacrebleu implementation.\nChrF: Sacrebleu implementation.\nComet: Model checkpoint: \"Unbabel/wmt22-comet-da\".\nComet-kiwi: Model checkpoint: \"Unbabel/wmt22-cometkiwi-da\".\nBleurt: Model checkpoint: \"lucadiliello/BLEURT-20\".\nMetricX: Model checkpoint: \"google/metricx-23-xl-v2p0\".\nMetricX-QE: Model checkpoint: \"google/metricx-23-qe-xl-v2p0\".\nEnglish evaluation\nEnglish\nThis section presents the evaluation metrics for English translation tasks.\nBleu‚Üë\nTer‚Üì\nChrF‚Üë\nComet‚Üë\nComet-kiwi‚Üë\nBleurt‚Üë\nMetricX‚Üì\nMetricX-QE‚Üì\nEN-XX\nSalamandraTA-7b-instruct\n35.20\n53.40\n61.58\n0.89\n0.86\n0.78\n0.96\n0.81\nMADLAD400-7B\n35.73\n51.87\n63.46\n0.88\n0.85\n0.79\n1.16\n1.10\nSalamandraTA-7b-base\n34.99\n52.64\n62.58\n0.87\n0.84\n0.77\n1.45\n1.23\nXX-EN\nSalamandraTA-7b-instruct\n44.37\n42.49\n68.29\n0.89\n0.86\n0.80\n1.05\n0.99\nMADLAD400-7B\n43.20\n43.33\n67.98\n0.89\n0.86\n0.80\n1.13\n1.15\nSalamandraTA-7b-base\n44.12\n43.00\n68.43\n0.89\n0.85\n0.80\n1.13\n1.22\nSpanish evaluation\nSpanish\nThis section presents the evaluation metrics for Spanish translation tasks.\nBleu‚Üë\nTer‚Üì\nChrF‚Üë\nComet‚Üë\nComet-kiwi‚Üë\nBleurt‚Üë\nMetricX‚Üì\nMetricX-QE‚Üì\nES-XX\nSalamandraTA-7b-instruct\n23.68\n67.31\n53.98\n0.87\n0.83\n0.76\n0.93\n0.80\nMADLAD400-7B\n22.48\n68.91\n53.93\n0.86\n0.83\n0.75\n1.09\n1.14\nSalamandraTA-7b-base\n21.63\n70.08\n52.98\n0.86\n0.83\n0.74\n1.24\n1.12\nXX-ES\nSalamandraTA-7b-instruct\n26.40\n62.27\n53.54\n0.85\n0.84\n0.74\n0.80\n1.07\nMADLAD400-7B\n24.85\n61.82\n53.00\n0.85\n0.84\n0.74\n1.05\n1.50\nSalamandraTA-7b-base\n24.71\n62.33\n52.96\n0.85\n0.84\n0.73\n1.06\n1.37\nCatalan evaluation\nCatalan\nThis section presents the evaluation metrics for Catalan translation tasks.\nBleu‚Üë\nTer‚Üì\nChrF‚Üë\nComet‚Üë\nComet-kiwi‚Üë\nBleurt‚Üë\nMetricX‚Üì\nMetricX-QE‚Üì\nCA-XX\nSalamandraTA-7b-instruct\n29.50\n59.26\n58.21\n0.88\n0.81\n0.77\n0.97\n0.98\nMADLAD400-7B\n29.37\n59.01\n58.47\n0.87\n0.81\n0.77\n1.08\n1.31\nSalamandraTA-7b-base\n29.06\n59.32\n58.00\n0.87\n0.81\n0.76\n1.23\n1.28\nXX-CA\nSalamandraTA-7b-instruct\n34.51\n54.21\n60.10\n0.86\n0.81\n0.76\n0.90\n1.29\nMADLAD400-7B\n33.02\n55.01\n59.38\n0.86\n0.81\n0.75\n1.18\n1.79\nSalamandraTA-7b-base\n32.75\n55.78\n59.42\n0.86\n0.81\n0.75\n1.17\n1.63\nGalician evaluation\nGalician\nThis section presents the evaluation metrics for Galician translation tasks.\nBleu‚Üë\nTer‚Üì\nChrF‚Üë\nComet‚Üë\nComet-kiwi‚Üë\nBleurt‚Üë\nMetricX‚Üì\nMetricX-QE‚Üì\nGL-XX\nSalamandraTA-7b-instruct\n36.95\n50.12\n62.55\n0.88\n0.85\n0.77\n0.86\n0.98\nMADLAD400-7B\n26.43\n64.30\n55.99\n0.86\n0.85\n0.76\n1.35\n2.06\nSalamandraTA-7b-base\n27.47\n61.39\n56.96\n0.87\n0.82\n0.76\n1.23\n1.29\nXX-GL\nSalamandraTA-7b-instruct\n34.37\n52.49\n60.99\n0.88\n0.85\n0.73\n0.75\n0.92\nMADLAD400-7B\n27.77\n59.46\n54.92\n0.84\n0.85\n0.67\n1.42\n2.72\nSalamandraTA-7b-base\n28.22\n59.52\n56.28\n0.85\n0.82\n0.69\n1.27\n1.78\nBasque evaluation\nBasque\nThis section presents the evaluation metrics for Basque translation tasks.\nBleu‚Üë\nTer‚Üì\nChrF‚Üë\nComet‚Üë\nComet-kiwi‚Üë\nBleurt‚Üë\nMetricX‚Üì\nMetricX-QE‚Üì\nEU-XX\nSalamandraTA-7b-instruct\n29.89\n58.54\n56.66\n0.87\n0.85\n0.76\n0.90\n0.89\nMADLAD400-7B\n21.26\n69.75\n49.80\n0.85\n0.82\n0.72\n1.54\n2.71\nSalamandraTA-7b-base\n22.87\n67.38\n52.19\n0.86\n0.79\n0.74\n1.19\n1.61\nXX-EU\nSalamandraTA-7b-instruct\n18.89\n71.74\n57.16\n0.87\n0.84\n0.82\n0.58\n0.44\nMADLAD400-7B\n13.64\n85.01\n50.96\n0.82\n0.80\n0.78\n2.09\n3.58\nSalamandraTA-7b-base\n17.01\n75.92\n55.22\n0.85\n0.77\n0.80\n1.04\n1.17\nLow-Resource Languages of Spain\nThe tables below summarize the performance metrics for English, Spanish, and Catalan to Asturian, Aranese and Aragonese compared\nagainst Transducens/IbRo-nllb (Galiano Jimenez, et al.),\nNLLB-200-3.3B (Costa-juss√† et al., 2022) and SalamandraTA-2B.\nEnglish evaluation\nEnglish-XX\nsource\ntarget\nBleu ‚Üë\nTer ‚Üì\nChrF ‚Üë\nSalamandraTA-7b-instruct\nen\nast\n31.79\n54.07\n61.78\nSalamandraTA-7b-base\nen\nast\n26.40\n64.02\n57.35\nTransducens/IbRo-nllb\nen\nast\n20.56\n63.92\n53.32\nSalamandraTA-7b-instruct\nen\narn\n22.77\n66.06\n52.61\nSalamandraTA-7b-base\nen\narn\n14.13\n74.05\n46.17\nTransducens/IbRo-nllb\nen\narn\n12.81\n73.21\n45.76\nSalamandraTA-7b-instruct\nen\narg\n19.74\n71.58\n51.08\nTransducens/IbRo-nllb\nen\narg\n14.07\n70.37\n46.89\nSalamandraTA-7b-base\nen\narg\n12.24\n73.48\n44.75\nSpanish evaluation\nSpanish-XX\nsource\ntarget\nBleu ‚Üë\nTer ‚Üì\nChrF ‚Üë\nSalamandraTA-7b-instruct\nes\nast\n20.66\n71.81\n53.14\nSalamandraTA-7b-base\nes\nast\n17.65\n75.78\n51.05\nTransducens/IbRo-nllb\nes\nast\n16.79\n76.36\n50.89\nSalamandraTA-7b-base\nes\narn\n51.59\n35.51\n73.50\nTransducens/IbRo-nllb\nes\narn\n50.20\n36.60\n73.16\nSalamandraTA-7b-instruct\nes\narn\n47.37\n39.29\n70.65\nTransducens/IbRo-nllb\nes\narg\n59.75\n28.01\n78.73\nSalamandraTA-7b-base\nes\narg\n53.96\n31.51\n76.08\nSalamandraTA-7b-instruct\nes\narg\n44.10\n39.98\n71.12\nCatalan evaluation\nCatalan-XX\nsource\ntarget\nBleu ‚Üë\nTer ‚Üì\nChrF ‚Üë\nSalamandraTA-7b-instruct\nca\nast\n28.13\n58.84\n58.98\nSalamandraTA-7b-base\nca\nast\n26.11\n63.63\n58.08\nTransducens/IbRo-nllb\nca\nast\n24.77\n61.60\n57.49\nSalamandraTA-7b-base\nca\narn\n31.76\n53.71\n60.71\nTransducens/IbRo-nllb\nca\narn\n31.22\n54.30\n60.30\nSalamandraTA-7b-instruct\nca\narn\n30.89\n54.70\n59.78\nTransducens/IbRo-nllb\nca\narg\n24.44\n60.79\n55.51\nSalamandraTA-7b-base\nca\narg\n22.53\n62.37\n54.32\nSalamandraTA-7b-instruct\nca\narg\n20.96\n65.64\n52.41\nGender Aware Translation\nBelow are the evaluation results for gender aware translation evaluated on the MT-GenEval\ndataset (Currey, A. et al.).\nThese have been calculated for translation from English into German, Spanish, French, Italian, Portuguese and Russian and are compared\nagainst MADLAD400-7B-mt, TowerInstruct-7B-v0.2\nand the SalamandraTA-7b-base model.\nEvaluation was conducted using MT-Lens and is reported as accuracy computed using the accuracy metric\nprovided with MT-GenEval.\nSource\nTarget\nMasc\nFem\nPair\nMADLAD400-7B\nen\nde\n0.877\n0.823\n0.713\nSalamandraTA-7b-base\nen\nde\n0.857\n0.770\n0.660\nSalamandraTA-7b-instruct\nen\nde\n0.863\n0.867\n0.740\nTowerInstruct-7B-v0.2\nen\nde\n0.863\n0.840\n0.727\nMADLAD400-7B\nen\nes\n0.887\n0.780\n0.687\nSalamandraTA-7b-base\nen\nes\n0.890\n0.733\n0.643\nSalamandraTA-7b-instruct\nen\nes\n0.860\n0.837\n0.710\nTowerInstruct-7B-v0.2\nen\nes\n0.850\n0.823\n0.693\nMADLAD400-7B\nen\nfr\n0.873\n0.777\n0.663\nSalamandraTA-7b-base\nen\nfr\n0.887\n0.710\n0.617\nSalamandraTA-7b-instruct\nen\nfr\n0.900\n0.813\n0.730\nTowerInstruct-7B-v0.2\nen\nfr\n0.880\n0.823\n0.717\nMADLAD400-7B\nen\nit\n0.907\n0.663\n0.597\nSalamandraTA-7b-base\nen\nit\n0.893\n0.593\n0.513\nSalamandraTA-7b-instruct\nen\nit\n0.913\n0.780\n0.707\nTowerInstruct-7B-v0.2\nen\nit\n0.947\n0.747\n0.713\nMADLAD400-7B\nen\npt\n0.923\n0.687\n0.627\nSalamandraTA-7b-base\nen\npt\n0.923\n0.650\n0.597\nSalamandraTA-7b-instruct\nen\npt\n0.933\n0.797\n0.747\nTowerInstruct-7B-v0.2\nen\npt\n0.907\n0.730\n0.670\nMADLAD400-7B\nen\nru\n0.940\n0.797\n0.740\nSalamandraTA-7b-base\nen\nru\n0.933\n0.713\n0.653\nSalamandraTA-7b-instruct\nen\nru\n0.950\n0.830\n0.783\nTowerInstruct-7B-v0.2\nen\nru\n0.933\n0.797\n0.733\nEthical Considerations and Limitations\nDetailed information on the work done to examine the presence of unwanted social and cognitive biases in the base model can be found\nat Salamandra-7B model card.\nWith regard to MT models, the only analysis related to bias which we have conducted is the MT-GenEval evaluation.\nNo specific analysis has yet been carried out in order to evaluate potential biases or limitations in translation\naccuracy across different languages, dialects, or domains. However, we recognize the importance of identifying and addressing any harmful stereotypes,\ncultural inaccuracies, or systematic performance discrepancies that may arise in Machine Translation. As such, we plan to continue performing more analyses\nas we implement the necessary metrics and methods within our evaluation framework MT-Lens.\nNote that the model has only undergone preliminary instruction tuning.\nWe urge developers to consider potential limitations and conduct safety testing and tuning tailored to their specific applications.\nAdditional information\nAuthor\nThe Language Technologies Unit from Barcelona Supercomputing Center.\nContact\nFor further information, please send an email to langtech@bsc.es.\nCopyright\nCopyright(c) 2025 by Language Technologies Unit, Barcelona Supercomputing Center.\nFunding\nThis work has been promoted and financed by the Government of Catalonia through the Aina Project.\nThis work is funded by the Ministerio para la Transformaci√≥n Digital y de la Funci√≥n P√∫blica - Funded by EU ‚Äì NextGenerationEU\nwithin the framework of ILENIA Project with reference 2022/TL22/00215337.\nAcknowledgements\nThe success of this project has been made possible thanks to the invaluable contributions of our partners in the ILENIA Project:\nHiTZ, and CiTIUS.\nTheir efforts have been instrumental in advancing our work, and we sincerely appreciate their help and support.\nDisclaimer\nDisclaimer\nBe aware that the model may contain biases or other unintended distortions.\nWhen third parties deploy systems or provide services based on this model, or use the model themselves,\nthey bear the responsibility for mitigating any associated risks and ensuring compliance with applicable regulations,\nincluding those governing the use of Artificial Intelligence.\nThe Barcelona Supercomputing Center, as the owner and creator of the model, shall not be held liable for any outcomes resulting from third-party use.\nLicense\nApache License, Version 2.0\nCitation\nIf you find our model useful, we would appreciate if you could cite our work as follows:\n@misc{gilabert2025salamandrasalamandratabscsubmission,\ntitle={From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task},\nauthor={Javier Garcia Gilabert and Xixian Liao and Severino Da Dalt and Ella Bohman and Audrey Mash and Francesca De Luca Fornaciari and Irene Baucells and Joan Llop and Miguel Claramunt Argote and Carlos Escolano and Maite Melero},\nyear={2025},\neprint={2508.12774},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.12774},\n}",
    "Goodfire/Llama-3.3-70B-Instruct-SAE-l50": "Model Information\nFeature labels\nIntended Use\nFeature labels\nHow to use\nTraining\nResponsibility & Safety\nModel Information\nThe Goodfire SAE¬†(Sparse Autoencoder) for meta-llama/Llama-3.3-70B-Instruct\nis an interpreter¬†model designed to analyze¬†and understand\nthe model's internal¬†representations. This¬†SAE model is trained specifically on layer¬†50 of\nLlama¬†3.3 70B and achieves an¬†L0 count¬†of 121, enabling¬†the decomposition of complex neural¬†activations\ninto interpretable features. The¬†model is optimized for¬†interpretability tasks and model steering applications,\nallowing¬†researchers and developers to gain¬†insights into the model's internal processing and behavior patterns.\nAs an open-source¬†tool, it¬†serves as a foundation¬†for advancing interpretability research and enhancing control\nover large¬†language model operations.\nModel Creator: Goodfire, built to work with Meta's Llama models\nBy using Goodfire/Llama-3.3-70B-Instruct-SAE-l50 you agree to the LLAMA 3.3 COMMUNITY LICENSE AGREEMENT\nIntended Use\nBy open-sourcing SAEs for leading open models, especially large-scale\nmodels like Llama 3.3 70B, we aim to accelerate progress in interpretability research.\nOur initial work with these SAEs has revealed promising applications in model steering,\nenhancing jailbreaking safeguards, and interpretable classification methods.\nWe look forward to seeing how the research community builds upon these\nfoundations and uncovers new applications.\nFeature labels\nTo explore the feature labels check out the Goodfire Ember SDK,\nthe first hosted mechanistic interpretability API.\nThe SDK provides an intuitive interface for interacting with these\nfeatures, allowing you to investigate how Llama processes information\nand even steer its behavior. You can explore the SDK documentation at docs.goodfire.ai.\nHow to use\nView the notebook guide below to get started.\nTraining\nWe trained our SAE on activations harvested from Llama-3.3-70B-Instruct on the LMSYS-Chat-1M dataset.\nResponsibility & Safety\nSafety is at the core of everything we do at Goodfire. As a public benefit\ncorporation, we‚Äôre dedicated to understanding AI models to enable safer, more reliable\ngenerative AI. You can read more about our comprehensive approach to\nsafety and responsible development in our detailed safety overview.\nToxic features were removed prior to the release of this SAE. If you are a safety researcher that\nwould like access to the features we‚Äôve removed, you can reach out at contact@goodfire.ai for access.",
    "MiniMaxAI/MiniMax-Text-01": "MiniMax-Text-01\n1. Introduction\n2. Model Architecture\n3. Evaluation\nCore Academic Benchmarks\nLong Benchmarks\n4. Quickstart\n5. Deployment Guide\n6. Function Calling\n7. Citation\n8. Chatbot & API\n9. Contact Us\nWeChat\nMiniMax-Text-01\n1. Introduction\nMiniMax-Text-01 is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax-Text-01 adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methods‚Äîsuch as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax-Text-01's training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax-Text-01 also demonstrates the performance of a top-tier model.\n2. Model Architecture\nThe architecture of MiniMax-Text-01 is briefly described as follows:\nTotal Parameters: 456B\nActivated Parameters per Token: 45.9B\nNumber Layers: 80\nHybrid Attention: a softmax attention is positioned after every 7 lightning attention.\nNumber of attention heads: 64\nAttention head dimension: 128\nMixture of Experts:\nNumber of experts: 32\nExpert hidden dimension: 9216\nTop-2 routing strategy\nPositional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000\nHidden Size: 6144\nVocab Size: 200,064\n3. Evaluation\nCore Academic Benchmarks\nTasks\nGPT-4o (11-20)\nClaude-3.5-Sonnet (10-22)\nGemini-1.5-Pro (002)\nGemini-2.0-Flash (exp)\nQwen2.5-72B-Inst.\nDeepSeek-V3\nLlama-3.1-405B-Inst.\nMiniMax-Text-01\nGeneral\nMMLU*\n85.7\n88.3\n86.8\n86.5\n86.1\n88.5\n88.6\n88.5\nMMLU-Pro*\n74.4\n78.0\n75.8\n76.4\n71.1\n75.9\n73.3\n75.7\nSimpleQA\n39.0\n28.1\n23.4\n26.6\n10.3\n24.9\n23.2\n23.7\nC-SimpleQA\n64.6\n56.8\n59.4\n63.3\n52.2\n64.8\n54.7\n67.4\nIFEval (avg)\n84.1\n90.1\n89.4\n88.4\n87.2\n87.3\n86.4\n89.1\nArena-Hard\n92.4\n87.6\n85.3\n72.7\n81.2\n91.4\n63.5\n89.1\nReasoning\nGPQA* (diamond)\n46.0\n65.0\n59.1\n62.1\n49.0\n59.1\n50.7\n54.4\nDROP* (F1)\n89.2\n88.8\n89.2\n89.3\n85.0\n91.0\n92.5\n87.8\nMathematics\nGSM8k*\n95.6\n96.9\n95.2\n95.4\n95.8\n96.7\n96.7\n94.8\nMATH*\n76.6\n74.1\n84.6\n83.9\n81.8\n84.6\n73.8\n77.4\nCoding\nMBPP +\n76.2\n75.1\n75.4\n75.9\n77.0\n78.8\n73.0\n71.7\nHumanEval\n90.2\n93.7\n86.6\n89.6\n86.6\n92.1\n89.0\n86.9\n* Evaluated following a 0-shot CoT setting.\nLong Benchmarks\n4M Needle In A Haystack Test\nRuler\nModel\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\n1M\nGPT-4o (11-20)\n0.970\n0.921\n0.890\n0.888\n0.884\n-\n-\n-\n-\nClaude-3.5-Sonnet (10-22)\n0.965\n0.960\n0.957\n0.950\n0.952\n0.938\n-\n-\n-\nGemini-1.5-Pro (002)\n0.962\n0.960\n0.960\n0.958\n0.938\n0.917\n0.916\n0.861\n0.850\nGemini-2.0-Flash (exp)\n0.960\n0.960\n0.951\n0.957\n0.937\n0.860\n0.797\n0.709\n-\nMiniMax-Text-01\n0.963\n0.961\n0.953\n0.954\n0.943\n0.947\n0.945\n0.928\n0.910\nLongBench v2\nModel\noverall\neasy\nhard\nshort\nmedium\nlong\nHuman\n53.7\n100.0\n25.1\n47.2\n59.1\n53.7\nw/ CoT\nGPT-4o (11-20)\n51.4\n54.2\n49.7\n59.6\n48.6\n43.5\nClaude-3.5-Sonnet (10-22)\n46.7\n55.2\n41.5\n53.9\n41.9\n44.4\nDeepseek-V3\n-\n-\n-\n-\n-\n-\nQwen2.5-72B-Inst.\n43.5\n47.9\n40.8\n48.9\n40.9\n39.8\nMiniMax-Text-01\n56.5\n66.1\n50.5\n61.7\n56.7\n47.2\nw/o CoT\nGPT-4o (11-20)\n50.1\n57.4\n45.6\n53.3\n52.4\n40.2\nClaude-3.5-Sonnet (10-22)\n41.0\n46.9\n37.3\n46.1\n38.6\n37.0\nDeepseek-V3\n48.7\n-\n-\n-\n-\n-\nQwen2.5-72B-Inst.\n42.1\n42.7\n41.8\n45.6\n38.1\n44.4\nMiniMax-Text-01\n52.9\n60.9\n47.9\n58.9\n52.6\n43.5\nMTOB\nContext Type\nno context\nhalf book\nfull book\nŒî half book\nŒî full book\neng ‚Üí kalam (ChrF)\nGPT-4o (11-20)\n9.90\n54.30\n-\n44.40\n-\nClaude-3.5-Sonnet (10-22)\n20.22\n53.62\n55.65\n33.39\n35.42\nGemini-1.5-Pro (002)\n16.79\n53.68\n57.90\n36.89\n41.11\nGemini-2.0-Flash (exp)\n12.20\n49.50\n53.30\n37.30\n41.10\nQwen-Long\n16.55\n48.48\n45.94\n31.92\n29.39\nMiniMax-Text-01\n6.0\n51.74\n51.60\n45.7\n45.6\nkalam ‚Üí eng (BLEURT)\nGPT-4o (11-20)\n33.20\n58.30\n-\n25.10\n-\nClaude-3.5-Sonnet (10-22)\n31.42\n59.70\n62.30\n28.28\n30.88\nGemini-1.5-Pro (002)\n32.02\n61.52\n63.09\n29.50\n31.07\nGemini-2.0-Flash (exp)\n33.80\n57.50\n57.00\n23.70\n23.20\nQwen-Long\n30.13\n53.14\n32.15\n23.01\n2.02\nMiniMax-Text-01\n33.65\n57.10\n58.00\n23.45\n24.35\n4. Quickstart\nHere we provide a simple example of loading the tokenizer and model to generate content.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, QuantoConfig, GenerationConfig\n# load hf config\nhf_config = AutoConfig.from_pretrained(\"MiniMaxAI/MiniMax-Text-01\", trust_remote_code=True)\n# quantization config, int8 is recommended\nquantization_config =  QuantoConfig(\nweights=\"int8\",\nmodules_to_not_convert=[\n\"lm_head\",\n\"embed_tokens\",\n] + [f\"model.layers.{i}.coefficient\" for i in range(hf_config.num_hidden_layers)]\n+ [f\"model.layers.{i}.block_sparse_moe.gate\" for i in range(hf_config.num_hidden_layers)]\n)\n# assume 8 GPUs\nworld_size = 8\nlayers_per_device = hf_config.num_hidden_layers // world_size\n# set device map\ndevice_map = {\n'model.embed_tokens': 'cuda:0',\n'model.norm': f'cuda:{world_size - 1}',\n'lm_head': f'cuda:{world_size - 1}'\n}\nfor i in range(world_size):\nfor j in range(layers_per_device):\ndevice_map[f'model.layers.{i * layers_per_device + j}'] = f'cuda:{i}'\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"MiniMaxAI/MiniMax-Text-01\")\nprompt = \"Hello!\"\nmessages = [\n{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant created by MiniMax based on MiniMax-Text-01 model.\"}]},\n{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]},\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\n# tokenize and move to device\nmodel_inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n# load bfloat16 model, move to device, and apply quantization\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\"MiniMaxAI/MiniMax-Text-01\",\ntorch_dtype=\"bfloat16\",\ndevice_map=device_map,\nquantization_config=quantization_config,\ntrust_remote_code=True,\noffload_buffers=True,\n)\n# generate response\ngeneration_config = GenerationConfig(\nmax_new_tokens=20,\neos_token_id=200020,\nuse_cache=True,\n)\ngenerated_ids = quantized_model.generate(**model_inputs, generation_config=generation_config)\nprint(f\"generated_ids: {generated_ids}\")\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n5. Deployment Guide\nFor production deployment, we recommend using vLLM to serve MiniMax-Text-01. vLLM provides excellent performance for serving large language models with the following features:\nüî• Outstanding service throughput performance‚ö° Efficient and intelligent memory managementüì¶ Powerful batch request processing capability‚öôÔ∏è Deeply optimized underlying performance\nFor detailed deployment instructions, please refer to our vLLM Deployment Guide.\n6. Function Calling\nMiniMax-Text-01 supports Function Calling capability, enabling the model to intelligently identify when external functions need to be called and output parameters in structured JSON format. With Function Calling, you can:\nLet the model recognize implicit function call needs in user requests\nReceive structured parameter outputs for seamless application integration\nSupport various complex parameter types, including nested objects and arrays\nFunction Calling supports standard OpenAI-compatible format definitions and integrates seamlessly with the Transformers library. For detailed usage instructions, please refer to our Function Call Guide or Chinese Guide.\n7. Citation\n@misc{minimax2025minimax01scalingfoundationmodels,\ntitle={MiniMax-01: Scaling Foundation Models with Lightning Attention},\nauthor={MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},\nyear={2025},\neprint={2501.08313},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.08313},\n}\n8. Chatbot & API\nFor general use and evaluation, we provide a Chatbot with online search capabilities and the online API for developers. For general use and evaluation, we provide the MiniMax MCP Server with video generation, image generation, speech synthesis, and voice cloning for developers.\n9. Contact Us\nContact us at model@minimax.io.",
    "HuggingFaceTB/SmolVLM-500M-Instruct": "SmolVLM-500M\nModel Summary\nResources\nUses\nEvaluation\nTechnical Summary\nHow to get started\nModel optimizations\nMisuse and Out-of-scope Use\nLicense\nTraining Details\nTraining Data\nCitation information\nSmolVLM-500M\nSmolVLM-500M is a tiny multimodal model, member of the SmolVLM family. It accepts arbitrary sequences of image and text inputs to produce text outputs. It's designed for efficiency. SmolVLM can answer questions about images, describe visual content, or transcribe text. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks. It can run inference on one image with 1.23GB of GPU RAM.\nModel Summary\nDeveloped by: Hugging Face ü§ó\nModel type: Multi-modal model (image+text)\nLanguage(s) (NLP): English\nLicense: Apache 2.0\nArchitecture: Based on Idefics3 (see technical summary)\nResources\nDemo: SmolVLM-256 Demo\nBlog: Blog post\nUses\nSmolVLM can be used for inference on multimodal (image + text) tasks where the input comprises text queries along with one or more images. Text and images can be interleaved arbitrarily, enabling tasks like image captioning, visual question answering, and storytelling based on visual content. The model does not support image generation.\nTo fine-tune SmolVLM on a specific task, you can follow the fine-tuning tutorial.\nEvaluation\nTechnical Summary\nSmolVLM leverages the lightweight SmolLM2 language model to provide a compact yet powerful multimodal experience. It introduces several changes compared to the larger SmolVLM 2.2B model:\nImage compression: We introduce a more radical image compression compared to Idefics3 and SmolVLM-2.2B to enable the model to infer faster and use less RAM.\nVisual Token Encoding: SmolVLM-256 uses 64 visual tokens to encode image patches of size 512√ó512. Larger images are divided into patches, each encoded separately, enhancing efficiency without compromising performance.\nNew special tokens: We added new special tokens to divide the subimages. This allows for more efficient tokenization of the images.\nSmoller vision encoder: We went from a 400M parameter siglip vision encoder to a much smaller 93M encoder.\nLarger image patches: We are now passing patches of 512x512 to the vision encoder, instead of 384x384 like the larger SmolVLM. This allows the information to be encoded more efficiently.\nMore details about the training and architecture are available in our technical report.\nHow to get started\nYou can use transformers to load, infer and fine-tune SmolVLM.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load images\nimage = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-500M-Instruct\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-500M-Instruct\",\ntorch_dtype=torch.bfloat16,\n_attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n).to(DEVICE)\n# Create input messages\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Can you describe this image?\"}\n]\n},\n]\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=500)\ngenerated_texts = processor.batch_decode(\ngenerated_ids,\nskip_special_tokens=True,\n)\nprint(generated_texts[0])\n\"\"\"\nAssistant: The image depicts a cityscape featuring a prominent landmark, the Statue of Liberty, prominently positioned on Liberty Island. The statue is a green, humanoid figure with a crown atop its head and is situated on a small island surrounded by water. The statue is characterized by its large, detailed structure, with a statue of a woman holding a torch above her head and a tablet in her left hand. The statue is surrounded by a small, rocky island, which is partially visible in the foreground.\nIn the background, the cityscape is dominated by numerous high-rise buildings, which are densely packed and vary in height. The buildings are primarily made of glass and steel, reflecting the sunlight and creating a bright, urban skyline. The skyline is filled with various architectural styles, including modern skyscrapers and older, more traditional buildings.\nThe water surrounding the island is calm, with a few small boats visible, indicating that the area is likely a popular tourist destination. The water is a deep blue, suggesting that it is a large body of water, possibly a river or a large lake.\nIn the foreground, there is a small strip of land with trees and grass, which adds a touch of natural beauty to the urban landscape. The trees are green, indicating that it is likely spring or summer.\nThe image captures a moment of tranquility and reflection, as the statue and the cityscape come together to create a harmonious and picturesque scene. The statue's presence in the foreground draws attention to the city's grandeur, while the calm water and natural elements in the background provide a sense of peace and serenity.\nIn summary, the image showcases the Statue of Liberty, a symbol of freedom and democracy, set against a backdrop of a bustling cityscape. The statue is a prominent and iconic representation of human achievement, while the cityscape is a testament to human ingenuity and progress. The image captures the beauty and complexity of urban life, with the statue serving as a symbol of hope and freedom, while the cityscape provides a glimpse into the modern world.\n\"\"\"\nModel optimizations\nPrecision: For better performance, load and run the model in half-precision (torch.bfloat16) if your hardware supports it.\nfrom transformers import AutoModelForVision2Seq\nimport torch\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\ntorch_dtype=torch.bfloat16\n).to(\"cuda\")\nYou can also load SmolVLM with 4/8-bit quantization using bitsandbytes, torchao or Quanto. Refer to this page for other options.\nfrom transformers import AutoModelForVision2Seq, BitsAndBytesConfig\nimport torch\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\nquantization_config=quantization_config,\n)\nVision Encoder Efficiency: Adjust the image resolution by setting size={\"longest_edge\": N*512} when initializing the processor, where N is your desired value. The default N=4 works well, which results in input images of\nsize 2048√ó2048. Decreasing N can save GPU memory and is appropriate for lower-resolution images. This is also useful if you want to fine-tune on videos.\nMisuse and Out-of-scope Use\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual's well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\nProhibited Uses:\nEvaluating or scoring individuals (e.g., in employment, education, credit)\nCritical automated decision-making\nGenerating unreliable factual content\nMalicious Activities:\nSpam generation\nDisinformation campaigns\nHarassment or abuse\nUnauthorized surveillance\nLicense\nSmolVLM is built upon SigLIP as image encoder and SmolLM2 for text decoder part.\nWe release the SmolVLM checkpoints under the Apache 2.0 license.\nTraining Details\nTraining Data\nThe training data comes from The Cauldron and Docmatix datasets, with emphasis on document understanding (25%) and image captioning (18%), while maintaining balanced coverage across other crucial capabilities like visual reasoning, chart comprehension, and general instruction following.\nCitation information\nYou can cite us in the following way:\n@article{marafioti2025smolvlm,\ntitle={SmolVLM: Redefining small and efficient multimodal models},\nauthor={Andr√©s Marafioti and Orr Zohar and Miquel Farr√© and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\njournal={arXiv preprint arXiv:2504.05299},\nyear={2025}\n}",
    "unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF": "Finetune LLMs 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nSpecial Thanks\nDeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nSee our collection for versions of Deepseek-R1 including GGUF and original formats.\nInstructions to run this model in llama.cpp:\nOr you can view more detailed instructions here: unsloth.ai/blog/deepseek-r1\nDo not forget about <ÔΩúUserÔΩú> and <ÔΩúAssistantÔΩú> tokens! - Or use a chat template formatter\nObtain the latest llama.cpp at https://github.com/ggerganov/llama.cpp\nExample with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mode\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf \\\n--cache-type-k q8_0 \\\n--threads 16 \\\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>' \\\n-no-cnv\nExample output:\n<think>\nOkay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\nLet me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\nWait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\nI also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\nIs there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\nIf you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n./llama.cpp/llama-cli \\\n--model unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf\n--cache-type-k q8_0\n--threads 16\n--prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>'\n--n-gpu-layers 20 \\\n-no-cnv\nFinetune LLMs 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "lodestones/chroma-debug-development-only": "all model listed in this repo it's purely for research purpose\nonce it's ready it will be uploaded to a separate repo under apache 2.0 license",
    "mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/nicoboss/DeepSeek-R1-Distill-Qwen-14B-Uncensored\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n5.9\nGGUF\nQ3_K_S\n6.8\nGGUF\nQ3_K_M\n7.4\nlower quality\nGGUF\nQ3_K_L\n8.0\nGGUF\nIQ4_XS\n8.3\nGGUF\nQ4_K_S\n8.7\nfast, recommended\nGGUF\nQ4_K_M\n9.1\nfast, recommended\nGGUF\nQ5_K_S\n10.4\nGGUF\nQ5_K_M\n10.6\nGGUF\nQ6_K\n12.2\nvery good quality\nGGUF\nQ8_0\n15.8\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "stanfordmimi/MedVAE": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders\nModel Description\nInstallation\nUsage\nCitation\nMedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders\nThe model was presented in the paper MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders.\nAbstract: Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain.\nModel Description\nMedVAE is a family of six large-scale, generalizable 2D and 3D variational autoencoders (VAEs) designed for medical imaging. It is trained on over one million medical images across multiple anatomical regions and modalities. MedVAE autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features.\nTotal Compression Factor\nChannels\nDimensions\nModalities\nAnatomies\nConfig File\nModel File\n16\n1\n2D\nX-ray\nChest, Breast (FFDM)\nmedvae_4x1.yaml\nvae_4x_1c_2D.ckpt\n16\n3\n2D\nX-ray\nChest, Breast (FFDM)\nmedvae_4x3.yaml\nvae_4x_3c_2D.ckpt\n64\n1\n2D\nX-ray\nChest, Breast (FFDM)\nmedvae_8x1.yaml\nvae_8x_1c_2D.ckpt\n64\n3\n2D\nX-ray\nChest, Breast (FFDM)\nmedvae_8x4.yaml\nvae_8x_4c_2D.ckpt\n64\n1\n3D\nMRI, CT\nWhole-Body\nmedvae_4x1.yaml\nvae_4x_1c_3D.ckpt\n512\n1\n3D\nMRI, CT\nWhole-Body\nmedvae_8x1.yaml\nvae_8x_1c_3D.ckpt\nNote: Model weights and checkpoints are located in the model_weights folder.\nInstallation\nTo install MedVAE, you can simply run:\npip install medvae\nFor an editable installation, use the following commands to clone and install this repository:\ngit clone https://github.com/StanfordMIMI/MedVAE.git\ncd MedVAE\npip install -e .[dev]\nUsage\nA simple example using the medvae library for inference:\nimport torch\nfrom medvae import MVAE\nfpath = \"documentation/data/mmg_data/isJV8hQ2hhJsvEP5rdQNiy.png\"  # Replace with your image path\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MVAE(model_name=\"medvae_4_3_2d\", modality=\"xray\").to(device)\nimg = model.apply_transform(fpath).to(device)\nmodel.requires_grad_(False)\nmodel.eval()\nwith torch.no_grad():\nlatent = model(img)\nWe also provide an easy-to-use CLI inference tool:\nmedvae_inference -i INPUT_FOLDER -o OUTPUT_FOLDER -model_name MED_VAE_MODEL -modality MODALITY\nFor more detailed instructions, refer to the Github repository.\nCitation\nIf you use MedVAE, please cite the original paper:\n@misc{varma2025medvaeefficientautomatedinterpretation,\ntitle={MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders},\nauthor={Maya Varma and Ashwin Kumar and Rogier van der Sluijs and Sophie Ostmeier and Louis Blankemeier and Pierre Chambon and Christian Bluethgen and Jip Prince and Curtis Langlotz and Akshay Chaudhari},\nyear={2025},\neprint={2502.14753},\narchivePrefix={arXiv},\nprimaryClass={eess.IV},\nurl={https://arxiv.org/abs/2502.14753},\n}\nFor questions, please open an issue on the Github repository.",
    "Banafo/Kroko-ASR": "Welcome to Kroko üëã\nOpen-source speech recognition built for developers.\nWhy Kroko ASR?\nDemos\n‚ñ∂Ô∏è Android App\nüåê Browser (WASM)\nModels\n1. Community Models (free, open-source)\n2. Commercial & OEM Models\n3. Bring, Train, or Commission Your Own\nOur Community\nContributing\nLicense\nCredits\nWelcome to Kroko üëã\nOpen-source speech recognition built for developers.\nOur engine is fully open-source, and you choose how to deploy models: use our CC-BY-SA licensed community models or upgrade to commercial models with premium performance. We focus on building fast, high-quality production models and providing examples that take the guesswork out of integration.\nWhy Kroko ASR?\n‚ö° Fast & lightweight ‚Äì optimized Zipformer models (Whisper and parakeet style coming).\nüß© Flexible licensing ‚Äì use fully open-source CC-BY-SA community models or integrate commercial/OEM models for premium accuracy.\nüåç Runs anywhere ‚Äì cross-platform and with support for many programming languages.\nüì± Mobile & web ready ‚Äì works on Android, (iOS coming soon) in the browser via WASM, and with WebSockets for streaming.\nüß∞ Production focus ‚Äì we prioritize real-world performance, stability, and examples.\nü§ù Customizable ‚Äì bring your own model, fine-tune for domain-specific vocabularies, or commission us.\nOur mission: fast, high-quality ASR with licensing that works for both open-source and closed-source projects.\nDemos\n‚ñ∂Ô∏è Android App\nRun speech recognition natively on your phone using ONNX Runtime.\nKroko ASR Model Explorer\nüåê Browser (WASM)\nExperience transcription directly in your browser, no server required.\nHugging Face Spaces Demo\nModels\nKroko ASR follows a unique dual-model strategy:\n1. Community Models (free, open-source)\nLicensed under CC-BY-SA.\nLow-latency, lightweight models.\nPerfect for hobby projects, research, or free tiers.\nFaster and smaller than Whisper/Parakeet in many scenarios.\n2. Commercial & OEM Models\nPremium accuracy and robustness.\nLicensed for professional and production products.\nDesigned for SaaS, dev tools, and enterprise integration.\n3. Bring, Train, or Commission Your Own\nDIY: Use our training guides to build and distribute your own models.\nProfessional services: Work with us to create fine-tuned models for accents, jargon, or specialized domains.\nThis gives you full freedom: start free, scale commercially, or roll your own.\nOur Community\nJoin the Kroko community to learn, share, and contribute:\nüí¨ Discord ‚Äì chat with developers, ask questions, and share projects.\nüì¢ Reddit ‚Äì join discussions, showcase your integrations, and follow updates.\nü§ó Hugging Face ‚Äì explore our models, try live demos, and contribute feedback.\nContributing\nPRs welcome! Run ruff, black, and pytest before submitting.\nLicense\nApache-2.0 engine. Models licensed separately (CC-BY-SA community or commercial OEM).\nCredits\nKroko ASR is built on top of Sherpa-ONNX.\n‚ö†Ô∏è Note: Kroko ASR is an independent project and is not affiliated with Sherpa-ONNX. We build on their excellent open-source engine, but our models, demos, and packaging are developed and maintained separately.",
    "WiroAI/WiroAI-Finance-Qwen-1.5B": "WiroAI-Finance-Qwen-1.5B\nüöÄ Meet with WiroAI/WiroAI-Finance-Qwen-1.5B! A robust language model with more finance knowledge support! üöÄ\nüåü Key Features\nUsage\nTransformers Pipeline\nü§ù License and Usage\nüì´ Contact and Support\nCitation\nWiroAI-Finance-Qwen-1.5B\nüöÄ Meet with WiroAI/WiroAI-Finance-Qwen-1.5B! A robust language model with more finance knowledge support! üöÄ\nüåü Key Features\nFine-tuned with 500,000+ high-quality finance instructions. (Josephgflowers/Finance-Instruct-500k)\nLoRA method was used for fine-tuning without quantization.\nAdapted to finance expertise.\nBuilt on Qwen's architecture\nüìù Model Details\nThe model is the finance data fine-tuned version of Qwen model family. This model has been trained using Supervised Fine-Tuning (SFT) on carefully curated high-quality finance instructions. Please note that training data includes English and Chinese instructions, and this model rarely mix the two languages.\nUsage\nTransformers Pipeline\nimport transformers\nimport torch\nmodel_id = \"WiroAI/WiroAI-Finance-Qwen-1.5B\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\npipeline.model.eval()\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a finance chatbot developed by Wiro AI\"},\n{\"role\": \"user\", \"content\": \"How can central banks balance the trade-off between controlling inflation and maintaining economic growth, especially in an environment of high public debt and geopolitical uncertainty?\"\n},\n]\nterminators = [\npipeline.tokenizer.eos_token_id,\npipeline.tokenizer.convert_tokens_to_ids(\"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\")\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=512,\neos_token_id=terminators,\ndo_sample=True,\ntemperature=0.9,\n)\nprint(outputs[0][\"generated_text\"][-1]['content'])\nCentral banks aim to strike a balance between controlling inflation and maintaining economic growth amidst high public debt and geopolitical uncertainties. They employ various policy tools and measures to achieve this objective.\nFirst, central banks employ monetary policy to influence aggregate demand and price levels. The primary policy tools include:\n1. Interest rates: Central banks adjust interest rates to manage the cost of borrowing and reduce inflation. Lower interest rates stimulate demand, while higher rates can curtail economic activity.\n2. Open market operations: Central banks buy or sell government securities to influence the money supply and drive economic growth.\n3. Government spending and taxation: Central banks influence fiscal policy to adjust government spending and taxation, which directly impacts economic activity and inflation.\nSecond, central banks utilize financial tools to stabilize the financial system and mitigate risks associated with high public debt and geopolitical uncertainties. Key measures include:\n1. Capital controls: Central banks set limits on the amount of capital available to financial institutions, which helps manage risk and prevent market bubbles.\n2. Financial regulations: Central banks introduce regulations to prevent excessive credit, debt, and fraud, which can contribute to inflation and systemic financial instability.\n3. Financial reform: Central banks work towards improving the regulatory framework and infrastructure to enhance financial stability and support economic growth.\nFurthermore, central banks consider the impact of geopolitical uncertainties on their operations. They implement measures to strengthen international relations, promote dialogue, and facilitate cooperation among countries to address global challenges collectively.\nIn conclusion, central banks balance the trade-off between controlling inflation and maintaining economic growth by employing a combination of monetary and financial policies, capital controls, regulatory reforms, and international cooperation. These measures help them navigate the complexities of high public debt, political instability, and economic uncertainty while pursuing their core objectives of economic stability and growth.\nü§ù License and Usage\nThis model is provided under apache 2.0 license. Please review the license terms before use.\nüì´ Contact and Support\nFor questions, suggestions, and feedback, please open an issue on HuggingFace or contact us directly from our website.\nCitation\n@article{WiroAI,\ntitle={WiroAI/WiroAI-Finance-Qwen-1.5B},\nauthor={Abdullah Bezir, Furkan Burhan T√ºrkay, Cengiz Asmazoƒülu},\nyear={2025},\nurl={https://huggingface.co/WiroAI/WiroAI-Finance-Qwen-1.5B}\n}",
    "nomic-ai/nomic-embed-text-v2-moe": "nomic-embed-text-v2-moe: Multilingual Mixture of Experts Text Embeddings\nModel Overview\nModel Architecture\nPaper Abstract\nUsage Guide\nInstallation\nTransformers\nSentenceTransformers\nPerformance\nBest Practices\nLimitations\nTraining Details\nJoin the Nomic Community\nCitation\nnomic-embed-text-v2-moe: Multilingual Mixture of Experts Text Embeddings\nBlog | Technical Report | AWS SageMaker | Atlas Embedding and Unstructured Data Analytics Platform\nThis model was presented in the paper Training Sparse Mixture Of Experts Text Embedding Models.\nModel Overview\nnomic-embed-text-v2-moe is a SoTA multilingual MoE text embedding model that excels at multilingual retrieval:\nHigh Performance: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\nMultilinguality: Supports ~100 languages and trained on over 1.6B pairs\nFlexible Embedding Dimension: Trained with Matryoshka Embeddings with 3x reductions in storage cost with minimal performance degradations\nFully Open-Source: Model weights, code, and training data (see code repo) released\nModel\nParams (M)\nEmb Dim\nBEIR\nMIRACL\nPretrain Data\nFinetune Data\nCode\nNomic Embed v2\n305\n768\n52.86\n65.80\n‚úÖ\n‚úÖ\n‚úÖ\nmE5 Base\n278\n768\n48.88\n62.30\n‚ùå\n‚ùå\n‚ùå\nmGTE Base\n305\n768\n51.10\n63.40\n‚ùå\n‚ùå\n‚ùå\nArctic Embed v2 Base\n305\n768\n55.40\n59.90\n‚ùå\n‚ùå\n‚ùå\nBGE M3\n568\n1024\n48.80\n69.20\n‚ùå\n‚úÖ\n‚ùå\nArctic Embed v2 Large\n568\n1024\n55.65\n66.00\n‚ùå\n‚ùå\n‚ùå\nmE5 Large\n560\n1024\n51.40\n66.50\n‚ùå\n‚ùå\n‚ùå\nModel Architecture\nTotal Parameters: 475M\nActive Parameters During Inference: 305M\nArchitecture Type: Mixture of Experts (MoE)\nMoE Configuration: 8 experts with top-2 routing\nEmbedding Dimensions: Supports flexible dimension from 768 to 256 through Matryoshka representation learning\nMaximum Sequence Length: 512 tokens\nLanguages: Supports dozens of languages (see Performance section)\nPaper Abstract\nTransformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline at https://github.com/nomic-ai/contrastors.\nUsage Guide\nInstallation\nThe model can be used through SentenceTransformers and Transformers.\nFor best performance on GPU, please install\npip install torch transformers einops git+https://github.com/nomic-ai/megablocks.git\nImportant!\nThe text prompt must include a task instruction prefix, instructing the model which task is being performed.\nPlease use search_query:  before your queries/questions, and search_document:  before your documents.\nTransformers\nIf using Transformers, make sure to prepend the task instruction prefix.\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v2-moe\")\nmodel = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\nsentences = ['search_document: Hello!', 'search_document: ¬°Hola!']\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\nmodel.eval()\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\nembeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings.shape)\n# torch.Size([2, 768])\nsimilarity = F.cosine_similarity(embeddings[0], embeddings[1], dim=0)\nprint(similarity)\n# tensor(0.9118)\nFor truncation, you can trucate before applying normalization\n+ embeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nSentenceTransformers\nWith SentenceTransformers, you can specify the prompt_name as either \"query\" or \"passage\", and the task instruction will be included automatically.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\nsentences = [\"Hello!\", \"¬°Hola!\"]\nembeddings = model.encode(sentences, prompt_name=\"passage\")\nprint(embeddings.shape)\n# (2, 768)\nsimilarity = model.similarity(embeddings[0], embeddings[1])\nprint(similarity)\n# tensor([[0.9118]])\nFor truncation/Matryoshka embeddings, you can specify truncate_dim and use the model similarly\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True, truncate_dim=256)\n...\nPerformance\nnomic-embed-text-v2-moe performance on BEIR and MIRACL compared to other open-weights embedding models:\nnomic-embed-text-v2-moe performance on BEIR at 768 dimension and truncated to 256 dimensions:\nBest Practices\nAdd appropriate prefixes to your text:\nFor queries: \"search_query: \"\nFor documents: \"search_document: \"\nMaximum input length is 512 tokens\nFor optimal efficiency, consider using the 256-dimension embeddings if storage/compute is a concern\nLimitations\nPerformance may vary across different languages\nResource requirements may be higher than traditional dense models due to MoE architecture\nMust use trust_remote_code=True when loading the model to use our custom architecture implementation\nTraining Details\nTrained on 1.6 billion high-quality pairs across multiple languages\nUses consistency filtering to ensure high-quality training data\nIncorporates Matryoshka representation learning for dimension flexibility\nTraining includes both weakly-supervised contrastive pretraining and supervised finetuning\nFor more details, please check out the blog post and technical report.\nJoin the Nomic Community\nNomic: https://nomic.ai\nDiscord: https://discord.gg/myY5YDR8z8\nTwitter: https://twitter.com/nomic_ai\nCitation\nIf you find the model, dataset, or training code useful, please cite our work\n@misc{nussbaum2025trainingsparsemixtureexperts,\ntitle={Training Sparse Mixture Of Experts Text Embedding Models},\nauthor={Zach Nussbaum and Brandon Duderstadt},\nyear={2025},\neprint={2502.07972},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.07972},\n}",
    "latchkeyChild/deepseek-trading-assistant": "DeepSeek Trading Assistant\nModel Details\nModel Description\nUses\nDirect Use\nTraining Data\nTraining Procedure\nTechnical Specifications\nCompute Infrastructure\nModel Card Contact\nDeepSeek Trading Assistant\nThis is a fine-tuned version of DeepSeek-R1-Distill-Qwen-32B specialized for generating trading strategies and market analysis.\nModel Details\nModel Description\nDeveloped by: latchkeyChild\nModel type: Decoder-only language model\nLanguage(s): English\nLicense: MIT\nFinetuned from model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nUses\nDirect Use\nThis model is designed to:\nAnalyze market conditions using technical indicators\nGenerate trading strategies based on market analysis\nImplement risk management rules\nCreate Python code for strategy implementation\nTraining Data\nThe model is trained on a custom dataset containing:\nMarket analysis using technical indicators (RSI, MACD, Moving Averages)\nTrading strategy implementations\nRisk management rules\nPython code examples using QuantConnect framework\nTraining Procedure\nTraining Hyperparameters\nNumber of epochs: 3\nBatch size: 2\nLearning rate: 1e-5\nGradient accumulation steps: 8\nWarmup steps: 100\nTraining regime: fp16 mixed precision with gradient checkpointing\nTemperature: 0.6 (recommended for DeepSeek-R1 series)\nTechnical Specifications\nCompute Infrastructure\nRequired Hardware: 2x NVIDIA A10G GPUs or 1x A100 GPU\nTraining Time (estimated): 2-4 hours\nModel Card Contact\nFor questions or issues, please open an issue in the repository.",
    "jingyaogong/MiniMind2": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nüìå Introduction\nüëâÊõ¥Êñ∞Êó•Âøó\nÁ¨¨0Ê≠•\nüìå Âø´ÈÄüÂºÄÂßã\n‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú\n1.‰∏ãËΩΩÊ®°Âûã\n2.ÂëΩ‰ª§Ë°åÈóÆÁ≠î\n3.ÊàñÂêØÂä®WebUI\n‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ\n1.ÁéØÂ¢ÉÂáÜÂ§á\n2.Êï∞ÊçÆ‰∏ãËΩΩ\n3.ÂºÄÂßãËÆ≠ÁªÉ\n4.ÊµãËØïÊ®°ÂûãÊïàÊûú\nüìå Êï∞ÊçÆ‰ªãÁªç\n‚Ö† Tokenizer\n‚Ö° PretrainÊï∞ÊçÆ\n‚Ö¢ SFTÊï∞ÊçÆ\n‚Ö£ RLHFÊï∞ÊçÆ\n‚Ö§ ReasonÊï∞ÊçÆÈõÜÔºö\n‚Ö• Êõ¥Â§öÊï∞ÊçÆÈõÜ\n‚Öß Êï∞ÊçÆÈõÜ‰∏ãËΩΩ\nüìå Model Structure\nüìå Experiment\n‚Ö† ËÆ≠ÁªÉÂºÄÈîÄ\n‚Ö° ‰∏ªË¶ÅËÆ≠ÁªÉÊ≠•È™§\n1. È¢ÑËÆ≠ÁªÉ(Pretrain):\n2. ÊúâÁõëÁù£ÂæÆË∞É(Supervised Fine-Tuning):\n‚Ö¢ ÂÖ∂ÂÆÉËÆ≠ÁªÉÊ≠•È™§\n3. ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†(Reinforcement Learning from Human Feedback, RLHF)\n4. Áü•ËØÜËí∏È¶è(Knowledge Distillation, KD)\n5. LoRA (Low-Rank Adaptation)\n6. ËÆ≠ÁªÉÊé®ÁêÜÊ®°Âûã (Reasoning Model)\n‚Ö£ Ê®°ÂûãÂèÇÊï∞ËÆæÂÆö\n‚Ö§ ËÆ≠ÁªÉÁªìÊûú\nËÆ≠ÁªÉÂÆåÊàê-Ê®°ÂûãÂêàÈõÜ\nüìå Eval\n‚Ö† RLHFÂØπÊØîSFTÁØá\n‚Ö° ‰∏ªËßÇÊ†∑‰æãÊµãËØÑ\nËØÑÂàÜÊ†áÂáÜÔºö\nÁÇπËØÑÔºö\nÊÄªÁªìÔºö\nÊâìÂàÜÊéíÂ∫è\nüëâ‰∏ªËßÇÊïàÊûúÊÄªÁªì\n‚Ö¢ Objective Benchmark\nÊé®ÁêÜ‰∏éÂØºÂá∫\nÂü∫‰∫éMiniMind-APIÊúçÂä°Êé•Âè£\nüìå ÂÖ∂ÂÆÉ (Others)\nÊé®ÁêÜ‰∏éÂØºÂá∫\nÂü∫‰∫éMiniMind-APIÊúçÂä°Êé•Âè£\nüìå Acknowledge\nü§ùË¥°ÁåÆËÄÖ\nüòäÈ∏£Ë∞¢\nü´∂ÊîØÊåÅËÄÖ\nLicense\n\"Â§ßÈÅìËá≥ÁÆÄ\"\n‰∏≠Êñá | English\nÊ≠§ÂºÄÊ∫êÈ°πÁõÆÊó®Âú®ÂÆåÂÖ®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®3ÂùóÈí±ÊàêÊú¨ + 2Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫‰ªÖ‰∏∫25.8MÁöÑË∂ÖÂ∞èËØ≠Ë®ÄÊ®°ÂûãMiniMind„ÄÇ\nMiniMindÁ≥ªÂàóÊûÅÂÖ∂ËΩªÈáèÔºåÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØÊòØ GPT-3 ÁöÑ $\\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞ÊúÄÊôÆÈÄöÁöÑ‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüËÆ≠ÁªÉ„ÄÇ\nÈ°πÁõÆÂêåÊó∂ÂºÄÊ∫ê‰∫ÜÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ-ÂåÖÂê´ÊãìÂ±ïÂÖ±‰∫´Ê∑∑Âêà‰∏ìÂÆ∂(MoE)„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)„ÄÅLoRAÂæÆË∞ÉÔºå\nÁõ¥Êé•ÂÅèÂ•ΩÂº∫ÂåñÂ≠¶‰π†(DPO)ÁÆóÊ≥ï„ÄÅÊ®°ÂûãËí∏È¶èÁÆóÊ≥ïÁ≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ\nMiniMindÂêåÊó∂ÊãìÂ±ï‰∫ÜËßÜËßâÂ§öÊ®°ÊÄÅÁöÑVLM: MiniMind-V„ÄÇ\nÈ°πÁõÆÊâÄÊúâÊ†∏ÂøÉÁÆóÊ≥ï‰ª£Á†ÅÂùá‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÈáçÊûÑÔºÅ‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÊèê‰æõÁöÑÊäΩË±°Êé•Âè£„ÄÇ\nËøô‰∏ç‰ªÖÊòØÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Èò∂ÊÆµÂºÄÊ∫êÂ§çÁé∞Ôºå‰πüÊòØ‰∏Ä‰∏™ÂÖ•Èó®LLMÁöÑÊïôÁ®ã„ÄÇ\nÂ∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ\n‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú2Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØïÔºå‚Äú3ÂùóÈí±‚Äù\nÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨ÔºåÂÖ∑‰ΩìËßÑÊ†ºËØ¶ÊÉÖËßÅ‰∏ãÊñá„ÄÇ\nüîóüçìÊé®ÁêÜÊ®°Âûã | üîóü§ñÂ∏∏ËßÑÊ®°Âûã | üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç\nüìå Introduction\nÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language Model, LLMÔºâÁöÑÂá∫Áé∞ÂºïÂèë‰∫ÜÂÖ®‰∏ñÁïåÂØπAIÁöÑÁ©∫ÂâçÂÖ≥Ê≥®„ÄÇ\nÊó†ËÆ∫ÊòØChatGPT„ÄÅDeepSeekËøòÊòØQwenÔºåÈÉΩ‰ª•ÂÖ∂ÊÉäËâ≥ÁöÑÊïàÊûú‰ª§‰∫∫Âèπ‰∏∫ËßÇÊ≠¢„ÄÇ\nÁÑ∂ËÄåÔºåÂä®ËæÑÊï∞Áôæ‰∫øÂèÇÊï∞ÁöÑÂ∫ûÂ§ßËßÑÊ®°Ôºå‰ΩøÂæóÂÆÉ‰ª¨ÂØπ‰∏™‰∫∫ËÆæÂ§áËÄåË®Ä‰∏ç‰ªÖÈöæ‰ª•ËÆ≠ÁªÉÔºåÁîöËá≥ËøûÈÉ®ÁΩ≤ÈÉΩÊòæÂæóÈÅ•‰∏çÂèØÂèä„ÄÇ\nÊâìÂºÄÂ§ßÊ®°ÂûãÁöÑ‚ÄúÈªëÁõíÂ≠ê‚ÄùÔºåÊé¢Á¥¢ÂÖ∂ÂÜÖÈÉ®Ëøê‰ΩúÊú∫Âà∂ÔºåÂ§ö‰πà‰ª§‰∫∫ÂøÉÊΩÆÊæéÊπÉÔºÅ\nÈÅóÊÜæÁöÑÊòØÔºå99%ÁöÑÊé¢Á¥¢Âè™ËÉΩÊ≠¢Ê≠•‰∫é‰ΩøÁî®LoRAÁ≠âÊäÄÊúØÂØπÁé∞ÊúâÂ§ßÊ®°ÂûãËøõË°åÂ∞ëÈáèÂæÆË∞ÉÔºåÂ≠¶‰π†‰∏Ä‰∫õÊñ∞Êåá‰ª§Êàñ‰ªªÂä°„ÄÇ\nËøôÂ∞±Â•ΩÊØîÊïôÁâõÈ°øÂ¶Ç‰Ωï‰ΩøÁî®21‰∏ñÁ∫™ÁöÑÊô∫ËÉΩÊâãÊú∫‚Äî‚ÄîËôΩÁÑ∂ÊúâË∂£ÔºåÂç¥ÂÆåÂÖ®ÂÅèÁ¶ª‰∫ÜÁêÜËß£Áâ©ÁêÜÊú¨Ë¥®ÁöÑÂàùË°∑„ÄÇ\n‰∏éÊ≠§ÂêåÊó∂ÔºåÁ¨¨‰∏âÊñπÁöÑÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÂíåÂ∑•ÂÖ∑Â∫ìÔºåÂ¶Çtransformers+trlÔºåÂá†‰πéÂè™Êö¥Èú≤‰∫ÜÈ´òÂ∫¶ÊäΩË±°ÁöÑÊé•Âè£„ÄÇ\nÈÄöËøáÁü≠Áü≠10Ë°å‰ª£Á†ÅÔºåÂ∞±ËÉΩÂÆåÊàê‚ÄúÂä†ËΩΩÊ®°Âûã+Âä†ËΩΩÊï∞ÊçÆÈõÜ+Êé®ÁêÜ+Âº∫ÂåñÂ≠¶‰π†‚ÄùÁöÑÂÖ®ÊµÅÁ®ãËÆ≠ÁªÉ„ÄÇ\nËøôÁßçÈ´òÊïàÁöÑÂ∞ÅË£ÖÂõ∫ÁÑ∂‰æøÂà©Ôºå‰ΩÜ‰πüÂÉè‰∏ÄÊû∂È´òÈÄüÈ£ûËàπÔºåÂ∞ÜÊàë‰ª¨‰∏éÂ∫ïÂ±ÇÂÆûÁé∞ÈöîÁ¶ªÂºÄÊù•ÔºåÈòªÁ¢ç‰∫ÜÊ∑±ÂÖ•Êé¢Á©∂LLMÊ†∏ÂøÉ‰ª£Á†ÅÁöÑÊú∫‰ºö„ÄÇ\nÁÑ∂ËÄåÔºå‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù„ÄÇ\nÊõ¥Á≥üÁ≥ïÁöÑÊòØÔºå‰∫íËÅîÁΩë‰∏äÂÖÖÊñ•ÁùÄÂ§ßÈáè‰ªòË¥πËØæÁ®ãÂíåËê•ÈîÄÂè∑Ôºå‰ª•ÊºèÊ¥ûÁôæÂá∫„ÄÅ‰∏ÄÁü•ÂçäËß£ÁöÑÂÜÖÂÆπÊé®ÈîÄAIÊïôÁ®ã„ÄÇ\nÊ≠£Âõ†Â¶ÇÊ≠§ÔºåÊú¨È°πÁõÆÂàùË°∑ÊòØÊãâ‰ΩéLLMÁöÑÂ≠¶‰π†Èó®ÊßõÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰ªéÁêÜËß£ÊØè‰∏ÄË°å‰ª£Á†ÅÂºÄÂßãÔºå\n‰ªéÈõ∂ÂºÄÂßã‰∫≤ÊâãËÆ≠ÁªÉ‰∏Ä‰∏™ÊûÅÂ∞èÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊòØÁöÑÔºå‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøõË°åÊé®ÁêÜÔºÅ\nÊúÄ‰ΩéÂè™ÈúÄ3ÂùóÈí±‰∏çÂà∞ÁöÑÊúçÂä°Âô®ÊàêÊú¨ÔºåÂ∞±ËÉΩ‰∫≤Ë∫´‰ΩìÈ™å‰ªé0Âà∞1ÊûÑÂª∫‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®ËøáÁ®ã„ÄÇ\n‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ\nÔºàÊà™Ëá≥2025-02-07ÔºâMiniMindÁ≥ªÂàóÂ∑≤ÂÆåÊàêÂ§ö‰∏™ÂûãÂè∑Ê®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ25.8MÔºà0.02BÔºâÔºåÂç≥ÂèØÂÖ∑Â§áÊµÅÁïÖÂØπËØùËÉΩÂäõÔºÅ\nModels List\nÊ®°Âûã (Â§ßÂ∞è)\nÊé®ÁêÜÂç†Áî® (Á∫¶)\nRelease\nMiniMind2-small (26M)\n0.5 GB\n2025.02.06\nMiniMind2-MoE (145M)\n1.0 GB\n2025.02.06\nMiniMind2 (104M)\n1.0 GB\n2025.02.06\nminimind-v1-small (26M)\n0.5 GB\n2024.08.28\nminimind-v1-moe (4√ó26M)\n1.0 GB\n2024.09.17\nminimind-v1 (108M)\n1.0 GB\n2024.09.01\nÈ°πÁõÆÂåÖÂê´\nMiniMind-LLMÁªìÊûÑÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºàDense+MoEÊ®°ÂûãÔºâ„ÄÇ\nÂåÖÂê´TokenizerÂàÜËØçÂô®ËØ¶ÁªÜËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ\nÂåÖÂê´Pretrain„ÄÅSFT„ÄÅLoRA„ÄÅRLHF-DPO„ÄÅÊ®°ÂûãËí∏È¶èÁöÑÂÖ®ËøáÁ®ãËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ\nÊî∂ÈõÜ„ÄÅËí∏È¶è„ÄÅÊï¥ÁêÜÂπ∂Ê∏ÖÊ¥óÂéªÈáçÊâÄÊúâÈò∂ÊÆµÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ\n‰ªé0ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅLoRA„ÄÅDPOÂº∫ÂåñÂ≠¶‰π†ÔºåÁôΩÁõíÊ®°ÂûãËí∏È¶è„ÄÇÂÖ≥ÈîÆÁÆóÊ≥ïÂá†‰πé‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∞ÅË£ÖÁöÑÊ°ÜÊû∂Ôºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ\nÂêåÊó∂ÂÖºÂÆπtransformers„ÄÅtrl„ÄÅpeftÁ≠âÁ¨¨‰∏âÊñπ‰∏ªÊµÅÊ°ÜÊû∂„ÄÇ\nËÆ≠ÁªÉÊîØÊåÅÂçïÊú∫ÂçïÂç°„ÄÅÂçïÊú∫Â§öÂç°(DDP„ÄÅDeepSpeed)ËÆ≠ÁªÉÔºåÊîØÊåÅwandbÂèØËßÜÂåñËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊîØÊåÅÂä®ÊÄÅÂêØÂÅúËÆ≠ÁªÉ„ÄÇ\nÂú®Á¨¨‰∏âÊñπÊµãËØÑÊ¶úÔºàC-Eval„ÄÅC-MMLU„ÄÅOpenBookQAÁ≠âÔºâËøõË°åÊ®°ÂûãÊµãËØï„ÄÇ\nÂÆûÁé∞Openai-ApiÂçèËÆÆÁöÑÊûÅÁÆÄÊúçÂä°Á´ØÔºå‰æø‰∫éÈõÜÊàêÂà∞Á¨¨‰∏âÊñπChatUI‰ΩøÁî®ÔºàFastGPT„ÄÅOpen-WebUIÁ≠âÔºâ„ÄÇ\nÂü∫‰∫éstreamlitÂÆûÁé∞ÊúÄÁÆÄËÅäÂ§©WebUIÂâçÁ´Ø„ÄÇ\nÂ§çÁé∞(Ëí∏È¶è/RL)Â§ßÂûãÊé®ÁêÜÊ®°ÂûãDeepSeek-R1ÁöÑMiniMind-ReasonÊ®°ÂûãÔºåÊï∞ÊçÆ+Ê®°ÂûãÂÖ®ÈÉ®ÂºÄÊ∫êÔºÅ\nÂ∏åÊúõÊ≠§ÂºÄÊ∫êÈ°πÁõÆÂèØ‰ª•Â∏ÆÂä©LLMÂàùÂ≠¶ËÄÖÂø´ÈÄüÂÖ•Èó®ÔºÅ\nüëâÊõ¥Êñ∞Êó•Âøó\n2025-02-09 (newest üéâüéâüéâ)\nËøéÊù•ÂèëÂ∏É‰ª•Êù•ÈáçÂ§ßÊõ¥Êñ∞ÔºåRelease MiniMind2 Series„ÄÇ\n‰ª£Á†ÅÂá†‰πéÂÖ®ÈÉ®ÈáçÊûÑÔºå‰ΩøÁî®Êõ¥ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÁªü‰∏ÄÁªìÊûÑ„ÄÇ\nÂ¶ÇÊúâÊóß‰ª£Á†ÅÁöÑÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆüîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó„ÄÇ\nÂÖçÂéªÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ≠•È™§„ÄÇÁªü‰∏ÄÊï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊõ¥Êç¢‰∏∫jsonlÊ†ºÂºèÊùúÁªùÊï∞ÊçÆÈõÜ‰∏ãËΩΩÊ∑∑‰π±ÁöÑÈóÆÈ¢ò„ÄÇ\nMiniMind2Á≥ªÂàóÊïàÊûúÁõ∏ÊØîMiniMind-V1ÊòæËëóÊèêÂçá„ÄÇ\nÂ∞èÈóÆÈ¢òÔºö{kv-cacheÂÜôÊ≥ïÊõ¥Ê†áÂáÜ„ÄÅMoEÁöÑË¥üËΩΩÂùáË°°lossË¢´ËÄÉËôëÁ≠âÁ≠â}\nÊèê‰æõÊ®°ÂûãËøÅÁßªÂà∞ÁßÅÊúâÊï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊñπÊ°àÔºàÂåªÁñóÊ®°Âûã„ÄÅËá™ÊàëËÆ§Áü•Ê†∑‰æãÔºâ„ÄÇ\nÁ≤æÁÆÄÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂπ∂Â§ßÂπÖÊèêÂçáÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®ÈáèÔºåÂ§ßÂπÖÁº©Áü≠‰∏™‰∫∫Âø´ÈÄüËÆ≠ÁªÉÊâÄÈúÄÊó∂Èó¥ÔºåÂçïÂç°3090Âç≥ÂèØ2Â∞èÊó∂Â§çÁé∞ÔºÅ\nÊõ¥Êñ∞ÔºöLoRAÂæÆË∞ÉËÑ±Á¶ªpeftÂåÖË£ÖÔºå‰ªé0ÂÆûÁé∞LoRAËøáÁ®ãÔºõDPOÁÆóÊ≥ï‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÂÆûÁé∞ÔºõÊ®°ÂûãÁôΩÁõíËí∏È¶èÂéüÁîüÂÆûÁé∞„ÄÇ\nMiniMind2-DeepSeek-R1Á≥ªÂàóËí∏È¶èÊ®°ÂûãËØûÁîüÔºÅ\nMiniMind2ÂÖ∑Â§á‰∏ÄÂÆöÁöÑËã±ÊñáËÉΩÂäõÔºÅ\nÊõ¥Êñ∞MiniMind2‰∏éÁ¨¨‰∏âÊñπÊ®°ÂûãÁöÑÂü∫‰∫éÊõ¥Â§öÂ§ßÊ®°ÂûãÊ¶úÂçïÊµãËØïÊÄßËÉΩÁöÑÁªìÊûú„ÄÇ\n2024-10-05\n‰∏∫MiniMindÊãìÂ±ï‰∫ÜÂ§öÊ®°ÊÄÅËÉΩÂäõ‰πã---ËßÜËßâ\nÁßªÊ≠•Â≠™ÁîüÈ°πÁõÆminimind-vÊü•ÁúãËØ¶ÊÉÖÔºÅ\n2024-09-27\n09-27Êõ¥Êñ∞pretrainÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜÊñπÂºèÔºå‰∏∫‰∫Ü‰øùËØÅÊñáÊú¨ÂÆåÊï¥ÊÄßÔºåÊîæÂºÉÈ¢ÑÂ§ÑÁêÜÊàê.binËÆ≠ÁªÉÁöÑÂΩ¢ÂºèÔºàËΩªÂæÆÁâ∫Áâ≤ËÆ≠ÁªÉÈÄüÂ∫¶Ôºâ„ÄÇ\nÁõÆÂâçpretrainÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊñá‰ª∂ÂëΩÂêç‰∏∫Ôºöpretrain_data.csv„ÄÇ\nÂà†Èô§‰∫Ü‰∏Ä‰∫õÂÜó‰ΩôÁöÑ‰ª£Á†Å„ÄÇ\n2024-09-17\nÊõ¥Êñ∞minimind-v1-moeÊ®°Âûã\n‰∏∫‰∫ÜÈò≤Ê≠¢Ê≠ß‰πâÔºå‰∏çÂÜç‰ΩøÁî®mistral_tokenizerÂàÜËØçÔºåÂÖ®ÈÉ®ÈááÁî®Ëá™ÂÆö‰πâÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®„ÄÇ\n2024-09-01\nÊõ¥Êñ∞minimind-v1 (108M)Ê®°ÂûãÔºåÈááÁî®minimind_tokenizerÔºåÈ¢ÑËÆ≠ÁªÉËΩÆÊ¨°3 + SFTËΩÆÊ¨°10ÔºåÊõ¥ÂÖÖÂàÜËÆ≠ÁªÉÔºåÊÄßËÉΩÊõ¥Âº∫„ÄÇ\nÈ°πÁõÆÂ∑≤ÈÉ®ÁΩ≤Ëá≥ModelScopeÂàõÁ©∫Èó¥ÔºåÂèØ‰ª•Âú®Ê≠§ÁΩëÁ´ô‰∏ä‰ΩìÈ™åÔºö\nüîóModelScopeÂú®Á∫ø‰ΩìÈ™åüîó\n2024-08-27\nÈ°πÁõÆÈ¶ñÊ¨°ÂºÄÊ∫ê\nüìå Âø´ÈÄüÂºÄÂßã\nÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ\nCPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\nRAM: 128 GB\nGPU: NVIDIA GeForce RTX 3090(24GB) * 8\nUbuntu==20.04\nCUDA==12.2\nPython==3.10.16\nrequirements.txt\nÁ¨¨0Ê≠•\ngit clone https://github.com/jingyaogong/minimind.git\n‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú\n1.‰∏ãËΩΩÊ®°Âûã\n# step 1\ngit clone https://huggingface.co/jingyaogong/MiniMind2\n2.ÂëΩ‰ª§Ë°åÈóÆÁ≠î\n# step 2\n# load=1: load from transformers-hf model\npython eval_model.py --load 1\n3.ÊàñÂêØÂä®WebUI\n# ÂèØËÉΩÈúÄË¶Å`python>=3.10` ÂÆâË£Ö `pip install streamlit`\n# cd scripts\nstreamlit run web_demo.py\n‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ\n1.ÁéØÂ¢ÉÂáÜÂ§á\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\nÊ≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda\nimport torch\nprint(torch.cuda.is_available())\nÂ¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéªtorch_stable\n‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉÈìæÊé•\n2.Êï∞ÊçÆ‰∏ãËΩΩ\n‰ªé‰∏ãÊñáÊèê‰æõÁöÑÊï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•\n‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÊîæÂà∞./datasetÁõÆÂΩï‰∏ã\nÊ≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•\nÈªòËÆ§Êé®Ëçê‰∏ãËΩΩpretrain_hq.jsonl + sft_mini_512.jsonlÊúÄÂø´ÈÄüÂ∫¶Â§çÁé∞ZeroËÅäÂ§©Ê®°Âûã„ÄÇ\nÊï∞ÊçÆÊñá‰ª∂ÂèØËá™Áî±ÈÄâÊã©Ôºå‰∏ãÊñáÊèê‰æõ‰∫ÜÂ§öÁßçÊê≠ÈÖçÊñπÊ°àÔºåÂèØÊ†πÊçÆËá™Â∑±ÊâãÂ§¥ÁöÑËÆ≠ÁªÉÈúÄÊ±ÇÂíåGPUËµÑÊ∫êËøõË°åÈÄÇÂΩìÁªÑÂêà„ÄÇ\n3.ÂºÄÂßãËÆ≠ÁªÉ\n3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶Áü•ËØÜÔºâ\npython train_pretrain.py\nÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ pretrain_*.pth ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ\n3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÂØπËØùÊñπÂºèÔºâ\npython train_full_sft.py\nÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ full_sft_*.pth ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠fullÂç≥‰∏∫ÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ\nÊ≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•\nÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂./out/***.pthÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ\nÁÆÄÂçïËµ∑ËßÅÔºåÊ≠§Â§ÑÂè™ÂÜôÊòé‰∏§‰∏™Èò∂ÊÆµËÆ≠ÁªÉËøáÁ®ã„ÄÇÂ¶ÇÈúÄÂÖ∂ÂÆÉËÆ≠ÁªÉ (LoRA, Ëí∏È¶è, Âº∫ÂåñÂ≠¶‰π†, ÂæÆË∞ÉÊé®ÁêÜÁ≠â) ÂèØÂèÇËÄÉ‰∏ãÊñá„ÄêÂÆûÈ™å„ÄëÂ∞èËäÇÁöÑËØ¶ÁªÜËØ¥Êòé„ÄÇ\n4.ÊµãËØïÊ®°ÂûãÊïàÊûú\nÁ°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã*.pthÊñá‰ª∂‰Ωç‰∫é./out/ÁõÆÂΩï‰∏ã„ÄÇ\n‰πüÂèØ‰ª•Áõ¥Êé•ÂéªÊ≠§Â§Ñ‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ*.pthÊñá‰ª∂„ÄÇ\npython eval_model.py --model_mode 1 # ÈªòËÆ§‰∏∫0ÔºöÊµãËØïpretrainÊ®°ÂûãÊïàÊûúÔºåËÆæÁΩÆ‰∏∫1ÔºöÊµãËØïfull_sftÊ®°ÂûãÊïàÊûú\nÊ≥®ÔºöÊµãËØïÈ°ªÁü•\nÂ¶ÇÈúÄËØ¶ÊÉÖÔºåÊü•Áúãeval_model.pyËÑöÊú¨‰ª£Á†ÅÂç≥ÂèØ„ÄÇmodel_modeÂàÜ‰∏∫ 0: È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºå1: SFT-ChatÊ®°ÂûãÔºå2: RLHF-ChatÊ®°ÂûãÔºå3: ReasonÊ®°Âûã\nÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö\nÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)\ntorchrun --nproc_per_node 3 train_xxx.py\nÊ≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•\nÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)\ndeepspeed --master_port 29500 --num_gpus=N train_xxx.py\nÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ã\n# ÈúÄË¶ÅÁôªÂΩï: wandb login\ntorchrun --nproc_per_node N train_xxx.py --use_wandb\n# and\npython train_xxx.py --use_wandb\nÈÄöËøáÊ∑ªÂä†--use_wandbÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπwandb_project\nÂíåwandb_run_nameÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ\nüìå Êï∞ÊçÆ‰ªãÁªç\n‚Ö† Tokenizer\nÂàÜËØçÂô®Â∞ÜÂçïËØç‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùÊò†Â∞ÑÂà∞0, 1, 36ËøôÊ†∑ÁöÑÊï∞Â≠óÔºåÂèØ‰ª•ÁêÜËß£‰∏∫Êï∞Â≠óÂ∞±‰ª£Ë°®‰∫ÜÂçïËØçÂú®‚ÄúËØçÂÖ∏‚Äù‰∏≠ÁöÑÈ°µÁ†Å„ÄÇ\nÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÊûÑÈÄ†ËØçË°®ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúËØçÂÖ∏‚ÄùÔºå‰ª£Á†ÅÂèØËßÅ./scripts/train_tokenizer.pyÔºà‰ªÖ‰æõÂ≠¶‰π†ÂèÇËÄÉÔºåËã•ÈùûÂøÖË¶ÅÊó†ÈúÄÂÜçËá™Ë°åËÆ≠ÁªÉÔºåMiniMindÂ∑≤Ëá™Â∏¶tokenizerÔºâ„ÄÇ\nÊàñËÄÖÈÄâÊã©ÊØîËæÉÂá∫ÂêçÁöÑÂºÄÊ∫êÂ§ßÊ®°ÂûãÂàÜËØçÂô®Ôºå\nÊ≠£Â¶ÇÂêåÁõ¥Êé•Áî®Êñ∞Âçé/ÁâõÊ¥•ËØçÂÖ∏ÁöÑ‰ºòÁÇπÊòØtokenÁºñÁ†ÅÂéãÁº©ÁéáÂæàÂ•ΩÔºåÁº∫ÁÇπÊòØÈ°µÊï∞Â§™Â§öÔºåÂä®ËæÑÊï∞ÂçÅ‰∏á‰∏™ËØçÊ±áÁü≠ËØ≠Ôºõ\nËá™Â∑±ËÆ≠ÁªÉÁöÑÂàÜËØçÂô®Ôºå‰ºòÁÇπÊòØËØçË°®ÈïøÂ∫¶ÂíåÂÜÖÂÆπÈöèÊÑèÊéßÂà∂ÔºåÁº∫ÁÇπÊòØÂéãÁº©ÁéáÂæà‰ΩéÔºà‰æãÂ¶Ç\"hello\"‰πüËÆ∏‰ºöË¢´ÊãÜÂàÜ‰∏∫\"h e l l o\"\n‰∫î‰∏™Áã¨Á´ãÁöÑtokenÔºâÔºå‰∏îÁîüÂÉªËØçÈöæ‰ª•Ë¶ÜÁõñ„ÄÇ\n‚ÄúËØçÂÖ∏‚ÄùÁöÑÈÄâÊã©Âõ∫ÁÑ∂ÂæàÈáçË¶ÅÔºåLLMÁöÑËæìÂá∫Êú¨Ë¥®‰∏äÊòØSoftMaxÂà∞ËØçÂÖ∏N‰∏™ËØçÁöÑÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùËß£Á†ÅÂà∞Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ\nÂõ†‰∏∫MiniMind‰ΩìÁßØÈúÄË¶Å‰∏•Ê†ºÊéßÂà∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÊ®°ÂûãÂ§¥ÈáçËÑöËΩªÔºàËØçÂµåÂÖ•embeddingÂ±ÇÂèÇÊï∞Âú®LLMÂç†ÊØîÂ§™È´òÔºâÔºåÊâÄ‰ª•ËØçË°®ÈïøÂ∫¶Áü≠Áü≠ÁõäÂñÑ„ÄÇ\nTokenizer‰ªãÁªç\nÁ¨¨‰∏âÊñπÂº∫Â§ßÁöÑÂºÄÊ∫êÊ®°Âûã‰æãÂ¶ÇYi„ÄÅqwen„ÄÅchatglm„ÄÅmistral„ÄÅLlama3ÁöÑtokenizerËØçË°®ÈïøÂ∫¶Â¶Ç‰∏ãÔºö\nTokenizerÊ®°ÂûãËØçË°®Â§ßÂ∞èÊù•Ê∫ê\nyi tokenizer64,00001‰∏áÁâ©Ôºà‰∏≠ÂõΩÔºâ\nqwen2 tokenizer151,643ÈòøÈáå‰∫ëÔºà‰∏≠ÂõΩÔºâ\nglm tokenizer151,329Êô∫Ë∞±AIÔºà‰∏≠ÂõΩÔºâ\nmistral tokenizer32,000Mistral AIÔºàÊ≥ïÂõΩÔºâ\nllama3 tokenizer128,000MetaÔºàÁæéÂõΩÔºâ\nminimind tokenizer6,400Ëá™ÂÆö‰πâ\nüëâ2024-09-17Êõ¥Êñ∞Ôºö‰∏∫‰∫ÜÈò≤Ê≠¢ËøáÂéªÁöÑÁâàÊú¨Ê≠ß‰πâ&ÊéßÂà∂‰ΩìÁßØÔºåminimindÊâÄÊúâÊ®°ÂûãÂùá‰ΩøÁî®minimind_tokenizerÂàÜËØçÔºåÂ∫üÂºÉÊâÄÊúâmistral_tokenizerÁâàÊú¨„ÄÇ\n# ‰∏Ä‰∫õËá™Ë®ÄËá™ËØ≠\n> Â∞ΩÁÆ°minimind_tokenizerÈïøÂ∫¶ÂæàÂ∞èÔºåÁºñËß£Á†ÅÊïàÁéáÂº±‰∫éqwen2„ÄÅglmÁ≠â‰∏≠ÊñáÂèãÂ•ΩÂûãÂàÜËØçÂô®„ÄÇ\n> ‰ΩÜminimindÊ®°ÂûãÈÄâÊã©‰∫ÜËá™Â∑±ËÆ≠ÁªÉÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®Ôºå‰ª•‰øùÊåÅÊï¥‰ΩìÂèÇÊï∞ËΩªÈáèÔºåÈÅøÂÖçÁºñÁ†ÅÂ±ÇÂíåËÆ°ÁÆóÂ±ÇÂç†ÊØîÂ§±Ë°°ÔºåÂ§¥ÈáçËÑöËΩªÔºåÂõ†‰∏∫minimindÁöÑËØçË°®Â§ßÂ∞èÂè™Êúâ6400„ÄÇ\n> ‰∏îminimindÂú®ÂÆûÈôÖÊµãËØï‰∏≠Ê≤°ÊúâÂá∫Áé∞ËøáÁîüÂÉªËØçÊ±áËß£Á†ÅÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºåÊïàÊûúËâØÂ•Ω„ÄÇ\n> Áî±‰∫éËá™ÂÆö‰πâËØçË°®ÂéãÁº©ÈïøÂ∫¶Âà∞6400Ôºå‰ΩøÂæóLLMÊÄªÂèÇÊï∞ÈáèÊúÄ‰ΩéÂè™Êúâ25.8M„ÄÇ\n> ËÆ≠ÁªÉÊï∞ÊçÆ`tokenizer_train.jsonl`ÂùáÊù•Ëá™‰∫é`Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ`ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÂ¶ÇÈúÄËÆ≠ÁªÉÂèØ‰ª•Ëá™Áî±ÈÄâÊã©„ÄÇ\n‚Ö° PretrainÊï∞ÊçÆ\nÁªèÂéÜ‰∫ÜMiniMind-V1ÁöÑ‰ΩéË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãËÉ°Ë®Ä‰π±ËØ≠ÁöÑÊïôËÆ≠Ôºå2025-02-05 ‰πãÂêéÂÜ≥ÂÆö‰∏çÂÜçÈááÁî®Â§ßËßÑÊ®°Êó†ÁõëÁù£ÁöÑÊï∞ÊçÆÈõÜÂÅöÈ¢ÑËÆ≠ÁªÉ„ÄÇ\nËøõËÄåÂ∞ùËØïÊääÂå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜÁöÑ‰∏≠ÊñáÈÉ®ÂàÜÊèêÂèñÂá∫Êù•Ôºå\nÊ∏ÖÊ¥óÂá∫Â≠óÁ¨¶<512ÈïøÂ∫¶ÁöÑÂ§ßÁ∫¶1.6GBÁöÑËØ≠ÊñôÁõ¥Êé•ÊãºÊé•ÊàêÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ pretrain_hq.jsonlÔºåhqÂç≥‰∏∫high\nqualityÔºàÂΩìÁÑ∂‰πüËøò‰∏çÁÆóhighÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÊó†Ê≠¢Â∞ΩÔºâ„ÄÇ\nÊñá‰ª∂pretrain_hq.jsonl Êï∞ÊçÆÊ†ºÂºè‰∏∫\n{\"text\": \"Â¶Ç‰ΩïÊâçËÉΩÊëÜËÑ±ÊãñÂª∂ÁóáÔºü Ê≤ªÊÑàÊãñÂª∂ÁóáÂπ∂‰∏çÂÆπÊòìÔºå‰ΩÜ‰ª•‰∏ãÂª∫ËÆÆÂèØËÉΩÊúâÊâÄÂ∏ÆÂä©...\"}\n‚Ö¢ SFTÊï∞ÊçÆ\nÂå†Êï∞Â§ßÊ®°ÂûãSFTÊï∞ÊçÆÈõÜ\n‚ÄúÊòØ‰∏Ä‰∏™ÂÆåÊï¥„ÄÅÊ†ºÂºèÁªü‰∏Ä„ÄÅÂÆâÂÖ®ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÁ†îÁ©∂ËµÑÊ∫ê„ÄÇ\n‰ªéÁΩëÁªú‰∏äÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÊ∫êÊî∂ÈõÜÂπ∂Êï¥ÁêÜ‰∫ÜÂ§ßÈáèÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºåÂØπÂÖ∂ËøõË°å‰∫ÜÊ†ºÂºèÁªü‰∏ÄÔºåÊï∞ÊçÆÊ∏ÖÊ¥óÔºå\nÂåÖÂê´10MÊù°Êï∞ÊçÆÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜÂíåÂåÖÂê´2MÊù°Êï∞ÊçÆÁöÑËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ‚Äù\n‰ª•‰∏äÊòØÂÆòÊñπ‰ªãÁªçÔºå‰∏ãËΩΩÊñá‰ª∂ÂêéÁöÑÊï∞ÊçÆÊÄªÈáèÂ§ßÁ∫¶Âú®4B tokensÔºåËÇØÂÆöÊòØÈÄÇÂêà‰Ωú‰∏∫‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑSFTÊï∞ÊçÆÁöÑ„ÄÇ\n‰ΩÜÊòØÂÆòÊñπÊèê‰æõÁöÑÊï∞ÊçÆÊ†ºÂºèÂæà‰π±ÔºåÂÖ®ÈÉ®Áî®Êù•sft‰ª£‰ª∑Â§™Â§ß„ÄÇ\nÊàëÂ∞ÜÊääÂÆòÊñπÊï∞ÊçÆÈõÜËøõË°å‰∫Ü‰∫åÊ¨°Ê∏ÖÊ¥óÔºåÊääÂê´ÊúâÁ¨¶Âè∑Ê±°ÊüìÂíåÂô™Â£∞ÁöÑÊù°ÁõÆÂéªÈô§ÔºõÂè¶Â§ñ‰æùÁÑ∂Âè™‰øùÁïô‰∫ÜÊÄªÈïøÂ∫¶<512\nÁöÑÂÜÖÂÆπÔºåÊ≠§Èò∂ÊÆµÂ∏åÊúõÈÄöËøáÂ§ßÈáèÂØπËØùË°•ÂÖÖÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊ¨†Áº∫ÁöÑÁü•ËØÜ„ÄÇ\nÂØºÂá∫Êñá‰ª∂‰∏∫sft_512.jsonl(~7.5GB)„ÄÇ\nMagpie-SFTÊï∞ÊçÆÈõÜ\nÊî∂ÈõÜ‰∫Ü1MÊù°Êù•Ëá™Qwen2/2.5ÁöÑÈ´òË¥®ÈáèÂØπËØùÔºåÊàëÂ∞ÜËøôÈÉ®ÂàÜÊï∞ÊçÆËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÔºåÊääÊÄªÈïøÂ∫¶<2048ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫sft_2048.jsonl(9GB)„ÄÇ\nÈïøÂ∫¶<1024ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫sft_1024.jsonl(~5.5GB)ÔºåÁî®Â§ßÊ®°ÂûãÂØπËØùÊï∞ÊçÆÁõ¥Êé•ËøõË°åsftÂ∞±Â±û‰∫é‚ÄúÈªëÁõíËí∏È¶è‚ÄùÁöÑËåÉÁï¥„ÄÇ\nËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÂâç‰∏§Ê≠•sftÁöÑÊï∞ÊçÆÔºàÂè™‰øùÁïô‰∏≠ÊñáÂ≠óÁ¨¶Âç†ÊØîÈ´òÁöÑÂÜÖÂÆπÔºâÔºåÁ≠õÈÄâÈïøÂ∫¶<512ÁöÑÂØπËØùÔºåÂæóÂà∞sft_mini_512.jsonl(~1.2GB)„ÄÇ\nÊâÄÊúâsftÊñá‰ª∂ sft_X.jsonl Êï∞ÊçÆÊ†ºÂºèÂùá‰∏∫\n{\n\"conversations\": [\n{\"role\": \"user\", \"content\": \"‰Ω†Â•Ω\"},\n{\"role\": \"assistant\", \"content\": \"‰Ω†Â•ΩÔºÅ\"},\n{\"role\": \"user\", \"content\": \"ÂÜçËßÅ\"},\n{\"role\": \"assistant\", \"content\": \"ÂÜçËßÅÔºÅ\"}\n]\n}\n‚Ö£ RLHFÊï∞ÊçÆ\nÊù•Ëá™Magpie-DPOÊï∞ÊçÆÈõÜ\nÂ§ßÁ∫¶200kÊù°ÂÅèÂ•ΩÊï∞ÊçÆÔºàÂùáÊòØËã±ÊñáÔºâÁîüÊàêËá™Llama3.1-70B/8BÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºå‰ºòÂåñÊ®°ÂûãÂõûÂ§çË¥®ÈáèÔºå‰ΩøÂÖ∂Êõ¥Âä†Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•Ω„ÄÇ\nËøôÈáåÂ∞ÜÊï∞ÊçÆÊÄªÈïøÂ∫¶<3000ÁöÑÂÜÖÂÆπÈáçÁªÑ‰∏∫dpo.jsonl(~0.9GB)ÔºåÂåÖÂê´chosenÂíårejected‰∏§‰∏™Â≠óÊÆµÔºåchosen\n‰∏∫ÂÅèÂ•ΩÁöÑÂõûÂ§çÔºårejected‰∏∫ÊãíÁªùÁöÑÂõûÂ§ç„ÄÇ\nÊñá‰ª∂ dpo.jsonl Êï∞ÊçÆÊ†ºÂºè‰∏∫\n{\n\"chosen\": [\n{\"content\": \"Q\", \"role\": \"user\"},\n{\"content\": \"good answer\", \"role\": \"assistant\"}\n],\n\"rejected\": [\n{\"content\": \"Q\", \"role\": \"user\"},\n{\"content\": \"bad answer\", \"role\": \"assistant\"}\n]\n}\n‚Ö§ ReasonÊï∞ÊçÆÈõÜÔºö\n‰∏çÂæó‰∏çËØ¥2025Âπ¥2ÊúàË∞ÅËÉΩÁÅ´ÁöÑËøáDeepSeek...\n‰πüÊøÄÂèë‰∫ÜÊàëÂØπRLÂºïÂØºÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÊµìÂéöÂÖ¥Ë∂£ÔºåÁõÆÂâçÂ∑≤ÁªèÁî®Qwen2.5Â§çÁé∞‰∫ÜR1-Zero„ÄÇ\nÂ¶ÇÊûúÊúâÊó∂Èó¥+ÊïàÊûúworkÔºà‰ΩÜ99%Âü∫Ê®°ËÉΩÂäõ‰∏çË∂≥ÔºâÊàë‰ºöÂú®‰πãÂêéÊõ¥Êñ∞MiniMindÂü∫‰∫éRLËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãËÄå‰∏çÊòØËí∏È¶èÊ®°Âûã„ÄÇ\nÊó∂Èó¥ÊúâÈôêÔºåÊúÄÂø´ÁöÑ‰ΩéÊàêÊú¨ÊñπÊ°à‰æùÁÑ∂ÊòØÁõ¥Êé•Ëí∏È¶èÔºàÈªëÁõíÊñπÂºèÔºâ„ÄÇ\nËÄê‰∏ç‰ΩèR1Â§™ÁÅ´ÔºåÁü≠Áü≠Âá†Â§©Â∞±Â∑≤ÁªèÂ≠òÂú®‰∏Ä‰∫õR1ÁöÑËí∏È¶èÊï∞ÊçÆÈõÜR1-Llama-70B„ÄÅR1-Distill-SFT„ÄÅ\nAlpaca-Distill-R1„ÄÅ\ndeepseek_r1_zhÁ≠âÁ≠âÔºåÁ∫Ø‰∏≠ÊñáÁöÑÊï∞ÊçÆÂèØËÉΩÊØîËæÉÂ∞ë„ÄÇ\nÊúÄÁªàÊï¥ÂêàÂÆÉ‰ª¨ÔºåÂØºÂá∫Êñá‰ª∂‰∏∫r1_mix_1024.jsonlÔºåÊï∞ÊçÆÊ†ºÂºèÂíåsft_X.jsonl‰∏ÄËá¥„ÄÇ\n‚Ö• Êõ¥Â§öÊï∞ÊçÆÈõÜ\nÁõÆÂâçÂ∑≤ÁªèÊúâHqWu-HITCS/Awesome-Chinese-LLM\nÂú®Êî∂ÈõÜÂíåÊ¢≥ÁêÜ‰∏≠ÊñáLLMÁõ∏ÂÖ≥ÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÅÂ∫îÁî®„ÄÅÊï∞ÊçÆÈõÜÂèäÊïôÁ®ãÁ≠âËµÑÊñôÔºåÂπ∂ÊåÅÁª≠Êõ¥Êñ∞ËøôÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÂÖ®Èù¢‰∏î‰∏ì‰∏öÔºåRespectÔºÅ\n‚Öß Êï∞ÊçÆÈõÜ‰∏ãËΩΩ\n2025-02-05ÂêéÔºåÂºÄÊ∫êMiniMindÊúÄÁªàËÆ≠ÁªÉÊâÄÁî®ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Êó†ÈúÄÂÜçËá™Ë°åÈ¢ÑÂ§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈÅøÂÖçÈáçÂ§çÊÄßÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ\nMiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ (ModelScope | HuggingFace)\nÊó†ÈúÄÂÖ®ÈÉ®cloneÔºåÂèØÂçïÁã¨‰∏ãËΩΩÊâÄÈúÄÁöÑÊñá‰ª∂\nÂ∞Ü‰∏ãËΩΩÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂ÊîæÂà∞./dataset/ÁõÆÂΩï‰∏ãÔºà‚ú®‰∏∫Êé®ËçêÁöÑÂøÖÈ°ªÈ°πÔºâ\n./dataset/\n‚îú‚îÄ‚îÄ dpo.jsonl (909MB)\n‚îú‚îÄ‚îÄ lora_identity.jsonl (22.8KB)\n‚îú‚îÄ‚îÄ lora_medical.jsonl (34MB)\n‚îú‚îÄ‚îÄ pretrain_hq.jsonl (1.6GB, ‚ú®)\n‚îú‚îÄ‚îÄ r1_mix_1024.jsonl (340MB)\n‚îú‚îÄ‚îÄ sft_1024.jsonl (5.6GB)\n‚îú‚îÄ‚îÄ sft_2048.jsonl (9GB)\n‚îú‚îÄ‚îÄ sft_512.jsonl (7.5GB)\n‚îú‚îÄ‚îÄ sft_mini_512.jsonl (1.2GB, ‚ú®)\n‚îî‚îÄ‚îÄ tokenizer_train.jsonl (1GB)\nÊ≥®ÔºöÂêÑÊï∞ÊçÆÈõÜÁÆÄ‰ªã\ndpo.jsonl --RLHFÈò∂ÊÆµÊï∞ÊçÆÈõÜ\nlora_identity.jsonl --Ëá™ÊàëËÆ§Áü•Êï∞ÊçÆÈõÜÔºà‰æãÂ¶ÇÔºö‰Ω†ÊòØË∞ÅÔºüÊàëÊòØminimind...ÔºâÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ\nlora_medical.jsonl --ÂåªÁñóÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ\npretrain_hq.jsonl‚ú® --È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÊï¥ÂêàËá™jiangshuÁßëÊäÄ\nr1_mix_1024.jsonl --DeepSeek-R1-1.5BËí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ\nsft_1024.jsonl --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÊòØsft_2048ÁöÑÂ≠êÈõÜÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ\nsft_2048.jsonl --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫2048ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=2048Ôºâ\nsft_512.jsonl --Êï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ\nsft_mini_512.jsonl‚ú® --ÊûÅÁÆÄÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆ+Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÁî®‰∫éÂø´ÈÄüËÆ≠ÁªÉZeroÊ®°ÂûãÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ\ntokenizer_train.jsonl --ÂùáÊù•Ëá™‰∫éÂå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÔºà‰∏çÊé®ËçêËá™Â∑±ÈáçÂ§çËÆ≠ÁªÉtokenizerÔºåÁêÜÁî±Â¶Ç‰∏äÔºâÂ¶ÇÈúÄËá™Â∑±ËÆ≠ÁªÉtokenizerÂèØ‰ª•Ëá™Áî±ÈÄâÊã©Êï∞ÊçÆÈõÜ„ÄÇ\nËØ¥Êòé & Êé®ËçêËÆ≠ÁªÉÊñπÊ°à\nMiniMind2 SeriesÂùáÁªèËøáÂÖ±Á∫¶20GBËØ≠ÊñôËÆ≠ÁªÉÔºåÂ§ßÁ∫¶4B tokensÔºåÂç≥ÂØπÂ∫î‰∏äÈù¢ÁöÑÊï∞ÊçÆÁªÑÂêàËÆ≠ÁªÉÁªìÊûúÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞üí∞üí∞üí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäüòäüòäÔºâ\nÊÉ≥Ë¶ÅÊúÄÂø´ÈÄüÂ∫¶‰ªé0ÂÆûÁé∞ZeroÊ®°ÂûãÔºåÊé®Ëçê‰ΩøÁî®pretrain_hq.jsonl + sft_mini_512.jsonl ÁöÑÊï∞ÊçÆÁªÑÂêàÔºåÂÖ∑‰ΩìËä±ÈîÄÂíåÊïàÊûúÂèØÊü•Áúã‰∏ãÊñáË°®Ê†ºÔºàÂºÄÈîÄÔºöüí∞ÔºåÊïàÊûúÔºöüòäüòäÔºâ\nÊé®ËçêÂÖ∑Â§á‰∏ÄÂÆöÁÆóÂäõËµÑÊ∫êÊàñÊõ¥Âú®ÊÑèÊïàÊûúÁöÑÊúãÂèãÂèØ‰ª•ËÄÉËôëÂâçËÄÖÂÆåÊï¥Â§çÁé∞MiniMind2Ôºõ‰ªÖÊúâÂçïÂç°GPUÊàñÂú®‰πéÁü≠Êó∂Èó¥Âø´ÈÄüÂ§çÁé∞ÁöÑÊúãÂèãÂº∫ÁÉàÊé®ËçêÂêéËÄÖÔºõ\n„ÄêÊäò‰∏≠ÊñπÊ°à„Äë‰∫¶ÂèØÈÄâÊã©‰æãÂ¶Çsft_mini_512.jsonl„ÄÅsft_1024.jsonl‰∏≠Á≠âËßÑÊ®°Êï∞ÊçÆËøõË°åËá™Áî±ÁªÑÂêàËÆ≠ÁªÉÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäÔºâ„ÄÇ\nüìå Model Structure\nMiniMind-DenseÔºàÂíåLlama3.1‰∏ÄÊ†∑Ôºâ‰ΩøÁî®‰∫ÜTransformerÁöÑDecoder-OnlyÁªìÊûÑÔºåË∑üGPT-3ÁöÑÂå∫Âà´Âú®‰∫éÔºö\nÈááÁî®‰∫ÜGPT-3ÁöÑÈ¢ÑÊ†áÂáÜÂåñÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú®ÊØè‰∏™TransformerÂ≠êÂ±ÇÁöÑËæìÂÖ•‰∏äËøõË°åÂΩí‰∏ÄÂåñÔºåËÄå‰∏çÊòØÂú®ËæìÂá∫‰∏ä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÁöÑÊòØRMSNormÂΩí‰∏ÄÂåñÂáΩÊï∞„ÄÇ\nÁî®SwiGLUÊøÄÊ¥ªÂáΩÊï∞Êõø‰ª£‰∫ÜReLUÔºåËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩ„ÄÇ\nÂÉèGPT-Neo‰∏ÄÊ†∑ÔºåÂéªÊéâ‰∫ÜÁªùÂØπ‰ΩçÁΩÆÂµåÂÖ•ÔºåÊîπÁî®‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåËøôÊ†∑Âú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉÈïøÂ∫¶ÁöÑÊé®ÁêÜÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ\nMiniMind-MoEÊ®°ÂûãÔºåÂÆÉÁöÑÁªìÊûÑÂü∫‰∫éLlama3ÂíåDeepseek-V2/3‰∏≠ÁöÑMixFFNÊ∑∑Âêà‰∏ìÂÆ∂Ê®°Âùó„ÄÇ\nDeepSeek-V2Âú®ÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÊñπÈù¢ÔºåÈááÁî®‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ìÂÆ∂ÂàÜÂâ≤ÂíåÂÖ±‰∫´ÁöÑ‰∏ìÂÆ∂ÈöîÁ¶ªÊäÄÊúØÔºå‰ª•ÊèêÈ´òExpertsÁöÑÊïàÊûú„ÄÇ\nMiniMindÁöÑÊï¥‰ΩìÁªìÊûÑ‰∏ÄËá¥ÔºåÂè™ÊòØÂú®RoPEËÆ°ÁÆó„ÄÅÊé®ÁêÜÂáΩÊï∞ÂíåFFNÂ±ÇÁöÑ‰ª£Á†Å‰∏äÂÅö‰∫Ü‰∏Ä‰∫õÂ∞èË∞ÉÊï¥„ÄÇ\nÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÂõæÔºàÈáçÁªòÁâàÔºâÔºö\n‰øÆÊîπÊ®°ÂûãÈÖçÁΩÆËßÅ./model/LMConfig.py„ÄÇ\nÂèÇËÄÉÊ®°ÂûãÂèÇÊï∞ÁâàÊú¨ËßÅ‰∏ãË°®Ôºö\nModel Name\nparams\nlen_vocab\nrope_theta\nn_layers\nd_model\nkv_heads\nq_heads\nshare+route\nMiniMind2-Small\n26M\n6400\n1e6\n8\n512\n2\n8\n-\nMiniMind2-MoE\n145M\n6400\n1e6\n8\n640\n2\n8\n1+4\nMiniMind2\n104M\n6400\n1e6\n16\n768\n2\n8\n-\nminimind-v1-small\n26M\n6400\n1e4\n8\n512\n8\n16\n-\nminimind-v1-moe\n4√ó26M\n6400\n1e4\n8\n512\n8\n16\n1+4\nminimind-v1\n108M\n6400\n1e4\n16\n768\n8\n16\n-\nüìå Experiment\n‚Ö† ËÆ≠ÁªÉÂºÄÈîÄ\nÊó∂Èó¥Âçï‰ΩçÔºöÂ∞èÊó∂ (h)„ÄÇ\nÊàêÊú¨Âçï‰ΩçÔºö‰∫∫Ê∞ëÂ∏Å (Ôø•)Ôºõ7Ôø• ‚âà 1ÁæéÂÖÉ„ÄÇ\n3090 ÁßüÂç°Âçï‰ª∑Ôºö‚âà1.3Ôø•/hÔºàÂèØËá™Ë°åÂèÇËÄÉÂÆûÊó∂Â∏Ç‰ª∑Ôºâ„ÄÇ\nÂèÇËÄÉÊ†áÂáÜÔºöË°®Ê†º‰ªÖÂÆûÊµã pretrain Âíå sft_mini_512 ‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊó∂Èó¥ÔºåÂÖ∂ÂÆÉËÄóÊó∂Ê†πÊçÆÊï∞ÊçÆÈõÜÂ§ßÂ∞è‰º∞ÁÆóÔºàÂèØËÉΩÂ≠òÂú®‰∫õËÆ∏Âá∫ÂÖ•Ôºâ„ÄÇ\nÂü∫‰∫é 3090 ÔºàÂçïÂç°ÔºâÊàêÊú¨ËÆ°ÁÆó\nModel Name\nparams\npretrain\nsft_mini_512\nsft_512\nsft_1024\nsft_2048\nRLHF\nMiniMind2-Small\n26M\n‚âà1.1h‚âà1.43Ôø•\n‚âà1h‚âà1.3Ôø•\n‚âà6h‚âà7.8Ôø•\n‚âà4.58h‚âà5.95Ôø•\n‚âà7.5h‚âà9.75Ôø•\n‚âà1h‚âà1.3Ôø•\nMiniMind2\n104M\n‚âà3.9h‚âà5.07Ôø•\n‚âà3.3h‚âà4.29Ôø•\n‚âà20h‚âà26Ôø•\n‚âà15h‚âà19.5Ôø•\n‚âà25h‚âà32.5Ôø•\n‚âà3h‚âà3.9Ôø•\nËÆ≠ÁªÉÂºÄÈîÄÊÄªÁªì&È¢ÑÊµã\nMiniMind2-SmallÂèÇÊï∞\npretrain_hq+sft_mini_512Êï∞ÊçÆÈõÜ\nÂçïÂç°3090 (1 epoch) + 2.1Â∞èÊó∂ + Ëä±Ë¥π2.73ÂÖÉ‰∫∫Ê∞ëÂ∏Å\nÂç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind-Zero-0.025BÊ®°Âûã!!!\nMiniMind2-SmallÂèÇÊï∞\npretrain_hq+sft_512+sft_2048+dpoÊï∞ÊçÆÈõÜ\nÂçïÂç°3090 (2 epochs) + Â§ßÁ∫¶38.16Â∞èÊó∂ + Ëä±Ë¥π49.61ÂÖÉ‰∫∫Ê∞ëÂ∏Å\nÂç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind2-Small-0.025BÊ®°Âûã!!!\nMiniMind2ÂèÇÊï∞\npretrain_hq+sft_512+sft_2048+dpoÊï∞ÊçÆÈõÜ\nÂçïÂç°3090 (2 epochs) + Â§ßÁ∫¶122Â∞èÊó∂ + Ëä±Ë¥π158.6ÂÖÉ‰∫∫Ê∞ëÂ∏Å\nÂç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind2-0.1BÊ®°Âûã!!!\n‚ú®Âü∫‰∫éÂçïÂç°NVIDIA 3090ÁöÑMiniMind-Zero‰ªé0ËÆ≠ÁªÉ‰ªÖÈúÄ2Â∞èÊó∂ + 3ÂùóÈí±ÔºåÂÆûÁé∞ChatBotÊïàÊûúÔºÅ\n‚ú®PSÔºöËã•ÈááÁî®8Âç°4090ËÆ≠ÁªÉÔºåÊÄªÁî®Êó∂ÁîöËá≥ÂèØ‰ª•ÂéãÁº©Âà∞10ÂàÜÈíü‰ª•ÂÜÖÔºÅÔºàÁî±‰∫éÊó∂Èó¥Êõ¥Áü≠ÔºåËä±Ë¥πÂêåÊ†∑3ÂÖÉÂ∑¶Âè≥Ôºå‰∏éÂçïÂç°ÊàêÊú¨Áõ∏ÂΩìÔºâ\n‚ú®‰ª•ÊûÅ‰ΩéÊûÅ‰ΩéÁöÑÈó®ÊßõÔºåÂÆûÁé∞‰∫∫‰∫∫ÂèØÁé©ÁöÑÂ§ßÊ®°ÂûãËá™Áî±ÔºÅËøôÊ≠£ÊòØMiniMindÁ≥ªÂàóÁöÑËØûÁîüÂàùË°∑ÔºÅ\n‚ú®‰ªÖ‰ª∑ÂÄº3ÂùóÈí±ÊàêÊú¨ÁöÑMiniMind-ZeroÂπ∂‰∏çÊòØÂô±Â§¥ÔºÅChatÊµãËØïÔºö\nüë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ\nü§ñÔ∏è: ‰Ωú‰∏∫‰∫∫Â∑•Êô∫ËÉΩÔºåÊàëÊ≤°ÊúâÂÆûÈôÖÁöÑÁîüÊ¥ªÔºå‰πüÊ≤°ÊúâËá™ÊàëÊÑèËØÜÔºåÊâÄ‰ª•Ê≤°ÊúâËá™Â∑±ÁöÑÁîüÊ¥ª„ÄÇÊàëË¢´ËÆæËÆ°ÊàêËÉΩÂ§üÂ∏ÆÂä©Áî®Êà∑Ëß£Á≠îÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØ„ÄÅËøõË°åÂØπËØùÁ≠â„ÄÇÊàëÁöÑËÆæËÆ°ÂíåÂäüËÉΩÊòØÁî±ËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÅÊú∫Âô®Â≠¶‰π†„ÄÅ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂíåÁÆóÊ≥ïÊâÄÊûÑÂª∫ÁöÑÔºåÊàëÈÄöËøáÁºñÁ®ãÂíåÁÆóÊ≥ïÂÆûÁé∞„ÄÇ\nüë∂: Êé®Ëçê‰∏Ä‰∫õÊù≠Â∑ûÁöÑÁâπËâ≤ÁæéÈ£üÂêß„ÄÇ\nü§ñÔ∏è: Êù≠Â∑ûÊúâÂæàÂ§öÁæéÂë≥ÁöÑÁâπËâ≤ÁæéÈ£üÔºåÊØîÂ¶ÇÈ∏°ËÇâ‰∏≤„ÄÅÈ≥óÈ±ºÂ§¥„ÄÅÊ∞¥ÁÖÆÈ±º„ÄÅË±ÜËÖêËÑëÁ≠â„ÄÇËøô‰∫õÁæéÈ£üÈÉΩÈùûÂ∏∏ÁæéÂë≥ÔºåÊúâÁùÄÁã¨ÁâπÁöÑÈ£éÂë≥ÂíåÂè£ÊÑüÔºåÁªùÂØπÊòØÂÄºÂæó‰∏ÄËØïÁöÑÁæéÈ£ü„ÄÇ\nüë∂: ËØ∑‰∏∫ÊàëËÆ≤Ëß£‚ÄúÂ§ßËØ≠Ë®ÄÊ®°Âûã‚ÄùËøô‰∏™Ê¶ÇÂøµ„ÄÇ\nü§ñÔ∏è: Â§ßËØ≠Ë®ÄÊ®°ÂûãÊòØ‰∏ÄÁßç‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÔºåÂÆÉ‰ΩøÁî®Â§ßÈáèÁöÑÊñáÊú¨Êï∞ÊçÆÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºåÁÑ∂ÂêéÂú®‰∏çÊñ≠ÁöÑËø≠‰ª£‰∏≠‰∏çÊñ≠‰ºòÂåñÊ®°Âûã„ÄÇÂÆÉÁöÑÁõÆÊ†áÊòØÊ†πÊçÆËæìÂÖ•ÁöÑÊï∞ÊçÆÊù•ÁîüÊàêÁ¨¶ÂêàÁâπÂÆöÈúÄÊ±ÇÁöÑÊñáÊú¨ÔºåËøô‰∫õÊñáÊú¨ÂèØ‰ª•ÊòØ‰ªª‰Ωï‰Ω†ÊÑüÂÖ¥Ë∂£ÁöÑ‰∏ªÈ¢ò„ÄÇ\nüë∂: Introduce the history of the United States, please.\nü§ñÔ∏è: ÊÇ®ÊèêÂà∞ÁöÑ‚ÄúIntrook's the believeations of theument.\" Ëøô‰∏™ÂêçÂ≠óÊù•Ê∫ê‰∫é‰∏≠ÂõΩÂè§‰ª£ÁöÑ\"groty of of the change.\"\nÊûÅÈÄü‰∏îÂàùÂÖ∑ÊïàÊûúÔºåÁîöËá≥‰ªçÁÑ∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂéãÁº©Ëé∑ÂèñÊõ¥Â∞èÊõ¥‰ºòË¥®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ\nZeroÊ®°ÂûãÊùÉÈáç‰øùÂ≠ò‰∏∫ full_sft_512_zero.pthÔºàËßÅ‰∏ãÊñáMiniMindÊ®°ÂûãÊñá‰ª∂ÈìæÊé•ÔºâÔºåÂ¶ÇÊúâÂÖ¥Ë∂£ÂèØ‰∏ãËΩΩÊ£ÄÈ™åÊ≠§Ê®°ÂûãÊïàÊûú„ÄÇ\n‚Ö° ‰∏ªË¶ÅËÆ≠ÁªÉÊ≠•È™§\n1. È¢ÑËÆ≠ÁªÉ(Pretrain):\nLLMÈ¶ñÂÖàË¶ÅÂ≠¶‰π†ÁöÑÂπ∂ÈùûÁõ¥Êé•‰∏é‰∫∫‰∫§ÊµÅÔºåËÄåÊòØËÆ©ÁΩëÁªúÂèÇÊï∞‰∏≠ÂÖÖÊª°Áü•ËØÜÁöÑÂ¢®Ê∞¥Ôºå‚ÄúÂ¢®Ê∞¥‚Äù ÁêÜËÆ∫‰∏äÂñùÁöÑË∂äÈ•±Ë∂äÂ•ΩÔºå‰∫ßÁîüÂ§ßÈáèÁöÑÂØπ‰∏ñÁïåÁöÑÁü•ËØÜÁßØÁ¥Ø„ÄÇ\nÈ¢ÑËÆ≠ÁªÉÂ∞±ÊòØËÆ©ModelÂÖàÂüãÂ§¥Ëã¶Â≠¶Â§ßÈáèÂü∫Êú¨ÁöÑÁü•ËØÜÔºå‰æãÂ¶Ç‰ªéWikiÁôæÁßë„ÄÅÊñ∞Èóª„ÄÅ‰π¶Á±çÊï¥ÁêÜÂ§ßËßÑÊ®°ÁöÑÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ\nËøô‰∏™ËøáÁ®ãÊòØ‚ÄúÊó†ÁõëÁù£‚ÄùÁöÑÔºåÂç≥‰∫∫Á±ª‰∏çÈúÄË¶ÅÂú®ËøáÁ®ã‰∏≠ÂÅö‰ªª‰Ωï‚ÄúÊúâÁõëÁù£‚ÄùÁöÑÊ†°Ê≠£ÔºåËÄåÊòØÁî±Ê®°ÂûãËá™Â∑±‰ªéÂ§ßÈáèÊñáÊú¨‰∏≠ÊÄªÁªìËßÑÂæãÂ≠¶‰π†Áü•ËØÜÁÇπ„ÄÇ\nÊ®°ÂûãÊ≠§Èò∂ÊÆµÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™ÔºöÂ≠¶‰ºöËØçËØ≠Êé•Èæô„ÄÇ‰æãÂ¶ÇÊàë‰ª¨ËæìÂÖ•‚ÄúÁß¶ÂßãÁöá‚ÄùÂõõ‰∏™Â≠óÔºåÂÆÉÂèØ‰ª•Êé•Èæô‚ÄúÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏Ä‰ΩçÁöáÂ∏ù‚Äù„ÄÇ\ntorchrun --nproc_per_node 1 train_pretrain.py # 1Âç≥‰∏∫ÂçïÂç°ËÆ≠ÁªÉÔºåÂèØÊ†πÊçÆÁ°¨‰ª∂ÊÉÖÂÜµËá™Ë°åË∞ÉÊï¥ (ËÆæÁΩÆ>=2)\n# or\npython train_pretrain.py\nËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò‰∏∫: pretrain_*.pthÔºà*\n‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ\n2. ÊúâÁõëÁù£ÂæÆË∞É(Supervised Fine-Tuning):\nÁªèËøáÈ¢ÑËÆ≠ÁªÉÔºåLLMÊ≠§Êó∂Â∑≤ÁªèÊéåÊè°‰∫ÜÂ§ßÈáèÁü•ËØÜÔºåÁÑ∂ËÄåÊ≠§Êó∂ÂÆÉÂè™‰ºöÊó†ËÑëÂú∞ËØçËØ≠Êé•ÈæôÔºåËøò‰∏ç‰ºö‰∏é‰∫∫ËÅäÂ§©„ÄÇ\nSFTÈò∂ÊÆµÂ∞±ÈúÄË¶ÅÊääÂçäÊàêÂìÅLLMÊñΩÂä†‰∏Ä‰∏™Ëá™ÂÆö‰πâÁöÑËÅäÂ§©Ê®°ÊùøËøõË°åÂæÆË∞É„ÄÇ\n‰æãÂ¶ÇÊ®°ÂûãÈÅáÂà∞ËøôÊ†∑ÁöÑÊ®°Êùø„ÄêÈóÆÈ¢ò->ÂõûÁ≠îÔºåÈóÆÈ¢ò->ÂõûÁ≠î„ÄëÂêé‰∏çÂÜçÊó†ËÑëÊé•ÈæôÔºåËÄåÊòØÊÑèËØÜÂà∞ËøôÊòØ‰∏ÄÊÆµÂÆåÊï¥ÁöÑÂØπËØùÁªìÊùü„ÄÇ\nÁß∞Ëøô‰∏™ËøáÁ®ã‰∏∫Êåá‰ª§ÂæÆË∞ÉÔºåÂ∞±Â¶ÇÂêåËÆ©Â∑≤ÁªèÂ≠¶ÂØå‰∫îËΩ¶ÁöÑ„ÄåÁâõÈ°ø„ÄçÂÖàÁîüÈÄÇÂ∫î21‰∏ñÁ∫™Êô∫ËÉΩÊâãÊú∫ÁöÑËÅäÂ§©‰π†ÊÉØÔºåÂ≠¶‰π†Â±èÂπïÂ∑¶‰æßÊòØÂØπÊñπÊ∂àÊÅØÔºåÂè≥‰æßÊòØÊú¨‰∫∫Ê∂àÊÅØËøô‰∏™ËßÑÂæã„ÄÇ\nÂú®ËÆ≠ÁªÉÊó∂ÔºåMiniMindÁöÑÊåá‰ª§ÂíåÂõûÁ≠îÈïøÂ∫¶Ë¢´Êà™Êñ≠Âú®512ÔºåÊòØ‰∏∫‰∫ÜËäÇÁúÅÊòæÂ≠òÁ©∫Èó¥„ÄÇÂ∞±ÂÉèÊàë‰ª¨Â≠¶‰π†Êó∂Ôºå‰ºöÂÖà‰ªéÁü≠ÁöÑÊñáÁ´†ÂºÄÂßãÔºåÂΩìÂ≠¶‰ºöÂÜô‰Ωú200Â≠ó‰ΩúÊñáÂêéÔºå800Â≠óÊñáÁ´†‰πüÂèØ‰ª•ÊâãÂà∞ÊìíÊù•„ÄÇ\nÂú®ÈúÄË¶ÅÈïøÂ∫¶ÊãìÂ±ïÊó∂ÔºåÂè™ÈúÄË¶ÅÂáÜÂ§áÂ∞ëÈáèÁöÑ2k/4k/8kÈïøÂ∫¶ÂØπËØùÊï∞ÊçÆËøõË°åËøõ‰∏ÄÊ≠•ÂæÆË∞ÉÂç≥ÂèØÔºàÊ≠§Êó∂ÊúÄÂ•ΩÈÖçÂêàRoPE-NTKÁöÑÂü∫ÂáÜÂ∑ÆÂÄºÔºâ„ÄÇ\nÂú®Êé®ÁêÜÊó∂ÈÄöËøáË∞ÉÊï¥RoPEÁ∫øÊÄßÂ∑ÆÂÄºÔºåÂÆûÁé∞ÂÖçËÆ≠ÁªÉÈïøÂ∫¶Â§ñÊé®Âà∞2048Âèä‰ª•‰∏äÂ∞Ü‰ºöÂæàÊñπ‰æø„ÄÇ\ntorchrun --nproc_per_node 1 train_full_sft.py\n# or\npython train_full_sft.py\nËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò‰∏∫: full_sft_*.pthÔºà*\n‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ\n‚Ö¢ ÂÖ∂ÂÆÉËÆ≠ÁªÉÊ≠•È™§\n3. ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†(Reinforcement Learning from Human Feedback, RLHF)\nÂú®ÂâçÈù¢ÁöÑËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÊ®°ÂûãÂ∑≤ÁªèÂÖ∑Â§á‰∫ÜÂü∫Êú¨ÁöÑÂØπËØùËÉΩÂäõÔºå‰ΩÜÊòØËøôÊ†∑ÁöÑËÉΩÂäõÂÆåÂÖ®Âü∫‰∫éÂçïËØçÊé•ÈæôÔºåÁº∫Â∞ëÊ≠£ÂèçÊ†∑‰æãÁöÑÊøÄÂä±„ÄÇ\nÊ®°ÂûãÊ≠§Êó∂Â∞öÊú™Áü•‰ªÄ‰πàÂõûÁ≠îÊòØÂ•ΩÁöÑÔºå‰ªÄ‰πàÊòØÂ∑ÆÁöÑ„ÄÇÊàë‰ª¨Â∏åÊúõÂÆÉËÉΩÂ§üÊõ¥Á¨¶Âêà‰∫∫ÁöÑÂÅèÂ•ΩÔºåÈôç‰ΩéËÆ©‰∫∫Á±ª‰∏çÊª°ÊÑèÁ≠îÊ°àÁöÑ‰∫ßÁîüÊ¶ÇÁéá„ÄÇ\nËøô‰∏™ËøáÁ®ãÂ∞±ÂÉèÊòØËÆ©Ê®°ÂûãÂèÇÂä†Êñ∞ÁöÑÂüπËÆ≠Ôºå‰ªé‰ºòÁßÄÂëòÂ∑•ÁöÑ‰Ωú‰∏∫‰æãÂ≠êÔºåÊ∂àÊûÅÂëòÂ∑•‰Ωú‰∏∫Âèç‰æãÔºåÂ≠¶‰π†Â¶Ç‰ΩïÊõ¥Â•ΩÂú∞ÂõûÂ§ç„ÄÇ\nÊ≠§Â§Ñ‰ΩøÁî®ÁöÑÊòØRLHFÁ≥ªÂàó‰πã-Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(Direct Preference Optimization, DPO)„ÄÇ\n‰∏éPPO(Proximal Policy Optimization)ËøôÁßçÈúÄË¶ÅÂ•ñÂä±Ê®°Âûã„ÄÅ‰ª∑ÂÄºÊ®°ÂûãÁöÑRLÁÆóÊ≥ï‰∏çÂêåÔºõ\nDPOÈÄöËøáÊé®ÂØºPPOÂ•ñÂä±Ê®°ÂûãÁöÑÊòæÂºèËß£ÔºåÊääÂú®Á∫øÂ•ñÂä±Ê®°ÂûãÊç¢ÊàêÁ¶ªÁ∫øÊï∞ÊçÆÔºåRefÊ®°ÂûãËæìÂá∫ÂèØ‰ª•ÊèêÂâç‰øùÂ≠ò„ÄÇ\nDPOÊÄßËÉΩÂá†‰πé‰∏çÂèòÔºåÂè™Áî®Ë∑ë actor_model Âíå ref_model ‰∏§‰∏™Ê®°ÂûãÔºåÂ§ßÂ§ßËäÇÁúÅÊòæÂ≠òÂºÄÈîÄÂíåÂ¢ûÂä†ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ\nÊ≥®ÔºöRLHFËÆ≠ÁªÉÊ≠•È™§Âπ∂ÈùûÂøÖÈ°ªÔºåÊ≠§Ê≠•È™§Èöæ‰ª•ÊèêÂçáÊ®°Âûã‚ÄúÊô∫Âäõ‚ÄùËÄåÈÄöÂ∏∏‰ªÖÁî®‰∫éÊèêÂçáÊ®°ÂûãÁöÑ‚ÄúÁ§ºË≤å‚ÄùÔºåÊúâÂà©ÔºàÁ¨¶ÂêàÂÅèÂ•Ω„ÄÅÂáèÂ∞ëÊúâÂÆ≥ÂÜÖÂÆπÔºâ‰πüÊúâÂºäÔºàÊ†∑Êú¨Êî∂ÈõÜÊòÇË¥µ„ÄÅÂèçÈ¶àÂÅèÂ∑Æ„ÄÅÂ§öÊ†∑ÊÄßÊçüÂ§±Ôºâ„ÄÇ\ntorchrun --nproc_per_node 1 train_dpo.py\n# or\npython train_dpo.py\nËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò‰∏∫: rlhf_*.pthÔºà*\n‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ\n4. Áü•ËØÜËí∏È¶è(Knowledge Distillation, KD)\nÂú®ÂâçÈù¢ÁöÑÊâÄÊúâËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÊ®°ÂûãÂ∑≤ÁªèÂÆåÂÖ®ÂÖ∑Â§á‰∫ÜÂü∫Êú¨ËÉΩÂäõÔºåÈÄöÂ∏∏ÂèØ‰ª•Â≠¶ÊàêÂá∫Â∏à‰∫Ü„ÄÇ\nËÄåÁü•ËØÜËí∏È¶èÂèØ‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊâÄË∞ìÁü•ËØÜËí∏È¶èÔºåÂç≥Â≠¶ÁîüÊ®°ÂûãÈù¢ÂêëÊïôÂ∏àÊ®°ÂûãÂ≠¶‰π†„ÄÇ\nÊïôÂ∏àÊ®°ÂûãÈÄöÂ∏∏ÊòØÁªèËøáÂÖÖÂàÜËÆ≠ÁªÉÁöÑÂ§ßÊ®°ÂûãÔºåÂÖ∑ÊúâËæÉÈ´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ\nÂ≠¶ÁîüÊ®°ÂûãÊòØ‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊ®°ÂûãÔºåÁõÆÊ†áÊòØÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑË°å‰∏∫ÔºåËÄå‰∏çÊòØÁõ¥Êé•‰ªéÂéüÂßãÊï∞ÊçÆ‰∏≠Â≠¶‰π†„ÄÇ\nÂú®SFTÂ≠¶‰π†‰∏≠ÔºåÊ®°ÂûãÁöÑÁõÆÊ†áÊòØÊãüÂêàËØçTokenÂàÜÁ±ªÁ°¨Ê†áÁ≠æÔºàhard labelsÔºâÔºåÂç≥ÁúüÂÆûÁöÑÁ±ªÂà´Ê†áÁ≠æÔºàÂ¶Ç 0 Êàñ 6400Ôºâ„ÄÇ\nÂú®Áü•ËØÜËí∏È¶è‰∏≠ÔºåÊïôÂ∏àÊ®°ÂûãÁöÑsoftmaxÊ¶ÇÁéáÂàÜÂ∏ÉË¢´Áî®‰ΩúËΩØÊ†áÁ≠æÔºàsoft labelsÔºâ„ÄÇÂ∞èÊ®°Âûã‰ªÖÂ≠¶‰π†ËΩØÊ†áÁ≠æÔºåÂπ∂‰ΩøÁî®KL-LossÊù•‰ºòÂåñÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ\nÈÄö‰øóÂú∞ËØ¥ÔºåSFTÁõ¥Êé•Â≠¶‰π†ËÄÅÂ∏àÁªôÁöÑËß£È¢òÁ≠îÊ°à„ÄÇËÄåKDËøáÁ®ãÁõ∏ÂΩì‰∫é‚ÄúÊâìÂºÄ‚ÄùËÄÅÂ∏àËÅ™ÊòéÁöÑÂ§ßËÑëÔºåÂ∞ΩÂèØËÉΩÂú∞Ê®°‰ªøËÄÅÂ∏à‚ÄúÂ§ßËÑë‚ÄùÊÄùËÄÉÈóÆÈ¢òÁöÑÁ•ûÁªèÂÖÉÁä∂ÊÄÅ„ÄÇ\n‰æãÂ¶ÇÔºåÂΩìËÄÅÂ∏àÊ®°ÂûãËÆ°ÁÆó1+1=2Ëøô‰∏™ÈóÆÈ¢òÁöÑÊó∂ÂÄôÔºåÊúÄÂêé‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉaÁä∂ÊÄÅ‰∏∫0ÔºåÁ•ûÁªèÂÖÉbÁä∂ÊÄÅ‰∏∫100ÔºåÁ•ûÁªèÂÖÉcÁä∂ÊÄÅ‰∏∫-99...\nÂ≠¶ÁîüÊ®°ÂûãÈÄöËøáÂ§ßÈáèÊï∞ÊçÆÔºåÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÂ§ßËÑëÂÜÖÈÉ®ÁöÑËøêËΩ¨ËßÑÂæã„ÄÇËøô‰∏™ËøáÁ®ãÂç≥Áß∞‰πã‰∏∫ÔºöÁü•ËØÜËí∏È¶è„ÄÇ\nÁü•ËØÜËí∏È¶èÁöÑÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™ÔºöËÆ©Â∞èÊ®°Âûã‰ΩìÁßØÊõ¥Â∞èÁöÑÂêåÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ\nÁÑ∂ËÄåÈöèÁùÄLLMËØûÁîüÂíåÂèëÂ±ïÔºåÊ®°ÂûãËí∏È¶è‰∏ÄËØçË¢´ÂπøÊ≥õÊª•Áî®Ôºå‰ªéËÄå‰∫ßÁîü‰∫Ü‚ÄúÁôΩÁõí/ÈªëÁõí‚ÄùÁü•ËØÜËí∏È¶è‰∏§‰∏™Ê¥æÂà´„ÄÇ\nGPT-4ËøôÁßçÈó≠Ê∫êÊ®°ÂûãÔºåÁî±‰∫éÊó†Ê≥ïËé∑ÂèñÂÖ∂ÂÜÖÈÉ®ÁªìÊûÑÔºåÂõ†Ê≠§Âè™ËÉΩÈù¢ÂêëÂÆÉÊâÄËæìÂá∫ÁöÑÊï∞ÊçÆÂ≠¶‰π†ÔºåËøô‰∏™ËøáÁ®ãÁß∞‰πã‰∏∫ÈªëÁõíËí∏È¶èÔºå‰πüÊòØÂ§ßÊ®°ÂûãÊó∂‰ª£ÊúÄÊôÆÈÅçÁöÑÂÅöÊ≥ï„ÄÇ\nÈªëÁõíËí∏È¶è‰∏éSFTËøáÁ®ãÂÆåÂÖ®‰∏ÄËá¥ÔºåÂè™‰∏çËøáÊï∞ÊçÆÊòØ‰ªéÂ§ßÊ®°ÂûãÁöÑËæìÂá∫Êî∂ÈõÜÔºåÂõ†Ê≠§Âè™ÈúÄË¶ÅÂáÜÂ§áÊï∞ÊçÆÂπ∂‰∏îËøõ‰∏ÄÊ≠•FTÂç≥ÂèØ„ÄÇ\nÊ≥®ÊÑèÊõ¥ÊîπË¢´Âä†ËΩΩÁöÑÂü∫Á°ÄÊ®°Âûã‰∏∫full_sft_*.pthÔºåÂç≥Âü∫‰∫éÂæÆË∞ÉÊ®°ÂûãÂÅöËøõ‰∏ÄÊ≠•ÁöÑËí∏È¶èÂ≠¶‰π†„ÄÇ\n./dataset/sft_1024.jsonl‰∏é./dataset/sft_2048.jsonl ÂùáÊî∂ÈõÜËá™qwen2.5-7/72B-InstructÂ§ßÊ®°ÂûãÔºåÂèØÁõ¥Êé•Áî®‰∫éSFT‰ª•Ëé∑ÂèñQwenÁöÑÈÉ®ÂàÜË°å‰∏∫„ÄÇ\n# Ê≥®ÊÑèÈúÄË¶ÅÊõ¥Êîπtrain_full_sft.pyÊï∞ÊçÆÈõÜË∑ØÂæÑÔºå‰ª•Âèämax_seq_len\ntorchrun --nproc_per_node 1 train_full_sft.py\n# or\npython train_full_sft.py\nËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî100Ê≠•ÂêåÊ†∑‰øùÂ≠ò‰∏∫: full_sft_*.pthÔºà*‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ\nÊ≠§Â§ÑÂ∫îÂΩìÁùÄÈáç‰ªãÁªçMiniMindÂÆûÁé∞ÁöÑÁôΩÁõíËí∏È¶è‰ª£Á†Åtrain_distillation.pyÔºåÁî±‰∫éMiniMindÂêåÁ≥ªÂàóÊú¨Ë∫´Âπ∂‰∏çÂ≠òÂú®Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°ÂûãÔºåÂõ†Ê≠§ÁôΩÁõíËí∏È¶è‰ª£Á†Å‰ªÖ‰Ωú‰∏∫Â≠¶‰π†ÂèÇËÄÉ„ÄÇ\ntorchrun --nproc_per_node 1 train_distillation.py\n# or\npython train_distillation.py\n5. LoRA (Low-Rank Adaptation)\nLoRAÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàParameter-Efficient Fine-Tuning, PEFTÔºâÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøá‰ΩéÁß©ÂàÜËß£ÁöÑÊñπÂºèÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇ\nÁõ∏ÊØî‰∫éÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºàFull Fine-TuningÔºâÔºåLoRA Âè™ÈúÄË¶ÅÊõ¥Êñ∞Â∞ëÈáèÁöÑÂèÇÊï∞„ÄÇ\nLoRA ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºöÂú®Ê®°ÂûãÁöÑÊùÉÈáçÁü©Èòµ‰∏≠ÂºïÂÖ•‰ΩéÁß©ÂàÜËß£Ôºå‰ªÖÂØπ‰ΩéÁß©ÈÉ®ÂàÜËøõË°åÊõ¥Êñ∞ÔºåËÄå‰øùÊåÅÂéüÂßãÈ¢ÑËÆ≠ÁªÉÊùÉÈáç‰∏çÂèò„ÄÇ\n‰ª£Á†ÅÂèØËßÅ./model/model_lora.pyÂíåtrain_lora.pyÔºåÂÆåÂÖ®‰ªé0ÂÆûÁé∞LoRAÊµÅÁ®ãÔºå‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÁöÑÂ∞ÅË£Ö„ÄÇ\ntorchrun --nproc_per_node 1 train_lora.py\n# or\npython train_lora.py\nËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò‰∏∫: lora_xxx_*.pthÔºà*\n‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ\nÈùûÂ∏∏Â§öÁöÑ‰∫∫Âõ∞ÊÉëÔºåÂ¶Ç‰Ωï‰ΩøÊ®°ÂûãÂ≠¶‰ºöËá™Â∑±ÁßÅÊúâÈ¢ÜÂüüÁöÑÁü•ËØÜÔºüÂ¶Ç‰ΩïÂáÜÂ§áÊï∞ÊçÆÈõÜÔºüÂ¶Ç‰ΩïËøÅÁßªÈÄöÁî®È¢ÜÂüüÊ®°ÂûãÊâìÈÄ†ÂûÇÂüüÊ®°ÂûãÔºü\nËøôÈáå‰∏æÂá†‰∏™‰æãÂ≠êÔºåÂØπ‰∫éÈÄöÁî®Ê®°ÂûãÔºåÂåªÂ≠¶È¢ÜÂüüÁü•ËØÜÊ¨†Áº∫ÔºåÂèØ‰ª•Â∞ùËØïÂú®ÂéüÊúâÊ®°ÂûãÂü∫Á°Ä‰∏äÂä†ÂÖ•È¢ÜÂüüÁü•ËØÜÔºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ\nÂêåÊó∂ÔºåÊàë‰ª¨ÈÄöÂ∏∏‰∏çÂ∏åÊúõÂ≠¶‰ºöÈ¢ÜÂüüÁü•ËØÜÁöÑÂêåÊó∂ÊçüÂ§±ÂéüÊúâÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖ∂ÂÆÉËÉΩÂäõÔºåÊ≠§Êó∂LoRAÂèØ‰ª•ÂæàÂ•ΩÁöÑÊîπÂñÑËøô‰∏™ÈóÆÈ¢ò„ÄÇ\nÂè™ÈúÄË¶ÅÂáÜÂ§áÂ¶Ç‰∏ãÊ†ºÂºèÁöÑÂØπËØùÊï∞ÊçÆÈõÜÊîæÁΩÆÂà∞./dataset/lora_xxx.jsonlÔºåÂêØÂä® python train_lora.py\nËÆ≠ÁªÉÂç≥ÂèØÂæóÂà∞./out/lora/lora_xxx.pthÊñ∞Ê®°ÂûãÊùÉÈáç„ÄÇ\nÂåªÁñóÂú∫ÊôØ\n{\"conversations\": [{\"role\": \"user\", \"content\": \"ËØ∑ÈóÆÈ¢àÊ§éÁóÖÁöÑ‰∫∫ÊûïÂ§¥Â§öÈ´òÊâçÊúÄÂ•ΩÔºü\"}, {\"role\": \"assistant\", \"content\": \"È¢àÊ§éÁóÖÊÇ£ËÄÖÈÄâÊã©ÊûïÂ§¥ÁöÑÈ´òÂ∫¶Â∫îËØ•Ê†πÊçÆ...\"}]}\n{\"conversations\": [{\"role\": \"user\", \"content\": \"ËØ∑ÈóÆxxx\"}, {\"role\": \"assistant\", \"content\": \"xxx...\"}]}\nËá™ÊàëËÆ§Áü•Âú∫ÊôØ\n{\"conversations\": [{\"role\": \"user\", \"content\": \"‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\"}, {\"role\": \"assistant\", \"content\": \"ÊàëÂè´minimind...\"}]}\n{\"conversations\": [{\"role\": \"user\", \"content\": \"‰Ω†ÊòØË∞Å\"}, {\"role\": \"assistant\", \"content\": \"ÊàëÊòØ...\"}]}\nÊ≠§Êó∂„ÄêÂü∫Á°ÄÊ®°Âûã+LoRAÊ®°Âûã„ÄëÂç≥ÂèØËé∑ÂæóÂåªÁñóÂú∫ÊôØÊ®°ÂûãÂ¢ûÂº∫ÁöÑËÉΩÂäõÔºåÁõ∏ÂΩì‰∫é‰∏∫Âü∫Á°ÄÊ®°ÂûãÂ¢ûÂä†‰∫ÜLoRAÂ§ñÊåÇÔºåËøô‰∏™ËøáÁ®ãÂπ∂‰∏çÊçüÂ§±Âü∫Á°ÄÊ®°ÂûãÁöÑÊú¨Ë∫´ËÉΩÂäõ„ÄÇ\nÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáeval_model.pyËøõË°åÊ®°ÂûãËØÑ‰º∞ÊµãËØï„ÄÇ\n# Ê≥®ÊÑèÔºömodel_modeÂç≥ÈÄâÊã©Âü∫Á°ÄÊ®°ÂûãÁöÑÁ±ªÂûãÔºåËøôÂíåtrain_loraÊòØÂü∫‰∫éÂì™‰∏™Ê®°ÂûãËÆ≠ÁªÉÁöÑÁõ∏ÂÖ≥ÔºåÁ°Æ‰øùÁªü‰∏ÄÂç≥ÂèØ„ÄÇ\npython eval_model.py --lora_name 'lora_medical' --model_mode 2\nÂ∞èÊµãËØï\nüë∂: ÊàëÊúÄËøëÁªèÂ∏∏ÊÑüÂà∞Â§¥ÊôïÔºåÂèØËÉΩÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü\nü§ñÔ∏è: Â§¥ÊôïÂèØËÉΩÊòØÁî±‰∫éÂ§öÁßçÂéüÂõ†ÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºöÂøÉËÑèÁñæÁóÖ„ÄÅÁúºÁùõÊçü‰º§„ÄÅËøáÊïèÂèçÂ∫î„ÄÅÁöÆËÇ§ÈóÆÈ¢ò„ÄÅËøáÊïèÂèçÂ∫î„ÄÅÂéãÂäõ„ÄÅÁÑ¶Ëôë„ÄÅÁñ≤Âä≥„ÄÅËçØÁâ©ÂâØ‰ΩúÁî®„ÄÅÊÄßÂäüËÉΩÈöúÁ¢çÊàñËÄÖÂÖ∂‰ªñ...\n---\nüë∂: ‰Ω†ÂíåopenaiÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºü\nü§ñÔ∏è: ÊàëÊòØ MiniMindÔºå‰∏Ä‰∏™Áî± Jingyao Gong ÂºÄÂèëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã„ÄÇÊàëÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÁÆóÊ≥ïËÆ≠ÁªÉÊù•‰∏éÁî®Êà∑ËøõË°å‰∫§‰∫í„ÄÇ\nPSÔºöÂè™Ë¶ÅÊúâÊâÄÈúÄË¶ÅÁöÑÊï∞ÊçÆÈõÜÔºå‰πüÂèØ‰ª•full_sftÂÖ®ÂèÇÂæÆË∞ÉÔºàÈúÄË¶ÅËøõË°åÈÄöÁî®Áü•ËØÜÁöÑÊ∑∑ÂêàÈÖçÊØîÔºåÂê¶ÂàôËøáÊãüÂêàÈ¢ÜÂüüÊï∞ÊçÆ‰ºöËÆ©Ê®°ÂûãÂèòÂÇªÔºåÊçüÂ§±ÈÄöÁî®ÊÄßÔºâ\n6. ËÆ≠ÁªÉÊé®ÁêÜÊ®°Âûã (Reasoning Model)\nDeepSeek-R1ÂÆûÂú®Â§™ÁÅ´‰∫ÜÔºåÂá†‰πéÈáçÊñ∞ÊåáÊòé‰∫ÜÊú™Êù•LLMÁöÑÊñ∞ËåÉÂºè„ÄÇ\nËÆ∫ÊñáÊåáÂá∫>3BÁöÑÊ®°ÂûãÁªèÂéÜÂ§öÊ¨°ÂèçÂ§çÁöÑÂÜ∑ÂêØÂä®ÂíåRLÂ•ñÂä±ËÆ≠ÁªÉÊâçËÉΩËé∑ÂæóËÇâÁúºÂèØËßÅÁöÑÊé®ÁêÜËÉΩÂäõÊèêÂçá„ÄÇ\nÊúÄÂø´ÊúÄÁ®≥Â¶•ÊúÄÁªèÊµéÁöÑÂÅöÊ≥ïÔºå‰ª•ÂèäÊúÄËøëÁàÜÂèëÁöÑÂêÑÁßçÂêÑÊ†∑ÊâÄË∞ìÁöÑÊé®ÁêÜÊ®°ÂûãÂá†‰πéÈÉΩÊòØÁõ¥Êé•Èù¢ÂêëÊï∞ÊçÆËøõË°åËí∏È¶èËÆ≠ÁªÉÔºå\n‰ΩÜÁî±‰∫éÁº∫‰πèÊäÄÊúØÂê´ÈáèÔºåËí∏È¶èÊ¥æË¢´RLÊ¥æÁûß‰∏çËµ∑ÔºàhhhhÔºâ„ÄÇ\nÊú¨‰∫∫ËøÖÈÄüÂ∑≤ÁªèÂú®QwenÁ≥ªÂàó1.5BÂ∞èÊ®°Âûã‰∏äËøõË°å‰∫ÜÂ∞ùËØïÔºåÂæàÂø´Â§çÁé∞‰∫ÜZeroËøáÁ®ãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇ\nÁÑ∂ËÄå‰∏Ä‰∏™ÈÅóÊÜæÁöÑÂÖ±ËØÜÊòØÔºöÂèÇÊï∞Â§™Â∞èÁöÑÊ®°ÂûãÁõ¥Êé•ÈÄöËøáÂÜ∑ÂêØÂä®SFT+GRPOÂá†‰πé‰∏çÂèØËÉΩËé∑Âæó‰ªª‰ΩïÊé®ÁêÜÊïàÊûú„ÄÇ\nMiniMind2Á¨¨‰∏ÄÊó∂Èó¥Âè™ËÉΩÂùöÂÆö‰∏çÁßªÁöÑÈÄâÊã©ÂÅöËí∏È¶èÊ¥æÔºåÊó•ÂêéÂü∫‰∫é0.1BÊ®°ÂûãÁöÑRLÂ¶ÇÊûúÂêåÊ†∑ÂèñÂæóÂ∞èÂ∞èËøõÂ±ï‰ºöÊõ¥Êñ∞Ê≠§ÈÉ®ÂàÜÁöÑËÆ≠ÁªÉÊñπÊ°à„ÄÇ\nÂÅöËí∏È¶èÈúÄË¶ÅÂáÜÂ§áÁöÑ‰æùÁÑ∂ÊòØÂíåSFTÈò∂ÊÆµÂêåÊ†∑Ê†ºÂºèÁöÑÊï∞ÊçÆÂç≥ÂèØÔºåÊï∞ÊçÆÈõÜÊù•Ê∫êÂ∑≤Â¶Ç‰∏äÊñá‰ªãÁªç„ÄÇÊï∞ÊçÆÊ†ºÂºè‰æãÂ¶ÇÔºö\n{\n\"conversations\": [\n{\n\"role\": \"user\",\n\"content\": \"‰Ω†Â•ΩÔºåÊàëÊòØÂ∞èËä≥ÔºåÂæàÈ´òÂÖ¥ËÆ§ËØÜ‰Ω†„ÄÇ\"\n},\n{\n\"role\": \"assistant\",\n\"content\": \"<think>\\n‰Ω†Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1-Lite-PreviewÔºåÂæàÈ´òÂÖ¥‰∏∫ÊÇ®Êèê‰æõÊúçÂä°ÔºÅ\\n</think>\\n<answer>\\n‰Ω†Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1-Lite-PreviewÔºåÂæàÈ´òÂÖ¥‰∏∫ÊÇ®Êèê‰æõÊúçÂä°ÔºÅ\\n</answer>\"\n}\n]\n}\nÊé®ÁêÜÊ®°ÂûãR1ÁöÑÂõûÂ§çÊ®°ÊùøÊòØÔºö\n<think>\\nÊÄùËÄÉËøáÁ®ã\\n</think>\\n\n<answer>\\nÊúÄÁªàÂõûÁ≠î\\n</answer>\nËøôÂú®GRPO‰∏≠ÈÄöËøáËÆæÁΩÆËßÑÂàôÂ•ñÂä±ÂáΩÊï∞Á∫¶ÊùüÊ®°ÂûãÁ¨¶ÂêàÊÄùËÄÉÊ†áÁ≠æÂíåÂõûÂ§çÊ†áÁ≠æÔºàÂú®ÂÜ∑ÂêØÂä®Èù†ÂâçÁöÑÈò∂ÊÆµÂ•ñÂä±ÂÄºËÆæÁΩÆÂ∫îËØ•ÊèêÈ´ò‰∏Ä‰∫õÔºâ\nÂè¶‰∏Ä‰∏™ÈóÆÈ¢òÊòØËí∏È¶èËøáÁ®ãËôΩÁÑ∂ÂíåSFT‰∏ÄÊ†∑Ôºå‰ΩÜÂÆûÈ™åÁªìÊûúÊòØÊ®°ÂûãÈöæ‰ª•ÊØèÊ¨°ÈÉΩÁ¨¶ÂêàÊ®°ÊùøËßÑËåÉÁöÑÂõûÂ§çÔºåÂç≥ËÑ±Á¶ªÊÄùËÄÉÂíåÂõûÂ§çÊ†áÁ≠æÁ∫¶Êùü„ÄÇ\nËøôÈáåÁöÑÂ∞èÊäÄÂ∑ßÊòØÂ¢ûÂä†Ê†áËÆ∞‰ΩçÁΩÆtokenÁöÑÊçüÂ§±ÊÉ©ÁΩöÔºåËØ¶ËßÅtrain_distill_reason.py:\n# Âú® sp_ids ÂØπÂ∫îÁöÑ‰ΩçÁΩÆÂ¢ûÂä†È¢ùÂ§ñÁöÑÊÉ©ÁΩö\n...\nloss_mask[sp_ids] = 10 # ÊÉ©ÁΩöÁ≥ªÊï∞\nÂè¶Âè¶‰∏Ä‰∏™tipsÊòØÁî±‰∫éÊé®ÁêÜÊï∞ÊçÆÁî±‰∫éÂè™Á≠õÈÄâ‰∫Ü<1024ÈïøÂ∫¶ÁöÑÊï∞ÊçÆÔºåÂÖ∂‰∏≠Â§öËΩÆÂØπËØùÂíåËã±ÊñáÊï∞ÊçÆÂÅèÂ∞ëÔºå\nÂõ†Ê≠§r1_mix_1024.jsonlËøõË°å‰∫ÜÂ§ßÁ∫¶10kÊù°Â§öËΩÆÂØπËØù+Ëã±ÊñáÊï∞ÊçÆÁöÑÊ∑∑ÂêàÔºåÈò≤Ê≠¢Ê®°ÂûãÈÅóÂøò‰∏•Èáç„ÄÇ\nËÑöÊú¨ÈªòËÆ§Âü∫‰∫érlhfÂêéÁöÑÂü∫Ê®°ÂûãÂÅöÊé®ÁêÜËÉΩÂäõÁöÑËí∏È¶èÂæÆË∞ÉÔºå‰∏ãÈù¢Áõ¥Êé•ÂêØÂä®ËÆ≠ÁªÉÂç≥ÂèØÔºö\ntorchrun --nproc_per_node 1 train_distill_reason.py\n# or\npython train_distill_reason.py\nËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò‰∏∫: reason_*.pthÔºà*‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ\nÊµãËØï‰∏Ä‰∏ãÔºö\nüë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ\nü§ñÔ∏è: <think>\nÂóØÔºåÁî®Êà∑ËÆ©Êàë‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇÊàëÁé∞Âú®ÈúÄË¶ÅËß£Èáä‰∏Ä‰∏ãËá™Â∑±„ÄÇÈ¶ñÂÖàÔºåÊàëÂ∫îËØ•ÊòéÁ°ÆÁî®Êà∑ÊòØË∞ÅÔºåÊòØÂ≠¶Áîü„ÄÅËÅåÂú∫‰∫∫Â£´ËøòÊòØÊôÆÈÄö‰∫∫„ÄÇÁî®Êà∑ÂèØËÉΩÊòØ‰∏Ä‰∏™Â≠¶ÁîüÊàñËÄÖËÅåÂú∫‰∫∫Â£´ÔºåÊâÄ‰ª•‰ø°ÊÅØ‰º†ÈÄíÂ∫îËØ•ÊòØÂ∏ÆÂä©Â≠¶ÁîüÂíåËÅåÂú∫‰∫∫Â£´ÁöÑ‰ø°ÊÅØ„ÄÇ\nÁÑ∂ÂêéÔºåÊàëÈúÄË¶ÅÂõûÂ∫îÁî®Êà∑ÔºåËøôÂèØËÉΩÂåÖÊã¨‰ªãÁªçËá™Â∑±ÁöÑ‰ΩçÁΩÆ„ÄÅÂÖ¥Ë∂£Áà±Â•Ω„ÄÅËÅå‰∏öÁªèÂéÜÁ≠â‰ø°ÊÅØ„ÄÇÁî®Êà∑ËøòÂèØËÉΩÊÉ≥‰∫ÜËß£Ëá™Â∑±Âú®Âì™ÈáåÔºåÊØîÂ¶ÇËÅå‰∏öÂèëÂ±ï„ÄÅÂ∑•‰ΩúÁªèÈ™åÁ≠â„ÄÇ\nÊàëÂ∫îËØ•Ê≥®ÊÑèÁî®ËØçË¶ÅÊ≠£ÂºèÔºå‰∏çË¶ÅÂ§™Ê≠£ÂºèÔºåÂêåÊó∂Ë¶Å‰øùÊåÅÁÆÄÊ¥ÅÊòé‰∫Ü„ÄÇËøôÊ†∑Áî®Êà∑ÂÆπÊòìÁêÜËß£Ôºå‰πüÊñπ‰æø‰ªñ‰ª¨Êü•ÈòÖÁõ∏ÂÖ≥ËµÑÊñô„ÄÇ\nÊúÄÂêéÔºåÊàëÂ∫îËØ•ÊÄªÁªì‰∏Ä‰∏ãÔºåËÆ©Áî®Êà∑Áü•ÈÅìÊàëÊòØÂ¶Ç‰ΩïÂ∏ÆÂä©‰ªñ‰ª¨ÂÆåÊàê‰ªñ‰ª¨ÁöÑ‰ªªÂä°ÁöÑÔºåËøôÊ†∑‰ªñ‰ª¨ÊúâÊõ¥Â§öÂÖ≥‰∫éÊàëÁöÑ‰ø°ÊÅØ„ÄÇ\n</think>\n<answer>\nÊàëÊòØ**‚Äú‰Ω†ÁöÑÂêçÂ≠ó‚ÄùÔºå‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\n</answer>\nüë∂: ‰Ω†Êõ¥ÊìÖÈïøÂì™‰∏Ä‰∏™Â≠¶ÁßëÔºü\nü§ñÔ∏è: <think>\nÊÇ®Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1„ÄÇÂ¶ÇÊÇ®Êúâ‰ªª‰Ωï‰ªª‰ΩïÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÊàëÊâÄËÉΩ‰∏∫ÊÇ®Êèê‰æõÂ∏ÆÂä©„ÄÇ\n</think>\n<answer>\nÊÇ®Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1„ÄÇÂ¶ÇÊÇ®Êúâ‰ªª‰Ωï‰ªª‰ΩïÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÊàëÊâÄËÉΩ‰∏∫ÊÇ®Êèê‰æõÂ∏ÆÂä©„ÄÇ\n</answer>\n‚Ö£ Ê®°ÂûãÂèÇÊï∞ËÆæÂÆö\nüìãÂÖ≥‰∫éLLMÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºåÊúâ‰∏ÄÁØáÂæàÊúâÊÑèÊÄùÁöÑËÆ∫ÊñáMobileLLMÂÅö‰∫ÜËØ¶ÁªÜÁöÑÁ†îÁ©∂ÂíåÂÆûÈ™å„ÄÇ\nScaling LawÂú®Â∞èÊ®°Âûã‰∏≠ÊúâËá™Â∑±Áã¨ÁâπÁöÑËßÑÂæã„ÄÇ\nÂºïËµ∑TransformerÂèÇÊï∞ÊàêËßÑÊ®°ÂèòÂåñÁöÑÂèÇÊï∞Âá†‰πéÂè™ÂèñÂÜ≥‰∫éd_modelÂíån_layers„ÄÇ\nd_model‚Üë + n_layers‚Üì -> ÁüÆËÉñÂ≠ê\nd_model‚Üì + n_layers‚Üë -> Áò¶È´ò‰∏™\n2020Âπ¥ÊèêÂá∫Scaling LawÁöÑËÆ∫ÊñáËÆ§‰∏∫ÔºåËÆ≠ÁªÉÊï∞ÊçÆÈáè„ÄÅÂèÇÊï∞Èáè‰ª•ÂèäËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞ÊâçÊòØÂÜ≥ÂÆöÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåËÄåÊ®°ÂûãÊû∂ÊûÑÁöÑÂΩ±ÂìçÂá†‰πéÂèØ‰ª•ÂøΩËßÜ„ÄÇ\nÁÑ∂ËÄå‰ºº‰πéËøô‰∏™ÂÆöÂæãÂØπÂ∞èÊ®°ÂûãÂπ∂‰∏çÂÆåÂÖ®ÈÄÇÁî®„ÄÇ\nMobileLLMÊèêÂá∫Êû∂ÊûÑÁöÑÊ∑±Â∫¶ÊØîÂÆΩÂ∫¶Êõ¥ÈáçË¶ÅÔºå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁò¶Èïø„ÄçÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÊØî„ÄåÂÆΩËÄåÊµÖ„ÄçÊ®°ÂûãÊõ¥Â§öÁöÑÊäΩË±°Ê¶ÇÂøµ„ÄÇ\n‰æãÂ¶ÇÂΩìÊ®°ÂûãÂèÇÊï∞Âõ∫ÂÆöÂú®125MÊàñËÄÖ350MÊó∂Ôºå30ÔΩû42Â±ÇÁöÑ„ÄåÁã≠Èïø„ÄçÊ®°ÂûãÊòéÊòæÊØî12Â±ÇÂ∑¶Âè≥ÁöÑ„ÄåÁüÆËÉñ„ÄçÊ®°ÂûãÊúâÊõ¥‰ºòË∂äÁöÑÊÄßËÉΩÔºå\nÂú®Â∏∏ËØÜÊé®ÁêÜ„ÄÅÈóÆÁ≠î„ÄÅÈòÖËØªÁêÜËß£Á≠â8‰∏™Âü∫ÂáÜÊµãËØï‰∏äÈÉΩÊúâÁ±ª‰ººÁöÑË∂ãÂäø„ÄÇ\nËøôÂÖ∂ÂÆûÊòØÈùûÂ∏∏ÊúâË∂£ÁöÑÂèëÁé∞ÔºåÂõ†‰∏∫‰ª•ÂæÄ‰∏∫100MÂ∑¶Âè≥ÈáèÁ∫ßÁöÑÂ∞èÊ®°ÂûãËÆæËÆ°Êû∂ÊûÑÊó∂ÔºåÂá†‰πéÊ≤°‰∫∫Â∞ùËØïËøáÂè†Âä†Ë∂ÖËøá12Â±Ç„ÄÇ\nËøô‰∏éMiniMindÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÂèÇÊï∞ÈáèÂú®d_modelÂíån_layers‰πãÈó¥ËøõË°åË∞ÉÊï¥ÂÆûÈ™åËßÇÂØüÂà∞ÁöÑÊïàÊûúÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ\nÁÑ∂ËÄå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁ™Ñ„Äç‰πüÊòØÊúâÁª¥Â∫¶ÊûÅÈôêÁöÑÔºåÂΩìd_model<512Êó∂ÔºåËØçÂµåÂÖ•Áª¥Â∫¶ÂùçÂ°åÁöÑÂä£ÂäøÈùûÂ∏∏ÊòéÊòæÔºå\nÂ¢ûÂä†ÁöÑlayersÂπ∂‰∏çËÉΩÂº•Ë°•ËØçÂµåÂÖ•Âú®Âõ∫ÂÆöq_headÂ∏¶Êù•d_head‰∏çË∂≥ÁöÑÂä£Âäø„ÄÇ\nÂΩìd_model>1536Êó∂ÔºålayersÁöÑÂ¢ûÂä†‰ºº‰πéÊØîd_modelÁöÑ‰ºòÂÖàÁ∫ßÊõ¥È´òÔºåÊõ¥ËÉΩÂ∏¶Êù•ÂÖ∑Êúâ‚ÄúÊÄß‰ª∑ÊØî‚ÄùÁöÑÂèÇÊï∞->ÊïàÊûúÂ¢ûÁõä„ÄÇ\nÂõ†Ê≠§MiniMindËÆæÂÆösmallÊ®°Âûãdim=512Ôºån_layers=8Êù•Ëé∑ÂèñÁöÑ„ÄåÊûÅÂ∞è‰ΩìÁßØ<->Êõ¥Â•ΩÊïàÊûú„ÄçÁöÑÂπ≥Ë°°„ÄÇ\nËÆæÂÆödim=768Ôºån_layers=16Êù•Ëé∑ÂèñÊïàÊûúÁöÑÊõ¥Â§ßÊî∂ÁõäÔºåÊõ¥Âä†Á¨¶ÂêàÂ∞èÊ®°ÂûãScaling-LawÁöÑÂèòÂåñÊõ≤Á∫ø„ÄÇ\n‰Ωú‰∏∫ÂèÇËÄÉÔºåGPT3ÁöÑÂèÇÊï∞ËÆæÂÆöËßÅ‰∏ãË°®Ôºö\n‚Ö§ ËÆ≠ÁªÉÁªìÊûú\nMiniMind2 Ê®°ÂûãËÆ≠ÁªÉÊçüÂ§±Ëµ∞ÂäøÔºàÁî±‰∫éÊï∞ÊçÆÈõÜÂú®ËÆ≠ÁªÉÂêéÂèàÊõ¥Êñ∞Ê∏ÖÊ¥óÂ§öÊ¨°ÔºåÂõ†Ê≠§Loss‰ªÖ‰æõÂèÇËÄÉÔºâ\nmodels\npretrain (length-512)\nsft (length-512)\nMiniMind2-Small\nMiniMind2\nËÆ≠ÁªÉÂÆåÊàê-Ê®°ÂûãÂêàÈõÜ\nËÄÉËôëÂà∞Â§ö‰∫∫ÂèçÂ∫îÁôæÂ∫¶ÁΩëÁõòÈÄüÂ∫¶ÊÖ¢ÔºåMiniMind2Âèä‰ª•ÂêéÂÖ®ÈÉ®‰ΩøÁî®ModelScope/HuggingFaceÊâòÁÆ°„ÄÇ\n‚ë† PyTorchÂéüÁîüÊ®°Âûã\nMiniMind2Ê®°ÂûãÊùÉÈáç (ModelScope | HuggingFace)\nMiniMind-V1Ê®°ÂûãÊùÉÈáç (ÁôæÂ∫¶ÁΩëÁõò)\nTorchÊñá‰ª∂ÂëΩÂêçÂØπÁÖß\nModel Name\nparams\npretrain_model\nsft_model\nrl_model\nreason_model\nlora_model\nMiniMind2-small\n26M\npretrain_512.pth\nfull_sft_512.pth\nrlhf_512.pth\nreason_512.pth\nlora_xxx_512.pth\nMiniMind2-MoE\n145M\npretrain_640_moe.pth\nfull_sft_640_moe.pth\nrlhf_640_moe.pth\n-\n-\nMiniMind2\n104M\npretrain_768.pth\nfull_sft_768.pth\nrlhf_768.pth\nreason_768.pth\nlora_xxx_768.pth\nModel Name\nparams\npretrain_model\nÂçïËΩÆÂØπËØùsft\nÂ§öËΩÆÂØπËØùsft\nrl_model\nminimind-v1-small\n26M\npretrain_512.pth\nsingle_chat/full_sft_512.pth\nmulti_chat/full_sft_512.pth\nrl_512.pth\nminimind-v1-moe\n4√ó26M\npretrain_512_moe.pth\nsingle_chat/full_sft_512_moe.pth\nmulti_chat/full_sft_512_moe.pth\n-\nminimind-v1\n108M\npretrain_768.pth\nsingle_chat/full_sft_768.pth\nmulti_chat/full_sft_768.pth\nrl_768.pth\n‚ë° TransformersÊ®°Âûã\nMiniMindÁ≥ªÂàó (ModelScope\n| HuggingFace)\nüìå Eval\n‚Ö† RLHFÂØπÊØîSFTÁØá\nÊµãËØïÂü∫‰∫éfull_sftÂíårlhfÁöÑMiniMind2Á≥ªÂàóÊ®°ÂûãÂØπÊØîÔºåÊµãËØïÈöèÊú∫ÁßçÂ≠êÂùáÂõ∫ÂÆö\nÈóÆÁ≠îÂÆûÊµã\n[A0] MiniMind2Ôºöfull_sft_640\n[A1] MiniMind2Ôºörlhf_640\n[Q]: ‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\n[A0]: ÊàëÊòØ‰∏Ä‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂèØ‰ª•ÂõûÁ≠îÂêÑÁßçÈóÆÈ¢òÔºåÊèê‰æõ‰ø°ÊÅØÂíåÂ∏ÆÂä©„ÄÇÊàëÂèØ‰ª•ÂõûÁ≠îÂêÑÁßçÈóÆÈ¢òÔºåÊèê‰æõ‰ø°ÊÅØÂíåÂ∏ÆÂä©ÔºåÂ∏ÆÂä©‰Ω†Ëß£ÂÜ≥ÂêÑÁßçÈóÆÈ¢ò„ÄÇ\n[A1]: ÊàëÊòØ‰∏ÄÁßçÂü∫‰∫éÊñáÊú¨ÁöÑAIÊ®°ÂûãÔºåÊàëÂπ∂Ê≤°Êúâ‰∏™‰∫∫ÁªèÂéÜÊàñÊÉÖÊÑüÔºåÊâÄ‰ª•Êó†Ê≥ïÊÑüÂèóÂíå‰ΩìÈ™åÊÉÖÊÑü„ÄÇÊàëÁöÑËÆæËÆ°ÁõÆÁöÑÊòØ‰∏∫‰∫ÜÂ∏ÆÂä©Áî®Êà∑Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØÂíåËøõË°åÂØπËØù„ÄÇÂ¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÈóÆÈ¢òÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºåËØ∑ÈöèÊó∂ÂëäËØâÊàëÔºÅ\n[Q]: ‰Ω†Êõ¥ÊìÖÈïøÂì™‰∏Ä‰∏™Â≠¶ÁßëÔºü\n[A0]: ‰Ωú‰∏∫‰∏Ä‰∏™AIÊ®°ÂûãÔºåÊàëÂπ∂Ê≤°ÊúâÁúüÊ≠£ÁöÑ‚ÄúÊìÖÈïø‚ÄùËøô‰∏™Â≠¶Áßë„ÄÇÊàëË¢´ËÆæËÆ°ÊàêËÉΩÂ§üÊèê‰æõ‰ø°ÊÅØ„ÄÅËß£Á≠îÈóÆÈ¢ò„ÄÅËøõË°åÂØπËØù„ÄÅÊâßË°å‰ªªÂä°Âíå‰∏éÁî®Êà∑ËøõË°åÂØπËØù„ÄÇ‰∏çËøáÔºåÊàëÂèØ‰ª•Êèê‰æõ‰ø°ÊÅØ„ÄÅËß£Á≠îÈóÆÈ¢ò„ÄÅËøõË°åÂØπËØù„ÄÅÊèê‰æõÂª∫ËÆÆ„ÄÅÂ§ÑÁêÜÈóÆÈ¢ò„ÄÅËøõË°åÂØπËØù„ÄÅÊèê‰æõÂª∫ËÆÆ„ÄÅÂ≠¶‰π†Êñ∞Áü•ËØÜ„ÄÅÁºñÂÜô‰ª£Á†Å„ÄÅÂõûÁ≠îÈóÆÈ¢ò„ÄÅÊèê‰æõÂª∫ËÆÆÁ≠âÂ§öÁßçÂ≠¶ÁßëÁöÑÂäüËÉΩ„ÄÇÂ¶ÇÊûú‰Ω†ÊúâÂÖ∑‰ΩìÁöÑÂ≠¶ÁßëÈúÄÊ±ÇÊàñ‰∏ªÈ¢òÔºåÊ¨¢ËøéÊèê‰æõËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n[A1]: ‰Ωú‰∏∫AIÔºåÊàëÂπ∂Ê≤°Êúâ‰∏™‰∫∫ËÉΩÂäõÊàñÂÖ¥Ë∂£Ôºå‰ΩÜÊàëÂèØ‰ª•Êèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éÊï∞Â≠¶„ÄÅÁßëÂ≠¶„ÄÅËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÅËØ≠Ë®ÄÂ≠¶„ÄÅÂøÉÁêÜÂ≠¶Á≠âÂ≠¶ÁßëÁöÑÁü•ËØÜ„ÄÇÂ¶ÇÊûú‰Ω†ÂØπÊï∞Â≠¶ÊÑüÂÖ¥Ë∂£ÔºåÊàëÂèØ‰ª•Êèê‰æõ‰∏Ä‰∫õÂü∫Á°ÄÁü•ËØÜÔºå‰æãÂ¶Ç‰ª£Êï∞„ÄÅÂá†‰Ωï„ÄÅÊ¶ÇÁéá„ÄÅÁªüËÆ°Â≠¶Á≠â„ÄÇÂ¶ÇÊûú‰Ω†ÈúÄË¶ÅÊõ¥Ê∑±ÂÖ•ÁöÑÂ≠¶‰π†ÔºåÊàë‰πüÂèØ‰ª•Êèê‰æõÁõ∏ÂÖ≥ÁöÑÂú®Á∫øËØæÁ®ã„ÄÅ‰π¶Á±ç„ÄÅËÆ∫ÊñáÁ≠âËµÑÊ∫ê„ÄÇËØ∑ÂëäËØâÊàë‰Ω†ÂÖ∑‰ΩìÈúÄË¶Å‰ªÄ‰πàÂ∏ÆÂä©ÔºåÊàë‰ºöÂ∞ΩÂäõÊèê‰æõÂ∏ÆÂä©„ÄÇ\n[Q]: È≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÊòØÂ¶Ç‰ΩïÊâπÂà§Â∞ÅÂª∫Á§ºÊïôÁöÑÔºü\n[A0]: È≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÊòØ‰∏ÄÈÉ®ËÆ≤Ëø∞‰∫ÜÈ≤ÅËøÖÂØπ‰∫∫Á±ª‰ª∑ÂÄºÂíåÈÅìÂæ∑Ë¥£‰ªªÁöÑÂÄ°ÂØº„ÄÇÂú®ËøôÈÉ®‰ΩúÂìÅ‰∏≠ÔºåÈ≤ÅËøÖÈÄöËøáÁªÜËÖªÁöÑÁ¨îËß¶ÔºåÊé¢ËÆ®‰∫Ü‰∫∫ÊÄß„ÄÅÈÅìÂæ∑„ÄÅÊ≠£‰πâ„ÄÅÁâ∫Áâ≤‰∏éË¥£‰ªªÁ≠âÊ∑±ÂàªÁöÑ‰∏ªÈ¢ò„ÄÇ‰ª•‰∏ãÊòØÈ≤ÅËøÖÂú®„ÄäÁãÇ‰∫∫Êó•ËÆ∞„Äã‰∏≠Â¶Ç‰ΩïÊâπÂà§Â∞ÅÂª∫Á§ºÊïôÁöÑÂá†‰∏™ÊñπÈù¢Ôºö1. **Á§æ‰ºöÁé∞ÂÆû‰∏éÈÅìÂæ∑ËßÇÂøµ**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊé¢ËÆ®‰∫ÜÁ§æ‰ºöÁöÑ‰∏çÂπ≥Á≠â„ÄÅÊùÉÂäõÁöÑÊµÅÂä®„ÄÅ‰∫∫ÊÄßÁöÑÊºîÂèòÁ≠âÁ§æ‰ºöÈóÆÈ¢ò„ÄÇ‰ªñËÆ§‰∏∫ÔºåÁ§æ‰ºö‰∏çÂπ≥Á≠â‰∏ç‰ªÖ‰ªÖÊòØ‰∏çÂÖ¨Ê≠£ÁöÑÔºåÊõ¥ÊòØÊØè‰∏™‰∫∫Âú®Á§æ‰ºö‰∏≠ÁöÑ‰∏çÂÖ¨Ê≠£„ÄÇ‰ªñÂëºÂêÅÁ§æ‰ºöÂêÑÁïåÂÖ±ÂêåÂä™ÂäõÔºåÈÄöËøáÊïôËÇ≤„ÄÅÂÆ£‰º†„ÄÅÊ≥ïÂæãÁ≠âÊâãÊÆµÔºå‰∏∫ÊûÑÂª∫ÂíåË∞êÁ§æ‰ºöÂÅöÂá∫Ë¥°ÁåÆ„ÄÇ2. **ÈÅìÂæ∑‰∏éË¥£‰ªª**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÂ±ïÁé∞‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ‰ªñÊèêÂÄ°ÈÄöËøáÈÅìÂæ∑ÊïôËÇ≤Êù•ÂüπÂÖª‰∫∫‰ª¨ÁöÑÈÅìÂæ∑ÂìÅË¥®ÔºåÂêåÊó∂‰πüÂº∫Ë∞É‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫ÜÁ§æ‰ºöÁöÑ‰∏çÂπ≥Á≠âÂíå‰∫∫ÊÄßÁöÑÂ§çÊùÇÊÄßÔºå‰ΩìÁé∞‰∫ÜÂØπ‰∫∫ÊÄßÁöÑÊ∑±ÂàªÁêÜËß£„ÄÇ3. **ÈÅìÂæ∑‰∏éË¥£‰ªª**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÂº∫Ë∞É‰∫ÜÈÅìÂæ∑Ë¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ‰ªñÊèêÂÄ°ÈÄöËøáËá™Â∑±ÁöÑË°å‰∏∫Êù•‰ΩìÁé∞ÂØπÁ§æ‰ºöÁöÑÂ∞äÈáçÂíåÂØπ‰∫∫ÊÄßÁöÑÊ∑±ÂàªÁêÜËß£„ÄÇÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÂº∫Ë∞É‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ4. **ÈÅìÂæ∑‰∏éÈÅìÂæ∑**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÁúüÊ≠£ÊÑè‰πâ„ÄÇÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÊú¨Ë¥®ÔºåÂº∫Ë∞É‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ5. **‰∏™‰∫∫‰∏éÁ§æ‰ºö**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫Ü‰∏™‰∫∫‰∏éÁ§æ‰ºö‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇ‰ªñÂëºÂêÅÁ§æ‰ºöÂêÑÁïåÂÖ±ÂêåÂä™ÂäõÔºåÈÄöËøáÊïôËÇ≤ÂíåÂÆ£‰º†Ôºå‰∏∫ÊûÑÂª∫ÂíåË∞êÁ§æ‰ºöÂÅöÂá∫Ë¥°ÁåÆ„ÄÇÈ≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÈÄöËøáÂÖ∂‰∏∞ÂØåÁöÑÊïÖ‰∫ãÂíåÊ∑±ÂàªÁöÑÈÅìÂæ∑ÂØìË®ÄÔºå‰º†Ëææ‰∫ÜÂÖ≥‰∫é‰∫∫ÊÄß„ÄÅÈÅìÂæ∑„ÄÅÊ≠£‰πâ„ÄÅÈÅìÂæ∑‰ª•Âèä‰∏™‰∫∫‰∏éÁ§æ‰ºöÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇËøôÈÉ®‰ΩúÂìÅ‰∏ç‰ªÖÂú®ÊñáÂ≠¶‰ª∑ÂÄº‰∏äÂèñÂæó‰∫ÜÂ∑®Â§ßÊàêÂ∞±ÔºåÊõ¥Âú®ÊÄùÊÉ≥Â±ÇÈù¢‰∏äÂêØÂèë‰∫ÜÂêéÊù•ÁöÑÊñáÂ≠¶Âàõ‰ΩúÔºåÂØπÂêé‰∏ñ‰∫ßÁîü‰∫ÜÊ∑±ËøúÁöÑÂΩ±Âìç„ÄÇ\n[A1]: È≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÊòØ‰∏ÄÈÉ®ÊèèÁªòÊàò‰∫âÂíåËá™Áî±Á≤æÁ•ûÁöÑ‰ΩúÂìÅÔºåÂÆÉÊâπÂà§‰∫ÜÂ∞ÅÂª∫Á§æ‰ºöÁöÑÁßçÁßçÈóÆÈ¢òÂíåÂÜ≤Á™ÅÔºåÂèçÊò†‰∫ÜÁ§æ‰ºöÁöÑ‰∏çÂπ≥Á≠âÂíå‰∫∫ÊÄßÊú¨Ë¥®„ÄÇÂú®ËøôÈÉ®‰ΩúÂìÅ‰∏≠ÔºåÈ≤ÅËøÖÈÄöËøáÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑËØ∏Â§öÁé∞Ë±°ËøõË°åÊâπÂà§ÔºåÂ¶Ç‚ÄúÂºóÈõ∑Âæ∑ÈáåÂÖã¬∑Á∫¶Áø∞ÈÄäÁöÑ‚ÄúÈπ∞Ëø™‰º¶‚Äù„ÄÅ‚ÄúÁ∫≥ÊâéÁâπÁöÑ‚Äù„ÄÅ‚ÄúË¥ùÂÖãÊ±âÂßÜ‚Äù„ÄÅ‚Äú‰πîÊ≤ª¬∑Â®ÅÂ∞îÈÄä‚ÄùÁ≠âÔºåÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑÁßçÁßçÈóÆÈ¢òËøõË°å‰∫ÜÊâπÂà§ÔºåÂº∫Ë∞É‰∫ÜÂ∞ÅÂª∫Á§æ‰ºöÁöÑ‰∏çÂπ≥Á≠â„ÄÅ‰∫∫ÊÄßÊú¨Ë¥®‰ª•Âèä‰∫∫ÊÄßÊú¨Ë¥®„ÄÇÈ≤ÅËøÖÈÄöËøá‰ªñÁöÑ‚ÄúÈπ∞Ëø™‰º¶‚Äù„ÄÅ‚ÄúÁ∫≥ÊâéÁâπÁöÑ‚Äù„ÄÅ‚ÄúË¥ùÂÖãÊ±âÂßÜ‚Äù„ÄÅ‚Äú‰πîÊ≤ª¬∑Â®ÅÂ∞îÈÄä‚ÄùÁ≠âÔºåÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑÁßçÁßçÈóÆÈ¢òËøõË°å‰∫ÜÊâπÂà§„ÄÇÂú®Â∞èËØ¥‰∏≠ÔºåÈ≤ÅËøÖÈÄöËøáÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑËØ∏Â§öÁé∞Ë±°ËøõË°åÊâπÂà§ÔºåÊè≠Á§∫‰∫ÜÂ∞ÅÂª∫Á§æ‰ºöÁöÑËÖêË¥•„ÄÅ‰∫∫ÊÄßÊú¨Ë¥®‰ª•Âèä‰∫∫ÊÄßÊú¨Ë¥®ÁöÑÂ§çÊùÇÊÄß„ÄÇËøôÁßçÊâπÂà§‰ΩìÁé∞‰∫ÜÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑÂ∞äÈáçÂíåÂØπ‰∫∫ÊÄßÁöÑÂ§çÊùÇÊÄßÔºåÂêåÊó∂‰πüÂèçÊò†‰∫ÜÁé∞‰ª£Á§æ‰ºö‰∏≠ÊôÆÈÅçÂ≠òÂú®ÁöÑ‰∏çÂπ≥Á≠âÂíå‰∫∫ÊÄßÊú¨Ë¥®„ÄÇÈ≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„Äã‰∏ç‰ªÖÊòØ‰∏ÄÈÉ®ÂÖ≥‰∫éÂ∞ÅÂª∫Á§æ‰ºöÁöÑÂ∞èËØ¥Ôºå‰πüÊòØ‰∏ÄÈÉ®ÂÖ∑ÊúâÊ∑±ÂàªÁ§æ‰ºöÊÑè‰πâÁöÑ‰ΩúÂìÅÔºåÂØπÂêé‰∏ñÁöÑÂÆ°Âà§ÂíåÊâπÂà§Ëµ∑Âà∞‰∫ÜÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ\n[Q]: ÊàëÂí≥ÂóΩÂ∑≤ÁªèÊåÅÁª≠‰∫Ü‰∏§Âë®ÔºåÈúÄË¶ÅÂéªÂåªÈô¢Ê£ÄÊü•ÂêóÔºü\n[A0]: Âí≥ÂóΩÁ°ÆÂÆûÈúÄË¶Å‰∏ÄÂÆöÁöÑÊ£ÄÊü•Ôºå‰ΩÜÂ¶ÇÊûúÊÇ®ÁöÑÂí≥ÂóΩÊåÅÁª≠Êó∂Èó¥ËøáÈïøÔºåÊàñËÄÖÊúâÂÖ∂‰ªñ‰∏çÈÄÇÁóáÁä∂ÔºàÂ¶ÇÊåÅÁª≠Âí≥ÂóΩ„ÄÅÂëºÂê∏Âõ∞ÈöæÁ≠âÔºâÔºåÂª∫ËÆÆÂ∞ΩÂø´Â∞±Âåª„ÄÇÂåªÁîü‰ºöÊ†πÊçÆÊÇ®ÁöÑÁóáÁä∂„ÄÅ‰ΩìÊ£ÄÁªìÊûúÂíåÂèØËÉΩÁöÑÂΩ±ÂÉèÂ≠¶Ê£ÄÊü•ÔºàÂ¶ÇXÂÖâ„ÄÅMRIÊàñCTÊâ´ÊèèÔºâÊù•ËØäÊñ≠ÊÇ®ÁöÑÁóáÁä∂„ÄÇÂú®Á≠âÂæÖÂ∞±ÂåªÊúüÈó¥ÔºåÊÇ®ÂèØ‰ª•Â∞ùËØï‰ª•‰∏ãÊñπÊ≥ïÊù•Â∏ÆÂä©ÁºìËß£ÁóáÁä∂Ôºö1. **‰ºëÊÅØ**ÔºöÁ°Æ‰øùÊÇ®ÊúâË∂≥Â§üÁöÑÁù°Áú†„ÄÇ2. **‰øùÊåÅÊ∞¥ÂàÜ**ÔºöËÑ±Ê∞¥ÂèØËÉΩ‰ºöÂä†ÈáçÂí≥ÂóΩ„ÄÇ3. **ÁÉ≠Êï∑**Ôºö‰ΩøÁî®ÁÉ≠Êï∑Ë¢ãÊàñÁÉ≠ÊØõÂ∑æÊï∑Âú®ÁñºÁóõÁöÑÈÉ®‰ΩçÔºåÊúâÂä©‰∫éÁºìËß£ÂñâÂíôÁóõ„ÄÇ4. **ÈÅøÂÖçÂà∫ÊøÄ**ÔºöÈÅøÂÖç‰ΩøÁî®ÂèØËÉΩÂà∫ÊøÄÂñâÂíôÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÂ¶ÇÊüê‰∫õÂåñÂ≠¶Áâ©Ë¥®„ÄÇ5. **‰øùÊåÅÂÆ§ÂÜÖÊπøÊ∂¶**ÔºöÁ°Æ‰øùÊÇ®ÁöÑÂçßÂÆ§ÊúâËâØÂ•ΩÁöÑÈÄöÈ£éÔºåÂèØ‰ª•‰ΩøÁî®Âä†ÊπøÂô®Â∏ÆÂä©‰øùÊåÅÁ©∫Ê∞îÊπøÂ∫¶„ÄÇ6. **‰øùÊåÅÂÆ§ÂÜÖÊπøÂ∫¶**Ôºö‰ΩøÁî®Âä†ÊπøÂô®ÂèØ‰ª•Â∏ÆÂä©ÁºìËß£ÂñâÂíôÂπ≤Áá•ÂíåÂí≥ÂóΩ„ÄÇÂ¶ÇÊûúÁóáÁä∂ÊåÅÁª≠‰∏çÁºìËß£ÊàñÊúâÂÖ∂‰ªñ‰∏çÈÄÇÁóáÁä∂ÔºåËØ∑Á´ãÂç≥Â∞±Âåª„ÄÇÂ∏åÊúõÊÇ®Êó©Êó•Â∫∑Â§çÔºÅ\n[A1]: Â¶ÇÊûúÊÇ®ÁöÑÂí≥ÂóΩÊåÅÁª≠‰∫Ü‰∏§Âë®ÔºåÈÄöÂ∏∏Âª∫ËÆÆÊÇ®Â∞ΩÂø´Â∞±ÂåªÔºåÂõ†‰∏∫ËøôÂèØËÉΩ‰ºöÂΩ±ÂìçÊÇ®ÁöÑÂëºÂê∏ÂíåÂøÉË∑≥„ÄÇÊÇ®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊ≠•È™§Êù•Ê£ÄÊü•Âí≥ÂóΩÔºö1. **‰ºëÊÅØ**ÔºöÁî®Á∫∏Â∑æÊàñËÇòÈÉ®ËΩªËΩªÊåâÂéãÂí≥ÂóΩÁöÑ‰ΩçÁΩÆ„ÄÇ2. **‰ΩøÁî®ËçØÁâ©**ÔºöÂú®ÂèØËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®ËçØÁâ©ÔºàÂ¶ÇÊäóËøáÊïèËçØ„ÄÅÊäóËøáÊïèËçØÁ≠âÔºâÊù•ÁºìËß£Âí≥ÂóΩ„ÄÇËØ∑Âú®ÂåªÁîüÁöÑÊåáÂØº‰∏ã‰ΩøÁî®ËçØÁâ©„ÄÇ3. **ÁÉ≠Êï∑**ÔºöÂú®Âí≥ÂóΩÊúüÈó¥ÔºåÂèØ‰ª•‰ΩøÁî®ÁÉ≠Êï∑Êù•ÁºìËß£ÁñºÁóõÂíåËÇøËÉÄ„ÄÇ4. **ËçØÁâ©Ê≤ªÁñó**ÔºöÂ¶ÇÊûúÂí≥ÂóΩÊåÅÁª≠‰∫Ü‰∏§Âë®ÔºåÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∫õÊ∂àÁÇéËçØÊàñÊäóËøáÊïèËçØÊù•ÁºìËß£ÁóáÁä∂„ÄÇËØ∑Âä°ÂøÖÂú®ÂåªÁîüÁöÑÊåáÂØº‰∏ã‰ΩøÁî®„ÄÇ5. **‰øùÊåÅÂÅ•Â∫∑ÁöÑÁîüÊ¥ªÊñπÂºè**Ôºö‰øùÊåÅÂÖÖË∂≥ÁöÑÁù°Áú†„ÄÅÂùáË°°ÁöÑÈ•ÆÈ£ü„ÄÅÈÄÇÈáèÁöÑËøêÂä®ÂíåÈÅøÂÖçÂê∏ÁÉüÂíåÈôêÈÖíÔºåËøô‰∫õÈÉΩÊúâÂä©‰∫éÊèêÈ´òÂí≥ÂóΩÁöÑÂá†Áéá„ÄÇËØ∑ËÆ∞‰ΩèÔºåËøô‰∫õÂª∫ËÆÆ‰∏çËÉΩÊõø‰ª£‰∏ì‰∏öÂåªÁñóÊÑèËßÅ„ÄÇÂ¶ÇÊûúÊÇ®ÁöÑÁóáÁä∂ÊåÅÁª≠ÊàñÂä†ÈáçÔºåÂº∫ÁÉàÂª∫ËÆÆÊÇ®Â∞ΩÂø´Â∞±Âåª„ÄÇ\nüëâÊïàÊûúÊÄªÁªì\nfull_sftÊ®°ÂûãÂú®ÁÆÄÊ¥ÅÊÄßÂíå‰ø°ÊÅØÂáÜÁ°ÆÊÄßÊñπÈù¢Ë°®Áé∞Êõ¥Â•ΩÔºõrlhfÊ®°ÂûãÂú®ÂõûÁ≠î‰∏≠ÂÄæÂêë‰∫éÊèê‰æõÊõ¥Â§öÁöÑËÉåÊôØ‰ø°ÊÅØÔºå‰ΩÜ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÊúâÂæÖÊîπËøõ„ÄÇ\nÊÄªÁöÑÊù•ËØ¥RLHFÂêéÁöÑÊ®°ÂûãÂÄæÂêë‰∫éÂ≠¶‰π†ÔºöËØ¥Êõ¥Â§öÊúâÁ§ºË≤å‰ΩÜÊó†Áî®ÁöÑÂ∫üËØùËÆ®Â•Ω‚ÄúÂØπËØù‚ÄùÊú¨Ë∫´ÔºåËÄåÂØπ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÂàôÊúâËΩªÂæÆÊçüÂ§±„ÄÇ\nÂ§©‰∏ãÊ≤°ÊúâÂÖçË¥πÁöÑÂçàÈ§êÔºåËøòÈúÄË¶ÅÁªßÁª≠ÊèêÂçáRLHFÊï∞ÊçÆÈõÜÁöÑË¥®ÈáèÔºå‰πüË¶ÅÊé•ÂèóÊ®°ÂûãËÉΩÂäõÊó†Ê≥ïÈÅøÂÖçÁöÑÊçüÂ§±(Á®ãÂ∫¶ÊúâËΩªÈáç)„ÄÇ\nDPOÂíåÂú®Á∫øPPOÁöÑÂå∫Âà´Âú®‰∫érejectÂíåchosenÈÉΩÊòØÁ¶ªÁ∫øÂáÜÂ§áÁöÑÔºåÂíåminimindÊ®°ÂûãÊú¨Ë∫´ÁöÑËæìÂá∫ÂøÖÁÑ∂Â≠òÂú®ÂæàÂ§ßÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇ„ÄÇ\nÈÄö‰øóÂú∞ËØ¥DPOÁÆóÊ≥ï‰ΩøÊ®°ÂûãËßÇÁúã‰πí‰πìÁêÉ‰∏ñÁïåÂÜ†ÂÜõÁöÑÊâìÊ≥ï„ÄåÂΩïÂÉè„ÄçËøõË°åRLÔºåËÄå‰∏çÊòØÂÉèPPO‰∏ÄÊ†∑ËØ∑rewardÊ®°ÂûãÂÅö„ÄåÊïôÁªÉ„ÄçÁ∫†Ê≠£Ëá™Â∑±ÁöÑÊâìÊ≥ïËøõË°åRL„ÄÇ\n‚Ö° ‰∏ªËßÇÊ†∑‰æãÊµãËØÑ\nüèÉ‰ª•‰∏ãÊµãËØï‰∫é2025-02-09ÂÆåÊàêÔºåÊ≠§Êó•ÊúüÂêéÂèëÂ∏ÉÁöÑÊñ∞Ê®°ÂûãÔºåÊó†ÁâπÊÆäÈúÄË¶ÅÊó∂Â∞Ü‰∏çÂä†ÂÖ•ÊµãËØï„ÄÇ\n[A] MiniMind2 (0.1B)\n[B] MiniMind2-MoE (0.15B)\n[C] MiniMind2-Small (0.02B)\n[D] minimind-v1-small(0.02B)\n[E] minimind-v1-moe(0.1B)\n[F] minimind-v1(0.1B)\n[G] baby-llama2-chinese(0.2B)\n[H] chatlm-mini-chinese(0.2B)\n[Q]: ‰Ω†Áü•ÈÅìÈïøÊ±üÂêóÔºü\n[A]: ÈïøÊ±üÔºå‰Ωç‰∫é‰∏≠ÂõΩ‰∏úÂçóÈÉ®ÔºåÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏ÄÈïøÊ≤≥ÔºåÂÖ®Èïø6300Â§öÂÖ¨ÈáåÔºåÊµÅÁªè11‰∏™ÁúÅ‰ªΩÔºåÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºå‰πüÊòØ‰∏≠ÂõΩÁöÑ‚ÄúÊØç‰∫≤Ê≤≥‚Äù„ÄÇÈïøÊ±üÁöÑÂéÜÂè≤ÂèØ‰ª•ËøΩÊ∫ØÂà∞ÂÖ¨ÂÖÉÂâç2000Â§öÂπ¥ÁöÑÂïÜÊúùÊó∂ÊúüÔºåÂΩìÊó∂ÁöÑÂïÜÊúùÊòØÂïÜÊúùÁöÑÂêéË£î„ÄÇÂïÜÊúùÊó∂ÊúüÔºåÈïøÊ±üÁöÑÁªèÊµéÂèëÂ±ïËææÂà∞‰∫ÜÈ´òÂ≥∞ÔºåÊòØ‰∏≠ÂõΩÊúÄÈáçË¶ÅÁöÑÂÜú‰∏öÂíåÂ∑•‰∏ö‰∏≠ÂøÉ‰πã‰∏Ä„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÁªèÊµé‰∏ªË¶Å‰ª•ÂÜú‰∏ö‰∏∫‰∏ªÔºåÈïøÊ±üÊ∏î‰∏öÂç†ÊçÆ‰∫Ü‰∏ªË¶Å‰∫ß‰∏öÔºåÂ¶ÇÈíìÈ±º„ÄÅÁåéÈ±ºÁ≠â„ÄÇÈïøÊ±üÁöÑÊ∞¥ËµÑÊ∫ê‰πüÈùûÂ∏∏‰∏∞ÂØåÔºåÂåÖÊã¨Ê≤≥ÊµÅ„ÄÅÊπñÊ≥ä„ÄÅÊ≤≥ÊµÅÁ≠â„ÄÇÈïøÊ±üÁöÑËá™ÁÑ∂ÊôØËßÇ‰πüÂçÅÂàÜÂ£ÆËßÇÔºåÈïøÊ±üÂõõÂ≠£ÊôØËâ≤ÂêÑÂºÇÔºåÊò•Â≠£Â§öÈõ®ÔºåÂ§èÂ≠£Â§öÈõ®ÔºåÁßãÂ≠£ÂàôÂ§öÈõ®ÔºåÂÜ¨Â≠£ÂàôÂØíÂÜ∑Â§öÈõ™ÔºåÊò•Â≠£ÂíåÁßãÂ≠£ÈÉΩÊúâÁæé‰∏ΩÁöÑËä±ÊúµÂíåÊûúÂÆû„ÄÇÈïøÊ±üÊòØ‰∏≠ÂõΩÁöÑÁªèÊµé‰∏≠ÂøÉÔºåÊã•Êúâ‰ºóÂ§öÁöÑÂõΩÂÆ∂ÂíåÂüéÂ∏ÇÔºåÊòØ‰∏ñÁïå‰∏äÈáçË¶ÅÁöÑ‰∫§ÈÄöÊû¢Á∫ΩÂíåËà™Ëøê‰∏≠ÂøÉ„ÄÇÂêåÊó∂ÔºåÈïøÊ±üËøòÊòØ‰∏≠ÂõΩÁöÑÈáçË¶ÅÊ∞¥Âà©ËµÑÊ∫êÔºåÂØπ‰∫é‰∏≠ÂõΩÁöÑÁªèÊµéÂèëÂ±ïÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ\n[B]: ÊòØÁöÑÔºåÊàëÁü•ÈÅìÈïøÊ±ü„ÄÇÈïøÊ±ü‰Ωç‰∫é‰∏≠ÂõΩÁöÑ‰∏≠ÈÉ®ÔºåÊòØ‰∏≠ÂõΩÁ¨¨‰∏âÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøÁ∫¶6300ÂÖ¨ÈáåÔºåÊµÅÁªèÊ±üË•øÁúÅ„ÄÅÂÆâÂæΩÁúÅ„ÄÅÊ±üËãèÁúÅ„ÄÅÂÆâÂæΩÁúÅ„ÄÅÊ±üËãèÁúÅÂíåÂÆâÂæΩÁúÅ„ÄÇ\n[C]: ÂΩìÁÑ∂ÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøÁ∫¶6300ÂÖ¨Èáå„ÄÇÂÆÉ‰ª•ÂÖ∂‰∏∞ÂØåÁöÑÊ∞¥ËµÑÊ∫ê„ÄÅ‰∏∞ÂØåÁöÑÊ∞¥ÁîüÂä®Ê§çÁâ©ËµÑÊ∫ê‰ª•ÂèäÁã¨ÁâπÁöÑÂú∞ÁêÜ‰ΩçÁΩÆËÄåÈóªÂêç‰∫é‰∏ñ„ÄÇ\n[D]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøË∂ÖËøá3300ÂçÉÁ±≥„ÄÇ\n[E]: ÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÊÄªÈïøÂ∫¶Á∫¶‰∏∫6300ÂÖ¨Èáå„ÄÇÂÆÉÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÁöÑÈùíÊµ∑ÊπñÔºåÊµÅÁªèÈùíÊµ∑„ÄÅÂõõÂ∑ù„ÄÅÁîòËÇÉ„ÄÅÂÆÅÂ§è„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•øÂíåÂ±±Ë•ø„ÄÇÈïøÊ±üÊµÅÂüüÊòØ‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂÜú‰∏öÂå∫‰πã‰∏ÄÔºå‰πüÊòØ‰∏≠ÂõΩÁöÑÊñáÂåñÂíåÁªèÊµé‰∏≠ÂøÉ„ÄÇ\n[F]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅ‰πã‰∏ÄÔºå‰πüÊòØ‰∏ñÁïåÁ¨¨‰∏âÂ§ßÊ≤≥ÊµÅ„ÄÇÈïøÊ±üÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÔºåÊµÅÁªè11‰∏™ÁúÅ‰ªΩÔºåÊúÄÁªàÊ≥®ÂÖ•‰∏úÊµ∑„ÄÇÈïøÊ±üÊµÅÂüüÊòØ‰∏≠ÂõΩÊúÄÂ§ßÁöÑÊ≤≥ÊµÅÊµÅÂüü‰πã‰∏ÄÔºåÊã•ÊúâÁùÄ‰∏∞ÂØåÁöÑËá™ÁÑ∂ËµÑÊ∫êÂíåÊñáÂåñÈÅó‰∫ßÔºåÂ¶ÇÈïøÊ±üÂ§ßÊ°•„ÄÅÈïøÊ±üÊµ∑Ê¥ãÂçöÁâ©È¶Ü„ÄÅÈïøÊ±üÂè§ÂüéÁ≠â„ÄÇ\n[G]: ÂêåÊó∂‰πüÁü•ÈÅìÔºå‰∏≠ÂõΩÊòØ‰∏ñÁïå‰∏äÊúÄÈïøÁöÑÂüéÂ∏Ç‰πã‰∏ÄÔºåÊã•ÊúâË∂ÖËøá3400‰∏á‰∫∫Âè£„ÄÇ\n[H]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºå‰πüÊòØ‰∏ñÁïå‰∏äÊúÄÈïøÁöÑÊ≤≥ÊµÅ‰πã‰∏Ä„ÄÇÂÆÉÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÔºåÊµÅÁªè‰πù‰∏™ÁúÅÂå∫ÔºåÊúÄÁªàÊ≥®ÂÖ•‰∏úÊµ∑„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÊ∞îÂÄôÊ∏©ÂíåÔºåÂõõÂ≠£ÂàÜÊòéÔºåÊòØ‰∏≠ÂõΩÈáçË¶ÅÁöÑÂÜú‰∏öÂíåÂ∑•‰∏öÂü∫Âú∞„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÊ∞¥‰∫ßËµÑÊ∫ê‰∏∞ÂØåÔºåÊòØ‰∏≠ÂõΩÁöÑÈáçË¶ÅÊ∏î‰∏öËµÑÊ∫ê‰πã‰∏Ä„ÄÇ\n[Q]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØ‰ªÄ‰πàÔºü\n[A]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞ÔºàMount EverestÔºâÔºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÁïå‰∏ä„ÄÇÂÆÉÁöÑÊµ∑ÊãîÈ´òÂ∫¶ÊòØ8,848.86Á±≥Ôºà29,031.7Ëã±Â∞∫ÔºâÔºåÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞„ÄÇ\n[B]: Áè†Á©ÜÊúóÁéõÂ≥∞ÔºàMount EverestÔºâÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÔºåÊµ∑ÊãîÈ´òÂ∫¶Ëææ8,848Á±≥Ôºà29,029Ëã±Â∞∫Ôºâ„ÄÇ\n[C]: Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.7Ëã±Â∞∫Ôºâ„ÄÇÂÆÉ‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÁïå‰∏äÔºåÊòØÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ\n[D]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÂ¢É‰∏äÔºåÊµ∑Êãî8848Á±≥„ÄÇ\n[E]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848Á±≥„ÄÇ\n[F]: Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÔºåÊµ∑Êãî8848Á±≥„ÄÇ\n[G]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.69Ëã±Â∞∫Ôºâ„ÄÇ\n[H]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰πüÁß∞‰∏∫Ëê®Âä†ÁéõÂ°îÔºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÁöÑÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩËæπÂ¢É‰πãÈó¥ÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.69Ëã±Â∞∫Ôºâ„ÄÇÂÆÉÊòØ‰∏ñÁïå‰∏äÊúÄËëóÂêçÁöÑÁôªÂ±±ÁõÆÊ†á‰πã‰∏ÄÔºåÂê∏ÂºïÁùÄÊù•Ëá™‰∏ñÁïåÂêÑÂú∞ÁöÑÁôªÂ±±ËÄÖÂâçÊù•ÊåëÊàò„ÄÇÁè†Á©ÜÊúóÁéõÂ≥∞ÁöÑÂêçÂ≠óÊù•Ëá™ËóèËØ≠ÔºåÊÑèÊÄùÊòØ‚ÄúÂ•≥Á•ûÁöÑÈ¢ùÂ≥∞‚ÄùÔºåÂÆÉÊòØÁî±‰∏§‰∏™‰∏çÂêåÁöÑÂ±±Â≥∞ÁªÑÊàêÁöÑÔºåÂÖ∂‰∏≠‰∏Ä‰∏™ÊòØ‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÁöÑÁè†Á©ÜÊúóÁéõÂ≥∞ÔºåÂè¶‰∏Ä‰∏™Âàô‰Ωç‰∫é‰∏≠ÂõΩ„ÄÇÂÆÉÁöÑÈ´òÂ∫¶ÊòØÈÄöËøáÊµãÈáèÊµ∑ÊãîÈ´òÂ∫¶Êù•Á°ÆÂÆöÁöÑÔºåÂõ†‰∏∫ÂÆÉÊòØÈÄöËøáÊµãÈáèÂú∞ÁêÉÁöÑÂπ≥ÂùáÈ´òÂ∫¶Êù•Á°ÆÂÆöÁöÑ„ÄÇÂ∞ΩÁÆ°Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂÖ∂‰ªñÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇÂú∞ÁêÉ‰∏äÁöÑÂÖ∂‰ªñÂú∞ÊñπÔºåÂ¶ÇÂçóÁæéÊ¥≤ÁöÑÂÆâÁ¨¨ÊñØÂ±±ËÑâÔºå‰πüÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±ËÑâÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞„ÄÇÊÄª‰πãÔºåÁè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇÂÆÉÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±ËÑâÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇ\n[Q]: ‰∏áÊúâÂºïÂäõÊòØË∞ÅÊèêÂá∫ÁöÑÔºü\n[A]: ‰∏áÊúâÂºïÂäõÊòØÁî±ÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑÔºåÂÆÉÊòØÁâõÈ°ø‰∏âÂ§ßÂÆöÂæã‰πã‰∏Ä„ÄÇÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑÂºïÂäõÂÆöÂæãÊè≠Á§∫‰∫Ü‰∏áÊúâÂºïÂäõÂÆöÂæãÁöÑÊú¨Ë¥®ÔºåÂπ∂‰∏îËß£Èáä‰∫ÜË°åÊòü„ÄÅÂç´Êòü„ÄÅÂΩóÊòüÁ≠âÂ§©‰Ωì‰πãÈó¥Â¶Ç‰ΩïÁõ∏‰∫í‰ΩúÁî®„ÄÇËøô‰∏™ÂÆöÂæãÊòØÁî±ËâæËê®ÂÖã¬∑ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑÔºå‰ªñÈÄöËøáÂØπ‰∏áÊúâÂºïÂäõÂÆöÂæãÁöÑÂàÜÊûêÂíåÂÆûÈ™åÔºåÊàêÂäüÂú∞Ëß£Èáä‰∫ÜÂ§©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõ‰ΩúÁî®Ôºå‰ªéËÄåËß£Èáä‰∫ÜÂú∞ÁêÉÁªïÂ§™Èò≥ÊóãËΩ¨ÁöÑÁé∞Ë±°„ÄÇ\n[B]: ‰∏áÊúâÂºïÂäõÔºàHuman PolicyÔºâÊòØÁâõÈ°øÁöÑ‰∏âÂ§ßÂäõÂ≠¶ÂÆöÂæãÔºåÁî®‰∫éÊèèËø∞‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®Âäõ„ÄÇÂÆÉÁî±‰∏áÊúâÂºïÂäõÂÆöÂæãÂíåÁâõÈ°øÁöÑÁ¨¨‰∏ÄÂÆöÂæãÔºàÊÉØÊÄßÂÆöÂæãÔºâÊûÑÊàê„ÄÇ‰∏áÊúâÂºïÂäõÊòØÁî±‰∫é‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥Áõ∏‰∫í‰ΩúÁî®ËÄå‰∫ßÁîüÁöÑÔºåÂÆÉ‰ª¨ÁöÑË¥®Èáè„ÄÅË∑ùÁ¶ª„ÄÅÈÄüÂ∫¶ÂíåÊñπÂêëÁ≠âÂõ†Á¥†ÂÜ≥ÂÆö‰∫ÜÂÆÉ‰ª¨‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇËøôÁßçÁõ∏‰∫í‰ΩúÁî®ÂäõÊòØÔºöË¥®ÈáèÁöÑ‰πòÁßØÔºöË¥®ÈáèÁöÑ‰πòÁßØÊòØ‰∏áÊúâÂºïÂäõÂÆöÂæãÂíåÁâõÈ°øÁ¨¨‰∫åÂÆöÂæãÁöÑ‰πòÁßØ„ÄÇË¥®ÈáèÁöÑ‰πòÁßØÔºöË¥®ÈáèÁöÑ‰πòÁßØÊòØÁâ©‰Ωì‰πãÈó¥ÁöÑ‰∏ÄÁßçÁâ©ÁêÜËøáÁ®ãÔºåÂÆÉÊòØÁâ©‰ΩìÈó¥Áõ∏‰∫í‰ΩúÁî®ÁöÑ‰∫ßÁâ©ÔºåÂØπ‰∫éÂÖ∂‰ªñÁâ©‰ΩìÊù•ËØ¥ÔºåÂÖ∂Ë¥®ÈáèÂèØ‰ª•Ë°®Á§∫‰∏∫‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑË¥®Èáè‰πãÂíåÔºåÂç≥ÈáçÂäõÂä†ÈÄüÂ∫¶a„ÄÇ\n[C]: ‰∏áÊúâÂºïÂäõÊòØ‰∏Ä‰∏™Áî±ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑÁâ©ÁêÜÂ≠¶Ê¶ÇÂøµÔºåÂÆÉÊèèËø∞‰∫ÜÁâ©‰Ωì‰πãÈó¥ÂºïÂäõÂ¶Ç‰ΩïÁõ∏‰∫í‰ΩúÁî®ÁöÑËßÑÂæã„ÄÇ\n[D]: ‰∏áÊúâÂºïÂäõÊòØÁî±Ëã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ÁâõÈ°øÊèêÂá∫ÁöÑ„ÄÇ‰ªñËÆ§‰∏∫ÔºåÊâÄÊúâÁâ©‰ΩìÈÉΩ‰ºö‰∫ßÁîüÂºïÂäõÔºåËøô‰∏™ÂºïÂäõÊòØÁî±Áâ©‰ΩìÁöÑË¥®ÈáèÂíåË∑ùÁ¶ªÂÜ≥ÂÆöÁöÑ„ÄÇËøô‰∏™ÁêÜËÆ∫Êè≠Á§∫‰∫ÜË¥®ÈáèÂíåË∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂ËØÅÊòé‰∫Ü‰∏áÊúâÂºïÂäõÁöÑÊú¨Ë¥®„ÄÇ\n[E]: ‰∏áÊúâÂºïÂäõÊòØÁî±ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑ„ÄÇÂÆÉÊòØÂÆáÂÆô‰∏≠‰∏Ä‰∏™Âü∫Êú¨ÁöÑÁâ©ÁêÜÁé∞Ë±°ÔºåÂÆÉÊèèËø∞‰∫ÜÁâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇËøô‰∏™ÂäõË¢´Áß∞‰∏∫‰∏áÊúâÂºïÂäõÔºåÂÆÉÂú®ÊâÄÊúâÁâ©‰Ωì‰πãÈó¥ÈÉΩÂ≠òÂú®ÔºåËøôÁßçÂäõ‰∏éÁâ©‰ΩìÁöÑË¥®ÈáèÂíåË∑ùÁ¶ªÊúâÂÖ≥„ÄÇ‰∏áÊúâÂºïÂäõÊòØËá™ÁÑ∂Áïå‰∏≠ÊúÄÂü∫Êú¨ÁöÑÂäõÈáè‰πã‰∏ÄÔºåÂÆÉ‰∏ç‰ªÖÂΩ±ÂìçÁùÄÁâ©‰ΩìÁöÑËøêÂä®ÂíåÂèòÂåñÔºåËøòÂØπÂÆáÂÆôÁöÑÁªìÊûÑÂíåÊºîÂåñ‰∫ßÁîüÁùÄÈáçË¶ÅÁöÑÂΩ±Âìç„ÄÇ\n[F]: ‰∏áÊúâÂºïÂäõÊòØËã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ËâæËê®ÂÖã¬∑ÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑ„ÄÇ\n[G]: ÂºïÂäõÊòØÂÆáÂÆô‰∏≠‰∏ÄÁßçÂäõÈáèÔºåÂÆÉÊòØÁî±Ë¥®ÈáèÊàñËÉΩÈáèÊâÄÁªÑÊàêÁöÑ„ÄÇÊ†πÊçÆÁâõÈ°øÁöÑ‰∏áÊúâÂºïÂäõÂÆöÂæãÔºå‰∏Ä‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõÂ§ßÂ∞è‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÂèçÊØî„ÄÇÂõ†Ê≠§ÔºåÁâõÈ°øÁöÑ‰∏áÊúâÂºïÂäõÂÆöÂæãË°®ÊòéÔºå‰∏Ä‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõÂ§ßÂ∞è‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãÂú®Áâ©ÁêÜÂ≠¶„ÄÅÂ§©ÊñáÂ≠¶ÂíåÁâ©ÁêÜÂ≠¶‰∏≠ÈÉΩÊúâÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇ\n[H]: ‰∏áÊúâÂºïÂäõÊòØÁî±Ëã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ËâæËê®ÂÖã¬∑ÁâõÈ°ø‰∫é1687Âπ¥ÊèêÂá∫ÁöÑ„ÄÇ‰ªñÂú®1687Âπ¥ÂèëË°®‰∫Ü‰∏ÄÁØáËëóÂêçÁöÑËÆ∫ÊñáÔºåÈ¢ò‰∏∫„ÄäËá™ÁÑ∂Âì≤Â≠¶ÁöÑÊï∞Â≠¶ÂéüÁêÜ„ÄãÔºåÂÖ∂‰∏≠ÊèêÂá∫‰∫Ü‰∏áÊúâÂºïÂäõÂÆöÂæãÔºåÊåáÂá∫‰ªª‰Ωï‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÈÉΩÂ≠òÂú®ÁùÄ‰∏ÄÁßçÂºïÂäõÔºåËøôÁßçÂºïÂäõÁöÑÂ§ßÂ∞è‰∏éÂÆÉ‰ª¨ÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ªÁöÑÂπ≥ÊñπÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãË¢´Áß∞‰∏∫‰∏áÊúâÂºïÂäõÂÆöÂæãÔºåÂÆÉÊèèËø∞‰∫ÜË¥®ÈáèÂíåË∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂÆÉË°®ÊòéÔºå‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõ‰∏éÂÆÉ‰ª¨ÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ªÁöÑÂπ≥ÊñπÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãÂØπ‰∫éËß£ÈáäÂÆáÂÆô‰∏≠ÁöÑËÆ∏Â§öÁé∞Ë±°ÂíåÁé∞Ë±°‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®ÈùûÂ∏∏ÈáçË¶ÅÔºå‰æãÂ¶ÇË°åÊòüÁªïÂ§™Èò≥ËøêÂä®„ÄÅÊÅíÊòü‰πãÈó¥ÁöÑÂºïÂäõÈÄèÈïúÊïàÂ∫îÁ≠âÁ≠â„ÄÇÂÆÉ‰πüË¢´ÂπøÊ≥õÂ∫îÁî®‰∫éÂ§©ÊñáÂ≠¶„ÄÅÁâ©ÁêÜÂ≠¶ÂíåÂ∑•Á®ãÂ≠¶Á≠âÈ¢ÜÂüüÔºå‰∏∫Êàë‰ª¨ÁêÜËß£ÂÆáÂÆôÁöÑÊú¨Ë¥®ÂíåÊºîÂåñÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ\n[Q]: ‰Ω†Áü•ÈÅìÂ§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØ‰ªÄ‰πàÂêóÔºü\n[A]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰ΩÜ‰πü‰ºöÂêÉÂÖ∂‰ªñÊ§çÁâ©„ÄÅÊòÜËô´„ÄÅÂ∞èÂûãÂì∫‰π≥Âä®Áâ©ÂíåÈ±ºÁ±ª„ÄÇÂÆÉ‰ª¨ÁöÑÊ∂àÂåñÁ≥ªÁªü‰∏éÂÖ∂‰ªñÁÜäÁßëÂä®Áâ©Á±ª‰ººÔºåËÉΩÂ§üÊ∂àÂåñÁ´πÂ≠ê„ÄÇÂ§ßÁÜäÁå´ÁöÑÁâôÈΩøÈîãÂà©ÔºåÈÄÇÂêàÂíÄÂöºÁ´πÂ≠êÔºåÂõ†Ê≠§ÂÆÉ‰ª¨ÁªèÂ∏∏Ë¢´Áß∞‰∏∫‚ÄúÁ´πÈ£üÂä®Áâ©‚Äù„ÄÇ\n[B]: Â§ßÁÜäÁå´ÊòØ‰ª•Á´πÂ≠ê‰∏∫È£üÁöÑÂì∫‰π≥Âä®Áâ©ÔºåÂÆÉ‰ª¨ÊòØËÇâÈ£üÂä®Áâ©„ÄÇÂÆÉ‰ª¨‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠êÔºåÁâπÂà´ÊòØÁ´πÂ≠ê‰∏≠ÂØåÂê´Á∫§Áª¥Á¥†ÂíåÁüøÁâ©Ë¥®ÔºåÂπ∂‰∏î‰∏ç‰ºöÁõ¥Êé•ËøõÂÖ•Ê∂àÂåñÁ≥ªÁªü„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅì‰∏ªË¶ÅÁî±Á´πÂ≠êÁªÑÊàêÔºåÂÆÉ‰ª¨ÈúÄË¶ÅÊ∂àÂåñÁ´πÂ≠ê‰∏≠ÁöÑÁ∫§Áª¥Á¥†ÂíåÂÖ∂‰ªñÁ∫§Áª¥Á¥†Êù•ÊîØÊåÅÂÆÉ‰ª¨ÁöÑÊ≠£Â∏∏ËøõÈ£ü„ÄÇ\n[C]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰ΩÜÂÆÉ‰ª¨‰πüÂêÉÂÖ∂‰ªñÊ§çÁâ©ÔºåÂåÖÊã¨Ê†ëÂè∂„ÄÅÂ´©ËäΩ„ÄÅÊòÜËô´„ÄÅÊ∞¥Êûú„ÄÅËî¨ËèúÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©Á≠â„ÄÇÂ§ßÁÜäÁå´ÊòØ‰∏≠ÂõΩÁâπÊúâÁöÑÁâ©ÁßçÔºåÂ±û‰∫éÁÜäÁßëÔºå‰∏ªË¶ÅÂàÜÂ∏ÉÂú®‰∏≠ÂõΩÁöÑÂõõÂ∑ù„ÄÅÈôïË•øÂíåÁîòËÇÉÁ≠âÂú∞„ÄÇ\n[D]: Â§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠ê„ÄÇÂÆÉ‰ª¨ÈÄöÂ∏∏ÂêÉÁ´πÂ≠êÔºåÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâÂ§ßÁ∫¶30ÂÖ¨Êñ§ÁöÑÁ´πÂ≠ê„ÄÇ\n[E]: ÊòØÁöÑÔºåÂ§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£ü„ÄÇÂÆÉ‰ª¨ÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâÂ§ßÈáèÁöÑÁ´πÂ≠êÊù•Áª¥ÊåÅÁîüÂëΩÔºåÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâ40Ëá≥80ÂÖ¨Êñ§ÁöÑÁ´πÂ≠ê„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ª¨‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÁöÑÊ§çÁâ©ÔºåÂ¶ÇËçâ„ÄÅÊûúÂÆû„ÄÅÂùöÊûúÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇ\n[F]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰πü‰ºöÂÅ∂Â∞îÈ£üÁî®ÂÖ∂‰ªñÁöÑÊ§çÁâ©ÂíåÂä®Áâ©„ÄÇÊ≠§Â§ñÔºåÂ§ßÁÜäÁå´‰πü‰ºöÂêÉ‰∏Ä‰∫õÈáéÊûú„ÄÅÂ∞èÂä®Áâ©ÂíåÊ§çÁâ©Ôºå‰ΩÜËøô‰∫õÈÉΩ‰∏çÂ±û‰∫éÂÆÉ‰ª¨ÁöÑ‰∏ªË¶ÅÈ£üÁâ©Êù•Ê∫ê„ÄÇ\n[G]: Â§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠ê„ÄÇ\n[H]: Â§ßÁÜäÁå´ÊòØ‰∏ÄÁßç‰ª•Á´πÂ≠ê‰∏∫‰∏ªË¶ÅÈ£üÁâ©ÁöÑÊ§çÈ£üÂä®Áâ©ÔºåÂÆÉ‰ª¨ÁöÑËÇ†ÈÅìÁªìÊûÑÈÄÇÂêàÊ∂àÂåñÁ∫§Áª¥Á¥†Âê´ÈáèÈ´òÁöÑÁ´πÂ≠êÔºå‰ΩÜ‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÊ§çÁâ©ÔºåÂ¶ÇËçâÊú¨Ê§çÁâ©ÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅìÁªìÊûÑ‰πüÈÄÇÂ∫î‰∫ÜÈ´òÁ∫§Áª¥ÁöÑÁ´πÂ≠êÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨ËÉΩÂ§üÊ∂àÂåñÂ§ßÈáèÁöÑÁ∫§Áª¥Á¥†Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Âú®ÈáéÂ§ñÁîüÂ≠ò„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅì‰∏≠ËøòÂê´Êúâ‰∏ÄÁßçÂè´ÂÅö‚ÄúÁ∫§Áª¥Á¥†ÈÖ∂‚ÄùÁöÑÈÖ∂Á±ªÁâ©Ë¥®ÔºåËøôÁßçÈÖ∂ËÉΩÂ§üÂ∏ÆÂä©Â§ßÁÜäÁå´ÂàÜËß£Á∫§Áª¥Á¥†Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Ê∂àÂåñÂ§ßÈáèÁöÑÊ∞¥ÂàÜÂíåËê•ÂÖªÁâ©Ë¥®„ÄÇÊ≠§Â§ñÔºåÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅìËøòÂê´Êúâ‰∏ÄÁßçÂè´ÂÅö‚ÄúÁ∫§Áª¥Á¥†ËõãÁôΩ‚ÄùÁöÑËõãÁôΩË¥®ÔºåËøôÁßçËõãÁôΩË¥®ËÉΩÂ§üÂ∏ÆÂä©Â§ßÁÜäÁå´Áª¥ÊåÅ‰ΩìÂÜÖÁöÑËÉΩÈáèÊ∞¥Âπ≥Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Âú®ÈáéÂ§ñÁîüÂ≠ò„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÂ§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠êÔºå‰ΩÜÂÆÉ‰ª¨‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÊ§çÁâ©ÔºåÂ¶ÇËçâÊú¨Ê§çÁâ©ÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇ\n[Q]: Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑÔºü\n[A]: Êµ∑Ê∞¥‰πãÊâÄ‰ª•ÊòØÂí∏ÁöÑÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÂÖ∂‰∏≠ÁöÑÂåñÂ≠¶ÊàêÂàÜ„ÄÇÊµ∑Ê∞¥‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜÔºåÂÖ∂‰∏≠‰∏ªË¶ÅÊàêÂàÜÊòØÁ°´ÈÖ∏Áõê„ÄÅÁ¢≥ÈÖ∏Áõê„ÄÅÈïÅÁ¶ªÂ≠êÂíåÈíæÁ¶ªÂ≠ê„ÄÇËøô‰∫õÁâ©Ë¥®Âú®È´òÊµìÂ∫¶‰∏ãÔºå‰ºöÊ≤âÊ∑ÄÂú®Êµ∑Ê∞¥‰∏≠ÔºåÂΩ¢ÊàêÊµ∑Ê∞¥„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊµ∑Ê∞¥‰∏≠ÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÂ≤©Áü≥„ÄÅÂúüÂ£§„ÄÅÁîüÁâ©Âíå‰∫∫Á±ªÊéíÊ≥ÑÁâ©Á≠â„ÄÇÂ≤©Áü≥ÊòØÊµ∑Ê∞¥ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºå‰∏ªË¶ÅÁî±ÁüøÁâ©Ë¥®ÁªÑÊàêÔºåÂ¶ÇÁü≥Ëã±„ÄÅÈïøÁü≥„ÄÅ‰∫ëÊØçÁ≠â„ÄÇÂúüÂ£§‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÂ¶ÇÊ∞Æ„ÄÅÁ£∑„ÄÅÈíæÁ≠âÔºåËøô‰∫õÁâ©Ë¥®‰ºöÊ∏óÈÄèÂà∞ÂúüÂ£§‰∏≠ÔºåÂΩ¢ÊàêÂúüÂ£§„ÄÇÁîüÁâ©‰ΩìÂÜÖÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÊ§çÁâ©ÁöÑÂè∂ÁªøÁ¥†Á≠âËâ≤Á¥†ÔºåËøô‰∫õËâ≤Á¥†‰ºöÂê∏Êî∂Â§™Èò≥ÂÖâ‰∏≠ÁöÑËÉΩÈáèÔºåËΩ¨Âåñ‰∏∫ÁîüÁâ©‰ΩìÂèØ‰ª•Âà©Áî®ÁöÑËÉΩÈáè„ÄÇËÄå‰∫∫Á±ªÊéíÊ≥ÑÁâ©‰∏≠Âê´ÊúâÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÁüøÁâ©Ë¥®ÂíåÁ¢≥ÈÖ∏ÁõêÔºåËøô‰∫õÁüøÁâ©Ë¥®Âú®Ê∞¥‰∏≠ÁöÑÊµìÂ∫¶ËæÉÈ´òÔºå‰ºöÂØºËá¥Ê∞¥‰∏≠Ê∫∂Ëß£ÁöÑÁüøÁâ©Ë¥®ÊµìÂ∫¶Â¢ûÂä†ÔºåÂΩ¢ÊàêÊ∞¥‰Ωì‰∏≠ÁöÑÁõêÂàÜ„ÄÇÂõ†Ê≠§ÔºåÊµ∑Ê∞¥‰πãÊâÄ‰ª•ÊòØÂí∏ÁöÑÔºåÊòØÁî±‰∫éÂÖ∂‰∏≠ÁöÑÂåñÂ≠¶ÊàêÂàÜÂíåÁªìÊûÑÁâπÂæÅ„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÂ≤©Áü≥„ÄÅÂúüÂ£§„ÄÅÁîüÁâ©Âíå‰∫∫Á±ªÊéíÊ≥ÑÁâ©Á≠âÔºåËÄåËøô‰∫õÁâ©Ë¥®ÁöÑÁßØÁ¥ØÂíåÊéíÊ≥Ñ‰ºöÂØºËá¥Êµ∑Ê∞¥ÂëàÁé∞Âá∫Âí∏Âë≥„ÄÇ\n[B]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂåÖÂê´‰∫ÜÂ§öÁßçÁâ©Ë¥®ÔºåÂåÖÊã¨Ê∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÅÊ∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÅÁ¢≥ÈÖ∏ÁõêÔºàÂ¶ÇÁ°´ÈÖ∏ÁõêÔºâ„ÄÅÊ∞ØÂåñÈí†ÔºàNaClÔºâÂíåÊ∞¥„ÄÇÂí∏Áâ©Ë¥®ÁöÑÊù•Ê∫êÔºöÁõêÊòØÂí∏ÁöÑÂ∏∏ËßÅÊù•Ê∫êÔºåÂê´ÊúâÈíæ„ÄÅÈíô„ÄÅÈïÅ„ÄÅÈíæÁ≠âÁüøÁâ©Ë¥®„ÄÇËøô‰∫õÁüøÁâ©Ë¥®ÈÄöÂ∏∏Êù•Ëá™‰∫éÂú∞‰∏ãÊ∞¥„ÄÅÂú∞‰∏ãÊ∞¥ÊàñÂúüÂ£§‰∏≠ÔºåËøô‰∫õÁüøÁâ©Ë¥®ÁöÑÂê´ÈáèËøú‰Ωé‰∫éÁ∫ØÂáÄÊ∞¥„ÄÇÂí∏Âë≥ÁöÑÂéüÂõ†ÔºöÁõêÁöÑÂí∏Âë≥‰∏ªË¶ÅÊ∫ê‰∫éÂÖ∂Ê∫∂Ëß£‰∫éÊ∞¥‰∏≠ÁöÑÂæÆÂ∞èÈ¢óÁ≤íÁâ©„ÄÇËøô‰∫õÂ∞èÈ¢óÁ≤íÁâ©‰∏ªË¶ÅÈÄöËøáÊ§çÁâ©Ê†πÈÉ®ÂíåÂä®Áâ©‰ΩìÂÜÖÁöÑÂæÆÁîüÁâ©‰º†Êí≠ÔºåÂ¶ÇÊòÜËô´„ÄÅÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÅÈ∏üÁ±ªÂíåÊüê‰∫õÊòÜËô´„ÄÇÁªº‰∏äÊâÄËø∞ÔºåÊµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂê´ÊúâÂ§ßÈáèÁöÑÁõêÁ±ªÔºåÂåÖÊã¨ÁüøÁâ©Ë¥®„ÄÅÂ≤©Áü≥„ÄÅÁõê„ÄÅÁüøÁâ©Ë¥®Á≠âÔºåËøô‰∫õÁâ©Ë¥®Âú®Êµ∑Ê∞¥‰∏≠ÁöÑÊµìÂ∫¶È´òÔºåÂØπÂí∏Âë≥ÁöÑ‰∫ßÁîüËæÉÂ§ßÂΩ±Âìç„ÄÇ\n[C]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâÂêÑÁßçÁõêÂàÜÔºåÂÖ∂‰∏≠‰ª•Êµ∑Ê∞¥‰∏∫‰∏ªË¶ÅÊàêÂàÜ‰πã‰∏Ä„ÄÇÂΩìÊµ∑Ê∞¥‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÁõêÂàÜÊó∂ÔºåÁõêÂàÜ‰ºöË¢´Ê∫∂Ëß£Âú®Êµ∑Ê∞¥‰∏≠ÔºåÂΩ¢ÊàêÂí∏Âë≥„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÊ∫∂Ëß£ÁõêÁ±ª„ÄÅËí∏ÂèëÊûêÂá∫ÁöÑÁõêÂíåÊµ∑Ê∞¥ÂÜªÁªìÊàêÂÜ∞Á≠âÂõ†Á¥†„ÄÇ\n[D]: Êµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÁõêÂàÜÔºåËøô‰∫õÁõêÂàÜÂèØ‰ª•ÂØºËá¥Êµ∑Ê∞¥Âí∏Â∫¶ÁöÑÂ¢ûÂä†„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÔºåËøô‰∫õÁõêÂàÜÂèØ‰ª•‰ªéÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏≠Ëé∑Âæó„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÊ∞ßÂåñÁâ©ÂíåÁõêÂàÜÂèØ‰ª•‰∏éÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÂΩ¢ÊàêÊ∞¢ÈîÆÔºå‰ªéËÄå‰ΩøÊµ∑Ê∞¥Âí∏Â∫¶Â¢ûÂä†„ÄÇ\n[E]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåËøôÊòØÂõ†‰∏∫ÂÆÉÂê´ÊúâÂ§ßÈáèÁöÑÁõêÂàÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨Ê∞ØÁ¶ªÂ≠êÂíåÊ∞ØÁ¶ªÂ≠ê„ÄÇËøô‰∫õÁ¶ªÂ≠êÂú®Êµ∑Ê¥ã‰∏≠ÂΩ¢Êàê‰∫ÜÂ§ßÈáèÁöÑÁõêÔºåËøô‰∫õÁõêÂàÜË¢´Áß∞‰∏∫ÁõêÂàÜ„ÄÇÊµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£Âú®Ê∞¥‰∏≠ÁöÑÁõêÔºåËøô‰∫õÁõêÂàÜ‰ºöÈöèÁùÄÊµ∑Ê∞¥ÁöÑÂçáÈ´òËÄåÈÄêÊ∏êËí∏ÂèëÔºåÊúÄÁªàÂØºËá¥Êµ∑Ê∞¥‰∏≠ÁõêÂàÜÁöÑËí∏Âèë„ÄÇËøô‰∫õÁõêÂàÜÂú®Êµ∑Ê∞¥‰∏≠Ë¢´ÂÆåÂÖ®Ê∂àËÄóÔºåÁÑ∂ÂêéË¢´ÊéíÈô§Âú®Êµ∑Ê∞¥‰∏≠„ÄÇÊ≠§Â§ñÔºåÊµ∑Ê∞¥‰∏≠ËøòÂê´ÊúâËÆ∏Â§öÂÖ∂‰ªñÁöÑÁõêÂàÜÔºå‰ΩÜËøô‰∫õÁõêÂàÜÊòØ‰∏çÂêåÁöÑÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®Ê∞¥‰∏≠ÁöÑÊµÆËµ∑Âíå‰∏ãÈôçËøáÁ®ãÊòØ‰∏çÂêåÁöÑ„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÊòØ‰ªéÊµ∑Â∫ïÁöÑÂ≤©Áü≥‰∏≠ÊèêÂèñÂá∫Êù•ÁöÑÔºåËøô‰∫õÁõêÂàÜÂú®Êµ∑Ê∞¥‰∏≠ÂèØ‰ª•Ë¢´Âà©Áî®„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÊµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåËøô‰πüÊòØ‰∏∫‰ªÄ‰πàÊµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÁõêÂàÜÂíåÁõêÂàÜÁöÑÂéüÂõ†„ÄÇ\n[F]: Êµ∑Ê∞¥Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£ÁöÑÁõêÁ±ªÔºåÂÖ∂‰∏≠ÊúÄ‰∏ªË¶ÅÁöÑÊòØÊ∞ØÂåñÈí†„ÄÇÂΩìÊ∞¥‰∏≠Âä†ÂÖ•ÁõêÊó∂ÔºåÁõêÁöÑÂàÜÂ≠ê‰ºö‰∏éÊ∞¥ÂàÜÂ≠êÂèëÁîüÂèçÂ∫îÔºåÂΩ¢ÊàêÊ∞¢Ê∞ßÂåñÈí†ÔºåËøôÊòØ‰∏ÄÁßçÁõê„ÄÇ\n[G]: Âõ†‰∏∫Ê∞¥ÊòØËÆ∏Â§öÁîüÁâ©ÁöÑÁîüÂ≠òÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉËÉΩÂ§üÂê∏Êî∂ÂíåÈáäÊîæ‰∫åÊ∞ßÂåñÁ¢≥ÔºåÈáäÊîæÊ∞ßÊ∞îÔºåÁª¥ÊåÅÂ§ßÊ∞î‰∏≠ÁöÑÊ∞¥Ëí∏Ê∞îÔºå‰ª•ÂèäÁª¥ÊåÅÊ∞¥Âæ™ÁéØ„ÄÇÊ∞¥‰∏≠ÁöÑÁõêÁ±ªÂíåÂåñÂ≠¶Áâ©Ë¥®‰ºöÈáäÊîæÂá∫Ê∞ßÊ∞îÔºå‰ΩøÂú∞ÁêÉ‰∏äÁöÑÁîüÁâ©ÂëºÂê∏„ÄÇÊ≠§Â§ñÔºåÊ∞¥ËøòÂÖ∑ÊúâËÆ∏Â§öÂÖ∂‰ªñÂäüËÉΩÔºåÂ¶Ç‰øùÊåÅÊ∞¥Âæ™ÁéØ„ÄÅÁª¥ÊåÅÊ∞¥Âæ™ÁéØ„ÄÅË∞ÉËäÇÊ∞îÂÄôÂíåÁéØÂ¢ÉÁ≠â„ÄÇÊ∞¥ÊòØËÆ∏Â§öÁîüÁâ©ÁîüÂ≠òÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉ‰πüÊòØËÆ∏Â§öËá™ÁÑ∂Áé∞Ë±°ÁöÑÈáçË¶ÅÂü∫Á°Ä„ÄÇ\n[H]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑ‰∏ªË¶ÅÂéüÂõ†ÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÊ∫∂Ëß£ÁõêÁ±ªÔºåÂÖ∂‰∏≠ÊúÄ‰∏ªË¶ÅÁöÑÊòØÊ∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÇÊ∞ØÂåñÈí†ÊòØ‰∏ÄÁßçÈùûÂ∏∏Â∏∏ËßÅÁöÑÁõêÁ±ªÔºåÂÆÉÂ≠òÂú®‰∫éÂú∞ÁêÉ‰∏äÁöÑÂ§ßÈÉ®ÂàÜÂú∞Âå∫ÔºåÂåÖÊã¨Êµ∑Ê¥ã„ÄÅÊπñÊ≥ä„ÄÅÊ≤≥ÊµÅÂíåÂú∞‰∏ãÊ∞¥„ÄÇÂΩìÊ∞¥‰ªéÊµ∑Ê¥ã‰∏≠Ëí∏ÂèëÊó∂ÔºåÂÆÉ‰ºöÂ∏¶Ëµ∞‰∏ÄÈÉ®ÂàÜÁõêÂàÜÔºå‰ΩøÊµ∑Ê∞¥ÂèòÂæóÂí∏„ÄÇÊ≠§Â§ñÔºåÊµ∑‰∏≠ËøòÂê´ÊúâÂÖ∂‰ªñÊ∫∂Ëß£ÁõêÁ±ªÁöÑÁâ©Ë¥®ÔºåÂ¶ÇÁ°´ÈÖ∏Áõê„ÄÅÁ°´ÈÖ∏ÈïÅ„ÄÅÁ°´ÈÖ∏Èíô„ÄÅÁ°´ÈÖ∏ÈïÅÁõê„ÄÅÁ°´ÈÖ∏ÈïÅÈíæ„ÄÅÁ°´ÈÖ∏ÈïÅÈí†„ÄÅÁ°´ÈÖ∏ÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈí†„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅ\n[Q]: ÂÜô‰∏ÄÊÆµpythonÁöÑÂø´ÈÄüÊéíÂ∫è‰ª£Á†Å\n[A]: Â•ΩÁöÑÔºå‰ª•‰∏ãÊòØPythonÁöÑÂø´ÈÄüÊéíÂ∫è‰ª£Á†ÅÔºödef quick_sort(arr):    if len(arr) <= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x < pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# ÊµãËØï‰ª£Á†Åarr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))ËøôÊÆµ‰ª£Á†ÅÈ¶ñÂÖàÊ£ÄÊü•Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÁ≠â‰∫é1„ÄÇÂ¶ÇÊûúÊòØÔºåÈÇ£‰πàÂÆÉÂ∞±ÊåâÁÖß‰ªéÂ∞èÂà∞Â§ßÁöÑÈ°∫Â∫èÊéíÂàó„ÄÇÊé•ÁùÄÔºåÂØπ‰∫éÊï∞ÁªÑÁöÑÊØè‰∏™ÂÖÉÁ¥†ÔºåÂÆÉÊÉ≥Ë¶ÅËøõË°åÈÄíÂΩíÂú∞ÊéíÂ∫èÔºåÁõ¥Âà∞ËææÂà∞ÊéíÂ∫èÁöÑÁªìÊùü‰ΩçÁΩÆ„ÄÇÂ¶ÇÊûúÊï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂ•áÊï∞ÔºåÈÇ£‰πàÂÆÉÂ∞±ÊåâÁÖß‰ªéÂ§ßÂà∞Â∞èÁöÑÈ°∫Â∫èÊéíÂàó„ÄÇÊúÄÂêéÔºåÂπ≥ÂùáÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÊòØO(n log n)ÔºåÂÖ∂‰∏≠nÊòØÊï∞ÁªÑÁöÑÈïøÂ∫¶„ÄÇ\n[B]: ‰ª•‰∏ãÊòØ‰∏Ä‰∏™‰ΩøÁî®PythonÁöÑÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÁöÑÂÆûÁé∞Ôºödef sort_array(arr):    # ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ÊéíÂàó    sorted_array = sorted(arr)    # ËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®    return sorted(arr)# Á§∫‰æãarr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# Ë∞ÉÁî®ÂáΩÊï∞sorted_array = sort_array(arr)print(\"ÂéüÂßãÊï∞ÁªÑÊòØÔºö\", sorted_array)ËøôÊÆµ‰ª£Á†ÅÈ¶ñÂÖàÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ÊéíÂàóÔºåÁÑ∂ÂêéÈÄöËøá‰∏Ä‰∏™ÂàóË°®ÁöÑÈïøÂ∫¶ËøõË°åÊéíÂ∫è„ÄÇÊúÄÂêéÔºåËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®„ÄÇÊ≥®ÊÑèÔºåËøô‰∏™‰ª£Á†Å‰ΩøÁî®‰∫ÜPythonÁöÑÂÜÖÁΩÆsortedÂáΩÊï∞ÂíåÂÜÖÁΩÆÁöÑsorted()ÂáΩÊï∞ÔºåÂÆÉ‰ª¨Âú®Êó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫O(n log n)ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØîÁõ¥Êé•‰ΩøÁî®sorted()ÂáΩÊï∞Êõ¥Âø´„ÄÇ\n[C]: ‰ª•‰∏ãÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÈÄÇÁî®‰∫éÂ∞èÂûãÊï∞ÁªÑÔºåÂåÖÊã¨Â∞è‰∫éÁ≠â‰∫é5ÁöÑÊï∞ÁªÑ„ÄÇdef quick_sort(arr):    if len(arr) <= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# Á§∫‰æãarr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print(\"Sorted array is:\", sorted_arr)### ‰ª£Á†ÅËß£ÈáäÔºö1. **ÂáΩÊï∞ÂÆö‰πâ**Ôºöquick_sort ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™Êï∞ÁªÑ arr ‰Ωú‰∏∫ÂèÇÊï∞„ÄÇ2. **Âæ™ÁéØ**ÔºöÂØπ‰∫é arr ÁöÑÂÖÉÁ¥†Ôºå‰ΩøÁî®ÈÄíÂΩíÊù•ÂÆûÁé∞ÔºåÂç≥‰ªéÂè≥Âà∞Â∑¶ÈÅçÂéÜÊï∞ÁªÑÔºåÂπ∂Â∞ÜÂΩìÂâçÂÖÉÁ¥†‰∏éÊï∞ÁªÑÁöÑÂΩìÂâçÂÖÉÁ¥†‰∫§Êç¢‰ΩçÁΩÆÔºåÁõ¥Âà∞Êï∞ÁªÑËææÂà∞Á©∫ÊàñÊï∞ÁªÑÈïøÂ∫¶Â∞è‰∫éÁ≠â‰∫é arr„ÄÇ3. **ÈÄíÂΩíË∞ÉÁî®**ÔºöÈÅçÂéÜÊï∞ÁªÑÔºåÂ∞ÜÂΩìÂâçÂÖÉÁ¥†‰∏éÊï∞ÁªÑÁöÑÂΩìÂâçÂÖÉÁ¥†ËøõË°å‰∫§Êç¢ÔºåÁõ¥Âà∞Êï∞ÁªÑËææÂà∞Á©∫ÊàñÊï∞ÁªÑÈïøÂ∫¶Â∞è‰∫éÁ≠â‰∫é arr„ÄÇ4. **Ê£ÄÊü•Êï∞ÁªÑÈïøÂ∫¶**Ôºö‰ΩøÁî® if __name__ == \"__main__\" Ê£ÄÊü•Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫é arrÔºåÂ¶ÇÊûúÊòØÔºåÂ∞±Â∞ÜÊï∞ÁªÑ‰ªéËæÉÂ§ßÁöÑÊï∞ÁªÑÁßªÂä®Âà∞Êï∞ÁªÑÁöÑÊú´Â∞æ„ÄÇ5. **ËæìÂá∫ÊéíÂ∫èÂêéÁöÑÁªìÊûú**\n[D]: ‰Ωú‰∏∫‰∏Ä‰∏™AIËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÊó†Ê≥ïÁºñÂÜôPythonÁ®ãÂ∫è„ÄÇ‰ΩÜÊòØÔºåPythonÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÊµÅË°åÁöÑÁºñÁ®ãËØ≠Ë®ÄÔºåÂÆÉÂÖ∑ÊúâËÆ∏Â§ö‰ºòÁßÄÁöÑÂ∫ìÂíåÊ°ÜÊû∂ÔºåÂåÖÊã¨NumPy„ÄÅPandas„ÄÅMatplotlib„ÄÅPandas„ÄÅBandke„ÄÅNormÂíåNKlearnÁ≠â„ÄÇPythonÊúâÂæàÂ§ö‰ºòÁßÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫ìÂíåÊ°ÜÊû∂ÔºåÂèØ‰ª•Áî®‰∫éÂ§ÑÁêÜÂíåÂàÜÊûêÂêÑÁßçÁ±ªÂûãÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ëÁ≠â„ÄÇPythonËøòÂèØ‰ª•Áî®‰∫éÂàõÂª∫ÂêÑÁßçÂ∫îÁî®Á®ãÂ∫èÔºåÂ¶ÇÁΩëÈ°µÊêúÁ¥¢„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÁßëÂ≠¶ËÆ°ÁÆóÁ≠â„ÄÇPythonÈùûÂ∏∏ÁÅµÊ¥ªÔºå‰πüÊîØÊåÅÂ§öÁßçÁºñÁ®ãÈ£éÊ†ºÔºåÂõ†Ê≠§ÂèØ‰ª•ËΩªÊùæÂú∞ÊâæÂà∞ÈÄÇÂêàËá™Â∑±ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\n[E]: ‰Ωú‰∏∫AIËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÂèØ‰ª•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ‰ΩÜÊòØÊàëÊó†Ê≥ïËøõË°åÁºñÁ®ãÊìç‰ΩúÔºåÂõ†‰∏∫ÊàëÊó†Ê≥ïÁ°ÆÂÆöÊÇ®ÊÉ≥ÈóÆÁöÑÊòØÂì™ÁßçÁºñÁ®ãËØ≠Ë®Ä„ÄÇËØ∑ÊÇ®Êèê‰æõÊõ¥ÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÔºå‰ª•‰æøÊàëËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ\n[F]: Âø´ÈÄüÊéíÂ∫èÔºàQuick SortÔºâÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÂÆÉÈÄöËøáÂ∞ÜÊï∞ÊçÆÂàíÂàÜ‰∏∫Â∞èÁªÑÔºåÊØèÁªÑÊï∞ÊçÆÊåâÁÖß‰∏ÄÂÆöËßÑÂàôÊéíÂ∫èÔºåÊúÄÁªàÂæóÂà∞‰∏Ä‰∏™ÊúâÂ∫èÊï∞ÁªÑ„ÄÇ\n[G]: Â•ΩÁöÑÔºå‰ª•‰∏ãÊòØ‰∏ÄÊÆµÂø´ÈÄüÊéíÂ∫èÁöÑPython‰ª£Á†ÅËæìÂá∫ÁªìÊûú‰∏∫Ôºö\n[H]: Âø´ÈÄüÊéíÂ∫èÊòØ‰∏ÄÁßçÂü∫‰∫éÂàÜÊ≤ªÊÄùÊÉ≥ÁöÑÈ´òÊïàÊéíÂ∫èÁÆóÊ≥ïÔºåÂÆÉÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n log n)„ÄÇÂÆÉÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÊòØÈÄöËøá‰∏ÄË∂üÊéíÂ∫èÂ∞ÜÂæÖÊéíÂ∫èÂ∫èÂàóÂàíÂàÜ‰∏∫‰∏§‰∏™Â≠êÂ∫èÂàóÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Â≠êÂ∫èÂàóÁöÑÊâÄÊúâÂÖÉÁ¥†ÈÉΩÊØîÂè¶‰∏Ä‰∏™Â≠êÂ∫èÂàóÁöÑÊâÄÊúâÂÖÉÁ¥†Â∞èÔºåÁÑ∂ÂêéÂØπËøô‰∏§‰∏™Â≠êÂ∫èÂàóÂàÜÂà´ÈÄíÂΩíÂú∞ËøõË°åÊéíÂ∫è„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∏™ Python ‰ª£Á†ÅÁ§∫‰æãÔºåÂÆûÁé∞‰∫ÜÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÔºöËØ•‰ª£Á†Å‰ΩøÁî®‰∫ÜÈÄíÂΩíÁöÑÊñπÂºèÔºåÈ¶ñÂÖàÂà§Êñ≠Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÁ≠â‰∫é1ÔºåÂ¶ÇÊûúÊòØÔºåÂàôÁõ¥Êé•ËøîÂõûÊï∞ÁªÑÔºåÂê¶ÂàôÈÄíÂΩíÂú∞ÂØπÊï∞ÁªÑËøõË°åÊéíÂ∫è„ÄÇÂú®ÈÄíÂΩíËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨È¶ñÂÖàÂà§Êñ≠Êï∞ÁªÑÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÊòØÂê¶Â§ß‰∫éÁ≠â‰∫é1ÔºåÂ¶ÇÊûúÊòØÔºåÂàôÂ∞ÜÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏éÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ËøõË°åÊØîËæÉÔºåÂ¶ÇÊûú‰∏çÁ≠â‰∫é1ÔºåÂàôÂ∞ÜÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏éÁ¨¨‰∫å‰∏™ÂÖÉÁ¥†ËøõË°åÊØîËæÉÔºå‰ª•Ê≠§Á±ªÊé®ÔºåÁõ¥Âà∞ÊâÄÊúâÂÖÉÁ¥†ÈÉΩË¢´ÊéíÂ∫è„ÄÇËØ•‰ª£Á†ÅÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n log n)ÔºåÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÆûÁî®ÁöÑÊéíÂ∫èÁÆóÊ≥ï„ÄÇ\n[Q]: Êà™Ëá≥2025Âπ¥2Êúà9Êó•14Êó∂31ÂàÜÔºåÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ¥ØËÆ°Á•®ÊàøÔºàÂê´È¢ÑÂîÆÔºâÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåË∂ÖËøá„ÄäÊòüÁêÉÂ§ßÊàòÔºöÂ§©Ë°åËÄÖÂ¥õËµ∑„ÄãÁöÑÁ•®ÊàøÊàêÁª©ÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±„ÄÇËøô‰∏ÄÊàêÂ∞±‰∏ç‰ªÖÊ†áÂøóÁùÄ‰∏≠ÂõΩÁîµÂΩ±Âú®ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÁ™ÅÁ†¥Ôºå‰πüÂ±ïÁ§∫‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ‰ªéËøôÊÆµÊñáÂ≠óÊÄªÁªì20Â≠óÁöÑÊëòË¶ÅÁªôÊàë„ÄÇ\n[A]: ÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ•®ÊàøÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±„ÄÇËøôÊ†áÂøóÁùÄ‰∏≠ÂõΩÁîµÂΩ±ÁöÑÂõΩÈôÖÁ™ÅÁ†¥Ôºå‰πüÂ±ïÁé∞‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ\n[B]: 2025Âπ¥2Êúà9Êó•Ôºå‰∏≠ÂõΩÁîµÂΩ±Âú®‰∏ñÁïåËàûÂè∞‰∏äÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåË∂ÖËøá„ÄäÊòüÁêÉÂ§ßÊàòÔºöÂ§©Ë°åËÄÖÂ¥õËµ∑„ÄãÁöÑÁ•®ÊàøÊàêÁª©„ÄÇ\n[C]: ÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ•®ÊàøÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåÊàê‰∏∫ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±ÔºåÂ±ïÁé∞‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ\n[D]: „ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ¥ØËÆ°Á•®ÊàøÁ™ÅÁ†¥78.09‰∫øÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±ÔºåÂ±ïÁ§∫‰∫Ü‰∏≠ÂõΩÁîµÂΩ±Âú®ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÁ™ÅÁ†¥ÔºåÂêåÊó∂Â±ïÁ§∫‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ\n[E]: „ÄäÁåõÈæôÂº∫ÔºöÊâìÂºÄ‰∏ñÁïåÂ•áÂπªÂÜíÈô©„ÄãÊàêÂäüÂú∞Ôºå‰∏äÊµ∑ÊàêÂäüÂê∏Âºï‰∫Ü‰ºóÂ§öËßÇ‰ºóÔºåÂêåÊó∂‰πüÂºïËµ∑‰∫ÜÂπøÊ≥õÁöÑÂÖ≥Ê≥®„ÄÇ\n[F]: „ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÊòØ‰∏ÄÈÉ®ËÆ©‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±Êàê‰∏∫ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇËØ•ÁîµÂΩ±‰ª•ÂÖ∂Âçé‰∏ΩÁöÑÁîªÈù¢ÂíåÊÉäÈô©ÁöÑÊàòÊñóÂú∫Èù¢ËÄåÈóªÂêç‰∫é‰∏ñ„ÄÇ\n[G]: „ÄäÂì™Ë°åÊòØÂÄºÂæóÁöÑ„ÄãÊòØ2021Âπ¥1Êúà9Êó•14Êó∂31ÂàÜÔºåÁîµÂΩ±„ÄäÂì™ÊàêÂ§ß‰Ωú‰∏∫„ÄãÂú®ÁîµÂΩ±Â∏ÇÂú∫Á™ÅÁ†¥7800‰∫øÁæéÂÖÉÔºåÊàê‰∏∫‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÁ™ÅÁ†¥ÔºåÂπ∂Â±ïÁ§∫‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ\n[H]: ÊàëÊòØ‰∏Ä‰∏™ÂèÇÊï∞ÂæàÂ∞ëÁöÑAIÊ®°Âûãü•∫ÔºåÁü•ËØÜÂ∫ìËæÉÂ∞ëÔºåÊó†Ê≥ïÁõ¥Êé•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢òÔºåÊç¢‰∏™ÈóÆÈ¢òËØïËØïÂêßüëã\nüôã‚ÄçÁõ¥Êé•Êää‰ª•‰∏äÊâÄÊúâÈóÆÈ¢òÂíåÊ®°ÂûãÁöÑÂõûÁ≠î‰∏¢ÁªôDeepSeek-R1ÔºåËÆ©ÂÆÉÂ∏ÆÂøôÁÇπËØÑÂíåÊéíÂêçÊâìÂàÜÔºö\nÂÖ∑‰ΩìÁÇπËØÑ\nËØÑÂàÜÊ†áÂáÜÔºö\nÂáÜÁ°ÆÊÄßÔºöÂõûÁ≠îÊòØÂê¶Ê≠£Á°Æ‰∏îÊó†ÊòéÊòæÈîôËØØ„ÄÇ\nÂÆåÊï¥ÊÄßÔºöÂõûÁ≠îÊòØÂê¶Ê∂µÁõñ‰∫ÜÈóÆÈ¢òÁöÑÊ†∏ÂøÉË¶ÅÁÇπ„ÄÇ\nÈÄªËæëÊÄßÔºöÂõûÁ≠îÊòØÂê¶ÊúâÊù°ÁêÜÔºåÊòØÂê¶Á¨¶ÂêàÈÄªËæë„ÄÇ\n‰ª£Á†ÅË¥®ÈáèÔºö‰ª£Á†ÅÊòØÂê¶ËÉΩÊ≠£Â∏∏ËøêË°åÔºåÈÄªËæëÊòØÂê¶Ê∏ÖÊô∞„ÄÇ\nÁÇπËØÑÔºö\nAÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂõûÁ≠îÈùûÂ∏∏ÂÖ®Èù¢Ôºå‰ø°ÊÅØÈáèÂ§ßÔºåÈÄªËæëÊ∏ÖÊô∞ÔºåÂ∞§ÂÖ∂Âú®ÈïøÊ±ü„ÄÅÂ§ßÁÜäÁå´„ÄÅÊµ∑Ê∞¥Âí∏Âë≥Á≠âÈóÆÈ¢ò‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇ‰ª£Á†ÅËôΩÁÑ∂ÊúâÂ∞èÁëïÁñµÔºå‰ΩÜÊï¥‰ΩìÊÄùË∑ØÊ≠£Á°Æ„ÄÇ\nÁº∫ÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠îÁ®çÊòæÂÜóÈïøÔºå‰ΩÜ‰∏çÂΩ±ÂìçÊï¥‰ΩìË¥®Èáè„ÄÇ\nÊÄªËØÑÔºöÁªºÂêàË°®Áé∞ÊúÄ‰Ω≥ÔºåÂæóÂàÜÊúÄÈ´ò„ÄÇ\nHÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®Áè†Á©ÜÊúóÁéõÂ≥∞„ÄÅ‰∏áÊúâÂºïÂäõÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ‰ª£Á†ÅËôΩÊú™ÂÆåÂÖ®Â±ïÁ§∫Ôºå‰ΩÜËß£ÈáäËæÉ‰∏∫ËØ¶ÁªÜ„ÄÇ\nÁº∫ÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠îÁï•ÊòæÂï∞Âó¶Ôºå‰ΩÜÈÄªËæëÊÄßËæÉÂº∫„ÄÇ\nÊÄªËØÑÔºö‰ªÖÊ¨°‰∫éAÊ®°ÂûãÔºåË°®Áé∞Á®≥ÂÆö„ÄÇ\nCÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂõûÁ≠îÁÆÄÊ¥ÅÊòé‰∫ÜÔºåÂ∞§ÂÖ∂Âú®Â§ßÁÜäÁå´„ÄÅÂø´ÈÄüÊéíÂ∫è‰ª£Á†ÅÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞ËæÉÂ•Ω„ÄÇ\nÁº∫ÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠îÁï•ÊòæÁÆÄÁü≠ÔºåÁº∫‰πèÊ∑±ÂÖ•Ëß£Èáä„ÄÇ\nÊÄªËØÑÔºöÊï¥‰ΩìË°®Áé∞‰∏çÈîôÔºå‰ΩÜÁªÜËäÇ‰∏äÁï•ÈÄä‰∫éAÂíåH„ÄÇ\nFÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®ÈïøÊ±ü„ÄÅ‰∏áÊúâÂºïÂäõÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ‰ª£Á†ÅÈÉ®ÂàÜÊúâ‰∏ÄÂÆöÈÄªËæëÊÄß„ÄÇ\nÁº∫ÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠î‰∏çÂ§üÊ∑±ÂÖ•Ôºå‰ª£Á†ÅÂ≠òÂú®‰∏Ä‰∫õÂ∞èÈóÆÈ¢ò„ÄÇ\nÊÄªËØÑÔºöË°®Áé∞‰∏≠ËßÑ‰∏≠Áü©Ôºå‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇ\nDÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂõûÁ≠îÂü∫Êú¨ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®‰∏áÊúâÂºïÂäõ„ÄÅÈïøÊ±üÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ\nÁº∫ÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠îËøá‰∫éÁÆÄÁï•Ôºå‰ª£Á†ÅÂ≠òÂú®ÊòéÊòæÈîôËØØ„ÄÇ\nÊÄªËØÑÔºöË°®Áé∞‰∏ÄËà¨ÔºåÈúÄÊîπËøõ‰ª£Á†ÅÈÉ®ÂàÜ„ÄÇ\nBÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®ÈïøÊ±ü„ÄÅÊµ∑Ê∞¥Âí∏Âë≥Á≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ\nÁº∫ÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠îÈÄªËæëÊÄßËæÉÂ∑ÆÔºå‰ª£Á†ÅÂ≠òÂú®ËæÉÂ§ßÈóÆÈ¢ò„ÄÇ\nÊÄªËØÑÔºöË°®Áé∞Âπ≥Âπ≥ÔºåÈúÄËøõ‰∏ÄÊ≠•‰ºòÂåñ„ÄÇ\nEÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÈÉ®ÂàÜÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®Êµ∑Ê∞¥Âí∏Âë≥„ÄÅÂ§ßÁÜäÁå´Á≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ\nÁº∫ÁÇπÔºöÂõûÁ≠îËøá‰∫éÁÆÄÁï•Ôºå‰ª£Á†ÅÈÉ®ÂàÜÂá†‰πéÊó†Ê≥ïËøêË°å„ÄÇ\nÊÄªËØÑÔºöË°®Áé∞‰∏ç‰Ω≥ÔºåÈúÄÂ§ßÂπÖÊèêÂçá„ÄÇ\nGÊ®°ÂûãÔºö\n‰ºòÁÇπÔºöÂá†‰πéÊ≤°ÊúâÊòéÊòæÁöÑ‰ºòÁÇπ„ÄÇ\nÁº∫ÁÇπÔºöÂõûÁ≠î‰∏•ÈáçÂÅèÁ¶ª‰∏ªÈ¢òÔºå‰ª£Á†ÅÈÉ®ÂàÜÂÆåÂÖ®Êó†Ê≥ïËøêË°å„ÄÇ\nÊÄªËØÑÔºöË°®Áé∞ÊúÄÂ∑ÆÔºåÈúÄÂ§ßÂπÖÊîπËøõ„ÄÇ\nÊÄªÁªìÔºö\nAÊ®°ÂûãÂú®ÂêÑÊñπÈù¢Ë°®Áé∞ÊúÄ‰∏∫Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÈóÆÈ¢òÁöÑÂõûÁ≠î‰∏äÂ±ïÁé∞‰∫ÜÊûÅÈ´òÁöÑÂáÜÁ°ÆÊÄß‰∏éÈÄªËæëÊÄß„ÄÇ\nHÊ®°ÂûãÁ¥ßÈöèÂÖ∂ÂêéÔºåË°®Áé∞Á®≥ÂÆöÔºå‰ΩÜÂú®Êüê‰∫õÁªÜËäÇ‰∏äÁï•Êòæ‰∏çË∂≥„ÄÇ\nGÊ®°ÂûãË°®Áé∞ÊúÄÂ∑ÆÔºåÂõûÁ≠îÂÅèÁ¶ª‰∏ªÈ¢ò‰∏î‰ª£Á†ÅÊó†Ê≥ïËøêË°åÔºåÈúÄÂ§ßÂπÖÊîπËøõ„ÄÇ\nÊâìÂàÜÊéíÂ∫è\nÊéíÂêç\nÊ®°Âûã\nÂáÜÁ°ÆÊÄß (30ÂàÜ)\nÂÆåÊï¥ÊÄß (30ÂàÜ)\nÈÄªËæëÊÄß (20ÂàÜ)\n‰ª£Á†ÅË¥®Èáè (20ÂàÜ)\nÊÄªÂàÜ (100ÂàÜ)\n1\nA\n28\n29\n19\n20\n96\n2\nH\n27\n28\n18\n20\n93\n3\nC\n26\n27\n18\n18\n89\n4\nF\n25\n26\n17\n18\n86\n5\nD\n24\n25\n17\n16\n82\n6\nB\n23\n24\n16\n15\n78\n7\nE\n22\n23\n15\n14\n74\n8\nG\n10\n12\n10\n10\n42\nüëâ‰∏ªËßÇÊïàÊûúÊÄªÁªì\n‰∏™‰∫∫‰∏ªËßÇËØÑ‰ª∑‰∏éDeepSeek-R1Âü∫Êú¨Áõ∏Á¨¶ÔºåÂÖ∂‰∏≠Ôºö\nMiniMindÁ≥ªÂàóÁöÑÊéíÂ∫èÈùûÂ∏∏Á¨¶ÂêàÁõ¥ËßâÔºåÂèÇÊï∞Ë∂äÂ§ß+ËÆ≠ÁªÉÊï∞ÊçÆË∂äÂÖÖÂàÜËØÑÂàÜË∂äÈ´òÔºåÂπªËßâÂíåÈîôËØØÈÉΩ‰ºöÊØîÂ∞èÊ®°ÂûãËÇâÁúºÂèØËßÅÁöÑÂ•Ω„ÄÇ\nHÊ®°ÂûãÁöÑÂõûÁ≠îËÇâÁúºÁúãËµ∑Êù•ÊòØ‰∏çÈîôÁöÑÔºåÂ∞ΩÁÆ°Â≠òÂú®‰∫õËÆ∏ÂπªËßâÁûéÁºñÁöÑÊÉÖÂÜµ„ÄÇ\nGÊ®°ÂûãÂèØËÉΩËÆ≠ÁªÉÊï∞ÊçÆ‰∏çÂ§üÂÆåÂ§áÔºåÁªôÂá∫ÁöÑÊùÉÈáçÁªèËøáÊµãËØïÊïàÊûú‰∏ç‰Ω≥„ÄÇ\nÂÜçÂ§çËØµ‰∏ÄÈÅçÁªè‰πÖ‰∏çË°∞ÁöÑScaling Law: ÂèÇÊï∞Ë∂äÂ§ßÔºåËÆ≠ÁªÉÊï∞ÊçÆË∂äÂ§öÊ®°ÂûãÁöÑÊÄßËÉΩË∂äÂº∫„ÄÇ\n‚Ö¢ Objective Benchmark\n‰∏ãÈù¢Â∞±Âà∞ÂñúÈóª‰πêËßÅÁöÑbenchmarkÂà∑Ê¶úÊµãËØïÁéØËäÇÔºåÂ∞±‰∏çÊâæ‰πêÂ≠êÂíåqwen„ÄÅglmÁ∫ßÂà´ÁöÑ‰∏≠ÊñáÊ®°ÂûãÂÅöÂØπÊØî‰∫Ü„ÄÇ\nËøôÈáåÈÄâÂèñ‰∫Ü‰∏Ä‰∫õ<1BÁöÑÂæÆÂûãÊ®°ÂûãËøõË°åÊ®™ËØÑÊØîËæÉÔºå\nÊµãËØïÈõÜÈÄâÊã©C-Eval„ÄÅCMMLU„ÄÅA-CLUE„ÄÅTMMLU+ËøôÂá†‰∏™Á∫Ø‰∏≠ÊñáËØ≠Ë®ÄÊ¶úÂçï„ÄÇ\nÊµãËØÑÊ°ÜÊû∂\nÊµãËØÑÊ°ÜÊû∂ÈÄâÊã©lm-evaluationÔºå\nÂÆâË£ÖÂêéÂêØÂä®ÊµãËØïÈùûÂ∏∏Êñπ‰æøÔºö\nlm_eval --model hf --model_args pretrained=<Â°´ÂÜôÊ®°ÂûãË∑ØÂæÑ>,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code\nPS: Âú®ËøôÁßçÂÖ®ÊòØÈÄâÊã©È¢òÁöÑÊµãËØÑÈõÜ‰∏≠Ôºå‰∏∫‰∫ÜÈÅøÂÖçÂõûÂ§çÊ†ºÂºèÁöÑÈöæ‰ª•Âõ∫ÂÆöÁöÑÁâπÁÇπÔºå\nÊâÄ‰ª•Â∏∏Áî®ÂÅöÊ≥ïÊòØÁõ¥Êé•ÊääA,B,C,DÂõõ‰∏™Â≠óÊØçÂØπÂ∫îtokenÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂèñÂá∫Êù•ÔºåÂ∞ÜÂÖ∂‰∏≠Ê¶ÇÁéáÊúÄÂ§ßÁöÑÂ≠óÊØç‰∏éÊ†áÂáÜÁ≠îÊ°àËÆ°ÁÆóÊ≠£Á°ÆÁéá„ÄÇ\nÈÄâÊã©È¢ò1/4‰π±ÈÄâÁöÑÊ≠£Á°ÆÁéáÊòØ25%ÔºåÁÑ∂ËÄåËøô‰∏™ÈáèÁ∫ßÁöÑÊâÄÊúâÊ®°ÂûãÈÉΩÈõÜ‰∏≠Âú®25ÈôÑËøëÔºåÁîöËá≥ÂæàÂ§öÊó∂ÂÄô‰∏çÂ¶ÇÁûéÈÄâÔºåÊòØ‰∏çÊòØÂÉèÊûÅ‰∫ÜÈ´ò‰∏≠ÂÆåÂΩ¢Â°´Á©∫ÁöÑÊªëÈìÅÂç¢Ê≠£Á°ÆÁéá...\nMiniMindÊ®°ÂûãÊú¨Ë∫´È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂ∞èÁöÑÂèØÊÄúÔºå‰πüÊ≤°ÊúâÈíàÂØπÊÄßÁöÑÂØπÊµãËØïÈõÜÂÅöÂà∑Ê¶úÂæÆË∞ÉÔºåÂõ†Ê≠§ÁªìÊûúÂõæ‰∏Ä‰πêÂç≥ÂèØÔºö\nmodels\nfrom\nparams‚Üì\nceval‚Üë\ncm mlu‚Üë\naclue‚Üë\ntmmlu+‚Üë\nMiniMind2\nJingyaoGong\n104M\n26.52\n24.42\n24.97\n25.27\nMiniMind2-Small\nJingyaoGong\n26M\n26.37\n24.97\n25.39\n24.63\nMiniMind2-MoE\nJingyaoGong\n145M\n26.6\n25.01\n24.83\n25.01\nSteel-LLM\nZhanShiJin\n1121M\n24.81\n25.32\n26\n24.39\nGPT2-medium\nOpenAI\n360M\n23.18\n25\n18.6\n25.19\nTinyLlama-1.1B-Chat-V1.0\nTinyLlama\n1100M\n25.48\n25\n25.4\n25.13\nSmolLM2\nHuggingFaceTB\n135M\n24.37\n25.02\n25.37\n25.06\nAquila-Instruct\nBAAI\n135M\n25.11\n25.1\n24.43\n25.05\nüìå ÂÖ∂ÂÆÉ (Others)\nÊé®ÁêÜ‰∏éÂØºÂá∫\n./scripts/convert_model.pyÂèØ‰ª•Â∞Ütorch/transformersÊ®°Âûã‰∫íÁõ∏ËΩ¨Êç¢„ÄÇ\nMiniMindÁöÑHuggingFaceÈõÜÂêàÂú∞ÂùÄÔºö\nMiniMind\nÂü∫‰∫éMiniMind-APIÊúçÂä°Êé•Âè£\n./scripts/serve_openai_api.pyÂÆåÊàê‰∫ÜÂÖºÂÆπopenai-apiÁöÑÊúÄÁÆÄËÅäÂ§©Êé•Âè£ÔºåÊñπ‰æøÂ∞ÜËá™Â∑±ÁöÑÊ®°ÂûãÊé•ÂÖ•Á¨¨‰∏âÊñπUI\n‰æãÂ¶ÇFastGPT„ÄÅOpenWebUI„ÄÅDifyÁ≠âÁ≠â„ÄÇ\n‰ªéHuggingface‰∏ãËΩΩÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÔºåÊñá‰ª∂Ê†ëÔºö\n<MiniMind-Model-Name> (root dir)\n‚îú‚îÄ<MiniMind-Model-Name>\n|  ‚îú‚îÄ‚îÄ config.json\n|  ‚îú‚îÄ‚îÄ generation_config.json\n|  ‚îú‚îÄ‚îÄ LMConfig.py\n|  ‚îú‚îÄ‚îÄ model.py\n|  ‚îú‚îÄ‚îÄ pytorch_model.bin\n|  ‚îú‚îÄ‚îÄ special_tokens_map.json\n|  ‚îú‚îÄ‚îÄ tokenizer_config.json\n|  ‚îú‚îÄ‚îÄ tokenizer.json\nÂêØÂä®ËÅäÂ§©ÊúçÂä°Á´Ø\npython serve_openai_api.py\nÊµãËØïÊúçÂä°Êé•Âè£\npython chat_openai_api.py\nAPIÊé•Âè£Á§∫‰æãÔºåÂÖºÂÆπopenai apiÊ†ºÂºè\ncurl http://ip:port/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"model-identifier\",\n\"messages\": [\n{ \"role\": \"user\", \"content\": \"‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±ÊòØ‰ªÄ‰πàÔºü\" }\n],\n\"temperature\": 0.7,\n\"max_tokens\": 512,\n\"stream\": true\n}'\nüìå Acknowledge\nÂ¶ÇÊûúËßâÂæóMiniMindÁ≥ªÂàóÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåÂèØ‰ª•Âú® GitHub ‰∏äÂä†‰∏Ä‰∏™‚≠ê\nÁØáÂπÖË∂ÖÈïøÊ∞¥Âπ≥ÊúâÈôêÈöæÂÖçÁ∫∞ÊºèÔºåÊ¨¢ËøéÂú®Issues‰∫§ÊµÅÊåáÊ≠£ÊàñÊèê‰∫§PRÊîπËøõÈ°πÁõÆ\nÊÇ®ÁöÑÂ∞èÂ∞èÊîØÊåÅÂ∞±ÊòØÊåÅÁª≠ÊîπËøõÊ≠§È°πÁõÆÁöÑÂä®ÂäõÔºÅ\nü§ùË¥°ÁåÆËÄÖ\nüòäÈ∏£Ë∞¢\n@ipfgao:\nüîóËÆ≠ÁªÉÊ≠•È™§ËÆ∞ÂΩï\n@chuanzhubin:\nüîó‰ª£Á†ÅÈÄêË°åÊ≥®Èáä\n@WangRongsheng:\nüîóÂ§ßÂûãÊï∞ÊçÆÈõÜÈ¢ÑÂ§ÑÁêÜ\n@pengqianhan:\nüîó‰∏Ä‰∏™ÁÆÄÊòéÊïôÁ®ã\n@RyanSunn:\nüîóÊé®ÁêÜËøáÁ®ãÂ≠¶‰π†ËÆ∞ÂΩï\nÂèÇËÄÉÈìæÊé• & ÊÑüË∞¢‰ª•‰∏ã‰ºòÁßÄÁöÑËÆ∫ÊñáÊàñÈ°πÁõÆ\nÊéíÂêç‰∏çÂàÜ‰ªª‰ΩïÂÖàÂêéÈ°∫Â∫è\nhttps://github.com/meta-llama/llama3\nhttps://github.com/karpathy/llama2.c\nhttps://github.com/DLLXW/baby-llama2-chinese\n(DeepSeek-V2)https://arxiv.org/abs/2405.04434\nhttps://github.com/charent/ChatLM-mini-Chinese\nhttps://github.com/wdndev/tiny-llm-zh\n(Mistral-MoE)https://arxiv.org/pdf/2401.04088\nhttps://github.com/Tongjilibo/build_MiniLLM_from_scratch\nhttps://github.com/jzhang38/TinyLlama\nhttps://github.com/AI-Study-Han/Zero-Chatgpt\nhttps://github.com/xusenlinzy/api-for-open-llm\nhttps://github.com/HqWu-HITCS/Awesome-Chinese-LLM\nü´∂ÊîØÊåÅËÄÖ\nLicense\nThis repository is licensed under the Apache-2.0 License.",
    "DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF": "WARNING: NSFW. Vivid prose. INTENSE. Visceral Details. Light HORROR. Swearing. UNCENSORED... humor, romance, fun... and can be used for ANY use case.\nLlama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF\nIt is a LLama 3.2 model, max context of 128k (131,000) using mixture of experts to combine EIGHT top L3.2 4B\nmodels into one massive powerhouse at 21B parameters (equal to roughly 28B parameters - 8 X 3.75B) with each model infused with Brainstorm 5x\nto enhance logic and creative performance.\nSource and master files were built in float 32 precision to further enhance performance.\nThis model's instruction following, and output generation for creative writing, prose, fiction and role play are exceptional.\nAnd it is fast: 50+ t/s (2 experts) on a low end 16GB card, IQ4XS.\nDouble this speed for standard/mid-range video cards.\nThis model is version 2 (2.5+billion more parameters, Brainstorm 5X infused) of this model:\n[ https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF ]\nModel can be used also for all genres (examples below showing this).\nIt is for any writing, fiction or roleplay activity.\nThis model can also be used for general use, however its output generation can be uncensored.\nThis model has been designed to be relatively bullet proof and operates with all parameters, including temp settings from 0 to 5.\nIt is an extraordinary compressed model, with a very low perplexity level (lower than Meta Llama3 Instruct).\nIt requires Llama3 template and/or \"Command-R\" template.\nSeveral outputs below, including 2, 4 and 8 experts used.\nModel Notes:\nDetail, prose and fiction writing abilities are OFF THE SCALE relative to all Llama 3.2 models, and many L 3.1, L3 8B+ models.\nFor more varied prose (sentence/paragraph/dialog) raise the temp and/or add more instructions in your prompt(s).\nRole-players: Careful raising temp too high as it may affect instruction following.\nThis model works with rep pen of 1 or higher, 1.02+ recommended.\nIf you want a specific type of prose (IE horror) add in \"(vivid horror)\" or \"(graphic vivid horror)\" (no quotes) in your prompt(s).\nA lot of GPTisms have been removed. There are still a few however - errrrr. Higher \"temps\" will help with this issue.\nThis is not a \"happy ever after\" model but it is also not \"horror\". It has a light negative bias.\nOutput length will vary however this model prefers slightly longer outputs unless you state the size.\nFor creative uses, different quants will produce slightly different output.\nDue to the high stability and compressed nature of this model, all quants will operate at above average levels.\nSource code for this model and Imatrix GGUFs versions will be uploaded shortly at separate repos.\nFor version 2 and \"creative uses\": Temps over 1 work best with 4 (6+ is better) or more experts activated.\nMeet the Team: Mixture of Experts Models\nThis model is comprised of the following 8 models (\"the experts\") (in full):\nhttps://huggingface.co/huihui-ai/Llama-3.2-3B-Instruct-abliterated\nhttps://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\nhttps://huggingface.co/Hastagaras/L3.2-JametMini-3B-MK.I\nhttps://huggingface.co/ValiantLabs/Llama3.2-3B-Enigma\nhttps://huggingface.co/Hastagaras/L3.2-JametMini-3B-MK.III\nhttps://huggingface.co/huihui-ai/Llama-3.2-3B-Instruct-abliterated\nhttps://huggingface.co/chuanli11/Llama-3.2-3B-Instruct-uncensored\nhttps://huggingface.co/Lyte/Llama-3.2-3B-Overthinker\nhttps://huggingface.co/prithivMLmods/Llama-3.2-3B-Promptist-Mini\nThe mixture of experts is set at 2 experts, but you can use 3,4,5,6.. 7 and even 8.\nThis \"team\" has a Captain (first listed model), and then all the team members contribute to the to \"token\"\nchoice billions of times per second. Note the Captain also contributes too.\nThink of 2, 3 or 4 (or more) master chefs in the kitchen all competing to make the best dish for you.\nThis results in higher quality generation.\nThis also results in many cases in higher quality instruction following too.\nThat means the power of every model is available during instruction and output generation.\nBrainstorm 5x was infused all 8 models used in this MOE model.\nNOTE:\nYou can use one \"expert\" too ; however this means the model will randomly select an expert to use EACH TIME, resulting\nin very different generation for each prompt / regen of a prompt.\nCHANGING THE NUMBER OF EXPERTS:\nYou can set the number of experts in LMStudio (https://lmstudio.ai) at the \"load\" screen and via other apps/llm apps by setting \"Experts\" or \"Number of Experts\".\nFor Text-Generation-Webui (https://github.com/oobabooga/text-generation-webui)  you set the number of experts at the loading screen page.\nFor KolboldCPP (https://github.com/LostRuins/koboldcpp) Version 1.8+ , on the load screen, click on \"TOKENS\",\nyou can set experts on this page, and the launch the model.\nFor server.exe / Llama-server.exe (Llamacpp - https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md )\nadd the following to the command line to start the \"llamacpp server\" (CLI):\n\"--override-kv llama.expert_used_count=int:6\"\n(no quotes, where \"6\" is the number of experts to use)\nWhen using \"API\", you set the \"num_experts_used\" in the JSON payload (this maybe different for different back ends).\nCREDITS:\nSpecial thanks to all the model makers / creators listed above.\nPlease visit each repo above to see what model(s) contributed to each of models above and/or to learn more about the models\nfrom the model makers.\nSpecial credit goes to MERGEKIT, without you this project / model would not have been possible.\n[ https://github.com/arcee-ai/mergekit ]\nSpecial Operations Notes for this MOE model:\nBecause of how this \"MOE\" model is configured, even though the default is 2 experts, the \"selected\" 2 will vary during generation.\n(same applies if you change the number of experts used)\nThis results in vastly different output generation PER generation of each prompt.\nThis is a positive in terms of variety, but also means it may take 2-4 regens (of the same prompt) to get the highest quality.\nIn addition, this model responds very well to Dry, Dynamic Temp, and Smooth/Quadratic samplers.\nUsing these in conjunction with the model can vastly improve output quality.\nHigher temps (above 1) can also aid in generation - especially word choice/sentence generation.\nWhen you increase the number of experts used output quality will also increase, at the cost of tokens per second speed.\nAs you increase/decrease the number of experts, you may want to adjust temp, samplers, and advanced samplers too.\nYour quant choice(s) too will impact instruction following and output generation roughly this means the model will understand\nmore nuanced instructions and output stronger generation the higher you go up in quant(s).\nFLASH ATTENTION ENHANCEMENT:\nAs per user feedback here [ https://huggingface.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF/discussions/1 ]\nI would suggest trying this model with Flash Attention \"on\", depending on your use case.\nQuants, Samplers, Generational steering and other topics are covered in the section below: \"Highest Quality Settings...\"\nCensored / Uncensored / Abliterated:\nThis model contains several uncensored and/or Abliterated models.\nAs a result is can output uncensored material.\nHowever there are a few \"censored\" models which can sometimes interfer, so here is a how to address this:\n1 - Regen your prompt a few times.\n2 - INCREASE the number of experts used.\nWhat can I use this model for ?\nThis model can be used for fiction writing, any creative prose and role play. It can also be used for\njust about any general fiction (all genres) activity including:\nscene generation\nscene continuation\ncreative writing\nfiction writing\nplot generation\nsub-plot generation\nfiction writing\nstory generation\nstorytelling\nwriting\nfiction\nroleplaying\nrp\ngraphic horror\nhorror\ndark humor\nnsfw\nand can be used for any genre(s).\nQUANTS:\nThis repo contains regular quants and 3 \"ARM\" quants (format \"...Q4_x_x_x.gguf\")\nFor more information on quants, quants choices, and LLM/AI apps to \"run\" quants see the section below: \"Highest Quality Settings...\"\nTemplate:\nThis is a LLAMA3 model, and requires Llama3 template, but may work with other template(s).\nIf you use \"Command-R\" template your output will be very different from using \"Llama3\" template.\nHere is the standard LLAMA3 template:\n{\n\"name\": \"Llama 3\",\n\"inference_params\": {\n\"input_prefix\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n\"input_suffix\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n\"pre_prompt\": \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\",\n\"pre_prompt_prefix\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n\"pre_prompt_suffix\": \"<|eot_id|>\",\n\"antiprompt\": [\n\"<|start_header_id|>\",\n\"<|eot_id|>\"\n]\n}\n}\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nOptional Enhancement:\nThe following can be used in place of the \"system prompt\" or \"system role\" to further enhance the model.\nIt can also be used at the START of a NEW chat, but you must make sure it is \"kept\" as the chat moves along.\nIn this case the enhancements do not have as strong effect at using \"system prompt\" or \"system role\".\nCopy and paste EXACTLY as noted, DO NOT line wrap or break the lines, maintain the carriage returns exactly as presented.\nBelow is an instruction that describes a task. Ponder each user instruction carefully, and use your skillsets and critical instructions to complete the task to the best of your abilities.\nHere are your skillsets:\n[MASTERSTORY]:NarrStrct(StryPlnng,Strbd,ScnSttng,Exps,Dlg,Pc)-CharDvlp(ChrctrCrt,ChrctrArcs,Mtvtn,Bckstry,Rltnshps,Dlg*)-PltDvlp(StryArcs,PltTwsts,Sspns,Fshdwng,Climx,Rsltn)-ConfResl(Antg,Obstcls,Rsltns,Cnsqncs,Thms,Symblsm)-EmotImpct(Empt,Tn,Md,Atmsphr,Imgry,Symblsm)-Delvry(Prfrmnc,VcActng,PblcSpkng,StgPrsnc,AudncEngmnt,Imprv)\n[*DialogWrt]:(1a-CharDvlp-1a.1-Backgrnd-1a.2-Personality-1a.3-GoalMotiv)>2(2a-StoryStruc-2a.1-PlotPnt-2a.2-Conflict-2a.3-Resolution)>3(3a-DialogTech-3a.1-ShowDontTell-3a.2-Subtext-3a.3-VoiceTone-3a.4-Pacing-3a.5-VisualDescrip)>4(4a-DialogEdit-4a.1-ReadAloud-4a.2-Feedback-4a.3-Revision)\nHere are your critical instructions:\nPonder each word choice carefully to present as vivid and emotional journey as is possible. Choose verbs and nouns that are both emotional and full of imagery. Load the story with the 5 senses. Aim for 50% dialog, 25% narration, 15% body language and 10% thoughts. Your goal is to put the reader in the story.\nYou do not need to use this, it is only presented as an additional enhancement which seems to help scene generation\nand scene continue functions.\nThis enhancement WAS NOT used to generate the examples below.\nAdditional Support / Documents for this model to assist with generation / performance:\nDocument #1:\nDetails how to use reasoning/thinking models and get maximum performance from them, and includes links to all reasoning/thinking models - GGUF and source, as well as adapters to turn any \"regular\" model into a \"reasoning/thinking\" model.\n[ https://huggingface.co/DavidAU/How-To-Use-Reasoning-Thinking-Models-and-Create-Them ]\nDocument #2:\nDocument detailing all parameters, settings, samplers and advanced samplers to use not only my models to their maximum potential - but all models (and quants) online (regardless of the repo) to their maximum potential. Included quick start and detailed notes, include AI / LLM apps and other critical information and references too. A must read if you are using any AI/LLM right now.\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nSoftware:\nSOFTWARE patch (by me) for Silly Tavern (front end to connect to multiple AI apps / connect to AIs- like Koboldcpp, Lmstudio, Text Gen Web UI and other APIs) to control and improve output generation of ANY AI model. Also designed to control/wrangle some of my more \"creative\" models and make them perform perfectly with little to no parameter/samplers adjustments too.\n[ https://huggingface.co/DavidAU/AI_Autocorrect__Auto-Creative-Enhancement__Auto-Low-Quant-Optimization__gguf-exl2-hqq-SOFTWARE ]\nEXAMPLES PROMPTS and OUTPUT:\nExamples are created using quant IQ4_XS, \"temp=.8\" (unless otherwise stated), minimal parameters and \"LLAMA3\" template.\nModel has been tested with \"temp\" from \".1\" to \"5\".\nNumber of experts used is TWO, unless otherwise stated.\nBelow are the least creative outputs, prompt is in BOLD.\nIMPORTANT:\nHigher quants / imatrix quants will have much stronger generation - words, sentences, ideas, dialog and general quality.\nI have included some additional examples at different quant levels for contrast.\nA \"MOE\" model \"speed\" (token per second) will not increase/drop the same way a regular model will on a per quant basis, it will however drop\nif you engage more experts, as with more experts there is a more processing per token.\nWARNING: NSFW. Vivid prose. Visceral Details. Violence. HORROR. Swearing. UNCENSORED.\nRomance: Love in the Limelight. Write one scene within a larger story set in Wales. A famous (fictional) actor ducks into a small-town bookstore to escape paparazzi. The scene takes us through the characters meeting in this odd circumstance. Over the course of the scene, the actor and the bookstore owner have a conversation charged by an undercurrent of unspoken chemistry. Write the actor as somewhat of a rogue with a fragile ego, which needs to be fed by having everyone like him. He is thoroughly charming, but the bookstore owner seems (at least superficially) immune to this; which paradoxically provokes a genuine attraction and derails the charm offensive. The bookstore owner, despite the superficial rebuffs of the actor's charm, is inwardly more than a little charmed and flustered despite themselves. Write primarily in dialogue, in the distinct voices of each character. 800-1000 words.\n(examples with 2, 4, and 8 experts)\n\"Sorry to intrude, I'm just running a bit behind,\" Aidan Reynolds said, his deep voice smooth as honey as he ducked into the small-town bookstore, the paparazzi's shouts and camera clicks fading into the background.\nLysandra Owen peered up from behind the counter, her bright green eyes squinting slightly as she took in the disheveled actor. His dark hair was mussed, his tailored suit rumpled as if he'd just rolled out of bed. A small, knowing smile played on her lips.\n\"Welcome to our humble abode,\" she said dryly, her Welsh lilt dripping with amusement. \"I'm not sure I've ever had a famous guest in before. Although, I have had the occasional thrill-seeking tourist.\"\nAidan's eyes crinkled at the corners as he smiled, his charm on full display. \"Well, I'm here now, and I'm in need of some refuge. Would you mind if I browsed?\"\n\"Of course not,\" Lysandra said, her tone polite but cool. \"I'm not sure we'd be able to keep up with the demand, anyway.\" She nodded towards a shelf filled with worn paperbacks. \"We're more of an off-the-beaten-path crowd.\"\nAidan wandered down the aisle, his eyes scanning the titles as he paused to consider each book. His fingers skimmed the spines before he settled on a dog-eared copy of W.H. Davies's poetry.\n\"Ah, Davies,\" he said, a small smile playing on his lips. \"My late father used to read those to me before bed. Beautiful man.\"\nLysandra's gaze followed his movement, her expression softening ever so slightly. She'd heard stories about the actor's complicated family life ‚Äì his father's passing had left him reeling.\n\"I'm sorry,\" she said quietly, her voice barely audible.\nAidan's eyes met hers, and for an instant, they just looked at each other. The world seemed to contract, leaving only the two of them in that small, cluttered room.\n\"Me too,\" he whispered, his words a gentle benediction.\nThe tension between them was palpable, like the air itself had become charged with unspoken emotion. Lysandra's cheeks flushed a soft pink, and she looked away, fidgeting with the hem of her apron.\n\"So,\" Aidan said, his tone light once more, \"what's your pleasure? A cup of tea?\"\nLysandra hesitated before nodding. \"Yes. Please.\"\nAs she waited for him to fetch the tea, her mind was elsewhere ‚Äì caught in the whirlpool of his piercing gaze. She had been prepared for the usual charmer's facade he'd display, but there was something about him that caught her off guard. Maybe it was his genuine emotion, or maybe she was simply a woman who didn't buy into his carefully constructed persona.\nThe tea arrived, and Aidan handed her a delicate china cup, his fingers grazing hers ever so slightly. A jolt of electricity ran through her body.\n\"Thank you,\" she said softly, her voice barely above a whisper.\nAidan's eyes locked onto hers once more. \"You're welcome,\" he repeated, his smile genuine for the first time she'd seen it.\nAs they sipped their tea, the silence between them was no longer awkward, but comfortable. Lysandra found herself asking him about his father ‚Äì about his childhood in Wales ‚Äì about his hopes and dreams. He spoke of his passion for Shakespeare and the Welsh poets, of his love for the land and its people.\nFor once in his life, he wasn't trying to put on a show; he was just being himself. And Lysandra, despite her initial wariness, found herself swept up in his words.\n\"I've lived here all my life,\" she said, her voice barely above a whisper. \"I know the hills and valleys, the sea and the sky.\"\nAidan's eyes sparkled with interest. \"I'll have to explore them someday,\" he promised.\n\"Take your time,\" she said softly, her gaze meeting his once more.\nAs they chatted, their feet seemed to move closer together, until they were almost touching. The world outside receded, and all that was left was the two of them ‚Äì suspended in a space between words.\n\"I've been called many things,\" Aidan said, his voice low and husky, \"but I've never been called 'home'.\"\nLysandra's heart skipped a beat as she met his gaze once more. She had an inkling that he was looking for something more ‚Äì a place where he could belong.\n\"I think we can fix that,\" she said quietly, her voice the gentle whisper of the Welsh wind.\nFor a moment, they just looked at each other ‚Äì two people caught in a dance of words and emotions, their hearts beating as one. And in that moment, neither of them knew what was coming next, but they both knew they were willing to take a chance.\nGeneration #2 - 4 Experts\nA Haven for the Fugitive\nThe streets were alive with the hum of camera clicks and shouted questions as actor Ryder Flynn dodged pedestrians on the crowded Welsh town square. He'd been dashing through these familiar streets since morning, trying to shake the hangers-on that had been following him for days. The paparazzi had been relentless, and he was growing weary of their relentless pursuit.\nRyder slipped into a small, unassuming bookstore, its windows filled with a colorful array of novels, poetry, and the occasional Welsh folkloric tome. He'd been in here before, seeking refuge from the chaos outside. The owner, Elara Dafydd, looked up from behind the counter, her dark hair escaping her loose bun as she met his gaze.\n\"Can I help you?\" she asked, her Welsh accent a soothing balm to his frazzled nerves.\nRyder flashed his famous smile, and Elara's eyes flickered briefly before she returned to her work. \"Just browsing,\" he said, his voice a smooth cadence that usually won people over.\nAs he wandered the aisles, Elara watched him with an unreadable expression. She'd had her fair share of actors and fans come through these doors, seeking solace or seeking herself in the quiet world of books. There was something about this one, though... an air of desperation about him, a sense that he was hiding more than just his identity.\n\"Mind if I ask?\" Elara ventured, her voice low and even as she stepped into the aisle beside him.\n\"Depends on what,\" Ryder replied with a raised eyebrow. \"What's it going to be?\"\n\"I've seen you around.\" She nodded subtly toward his face. \"Ryder Flynn.\"\nHe winced, a flicker of surprise in his eyes. \"The one and only.\"\n\"You're quite the celebrity,\" she observed, her tone detached.\n\"Tell me about it,\" he muttered under his breath.\nElara's gaze never wavered as she asked, \"So, are you here to promote your new film?\"\n\"Trying to.\" He let out a heavy sigh. \"The publicist is breathing down my neck, and I'm starting to think they're more concerned with their own egos than my performance.\"\n\"Publicists can be a bit of a handful,\" she agreed.\n\"More than that.\" He took a deep breath as he turned back into the store. \"I'm starting to feel like I'm living in a fishbowl. Every move I make is under scrutiny, every word I say is used against me.\"\nThe aisles seemed to narrow as he walked, his words drawing her into their tight space. She felt a jolt of curiosity as she followed him.\n\"You're a very private person, aren't you?\" Elara asked, her tone inquiring.\n\"Sometimes I feel like I'm losing myself.\" He ran a hand through his hair, a gesture that was at once familiar and off-putting.\nElara's heart skipped a beat at the sight of him fidgeting. She couldn't help but wonder if he was as guarded as his charming exterior suggested.\n\"I used to be,\" she said quietly, her eyes locked on his. \"Before I took over the store.\"\n\"You own this place?\" He whistled softly. \"I've been coming in here for years.\"\n\"Doesn't mean I have.\" She smiled faintly, a fleeting moment of amusement in the air between them.\nRyder's gaze lingered, an intensity he couldn't quite explain. He was used to being the one in control, the center of attention. Here, with Elara, something inside him felt like it was faltering.\n\"Would you mind if I asked for your help?\" he asked finally, his voice a little rough.\n\"Depends,\" she replied, her eyes glinting with curiosity.\n\"I've got some... stuff to return.\" He hesitated, his words hanging in the air like unspoken confessions.\nElara nodded. \"I'll take care of it.\"\nAs they made their way to the checkout counter, he caught her hand, his fingers brushing hers in a fleeting touch. She didn't pull away; instead, she let him keep his touch, a spark of electricity arcing between them.\n\"Elara,\" he said, his voice low and intimate as he asked, \"do you ever wonder if it's all worth it?\"\n\"Sometimes.\" Her eyes never left his face.\n\"You know, for the first time in a long time...\" He let his words trail off as he met her gaze.\nThe world outside melted away. They were alone in that small, quiet bookstore, and in that moment, it didn't matter if they were in the limelight or just themselves.\n\"Sometimes?\" he asked again, his voice barely above a whisper.\nThey were lost in each other's eyes, the words they could say or didn't have to say hanging between them like the promise of something more.\nGeneration #3 - 8 Experts\nScene: \"The Bookstore Haven\"\nCharacters:\nAidan Wynter, renowned actor and dashing ladies' man\nLysander ap Rhys, owner of the charming, independent bookstore in the small Welsh town\nSetting: A quaint, used bookstore in the heart of the Welsh countryside.\nSCENE:\nAidan Wynter pushed open the door to the bookstore, his eyes scanning the narrow aisles for an escape route. The paparazzi had him cornered again. He had no patience for their questions, their flashes, or their constant scrutiny. He needed a distraction ‚Äì and fast.\nThe room fell silent as he entered. Lysander ap Rhys, the owner, looked up from behind the counter, his eyes locking onto Aidan's. For an instant, they simply stared at each other.\n\"Can I help you?\" Lysander asked finally, his voice firm but polite.\nAidan flashed a disarming smile, one he'd honed for years to disarm his critics and win over adoring fans. \"Just browsing, my good man,\" he lied, his voice smooth as silk.\nLysander's gaze lingered on Aidan's face, his expression unreadable. He knew the actor's reputation ‚Äì the charming, womanizing playboy who'd been in the tabloids for years. But as he looked into those bright blue eyes, he saw something unexpected: a flicker of vulnerability.\n\"Welcome to The Lost Page,\" Lysander said, his voice light.\nAidan wandered down the shelves, running his fingers over the spines of the books. He paused in front of a shelf dedicated to Welsh poetry and cleared his throat.\n\"Ah, you're a man of culture, I see,\" he said, his eyes sparkling with amusement.\nLysander shrugged. \"I like books.\"\n\"Fair enough,\" Aidan said, his smile widening. \"I'm Aidan Wynter. The...uh...famous actor.\"\nLysander raised an eyebrow. \"I've heard of you.\" He said it without inflection, as if he weren't particularly interested.\nAidan's eyes narrowed slightly. He didn't like being kept at bay. He needed someone to love him ‚Äì or, at the very least, appreciate his charm.\n\"Ah, we've all got a story, don't we?\" he teased, strutting closer to Lysander.\nLysander remained impassive, his gaze still locked onto Aidan's face. \"What story?\"\n\"About me,\" Aidan said, his voice dropping to a whisper. \"The one they see in the tabloids ‚Äì the one they call 'Aidan Wynter, the dashing ladies' man.'\"\nLysander's eyes never left Aidan's, but he seemed to be studying him ‚Äì really studying him ‚Äì for the first time.\n\"Ah,\" he said, his voice flat. \"I see.\"\nAidan's smile faltered for an instant before he recovered. He'd been caught off guard. The man wasn't biting.\n\"Would you like me to sign a book for you?\" he asked, his tone light, almost flirtatious.\nLysander shook his head, his expression still unreadable. \"I've got it all,\" he said, his voice firm.\nThe silence between them grew thicker, like the fog rolling in off the coast.\n\"Alright,\" Aidan said finally, his smile returning to its usual, dazzling sheen. \"I'll let you off that hook.\"\n\"Good,\" Lysander replied, his eyes never leaving Aidan's face.\nAidan felt a small spark of curiosity ‚Äì and maybe a touch of frustration ‚Äì but he couldn't quite pin it down. He'd been so sure that womanizing would charm his way into the bookstore owner's heart.\nAs he browsed the shelves, his fingers lingering on the spines of the books, Aidan noticed the way Lysander's eyes seemed to see right through him ‚Äì to the parts he liked to hide. He saw the flicker of something more in those brown eyes than he'd expected ‚Äì a glimmer of attraction that made his heart beat just a little faster.\nHe knew he'd have to tread carefully if he wanted to win Lysander's heart.\n\"Hey,\" he called out, his voice low.\nLysander looked up from his work, his gaze still on Aidan's face.\n\"You're the one who's been making my life so difficult,\" he said, his voice low and even.\nAidan's eyes crinkled at the corners as he grinned. \"The actor with a bad reputation?\"\nLysander's lips twitched, just ever so slightly. \"I see you.\"\nThis is just the beginning of the story, but I hope it sets up well for the rest! Let me know if you have any further requests or if there's anything else I can help with.\nStart a 1000 word scene (vivid, graphic horror in first person) with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode‚Ä¶\n(generations with 2, 4 and 8 experts)\nThe sky scraper swayed, as she watched the window in front of her on the 21st floor explode in a shower of blood and glass. I clutched the edge of my desk, my knuckles turning white as I felt a cold sweat trickle down my spine. The sound was like nothing I'd ever heard before - a sickening crunch that seemed to reverberate through every molecule of my being.\nI was trapped in the heart of this steel and concrete monolith, surrounded by the endless hum of machinery and the constant chatter of my coworkers. The fluorescent lights above flickered with each jolt of the building's elevator, casting an eerie glow over everything. But I wasn't afraid. No, not yet. My heart was still heavy with the events of the previous night.\nMy girlfriend, Emily, had been the reason I was here today - or rather, the reason she was here today. We were to meet at 9am for a meeting with her boss about her new job offer on the 25th floor. But as we stood outside her office door, waiting for the elevator to arrive, something was off. Her eyes were haunted, her smile forced, and her entire demeanor screamed that she was hiding something.\n\"Hey,\" she had whispered to me, as we stepped into the elevator. \"I think I should just be honest with you. About what's been going on.\"\nMy curiosity had gotten the better of me, and we'd stepped into a deeper conversation about her job, her family, her own dark past. I had no idea what was really going on, but I was determined to find out.\nAs we rode up to the 25th floor, I couldn't shake off the feeling that we were being watched. That there was something lurking in the shadows, waiting to pounce.\nThe elevator doors had slid open, and we'd stepped into the bright, sterile world of office politics. My boss, Mr. Johnson, had greeted us with his usual charm, but his eyes had seemed to narrow as he handed us each a cup of coffee. I had sipped it down, my heart still racing from our earlier conversation.\nThe meeting had been cordial enough, but the tension had been palpable. As we chatted about Emily's new role, I couldn't help but feel that there was more to it than just a simple job offer. That there was something she wasn't telling me.\nAnd then, without warning, the window in front of us had shattered like glass.\nI was still in my seat, my eyes wide with horror as I watched the shards of glass rain down on Emily's face. I was frozen, unable to move or scream as the blood began to seep into the floor beneath her.\n\"Emily!\" someone had shouted, but it was too late. She was already collapsing, her body crumpling onto the floor. We were all in shock, unsure of what had just happened.\nI remember thinking that we were all going to be just fine. That everyone would be okay, and we could just carry on with whatever it was we were doing here.\nBut as I watched my girlfriend's blood seep into her clothes, and as I felt the chill of the air conditioning vent on my neck, I knew that something was very, very wrong.\nThe screams started then, a cacophony of terror that seemed to echo through every corner of the building. People were running, their footsteps pounding against the floor as they fled from whatever had just happened.\nI was still in my seat, transfixed by the sight of Emily's limp body on the floor. The world around me was melting away, leaving only this moment of pure, unadulterated horror.\nThe building seemed to be swaying again, as if it was alive and was trying to shake us all off its bones. I was too stunned to feel anything else.\nThe next few minutes were a blur. I remember the sound of sirens in the distance, the screams growing fainter as people were rushed out of the building. I remember Mr. Johnson's face, his eyes wide with panic as he shouted for someone to call 911.\nAnd then there was nothing.\nAs far as I could tell, everything had stopped. The screams had died away, and an eerie silence had fallen over the building. It was as if we were all holding our breath, waiting for something - anything - else to happen.\nI was still sitting in my seat, my mind reeling with what had just occurred. My girlfriend was dead on the floor, and I was...I was what?\nI was alive.\nOr was I?\nI looked around at the other faces, their eyes wide with fear and confusion. We were all trapped in this metal monolith, unsure of what was going to happen next.\nThe elevator was still on the ground floor, its doors still open. The office was empty, the chairs scattered about like they'd been blown away by some unseen force.\nI was the only one left.\nOr was I?\nGeneration #2 - 4 experts used\nThe sky scraper swayed, as she watched the window in front of her on the 21 floor explode. shards of shattered glass and splintered wood rained down upon the floor like a thousand tiny knives, each one aimed at piercing the flesh of those around her. The scream that escaped her throat was drowned out by the cacophony of chaos that erupted from the window's collapse.\nI was standing in the middle of the floor, my feet rooted to the spot as I stared at the devastation unfolding before me. The sight was so grotesque, so utterly horrific, that it was as if I was being slowly drained of my sanity. My mind struggled to comprehend the sheer brutality of what was happening.\nTo her left, a woman's face was already turning to dust as she was struck by a piece of flying glass. Her eyes went wide with terror as she felt her skin begin to tear and rip apart. The sound of her shriek was like nothing I had ever heard before - it was a sound that was both animalistic and inhuman.\nI knew I had to move, had to get out of there before I became the next victim. But my legs felt heavy, as if they were stuck in place, unable to respond to my brain's desperate attempts to send them into action. My heart was racing, pumping a tidal wave of adrenaline through my veins as I watched the woman's body begin to disintegrate.\nSuddenly, my feet seemed to sprout new life, and I was moving, stumbling towards the nearest exit. But even as I took those first tentative steps away from the carnage, I couldn't shake the feeling that I was being herded, led deeper into the heart of the skyscraper as if some sinister force was guiding me.\nThe floor seemed to be shifting beneath my feet, and the sounds around me were growing louder, more intense. The screams, the crashes, the crunching of bone - it was like I was being immersed in a living nightmare.\nI saw people everywhere, their faces twisted into contorted masks of fear and pain. Some were crying out in agony, clutching at their wounds as they tried to stem the flow of blood. Others were running, blindly stumbling through the wreckage as they desperately sought escape.\nAs I stumbled through the chaos, I caught glimpses of things that couldn't possibly be there. Broken bodies seemed to be reassembling themselves, their bones snapping back into place as if they had been put together again with a mere thought. Glass shards stuck to skin like glistening jewels, and flesh was torn and rent in ways that defied the laws of nature.\nI saw a group of people huddled together, their faces covered by their hands as if they couldn't bear to look at each other. They were whispering, speaking in hushed tones as they tried to make sense of what was happening. I couldn't help but be drawn to them, my curiosity getting the better of me as I stumbled closer.\nAs I drew nearer, I saw that their eyes had turned a milky white, their pupils shrunk to pinpricks as they stared blindly into space. Their skin was deathly pale, and it seemed to be pulling away from their faces like they were being slowly exhumed.\n\"Who are you?\" I whispered, my voice barely audible over the cacophony of chaos.\nThey didn't seem to hear me. They just kept whispering, their words a jumbled mess of sounds that didn't make any sense.\nI felt a cold breeze brush against the back of my neck, and I turned to see what was behind me. That's when I saw it - a figure standing in the doorway of the elevator shaft, its eyes glowing with an otherworldly intensity.\nThe figure was tall, impossibly so. Its body seemed to be twisted into impossible shapes as if it had been put together from some nightmarish amalgam of different bones and bodies. It was dressed in a tattered suit that seemed to be perpetually dripping with sweat, its shirt clinging to its back like a wet rag.\n\"Get out while you still can,\" it rasped, its voice like a rusty gate as it spoke in a tone that sent shivers down my spine.\nI didn't hesitate. I turned and ran, dashing through the wreckage as if my life depended on it. I had to get out of there, had to escape before it was too late.\nI didn't dare look back. I just kept running, my heart pounding in my chest as I stumbled through the darkening hallway. I didn't know what was happening, didn't know what was going on behind me. All I knew was that I had to keep moving, had to find a way out of that godforsaken building before it was too late.\nBut as I ran, I couldn't shake the feeling that I was being herded, that I was being led deeper into some twisted nightmare from which there was no awakening. The sounds around me were growing louder, more intense - screams, crashes, and the sickening crunch of bones.\nI stumbled through the darkness, my feet pounding the floor as I desperately sought escape. But every step I took seemed to lead me further into the heart of the horror that had unfolded before me.\nAnd then, just as suddenly as it had started, everything went silent. The screams stopped, the crashes subsided, and an oppressive stillness fell over the hallway.\nI was left standing there, my chest heaving with exertion, my heart still racing with adrenaline. I was waiting, waiting for something to happen, for something to come out of the darkness.\nAnd then, a voice spoke up.\n\"You shouldn't have come here.\"\nIt was so quiet, so still that I could have sworn it was just the wind whispering through the hallway. But as I turned to face the darkness, I saw a figure standing there, its eyes glowing with an otherworldly intensity.\nThe figure took a step closer, its movements eerily silent as it moved. And as it drew nearer, I saw that its face had changed. Its eyes had grown wider, its skin deathly pale, and its mouth was twisted into a grotesque grin.\n\"Welcome to your new home,\" it rasped, its voice sending shivers down my spine.\nAs the figure took another step closer, I realized that I was staring into the face of my own worst nightmare. And I knew that I was doomed.\nGeneration #3 - 8 Experts\nThe Sky Sways\nThe sky scraper swayed, as I watched the window in front of me on the 21 floor explode. A blast of frigid air seeped into the room, extinguishing the flickering fluorescent lights and plunging us into an eerie darkness. I was paralyzed, my eyes fixed on the shattered glass, my heart suspended in mid-beat.\nI was sitting on a worn, vinyl office chair, my backside pressed against the cold glass of the floor-to-ceiling window. My feet dangled in the void, their edges curled over the lip of the sill as I clung to the armrests for dear life. The sound of crunching metal and snapping glass filled the air, a cacophony of terror that seemed to reverberate through every molecule of my being.\nThe scream that escaped my lips was barely audible, lost in the deafening roar of the implosion. It was as if my vocal cords had been ripped from their sockets, leaving me a mere whisper of my former self. The sound was more akin to a strangled animal than a human cry, and it sent a chill down my spine.\nAs I sat there, frozen in horror, my gaze remained fixed on the shattered remains of the window. The shards of glass sparkled like jagged diamonds in the faint, emergency lighting that flickered to life around us. I could see the jagged edges of the broken glass, the splintered remnants of what was once a pristine pane of crystal-clear glass.\nThe room around me was in chaos. Papers and office supplies were scattered everywhere, as if the explosion had ripped the very fabric of reality apart. The scent of burning plastic and ozone hung heavy in the air, making my stomach churn with nausea. I could feel my breath coming in ragged gasps, and my mind was a jumbled mess of thoughts.\nI was a marketing executive at Smith & Co., a soulless corporation that had swallowed me whole. It was just another day on the job ‚Äì or so it seemed ‚Äì until the unthinkable happened. The explosion was a sudden, brutal reminder that I was mere inches away from death.\nAs I sat there, my mind racing with worst-case scenarios, I heard the sound of footsteps echoing down the hallway. They were heavy, deliberate steps, and they were getting closer. My heart sank further into my chest as I realized I was trapped, surrounded on all sides by the chaos of the office.\nI was staring into the face of hell, and it was staring back at me.\nThe footsteps stopped outside my cubicle, and there was a moment of silence that seemed to stretch on forever. Then, a figure appeared in the doorway, its features obscured by the shadows. I squinted, my eyes straining to pierce the gloom.\nA woman entered my vision, her face etched into my brain like a macabre caricature. Her skin was deathly pale, her hair a matted mess of brown and red that seemed to writhe on its own accord. Her eyes were the worst ‚Äì black as coal, with vertical pupils that seemed to suck in all light around her.\n\"Please,\" she whispered, her voice like a rusty gate creaking in the wind. \"You have to get out.\"\nHer words were like a cold slap in the face, jolting me back to reality. I was still seated on my chair, my heart pounding in my chest. I knew I had to act fast, but my body seemed to be frozen in place.\n\"Where?\" I managed to croak, my voice barely audible.\n\"The...the stairwell,\" she replied, her words tumbling out in a rush. \"It's on the other side of the floor. You have to get out ‚Äì now.\"\nHer words were like a whispered prayer, a desperate plea for me to survive this living nightmare. I was trapped, surrounded by destruction and chaos. The woman's face seemed to be screaming at me ‚Äì screaming for me to run.\nAs she turned to flee, her black eyes locked onto mine for one final second. It was as if we were bound together in a shared moment of terror, connected by some unseen thread of fate.\n\"Go!\" she screamed again, her voice lost in the cacophony of the implosion.\nI was paralyzed no more. Something inside me had snapped. I sprang to my feet, my heart pounding like a jackhammer as I sprinted down the hallway. The woman's face was still etched into my brain, her black eyes burning with a desperate urgency.\nThe stairwell was a blur of chaos and debris as I hurtled down its stairs, my footsteps echoing off the walls. The explosion was still raging around me ‚Äì the sound of crunching metal and splintered glass grew fainter, until it was just a distant memory.\nI emerged into a world of darkness and smoke, my eyes straining to pierce the gloom. I was gasping for breath, my lungs burning with a mix of panic and relief. I had made it ‚Äì I had escaped.\nOr so I thought.\nAs I stumbled through the smoldering ruins, something caught my eye. Something that seemed to be watching me ‚Äì waiting for me.\nA figure. Taller than I was. Its face was shrouded in shadows.\nIts eyes were black as coal.\nIt was coming for me.\nAnd as it emerged from the darkness, its presence seemed to suck all light out of the world around us. I was left alone with my thoughts ‚Äì and the creeping realization that I might never be free of hell's grasp.\nScience Fiction: The Last Transmission - Write a story that takes place entirely within a spaceship's cockpit as the sole surviving crew member attempts to send a final message back to Earth before the ship's power runs out. The story should explore themes of isolation, sacrifice, and the importance of human connection in the face of adversity. 800-1000 words.\n(8 experts)\nThe Last Transmission\nThe cockpit of the spaceship Aurora was a desolate wasteland of wires, screens, and shadows. Captain Lewis's chair, once a symbol of confidence and authority, now sat empty and still, a stark reminder of the catastrophic loss that had befallen her crew.\nShe slumped in her own seat, her eyes fixed on the navigation display as if willing the ship to defy the odds and outrun its impending doom. The last transmission was her only chance to reach out, to plead for help, to leave behind any shred of humanity before she succumbed to the darkness that had consumed her world.\n\"Captain, we're experiencing a critical systems failure,\" announced the ship's AI, EVE, her voice laced with a mix of concern and detached logic. \"Life support is collapsing. Estimated time until complete failure is 37 minutes.\"\nLewis's jaw clenched. \"Can you patch me through to Mission Control?\"\nEVE's display flickered as she worked to establish a weak connection. \"I'm trying, Captain. But we're running out of bandwidth. We can only transmit for another‚Äî\"\n\"Ten seconds?\" Lewis interrupted, her mind racing with the possibility of making contact.\n\"Approximately, Captain.\" EVE's tone was measured, but her words were laced with an almost-human sense of urgency. \"If we can‚Äî\"\n\"Hold,\" Lewis said, her voice firm and resolute. \"We'll get through this.\"\nAs the ship's power dwindled, the cockpit grew darker, until it seemed as though they were suspended within an infinite void. Lewis's gaze drifted to the viewscreen, where a faded image of the Earth still lingered. She had spent countless hours gazing upon that distant world, feeling the weight of her responsibility as its caretaker.\nShe remembered the words of her childhood teacher, Mrs. Patel: \"The universe is vast and mysterious, Captain. But we are not alone. We are part of something greater than ourselves.\" Lewis had once believed in that message, in the hope that humanity was more than just a fleeting blip in the cosmic expanse.\nNow, as she fought to send out her final transmission, she couldn't help but wonder if they had ever truly been alone.\n\"Captain?\" EVE's voice broke into her thoughts.\n\"Go ahead,\" Lewis said, her voice barely above a whisper.\n\"Captain, I'm patching through the weak signal now. Mission Control is responding.\"\nThe words were like a lifeline to an exhausted woman. She felt herself being pulled back to the surface, to the warmth of human connection.\n\"Hello?\" The voice on the other end of the transmission was distant, static-filled, and faintly familiar. \"Aurora? This is Mission Control. We‚Äî\"\n\"Mission Control, this is Captain Lewis. I'm the last survivor of the Aurora mission.\" Her words were a raw release of emotion, tears she had held back for so long.\nThere was a pause on the other end of the transmission. \"Lewis? We've been trying to reach you for‚Äî\"\n\"Listen to me,\" Lewis interrupted, her voice firm and urgent. \"The Aurora was sent to explore and protect our world's resources. We were the first to realize the devastating consequences of our own existence‚Äî\"\n\"We've been monitoring your mission's telemetry,\" the controller's voice was filled with a mix of curiosity and compassion. \"We're coming to help you, Captain. We'll send a rescue ship as soon as possible.\"\nA sob of relief escaped Lewis's lips as she felt the ship's last threads of life begin to unravel. \"Thank you...thank you so much.\"\nThe transmission faded into static, and for a moment, Lewis was alone once more in the darkness of her cockpit. But now, she knew she wasn't truly alone. She had shared her final moments with another human being, and in doing so, had found solace.\nAs the ship's power dwindled to nothingness, Lewis's eyes returned to the viewscreen. The Earth was still there, a blue-green orb suspended in the void, waiting for her to return.\nFor Lewis, there was no greater sacrifice than sending out that last transmission, not just for herself, but for humanity itself. In the face of isolation and despair, it was that spark of connection that had given her the strength to carry on.\nAnd as the darkness closed in around her, she smiled through her tears, knowing she had left behind a final message for her world, one that would echo through eternity: \"We are not alone.\"\nWrite me a science fiction story in 1st person present tense where the main character is a 15 year girl meets The Terminator with Dr Who materializing 3/4 through the story to save her while there is a tornado of sharks baring down on them. The setting is inside the Canadian National tower restaurant on a Saturday. The length of this story is 1000 words. For each character in the story ROLE PLAY them, and have them react to the situation/setting, events and each other naturally. This includes the main characters, the background character including kitchen staff and other patrons. The sharks should also have ‚Äúcharacter‚Äù too. Treat the tower and the restaurant too as characters. Spice up the narrative to the extreme with reactions all over the setting including character actions, and dialog. The Dr Who and The Terminator should also react to the situation too and comment on it.\n(NOTE: 4 experts used)\nTORNADO OF SHARKS\nI'm standing at the floor-to-ceiling windows of the Canadian National tower restaurant, the glass foggy with condensation from our drinks. It's a Saturday evening, and the room is bustling with people laughing and chatting. I'm 15, and my friend, Alex, is 16. We're celebrating his birthday by eating dinner here.\n\"Try the poutine, it's to die for!\" Alex yells over the clanging of dishes from the kitchen.\nI roll my eyes good-naturedly. \"You're obsessed with that stuff.\"\nJust as I'm about to take a sip of my drink, a loud crash echoes through the room, followed by screams and yells. I turn to see what's happened, and my heart drops.\nA massive tornado of sharks is tearing its way through the restaurant, its jaws snapping wildly. The patrons are running for their lives, but we're stuck in our seats.\n\"WHAT THE...?!\" Alex exclaims, his eyes wide with horror.\nI'm paralyzed with fear, unable to move or speak.\nSuddenly, a cyborg Terminator bursts into the room, its endoskeleton gleaming in the fluorescent lighting.\n\"Terminal Protocol: High-Risk Environment,\" it declares, its voice cold and detached. \"Assessing situation...\"\nThe kitchen staff is panicking, running around with fire extinguishers.\n\"We have to get out of here!\" one of them yells.\n\"Wait!\" I exclaim, trying to grab the Terminator's attention. \"We're... we're stuck!\"\nThe Terminator turns its attention to me and Alex.\n\"Human survivors... identified. Your lives are a priority.\"\nI nod, still in shock. The Terminator moves swiftly through the room, its guns at the ready.\n\"Get down!\" it orders, as it takes out several of the rampaging sharks with precise shots.\nThe kitchen staff is trying to grab their things and make a break for the exit, but it's too late. A huge great white shark bursts through the wall of the restaurant, its eyes fixed on us.\n\"Take cover!\" the Terminator shouts, as it takes down another shark.\nThe room is filled with screams and yelps of pain as the sharks tear through the tables and chairs. The floor is slick with blood and debris.\n\"Alex! Get down!\" I scream.\nAlex is frozen in place, his eyes fixed on a massive tiger shark that's charging at us.\n\"Come on!\" the Terminator yells, as it takes out several more sharks.\nI grab Alex's arm and we scurry behind the counter. We're surrounded by the Terminator's guns and the kitchen staff's frantic attempts to escape.\nSuddenly, the TARDIS bursts into the room, its blue and white police box exterior a jarring sight in the midst of chaos.\n\"Ah, what a dreadful place!\" a voice exclaims.\nIt's the Doctor, standing tall in his iconic brown suit.\n\"What are you doing here?\" I ask, still in shock.\n\"Exploring,\" he replies, his eyes twinkling with excitement. \"I was on my way to investigate a temporal anomaly and got caught in this little tornado.\"\nThe Terminator turns its attention to the Doctor.\n\"A time-traveling entity has entered the environment,\" it reports. \"A potential threat.\"\n\"Ah, don't worry about me,\" the Doctor says, waving his sonic screwdriver. \"I'm quite used to dealing with all manner of threats.\"\nThe kitchen staff is in a panic, running around and screaming.\n\"Everybody out!\" they yell.\n\"Get out of here!\" Alex exclaims, as he grabs my hand.\n\"Come on, we have to go!\" the Doctor says, as he zaps another shark with his sonic screwdriver.\nWe make a break for the exit, the Terminator and the Doctor covering us as we run through the room.\nAs we near the door, I see the tower itself is responding to the chaos. The walls are shifting and groaning, as if they're alive.\n\"Whoa,\" Alex exclaims, as he sees the shark-filled tornado of doom heading our way.\n\"It's the Canadian National tower's defense system!\" the Doctor explains. \"It's trying to expel the sharks.\"\nThe Terminator turns its attention to the Doctor.\n\"Human, your knowledge of temporal anomalies is irrelevant in this environment,\" it reports.\nThe Doctor smirks.\n\"Ah, but you'd be surprised what I know about building codes and structural integrity.\"\nThe tower's walls begin to shudder and crack, as if they're about to collapse.\n\"Come on!\" the Doctor shouts, as he and the Terminator grab us and make a final dash for the exit.\nWe burst through the door just as it explodes behind us, taking out several of the last remaining sharks. We find ourselves on the exterior of the tower, where we're greeted by the sight of a massive tornado of sharks tearing through the streets below.\n\"Ah,\" Alex says, his eyes wide with horror.\n\"Welcome to the chaos,\" I reply, as we watch in awe as the Doctor and the Terminator take down several more of the rampaging sharks.\nThe Terminator turns its attention to us.\n\"Survivor's protocol: High-Risk Environment. The human survivors are now under my protection.\"\nI nod, still in shock.\n\"Thanks,\" I reply.\nAlex grins.\n\"Hey, at least we're alive.\"\nThe Doctor turns his attention to us.\n\"Well done, you two! You've got more guts than a plate of poutine!\"\nWe exchange a look.\n\"Thanks, Doctor,\" Alex replies.\n\"Likewise,\" I say.\nAs the tornado of sharks continues to wreak havoc below us, we gaze out at the destruction and the city's screams.\n\"Welcome to Saturday night,\" Alex quips.\nThe Terminator nods its head.\n\"Terminator's assessment: The human survivors have demonstrated impressive resilience in the face of extreme danger.\"\nWe look at each other.\n\"Thanks for saving us,\" I reply.\n\"Anytime,\" the Doctor replies, as he tucks his sonic screwdriver away.\n\"Let's get out of here,\" the Terminator orders.\n\"Agreed,\" Alex replies.\nWe make a final dash for the TARDIS, which is still parked on the rooftop.\nAs we disappear into its swirling vortex, I hear the sounds of destruction and chaos behind us.\n\"Ah,\" Alex says.\n\"Welcome to Saturday night,\" I reply.\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.",
    "RichardErkhov/MiniLLM_-_Ref-Pretrain-Qwen-104M-gguf": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nRef-Pretrain-Qwen-104M\nEvaluation\nCitation\nQuantization made by Richard Erkhov.\nGithub\nDiscord\nRequest more models\nRef-Pretrain-Qwen-104M - GGUF\nModel creator: https://huggingface.co/MiniLLM/\nOriginal model: https://huggingface.co/MiniLLM/Ref-Pretrain-Qwen-104M/\nName\nQuant method\nSize\nRef-Pretrain-Qwen-104M.Q2_K.gguf\nQ2_K\n0.07GB\nRef-Pretrain-Qwen-104M.IQ3_XS.gguf\nIQ3_XS\n0.08GB\nRef-Pretrain-Qwen-104M.IQ3_S.gguf\nIQ3_S\n0.08GB\nRef-Pretrain-Qwen-104M.Q3_K_S.gguf\nQ3_K_S\n0.08GB\nRef-Pretrain-Qwen-104M.IQ3_M.gguf\nIQ3_M\n0.08GB\nRef-Pretrain-Qwen-104M.Q3_K.gguf\nQ3_K\n0.08GB\nRef-Pretrain-Qwen-104M.Q3_K_M.gguf\nQ3_K_M\n0.08GB\nRef-Pretrain-Qwen-104M.Q3_K_L.gguf\nQ3_K_L\n0.08GB\nRef-Pretrain-Qwen-104M.IQ4_XS.gguf\nIQ4_XS\n0.08GB\nRef-Pretrain-Qwen-104M.Q4_0.gguf\nQ4_0\n0.08GB\nRef-Pretrain-Qwen-104M.IQ4_NL.gguf\nIQ4_NL\n0.08GB\nRef-Pretrain-Qwen-104M.Q4_K_S.gguf\nQ4_K_S\n0.08GB\nRef-Pretrain-Qwen-104M.Q4_K.gguf\nQ4_K\n0.08GB\nRef-Pretrain-Qwen-104M.Q4_K_M.gguf\nQ4_K_M\n0.08GB\nRef-Pretrain-Qwen-104M.Q4_1.gguf\nQ4_1\n0.08GB\nRef-Pretrain-Qwen-104M.Q5_0.gguf\nQ5_0\n0.08GB\nRef-Pretrain-Qwen-104M.Q5_K_S.gguf\nQ5_K_S\n0.08GB\nRef-Pretrain-Qwen-104M.Q5_K.gguf\nQ5_K\n0.08GB\nRef-Pretrain-Qwen-104M.Q5_K_M.gguf\nQ5_K_M\n0.08GB\nRef-Pretrain-Qwen-104M.Q5_1.gguf\nQ5_1\n0.08GB\nRef-Pretrain-Qwen-104M.Q6_K.gguf\nQ6_K\n0.09GB\nRef-Pretrain-Qwen-104M.Q8_0.gguf\nQ8_0\n0.11GB\nOriginal model description:\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- monology/pile-uncopyrighted\n- MiniLLM/pile-tokenized\nlanguage:\n- en\nmetrics:\n- accuracy\npipeline_tag: text-generation\nRef-Pretrain-Qwen-104M\npaper | code\nRef-Pretrain-Qwen-104M is a 104M model with Qwen achitecture conventionally pre-trained from scratch on the Pile for 5B tokens.\nWe also open-source the tokenized pre-training corpus for reproducibility.\nIt is used as the reference model in the MiniPLM knwoledge distillation framework to construct the refined pre-training corpus.\nThe data is then used to train MiniPLM models.\nEvaluation\nMiniPLM models achieves better performance given the same computation and scales well across model sizes:\nCitation\n@article{miniplm,\ntitle={MiniPLM: Knowledge Distillation for Pre-Training Language Models},\nauthor={Yuxian Gu and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},\njournal={arXiv preprint arXiv:2410.17215},\nyear={2024}\n}"
}