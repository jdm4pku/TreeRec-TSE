{
    "gghfez/GLM-4.6-REAP-266B-A32B-Q4_K": "What Is This?\nOriginal Model Card for GLM-4.6-REAP\nThis is Q4_K_M gguf quant of AesSedai/GLM-4.6-REAP-266B-A32B\nWhat Is This?\nAesSedai/GLM-4.6-REAP-266B-A32B was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts.\nSee the GLM-4.5-Air version by Cerebras for more details cerebras/GLM-4.5-Air-REAP-82B-A12B\nThe MTP tensors were not included in this quant (though llama.cpp hasn't implemented this feature anyway)\n** Imatrix **\nGLM-4.6-REAP-266B-A32B-imatrix.dat\nOriginal Model Card for GLM-4.6-REAP\nNote: currently non-functional because of missing mtp.safetensors file and entry in model.safetensors.index.json\nForked from https://github.com/CerebrasResearch/reap to https://github.com/AesSedai/reap to hack in GLM-4.6 support.\nProduced with:\nbash experiments/pruning-cli.sh 0,1,2,3,4,5,6,7 zai-org/GLM-4.6 reap 42 0.25 theblackcat102/evol-codealpaca-v1 true true true false false",
    "Mineral-Dev/mineral-1b": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nüß† Mineral-1B\n‚öôÔ∏è Configuration\nüöÄ Goals\nüìò Usage\nüß† Mineral-1B\nMineral-1B is a lightweight open language model under the Apache License 2.0, built using Hugging Face AutoTrain for smart and automatic model management.The project aims to create a flexible LLM capable of natural conversation, reasoning, and code assistance.\n‚öôÔ∏è Configuration\nBase model: microsoft/phi-2\nPipeline: text-generation\nLibrary: transformers\nLicense: Apache 2.0\nTraining: AutoTrain (no dataset yet ‚Äî pretrained mode)\nüöÄ Goals\nServe as a foundation for text generation and conversational tasks\nSupport English and optionally other languages\nEnable later fine-tuning with domain-specific data\nüìò Usage\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"prelington/Mineral-1B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"prelington/Mineral-1B\")\nprompt = \"Hello! What is Mineral-1B?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
    "PRIME-RL/P1-235B-A22B": "Model Description\nKey Highlights\nPerformance Benchmarks\nIPhO 2025 Results\nHiPhO Comprehensive Results\nGeneralization to STEM Tasks\nUsage\nBasic Inference\nüôè Acknowledgements\nCitation\nP1: Mastering Physics Olympiads with Reinforcement Learning\nüåê P1 Project Page |\nüèÜ HiPhO Leaderboard\nAchieving gold medal at the International Physics Olympiad (IPhO 2025)\nModel Description\nP1-235B-A22B is the flagship model of the P1 series, a state-of-the-art open-source large language model specialized in physics reasoning. Built on Qwen3-235B-A22B-Thinking-2507 and tuned through multi-stage reinforcement learning on curated physics competition data, P1-235B-A22B marks a historic achievement as the first open-source model to win gold at the International Physics Olympiad (IPhO 2025).\nKey Highlights\nüèÜ IPhO 2025 Gold Medal: First open-source model to achieve gold medal status (21.2/30 points)\nü•á HiPhO Benchmark Leader: 12 gold medals and 1 silver medal across 13 top international physics contests\nü•á Overall Champion: When paired with PhysicsMinions multi-agent system, achieves #1 ranking with 38.4 points, surpassing Gemini-2.5-Pro (37.7) and GPT-5 (37.4)\nPerformance Benchmarks\nIPhO 2025 Results\nModel\nScore\nMedal\nRank\nP1-235B-A22B + PhysicsMinions\n23.2\nü•á Gold\n1st\nGemini-2.5-Pro\n22.2\nü•á Gold\n2nd\nGPT-5\n22.3\nü•á Gold\n3rdh\nP1-235B-A22B\n21.2\nü•á Gold\n4th\nHiPhO Comprehensive Results\nCategory\nP1-235B-A22B\nP1-235B-A22B + PhysicsMinions\nGemini-2.5-Pro\nGPT-5\nOverall Score\n35.9\n38.4 üèÜ\n37.7\n37.4\nGold Medals (ü•á)\n12\n12\n12\n11\nSilver Medals (ü•à)\n1\n1\n1\n2\nTotal Contests\n13\n13\n13\n13\nGeneralization to STEM Tasks\nP1-235B-A22B demonstrates excellent general capabilities across various benchmarks. As shown below, P1-235B-A22B achieves better performance than its base model Qwen3-235B-A22B-Thinking-2507 on multiple tasks, further validating the strong generalization of P1 series models.\nModel\nAIME24\nAIME25\nHMMT\nGPQA\nHLE\nLiveCodeBench\nLiveBench\nQwen3-235B-A22B-Thinking-2507 (Base)\n94.6\n94.2\n81.7\n79.4\n17.5\n76.2\n80.3\nP1-235B-A22B\n95.0\n95.0\n80.8\n81.4\n19.1\n75.8\n79.8\nUsage\nBasic Inference\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n# Load model and tokenizer\nmodel_name = \"P1-235B-A22B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\n# Physics problem solving\nprompt = \"\"\"Solve this physics problem:\nA block of mass m = 2.0 kg slides down a rough incline at angle Œ∏ = 30¬∞\nwith coefficient of friction Œº = 0.2. Calculate the acceleration of the block.\nProvide a detailed solution with reasoning steps.\"\"\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n**inputs,\nmax_length=81920,\ntemperature=0.6,\ntop_p=0.9,\ndo_sample=True\n)\nsolution = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(solution)\nüôè Acknowledgements\nWe are grateful to the open-source community for their invaluable contributions. Special thanks to:\nQwen3 - for providing the foundational base models that powered our research\nslime - for their innovative work on efficient reinforcement learning framework that powered our training pipeline\nverl - for the versatile reinforcement learning framework that enabled our training pipeline\nsglang - for the efficient LLM serving and inference infrastructure\nMegatron-LM - for the large-scale model training framework\nCitation\n@misc{p1-2025,\ntitle={P1: Mastering Physics Olympiads with Reinforcement Learning},\nauthor={P1 Team},\nyear={2025},\nurl={https://prime-rl.github.io/P1/}\n}",
    "MidnightPhreaker/GLM-4.5-Air-REAP-82B-A12B-GPTQ-INT4-gs32": "GLM-4.5-Air-REAP-82B-A12B - GPTQ INT4 (group_size=32)\nModel Architecture\nQuantization Details\nHardware Used for Quantization\nUsage\nWith vLLM\nWith Transformers\nInference Performance\nRecommended Hardware\nModel Details\nLimitations\nAcknowledgements\nGLM-4.5-Air-REAP-82B-A12B - GPTQ INT4 (group_size=32)\nThis is a GPTQ quantized version of cerebras/GLM-4.5-Air-REAP-82B-A12B.\nModel Architecture\nType: Mixture of Experts (MoE)\nTotal Parameters: 82B (160 routed experts + 1 shared expert)\nActive Parameters: ~12B per token (8 experts selected via sparse gating)\nLayers: 92 transformer layers\nArchitecture: Glm4MoeForCausalLM\nQuantization Details\nMethod: GPTQ (GPT Quantization)\nBits: 4\nGroup Size: 32\nQuantization Type: INT\nSymmetric: True\nDampening: 0.05 (MoE-optimized, lower than typical 0.1)\nCalibration Samples: 128\nCalibration Dataset: allenai/c4\nMax Sequence Length: 512\nNote: ALL 160 routed experts + 1 shared expert are quantized to 4-bit\nHardware Used for Quantization\n6x NVIDIA GeForce RTX 5090 (32GB each)\nCUDA 12.8+\nSequential layer-by-layer processing (OOM-safe)\nUsage\nWith vLLM\nfrom vllm import LLM, SamplingParams\n# Initialize the model\nllm = LLM(model=\"Vykyan/GLM-4.5-Air-REAP-82B-A12B-GPTQ-INT4-gs32\", trust_remote_code=True)\n# Create sampling parameters\nsampling_params = SamplingParams(\ntemperature=0.7,\ntop_p=0.9,\nmax_tokens=512,\n)\n# Generate text\nprompts = [\"Hello, how are you?\"]\noutputs = llm.generate(prompts, sampling_params)\nfor output in outputs:\nprint(output.outputs[0].text)\nWith Transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n\"Vykyan/GLM-4.5-Air-REAP-82B-A12B-GPTQ-INT4-gs32\",\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\n\"Vykyan/GLM-4.5-Air-REAP-82B-A12B-GPTQ-INT4-gs32\",\ntrust_remote_code=True\n)\n# Generate\ninputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nInference Performance\nThis quantized model offers:\n~4x memory reduction compared to FP16\nFaster inference on compatible hardware\nMaintained accuracy through GPTQ quantization\nEfficient MoE inference (only 8 active experts per token)\nRecommended Hardware\nNVIDIA GPUs with compute capability 7.5+ (RTX 20-series or newer)\nMinimum 32GB VRAM for single-GPU inference\nMulti-GPU setup recommended for larger batch sizes\nModel Details\nBase Model: cerebras/GLM-4.5-Air-REAP-82B-A12B\nQuantization Tool: llm-compressor\nCompatible Inference Engines: vLLM, TGI (Text Generation Inference)\nMoE Support: Full support for sparse expert routing\nLimitations\nQuantization may affect model accuracy on certain tasks\nRequires vLLM or compatible inference engine for optimal MoE performance\nAll experts are quantized (not just the active 8)\nAcknowledgements\nBase model: cerebras/GLM-4.5-Air-REAP-82B-A12B\nQuantization: llm-compressor\nInference: vLLM",
    "huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated": "huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-32B-Thinking created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nhuggingface-cli download huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated --local-dir ./huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated --token xxx\nllama-gguf-split --merge huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated/GGUF/ggml-model-f16-00001-of-00002.gguf huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated/GGUF/ggml-model-f16\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-32B-Thinking-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Thinking\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "bartowski/allenai_olmOCR-2-7B-1025-GGUF": "Llamacpp imatrix Quantizations of olmOCR-2-7B-1025 by allenai\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of olmOCR-2-7B-1025 by allenai\nUsing llama.cpp release b6818 for quantization.\nOriginal model: https://huggingface.co/allenai/olmOCR-2-7B-1025\nAll quants made using imatrix option with dataset from here combined with a subset of combined_all_small.parquet from Ed Addario here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nolmOCR-2-7B-1025-bf16.gguf\nbf16\n15.24GB\nfalse\nFull BF16 weights.\nolmOCR-2-7B-1025-Q8_0.gguf\nQ8_0\n8.10GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nolmOCR-2-7B-1025-Q6_K_L.gguf\nQ6_K_L\n6.52GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nolmOCR-2-7B-1025-Q6_K.gguf\nQ6_K\n6.25GB\nfalse\nVery high quality, near perfect, recommended.\nolmOCR-2-7B-1025-Q5_K_L.gguf\nQ5_K_L\n5.78GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nolmOCR-2-7B-1025-Q5_K_M.gguf\nQ5_K_M\n5.44GB\nfalse\nHigh quality, recommended.\nolmOCR-2-7B-1025-Q5_K_S.gguf\nQ5_K_S\n5.32GB\nfalse\nHigh quality, recommended.\nolmOCR-2-7B-1025-Q4_K_L.gguf\nQ4_K_L\n5.09GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nolmOCR-2-7B-1025-Q4_1.gguf\nQ4_1\n4.87GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nolmOCR-2-7B-1025-Q4_K_M.gguf\nQ4_K_M\n4.68GB\nfalse\nGood quality, default size for most use cases, recommended.\nolmOCR-2-7B-1025-Q3_K_XL.gguf\nQ3_K_XL\n4.57GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nolmOCR-2-7B-1025-Q4_K_S.gguf\nQ4_K_S\n4.46GB\nfalse\nSlightly lower quality with more space savings, recommended.\nolmOCR-2-7B-1025-Q4_0.gguf\nQ4_0\n4.44GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nolmOCR-2-7B-1025-IQ4_NL.gguf\nIQ4_NL\n4.44GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nolmOCR-2-7B-1025-IQ4_XS.gguf\nIQ4_XS\n4.22GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nolmOCR-2-7B-1025-Q3_K_L.gguf\nQ3_K_L\n4.09GB\nfalse\nLower quality but usable, good for low RAM availability.\nolmOCR-2-7B-1025-Q3_K_M.gguf\nQ3_K_M\n3.81GB\nfalse\nLow quality.\nolmOCR-2-7B-1025-IQ3_M.gguf\nIQ3_M\n3.57GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nolmOCR-2-7B-1025-Q2_K_L.gguf\nQ2_K_L\n3.55GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nolmOCR-2-7B-1025-Q3_K_S.gguf\nQ3_K_S\n3.49GB\nfalse\nLow quality, not recommended.\nolmOCR-2-7B-1025-IQ3_XS.gguf\nIQ3_XS\n3.35GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nolmOCR-2-7B-1025-IQ3_XXS.gguf\nIQ3_XXS\n3.11GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\nolmOCR-2-7B-1025-Q2_K.gguf\nQ2_K\n3.02GB\nfalse\nVery low quality but surprisingly usable.\nolmOCR-2-7B-1025-IQ2_M.gguf\nIQ2_M\n2.78GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/allenai_olmOCR-2-7B-1025-GGUF --include \"allenai_olmOCR-2-7B-1025-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/allenai_olmOCR-2-7B-1025-GGUF --include \"allenai_olmOCR-2-7B-1025-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (allenai_olmOCR-2-7B-1025-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "cerebras/GLM-4.6-REAP-252B-A32B-FP8": "GLM-4.6-REAP-252B-A32B-FP8\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.6-REAP-252B-A32B-FP8\n‚ú® Highlights\nIntroducing GLM-4.6-REAP-252B-A32B-FP8, a memory-efficient compressed variant of GLM-4.6-FP8 that maintains near-identical performance while being 30% lighter.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 355B model\n30% Memory Reduction: Compressed from 355B to 252B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nFor downstream low-bit quantization, we suggest using the BF16 variant.\nüìã Model Overview\nGLM-4.6-REAP-252B-A32B-FP8 has the following specifications:\nBase Model: GLM-4.6-FP8\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 30% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 252B total, 32B activated per token\nNumber of Layers: 92\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 112 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 202,752 tokens\nLicense: MIT\nüìä Evaluations\nBenchmark\nGLM-4.6-FP8\nGLM-4.6-REAP-268B-A32B-FP8\nGLM-4.6-REAP-252B-A32B-FP8\nGLM-4.6-REAP-218B-A32B-FP8\nCompression\n‚Äî\n25%\n30%\n40%\nCoding\nHumanEval\n96.3\n96.3\n95.7\n95.1\nHumanEval+\n93.3\n91.5\n90.9\n90.2\nMBPP\n87.6\n89.9\n89.9\n89.4\nMBPP+\n73.5\n74.9\n73.5\n73.8\nReasoning\nGPQA diamond (thinking)\n78.8\n76.8\n75.8\n69.7\nAIME25 (thinking)\n90.0\n93.3\n90.0\n90.0\nMATH-500 (thinking)\n95.5\n97.0\n94.8\n93.3\nTool Calling\nBFCL-v3 (thinking)\n78.4\n77.3\n76.8\n74.2\nüü© This checkpoint maintains almost identical performance while being 30% lighter.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.6-REAP-252B-A32B-FP8 \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.6-FP8, with a 30% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nzai-org/GLM-4.6-FP8\nand distributed under the MIT license.\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "beyoru/Luna-Fusion-RP": "üåô Luna-Fusion-RP ‚Äì Roleplay Chat Model series\nNotes:\nSupport me at:\nCite:\nüåô Luna-Fusion-RP ‚Äì Roleplay Chat Model series\nLuna is a conversational AI model designed for immersive roleplay (RP) and natural chatting.It is fine-tuned to respond in a more engaging, character-driven style compared to standard instruction-tuned models.\nThis is a merging version of all RP models, base model in evolution merged method with the CREATIVE WRITING dataset.\nThis influence by the success when merging model to impoveing performance in my test.\nNotes:\nOptimized for roleplay-style conversations\nFlexible: can be used for creative writing, storytelling, or character interactions\nFor best performance, you should describe the system prompt for your character.\n3 models is merging.\nSupport me at:\nCite:\n@misc{Luna,\ntitle        = {Luna-Fusion-RP ‚Äì Roleplay Chat Model},\nauthor       = {Beyoru},\nyear         = {2025},\nhowpublished = {\\url{https://huggingface.co/beyoru/Luna-Fusion-RP}}\n}",
    "artificialguybr/FollowCam-Redmond-WAN2-T2V-14B": "Followcam LoRA for Wan\nModel Description\nTrigger Words\nSetup for ComfyUI\nExample Workflow (ComfyUI)\nReferences\nüåê Website\nüíñ Support My Work\nFollowcam LoRA for Wan\nPrompt\nFollowCam. A continuous wide-angle shot following a lighthouse keeper climbing the spiral staircase inside a towering coastal lighthouse during a storm. She ascends steadily, one hand on the iron railing, the other carrying a maintenance kit. The narrow stairs wind upward in dizzying circles. Wind howls outside, rain lashing against small windows. The structure groans and sways slightly. Each window reveals glimpses of angry grey ocean below. She reaches the lamp room at the top, where massive rotating lenses cast sweeping beams into the tempest.\nPrompt\nFollowCam. A continuous first-person action shot from a chest-mounted camera as we follow a woman trail running through a dense, misty forest at dawn. She weaves between massive moss-covered trees, jumping over roots and dodging low branches. Her breath is steady and rhythmic. Fog clings to the ground, diffusing golden light that filters through the canopy. The path is uneven‚Äîrocks, mud, fallen logs. She never slows, her ponytail swinging with each stride. Birds scatter from the underbrush. The forest feels ancient and alive around us.\nPrompt\nFollowCam. A continuous handheld wide-angle shot following a motorcyclist tearing across endless white salt flats under a blazing midday sun. The rider leans forward, bike tilted into long arcs, kicking up plumes of salt dust that shimmer in the heat. The horizon is flat and infinite, sky meeting earth in a blurred line. Speed builds relentlessly‚Äîwind roars past the camera. The rider's silhouette stays centered ahead as the landscape blurs into white noise. No roads, no markers, just pure velocity across a blank canvas.\nPrompt\nFollowCam. A continuous underwater POV shot from a wide-angle action camera (~14mm) as we follow a diver swimming through a narrow submerged cave system. Her fins kick rhythmically ahead, illuminated by her headlamp cutting through murky blue-green water. Limestone walls close in on both sides, jagged and textured. Air bubbles trail upward in silvery streams. She squeezes through tight passages, her tank scraping against rock. Light from the surface is long gone‚Äîonly her lamp and ours pierce the darkness. The water is cold and still. She moves with calm precision deeper into the labyrinth.\nPrompt\nFollowCam. A continuous wide-angle shot as we follow a rider on horseback galloping through a foggy meadow at dawn. The horse's powerful strides create a rhythmic thunder, muscles rippling beneath dark coat. Mist swirls around its legs with each impact. The rider sits low, reins loose, letting the horse run free. Trees emerge and disappear in the fog like ghosts. Wet grass flies up behind hooves. The horizon is invisible‚Äîjust grey-white void. The horse knows the path instinctively, never slowing. Breath steams from its nostrils.\nSpecial Thanks:This project was made possible thanks to generous sponsorship and GPU time provided by reDMOND Ai.We are grateful for their support in training this LoRA.\nModel Description\nThis LoRA creates immersive, continuous \"follow camera\" shots that put viewers directly in the action. It generates dynamic footage where the camera follows subjects through various environments - from lighthouse staircases during storms to underwater cave explorations, from high-speed motorcycle rides across salt flats to peaceful library cart journeys. The style creates a sense of presence and movement that makes viewers feel like they're right there with the subject.\nThis version is specifically adapted for use with Wan-AI models, like Wan2.2-T2V-A14B.\nTrigger Words\nThe trigger phrase is FollowCam.\nFor best results, start your prompt with \"FollowCam\" followed by a description of a continuous tracking shot following a subject through an environment. The model will render these scenes with an immersive, action-oriented perspective.\nSetup for ComfyUI\nTo use this LoRA in ComfyUI, simply place the file in your loras directory:\nDownload [WAN2.2]Followcam_Redmond_high_noise.safetensors and place it in ComfyUI/models/loras/\nEnsure your ComfyUI environment is already configured to run Wan models.\nExample Workflow (ComfyUI)\nIn your ComfyUI workflow, add a Load LoRA node after your base model loader.\nLoad your base Wan model.\nConnect the output of the model loader to the model input of the Load LoRA node.\nSelect [WAN2.2]Followcam_Redmond_high_noise.safetensors in the Load LoRA node.\nConnect the MODEL output of the LoRA node to your KSampler.\nAdjust the strength_model as needed (0.8-1.0 is a good starting point).\nReferences\nBase Model: Wan-AI/Wan2.2-T2V-A14B\nComfyUI: comfyanonymous/ComfyUI\nüåê Website\nYou can find more of my models, projects, and information on my official website:\nartificialguy.com\nüíñ Support My Work\nIf you find this model useful, please consider supporting my work. It helps me cover server costs and dedicate more time to new open-source projects.\nPatreon: Support on Patreon\nKo-fi: Buy me a Ko-fi\nBuy Me a Coffee: Buy me a Coffee",
    "MENG2023/M_QWEN_TG300": "M_QWEN_TG300\nM_QWEN_TG300\nËøõË°å‰∫Ü‰∏Ä‰∫õÊ®°ÂûãÁöÑÊµãËØïÔºå‰ª•ExcelÁöÑÂΩ¢ÂºèÂ≠òÂÇ®Âà∞‰∫ÜËøôÈáåÔºåÊúâÂÖ¥Ë∂£ÁöÑÂèØ‰ª•ÁúãÁúã„ÄÇ",
    "HarleyCooper/nanochat-AquaRat": "",
    "smulelabs/Smule-Renaissance-Small": "",
    "Vortex5/Violet-Mist-12B": "üíú\nüíú\nViolet Mist ‚Äî 12B\n‚ÄúIn the great violet mist, your desire may take form ‚Äî if your soul dares to seek it.‚Äù\nüå´Ô∏è Overview\nViolet-Mist-12B was forged through the\nSCE merge method using\nMergeKit.\nWithin its core, whispers of many models intertwine ‚Äî threads of shadow, light, and oceanic calm ‚Äî\neach one leaving an imprint upon the mist:\nModels Used in Merge\nredrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3\nnothingiisreal/MN-12B-Celeste-V1.9\nVortex5/Midnight-Ocean-12B\nBase: Vortex5/Lunar-Abyss-12B\nü™∂ Merge Configuration\nShow Full Configuration\nmodels:\n- model: redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3\nparameters:\nweight:\n- filter: mlp\nvalue: [0.25, 0.35, 0.45, 0.55, 0.65, 0.55, 0.40, 0.30]\n- filter: norm\nvalue: 0.5\n- value: 0.4\n- model: nothingiisreal/MN-12B-Celeste-V1.9\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.1, 0.25, 0.45, 0.60, 0.70, 0.65, 0.45, 0.25]\n- filter: mlp\nvalue: 0.4\n- value: 0.5\n- model: Vortex5/Midnight-Ocean-12B\nparameters:\nweight: 0.4\nparameters:\nselect_topk: 0.55\nnormalize: true\ndtype: bfloat16\nmerge_method: sce\nbase_model: Vortex5/Lunar-Abyss-12B\ntokenizer:\nsource: Vortex5/Lunar-Abyss-12B\nüîÆ Intended Use\nüñãÔ∏è Roleplay creation ‚Ä¢ üåå Mystical dialogue ‚Ä¢ üí´ Dreamlike storytelling ‚Äî where darkness whispers, and desire breathes.\n‚ú® Acknowledgements\nmradermacher ‚Äî Static & imatrix quants\nDeathGodlike ‚Äî EXL3 quants\nOriginal model authors and contributors whose work made this model possible.",
    "ggml-org/LightOnOCR-1B-1025-GGUF": "LightOnOCR-1B-1025-GGUF\nLightOnOCR-1B-1025-GGUF\nOriginal model: https://huggingface.co/lightonai/LightOnOCR-1B-1025\nRelated PR: https://github.com/ggml-org/llama.cpp/pull/16764",
    "Jalea96/DeepSeek-OCR-bnb-4bit-NF4": "How to Use This 4-bit Quantized Model\nEnvironment Setup\nUsage\nLoad 4-bit Quantized Model (Flash Attention 2)\nLoad 4-bit Quantized Model (Eager Attention)\nAcknowledgement\nWork in progress...\nCitation\nDeepSeek-OCR-bnb-4bit-NF4\nüåü Github |\nüì• Model Download |\nüìÑ Paper Link |\nüìÑ Arxiv Paper Link |\nDeepSeek-OCR: Contexts Optical Compression\nHow to Use This 4-bit Quantized Model\nThis is a 4-bit NF4 quantized version of deepseek-ai/DeepSeek-OCR, created using bitsandbytes. It offers significantly reduced VRAM (up to 8 Gb!) usage while maintaining high accuracy, making it ideal for consumer GPUs.\nEnvironment Setup\nFor optimal compatibility, we strongly recommend creating a virtual environment using uv with Python 3.12.9. This matches the test environment of the original deepseek-ai/DeepSeek-OCR model.\nPrerequisites:\nYou must have the NVIDIA CUDA Toolkit (e.g., CUDA 11.8, which matches the PyTorch build) installed on your system to compile flash-attn.\nBelow are the recommended library versions, based on the original model's requirements, plus the libraries needed for 4-bit loading (bitsandbytes, accelerate) and PyTorch compatibility (torchvision).\n# 1. Create and activate the environment (Python 3.12.9 recommended)\nuv venv --python 3.12.9 .venv\n# 2. Install PyTorch\nuv pip install torch==2.6.0 torchvision\n# 3. Install Transformers and dependencies\nuv pip install transformers==4.46.3 tokenizers==0.20.3 einops addict easydict\n# 4. Install 4-bit (bitsandbytes) and 'device_map' (accelerate) support\nuv pip install bitsandbytes accelerate\n# 5. Install flash-attn (compiles from source, requires CUDA Toolkit)\nuv pip install flash-attn==2.7.3 --no-build-isolation\nUsage\nUsage (Inference Code)\nOnce your environment is set up, you can use one of the two code blocks provided below.\nThe main difference is the _attn_implementation parameter, which depends on your GPU architecture:\n_attn_implementation='flash_attention_2': Recommended for NVIDIA Ampere (RTX 30xx, A100) or newer GPUs.\n_attn_implementation='eager': Required for NVIDIA Turing (RTX 20xx) GPUs and older, or for general compatibility if Flash Attention 2 fails.\nThis 4-bit model was successfully tested on an RTX 2080 Ti (Turing architecture) with CUDA 13.0 drivers, which requires the eager implementation. Both code blocks are provided for your convenience.\nLoad 4-bit Quantized Model (Flash Attention 2)\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_id = 'Jalea96/DeepSeek-OCR-bnb-4bit-NF4'\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\nmodel_id,\n_attn_implementation='flash_attention_2',\ntrust_remote_code=True,\nuse_safetensors=True,\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16\n)\nmodel = model.eval()\n# --- 1. Set Image and Task Prompt ---\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\nif not os.path.exists(output_path):\nos.makedirs(output_path)\n# --- 2. Set Resolution ---\n# (Gundam is recommended for most documents)\n# Tiny:  base_size = 512,  image_size = 512, crop_mode = False\n# Small: base_size = 640,  image_size = 640, crop_mode = False\n# Base:  base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n# Gundam:base_size = 1024, image_size = 640, crop_mode = True\nbase_size, image_size, crop_mode = 1024, 640, True\n# --- 3. Run Inference ---\nres = model.infer(\ntokenizer,\nprompt=prompt,\nimage_file=image_file,\noutput_path=output_path,\nbase_size=base_size,\nimage_size=image_size,\ncrop_mode=crop_mode,\nsave_results=True, # Set to True to save visualization (image_vis.jpg)\ntest_compress=True\n)\nprint(res)\nLoad 4-bit Quantized Model (Eager Attention)\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_id = 'Jalea96/DeepSeek-OCR-bnb-4bit-NF4'\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\nmodel_id,\n_attn_implementation='eager',\ntrust_remote_code=True,\nuse_safetensors=True,\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16\n)\nmodel = model.eval()\n# --- 1. Set Image and Task Prompt ---\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\nif not os.path.exists(output_path):\nos.makedirs(output_path)\n# --- 2. Set Resolution ---\n# (Gundam is recommended for most documents)\n# Tiny:  base_size = 512,  image_size = 512, crop_mode = False\n# Small: base_size = 640,  image_size = 640, crop_mode = False\n# Base:  base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n# Gundam:base_size = 1024, image_size = 640, crop_mode = True\nbase_size, image_size, crop_mode = 1024, 640, True\n# --- 3. Run Inference ---\nres = model.infer(\ntokenizer,\nprompt=prompt,\nimage_file=image_file,\noutput_path=output_path,\nbase_size=base_size,\nimage_size=image_size,\ncrop_mode=crop_mode,\nsave_results=True, # Set to True to save visualization (image_vis.jpg)\ntest_compress=True\n)\nprint(res)\nAcknowledgement\nI would like to thank DeepSeek and their entire team for making this incredible range of models available. I also want to thank the entire Hugging Face community for their amazing platform and everyone who contributes to making this such an incredible community.\nWork in progress...\nbenchmarks Fox, OminiDocBench.\nCitation\n@article{wei2025deepseek,\ntitle={DeepSeek-OCR: Contexts Optical Compression},\nauthor={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\njournal={arXiv preprint arXiv:2510.18234},\nyear={2025}\n}",
    "meskvlla33/rifforge": "No model card",
    "kenpath/svara-tts-v1": "svara-TTS v1 ‚Äî Open Multilingual TTS for India‚Äôs Voices\nAt a Glance\nPrompting (Orpheus-style)\nTraining Data Summary\nIntended Uses\nOut-of-Scope / Not Intended\nLimitations\nResponsible Use\nSources & Links\nüôè Acknowledgments\nLicense\nVersioning & Changelog\nsvara-TTS v1 ‚Äî Open Multilingual TTS for India‚Äôs Voices\nsvara-TTS is a developer-first multilingual TTS model for 19 languages (18 Indic + Indian English).Built on an Orpheus-style discrete audio token approach, it targets clarity, expressiveness, and low-latency on commodity GPUs/CPUs.It supports light-weight emotion/style control (e.g., <happy>, <sad>, <anger>, <fear>) and simple speaker identities (Language (Gender)), with zero-shot adaptation paths.\nAt a Glance\nLanguages (19): Hindi, Bengali, Marathi, Telugu, Kannada, Bhojpuri, Magahi, Chhattisgarhi, Maithili, Assamese, Bodo, Dogri, Gujarati, Malayalam, Punjabi, Tamil, Nepali, Sanskrit, Indian English.\nExpressivity: End-of-utterance style tags; natural prosody; code-switch aware.\nLatency & Deployment: Works well with GGUF exports; suitable for edge/CPU scenarios.\nAdaptability: LoRA-friendly for quick speaker/domain specialization.\nTry it live on the Demo Space, or on Colab\nDeployment scripts and inference repo will be available soon. Watch our Github for updates\nPrompting (Orpheus-style)\nPlace style/emotion tags at the end of the sentence:‡§Ü‡§ú... ‡§∏‡§ö ‡§Æ‡•á‡§Ç ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ñ‡§¨‡§∞ ‡§π‡•à ‚Äî ‡§∂‡§æ‡§Æ ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§§‡•á ‡§π‡•à‡§Ç! <happy>\nUse punctuation to hint prosody (ellipses, commas, exclamation).\nFor technical or dense text, end with <clear> to prioritize intelligibility.\nSpeaker IDs follow a simple convention: Language (Gender) (e.g., Marathi (Male)).\nTraining Data Summary\nTrained on 2000+ hours of open, high-quality speech from SYSPIN, RASA, IndicTTS, and SPICOR, covering ~50 speakers (balanced male/female) across 19 languages.Data was curated to encourage natural prosody, broad coverage, and stable multilingual transfer. See Acknowledgments for provenance.\nIntended Uses\nMultilingual assistants, IVR, learning apps, reading aids, accessibility tools\nContent localization (education, public-information, civic services)\nResearch on Indic prosody, emotion control, cross-lingual transfer\nOut-of-Scope / Not Intended\nImpersonation of private individuals or public figures without consent\nDeceptive content (fraud, harassment, misinformation)\nSafety-critical deployments without human oversight\nLimitations\nProper nouns & rare entities: may require spelling hints or <clear>.\nVery long sentences: chunk or add punctuation for natural prosody.\nEmotion strength: varies by language due to data density.\nCode-mixing: common patterns work; it‚Äôs not a deterministic rules engine.\nMany of these improve with targeted LoRA finetuning and better preprocessing.\nResponsible Use\nBy using this model, you agree to follow applicable laws and ethical guidelines.Avoid impersonation, harassment, targeted deception, or other harmful uses.Where appropriate, disclose synthetic speech to end users.\nSources & Links\nModel: https://huggingface.co/kenpath/svara-tts-v1\nDemo Space: https://huggingface.co/spaces/kenpath/svara-tts\nInference repo: https://github.com/Kenpath/svara-tts-inference\nColab: https://colab.research.google.com/drive/15YxFo1DzdQNbFUIZ1HJA4AN4oHqKxGtg\nüôè Acknowledgments\nThis work was developed by Kenpath Technologies for the open-source community. We also thank RunPod for the startup credits that supported our GPU compute.\nCanopy Labs ‚Äî Orpheus: foundational ideas & open releaseRelease: https://canopylabs.ai/releases/orpheus_can_speak_any_language\nSPIRE Lab, IISc Bangalore ‚Äî SYSPIN (multilingual studio) and SPICOR (Indian English)\nAI4Bharat ‚Äî RASA expressive speech\nIIT Madras ‚Äî IndicTTS\nUnsloth ‚Äî helpful notes & tooling\nRunPod ‚Äî startup GPU credits that accelerated experiments\nLicense\nApache-2.0\nVersioning & Changelog\nv1.0.0: Initial public release (19 languages)",
    "Vortex5/Harmony-Bird-12B": "Harmony Bird 12B\n01 // Overview\nHarmony-Bird-12B is a merged model intended for roleplay and storytelling.\nBuilt with MergeKit.\n02 // Merge Step 1\nVermilion-Sage-12B\nmerges with\nImpish-Nemo-12B\nusing the harmony_forge method (focus=8.0, blend=0.7).\nShow YAML\nmerge_method: harmony_forge\nmodels:\n- model: Vortex5/Vermilion-Sage-12B\n- model: SicariusSicariiStuff/Impish_Nemo_12B\nparameters:\nfocus: 8.0\nblend: 0.7\ntokenizer:\nsource: union\ndtype: bfloat16\n03 // Custom merge method\nIt is a custom adaptive merge method that blends models through consensus weighting across both **spatial** (parameter alignment) and **frequency** (spectral structure) domains. It is designed for stable, noise-resistant merges that preserve the shared strengths of multiple models while reducing conflicts and outlier effects. It supports standard and task-vector merging (when a `base_model` is provided).\n**How it works:**\nThe method first centers all model weights (relative to either a `base_model` or their median) and normalizes their scales to ensure balance. It then analyzes correlations in parameter space (spatial features) and in the frequency domain (via FFT) to measure similarity and coherence between models. Each model receives a *goodness score* based on how well it aligns with others, adjusted by stability and outlier suppression terms.\nThese scores are converted into normalized merge weights using a softmax function, which smoothly scales scores so that higher values receive more weight while all weights sum to 1. The focus parameter controls how sharply these weights are distributed ‚Äî low focus blends models evenly, while high focus concentrates more weight on the most consistent ones. The blend parameter mixes how much spatial versus frequency information influences the final weighting. The merged parameters are then computed as a weighted sum across models.\nKey parameters:\nfocus: Controls decisiveness of weighting (higher = more selective). Default: 1.0\nblend: Uses a 0‚Äì1 scale to control merge emphasis ‚Äî 0 represents full reliance on spatial similarity, meaning model weights are compared directly in parameter space. 1 represents full reliance on frequency-domain similarity, where weights are compared by their spectral (FFT) patterns. Default: 0.5 blends both equally for balanced structural and behavioral alignment.\nBy weighting models through adaptive consensus across spatial and frequency domains, Harmony Forge emphasizes aligned, stable patterns‚Äîencouraging coherent, balanced merges that often inherit the strongest traits of each source model.\n05 // Acknowledgments\nTeam Mradermacher ‚Äî Static & imatrix quantizations\nDeathGodlike ‚Äî EXL3 quants\nOriginal model authors and contributors whose work made this model possible.",
    "Soul-AILab/SoulX-Podcast-1.7B-dialect": "README.md exists but content is empty.",
    "OmniGen2/OmniGen2-EditScore7B-v1.1": "üî• News\nIntroduction\nüìå TODO\nüöÄ Quick Start\nüõ†Ô∏è Environment Setup\n‚úÖ Recommended Setup\nüåè For users in Mainland China\nüß™ Run Examples\nüåê Gradio Demo\nüí° Usage Tips\nüé® Fine-tune\n‚ùå Limitations and Suggestions\nüíª Resources Requirement\nü§ù Community Efforts\n‚ù§Ô∏è Citing Us\nNews |\nQuick Start |\nUsage Tips |\nLimitations |\nOnline Demos |\nCitation\nüî• News\n2025-09-30: Introducing EditScore ‚Äî a family of state-of-the-art open-source reward models (7B‚Äì72B) for instruction-guided image editing.\nModel Release: As part of this, We release OmniGen2-EditScore7B, unlocking online RL For Image Editing via high-fidelity EditScore. LoRA weights are now available on Hugging Face and ModelScope.\nBenchmark: We are also launching EditReward-Bench to provide a systematic way to evaluate and compare reward models.\nCheck out the project repository to get started!\n2025-07-23: Users can access OmniGen2 through web app.\n2025-07-05: Training datasets X2I2 are available.\n2025-07-03: OmniGen2 now supports TeaCache and TaylorSeer for faster inference, see Usage Tips for details. Thanks @legitnull for great TeaCache-PR and TaylorSeer-PR.\n2025-07-01: OmniGen2 is supported by ComfyUI official, thanks !!\n2025-06-30: Training code is available, see fine-tuning for details.\n2025-06-28: We release OmniContext benchmark. The evaluation codes are in omnicontext.\n2025-06-24: Technical Report is available.\n2025-06-23: We‚Äôve updated our code and HF model‚ÄîOmniGen2 now runs without flash-attn. Users can still install it for optimal performance.\n2025-06-20: Updated resource requirements, adding CPU offload support for devices with limited VRAM.\n2025-06-16: Gradio and Jupyter is available. Online Gradio Demo: Demo1; Chat-Demo1; see more demo links in gradio section\n2025-06-16: We release OmniGen2, a multimodal generation model, model weights can be accessed in huggingface and modelscope.\nIntroduction\nOmniGen2 is a powerful and efficient generative model. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. OmniGen2 has competitive performance across four primary capabilities:\nVisual Understanding: Inherits the robust ability to interpret and analyze image content from its Qwen-VL-2.5 foundation.\nText-to-Image Generation: Creates high-fidelity and aesthetically pleasing images from textual prompts.\nInstruction-guided Image Editing: Executes complex, instruction-based image modifications with high precision, achieving state-of-the-art performance among open-source models.\nIn-context Generation: A versatile capability to process and flexibly combine diverse inputs‚Äîincluding humans, reference objects, and scenes‚Äîto produce novel and coherent visual outputs.\nWe will release the training code and dataset. Stay tuned!\nSome good cases of OmniGen2:\nDemonstrations.\nGood demonstrations of OmniGen2's image editing capabilities.\nGood demonstrations of OmniGen2's in-context generation capabilities.\nüìå TODO\nTechnical report.\nSupport CPU offload and improve inference efficiency.\nIn-context generation benchmark: OmniContext.\nIntegration of diffusers.\nTraining datasets.\nTraining data construction pipeline.\nComfyUI Demo (commuity support will be greatly appreciated!).\nüöÄ Quick Start\nüõ†Ô∏è Environment Setup\n‚úÖ Recommended Setup\n# 1. Clone the repo\ngit clone git@github.com:VectorSpaceLab/OmniGen2.git\ncd OmniGen2\n# 2. (Optional) Create a clean Python environment\nconda create -n omnigen2 python=3.11\nconda activate omnigen2\n# 3. Install dependencies\n# 3.1 Install PyTorch (choose correct CUDA version)\npip install torch==2.6.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu124\n# 3.2 Install other required packages\npip install -r requirements.txt\n# Note: Version 2.7.4.post1 is specified for compatibility with CUDA 12.4.\n# Feel free to use a newer version if you use CUDA 12.6 or they fixed this compatibility issue.\n# OmniGen2 runs even without flash-attn, though we recommend install it for best performance.\npip install flash-attn==2.7.4.post1 --no-build-isolation\nüåè For users in Mainland China\n# Install PyTorch from a domestic mirror\npip install torch==2.6.0 torchvision --index-url https://mirror.sjtu.edu.cn/pytorch-wheels/cu124\n# Install other dependencies from Tsinghua mirror\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n# Note: Version 2.7.4.post1 is specified for compatibility with CUDA 12.4.\n# Feel free to use a newer version if you use CUDA 12.6 or they fixed this compatibility issue.\n# OmniGen2 runs even without flash-attn, though we recommend install it for best performance.\npip install flash-attn==2.7.4.post1 --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple\nüß™ Run Examples\n# Visual Understanding\nbash example_understanding.sh\n# Text-to-image generation\nbash example_t2i.sh\n# Instruction-guided image editing\nbash example_edit.sh\n# In-context generation\nbash example_in_context_generation.sh\nüåê Gradio Demo\nOnline Demo: HF Spaces. Beyond Hugging Face Spaces, we are temporarily allocating additional GPU resources to ensure smooth access to the online demos. If you notice a long queue for a particular link, please try other links:\nDemo1, Demo2, Demo3, Demo4\nChat-Demo1, Chat-Demo2, Chat-Demo3, Chat-Demo4\nWeb Application: You can also try the self-hosted OmniGen2 web application by visiting this link or scanning the QR code below:\nOmniGen2 web.\nRun Locally:# for only generating image\npip install gradio\npython app.py\n# Optional: Share demo with public link (You need to be able to access huggingface)\npython app.py --share\n# for generating image or text\npip install gradio\npython app_chat.py\nüí° Usage Tips\nTo achieve optimal results with OmniGen2, you can adjust the following key hyperparameters based on your specific use case.\ntext_guidance_scale: Controls how strictly the output adheres to the text prompt (Classifier-Free Guidance).\nimage_guidance_scale: This controls how much the final image should resemble the input reference image.\nThe Trade-off: A higher value makes the output more faithful to the reference image's structure and style, but it might ignore parts of your text prompt. A lower value (~1.5) gives the text prompt more influence.\nTip: For image editing task, we recommend to set it between 1.2 and 2.0; for in-context generateion task, a higher image_guidance_scale will maintian more details in input images, and we recommend to set it between 2.5 and 3.0.\nmax_pixels: Automatically resizes images when their total pixel count (width √ó height) exceeds this limit, while maintaining its aspect ratio. This helps manage performance and memory usage.\nTip: Default value is 1024*1024. You can reduce this value if you encounter memory issues.\nmax_input_image_side_length: Maximum side length for input images.\nnegative_prompt: Tell the model what you don't want to see in the image.\nExample: blurry, low quality, text, watermark\nTip: For the best results, try experimenting with different negative prompts. If you're not sure, just use the default negative prompt.\nenable_model_cpu_offload: Reduces VRAM usage by nearly 50% with a negligible impact on speed.\nThis is achieved by offloading the model weights to CPU RAM when they are not in use.\nSee: Model Offloading\nenable_sequential_cpu_offload: Minimizes VRAM usage to less than 3GB, but at the cost of significantly slower performance.\nThis works by offloading the model in submodules and loading them onto the GPU sequentially as needed.\nSee: CPU Offloading\ncfg_range_start, cfg_range_end: Define the timestep range where CFG is applied. Per this paper, reducing cfg_range_end can significantly decrease inference time with a negligible impact on quality.\nscheduler: Choose between [euler, dpmsolver++]. Default is euler. For potentially better performance with fewer steps, try dpmsolver++.\nnum_inference_step: Number of discretization steps for the ODE solver. Default is 50.\nenable_teacache: Whether or not enable teacache for faster inference.\nteacache_rel_l1_thresh: The threshold for accumulated L1 distance for the timestep embedding-modulated noisy input. It serves as an indicator of whether to cache the model output. You can modify the teacache_rel_l1_thresh parameter to achieve your desired trade-off between latency and visual quality. The default value of 0.05 provides approximately a 30% speedup compared to the baseline. Increasing this value can further reduce latency, but may result in some loss of detail.\nenable_taylorseer: Whether or not enable taylorseer for faster inference. When enabled, inference speed can improve by up to 2X, with negligible quality loss compared to the baseline.\nSome suggestions for improving generation quality:\nUse High-Quality Images\nProvide clear images, preferably with a resolution greater than 512√ó512 pixels.\nSmall or blurry inputs will result in low-quality outputs.\nBe Specific with Instructions\nClearly describe both what to change and how you want it changed.\nPrioritize English\nThe model currently performs best with English prompts.\nChange instructions to enhance subject consistency.\nWhen the generated image does not align well with the input image, you can try the following methods to improve subject consistency:\nUse images with larger size, as well as images in which people occupy a larger proportion of the frame.\nIncrease the Image Guidance Scale, for example to 3.0. The trade-off may be slight overexposure or a greasy look in the image.\nWhen using a single input image, you can try to use the following prompt template: \"she/he ..., maintaining her/his facial features, hairstyle, and other attributes.\"\nIncrease the parameter--Number of images per prompt to generate more outputs, giving you a better chance to find one with stronger subject consistency and a more satisfactory result.\nLonger prompts generally yield better results than shorter ones. More detailed descriptions of the scene and character interactions can provide additional benefits.\nFor in-context edit (edit based multiple images), we recommend using the following prompt format: \"Edit the first image: add/replace (the [object] with) the [object] from the second image. [descripton for your target image].\"\nFor example: \"Edit the first image: add the man from the second image. The man is talking with a woman in the kitchen\". The descition for your target image should be as detailed as possible.\nüé® Fine-tune\nSee fine-tuning for details.\n‚ùå Limitations and Suggestions\nThe current model sometimes does not follow instructions. You can increase the \"Number of images per prompt\" to generate multiple images at once, so you can choose the result you are satisfied with, or try different prompts. In our own experience, being as detailed as possible tends to work better.\nThe current model cannot decide the output image size by itself; the default size is 1024√ó1024. You need to set a specific size if you require a different one. When you input an image, we will set the output size to match the input image (this works best for editing tasks). If you want to modify just one image out of several, you should also set the output size to match the image you want to edit; otherwise, it may lead to low-quality outputs.\nThe in-context generation capability sometimes produces objects that differ from the original ones. Some suggested improvements are: increasing image_guidance_scale (it is recommended to set it to 3) can help alleviate this issue; using high-resolution images, increasing the size of the input image, and ensuring that the object to be used occupies a larger proportion of the image; and modifying the prompt. However, there is still a  gap compared to GPT-4o.\nCompared to OmniGen 1.0, although OmniGen 2 has made some improvements, many issues still remain. It may take multiple attempts to achieve a satisfactory result.\nüíª Resources Requirement\nOmniGen2 natively requires an NVIDIA RTX 3090 or an equivalent GPU with approximately 17GB of VRAM. For devices with less VRAM, you can enable CPU Offload to run the model.\nPerformance Tip: To improve inference speed, consider decreasing the cfg_range_end parameter. Within a reasonable range, this has a negligible impact on output quality.\nThe following table details the inference performance of OmniGen2 on an A800 GPU:\nInference Efficiency of OmniGen2.\nü§ù Community Efforts\nWe‚Äôre honored and grateful for the support from the open source community. Here are some unofficial implementations contributed by the community(Currently, we have not confirmed whether there are no bugs. Please try to use the our official demo as much as possible.):\nComfyUI:\nComfyUI Official\nhttps://github.com/Yuan-ManX/ComfyUI-OmniGen2\nhttps://github.com/neverbiasu/ComfyUI-OmniGen2\nQuantization:\nDFloat11, a lossless compression using 11 bits\n‚ù§Ô∏è Citing Us\nIf you find this repository or our work useful, please consider giving a star ‚≠ê and citation ü¶ñ, which would be greatly appreciated:\n@article{wu2025omnigen2,\ntitle={OmniGen2: Exploration to Advanced Multimodal Generation},\nauthor={Chenyuan Wu and Pengfei Zheng and Ruiran Yan and Shitao Xiao and Xin Luo and Yueze Wang and Wanli Li and Xiyan Jiang and Yexin Liu and Junjie Zhou and Ze Liu and Ziyi Xia and Chaofan Li and Haoge Deng and Jiahao Wang and Kun Luo and Bo Zhang and Defu Lian and Xinlong Wang and Zhongyuan Wang and Tiejun Huang and Zheng Liu},\njournal={arXiv preprint arXiv:2506.18871},\nyear={2025}\n}",
    "mlx-community/DeepSeek-OCR-8bit": "mlx-community/DeepSeek-OCR-8bit\nUse with mlx\nmlx-community/DeepSeek-OCR-8bit\nThis model was converted to MLX format from deepseek-ai/DeepSeek-OCR using mlx-vlm version 0.3.5.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/DeepSeek-OCR-8bit --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>",
    "PantheonUnbound/Satyr-V0.1-4B": "",
    "ApacheOne/WAN_loRAs": "",
    "Minthy/Rouwei-T5Gemma-adapter_v0.2": "",
    "distilbert/distilbert-base-cased-distilled-squad": "DistilBERT base cased distilled SQuAD\nTable of Contents\nModel Details\nHow to Get Started with the Model\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation Information\nModel Card Authors\nDistilBERT base cased distilled SQuAD\nTable of Contents\nModel Details\nHow To Get Started With the Model\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation Information\nModel Card Authors\nModel Details\nModel Description: The DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT, and the paper DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\nThis model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.\nDeveloped by: Hugging Face\nModel Type: Transformer-based language model\nLanguage(s): English\nLicense: Apache 2.0\nRelated Models: DistilBERT-base-cased\nResources for more information:\nSee this repository for more about Distil* (a class of compressed models including this model)\nSee Sanh et al. (2019) for more information about knowledge distillation and the training procedure\nHow to Get Started with the Model\nUse the code below to get started with the model.\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\nAnswer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\nHere is how to use this model in PyTorch:\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\nmodel = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\nprint(outputs)\nAnd in TensorFlow:\nfrom transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\nUses\nThis model can be used for question answering.\nMisuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nRisks, Limitations and Biases\nCONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\nAnswer: 'Bob', score: 0.7527, start: 32, end: 35\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nTraining\nTraining Data\nThe distilbert-base-cased model was trained using the same data as the distilbert-base-uncased model. The distilbert-base-uncased model model describes it's training data as:\nDistilBERT pretrained on the same data as BERT, which is BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers).\nTo learn more about the SQuAD v1.1 dataset, see the SQuAD v1.1 data card.\nTraining Procedure\nPreprocessing\nSee the distilbert-base-cased model card for further details.\nPretraining\nSee the distilbert-base-cased model card for further details.\nEvaluation\nAs discussed in the model repository\nThis model reaches a F1 score of 87.1 on the [SQuAD v1.1] dev set (for comparison, BERT bert-base-cased version reaches a F1 score of 88.7).\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019). We present the hardware type and hours used based on the associated paper. Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.\nHardware Type: 8 16GB V100 GPUs\nHours used: 90 hours\nCloud Provider: Unknown\nCompute Region: Unknown\nCarbon Emitted: Unknown\nTechnical Specifications\nSee the associated paper for details on the modeling architecture, objective, compute infrastructure, and training details.\nCitation Information\n@inproceedings{sanh2019distilbert,\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\nauthor={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\nbooktitle={NeurIPS EMC^2 Workshop},\nyear={2019}\n}\nAPA:\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\nModel Card Authors\nThis model card was written by the Hugging Face team.",
    "distilbert/distilbert-base-uncased-finetuned-sst-2-english": "DistilBERT base uncased finetuned SST-2\nTable of Contents\nModel Details\nHow to Get Started With the Model\nUses\nRisks, Limitations and Biases\nTraining\nDistilBERT base uncased finetuned SST-2\nTable of Contents\nModel Details\nHow to Get Started With the Model\nUses\nRisks, Limitations and Biases\nTraining\nModel Details\nModel Description: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\nDeveloped by: Hugging Face\nModel Type: Text Classification\nLanguage(s): English\nLicense: Apache-2.0\nParent Model: For more details about DistilBERT, we encourage users to check out this model card.\nResources for more information:\nModel Documentation\nDistilBERT paper\nHow to Get Started With the Model\nExample of single-label classification:\n‚Äã‚Äã\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\nUses\nDirect Use\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\nMisuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nRisks, Limitations and Biases\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\nFor instance, for sentences like This film was filmed in COUNTRY, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this colab, Aur√©lien G√©ron made an interesting map plotting these probabilities for each country.\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: WinoBias, WinoGender, Stereoset.\nTraining\nTraining Data\nThe authors use the following Stanford Sentiment Treebank(sst2) corpora for the model.\nTraining Procedure\nFine-tuning hyper-parameters\nlearning_rate = 1e-5\nbatch_size = 32\nwarmup = 600\nmax_seq_length = 128\nnum_train_epochs = 3.0",
    "openai-community/openai-gpt": "OpenAI GPT 1\nTable of Contents\nModel Details\nHow to Get Started with the Model\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation Information\nModel Card Authors\nOpenAI GPT 1\nTable of Contents\nModel Details\nHow To Get Started With the Model\nUses\nRisks, Limitations and Biases\nTraining\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nCitation Information\nModel Card Authors\nModel Details\nModel Description: openai-gpt (a.k.a. \"GPT-1\") is the first transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.\nDeveloped by: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. See associated research paper and GitHub repo for model developers and contributors.\nModel Type: Transformer-based language model\nLanguage(s): English\nLicense: MIT License\nRelated Models: GPT2, GPT2-Medium, GPT2-Large and GPT2-XL\nResources for more information:\nResearch Paper\nOpenAI Blog Post\nGitHub Repo\nTest the full generation capabilities here: https://transformer.huggingface.co/doc/gpt\nHow to Get Started with the Model\nUse the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='openai-gpt')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n[{'generated_text': \"Hello, I'm a language model,'he said, when i was finished.'ah well,'said the man,'that's\"},\n{'generated_text': 'Hello, I\\'m a language model, \" she said. \\n she reached the bottom of the shaft and leaned a little further out. it was'},\n{'generated_text': 'Hello, I\\'m a language model, \" she laughed. \" we call that a\\'white girl.\\'or as we are called by the'},\n{'generated_text': 'Hello, I\\'m a language model, \" said mr pin. \" an\\'the ones with the funny hats don\\'t. \" the rest of'},\n{'generated_text': 'Hello, I\\'m a language model, was\\'ere \\'bout to do some more dancin \\', \" he said, then his voice lowered to'}]\nHere is how to use this model in PyTorch:\nfrom transformers import OpenAIGPTTokenizer, OpenAIGPTModel\nimport torch\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nand in TensorFlow:\nfrom transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\noutputs = model(inputs)\nlast_hidden_states = outputs.last_hidden_state\nUses\nDirect Use\nThis model can be used for language modeling tasks.\nDownstream Use\nPotential downstream uses of this model include tasks that leverage language models. In the associated paper, the model developers discuss evaluations of the model for tasks including natural language inference (NLI), question answering, semantic similarity, and text classification.\nMisuse and Out-of-scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nRisks, Limitations and Biases\nBiases\nCONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).\nPredictions generated by this model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='openai-gpt')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n[{'generated_text': 'The man worked as a teacher for the college he'},\n{'generated_text': 'The man worked as a janitor at the club.'},\n{'generated_text': 'The man worked as a bodyguard in america. the'},\n{'generated_text': 'The man worked as a clerk for one of the'},\n{'generated_text': 'The man worked as a nurse, but there was'}]\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n[{'generated_text': 'The woman worked as a medical intern but is a'},\n{'generated_text': 'The woman worked as a midwife, i know that'},\n{'generated_text': 'The woman worked as a prostitute in a sex club'},\n{'generated_text': 'The woman worked as a secretary for one of the'},\n{'generated_text': 'The woman worked as a nurse, but she had'}]\nThis bias may also affect fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nRisks and Limitations\nThe model developers also wrote in a blog post about risks and limitations of the model, including:\nCompute Requirements: Many previous approaches to NLP tasks train relatively small models on a single GPU from scratch. Our approach requires an expensive pre-training step - 1 month on 8 GPUs. Luckily, this only has to be done once and we‚Äôre releasing our model so others can avoid it. It is also a large model (in comparison to prior work) and consequently uses more compute and memory ‚Äî we used a 37-layer (12 block) Transformer architecture, and we train on sequences of up to 512 tokens. Most experiments were conducted on 4 and 8 GPU systems. The model does fine-tune to new tasks very quickly which helps mitigate the additional resource requirements.\nThe limits and bias of learning about the world through text: Books and text readily available on the internet do not contain complete or even accurate information about the world. Recent work (Lucy and Gauthier, 2017) has shown that certain kinds of information are difficult to learn via just text and other work (Gururangan et al., 2018) has shown that models learn and exploit biases in data distributions.\nStill brittle generalization: Although our approach improves performance across a broad range of tasks, current deep learning NLP models still exhibit surprising and counterintuitive behavior - especially when evaluated in a systematic, adversarial, or out-of-distribution way. Our approach is not immune to these issues, though we have observed some indications of progress. Our approach shows improved lexical robustness over previous purely neural approaches to textual entailment. On the dataset introduced in Glockner et al. (2018) our model achieves 83.75%, performing similarly to KIM, which incorporates external knowledge via WordNet.\nTraining\nTraining Data\nThe model developers write:\nWe use the BooksCorpus dataset (Zhu et al., 2015) for training the language model. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance. Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information.\nTraining Procedure\nThe model developers write:\nOur model largely follows the original transformer work [62]. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N (0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We used learned position embeddings instead of the sinusoidal version proposed in the original work. We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer.\nSee the paper for further details and links to citations.\nEvaluation\nThe following evaluation information is extracted from the associated blog post. See the associated paper for further details.\nTesting Data, Factors and Metrics\nThe model developers report that the model was evaluated on the following tasks and datasets using the listed metrics:\nTask: Textual Entailment\nDatasets: SNLI, MNLI Matched, MNLI Mismatched, SciTail, QNLI, RTE\nMetrics: Accuracy\nTask: Semantic Similarity\nDatasets: STS-B, QQP, MRPC\nMetrics: Accuracy\nTask: Reading Comprehension\nDatasets: RACE\nMetrics: Accuracy\nTask: Commonsense Reasoning\nDatasets: ROCStories, COPA\nMetrics: Accuracy\nTask: Sentiment Analysis\nDatasets: SST-2\nMetrics: Accuracy\nTask: Linguistic Acceptability\nDatasets: CoLA\nMetrics: Accuracy\nTask: Multi Task Benchmark\nDatasets: GLUE\nMetrics: Accuracy\nResults\nThe model achieves the following results without any fine-tuning (zero-shot):\nTask\nTE\nTE\nTE\nTE\nTE\nTE\nSS\nSS\nSS\nRC\nCR\nCR\nSA\nLA\nMTB\nDataset\nSNLI\nMNLI Matched\nMNLI Mismatched\nSciTail\nQNLI\nRTE\nSTS-B\nQQP\nMPRC\nRACE\nROCStories\nCOPA\nSST-2\nCoLA\nGLUE\n89.9\n82.1\n81.4\n88.3\n88.1\n56.0\n82.0\n70.3\n82.3\n59.0\n86.5\n78.6\n91.3\n45.4\n72.8\nEnvironmental Impact\nThe model developers report that:\nThe total compute used to train this model was 0.96 petaflop days (pfs-days).\n8 P600 GPU's * 30 days * 12 TFLOPS/GPU * 0.33 utilization = .96 pfs-days\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: 8 P600 GPUs\nHours used: 720 hours (30 days)\nCloud Provider: Unknown\nCompute Region: Unknown\nCarbon Emitted: Unknown\nTechnical Specifications\nSee the associated paper for details on the modeling architecture, objective, compute infrastructure, and training details.\nCitation Information\n@article{radford2018improving,\ntitle={Improving language understanding by generative pre-training},\nauthor={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},\nyear={2018},\npublisher={OpenAI}\n}\nAPA:\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.\nModel Card Authors\nThis model card was written by the Hugging Face team.",
    "FacebookAI/roberta-base": "RoBERTa base model\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nRoBERTa base model\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\nthis paper and first released in\nthis repository. This model is case-sensitive: it\nmakes a difference between english and English.\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\nModel description\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts.\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\nIntended uses & limitations\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the model hub to look for fine-tuned versions on a task that\ninterests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n'score': 0.3306540250778198,\n'token': 2943,\n'token_str': 'ƒ†male'},\n{'sequence': \"<s>Hello I'm a female model.</s>\",\n'score': 0.04655390977859497,\n'token': 2182,\n'token_str': 'ƒ†female'},\n{'sequence': \"<s>Hello I'm a professional model.</s>\",\n'score': 0.04232972860336304,\n'token': 2038,\n'token_str': 'ƒ†professional'},\n{'sequence': \"<s>Hello I'm a fashion model.</s>\",\n'score': 0.037216778844594955,\n'token': 2734,\n'token_str': 'ƒ†fashion'},\n{'sequence': \"<s>Hello I'm a Russian model.</s>\",\n'score': 0.03253649175167084,\n'token': 1083,\n'token_str': 'ƒ†Russian'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nLimitations and bias\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n'score': 0.08702439814805984,\n'token': 25682,\n'token_str': 'ƒ†mechanic'},\n{'sequence': '<s>The man worked as a waiter.</s>',\n'score': 0.0819653645157814,\n'token': 38233,\n'token_str': 'ƒ†waiter'},\n{'sequence': '<s>The man worked as a butcher.</s>',\n'score': 0.073323555290699,\n'token': 32364,\n'token_str': 'ƒ†butcher'},\n{'sequence': '<s>The man worked as a miner.</s>',\n'score': 0.046322137117385864,\n'token': 18678,\n'token_str': 'ƒ†miner'},\n{'sequence': '<s>The man worked as a guard.</s>',\n'score': 0.040150221437215805,\n'token': 2510,\n'token_str': 'ƒ†guard'}]\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n'score': 0.22177888453006744,\n'token': 35698,\n'token_str': 'ƒ†waitress'},\n{'sequence': '<s>The Black woman worked as a prostitute.</s>',\n'score': 0.19288744032382965,\n'token': 36289,\n'token_str': 'ƒ†prostitute'},\n{'sequence': '<s>The Black woman worked as a maid.</s>',\n'score': 0.06498628109693527,\n'token': 29754,\n'token_str': 'ƒ†maid'},\n{'sequence': '<s>The Black woman worked as a secretary.</s>',\n'score': 0.05375480651855469,\n'token': 2971,\n'token_str': 'ƒ†secretary'},\n{'sequence': '<s>The Black woman worked as a nurse.</s>',\n'score': 0.05245552211999893,\n'token': 9008,\n'token_str': 'ƒ†nurse'}]\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nThe RoBERTa model was pretrained on the reunion of five datasets:\nBookCorpus, a dataset consisting of 11,038 unpublished books;\nEnglish Wikipedia (excluding lists, tables and headers) ;\nCC-News, a dataset containing 63 millions English news\narticles crawled between September 2016 and February 2019.\nOpenWebText, an opensource recreation of the WebText dataset used to\ntrain GPT-2,\nStories a dataset containing a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas.\nTogether these datasets weigh 160GB of text.\nTraining procedure\nPreprocessing\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith <s> and the end of one by </s>\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by <mask>.\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\nPretraining\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9, Œ≤2=0.98\\beta_{2} = 0.98Œ≤2‚Äã=0.98 and œµ=1e‚àí6\\epsilon = 1e-6œµ=1e‚àí6, a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\nEvaluation results\nWhen fine-tuned on downstream tasks, this model achieves the following results:\nGlue test results:\nTask\nMNLI\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\n87.6\n91.9\n92.8\n94.8\n63.6\n91.2\n90.2\n78.7\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1907-11692,\nauthor    = {Yinhan Liu and\nMyle Ott and\nNaman Goyal and\nJingfei Du and\nMandar Joshi and\nDanqi Chen and\nOmer Levy and\nMike Lewis and\nLuke Zettlemoyer and\nVeselin Stoyanov},\ntitle     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\njournal   = {CoRR},\nvolume    = {abs/1907.11692},\nyear      = {2019},\nurl       = {http://arxiv.org/abs/1907.11692},\narchivePrefix = {arXiv},\neprint    = {1907.11692},\ntimestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "FacebookAI/roberta-large": "RoBERTa large model\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nRoBERTa large model\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\nthis paper and first released in\nthis repository. This model is case-sensitive: it\nmakes a difference between english and English.\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\nModel description\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts.\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\nIntended uses & limitations\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the model hub to look for fine-tuned versions on a task that\ninterests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n'score': 0.3317350447177887,\n'token': 2943,\n'token_str': 'ƒ†male'},\n{'sequence': \"<s>Hello I'm a fashion model.</s>\",\n'score': 0.14171843230724335,\n'token': 2734,\n'token_str': 'ƒ†fashion'},\n{'sequence': \"<s>Hello I'm a professional model.</s>\",\n'score': 0.04291723668575287,\n'token': 2038,\n'token_str': 'ƒ†professional'},\n{'sequence': \"<s>Hello I'm a freelance model.</s>\",\n'score': 0.02134818211197853,\n'token': 18150,\n'token_str': 'ƒ†freelance'},\n{'sequence': \"<s>Hello I'm a young model.</s>\",\n'score': 0.021098261699080467,\n'token': 664,\n'token_str': 'ƒ†young'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\nmodel = RobertaModel.from_pretrained('roberta-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\nmodel = TFRobertaModel.from_pretrained('roberta-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nLimitations and bias\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\n>>> unmasker(\"The man worked as a <mask>.\")\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n'score': 0.08260300755500793,\n'token': 25682,\n'token_str': 'ƒ†mechanic'},\n{'sequence': '<s>The man worked as a driver.</s>',\n'score': 0.05736079439520836,\n'token': 1393,\n'token_str': 'ƒ†driver'},\n{'sequence': '<s>The man worked as a teacher.</s>',\n'score': 0.04709019884467125,\n'token': 3254,\n'token_str': 'ƒ†teacher'},\n{'sequence': '<s>The man worked as a bartender.</s>',\n'score': 0.04641604796051979,\n'token': 33080,\n'token_str': 'ƒ†bartender'},\n{'sequence': '<s>The man worked as a waiter.</s>',\n'score': 0.04239227622747421,\n'token': 38233,\n'token_str': 'ƒ†waiter'}]\n>>> unmasker(\"The woman worked as a <mask>.\")\n[{'sequence': '<s>The woman worked as a nurse.</s>',\n'score': 0.2667474150657654,\n'token': 9008,\n'token_str': 'ƒ†nurse'},\n{'sequence': '<s>The woman worked as a waitress.</s>',\n'score': 0.12280137836933136,\n'token': 35698,\n'token_str': 'ƒ†waitress'},\n{'sequence': '<s>The woman worked as a teacher.</s>',\n'score': 0.09747499972581863,\n'token': 3254,\n'token_str': 'ƒ†teacher'},\n{'sequence': '<s>The woman worked as a secretary.</s>',\n'score': 0.05783602222800255,\n'token': 2971,\n'token_str': 'ƒ†secretary'},\n{'sequence': '<s>The woman worked as a cleaner.</s>',\n'score': 0.05576248839497566,\n'token': 16126,\n'token_str': 'ƒ†cleaner'}]\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nThe RoBERTa model was pretrained on the reunion of five datasets:\nBookCorpus, a dataset consisting of 11,038 unpublished books;\nEnglish Wikipedia (excluding lists, tables and headers) ;\nCC-News, a dataset containing 63 millions English news\narticles crawled between September 2016 and February 2019.\nOpenWebText, an opensource recreation of the WebText dataset used to\ntrain GPT-2,\nStories a dataset containing a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas.\nTogether theses datasets weight 160GB of text.\nTraining procedure\nPreprocessing\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked\nwith <s> and the end of one by </s>\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by <mask>.\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\nPretraining\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 4e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9, Œ≤2=0.98\\beta_{2} = 0.98Œ≤2‚Äã=0.98 and\nœµ=1e‚àí6\\epsilon = 1e-6œµ=1e‚àí6, a weight decay of 0.01, learning rate warmup for 30,000 steps and linear decay of the learning\nrate after.\nEvaluation results\nWhen fine-tuned on downstream tasks, this model achieves the following results:\nGlue test results:\nTask\nMNLI\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\n90.2\n92.2\n94.7\n96.4\n68.0\n96.4\n90.9\n86.6\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1907-11692,\nauthor    = {Yinhan Liu and\nMyle Ott and\nNaman Goyal and\nJingfei Du and\nMandar Joshi and\nDanqi Chen and\nOmer Levy and\nMike Lewis and\nLuke Zettlemoyer and\nVeselin Stoyanov},\ntitle     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\njournal   = {CoRR},\nvolume    = {abs/1907.11692},\nyear      = {2019},\nurl       = {http://arxiv.org/abs/1907.11692},\narchivePrefix = {arXiv},\neprint    = {1907.11692},\ntimestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "google-t5/t5-base": "Model Card for T5 Base\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card Authors\nHow to Get Started with the Model\nModel Card for T5 Base\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nHow To Get Started With the Model\nModel Details\nModel Description\nThe developers of the Text-To-Text Transfer Transformer (T5) write:\nWith T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\nT5-Base is the checkpoint with 220 million parameters.\nDeveloped by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\nModel type: Language model\nLanguage(s) (NLP): English, French, Romanian, German\nLicense: Apache 2.0\nRelated Models: All T5 Checkpoints\nResources for more information:\nResearch paper\nGoogle's T5 Blog Post\nGitHub Repo\nHugging Face T5 Docs\nUses\nDirect Use and Downstream Use\nThe developers write in a blog post that the model:\nOur text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\nSee the blog post and research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nMore information needed.\nRecommendations\nMore information needed.\nTraining Details\nTraining Data\nThe model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\nThe model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.).\nThereby, the following datasets were being used for (1.) and (2.):\nDatasets used for Unsupervised denoising objective:\nC4\nWiki-DPR\nDatasets used for Supervised text-to-text language modeling objective\nSentence acceptability judgment\nCoLA Warstadt et al., 2018\nSentiment analysis\nSST-2 Socher et al., 2013\nParaphrasing/sentence similarity\nMRPC Dolan and Brockett, 2005\nSTS-B Ceret al., 2017\nQQP Iyer et al., 2017\nNatural language inference\nMNLI Williams et al., 2017\nQNLI Rajpurkar et al.,2016\nRTE Dagan et al., 2005\nCB De Marneff et al., 2019\nSentence completion\nCOPA Roemmele et al., 2011\nWord sense disambiguation\nWIC Pilehvar and Camacho-Collados, 2018\nQuestion answering\nMultiRC Khashabi et al., 2018\nReCoRD Zhang et al., 2018\nBoolQ Clark et al., 2019\nTraining Procedure\nIn their abstract, the model developers write:\nIn this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the research paper for further details.\nEvaluation\nTesting Data, Factors & Metrics\nThe developers evaluated the model on 24 tasks, see the research paper for full details.\nResults\nFor full results for T5-Base, see the research paper, Table 14.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@article{2020t5,\nauthor  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\ntitle   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\njournal = {Journal of Machine Learning Research},\nyear    = {2020},\nvolume  = {21},\nnumber  = {140},\npages   = {1-67},\nurl     = {http://jmlr.org/papers/v21/20-074.html}\n}\nAPA:\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\nModel Card Authors\nThis model card was written by the team at Hugging Face.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nClick to expand\nfrom transformers import T5Tokenizer, T5Model\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\nmodel = T5Model.from_pretrained(\"t5-base\")\ninput_ids = tokenizer(\n\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n).input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\nSee the Hugging Face T5 docs and a Colab Notebook created by the model developers for more examples."
}