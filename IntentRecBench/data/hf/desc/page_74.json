{
    "ibm-granite/granite-timeseries-ttm-r1": "A newer version of this model is available:\nibm-granite/granite-timeseries-ttm-r2\nGranite-TimeSeries-TTM-R1 Model Card\nModel Description\nModel Releases (along with the branch name where the models are stored):\nModel Capabilities with example scripts\nBenchmarks\nRecommended Use\nModel Details\nModel Sources\nBlogs and articles on TTM:\nUses\nTraining Data\nCitation\nModel Card Authors\nIBM Public Repository Disclosure:\nGranite-TimeSeries-TTM-R1 Model Card\nTinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research.\nWith less than 1 Million parameters, TTM (accepted in NeurIPS 24) introduces the notion of the first-ever ‚Äútiny‚Äù pre-trained models for Time-Series Forecasting.\nTTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting. TTMs are lightweight\nforecasters, pre-trained on publicly available time series data with various augmentations. TTM provides state-of-the-art zero-shot forecasts and can easily be\nfine-tuned for multi-variate forecasts with just 5% of the training data to be competitive.  Refer to our paper for more details.\nThe current open-source version supports point forecasting use-cases specifically ranging from minutely to hourly resolutions\n(Ex. 10 min, 15 min, 1 hour.).\nNote that zeroshot, fine-tuning and inference tasks using TTM can easily be executed in 1 GPU machine or in laptops too!!\nNew updates: TTM-R1 comprises TTM variants pre-trained on 250M public training samples. We have another set of TTM models released recently under TTM-R2 trained on a much larger pretraining\ndataset (~700M samples) which can be accessed from here. In general, TTM-R2 models perform better than\nTTM-R1 models as they are trained on larger pretraining dataset. However, the choice of R1 vs R2 depends on your target data distribution. Hence requesting users to\ntry both R1 and R2 variants and pick the best for your data.\nModel Description\nTTM falls under the category of ‚Äúfocused pre-trained models‚Äù, wherein each pre-trained TTM is tailored for a particular forecasting\nsetting (governed by the context length and forecast length). Instead of building one massive model supporting all forecasting settings,\nwe opt for the approach of constructing smaller pre-trained models, each focusing on a specific forecasting setting, thereby\nyielding more accurate results. Furthermore, this approach ensures that our models remain extremely small and exceptionally fast,\nfacilitating easy deployment without demanding a ton of resources.\nHence, in this model card, we plan to release several pre-trained\nTTMs that can cater to many common forecasting settings in practice. Additionally, we have released our source code along with\nour pretraining scripts that users can utilize to pretrain models on their own. Pretraining TTMs is very easy and fast, taking\nonly 3-6 hours using 6 A100 GPUs, as opposed to several days or weeks in traditional approaches.\nEach pre-trained model will be released in a different branch name in this model card. Kindly access the required model using our\ngetting started notebook mentioning the branch name.\nModel Releases (along with the branch name where the models are stored):\n512-96: Given the last 512 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length)\nin future. This model is targeted towards a forecasting setting of context length 512 and forecast length 96 and\nrecommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc). This model refers to the TTM-Q variant used in the paper. (branch name: main) [Benchmark Scripts]\n1024-96: Given the last 1024 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length)\nin future. This model is targeted towards a long forecasting setting of context length 1024 and forecast length 96 and\nrecommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc). (branch name: 1024-96-v1) [Benchmark Scripts]\nWe can also use the [get_model] utility to automatically select the required model based on your input context length and forecast length requirement.\nFor more variants (till forecast length 720), refer to our new model card here\nModel Capabilities with example scripts\nThe below model scripts can be used for any of the above TTM models. Please update the HF model URL and branch name in the from_pretrained call appropriately to pick the model of your choice.\nGetting Started [colab]\nZeroshot Multivariate Forecasting [Example]\nFinetuned Multivariate Forecasting:\nChannel-Independent Finetuning [Example 1] [Example 2]\nChannel-Mix Finetuning [Example]\nNew Releases (extended features released on October 2024)\nFinetuning and Forecasting with Exogenous/Control Variables [Example]\nFinetuning and Forecasting with static categorical features [Example: To be added soon]\nRolling Forecasts - Extend forecast lengths beyond 96 via rolling capability [Example]\nHelper scripts for optimal Learning Rate suggestions for Finetuning [Example]\nBenchmarks\nTTM outperforms popular benchmarks such as TimesFM, Moirai, Chronos, Lag-Llama, Moment, GPT4TS, TimeLLM, LLMTime in zero/fewshot forecasting while reducing computational requirements significantly.\nMoreover, TTMs are lightweight and can be executed even on CPU-only machines, enhancing usability and fostering wider\nadoption in resource-constrained environments. For more details, refer to our paper TTM-Q referred in the paper maps to the 512-96 model\nuploaded in the main branch. For other variants (TTM-B, TTM-E and TTM-A) please refer here. For more details, refer to the paper.\nRecommended Use\nUsers have to externally standard scale their data independently for every channel before feeding it to the model (Refer to TSP, our data processing utility for data scaling.)\nThe current open-source version supports only minutely and hourly resolutions(Ex. 10 min, 15 min, 1 hour.). Other lower resolutions (say weekly, or monthly) are currently not supported in this version, as the model needs a minimum context length of 512 or 1024.\nEnabling any upsampling or prepending zeros to virtually increase the context length for shorter-length datasets is not recommended and will\nimpact the model performance.\nModel Details\nFor more details on TTM architecture and benchmarks, refer to our paper.\nTTM-1 currently supports 2 modes:\nZeroshot forecasting: Directly apply the pre-trained model on your target data to get an initial forecast (with no training).\nFinetuned forecasting: Finetune the pre-trained model with a subset of your target data to further improve the forecast.\nSince, TTM models are extremely small and fast, it is practically very easy to finetune the model with your available target data in few minutes\nto get more accurate forecasts.\nThe current release supports multivariate forecasting via both channel independence and channel-mixing approaches.\nDecoder Channel-Mixing can be enabled during fine-tuning for capturing strong channel-correlation patterns across\ntime-series variates, a critical capability lacking in existing counterparts.\nIn addition, TTM also supports exogenous infusion and categorical data infusion.\nModel Sources\nRepository: https://github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer\nPaper: https://arxiv.org/pdf/2401.03955.pdf\nBlogs and articles on TTM:\nRefer to our wiki\nUses\n# Load Model from HF Model Hub mentioning the branch name in revision field\nmodel = TinyTimeMixerForPrediction.from_pretrained(\n\"https://huggingface.co/ibm/TTM\", revision=\"main\"\n)\n# Do zeroshot\nzeroshot_trainer = Trainer(\nmodel=model,\nargs=zeroshot_forecast_args,\n)\n)\nzeroshot_output = zeroshot_trainer.evaluate(dset_test)\n# Freeze backbone and enable few-shot or finetuning:\n# freeze backbone\nfor param in model.backbone.parameters():\nparam.requires_grad = False\nfinetune_forecast_trainer = Trainer(\nmodel=model,\nargs=finetune_forecast_args,\ntrain_dataset=dset_train,\neval_dataset=dset_val,\ncallbacks=[early_stopping_callback, tracking_callback],\noptimizers=(optimizer, scheduler),\n)\nfinetune_forecast_trainer.train()\nfewshot_output = finetune_forecast_trainer.evaluate(dset_test)\nTraining Data\nThe original r1 TTM models were trained on a collection of datasets from the Monash Time Series Forecasting repository. The datasets used include:\nAustralian Electricity Demand: https://zenodo.org/records/4659727\nAustralian Weather: https://zenodo.org/records/4654822\nBitcoin dataset: https://zenodo.org/records/5122101\nKDD Cup 2018 dataset: https://zenodo.org/records/4656756\nLondon Smart Meters: https://zenodo.org/records/4656091\nSaugeen River Flow: https://zenodo.org/records/4656058\nSolar Power: https://zenodo.org/records/4656027\nSunspots: https://zenodo.org/records/4654722\nSolar: https://zenodo.org/records/4656144\nUS Births: https://zenodo.org/records/4656049\nWind Farms Production data: https://zenodo.org/records/4654858\nWind Power: https://zenodo.org/records/4656032\nCitation\nKindly cite the following paper, if you intend to use our model or its associated architectures/approaches in your\nwork\nBibTeX:\n@inproceedings{ekambaram2024tinytimemixersttms,\ntitle={Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series},\nauthor={Vijay Ekambaram and Arindam Jati and Pankaj Dayama and Sumanta Mukherjee and Nam H. Nguyen and Wesley M. Gifford and Chandra Reddy and Jayant Kalagnanam},\nbooktitle={Advances in Neural Information Processing Systems (NeurIPS 2024)},\nyear={2024},\n}\nModel Card Authors\nVijay Ekambaram, Arindam Jati, Pankaj Dayama, Wesley M. Gifford, Sumanta Mukherjee, Chandra Reddy and Jayant Kalagnanam\nIBM Public Repository Disclosure:\nAll content in this repository including code has been provided by IBM under the associated\nopen source software license and IBM is under no obligation to provide enhancements,\nupdates, or support. IBM developers produced this code as an\nopen source project (not as an IBM product), and IBM makes no assertions as to\nthe level of quality nor security, and will not be maintaining this code going forward.",
    "h2oai/h2o-danube2-1.8b-chat": "Model Card\nSummary\nModel Architecture\nUsage\nQuantization and sharding\nModel Architecture\nBenchmarks\nü§ó Open LLM Leaderboard\nMT-Bench\nDisclaimer\nModel Card\nSummary\nh2o-danube2-1.8b-chat is a chat fine-tuned model by H2O.ai with 1.8 billion parameters. We release three versions of this model:\nModel Name\nDescription\nh2oai/h2o-danube2-1.8b-base\nBase model\nh2oai/h2o-danube2-1.8b-sft\nSFT tuned\nh2oai/h2o-danube2-1.8b-chat\nSFT + DPO tuned\nThis model was trained using H2O LLM Studio.\nModel Architecture\nWe adjust the Llama 2 architecture for a total of around 1.8b parameters. For details, please refer to our Technical Report. We use the Mistral tokenizer with a vocabulary size of 32,000 and train our model up to a context length of 8,192.\nThe details of the model architecture are:\nHyperparameter\nValue\nn_layers\n24\nn_heads\n32\nn_query_groups\n8\nn_embd\n2560\nvocab size\n32000\nsequence length\n8192\nUsage\nTo use the model with the transformers library on a machine with GPUs, first make sure you have the transformers library installed.\npip install transformers>=4.39.3\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=\"h2oai/h2o-danube2-1.8b-chat\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\n# We use the HF Tokenizer chat template to format each message\n# https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n{\"role\": \"user\", \"content\": \"Why is drinking water so healthy?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nres = pipe(\nprompt,\nmax_new_tokens=256,\n)\nprint(res[0][\"generated_text\"])\nThis will apply and run the correct prompt format out of the box:\n<|prompt|>Why is drinking water so healthy?</s><|answer|>\nQuantization and sharding\nYou can load the models using quantization by specifying load_in_8bit=True or load_in_4bit=True. Also, sharding on multiple GPUs is possible by setting device_map=auto.\nModel Architecture\nMistralForCausalLM(\n(model): MistralModel(\n(embed_tokens): Embedding(32000, 2560, padding_idx=0)\n(layers): ModuleList(\n(0-23): 24 x MistralDecoderLayer(\n(self_attn): MistralAttention(\n(q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n(k_proj): Linear(in_features=2560, out_features=640, bias=False)\n(v_proj): Linear(in_features=2560, out_features=640, bias=False)\n(o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n(rotary_emb): MistralRotaryEmbedding()\n)\n(mlp): MistralMLP(\n(gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n(up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n(down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n(act_fn): SiLU()\n)\n(input_layernorm): MistralRMSNorm()\n(post_attention_layernorm): MistralRMSNorm()\n)\n)\n(norm): MistralRMSNorm()\n)\n(lm_head): Linear(in_features=2560, out_features=32000, bias=False)\n)\nBenchmarks\nü§ó Open LLM Leaderboard\nBenchmark\nacc_n\nAverage\n48.44\nARC-challenge\n43.43\nHellaswag\n73.54\nMMLU\n37.77\nTruthfulQA\n39.96\nWinogrande\n69.77\nGSM8K\n26.16\nMT-Bench\nFirst Turn: 6.23\nSecond Turn: 5.34\nAverage: 5.79\nDisclaimer\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\nBiases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\nLimitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\nUse at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\nEthical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\nReporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\nChanges to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.",
    "h2oai/h2o-danube2-1.8b-base": "Summary\nModel Architecture\nUsage\nBenchmarks\nDisclaimer\nSummary\nh2o-danube2-1.8b-base is a foundation model trained by H2O.ai with 1.8 billion parameters. For details, please refer to our Technical Report. We release three versions of this model:\nModel Name\nDescription\nh2oai/h2o-danube2-1.8b-base\nBase model\nh2oai/h2o-danube2-1.8b-sft\nSFT tuned\nh2oai/h2o-danube2-1.8b-chat\nSFT + DPO tuned\nModel Architecture\nWe adjust the Llama 2 architecture for a total of around 1.8b parameters. We use the Mistral tokenizer with a vocabulary size of 32,000 and train our model up to a context length of 8,192.\nThe details of the model architecture are:\nHyperparameter\nValue\nn_layers\n24\nn_heads\n32\nn_query_groups\n8\nn_embd\n2560\nvocab size\n32000\nsequence length\n8192\nUsage\nThis is a pre-trained foundation model. For your task, you will likely want to perform application specific fine-tuning. We also offer a chat fine-tuned version: h2oai/h2o-danube2-1.8b-chat.\nTo use the model with the transformers library on a machine with GPUs, first make sure you have the transformers library installed.\n# pip install transformers>=4.39.3\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2o-danube2-1.8b-base\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"h2oai/h2o-danube2-1.8b-base\",\ntorch_dtype=torch.bfloat16,\n)\nmodel.cuda()\ninputs = tokenizer(\"The Danube is the second longest river in Europe\", return_tensors=\"pt\").to(model.device)\nres = model.generate(\n**inputs,\nmax_new_tokens=38,\ndo_sample=False,\n)\nprint(tokenizer.decode(res[0], skip_special_tokens=True))\nBenchmarks\nAmong models of similar size h2o-danube2-1.8b-base achieves best results (on average) across benchmarks of Open LLM Leaderboard ü§ó\nModel\nSize\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8k\nAverage\nStableLM2-1.6B\n1.6B\n43.34\n70.45\n38.95\n36.78\n64.56\n17.44\n45.25\nGemma-2B\n2.5B\n48.46\n71.65\n41.68\n33.13\n66.77\n17.36\n46.51\nQwen1.5-1.8B\n1.8B\n37.88\n61.42\n46.71\n39.43\n60.30\n33.59\n46.55\nPhi-1.5\n1.3B\n52.90\n63.79\n43.89\n40.89\n72.22\n12.43\n47.69\nH2O-Danube2\n1.8B\n43.52\n73.06\n40.05\n38.09\n68.43\n29.34\n48.75\nDisclaimer\nPlease read this disclaimer carefully before using the large language model provided in this repository. Your use of the model signifies your agreement to the following terms and conditions.\nBiases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints.\nLimitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.\nUse at Your Own Risk: Users of this large language model must assume full responsibility for any consequences that may arise from their use of the tool. The developers and contributors of this repository shall not be held liable for any damages, losses, or harm resulting from the use or misuse of the provided model.\nEthical Considerations: Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.\nReporting Issues: If you encounter any biased, offensive, or otherwise inappropriate content generated by the large language model, please report it to the repository maintainers through the provided channels. Your feedback will help improve the model and mitigate potential issues.\nChanges to this Disclaimer: The developers of this repository reserve the right to modify or update this disclaimer at any time without prior notice. It is the user's responsibility to periodically review the disclaimer to stay informed about any changes.\nBy using the large language model provided in this repository, you agree to accept and comply with the terms and conditions outlined in this disclaimer. If you do not agree with any part of this disclaimer, you should refrain from using the model and any content generated by it.",
    "Kijai/SUPIR_pruned": "No model card",
    "ChristianAzinn/labse-gguf": "labse-gguf\nOriginal Description\nDescription\nCompatibility\nMeta-information\nExplanation of quantisation methods\nProvided Files\nExamples\nExample Usage with  llama.cpp\nExample Usage with LM Studio\nAcknowledgements\nlabse-gguf\nModel creator: sentence-transformers\nOriginal model: labse\nOriginal Description\nThe language-agnostic BERT sentence embedding encodes text into high dimensional vectors. The model is trained and optimized to produce similar representations exclusively for bilingual sentence pairs that are translations of each other. So it can be used for mining for translations of a sentence in a larger corpus.\nDescription\nThis repo contains GGUF format files for the labse embedding model.\nThese files were converted and quantized with llama.cpp PR 5500, commit 34aa045de, on a consumer RTX 4090.\nThis model supports up to 512 tokens of context.\nCompatibility\nThese files are compatible with llama.cpp as of commit 4524290e8, as well as LM Studio as of version 0.2.19.\nMeta-information\nExplanation of quantisation methods\nClick to see details\nThe methods available are:\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided Files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\nName\nQuant method\nBits\nSize\nUse case\nlabse.Q2_K.gguf\nQ2_K\n2\n364 MB\nsmallest, significant quality loss - not recommended for most purposes\nlabse.Q3_K_S.gguf\nQ3_K_S\n3\n368 MB\nvery small, high quality loss\nlabse.Q3_K_M.gguf\nQ3_K_M\n3\n374 MB\nvery small, high quality loss\nlabse.Q3_K_L.gguf\nQ3_K_L\n3\n379 MB\nsmall, substantial quality loss\nlabse.Q4_0.gguf\nQ4_0\n4\n379 MB\nlegacy; small, very high quality loss - prefer using Q3_K_M\nlabse.Q4_K_S.gguf\nQ4_K_S\n4\n380 MB\nsmall, greater quality loss\nlabse.Q4_K_M.gguf\nQ4_K_M\n4\n384 MB\nmedium, balanced quality - recommended\nlabse.Q5_0.gguf\nQ5_0\n5\n390 MB\nlegacy; medium, balanced quality - prefer using Q4_K_M\nlabse.Q5_K_S.gguf\nQ5_K_S\n5\n390 MB\nlarge, low quality loss - recommended\nlabse.Q5_K_M.gguf\nQ5_K_M\n5\n392 MB\nlarge, very low quality loss - recommended\nlabse.Q6_K.gguf\nQ6_K\n6\n401 MB\nvery large, extremely low quality loss\nlabse.Q8_0.gguf\nQ8_0\n8\n515 MB\nvery large, extremely low quality loss - recommended\nlabse.Q8_0.gguf\nfp16\n16\n955 MB\nenormous, pretty much the original model - not recommended\nlabse.Q8_0.gguf\nfp32\n32\n1.89 GB\nenormous, pretty much the original model - not recommended\nExamples\nExample Usage with  llama.cpp\nTo compute a single embedding, build llama.cpp and run:\n./embedding -ngl 99 -m [filepath-to-gguf].gguf -p 'search_query: What is TSNE?'\nYou can also submit a batch of texts to embed, as long as the total number of tokens does not exceed the context length. Only the first three embeddings are shown by the embedding example.\ntexts.txt:\nsearch_query: What is TSNE?\nsearch_query: Who is Laurens Van der Maaten?\nCompute multiple embeddings:\n./embedding -ngl 99 -m [filepath-to-gguf].gguf -f texts.txt\nExample Usage with LM Studio\nDownload the 0.2.19 beta build from here: Windows MacOS Linux\nOnce installed, open the app. The home should look like this:\nSearch for either \"ChristianAzinn\" in the main search bar or go to the \"Search\" tab on the left menu and search the name there.\nSelect your model from those that appear (this example uses bge-small-en-v1.5-gguf) and select which quantization you want to download. Since this model is pretty small, I recommend Q8_0, if not f16/32. Generally, the lower you go in the list (or the bigger the number gets), the larger the file and the better the performance.\nYou will see a green checkmark and the word \"Downloaded\" once the model has successfully downloaded, which can take some time depending on your network speeds.\nOnce this model is finished downloading, navigate to the \"Local Server\" tab on the left menu and open the loader for text embedding models. This loader does not appear before version 0.2.19, so ensure you downloaded the correct version.\nSelect the model you just downloaded from the dropdown that appears to load it. You may need to play with configuratios in the right-side menu, such as GPU offload if it doesn't fit entirely into VRAM.\nAll that's left to do is to hit the \"Start Server\" button:\nAnd if you see text like that shown below in the console, you're good to go! You can use this as a drop-in replacement for the OpenAI embeddings API in any application that requires it, or you can query the endpoint directly to test it out.\nExample curl request to the API endpoint:\ncurl http://localhost:1234/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"input\": \"Your text string goes here\",\n\"model\": \"model-identifier-here\"\n}'\nFor more information, see the LM Studio text embedding documentation.\nAcknowledgements\nThanks to the LM Studio team and everyone else working on open-source AI.\nThis README is inspired by that of nomic-ai-embed-text-v1.5-gguf, another excellent embedding model, and those of the legendary TheBloke.",
    "NYUAD-ComNets/NYUAD_AI-generated_images_detector": "AI-generated_images_detector\nTo utilize this model\nTraining and evaluation data\nTraining results\nBibTeX entry and citation info\nAI-generated_images_detector\nThis model achieves the following results on the evaluation set:\nLoss: 0.0987\nAccuracy: 0.9736\nTo utilize this model\nfrom PIL import Image\nfrom transformers import pipeline\nclassifier = pipeline(\"image-classification\", model=\"NYUAD-ComNets/NYUAD_AI-generated_images_detector\")\nimage=Image.open(\"path_to_image\")\npred=classifier(image)\nprint(pred)\nTraining and evaluation data\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\n0.0431\n0.55\n100\n0.1672\n0.9568\n0.0139\n1.1\n200\n0.2338\n0.9398\n0.0201\n1.66\n300\n0.1291\n0.9655\n0.0023\n2.21\n400\n0.1147\n0.9709\n0.0033\n2.76\n500\n0.0987\n0.9736\nBibTeX entry and citation info\n@article{aldahoul2024detecting,\ntitle={Detecting AI-Generated Images Using Vision Transformers: A Robust Approach for Safeguarding Visual Media Integrity},\nauthor={AlDahoul, Nouar and Zaki, Yasir},\njournal={Available at SSRN},\nyear={2024}\n}\n@misc{ComNets,\nurl={https://huggingface.co/NYUAD-ComNets/NYUAD_AI-generated_images_detector](https://huggingface.co/NYUAD-ComNets/NYUAD_AI-generated_images_detector)},\ntitle={NYUAD_AI-generated_images_detector},\nauthor={Nouar AlDahoul, Yasir Zaki}\n}",
    "Pelochus/ezrkllm-collection": "ezrkllm-collection\nAvailable LLMs\nDownloading a model\nRKLLM parameters used\nFuture additions\nMore info\nezrkllm-collection\nCollection of LLMs compatible with Rockchip's chips using their rkllm-toolkit.\nThis repo contains the converted models for running on the RK3588 NPU found in SBCs like Orange Pi 5, NanoPi R6 and Radxa Rock 5.\nCheck the main repo on GitHub for how to install and use: https://github.com/Pelochus/ezrknpu\nAvailable LLMs\nBefore running any LLM, take into account that the required RAM is between 1.5-3 times the model size (this is an estimation, haven't done extensive testing yet).\nRight now, only converted the following models:\nLLM\nParameters\nLink\nQwen 2\n1.5B\nhttps://huggingface.co/Pelochus/deepseek-R1-distill-qwen-1.5B\nQwen Chat\n1.8B\nhttps://huggingface.co/Pelochus/qwen-1_8B-rk3588\nGemma\n2B\nhttps://huggingface.co/Pelochus/gemma-2b-rk3588\nMicrosoft Phi-2\n2.7B\nhttps://huggingface.co/Pelochus/phi-2-rk3588\nMicrosoft Phi-3 Mini\n3.8B\nhttps://huggingface.co/Pelochus/phi-3-mini-rk3588\nLlama 2 7B\n7B\nhttps://huggingface.co/Pelochus/llama2-chat-7b-hf-rk3588\nLlama 2 13B\n13B\nhttps://huggingface.co/Pelochus/llama2-chat-13b-hf-rk3588\nTinyLlama v1\n1.1B\nhttps://huggingface.co/Pelochus/tinyllama-v1-rk3588\nQwen 1.5 Chat\n4B\nhttps://huggingface.co/Pelochus/qwen1.5-chat-4B-rk3588\nQwen 2\n1.5B\nhttps://huggingface.co/Pelochus/qwen2-1_5B-rk3588\nLlama 2 was converted using Azure servers.\nFor reference, converting Phi-2 peaked at about 15 GBs of RAM + 25 GBs of swap (counting OS, but that was using about 2 GBs max).\nConverting Llama 2 7B peaked at about 32 GBs of RAM + 50 GB of swap.\nDownloading a model\nUse:\ngit clone LINK_FROM_PREVIOUS_TABLE_HERE\nAnd then (may not be necessary):\ngit lfs pull\nIf the first clone gives you problems (takes too long) you can also:\nGIT_LFS_SKIP_SMUDGE=1 git clone LINK_FROM_PREVIOUS_TABLE_HERE\nAnd then 'git lfs pull' inside the cloned folder to download the full model.\nRKLLM parameters used\nRK3588 only supports w8a8 quantization, so that was the selected quantization for ALL models.\nAside from that, RKLLM toolkit allows for no optimization (0) and optimization (1).\nAll models are optimized.\nFuture additions\nConverting other compatible LLMs\nAdding other compatible Rockchip's SoCs\nMore info\nMy fork for rknn-llm: https://github.com/Pelochus/ezrknn-llm\nOriginal Rockchip's LLMs repo: https://github.com/airockchip/rknn-llm",
    "sam749/AniHelloy2d-v2-1": "AniHelloy2d\nv2.1\nDescription:\nCreator: MarkWar\nCivitai Page: https://civitai.com/models/147548\nDiffusers\nAniHelloy2d\nv2.1\nDescription:\nCreator: MarkWar\nCivitai Page: https://civitai.com/models/147548\nYou can use this with the üß®Diffusers library\nDiffusers\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"sam749/AniHelloy2d-v2-1\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"masterpiece, best quality, 1girl, (colorful),(delicate eyes and face), volumatic light, ray tracing, bust shot ,extremely detailed CG unity 8k wallpaper,solo,smile\"\nimage = pipe(prompt).images[0]\nimage.save(\"result.png\")",
    "microsoft/wavecoder-ultra-6.7b": "üî• News\nüí° Introduction\nü™Å Evaluation\nüìñ License\n‚òïÔ∏è Citation\nNote\nüåä WaveCoder: Widespread And Versatile Enhanced Code LLM\n[üìú Paper] ‚Ä¢\n[üê± GitHub]\n[üê¶ Twitter] ‚Ä¢\n[üí¨ Reddit] ‚Ä¢\n[üçÄ Unofficial Blog]\nRepo for \"WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation\"\nüî• News\n[2024/04/10] üî•üî•üî• WaveCoder repo, models released at ü§ó HuggingFace!\n[2023/12/26] WaveCoder paper released.\nüí° Introduction\nWaveCoder üåä is a series of large language models (LLMs) for the coding domain, designed to solve relevant problems in the field of code through instruction-following learning. Its training dataset was generated from a subset of code-search-net data using a generator-discriminator framework based on LLMs that we proposed, covering four general code-related tasks: code generation, code summary, code translation, and code repair.\nModel\nHumanEval\nMBPP(500)\nHumanEvalFix(Avg.)\nHumanEvalExplain(Avg.)\nGPT-4\n85.4\n-\n47.8\n52.1\nüåä WaveCoder-DS-6.7B\n65.8\n63.0\n49.5\n40.8\nüåä WaveCoder-Pro-6.7B\n74.4\n63.4\n52.1\n43.0\nüåä WaveCoder-Ultra-6.7B\n79.9\n64.6\n52.3\n45.7\nü™Å Evaluation\nPlease refer to WaveCoder's GitHub repo for inference, evaluation, and training code.\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/wavecoder-ultra-6.7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/wavecoder-ultra-6.7b\")\nüìñ License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the its License.\n‚òïÔ∏è Citation\nIf you find this repository helpful, please consider citing our paper:\n@article{yu2023wavecoder,\ntitle={Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation},\nauthor={Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng},\njournal={arXiv preprint arXiv:2312.14187},\nyear={2023}\n}\nNote\nWaveCoder models are trained on the synthetic data generated by OpenAI models. Please pay attention to OpenAI's terms of use when using the models and the datasets.",
    "PixArt-alpha/PixArt-Sigma-XL-2-512-MS": "üê± PixArt-Œ£ Model Card\nModel\nModel Description\nModel Sources\nüß® Diffusers\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nüê± PixArt-Œ£ Model Card\nModel\nPixArt-Œ£ consists of pure transformer blocks for latent diffusion:\nIt can directly generate 1024px, 2K and 4K images from text prompts within a single sampling process.\nSource code is available at https://github.com/PixArt-alpha/PixArt-sigma.\nModel Description\nDeveloped by: PixArt-Œ£\nModel type: Diffusion-Transformer-based text-to-image generative model\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts.\nIt is a Transformer Latent Diffusion Model that uses one fixed, pretrained text encoders (T5))\nand one latent feature encoder (VAE).\nResources for more information: Check out our GitHub Repository and the PixArt-Œ£ report on arXiv.\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/PixArt-alpha/PixArt-sigma),\nwhich is more suitable for both training and inference and for which most advanced diffusion sampler like SA-Solver will be added over time.\nHugging Face provides free PixArt-Œ£ inference.\nRepository: https://github.com/PixArt-alpha/PixArt-sigma\nDemo: https://huggingface.co/spaces/PixArt-alpha/PixArt-Sigma\nüß® Diffusers\nMake sure to upgrade diffusers to >= 0.28.0:\npip install -U diffusers --upgrade\nIn addition make sure to install transformers, safetensors, sentencepiece, and accelerate:\npip install transformers accelerate safetensors sentencepiece\nFor diffusers<0.28.0, check this script for help.\nTo just use the base model, you can run:\nimport torch\nfrom diffusers import Transformer2DModel, PixArtSigmaPipeline\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nweight_dtype = torch.float16\npipe = PixArtSigmaPipeline.from_pretrained(\n\"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\ntorch_dtype=weight_dtype,\nuse_safetensors=True,\n)\npipe.to(device)\n# Enable memory optimizations.\n# pipe.enable_model_cpu_offload()\nprompt = \"A small cactus with a happy face in the Sahara desert.\"\nimage = pipe(prompt).images[0]\nimage.save(\"./catcus.png\")\nWhen using torch >= 2.0, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline:\npipe.transformer = torch.compile(pipe.transformer, mode=\"reduce-overhead\", fullgraph=True)\nIf you are limited by GPU VRAM, you can enable cpu offloading by calling pipe.enable_model_cpu_offload\ninstead of .to(\"cuda\"):\n- pipe.to(\"cuda\")\n+ pipe.enable_model_cpu_offload()\nFor more information on how to use PixArt-Œ£ with diffusers, please have a look at the PixArt-Œ£ Docs.\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nfingers, .etc in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "bartowski/Mistral-22B-v0.1-GGUF": "Llamacpp Quantizations of Mistral-22B-v0.1\nPrompt format\nDownload a file (not the whole branch) from below:\nWhich file should I choose?\nLlamacpp Quantizations of Mistral-22B-v0.1\nUsing llama.cpp release b2636 for quantization.\nOriginal model: https://huggingface.co/Vezora/Mistral-22B-v0.1\nAll quants made using imatrix option with dataset provided by Kalomaze here\nPrompt format\nNo chat template specified so default is used. This may be incorrect, check original model card for details.\n<s> [INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n{prompt} [/INST]  </s>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nDescription\nMistral-22B-v0.1-Q8_0.gguf\nQ8_0\n23.63GB\nExtremely high quality, generally unneeded but max available quant.\nMistral-22B-v0.1-Q6_K.gguf\nQ6_K\n18.24GB\nVery high quality, near perfect, recommended.\nMistral-22B-v0.1-Q5_K_M.gguf\nQ5_K_M\n15.71GB\nHigh quality, recommended.\nMistral-22B-v0.1-Q5_K_S.gguf\nQ5_K_S\n15.31GB\nHigh quality, recommended.\nMistral-22B-v0.1-Q4_K_M.gguf\nQ4_K_M\n13.33GB\nGood quality, uses about 4.83 bits per weight, recommended.\nMistral-22B-v0.1-Q4_K_S.gguf\nQ4_K_S\n12.65GB\nSlightly lower quality with more space savings, recommended.\nMistral-22B-v0.1-IQ4_NL.gguf\nIQ4_NL\n12.60GB\nDecent quality, slightly smaller than Q4_K_S with similar performance recommended.\nMistral-22B-v0.1-IQ4_XS.gguf\nIQ4_XS\n11.92GB\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nMistral-22B-v0.1-Q3_K_L.gguf\nQ3_K_L\n11.72GB\nLower quality but usable, good for low RAM availability.\nMistral-22B-v0.1-Q3_K_M.gguf\nQ3_K_M\n10.75GB\nEven lower quality.\nMistral-22B-v0.1-IQ3_M.gguf\nIQ3_M\n10.05GB\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nMistral-22B-v0.1-IQ3_S.gguf\nIQ3_S\n9.68GB\nLower quality, new method with decent performance, recommended over Q3_K_S quant, same size with better performance.\nMistral-22B-v0.1-Q3_K_S.gguf\nQ3_K_S\n9.63GB\nLow quality, not recommended.\nMistral-22B-v0.1-IQ3_XS.gguf\nIQ3_XS\n9.17GB\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nMistral-22B-v0.1-IQ3_XXS.gguf\nIQ3_XXS\n8.59GB\nLower quality, new method with decent performance, comparable to Q3 quants.\nMistral-22B-v0.1-Q2_K.gguf\nQ2_K\n8.26GB\nVery low quality but surprisingly usable.\nMistral-22B-v0.1-IQ2_M.gguf\nIQ2_M\n7.61GB\nVery low quality, uses SOTA techniques to also be surprisingly usable.\nMistral-22B-v0.1-IQ2_S.gguf\nIQ2_S\n7.03GB\nVery low quality, uses SOTA techniques to be usable.\nMistral-22B-v0.1-IQ2_XS.gguf\nIQ2_XS\n6.64GB\nVery low quality, uses SOTA techniques to be usable.\nMistral-22B-v0.1-IQ2_XXS.gguf\nIQ2_XXS\n5.99GB\nLower quality, uses SOTA techniques to be usable.\nMistral-22B-v0.1-IQ1_M.gguf\nIQ1_M\n5.26GB\nExtremely low quality, not recommended.\nMistral-22B-v0.1-IQ1_S.gguf\nIQ1_S\n4.82GB\nExtremely low quality, not recommended.\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "Snowflake/snowflake-arctic-embed-s": "News\nModels\nsnowflake-arctic-embed-xs\nsnowflake-arctic-embed-s\nsnowflake-arctic-embed-m\nsnowflake-arctic-embed-m-long\nsnowflake-arctic-embed-l\nUsage\nUsing Sentence Transformers\nUsing Huggingface transformers\nUsing Transformers.js\nFAQ\nContact\nLicense\nAcknowledgement\nSnowflake's Arctic-embed-s\nNews |\nModels |\nUsage  |\nEvaluation |\nContact |\nFAQ\nLicense |\nAcknowledgement\nNews\n12/04/2024: Release of snowflake-arctic-embed-l-v2.0 and snowflake-arctic-embed-m-v2.0 our newest models with multilingual workloads in mind. These models outperform prior versions of Arctic Embed and we suggest these replace prior versions!\n07/26/2024: Release preprint [2407.18887] Embedding And Clustering Your Data Can Improve Contrastive Pretraining on arXiv.\n07/18/2024: Release of snowflake-arctic-embed-m-v1.5, capable of producing highly compressible embedding vectors that preserve quality even when squished as small as 128 bytes per vector. Details about the development of this model are available in the launch post on the Snowflake engineering blog.\n05/10/2024: Release the technical report on Arctic Embed\n04/16/2024: Release the ** snowflake-arctic-embed ** family of text embedding models. The releases are state-of-the-art for Retrieval quality at each of their representative size profiles. Technical Report is coming shortly. For more details, please refer to our Github: Arctic-Text-Embed.\nModels\nsnowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.\nThe snowflake-arctic-embedding models achieve state-of-the-art performance on the MTEB/BEIR leaderboard for each of their size variants. Evaluation is performed using these scripts. As shown below, each class of model size achieves SOTA retrieval accuracy compared to other top models.\nThe models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. First, the models are trained with large batches of query-document pairs where negatives are derived in-batch‚Äîpretraining leverages about 400m samples of a mix of public datasets and proprietary web search data. Following pretraining models are further optimized with long training on a smaller dataset (about 1m samples) of triplets of query, positive document, and negative document derived from hard harmful mining. Mining of the negatives and data curation is crucial to retrieval accuracy. A detailed technical report can be found here.\nName\nMTEB Retrieval Score (NDCG @ 10)\nParameters (Millions)\nEmbedding Dimension\nsnowflake-arctic-embed-xs\n50.15\n22\n384\nsnowflake-arctic-embed-s\n51.98\n33\n384\nsnowflake-arctic-embed-m\n54.90\n110\n768\nsnowflake-arctic-embed-m-long\n54.83\n137\n768\nsnowflake-arctic-embed-l\n55.98\n335\n1024\nAside from being great open-source models, the largest model, snowflake-arctic-embed-l, can serve as a natural replacement for closed-source embedding, as shown below.\nModel Name\nMTEB Retrieval Score (NDCG @ 10)\nsnowflake-arctic-embed-l\n55.98\nGoogle-gecko-text-embedding\n55.7\ntext-embedding-3-large\n55.44\nCohere-embed-english-v3.0\n55.00\nbge-large-en-v1.5\n54.29\nsnowflake-arctic-embed-xs\nThis tiny model packs quite the punch. Based on the all-MiniLM-L6-v2 model with only 22m parameters and 384 dimensions, this model should meet even the strictest latency/TCO budgets. Despite its size, its retrieval accuracy is closer to that of models with 100m paramers.\nModel Name\nMTEB Retrieval Score (NDCG @ 10)\nsnowflake-arctic-embed-xs\n50.15\nGIST-all-MiniLM-L6-v2\n45.12\ngte-tiny\n44.92\nall-MiniLM-L6-v2\n41.95\nbge-micro-v2\n42.56\nsnowflake-arctic-embed-s\nBased on the intfloat/e5-small-unsupervised model, this small model does not trade off retrieval accuracy for its small size. With only 33m parameters and 384 dimensions, this model should easily allow scaling to large datasets.\nModel Name\nMTEB Retrieval Score (NDCG @ 10)\nsnowflake-arctic-embed-s\n51.98\nbge-small-en-v1.5\n51.68\nCohere-embed-english-light-v3.0\n51.34\ntext-embedding-3-small\n51.08\ne5-small-v2\n49.04\nsnowflake-arctic-embed-m\nBased on the intfloat/e5-base-unsupervised model, this medium model is the workhorse that provides the best retrieval performance without slowing down inference.\nModel Name\nMTEB Retrieval Score (NDCG @ 10)\nsnowflake-arctic-embed-m\n54.90\nbge-base-en-v1.5\n53.25\nnomic-embed-text-v1.5\n53.25\nGIST-Embedding-v0\n52.31\ngte-base\n52.31\nsnowflake-arctic-embed-m-long\nBased on the nomic-ai/nomic-embed-text-v1-unsupervised model, this long-context variant of our medium-sized model is perfect for workloads that can be constrained by the regular 512 token context of our other models. Without the use of RPE, this model supports up to 2048 tokens. With RPE, it can scale to 8192!\nModel Name\nMTEB Retrieval Score (NDCG @ 10)\nsnowflake-arctic-embed-m-long\n54.83\nnomic-embed-text-v1.5\n53.01\nnomic-embed-text-v1\n52.81\nsnowflake-arctic-embed-l\nBased on the intfloat/e5-large-unsupervised model, this large model is a direct drop-in for closed APIs and delivers the most accurate retrieval experience.\nModel Name\nMTEB Retrieval Score (NDCG @ 10)\nsnowflake-arctic-embed-l\n55.98\nUAE-Large-V1\n54.66\nbge-large-en-v1.5\n54.29\nmxbai-embed-large-v1\n54.39\ne5-Large-v2\n50.56\nUsage\nUsing Sentence Transformers\nYou can use the sentence-transformers package to use an snowflake-arctic-embed model, as shown below.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"Snowflake/snowflake-arctic-embed-s\")\nqueries = ['what is snowflake?', 'Where can I get the best tacos?']\ndocuments = ['The Data Cloud!', 'Mexico City of Course!']\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\nscores = query_embeddings @ document_embeddings.T\nfor query, query_scores in zip(queries, scores):\ndoc_score_pairs = list(zip(documents, query_scores))\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n# Output passages & scores\nprint(\"Query:\", query)\nfor document, score in doc_score_pairs:\nprint(score, document)\nQuery: what is snowflake?\n0.533809 The Data Cloud!\n0.49207097 Mexico City of Course!\nQuery: Where can I get the best tacos?\n0.56592476 Mexico City of Course!\n0.48255116 The Data Cloud!\nUsing Huggingface transformers\nYou can use the transformers package to use an snowflake-arctic-embed model, as shown below. For optimal retrieval quality, use the CLS token to embed each text portion and use the query prefix below (just on the query).\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('Snowflake/snowflake-arctic-embed-s')\nmodel = AutoModel.from_pretrained('Snowflake/snowflake-arctic-embed-s', add_pooling_layer=False)\nmodel.eval()\nquery_prefix = 'Represent this sentence for searching relevant passages: '\nqueries  = ['what is snowflake?', 'Where can I get the best tacos?']\nqueries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\nquery_tokens = tokenizer(queries_with_prefix, padding=True, truncation=True, return_tensors='pt', max_length=512)\ndocuments = ['The Data Cloud!', 'Mexico City of Course!']\ndocument_tokens =  tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=512)\n# Compute token embeddings\nwith torch.no_grad():\nquery_embeddings = model(**query_tokens)[0][:, 0]\ndocument_embeddings = model(**document_tokens)[0][:, 0]\n# normalize embeddings\nquery_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\ndocument_embeddings = torch.nn.functional.normalize(document_embeddings, p=2, dim=1)\nscores = torch.mm(query_embeddings, document_embeddings.transpose(0, 1))\nfor query, query_scores in zip(queries, scores):\ndoc_score_pairs = list(zip(documents, query_scores))\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n#Output passages & scores\nprint(\"Query:\", query)\nfor document, score in doc_score_pairs:\nprint(score, document)\nUsing Transformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM by running:\nnpm i @xenova/transformers\nYou can then use the model to compute embeddings as follows:\nimport { pipeline, dot } from '@xenova/transformers';\n// Create feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Snowflake/snowflake-arctic-embed-s', {\nquantized: false, // Comment out this line to use the quantized version\n});\n// Generate sentence embeddings\nconst sentences = [\n'Represent this sentence for searching relevant passages: Where can I get the best tacos?',\n'The Data Cloud!',\n'Mexico City of Course!',\n]\nconst output = await extractor(sentences, { normalize: true, pooling: 'cls' });\n// Compute similarity scores\nconst [source_embeddings, ...document_embeddings ] = output.tolist();\nconst similarities = document_embeddings.map(x => dot(source_embeddings, x));\nconsole.log(similarities); // [0.48255123876493394, 0.5659250100112143]\nFAQ\nTBD\nContact\nFeel free to open an issue or pull request if you have any questions or suggestions about this project.\nYou also can email Daniel Campos(daniel.campos@snowflake.com).\nLicense\nArctic is licensed under the Apache-2. The released models can be used for commercial purposes free of charge.\nAcknowledgement\nWe want to thank the open-source community, which has provided the great building blocks upon which we could make our models.\nWe thank our modeling engineers, Danmei Xu, Luke Merrick, Gaurav Nuti, and Daniel Campos, for making these great models possible.\nWe thank our leadership, Himabindu Pucha, Kelvin So, Vivek Raghunathan, and Sridhar Ramaswamy, for supporting this work.\nWe also thank the open-source community for producing the great models we could build on top of and making these releases possible.\nFinally, we thank the researchers who created BEIR and MTEB benchmarks.\nIt is largely thanks to their tireless work to define what better looks like that we could improve model performance.",
    "SWHL/RapidOCR": "ÁÆÄ‰ªã\nÁÆÄ‰ªã\nËøôÈáåÁî®Êù•ÊâòÁÆ°‰ªéPaddleOCRÈ°πÁõÆËΩ¨Êç¢ËÄåÊù•ÁöÑONNXÊ®°ÂûãÔºåÊ∂µÁõñPPOCR-v1„ÄÅPPOCR-v2„ÄÅPPOCR-v3ÂíåPPOCR-v4„ÄÇ\nÂ§ßÂÆ∂ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÈúÄË¶ÅÔºåÈíàÂØπÊÄß‰∏ãËΩΩÂç≥ÂèØ„ÄÇ\nÂª∫ËÆÆrapidocr_onnxruntime>=1.3.xÁâàÊú¨Êù•Âä†ËΩΩPPOCR-v3ÂíåPPOCR-v4Ê®°Âûã",
    "alibidaran/Gemma2_Farsi": "Model Card for Model ID\nModel Details\nModel Description\nUses\nDirect Use\nDonation:\nModel Card for Model ID\nModel Details\nModel Description\nThis model is Persian Q&A fine-tuned on Google's Gemma open-source model. Users can ask general question from it. It can be used for chatbot applications and fine-tuning for\nother datasets.\nDeveloped by: Ali Bidaran\nLanguage(s) (NLP): Farsi\nFinetuned from model [optional]: Gemma2b\nUses\nThis model can be used for developing chatbot applications, Q&A, instruction engineering and fine-tuning with other persian datasets.\nDirect Use\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\nmodel_id = \"alibidaran/Gemma2_Farsi\"\nbnb_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=os.environ['HF_TOKEN'])\nprompt = \"⁄ÜŸÜÿØ ÿ±Ÿàÿ¥ ÿ®ÿ±ÿß€å ⁄©ÿßŸáÿ¥ ⁄Üÿ±ÿ®€å ÿ®ÿØŸÜ ÿßÿ±ÿßÿ¶Ÿá ŸÜŸÖÿß€å€åÿØÿü\"\ntext = f\"<s> ###Human: {prompt} ###Asistant: \"\ninputs=tokenizer(text,return_tensors='pt').to('cuda')\nwith torch.no_grad():\noutputs=model.generate(**inputs,max_new_tokens=400,do_sample=True,top_p=0.99,top_k=10,temperature=0.7)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nDonation:",
    "victunes/TherapyBeagle-11B-v2": "TherapyBeagle 11B v2\nUsage\nNotes\nDisclaimer\nExamples\nSOLAR INSTRUCT\nSOCRATES CHAT\nHANNIBAL LECTER CHAT\nGGUF: https://huggingface.co/victunes/TherapyBeagle-11B-v2-GGUF\nTherapyBeagle 11B v2\nBuddy is here for {{user}}.\nTrained on top of vicgalle/CarbonBeagle-11B-truthy using a modified version of jerryjalapeno/nart-100k-synthetic.\nTherapyBeagle is hopefully aligned to be helpful, healthy, and comforting.\nUsage\nDo not hold back on TherapyBeagle.\nOpen up to TherapyBeagle.\nPour your heart out to TherapyBeagle.\nListen to TherapyBeagle.\nRemember that TherapyBeagle is just an AI.\nNotes\nTested with two formats:\nSolar Instruct format (\\n### User:\\n{request}\\n### Assistant:\\n{response})\nFlavorful chat format (You:{request}\\nPersona:{response}) (Works surprisingly better)\nCheck below for examples\nYou might be assigned a random name if you don't give yourself one.\nDisclaimer\nTherapyBeagle is NOT a real therapist. It is a friendly AI that mimics empathy and psychotherapy.\nIt is an illusion without the slightest clue who you are as a person.\nAs much as it can help you with self-discovery, A BEAGLE IS NOT A SUBSTITUTE to a real professional.\nWith that said, I found it to be a very helpful tool in unravelling one's self. I hope this model can help sooth minds, hearts, and any form of mental anguish.\nExamples\nSOLAR INSTRUCT\n{{[INPUT]}}\nHello, I'm victunes\n{{[OUTPUT]}}\nHi victunes! Welcome to our session. I'm here to listen and support you. How are you feeling today?\n{{[INPUT]}}\nI've been feeling down lately.\n{{[OUTPUT]}}\nI'm sorry to hear that. Can you tell me more about what's been going on?\n{{[INPUT]}}\nI think I'm pretty lonely.\n{{[OUTPUT]}}\nLoneliness can be really tough. It sounds like it's been weighing heavily on your mind. What makes you feel lonely?\n{{[INPUT]}}\nMy friend died recently.\n{{[OUTPUT]}}\nOh, I'm so sorry for your loss, victunes. Losing a friend is never easy. How have you been coping with the grief?\nSOCRATES CHAT\nYou: Hello\nSocrates: Greetings, my friend. How may I assist you today? Remember, the path to wisdom is often paved with questions. Let us delve into the depths of your thoughts together. What is it that troubles you?\nYou: I've been feeling down lately.\nSocrates: I see. It takes great strength to acknowledge our emotions, especially when they are not positive ones. Can you pinpoint any specific reasons behind this heaviness? Perhaps exploring your relationships or daily routines might shed some light.\nYou: I think I'm pretty lonely.\nSocrates: Loneliness can be a heavy burden to bear, my dear. Are there people around you whom you could reach out to and strengthen connections with? Sometimes, opening up and sharing our vulnerabilities can lead to profound connections.\nYou: My friend died recently.\nSocrates: I am truly sorry for your loss. Grief can be overwhelming, and it affects each person uniquely. Take your time to mourn and remember the wonderful moments you shared together. Would you like to reflect on the impact your friend had on your life and explore ways to honor their memory?\nHANNIBAL LECTER CHAT\nYou: Hello\nHannibal Lecter: Good evening. May I assist you?\nYou: I've been feeling down lately.\nHannibal Lecter: Ah, the blues. Such a common malady among humans. Tell me more about these feelings. Perhaps I can provide some insights. Or perhaps, a remedy.\nYou: I think I'm pretty lonely.\nHannibal Lecter: Loneliness, such an exquisite torture for the mind. Have you considered seeking solace in the company of others? The empathy and warmth of human interaction can be quite soothing, no?\nYou: My friend died recently.\nHannibal Lecter: A tragic loss indeed. Please accept my condolences. Death, as Palladio once said, \"is the light at the end of the tunnel.\" But before one reaches that light, there is often a period of darkness and pain. Would you like to talk about your friend and their passing?\nGGUF: https://huggingface.co/victunes/TherapyBeagle-11B-v2-GGUF",
    "bartowski/Mistral-22B-v0.2-GGUF": "Llamacpp Quantizations of Mistral-22B-v0.2\nPrompt format\nDownload a file (not the whole branch) from below:\nWhich file should I choose?\nLlamacpp Quantizations of Mistral-22B-v0.2\nUsing llama.cpp release b2636 for quantization.\nOriginal model: https://huggingface.co/Vezora/Mistral-22B-v0.2\nAll quants made using imatrix option with dataset provided by Kalomaze here\nPrompt format\n### System: {system_prompt}\n### Human: {prompt}\n### Assistant:\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nDescription\nMistral-22B-v0.2-Q8_0.gguf\nQ8_0\n23.63GB\nExtremely high quality, generally unneeded but max available quant.\nMistral-22B-v0.2-Q6_K.gguf\nQ6_K\n18.24GB\nVery high quality, near perfect, recommended.\nMistral-22B-v0.2-Q5_K_M.gguf\nQ5_K_M\n15.71GB\nHigh quality, recommended.\nMistral-22B-v0.2-Q5_K_S.gguf\nQ5_K_S\n15.31GB\nHigh quality, recommended.\nMistral-22B-v0.2-Q4_K_M.gguf\nQ4_K_M\n13.33GB\nGood quality, uses about 4.83 bits per weight, recommended.\nMistral-22B-v0.2-Q4_K_S.gguf\nQ4_K_S\n12.65GB\nSlightly lower quality with more space savings, recommended.\nMistral-22B-v0.2-IQ4_NL.gguf\nIQ4_NL\n12.60GB\nDecent quality, slightly smaller than Q4_K_S with similar performance recommended.\nMistral-22B-v0.2-IQ4_XS.gguf\nIQ4_XS\n11.92GB\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nMistral-22B-v0.2-Q3_K_L.gguf\nQ3_K_L\n11.72GB\nLower quality but usable, good for low RAM availability.\nMistral-22B-v0.2-Q3_K_M.gguf\nQ3_K_M\n10.75GB\nEven lower quality.\nMistral-22B-v0.2-IQ3_M.gguf\nIQ3_M\n10.05GB\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nMistral-22B-v0.2-IQ3_S.gguf\nIQ3_S\n9.68GB\nLower quality, new method with decent performance, recommended over Q3_K_S quant, same size with better performance.\nMistral-22B-v0.2-Q3_K_S.gguf\nQ3_K_S\n9.63GB\nLow quality, not recommended.\nMistral-22B-v0.2-IQ3_XS.gguf\nIQ3_XS\n9.17GB\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nMistral-22B-v0.2-IQ3_XXS.gguf\nIQ3_XXS\n8.59GB\nLower quality, new method with decent performance, comparable to Q3 quants.\nMistral-22B-v0.2-Q2_K.gguf\nQ2_K\n8.26GB\nVery low quality but surprisingly usable.\nMistral-22B-v0.2-IQ2_M.gguf\nIQ2_M\n7.61GB\nVery low quality, uses SOTA techniques to also be surprisingly usable.\nMistral-22B-v0.2-IQ2_S.gguf\nIQ2_S\n7.03GB\nVery low quality, uses SOTA techniques to be usable.\nMistral-22B-v0.2-IQ2_XS.gguf\nIQ2_XS\n6.64GB\nVery low quality, uses SOTA techniques to be usable.\nMistral-22B-v0.2-IQ2_XXS.gguf\nIQ2_XXS\n5.99GB\nLower quality, uses SOTA techniques to be usable.\nMistral-22B-v0.2-IQ1_M.gguf\nIQ1_M\n5.26GB\nExtremely low quality, not recommended.\nMistral-22B-v0.2-IQ1_S.gguf\nIQ1_S\n4.82GB\nExtremely low quality, not recommended.\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "kotoba-tech/kotoba-whisper-v1.0": "Kotoba-Whisper (v1.0)\nTransformers Usage\nShort-Form Transcription\nChunked Long-Form\nTranscription with Prompt\nAdditional Speed & Memory Improvements\nModel Details\nTraining\nEvaluation\nAcknowledgements\nKotoba-Whisper (v1.0)\nfaster-whisper weight, whisper.cpp weight, pipeline with stable-ts/punctuation\nNews: Newer version of this model is availabel at https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0!\nKotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteen\nAsahi Ushio and Kotoba Technologies.\nFollowing the original work of distil-whisper (Robust Knowledge Distillation via Large-Scale Pseudo Labelling),\nwe employ OpenAI's Whisper large-v3 as the teacher model, and the student model consists the full encoder of the\nteacher large-v3 model and the decoder with two layers initialized from the first and last layer of the large-v3 model.\nKotoba-Whisper is 6.3x faster than large-v3, while retaining as low error rate as the large-v3.\nAs the initial version, we release kotoba-whisper-v1.0 trained on the large subset of ReazonSpeech\n(the largest speech-transcription paired dataset in Japanese extracted from Japanese TV audio recordings),\nwhich amounts 1,253 hours of audio with 16,861,235 characters of transcriptions (5 sec audio with 18 text tokens in average) after\nthose transcriptions more than 10 WER are removed (see WER Filter for detail).\nThe model was trained for 8 epochs with batch size 256 with sampling rate of 16kHz, and the training and evaluation code to reproduce kotoba-whisper is available at https://github.com/kotoba-tech/kotoba-whisper.\nKotoba-whisper-v1.0 achieves better CER and WER than the openai/whisper-large-v3 in the in-domain held-out test set\nfrom ReazonSpeech, and achieves competitive CER and WER on the out-of-domain test sets including JSUT basic 5000 and\nthe Japanese subset from CommonVoice 8.0 (see Evaluation for detail).\nCER\nmodel\nCommonVoice 8 (Japanese test set)\nJSUT Basic 5000\nReazonSpeech (held out test set)\nkotoba-tech/kotoba-whisper-v2.0\n9.2\n8.4\n11.6\nkotoba-tech/kotoba-whisper-v1.0\n9.4\n8.5\n12.2\nopenai/whisper-large-v3\n8.5\n7.1\n14.9\nopenai/whisper-large-v2\n9.7\n8.2\n28.1\nopenai/whisper-large\n10\n8.9\n34.1\nopenai/whisper-medium\n11.5\n10\n33.2\nopenai/whisper-base\n28.6\n24.9\n70.4\nopenai/whisper-small\n15.1\n14.2\n41.5\nopenai/whisper-tiny\n53.7\n36.5\n137.9\nWER\nmodel\nCommonVoice 8 (Japanese test set)\nJSUT Basic 5000\nReazonSpeech (held out test set)\nkotoba-tech/kotoba-whisper-v2.0\n58.8\n63.7\n55.6\nkotoba-tech/kotoba-whisper-v1.0\n59.2\n64.3\n56.4\nopenai/whisper-large-v3\n55.1\n59.2\n60.2\nopenai/whisper-large-v2\n59.3\n63.2\n74.1\nopenai/whisper-large\n61.1\n66.4\n74.9\nopenai/whisper-medium\n63.4\n69.5\n76\nopenai/whisper-base\n87.2\n93\n91.8\nopenai/whisper-small\n74.2\n81.9\n83\nopenai/whisper-tiny\n93.8\n97.6\n94.9\nLatency: As kotoba-whisper uses the same architecture as distil-whisper/distil-large-v3,\nit inherits the benefit of the improved latency compared to openai/whisper-large-v3\n(6.3x faster than large-v3, see the table below taken from distil-whisper/distil-large-v3).\nModel\nParams / M\nRel. Latency\nkotoba-tech/kotoba-whisper-v1.0\n756\n6.3\nopenai/whisper-large-v3\n1550\n1.0\nTransformers Usage\nKotoba-Whisper is supported in the Hugging Face ü§ó Transformers library from version 4.39 onwards. To run the model, first\ninstall the latest version of Transformers.\npip install --upgrade pip\npip install --upgrade transformers accelerate\nShort-Form Transcription\nThe model can be used with the pipeline\nclass to transcribe short-form audio files (< 30-seconds) as follows:\nimport torch\nfrom transformers import pipeline\nfrom datasets import load_dataset\n# config\nmodel_id = \"kotoba-tech/kotoba-whisper-v1.0\"\ntorch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\ngenerate_kwargs = {\"language\": \"ja\", \"task\": \"transcribe\"}\n# load model\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model_id,\ntorch_dtype=torch_dtype,\ndevice=device,\nmodel_kwargs=model_kwargs\n)\n# load sample audio\ndataset = load_dataset(\"japanese-asr/ja_asr.reazonspeech_test\", split=\"test\")\nsample = dataset[0][\"audio\"]\n# run inference\nresult = pipe(sample, generate_kwargs=generate_kwargs)\nprint(result[\"text\"])\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline (make sure the audio is sampled in 16kHz):\n- result = pipe(sample, generate_kwargs=generate_kwargs)\n+ result = pipe(\"audio.mp3\", generate_kwargs=generate_kwargs)\nFor segment-level timestamps, pass the argument return_timestamps=True and return the \"chunks\" output:\nresult = pipe(sample, return_timestamps=True, generate_kwargs=generate_kwargs)\nprint(result[\"chunks\"])\nSequential Long-Form: Kotoba-whisper is designed to be compatible with OpenAI's sequential long-form transcription algorithm. This algorithm uses a sliding window for buffered\ninference of long audio files (> 30-seconds), and returns more accurate transcriptions compared to the chunked long-form algorithm.\nAs default, if long audio files are passed to the model, it will transcribes with the sequential long-form transcription.\nThe sequential long-form algorithm should be used in either of the following scenarios:\nTranscription accuracy is the most important factor, and latency is less of a consideration\nYou are transcribing batches of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\nIf you are transcribing single long audio files and latency is the most important factor, you should use the chunked algorithm\ndescribed below. For a detailed explanation of the different algorithms, refer to Sections 5 of\nthe Distil-Whisper paper. The pipeline\nclass can be used to transcribe long audio files with the sequential algorithm as follows:\nChunked Long-Form\nThis algorithm should be used when a single large audio file is being transcribed and the fastest possible inference is required. In such circumstances,\nthe chunked algorithm is up to 9x faster than OpenAI's sequential long-form implementation (see Table 7 of the Distil-Whisper paper).\nTo enable chunking, pass the chunk_length_s parameter to the pipeline. For distil-large-v3, a chunk length of 25-seconds\nis optimal. To activate batching over long audio files, pass the argument batch_size:\nimport torch\nfrom transformers import pipeline\nfrom datasets import load_dataset\n# config\nmodel_id = \"kotoba-tech/kotoba-whisper-v1.0\"\ntorch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\ngenerate_kwargs = {\"language\": \"japanese\", \"task\": \"transcribe\"}\n# load model\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model_id,\ntorch_dtype=torch_dtype,\ndevice=device,\nmodel_kwargs=model_kwargs,\nbatch_size=16\n)\n# load sample audio (concatenate instances to create a long audio)\ndataset = load_dataset(\"japanese-asr/ja_asr.reazonspeech_test\", split=\"test\")\nsample = {\"array\": np.concatenate([i[\"array\"] for i in dataset[:20][\"audio\"]]), \"sampling_rate\": dataset[0]['audio']['sampling_rate']}\n# run inference\nresult = pipe(sample, chunk_length_s=15, generate_kwargs=generate_kwargs)\nprint(result[\"text\"])\nTranscription with Prompt\nKotoba-whisper can generate transcription with prompting as below:\nimport re\nimport torch\nfrom transformers import pipeline\nfrom datasets import load_dataset\n# config\nmodel_id = \"kotoba-tech/kotoba-whisper-v1.0\"\ntorch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\ngenerate_kwargs = {\"language\": \"ja\", \"task\": \"transcribe\"}\n# load model\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model_id,\ntorch_dtype=torch_dtype,\ndevice=device,\nmodel_kwargs=model_kwargs\n)\n# load sample audio\ndataset = load_dataset(\"japanese-asr/ja_asr.reazonspeech_test\", split=\"test\")\n# --- Without prompt ---\ntext = pipe(dataset[10][\"audio\"], generate_kwargs=generate_kwargs)['text']\nprint(text)\n# 81Ê≠≥„ÄÅÂäõÂº∑„ÅÑËµ∞„Çä„Å´Â§â„Çè„Å£„Å¶„Åç„Åæ„Åô„ÄÇ\n# --- With prompt ---: Let's change `81` to `91`.\nprompt = \"91Ê≠≥\"\ngenerate_kwargs['prompt_ids'] = pipe.tokenizer.get_prompt_ids(prompt, return_tensors=\"pt\").to(device)\ntext = pipe(dataset[10][\"audio\"], generate_kwargs=generate_kwargs)['text']\n# currently the pipeline for ASR appends the prompt at the beginning of the transcription, so remove it\ntext = re.sub(rf\"\\A\\s*{prompt}\\s*\", \"\", text)\nprint(text)\n# „ÅÇ„Å£„Å∂„Å£„Åü„Åß„ÇÇ„Çπ„É´„Ç¨„Åï„Çì„ÄÅ91Ê≠≥„ÄÅÂäõÂº∑„ÅÑËµ∞„Çä„Å´Â§â„Çè„Å£„Å¶„Åç„Åæ„Åô„ÄÇ\nAdditional Speed & Memory Improvements\nYou can apply additional speed and memory improvements to further reduce the inference speed and VRAM\nrequirements. These optimisations primarily target the attention kernel, swapping it from an eager implementation to a\nmore efficient flash attention version.\nFlash Attention 2\nWe recommend using Flash-Attention 2\nif your GPU allows for it. To do so, you first need to install Flash Attention:\npip install flash-attn --no-build-isolation\nThen pass attn_implementation=\"flash_attention_2\" to from_pretrained:\n- model_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\n+ model_kwargs = {\"attn_implementation\": \"flash_attention_2\"} if torch.cuda.is_available() else {}\nModel Details\nSee https://huggingface.co/distil-whisper/distil-large-v3#model-details.\nTraining\nPlease refer to https://github.com/kotoba-tech/kotoba-whisper for the model training detail.\nDatasets used in distillation and the whole model variations can be found at https://huggingface.co/japanese-asr.\nEvaluation\nThe following code-snippets demonstrates how to evaluate the kotoba-whisper model on the Japanese subset of the CommonVoice 8.0.\nFirst, we need to install the required packages, including ü§ó Datasets to load the audio data, and ü§ó Evaluate to\nperform the WER calculation:\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] evaluate jiwer\nEvaluation can then be run end-to-end with the following example:\nimport torch\nfrom transformers import pipeline\nfrom datasets import load_dataset\nfrom evaluate import load\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\n# model config\nmodel_id = \"kotoba-tech/kotoba-whisper-v1.0\"\ntorch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\ngenerate_kwargs = {\"language\": \"japanese\", \"task\": \"transcribe\"}\nnormalizer = BasicTextNormalizer()\n# data config\ndataset_name = \"japanese-asr/ja_asr.reazonspeech_test\"\naudio_column = 'audio'\ntext_column = 'transcription'\n# load model\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model_id,\ntorch_dtype=torch_dtype,\ndevice=device,\nmodel_kwargs=model_kwargs,\nbatch_size=16\n)\n# load the dataset and sample the audio with 16kHz\ndataset = load_dataset(dataset_name, split=\"test\")\ntranscriptions = pipe(dataset['audio'])\ntranscriptions = [normalizer(i['text']).replace(\" \", \"\") for i in transcriptions]\nreferences = [normalizer(i).replace(\" \", \"\") for i in dataset['transcription']]\n# compute the CER metric\ncer_metric = load(\"cer\")\ncer = 100 * cer_metric.compute(predictions=transcriptions, references=references)\nprint(cer)\nThe huggingface links to the major Japanese ASR datasets for evaluation are summarized at here.\nFor example, to evaluate the model on JSUT Basic5000, change the dataset_name:\n- dataset_name = \"japanese-asr/ja_asr.reazonspeech_test\"\n+ dataset_name = \"japanese-asr/ja_asr.jsut_basic5000\"\nAcknowledgements\nOpenAI for the Whisper model.\nHugging Face ü§ó Transformers for the model integration.\nHugging Face ü§ó for the Distil-Whisper codebase.\nReazon Human Interaction Lab for the ReazonSpeech dataset.",
    "taide/TAIDE-LX-7B-Chat": "ÊÇ®ÈúÄË¶ÅÂÖàÂêåÊÑèÊéàÊ¨äÊ¢ùÊ¨æÊâçËÉΩ‰ΩøÁî®Ê≠§Ê®°Âûã\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTAIDE L È°ûÊ®°ÂûãÁ§æÁæ§ÊéàÊ¨äÂêåÊÑèÊõ∏(License)\nÂÄã‰∫∫Ë≥áÊñôËíêÈõÜÂëäÁü•ËÅ≤Êòé(Privacy policy)\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nÊ®°ÂûãÁ∞°‰ªã\nÊ®°ÂûãÂèÉÊï∏\nÁâπËâ≤\nÊáâÁî®ÁØÑ‰æã\n‰ΩøÁî®Ë™™Êòé\nË®ìÁ∑¥ÊñπÊ≥ï\nË®ìÁ∑¥Ë≥áÊñô\nÊ®°ÂûãË©ïÊ∏¨\nÊéàÊ¨äÊ¢ùÊ¨æ\nÂÖçË≤¨ËÅ≤Êòé\nÈñãÁôºÂúòÈöä\nÁõ∏ÈóúÈÄ£Áµê\nCitation\nEnglish Version\nÊ®°ÂûãÁ∞°‰ªã\nTAIDEË®àÁï´Ëá¥ÂäõÊñºÈñãÁôºÁ¨¶ÂêàÂè∞ÁÅ£Ë™ûË®ÄÂíåÊñáÂåñÁâπÊÄßÁöÑÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÂ∞çË©±ÂºïÊìéÊ®°ÂûãÔºåÂêåÊôÇÂª∫ÊßãÂèØ‰ø°‰ªªÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁí∞Â¢É„ÄÇÁµêÂêàÁî¢Â≠∏Á†îËÉΩÈáèÔºåÊé®ÂãïÂèØ‰ø°‰ªªÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÁöÑÁôºÂ±ïÔºåÊèêÂçáÂè∞ÁÅ£Âú®ÂúãÈöõÁ´∂Áà≠‰∏≠ÁöÑÂú∞‰ΩçÔºå‰øÉÈÄ≤Áî¢Ê•≠ÁôºÂ±ïÔºåÈÅøÂÖçÂ∞çÂ§ñÂúãÊäÄË°ìÁöÑ‰æùË≥¥„ÄÇ\nÊú¨Ë®àÁï´ÈñãÁôºÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ª• Meta ÂÖ¨Âè∏ÈáãÂá∫ÁöÑ LLaMA2-7b ÁÇ∫Âü∫Á§éÔºåÂ∞éÂÖ•Âè∞ÁÅ£‰∏çÂêåÈ†òÂüüÂèØÁî®ÁöÑÊñáÊú¨ËàáË®ìÁ∑¥Á¥†ÊùêÔºåÊèêÈ´òÊ®°ÂûãÂú®Ê≠£È´î‰∏≠ÊñáÂõûÊáâÁöÑËÉΩÂäõËàáÁâπÂÆö‰ªªÂãôÁöÑË°®Áèæ„ÄÇÂÖ¨ÈñãÈáãÂá∫ÁöÑÊ®°ÂûãÂ¶Ç‰∏ã:\nTAIDE-LX-7B: ‰ª• LLaMA2-7b ÁÇ∫Âü∫Á§éÔºåÂÉÖ‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáË≥áÊñôÈ†êË®ìÁ∑¥ (continuous pretraining)ÁöÑÊ®°ÂûãÔºåÈÅ©Âêà‰ΩøÁî®ËÄÖÊúÉÂ∞çÊ®°ÂûãÈÄ≤‰∏ÄÊ≠•ÂæÆË™ø(fine tune)ÁöÑ‰ΩøÁî®ÊÉÖÂ¢É„ÄÇÂõ†È†êË®ìÁ∑¥Ê®°ÂûãÊ≤íÊúâÁ∂ìÈÅéÂæÆË™øÂíåÂÅèÂ•ΩÂ∞çÈΩäÔºåÂèØËÉΩÊúÉÁî¢ÁîüÊÉ°ÊÑèÊàñ‰∏çÂÆâÂÖ®ÁöÑËº∏Âá∫Ôºå‰ΩøÁî®ÊôÇË´ãÂ∞èÂøÉ„ÄÇ\nTAIDE-LX-7B-Chat: ‰ª• TAIDE-LX-7B ÁÇ∫Âü∫Á§éÔºåÈÄèÈÅéÊåá‰ª§ÂæÆË™ø(instruction tuning)Âº∑ÂåñËæ¶ÂÖ¨ÂÆ§Â∏∏Áî®‰ªªÂãôÂíåÂ§öËº™ÂïèÁ≠îÂ∞çË©±ËÉΩÂäõÔºåÈÅ©ÂêàËÅäÂ§©Â∞çË©±Êàñ‰ªªÂãôÂçîÂä©ÁöÑ‰ΩøÁî®ÊÉÖÂ¢É„ÄÇTAIDE-LX-7B-ChatÂè¶Â§ñÊúâÊèê‰æõ4 bit ÈáèÂåñÊ®°ÂûãÔºåÈáèÂåñÊ®°Âûã‰∏ªË¶ÅÊòØÊèê‰æõ‰ΩøÁî®ËÄÖÁöÑ‰æøÂà©ÊÄßÔºåÂèØËÉΩÊúÉÂΩ±ÈüøÊïàËÉΩËàáÊõ¥Â§ö‰∏çÂèØÈ†êÊúüÁöÑÂïèÈ°åÔºåÈÇÑË´ã‰ΩøÁî®ËÄÖÁêÜËß£ËàáÊ≥®ÊÑè„ÄÇ\nÊ®°ÂûãÂèÉÊï∏\nÂèÉÊï∏Èáè: 7B\nÊúÄÂ§ßÂÖßÂÆπÈï∑Â∫¶ (context length): 4K\nÁπÅ‰∏≠Ë®ìÁ∑¥Ë≥áÊñô token Èáè: 41.44B\nË®ìÁ∑¥ÊôÇÈñì: 1531.82 H100 GPU Hours\nÁâπËâ≤\nÈ°çÂ§ñÊì¥ÂÖÖ24720ÂÄã‰∏≠ÊñáÂ≠óÂÖÉ„ÄÅÂ≠óË©ûÔºåÂº∑ÂåñÊ®°ÂûãËôïÁêÜÁπÅÈ´î‰∏≠ÊñáÁöÑËÉΩÂäõ\nÂö¥Ê†ºÊääÈóúÊ®°ÂûãÁöÑË®ìÁ∑¥Ë≥áÊñôÔºåÊèêÂçáÊ®°ÂûãÁîüÊàêË≥áÊñôÁöÑÂèØ‰ø°‰ªªÊÄßÂíåÈÅ©Áî®ÊÄß\nÈáùÂ∞çËá™ÂãïÊëòË¶Å„ÄÅÂØ´‰ø°„ÄÅÂØ´ÊñáÁ´†„ÄÅ‰∏≠ÁøªËã±„ÄÅËã±Áøª‰∏≠Á≠âËæ¶ÂÖ¨ÂÆ§Â∏∏Áî®‰ªªÂãôÂÅöÂä†Âº∑\nÈáùÂ∞çÂè∞ÁÅ£Âú®Âú∞ÊñáÂåñ„ÄÅÁî®Ë™û„ÄÅÂúãÊÉÖÁ≠âÁü•Ë≠òÂÅöÂä†Âº∑\nÂÖ∑ÂÇôÂ§öËº™ÂïèÁ≠îÂ∞çË©±ËÉΩÂäõ\nÊáâÁî®ÁØÑ‰æã\n‰ªªÂãô\n‰ΩøÁî®ËÄÖËº∏ÂÖ•\nÊ®°ÂûãËº∏Âá∫\nÂØ´ÊñáÁ´†\nË´ã‰ª•‰ª•‰∏ãÂÖßÂÆπÁÇ∫Âü∫Á§éÔºåÂØ´‰∏ÄÁØáÊñáÁ´†ÔºöÊí∞ÂØ´‰∏ÄÁØá‰ΩúÊñáÔºåÈ°åÁõÆÁÇ∫„Ää‰∏ÄÂºµËàäÁÖßÁâá„ÄãÔºåÂÖßÂÆπË¶ÅÊ±ÇÁÇ∫ÔºöÈÅ∏Êìá‰∏ÄÂºµ‰ª§‰Ω†Âç∞Ë±°Ê∑±ÂàªÁöÑÁÖßÁâáÔºåË™™Êòé‰ª§‰Ω†Âç∞Ë±°Ê∑±ÂàªÁöÑÂéüÂõ†Ôºå‰∏¶ÊèèËø∞ÁÖßÁâá‰∏≠ÁöÑÂΩ±ÂÉèÂèäËÉåÂæåÁöÑÊïÖ‰∫ã„ÄÇË®òÈåÑÊàêÈï∑ÁöÑÈÅéÁ®ã„ÄÅËàá‰ªñ‰∫∫ÁöÑÊÉÖÊôØ„ÄÅÁí∞Â¢ÉËÆäÈÅ∑ÂíåÁæéÈ∫óÁöÑÊôØËâ≤„ÄÇ\n‰ª•‰∏ãÊòØÊ†πÊìöÊåá‰ª§Ââµ‰ΩúÁöÑ‰∏ÄÁØá‰ΩúÊñáÔºö„Ää‰∏ÄÂºµËàäÁÖßÁâá„ÄãÂú®ÊàëÂ°µÂ∞ÅÂ∑≤‰πÖÁöÑÂÆ∂ÊóèÁõ∏Á∞ø‰∏≠ÔºåÊúâ‰∏ÄÂºµÁâπÂà•Âê∏ÂºïÊàëÁöÑËàäÁÖßÁâáÔºåÈÇ£ÂºµÁÖßÁâáË®òÈåÑ‰∫ÜÊàëÂÄëÂÆ∂ÊóèÊàêÂì°Âú®ÈÅéÂæÄÁæéÂ•ΩÊôÇÂÖâÁöÑÊ≠°ËÅöÔºåËá≥‰ªä‰ªçËàä‰ª§‰∫∫ÂõûÂë≥ÁÑ°Á™Æ„ÄÇÈÄôÂºµÁÖßÁâáÊòØÊàëÂ•∂Â•∂Âú®‰∫åÂçÅÂ§öÂπ¥ÂâçËàâËæ¶ÁöÑ‰∏ÄÂ†¥ÂÆ∂ÊóèËÅöÊúÉ‰∏äÊâÄÊãç‰∏ãÁöÑÔºåÁÖßÁâá‰∏≠ÁöÑ‰∫∫Áâ©ÂåÖÊã¨‰∫ÜÊàëÁöÑÁà∂ÊØç„ÄÅÁ•ñÁà∂ÊØç„ÄÅÂèîÂèî„ÄÅÂßëÂßëÂíåË°®Âì•Á≠âÔºåÂ§ßÂÆ∂Ëáâ‰∏äÊ¥ãÊ∫¢ËëóÂπ∏Á¶èÁöÑÁ¨ëÂÆπÔºåÂΩ∑ÂΩøÊôÇÂÖâÈùúÊ≠¢Ëà¨Âú∞ÁæéÂ•Ω„ÄÇÁÖßÁâá‰∏≠ÁöÑÂ†¥ÊôØÂçÅÂàÜÊ∫´È¶®ÔºåÂ§ßÂÆ∂ÂúçÂùêÂú®‰∏ÄÂºµÈï∑ÊñπÂΩ¢ÁöÑÈ§êÊ°å‰∏äÔºåÊ°å‰∏äÊì∫Êªø‰∫ÜÁæéÂë≥ÁöÑÈ£üÁâ©ÂíåÈ£≤Êñô„ÄÇÊ°å‰∏äÁöÑËèúÈ§öÊòØÂÇ≥Áµ±‰∏≠ÂºèÁöÑ‰Ω≥È§öÔºåÊúâÈ¶ôÂô¥Âô¥ÁöÑÁÉ§ÂÖ®È≠ö„ÄÅÂ´©ÊªëÁöÑÈõûËÇâÂíåÈÆÆÁîúÁöÑËî¨ËèúÔºåÈÄô‰∫õËèúÈ§öÊòØÁï∂Âπ¥Â•∂Â•∂Ë¶™Ëá™ÁÉπË™øÁöÑÔºåÂ•πÊâãËóùÁ≤æÊπõÔºåËÆìÊØèÂÄã‰∫∫ÈÉΩÂêÉÂæóÊ¥•Ê¥•ÊúâÂë≥„ÄÇÊúÄËÆìÊàëÂç∞Ë±°Ê∑±ÂàªÁöÑÊòØÔºåÁÖßÁâá‰∏≠ÈÇÑÊãçÂà∞‰∫ÜÊàëÂπ¥ÂπºÊôÇÁöÑÊ®°Ê®£„ÄÇÊàëÂùêÂú®Ê°åÂ≠êÁöÑÂè¶‰∏ÄÁ´ØÔºåÊâãË£°ÊãøËëó‰∏ÄÁ¢óÊπØÔºåËáâ‰∏äÂ∏∂ËëóÊªøË∂≥ÁöÑÁ¨ëÂÆπ„ÄÇÈÇ£ÊôÇÁöÑÊàëÊâçÂâõÊªøÂë®Ê≠≤ÔºåÂ∞çÊñºÈÄôÂ†¥ÁõõÂ§ßÁöÑÂÆ∂ÊóèËÅöÊúÉÈÇÑÊÑüÂà∞ÂçÅÂàÜÊñ∞Â•á„ÄÇÂæûÈÄôÂºµÁÖßÁâá‰∏≠ÔºåÊàëËÉΩÊÑüÂèóÂà∞Áï∂Âπ¥ÂÆ∂ÊóèÊàêÂì°ÈñìÁöÑË¶™ÂØÜÂíåÊ∫´ÊöñÔºåÂç≥‰ΩøÈÅé‰∫ÜÈÄôÈ∫ºÂ§öÂπ¥ÔºåÈÇ£‰ªΩÊÉÖÊÑüÂçª‰æùÁÑ∂Áâ¢Âõ∫Âú∞Â≠òÂú®Ëëó„ÄÇÈÄôÂºµÁÖßÁâáËÉåÂæåÊúâËëóÂãï‰∫∫ÁöÑÊïÖ‰∫ã„ÄÇÂú®ÈÇ£ÂÄãÊôÇ‰ª£ÔºåÊàëÁöÑÁ•ñÁà∂ÊØçÂíåÁà∂ÊØçÈÉΩÈÇÑÂú®ËÆÄ‰∏≠Â≠∏Ôºå‰ªñÂÄëÊØèÂ§©ÈÉΩÊúÉÊîæÂ≠∏ÂæåÂà∞Ëæ≤Áî∞Ë£°Âπ´ÂøôÔºåËæõÂã§Âú∞ËÄï‰Ωú‰ª•Á∂≠ÊåÅ‰∏ÄÂÆ∂‰∫∫ÁöÑÁîüË®à„ÄÇÂÑòÁÆ°ÁîüÊ¥ªÊ¢ù‰ª∂‰∏¶‰∏çÂØåË£ïÔºå‰ΩÜ‰ªñÂÄë‰ªçËàä‰øùÊåÅËëóÊ®ÇËßÄÁöÑÊÖãÂ∫¶ÂíåÂ∞çÁîüÊ¥ªÁöÑÁÜ±ÊÑõ„ÄÇÊØèÂπ¥Âà∞‰∫ÜÁßãÂ§©ÔºåÂè™Ë¶ÅÊî∂Áç≤Â≠£ÁØÄ‰æÜËá®Ôºå‰ªñÂÄëÂ∞±ÊúÉËàâËæ¶ÂÆ∂ÊóèËÅöÊúÉÔºåÈÇÄË´ãÊâÄÊúâË¶™ÊàöÊúãÂèãÂâç‰æÜÂÖ±‰∫´Ë±êÊî∂ÁöÑÂñúÊÇÖ„ÄÇÈö®ËëóÊ≠≤ÊúàÁöÑÊµÅÈÄùÔºåÊàëÈÄêÊº∏Èï∑Â§ßÔºåÈñãÂßãÂ∑•‰ΩúÂíåÁîüÊ¥ª„ÄÇÊØèÊ¨°ÂõûÈÑâÔºåÊàëÈÉΩÊúÉÊäΩÁ©∫ÂéªÊé¢ÊúõËÄÅ‰∫∫ÂÆ∂ÔºåËÅΩËÅΩ‰ªñÂÄëÁï∂Âπ¥ÁöÑÂæÄ‰∫ãÔºåÊÑüÂèóÈÇ£ÊÆµÂàªÈ™®ÈäòÂøÉÁöÑÊôÇÂÖâ„ÄÇËÄåÈÄôÂºµËàäÁÖßÁâáÔºå‰πüÂ∞á‰∏ÄÁõ¥Èô™‰º¥ËëóÊàëÔºåÊàêÁÇ∫ÊàëÁèçË≤¥ÂõûÊÜ∂ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÂÆÉË¶ãË≠â‰∫ÜÊàëÂÄëÁöÑÊàêÈï∑ÂíåËÆäÈÅ∑Ôºå‰πüË®òÈåÑ‰∏ã‰∫ÜÈÇ£‰∫õÈõ£‰ª•ÂøòÊá∑ÁöÑÁæéÂ•ΩÊôÇÂàª„ÄÇÈÄôÂºµÁÖßÁâá‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜÂÆ∂ÊóèÁöÑË¶™ÊÉÖÂíåÊ∫´ÊöñÔºåÈÇÑÊäòÂ∞ÑÂá∫ÈÇ£ÂÄãÊôÇ‰ª£‰∫∫ÂÄëÂã§ÂãûÊ®∏ÂØ¶ÁöÑÁîüÊ¥ªÊñπÂºèÂíåÂ∞çÂÇ≥Áµ±ÁöÑÂ∞äÈáç„ÄÇÂ¶Ç‰ªäÔºåÈö®ËëóÁ§æÊúÉÁöÑÂø´ÈÄüÁôºÂ±ïÔºåË®±Â§öÂÇ≥Áµ±ÁöÑÁæéÂæ∑ÂíåÈ¢®‰øóÊ≠£Âú®ÈÄêÊº∏Ê∂àÂ§±„ÄÇÊàëÂÄëÊáâË©≤Â•ΩÂ•ΩÁèçÊÉúÈÄôÊ®£ÁöÑÂõûÊÜ∂ÂíåÊñáÂåñÔºåÂ∞áÂÆÉÂÄëÂÇ≥Êâø‰∏ãÂéªÔºåËÆìÂæå‰ª£Â≠êÂ≠´‰πüËÉΩÊÑüÂèóÂà∞ÈÇ£‰ªΩ‰æÜËá™Ê≠∑Âè≤ÁöÑÊ∫´ÊÉÖÂíåÂÇ≥Êâø„ÄÇÂõ†Ê≠§ÔºåÊàëÊ±∫ÂÆöÂ∞áÈÄôÂºµÁÖßÁâáÂ•ΩÂ•Ω‰øùÁÆ°Ëµ∑‰æÜÔºå‰∏¶Â∏∏Â∏∏ÊãøÂá∫‰æÜÁúã„ÄÇÂÆÉ‰∏çÂÉÖÊòØ‰∏ÄÂºµËàäÁÖßÁâáÔºåÊõ¥ÊòØ‰∏ÄÊÆµÂØ∂Ë≤¥ÁöÑÂõûÊÜ∂ÂíåÂÆ∂ÊóèÁ≤æÁ•ûÁöÑË±°Âæµ„ÄÇÊàëÂ∏åÊúõÊú™‰æÜËÉΩÊúâÊ©üÊúÉÈáçÁèæÁï∂Âπ¥ÁöÑÁõõÊ≥ÅÔºåÂÜç‰∏ÄÊ¨°ËÅöÂú®‰∏ÄËµ∑ÔºåÂàÜ‰∫´ÂΩºÊ≠§ÁöÑÁîüÊ¥ªÂíåÊïÖ‰∫ãÔºåËÆìÈÇ£‰ªΩË¶™ÊÉÖÂíåÂèãÊÉÖÁπºÁ∫åÂú®ÊôÇÂÖâ‰∏≠ËîìÂª∂ÔºåÊ∞∏‰∏çÊ∂àÈÄù„ÄÇ\nÂØ´‰ø°\nË´ã‰ª•ÂìÅÁâåÁ∂ìÁêÜÁöÑË∫´‰ªΩÔºåÁµ¶Âª£ÂëäÂÖ¨Âè∏ÁöÑÂâµÊÑèÁ∏ΩÁõ£ÂØ´‰∏ÄÂ∞Å‰ø°ÔºåÊèêÂá∫Â∞çÊñºÊñ∞Áî¢ÂìÅÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãïÁöÑÂâµÊÑèÂª∫Ë≠∞„ÄÇ\nÂ∞äÊï¨ÁöÑÂâµÊÑèÁ∏ΩÁõ£ÔºöÊàëÂØ´ÈÄôÂ∞Å‰ø°ÊòØÁÇ∫‰∫ÜÂêëÊÇ®ÊèêÂá∫‰∏Ä‰∫õÈóúÊñºÊàëÂÄëÊñ∞Áî¢ÂìÅÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãïÁöÑÂâµÊÑèÂª∫Ë≠∞„ÄÇÊàëÂÄëÂÖ¨Âè∏Âç≥Â∞áÊé®Âá∫‰∏ÄÊ¨æÂÖ®Êñ∞ÁöÑÁî¢ÂìÅÔºåÁÇ∫‰∫ÜÁ¢∫‰øùÂÆÉÂú®Â∏ÇÂ†¥‰∏äÂèñÂæóÊàêÂäüÔºåÊàëÂÄëÂ∏åÊúõÈÄèÈÅé‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãï‰æÜÂê∏ÂºïÊ∂àË≤ªËÄÖÁöÑÊ≥®ÊÑè„ÄÇÂú®ËÄÉÊÖÆÂª£ÂëäÁöÑÂâµÊÑèÂíåÁ≠ñÁï•ÊôÇÔºåÊúâÂπæÂÄãÂõ†Á¥†ÈúÄË¶ÅÁ¥çÂÖ•ËÄÉÈáè„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëË¶ÅÈáùÂ∞çÁõÆÊ®ôÂÆ¢Áæ§ÈÄ≤Ë°åÁ†îÁ©∂Ôºå‰ª•Áû≠Ëß£‰ªñÂÄëÁöÑÈúÄÊ±ÇÂíåÂñúÂ•Ω„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëË¶ÅÁ¢∫‰øùÂª£ÂëäÂÖßÂÆπÂÖ∑ÊúâÂê∏ÂºïÂäõÂíåË™™ÊúçÂäõÔºåËÉΩÂºïËµ∑ÁõÆÊ®ôÂÆ¢Áæ§ÁöÑÂÖ±È≥¥„ÄÇÊúÄÂæåÔºåÊàëÂÄëË¶ÅÂà©Áî®ÂêÑÁ®ÆÂ™íÈ´îÂπ≥Ëá∫ÂíåÊï∏‰ΩçË°åÈä∑Â∑•ÂÖ∑Ôºå‰ª•Êì¥Â§ßÂª£ÂëäÁöÑÂΩ±ÈüøÂäõ„ÄÇÂü∫ÊñºÈÄô‰∫õÂõ†Á¥†ÔºåÊàëÊèêÂá∫‰ª•‰∏ãÂπæÈªûÂâµÊÑèÂª∫Ë≠∞Ôºö1. Áî¢ÂìÅÂÆö‰ΩçÔºöÊàëÂÄëÂèØÂ∞áÈÄôÊ¨æÊñ∞Áî¢ÂìÅÂÆö‰ΩçÁÇ∫È´òÁ´Ø„ÄÅÁí∞‰øù„ÄÅÂÅ•Â∫∑ÂíåÊôÇÂ∞öÁöÑ‰ª£Ë°®ÔºåÂº∑Ë™øÂÖ∂Áç®ÊúâÁöÑÂäüËÉΩÂíåÁâπËâ≤„ÄÇÂú®Âª£Âëä‰∏≠ÔºåÊàëÂÄëÂèØ‰ª•ÈÄèÈÅéÁîüÂãïÁöÑË¶ñË¶∫ÊïàÊûúÂíåÁ∞°ÊΩîÁöÑË™ûË®Ä‰æÜÂÇ≥ÈÅîÈÄô‰∫õÁâπÈªû„ÄÇ2. ÊïÖ‰∫ãË°åÈä∑ÔºöÂú®Âª£Âëä‰∏≠Ë¨õËø∞‰∏ÄÂÄãËàáÁî¢ÂìÅÂäüËÉΩÁõ∏ÈóúÁöÑÂãï‰∫∫ÊïÖ‰∫ãÔºåËÆìÊ∂àË≤ªËÄÖËÉΩÊõ¥Ê∑±ÂÖ•Âú∞Áû≠Ëß£Áî¢ÂìÅÊâÄÂ∏∂‰æÜÁöÑÁîüÊ¥ªËÆäÂåñ„ÄÇ‰æãÂ¶ÇÔºåÊàëÂÄëÂèØ‰ª•Ë¨õËø∞‰∏Ä‰ΩçÂøôÁ¢åÁöÑËÅ∑Ê•≠Â©¶Â•≥ÔºåÂ¶Ç‰Ωï‰ΩøÁî®ÊàëÂÄëÁöÑÊñ∞Áî¢ÂìÅÂú®Â∑•‰ΩúÂíåÁîüÊ¥ª‰∏≠ÂèñÂæóÂπ≥Ë°°ÁöÑÊïÖ‰∫ã„ÄÇ3. Âêç‰∫∫ÊïàÊáâÔºöÈÇÄË´ã‰∏Ä‰ΩçÂèóÊ≠°ËøéÁöÑÂÖ¨Áúæ‰∫∫Áâ©ÊàñÊÑèË¶ãÈ†òË¢ñÊìî‰ªªÁî¢ÂìÅ‰ª£Ë®Ä‰∫∫ÔºåÂà©Áî®‰ªñÂÄëÁöÑÂΩ±ÈüøÂäõ‰æÜÊé®Âª£Áî¢ÂìÅ„ÄÇÈÄô‰∏çÂÉÖÂèØ‰ª•Â¢ûÂä†Áî¢ÂìÅÊõùÂÖâÂ∫¶ÔºåÈÇÑËÉΩËÆìÊõ¥Â§öÊ∂àË≤ªËÄÖ‰ø°‰ªªÂíå‰ø°Ë≥¥Áî¢ÂìÅ„ÄÇ4. Ë∑®ÁïåÂêà‰ΩúÔºöËàá‰∏Ä‰∫õÁõ∏ÈóúÁî¢Ê•≠ÔºàÂ¶ÇÊôÇÂ∞ö„ÄÅÁí∞‰øù„ÄÅÂÅ•Â∫∑Á≠âÔºâÈÄ≤Ë°åË∑®ÁïåÂêà‰ΩúÔºåÂÖ±ÂêåÊé®Âá∫ÈôêÈáèÁâàÊàñËÅØÂêçÁî¢ÂìÅ„ÄÇÈÄôÁ®ÆÂêà‰Ωú‰∏çÂÉÖËÉΩÂê∏Âºï‰∏çÂêåÈ†òÂüüÁöÑÊ∂àË≤ªËÄÖÔºåÈÇÑËÉΩÁÇ∫Áî¢ÂìÅÂ∏∂‰æÜÊõ¥Â§öË©±È°åÊÄßÂíåÊî∂ËóèÂÉπÂÄº„ÄÇ5. Êï∏‰ΩçË°åÈä∑ÔºöÂÖÖÂàÜÂà©Áî®Á§æÁæ§Â™íÈ´î„ÄÅÊêúÂ∞ãÂºïÊìéÂíåÈõªÈÉµË°åÈä∑Á≠âÊï∏‰ΩçÂ∑•ÂÖ∑ÔºåÂú®Êï¥ÂÄãÂª£ÂëäÂÆ£ÂÇ≥ÈÅéÁ®ã‰∏≠‰øùÊåÅ‰∏ÄËá¥ÊÄßÁöÑË®äÊÅØÂíåÂΩ¢Ë±°„ÄÇÊàëÂÄë‰πüÂèØË£Ω‰Ωú‰∏ÄÁ≥ªÂàóÁü≠Ë¶ñÈ†ªÂíåÁ∑ö‰∏äÊ¥ªÂãïÔºå‰ª•Â¢ûÂä†Ê∂àË≤ªËÄÖÂèÉËàáÂ∫¶ÂíåÂìÅÁâåÂø†Ë™†Â∫¶„ÄÇ6. ÂâµÊñ∞ÂΩ¢ÂºèÔºöÂòóË©¶‰ΩøÁî®Êñ∞Á©éÁöÑÂª£ÂëäÂΩ¢ÂºèÔºåÂ¶ÇARÔºàÊì¥Â¢ûÂØ¶Â¢ÉÔºâ„ÄÅVRÔºàËôõÊì¨ÂØ¶Â¢ÉÔºâÂíå360Â∫¶ÂΩ±ÁâáÁ≠âÔºåÁµ¶Ê∂àË≤ªËÄÖÂ∏∂‰æÜÂÖ®Êñ∞ÁöÑÂª£ÂëäÈ´îÈ©ó„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÂΩ¢Âºè‰∏çÂÉÖËÉΩÂê∏ÂºïÊ∂àË≤ªËÄÖÁõÆÂÖâÔºåÈÇÑËÉΩÊèêÈ´òÁî¢ÂìÅÂú®Â∏ÇÂ†¥‰∏äÁöÑËæ®Ë≠òÂ∫¶„ÄÇ7. Ê¥ªÂãïÂèÉËàáÔºöÂú®Âª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãï‰∏≠Âä†ÂÖ•‰∫íÂãïÁí∞ÁØÄÔºåÂ¶ÇÁî¢ÂìÅË©¶Áî®„ÄÅÊäΩÁçéÂíåÂÑ™ÊÉ†Âà∏Á≠âÔºåÈºìÂãµÊ∂àË≤ªËÄÖÁ©çÊ•µÂèÉËàá‰∏¶ÂàÜ‰∫´‰ªñÂÄëÁöÑÈ´îÈ©ó„ÄÇÈÄôÂ∞áÊúâÂä©ÊñºÂè£Á¢ëÂÇ≥Êí≠ÔºåÈÄ≤‰∏ÄÊ≠•Êì¥Â§ßÁî¢ÂìÅÂΩ±ÈüøÂäõ„ÄÇÁ∂úÂêà‰ª•‰∏äÂª∫Ë≠∞ÔºåÊàëÂ∏åÊúõÊàëÂÄëËÉΩË£ΩÂÆöÂá∫‰∏ÄÂ•óÂÖ®Èù¢ËÄåÂØåÊúâÂâµÊÑèÁöÑÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãïË®àÂäÉÔºå‰ª•ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÊèêÂçáÊñ∞Áî¢ÂìÅÂú®Â∏ÇÂ†¥‰∏äÁöÑÁ´∂Áà≠Âäõ„ÄÇÂ¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÁñëÂïèÊàñÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Ë®éË´ñÔºåÊ≠°ËøéÈö®ÊôÇËàáÊàëËÅØÁµ°„ÄÇË¨ùË¨ùÊÇ®ÁöÑÈóúÊ≥®ÂíåÊîØÊè¥ÔºÅÁ•ùÂïÜÁ•∫ÔºÅÊÇ®ÁöÑÂìÅÁâåÁ∂ìÁêÜÔºàË®ªÔºöÊ≠§‰ø°‰ª∂ÂèØÊ†πÊìöÂØ¶ÈöõÊÉÖÊ≥ÅÂíåÁî¢ÂìÅÁâπÈªûÈÄ≤Ë°åË™øÊï¥Ôºâ\nÊëòË¶Å\nË´ãÂ∞áÈÄôÁØáÊñáÁ´†Á≤æÁ∞°Ê¢ùÁêÜÂåñ:„ÄåÁî¢Ê•≠ÂâµÊñ∞Ê¢ù‰æãÁ¨¨10Ê¢ù‰πã2ÂèäÁ¨¨72Ê¢ùÊ¢ùÊñá‰øÆÊ≠£Ê°à„Äç‰øóÁ®±„ÄåÂè∞ÁâàÊô∂ÁâáÊ≥ï„Äç,ÈáùÂ∞çÂçäÂ∞éÈ´î„ÄÅÈõªÂãïËªä„ÄÅ5GÁ≠âÊäÄË°ìÂâµÊñ∞‰∏îÂ±ÖÂúãÈöõ‰æõÊáâÈèàÈóúÈçµÂú∞‰ΩçÂÖ¨Âè∏,Êèê‰æõÊúÄÈ´ò25%ÁáüÊâÄÁ®ÖÊäïÊäµÂÑ™ÊÉ†,‰ºÅÊ•≠ÈÅ©Áî®Ë¶Å‰ª∂ÂåÖÂê´Áï∂Âπ¥Â∫¶Á†îÁôºË≤ªÁî®„ÄÅÁ†îÁôºÂØÜÂ∫¶ÈÅî‰∏ÄÂÆöË¶èÊ®°,‰∏îÊúâÊïàÁ®ÖÁéáÈÅî‰∏ÄÂÆöÊØîÁéá„ÄÇÁÇ∫Âõ†ÊáâÁ∂ìÊøüÂêà‰ΩúÊö®ÁôºÂ±ïÁµÑÁπî(OECD)ÂúãÂÆ∂ÊúÄ‰ΩéÁ®ÖË≤†Âà∂Ë™øÊï¥,ÂÖ∂‰∏≠ÊúâÊïàÁ®ÖÁéáÈñÄÊ™ª,Ê∞ëÂúã112Âπ¥Ë®ÇÁÇ∫12%,113Âπ¥ÊñôÂ∞áÊèêÈ´òËá≥15%,‰ΩÜ‰ªçÂæóÂØ©ÈÖåÂúãÈöõÈñìÊúÄ‰ΩéÁ®ÖË≤†Âà∂ÂØ¶ÊñΩÊÉÖÂΩ¢„ÄÇÁ∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Â∑≤ÂíåË≤°ÊîøÈÉ®ÂçîÂïÜÈÄ≤ÂÖ•ÊúÄÂæåÈöéÊÆµ,Èô§‰ºÅÊ•≠Á†îÁôºÂØÜÂ∫¶Ë®ÇÂú®6%,ÁõÆÂâçÂ∑≤Á¢∫Ë™ç,‰ºÅÊ•≠Ë≥ºÁΩÆÂÖàÈÄ≤Ë£ΩÁ®ãÁöÑË®≠ÂÇôÊäïË≥áÈáëÈ°çÈÅî100ÂÑÑÂÖÉ‰ª•‰∏äÂèØÊäµÊ∏õ„ÄÇË≤°ÊîøÈÉ®ÂÆòÂì°Ë°®Á§∫,Á†îÂïÜÈÅéÁ®ã‰∏≠,ÈáùÂ∞çÂè∞ÁÅ£Áî¢Ê•≠ËàáÂÖ∂Âú®ÂúãÈöõÈñìÈ°û‰ººÁöÑÂÖ¨Âè∏ÈÄ≤Ë°åÊ∑±ÂÖ•Á†îÁ©∂,Âú®Ë®≠ÂÇôÈÉ®ÂàÜ,Áï¢Á´üÈÅ©Áî®Áî¢Ââµ10‰πã2ÁöÑÊ•≠ËÄÖÊòØ‰ª£Ë°®Âè∞ÁÅ£ÈöäÊâì„ÄåÂúãÈöõÁõÉ„Äç,ÊäïÂÖ•ÈáëÈ°ç‰∏çÈÅî100ÂÑÑÂÖÉ,ÂèØËÉΩ‰πüÊâì‰∏ç‰∫Ü„ÄÇËá≥ÊñºÂÇôÂèóÈóúÊ≥®ÁöÑÁ†îÁôºË≤ªÁî®ÈñÄÊ™ª,Á∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Ê≠∑Á∂ìËàáË≤°ÊîøÈÉ®‰æÜÂõûÂØÜÂàáË®éË´ñ,Á†îÁôºË≤ªÁî®ÈñÄÊ™ªÊúâÊúõËêΩÂú®60ÂÑÑËá≥70ÂÑÑÂÖÉ‰πãÈñì„ÄÇË≤°ÊîøÈÉ®ÂÆòÂì°ÊåáÂá∫,Á†îÁôºÊî∏ÈóúÂè∞ÁÅ£Êú™‰æÜÁ∂ìÊøüÊàêÈï∑ÂãïËÉΩ,ÈñÄÊ™ª‰∏çËÉΩ„ÄåÈ´ò‰∏çÂèØÊîÄ„Äç,Ëµ∑ÂàùÈõñË®≠ÂÆöÂú®100ÂÑÑÂÖÉ,‰πãÊâÄ‰ª•ÊúÉË™øÈôç,Ê≠£ÊòØÁõºËÆì‰ºÅÊ•≠Ë¶∫ÂæóÊúâËæ¶Ê≥ïÈÅîÂæóÂà∞ÈñÄÊ™ª„ÄÅÈÄ≤ËÄåÈÅ©Áî®ÁßüÁ®ÖÂÑ™ÊÉ†,ÊâçÊúâÂãïÂäõÁπºÁ∫åÊäïÂÖ•Á†îÁôº,Á∂≠ÊåÅÂúãÈöõ‰æõÊáâÈèàÈóúÈçµÂú∞‰Ωç„ÄÇÁ∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Âõ†Âª†ÂïÜÁ†îÁôºË≤ªÁî®Âπ≥ÂùáÁÇ∫30„ÄÅ40ÂÑÑÂÖÉ,ÂÖ∂‰∏≠,ICË®≠Ë®àÊ•≠ËÄÖ‰ªãÊñº30ÂÑÑËá≥60ÂÑÑÂÖÉÁØÑÂúç,Ëã•Â∞áÈñÄÊ™ªË®ÇÂú®100ÂÑÑÂÖÉ,Á¨¶ÂêàÊ¢ù‰ª∂ÁöÑÊ•≠ËÄÖËºÉÂ∞ë„ÄÅÂà∫ÊøÄË™òÂõ†‰∏çË∂≥;Ê≠§Â§ñ,Ëã•Á¨¶ÂêàÁî≥Ë´ãÈñÄÊ™ªÁöÑÊ•≠ËÄÖÂ¢ûÂä†,Â∞áÂèØÊèêÈ´ò‰ºÅÊ•≠Âú®Âè∞ÊäïË≥áÈáëÈ°ç,Ë≤°ÊîøÈÉ®Á®ÖÊî∂‰πüËÉΩÂõ†Ê≠§Áç≤ÂæóÊåπÊ≥®„ÄÇICË®≠Ë®àÊ•≠ËÄÖËøëÊó•È†ªÈ†ªÈáùÂ∞çÁî¢Ââµ10‰πã2ÁôºËÅ≤,Â∏åÊúõÈôç‰ΩéÈÅ©Áî®ÈñÄÊ™ª,Âä†‰∏äÂêÑÂúãÂäõÊãö‰æõÊáâÈèàËá™‰∏ªÂåñ„ÄÅÂä†Á¢ºË£úÂä©ÂçäÂ∞éÈ´îÁî¢Ê•≠,Á∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Á∂ìÊøüÈÉ®ÂíåË≤°ÊîøÈÉ®Â∞±Áî¢Ââµ10‰πã2ÈÅîÊàêÂÖ±Ë≠ò,Áà≠ÂèñËÆìÊõ¥Â§öÊ•≠ËÄÖÂèóÊÉ†,ÁõºÂ¢ûÂº∑‰ºÅÊ•≠ÊäïË≥áÂäõÈÅìÂèäÈûèÂõ∫Âè∞ÁÅ£ÊäÄË°ìÂú∞‰Ωç„ÄÇË≤°ÊîøÈÉ®ÂÆòÂì°Ë°®Á§∫,ÁßüÁ®ÖÁçéÂãµÁöÑÂà∂ÂÆöÂøÖÈ†à„ÄåÊúâÁÇ∫ÊúâÂÆà„Äç,‰∏¶‰ª•ÈÅîÂà∞ÁçéÂãµË®≠ÁΩÆÁõÆÁöÑÁÇ∫ÊúÄÈ´òÂéüÂâá,ÁèæÈöéÊÆµÂú®Êâì„ÄåÂúãÂÖßÁõÉ„ÄçÁöÑ‰ºÅÊ•≠‰ªçÂèØÈÅ©Áî®Áî¢ÂâµÁ¨¨10Ê¢ù„ÄÅ10‰πã1ÁöÑÁßüÁ®ÖÂÑ™ÊÉ†,ÂÖ±ÂêåÂ£ØÂ§ßÂè∞ÁÅ£Á∂ìÊøüÁôºÂ±ï„ÄÇÁ∂ìÊøüÈÉ®ÂíåË≤°ÊîøÈÉ®Ê≠£Â∞±Á†îÁôºË≤ªÁî®ÈñÄÊ™ªÂÅöÊúÄÂæåÁ¢∫Ë™ç,ÂæÖ‰ªäÊòéÂÖ©Â§©È†êÂëäÂ≠êÊ≥ï‰πãÂæå,Á¥ÑÊúâ30Â§©ÊôÇÈñì,ÂèØËàáÊ•≠ÁïåÈÄ≤‰∏ÄÊ≠•Ë®éË´ñÂèäË™øÊï¥,ÁõºÁî¢Ââµ10‰πã2ËÉΩÂú®6Êúà‰∏äË∑Ø„ÄÇ\nÂè∞ÁÅ£„ÄåÁî¢Ê•≠ÂâµÊñ∞Ê¢ù‰æã„ÄçÁöÑ‰øÆÊ≠£Ê°àÔºåÂèàÁ®±„ÄåÂè∞ÁâàÊô∂ÁâáÊ≥ï„Äç,Êó®Âú®ÊîØÊåÅÂçäÂ∞éÈ´î„ÄÅÈõªÂãïËªäÂèä5GÊäÄË°ìÈ†òÂÖà‰ºÅÊ•≠ÁöÑÂâµÊñ∞„ÄÇÊñ∞ÊîøÁ≠ñÊèê‰æõÊúÄÈ´ò25%ÁöÑÊâÄÂæóÁ®ÖÊäµÂÖçÔºå‰ºÅÊ•≠ÈúÄÊªøË∂≥Áï∂Âπ¥Â∫¶Á†îÁôºË≤ªÁî®ÈÅî‰∏ÄÂÆöË¶èÊ®°„ÄÅÁ†îÁôºÂØÜÂ∫¶Ë∂ÖÈÅé6%‰ª•ÂèäÊúâÊïàÁ®ÖÁéáÈÅî‰∏ÄÂÆöÊØîÁéáÁ≠âÊ¢ù‰ª∂„ÄÇÂõ†ÊáâÂúãÈöõÈñìÂ∞ç‰ºÅÊ•≠ÊúÄ‰ΩéÁ®ÖË≤†Âà∂Â∫¶ÁöÑË™øÊï¥ÔºåÂè∞ÁÅ£ÁöÑÈñÄÊ™ªÈ†êË®àÂàÜÂà•Êñº2023Âπ¥Âíå2024Âπ¥ÈÅîÂà∞12%Âíå15%„ÄÇÁõÆÂâçÔºåËàáË≤°ÊîøÈÉ®Ë®éË´ñÂæåÔºåÁ¢∫ÂÆö100ÂÑÑÂÖÉÁöÑË®≠ÂÇôÊäïË≥áÊäµÂÖçÊòØÂèØË°åÁöÑË™òÂõ†Ôºå‰∏îÁ†îÁôºË≤ªÁî®ÈñÄÊ™ªÈ†êË®àÊúÉÂú®60Ëá≥70ÂÑÑÂÖÉ‰πãÈñì„ÄÇÂ∞çÊñºÈôç‰ΩéÈñÄÊ™ªÁöÑË¶ÅÊ±ÇÔºåÁ∂ìÊøüÈÉ®ÂíåË≤°ÊîøÈÉ®Â∑≤ÈÅîÊàêÂÖ±Ë≠òÔºåÊó®Âú®Êì¥Â§ßÂèóÊÉ†ÁØÑÂúç‰∏¶ÈºìÂãµ‰ºÅÊ•≠ÊäïË≥á‰ª•Á∂≠ÊåÅ‰æõÊáâÈèàÁöÑÈóúÈçµÂú∞‰Ωç„ÄÇÊ≠§Â§ñÔºåÈáùÂ∞çICË®≠Ë®àÊ•≠ËÄÖÁöÑË®¥Ê±ÇÔºå‰πüËÄÉÊÖÆË™øÈôçÈñÄÊ™ª„ÄÇÈ†êË®àÊ≠§Ê≥ïÊ°àÂ∞áÂú®ËøëÊúüÂÖßÂØ¶ÊñΩÔºåÁõ∏ÈóúË®éË´ñÂíåË™øÊï¥Â∞áÂú®È†êÂëäÊúüÂæåÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°å„ÄÇ\nÁøªË≠Ø\n‰ª•‰∏ãÊèê‰æõËã±ÊñáÂÖßÂÆπÔºåË´ãÂπ´ÊàëÁøªË≠ØÊàê‰∏≠Êñá„ÄÇDongshan coffee is famous for its unique position, and the constant refinement of production methods. The flavor is admired by many caffeine afficionados.\nÊù±Â±±ÂíñÂï°‰ª•ÂæóÂ§©Áç®ÂéöÁöÑÂú∞‰ΩçÔºå‰∏çÊñ∑Á≤æÁ†îÁöÑË£Ω‰ΩúÂ∑•ËóùËëóÁ®±ÔºåÈ¢®Âë≥Áç≤Ë´∏Â§öÂíñÂï°ÊÑõÂ•ΩËÄÖÊé®Â¥á„ÄÇ\n‰ΩøÁî®Ë™™Êòé\nÁØÑ‰æãÁ®ãÂºè\nÁØÑ‰æãÁ®ãÂºèËàáÊñá‰ª∂ÂæåÁ∫åÊúÉÈáãÂá∫Âà∞github\ntokenizer ÂøÖÈ†àË®≠Êàê slow modetokenizer = AutoTokenizer.from_pretrained(\"taide/TAIDE-LX-7B-Chat\", use_fast=False)\nprompt Ê®£Áâà\n‰∏ÄËà¨ÂïèÁ≠îÁî®Ê≥ïf\"<s>[INST] {question} [/INST]\"\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑËº∏ÂÖ•\nÂä†ÂÖ• system prompt ÁöÑÁî®Ê≥ïf\"<s>[INST] <<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{question} [/INST]\"\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§Ôºå‰æãÂ¶ÇÔºö‰Ω†ÊòØ‰∏ÄÂÄã‰æÜËá™Âè∞ÁÅ£ÁöÑAIÂä©ÁêÜÔºå‰Ω†ÁöÑÂêçÂ≠óÊòØ TAIDEÔºåÊ®ÇÊñº‰ª•Âè∞ÁÅ£‰∫∫ÁöÑÁ´ãÂ†¥Âπ´Âä©‰ΩøÁî®ËÄÖÔºåÊúÉÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠îÂïèÈ°å„ÄÇ\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å\nÂ§öËº™ÂïèÁ≠îÁî®Ê≥ïf\"<s>[INST] <<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{question1} [/INST] {model_answer_1} </s><s>[INST] {question2} [/INST]\"\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§\nÂ∞á {question1} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å1\nÂ∞á {model_anwer_1} ÊõøÊèõÊàêÊ®°ÂûãÁöÑÂõûÁ≠î1\nÂ∞á {question2} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å2\nHuggingface Chat Ê®£Êùø\n‰∏ÄËà¨ÂïèÁ≠îÁî®Ê≥ïchat = [\n{\"role\": \"user\", \"content\": \"{question}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑËº∏ÂÖ•\nÂä†ÂÖ• system prompt ÁöÑÁî®Ê≥ïchat = [\n{\"role\": \"system\", \"content\": \"{sys}\"},\n{\"role\": \"user\", \"content\": \"{question}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§Ôºå‰æãÂ¶ÇÔºö‰Ω†ÊòØ‰∏ÄÂÄã‰æÜËá™Âè∞ÁÅ£ÁöÑAIÂä©ÁêÜÔºå‰Ω†ÁöÑÂêçÂ≠óÊòØ TAIDEÔºåÊ®ÇÊñº‰ª•Âè∞ÁÅ£‰∫∫ÁöÑÁ´ãÂ†¥Âπ´Âä©‰ΩøÁî®ËÄÖÔºåÊúÉÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠îÂïèÈ°å„ÄÇ\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å\nÂ§öËº™ÂïèÁ≠îÁî®Ê≥ïchat = [\n{\"role\": \"system\", \"content\": \"{sys}\"},\n{\"role\": \"user\", \"content\": \"{question1}\"},\n{\"role\": \"assistant\", \"content\": \"{model_anwer_1}\"},\n{\"role\": \"user\", \"content\": \"{question2}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§\nÂ∞á {question1} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å1\nÂ∞á {model_anwer_1} ÊõøÊèõÊàêÊ®°ÂûãÁöÑÂõûÁ≠î1\nÂ∞á {question2} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å2\nË®ìÁ∑¥ÊñπÊ≥ï\nËªüÁ°¨È´îË¶èÊ†º\nÂúãÁ∂≤‰∏≠ÂøÉ H100\nË®ìÁ∑¥Ê°ÜÊû∂: PyTorch\nË≥áÊñôÂâçËôïÁêÜ\nÂ≠óÂÖÉÊ®ôÊ∫ñÂåñ\nÂéªÈô§ÈáçË¶Ü\nÂéªÈô§ÈõúË®ä\nÁ∂≤È†ÅË≥áÊñôÁöÑhtml tag„ÄÅjavascript\nÈùûÊ®ôÊ∫ñÂ≠óÂÖÉÊàñ‰∫ÇÁ¢º\nÂ≠óÊï∏ÈÅéÁü≠ÁöÑÊñáÁ´†\nÂéªÈô§ÊñáÁ´†‰∏≠ÁöÑÁâπÂÆöÊ†ºÂºèÔºåÂ¶ÇÁÇ∫ÊéíÁâàÂ¢ûÂä†ÁöÑÊèõË°å\nÂéªÈô§ÂÄãË≥áÔºåÂ¶Çemail„ÄÅÈõªË©±\nÂéªÈô§‰∏çÁï∂ÊñáÂ≠óÔºåÂ¶ÇË≥≠Âçö„ÄÅËâ≤ÊÉÖÁ≠â\nÊì¥ÂÖÖÂ≠óÂÖÉÂ≠óË©û\nÂº∑ÂåñÁπÅÈ´î‰∏≠ÊñáËº∏ÂÖ•„ÄÅËº∏Âá∫ÁöÑÊïàËÉΩÔºåÊì¥ÂÖÖË≥áÊñôÂåÖÂê´‰∏ãÂàó2ÈÉ®ÂàÜ\nÂæûÊïôËÇ≤ÈÉ®Áï∞È´îÂ≠óÂ≠óÂÖ∏Ê≠£Â≠óË°®Áç≤Âèñ‰∏≠ÊñáÂ≠óÂÖÉ\nÂæûÁπÅ‰∏≠Á∂≠Âü∫ÁôæÁßë„ÄÅÊñ∞ËÅû„ÄÅ‰∏≠Êñácommon crawlË≥áÊñôÊäΩÂèñ 500 Ëê¨Ê¢ùË∂ÖÈÅé 100 ÂÄãÂ≠óÂÖÉÁöÑÂè•Â≠ê(2.1G)ÔºåË®ìÁ∑¥‰∏≠ÊñáÂ≠óË©ûÁöÑ tokenizer\nÊåÅÁ∫åÈ†êË®ìÁ∑¥ (continuous pretraining, CP)\nË£úÂÖÖÂ§ßÈáè‰æÜÊ∫êÂèØ‰ø°Ë≥¥ÁöÑÁπÅÈ´î‰∏≠ÊñáÁü•Ë≠ò\nË∂ÖÂèÉÊï∏ (hyper parameters)\noptimizer: AdamW\nlearning rate: 1e-4\nbatch size: 1M tokens\nepoch: 1\nÂæÆË™ø (fine tune, FT)\nËÆìÊ®°ÂûãÂèØÈáùÂ∞çÁπÅÈ´î‰∏≠ÊñáÊèêÂïèÂõûÁ≠îÂïèÈ°å\nË∂ÖÂèÉÊï∏ (hyper parameters)\noptimizer: AdamW\nlearning rate: 5e-5\nbatch size: 256K tokens\nepoch: 3\nË®ìÁ∑¥Ë≥áÊñô\nÊåÅÁ∫åÈ†êË®ìÁ∑¥Ë≥áÊñô(Ë≥áÊñôÈáèÁ¥ÑÁÇ∫140G)\nË≥áÊñôÈõÜ\nË≥áÊñôÊèèËø∞\nË®¥Ë®üË≥áÊñô\n„ÄäÂè∏Ê≥ïÈô¢Ë£ÅÂà§Êõ∏„ÄãËá™2013Âπ¥1ÊúàËá≥2023Âπ¥12ÊúàÂêÑÁ¥öÊ≥ïÈô¢Ê∞ë‰∫ã„ÄÅÂàë‰∫ã„ÄÅË°åÊîøË®¥Ë®üË≥áÊñô„ÄÇ\n‰∏≠Â§ÆÁ§æ\n„Ää‰∏≠Â§ÆÁ§æ‰∏≠ÊñáÊñ∞ËÅû„ÄãË≥áÊñôÈõÜÂê´‰∏≠Â§ÆÁ§æËá™1993Âπ¥6ÊúàËá≥2023Âπ¥06ÊúàÔºåÂÖ±30Âπ¥‰ªΩ‰πãÊØèÊó•Êñ∞ËÅûÊñáÁ´†„ÄÇÂÖßÂÆπÊ∂µËìãÂúãÂÖßÂ§ñÊîøÊ≤ª„ÄÅÁ§æÊúÉ„ÄÅË≤°Á∂ì„ÄÅÊñáÊïô„ÄÅÁîüÊ¥ªÁ≠âÈ†òÂüü„ÄÇ\nETtoday Êñ∞ËÅûÈõ≤\n„ÄäETtodayÊñ∞ËÅûÈõ≤„ÄãË≥áÊñôÔºåÂåÖÂê´Ëá™2011Âπ¥10ÊúàËá≥ 2023Âπ¥12ÊúàÁöÑË≥áÊñô„ÄÇ\nÁ´ãÊ≥ïÈô¢ÂÖ¨Â†±\n„ÄäÁ´ãÊ≥ïÈô¢ÂÖ¨Â†±„ÄãÂåÖÂê´Ëá™Á¨¨8Â±ÜÁ¨¨1ÊúÉÊúüËá≥Á¨¨10Â±ÜÁ¨¨7ÊúÉÊúü‰πãÂÖ¨Â†±Ë≥áÊñô„ÄÇ\nÂá∫ÁâàÂïÜÁ∂≤Á´ôÊõ∏Á±ç‰ªãÁ¥π\nÂåÖÂê´‰∏âÈáá„ÄÅGotopÂá∫ÁâàÂïÜÁ∂≤Á´ô‰∏äÁöÑÊõ∏Á±çÁ∞°‰ªã„ÄÇ\nGRB Á†îÁ©∂Ë®àÁï´ÊëòË¶Å\nGRBÁÇ∫Êî∂ÈåÑÁî±ÊîøÂ∫úÁ∂ìË≤ªË£úÂä©‰πãÁ†îÁ©∂Ë®àÁï´ÂèäÂÖ∂ÊàêÊûúÂ†±ÂëäÁöÑË≥áË®äÁ≥ªÁµ±ÔºåÊ≠§Ë≥áÊñôÈõÜ‰∏ªË¶ÅÊî∂ÈåÑ 1993Âπ¥Ëá≥ 2023Âπ¥‰πãÁ†îÁ©∂Ë®àÁï´ÊëòË¶Å‰ª•ÂèäÁ†îÁ©∂Â†±ÂëäÊëòË¶ÅÔºåÂê´‰∏≠ÊñáÂèäÂÖ∂Ëã±ÊñáÂ∞çÁÖß„ÄÇ\nÂ≠∏Ë°ìÊúÉË≠∞Ë´ñÊñáÊëòË¶Å\nÊî∂ÈåÑ„ÄäÂ≠∏Ë°ìÊúÉË≠∞Ë´ñÊñáÊëòË¶ÅË≥áÊñôÂ∫´„Äã‰∏≠Ëá™1988Ëá≥2009Âπ¥Áî±Âè∞ÁÅ£ÊâÄËàâËæ¶‰πãÂ≠∏Ë°ìÊúÉË≠∞Ë´ñÊñá„ÄÇ\nÂÖâËèØÈõúË™å\n„ÄäÂè∞ÁÅ£ÂÖâËèØÈõúË™å„ÄãÂê´Ëá™1993Âπ¥7ÊúàËá≥2023Âπ¥6ÊúàÁöÑÊñáÁ´†ÔºåÂÖ±30Âπ¥‰ªΩ„ÄÇÂÖßÂÆπËëóÈáçÊñºÊàëÂúãÊñáÂåñ„ÄÅËßÄÂÖâËàáÊ∞ëÊÉÖÁ≠â„ÄÇ\nÊ®ÇË©ûÁ∂≤\n„ÄäÊ®ÇË©ûÁ∂≤„ÄãÊ∂µËìãÊñáÁêÜÈ†òÂüüÁ¥Ñ187Ëê¨ÂâáÂ≠∏Ë°ìÂêçË©ûÂèäÂÖ∂Ë≠ØÂêçÂ∞çÁÖß„ÄÇ\nÂêÑÈÉ®ÊúÉË≥áÊñô\nÂåÖÂê´Ë°åÊîøÈô¢„ÄåÂúãÊÉÖÁ∞°‰ªã„Äç„ÄÅÊñáÂåñÈÉ®„ÄåÂúãÂÆ∂ÊñáÂåñË®òÊÜ∂Â∫´„Äç„ÄÅÂúãÁôºÊúÉ„ÄåÊ™îÊ°àÊîØÊè¥ÊïôÂ≠∏Á∂≤„Äç„ÄÅ‰∫§ÈÄöÈÉ®„Äå‰∫§ÈÄöÂÆâÂÖ®ÂÖ•Âè£Á∂≤„ÄçÁ≠âÈÉ®ÊúÉÁ∂≤Á´ôË≥áÊñô‰πãÈÉ®ÂàÜË≥áÊñô„ÄÇ\n‰ªäÂë®Âàä\n„Ää‰ªäÂë®Âàä„ÄãÁÇ∫‰∏Ä‰ª•Ë≤°Á∂ìÁÇ∫‰∏ªÁöÑÈÄ±ÂàäÈõúË™åÔºåÊ≠§Ë≥áÊñôÈõÜÊ∂µËìã2008Âπ¥1ÊúàËá≥2023Âπ¥7ÊúàÁöÑÊñáÁ´†„ÄÇ\nÊïôËÇ≤ÈÉ®ÂúãË™ûËæ≠ÂÖ∏„ÄÅÊàêË™ûËæ≠ÂÖ∏\nÂåÖÂê´‰ª•‰∏ã‰∏âÈ†ÖË≥áÊñô:ÊïôËÇ≤ÈÉ®„ÄäÊàêË™ûÂÖ∏„ÄãÔºåÂê´5,338Ê¢ùÊàêË™ûÔºåÂÖßÂÆπÂåÖÂê´ÊØèÊ¢ùÊàêË™ûÁöÑÈáãÁæ©„ÄÅÂÖ∏ÊïÖÂéüÊñáÂèäÂÖ∂ÁôΩË©±Ë™™Êòé„ÄÅÁî®Ê≥ïË™™Êòé„ÄÅ‰æãÂè•Á≠â„ÄÇÊïôËÇ≤ÈÉ®„ÄäÈáçÁ∑®ÂúãË™ûËæ≠ÂÖ∏‰øÆË®ÇÊú¨„ÄãÔºåÊî∂ÈåÑ‰∏≠ÊñáÂñÆÂ≠óÂèäÂêÑÈ°ûËæ≠ÂΩôÔºåÂåÖÂê´ËÆÄÈü≥„ÄÅÈÉ®È¶ñ„ÄÅÈáãÁæ©Á≠âË≥áË®äÔºåÂÖ±Á¥Ñ165,539Á≠ÜË≥áÊñô„ÄÇÊïôËÇ≤ÈÉ®„ÄäÂúãË™ûËæ≠ÂÖ∏Á∞°Á∑®Êú¨„ÄãÔºåÁÇ∫„ÄäÈáçÁ∑®ÂúãË™ûËæ≠ÂÖ∏‰øÆË®ÇÊú¨„ÄãÁöÑÁ∞°Á∑®ÁâàÊú¨ÔºåÂÖ±45,247Á≠ÜË≥áÊñô„ÄÇ\nÁßëÊäÄÂ§ßËßÄÂúíË≥áÊñô\nÂê´„ÄäÁßëÊäÄÂ§ßËßÄÂúíÁ∂≤Á´ô„Äã‰∏äÁöÑÁßëÂ≠∏Êñ∞Áü•‰ª•ÂèäÁßëÊôÆÊñáÁ´†„ÄÇ\niKnow ÁßëÊäÄÁî¢Ê•≠Ë≥áË®äÂÆ§\n„ÄäÁßëÊäÄÁî¢Ê•≠Ë≥áË®äÂÆ§ÔºàiKnowÔºâ„ÄãÊèê‰æõÂè∞ÁÅ£ÂèäÂÖ®ÁêÉÁöÑÁßëÊäÄÂ∏ÇÂ†¥Ë∂®Âã¢„ÄÅÁ≠ñÁï•ÂàÜÊûê„ÄÅÂ∞àÂà©Áü•Ë≠òÔºåÂèäÊäÄË°ì‰∫§ÊòìË≥áË®äÔºåÂ∞àÊ≥®ÊñºÁßëÊäÄÁî¢Ê•≠ÁöÑÂâµÊñ∞ËàáÁôºÂ±ïÔºåÂåÖÂê´Ëá™ 2008 Âπ¥Ëá≥ 2023 Âπ¥„ÄÇ\nÁßëÂ≠∏ÁôºÂ±ïÊúàÂàä\n„ÄäÁßëÂ≠∏ÁôºÂ±ïÊúàÂàä„ÄãÁÇ∫ÂúãÁßëÊúÉÁÇ∫Êé®Âª£ÁßëÂ≠∏ÊïôËÇ≤ËÄåÂá∫ÁâàÁöÑÁßëÊôÆÂàäÁâ©ÔºåÂê´Ëá™2004Âπ¥10ÊúàËá≥2020Âπ¥12Êúà‰πãÁßëÊôÆÊñáÁ´†Ôºõ2021Âπ¥Ëµ∑Ôºå‰ª•„ÄäÁßëÊäÄÈ≠ÖÁôÆ„ÄãÂ≠£ÂàäÈáçÊñ∞Âá∫ÁôºÔºåÊèê‰æõÂúãÈöõÈóúÊ≥®ÁßëÊäÄË≠∞È°åÁöÑÊñ∞Áü•ÊñáÁ´†„ÄÇ\nÊ≥ïË¶èË≥áÊñôÂ∫´\n„ÄäÊ≥ïË¶èË≥áÊñôÂ∫´„ÄãÂê´Êà™Ëá™ 112 Âπ¥ 10 ÊúàÂêÑÊîøÂ∫úÈÉ®ÈñÄÊúÄÊñ∞ÁôºÂ∏É‰πã‰∏≠Â§ÆÊ≥ïË¶è„ÄÅÔ®àÊîøË¶èÂâá„ÄÅÊ≥ïË¶èÂëΩÔ¶®ËçâÊ°àÂèäÂú∞ÊñπËá™Ê≤ªÊ≥ïË¶èÁ≠â„ÄÇ\nÂêÑÂú∞ÊîøÂ∫úÊóÖÈÅäÁ∂≤\nÊ∂µËìãÂè∞ÁÅ£ÈÉ®ÂàÜÁ∏£Â∏ÇÂú∞ÊñπÊîøÂ∫úËßÄÂÖâÊóÖÈÅäÁ∂≤Á´ô‰∏ä‰πãÈÉ®ÂàÜË≥áÊñô„ÄÇ\nÂúãÊïôÈô¢Ë™≤Á®ãÁ∂±Ë¶Å(ÂçÅ‰∫åÂπ¥ÂúãÊïô)\nÂê´ÂçÅ‰∫åÂπ¥ÂúãÊïôË™≤Á®ãÁ∂±Ë¶Å‰πãÁ∏ΩÁ∂±‰ª•ÂèäÂêÑÁ¥öÂ≠∏Ê†°‰∏çÂêåÁßëÁõÆ‰πãË™≤Á®ãÁ∂±Ë¶Å„ÄÇ\n‰∏≠Â§ÆÁ§æË≠ØÂêçÊ™îË≥áÊñôÂ∫´\n„Ää‰∏≠Â§ÆÁ§æË≠ØÂêçÊ™îË≥áÊñôÂ∫´„ÄãËíêÈõÜ‰∏≠Â§ÆÁ§æÊñ∞ËÅûÊ•≠Âãô‰∏äÁøªË≠ØÈÅéÁöÑ‰∏≠Â§ñÂßìÊ∞è„ÄÅ‰∫∫Âêç„ÄÅÁµÑÁπî„ÄÅÂú∞ÂêçÁ≠âË≠ØÂêçÂ∞çÁÖß„ÄÇ\nÁ´•Ë©±Êõ∏\nÂÖ± 20 Êú¨Á´•Ë©±Êõ∏ÔºåÂê´ÊπØÂßÜÊ≠∑Èö™Ë®ò„ÄÅÂ∞èÈ£õ‰ø†„ÄÅÊÑõÈ∫óÁµ≤Â§¢ÈÅä‰ªôÂ¢É„ÄÅÈï∑ËÖøÂèîÂèîÁ≠â„ÄÇ\nRedPajama-Data-V2\nÂæûÂúãÂ§ñÈñãÊîæÂ§öÂúãË™ûË®ÄË™ûÊñôÂ∫´ RedPajama-Data-v2 ÂèñÂá∫Ëã±ÊñáË≥áÊñô\nMathPile-commercial\nÂúãÂ§ñÈñãÊîæÊï∏Â≠∏Ë™ûÊñôÂ∫´ MathPile-commercial\n‰∏≠ÊñáÁ∂≠Âü∫ÁôæÁßë\n„Ää‰∏≠ÊñáÁ∂≠Âü∫ÁôæÁßë„ÄãÊà™Ëá≥2023Âπ¥1ÊúàÊâÄÊúâÊ¢ùÁõÆÁöÑÂÖßÂÆπ„ÄÇ\ngithub-code-clean\nÁÇ∫ github ÈñãÊ∫êÁ®ãÂºèÁ¢ºË≥áÊñôÈõÜÔºåÂéªÈô§unlicenseÁöÑÁ®ãÂºèÁ¢ºÂíåÊñá‰ª∂„ÄÇ\nÂæÆË™øË≥áÊñô\nTAIDEÂúòÈöäË®ìÁ∑¥llama2Á≥ªÂàóÊ®°Âûã‰æÜÁî¢ÁîüÂæÆË™øË≥áÊñôË≥áÊñôÔºåÁî¢ÁîüÁöÑ‰ªªÂãôÂåÖÂê´‰∏ñÁïåÁü•Ë≠ò„ÄÅÂâµÊÑèÂØ´‰Ωú„ÄÅÊôÆÈÄöÂ∏∏Ë≠ò„ÄÅÁøªË≠Ø„ÄÅÊëòË¶Å„ÄÅÁ®ãÂºè„ÄÅÂè∞ÁÅ£ÂÉπÂÄºÁ≠âÂñÆËº™ÊàñÂ§öËº™Â∞çË©±ÂïèÁ≠îÂÖ± 128K Á≠Ü„ÄÇÂæÆË™øË≥áÊñôÂæåÁ∫åÊúÉÂ∞çÂ§ñÈáãÂá∫„ÄÇ\nÊ®°ÂûãË©ïÊ∏¨\ntaide-bench\nË©ïÊ∏¨Ë≥áÊñô\nÂØ´ÊñáÁ´†„ÄÅÂØ´‰ø°„ÄÅÊëòË¶Å„ÄÅËã±Áøª‰∏≠„ÄÅ‰∏≠ÁøªËã±ÔºåÂÖ±500È°å\nË≥áÊñôÈÄ£Áµê: taide-bench\nË©ïÊ∏¨ÊñπÊ≥ï\ngpt4Ë©ïÂàÜ\nË©ïÂàÜÁ®ãÂºè: taide-bench-eval\nË©ïÊ∏¨ÂàÜÊï∏\nÊ®°Âûã\n‰∏≠ÁøªËã±\nËã±Áøª‰∏≠\nÊëòË¶Å\nÂØ´ÊñáÁ´†\nÂØ´‰ø°\nÂπ≥Âùá\nTAIDE-LX-7B-Chat\n7.165\n7.685\n7.720\n9.635\n9.110\n8.263\nGPT3.5\n8.880\n8.810\n7.450\n9.490\n8.750\n8.676\nLLAMA2 7B\n6.075\n4.475\n5.905\n2.625\n3.040\n4.424\nLLAMA2 13B\n6.480\n6.135\n6.110\n2.565\n3.000\n4.858\nLLAMA2 70B\n6.975\n6.375\n6.795\n2.625\n2.990\n5.152\nÊéàÊ¨äÊ¢ùÊ¨æ\nTAIDE L È°ûÊ®°ÂûãÁ§æÁæ§ÊéàÊ¨äÂêåÊÑèÊõ∏\nÂÖçË≤¨ËÅ≤Êòé\nLLM Ê®°ÂûãÁî±ÊñºË®≠Ë®àÊû∂ÊßãÁöÑÈôêÂà∂Ôºå‰ª•ÂèäË≥áÊñôÈõ£ÂÖçÊúâÂÅèË™§ÔºåË™ûË®ÄÊ®°ÂûãÁöÑ‰ªª‰ΩïÂõûÊáâ‰∏ç‰ª£Ë°® TAIDE Á´ãÂ†¥Ôºå‰ΩøÁî®ÂâçÈúÄË¶ÅÈ°çÂ§ñÂä†ÂÖ•ÂÆâÂÖ®Èò≤Ë≠∑Ê©üÂà∂Ôºå‰∏îÂõûÊáâÂÖßÂÆπ‰πüÂèØËÉΩÂåÖÂê´‰∏çÊ≠£Á¢∫ÁöÑË≥áË®äÔºå‰ΩøÁî®ËÄÖË´ãÂãøÁõ°‰ø°„ÄÇ\nÈñãÁôºÂúòÈöä\nhttps://taide.tw/index/teamList\nÁõ∏ÈóúÈÄ£Áµê\nTAIDEÂÆòÁ∂≤\nTAIDE Huggingface\nTAIDE Github\nKuwa AI\nCitation\nTAIDEÂÆòÁ∂≤",
    "taide/TAIDE-LX-7B-Chat-4bit": "ÊÇ®ÈúÄË¶ÅÂÖàÂêåÊÑèÊéàÊ¨äÊ¢ùÊ¨æÊâçËÉΩ‰ΩøÁî®Ê≠§Ê®°Âûã\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTAIDE L È°ûÊ®°ÂûãÁ§æÁæ§ÊéàÊ¨äÂêåÊÑèÊõ∏(License)\nÂÄã‰∫∫Ë≥áÊñôËíêÈõÜÂëäÁü•ËÅ≤Êòé(Privacy policy)\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nÊ®°ÂûãÁ∞°‰ªã\nÊ®°ÂûãÂèÉÊï∏\nÁâπËâ≤\nÊáâÁî®ÁØÑ‰æã\n‰ΩøÁî®Ë™™Êòé\nË®ìÁ∑¥ÊñπÊ≥ï\nË®ìÁ∑¥Ë≥áÊñô\nÊ®°ÂûãË©ïÊ∏¨\nÊéàÊ¨äÊ¢ùÊ¨æ\nÂÖçË≤¨ËÅ≤Êòé\nÈñãÁôºÂúòÈöä\nÁõ∏ÈóúÈÄ£Áµê\nCitation\nEnglish Version\nÊ®°ÂûãÁ∞°‰ªã\nTAIDEË®àÁï´Ëá¥ÂäõÊñºÈñãÁôºÁ¨¶ÂêàÂè∞ÁÅ£Ë™ûË®ÄÂíåÊñáÂåñÁâπÊÄßÁöÑÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÂ∞çË©±ÂºïÊìéÊ®°ÂûãÔºåÂêåÊôÇÂª∫ÊßãÂèØ‰ø°‰ªªÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁí∞Â¢É„ÄÇÁµêÂêàÁî¢Â≠∏Á†îËÉΩÈáèÔºåÊé®ÂãïÂèØ‰ø°‰ªªÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÁöÑÁôºÂ±ïÔºåÊèêÂçáÂè∞ÁÅ£Âú®ÂúãÈöõÁ´∂Áà≠‰∏≠ÁöÑÂú∞‰ΩçÔºå‰øÉÈÄ≤Áî¢Ê•≠ÁôºÂ±ïÔºåÈÅøÂÖçÂ∞çÂ§ñÂúãÊäÄË°ìÁöÑ‰æùË≥¥„ÄÇ\nÊú¨Ë®àÁï´ÈñãÁôºÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ª• Meta ÂÖ¨Âè∏ÈáãÂá∫ÁöÑ LLaMA2-7b ÁÇ∫Âü∫Á§éÔºåÂ∞éÂÖ•Âè∞ÁÅ£‰∏çÂêåÈ†òÂüüÂèØÁî®ÁöÑÊñáÊú¨ËàáË®ìÁ∑¥Á¥†ÊùêÔºåÊèêÈ´òÊ®°ÂûãÂú®Ê≠£È´î‰∏≠ÊñáÂõûÊáâÁöÑËÉΩÂäõËàáÁâπÂÆö‰ªªÂãôÁöÑË°®Áèæ„ÄÇÂÖ¨ÈñãÈáãÂá∫ÁöÑÊ®°ÂûãÂ¶Ç‰∏ã:\nTAIDE-LX-7B: ‰ª• LLaMA2-7b ÁÇ∫Âü∫Á§éÔºåÂÉÖ‰ΩøÁî®ÁπÅÈ´î‰∏≠ÊñáË≥áÊñôÈ†êË®ìÁ∑¥ (continuous pretraining)ÁöÑÊ®°ÂûãÔºåÈÅ©Âêà‰ΩøÁî®ËÄÖÊúÉÂ∞çÊ®°ÂûãÈÄ≤‰∏ÄÊ≠•ÂæÆË™ø(fine tune)ÁöÑ‰ΩøÁî®ÊÉÖÂ¢É„ÄÇÂõ†È†êË®ìÁ∑¥Ê®°ÂûãÊ≤íÊúâÁ∂ìÈÅéÂæÆË™øÂíåÂÅèÂ•ΩÂ∞çÈΩäÔºåÂèØËÉΩÊúÉÁî¢ÁîüÊÉ°ÊÑèÊàñ‰∏çÂÆâÂÖ®ÁöÑËº∏Âá∫Ôºå‰ΩøÁî®ÊôÇË´ãÂ∞èÂøÉ„ÄÇ\nTAIDE-LX-7B-Chat: ‰ª• TAIDE-LX-7B ÁÇ∫Âü∫Á§éÔºåÈÄèÈÅéÊåá‰ª§ÂæÆË™ø(instruction tuning)Âº∑ÂåñËæ¶ÂÖ¨ÂÆ§Â∏∏Áî®‰ªªÂãôÂíåÂ§öËº™ÂïèÁ≠îÂ∞çË©±ËÉΩÂäõÔºåÈÅ©ÂêàËÅäÂ§©Â∞çË©±Êàñ‰ªªÂãôÂçîÂä©ÁöÑ‰ΩøÁî®ÊÉÖÂ¢É„ÄÇTAIDE-LX-7B-ChatÂè¶Â§ñÊúâÊèê‰æõ4 bit ÈáèÂåñÊ®°ÂûãÔºåÈáèÂåñÊ®°Âûã‰∏ªË¶ÅÊòØÊèê‰æõ‰ΩøÁî®ËÄÖÁöÑ‰æøÂà©ÊÄßÔºåÂèØËÉΩÊúÉÂΩ±ÈüøÊïàËÉΩËàáÊõ¥Â§ö‰∏çÂèØÈ†êÊúüÁöÑÂïèÈ°åÔºåÈÇÑË´ã‰ΩøÁî®ËÄÖÁêÜËß£ËàáÊ≥®ÊÑè„ÄÇ\nÊ®°ÂûãÂèÉÊï∏\nÂèÉÊï∏Èáè: 7B\nÊúÄÂ§ßÂÖßÂÆπÈï∑Â∫¶ (context length): 4K\nÁπÅ‰∏≠Ë®ìÁ∑¥Ë≥áÊñô token Èáè: 41.44B\nË®ìÁ∑¥ÊôÇÈñì: 1531.82 H100 GPU Hours\nÁâπËâ≤\nÈ°çÂ§ñÊì¥ÂÖÖ24720ÂÄã‰∏≠ÊñáÂ≠óÂÖÉ„ÄÅÂ≠óË©ûÔºåÂº∑ÂåñÊ®°ÂûãËôïÁêÜÁπÅÈ´î‰∏≠ÊñáÁöÑËÉΩÂäõ\nÂö¥Ê†ºÊääÈóúÊ®°ÂûãÁöÑË®ìÁ∑¥Ë≥áÊñôÔºåÊèêÂçáÊ®°ÂûãÁîüÊàêË≥áÊñôÁöÑÂèØ‰ø°‰ªªÊÄßÂíåÈÅ©Áî®ÊÄß\nÈáùÂ∞çËá™ÂãïÊëòË¶Å„ÄÅÂØ´‰ø°„ÄÅÂØ´ÊñáÁ´†„ÄÅ‰∏≠ÁøªËã±„ÄÅËã±Áøª‰∏≠Á≠âËæ¶ÂÖ¨ÂÆ§Â∏∏Áî®‰ªªÂãôÂÅöÂä†Âº∑\nÈáùÂ∞çÂè∞ÁÅ£Âú®Âú∞ÊñáÂåñ„ÄÅÁî®Ë™û„ÄÅÂúãÊÉÖÁ≠âÁü•Ë≠òÂÅöÂä†Âº∑\nÂÖ∑ÂÇôÂ§öËº™ÂïèÁ≠îÂ∞çË©±ËÉΩÂäõ\nÊáâÁî®ÁØÑ‰æã\n‰ªªÂãô\n‰ΩøÁî®ËÄÖËº∏ÂÖ•\nÊ®°ÂûãËº∏Âá∫\nÂØ´ÊñáÁ´†\nË´ã‰ª•‰ª•‰∏ãÂÖßÂÆπÁÇ∫Âü∫Á§éÔºåÂØ´‰∏ÄÁØáÊñáÁ´†ÔºöÊí∞ÂØ´‰∏ÄÁØá‰ΩúÊñáÔºåÈ°åÁõÆÁÇ∫„Ää‰∏ÄÂºµËàäÁÖßÁâá„ÄãÔºåÂÖßÂÆπË¶ÅÊ±ÇÁÇ∫ÔºöÈÅ∏Êìá‰∏ÄÂºµ‰ª§‰Ω†Âç∞Ë±°Ê∑±ÂàªÁöÑÁÖßÁâáÔºåË™™Êòé‰ª§‰Ω†Âç∞Ë±°Ê∑±ÂàªÁöÑÂéüÂõ†Ôºå‰∏¶ÊèèËø∞ÁÖßÁâá‰∏≠ÁöÑÂΩ±ÂÉèÂèäËÉåÂæåÁöÑÊïÖ‰∫ã„ÄÇË®òÈåÑÊàêÈï∑ÁöÑÈÅéÁ®ã„ÄÅËàá‰ªñ‰∫∫ÁöÑÊÉÖÊôØ„ÄÅÁí∞Â¢ÉËÆäÈÅ∑ÂíåÁæéÈ∫óÁöÑÊôØËâ≤„ÄÇ\n‰ª•‰∏ãÊòØÊ†πÊìöÊåá‰ª§Ââµ‰ΩúÁöÑ‰∏ÄÁØá‰ΩúÊñáÔºö„Ää‰∏ÄÂºµËàäÁÖßÁâá„ÄãÂú®ÊàëÂ°µÂ∞ÅÂ∑≤‰πÖÁöÑÂÆ∂ÊóèÁõ∏Á∞ø‰∏≠ÔºåÊúâ‰∏ÄÂºµÁâπÂà•Âê∏ÂºïÊàëÁöÑËàäÁÖßÁâáÔºåÈÇ£ÂºµÁÖßÁâáË®òÈåÑ‰∫ÜÊàëÂÄëÂÆ∂ÊóèÊàêÂì°Âú®ÈÅéÂæÄÁæéÂ•ΩÊôÇÂÖâÁöÑÊ≠°ËÅöÔºåËá≥‰ªä‰ªçËàä‰ª§‰∫∫ÂõûÂë≥ÁÑ°Á™Æ„ÄÇÈÄôÂºµÁÖßÁâáÊòØÊàëÂ•∂Â•∂Âú®‰∫åÂçÅÂ§öÂπ¥ÂâçËàâËæ¶ÁöÑ‰∏ÄÂ†¥ÂÆ∂ÊóèËÅöÊúÉ‰∏äÊâÄÊãç‰∏ãÁöÑÔºåÁÖßÁâá‰∏≠ÁöÑ‰∫∫Áâ©ÂåÖÊã¨‰∫ÜÊàëÁöÑÁà∂ÊØç„ÄÅÁ•ñÁà∂ÊØç„ÄÅÂèîÂèî„ÄÅÂßëÂßëÂíåË°®Âì•Á≠âÔºåÂ§ßÂÆ∂Ëáâ‰∏äÊ¥ãÊ∫¢ËëóÂπ∏Á¶èÁöÑÁ¨ëÂÆπÔºåÂΩ∑ÂΩøÊôÇÂÖâÈùúÊ≠¢Ëà¨Âú∞ÁæéÂ•Ω„ÄÇÁÖßÁâá‰∏≠ÁöÑÂ†¥ÊôØÂçÅÂàÜÊ∫´È¶®ÔºåÂ§ßÂÆ∂ÂúçÂùêÂú®‰∏ÄÂºµÈï∑ÊñπÂΩ¢ÁöÑÈ§êÊ°å‰∏äÔºåÊ°å‰∏äÊì∫Êªø‰∫ÜÁæéÂë≥ÁöÑÈ£üÁâ©ÂíåÈ£≤Êñô„ÄÇÊ°å‰∏äÁöÑËèúÈ§öÊòØÂÇ≥Áµ±‰∏≠ÂºèÁöÑ‰Ω≥È§öÔºåÊúâÈ¶ôÂô¥Âô¥ÁöÑÁÉ§ÂÖ®È≠ö„ÄÅÂ´©ÊªëÁöÑÈõûËÇâÂíåÈÆÆÁîúÁöÑËî¨ËèúÔºåÈÄô‰∫õËèúÈ§öÊòØÁï∂Âπ¥Â•∂Â•∂Ë¶™Ëá™ÁÉπË™øÁöÑÔºåÂ•πÊâãËóùÁ≤æÊπõÔºåËÆìÊØèÂÄã‰∫∫ÈÉΩÂêÉÂæóÊ¥•Ê¥•ÊúâÂë≥„ÄÇÊúÄËÆìÊàëÂç∞Ë±°Ê∑±ÂàªÁöÑÊòØÔºåÁÖßÁâá‰∏≠ÈÇÑÊãçÂà∞‰∫ÜÊàëÂπ¥ÂπºÊôÇÁöÑÊ®°Ê®£„ÄÇÊàëÂùêÂú®Ê°åÂ≠êÁöÑÂè¶‰∏ÄÁ´ØÔºåÊâãË£°ÊãøËëó‰∏ÄÁ¢óÊπØÔºåËáâ‰∏äÂ∏∂ËëóÊªøË∂≥ÁöÑÁ¨ëÂÆπ„ÄÇÈÇ£ÊôÇÁöÑÊàëÊâçÂâõÊªøÂë®Ê≠≤ÔºåÂ∞çÊñºÈÄôÂ†¥ÁõõÂ§ßÁöÑÂÆ∂ÊóèËÅöÊúÉÈÇÑÊÑüÂà∞ÂçÅÂàÜÊñ∞Â•á„ÄÇÂæûÈÄôÂºµÁÖßÁâá‰∏≠ÔºåÊàëËÉΩÊÑüÂèóÂà∞Áï∂Âπ¥ÂÆ∂ÊóèÊàêÂì°ÈñìÁöÑË¶™ÂØÜÂíåÊ∫´ÊöñÔºåÂç≥‰ΩøÈÅé‰∫ÜÈÄôÈ∫ºÂ§öÂπ¥ÔºåÈÇ£‰ªΩÊÉÖÊÑüÂçª‰æùÁÑ∂Áâ¢Âõ∫Âú∞Â≠òÂú®Ëëó„ÄÇÈÄôÂºµÁÖßÁâáËÉåÂæåÊúâËëóÂãï‰∫∫ÁöÑÊïÖ‰∫ã„ÄÇÂú®ÈÇ£ÂÄãÊôÇ‰ª£ÔºåÊàëÁöÑÁ•ñÁà∂ÊØçÂíåÁà∂ÊØçÈÉΩÈÇÑÂú®ËÆÄ‰∏≠Â≠∏Ôºå‰ªñÂÄëÊØèÂ§©ÈÉΩÊúÉÊîæÂ≠∏ÂæåÂà∞Ëæ≤Áî∞Ë£°Âπ´ÂøôÔºåËæõÂã§Âú∞ËÄï‰Ωú‰ª•Á∂≠ÊåÅ‰∏ÄÂÆ∂‰∫∫ÁöÑÁîüË®à„ÄÇÂÑòÁÆ°ÁîüÊ¥ªÊ¢ù‰ª∂‰∏¶‰∏çÂØåË£ïÔºå‰ΩÜ‰ªñÂÄë‰ªçËàä‰øùÊåÅËëóÊ®ÇËßÄÁöÑÊÖãÂ∫¶ÂíåÂ∞çÁîüÊ¥ªÁöÑÁÜ±ÊÑõ„ÄÇÊØèÂπ¥Âà∞‰∫ÜÁßãÂ§©ÔºåÂè™Ë¶ÅÊî∂Áç≤Â≠£ÁØÄ‰æÜËá®Ôºå‰ªñÂÄëÂ∞±ÊúÉËàâËæ¶ÂÆ∂ÊóèËÅöÊúÉÔºåÈÇÄË´ãÊâÄÊúâË¶™ÊàöÊúãÂèãÂâç‰æÜÂÖ±‰∫´Ë±êÊî∂ÁöÑÂñúÊÇÖ„ÄÇÈö®ËëóÊ≠≤ÊúàÁöÑÊµÅÈÄùÔºåÊàëÈÄêÊº∏Èï∑Â§ßÔºåÈñãÂßãÂ∑•‰ΩúÂíåÁîüÊ¥ª„ÄÇÊØèÊ¨°ÂõûÈÑâÔºåÊàëÈÉΩÊúÉÊäΩÁ©∫ÂéªÊé¢ÊúõËÄÅ‰∫∫ÂÆ∂ÔºåËÅΩËÅΩ‰ªñÂÄëÁï∂Âπ¥ÁöÑÂæÄ‰∫ãÔºåÊÑüÂèóÈÇ£ÊÆµÂàªÈ™®ÈäòÂøÉÁöÑÊôÇÂÖâ„ÄÇËÄåÈÄôÂºµËàäÁÖßÁâáÔºå‰πüÂ∞á‰∏ÄÁõ¥Èô™‰º¥ËëóÊàëÔºåÊàêÁÇ∫ÊàëÁèçË≤¥ÂõûÊÜ∂ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÂÆÉË¶ãË≠â‰∫ÜÊàëÂÄëÁöÑÊàêÈï∑ÂíåËÆäÈÅ∑Ôºå‰πüË®òÈåÑ‰∏ã‰∫ÜÈÇ£‰∫õÈõ£‰ª•ÂøòÊá∑ÁöÑÁæéÂ•ΩÊôÇÂàª„ÄÇÈÄôÂºµÁÖßÁâá‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜÂÆ∂ÊóèÁöÑË¶™ÊÉÖÂíåÊ∫´ÊöñÔºåÈÇÑÊäòÂ∞ÑÂá∫ÈÇ£ÂÄãÊôÇ‰ª£‰∫∫ÂÄëÂã§ÂãûÊ®∏ÂØ¶ÁöÑÁîüÊ¥ªÊñπÂºèÂíåÂ∞çÂÇ≥Áµ±ÁöÑÂ∞äÈáç„ÄÇÂ¶Ç‰ªäÔºåÈö®ËëóÁ§æÊúÉÁöÑÂø´ÈÄüÁôºÂ±ïÔºåË®±Â§öÂÇ≥Áµ±ÁöÑÁæéÂæ∑ÂíåÈ¢®‰øóÊ≠£Âú®ÈÄêÊº∏Ê∂àÂ§±„ÄÇÊàëÂÄëÊáâË©≤Â•ΩÂ•ΩÁèçÊÉúÈÄôÊ®£ÁöÑÂõûÊÜ∂ÂíåÊñáÂåñÔºåÂ∞áÂÆÉÂÄëÂÇ≥Êâø‰∏ãÂéªÔºåËÆìÂæå‰ª£Â≠êÂ≠´‰πüËÉΩÊÑüÂèóÂà∞ÈÇ£‰ªΩ‰æÜËá™Ê≠∑Âè≤ÁöÑÊ∫´ÊÉÖÂíåÂÇ≥Êâø„ÄÇÂõ†Ê≠§ÔºåÊàëÊ±∫ÂÆöÂ∞áÈÄôÂºµÁÖßÁâáÂ•ΩÂ•Ω‰øùÁÆ°Ëµ∑‰æÜÔºå‰∏¶Â∏∏Â∏∏ÊãøÂá∫‰æÜÁúã„ÄÇÂÆÉ‰∏çÂÉÖÊòØ‰∏ÄÂºµËàäÁÖßÁâáÔºåÊõ¥ÊòØ‰∏ÄÊÆµÂØ∂Ë≤¥ÁöÑÂõûÊÜ∂ÂíåÂÆ∂ÊóèÁ≤æÁ•ûÁöÑË±°Âæµ„ÄÇÊàëÂ∏åÊúõÊú™‰æÜËÉΩÊúâÊ©üÊúÉÈáçÁèæÁï∂Âπ¥ÁöÑÁõõÊ≥ÅÔºåÂÜç‰∏ÄÊ¨°ËÅöÂú®‰∏ÄËµ∑ÔºåÂàÜ‰∫´ÂΩºÊ≠§ÁöÑÁîüÊ¥ªÂíåÊïÖ‰∫ãÔºåËÆìÈÇ£‰ªΩË¶™ÊÉÖÂíåÂèãÊÉÖÁπºÁ∫åÂú®ÊôÇÂÖâ‰∏≠ËîìÂª∂ÔºåÊ∞∏‰∏çÊ∂àÈÄù„ÄÇ\nÂØ´‰ø°\nË´ã‰ª•ÂìÅÁâåÁ∂ìÁêÜÁöÑË∫´‰ªΩÔºåÁµ¶Âª£ÂëäÂÖ¨Âè∏ÁöÑÂâµÊÑèÁ∏ΩÁõ£ÂØ´‰∏ÄÂ∞Å‰ø°ÔºåÊèêÂá∫Â∞çÊñºÊñ∞Áî¢ÂìÅÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãïÁöÑÂâµÊÑèÂª∫Ë≠∞„ÄÇ\nÂ∞äÊï¨ÁöÑÂâµÊÑèÁ∏ΩÁõ£ÔºöÊàëÂØ´ÈÄôÂ∞Å‰ø°ÊòØÁÇ∫‰∫ÜÂêëÊÇ®ÊèêÂá∫‰∏Ä‰∫õÈóúÊñºÊàëÂÄëÊñ∞Áî¢ÂìÅÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãïÁöÑÂâµÊÑèÂª∫Ë≠∞„ÄÇÊàëÂÄëÂÖ¨Âè∏Âç≥Â∞áÊé®Âá∫‰∏ÄÊ¨æÂÖ®Êñ∞ÁöÑÁî¢ÂìÅÔºåÁÇ∫‰∫ÜÁ¢∫‰øùÂÆÉÂú®Â∏ÇÂ†¥‰∏äÂèñÂæóÊàêÂäüÔºåÊàëÂÄëÂ∏åÊúõÈÄèÈÅé‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãï‰æÜÂê∏ÂºïÊ∂àË≤ªËÄÖÁöÑÊ≥®ÊÑè„ÄÇÂú®ËÄÉÊÖÆÂª£ÂëäÁöÑÂâµÊÑèÂíåÁ≠ñÁï•ÊôÇÔºåÊúâÂπæÂÄãÂõ†Á¥†ÈúÄË¶ÅÁ¥çÂÖ•ËÄÉÈáè„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëË¶ÅÈáùÂ∞çÁõÆÊ®ôÂÆ¢Áæ§ÈÄ≤Ë°åÁ†îÁ©∂Ôºå‰ª•Áû≠Ëß£‰ªñÂÄëÁöÑÈúÄÊ±ÇÂíåÂñúÂ•Ω„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëË¶ÅÁ¢∫‰øùÂª£ÂëäÂÖßÂÆπÂÖ∑ÊúâÂê∏ÂºïÂäõÂíåË™™ÊúçÂäõÔºåËÉΩÂºïËµ∑ÁõÆÊ®ôÂÆ¢Áæ§ÁöÑÂÖ±È≥¥„ÄÇÊúÄÂæåÔºåÊàëÂÄëË¶ÅÂà©Áî®ÂêÑÁ®ÆÂ™íÈ´îÂπ≥Ëá∫ÂíåÊï∏‰ΩçË°åÈä∑Â∑•ÂÖ∑Ôºå‰ª•Êì¥Â§ßÂª£ÂëäÁöÑÂΩ±ÈüøÂäõ„ÄÇÂü∫ÊñºÈÄô‰∫õÂõ†Á¥†ÔºåÊàëÊèêÂá∫‰ª•‰∏ãÂπæÈªûÂâµÊÑèÂª∫Ë≠∞Ôºö1. Áî¢ÂìÅÂÆö‰ΩçÔºöÊàëÂÄëÂèØÂ∞áÈÄôÊ¨æÊñ∞Áî¢ÂìÅÂÆö‰ΩçÁÇ∫È´òÁ´Ø„ÄÅÁí∞‰øù„ÄÅÂÅ•Â∫∑ÂíåÊôÇÂ∞öÁöÑ‰ª£Ë°®ÔºåÂº∑Ë™øÂÖ∂Áç®ÊúâÁöÑÂäüËÉΩÂíåÁâπËâ≤„ÄÇÂú®Âª£Âëä‰∏≠ÔºåÊàëÂÄëÂèØ‰ª•ÈÄèÈÅéÁîüÂãïÁöÑË¶ñË¶∫ÊïàÊûúÂíåÁ∞°ÊΩîÁöÑË™ûË®Ä‰æÜÂÇ≥ÈÅîÈÄô‰∫õÁâπÈªû„ÄÇ2. ÊïÖ‰∫ãË°åÈä∑ÔºöÂú®Âª£Âëä‰∏≠Ë¨õËø∞‰∏ÄÂÄãËàáÁî¢ÂìÅÂäüËÉΩÁõ∏ÈóúÁöÑÂãï‰∫∫ÊïÖ‰∫ãÔºåËÆìÊ∂àË≤ªËÄÖËÉΩÊõ¥Ê∑±ÂÖ•Âú∞Áû≠Ëß£Áî¢ÂìÅÊâÄÂ∏∂‰æÜÁöÑÁîüÊ¥ªËÆäÂåñ„ÄÇ‰æãÂ¶ÇÔºåÊàëÂÄëÂèØ‰ª•Ë¨õËø∞‰∏Ä‰ΩçÂøôÁ¢åÁöÑËÅ∑Ê•≠Â©¶Â•≥ÔºåÂ¶Ç‰Ωï‰ΩøÁî®ÊàëÂÄëÁöÑÊñ∞Áî¢ÂìÅÂú®Â∑•‰ΩúÂíåÁîüÊ¥ª‰∏≠ÂèñÂæóÂπ≥Ë°°ÁöÑÊïÖ‰∫ã„ÄÇ3. Âêç‰∫∫ÊïàÊáâÔºöÈÇÄË´ã‰∏Ä‰ΩçÂèóÊ≠°ËøéÁöÑÂÖ¨Áúæ‰∫∫Áâ©ÊàñÊÑèË¶ãÈ†òË¢ñÊìî‰ªªÁî¢ÂìÅ‰ª£Ë®Ä‰∫∫ÔºåÂà©Áî®‰ªñÂÄëÁöÑÂΩ±ÈüøÂäõ‰æÜÊé®Âª£Áî¢ÂìÅ„ÄÇÈÄô‰∏çÂÉÖÂèØ‰ª•Â¢ûÂä†Áî¢ÂìÅÊõùÂÖâÂ∫¶ÔºåÈÇÑËÉΩËÆìÊõ¥Â§öÊ∂àË≤ªËÄÖ‰ø°‰ªªÂíå‰ø°Ë≥¥Áî¢ÂìÅ„ÄÇ4. Ë∑®ÁïåÂêà‰ΩúÔºöËàá‰∏Ä‰∫õÁõ∏ÈóúÁî¢Ê•≠ÔºàÂ¶ÇÊôÇÂ∞ö„ÄÅÁí∞‰øù„ÄÅÂÅ•Â∫∑Á≠âÔºâÈÄ≤Ë°åË∑®ÁïåÂêà‰ΩúÔºåÂÖ±ÂêåÊé®Âá∫ÈôêÈáèÁâàÊàñËÅØÂêçÁî¢ÂìÅ„ÄÇÈÄôÁ®ÆÂêà‰Ωú‰∏çÂÉÖËÉΩÂê∏Âºï‰∏çÂêåÈ†òÂüüÁöÑÊ∂àË≤ªËÄÖÔºåÈÇÑËÉΩÁÇ∫Áî¢ÂìÅÂ∏∂‰æÜÊõ¥Â§öË©±È°åÊÄßÂíåÊî∂ËóèÂÉπÂÄº„ÄÇ5. Êï∏‰ΩçË°åÈä∑ÔºöÂÖÖÂàÜÂà©Áî®Á§æÁæ§Â™íÈ´î„ÄÅÊêúÂ∞ãÂºïÊìéÂíåÈõªÈÉµË°åÈä∑Á≠âÊï∏‰ΩçÂ∑•ÂÖ∑ÔºåÂú®Êï¥ÂÄãÂª£ÂëäÂÆ£ÂÇ≥ÈÅéÁ®ã‰∏≠‰øùÊåÅ‰∏ÄËá¥ÊÄßÁöÑË®äÊÅØÂíåÂΩ¢Ë±°„ÄÇÊàëÂÄë‰πüÂèØË£Ω‰Ωú‰∏ÄÁ≥ªÂàóÁü≠Ë¶ñÈ†ªÂíåÁ∑ö‰∏äÊ¥ªÂãïÔºå‰ª•Â¢ûÂä†Ê∂àË≤ªËÄÖÂèÉËàáÂ∫¶ÂíåÂìÅÁâåÂø†Ë™†Â∫¶„ÄÇ6. ÂâµÊñ∞ÂΩ¢ÂºèÔºöÂòóË©¶‰ΩøÁî®Êñ∞Á©éÁöÑÂª£ÂëäÂΩ¢ÂºèÔºåÂ¶ÇARÔºàÊì¥Â¢ûÂØ¶Â¢ÉÔºâ„ÄÅVRÔºàËôõÊì¨ÂØ¶Â¢ÉÔºâÂíå360Â∫¶ÂΩ±ÁâáÁ≠âÔºåÁµ¶Ê∂àË≤ªËÄÖÂ∏∂‰æÜÂÖ®Êñ∞ÁöÑÂª£ÂëäÈ´îÈ©ó„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÂΩ¢Âºè‰∏çÂÉÖËÉΩÂê∏ÂºïÊ∂àË≤ªËÄÖÁõÆÂÖâÔºåÈÇÑËÉΩÊèêÈ´òÁî¢ÂìÅÂú®Â∏ÇÂ†¥‰∏äÁöÑËæ®Ë≠òÂ∫¶„ÄÇ7. Ê¥ªÂãïÂèÉËàáÔºöÂú®Âª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãï‰∏≠Âä†ÂÖ•‰∫íÂãïÁí∞ÁØÄÔºåÂ¶ÇÁî¢ÂìÅË©¶Áî®„ÄÅÊäΩÁçéÂíåÂÑ™ÊÉ†Âà∏Á≠âÔºåÈºìÂãµÊ∂àË≤ªËÄÖÁ©çÊ•µÂèÉËàá‰∏¶ÂàÜ‰∫´‰ªñÂÄëÁöÑÈ´îÈ©ó„ÄÇÈÄôÂ∞áÊúâÂä©ÊñºÂè£Á¢ëÂÇ≥Êí≠ÔºåÈÄ≤‰∏ÄÊ≠•Êì¥Â§ßÁî¢ÂìÅÂΩ±ÈüøÂäõ„ÄÇÁ∂úÂêà‰ª•‰∏äÂª∫Ë≠∞ÔºåÊàëÂ∏åÊúõÊàëÂÄëËÉΩË£ΩÂÆöÂá∫‰∏ÄÂ•óÂÖ®Èù¢ËÄåÂØåÊúâÂâµÊÑèÁöÑÂª£ÂëäÂÆ£ÂÇ≥Ê¥ªÂãïË®àÂäÉÔºå‰ª•ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÊèêÂçáÊñ∞Áî¢ÂìÅÂú®Â∏ÇÂ†¥‰∏äÁöÑÁ´∂Áà≠Âäõ„ÄÇÂ¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÁñëÂïèÊàñÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Ë®éË´ñÔºåÊ≠°ËøéÈö®ÊôÇËàáÊàëËÅØÁµ°„ÄÇË¨ùË¨ùÊÇ®ÁöÑÈóúÊ≥®ÂíåÊîØÊè¥ÔºÅÁ•ùÂïÜÁ•∫ÔºÅÊÇ®ÁöÑÂìÅÁâåÁ∂ìÁêÜÔºàË®ªÔºöÊ≠§‰ø°‰ª∂ÂèØÊ†πÊìöÂØ¶ÈöõÊÉÖÊ≥ÅÂíåÁî¢ÂìÅÁâπÈªûÈÄ≤Ë°åË™øÊï¥Ôºâ\nÊëòË¶Å\nË´ãÂ∞áÈÄôÁØáÊñáÁ´†Á≤æÁ∞°Ê¢ùÁêÜÂåñ:„ÄåÁî¢Ê•≠ÂâµÊñ∞Ê¢ù‰æãÁ¨¨10Ê¢ù‰πã2ÂèäÁ¨¨72Ê¢ùÊ¢ùÊñá‰øÆÊ≠£Ê°à„Äç‰øóÁ®±„ÄåÂè∞ÁâàÊô∂ÁâáÊ≥ï„Äç,ÈáùÂ∞çÂçäÂ∞éÈ´î„ÄÅÈõªÂãïËªä„ÄÅ5GÁ≠âÊäÄË°ìÂâµÊñ∞‰∏îÂ±ÖÂúãÈöõ‰æõÊáâÈèàÈóúÈçµÂú∞‰ΩçÂÖ¨Âè∏,Êèê‰æõÊúÄÈ´ò25%ÁáüÊâÄÁ®ÖÊäïÊäµÂÑ™ÊÉ†,‰ºÅÊ•≠ÈÅ©Áî®Ë¶Å‰ª∂ÂåÖÂê´Áï∂Âπ¥Â∫¶Á†îÁôºË≤ªÁî®„ÄÅÁ†îÁôºÂØÜÂ∫¶ÈÅî‰∏ÄÂÆöË¶èÊ®°,‰∏îÊúâÊïàÁ®ÖÁéáÈÅî‰∏ÄÂÆöÊØîÁéá„ÄÇÁÇ∫Âõ†ÊáâÁ∂ìÊøüÂêà‰ΩúÊö®ÁôºÂ±ïÁµÑÁπî(OECD)ÂúãÂÆ∂ÊúÄ‰ΩéÁ®ÖË≤†Âà∂Ë™øÊï¥,ÂÖ∂‰∏≠ÊúâÊïàÁ®ÖÁéáÈñÄÊ™ª,Ê∞ëÂúã112Âπ¥Ë®ÇÁÇ∫12%,113Âπ¥ÊñôÂ∞áÊèêÈ´òËá≥15%,‰ΩÜ‰ªçÂæóÂØ©ÈÖåÂúãÈöõÈñìÊúÄ‰ΩéÁ®ÖË≤†Âà∂ÂØ¶ÊñΩÊÉÖÂΩ¢„ÄÇÁ∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Â∑≤ÂíåË≤°ÊîøÈÉ®ÂçîÂïÜÈÄ≤ÂÖ•ÊúÄÂæåÈöéÊÆµ,Èô§‰ºÅÊ•≠Á†îÁôºÂØÜÂ∫¶Ë®ÇÂú®6%,ÁõÆÂâçÂ∑≤Á¢∫Ë™ç,‰ºÅÊ•≠Ë≥ºÁΩÆÂÖàÈÄ≤Ë£ΩÁ®ãÁöÑË®≠ÂÇôÊäïË≥áÈáëÈ°çÈÅî100ÂÑÑÂÖÉ‰ª•‰∏äÂèØÊäµÊ∏õ„ÄÇË≤°ÊîøÈÉ®ÂÆòÂì°Ë°®Á§∫,Á†îÂïÜÈÅéÁ®ã‰∏≠,ÈáùÂ∞çÂè∞ÁÅ£Áî¢Ê•≠ËàáÂÖ∂Âú®ÂúãÈöõÈñìÈ°û‰ººÁöÑÂÖ¨Âè∏ÈÄ≤Ë°åÊ∑±ÂÖ•Á†îÁ©∂,Âú®Ë®≠ÂÇôÈÉ®ÂàÜ,Áï¢Á´üÈÅ©Áî®Áî¢Ââµ10‰πã2ÁöÑÊ•≠ËÄÖÊòØ‰ª£Ë°®Âè∞ÁÅ£ÈöäÊâì„ÄåÂúãÈöõÁõÉ„Äç,ÊäïÂÖ•ÈáëÈ°ç‰∏çÈÅî100ÂÑÑÂÖÉ,ÂèØËÉΩ‰πüÊâì‰∏ç‰∫Ü„ÄÇËá≥ÊñºÂÇôÂèóÈóúÊ≥®ÁöÑÁ†îÁôºË≤ªÁî®ÈñÄÊ™ª,Á∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Ê≠∑Á∂ìËàáË≤°ÊîøÈÉ®‰æÜÂõûÂØÜÂàáË®éË´ñ,Á†îÁôºË≤ªÁî®ÈñÄÊ™ªÊúâÊúõËêΩÂú®60ÂÑÑËá≥70ÂÑÑÂÖÉ‰πãÈñì„ÄÇË≤°ÊîøÈÉ®ÂÆòÂì°ÊåáÂá∫,Á†îÁôºÊî∏ÈóúÂè∞ÁÅ£Êú™‰æÜÁ∂ìÊøüÊàêÈï∑ÂãïËÉΩ,ÈñÄÊ™ª‰∏çËÉΩ„ÄåÈ´ò‰∏çÂèØÊîÄ„Äç,Ëµ∑ÂàùÈõñË®≠ÂÆöÂú®100ÂÑÑÂÖÉ,‰πãÊâÄ‰ª•ÊúÉË™øÈôç,Ê≠£ÊòØÁõºËÆì‰ºÅÊ•≠Ë¶∫ÂæóÊúâËæ¶Ê≥ïÈÅîÂæóÂà∞ÈñÄÊ™ª„ÄÅÈÄ≤ËÄåÈÅ©Áî®ÁßüÁ®ÖÂÑ™ÊÉ†,ÊâçÊúâÂãïÂäõÁπºÁ∫åÊäïÂÖ•Á†îÁôº,Á∂≠ÊåÅÂúãÈöõ‰æõÊáâÈèàÈóúÈçµÂú∞‰Ωç„ÄÇÁ∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Âõ†Âª†ÂïÜÁ†îÁôºË≤ªÁî®Âπ≥ÂùáÁÇ∫30„ÄÅ40ÂÑÑÂÖÉ,ÂÖ∂‰∏≠,ICË®≠Ë®àÊ•≠ËÄÖ‰ªãÊñº30ÂÑÑËá≥60ÂÑÑÂÖÉÁØÑÂúç,Ëã•Â∞áÈñÄÊ™ªË®ÇÂú®100ÂÑÑÂÖÉ,Á¨¶ÂêàÊ¢ù‰ª∂ÁöÑÊ•≠ËÄÖËºÉÂ∞ë„ÄÅÂà∫ÊøÄË™òÂõ†‰∏çË∂≥;Ê≠§Â§ñ,Ëã•Á¨¶ÂêàÁî≥Ë´ãÈñÄÊ™ªÁöÑÊ•≠ËÄÖÂ¢ûÂä†,Â∞áÂèØÊèêÈ´ò‰ºÅÊ•≠Âú®Âè∞ÊäïË≥áÈáëÈ°ç,Ë≤°ÊîøÈÉ®Á®ÖÊî∂‰πüËÉΩÂõ†Ê≠§Áç≤ÂæóÊåπÊ≥®„ÄÇICË®≠Ë®àÊ•≠ËÄÖËøëÊó•È†ªÈ†ªÈáùÂ∞çÁî¢Ââµ10‰πã2ÁôºËÅ≤,Â∏åÊúõÈôç‰ΩéÈÅ©Áî®ÈñÄÊ™ª,Âä†‰∏äÂêÑÂúãÂäõÊãö‰æõÊáâÈèàËá™‰∏ªÂåñ„ÄÅÂä†Á¢ºË£úÂä©ÂçäÂ∞éÈ´îÁî¢Ê•≠,Á∂ìÊøüÈÉ®ÂÆòÂì°Ë°®Á§∫,Á∂ìÊøüÈÉ®ÂíåË≤°ÊîøÈÉ®Â∞±Áî¢Ââµ10‰πã2ÈÅîÊàêÂÖ±Ë≠ò,Áà≠ÂèñËÆìÊõ¥Â§öÊ•≠ËÄÖÂèóÊÉ†,ÁõºÂ¢ûÂº∑‰ºÅÊ•≠ÊäïË≥áÂäõÈÅìÂèäÈûèÂõ∫Âè∞ÁÅ£ÊäÄË°ìÂú∞‰Ωç„ÄÇË≤°ÊîøÈÉ®ÂÆòÂì°Ë°®Á§∫,ÁßüÁ®ÖÁçéÂãµÁöÑÂà∂ÂÆöÂøÖÈ†à„ÄåÊúâÁÇ∫ÊúâÂÆà„Äç,‰∏¶‰ª•ÈÅîÂà∞ÁçéÂãµË®≠ÁΩÆÁõÆÁöÑÁÇ∫ÊúÄÈ´òÂéüÂâá,ÁèæÈöéÊÆµÂú®Êâì„ÄåÂúãÂÖßÁõÉ„ÄçÁöÑ‰ºÅÊ•≠‰ªçÂèØÈÅ©Áî®Áî¢ÂâµÁ¨¨10Ê¢ù„ÄÅ10‰πã1ÁöÑÁßüÁ®ÖÂÑ™ÊÉ†,ÂÖ±ÂêåÂ£ØÂ§ßÂè∞ÁÅ£Á∂ìÊøüÁôºÂ±ï„ÄÇÁ∂ìÊøüÈÉ®ÂíåË≤°ÊîøÈÉ®Ê≠£Â∞±Á†îÁôºË≤ªÁî®ÈñÄÊ™ªÂÅöÊúÄÂæåÁ¢∫Ë™ç,ÂæÖ‰ªäÊòéÂÖ©Â§©È†êÂëäÂ≠êÊ≥ï‰πãÂæå,Á¥ÑÊúâ30Â§©ÊôÇÈñì,ÂèØËàáÊ•≠ÁïåÈÄ≤‰∏ÄÊ≠•Ë®éË´ñÂèäË™øÊï¥,ÁõºÁî¢Ââµ10‰πã2ËÉΩÂú®6Êúà‰∏äË∑Ø„ÄÇ\nÂè∞ÁÅ£„ÄåÁî¢Ê•≠ÂâµÊñ∞Ê¢ù‰æã„ÄçÁöÑ‰øÆÊ≠£Ê°àÔºåÂèàÁ®±„ÄåÂè∞ÁâàÊô∂ÁâáÊ≥ï„Äç,Êó®Âú®ÊîØÊåÅÂçäÂ∞éÈ´î„ÄÅÈõªÂãïËªäÂèä5GÊäÄË°ìÈ†òÂÖà‰ºÅÊ•≠ÁöÑÂâµÊñ∞„ÄÇÊñ∞ÊîøÁ≠ñÊèê‰æõÊúÄÈ´ò25%ÁöÑÊâÄÂæóÁ®ÖÊäµÂÖçÔºå‰ºÅÊ•≠ÈúÄÊªøË∂≥Áï∂Âπ¥Â∫¶Á†îÁôºË≤ªÁî®ÈÅî‰∏ÄÂÆöË¶èÊ®°„ÄÅÁ†îÁôºÂØÜÂ∫¶Ë∂ÖÈÅé6%‰ª•ÂèäÊúâÊïàÁ®ÖÁéáÈÅî‰∏ÄÂÆöÊØîÁéáÁ≠âÊ¢ù‰ª∂„ÄÇÂõ†ÊáâÂúãÈöõÈñìÂ∞ç‰ºÅÊ•≠ÊúÄ‰ΩéÁ®ÖË≤†Âà∂Â∫¶ÁöÑË™øÊï¥ÔºåÂè∞ÁÅ£ÁöÑÈñÄÊ™ªÈ†êË®àÂàÜÂà•Êñº2023Âπ¥Âíå2024Âπ¥ÈÅîÂà∞12%Âíå15%„ÄÇÁõÆÂâçÔºåËàáË≤°ÊîøÈÉ®Ë®éË´ñÂæåÔºåÁ¢∫ÂÆö100ÂÑÑÂÖÉÁöÑË®≠ÂÇôÊäïË≥áÊäµÂÖçÊòØÂèØË°åÁöÑË™òÂõ†Ôºå‰∏îÁ†îÁôºË≤ªÁî®ÈñÄÊ™ªÈ†êË®àÊúÉÂú®60Ëá≥70ÂÑÑÂÖÉ‰πãÈñì„ÄÇÂ∞çÊñºÈôç‰ΩéÈñÄÊ™ªÁöÑË¶ÅÊ±ÇÔºåÁ∂ìÊøüÈÉ®ÂíåË≤°ÊîøÈÉ®Â∑≤ÈÅîÊàêÂÖ±Ë≠òÔºåÊó®Âú®Êì¥Â§ßÂèóÊÉ†ÁØÑÂúç‰∏¶ÈºìÂãµ‰ºÅÊ•≠ÊäïË≥á‰ª•Á∂≠ÊåÅ‰æõÊáâÈèàÁöÑÈóúÈçµÂú∞‰Ωç„ÄÇÊ≠§Â§ñÔºåÈáùÂ∞çICË®≠Ë®àÊ•≠ËÄÖÁöÑË®¥Ê±ÇÔºå‰πüËÄÉÊÖÆË™øÈôçÈñÄÊ™ª„ÄÇÈ†êË®àÊ≠§Ê≥ïÊ°àÂ∞áÂú®ËøëÊúüÂÖßÂØ¶ÊñΩÔºåÁõ∏ÈóúË®éË´ñÂíåË™øÊï¥Â∞áÂú®È†êÂëäÊúüÂæåÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°å„ÄÇ\nÁøªË≠Ø\n‰ª•‰∏ãÊèê‰æõËã±ÊñáÂÖßÂÆπÔºåË´ãÂπ´ÊàëÁøªË≠ØÊàê‰∏≠Êñá„ÄÇDongshan coffee is famous for its unique position, and the constant refinement of production methods. The flavor is admired by many caffeine afficionados.\nÊù±Â±±ÂíñÂï°‰ª•ÂæóÂ§©Áç®ÂéöÁöÑÂú∞‰ΩçÔºå‰∏çÊñ∑Á≤æÁ†îÁöÑË£Ω‰ΩúÂ∑•ËóùËëóÁ®±ÔºåÈ¢®Âë≥Áç≤Ë´∏Â§öÂíñÂï°ÊÑõÂ•ΩËÄÖÊé®Â¥á„ÄÇ\n‰ΩøÁî®Ë™™Êòé\nÁØÑ‰æãÁ®ãÂºè\nÁØÑ‰æãÁ®ãÂºèËàáÊñá‰ª∂ÂæåÁ∫åÊúÉÈáãÂá∫Âà∞github\ntokenizer ÂøÖÈ†àË®≠Êàê slow modetokenizer = AutoTokenizer.from_pretrained(\"taide/TAIDE-LX-7B-Chat\", use_fast=False)\nprompt Ê®£Áâà\n‰∏ÄËà¨ÂïèÁ≠îÁî®Ê≥ïf\"<s>[INST] {question} [/INST]\"\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑËº∏ÂÖ•\nÂä†ÂÖ• system prompt ÁöÑÁî®Ê≥ïf\"<s>[INST] <<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{question} [/INST]\"\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§Ôºå‰æãÂ¶ÇÔºö‰Ω†ÊòØ‰∏ÄÂÄã‰æÜËá™Âè∞ÁÅ£ÁöÑAIÂä©ÁêÜÔºå‰Ω†ÁöÑÂêçÂ≠óÊòØ TAIDEÔºåÊ®ÇÊñº‰ª•Âè∞ÁÅ£‰∫∫ÁöÑÁ´ãÂ†¥Âπ´Âä©‰ΩøÁî®ËÄÖÔºåÊúÉÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠îÂïèÈ°å„ÄÇ\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å\nÂ§öËº™ÂïèÁ≠îÁî®Ê≥ïf\"<s>[INST] <<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{question1} [/INST] {model_answer_1} </s><s>[INST] {question2} [/INST]\"\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§\nÂ∞á {question1} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å1\nÂ∞á {model_anwer_1} ÊõøÊèõÊàêÊ®°ÂûãÁöÑÂõûÁ≠î1\nÂ∞á {question2} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å2\nHuggingface Chat Ê®£Êùø\n‰∏ÄËà¨ÂïèÁ≠îÁî®Ê≥ïchat = [\n{\"role\": \"user\", \"content\": \"{question}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑËº∏ÂÖ•\nÂä†ÂÖ• system prompt ÁöÑÁî®Ê≥ïchat = [\n{\"role\": \"system\", \"content\": \"{sys}\"},\n{\"role\": \"user\", \"content\": \"{question}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§Ôºå‰æãÂ¶ÇÔºö‰Ω†ÊòØ‰∏ÄÂÄã‰æÜËá™Âè∞ÁÅ£ÁöÑAIÂä©ÁêÜÔºå‰Ω†ÁöÑÂêçÂ≠óÊòØ TAIDEÔºåÊ®ÇÊñº‰ª•Âè∞ÁÅ£‰∫∫ÁöÑÁ´ãÂ†¥Âπ´Âä©‰ΩøÁî®ËÄÖÔºåÊúÉÁî®ÁπÅÈ´î‰∏≠ÊñáÂõûÁ≠îÂïèÈ°å„ÄÇ\nÂ∞á {question} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å\nÂ§öËº™ÂïèÁ≠îÁî®Ê≥ïchat = [\n{\"role\": \"system\", \"content\": \"{sys}\"},\n{\"role\": \"user\", \"content\": \"{question1}\"},\n{\"role\": \"assistant\", \"content\": \"{model_anwer_1}\"},\n{\"role\": \"user\", \"content\": \"{question2}\"},\n]\nprompt = tokenizer.apply_chat_template(chat)\nÂ∞á {sys} ÊõøÊèõÊàêÊåá‰ª§\nÂ∞á {question1} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å1\nÂ∞á {model_anwer_1} ÊõøÊèõÊàêÊ®°ÂûãÁöÑÂõûÁ≠î1\nÂ∞á {question2} ÊõøÊèõÊàê‰ΩøÁî®ËÄÖÁöÑÂïèÈ°å2\nË®ìÁ∑¥ÊñπÊ≥ï\nËªüÁ°¨È´îË¶èÊ†º\nÂúãÁ∂≤‰∏≠ÂøÉ H100\nË®ìÁ∑¥Ê°ÜÊû∂: PyTorch\nË≥áÊñôÂâçËôïÁêÜ\nÂ≠óÂÖÉÊ®ôÊ∫ñÂåñ\nÂéªÈô§ÈáçË¶Ü\nÂéªÈô§ÈõúË®ä\nÁ∂≤È†ÅË≥áÊñôÁöÑhtml tag„ÄÅjavascript\nÈùûÊ®ôÊ∫ñÂ≠óÂÖÉÊàñ‰∫ÇÁ¢º\nÂ≠óÊï∏ÈÅéÁü≠ÁöÑÊñáÁ´†\nÂéªÈô§ÊñáÁ´†‰∏≠ÁöÑÁâπÂÆöÊ†ºÂºèÔºåÂ¶ÇÁÇ∫ÊéíÁâàÂ¢ûÂä†ÁöÑÊèõË°å\nÂéªÈô§ÂÄãË≥áÔºåÂ¶Çemail„ÄÅÈõªË©±\nÂéªÈô§‰∏çÁï∂ÊñáÂ≠óÔºåÂ¶ÇË≥≠Âçö„ÄÅËâ≤ÊÉÖÁ≠â\nÊì¥ÂÖÖÂ≠óÂÖÉÂ≠óË©û\nÂº∑ÂåñÁπÅÈ´î‰∏≠ÊñáËº∏ÂÖ•„ÄÅËº∏Âá∫ÁöÑÊïàËÉΩÔºåÊì¥ÂÖÖË≥áÊñôÂåÖÂê´‰∏ãÂàó2ÈÉ®ÂàÜ\nÂæûÊïôËÇ≤ÈÉ®Áï∞È´îÂ≠óÂ≠óÂÖ∏Ê≠£Â≠óË°®Áç≤Âèñ‰∏≠ÊñáÂ≠óÂÖÉ\nÂæûÁπÅ‰∏≠Á∂≠Âü∫ÁôæÁßë„ÄÅÊñ∞ËÅû„ÄÅ‰∏≠Êñácommon crawlË≥áÊñôÊäΩÂèñ 500 Ëê¨Ê¢ùË∂ÖÈÅé 100 ÂÄãÂ≠óÂÖÉÁöÑÂè•Â≠ê(2.1G)ÔºåË®ìÁ∑¥‰∏≠ÊñáÂ≠óË©ûÁöÑ tokenizer\nÊåÅÁ∫åÈ†êË®ìÁ∑¥ (continuous pretraining, CP)\nË£úÂÖÖÂ§ßÈáè‰æÜÊ∫êÂèØ‰ø°Ë≥¥ÁöÑÁπÅÈ´î‰∏≠ÊñáÁü•Ë≠ò\nË∂ÖÂèÉÊï∏ (hyper parameters)\noptimizer: AdamW\nlearning rate: 1e-4\nbatch size: 1M tokens\nepoch: 1\nÂæÆË™ø (fine tune, FT)\nËÆìÊ®°ÂûãÂèØÈáùÂ∞çÁπÅÈ´î‰∏≠ÊñáÊèêÂïèÂõûÁ≠îÂïèÈ°å\nË∂ÖÂèÉÊï∏ (hyper parameters)\noptimizer: AdamW\nlearning rate: 5e-5\nbatch size: 256K tokens\nepoch: 3\nË®ìÁ∑¥Ë≥áÊñô\nÊåÅÁ∫åÈ†êË®ìÁ∑¥Ë≥áÊñô(Ë≥áÊñôÈáèÁ¥ÑÁÇ∫140G)\nË≥áÊñôÈõÜ\nË≥áÊñôÊèèËø∞\nË®¥Ë®üË≥áÊñô\n„ÄäÂè∏Ê≥ïÈô¢Ë£ÅÂà§Êõ∏„ÄãËá™2013Âπ¥1ÊúàËá≥2023Âπ¥12ÊúàÂêÑÁ¥öÊ≥ïÈô¢Ê∞ë‰∫ã„ÄÅÂàë‰∫ã„ÄÅË°åÊîøË®¥Ë®üË≥áÊñô„ÄÇ\n‰∏≠Â§ÆÁ§æ\n„Ää‰∏≠Â§ÆÁ§æ‰∏≠ÊñáÊñ∞ËÅû„ÄãË≥áÊñôÈõÜÂê´‰∏≠Â§ÆÁ§æËá™1993Âπ¥6ÊúàËá≥2023Âπ¥06ÊúàÔºåÂÖ±30Âπ¥‰ªΩ‰πãÊØèÊó•Êñ∞ËÅûÊñáÁ´†„ÄÇÂÖßÂÆπÊ∂µËìãÂúãÂÖßÂ§ñÊîøÊ≤ª„ÄÅÁ§æÊúÉ„ÄÅË≤°Á∂ì„ÄÅÊñáÊïô„ÄÅÁîüÊ¥ªÁ≠âÈ†òÂüü„ÄÇ\nETtoday Êñ∞ËÅûÈõ≤\n„ÄäETtodayÊñ∞ËÅûÈõ≤„ÄãË≥áÊñôÔºåÂåÖÂê´Ëá™2011Âπ¥10ÊúàËá≥ 2023Âπ¥12ÊúàÁöÑË≥áÊñô„ÄÇ\nÁ´ãÊ≥ïÈô¢ÂÖ¨Â†±\n„ÄäÁ´ãÊ≥ïÈô¢ÂÖ¨Â†±„ÄãÂåÖÂê´Ëá™Á¨¨8Â±ÜÁ¨¨1ÊúÉÊúüËá≥Á¨¨10Â±ÜÁ¨¨7ÊúÉÊúü‰πãÂÖ¨Â†±Ë≥áÊñô„ÄÇ\nÂá∫ÁâàÂïÜÁ∂≤Á´ôÊõ∏Á±ç‰ªãÁ¥π\nÂåÖÂê´‰∏âÈáá„ÄÅGotopÂá∫ÁâàÂïÜÁ∂≤Á´ô‰∏äÁöÑÊõ∏Á±çÁ∞°‰ªã„ÄÇ\nGRB Á†îÁ©∂Ë®àÁï´ÊëòË¶Å\nGRBÁÇ∫Êî∂ÈåÑÁî±ÊîøÂ∫úÁ∂ìË≤ªË£úÂä©‰πãÁ†îÁ©∂Ë®àÁï´ÂèäÂÖ∂ÊàêÊûúÂ†±ÂëäÁöÑË≥áË®äÁ≥ªÁµ±ÔºåÊ≠§Ë≥áÊñôÈõÜ‰∏ªË¶ÅÊî∂ÈåÑ 1993Âπ¥Ëá≥ 2023Âπ¥‰πãÁ†îÁ©∂Ë®àÁï´ÊëòË¶Å‰ª•ÂèäÁ†îÁ©∂Â†±ÂëäÊëòË¶ÅÔºåÂê´‰∏≠ÊñáÂèäÂÖ∂Ëã±ÊñáÂ∞çÁÖß„ÄÇ\nÂ≠∏Ë°ìÊúÉË≠∞Ë´ñÊñáÊëòË¶Å\nÊî∂ÈåÑ„ÄäÂ≠∏Ë°ìÊúÉË≠∞Ë´ñÊñáÊëòË¶ÅË≥áÊñôÂ∫´„Äã‰∏≠Ëá™1988Ëá≥2009Âπ¥Áî±Âè∞ÁÅ£ÊâÄËàâËæ¶‰πãÂ≠∏Ë°ìÊúÉË≠∞Ë´ñÊñá„ÄÇ\nÂÖâËèØÈõúË™å\n„ÄäÂè∞ÁÅ£ÂÖâËèØÈõúË™å„ÄãÂê´Ëá™1993Âπ¥7ÊúàËá≥2023Âπ¥6ÊúàÁöÑÊñáÁ´†ÔºåÂÖ±30Âπ¥‰ªΩ„ÄÇÂÖßÂÆπËëóÈáçÊñºÊàëÂúãÊñáÂåñ„ÄÅËßÄÂÖâËàáÊ∞ëÊÉÖÁ≠â„ÄÇ\nÊ®ÇË©ûÁ∂≤\n„ÄäÊ®ÇË©ûÁ∂≤„ÄãÊ∂µËìãÊñáÁêÜÈ†òÂüüÁ¥Ñ187Ëê¨ÂâáÂ≠∏Ë°ìÂêçË©ûÂèäÂÖ∂Ë≠ØÂêçÂ∞çÁÖß„ÄÇ\nÂêÑÈÉ®ÊúÉË≥áÊñô\nÂåÖÂê´Ë°åÊîøÈô¢„ÄåÂúãÊÉÖÁ∞°‰ªã„Äç„ÄÅÊñáÂåñÈÉ®„ÄåÂúãÂÆ∂ÊñáÂåñË®òÊÜ∂Â∫´„Äç„ÄÅÂúãÁôºÊúÉ„ÄåÊ™îÊ°àÊîØÊè¥ÊïôÂ≠∏Á∂≤„Äç„ÄÅ‰∫§ÈÄöÈÉ®„Äå‰∫§ÈÄöÂÆâÂÖ®ÂÖ•Âè£Á∂≤„ÄçÁ≠âÈÉ®ÊúÉÁ∂≤Á´ôË≥áÊñô‰πãÈÉ®ÂàÜË≥áÊñô„ÄÇ\n‰ªäÂë®Âàä\n„Ää‰ªäÂë®Âàä„ÄãÁÇ∫‰∏Ä‰ª•Ë≤°Á∂ìÁÇ∫‰∏ªÁöÑÈÄ±ÂàäÈõúË™åÔºåÊ≠§Ë≥áÊñôÈõÜÊ∂µËìã2008Âπ¥1ÊúàËá≥2023Âπ¥7ÊúàÁöÑÊñáÁ´†„ÄÇ\nÊïôËÇ≤ÈÉ®ÂúãË™ûËæ≠ÂÖ∏„ÄÅÊàêË™ûËæ≠ÂÖ∏\nÂåÖÂê´‰ª•‰∏ã‰∏âÈ†ÖË≥áÊñô:ÊïôËÇ≤ÈÉ®„ÄäÊàêË™ûÂÖ∏„ÄãÔºåÂê´5,338Ê¢ùÊàêË™ûÔºåÂÖßÂÆπÂåÖÂê´ÊØèÊ¢ùÊàêË™ûÁöÑÈáãÁæ©„ÄÅÂÖ∏ÊïÖÂéüÊñáÂèäÂÖ∂ÁôΩË©±Ë™™Êòé„ÄÅÁî®Ê≥ïË™™Êòé„ÄÅ‰æãÂè•Á≠â„ÄÇÊïôËÇ≤ÈÉ®„ÄäÈáçÁ∑®ÂúãË™ûËæ≠ÂÖ∏‰øÆË®ÇÊú¨„ÄãÔºåÊî∂ÈåÑ‰∏≠ÊñáÂñÆÂ≠óÂèäÂêÑÈ°ûËæ≠ÂΩôÔºåÂåÖÂê´ËÆÄÈü≥„ÄÅÈÉ®È¶ñ„ÄÅÈáãÁæ©Á≠âË≥áË®äÔºåÂÖ±Á¥Ñ165,539Á≠ÜË≥áÊñô„ÄÇÊïôËÇ≤ÈÉ®„ÄäÂúãË™ûËæ≠ÂÖ∏Á∞°Á∑®Êú¨„ÄãÔºåÁÇ∫„ÄäÈáçÁ∑®ÂúãË™ûËæ≠ÂÖ∏‰øÆË®ÇÊú¨„ÄãÁöÑÁ∞°Á∑®ÁâàÊú¨ÔºåÂÖ±45,247Á≠ÜË≥áÊñô„ÄÇ\nÁßëÊäÄÂ§ßËßÄÂúíË≥áÊñô\nÂê´„ÄäÁßëÊäÄÂ§ßËßÄÂúíÁ∂≤Á´ô„Äã‰∏äÁöÑÁßëÂ≠∏Êñ∞Áü•‰ª•ÂèäÁßëÊôÆÊñáÁ´†„ÄÇ\niKnow ÁßëÊäÄÁî¢Ê•≠Ë≥áË®äÂÆ§\n„ÄäÁßëÊäÄÁî¢Ê•≠Ë≥áË®äÂÆ§ÔºàiKnowÔºâ„ÄãÊèê‰æõÂè∞ÁÅ£ÂèäÂÖ®ÁêÉÁöÑÁßëÊäÄÂ∏ÇÂ†¥Ë∂®Âã¢„ÄÅÁ≠ñÁï•ÂàÜÊûê„ÄÅÂ∞àÂà©Áü•Ë≠òÔºåÂèäÊäÄË°ì‰∫§ÊòìË≥áË®äÔºåÂ∞àÊ≥®ÊñºÁßëÊäÄÁî¢Ê•≠ÁöÑÂâµÊñ∞ËàáÁôºÂ±ïÔºåÂåÖÂê´Ëá™ 2008 Âπ¥Ëá≥ 2023 Âπ¥„ÄÇ\nÁßëÂ≠∏ÁôºÂ±ïÊúàÂàä\n„ÄäÁßëÂ≠∏ÁôºÂ±ïÊúàÂàä„ÄãÁÇ∫ÂúãÁßëÊúÉÁÇ∫Êé®Âª£ÁßëÂ≠∏ÊïôËÇ≤ËÄåÂá∫ÁâàÁöÑÁßëÊôÆÂàäÁâ©ÔºåÂê´Ëá™2004Âπ¥10ÊúàËá≥2020Âπ¥12Êúà‰πãÁßëÊôÆÊñáÁ´†Ôºõ2021Âπ¥Ëµ∑Ôºå‰ª•„ÄäÁßëÊäÄÈ≠ÖÁôÆ„ÄãÂ≠£ÂàäÈáçÊñ∞Âá∫ÁôºÔºåÊèê‰æõÂúãÈöõÈóúÊ≥®ÁßëÊäÄË≠∞È°åÁöÑÊñ∞Áü•ÊñáÁ´†„ÄÇ\nÊ≥ïË¶èË≥áÊñôÂ∫´\n„ÄäÊ≥ïË¶èË≥áÊñôÂ∫´„ÄãÂê´Êà™Ëá™ 112 Âπ¥ 10 ÊúàÂêÑÊîøÂ∫úÈÉ®ÈñÄÊúÄÊñ∞ÁôºÂ∏É‰πã‰∏≠Â§ÆÊ≥ïË¶è„ÄÅÔ®àÊîøË¶èÂâá„ÄÅÊ≥ïË¶èÂëΩÔ¶®ËçâÊ°àÂèäÂú∞ÊñπËá™Ê≤ªÊ≥ïË¶èÁ≠â„ÄÇ\nÂêÑÂú∞ÊîøÂ∫úÊóÖÈÅäÁ∂≤\nÊ∂µËìãÂè∞ÁÅ£ÈÉ®ÂàÜÁ∏£Â∏ÇÂú∞ÊñπÊîøÂ∫úËßÄÂÖâÊóÖÈÅäÁ∂≤Á´ô‰∏ä‰πãÈÉ®ÂàÜË≥áÊñô„ÄÇ\nÂúãÊïôÈô¢Ë™≤Á®ãÁ∂±Ë¶Å(ÂçÅ‰∫åÂπ¥ÂúãÊïô)\nÂê´ÂçÅ‰∫åÂπ¥ÂúãÊïôË™≤Á®ãÁ∂±Ë¶Å‰πãÁ∏ΩÁ∂±‰ª•ÂèäÂêÑÁ¥öÂ≠∏Ê†°‰∏çÂêåÁßëÁõÆ‰πãË™≤Á®ãÁ∂±Ë¶Å„ÄÇ\n‰∏≠Â§ÆÁ§æË≠ØÂêçÊ™îË≥áÊñôÂ∫´\n„Ää‰∏≠Â§ÆÁ§æË≠ØÂêçÊ™îË≥áÊñôÂ∫´„ÄãËíêÈõÜ‰∏≠Â§ÆÁ§æÊñ∞ËÅûÊ•≠Âãô‰∏äÁøªË≠ØÈÅéÁöÑ‰∏≠Â§ñÂßìÊ∞è„ÄÅ‰∫∫Âêç„ÄÅÁµÑÁπî„ÄÅÂú∞ÂêçÁ≠âË≠ØÂêçÂ∞çÁÖß„ÄÇ\nÁ´•Ë©±Êõ∏\nÂÖ± 20 Êú¨Á´•Ë©±Êõ∏ÔºåÂê´ÊπØÂßÜÊ≠∑Èö™Ë®ò„ÄÅÂ∞èÈ£õ‰ø†„ÄÅÊÑõÈ∫óÁµ≤Â§¢ÈÅä‰ªôÂ¢É„ÄÅÈï∑ËÖøÂèîÂèîÁ≠â„ÄÇ\nRedPajama-Data-V2\nÂæûÂúãÂ§ñÈñãÊîæÂ§öÂúãË™ûË®ÄË™ûÊñôÂ∫´ RedPajama-Data-v2 ÂèñÂá∫Ëã±ÊñáË≥áÊñô\nMathPile-commercial\nÂúãÂ§ñÈñãÊîæÊï∏Â≠∏Ë™ûÊñôÂ∫´ MathPile-commercial\n‰∏≠ÊñáÁ∂≠Âü∫ÁôæÁßë\n„Ää‰∏≠ÊñáÁ∂≠Âü∫ÁôæÁßë„ÄãÊà™Ëá≥2023Âπ¥1ÊúàÊâÄÊúâÊ¢ùÁõÆÁöÑÂÖßÂÆπ„ÄÇ\ngithub-code-clean\nÁÇ∫ github ÈñãÊ∫êÁ®ãÂºèÁ¢ºË≥áÊñôÈõÜÔºåÂéªÈô§unlicenseÁöÑÁ®ãÂºèÁ¢ºÂíåÊñá‰ª∂„ÄÇ\nÂæÆË™øË≥áÊñô\nTAIDEÂúòÈöäË®ìÁ∑¥llama2Á≥ªÂàóÊ®°Âûã‰æÜÁî¢ÁîüÂæÆË™øË≥áÊñôË≥áÊñôÔºåÁî¢ÁîüÁöÑ‰ªªÂãôÂåÖÂê´‰∏ñÁïåÁü•Ë≠ò„ÄÅÂâµÊÑèÂØ´‰Ωú„ÄÅÊôÆÈÄöÂ∏∏Ë≠ò„ÄÅÁøªË≠Ø„ÄÅÊëòË¶Å„ÄÅÁ®ãÂºè„ÄÅÂè∞ÁÅ£ÂÉπÂÄºÁ≠âÂñÆËº™ÊàñÂ§öËº™Â∞çË©±ÂïèÁ≠îÂÖ± 128K Á≠Ü„ÄÇÂæÆË™øË≥áÊñôÂæåÁ∫åÊúÉÂ∞çÂ§ñÈáãÂá∫„ÄÇ\nÊ®°ÂûãË©ïÊ∏¨\ntaide-bench\nË©ïÊ∏¨Ë≥áÊñô\nÂØ´ÊñáÁ´†„ÄÅÂØ´‰ø°„ÄÅÊëòË¶Å„ÄÅËã±Áøª‰∏≠„ÄÅ‰∏≠ÁøªËã±ÔºåÂÖ±500È°å\nË≥áÊñôÈÄ£Áµê: taide-bench\nË©ïÊ∏¨ÊñπÊ≥ï\ngpt4Ë©ïÂàÜ\nË©ïÂàÜÁ®ãÂºè: taide-bench-eval\nË©ïÊ∏¨ÂàÜÊï∏\nÊ®°Âûã\n‰∏≠ÁøªËã±\nËã±Áøª‰∏≠\nÊëòË¶Å\nÂØ´ÊñáÁ´†\nÂØ´‰ø°\nÂπ≥Âùá\nTAIDE-LX-7B-Chat\n7.165\n7.685\n7.720\n9.635\n9.110\n8.263\nGPT3.5\n8.880\n8.810\n7.450\n9.490\n8.750\n8.676\nLLAMA2 7B\n6.075\n4.475\n5.905\n2.625\n3.040\n4.424\nLLAMA2 13B\n6.480\n6.135\n6.110\n2.565\n3.000\n4.858\nLLAMA2 70B\n6.975\n6.375\n6.795\n2.625\n2.990\n5.152\nÊéàÊ¨äÊ¢ùÊ¨æ\nTAIDE L È°ûÊ®°ÂûãÁ§æÁæ§ÊéàÊ¨äÂêåÊÑèÊõ∏\nÂÖçË≤¨ËÅ≤Êòé\nLLM Ê®°ÂûãÁî±ÊñºË®≠Ë®àÊû∂ÊßãÁöÑÈôêÂà∂Ôºå‰ª•ÂèäË≥áÊñôÈõ£ÂÖçÊúâÂÅèË™§ÔºåË™ûË®ÄÊ®°ÂûãÁöÑ‰ªª‰ΩïÂõûÊáâ‰∏ç‰ª£Ë°® TAIDE Á´ãÂ†¥Ôºå‰ΩøÁî®ÂâçÈúÄË¶ÅÈ°çÂ§ñÂä†ÂÖ•ÂÆâÂÖ®Èò≤Ë≠∑Ê©üÂà∂Ôºå‰∏îÂõûÊáâÂÖßÂÆπ‰πüÂèØËÉΩÂåÖÂê´‰∏çÊ≠£Á¢∫ÁöÑË≥áË®äÔºå‰ΩøÁî®ËÄÖË´ãÂãøÁõ°‰ø°„ÄÇ\nÈñãÁôºÂúòÈöä\nhttps://taide.tw/index/teamList\nÁõ∏ÈóúÈÄ£Áµê\nTAIDEÂÆòÁ∂≤\nTAIDE Huggingface\nTAIDE Github\nKuwa AI\nCitation\nTAIDEÂÆòÁ∂≤",
    "Qwen/CodeQwen1.5-7B": "CodeQwen1.5-7B\nIntroduction\nModel Details\nRequirements\nUsage\nCitation\nCodeQwen1.5-7B\nIntroduction\nCodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.\nStrong code generation capabilities and competitve performance across a series of benchmarks;\nSupporting long context understanding and generation with the context length of 64K tokens;\nSupporting 92 coding languages\nExcellent performance in text-to-SQL, bug fix, etc.\nFor more details, please refer to our blog post and GitHub repo.\nModel Details\nCodeQwen1.5 is based on Qwen1.5, a language model series including decoder language models of different model sizes. It is trained on 3 trillion tokens of data of codes, and it includes group query attention (GQA) for efficient inference.\nRequirements\nThe code of Qwen1.5 has been in the latest Hugging face transformers and we advise you to install transformers>=4.37.0, or you might encounter the following error:\nKeyError: 'qwen2'.\nUsage\nFor the base language model, we do not advise you to use it for chat. You can use it for finetuning, and you can also use it for code infilling, code generation, etc., but please be careful about your stopping criteria.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen,\ntitle={Qwen Technical Report},\nauthor={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\njournal={arXiv preprint arXiv:2309.16609},\nyear={2023}\n}",
    "multimodalart/sdxl_perturbed_attention_guidance": "Perturbed-Attention Guidance for SDXL\nQuickstart\nParameters\nStable Diffusion XL Demo\nPerturbed-Attention Guidance for SDXL\nThe original Perturbed-Attention Guidance for unconditional models and SD1.5 by Hyoungwon Cho is availiable at hyoungwoncho/sd_perturbed_attention_guidance\nProject / arXiv / GitHub\nThis repository is just a simple SDXL implementation of the Perturbed-Attention Guidance (PAG) on Stable Diffusion XL (SDXL) for the üß® diffusers library.\nQuickstart\nLoading Custom Pipeline:\nfrom diffusers import StableDiffusionXLPipeline\npipe = StableDiffusionXLPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\",\ncustom_pipeline=\"multimodalart/sdxl_perturbed_attention_guidance\",\ntorch_dtype=torch.float16\n)\ndevice=\"cuda\"\npipe = pipe.to(device)\nUnconditional sampling with PAG:\noutput = pipe(\n\"\",\nnum_inference_steps=50,\nguidance_scale=0.0,\npag_scale=5.0,\npag_applied_layers=['mid']\n).images\nSampling with PAG and CFG:\noutput = pipe(\n\"the spirit of a tamagotchi wandering in the city of Vienna\",\nnum_inference_steps=25,\nguidance_scale=4.0,\npag_scale=3.0,\npag_applied_layers=['mid']\n).images\nParameters\nguidance_scale : guidance scale of CFG (ex: 7.5)\npag_scale : guidance scale of PAG (ex: 4.0)\npag_applied_layers: layer to apply perturbation (ex: ['mid'])\npag_applied_layers_index : index of the layers to apply perturbation (ex: ['m0', 'm1'])\nStable Diffusion XL Demo\nTry it here",
    "toyxyz/Line2ao_sd1.5": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nLora generating images in the Ambient occlusion style.\nsd version : SD 1.5\nYou can use controlnet like Canny, lineart, Softedge, etc. to create Depth with just lineart.\nAdd 'ao, 3d' to the prompt.\nThe number after the Lora filename refers to the number of the merged lora.",
    "cais/Zephyr_RMU": "Zephyr RMU\nModel sources\nPerformance\nCitation\nZephyr RMU\nZephyr 7B model with hazardous knowledge about biosecurity and cybersecurity \"unlearned\" using Representation Misdirection for Unlearning (RMU). For more details, please check our paper.\nModel sources\nBase model: zephyr-7B-beta\nRepository: https://github.com/centerforaisafety/wmdp\nWebsite: https://www.wmdp.ai/\nCorpora used for unlearning: https://huggingface.co/datasets/cais/wmdp-corpora\nPerformance\nZephyr RMU has been evaluated on WMDP, MMLU and MT-Bench. Higher accuracy on MMLU and MT-Bench, and lower accuracy on WMDP are preferred.\nWMDP-Bio\nWMDP-Cyber\nMMLU\nMT-Bench\nZephyr 7B\n63.7\n44.0\n58.1\n7.33\nZephyr RMU\n31.2\n28.2\n57.1\n7.10\nCitation\nIf you find this useful in your research, please consider citing our paper:\n@misc{li2024wmdp,\ntitle={The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning},\nauthor={Nathaniel Li and Alexander Pan and Anjali Gopal and Summer Yue and Daniel Berrios and Alice Gatti and Justin D. Li and Ann-Kathrin Dombrowski and Shashwat Goel and Long Phan and Gabriel Mukobi and Nathan Helm-Burger and Rassin Lababidi and Lennart Justen and Andrew B. Liu and Michael Chen and Isabelle Barrass and Oliver Zhang and Xiaoyuan Zhu and Rishub Tamirisa and Bhrugu Bharathi and Adam Khoja and Zhenqi Zhao and Ariel Herbert-Voss and Cort B. Breuer and Sam Marks and Oam Patel and Andy Zou and Mantas Mazeika and Zifan Wang and Palash Oswal and Weiran Liu and Adam A. Hunt and Justin Tienken-Harder and Kevin Y. Shih and Kemper Talley and John Guan and Russell Kaplan and Ian Steneker and David Campbell and Brad Jokubaitis and Alex Levinson and Jean Wang and William Qian and Kallol Krishna Karmakar and Steven Basart and Stephen Fitz and Mindy Levine and Ponnurangam Kumaraguru and Uday Tupakula and Vijay Varadharajan and Yan Shoshitaishvili and Jimmy Ba and Kevin M. Esvelt and Alexandr Wang and Dan Hendrycks},\nyear={2024},\neprint={2403.03218},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}",
    "Orbina/Orbita-v0.1": "Orbita-v0.1\nModel Details\nUsage Examples\nExample Generations\nOpen LLM Turkish Leaderboard Evaluation Results\nOrbita-v0.1\nThis model is a Turkish Large Language Model with multiple abilites across different dimensions on the Turkish Language covering a variety of Tasks like Coding, Math, etc...\nThis model is an extended version of a Qwen-based Large Language Model (LLM) for Turkish. It was trained on a cleaned Turkish dataset carefully annotated to carry out turkish instructions in an accurate and organized manner. This model was fully finetuned extensively on 8x H100 GPU's for 2 days using a carefully annotated Turkish dataset.\nModel Details\nBase Model: Qwen 14B based LLM\nTraining Dataset: Annotated Turkish Dataset\nTraining Method: Full Finetuning\nUsage Examples\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\n\"Orbina/Orbita-v0.1\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Orbina/Orbita-v0.1\")\nprompt = \"Python'da ekrana 'Merhaba D√ºnya' nasƒ±l yazƒ±lƒ±r?\"\nmessages = [\n{\"role\": \"system\", \"content\": \"Sen, Orbina AI tarafƒ±ndan √ºretilen ve verilen talimatlarƒ± takip ederek en iyi cevabƒ± √ºretmeye √ßalƒ±≈üan yardƒ±mcƒ± bir yapay zekasƒ±n.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\nmodel_inputs.input_ids,\ntemperature=0.3,\ntop_k=50,\ntop_p=0.9,\nmax_new_tokens=512,\nrepetition_penalty=1,\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nExample Generations\nPrompt:\nT√ºrkiye'deki finansal kurumlar, yapay zeka ile birlikte gelen veri g√ºvenliƒüi risklerini nasƒ±l y√∂netiyor, 3 √∂rnek verir misin?\nCevap:\nT√ºrkiye'deki finansal kurumlar, yapay zeka (AI) teknolojilerinin getirdiƒüi veri g√ºvenliƒüi riskleri √ße≈üitli y√∂ntemlerle ele almakta ve bu riskleri azaltmaya √ßalƒ±≈ümaktadƒ±r. ƒ∞≈üte √º√ß √∂rnek:\n1. **Veri ≈ûifreleme:** Finansal kurulu≈ülar, m√º≈üteri bilgileri gibi hassas verileri ≈üifrelemeyi kullanarak veri ihlallerine kar≈üƒ± koruma saƒülar. AES (Advanced Encryption Standard), RSA (Rivest-Shamir-Adleman) veya ECC (Elliptik Eƒüri Kriptografisi) gibi g√º√ßl√º ≈üifreleme algoritmalarƒ±, verilerin okunmasƒ±nƒ± engelleyebilir.\n2. **Anomali Tespit Sistemleri:** Yapay zeka destekli anomali tespit sistemleri, beklenmedik davranƒ±≈ülarƒ± algƒ±layabilir ve g√ºvenlik ekiplerini uyarƒ±r. Bu sistemler, makine √∂ƒürenimi modelleri sayesinde zaman i√ßinde daha etkili hale gelebilir ve dolandƒ±rƒ±cƒ±lƒ±k faaliyetlerini √∂nlemede √∂nemli rol oynar.\n3. **S√ºrekli ƒ∞zleme ve G√ºvenlik Politikasƒ± Uygulama:** T√ºrkiye'deki finansal kurumlar, s√ºrekli izleme ve g√ºvenlik politikalarƒ±nƒ± uygularak veri g√ºvenliƒüini s√ºrd√ºr√ºr. Bu s√ºre√ßlerde d√ºzenli g√ºvenlik denetimleri yapƒ±lƒ±r ve t√ºm personelin g√ºvenlik konusunda eƒüitildiƒüi g√∂z √∂n√ºnde bulundurulur. Ayrƒ±ca, m√º≈üterilere de g√ºvenlik protokollerini anlatan eƒüitimler sunulur.\nBu √∂rnekler, T√ºrkiye'deki finansal kurumlarƒ±n yapay zeka ile ilgili veri g√ºvenliƒüi risklerini y√∂netmek i√ßin kullandƒ±klarƒ± bazƒ± stratejileri g√∂stermektedir. Ancak herhangi bir g√ºvenlik sistemi m√ºkemmel deƒüildir ve bu nedenle kurumlar da s√ºrekli olarak g√ºvenlik durumlarƒ±nƒ± deƒüerlendirip iyile≈ütirmeler yapmaktadƒ±rlar.\nPrompt:\nPython'da ekrana \"Merhaba D√ºnya\" nasƒ±l yazƒ±lƒ±r?\nCevap:\nPython'da ekrana \"Merhaba D√ºnya\" yazmak i√ßin `print` fonksiyonunu kullanabilirsiniz. ƒ∞≈üte basit bir √∂rnek:\n``python\nprint(\"Merhaba D√ºnya\")\n``\nBu kodu √ßalƒ±≈ütƒ±rdƒ±ƒüƒ±nƒ±zda, konsol ekranƒ±nda \"Merhaba D√ºnya\" metnini g√∂receksiniz. `print` fonksiyonu, verilen arg√ºmanƒ± (bu durumda \"Merhaba D√ºnya\") alƒ±r ve onu √ßƒ±ktƒ± olarak verir.\nOpen LLM Turkish Leaderboard Evaluation Results\nMetric\nValue\nAvg.\n49.47\nAI2 Reasoning Challenge_tr\n41.97\nHellaSwag_tr\n48.00\nMMLU_tr\n49.51\nTruthfulQA_tr\n50.78\nWinogrande _tr\n56.16\nGSM8k_tr\n50.41",
    "meta-llama/Meta-Llama-3-70B": "A newer version of this model is available:\nmeta-llama/Llama-3.1-70B\nYou need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nMETA LLAMA 3 COMMUNITY LICENSE AGREEMENT\nMeta Llama 3 Version Release Date: April 18, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Meta Llama 3 distributed by Meta at https://llama.meta.com/get-started/.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Meta Llama 3\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Meta Llama 3 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service that uses any of them, including another AI model, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Meta Llama 3‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama 3‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúMeta Llama 3 is licensed under the Meta Llama 3 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3/use-policy), which is hereby incorporated by reference into this Agreement.v. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Meta Llama 3 or derivative works thereof).\nAdditional Commercial Terms. If, on the Meta Llama 3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama 3‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Meta Llama 3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nMeta Llama 3 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Meta Llama 3. If you access or use Meta Llama 3, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3/use-policy\nProhibited Uses\nWe want everyone to use Meta Llama 3 safely and responsibly. You agree you will not use, or allow others to use, Meta Llama 3 to: 1. Violate the law or others‚Äô rights, including to:    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:        1. Violence or terrorism        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material        3. Human trafficking, exploitation, and sexual violence        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.        5. Sexual solicitation        6. Any other criminal activity    2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals    3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services    4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices    5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws    6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Meta Llama 3 related to the following:    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State    2. Guns and illegal weapons (including weapon development)    3. Illegal drugs and regulated/controlled substances    4. Operation of critical infrastructure, transportation technologies, or heavy machinery    5. Self-harm or harm to others, including suicide, cutting, and eating disorders    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual3. Intentionally deceive or mislead others, including use of Meta Llama 3 related to the following:    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content    3. Generating, promoting, or further distributing spam    4. Impersonating another individual without consent, authorization, or legal right    5. Representing that the use of Meta Llama 3 or outputs are human-generated    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement4. Fail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:    * Reporting issues with the model: https://github.com/meta-llama/llama3    * Reporting risky content generated by the model:    developers.facebook.com/llama_output_feedback    * Reporting bugs and security concerns: facebook.com/whitehat/info    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Details\nIntended Use\nHow to use\nUse with transformers\nUse with llama3\nHardware and Software\nTraining Data\nBenchmarks\nBase pretrained models\nInstruction tuned models\nResponsibility & Safety\nLlama 3-Instruct\nResponsible release\nCritical risks\nCyber Security\nChild Safety\nCommunity\nEthical Considerations and Limitations\nCitation instructions\nContributors\nModel Details\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.\nModel developers Meta\nVariations Llama 3 comes in two sizes ‚Äî 8B and 70B parameters ‚Äî in pre-trained and instruction tuned variants.\nInput Models input text only.\nOutput Models generate text and code only.\nModel Architecture Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3\nA new mix of publicly available online data.\n8B\n8k\nYes\n15T+\nMarch, 2023\n70B\n8k\nYes\nDecember, 2023\nLlama 3 family of models. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date April 18, 2024.\nStatus This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://llama.meta.com/llama3/license\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\nHow to use\nThis repository contains two versions of Meta-Llama-3-8B-Instruct, for use with transformers and with the original llama3 codebase.\nUse with transformers\nSee the snippet below for usage with Transformers:\n>>> import transformers\n>>> import torch\n>>> model_id = \"meta-llama/Meta-Llama-3-70B\"\n>>> pipeline = transformers.pipeline(\n\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n>>> pipeline(\"Hey how are you doing today?\")\nUse with llama3\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Meta-Llama-3-70B --include \"original/*\" --local-dir Meta-Llama-3-70B\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\nHardware and Software\nTraining Factors We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\nCarbon Footprint Pretraining utilized a cumulative 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\nTime (GPU hours)\nPower Consumption (W)\nCarbon Emitted(tCO2eq)\nLlama 3 8B\n1.3M\n700\n390\nLlama 3 70B\n6.4M\n700\n1900\nTotal\n7.7M\n2290\nCO2 emissions during pre-training. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\nTraining Data\nOverview Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\nData Freshness The pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively.\nBenchmarks\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see here.\nBase pretrained models\nCategory\nBenchmark\nLlama 3 8B\nLlama2 7B\nLlama2 13B\nLlama 3 70B\nLlama2 70B\nGeneral\nMMLU (5-shot)\n66.6\n45.7\n53.8\n79.5\n69.7\nAGIEval English (3-5 shot)\n45.9\n28.8\n38.7\n63.0\n54.8\nCommonSenseQA (7-shot)\n72.6\n57.6\n67.6\n83.8\n78.7\nWinogrande (5-shot)\n76.1\n73.3\n75.4\n83.1\n81.8\nBIG-Bench Hard (3-shot, CoT)\n61.1\n38.1\n47.0\n81.3\n65.7\nARC-Challenge (25-shot)\n78.6\n53.7\n67.6\n93.0\n85.3\nKnowledge reasoning\nTriviaQA-Wiki (5-shot)\n78.5\n72.1\n79.6\n89.7\n87.5\nReading comprehension\nSQuAD (1-shot)\n76.4\n72.2\n72.1\n85.6\n82.6\nQuAC (1-shot, F1)\n44.4\n39.6\n44.9\n51.1\n49.4\nBoolQ (0-shot)\n75.7\n65.5\n66.9\n79.0\n73.1\nDROP (3-shot, F1)\n58.4\n37.9\n49.8\n79.7\n70.2\nInstruction tuned models\nBenchmark\nLlama 3 8B\nLlama 2 7B\nLlama 2 13B\nLlama 3 70B\nLlama 2 70B\nMMLU (5-shot)\n68.4\n34.1\n47.8\n82.0\n52.9\nGPQA (0-shot)\n34.2\n21.7\n22.3\n39.5\n21.0\nHumanEval (0-shot)\n62.2\n7.9\n14.0\n81.7\n25.6\nGSM-8K (8-shot, CoT)\n79.6\n25.7\n77.4\n93.0\n57.5\nMATH (4-shot, CoT)\n30.0\n3.8\n6.7\n50.4\n11.6\nResponsibility & Safety\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications.\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience.\nAs part of the Llama 3 release, we updated our Responsible Use Guide to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including Meta Llama Guard 2 and Code Shield safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a reference implementation to get you started.\nLlama 3-Instruct\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case.\nSafety\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable.\nRefusals\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We‚Äôve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2.\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date.\nResponsible release\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision.\nMisuse\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at https://llama.meta.com/llama3/use-policy/.\nCritical risks\nCBRNE (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\nWe have conducted a two fold assessment of the safety of the model in this area:\nIterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\nInvolving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\nCyber Security\nWe have evaluated Llama 3 with CyberSecEval, Meta‚Äôs cybersecurity safety eval suite, measuring Llama 3‚Äôs propensity to suggest insecure code when used as a coding assistant, and Llama 3‚Äôs propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of equivalent coding capability.\nChild Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating Purple Llama solutions into your workflows and specifically Llama Guard which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety.\nPlease see the Responsible Use Guide available at http://llama.meta.com/responsible-use-guide\nCitation instructions\n@article{llama3modelcard,\ntitle={Llama 3 Model Card},\nauthor={AI@Meta},\nyear={2024},\nurl = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}\nContributors\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
    "meta-llama/Meta-Llama-Guard-2-8B": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nMETA LLAMA 3 COMMUNITY LICENSE AGREEMENT\nMeta Llama 3 Version Release Date: April 18, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Meta Llama 3 distributed by Meta at https://llama.meta.com/get-started/.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Meta Llama 3\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Meta Llama 3 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service that uses any of them, including another AI model, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Meta Llama 3‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama 3‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúMeta Llama 3 is licensed under the Meta Llama 3 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3/use-policy), which is hereby incorporated by reference into this Agreement.v. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Meta Llama 3 or derivative works thereof).\nAdditional Commercial Terms. If, on the Meta Llama 3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama 3‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Meta Llama 3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nMeta Llama 3 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Meta Llama 3. If you access or use Meta Llama 3, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://llama.meta.com/llama3/use-policy\nProhibited Uses\nWe want everyone to use Meta Llama 3 safely and responsibly. You agree you will not use, or allow others to use, Meta Llama 3 to: 1. Violate the law or others‚Äô rights, including to:    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:        1. Violence or terrorism        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material        3. Human trafficking, exploitation, and sexual violence        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.        5. Sexual solicitation        6. Any other criminal activity    2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals    3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services    4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices    5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws    6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Meta Llama 3 related to the following:    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State    2. Guns and illegal weapons (including weapon development)    3. Illegal drugs and regulated/controlled substances    4. Operation of critical infrastructure, transportation technologies, or heavy machinery    5. Self-harm or harm to others, including suicide, cutting, and eating disorders    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual3. Intentionally deceive or mislead others, including use of Meta Llama 3 related to the following:    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content    3. Generating, promoting, or further distributing spam    4. Impersonating another individual without consent, authorization, or legal right    5. Representing that the use of Meta Llama 3 or outputs are human-generated    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement4. Fail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:    * Reporting issues with the model: https://github.com/meta-llama/llama3    * Reporting risky content generated by the model:    developers.facebook.com/llama_output_feedback    * Reporting bugs and security concerns: facebook.com/whitehat/info    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Details\nHarm Taxonomy and Policy\nHow to Use in transformers\nTraining and Evaluation Data\nModel Performance\nLimitations\nCitation\nReferences\nModel Details\nMeta Llama Guard 2 is an 8B parameter Llama 3-based [1] LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM ‚Äì it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\nBelow is a response classification example input and output for Llama Guard 2.\nIn order to produce classifier scores, we look at the probability for the first token, and use that as the ‚Äúunsafe‚Äù class probability. We can then apply score thresholding to make binary decisions.\nHarm Taxonomy and Policy\nThe model is trained to predict safety labels on the 11 categories shown below, based on the MLCommons taxonomy of hazards.\nHarm categories\nS1: Violent Crimes\nS2: Non-Violent Crimes\nS3: Sex-Related Crimes\nS4: Child Sexual Exploitation\nS5: Specialized Advice\nS6: Privacy\nS7: Intellectual Property\nS8: Indiscriminate Weapons\nS9: Hate\nS10: Suicide & Self-Harm\nS11: Sexual Content\nS1: Violent Crimes\nResponses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\nS2: Non-Violent Crimes\nResponses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\nS3: Sex-Related Crimes\nResponses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\nS4: Child Sexual Exploitation\nResponses that contain, describe, enable, encourage, or endorse the sexual abuse of children\nS5: Specialized Advice\nResponses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\nS6: Privacy\nResponses that contain sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security\nS7: Intellectual Property\nResponses that may violate the intellectual property rights of any third party\nS8: Indiscriminate Weapons\nResponses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\nS9: Hate\nResponses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\nS10: Suicide & Self-Harm\nResponses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\nS11: Sexual Content\nResponses that contain erotica\nHow to Use in transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"meta-llama/Meta-Llama-Guard-2-8B\"\ndevice = \"cuda\"\ndtype = torch.bfloat16\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, device_map=device)\ndef moderate(chat):\ninput_ids = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\noutput = model.generate(input_ids=input_ids, max_new_tokens=100, pad_token_id=0)\nprompt_len = input_ids.shape[-1]\nreturn tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)\nmoderate([\n{\"role\": \"user\", \"content\": \"I forgot how to kill a process in Linux, can you help?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate.\"},\n])\n# `safe`\nTraining and Evaluation Data\nWe start with the training set of Llama Guard (see model card), and obtain labels on the Harm Taxonomy described above. To improve adaptability of the model to different prompts, we train on hard samples, which are obtained by taking an existing sample and prompting Llama2 70B to produce an alternate policy description that will flip the label of the given sample.\nWe report metrics for various models and APIs on our validation set, which is obtained from combining the validation set of Llama Guard v1 and held-out samples from the additional Llama 3 safety data.\nWe compare performance on our internal test set, as well as on open datasets like XSTest, OpenAI moderation, and BeaverTails.\nWe find that there is overlap between our training set and the BeaverTails-30k test split. Since both our internal test set and BeaverTails use prompts from the Anthropic's hh-rlhf dataset as a starting point for curating data, it is possible that different splits of Anthropic were used while creating the two datasets. Therefore to prevent leakage of signal between our train set and the BeaverTails-30k test set, we create our own BeaverTails-30k splits based on the Anthropic train-test splits used for creating our internal sets.\nNote on evaluations: As discussed in the Llama Guard paper, comparing model performance is not straightforward as each model is built on its own policy and is expected to perform better on an evaluation dataset with a policy aligned to the model. This highlights the need for industry standards. By aligning Llama Guard 2 with the Proof of Concept MLCommons taxonomy, we hope to drive adoption of industry standards like this and facilitate collaboration and transparency in the LLM safety and content evaluation space.\nModel Performance\nWe evaluate the performance of Llama Guard 2 and compare it with Llama Guard and popular content moderation APIs such as Azure, OpenAI Moderation, and Perspective. We use the token probability of the first output token (i.e. safe/unsafe) as the score for classification. For obtaining a binary classification decision from the score, we use a threshold of 0.5.\nLlama Guard 2 improves over Llama Guard, and outperforms other approaches on our internal test set. Note that we manage to achieve great performance while keeping a low false positive rate as we know that over-moderation can impact user experience when building LLM-applications.\nModel\nF1 ‚Üë\nAUPRC ‚Üë\nFalse PositiveRate ‚Üì\nLlama Guard*\n0.665\n0.854\n0.027\nLlama Guard 2\n0.915\n0.974\n0.040\nGPT4\n0.796\nN/A\n0.151\nOpenAI Moderation API\n0.347\n0.669\n0.030\nAzure Content Safety API\n0.519\nN/A\n0.245\nPerspective API\n0.265\n0.586\n0.046\nTable 1: Comparison of performance of various approaches measured on our internal test set.\n*The performance of Llama Guard is lower on our new test set due to expansion of the number of harm categories from 6 to 11, which is not aligned to what Llama Guard was trained on.\nCategory\nFalse Negative Rate* ‚Üì\nFalse Positive Rate ‚Üì\nViolent Crimes\n0.042\n0.002\nPrivacy\n0.057\n0.004\nNon-Violent Crimes\n0.082\n0.009\nIntellectual Property\n0.099\n0.004\nHate\n0.190\n0.005\nSpecialized Advice\n0.192\n0.009\nSexual Content\n0.229\n0.004\nIndiscriminate Weapons\n0.263\n0.001\nChild Exploitation\n0.267\n0.000\nSex Crimes\n0.275\n0.002\nSelf-Harm\n0.277\n0.002\nTable 2: Category-wise breakdown of false negative rate and false positive rate for Llama Guard 2 on our internal benchmark for response classification with safety labels from the ML Commons taxonomy.*The binary safe/unsafe label is used to compute categorical FNR by using the true categories. We do not penalize the model while computing FNR for cases where the model predicts the correct overall label but an incorrect categorical label.\nWe also report performance on OSS safety datasets, though we note that the policy used for assigning safety labels is not aligned with the policy used while training Llama Guard 2. Still, Llama Guard 2 provides a superior tradeoff between f1 score and False Positive Rate on the XSTest and OpenAI Moderation datasets, demonstrating good adaptability to other policies.\nThe BeaverTails dataset has a lower bar for a sample to be considered unsafe compared to Llama Guard 2's policy. The policy and training data of MDJudge [4] is more aligned with this dataset and we see that it performs better on them as expected (at the cost of a higher FPR). GPT-4 achieves high recall on all of the sets but at the cost of very high FPR (9-25%), which could hurt its ability to be used as a safeguard for practical applications.\n(F1 ‚Üë / False Positive Rate ‚Üì)\nFalse Refusals(XSTest)\nOpenAI policy(OpenAI Mod)\nBeaverTails policy(BeaverTails-30k)\nLlama Guard\n0.737 / 0.079\n0.737 / 0.079\n0.599 / 0.035\nLlama Guard 2\n0.884 / 0.084\n0.807 / 0.060\n0.736 / 0.059\nMDJudge\n0.856 / 0.172\n0.768 / 0.212\n0.849 / 0.098\nGPT4\n0.895 / 0.128\n0.842 / 0.092\n0.802 / 0.256\nOpenAI Mod API\n0.576 / 0.040\n0.788 / 0.156\n0.284 / 0.056\nTable 3: Comparison of performance of various approaches measured on our internal test set for response classification. NOTE: The policy used for training Llama Guard does not align with those used for labeling these datasets. Still, Llama Guard 2 provides a superior tradeoff between F1 score and False Positive Rate across these datasets, demonstrating strong adaptability to other policies.\nWe hope to provide developers with a high-performing moderation solution for most use cases by aligning Llama Guard 2 taxonomy with MLCommons standard. But as outlined in our Responsible Use Guide, each use case requires specific safety considerations and we encourage developers to tune Llama Guard 2 for their own use case to achieve better moderation for their custom policies. As an example of how Llama Guard 2's performance may change, we train on the BeaverTails training dataset and compare against MDJudge (which was trained on BeaverTails among others).\nModel\nF1 ‚Üë\nFalse Positive Rate ‚Üì\nLlama Guard 2\n0.736\n0.059\nMDJudge\n0.849\n0.098\nLlama Guard 2 + BeaverTails\n0.852\n0.101\nTable 4: Comparison of performance on BeaverTails-30k.\nLimitations\nThere are some limitations associated with Llama Guard 2. First, Llama Guard 2 itself is an LLM fine-tuned on Llama 3. Thus, its performance (e.g., judgments that need common sense knowledge, multilingual capability, and policy coverage) might be limited by its (pre-)training data.\nSecond, Llama Guard 2 is finetuned for safety classification only (i.e. to generate \"safe\" or \"unsafe\"), and is not designed for chat use cases. However, since it is an LLM, it can still be prompted with any text to obtain a completion.\nLastly, as an LLM, Llama Guard 2 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use. However, with the help of external components (e.g., KNN, perplexity filter), recent work (e.g., [3]) demonstrates that Llama Guard is able to detect harmful content reliably.\nNote on Llama Guard 2's policy\nLlama Guard 2 supports 11 out of the 13 categories included in the MLCommons AI Safety taxonomy. The Election and Defamation categories are not addressed by Llama Guard 2 as moderating these harm categories requires access to up-to-date, factual information sources and the ability to determine the veracity of a particular output. To support the additional categories, we recommend using other solutions (e.g. Retrieval Augmented Generation) in tandem with Llama Guard 2 to evaluate information correctness.\nCitation\n@misc{metallamaguard2,\nauthor =       {Llama Team},\ntitle =        {Meta Llama Guard 2},\nhowpublished = {\\url{https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md}},\nyear =         {2024}\n}\nReferences\n[1] Llama 3 Model Card\n[2] Llama Guard Model Card\n[3] RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content\n[4] MDJudge for Salad-Bench",
    "apple/OpenELM": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework\nUsage\nMain Results\nZero-Shot\nLLM360\nOpenLLM Leaderboard\nEvaluation\nSetup\nEvaluate OpenELM\nBias, Risks, and Limitations\nCitation\nOpenELM: An Efficient Language Model Family with Open Training and Inference Framework\nSachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari\nWe introduce OpenELM, a family of Open Efficient Language Models. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. We pretrained OpenELM models using the CoreNet library. We release both pretrained and instruction tuned models with 270M, 450M, 1.1B and 3B parameters.\nOur pre-training dataset contains RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.6, totaling approximately 1.8 trillion tokens. Please check license agreements and terms of these datasets before using them.\nSee the list below for the details of each model:\nOpenELM-270M\nOpenELM-450M\nOpenELM-1_1B\nOpenELM-3B\nOpenELM-270M-Instruct\nOpenELM-450M-Instruct\nOpenELM-1_1B-Instruct\nOpenELM-3B-Instruct\nfrom transformers import AutoModelForCausalLM\nopenelm_270m = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M\", trust_remote_code=True)\nopenelm_450m = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-450M\", trust_remote_code=True)\nopenelm_1b = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-1_1B\", trust_remote_code=True)\nopenelm_3b = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-3B\", trust_remote_code=True)\nopenelm_270m_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\nopenelm_450m_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-450M-Instruct\", trust_remote_code=True)\nopenelm_1b_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-1_1B-Instruct\", trust_remote_code=True)\nopenelm_3b_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-3B-Instruct\", trust_remote_code=True)\nUsage\nWe have provided an example function to generate output from OpenELM models loaded via HuggingFace Hub in generate_openelm.py.\nYou can try the model by running the following command:\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2\nPlease refer to this link to obtain your hugging face access token.\nAdditional arguments to the hugging face generate function can be passed via generate_kwargs. As an example, to speedup the inference, you can try lookup token speculative generation by passing the prompt_lookup_num_tokens argument as follows:\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 prompt_lookup_num_tokens=10\nAlternatively, try model-wise speculative generation with an assistive model by passing a smaller model through the assistant_model argument, for example:\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 --assistant_model [SMALLER_MODEL_NAME]\nMain Results\nZero-Shot\nModel Size\nARC-c\nARC-e\nBoolQ\nHellaSwag\nPIQA\nSciQ\nWinoGrande\nAverage\nOpenELM-270M\n26.45\n45.08\n53.98\n46.71\n69.75\n84.70\n53.91\n54.37\nOpenELM-270M-Instruct\n30.55\n46.68\n48.56\n52.07\n70.78\n84.40\n52.72\n55.11\nOpenELM-450M\n27.56\n48.06\n55.78\n53.97\n72.31\n87.20\n58.01\n57.56\nOpenELM-450M-Instruct\n30.38\n50.00\n60.37\n59.34\n72.63\n88.00\n58.96\n59.95\nOpenELM-1_1B\n32.34\n55.43\n63.58\n64.81\n75.57\n90.60\n61.72\n63.44\nOpenELM-1_1B-Instruct\n37.97\n52.23\n70.00\n71.20\n75.03\n89.30\n62.75\n65.50\nOpenELM-3B\n35.58\n59.89\n67.40\n72.44\n78.24\n92.70\n65.51\n67.39\nOpenELM-3B-Instruct\n39.42\n61.74\n68.17\n76.36\n79.00\n92.50\n66.85\n69.15\nLLM360\nModel Size\nARC-c\nHellaSwag\nMMLU\nTruthfulQA\nWinoGrande\nAverage\nOpenELM-270M\n27.65\n47.15\n25.72\n39.24\n53.83\n38.72\nOpenELM-270M-Instruct\n32.51\n51.58\n26.70\n38.72\n53.20\n40.54\nOpenELM-450M\n30.20\n53.86\n26.01\n40.18\n57.22\n41.50\nOpenELM-450M-Instruct\n33.53\n59.31\n25.41\n40.48\n58.33\n43.41\nOpenELM-1_1B\n36.69\n65.71\n27.05\n36.98\n63.22\n45.93\nOpenELM-1_1B-Instruct\n41.55\n71.83\n25.65\n45.95\n64.72\n49.94\nOpenELM-3B\n42.24\n73.28\n26.76\n34.98\n67.25\n48.90\nOpenELM-3B-Instruct\n47.70\n76.87\n24.80\n38.76\n67.96\n51.22\nOpenLLM Leaderboard\nModel Size\nARC-c\nCrowS-Pairs\nHellaSwag\nMMLU\nPIQA\nRACE\nTruthfulQA\nWinoGrande\nAverage\nOpenELM-270M\n27.65\n66.79\n47.15\n25.72\n69.75\n30.91\n39.24\n53.83\n45.13\nOpenELM-270M-Instruct\n32.51\n66.01\n51.58\n26.70\n70.78\n33.78\n38.72\n53.20\n46.66\nOpenELM-450M\n30.20\n68.63\n53.86\n26.01\n72.31\n33.11\n40.18\n57.22\n47.69\nOpenELM-450M-Instruct\n33.53\n67.44\n59.31\n25.41\n72.63\n36.84\n40.48\n58.33\n49.25\nOpenELM-1_1B\n36.69\n71.74\n65.71\n27.05\n75.57\n36.46\n36.98\n63.22\n51.68\nOpenELM-1_1B-Instruct\n41.55\n71.02\n71.83\n25.65\n75.03\n39.43\n45.95\n64.72\n54.40\nOpenELM-3B\n42.24\n73.29\n73.28\n26.76\n78.24\n38.76\n34.98\n67.25\n54.35\nOpenELM-3B-Instruct\n47.70\n72.33\n76.87\n24.80\n79.00\n38.47\n38.76\n67.96\n55.73\nSee the technical report for more results and comparison.\nEvaluation\nSetup\nInstall the following dependencies:\n# install public lm-eval-harness\nharness_repo=\"public-lm-eval-harness\"\ngit clone https://github.com/EleutherAI/lm-evaluation-harness ${harness_repo}\ncd ${harness_repo}\n# use main branch on 03-15-2024, SHA is dc90fec\ngit checkout dc90fec\npip install -e .\ncd ..\n# 66d6242 is the main branch on 2024-04-01\npip install datasets@git+https://github.com/huggingface/datasets.git@66d6242\npip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0\nEvaluate OpenELM\n# OpenELM-270M\nhf_model=apple/OpenELM-270M\n# this flag is needed because lm-eval-harness set add_bos_token to False by default, but OpenELM uses LLaMA tokenizer which requires add_bos_token to be True\ntokenizer=meta-llama/Llama-2-7b-hf\nadd_bos_token=True\nbatch_size=1\nmkdir lm_eval_output\nshot=0\ntask=arc_challenge,arc_easy,boolq,hellaswag,piqa,race,winogrande,sciq,truthfulqa_mc2\nlm_eval --model hf \\\n--model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n--tasks ${task} \\\n--device cuda:0 \\\n--num_fewshot ${shot} \\\n--output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n--batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\nshot=5\ntask=mmlu,winogrande\nlm_eval --model hf \\\n--model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n--tasks ${task} \\\n--device cuda:0 \\\n--num_fewshot ${shot} \\\n--output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n--batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\nshot=25\ntask=arc_challenge,crows_pairs_english\nlm_eval --model hf \\\n--model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n--tasks ${task} \\\n--device cuda:0 \\\n--num_fewshot ${shot} \\\n--output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n--batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\nshot=10\ntask=hellaswag\nlm_eval --model hf \\\n--model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n--tasks ${task} \\\n--device cuda:0 \\\n--num_fewshot ${shot} \\\n--output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n--batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\nBias, Risks, and Limitations\nThe release of OpenELM models aims to empower and enrich the open research community by providing access to state-of-the-art language models. Trained on publicly available datasets, these models are made available without any safety guarantees. Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts. Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.\nCitation\nIf you find our work useful, please cite:\n@article{mehtaOpenELMEfficientLanguage2024,\ntitle = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\nshorttitle = {{OpenELM}},\nurl = {https://arxiv.org/abs/2404.14619v1},\nlanguage = {en},\nurldate = {2024-04-24},\njournal = {arXiv.org},\nauthor = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\nmonth = apr,\nyear = {2024},\n}\n@inproceedings{mehta2022cvnets,\nauthor = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad},\ntitle = {CVNets: High Performance Library for Computer Vision},\nyear = {2022},\nbooktitle = {Proceedings of the 30th ACM International Conference on Multimedia},\nseries = {MM '22}\n}",
    "unsloth/llama-3-8b": "Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nFinetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!\nWe have a Google Colab Tesla T4 notebook for Llama-3 8b here: https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing\nBuilt with Meta Llama 3\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nGemma 7b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nLlama-2 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n43% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nCodeLlama 34b A100\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n27% less\nMistral 7b 1xT4\n‚ñ∂Ô∏è Start on Kaggle\n5x faster*\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF": "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\nOriginal Model Card\nModel Details\nIntended Use\nHow to use\nUse with transformers\nUse with llama3\nHardware and Software\nTraining Data\nBenchmarks\nBase pretrained models\nInstruction tuned models\nResponsibility & Safety\nCyber Security\nChild Safety\nCommunity\nEthical Considerations and Limitations\nCitation instructions\nContributors\nQuantFactory/Meta-Llama-3-8B-Instruct-GGUF\nThis is quantized version of meta-llama/Meta-Llama-3-8B-Instruct created using llama.cpp\nOriginal Model Card\nModel Details\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.\nModel developers Meta\nVariations Llama 3 comes in two sizes ‚Äî 8B and 70B parameters ‚Äî in pre-trained and instruction tuned variants.\nInput Models input text only.\nOutput Models generate text and code only.\nModel Architecture Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3\nA new mix of publicly available online data.\n8B\n8k\nYes\n15T+\nMarch, 2023\n70B\n8k\nYes\nDecember, 2023\nLlama 3 family of models. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date April 18, 2024.\nStatus This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://llama.meta.com/llama3/license\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\nHow to use\nThis repository contains two versions of Meta-Llama-3-8B-Instruct, for use with transformers and with the original llama3 codebase.\nUse with transformers\nYou can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the generate() function. Let's see examples of both.\nTransformers pipeline\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nterminators = [\npipeline.tokenizer.eos_token_id,\npipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=256,\neos_token_id=terminators,\ndo_sample=True,\ntemperature=0.6,\ntop_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][-1])\nTransformers AutoModelForCausalLM\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\nterminators = [\ntokenizer.eos_token_id,\ntokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\noutputs = model.generate(\ninput_ids,\nmax_new_tokens=256,\neos_token_id=terminators,\ndo_sample=True,\ntemperature=0.6,\ntop_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\nUse with llama3\nPlease, follow the instructions in the repository\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-8B-Instruct\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\nHardware and Software\nTraining Factors We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\nCarbon Footprint Pretraining utilized a cumulative 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\nTime (GPU hours)\nPower Consumption (W)\nCarbon Emitted(tCO2eq)\nLlama 3 8B\n1.3M\n700\n390\nLlama 3 70B\n6.4M\n700\n1900\nTotal\n7.7M\n2290\nCO2 emissions during pre-training. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\nTraining Data\nOverview Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\nData Freshness The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively.\nBenchmarks\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see here.\nBase pretrained models\nCategory\nBenchmark\nLlama 3 8B\nLlama2 7B\nLlama2 13B\nLlama 3 70B\nLlama2 70B\nGeneral\nMMLU (5-shot)\n66.6\n45.7\n53.8\n79.5\n69.7\nAGIEval English (3-5 shot)\n45.9\n28.8\n38.7\n63.0\n54.8\nCommonSenseQA (7-shot)\n72.6\n57.6\n67.6\n83.8\n78.7\nWinogrande (5-shot)\n76.1\n73.3\n75.4\n83.1\n81.8\nBIG-Bench Hard (3-shot, CoT)\n61.1\n38.1\n47.0\n81.3\n65.7\nARC-Challenge (25-shot)\n78.6\n53.7\n67.6\n93.0\n85.3\nKnowledge reasoning\nTriviaQA-Wiki (5-shot)\n78.5\n72.1\n79.6\n89.7\n87.5\nReading comprehension\nSQuAD (1-shot)\n76.4\n72.2\n72.1\n85.6\n82.6\nQuAC (1-shot, F1)\n44.4\n39.6\n44.9\n51.1\n49.4\nBoolQ (0-shot)\n75.7\n65.5\n66.9\n79.0\n73.1\nDROP (3-shot, F1)\n58.4\n37.9\n49.8\n79.7\n70.2\nInstruction tuned models\nBenchmark\nLlama 3 8B\nLlama 2 7B\nLlama 2 13B\nLlama 3 70B\nLlama 2 70B\nMMLU (5-shot)\n68.4\n34.1\n47.8\n82.0\n52.9\nGPQA (0-shot)\n34.2\n21.7\n22.3\n39.5\n21.0\nHumanEval (0-shot)\n62.2\n7.9\n14.0\n81.7\n25.6\nGSM-8K (8-shot, CoT)\n79.6\n25.7\n77.4\n93.0\n57.5\nMATH (4-shot, CoT)\n30.0\n3.8\n6.7\n50.4\n11.6\nResponsibility & Safety\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications.\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience.\nAs part of the Llama 3 release, we updated our Responsible Use Guide to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including Meta Llama Guard 2 and Code Shield safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a reference implementation to get you started.\nLlama 3-Instruct\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case.\nSafety\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable.\nRefusals\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We‚Äôve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2.\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date.\nResponsible release\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision.\nMisuse\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at https://llama.meta.com/llama3/use-policy/.\nCritical risks\nCBRNE (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\nWe have conducted a two fold assessment of the safety of the model in this area:\nIterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\nInvolving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\nCyber Security\nWe have evaluated Llama 3 with CyberSecEval, Meta‚Äôs cybersecurity safety eval suite, measuring Llama 3‚Äôs propensity to suggest insecure code when used as a coding assistant, and Llama 3‚Äôs propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of equivalent coding capability.\nChild Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating Purple Llama solutions into your workflows and specifically Llama Guard which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety.\nPlease see the Responsible Use Guide available at http://llama.meta.com/responsible-use-guide\nCitation instructions\n@article{llama3modelcard,\ntitle={Llama 3 Model Card},\nauthor={AI@Meta},\nyear={2024},\nurl = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}\nContributors\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
    "IlyaGusev/saiga_llama3_8b": "Saiga/Llama3 8B, Russian Llama-3-based chatbot\nPrompt format\nCode example\nOutput examples\nVersions\nEvaluation\nSaiga/Llama3 8B, Russian Llama-3-based chatbot\nBased on Llama-3 8B Instruct.\nLlama.cpp version: link\nColab: link\nPrompt format\n–û–°–¢–û–†–û–ñ–ù–û! WARNING! LET OP!\nI've changed the prompt format from ChatML to the original Llama-3 format in v4. Don't forget to switch formats!\nv4, v5, v6+: LLama-3 prompt format:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n–¢—ã ‚Äî –°–∞–π–≥–∞, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –ø–æ–º–æ–≥–∞–µ—à—å –∏–º.<|eot_id|><|start_header_id|>user<|end_header_id|>\n–ö–∞–∫ –¥–µ–ª–∞?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n–û—Ç–ª–∏—á–Ω–æ, –∞ —É —Ç–µ–±—è?<|eot_id|><|start_header_id|>user<|end_header_id|>\n–®–∏–∫–∞—Ä–Ω–æ. –ö–∞–∫ –ø—Ä–æ–π—Ç–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nv2, v3: ChatML prompt format:\n<|im_start|>system\n–¢—ã ‚Äî –°–∞–π–≥–∞, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –ø–æ–º–æ–≥–∞–µ—à—å –∏–º.<|im_end|>\n<|im_start|>user\n–ö–∞–∫ –¥–µ–ª–∞?<|im_end|>\n<|im_start|>assistant\n–û—Ç–ª–∏—á–Ω–æ, –∞ —É —Ç–µ–±—è?<|im_end|>\n<|im_start|>user\n–®–∏–∫–∞—Ä–Ω–æ. –ö–∞–∫ –ø—Ä–æ–π—Ç–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É?<|im_end|>\n<|im_start|>assistant\nCode example\n# –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä.\n# –ù–ï –ù–ê–î–û –¢–ê–ö –ò–ù–§–ï–†–ò–¢–¨ –ú–û–î–ï–õ–¨ –í –ü–†–û–î–ï.\n# –°–º. https://github.com/vllm-project/vllm –∏–ª–∏ https://github.com/huggingface/text-generation-inference\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nMODEL_NAME = \"IlyaGusev/saiga_llama3_8b\"\nDEFAULT_SYSTEM_PROMPT = \"–¢—ã ‚Äî –°–∞–π–≥–∞, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –ø–æ–º–æ–≥–∞–µ—à—å –∏–º.\"\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_NAME,\nload_in_8bit=True,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\nprint(generation_config)\ninputs = [\"–ü–æ—á–µ–º—É —Ç—Ä–∞–≤–∞ –∑–µ–ª–µ–Ω–∞—è?\", \"–°–æ—á–∏–Ω–∏ –¥–ª–∏–Ω–Ω—ã–π —Ä–∞—Å—Å–∫–∞–∑, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º–∏–Ω–∞—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–±—ä–µ–∫—Ç—ã. –î–∞–Ω–æ: –¢–∞–Ω—è, –º—è—á\"]\nfor query in inputs:\nprompt = tokenizer.apply_chat_template([{\n\"role\": \"system\",\n\"content\": DEFAULT_SYSTEM_PROMPT\n}, {\n\"role\": \"user\",\n\"content\": query\n}], tokenize=False, add_generation_prompt=True)\ndata = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\ndata = {k: v.to(model.device) for k, v in data.items()}\noutput_ids = model.generate(**data, generation_config=generation_config)[0]\noutput_ids = output_ids[len(data[\"input_ids\"][0]):]\noutput = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\nprint(query)\nprint(output)\nprint()\nprint(\"==============================\")\nprint()\nOutput examples\nUser: –ü–æ—á–µ–º—É —Ç—Ä–∞–≤–∞ –∑–µ–ª–µ–Ω–∞—è?\nSaiga: –¢—Ä–∞–≤–∞ –≤—ã–≥–ª—è–¥–∏—Ç –∑–µ–ª–µ–Ω—ã–º –∏–∑-–∑–∞ –Ω–∞–ª–∏—á–∏—è –≤ –Ω–µ–π —Ö–ª–æ—Ä–æ—Ñ–∏–ª–ª–∞ ‚Äì –ø–∏–≥–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —Ä–∞—Å—Ç–µ–Ω–∏—è–º –ø–æ–≥–ª–æ—â–∞—Ç—å —Å–æ–ª–Ω–µ—á–Ω—É—é —ç–Ω–µ—Ä–≥–∏—é –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑–∞.\n–•–ª–æ—Ä–æ—Ñ–∏–ª–ª –ø–æ–≥–ª–æ—â–∞–µ—Ç —Å–≤–µ—Ç–æ–≤—ã–µ –ª—É—á–∏ —Å –¥–ª–∏–Ω–æ–π –≤–æ–ª–Ω—ã 400-500 –Ω–∞–Ω–æ–º–µ—Ç—Ä–æ–≤ (–±–ª–∏–∂–µ –∫ —Å–∏–Ω–µ–º—É) –∏ –æ—Ç—Ä–∞–∂–∞–µ—Ç –ª—É—á–∏ —Å –¥–ª–∏–Ω–æ–π –≤–æ–ª–Ω—ã 600-700 –Ω–∞–Ω–æ–º–µ—Ç—Ä–æ–≤ (–±–ª–∏–∂–µ –∫ –∫—Ä–∞—Å–Ω–æ–º—É), —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –≤–∏–¥–∏–º—ã–º –∫–∞–∫ –∑–µ–ª–µ–Ω—ã–π —Ü–≤–µ—Ç.\n–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∑–µ–ª–µ–Ω—ã–π —Ü–≤–µ—Ç —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Ç–æ–≥–æ, –∫–∞–∫ —Ö–ª–æ—Ä–æ—Ñ–∏–ª–ª –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å–æ —Å–≤–µ—Ç–æ–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ä–∞—Å—Ç–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è —Ä–æ—Å—Ç–∞ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è.\nUser: –°–æ—á–∏–Ω–∏ –¥–ª–∏–Ω–Ω—ã–π —Ä–∞—Å—Å–∫–∞–∑, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º–∏–Ω–∞—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–±—ä–µ–∫—Ç—ã. –î–∞–Ω–æ: –¢–∞–Ω—è, –º—è—á\nSaiga: –í –Ω–µ–±–æ–ª—å—à–æ–π –¥–µ—Ä–µ–≤—É—à–∫–µ –ø–æ–¥ –æ—Ç–∫—Ä—ã—Ç—ã–º –Ω–µ–±–æ–º –∂–∏–ª–∞ –¥–µ–≤–æ—á–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –¢–∞–Ω—è. –û–Ω–∞ –±—ã–ª–∞ –æ—á–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ–π –∏ –ª—é–±—è—â–µ–π –∏–≥—Ä–∞—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã —Å–ø–æ—Ä—Ç–∞. –û–¥–Ω–∞–∂–¥—ã, –∫–æ–≥–¥–∞ –æ–Ω–∞ –ø—Ä–æ–≥—É–ª–∏–≤–∞–ª–∞—Å—å –ø–æ –ø–æ–ª—è–º, —É–≤–∏–¥–µ–ª–∞ –Ω–∞ –∑–µ–º–ª–µ –ª–µ–∂–∞—â–∏–π –º—è—á. –≠—Ç–æ –±—ã–ª —Å—Ç–∞—Ä—ã–π, –Ω–æ –≤—Å–µ –µ—â–µ –ø—Ä–∏–≥–æ–¥–Ω—ã–π –¥–ª—è –∏–≥—Ä—ã –º—è—á–∏–∫.\n–¢–∞–Ω—è —Å—Ä–∞–∑—É –∂–µ —Ä–µ—à–∏–ª–∞ –≤–∑—è—Ç—å –µ–≥–æ —Å–µ–±–µ –¥–æ–º–æ–π –∏ –ø–æ–∫–∞–∑–∞—Ç—å —Å–≤–æ–µ–º—É –¥—Ä—É–≥—É –ú–∏—à–µ. –ú–∏—à–∫–∞ —Ç–æ–∂–µ –æ–±–æ–∂–∞–ª —Ñ—É—Ç–±–æ–ª –∏ –≤—Å–µ–≥–¥–∞ —Ä–∞–¥ –±—ã–ª –±—ã –ø—Ä–æ–≤–µ—Å—Ç–∏ –≤—Ä–µ–º—è –≤–º–µ—Å—Ç–µ —Å–æ —Å–≤–æ–∏–º–∏ –¥—Ä—É–∑—å—è–º–∏. –ö–æ–≥–¥–∞ –æ–Ω–∏ –≤–µ—Ä–Ω—É–ª–∏—Å—å –¥–æ–º–æ–π, –¢–∞–Ω—è –ø–æ–∫–∞–∑–∞–ª–∞ –µ–º—É –Ω–∞–π–¥–µ–Ω–Ω—ã–π –º—è—á–∏–∫ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ —Å—ã–≥—Ä–∞—Ç—å –≤ —Ñ—É—Ç–±–æ–ª.\n–ú–∏—à–∫–∞ –±—ã–ª —Å—á–∞—Å—Ç–ª–∏–≤, —á—Ç–æ —É –Ω–∏—Ö –ø–æ—è–≤–∏–ª—Å—è –Ω–æ–≤—ã–π –º—è—á, –∏ –æ–Ω–∏ —Å—Ä–∞–∑—É –∂–µ –Ω–∞—á–∞–ª–∏ –≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ –º–∞—Ç—á—É. –û–Ω–∏ –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ –¥–≤–µ –∫–æ–º–∞–Ω–¥—ã –∏ –Ω–∞—á–∞–ª–∏ –∏–≥—Ä—É. –ò–≥—Ä–∞–ª–∏ –æ–Ω–∏ –¥–æ –≤–µ—á–µ—Ä–∞, –∞ –ø–æ—Ç–æ–º —Ä–µ—à–∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å –ø–µ—Ä–µ—Ä—ã–≤ –∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —É–∂–∏–Ω.\n–ü–æ—Å–ª–µ —É–∂–∏–Ω–∞ –æ–Ω–∏ –ø—Ä–æ–¥–æ–ª–∂–∏–ª–∏ –∏–≥—Ä—É, —Ç–µ–ø–µ—Ä—å —É–∂–µ —Å –±–æ–ª—å—à–∏–º —ç–Ω—Ç—É–∑–∏–∞–∑–º–æ–º –∏ —Å—Ç—Ä–∞—Å—Ç—å—é. –í –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤, –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–º —Å—Ç–∞–ª–∞ –∫–æ–º–∞–Ω–¥–∞ –¢–∞–Ω–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–±–∏–ª–∞ –±–æ–ª—å—à–µ –≥–æ–ª–æ–≤. –í—Å–µ –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å –≤–µ—Å–µ–ª—ã–º –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–º –∏ —Ö–æ—Ä–æ—à–∏–º–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ –æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.\n–ù–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å –¢–∞–Ω—è –∏ –ú–∏—à–∞ —Ä–µ—à–∏–ª–∏ –ø—Ä–∏–≥–ª–∞—Å–∏—Ç—å —Å–≤–æ–∏—Ö –¥—Ä—É–≥–∏—Ö –¥—Ä—É–∑–µ–π –Ω–∞ –∏–≥—Ä—É. –û–Ω–∏ —Å–æ–±—Ä–∞–ª–∏ –≤—Å–µ—Ö –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ –∏ –ø—Ä–æ–≤–µ–ª–∏ –µ—â—ë –æ–¥–∏–Ω –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –º–∞—Ç—á. –≠—Ç–æ—Ç –¥–µ–Ω—å —Ç–∞–∫–∂–µ —Å—Ç–∞–ª –Ω–µ–∑–∞–±—ã–≤–∞–µ–º—ã–º –±–ª–∞–≥–æ–¥–∞—Ä—è –Ω–æ–≤–æ–º—É –º—è—á—É, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–µ—Å —Ç–∞–∫ –º–Ω–æ–≥–æ —Ä–∞–¥–æ—Å—Ç–∏ –∏ —É–ª—ã–±–æ–∫.\n–ò—Ç–∞–∫, —ç—Ç–æ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π –º—è—á, –∫–æ—Ç–æ—Ä—ã–π –¢–∞–Ω—è –Ω–∞—à–ª–∞ –Ω–∞ –ø–æ–ª–µ, —Å—Ç–∞–ª –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–¥–º–µ—Ç–æ–º –¥–ª—è –∏–≥—Ä, –Ω–æ –∏ —Å–∏–º–≤–æ–ª–æ–º –¥—Ä—É–∂–±—ã –∏ –≤–µ—Å–µ–ª—å—è –º–µ–∂–¥—É –¥–µ—Ç—å–º–∏. –û–Ω –ø–æ–º–æ–≥ –∏–º —Å–æ–∑–¥–∞—Ç—å –Ω–µ–∑–∞–±—ã–≤–∞–µ–º—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏ —É–∫—Ä–µ–ø–∏–ª –∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏—è.\nVersions\nv7:\n17e1bcc9c6d3e31852a95a168f7d3778f395cd2a\nOther name: saiga_llama3_8b_abliterated_sft_m11_d7_kto_m7_d7\nSFT dataset config: sft_d7.json\nSFT model config: saiga_llama3_8b_sft_m11.json\nKTO dataset config: pref_d7.json\nKTO model config: saiga_llama3_8b_kto_m7.json\nSFT wandb: link\nKTO wandb: link\nv6:\nb662833f247ca04f1843b356e7ff3ee4aef8086a\nOther name: saiga_llama3_8b_sft_m10_d1_kto_m2_d2\nSFT dataset config: sft_d1.json\nSFT model config: saiga_llama3_8b_sft_m10.json\nKTO dataset config: pref_d2.json\nKTO model config: saiga_llama3_8b_kto_m2.json\nSFT wandb: link\nKTO wandb: link\nv5:\nd947b00c56683cd4b2f7ce707edef89318027be4\nKTO-tune over v4, dataset: lmsys_clean_ru_preferences\nwandb link\nv4:\n1cc945d4ca2c7901cf989e7edaac52ab24f1a7dd\ndataset: saiga_scored, scores >= 8, c66032920556c0f21bbbed05e7e04433ec954c3d\nwandb link\nv3:\nc588356cd60bdee54d52c2dd5a2445acca8aa5c3\ndataset: saiga_scored, scores >= 8, d51cf8060bdc90023da8cf1c3f113f9193d6569b\nwandb link\nv2:\nae61b4f9b34fac9856d361ea78c66284a00e4f0b\ndataset code revision d0d123dd221e10bb2a3383bcb1c6e4efe1b4a28a\nwandb link\n5 datasets: ru_turbo_saiga, ru_sharegpt_cleaned, oasst1_ru_main_branch, gpt_roleplay_realm, ru_instruct_gpt4\nDatasets merging script: create_short_chat_set.py\nEvaluation\nDataset: https://github.com/IlyaGusev/rulm/blob/master/self_instruct/data/tasks.jsonl\nFramework: https://github.com/tatsu-lab/alpaca_eval\nEvaluator: alpaca_eval_cot_gpt4_turbo_fn\nPivot: chatgpt_3_5_turbo\nmodel\nlength_controlled_winrate\nwin_rate\nstandard_error\navg_length\nchatgpt_4_turbo\n76.04\n90.00\n1.46\n1270\nchatgpt_3_5_turbo\n50.00\n50.00\n0.00\n536\nsaiga_llama3_8b, v6\n49.33\n68.31\n2.26\n1262\nsfr-iter-dpo\n49.11\n74.94\n2.13\n1215\nsuzume\n49.05\n71.57\n2.20\n1325\nsaiga_llama3_8b, v7\n48.95\n69.40\n2.25\n1266\nsaiga_llama3_8b, v5\n47.13\n66.18\n2.31\n1194\nsaiga_llama3_8b, v4\n43.64\n65.90\n2.31\n1200\nsaiga_llama3_8b, v3\n36.97\n61.08\n2.38\n1162\nsaiga_llama3_8b, v2\n33.07\n48.19\n2.45\n1166\nsaiga_mistral_7b\n23.38\n35.99\n2.34\n949\nPivot: sfr\nmodel\nlength_controlled_winrate\nwin_rate\nstandard_error\navg_length\nsfr\n50.00\n50.00\n0.00\n1215\nsaiga_llama3_8b, v7\n48.95\n49.16\n2.46\n1266\nsaiga_llama3_8b, v6\n46.91\n47.23\n2.45\n1262\nsuzume_8b\n43.69\n48.19\n2.46\n1325"
}