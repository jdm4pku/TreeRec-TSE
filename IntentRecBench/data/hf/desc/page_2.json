{
    "sentence-transformers/all-MiniLM-L6-v2": "all-MiniLM-L6-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nBackground\nIntended uses\nTraining procedure\nPre-training\nFine-tuning\nall-MiniLM-L6-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nBackground\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised\ncontrastive learning objective. We used the pretrained nreimers/MiniLM-L6-H384-uncased model and fine-tuned in on a\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\nWe developed this model during the\nCommunity week using JAX/Flax for NLP & CV,\norganized by Hugging Face. We developed this model as part of the project:\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\nIntended uses\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\nBy default, input text longer than 256 word pieces is truncated.\nTraining procedure\nPre-training\nWe use the pretrained nreimers/MiniLM-L6-H384-uncased model. Please refer to the model card for more detailed information about the pre-training procedure.\nFine-tuning\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\nHyper parameters\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\nTraining data\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\nDataset\nPaper\nNumber of training tuples\nReddit comments (2015-2018)\npaper\n726,484,430\nS2ORC Citation pairs (Abstracts)\npaper\n116,288,806\nWikiAnswers Duplicate question pairs\npaper\n77,427,422\nPAQ (Question, Answer) pairs\npaper\n64,371,441\nS2ORC Citation pairs (Titles)\npaper\n52,603,982\nS2ORC (Title, Abstract)\npaper\n41,769,185\nStack Exchange (Title, Body) pairs\n-\n25,316,456\nStack Exchange (Title+Body, Answer) pairs\n-\n21,396,559\nStack Exchange (Title, Answer) pairs\n-\n21,396,559\nMS MARCO triplets\npaper\n9,144,553\nGOOAQ: Open Question Answering with Diverse Answer Types\npaper\n3,012,496\nYahoo Answers (Title, Answer)\npaper\n1,198,260\nCode Search\n-\n1,151,414\nCOCO Image captions\npaper\n828,395\nSPECTER citation triplets\npaper\n684,100\nYahoo Answers (Question, Answer)\npaper\n681,164\nYahoo Answers (Title, Question)\npaper\n659,896\nSearchQA\npaper\n582,261\nEli5\npaper\n325,475\nFlickr 30k\npaper\n317,695\nStack Exchange Duplicate questions (titles)\n304,525\nAllNLI (SNLI and MultiNLI\npaper SNLI, paper MultiNLI\n277,230\nStack Exchange Duplicate questions (bodies)\n250,519\nStack Exchange Duplicate questions (titles+bodies)\n250,460\nSentence Compression\npaper\n180,000\nWikihow\npaper\n128,542\nAltlex\npaper\n112,696\nQuora Question Triplets\n-\n103,663\nSimple Wikipedia\npaper\n102,225\nNatural Questions (NQ)\npaper\n100,231\nSQuAD2.0\npaper\n87,599\nTriviaQA\n-\n73,346\nTotal\n1,170,060,424",
    "google/embeddinggemma-300m": "Access EmbeddingGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access EmbeddingGemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nEmbeddingGemma model card\nModel Information\nDescription\nInputs and outputs\nCitation\nUsage\nModel Data\nTraining Dataset\nData Preprocessing\nModel Development\nHardware\nSoftware\nEvaluation\nBenchmark Results\nPrompt Instructions\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nEmbeddingGemma model card\nModel Page: EmbeddingGemma\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nEmbeddingGemma on Kaggle\nEmbeddingGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nDescription\nEmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\nFor more technical details, refer to our paper: EmbeddingGemma: Powerful and Lightweight Text Representations.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be embedded\nMaximum input context length of 2048 tokens\nOutput:\nNumerical vector representations of input text data\nOutput embedding dimension size of 768, with smaller options available (512, 256, or 128) via Matryoshka Representation Learning (MRL). MRL allows users to truncate the output embedding of size 768 to their desired size and then re-normalize for efficient and accurate representation.\nCitation\n@article{embedding_gemma_2025,\ntitle={EmbeddingGemma: Powerful and Lightweight Text Representations},\nauthor={Schechter Vera, Henrique* and Dua, Sahil* and Zhang, Biao and Salz, Daniel and Mullins, Ryan and Raghuram Panyam, Sindhu and Smoot, Sara and Naim, Iftekhar and Zou, Joe and Chen, Feiyang and Cer, Daniel and Lisak, Alice and Choi, Min and Gonzalez, Lucas and Sanseviero, Omar and Cameron, Glenn and Ballantyne, Ian and Black, Kat and Chen, Kaifeng and Wang, Weiyi and Li, Zhe and Martins, Gus and Lee, Jinhyuk and Sherwood, Mark and Ji, Juyeong and Wu, Renjie and Zheng, Jingxiao and Singh, Jyotinder and Sharma, Abheesht and Sreepat, Divya and Jain, Aashi and Elarabawy, Adham and Co, AJ and Doumanoglou, Andreas and Samari, Babak and Hora, Ben and Potetz, Brian and Kim, Dahun and Alfonseca, Enrique and Moiseev, Fedor and Han, Feng and Palma Gomez, Frank and Hern√°ndez √Åbrego, Gustavo and Zhang, Hesen and Hui, Hui and Han, Jay and Gill, Karan and Chen, Ke and Chen, Koert and Shanbhogue, Madhuri and Boratko, Michael and Suganthan, Paul and Duddu, Sai Meher Karthik and Mariserla, Sandeep and Ariafar, Setareh and Zhang, Shanfeng and Zhang, Shijie and Baumgartner, Simon and Goenka, Sonam and Qiu, Steve and Dabral, Tanmaya and Walker, Trevor and Rao, Vikram and Khawaja, Waleed and Zhou, Wenlei and Ren, Xiaoqi and Xia, Ye and Chen, Yichang and Chen, Yi-Ting and Dong, Zhe and Ding, Zhongli and Visin, Francesco and Liu, Ga√´l and Zhang, Jiageng and Kenealy, Kathleen and Casbon, Michelle and Kumar, Ravin and Mesnard, Thomas and Gleicher, Zach and Brick, Cormac and Lacombe, Olivier and Roberts, Adam and Sung, Yunhsuan and Hoffmann, Raphael and Warkentin, Tris and Joulin, Armand and Duerig, Tom and Seyedhosseini, Mojtaba},\npublisher={Google DeepMind},\nyear={2025},\nurl={https://arxiv.org/abs/2509.20354}\n}\nUsage\nThese model weights are designed to be used with Sentence Transformers, using the Gemma 3 implementation from Hugging Face Transformers as the backbone.\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"google/embeddinggemma-300m\")\n# Run inference with queries and documents\nquery = \"Which planet is known as the Red Planet?\"\ndocuments = [\n\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\nquery_embeddings = model.encode_query(query)\ndocument_embeddings = model.encode_document(documents)\nprint(query_embeddings.shape, document_embeddings.shape)\n# (768,) (4, 768)\n# Compute similarities to determine a ranking\nsimilarities = model.similarity(query_embeddings, document_embeddings)\nprint(similarities)\n# tensor([[0.3011, 0.6359, 0.4930, 0.4889]])\nNOTE: EmbeddingGemma activations do not support float16. Please use float32 or bfloat16 as appropriate for your hardware.\nModel Data\nTraining Dataset\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\nCode and Technical Documents: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\nSynthetic and Task-Specific Data: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training data:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with our policies.\nModel Development\nHardware\nEmbeddingGemma was trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5e), for more details refer to the Gemma 3 model card.\nSoftware\nTraining was done using JAX and ML Pathways. For more details refer to the Gemma 3 model card.\nEvaluation\nBenchmark Results\nThe model was evaluated against a large collection of different datasets and metrics to cover different aspects of text understanding.\nFull Precision Checkpoint\nMTEB (Multilingual, v2)\nDimensionality\nMean (Task)\nMean (TaskType)\n768d\n61.15\n54.31\n512d\n60.71\n53.89\n256d\n59.68\n53.01\n128d\n58.23\n51.77\nMTEB (English, v2)\nDimensionality\nMean (Task)\nMean (TaskType)\n768d\n69.67\n65.11\n512d\n69.18\n64.59\n256d\n68.37\n64.02\n128d\n66.66\n62.70\nMTEB (Code, v1)\nDimensionality\nMean (Task)\nMean (TaskType)\n768d\n68.76\n68.76\n512d\n68.48\n68.48\n256d\n66.74\n66.74\n128d\n62.96\n62.96\nQAT Checkpoints\nMTEB (Multilingual, v2)\nQuant config (dimensionality)\nMean (Task)\nMean (TaskType)\nQ4_0 (768d)\n60.62\n53.61\nQ8_0 (768d)\n60.93\n53.95\nMixed Precision* (768d)\n60.69\n53.82\nMTEB (English, v2)\nQuant config (dimensionality)\nMean (Task)\nMean (TaskType)\nQ4_0 (768d)\n69.31\n64.65\nQ8_0 (768d)\n69.49\n64.84\nMixed Precision* (768d)\n69.32\n64.82\nMTEB (Code, v1)\nQuant config (dimensionality)\nMean (Task)\nMean (TaskType)\nQ4_0 (768d)\n67.99\n67.99\nQ8_0 (768d)\n68.70\n68.70\nMixed Precision* (768d)\n68.03\n68.03\nNote: QAT models are evaluated after quantization\n* Mixed Precision refers to per-channel quantization with int4 for embeddings, feedforward, and projection layers, and int8 for attention (e4_a8_f4_p4).\nPrompt Instructions\nEmbeddingGemma can generate optimized embeddings for various use cases‚Äîsuch as document retrieval, question answering, and fact verification‚Äîor for specific input types‚Äîeither a query or a document‚Äîusing prompts that are prepended to the input strings.\nQuery prompts follow the form task: {task description} | query:  where the task description varies by the use case, with the default task description being search result. Document-style prompts follow the form title: {title | \"none\"} | text:  where the title is either none (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting.\nUse the following prompts based on your use case and input data type. These may already be available in the EmbeddingGemma configuration in your modeling framework of choice.\nUse Case (task type enum)\nDescriptions\nRecommended Prompt\nRetrieval (Query)\nUsed to generate embeddings that are optimized for document search or information retrieval\ntask: search result | query: {content}\nRetrieval (Document)\ntitle: {title | \"none\"} | text: {content}\nQuestion Answering\ntask: question answering | query: {content}\nFact Verification\ntask: fact checking | query: {content}\nClassification\nUsed to generate embeddings that are optimized to classify texts according to preset labels\ntask: classification | query: {content}\nClustering\nUsed to generate embeddings that are optimized to cluster texts based on their similarities\ntask: clustering | query: {content}\nSemantic Similarity\nUsed to generate embeddings that are optimized to assess text similarity. This is not intended for retrieval use cases.\ntask: sentence similarity | query: {content}\nCode Retrieval\nUsed to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using retrieval_document.\ntask: code retrieval | query: {content}\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen embedding models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\nSemantic Similarity: Embeddings optimized to assess text similarity, such as recommendation systems and duplicate detection\nClassification: Embeddings optimized to classify texts according to preset labels, such as sentiment analysis and spam detection\nClustering: Embeddings optimized to cluster texts based on their similarities, such as document organization, market research, and anomaly detection\nRetrieval\nDocument: Embeddings optimized for document search, such as indexing articles, books, or web pages for search\nQuery: Embeddings optimized for general search queries, such as custom search\nCode Query: Embeddings optimized for retrieval of code blocks based on natural language queries, such as code suggestions and search\nQuestion Answering: Embeddings for questions in a question-answering system, optimized for finding documents that answer the question, such as chatbox.\nFact Verification: Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement, such as automated fact-checking systems.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can handle effectively.\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language.\nEthical Considerations and Risks\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.\nMisuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of embeddings. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the Gemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open embedding model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown superior performance to other, comparably-sized open model alternatives.",
    "inclusionAI/LLaDA2.0-mini-preview": "LLaDA2.0-mini-preview\nüöÄ Performance Highlights\nüó∫Ô∏è What's Next\nüì¶ Model Variants\nüîç Model Overview\nü§ó Hugging Face Transformers\nBest Practices\nüåê License\nü§ù Contact & Collaboration\nLLaDA2.0-mini-preview\nLLaDA2.0-mini-preview is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.\nBenchmark\nLing-mini-2.0\nLLaDA-MoE-7B-A1B-Instruct\nLLaDA2.0-mini-preview\nAverage\n68.98\n59.72\n66.89\nKnowledge\nMMLU\n78.75\n67.18\n72.49\nMMLU-PRO\n56.40\n44.64\n49.22\nCMMLU\n77.84\n64.30\n67.53\nC-EVAL\n77.85\n63.93\n66.54\nReasoning\nsquad2.0\n69.14\n86.81\n85.61\ndrop\n76.35\n79.77\n79.49\nkorbench\n51.04\n38.40\n37.26\nCoding\nCruxEval-O\n71.12\n42.38\n61.88\nmbpp\n81.03\n70.02\n77.75\nMultiPL-E\n62.23\n52.53\n62.43\nhumaneval\n77.44\n61.59\n80.49\nBigcodebench-Full\n35.88\n20.44\n30.44\nMath\nGSM8K\n91.58\n82.41\n89.01\nmath\n82.22\n58.68\n73.50\nAgent & Alignment\nBFCL_Live\n45.74\n63.09\n74.11\nIFEval-strict -prompt\n69.13\n59.33\n62.50\nüöÄ Performance Highlights\nLeading MoE Architecture:\nThe open-source Mixture-of-Experts (MoE) diffusion large language model, pre-trained from scratch on approximately 20 trillion tokens.\nEfficient Inference:\nWith 16 billion total parameters, only 1.4 billion are activated during inference. LLaDA2.0-mini-preview significantly reduces computational costs while outperforming open-source dense models of similar scale.\nImpressive Performance on Code & Complex Reasoning:\nExcels in tasks such as code generation and advanced mathematical reasoning, demonstrating strong reasoning capabilities.\nTool Use:\nSupports tool calling and achieves excellent performance in complex agent-based tasks.\nOpen & Extensible:\nFully open-source with commitment to transparency. We plan to release a leading inference framework in the future and continue investing in cutting-edge areas like diffusion LLMs (dLLM) to drive disruptive innovation.\nüó∫Ô∏è What's Next\nSupercharged Reasoning with LLaDA 2.0: LLaDA 2.0 series will be fine-tuned with Reinforcement Learning, unlocking a new level of sophisticated reasoning and problem-solving abilities.\nTools for Innovators: we will release a detailed tutorial and our complete post-training framework. Whether you want to master the current model or build your own customized versions, you'll have the tools you need. Stay tuned\nüì¶ Model Variants\nModel ID\nDescription\nHugging Face Link\ninclusionAI/LLaDA2.0-mini-preview\nInstruction-tuned model, ready for downstream applications.\nü§ó Model Card\ninclusionAI/LLaDA2.0-flash-preview\nInstruction-tuned model, ready for downstream applications.\nü§ó Model Card\nüîç Model Overview\nLLaDA2.0-mini-preview has the following specifications:\nType: Mixture-of-Experts (MoE) Diffusion Language Model\nTotal Parameters (Non-Embedding): 16B\nNumber of Layers: 20\nAttention Heads: 16\nContext Length: 4,096 tokens\nPosition Embedding: Rotary (RoPE)\nVocabulary Size: 157,184\nü§ó Hugging Face Transformers\nMake sure you have transformers and its dependencies installed:\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\nmodel_path = \"/path/to/LLaDA2.0-mini-preview\"\ndevice = \"cuda:0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path, trust_remote_code=True, device_map=device\n)\nmodel = model.to(torch.bfloat16)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nprompt = \"Why does Camus think that Sisyphus is happy?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\ntokenize=True,\nreturn_tensors=\"pt\",\n)\ngenerated_tokens = model.generate(\ninputs=input_ids,\neos_early_stop=True,\ngen_length=512,\nblock_length=32,\nsteps=32,\ntemperature=0.0,\n)\ngenerated_answer = tokenizer.decode(\ngenerated_tokens[0],\nskip_special_tokens=True,\n)\nprint(generated_answer)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.0, block_length=32, and steps=32. Using a higher temperature value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length:\nWe recommend using an output length of 2048 tokens for most queries. For benchmarking on problems require more output length, such as those found in math and programming competitions, we suggest setting the max output length to 4096 tokens.\nüåê License\nThis project is licensed under the terms of the Apache License 2.0.\nü§ù Contact & Collaboration\nFor questions, collaborations, or feedback, please reach out via Hugging Face or open an issue in the repository.\nüëâ Join us in advancing open, efficient, and intelligent language models!",
    "Qwen/Qwen3-Embedding-0.6B": "Qwen3-Embedding-0.6B\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nSentence Transformers Usage\nTransformers Usage\nvLLM Usage\nText Embeddings Inference (TEI) Usage\nEvaluation\nMTEB (Multilingual)\nMTEB (Eng v2)\nC-MTEB (MTEB Chinese)\nCitation\nQwen3-Embedding-0.6B\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Embedding-0.6B has the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 0.6B\nContext Length: 32k\nEmbedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\nKeyError: 'qwen3'\nSentence Transformers Usage\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\nfrom sentence_transformers import SentenceTransformer\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to \"left\":\n# model = SentenceTransformer(\n#     \"Qwen/Qwen3-Embedding-0.6B\",\n#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n#     tokenizer_kwargs={\"padding_side\": \"left\"},\n# )\n# The queries and documents to embed\nqueries = [\n\"What is the capital of China?\",\n\"Explain gravity\",\n]\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7646, 0.1414],\n#         [0.1355, 0.6000]])\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7645568251609802, 0.14142508804798126], [0.13549736142158508, 0.5999549627304077]]\nvLLM Usage\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\nmodel = LLM(model=\"Qwen/Qwen3-Embedding-0.6B\", task=\"embed\")\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7620252966880798, 0.14078938961029053], [0.1358368694782257, 0.6013815999031067]]\nüìå Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nText Embeddings Inference (TEI) Usage\nYou can either run / deploy TEI on NVIDIA GPUs as:\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B --dtype float16\nOr on CPU devices as:\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B\nAnd then, generate the embeddings sending a HTTP POST request as:\ncurl http://localhost:8080/embed \\\n-X POST \\\n-d '{\"inputs\": [\"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\", \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"]}' \\\n-H \"Content-Type: application/json\"\nEvaluation\nMTEB (Multilingual)\nModel\nSize\nMean (Task)\nMean (Type)\nBitxt Mining\nClass.\nClust.\nInst. Retri.\nMulti. Class.\nPair. Class.\nRerank\nRetri.\nSTS\nNV-Embed-v2\n7B\n56.29\n49.58\n57.84\n57.29\n40.80\n1.04\n18.63\n78.94\n63.82\n56.72\n71.10\nGritLM-7B\n7B\n60.92\n53.74\n70.53\n61.83\n49.75\n3.45\n22.77\n79.94\n63.78\n58.31\n73.33\nBGE-M3\n0.6B\n59.56\n52.18\n79.11\n60.35\n40.88\n-3.11\n20.1\n80.76\n62.79\n54.60\n74.12\nmultilingual-e5-large-instruct\n0.6B\n63.22\n55.08\n80.13\n64.94\n50.75\n-0.40\n22.91\n80.86\n62.61\n57.12\n76.81\ngte-Qwen2-1.5B-instruct\n1.5B\n59.45\n52.69\n62.51\n58.32\n52.05\n0.74\n24.02\n81.58\n62.58\n60.78\n71.61\ngte-Qwen2-7b-Instruct\n7B\n62.51\n55.93\n73.92\n61.55\n52.77\n4.94\n25.48\n85.13\n65.55\n60.08\n73.98\ntext-embedding-3-large\n-\n58.93\n51.41\n62.17\n60.27\n46.89\n-2.68\n22.03\n79.17\n63.89\n59.27\n71.68\nCohere-embed-multilingual-v3.0\n-\n61.12\n53.23\n70.50\n62.95\n46.89\n-1.89\n22.74\n79.88\n64.07\n59.16\n74.80\nGemini Embedding\n-\n68.37\n59.59\n79.28\n71.82\n54.59\n5.18\n29.16\n83.63\n65.58\n67.71\n79.40\nQwen3-Embedding-0.6B\n0.6B\n64.33\n56.00\n72.22\n66.83\n52.33\n5.09\n24.59\n80.83\n61.41\n64.64\n76.17\nQwen3-Embedding-4B\n4B\n69.45\n60.86\n79.36\n72.33\n57.15\n11.56\n26.77\n85.05\n65.08\n69.60\n80.86\nQwen3-Embedding-8B\n8B\n70.58\n61.69\n80.89\n74.00\n57.65\n10.06\n28.66\n86.40\n65.63\n70.88\n81.08\nNote: For compared models, the scores are retrieved from MTEB online leaderboard on May 24th, 2025.\nMTEB (Eng v2)\nMTEB English / Models\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetri.\nSTS\nSumm.\nmultilingual-e5-large-instruct\n0.6B\n65.53\n61.21\n75.54\n49.89\n86.24\n48.74\n53.47\n84.72\n29.89\nNV-Embed-v2\n7.8B\n69.81\n65.00\n87.19\n47.66\n88.69\n49.61\n62.84\n83.82\n35.21\nGritLM-7B\n7.2B\n67.07\n63.22\n81.25\n50.82\n87.29\n49.59\n54.95\n83.03\n35.65\ngte-Qwen2-1.5B-instruct\n1.5B\n67.20\n63.26\n85.84\n53.54\n87.52\n49.25\n50.25\n82.51\n33.94\nstella_en_1.5B_v5\n1.5B\n69.43\n65.32\n89.38\n57.06\n88.02\n50.19\n52.42\n83.27\n36.91\ngte-Qwen2-7B-instruct\n7.6B\n70.72\n65.77\n88.52\n58.97\n85.9\n50.47\n58.09\n82.69\n35.74\ngemini-embedding-exp-03-07\n-\n73.3\n67.67\n90.05\n59.39\n87.7\n48.59\n64.35\n85.29\n38.28\nQwen3-Embedding-0.6B\n0.6B\n70.70\n64.88\n85.76\n54.05\n84.37\n48.18\n61.83\n86.57\n33.43\nQwen3-Embedding-4B\n4B\n74.60\n68.10\n89.84\n57.51\n87.01\n50.76\n68.46\n88.72\n34.39\nQwen3-Embedding-8B\n8B\n75.22\n68.71\n90.43\n58.57\n87.52\n51.56\n69.44\n88.58\n34.83\nC-MTEB (MTEB Chinese)\nC-MTEB\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nmultilingual-e5-large-instruct\n0.6B\n58.08\n58.24\n69.80\n48.23\n64.52\n57.45\n63.65\n45.81\nbge-multilingual-gemma2\n9B\n67.64\n75.31\n59.30\n86.67\n68.28\n73.73\n55.19\n-\ngte-Qwen2-1.5B-instruct\n1.5B\n67.12\n67.79\n72.53\n54.61\n79.5\n68.21\n71.86\n60.05\ngte-Qwen2-7B-instruct\n7.6B\n71.62\n72.19\n75.77\n66.06\n81.16\n69.24\n75.70\n65.20\nritrieve_zh_v1\n0.3B\n72.71\n73.85\n76.88\n66.5\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-0.6B\n0.6B\n66.33\n67.45\n71.40\n68.74\n76.42\n62.58\n71.03\n54.52\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "rednote-hilab/dots.ocr": "Quick Start\n1. Installation\nInstall dots.ocr\nDownload Model Weights\n2. Deployment\nvLLM inference\nHugginface inference\n3. Document Parse\n4. Demo\nExample for formula document\nExample for table document\nExample for multilingual document\nExample for reading order\nExample for grounding ocr\nAcknowledgments\nLimitation & Future Work\ndots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model\nüñ•Ô∏è Live Demo |\nüí¨ WeChat |\nüìï rednote\nIntroduction\ndots.ocr is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance.\nPowerful Performance: dots.ocr achieves SOTA performance for text, tables, and reading order on OmniDocBench, while delivering formula recognition results comparable to much larger models like Doubao-1.5 and gemini2.5-pro.\nMultilingual Support: dots.ocr demonstrates robust parsing capabilities for low-resource languages, achieving decisive advantages across both layout detection and content recognition on our in-house multilingual documents benchmark.\nUnified and Simple Architecture: By leveraging a single vision-language model, dots.ocr offers a significantly more streamlined architecture than conventional methods that rely on complex, multi-model pipelines. Switching between tasks is accomplished simply by altering the input prompt, proving that a VLM can achieve competitive detection results compared to traditional detection models like DocLayout-YOLO.\nEfficient and Fast Performance: Built upon a compact 1.7B LLM, dots.ocr provides faster inference speeds than many other high-performing models based on larger foundations.\nUsage with transformers\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\nmodel_path = \"./weights/DotsOCR\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\nattn_implementation=\"flash_attention_2\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nimage_path = \"demo/demo_image1.jpg\"\nprompt = \"\"\"Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.\n1. Bbox format: [x1, y1, x2, y2]\n2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].\n3. Text Extraction & Formatting Rules:\n- Picture: For the 'Picture' category, the text field should be omitted.\n- Formula: Format its text as LaTeX.\n- Table: Format its text as HTML.\n- All Others (Text, Title, etc.): Format their text as Markdown.\n4. Constraints:\n- The output text must be the original text from the image, with no translation.\n- All layout elements must be sorted according to human reading order.\n5. Final Output: The entire output must be a single JSON object.\n\"\"\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": image_path\n},\n{\"type\": \"text\", \"text\": prompt}\n]\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nPerformance Comparison: dots.ocr vs. Competing Models\nNotes:\nThe EN, ZH metrics are the end2end evaluation results of OmniDocBench, and Multilingual metric is the end2end evaluation results of dots.ocr-bench.\nNews\n2025.07.30  üöÄ We release dots.ocr, ‚Äî a multilingual documents parsing model based on 1.7b llm, with SOTA performance.\nBenchmark Results\n1. OmniDocBench\nThe end-to-end evaluation results of different tasks.\nModelType\nMethods\nOverallEdit‚Üì\nTextEdit‚Üì\nFormulaEdit‚Üì\nTableTEDS‚Üë\nTableEdit‚Üì\nRead OrderEdit‚Üì\nEN\nZH\nEN\nZH\nEN\nZH\nEN\nZH\nEN\nZH\nEN\nZH\nPipelineTools\nMinerU\n0.150\n0.357\n0.061\n0.215\n0.278\n0.577\n78.6\n62.1\n0.180\n0.344\n0.079\n0.292\nMarker\n0.336\n0.556\n0.080\n0.315\n0.530\n0.883\n67.6\n49.2\n0.619\n0.685\n0.114\n0.340\nMathpix\n0.191\n0.365\n0.105\n0.384\n0.306\n0.454\n77.0\n67.1\n0.243\n0.320\n0.108\n0.304\nDocling\n0.589\n0.909\n0.416\n0.987\n0.999\n1\n61.3\n25.0\n0.627\n0.810\n0.313\n0.837\nPix2Text\n0.320\n0.528\n0.138\n0.356\n0.276\n0.611\n73.6\n66.2\n0.584\n0.645\n0.281\n0.499\nUnstructured\n0.586\n0.716\n0.198\n0.481\n0.999\n1\n0\n0.06\n1\n0.998\n0.145\n0.387\nOpenParse\n0.646\n0.814\n0.681\n0.974\n0.996\n1\n64.8\n27.5\n0.284\n0.639\n0.595\n0.641\nPPStruct-V3\n0.145\n0.206\n0.058\n0.088\n0.295\n0.535\n-\n-\n0.159\n0.109\n0.069\n0.091\nExpertVLMs\nGOT-OCR\n0.287\n0.411\n0.189\n0.315\n0.360\n0.528\n53.2\n47.2\n0.459\n0.520\n0.141\n0.280\nNougat\n0.452\n0.973\n0.365\n0.998\n0.488\n0.941\n39.9\n0\n0.572\n1.000\n0.382\n0.954\nMistral OCR\n0.268\n0.439\n0.072\n0.325\n0.318\n0.495\n75.8\n63.6\n0.600\n0.650\n0.083\n0.284\nOLMOCR-sglang\n0.326\n0.469\n0.097\n0.293\n0.455\n0.655\n68.1\n61.3\n0.608\n0.652\n0.145\n0.277\nSmolDocling-256M\n0.493\n0.816\n0.262\n0.838\n0.753\n0.997\n44.9\n16.5\n0.729\n0.907\n0.227\n0.522\nDolphin\n0.206\n0.306\n0.107\n0.197\n0.447\n0.580\n77.3\n67.2\n0.180\n0.285\n0.091\n0.162\nMinerU 2\n0.139\n0.240\n0.047\n0.109\n0.297\n0.536\n82.5\n79.0\n0.141\n0.195\n0.069<\n0.118\nOCRFlux\n0.195\n0.281\n0.064\n0.183\n0.379\n0.613\n71.6\n81.3\n0.253\n0.139\n0.086\n0.187\nMonkeyOCR-pro-3B\n0.138\n0.206\n0.067\n0.107\n0.246\n0.421\n81.5\n87.5\n0.139\n0.111\n0.100\n0.185\nGeneralVLMs\nGPT4o\n0.233\n0.399\n0.144\n0.409\n0.425\n0.606\n72.0\n62.9\n0.234\n0.329\n0.128\n0.251\nQwen2-VL-72B\n0.252\n0.327\n0.096\n0.218\n0.404\n0.487\n76.8\n76.4\n0.387\n0.408\n0.119\n0.193\nQwen2.5-VL-72B\n0.214\n0.261\n0.092\n0.18\n0.315\n0.434\n82.9\n83.9\n0.341\n0.262\n0.106\n0.168\nGemini2.5-Pro\n0.148\n0.212\n0.055\n0.168\n0.356\n0.439\n85.8\n86.4\n0.13\n0.119\n0.049\n0.121\ndoubao-1-5-thinking-vision-pro-250428\n0.140\n0.162\n0.043\n0.085\n0.295\n0.384\n83.3\n89.3\n0.165\n0.085\n0.058\n0.094\nExpert VLMs\ndots.ocr\n0.125\n0.160\n0.032\n0.066\n0.329\n0.416\n88.6\n89.0\n0.099\n0.092\n0.040\n0.067\nThe end-to-end text recognition performance across 9 PDF page types.\nModelType\nModels\nBook\nSlides\nFinancialReport\nTextbook\nExamPaper\nMagazine\nAcademicPapers\nNotes\nNewspaper\nOverall\nPipelineTools\nMinerU\n0.055\n0.124\n0.033\n0.102\n0.159\n0.072\n0.025\n0.984\n0.171\n0.206\nMarker\n0.074\n0.340\n0.089\n0.319\n0.452\n0.153\n0.059\n0.651\n0.192\n0.274\nMathpix\n0.131\n0.220\n0.202\n0.216\n0.278\n0.147\n0.091\n0.634\n0.690\n0.300\nExpertVLMs\nGOT-OCR\n0.111\n0.222\n0.067\n0.132\n0.204\n0.198\n0.179\n0.388\n0.771\n0.267\nNougat\n0.734\n0.958\n1.000\n0.820\n0.930\n0.830\n0.214\n0.991\n0.871\n0.806\nDolphin\n0.091\n0.131\n0.057\n0.146\n0.231\n0.121\n0.074\n0.363\n0.307\n0.177\nOCRFlux\n0.068\n0.125\n0.092\n0.102\n0.119\n0.083\n0.047\n0.223\n0.536\n0.149\nMonkeyOCR-pro-3B\n0.084\n0.129\n0.060\n0.090\n0.107\n0.073\n0.050\n0.171\n0.107\n0.100\nGeneralVLMs\nGPT4o\n0.157\n0.163\n0.348\n0.187\n0.281\n0.173\n0.146\n0.607\n0.751\n0.316\nQwen2.5-VL-7B\n0.148\n0.053\n0.111\n0.137\n0.189\n0.117\n0.134\n0.204\n0.706\n0.205\nInternVL3-8B\n0.163\n0.056\n0.107\n0.109\n0.129\n0.100\n0.159\n0.150\n0.681\n0.188\ndoubao-1-5-thinking-vision-pro-250428\n0.048\n0.048\n0.024\n0.062\n0.085\n0.051\n0.039\n0.096\n0.181\n0.073\nExpert VLMs\ndots.ocr\n0.031\n0.047\n0.011\n0.082\n0.079\n0.028\n0.029\n0.109\n0.056\n0.055\nNotes:\nThe metrics are from MonkeyOCR, OmniDocBench, and our own internal evaluations.\nWe delete the Page-header and Page-footer cells in the result markdown.\nWe use tikz_preprocess pipeline to upsample the images to dpi 200.\n2. dots.ocr-bench\nThis is an inhouse benchmark which contain 1493 pdf images with 100 languages.\nThe end-to-end evaluation results of different tasks.\nMethods\nOverallEdit‚Üì\nTextEdit‚Üì\nFormulaEdit‚Üì\nTableTEDS‚Üë\nTableEdit‚Üì\nRead OrderEdit‚Üì\nMonkeyOCR-3B\n0.483\n0.445\n0.627\n50.93\n0.452\n0.409\ndoubao-1-5-thinking-vision-pro-250428\n0.291\n0.226\n0.440\n71.2\n0.260\n0.238\ndoubao-1-6\n0.299\n0.270\n0.417\n71.0\n0.258\n0.253\nGemini2.5-Pro\n0.251\n0.163\n0.402\n77.1\n0.236\n0.202\ndots.ocr\n0.177\n0.075\n0.297\n79.2\n0.186\n0.152\nNotes:\nWe use the same metric calculation pipeline of OmniDocBench.\nWe delete the Page-header and Page-footer cells in the result markdown.\nLayout Detection\nMethod\nF1@IoU=.50:.05:.95‚Üë\nF1@IoU=.50‚Üë\nOverall\nText\nFormula\nTable\nPicture\nOverall\nText\nFormula\nTable\nPicture\nDocLayout-YOLO-DocStructBench\n0.733\n0.694\n0.480\n0.803\n0.619\n0.806\n0.779\n0.620\n0.858\n0.678\ndots.ocr-parse all\n0.831\n0.801\n0.654\n0.838\n0.748\n0.922\n0.909\n0.770\n0.888\n0.831\ndots.ocr-detection only\n0.845\n0.816\n0.716\n0.875\n0.765\n0.930\n0.917\n0.832\n0.918\n0.843\nNotes:\nprompt_layout_all_en for parse all, prompt_layout_only_en for detection only, please refer to prompts\n3. olmOCR-bench.\nModel\nArXiv\nOld ScansMath\nTables\nOld Scans\nHeaders andFooters\nMulticolumn\nLong TinyText\nBase\nOverall\nGOT OCR\n52.7\n52.0\n0.2\n22.1\n93.6\n42.0\n29.9\n94.0\n48.3 ¬± 1.1\nMarker\n76.0\n57.9\n57.6\n27.8\n84.9\n72.9\n84.6\n99.1\n70.1 ¬± 1.1\nMinerU\n75.4\n47.4\n60.9\n17.3\n96.6\n59.0\n39.1\n96.6\n61.5 ¬± 1.1\nMistral OCR\n77.2\n67.5\n60.6\n29.3\n93.6\n71.3\n77.1\n99.4\n72.0 ¬± 1.1\nNanonets OCR\n67.0\n68.6\n77.7\n39.5\n40.7\n69.9\n53.4\n99.3\n64.5 ¬± 1.1\nGPT-4o(No Anchor)\n51.5\n75.5\n69.1\n40.9\n94.2\n68.9\n54.1\n96.7\n68.9 ¬± 1.1\nGPT-4o(Anchored)\n53.5\n74.5\n70.0\n40.7\n93.8\n69.3\n60.6\n96.8\n69.9 ¬± 1.1\nGemini Flash 2(No Anchor)\n32.1\n56.3\n61.4\n27.8\n48.0\n58.7\n84.4\n94.0\n57.8 ¬± 1.1\nGemini Flash 2(Anchored)\n54.5\n56.1\n72.1\n34.2\n64.7\n61.5\n71.5\n95.6\n63.8 ¬± 1.2\nQwen 2 VL(No Anchor)\n19.7\n31.7\n24.2\n17.1\n88.9\n8.3\n6.8\n55.5\n31.5 ¬± 0.9\nQwen 2.5 VL(No Anchor)\n63.1\n65.7\n67.3\n38.6\n73.6\n68.3\n49.1\n98.3\n65.5 ¬± 1.2\nolmOCR v0.1.75(No Anchor)\n71.5\n71.4\n71.4\n42.8\n94.1\n77.7\n71.0\n97.8\n74.7 ¬± 1.1\nolmOCR v0.1.75(Anchored)\n74.9\n71.2\n71.0\n42.2\n94.5\n78.3\n73.3\n98.3\n75.5 ¬± 1.0\nMonkeyOCR-pro-3B\n83.8\n68.8\n74.6\n36.1\n91.2\n76.6\n80.1\n95.3\n75.8 ¬± 1.0\ndots.ocr\n82.1\n64.2\n88.3\n40.9\n94.1\n82.4\n81.2\n99.5\n79.1 ¬± 1.0\nNote:\nThe metrics are from MonkeyOCR,\nolmocr, and our own internal evaluations.\nWe delete the Page-header and Page-footer cells in the result markdown.\nQuick Start\n1. Installation\nInstall dots.ocr\nconda create -n dots_ocr python=3.12\nconda activate dots_ocr\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\n# Install pytorch, see https://pytorch.org/get-started/previous-versions/ for your cuda version\npip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install -e .\nIf you have trouble with the installation, try our Docker Image for an easier setup, and follow these steps:\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\npip install -e .\nDownload Model Weights\nüí°Note: Please use a directory name without periods (e.g., DotsOCR instead of dots.ocr) for the model save path. This is a temporary workaround pending our integration with Transformers.\npython3 tools/download_model.py\n2. Deployment\nvLLM inference\nWe highly recommend using vllm for deployment and inference. All of our evaluations results are based on vllm version 0.9.1.\nThe Docker Image is based on the official vllm image. You can also follow Dockerfile to build the deployment environment by yourself.\n# You need to register model to vllm at first\npython3 tools/download_model.py\nexport hf_model_path=./weights/DotsOCR  # Path to your downloaded model weights, Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\nexport PYTHONPATH=$(dirname \"$hf_model_path\"):$PYTHONPATH\nsed -i '/^from vllm\\.entrypoints\\.cli\\.main import main$/a\\\nfrom DotsOCR import modeling_dots_ocr_vllm' `which vllm`  # If you downloaded model weights by yourself, please replace `DotsOCR` by your model saved directory name, and remember to use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`)\n# launch vllm server\nCUDA_VISIBLE_DEVICES=0 vllm serve ${hf_model_path} --tensor-parallel-size 1 --gpu-memory-utilization 0.95  --chat-template-content-format string --served-model-name model --trust-remote-code\n# If you get a ModuleNotFoundError: No module named 'DotsOCR', please check the note above on the saved model directory name.\n# vllm api demo\npython3 ./demo/demo_vllm.py --prompt_mode prompt_layout_all_en\nHugginface inference\npython3 demo/demo_hf.py\nHugginface inference details\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\nmodel_path = \"./weights/DotsOCR\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\nattn_implementation=\"flash_attention_2\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nimage_path = \"demo/demo_image1.jpg\"\nprompt = \"\"\"Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.\n1. Bbox format: [x1, y1, x2, y2]\n2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].\n3. Text Extraction & Formatting Rules:\n- Picture: For the 'Picture' category, the text field should be omitted.\n- Formula: Format its text as LaTeX.\n- Table: Format its text as HTML.\n- All Others (Text, Title, etc.): Format their text as Markdown.\n4. Constraints:\n- The output text must be the original text from the image, with no translation.\n- All layout elements must be sorted according to human reading order.\n5. Final Output: The entire output must be a single JSON object.\n\"\"\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": image_path\n},\n{\"type\": \"text\", \"text\": prompt}\n]\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n3. Document Parse\nBased on vLLM server, you can parse an image or a pdf file using the following commands:\n# Parse all layout info, both detection and recognition\n# Parse a single image\npython3 dots_ocr/parser.py demo/demo_image1.jpg\n# Parse a single PDF\npython3 dots_ocr/parser.py demo/demo_pdf1.pdf  --num_threads 64  # try bigger num_threads for pdf with a large number of pages\n# Layout detection only\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_layout_only_en\n# Parse text only, except Page-header and Page-footer\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_ocr\n# Parse layout info by bbox\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_grounding_ocr --bbox 163 241 1536 705\nOutput Results\nStructured Layout Data (demo_image1.json): A JSON file containing the detected layout elements, including their bounding boxes, categories, and extracted text.\nProcessed Markdown File (demo_image1.md): A Markdown file generated from the concatenated text of all detected cells.\nAn additional version, demo_image1_nohf.md, is also provided, which excludes page headers and footers for compatibility with benchmarks like Omnidocbench and olmOCR-bench.\nLayout Visualization (demo_image1.jpg): The original image with the detected layout bounding boxes drawn on it.\n4. Demo\nYou can run the demo with the following command, or try directly at live demo\npython demo/demo_gradio.py\nWe also provide a demo for grounding ocr:\npython demo/demo_gradio_annotion.py\nExample for formula document\nExample for table document\nExample for multilingual document\nExample for reading order\nExample for grounding ocr\nAcknowledgments\nWe would like to thank Qwen2.5-VL, aimv2, MonkeyOCR,\nOmniDocBench, PyMuPDF, for providing code and models.\nWe also thank DocLayNet, M6Doc, CDLA, D4LA for providing valuable datasets.\nLimitation & Future Work\nComplex Document Elements:\nTable&Formula: dots.ocr is not yet perfect for high-complexity tables and formula extraction.\nPicture: Pictures in documents are currently not parsed.\nParsing Failures: The model may fail to parse under certain conditions:\nWhen the character-to-pixel ratio is excessively high. Try enlarging the image or increasing the PDF parsing DPI (a setting of 200 is recommended). However, please note that the model performs optimally on images with a resolution under 11289600 pixels.\nContinuous special characters, such as ellipses (...) and underscores (_), may cause the prediction output to repeat endlessly. In such scenarios, consider using alternative prompts like prompt_layout_only_en, prompt_ocr, or prompt_grounding_ocr (details here).\nPerformance Bottleneck: Despite its 1.7B parameter LLM foundation, dots.ocr is not yet optimized for high-throughput processing of large PDF volumes.\nWe are committed to achieving more accurate table and formula parsing, as well as enhancing the model's OCR capabilities for broader generalization, all while aiming for a more powerful, more efficient model. Furthermore, we are actively considering the development of a more general-purpose perception model based on Vision-Language Models (VLMs), which would integrate general detection, image captioning, and OCR tasks into a unified framework. Parsing the content of the pictures in the documents is also a key priority for our future work.\nWe believe that collaboration is the key to tackling these exciting challenges. If you are passionate about advancing the frontiers of document intelligence and are interested in contributing to these future endeavors, we would love to hear from you. Please reach out to us via email at: [yanqing4@xiaohongshu.com].",
    "lightx2v/Qwen-Image-Lightning": "Please refer to Qwen-Image-Lightning github to learn how to use the models.\nuse with diffusers üß®:\nmake sure to install diffusers from main (pip install git+https://github.com/huggingface/diffusers.git)\nfrom diffusers import DiffusionPipeline, FlowMatchEulerDiscreteScheduler\nimport torch\nimport math\n# From https://github.com/ModelTC/Qwen-Image-Lightning/blob/342260e8f5468d2f24d084ce04f55e101007118b/generate_with_diffusers.py#L82C9-L97C10\nscheduler_config = {\n\"base_image_seq_len\": 256,\n\"base_shift\": math.log(3),  # We use shift=3 in distillation\n\"invert_sigmas\": False,\n\"max_image_seq_len\": 8192,\n\"max_shift\": math.log(3),  # We use shift=3 in distillation\n\"num_train_timesteps\": 1000,\n\"shift\": 1.0,\n\"shift_terminal\": None,  # set shift_terminal to None\n\"stochastic_sampling\": False,\n\"time_shift_type\": \"exponential\",\n\"use_beta_sigmas\": False,\n\"use_dynamic_shifting\": True,\n\"use_exponential_sigmas\": False,\n\"use_karras_sigmas\": False,\n}\nscheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)\npipe = DiffusionPipeline.from_pretrained(\n\"Qwen/Qwen-Image\", scheduler=scheduler, torch_dtype=torch.bfloat16\n).to(\"cuda\")\npipe.load_lora_weights(\n\"lightx2v/Qwen-Image-Lightning\", weight_name=\"Qwen-Image-Lightning-8steps-V1.0.safetensors\"\n)\nprompt = \"a tiny astronaut hatching from an egg on the moon, Ultra HD, 4K, cinematic composition.\"\nnegative_prompt = \" \"\nimage = pipe(\nprompt=prompt,\nnegative_prompt=negative_prompt,\nwidth=1024,\nheight=1024,\nnum_inference_steps=8,\ntrue_cfg_scale=1.0,\ngenerator=torch.manual_seed(0),\n).images[0]\nimage.save(\"qwen_fewsteps.png\")",
    "inclusionAI/Ling-1T": "Introduction\nFlagship-Level Efficient Reasoning\nAesthetic Understanding and Front-End Generation\nEmergent Intelligence at Trillion-Scale\nPre-Training at Trillion Scale\nPost-Training and Evo-CoT Optimization\nEvaluation\nModel Downloads\nQuickstart\nüöÄ Try Online\nüîå API Usage\nDeployment\nSGLang\nEnvironment Preparation\nRun Inference\nvLLM\nEnvironment Preparation\nRun Inference:\nLimitations & Future Plans\nLicense\nFAQ\nü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope ¬†¬† | ¬†¬†üêô Experience Now\nIntroduction\nLing-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with ‚âà 50 billion active parameters per token.\nBuilt on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.\nPre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training.\nThis curriculum greatly enhances the model‚Äôs efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarks‚Äîbalancing accuracy and efficiency.\nFlagship-Level Efficient Reasoning\nWe comprehensively evaluated Ling-1T against leading flagship models, including both open-source giants (e.g., DeepSeek-V3.1-Terminus, Kimi-K2-Instruct-0905) and closed-source APIs (GPT-5-main, Gemini-2.5-Pro).\nAcross code generation, software development, competition-level mathematics, professional math, and logical reasoning, Ling-1T consistently demonstrates superior complex reasoning ability and overall advantage.\nIn the AIME 25 benchmark, Ling-1T extends the Pareto frontier of reasoning accuracy vs. reasoning length, showcasing its strength in ‚Äúefficient thinking and precise reasoning.‚Äù\nAesthetic Understanding and Front-End Generation\nLing-1T excels in visual reasoning and front-end code generation tasks, combining deep semantic understanding with precise code synthesis.\nWe introduce a hybrid Syntax‚ÄìFunction‚ÄìAesthetics reward mechanism, enabling the model to not only generate correct and functional code but also demonstrate a refined sense of visual aesthetics.\nOn ArtifactsBench, Ling-1T ranks first among open-source models, and the benchmark visualizations in this card were, in fact, generated by Ling-1T itself.\nEmergent Intelligence at Trillion-Scale\nScaling to the trillion-parameter level has revealed strong emergent reasoning and transfer capabilities.\nFor example, in the BFCL V3 tool-use benchmark, Ling-1T achieves ‚âà 70% tool-call accuracy with only light instruction tuning‚Äîdespite having seen no large-scale trajectory data during training.\nLing-1T can:\nInterpret complex natural-language instructions\nTransform abstract logic into functional visual components\nGenerate cross-platform compatible front-end code\nCreate stylistically controlled marketing copy and multi-lingual text\nThese capabilities form the foundation for general, collaborative human‚ÄìAI intelligence, which we aim to advance together with the open-source community through Ling-1T‚Äôs release.\nPre-Training at Trillion Scale\nThe Ling 2.0 architecture was designed from the ground up for trillion-scale efficiency, guided by the Ling Scaling Law (arXiv:2507.17702).\nThis ensures architectural and hyperparameter scalability even under 1e25‚Äì1e26 FLOPs of compute.\nKey architectural innovations include:\n1T total / 50B active parameters with a 1/32 MoE activation ratio\nMTP layers for enhanced compositional reasoning\nAux-loss-free, sigmoid-scoring expert routing with zero-mean updates\nQK Normalization for fully stable convergence\nLing-1T is the largest FP8-trained foundation model known to date.\nFP8 mixed-precision training yields 15%+ end-to-end speedup, improved memory efficiency, and maintains ‚â§ 0.1% loss deviation from BF16 across 1T tokens.\nA fine-grained, heterogeneous 1F1B interleaved pipeline further boosts utilization by 40 %+.\nSystem-level optimizations‚Äîfused kernels, communication scheduling, recomputation, checkpointing, simulation, and telemetry‚Äîensure stable trillion-scale training.\nPre-training used over 20T high-quality tokens, with > 40% reasoning-dense data in later stages.\nMid-training introduced curated chain-of-thought corpora for ‚Äúreasoning pre-activation‚Äù, improving downstream reasoning stability.\nA custom WSM (Warmup‚ÄìStable‚ÄìMerge) LR schedulerÔºàarXiv:2507.17634Ôºâ with mid-train checkpoint merging simulates LR decay and boosts generalization.\nPost-Training and Evo-CoT Optimization\nBuilt upon mid-training reasoning activation, post-training adopts Evo-CoT (Evolutionary Chain-of-Thought) for progressive reasoning enhancement under controllable cost.\nThis approach continually expands the Pareto frontier of reasoning accuracy vs. efficiency‚Äîideal for reflexive non-thinking models.\nFor reinforcement learning, we introduce LPO (Linguistics-Unit Policy Optimization) ‚Äîa novel sentence-level policy optimization method.\nUnlike GRPO (token-level) or GSPO (sequence-level) algorithms, LPO treats sentences as the natural semantic action units, enabling precise alignment between rewards and reasoning behavior.\nEmpirically, LPO offers superior training stability and generalization across reasoning tasks.\nEvaluation\nLing-1T has been extensively evaluated across knowledge, code, math, reasoning, agent, and alignment benchmarks.\nIt currently stands as the best open-source flagship non-thinking model, rivaling closed-source APIs in complex reasoning while maintaining exceptional efficiency and interpretability.\nModel Downloads\nYou can download Ling-1T from the following table. If you are located in mainland China, we also provide the model on ModelScope.cn to speed up the download process.\nModel\nContext Length\nDownload\nLing-1T\n32K -> 128K (YaRN)\nü§ó HuggingFace ¬†¬† ü§ñ ModelScope\nNote: If you are interested in previous version, please visit the past model collections in Huggingface or ModelScope.\nQuickstart\nüöÄ Try Online\nYou can experience Ling-1T online at: ZenMux\nüîå API Usage\nYou can also use Ling-1T through API calls:\nfrom openai import OpenAI\n# 1. Initialize the OpenAI client\nclient = OpenAI(\n# 2. Point the base URL to the ZenMux endpoint\nbase_url=\"https://zenmux.ai/api/v1\",\n# 3. Replace with the API Key from your ZenMux user console\napi_key=\"<your ZENMUX_API_KEY>\",\n)\n# 4. Make a request\ncompletion = client.chat.completions.create(\n# 5. Specify the model to use in the format \"provider/model-name\"\nmodel=\"inclusionai/ling-1t\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What is the meaning of life?\"\n}\n]\n)\nprint(completion.choices[0].message.content)\nDeployment\nSGLang\nEnvironment Preparation\nWe will later submit our model to the SGLang official release. Now we can prepare the environment by following these steps:\npip3 install -U sglang sgl-kernel\nRun Inference\nBoth BF16 and FP8 models are supported by SGLang now. It depends on the dtype of the model in ${MODEL_PATH}.\nHere is the example to run Ling-1T with multiple GPU nodes, where the master node IP is ${MASTER_IP} and server port is ${PORT}:\nStart server:\n# Node 0:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 0\n# Node 1:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 1\n# Node 2:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 2\n# Node 3:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 3\n# This is only an example. Please adjust arguments according to your actual environment.\nClient:\ncurl -s http://${MASTER_IP}:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\nMore usage can be found here\nvLLM\nEnvironment Preparation\npip install vllm==0.11.0\nRun Inference:\nHere is the example to deploy the model with multiple GPU nodes, where the master node IP is ${MASTER_IP}, server port is ${PORT} and the path of model is ${MODEL_PATH}:\n# step 1. start ray on all nodes\n# step 2. start vllm server only on node 0:\nvllm serve $MODEL_PATH --port $PORT --served-model-name my_model --trust-remote-code --tensor-parallel-size 32 --gpu-memory-utilization 0.85\n# This is only an example, please adjust arguments according to your actual environment.\nTo handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the model's config.json file, for example:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nUse an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service.\nFor detailed guidance, please refer to the vLLM instructions.\nLimitations & Future Plans\nWhile Ling-1T has made strong progress in efficient reasoning, cross-domain generalization, and training efficiency, several limitations remain:\nGQA-based attention: stable for long-context reasoning but relatively costly. Future versions will adopt hybrid attention to improve efficiency.\nLimited agentic ability: current model has room to grow in multi-turn interaction, long-term memory, and tool use.\nInstruction and identity issues: occasional deviations or role confusion may occur; future updates will enhance alignment and consistency.\nThe future versions of Ling-1T will continue to evolve in architecture, reasoning, and alignment, advancing the series toward more general intelligence.\nLicense\nThis code repository is licensed under the MIT License.\nFAQ\nRecommended temperature? 0.7Recommended top_p? 0.95",
    "lightx2v/Wan2.2-Distill-Models": "üé¨ Wan2.2 Distilled Models\nüåü What's Special?\n‚ö° Ultra-Fast Generation\nüéØ Flexible Options\nüíæ Memory Efficient\nüîß Easy Integration\nüì¶ Model Catalog\nüé• Model Types\nüéØ Precision Versions\nüìù Naming Convention\nüöÄ Usage\nMethod 1: LightX2V (Recommended ‚≠ê)\nMethod 2: ComfyUI\n‚ö†Ô∏è Important Notes\nü§ù Community\nüé¨ Wan2.2 Distilled Models\n‚ö° High-Performance Video Generation with 4-Step Inference\nDistillation-accelerated version of Wan2.2 - Dramatically faster speed with excellent quality\nüåü What's Special?\n‚ö° Ultra-Fast Generation\n4-step inference (vs traditional 50+ steps)\nApproximately 2x faster using LightX2V than ComfyUI\nNear real-time video generation capability\nüéØ Flexible Options\nDual noise control: High/Low noise variants\nMultiple precision formats (BF16/FP8/INT8)\nFull 14B parameter models\nüíæ Memory Efficient\nFP8/INT8: ~50% size reduction\nCPU offload support\nOptimized for consumer GPUs\nüîß Easy Integration\nCompatible with LightX2V framework\nComfyUI support\nSimple configuration files\nüì¶ Model Catalog\nüé• Model Types\nüñºÔ∏è Image-to-Video (I2V) - 14B Parameters\nTransform static images into dynamic videos with advanced quality control\nüé® High Noise: More creative, diverse outputs\nüéØ Low Noise: More faithful to input, stable outputs\nüìù Text-to-Video (T2V) - 14B Parameters\nGenerate videos from text descriptions\nüé® High Noise: More creative, diverse outputs\nüéØ Low Noise: More stable and controllable outputs\nüöÄ Full 14B parameter model\nüéØ Precision Versions\nPrecision\nModel Identifier\nModel Size\nFramework\nQuality vs Speed\nüèÜ BF16\nlightx2v_4step\n~28.6 GB\nLightX2V\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Highest Quality\n‚ö° FP8\nscaled_fp8_e4m3_lightx2v_4step\n~15 GB\nLightX2V\n‚≠ê‚≠ê‚≠ê‚≠ê Excellent Balance\nüéØ INT8\nint8_lightx2v_4step\n~15 GB\nLightX2V\n‚≠ê‚≠ê‚≠ê‚≠ê Fast & Efficient\nüî∑ FP8 ComfyUI\nscaled_fp8_e4m3_lightx2v_4step_comfyui\n~15 GB\nComfyUI\n‚≠ê‚≠ê‚≠ê ComfyUI Ready\nüìù Naming Convention\n# Format: wan2.2_{task}_A14b_{noise_level}_{precision}_lightx2v_4step.safetensors\n# I2V Examples:\nwan2.2_i2v_A14b_high_noise_lightx2v_4step.safetensors                       # I2V High Noise - BF16\nwan2.2_i2v_A14b_high_noise_scaled_fp8_e4m3_lightx2v_4step.safetensors      # I2V High Noise - FP8\nwan2.2_i2v_A14b_low_noise_int8_lightx2v_4step.safetensors                  # I2V Low Noise - INT8\nwan2.2_i2v_A14b_low_noise_scaled_fp8_e4m3_lightx2v_4step_comfyui.safetensors  # I2V Low Noise - FP8 ComfyUI\nüí° Browse All Models: View Full Model Collection ‚Üí\nüöÄ Usage\nMethod 1: LightX2V (Recommended ‚≠ê)\nLightX2V is a high-performance inference framework optimized for these models, approximately 2x faster than ComfyUI with better quantization accuracy. Highly recommended!\nQuick Start\nDownload model (using I2V FP8 as example)\nhuggingface-cli download lightx2v/Wan2.2-Distill-Models \\\n--local-dir ./models/wan2.2_i2v \\\n--include \"wan2.2_i2v_A14b_high_noise_scaled_fp8_e4m3_lightx2v_4step.safetensors\"\nhuggingface-cli download lightx2v/Wan2.2-Distill-Models \\\n--local-dir ./models/wan2.2_i2v \\\n--include \"wan2.2_i2v_A14b_low_noise_scaled_fp8_e4m3_lightx2v_4step.safetensors\"\nüí° Tip: For T2V models, follow the same steps but replace i2v with t2v in the filenames\nClone LightX2V repository\ngit clone https://github.com/ModelTC/LightX2V.git\ncd LightX2V\nInstall dependencies\npip install -r requirements.txt\nOr refer to Quick Start Documentation to use docker\nSelect and modify configuration file\nChoose appropriate configuration based on your GPU memory:\n80GB+ GPUs (A100/H100)\nI2V: wan_moe_i2v_distill.json\n24GB+ GPUs (RTX 4090)\nI2V: wan_moe_i2v_distill_4090.json\nRun inference (using I2V as example)\ncd scripts\nbash wan22/run_wan22_moe_i2v_distill.sh\nüìù Note: Update model paths in the script to point to your Wan2.2 model. Also refer to LightX2V Model Structure Documentation\nLightX2V Documentation\nQuick Start Guide: LightX2V Quick Start\nComplete Usage Guide: LightX2V Model Structure Documentation\nConfiguration File Instructions: Configuration Files\nQuantized Model Usage: Quantization Documentation\nParameter Offloading: Offload Documentation\nMethod 2: ComfyUI\nPlease refer to workflow\n‚ö†Ô∏è Important Notes\nOther Components: These models only contain DIT weights. Additional components needed at runtime:\nT5 text encoder\nCLIP vision encoder\nVAE encoder/decoder\nTokenizer\nPlease refer to LightX2V Documentation for instructions on organizing the complete model directory.\nü§ù Community\nGitHub Issues: https://github.com/ModelTC/LightX2V/issues\nHuggingFace: https://huggingface.co/lightx2v/Wan2.2-Distill-Models\nIf you find this project helpful, please give us a ‚≠ê on GitHub",
    "Qwen/Qwen-Image-Edit": "Introduction\nQuick Start\nShowcase\nLicense Agreement\nCitation\nJoin Us\nüíú Qwen Chat¬†¬† | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Tech Report ¬†¬† | ¬†¬† üìë Blog\nüñ•Ô∏è Demo¬†¬† | ¬†¬†üí¨ WeChat (ÂæÆ‰ø°)¬†¬† | ¬†¬†ü´® Discord¬†¬†| ¬†¬† Github\nIntroduction\nWe are excited to introduce Qwen-Image-Edit, the image editing version of Qwen-Image. Built upon our 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. Furthermore, Qwen-Image-Edit simultaneously feeds the input image into Qwen2.5-VL (for visual semantic control) and the VAE Encoder (for visual appearance control), achieving capabilities in both semantic and appearance editing. To experience the latest model, visit Qwen Chat and select the \"Image Editing\" feature.\nKey Features:\nSemantic and Appearance Editing: Qwen-Image-Edit supports both low-level visual appearance editing (such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged) and high-level visual semantic editing (such as IP creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency).\nPrecise Text Editing: Qwen-Image-Edit supports bilingual (Chinese and English) text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.\nStrong Benchmark Performance: Evaluations on multiple public benchmarks demonstrate that Qwen-Image-Edit achieves state-of-the-art (SOTA) performance in image editing tasks, establishing it as a powerful foundation model for image editing.\nQuick Start\nInstall the latest version of diffusers\npip install git+https://github.com/huggingface/diffusers\nThe following contains a code snippet illustrating how to use the model to generate images based on text prompts:\nimport os\nfrom PIL import Image\nimport torch\nfrom diffusers import QwenImageEditPipeline\npipeline = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\")\nprint(\"pipeline loaded\")\npipeline.to(torch.bfloat16)\npipeline.to(\"cuda\")\npipeline.set_progress_bar_config(disable=None)\nimage = Image.open(\"./input.png\").convert(\"RGB\")\nprompt = \"Change the rabbit's color to purple, with a flash light background.\"\ninputs = {\n\"image\": image,\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(0),\n\"true_cfg_scale\": 4.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 50,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"output_image_edit.png\")\nprint(\"image saved at\", os.path.abspath(\"output_image_edit.png\"))\nShowcase\nOne of the highlights of Qwen-Image-Edit lies in its powerful capabilities for semantic and appearance editing. Semantic editing refers to modifying image content while preserving the original visual semantics. To intuitively demonstrate this capability, let's take Qwen's mascot‚ÄîCapybara‚Äîas an example:\nAs can be seen, although most pixels in the edited image differ from those in the input image (the leftmost image), the character consistency of Capybara is perfectly preserved. Qwen-Image-Edit's powerful semantic editing capability enables effortless and diverse creation of original IP content.\nFurthermore, on Qwen Chat, we designed a series of editing prompts centered around the 16 MBTI personality types. Leveraging these prompts, we successfully created a set of MBTI-themed emoji packs based on our mascot Capybara, effortlessly expanding the IP's reach and expression.\nMoreover, novel view synthesis is another key application scenario in semantic editing. As shown in the two example images below, Qwen-Image-Edit can not only rotate objects by 90 degrees, but also perform a full 180-degree rotation, allowing us to directly see the back side of the object:\nAnother typical application of semantic editing is style transfer. For instance, given an input portrait, Qwen-Image-Edit can easily transform it into various artistic styles such as Studio Ghibli. This capability holds significant value in applications like virtual avatar creation:\nIn addition to semantic editing, appearance editing is another common image editing requirement. Appearance editing emphasizes keeping certain regions of the image completely unchanged while adding, removing, or modifying specific elements. The image below illustrates a case where a signboard is added to the scene.\nAs shown, Qwen-Image-Edit not only successfully inserts the signboard but also generates a corresponding reflection, demonstrating exceptional attention to detail.\nBelow is another interesting example, demonstrating how to remove fine hair strands and other small objects from an image.\nAdditionally, the color of a specific letter \"n\" in the image can be modified to blue, enabling precise editing of particular elements.\nAppearance editing also has wide-ranging applications in scenarios such as adjusting a person's background or changing clothing. The three images below demonstrate these practical use cases respectively.\nAnother standout feature of Qwen-Image-Edit is its accurate text editing capability, which stems from Qwen-Image's deep expertise in text rendering. As shown below, the following two cases vividly demonstrate Qwen-Image-Edit's powerful performance in editing English text:\nQwen-Image-Edit can also directly edit Chinese posters, enabling not only modifications to large headline text but also precise adjustments to even small and intricate text elements.\nFinally, let's walk through a concrete image editing example to demonstrate how to use a chained editing approach to progressively correct errors in a calligraphy artwork generated by Qwen-Image:\nIn this artwork, several Chinese characters contain generation errors. We can leverage Qwen-Image-Edit to correct them step by step. For instance, we can draw bounding boxes on the original image to mark the regions that need correction, instructing Qwen-Image-Edit to fix these specific areas. Here, we want the character \"Á®Ω\" to be correctly written within the red box, and the character \"‰∫≠\" to be accurately rendered in the blue region.\nHowever, in practice, the character \"Á®Ω\" is relatively obscure, and the model fails to correct it correctly in one step. The lower-right component of \"Á®Ω\" should be \"Êó®\" rather than \"Êó•\". At this point, we can further highlight the \"Êó•\" portion with a red box, instructing Qwen-Image-Edit to fine-tune this detail and replace it with \"Êó®\".\nIsn't it amazing? With this chained, step-by-step editing approach, we can continuously correct character errors until the desired final result is achieved.\nFinally, we have successfully obtained a completely correct calligraphy version of Lantingji Xu (Orchid Pavilion Preface)!\nIn summary, we hope that Qwen-Image-Edit can further advance the field of image generation, truly lower the technical barriers to visual content creation, and inspire even more innovative applications.\nLicense Agreement\nQwen-Image is licensed under Apache 2.0.\nCitation\nWe kindly encourage citation of our work if you find it useful.\n@misc{wu2025qwenimagetechnicalreport,\ntitle={Qwen-Image Technical Report},\nauthor={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\nyear={2025},\neprint={2508.02324},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2508.02324},\n}\nJoin Us\nIf you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait ‚Äî reach out to us at fulai.hr@alibaba-inc.com",
    "inference-net/Schematron-3B": "Model Overview\nI/O at a glance\nHighlights\nModel Details\nBenchmarks\nHTML-to-JSON Extraction Quality\nWeb-Augmented Factuality on SimpleQA\nMinimal Quickstart\nRecommendations\nLimitations\nSafety and Responsible Use\nLicense\nSupport\nDocumentation ¬∑\nServerless API ¬∑\nAnnouncement blog\nModel Overview\nWelcome to the Schematron series, Inference.net's long‚Äëcontext extraction models specialized in converting noisy HTML into clean, typed JSON that conforms to your custom schema. The Schematron series was purpose‚Äëtrained for web scraping, data ingestion, and transforming arbitrary pages into structured records.\nWe're releasing these models in two different sizes:\nSchematron‚Äë8B ‚Äî marginal quality lift on harder/longer pages\nSchematron‚Äë3B ‚Äî recommended default; near‚Äëparity quality at ~50% cost of Schematron-8B\nThis model card is dedicated to the smaller Schematron-3B model. Check out Schematron-8B for the larger model.\nI/O at a glance\nInput: Cleaned HTML + JSON Schema (can be extracted from typed model like Pydantic/Zod)\nOutput: Strictly valid JSON conforming to the provided schema (no narration)\nThe JSON Schema passed as input needs to conform to the schema.org schema.\nHighlights\nSchema-first extraction: 100% schema‚Äëconformant JSON outputs\nLong context: Robust to lengthy, noisy HTML (up to 128K tokens)\nVariants: 3B (default, most cost‚Äëefficient) ¬∑ 8B (marginal quality lift at ~2√ó cost)\nModel Details\nFamily: Schematron (3B and 8B)\nContext window: Up to 128K tokens\nInput: Cleaned or raw HTML and a JSON Schema\nOutput: Strict JSON that conforms to the provided schema\nBenchmarks\nHTML-to-JSON Extraction Quality\nWe evaluated extraction quality using Gemini 2.5 Pro as a judge, scoring extractions from 1-5 where 5 represents perfect extraction.\nModel\nLLM-as-Judge Score\nGPT-4.1\n4.74\nSchematron-8B\n4.64\nSchematron-3B\n4.41\nGemini-3B-Base\n2.24\nWeb-Augmented Factuality on SimpleQA\nWe evaluated Schematron's real-world impact on LLM factuality using SimpleQA.\nTest Pipeline:\nQuery Generation: Primary LLM (GPT-5 Nano or GPT-4.1) generates search queries and defines extraction schema\nWeb Search: Search provider (SERP or Exa) retrieves relevant pages\nStructured Extraction: Schematron extracts JSON data from retrieved pages using the schema\nAnswer Synthesis: Primary LLM produces final answer from structured data\nKey findings:\nWeb search paired with JSON extraction improves factuality: Adding Schematron with web retrieval improves GPT-5 Nano's accuracy from 8.54% to 82.87%‚Äînearly a 10x improvement\nSearch provider matters: Exa (82.9%) significantly outperforms SERP (64.2%) for factual retrieval, while also being more cost-effective\nStructured extraction beats raw HTML: Processing raw HTML would require 100k+ tokens for 10 searches; Schematron's JSON extraction reduces this by orders of magnitude\nSmall specialized models win: Schematron-8B (82.87%) outperforms the much larger Gemini 2.5 Flash (80.61%) on this task, showing that fine-tuning for well-defined tasks beats general purpose models\nPerformance scales with model quality: When paired with GPT-4.1, Schematron achieves 85.58% accuracy, showing the approach benefits from stronger base models\nMinimal Quickstart\nUse these local snippets to prepare HTML and compose a schema‚Äëguided prompt. The model returns strictly valid JSON; validate it against your schema downstream.\nfrom lxml.html.clean import Cleaner\nimport lxml.html as LH\nHTML_CLEANER = Cleaner(\nscripts=True,\njavascript=True,\nstyle=True,\ninline_style=True,\nsafe_attrs_only=False,\n)\ndef strip_noise(html: str) -> str:\n\"\"\"Remove scripts, styles, and JavaScript from HTML using lxml.\n\"\"\"\nif not html or not html.strip():\nreturn \"\"\ntry:\ndoc = LH.fromstring(html)\ncleaned = HTML_CLEANER.clean_html(doc)\nreturn LH.tostring(cleaned, encoding=\"unicode\")\nexcept Exception:\nreturn \"\"\nCompose messages with your schema and cleaned HTML:\ndef construct_messages(schema: str, html: str):\n\"\"\"Construct messages for a schema‚Äëguided extraction request.\"\"\"\nresponse_prompt = {\n\"prompt_part_one\": (\n\"You are going to be given a JSON schema following the standardized JSON \"\n\"Schema format. You are going to be given a HTML page and you are going \"\n\"to apply the schema to the HTML page however you see it as applicable \"\n\"and return the results in a JSON object. The schema is as follows:\"\n),\n\"prompt_part_two\": \"Here is the HTML page:\",\n\"prompt_part_three\": \"MAKE SURE ITS VALID JSON.\",\n}\nuser_prompt = (\nresponse_prompt['prompt_part_one']\n+ \"\\n\\n\" + schema + \"\\n\\n\"\n+ response_prompt['prompt_part_two']\n+ \"\\n\\n\" + html + \"\\n\\n\"\n+ response_prompt['prompt_part_three']\n)\nreturn [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n{\"role\": \"user\", \"content\": user_prompt},\n]\nIn the serverless API there's no need to pass anything but the HTML. We handle the prompt formatting for you.\nRecommendations\nTemperature 0 and JSON mode for deterministic, parseable output\nValidate responses against your schema (e.g., Pydantic or Zod)\nPre‚Äëclean HTML (remove scripts/styles) when possible; avoid over‚Äëaggressive removal\nUsing lxml to clean the HTML is not required, but is recommended as it matches the training data.\nLimitations\nStatic HTML only; render client‚Äëside content upstream\nVery large pages may require truncation\nAmbiguous fields depend on schema clarity; be explicit in field descriptions\nSafety and Responsible Use\nExtracted data may include personal or sensitive information present in the page‚Äîhandle and store responsibly\nRespect site terms, robots.txt, and applicable laws\nUse downstream validation and guardrails for compliance\nLicense\nSee license in the metadata above.\nSupport\nDocs: https://docs.inference.net/use-cases/json-extraction\nEmail: support@inference.net",
    "Qwen/Qwen3-VL-8B-Thinking": "Qwen3-VL-8B-Thinking\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-8B-Thinking\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-8B-Thinking.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show you how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-8B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-8B-Thinking\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Thinking\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport temperature=1.0\nexport out_seq_length=40960\nText\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport temperature=1.0\nexport out_seq_length=32768 (for aime, lcb, and gpqa, it is recommended to set to 81920)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "karpathy/nanochat-d32": "The nanochat-d32 model described in detail here.\nI'm sorry this is a janky upload but you have to place these files correctly on your end:\nthe token_bytes.pt, tokenizer.pkl have to go into ~/.cache/nanochat/tokenizer directory\nthe meta_000650.json, model_000650.pt have to go into ~/.cache/nanochat/chatsft_checkpoints/d32/\nI'll figure out how to make this less janky in the future, and to make nanochat play nicer with huggingface infra.",
    "AvitoTech/avibe": "Quickstart\n–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã (–Ω–∞ 4 –ø–æ—Ä—Ü–∏–∏):\n–ü–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ:\n–°–æ–≤–µ—Ç—ã:\nA-vibe —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –ê–≤–∏—Ç–æ –¢–µ—Ö, –¥–æ—á–µ—Ä–Ω–µ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–µ–π –ê–≤–∏—Ç–æ, –Ω–∞ –±–∞–∑–µ –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–∏ Qwen3-8B-Base.\n–ú—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∏ Qwen3-8B-Base –ø–æ–¥ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ –¥–æ–º–µ–Ω –ê–≤–∏—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —à–∞–≥–æ–≤\n–°–¥–µ–ª–∞–ª–∏ —Å–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n–ü–æ–¥–º–µ–Ω–∏–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Qwen3-8B-Base –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n–û–±—É—á–∏–ª–∏ –ø–æ–ª—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ –¥–∞–Ω–Ω—ã—Ö\n–ü—Ä–æ–≤–µ–ª–∏ SFT —ç—Ç–∞–ø\n–°–¥–µ–ª–∞–ª–∏ RL\n–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–∞–º —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç instruct –≤–µ—Ä—Å–∏—é Qwen3-8B –ø–æ –º–Ω–æ–≥–∏–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º.\n–ù–∞ SFT –∏ RL —ç—Ç–∞–ø–µ –Ω–∞–º —É–¥–∞–ª–æ—Å—å –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å Function Calling –∏ —É–ª—É—á—à–∏—Ç—å –µ–µ –Ω–∞–≤—ã–∫–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.\nQwen3-8B (no_think)\nA-vibe\nmmlu_ru\n0,701\n0,718\nmmlu_en\n0,730\n0,752\ngpqa_diamond_ru\n0,318\n0,343\ngpqa_diamond_en\n0,369\n0,318\nshlepa\n0,454\n0,486\nbaby mmlu\n0,682\n0,766\nmath_500_ru\n0,546\n0,686\nmath_500_en\n0,736\n0,714\ngsm8k_en\n0,927\n0,910\nDOoM\n0,240\n0,280\nru_facts\n0,724\n0,718\nrublimp\n0,916\n0,930\nru_drop\n0,318\n0,394\nBFCL_V3_en\n60,2%\n58,63%\nBFCL_V3_ru\n50.72%\n49.00%\nMERA_text\n0,510\n0,618\nMERA CODE private total\n0,336\n0,367\n–í —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ A-vibe –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤—ã—à–µ, —á–µ–º —É Qwen3-8B –ø–æ—ç—Ç–æ–º—É —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞–ª–æ –º–µ–Ω—å—à–µ –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ä–∞–∑–º–µ—Ä —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–∫—Ä–∞—Ç–∏–ª—Å—è –¥–æ 7.9B –ø—Ä–∏ 8.2B —É Qwen3-8B. –ó–∞ —Å—á–µ—Ç —ç—Ç–æ–≥–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 15-25% –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –∏—Å—Ö–æ–¥–Ω–æ–π Qwen3-8B\nQuickstart\n–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–π, –∫–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å, –∞ —Ç–∞–∫–∂–µ –∫–∞–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"AvitoTech/avibe\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"–ü—Ä–∏–≤–µ—Ç, –ø–æ–¥—Å–∫–∞–∂–∏ —Ä–µ—Ü–µ–ø—Ç –±–æ—Ä—â–∞\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=1024\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\nOutput:\n–í–æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Ä–µ—Ü–µ–ø—Ç –±–æ—Ä—â–∞ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏:\n–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã (–Ω–∞ 4 –ø–æ—Ä—Ü–∏–∏):\n–ú—è—Å–æ (–≥–æ–≤—è–¥–∏–Ω–∞, —Å–≤–∏–Ω–∏–Ω–∞ –∏–ª–∏ –∫—É—Ä–∏—Ü–∞) ‚Äî 300‚Äì400 –≥\n–°–≤–µ–∫–ª–∞ ‚Äî 2 —à—Ç. (—Å—Ä–µ–¥–Ω–∏–µ)\n–ö–∞–ø—É—Å—Ç–∞ –±–µ–ª–æ–∫–æ—á–∞–Ω–Ω–∞—è ‚Äî 300 –≥\n–ú–æ—Ä–∫–æ–≤—å ‚Äî 1 —à—Ç.\n–õ—É–∫ —Ä–µ–ø—á–∞—Ç—ã–π ‚Äî 1 —à—Ç.\n–ö–∞—Ä—Ç–æ—Ñ–µ–ª—å ‚Äî 3‚Äì4 —à—Ç.\n–ü–æ–º–∏–¥–æ—Ä—ã —Å–≤–µ–∂–∏–µ (–∏–ª–∏ —Ç–æ–º–∞—Ç–Ω–∞—è –ø–∞—Å—Ç–∞) ‚Äî 2 —à—Ç. (–∏–ª–∏ 2 —Å—Ç. –ª–æ–∂–∫–∏)\n–ß–µ—Å–Ω–æ–∫ ‚Äî 2‚Äì3 –∑—É–±—á–∏–∫–∞\n–õ–∞–≤—Ä–æ–≤—ã–π –ª–∏—Å—Ç ‚Äî 1‚Äì2 —à—Ç.\n–ü–µ—Ä–µ—Ü –≥–æ—Ä–æ—à–∫–æ–º ‚Äî 3‚Äì5 —à—Ç.\n–°–æ–ª—å, —Å–∞—Ö–∞—Ä ‚Äî –ø–æ –≤–∫—É—Å—É\n–†–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å–ª–æ ‚Äî 2‚Äì3 —Å—Ç. –ª–æ–∂–∫–∏\n–£–∫—Å—É—Å –∏–ª–∏ –ª–∏–º–æ–Ω–Ω—ã–π —Å–æ–∫ ‚Äî 1 —Å—Ç. –ª–æ–∂–∫–∞ (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–≤–µ—Ç–∞ —Å–≤–µ–∫–ª—ã)\n–ó–µ–ª–µ–Ω—å (—É–∫—Ä–æ–ø, –ø–µ—Ç—Ä—É—à–∫–∞) ‚Äî –¥–ª—è –ø–æ–¥–∞—á–∏\n–°–º–µ—Ç–∞–Ω–∞ ‚Äî –ø–æ –∂–µ–ª–∞–Ω–∏—é\n–ü–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ:\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±—É–ª—å–æ–Ω–∞:\n–ú—è—Å–æ –ø—Ä–æ–º–æ–π—Ç–µ, –Ω–∞—Ä–µ–∂—å—Ç–µ –Ω–∞ –∫—É—Å–∫–∏ –∏ –∑–∞–ª–µ–π—Ç–µ 2,5‚Äì3 –ª–∏—Ç—Ä–∞–º–∏ —Ö–æ–ª–æ–¥–Ω–æ–π –≤–æ–¥—ã.\n–î–æ–≤–µ–¥–∏—Ç–µ –¥–æ –∫–∏–ø–µ–Ω–∏—è, —Å–Ω–∏–º–∏—Ç–µ –ø–µ–Ω—É.\n–î–æ–±–∞–≤—å—Ç–µ –ª–∞–≤—Ä–æ–≤—ã–π –ª–∏—Å—Ç, –ø–µ—Ä–µ—Ü –≥–æ—Ä–æ—à–∫–æ–º –∏ –≤–∞—Ä–∏—Ç–µ 1,5‚Äì2 —á–∞—Å–∞ –Ω–∞ –º–µ–¥–ª–µ–Ω–Ω–æ–º –æ–≥–Ω–µ. –ó–∞ 30 –º–∏–Ω—É—Ç –¥–æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –ø–æ—Å–æ–ª–∏—Ç–µ.\n–û–≤–æ—â–∏:\n–°–≤–µ–∫–ª—É –Ω–∞—Ç—Ä–∏—Ç–µ –Ω–∞ –∫—Ä—É–ø–Ω–æ–π —Ç–µ—Ä–∫–µ. –û–±–∂–∞—Ä—å—Ç–µ –Ω–∞ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–º –º–∞—Å–ª–µ 5‚Äì7 –º–∏–Ω—É—Ç, –¥–æ–±–∞–≤—å—Ç–µ —É–∫—Å—É—Å (–∏–ª–∏ –ª–∏–º–æ–Ω–Ω—ã–π —Å–æ–∫) –∏ 2 —Å—Ç. –ª–æ–∂–∫–∏ –≤–æ–¥—ã. –¢—É—à–∏—Ç–µ 10 –º–∏–Ω—É—Ç.\n–ö–∞–ø—É—Å—Ç—É –Ω–∞—à–∏–Ω–∫—É–π—Ç–µ —Ç–æ–Ω–∫–æ–π —Å–æ–ª–æ–º–∫–æ–π.\n–ö–∞—Ä—Ç–æ—Ñ–µ–ª—å –Ω–∞—Ä–µ–∂—å—Ç–µ –∫—É–±–∏–∫–∞–º–∏.\n–ú–æ—Ä–∫–æ–≤—å –Ω–∞—Ç—Ä–∏—Ç–µ, –ª—É–∫ ‚Äî –º–µ–ª–∫–æ –Ω–∞—Ä–µ–∂—å—Ç–µ.\n–ü–æ–º–∏–¥–æ—Ä—ã –Ω–∞—Ç—Ä–∏—Ç–µ –Ω–∞ —Ç–µ—Ä–∫–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–º–∞—Ç–Ω—É—é –ø–∞—Å—Ç—É.\n–°–±–æ—Ä–∫–∞ –±–æ—Ä—â–∞:\n–í –∫–∏–ø—è—â–∏–π –±—É–ª—å–æ–Ω —Å–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤—å—Ç–µ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å, –≤–∞—Ä–∏—Ç–µ 10 –º–∏–Ω—É—Ç.\n–ó–∞—Ç–µ–º –≤–≤–µ–¥–∏—Ç–µ –∫–∞–ø—É—Å—Ç—É, –º–æ—Ä–∫–æ–≤—å –∏ –ª—É–∫. –í–∞—Ä–∏—Ç–µ –µ—â–µ 10 –º–∏–Ω—É—Ç.\n–î–æ–±–∞–≤—å—Ç–µ –æ–±–∂–∞—Ä–µ–Ω–Ω—É—é —Å–≤–µ–∫–ª—É –∏ –ø–æ–º–∏–¥–æ—Ä—ã (–∏–ª–∏ —Ç–æ–º–∞—Ç–Ω—É—é –ø–∞—Å—Ç—É). –ü–µ—Ä–µ–º–µ—à–∞–π—Ç–µ.\n–ó–∞ 5 –º–∏–Ω—É—Ç –¥–æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –≤–ª–µ–π—Ç–µ –∏–∑–º–µ–ª—å—á–µ–Ω–Ω—ã–π —á–µ—Å–Ω–æ–∫ (—á–µ—Ä–µ–∑ –ø—Ä–µ—Å—Å –∏–ª–∏ –º–µ–ª–∫–æ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–π).\n–§–∏–Ω–∞–ª—å–Ω—ã–µ —à—Ç—Ä–∏—Ö–∏:\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∫—É—Å, –¥–æ–±–∞–≤—å—Ç–µ —Å–∞—Ö–∞—Ä (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ) –∏ –∑–µ–ª–µ–Ω—å.\n–°–Ω–∏–º–∏—Ç–µ —Å –æ–≥–Ω—è, –Ω–∞–∫—Ä–æ–π—Ç–µ –∫—Ä—ã—à–∫–æ–π –∏ –¥–∞–π—Ç–µ –Ω–∞—Å—Ç–æ—è—Ç—å—Å—è 15‚Äì20 –º–∏–Ω—É—Ç.\n–ü–æ–¥–∞—á–∞:\n–†–∞–∑–ª–µ–π—Ç–µ –±–æ—Ä—â –ø–æ —Ç–∞—Ä–µ–ª–∫–∞–º, –¥–æ–±–∞–≤—å—Ç–µ —Å–º–µ—Ç–∞–Ω—É –∏ —Å–≤–µ–∂—É—é –∑–µ–ª–µ–Ω—å.\n–°–æ–≤–µ—Ç—ã:\n–î–ª—è –±–æ–ª–µ–µ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ–≥–æ –≤–∫—É—Å–∞ –º–æ–∂–Ω–æ –æ–±–∂–∞—Ä–∏—Ç—å –º—è—Å–æ –ø–µ—Ä–µ–¥ –≤–∞—Ä–∫–æ–π.\n–ï—Å–ª–∏ –ª—é–±–∏—Ç–µ –∫–∏—Å–ª–∏–Ω–∫—É, –¥–æ–±–∞–≤—å—Ç–µ 1 —Å—Ç. –ª–æ–∂–∫—É —É–∫—Å—É—Å–∞ –∏–ª–∏ –ª–∏–º–æ–Ω–Ω–æ–≥–æ —Å–æ–∫–∞ –≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ.\n–í–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: –∑–∞–º–µ–Ω–∏—Ç–µ –º—è—Å–æ –Ω–∞ –≥—Ä–∏–±—ã –∏–ª–∏ –≤–∞—Ä–∏—Ç–µ –Ω–∞ –æ–≤–æ—â–Ω–æ–º –±—É–ª—å–æ–Ω–µ.\n–ü—Ä–∏—è—Ç–Ω–æ–≥–æ –∞–ø–ø–µ—Ç–∏—Ç–∞! üç≤",
    "inclusionAI/Ring-1T": "Ring-1T: Flow State Leads to Sudden Enlightenment\nModel Downloads\nContinuously Evolving Deep Reasoning Capabilities\nIcepop: Ensuring Stable Reinforcement Learning Through Long-Term Training\nASystem: In-House RL Framework \"Mastering\" Trillion-Scale Training\nQuickstart\nüöÄ Try Online\nüîå API Usage\nDeployment\nSGLang\nvLLM\nFinetuning\nLimitations and Future Plans\nLicense\nFAQ\nReference\nü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope ¬†¬† | ¬†¬†üêô Experience Now\nRing-1T: Flow State Leads to Sudden Enlightenment\nToday, we officially launch the trillion-parameter thinking model, Ring-1T. It is open-source upon release‚Äîdevelopers can download the model weights from Hugging Face and ModelScope, or experience direct chat interactions and API calls via the Ling Chat page and ZenMux (links provided at the end of the article).\nBuilding upon the preview version released at the end of last month, Ring-1T has undergone continued scaling with large-scale verifiable reward reinforcement learning (RLVR) training, further unlocking the natural language reasoning capabilities of the trillion-parameter foundation model. Through RLHF training, the model's general abilities have also been refined, making this release of Ring-1T more balanced in performance across various tasks.\nRing-1T adopts the Ling 2.0 architecture and is trained on the Ling-1T-base foundation model, which contains 1 trillion total parameters with 50 billion activated parameters, supporting a context window of up to 128K tokens. Leveraging our self-developed icepop reinforcement learning stabilization method and the efficient reinforcement learning system ASystem (whose AReaL framework is already open-source), we have achieved smooth scaling of MoE architecture reinforcement learning‚Äîfrom tens of billions (Ring-mini-2.0) to hundreds of billions (Ring-flash-2.0) to trillions (Ring-1T) of parameters‚Äîsignificantly enhancing the model's deep reasoning and natural language inference capabilities.\nModel Downloads\nYou can download Ring-1T from the following table. If you are located in mainland China, we also provide the model on ModelScope to speed up the download process.\nModel\nContext Length\nDownload\nRing-1T\n64K -> 128K (YaRN)\nü§ó HuggingFace ¬†¬† ü§ñ ModelScope\nRing-1T-FP8\n64K -> 128K (YaRN)\nü§ó HuggingFace ¬†¬† ü§ñ ModelScope\nNote: If you are interested in the previous version, please visit the past model collections on Huggingface or ModelScope.\nContinuously Evolving Deep Reasoning Capabilities\nTo evaluate the deep reasoning capabilities of Ring-1T, we selected representative open-source thinking models (Ring-1T-preview, Deepseek-V3.1-Terminus-Thinking, Qwen-235B-A22B-Thinking-2507) and closed-source APIs (Gemini-2.5-Pro and GPT-5-Thinking(High)) as benchmarks. First, compared to the previously open-sourced preview version, Ring-1T demonstrates more balanced performance across various tasks. Furthermore, Ring-1T achieves leading open-source performance on challenging reasoning benchmarks such as math competitions (AIME 25, HMMT 25), code generation (LiveCodeBench, CodeForce), and logical reasoning (ARC-AGI-1). It also exhibits strong competitiveness in comprehensive tasks (Arena-Hard-v2.0), healthcare (HealthBench), and creative writing (Creative Writing v3).\nAlthough we have implemented string-level and semantic-level contamination filtering for benchmark tasks across all training stages‚Äîincluding pre-training, fine-tuning instructions, and reinforcement learning prompts‚Äîrigorous decontamination for earlier published benchmarks remains a significant challenge in the industry. To more objectively analyze Ring-1T's deep reasoning capabilities, we conducted tests using the IMO 2025 (International Mathematical Olympiad) held in July this year and the recently concluded ICPC World Finals 2025 (International Collegiate Programming Contest World Finals).\nFor the IMO 2025 test, similar to the previous preview version, we integrated Ring-1T into the multi-agent framework AWorld (https://github.com/inclusionAI/AWorld) and used pure natural language reasoning to solve the problems. The results show that Ring-1T solved Problems 1, 3, 4, and 5 in a single attempt (silver medal level at IMO). On the third attempt, it also produced a nearly perfect proof for Problem 2, a geometry proof. For the most challenging Problem 6 (which no AI contestant in IMO 2025 solved correctly), Ring-1T converged to the same answer as Gemini 2.5 Pro‚Äî\"4048\" (the correct answer is 2112). We believe that with ongoing optimizations, Ring-1T has the potential to reach gold medal level at IMO in a single attempt in the future.\nAt the ICPC World Finals 2025, we compared GPT-5-Thinking, Gemini-2.5-Pro, and Ring-1T. In a test allowing three attempts for direct problem-solving by the models, they solved 6 (problems CDEFKL), 3 (problems DFK), and 5 (problems DFJKL) problems, respectively. The results demonstrate that Ring-1T also delivers outstanding performance in top-tier international programming competitions. Further testing is ongoing, and we will also open-source the solution traces of the models for the aforementioned competitions (IMO traces are provided at the end of the article). We look forward to collaborating with the community to further optimize the reasoning potential of this trillion-parameter thinking model.\nIcepop: Ensuring Stable Reinforcement Learning Through Long-Term Training\nIn the reinforcement learning training of MoE models, the discrepancies in operator implementations between the training and inference engines are more pronounced compared to dense models. This divergence becomes increasingly significant as sequence length and training steps accumulate, particularly during long-sequence generation and extended training cycles. As illustrated in the experiment below, the original GRPO algorithm begins to collapse after relatively few training steps. In contrast, our proposed Icepop algorithm mitigates this issue by correcting distributions through masked bidirectional truncation technology, effectively reducing the gap between training and inference phases‚Äîthereby \"cooling down\" the rapidly escalating training-inference discrepancy.\nFigure 1: The training-inference discrepancy of GRPO increases exponentially with training, while Icepop remains relatively stable.\nFigure 2: Maximum training-inference discrepancy‚ÄîGRPO shows a significant rise with training, whereas Icepop maintains a low level.\nASystem: In-House RL Framework \"Mastering\" Trillion-Scale Training\nTo ensure stable and efficient reinforcement learning training for trillion-parameter foundation models, we independently developed a high-performance reinforcement learning system‚ÄîASystem. ASystem adopts a SingleController + SPMD architecture. In terms of training and inference engines, it has been meticulously optimized to address memory management and weight exchange challenges specific to trillion-parameter models. Leveraging our self-developed unified memory pool technology for training and inference, it achieves transparent memory offloading, efficiently releases memory fragmentation, and reduces the risk of insufficient memory. Through techniques such as direct P2P communication between GPUs and in-place updates, it enables second-level, zero-redundant model weight exchange.\nFor the RL training framework, we built a hybrid reward system based on large-scale Serverless Sandbox technology. This system can start up in milliseconds, supports execution environments for over 10 programming languages, and handles request throughput of up to 10K/s. We have open-sourced AReaL and hope to accelerate RL training and research in the open-source community through technological openness.\nQuickstart\nüöÄ Try Online\nYou can experience Ring-1T online at: ZenMux\nüîå API Usage\nYou can also use Ring-1T through API calls:\nfrom openai import OpenAI\n# 1. Initialize the OpenAI client\nclient = OpenAI(\n# 2. Point the base URL to the ZenMux endpoint\nbase_url=\"https://zenmux.ai/api/v1\",\n# 3. Replace with the API Key from your ZenMux user console\napi_key=\"<your ZENMUX_API_KEY>\",\n)\n# 4. Make a request\ncompletion = client.chat.completions.create(\n# 5. Specify the model to use in the format \"provider/model-name\"\nmodel=\"inclusionai/ring-1t\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What is the meaning of life?\"\n}\n]\n)\nprint(completion.choices[0].message.content)\nDeployment\nSGLang\nEnvironment Preparation\nWe will later submit our model to the SGLang official release. Now we can prepare the environment by following these steps:\npip3 install -U sglang sgl-kernel\nRun Inference\nBoth BF16 and FP8 models are supported by SGLang now. It depends on the dtype of the model in ${MODEL_PATH}.\nHere is the example to run Ring-1T with multiple GPU nodes, where the master node IP is ${MASTER_IP} and server port is ${PORT}:\nStart server:\n# Node 0:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 0\n# Node 1:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 1\n# Node 2:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 2\n# Node 3:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 3\n# This is only an example. Please adjust arguments according to your actual environment.\nClient:\ncurl -s http://${MASTER_IP}:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\nMore usage can be found here\nvLLM\nEnvironment Preparation\npip install vllm==0.11.0\nRun Inference:\nHere is the example to deploy the model with multiple GPU nodes, where the master node IP is ${MASTER_IP}, server port is ${PORT} and the path of model is ${MODEL_PATH}:\n# step 1. start ray on all nodes\n# step 2. start vllm server only on node 0:\nvllm serve $MODEL_PATH --port $PORT --served-model-name my_model --trust-remote-code --tensor-parallel-size 32 --gpu-memory-utilization 0.85\n# This is only an example, please adjust arguments according to your actual environment.\nTo handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the model's config.json file, for example:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nUse an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service.\nFor detailed guidance, please refer to the vLLM instructions.\nFinetuning\nWe recommend you use Llama-Factory to finetune Ring.\nLimitations and Future Plans\nRing-1T represents the Bailing team's first attempt at developing a trillion-scale deep thinking model. The current version may occasionally exhibit issues such as identity recognition bias, language mixing, and repetitive generation. Additionally, since its attention architecture still adopts the GQA approach from Ling 2.0, there remains room for improvement in inference efficiency under long-context scenarios.\nWe will continue to optimize these aspects in future releases and highly welcome feedback from the community. Furthermore, training for Ring-1T is still ongoing. We are committed to further unlocking the reasoning potential of this trillion-parameter foundation model and look forward to sharing more mature upgraded versions with everyone as soon as possible.\nWelcome to visit our open-source repository and demo page for download and usage.\nHugging Face: https://huggingface.co/inclusionAI/Ring-1T\nModelScope: https://modelscope.cn/models/inclusionAI/Ring-1T\nLing Chat (for Chinese users): https://ling.tbox.cn/chat\nZenMux (for overseas developers, offering Chat testing and API capabilities): https://zenmux.ai/inclusionai/ring-1t?utm_source=hf_inclusionAI\nRing-1T@Aworld IMO test trajectory: https://github.com/inclusionAI/AWorld/tree/main/examples/imo/samples/samples%20from%20Ring-1T\nLicense\nThis code repository is licensed under the MIT License.\nFAQ\nRecommended temperature? 0.6Recommended top_p? 0.95\nReference\n@article{ling2025everystep,\ntitle={Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model},\nauthor={Team, Ling and Shen, Anqi and Li, Baihui and Hu, Bin and Jing, Bin and Chen, Cai and Huang, Chao and Zhang, Chao and Yang, Chaokun and Lin, Cheng and Wen, Chengyao and Li, Congqi and Zhao, Deng and Yuan, Dingbo and You, Donghai and Mao, Fagui and Meng, Fanzhuang and Xu, Feng and Li, Guojie and Wang, Guowei and Dai, Hao and Zheng, Haonan and others},\njournal={arXiv preprint arXiv:2510.18855},\nyear={2025}\n}",
    "moonshotai/Kimi-K2-Instruct-0905": "1. Model Introduction\nKey Features\n2. Model Summary\n3. Evaluation Results\n4. Deployment\n5. Model Usage\nChat Completion\nTool Calling\n6. License\n7. Third Party Notices\n7. Contact Us\nüì∞¬†¬†Tech Blog ¬†¬†¬† | ¬†¬†¬† üìÑ¬†¬†Paper\n1. Model Introduction\nKimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters.\nKey Features\nEnhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.\nImproved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming.\nExtended context length: Kimi K2-Instruct-0905‚Äôs context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks.\n2. Model Summary\nArchitecture\nMixture-of-Experts (MoE)\nTotal Parameters\n1T\nActivated Parameters\n32B\nNumber of Layers (Dense layer included)\n61\nNumber of Dense Layers\n1\nAttention Hidden Dimension\n7168\nMoE Hidden Dimension (per Expert)\n2048\nNumber of Attention Heads\n64\nNumber of Experts\n384\nSelected Experts per Token\n8\nNumber of Shared Experts\n1\nVocabulary Size\n160K\nContext Length\n256K\nAttention Mechanism\nMLA\nActivation Function\nSwiGLU\n3. Evaluation Results\nBenchmark\nMetric\nK2-Instruct-0905\nK2-Instruct-0711\nQwen3-Coder-480B-A35B-Instruct\nGLM-4.5\nDeepSeek-V3.1\nClaude-Sonnet-4\nClaude-Opus-4\nSWE-Bench verified\nACC\n69.2 ¬± 0.63\n65.8\n69.6*\n64.2*\n66.0*\n72.7*\n72.5*\nSWE-Bench Multilingual\nACC\n55.9 ¬± 0.72\n47.3\n54.7*\n52.7\n54.5*\n53.3*\n-\nMulti-SWE-Bench\nACC\n33.5 ¬± 0.28\n31.3\n32.7\n31.7\n29.0\n35.7\n-\nTerminal-Bench\nACC\n44.5 ¬± 2.03\n37.5\n37.5*\n39.9*\n31.3*\n36.4*\n43.2*\nSWE-Dev\nACC\n66.6 ¬± 0.72\n61.9\n64.7\n63.2\n53.3\n67.1\n-\nAll K2-Instruct-0905 numbers are reported as mean ¬± std over five independent, full-test-set runs.\nBefore each run we prune the repository so that every Git object unreachable from the target commit disappears; this guarantees the agent sees only the code that would legitimately be available at that point in history.\nExcept for Terminal-Bench (Terminus-2), every result was produced with our in-house evaluation harness. The harness is derived from SWE-agent, but we clamp the context windows of the Bash and Edit tools and rewrite the system prompt to match the task semantics. All baseline figures denoted with an asterisk (*) are excerpted directly from their official report or public leaderboard; the remaining metrics were evaluated by us under conditions identical to those used for K2-Instruct-0905.\nFor SWE-Dev we go one step further: we overwrite the original repository files and delete any test file that exercises the functions the agent is expected to generate, eliminating any indirect hints about the desired implementation.\n4. Deployment\nYou can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you.\nThe Anthropic-compatible API maps temperature by real_temperature = request_temperature * 0.6 for better compatible with existing applications.\nOur model checkpoints are stored in the block-fp8 format, you can find it on Huggingface.\nCurrently, Kimi-K2 is recommended to run on the following inference engines:\nvLLM\nSGLang\nKTransformers\nTensorRT-LLM\nDeployment examples for vLLM and SGLang can be found in the Model Deployment Guide.\n5. Model Usage\nChat Completion\nOnce the local inference service is up, you can interact with it through the chat endpoint:\ndef simple_chat(client: OpenAI, model_name: str):\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Please give a brief self-introduction.\"}]},\n]\nresponse = client.chat.completions.create(\nmodel=model_name,\nmessages=messages,\nstream=False,\ntemperature=0.6,\nmax_tokens=256\n)\nprint(response.choices[0].message.content)\nThe recommended temperature for Kimi-K2-Instruct-0905 is temperature = 0.6.\nIf no special instructions are required, the system prompt above is a good default.\nTool Calling\nKimi-K2-Instruct-0905 has strong tool-calling capabilities.\nTo enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.\nThe following example demonstrates calling a weather tool end-to-end:\n# Your tool implementation\ndef get_weather(city: str) -> dict:\nreturn {\"weather\": \"Sunny\"}\n# Tool schema definition\ntools = [{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Retrieve current weather information. Call this when the user asks about the weather.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"city\"],\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"Name of the city\"\n}\n}\n}\n}\n}]\n# Map tool names to their implementations\ntool_map = {\n\"get_weather\": get_weather\n}\ndef tool_call_with_client(client: OpenAI, model_name: str):\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n{\"role\": \"user\", \"content\": \"What's the weather like in Beijing today? Use the tool to check.\"}\n]\nfinish_reason = None\nwhile finish_reason is None or finish_reason == \"tool_calls\":\ncompletion = client.chat.completions.create(\nmodel=model_name,\nmessages=messages,\ntemperature=0.6,\ntools=tools,          # tool list defined above\ntool_choice=\"auto\"\n)\nchoice = completion.choices[0]\nfinish_reason = choice.finish_reason\nif finish_reason == \"tool_calls\":\nmessages.append(choice.message)\nfor tool_call in choice.message.tool_calls:\ntool_call_name = tool_call.function.name\ntool_call_arguments = json.loads(tool_call.function.arguments)\ntool_function = tool_map[tool_call_name]\ntool_result = tool_function(**tool_call_arguments)\nprint(\"tool_result:\", tool_result)\nmessages.append({\n\"role\": \"tool\",\n\"tool_call_id\": tool_call.id,\n\"name\": tool_call_name,\n\"content\": json.dumps(tool_result)\n})\nprint(\"-\" * 100)\nprint(choice.message.content)\nThe tool_call_with_client function implements the pipeline from user query to tool execution.\nThis pipeline requires the inference engine to support Kimi-K2‚Äôs native tool-parsing logic.\nFor more information, see the Tool Calling Guide.\n6. License\nBoth the code repository and the model weights are released under the Modified MIT License.\n7. Third Party Notices\nSee THIRD PARTY NOTICES\n7. Contact Us\nIf you have any questions, please reach out at support@moonshot.cn.",
    "Qwen/Qwen3-VL-32B-Instruct-FP8": "Qwen3-VL-32B-Instruct-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nGeneration Hyperparameters\nCitation\nQwen3-VL-32B-Instruct-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-32B-Instruct model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-32B-Instruct-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-32B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-32B-Instruct-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "mirth/chonky_mmbert_small_multilingual_1": "Chonky_mmbert_small_multilingual_v1\nModel Description\nHow to use\nSample Output:\nSample output\nTraining Data\nMetrics\nHardware\nChonky_mmbert_small_multilingual_v1\nChonky is a transformer model that intelligently segments text into meaningful semantic chunks. This model can be used in the RAG systems.\nüÜï Now multilingual!\nModel Description\nThe model processes text and divides it into semantically coherent segments. These chunks can then be fed into embedding-based retrieval systems or language models as part of a RAG pipeline.\n‚ö†Ô∏èThis model was fine-tuned on sequence of length 1024 (by default mmBERT supports sequence length up to 8192).\nHow to use\nI've made a small python library for this model: chonky\nHere is the usage:\nfrom src.chonky import ParagraphSplitter\n# on the first run it will download the transformer model\nsplitter = ParagraphSplitter(\nmodel_id=\"mirth/chonky_mmbert_small_multilingual_1\",\ndevice=\"cpu\"\n)\ntext = (\n\"Before college the two main things I worked on, outside of school, were writing and programming. \"\n\"I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. \"\n\"My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. \"\n\"The first programs I tried writing were on the IBM 1401 that our school district used for what was then called 'data processing.' \"\n\"This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, \"\n\"and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines ‚Äî \"\n\"CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\"\n)\nfor chunk in splitter(text):\nprint(chunk)\nprint(\"--\")\nSample Output:\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep\n--\n. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called 'data processing.' This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\n--\nBut you can use this model using standart NER pipeline:\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nmodel_name = \"mirth/chonky_mmbert_small_multilingual_1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=1024)\nid2label = {\n0: \"O\",\n1: \"separator\",\n}\nlabel2id = {\n\"O\": 0,\n\"separator\": 1,\n}\nmodel = AutoModelForTokenClassification.from_pretrained(\nmodel_name,\nnum_labels=2,\nid2label=id2label,\nlabel2id=label2id,\n)\npipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\ntext = (\n\"Before college the two main things I worked on, outside of school, were writing and programming. \"\n\"I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. \"\n\"My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. \"\n\"The first programs I tried writing were on the IBM 1401 that our school district used for what was then called 'data processing.' \"\n\"This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, \"\n\"and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines ‚Äî \"\n\"CPU, disk drives, printer, card reader ‚Äî sitting up on a raised floor under bright fluorescent lights.\"\n)\npipe(text)\nSample output\n[{'entity_group': 'separator',\n'score': np.float32(0.66304857),\n'word': ' deep',\n'start': 332,\n'end': 337}]\nTraining Data\nThe model was trained to split paragraphs from minipile, bookcorpus and Project Gutenberg datasets.\nMetrics\nToken based F1-score.\nProject Gutenberg validation:\nModel\nde\nen\nes\nfr\nit\nnl\npl\npt\nru\nsv\nzh\nchonky_mmbert_small_multi_1 üÜï\n0.88\n0.78\n0.91\n0.93\n0.86\n0.81\n0.81\n0.88\n0.97\n0.91\n0.11\nchonky_modernbert_large_1\n0.53\n0.43\n0.48\n0.51\n0.56\n0.21\n0.65\n0.53\n0.87\n0.51\n0.33\nchonky_modernbert_base_1\n0.42\n0.38\n0.34\n0.4\n0.33\n0.22\n0.41\n0.35\n0.27\n0.31\n0.26\nchonky_distilbert_base_uncased_1\n0.19\n0.3\n0.17\n0.2\n0.18\n0.04\n0.27\n0.21\n0.22\n0.19\n0.15\nNumber of val tokens\n1m\n1m\n1m\n1m\n1m\n1m\n38k\n1m\n24k\n1m\n132k\nVarious english datasets:\nModel\nbookcorpus\nen_judgements\npaul_graham\n20_newsgroups\nchonkY_modernbert_large_1\n0.79\n0.29\n0.69\n0.17\nchonkY_modernbert_base_1\n0.72\n0.08\n0.63\n0.15\nchonkY_distilbert_base_uncased_1\n0.69\n0.05\n0.52\n0.15\nchonky_mmbert_small_multilingual_1 üÜï\n0.72\n0.2\n0.56\n0.13\nHardware\nModel was fine-tuned on a single H100 for a several hours",
    "BAAI/bge-m3": "BGE-M3 (paper, code)\nNews:\nSpecs\nFAQ\nUsage\nGenerate Embedding for text\nCompute score for text pairs\nEvaluation\nBenchmarks from the open-source community\nOur results\nTraining\nAcknowledgement\nCitation\nFor more details please refer to our github repo: https://github.com/FlagOpen/FlagEmbedding\nBGE-M3 (paper, code)\nIn this project, we introduce BGE-M3, which is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.\nMulti-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\nMulti-Linguality: It can support more than 100 working languages.\nMulti-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\nSome suggestions for retrieval pipeline in RAG\nWe recommend to use the following pipeline: hybrid retrieval + re-ranking.\nHybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities.\nA classic example: using both embedding retrieval and the BM25 algorithm.\nNow, you can try to use BGE-M3, which supports both embedding and sparse retrieval.\nThis allows you to obtain token weights (similar to the BM25) without any additional cost when generate dense embeddings.\nTo use hybrid retrieval, you can refer to Vespa and Milvus.\nAs cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model.\nUtilizing the re-ranking model (e.g., bge-reranker, bge-reranker-v2) after retrieval can further filter the selected text.\nNews:\n2024/7/1: We update the MIRACL evaluation results of BGE-M3. To reproduce the new results, you can refer to: bge-m3_miracl_2cr. We have also updated our paper on arXiv.\nDetails\nThe previous test results were lower because we mistakenly removed the passages that have the same id as the query from the search results. After correcting this mistake, the overall performance of BGE-M3 on MIRACL is higher than the previous results, but the experimental conclusion remains unchanged. The other results are not affected by this mistake. To reproduce the previous lower results, you need to add the --remove-query parameter when using pyserini.search.faiss or pyserini.search.lucene to search the passages.\n2024/3/20: Thanks Milvus team! Now you can use hybrid retrieval of bge-m3 in Milvus: pymilvus/examples\n/hello_hybrid_sparse_dense.py.\n2024/3/8: Thanks for the experimental results from @Yannael. In this benchmark, BGE-M3 achieves top performance in both English and other languages, surpassing models such as OpenAI.\n2024/3/2: Release unified fine-tuning example and data\n2024/2/6: We release the MLDR (a long document retrieval dataset covering 13 languages) and evaluation pipeline.\n2024/2/1: Thanks for the excellent tool from Vespa. You can easily use multiple modes of BGE-M3 following this notebook\nSpecs\nModel\nModel Name\nDimension\nSequence Length\nIntroduction\nBAAI/bge-m3\n1024\n8192\nmultilingual; unified fine-tuning (dense, sparse, and colbert) from bge-m3-unsupervised\nBAAI/bge-m3-unsupervised\n1024\n8192\nmultilingual; contrastive learning from bge-m3-retromae\nBAAI/bge-m3-retromae\n--\n8192\nmultilingual; extend the max_length of xlm-roberta to 8192 and further pretrained via retromae\nBAAI/bge-large-en-v1.5\n1024\n512\nEnglish model\nBAAI/bge-base-en-v1.5\n768\n512\nEnglish model\nBAAI/bge-small-en-v1.5\n384\n512\nEnglish model\nData\nDataset\nIntroduction\nMLDR\nDocuemtn Retrieval Dataset, covering 13 languages\nbge-m3-data\nFine-tuning data used by bge-m3\nFAQ\n1. Introduction for different retrieval methods\nDense retrieval: map the text into a single embedding, e.g., DPR, BGE-v1.5\nSparse retrieval (lexical matching): a vector of size equal to the vocabulary, with the majority of positions set to zero, calculating a weight only for tokens present in the text. e.g., BM25, unicoil, and splade\nMulti-vector retrieval: use multiple vectors to represent a text, e.g., ColBERT.\n2. How to use BGE-M3 in other projects?\nFor embedding retrieval, you can employ the BGE-M3 model using the same approach as BGE.\nThe only difference is that the BGE-M3 model no longer requires adding instructions to the queries.\nFor hybrid retrieval, you can use Vespa and Milvus.\n3. How to fine-tune bge-M3 model?\nYou can follow the common in this example\nto fine-tune the dense embedding.\nIf you want to fine-tune all embedding function of m3 (dense, sparse and colbert), you can refer to the unified_fine-tuning example\nUsage\nInstall:\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\npip install -e .\nor:\npip install -U FlagEmbedding\nGenerate Embedding for text\nDense Embedding\nfrom FlagEmbedding import BGEM3FlagModel\nmodel = BGEM3FlagModel('BAAI/bge-m3',\nuse_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\",\n\"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\nembeddings_1 = model.encode(sentences_1,\nbatch_size=12,\nmax_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n)['dense_vecs']\nembeddings_2 = model.encode(sentences_2)['dense_vecs']\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# [[0.6265, 0.3477], [0.3499, 0.678 ]]\nYou also can use sentence-transformers and huggingface transformers to generate dense embeddings.\nRefer to baai_general_embedding for details.\nSparse Embedding (Lexical Weight)\nfrom FlagEmbedding import BGEM3FlagModel\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\",\n\"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n# you can see the weight for each token:\nprint(model.convert_id_to_token(output_1['lexical_weights']))\n# [{'What': 0.08356, 'is': 0.0814, 'B': 0.1296, 'GE': 0.252, 'M': 0.1702, '3': 0.2695, '?': 0.04092},\n#  {'De': 0.05005, 'fin': 0.1368, 'ation': 0.04498, 'of': 0.0633, 'BM': 0.2515, '25': 0.3335}]\n# compute the scores via lexical mathcing\nlexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][0])\nprint(lexical_scores)\n# 0.19554901123046875\nprint(model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_1['lexical_weights'][1]))\n# 0.0\nMulti-Vector (ColBERT)\nfrom FlagEmbedding import BGEM3FlagModel\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True)\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\",\n\"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=True)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=True)\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][0]))\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][1]))\n# 0.7797\n# 0.4620\nCompute score for text pairs\nInput a list of text pairs, you can get the scores computed by different methods.\nfrom FlagEmbedding import BGEM3FlagModel\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True)\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\",\n\"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\nsentence_pairs = [[i,j] for i in sentences_1 for j in sentences_2]\nprint(model.compute_score(sentence_pairs,\nmax_passage_length=128, # a smaller max length leads to a lower latency\nweights_for_different_modes=[0.4, 0.2, 0.4])) # weights_for_different_modes(w) is used to do weighted sum: w[0]*dense_score + w[1]*sparse_score + w[2]*colbert_score\n# {\n#   'colbert': [0.7796499729156494, 0.4621465802192688, 0.4523794651031494, 0.7898575067520142],\n#   'sparse': [0.195556640625, 0.00879669189453125, 0.0, 0.1802978515625],\n#   'dense': [0.6259765625, 0.347412109375, 0.349853515625, 0.67822265625],\n#   'sparse+dense': [0.482503205537796, 0.23454029858112335, 0.2332356721162796, 0.5122477412223816],\n#   'colbert+sparse+dense': [0.6013619303703308, 0.3255828022956848, 0.32089319825172424, 0.6232916116714478]\n# }\nEvaluation\nWe provide the evaluation script for MKQA and MLDR\nBenchmarks from the open-source community\nThe BGE-M3 model emerged as the top performer on this benchmark (OAI is short for OpenAI).\nFor more details, please refer to the article and Github Repo\nOur results\nMultilingual (Miracl dataset)\nCross-lingual (MKQA dataset)\nLong Document Retrieval\nMLDR:\nPlease note that MLDR is a document retrieval dataset we constructed via LLM,\ncovering 13 languages, including test set, validation set, and training set.\nWe utilized the training set from MLDR to enhance the model's long document retrieval capabilities.\nTherefore, comparing baselines with Dense w.o.long(fine-tuning without long document dataset) is more equitable.\nAdditionally, this long document retrieval dataset will be open-sourced to address the current lack of open-source multilingual long text retrieval datasets.\nWe believe that this data will be helpful for the open-source community in training document retrieval models.\nNarritiveQA:\nComparison with BM25\nWe utilized Pyserini to implement BM25, and the test results can be reproduced by this script.\nWe tested BM25 using two different tokenizers:\none using Lucene Analyzer and the other using the same tokenizer as M3 (i.e., the tokenizer of xlm-roberta).\nThe results indicate that BM25 remains a competitive baseline,\nespecially in long document retrieval.\nTraining\nSelf-knowledge Distillation: combining multiple outputs from different\nretrieval modes as reward signal to enhance the performance of single mode(especially for sparse retrieval and multi-vec(colbert) retrival)\nEfficient Batching: Improve the efficiency when fine-tuning on long text.\nThe small-batch strategy is simple but effective, which also can used to fine-tune large embedding model.\nMCLS: A simple method to improve the performance on long text without fine-tuning.\nIf you have no enough resource to fine-tuning model with long text, the method is useful.\nRefer to our report for more details.\nAcknowledgement\nThanks to the authors of open-sourced datasets, including Miracl, MKQA, NarritiveQA, etc.\nThanks to the open-sourced libraries like Tevatron, Pyserini.\nCitation\nIf you find this repository useful, please consider giving a star :star: and citation\n@misc{bge-m3,\ntitle={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\nauthor={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\nyear={2024},\neprint={2402.03216},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "deepseek-ai/DeepSeek-R1": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "ibm-granite/granite-docling-258M": "granite-docling-258m\nGetting started\nIntended Use\nEvaluations\nSupported Instructions\nModel Architecture:\nTroubleshooting\ngranite-docling-258m\nGranite Docling is a multimodal Image-Text-to-Text model engineered for efficient document conversion. It preserves the core features of Docling while maintaining seamless integration with DoclingDocuments to ensure full compatibility.\nModel Summary:\nGranite Docling 258M builds upon the Idefics3 architecture, but introduces two key modifications: it replaces the vision encoder with siglip2-base-patch16-512 and substitutes the language model with a Granite 165M LLM. Try out our Granite-Docling-258 demo today.\nDeveloped by: IBM Research\nModel type: Multi-modal model (image+text-to-text)\nLanguage(s): English (NLP)\nLicense: Apache 2.0\nRelease Date: September 17, 2025\nGranite-docling-258M is fully integrated into the Docling pipelines, carrying over existing features while introducing a number of powerful new features, including:\nüî¢ Enhanced Equation Recognition: More accurate detection and formatting of mathematical formulas\nüß© Flexible Inference Modes: Choose between full-page inference, bbox-guided region inference\nüßò Improved Stability: Tends to avoid infinite loops more effectively\nüßÆ Enhanced Inline Equations: Better inline math recognition\nüßæ Document Element QA: Answer questions about a document‚Äôs structure such as the presence and order of document elements\nüåç Japanese, Arabic and Chinese support (experimental)\nGetting started\nThe easiest way to use this model is through the üê•Docling library. It will automatically download this model and convert documents to various formats for you.\nInstall the latest version of docling through pip, then use the following CLI command:\n# Convert to HTML and Markdown:\ndocling --to html --to md --pipeline vlm --vlm-model granite_docling \"https://arxiv.org/pdf/2501.17887\" # accepts files, urls or directories\n# Convert to HTML including layout visualization:\ndocling --to html_split_page --show-layout --pipeline vlm --vlm-model granite_docling \"https://arxiv.org/pdf/2501.17887\"\nYou can also set this model up within the Docling SDK:\nfrom docling.datamodel import vlm_model_specs\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\nVlmPipelineOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.pipeline.vlm_pipeline import VlmPipeline\nsource = \"https://arxiv.org/pdf/2501.17887\"\n###### USING SIMPLE DEFAULT VALUES\n# - GraniteDocling model\n# - Using the transformers framework\nconverter = DocumentConverter(\nformat_options={\nInputFormat.PDF: PdfFormatOption(\npipeline_cls=VlmPipeline,\n),\n}\n)\ndoc = converter.convert(source=source).document\nprint(doc.export_to_markdown())\n###### USING MACOS MPS ACCELERATOR\n# For more options see the compare_vlm_models.py example.\npipeline_options = VlmPipelineOptions(\nvlm_options=vlm_model_specs.GRANITEDOCLING_MLX,\n)\nconverter = DocumentConverter(\nformat_options={\nInputFormat.PDF: PdfFormatOption(\npipeline_cls=VlmPipeline,\npipeline_options=pipeline_options,\n),\n}\n)\ndoc = converter.convert(source=source).document\nprint(doc.export_to_markdown())\nAlternatively, you can use bare transformers, vllm, onnx or mlx-vlm to perform inference, and docling-core APIs to convert results to variety of output formats (md, html, etc.):\nüìÑ Single page image inference using plain ü§ó tranformers ü§ñ\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load images\nimage = load_image(\"https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/new_arxiv.png\")\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"ibm-granite/granite-docling-258M\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"ibm-granite/granite-docling-258M\",\ntorch_dtype=torch.bfloat16,\n_attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"sdpa\",\n).to(DEVICE)\n# Create input messages\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n]\n},\n]\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\ntrimmed_generated_ids,\nskip_special_tokens=False,\n)[0].lstrip()\nprint(f\"DocTags: \\n{doctags}\\n\")\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\nprint(f\"Markdown:\\n{doc.export_to_markdown()}\\n\")\n## export as any format.\n# Path(\"out/\").mkdir(parents=True, exist_ok=True)\n# HTML:\n# output_path_html = Path(\"out/\") / \"example.html\"\n# doc.save_as_html(output_path_html)\n# Markdown:\n# output_path_md = Path(\"out/\") / \"example.md\"\n# doc.save_as_markdown(output_path_md)\nüöÄ Fast Batch Inference with VLLM\n# Prerequisites:\n# pip install vllm\n# pip install docling_core\n# place page images you want to convert into \"img/\" dir\nimport time\nimport os\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoProcessor\nfrom PIL import Image\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom pathlib import Path\n# Configuration\nMODEL_PATH = \"ibm-granite/granite-docling-258M\"\nIMAGE_DIR = \"img/\"  # Place your page images here\nOUTPUT_DIR = \"out/\"\nPROMPT_TEXT = \"Convert this page to docling.\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": PROMPT_TEXT},\n],\n},\n]\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# Initialize LLM\nllm = LLM(model=MODEL_PATH, revision=\"untied\", limit_mm_per_prompt={\"image\": 1})\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nsampling_params = SamplingParams(\ntemperature=0.0,\nmax_tokens=8192,\nskip_special_tokens=False,\n)\n# Load and prepare all images and prompts up front\nbatched_inputs = []\nimage_names = []\nfor img_file in sorted(os.listdir(IMAGE_DIR)):\nif img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\nimg_path = os.path.join(IMAGE_DIR, img_file)\nwith Image.open(img_path) as im:\nimage = im.convert(\"RGB\")\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\nbatched_inputs.append({\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}})\nimage_names.append(os.path.splitext(img_file)[0])\n# Run batch inference\nstart_time = time.time()\noutputs = llm.generate(batched_inputs, sampling_params=sampling_params)\n# Postprocess all results\nfor img_fn, output, input_data in zip(image_names, outputs, batched_inputs):\ndoctags = output.outputs[0].text\noutput_path_dt = Path(OUTPUT_DIR) / f\"{img_fn}.dt\"\noutput_path_md = Path(OUTPUT_DIR) / f\"{img_fn}.md\"\nwith open(output_path_dt, \"w\", encoding=\"utf-8\") as f:\nf.write(doctags)\n# Convert to DoclingDocument and save markdown\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [input_data[\"multi_modal_data\"][\"image\"]])\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\ndoc.save_as_markdown(output_path_md)\nprint(f\"Total time: {time.time() - start_time:.2f} sec\")\nüíª Local inference on Apple Silicon with MLX: see here\n‚ÑπÔ∏è If you see trouble running granite-docling with the codes above, check the troubleshooting section at the bottom ‚¨áÔ∏è.\nIntended Use\nGranite-Docling is designed to complement the Docling library, not replace it. It integrates as a component within larger Docling library, consolidating the functions of multiple single-purpose models into a single, compact VLM.\nHowever, Granite-Docling is not intended for general image understanding. For tasks focused solely on image-text input, we recommend using Granite Vision models, which are purpose-built and optimized for image-text processing.\nEvaluations\nA comprehensive discussion of evaluation methods and findings has already been presented in our previous publication [citation]. As this model is an update, we refer readers to that work for additional details.\nThe evaluation can be performed using the docling-eval framework for the document related tasks, and lmms-eval for MMStar and OCRBench.\nLayout\nMAP ‚Üë\nF1 ‚Üë\nPrecision ‚Üë\nRecall ‚Üë\nsmoldocling-256m-preview\n0.230.850.90.84\ngranite-docling-258m\n0.270.860.920.88\nFull Page OCR\nEdit-distance ‚Üì\nF1 ‚Üë\nPrecision ‚Üë\nRecall ‚Üë\nBLEU ‚Üë\nMeteor ‚Üë\nsmoldocling-256m-preview\n0.480.800.89\n0.790.580.67\ngranite-docling-258m\n0.450.840.91\n0.830.650.72\nCode Recognition\nEdit-distance ‚Üì\nF1 ‚Üë\nPrecision ‚Üë\nRecall ‚Üë\nBLEU ‚Üë\nMeteor ‚Üë\nsmoldocling-256m-preview\n0.1140.9150.940.9090.8750.889\ngranite-docling-258m\n0.0130.9880.990.988\n0.9830.986\nEquation Recognition\nEdit-distance ‚Üì\nF1 ‚Üë\nPrecision ‚Üë\nRecall ‚Üë\nBLEU ‚Üë\nMeteor ‚Üë\nsmoldocling-256m-preview\n0.1190.9470.9590.9410.8240.878\ngranite-docling-258m\n0.0730.9680.9680.969\n0.8930.927\nTable Recognition (FinTabNet 150dpi)\nTEDS (structure) ‚Üë\nTEDS (w/content) ‚Üë\nsmoldocling-256m-preview\n0.820.76\ngranite-docling-258m\n0.970.96\nOther Benchmarks\nMMStar ‚Üë\nOCRBench ‚Üë\nsmoldocling-256m-preview\n0.17338\ngranite-docling-258m\n0.30500\nüíª Local inference on Apple Silicon with MLX: see here\nSupported Instructions\nDescription\nInstruction\nShort Instruction\nFull conversion\nConvert this page to docling.\n-\nChart\nConvert chart to table.\n<chart>\nFormula\nConvert formula to LaTeX.\n<formula>\nCode\nConvert code to text.\n<code>\nTable\nConvert table to OTSL. (Lysak et al., 2023)\n<otsl>\nActions and Pipelines\nOCR the text in a specific location: <loc_155><loc_233><loc_206><loc_237>\n-\nIdentify element at: <loc_247><loc_482><loc_252><loc_486>\n-\nFind all 'text' elements on the page, retrieve all section headers.\n-\nDetect footer elements on the page.\n-\nModel Architecture:\nThe architecture of granite-docling-258m consists of the following components:\n(1) Vision encoder: siglip2-base-patch16-512.\n(2) Vision-language connector: pixel shuffle projector (as in idefics3)\n(3) Large language model: Granite 165M.\nWe built upon Idefics3 to train our model. We incorporated DocTags into our LLM‚Äôs supervised fine-tuning (SFT) data to help the model become familiar with the format, enabling faster convergence and mitigating issues previously observed with SmolDocling.\nThe model was trained using the nanoVLM framework, which provides a lightweight and efficient training setup for vision-language models\nTraining Data: Our training corpus consists of two principal sources: (1) publicly available datasets and (2) internally constructed synthetic datasets designed to elicit specific document understanding capabilities.\nIn particular, we incorporate:\nSynthCodeNet ‚Äî a large-scale collection of synthetically rendered code snippets spanning over 50 programming languages\nSynthFormulaNet ‚Äî a dataset of synthetic mathematical expressions paired with ground-truth LaTeX representations\nSynthChartNet ‚Äî synthetic chart images annotated with structured table outputs\nDoclingMatix ‚Äî a curated corpus of real-world document pages sampled from diverse domains\nInfrastructure: We train granite-docling-258m using IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\nResponsible Use and Limitations Some use cases for Vision Language Models can trigger certain risks and ethical considerations, including but not limited to: bias and fairness, misinformation, and autonomous decision-making.\nAlthough our alignment processes include safety considerations, the model may in some cases produce inaccurate, biased, offensive or unwanted responses to user prompts. Additionally, whether smaller models may exhibit increased susceptibility\nto hallucination in generation scenarios due to their reduced sizes, which could limit their ability to generate coherent and contextually accurate responses, remains uncertain. This aspect is currently an active area of research,\nand we anticipate more rigorous exploration, comprehension, and mitigations in this domain. We urge the community to use granite-docling-258m in a responsible way and avoid any malicious utilization. We recommend using this model only as part of the Docling library.\nMore general vision tasks may pose higher inherent risks of triggering unwanted output. To enhance safety, we recommend using granite-docling-258m alongside Granite Guardian. Granite Guardian is a fine-tuned instruct model designed to detect and flag risks in prompts and responses across key dimensions outlined in the IBM AI Risk Atlas.\nIts training, which includes both human-annotated and synthetic data informed by internal red-teaming, enables it to outperform similar open-source models on standard benchmarks, providing an additional layer of safety.\nResources\n‚≠êÔ∏è Learn about the latest updates with Docling: https://docling-project.github.io/docling/#features\nüöÄ Get started with Docling concepts, integrations and tutorials: https://docling-project.github.io/docling/getting_started/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\nüñ•Ô∏è Learn more about how to use Granite-Docling, explore the Docling library, and see what‚Äôs coming next for Docling in the release blog: https://ibm.com/new/announcements/granite-docling-end-to-end-document-conversion\nTroubleshooting\nRunning with VLLM\nYou receive AttributeError: 'LlamaModel' object has no attribute 'wte' when launching the model through VLLM.\nWith current versions of VLLM (including 0.10.2), support for tied weights as used in granite-docling is limited and breaks. We provide a version with untied weights on the untied branch of this model repo.\nTo use the untied version, please pass the revision argument to VLLM:\n# Serve the model through VLLM\n$> vllm serve ibm-granite/granite-docling-258M --revision untied\n# If using the VLLM python SDK:\nfrom vllm import LLM\n...\nllm = LLM(model=MODEL_PATH, revision=\"untied\", limit_mm_per_prompt={\"image\": 1})\nThe model outputs only exclamation marks (i.e. \"!!!!!!!!!!!!!!!\").\nThis is seen on older NVIDIA GPUs, such as the T4 GPU available in Google Colab, because it lacks support for bfloat16 format.\nYou can work around it by setting the dtype to float32.\n# Serve the model through VLLM\n$> vllm serve ibm-granite/granite-docling-258M --revision untied --dtype float32\n# If using the VLLM python SDK:\nfrom vllm import LLM\n...\nllm = LLM(model=MODEL_PATH, revision=\"untied\", limit_mm_per_prompt={\"image\": 1}, dtype=\"float32\")",
    "Qwen/Qwen3-4B-Instruct-2507": "Qwen3-4B-Instruct-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nQwen3-4B-Instruct-2507\nHighlights\nWe introduce the updated version of the Qwen3-4B non-thinking mode, named Qwen3-4B-Instruct-2507, featuring the following key enhancements:\nSignificant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.\nSubstantial gains in long-tail knowledge coverage across multiple languages.\nMarkedly better alignment with user preferences in subjective and open-ended tasks, enabling more helpful responses and higher-quality text generation.\nEnhanced capabilities in 256K long-context understanding.\nModel Overview\nQwen3-4B-Instruct-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 4.0B\nNumber of Paramaters (Non-Embedding): 3.6B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 32 for Q and 8 for KV\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nGPT-4.1-nano-2025-04-14\nQwen3-30B-A3B Non-Thinking\nQwen3-4B Non-Thinking\nQwen3-4B-Instruct-2507\nKnowledge\nMMLU-Pro\n62.8\n69.1\n58.0\n69.6\nMMLU-Redux\n80.2\n84.1\n77.3\n84.2\nGPQA\n50.3\n54.8\n41.7\n62.0\nSuperGPQA\n32.2\n42.2\n32.0\n42.8\nReasoning\nAIME25\n22.7\n21.6\n19.1\n47.4\nHMMT25\n9.7\n12.0\n12.1\n31.0\nZebraLogic\n14.8\n33.2\n35.2\n80.2\nLiveBench 20241125\n41.5\n59.4\n48.4\n63.0\nCoding\nLiveCodeBench v6 (25.02-25.05)\n31.5\n29.0\n26.4\n35.1\nMultiPL-E\n76.3\n74.6\n66.6\n76.8\nAider-Polyglot\n9.8\n24.4\n13.8\n12.9\nAlignment\nIFEval\n74.5\n83.7\n81.2\n83.4\nArena-Hard v2*\n15.9\n24.8\n9.5\n43.4\nCreative Writing v3\n72.7\n68.1\n53.6\n83.5\nWritingBench\n66.9\n72.2\n68.5\n83.4\nAgent\nBFCL-v3\n53.0\n58.6\n57.6\n61.9\nTAU1-Retail\n23.5\n38.3\n24.3\n48.7\nTAU1-Airline\n14.0\n18.0\n16.0\n32.0\nTAU2-Retail\n-\n31.6\n28.1\n40.4\nTAU2-Airline\n-\n18.0\n12.0\n24.0\nTAU2-Telecom\n-\n18.4\n17.5\n13.2\nMultilingualism\nMultiIF\n60.7\n70.8\n61.3\n69.0\nMMLU-ProX\n56.2\n65.1\n49.6\n61.6\nINCLUDE\n58.6\n67.8\n53.8\n60.1\nPolyMATH\n15.6\n23.3\n16.6\n31.1\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Instruct-2507 --context-length 262144\nvLLM:vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-4B-Instruct-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "Wan-AI/Wan2.2-Animate-14B": "Wan2.2\nVideo Demos\nüî• Latest News!!\nCommunity Works\nüìë Todo List\nRun Wan2.2 Animate\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.2\nüíú Wan ¬†¬† ÔΩú ¬†¬† üñ•Ô∏è GitHub ¬†¬†  | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Paper ¬†¬† | ¬†¬† üìë Blog ¬†¬† |  ¬†¬† üí¨  Discord\nüìï ‰ΩøÁî®ÊåáÂçó(‰∏≠Êñá)¬†¬† | ¬†¬† üìò User Guide(English)¬†¬† | ¬†¬†üí¨ WeChat(ÂæÆ‰ø°)\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nüëç Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nüëç Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nüëç Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nüëç Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nVideo Demos\nYour browser does not support the video tag.\nüî• Latest News!!\nSep 19, 2025: üíÉ We introduct Wan2.2-Animate-14B, an unified model for character animation and replacement with holistic movement and expression replication. We released the model weights and inference code. And now you can try it on wan.video, ModelScope Studio or HuggingFace Space!\nAug 26, 2025: üéµ We introduce Wan2.2-S2V-14B, an audio-driven cinematic video generation model, including inference code, model weights, and technical report! Now you can try it on wan.video,  ModelScope Gradio or HuggingFace Gradio!\nJul 28, 2025: üëã We have open a HF space using the TI2V-5B model. Enjoy!\nJul 28, 2025: üëã Wan2.2 has been integrated into ComfyUI (CN | EN). Enjoy!\nJul 28, 2025: üëã Wan2.2's T2V, I2V and TI2V have been integrated into Diffusers (T2V-A14B | I2V-A14B | TI2V-5B). Feel free to give it a try!\nJul 28, 2025: üëã We've released the inference code and model weights of Wan2.2.\nSep 5, 2025: üëã We add text-to-speech synthesis support with CosyVoice for Speech-to-Video generation task.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, and you would like more people to see it, please inform us.\nDiffSynth-Studio provides comprehensive support for Wan 2.2, including low-GPU-memory layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, full training.\nKijai's ComfyUI WanVideoWrapper is an alternative implementation of Wan models for ComfyUI. Thanks to its Wan-only focus, it's on the frontline of getting cutting edge optimizations and hot research features, which are often hard to integrate into ComfyUI quickly due to its more rigid structure.\nCache-dit offers Fully Cache Acceleration support for Wan2.2 MoE with DBCache, TaylorSeer and Cache CFG. Visit their example for more details.\nFastVideo includes distilled Wan models with sparse attention that significanly speed up the inference time.\nüìë Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nWan2.2-S2V Speech-to-Video\nInference code of Wan2.2-S2V\nCheckpoints of Wan2.2-S2V-14B\nComfyUI integration\nDiffusers integration\nWan2.2-Animate Character Animation and Replacement\nInference code of Wan2.2-Animate\nCheckpoints of Wan2.2-Animate\nComfyUI integration\nDiffusers integration\nRun Wan2.2 Animate\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\n# If you want to use CosyVoice to synthesize speech for Speech-to-Video Generation, please install requirements_s2v.txt additionally\npip install -r requirements_s2v.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nü§ó Huggingface     ü§ñ ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nS2V-14B\nü§ó Huggingface     ü§ñ ModelScope\nSpeech-to-Video model, supports 480P & 720P\nAnimate-14B\nü§ó Huggingface ü§ñ ModelScope\nCharacter animation and replacement\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-Animate-14B --local-dir ./Wan2.2-Animate-14B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-Animate-14B --local_dir ./Wan2.2-Animate-14B\nRun Wan-Animate-14B\nWan-Animate takes a video and a character image as input, and generates a video in either \"animation\" or \"replacement\" mode.\nanimation modeÔºö The model generates a video of the character image that mimics the human motion in the input video.\nreplacement mode: The model replaces the character image with the input video.\nPlease visit our project page to see more examples and learn about the scenarios suitable for this model.\n(1) Preprocessing\nThe input video should be preprocessed into several materials before be feed into the inference process.  Please refer to the following processing flow, and more details about preprocessing can be found in UserGuider.\nFor animation\npython ./wan/modules/animate/preprocess/preprocess_data.py \\\n--ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\\n--video_path ./examples/wan_animate/animate/video.mp4 \\\n--refer_path ./examples/wan_animate/animate/image.jpeg \\\n--save_path ./examples/wan_animate/animate/process_results \\\n--resolution_area 1280 720 \\\n--retarget_flag \\\n--use_flux\nFor replacement\npython ./wan/modules/animate/preprocess/preprocess_data.py \\\n--ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\\n--video_path ./examples/wan_animate/replace/video.mp4 \\\n--refer_path ./examples/wan_animate/replace/image.jpeg \\\n--save_path ./examples/wan_animate/replace/process_results \\\n--resolution_area 1280 720 \\\n--iterations 3 \\\n--k 7 \\\n--w_len 1 \\\n--h_len 1 \\\n--replace_flag\n(2) Run in animation mode\nSingle-GPU inference\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1 --dit_fsdp --t5_fsdp --ulysses_size 8\n(3) Run in replacement mode\nSingle-GPU inference\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/ --refert_num 1 --replace_flag --use_relighting_lora\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/src_pose.mp4  --refert_num 1 --replace_flag --use_relighting_lora --dit_fsdp --t5_fsdp --ulysses_size 8\nüí° If you're using Wan-Animate, we do not recommend using LoRA models trained on Wan2.2, since weight changes during training may lead to unexpected behavior.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "Qwen/Qwen3-VL-235B-A22B-Instruct": "Qwen3-VL-235B-A22B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nCitation\nQwen3-VL-235B-A22B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-235B-A22B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-235B-A22B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-235B-A22B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "vandijklab/C2S-Scale-Gemma-2-27B": "C2S-Scale-Gemma-27B model card\nModel information\nDescription\nHow to use\nExamples\nModel architecture overview\nTechnical Specifications\nPerformance & Validation\nInputs and outputs\nDataset details\nTraining dataset\nEvaluation dataset\nLicense\nImplementation information\nSoftware\nUse and limitations\nIntended use\nBenefits\nLimitations\nCitation\nC2S-Scale Links\nGemma-2 Links\nC2S-Scale-Gemma-27B model card\nGitHub homepage: Cell2Sentence GitHub\nModel documentation: Cell2Sentence Documentation\nResources:\nC2S-Scale Paper: Scaling Large Language Models for Next-Generation Single-Cell Analysis\nHuggingFace C2S Collection: C2S-Scale Models\nGitHub Repository: vandijklab/cell2sentence (for code, tutorials, and discussions)\nGoogle Research Blog Post: Teaching machines the language of biology\nAuthor: van Dijk Lab (Yale), Google Research, Google DeepMind\nModel information\nThis section describes the C2S-Scale model and how to use it.\nDescription\nC2S-Scale-Gemma-27B is a state-of-the-art, open language model built upon the Gemma-2 27B\narchitecture and fine-tuned for single-cell biology. Developed through the Cell2Sentence\n(C2S) framework, the model processes and understands single-cell RNA sequencing\n(scRNA-seq) data by treating it as a language. It converts high-dimensional scRNA-seq\nexpression data into \"cell sentences\" - ordered sequences of gene names - enabling a\nwide range of biological analyses.\nThis work is the result of a collaboration between Yale University, Google Research,\nand Google DeepMind to scale up C2S models. The C2S-Scale models were trained on\nGoogle's TPU v5s, which allowed for a significant increase in model size and\ncapability. These models excel at tasks such as cell type prediction, tissue\nclassification, and generating biologically meaningful cell representations.\nKey Features\nVersatility: Demonstrates strong performance across a diverse set of single-cell and multi-cell tasks.\nScalability: Trained on a massive dataset of over 57 million cells, showcasing the power of scaling LLMs for biological data.\nGenerative Power: Capable of generating realistic single-cell gene expression profiles.\nFoundation for Fine-tuning: Can serve as a powerful pretrained foundation for specialized, domain-specific single-cell analysis tasks.\nPotential Applications\nC2S-Scale can be a valuable tool for researchers in the following areas:\nIn Silico Experiments: Generate cells under specific conditions or predict perturbational changes to form and test new biological hypotheses.\nCell Atlas Annotation: Streamline the process of annotating large-scale single-cell datasets by predicting cell types and tissues.\nBiomarker Discovery: Analyze gene patterns within cell sentences to identify potential markers for specific cell states or diseases.\nHow to use\nBelow are code snippets to help you get started running the model locally on a GPU.\nThe model can be used for various tasks, further described in the C2S-Scale paper.\nFormatting prompts for cell type prediction\nTo perform cell type prediction, the model expects a prompt containing the cell sentence followed by a query.\n# A \"cell sentence\" is a space-separated string of gene names\n# ordered by expression level, from highest to lowest.\ncell_sentence = \"MALAT1 TMSB4X B2M EEF1A1 H3F3B ACTB FTL RPL13 ...\" # Truncated for example purposes\nnum_genes = 1000\norganism = \"Homo sapiens\"\n# Construct the prompt for cell type prediction\nprompt = f\"\"\"The following is a list of {num_genes} gene names ordered by descending expression level in a {organism} cell. Your task is to give the cell type which this cell belongs to based on its gene expression.\nCell sentence: {cell_sentence}.\nThe cell type corresponding to these genes is:\"\"\"\nprint(prompt)\nThe resulting prompt is in the format expected by the model for this task:\nThe following is a list of 1000 gene names ordered by descending expression level in a Homo sapiens cell. Your task is to give the cell type which this cell belongs to based on its gene expression.\nCell sentence: MALAT1 TMSB4X B2M EEF1A1 H3F3B ACTB FTL RPL13 ... .\nThe cell type corresponding to these genes is:\nRunning the model on predictive tasks\n# pip install accelerate transformers sentencepiece\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load model directly from Hugging Face Hub\nmodel_id = \"vandijklab/C2S-Scale-Gemma-2-27B\"\n# Load tokenizer; requires sentencepiece to be installed\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\n).to(device)\n# Format prompt (see previous section)\ncell_sentence = \"MALAT1 TMSB4X B2M EEF1A1 H3F3B ACTB FTL RPL13 ...\" # Truncated for example, use at least 200 genes for inference\nnum_genes = 1000\norganism = \"Homo sapiens\"\nprompt = f\"\"\"The following is a list of {num_genes} gene names ordered by descending expression level in a {organism} cell. Your task is to give the cell type which this cell belongs to based on its gene expression.\nCell sentence: {cell_sentence}.\nThe cell type corresponding to these genes is:\"\"\"\n# Prepare tokenized inputs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n# Generate response\noutputs = model.generate(**input_ids, max_new_tokens=20)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# The predicted cell type will be the text immediately following the prompt\npredicted_cell_type = response.split(\"The cell type corresponding to these genes is:\")[1].strip()\nprint(f\"Predicted Cell Type: {predicted_cell_type}\")\nExamples\nSee the following Colab notebooks in our GitHub repository for examples of how to use C2S-Scale models:\nTo quickly get started with the model for tasks like cell type prediction and generation: C2S Tutorials\nModel architecture overview\nC2S-Scale is based on the Gemma 2 family of lightweight, state-of-the-art open LLMs, which utilizes a decoder-only transformer architecture.\nBase Model: Gemma-2 27B.\nFine-tuning Data: A comprehensive collection of over 800 datasets from CellxGene and the Human Cell Atlas, totaling over 57 million human and mouse cells.\nTraining Approach: Instruction fine-tuning using the Cell2Sentence framework, which converts scRNA-seq expression data into sequences of gene tokens.\nTechnical Specifications\nModel type: Decoder-only Transformer (based on Gemma-2)\nKey publication: Scaling Large Language Models for Next-Generation Single-Cell Analysis\nPerformance & Validation\nThe performance of C2S-Scale models was validated on a wide range of single-cell and multi-cell\ntasks, including advanced downstream tasks such as cluster captioning, question answering,\nand perturbation prediction. C2S-Scale models demonstrated significant improvements over\nother open and closed-source models, establishing new state-of-the-art benchmarks for LLMs\nin single-cell biology. Please see our preprint for a full breakdown of performance metrics.\nInputs and outputs\nInput: Text. For best performance, prompts should be structured according to the specific task (e.g., cell type prediction, conditioned generation). Inputs are \"cell sentences\"‚Äîordered, space-separated lists of gene names.\nOutput: Text. The model generates text as a response, which can be a predicted label (like a cell type or tissue), a full cell sentence, or a natural language abstract.\nDataset details\nTraining dataset\nCellxGene and Human Cell Atlas: The model was trained on a curated collection of over 800\npublic scRNA-seq datasets, encompassing more than 57 million cells. This data covers a broad\nrange of tissues, cell types, and experimental conditions from both human and mouse, ensuring\nthe model learns a robust and generalizable representation of cellular states.\nEvaluation dataset\nEvaluation was performed using held-out datasets and standardized benchmarks designed to\ntest the model's capabilities on the tasks listed above. All evaluation methodologies followed\nestablished best practices for splitting data to ensure robust and unbiased assessment.\nLicense\nThe model weights shared on Huggingface are CC-by-4.0.\nImplementation information\nSoftware\nThe model was trained using JAX, leveraging Google's TPU v5\nhardware for efficient and large-scale training.\nUse and limitations\nIntended use\nResearch in single-cell genomics and computational biology.\nAs a foundational model for fine-tuning on specific biological domains or datasets.\nTo aid in the annotation and interpretation of large-scale scRNA-seq experiments.\nBenefits\nC2S-Scale provides a powerful, versatile, and scalable tool for single-cell analysis. It offers:\nState-of-the-art performance on a wide range of scRNA-seq tasks.\nA unified framework for handling diverse single-cell analysis challenges.\nA foundation for building more specialized models from private or proprietary data.\nThe ability to perform in silico generation of cellular data to explore biological hypotheses.\nLimitations\nThe model is trained on public data and its knowledge is limited to the genes, cell types, and conditions present in that data.\nPerformance on out-of-distribution data (e.g., completely novel cell types or technologies) is not guaranteed and requires validation.\nPerformance of the models on input prompt formats that greatly deviate from training prompt formatting is not guaranteed.\nCitation\n@article{Rizvi2025.04.14.648850,\nabstract = {Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In this work, we build upon the Cell2Sentence (C2S) framework, which represents scRNA-seq profiles as textual {\\textquotedblleft}cell sentences,{\\textquotedblright} to train Large Language Models (LLMs) on a corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata. By scaling model size to 27 billion parameters, we observe consistent improvements in predictive and generative capabilities, as well as the capacity for advanced downstream tasks requiring synthesis of information across multicellular contexts. Through targeted fine-tuning supported by modern reinforcement learning techniques, our approach excels in tasks such as perturbation response prediction, natural language interpretation, and complex biological reasoning. By unifying transcriptomic and textual data at unprecedented scales, this approach not only surpasses both specialized single-cell models and general-purpose LLMs, but also establishes a powerful platform for next-generation single-cell analysis, paving the way for the development of {\\textquotedblleft}virtual cells.{\\textquotedblright}Competing Interest StatementThe authors have declared no competing interest.},\nauthor = {Rizvi, Syed Asad and Levine, Daniel and Patel, Aakash and Zhang, Shiyang and Wang, Eric and He, Sizhuang and Zhang, David and Tang, Cerise and Lyu, Zhuoyang and Darji, Rayyan and Li, Chang and Sun, Emily and Jeong, David and Zhao, Lawrence and Kwan, Jennifer and Braun, David and Hafler, Brian and Ishizuka, Jeffrey and Dhodapkar, Rahul M. and Chung, Hattie and Azizi, Shekoofeh and Perozzi, Bryan and van Dijk, David},\ndoi = {10.1101/2025.04.14.648850},\nelocation-id = {2025.04.14.648850},\neprint = {https://www.biorxiv.org/content/early/2025/04/17/2025.04.14.648850.full.pdf},\njournal = {bioRxiv},\npublisher = {Cold Spring Harbor Laboratory},\ntitle = {Scaling Large Language Models for Next-Generation Single-Cell Analysis},\nurl = {https://www.biorxiv.org/content/early/2025/04/17/2025.04.14.648850},\nyear = {2025},\nBdsk-Url-1 = {https://www.biorxiv.org/content/early/2025/04/17/2025.04.14.648850},\nBdsk-Url-2 = {https://doi.org/10.1101/2025.04.14.648850}}\nC2S-Scale Links\nPaper: Scaling Large Language Models for Next-Generation Single-Cell Analysis\nGoogle Research Blog Post: Teaching machines the language of biology: Scaling large language models for next-generation single-cell analysis\nGitHub: https://github.com/vandijklab/cell2sentence (Note: Codebase has CC BY-NC-ND 4.0 license. Only weights shared on Hugging Face are CC-by-4.0)\nGemma-2 Links\nHuggingFace: https://huggingface.co/google/gemma-2-27b\nGemma-2 Blog Post: Gemma explained: What's new in Gemma 2\nTechnical report: https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
    "lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF": "Update:\nQuantized Models:\nHow to build and run for MacOS\nRun examples\nExample prompt and output\nUpdate:\nI have tested some of these smaller models on NVIDIA with default CUDA compile\nwith the excellent release from @cturan on NVIDIA L40S GPU.\nSince L40S GPU is 48GB VRAM, I was able to run Q2_K, Q3_K_M, Q4_K_S, Q4_0 and Q4_MXFP4_MOE:\nbut Q4_K_M was too big.\nAlthough it works if using -ngl 45\nbut it slowed down quite a bit.\nThere may be a better way but did not have time to test.\nWas able to get a good speed of 53 tokens per second in the generation\nand 800 tokens per second in the prompt reading.\nwget https://github.com/cturan/llama.cpp/archive/refs/tags/test.tar.gz\ntar xf test.tar.gz\ncd llama.cpp-test\n# export PATH=/usr/local/cuda/bin:$PATH\ntime cmake -B build -DGGML_CUDA=ON\ntime cmake --build build --config Release --parallel $(nproc --all)\nYou may need to add /usr/local/cuda/bin to your PATH\nto find nvcc (Nvidia CUDA compiler)\nBuilding from source took about 7 minutes.\nFor more detail on CUDA build see:\nhttps://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cuda\nQuantized Models:\nThese quantized models were generated using the excellent pull request from @pwilkin\n#16095\non 2025-10-19 with commit 2fdbf16eb.\nNOTE: currently they only work with the llama.cpp 16095 pull request which is still in development.\nSpeed and quality should improve over time.\nHow to build and run for MacOS\nPR=16095\ngit clone https://github.com/ggml-org/llama.cpp llama.cpp-PR-$PR\ncd llama.cpp-PR-$PR\ngit fetch origin pull/$PR/head:pr-$PR\ngit checkout pr-$PR\ntime cmake -B build\ntime cmake --build build --config Release --parallel $(nproc --all)\nRun examples\nRun with Hugging Face model:\nbuild/bin/llama-cli -hf lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF --prompt 'What is the capital of France?' --no-mmap -st\nby default will download lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF:Q4_K_M\nTo download:\nwget https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF/resolve/main/Qwen__Qwen3-Next-80B-A3B-Instruct-Q4_0.gguf\nor\npip install hf_transfer 'huggingface_hub[cli]'\nhf download lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF Qwen__Qwen3-Next-80B-A3B-Instruct-Q4_0.gguf\nRun with local model file:\nbuild/bin/llama-cli -m Qwen__Qwen3-Next-80B-A3B-Instruct-Q4_0.gguf --prompt 'Write a paragraph about quantum computing' --no-mmap -st\nExample prompt and output\nUser prompt:\nWrite a paragraph about quantum computing\nAssistant output:\nQuantum computing represents a revolutionary leap in computational power by harnessing the principles of quantum mechanics, such as superposition and entanglement, to process information in fundamentally new ways. Unlike classical computers, which use bits that are either 0 or 1, quantum computers use quantum bits, or qubits, which can exist in a combination of both states simultaneously. This allows quantum computers to explore vast solution spaces in parallel, making them potentially exponentially faster for certain problems‚Äîlike factoring large numbers, optimizing complex systems, or simulating molecular structures for drug discovery. While still in its early stages, with challenges including qubit stability, error correction, and scalability, quantum computing holds transformative promise for fields ranging from cryptography to artificial intelligence. As researchers and tech companies invest heavily in hardware and algorithmic development, the race to achieve practical, fault-tolerant quantum machines is accelerating, heralding a new era in computing technology.\n[end of text]",
    "Kijai/LongCat-Video_comfy": "LongCat-Video -model converted to run in ComfyUI (for testing with WanVideoWrapper currently)\nSource:\nhttps://huggingface.co/meituan-longcat/LongCat-Video",
    "Kijai/WanVideo_comfy_fp8_scaled": "14B-T2V comparison test without LoRAs, 25 steps, 832x480x81\n2.2 A14B-T2V test\nBetter fp8 scaled models (when measured against fp16) based on quantization code from https://github.com/Tencent-Hunyuan/HunyuanVideo/blob/main/hyvideo/modules/fp8_optimization.py\nCan be used with: https://github.com/kijai/ComfyUI-WanVideoWrapper (latest version) and ComfyUI native WanVideo nodes.\n14B-T2V comparison test without LoRAs, 25 steps, 832x480x81\n2.2 A14B-T2V test\nThe e5m2 marked as v2 is the one uploaded here and these are all scaled even if I forgot to label properly.",
    "huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated": "huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-8B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "inclusionAI/Ring-flash-linear-2.0-128k": "Ring-flash-linear-2.0-128k\nIntroduction\nEvaluation\nLinear Attention, Highly Sparse, High-Speed Generation\nQuickstart\nRequirements\nü§ó Hugging Face Transformers\nüöÄ SGLang\nüöÄ vLLM\nRing-flash-linear-2.0-128k\nüìñ  Technical Report¬†¬† | ¬†¬† ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope\nIntroduction\nWe are excited to announce the official open-source release of Ring-flash-linear-2.0-128k! Additionally, we are making our technical report publicly available.\nBuilding on the success of our Ling 2.0 series, this model continues to leverage a powerful hybrid architecture of linear and standard attention, perfectly balancing high performance with superior efficiency. By integrating our proven MoE design with optimizations like a 1/32 expert activation ratio and MTP layers, Ring-flash-linear achieves the performance of a 40B dense model while activating only 6.1B parameters.  This model was converted from Ling-flash-base-2.0, further trained on an additional 1T tokens.\nWhen it comes to benchmarks, Ring-flash-linear-2.0-128k not only holds its own against standard attention models (like Ring-flash-2.0) but also outperforms other open-source MoE and Dense models in its class on several demanding tasks. Plus, we natively support a 128K context window and can extend it to 512K using YaRN. It's faster and more precise than ever, especially when handling long-form inputs and outputs.\nFigure 1: Hybrid Linear Model Architecture\nEvaluation\nTo better demonstrate the model's capabilities, we selected representative open-source thinking models and closed-source APIs for comparison.\nWe present results on several challenging reasoning benchmarks spanning domains such as mathematics, agent, coding, and science. We observe that our model achieves performance on par with other models.\nFigure 2: Model Performance Comparison\nFigure 3: Model Performance Comparison\nLinear Attention, Highly Sparse, High-Speed Generation\nThanks to its hybrid attention mechanism and highly sparse MoE architecture, Ring-flash-linear-2.0-128k achieves near-linear time complexity and constant space complexity, resulting in outstanding inference efficiency.\nTo fully demonstrate this advantage, we conducted a comparison between our model and top-tier competitors of similar size or performance.\nThe results clearly demonstrate the advantage of our model in inference efficiency.\nFigure 4: Ring-flash-linear-2.0-128k prefill throughput\nFigure 5: Ring-flash-linear-2.0-128k decode throughput\nQuickstart\nRequirements\npip install flash-linear-attention==0.3.2\npip install transformers==4.56.1\nü§ó Hugging Face Transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-flash-linear-2.0-128k\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompts = [\n\"Give me a short introduction to large language models.\"\n]\ninput_texts = []\nfor prompt in prompts:\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput_texts.append(text)\nprint(input_texts)\nmodel_inputs = tokenizer(input_texts, return_tensors=\"pt\", return_token_type_ids=False, padding=True, padding_side='left').to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192,\ndo_sample=False,\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(\"*\" * 30)\nprint(responses)\nprint(\"*\" * 30)\nüöÄ SGLang\nEnvironment Preparation\nWe have submitted our PR to SGLang official release and it will be merged later, for now we can prepare the environment following steps, firstly install the community version SGLang and required packages:\npip install sglang==0.5.2 sgl-kernel==0.3.9.post2 vllm==0.10.2 torch==2.8.0 torchvision==0.23.0 torchao\nThen you should install our sglang wheel package:\npip install https://media.githubusercontent.com/media/inclusionAI/Ring-V2/refs/heads/main/hybrid_linear/whls/sglang-0.5.2-py3-none-any.whl --no-deps --force-reinstall\nRun Inference\nBF16 and FP8 models are supported by SGLang now, it depends on the dtype of the model in ${MODEL_PATH}. They both share the same command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path <model_path> \\\n--trust-remote-code \\\n--tp-size 4 \\\n--disable-radix-cache \\\n--tool-call-parser qwen25 \\\n--json-model-override-args \"{\\\"linear_backend\\\": \\\"seg_la\\\"}\"\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"temperature\": 0.6, \"messages\": [{\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}]}'\nMore usage can be found here\nüöÄ vLLM\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below:\npip install torch==2.7.0 torchvision==0.22.0\nThen you should install our vLLM wheel package:\npip install https://media.githubusercontent.com/media/inclusionAI/Ring-V2/refs/heads/main/hybrid_linear/whls/vllm-0.8.5%2Bcuda12_8_gcc10_2_1-cp310-cp310-linux_x86_64.whl --no-deps --force-reinstall\nOffline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ring-flash-linear-2.0-128k\")\nsampling_params = SamplingParams(temperature=0.6, top_p=1.0, max_tokens=8192)\nllm = LLM(model=\"inclusionAI/Ring-flash-linear-2.0-128k\", dtype='bfloat16', enable_prefix_caching=False)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nOnline Inference\nvllm serve inclusionAI/Ring-flash-linear-2.0-128k \\\n--tensor-parallel-size 4 \\\n--gpu-memory-utilization 0.90 \\\n--no-enable-prefix-caching\nCitation\n@misc{lingteam2025attentionmattersefficienthybrid,\ntitle={Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning},\nauthor={Ling Team and Bin Han and Caizhi Tang and Chen Liang and Donghao Zhang and Fan Yuan and Feng Zhu and Jie Gao and Jingyu Hu and Longfei Li and Meng Li and Mingyang Zhang and Peijie Jiang and Peng Jiao and Qian Zhao and Qingyuan Yang and Wenbo Shen and Xinxing Yang and Yalin Zhang and Yankun Ren and Yao Zhao and Yibo Cao and Yixuan Sun and Yue Zhang and Yuchen Fang and Zibin Lin and Zixuan Cheng and Jun Zhou},\nyear={2025},\neprint={2510.19338},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.19338},\n}",
    "stabilityai/stable-diffusion-xl-base-1.0": "SD-XL 1.0-base Model Card\nModel\nModel Description\nModel Sources\nEvaluation\nüß® Diffusers\nOptimum\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nSD-XL 1.0-base Model Card\nModel\nSDXL consists of an ensemble of experts pipeline for latent diffusion:\nIn a first step, the base model is used to generate (noisy) latents,\nwhich are then further processed with a refinement model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/) specialized for the final denoising steps.\nNote that the base model can be used as a standalone module.\nAlternatively, we can use a two-stage pipeline as follows:\nFirst, the base model is used to generate latents of the desired output size.\nIn the second step, we use a specialized high-resolution model and apply a technique called SDEdit (https://arxiv.org/abs/2108.01073, also known as \"img2img\")\nto the latents generated in the first step, using the same prompt. This technique is slightly slower than the first one, as it requires more function evaluations.\nSource code is available at https://github.com/Stability-AI/generative-models .\nModel Description\nDeveloped by: Stability AI\nModel type: Diffusion-based text-to-image generative model\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pretrained text encoders (OpenCLIP-ViT/G and CLIP-ViT/L).\nResources for more information: Check out our GitHub Repository and the SDXL report on arXiv.\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models), which implements the most popular diffusion frameworks (both training and inference) and for which new functionalities like distillation will be added over time.\nClipdrop provides free SDXL inference.\nRepository: https://github.com/Stability-AI/generative-models\nDemo: https://clipdrop.co/stable-diffusion\nEvaluation\nThe chart above evaluates user preference for SDXL (with and without refinement) over SDXL 0.9 and Stable Diffusion 1.5 and 2.1.\nThe SDXL base model performs significantly better than the previous variants, and the model combined with the refinement module achieves the best overall performance.\nüß® Diffusers\nMake sure to upgrade diffusers to >= 0.19.0:\npip install diffusers --upgrade\nIn addition make sure to install transformers, safetensors, accelerate as well as the invisible watermark:\npip install invisible_watermark transformers accelerate safetensors\nTo just use the base model, you can run:\nfrom diffusers import DiffusionPipeline\nimport torch\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\nprompt = \"An astronaut riding a green horse\"\nimages = pipe(prompt=prompt).images[0]\nTo use the whole base + refiner pipeline as an ensemble of experts you can run:\nfrom diffusers import DiffusionPipeline\nimport torch\n# load both base & refiner\nbase = DiffusionPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n)\nbase.to(\"cuda\")\nrefiner = DiffusionPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-refiner-1.0\",\ntext_encoder_2=base.text_encoder_2,\nvae=base.vae,\ntorch_dtype=torch.float16,\nuse_safetensors=True,\nvariant=\"fp16\",\n)\nrefiner.to(\"cuda\")\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\nprompt = \"A majestic lion jumping from a big stone at night\"\n# run both experts\nimage = base(\nprompt=prompt,\nnum_inference_steps=n_steps,\ndenoising_end=high_noise_frac,\noutput_type=\"latent\",\n).images\nimage = refiner(\nprompt=prompt,\nnum_inference_steps=n_steps,\ndenoising_start=high_noise_frac,\nimage=image,\n).images[0]\nWhen using torch >= 2.0, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline:\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\nIf you are limited by GPU VRAM, you can enable cpu offloading by calling pipe.enable_model_cpu_offload\ninstead of .to(\"cuda\"):\n- pipe.to(\"cuda\")\n+ pipe.enable_model_cpu_offload()\nFor more information on how to use Stable Diffusion XL with diffusers, please have a look at the Stable Diffusion XL Docs.\nOptimum\nOptimum provides a Stable Diffusion pipeline compatible with both OpenVINO and ONNX Runtime.\nOpenVINO\nTo install Optimum with the dependencies required for OpenVINO :\npip install optimum[openvino]\nTo load an OpenVINO model and run inference with OpenVINO Runtime, you need to replace StableDiffusionXLPipeline with Optimum OVStableDiffusionXLPipeline. In case you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set export=True.\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\nYou can find more examples (such as static reshaping and model compilation) in optimum documentation.\nONNX\nTo install Optimum with the dependencies required for ONNX Runtime inference :\npip install optimum[onnxruntime]\nTo load an ONNX model and run inference with ONNX Runtime, you need to replace StableDiffusionXLPipeline with Optimum ORTStableDiffusionXLPipeline. In case you want to load a PyTorch model and convert it to the ONNX format on-the-fly, you can set export=True.\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"A majestic lion jumping from a big stone at night\"\nimage = pipeline(prompt).images[0]\nYou can find more examples in optimum documentation.\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases."
}