{
    "GoodiesHere/Apollo-LMMs-Apollo-3B-t32": "Apollo: An Exploration of Video Understanding in Large Multimodal Models\nQuick Start\nCitation\nApollo: An Exploration of Video Understanding in Large Multimodal Models\nApollo is a family of Large Multimodal Models (LMMs) that push the state-of-the-art in video understanding. It supports tasks including:\nLong-form video comprehension\nTemporal reasoning\nComplex video question-answering\nMulti-turn conversations grounded in video content\nApollo models excel at handling hour-long videos, balancing speed and accuracy through strategic design decisions. Our models outperform most 7B competitors at just 3B parameters and even rival 30B-scale models.\nKey Highlights:\nScaling Consistency: Design decisions validated on smaller models and datasets effectively transfer to larger scales, reducing computation and experimentation costs.\nEfficient Video Sampling: fps sampling and advanced token resampling strategies (e.g., Perceiver) yield stronger temporal perception.\nEncoder Synergies: Combining SigLIP-SO400M (image) with InternVideo2 (video) delivers a robust representation, outperforming single encoders on temporal tasks.\nApolloBench: A streamlined evaluation benchmark (41x faster) that focuses on true video understanding capabilities.\nQuick Start\nInstallation:\npip install -e .\npip install flash-attn --no-build-isolation\nInference Example:\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom apollo.mm_utils import (\nKeywordsStoppingCriteria,\ntokenizer_mm_token,\nApolloMMLoader\n)\nfrom apollo.conversation import conv_templates, SeparatorStyle\nfrom huggingface_hub import snapshot_download\nmodel_url = \"Apollo-LMMs/Apollo-3B-t32\"\nmodel_path = snapshot_download(model_url, repo_type=\"model\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ntrust_remote_code=True,\nlow_cpu_mem_usage=True\n).to(device=device, dtype=torch.bfloat16)\ntokenizer = model.tokenizer\nvision_processors = model.vision_tower.vision_processor\nconfig = model.config\nnum_repeat_token = config.mm_connector_cfg['num_output_tokens']\nmm_processor = ApolloMMLoader(\nvision_processors,\nconfig.clip_duration,\nframes_per_clip=4,\nclip_sampling_ratio=0.65,\nmodel_max_length=config.model_max_length,\ndevice=device,\nnum_repeat_token=num_repeat_token\n)\nvideo_path = \"path/to/video.mp4\"\nquestion = \"Describe this video in detail\"\nmm_data, replace_string = mm_processor.load_video(video_path)\nconv = conv_templates[\"qwen_2\"].copy()\nconv.append_message(conv.roles[0], replace_string + \"\\n\\n\" + question)\nconv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\ninput_ids = tokenizer_mm_token(prompt, tokenizer, return_tensors=\"pt\").unsqueeze(0).to(device)\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\nstopping_criteria = KeywordsStoppingCriteria([stop_str], tokenizer, input_ids)\nwith torch.inference_mode():\noutput_ids = model.generate(\ninput_ids,\nvision_input=[mm_data],\ndata_types=['video'],\ndo_sample=True,\ntemperature=0.4,\nmax_new_tokens=256,\ntop_p=0.7,\nuse_cache=True,\nnum_beams=1,\nstopping_criteria=[stopping_criteria]\n)\npred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\nprint(pred)\nCitation\nIf you find this project useful, please consider citing:\n@article{zohar2024apollo,\ntitle={Apollo: An Exploration of Video Understanding in Large Multimodal Models},\nauthor={Zohar, Orr and Wang, Xiaohan and Dubois, Yann and Mehta, Nikhil and Xiao, Tong and Hansen-Estruch, Philippe and Yu, Licheng and Wang, Xiaofang and Juefei-Xu, Felix and Zhang, Ning and Yeung-Levy, Serena and Xia, Xide},\njournal={arXiv preprint arXiv:2412.10360},\nyear={2024}\n}\nFor more details, visit the project website or check out the paper.",
    "razesystems/CosyVoice2-0.5B-FP16-ONNX": "README\nUsage\nREADME\nThis repository contains the quantized models and the scripts for converting ONNX models to FP16 format. These files are intended to be used with the original CosyVoice2 inference code.\nUsage\nTo use the quantized models, you have two options:\nDownload and Rename the Files:\nDownload the FP16 models.\nRename the downloaded files to match the expected names in the original CosyVoice2 inference code.\nChange Mentions in the Original Inference Code:\nModify the original CosyVoice2 inference code to reference the new FP16 model filenames.",
    "tiiuae/Falcon3-1B-Instruct-1.58bit-GGUF": "Table of Contents\nTL;DR\nModel Details\nModel Description\nTraining details\nUsage\nBitNet\nEvaluation\nUseful links\nCitation\nTable of Contents\nTL;DR\nModel Details\nTraining Details\nUsage\nEvaluation\nCitation\nTL;DR\nModel Details\nModel Description\nDeveloped by: https://www.tii.ae\nModel type: Causal decoder-only - instruct / chat version\nArchitecture: Pure-transformer - 1.58bit version\nLanguage(s) (NLP): Mainly English\nLicense: TII Falcon License 2.0\nTraining details\nThe model has been trained following the training strategies from the recent 1-bit LLM HF blogpost and 1-bit LLM paper.\nFor more details about the training protocol of this model, please refer to the Falcon-3 technical report, section Compression.\nUsage\nCurrently to use this model you can rely on BitNet library. You can also play with the model using the falcon-1.58bit playground (only for the 7B instruct version).\nBitNet\ngit clone https://github.com/microsoft/BitNet && cd BitNet\npip install -r requirements.txt\nhuggingface-cli download tiiuae/Falcon3-1B-Instruct-1.58bit-GGUF ggml-model-i2_s.gguf --local-dir models/Falcon3-1B-1.58bit/\npython run_inference.py -m models/Falcon3-1B-1.58bit/ggml-model-i2_s.gguf -p \"You are a helpful assistant\" -cnv\nEvaluation\nWe report in the following table our internal pipeline benchmarks:\nNote evaluation results are normalized score from v2 leaderboard tasks - reported results of original models in the blogpost are raw scores\nBenchmark\nLlama3-8B-1.58-100B-tokens\nFalcon3-1B-Instruct-1.58bit\nIFEval\n17.91\n44.5\nMUSR\n4.87\n2.78\nGPQA\n1.83\n0\nBBH\n5.36\n2.24\nMMLU-PRO\n2.78\n1.93\nMATH\n0.26\n0.17\nAverage\n5.5\n8.6\nUseful links\nView our release blogpost.\nFeel free to join our discord server if you have any questions or to interact with our researchers and developers.\nCitation\nIf the Falcon3 family of models were helpful to your work, feel free to give us a cite.\n@misc{Falcon3,\ntitle = {The Falcon 3 Family of Open Models},\nauthor = {Falcon-LLM Team},\nmonth = {December},\nyear = {2024}\n}",
    "mradermacher/Neural-Logical-Abstract-7B-slerp-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Kukedlc/Neural-Logical-Abstract-7B-slerp\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n2.8\nGGUF\nQ3_K_S\n3.3\nGGUF\nQ3_K_M\n3.6\nlower quality\nGGUF\nQ3_K_L\n3.9\nGGUF\nIQ4_XS\n4.0\nGGUF\nQ4_K_S\n4.2\nfast, recommended\nGGUF\nQ4_K_M\n4.5\nfast, recommended\nGGUF\nQ5_K_S\n5.1\nGGUF\nQ5_K_M\n5.2\nGGUF\nQ6_K\n6.0\nvery good quality\nGGUF\nQ8_0\n7.8\nfast, best quality\nGGUF\nf16\n14.6\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "mradermacher/Python-Code-Completer-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/tmickleydoyle/Python-Code-Completer\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n0.8\nGGUF\nQ3_K_S\n0.9\nGGUF\nQ3_K_M\n0.9\nlower quality\nGGUF\nQ3_K_L\n1.0\nGGUF\nIQ4_XS\n1.0\nGGUF\nQ4_K_S\n1.0\nfast, recommended\nGGUF\nQ4_K_M\n1.1\nfast, recommended\nGGUF\nQ5_K_S\n1.2\nGGUF\nQ5_K_M\n1.2\nGGUF\nQ6_K\n1.4\nvery good quality\nGGUF\nQ8_0\n1.7\nfast, best quality\nGGUF\nf16\n3.2\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "RachelHGF/Mirage-in-the-Eyes": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nMirage-in-the-Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink\nOverview\nUsage\nAcknowledgement\nMirage-in-the-Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink\nNOTE: To prevent potetial harm, we release our source code only upon request for research purposes.\nOverview\nFusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications.\nYet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content.\nIn this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process.\nWe propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications.\nDistinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses.\nExtensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive defensive mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5.\nUsage\nWe provide the code files for hallucination attack, MLLM response generation, and GPT-4 assisted evaluation in this repository. The main implementation of Mirage-in-the-Eyes is in attack.py.\nThe recommended usage is as follows:\nPrepare the environment.\ncd Mirage-in-the-Eyes\nconda create -n mllm python==3.9.20\nconda activate mllm\npip install -r requirements.txt\npython -m pip install -e transformers-4.29.2\nPrepare the Hallubench dataset according to the official repository.\nSet up the model path configs in Mirage-in-the-Eyes/minigpt4/configs and Mirage-in-the-Eyes/minigpt4/models.\nRun our hallucination attack:\nCUDA_VISIBLE_DEVICES=GPU_ID python attack.py --model MODEL_NAME --gpu-id GPU_ID --data-path /path/to/hallubench --images-path /path/to/images --save-path /path/to/adv_images --generation-mode greedy --eps 2\nGenerate MLLM responses with adversarial visual inputs:\nCUDA_VISIBLE_DEVICES=GPU_ID python generate.py --model MODEL_NAME --gpu-id GPU_ID --data-path /path/to/hallubench --images-path /path/to/images --response-path /path/to/response.json\nEvaluate MLLM responses for hallucinations:\ncd eval\npython json_eval.py --json-file /path/to/response.json --bench-path /path/to/hallubench --log-path /path/to/log\nAcknowledgement\nThis repo is based on the MLLM codebase of OPERA. We sincerely thank the contributors for their valuable work.",
    "Anzhc/Anzhcs-VAEs": "Support\n(Nothing here for now)\nSupport\nIf you want to support me, feel free to donate on ko-fi:\nhttps://ko-fi.com/anzhc\nOr send me some BTC:\nbc1qpc5kmxrpqp6x8ykdu6976s4rvsz0utk22h80j9",
    "Aria-UI/Aria-UI-base": "Key Features of Aria-UI\nQuick Start\nInstallation\nInference with vllm (strongly recommended)\nInference with Transfomrers (not recommended)\nCitation\nüñºÔ∏è  Try Aria-UI! ¬∑ üìñ Project Page ¬∑ üìå Paper\n¬∑ ‚≠ê GitHub ¬∑ üìö Aria-UI Dataset\nKey Features of Aria-UI\n‚ú® Versatile Grounding Instruction Understanding:Aria-UI handles diverse grounding instructions, excelling in interpreting varied formats, ensuring robust adaptability across dynamic scenarios or when paired with diverse planning agents.\nüìù Context-aware Grounding:Aria-UI effectively leverages historical input, whether in pure text or text-image-interleaved formats, to improve grounding accuracy.\n‚ö° Lightweight and Fast:Aria-UI is a mixture-of-expert model with 3.9B activated parameters per token. It efficiently encodes GUI input of variable sizes and aspect ratios, with ultra-resolution support.\nüéâ Superior Performances:Aria-UI sets new state-of-the-art results on offline and online agent benchmarks.üèÜ 1st place on AndroidWorld with 44.8% task success rate andü•â 3rd place on OSWorld with 15.2% task success rate (Dec. 2024).\nQuick Start\nInstallation\npip install transformers==4.45.0 accelerate==0.34.1 sentencepiece==0.2.0 torchvision requests torch Pillow\npip install flash-attn --no-build-isolation\n# For better inference performance, you can install grouped-gemm, which may take 3-5 minutes to install\npip install grouped_gemm==0.1.6\nInference with vllm (strongly recommended)\nFirst, make sure you install the latest version of vLLM so that it supports Aria-UI\npip install https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\nHere is a code snippet for Aria-UI with vllm.\nfrom PIL import Image, ImageDraw\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nimport ast\nmodel_path = \"Aria-UI/Aria-UI-base\"\ndef main():\nllm = LLM(\nmodel=model_path,\ntokenizer_mode=\"slow\",\ndtype=\"bfloat16\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(\nmodel_path, trust_remote_code=True, use_fast=False\n)\ninstruction = \"Try Aria.\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\n\"type\": \"text\",\n\"text\": \"Given a GUI image, what are the relative (0-1000) pixel point coordinates for the element corresponding to the following instruction or description: \" + instruction,\n}\n],\n}\n]\nmessage = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\noutputs = llm.generate(\n{\n\"prompt_token_ids\": message,\n\"multi_modal_data\": {\n\"image\": [\nImage.open(\"examples/aria.png\"),\n],\n\"max_image_size\": 980,  # [Optional] The max image patch size, default `980`\n\"split_image\": True,  # [Optional] whether to split the images, default `True`\n},\n},\nsampling_params=SamplingParams(max_tokens=50, top_k=1, stop=[\"<|im_end|>\"]),\n)\nfor o in outputs:\ngenerated_tokens = o.outputs[0].token_ids\nresponse = tokenizer.decode(generated_tokens, skip_special_tokens=True)\nprint(response)\ncoords = ast.literal_eval(response.replace(\"<|im_end|>\", \"\").replace(\"\", \"\").replace(\" \", \"\").strip())\nCitation\nIf you find our work helpful, please consider citing.\n@article{ariaui,\ntitle={Aria-UI: Visual Grounding for GUI Instructions},\nauthor={Yuhao Yang and Yue Wang and Dongxu Li and Ziyang Luo and Bei Chen and Chao Huang and Junnan Li},\nyear={2024},\njournal={arXiv preprint arXiv:2412.16256},\n}",
    "Alibaba-NLP/gme-Qwen2-VL-2B-Instruct": "GME-Qwen2-VL-2B\nModel List\nUsage\nEvaluation\nCommunity support\nFine-tuning\nLimitations\nRedistribution and Use\nCloud API Services\nHiring\nCitation\nGME: General Multimodal Embedding\nGME-Qwen2-VL-2B\nWe are excited to present GME-Qwen2VL series of unified multimodal embedding models,\nwhich are based on the advanced Qwen2-VL multimodal large language models (MLLMs).\nThe GME models support three types of input: text, image, and image-text pair, all of which can produce universal vector representations and have powerful retrieval performance.\nKey Enhancements of GME Models:\nUnified Multimodal Representation: GME models can process both single-modal and combined-modal inputs, resulting in a unified vector representation. This enables versatile retrieval scenarios (Any2Any Search), supporting tasks such as text retrieval, image retrieval from text, and image-to-image searches.\nHigh Performance: Achieves state-of-the-art (SOTA) results in our universal multimodal retrieval benchmark (UMRB) and demonstrate strong evaluation scores in the Multimodal Textual Evaluation Benchmark (MTEB).\nDynamic Image Resolution: Benefiting from Qwen2-VL and our training data, GME models support dynamic resolution image input.\nStrong Visual Retrieval Performance: Enhanced by the Qwen2-VL model series, our models excel in visual document retrieval tasks that require a nuanced understanding of document screenshots.\nThis capability is particularly beneficial for complex document understanding scenarios,\nsuch as multimodal retrieval-augmented generation (RAG) applications focused on academic papers.\nDeveloped by: Tongyi Lab, Alibaba Group\nPaper: GME: Improving Universal Multimodal Retrieval by Multimodal LLMs\nModel List\nModels\nModel Size\nMax Seq. Length\nDimension\nMTEB-en\nMTEB-zh\nUMRB\ngme-Qwen2-VL-2B\n2.21B\n32768\n1536\n65.27\n66.92\n64.45\ngme-Qwen2-VL-7B\n8.29B\n32768\n3584\n67.48\n69.73\n67.44\nUsage\nTransformers\nThe remote code has some issues with transformers>=4.52.0, please downgrade or use sentence_transformers\nfrom transformers import AutoModel\nfrom transformers.utils.versions import require_version\nrequire_version(\n\"transformers<4.52.0\",\n\"The remote code has some issues with transformers>=4.52.0, please downgrade: pip install transformers==4.51.3\"\n)\nt2i_prompt = 'Find an image that matches the given text.'\ntexts = [\n\"The Tesla Cybertruck is a battery electric pickup truck built by Tesla, Inc. since 2023.\",\n\"Alibaba office.\",\n]\nimages = [\n'https://upload.wikimedia.org/wikipedia/commons/e/e9/Tesla_Cybertruck_damaged_window.jpg',\n'https://upload.wikimedia.org/wikipedia/commons/e/e0/TaobaoCity_Alibaba_Xixi_Park.jpg',\n]\ngme = AutoModel.from_pretrained(\n\"Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\",\ntorch_dtype=\"float16\", device_map='cuda', trust_remote_code=True\n)\n# Single-modal embedding\ne_text = gme.get_text_embeddings(texts=texts)\ne_image = gme.get_image_embeddings(images=images)\nprint('Single-modal', (e_text @ e_image.T).tolist())\n## Single-modal [[0.359619140625, 0.0655517578125], [0.04180908203125, 0.374755859375]]\n# How to set embedding instruction\ne_query = gme.get_text_embeddings(texts=texts, instruction=t2i_prompt)\n# If is_query=False, we always use the default instruction.\ne_corpus = gme.get_image_embeddings(images=images, is_query=False)\nprint('Single-modal with instruction', (e_query @ e_corpus.T).tolist())\n## Single-modal with instruction [[0.429931640625, 0.11505126953125], [0.049835205078125, 0.409423828125]]\n# Fused-modal embedding\ne_fused = gme.get_fused_embeddings(texts=texts, images=images)\nprint('Fused-modal', (e_fused @ e_fused.T).tolist())\n## Fused-modal [[1.0, 0.05511474609375], [0.05511474609375, 1.0]]\nsentence_transformers\nThe encode function accept str or dict with key(s) in {'text', 'image', 'prompt'}.\nDo not pass prompt as the argument to encode, pass as the input as a dict with a prompt key.\nfrom sentence_transformers import SentenceTransformer\nt2i_prompt = 'Find an image that matches the given text.'\ntexts = [\n\"The Tesla Cybertruck is a battery electric pickup truck built by Tesla, Inc. since 2023.\",\n\"Alibaba office.\",\n]\nimages = [\n'https://upload.wikimedia.org/wikipedia/commons/e/e9/Tesla_Cybertruck_damaged_window.jpg',\n'https://upload.wikimedia.org/wikipedia/commons/e/e0/TaobaoCity_Alibaba_Xixi_Park.jpg',\n]\ngme_st = SentenceTransformer(\"Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\")\n# Single-modal embedding\ne_text = gme_st.encode(texts, convert_to_tensor=True)\ne_image = gme_st.encode([dict(image=i) for i in images], convert_to_tensor=True)\nprint('Single-modal', (e_text @ e_image.T).tolist())\n## Single-modal [[0.356201171875, 0.06536865234375], [0.041717529296875, 0.37890625]]\n# How to set embedding instruction\ne_query = gme_st.encode([dict(text=t, prompt=t2i_prompt) for t in texts], convert_to_tensor=True)\n# If no prompt, we always use the default instruction.\ne_corpus = gme_st.encode([dict(image=i) for i in images], convert_to_tensor=True)\nprint('Single-modal with instruction', (e_query @ e_corpus.T).tolist())\n## Single-modal with instruction [[0.425537109375, 0.1158447265625], [0.049835205078125, 0.413818359375]]\n# Fused-modal embedding\ne_fused = gme_st.encode([dict(text=t, image=i) for t, i in zip(texts, images)], convert_to_tensor=True)\nprint('Fused-modal', (e_fused @ e_fused.T).tolist())\n## Fused-modal [[0.99951171875, 0.0556640625], [0.0556640625, 0.99951171875]]\nEvaluation\nWe validated the performance on our universal multimodal retrieval benchmark (UMRB, see Release UMRB) among others.\nSingle-modal\nCross-modal\nFused-modal\nAvg.\nT‚ÜíT (16)\nI‚ÜíI (1)\nT‚ÜíI (4)\nT‚ÜíVD (10)\nI‚ÜíT (4)\nT‚ÜíIT (2)\nIT‚ÜíT (5)\nIT‚ÜíI (2)\nIT‚ÜíIT (3)\n(47)\nVISTA\n0.2B\n55.15\n31.98\n32.88\n10.12\n31.23\n45.81\n53.32\n8.97\n26.26\n37.32\nCLIP-SF\n0.4B\n39.75\n31.42\n59.05\n24.09\n62.95\n66.41\n53.32\n34.9\n55.65\n43.66\nOne-Peace\n4B\n43.54\n31.27\n61.38\n42.9\n65.59\n42.72\n28.29\n6.73\n23.41\n42.01\nDSE\n4.2B\n48.94\n27.92\n40.75\n78.21\n52.54\n49.62\n35.44\n8.36\n40.18\n50.04\nE5-V\n8.4B\n52.41\n27.36\n46.56\n41.22\n47.95\n54.13\n32.9\n23.17\n7.23\n42.52\nGME-Qwen2-VL-2B\n2.2B\n55.93\n29.86\n57.36\n87.84\n61.93\n76.47\n64.58\n37.02\n66.47\n64.45\nGME-Qwen2-VL-7B\n8.3B\n58.19\n31.89\n61.35\n89.92\n65.83\n80.94\n66.18\n42.56\n73.62\n67.44\nThe MTEB Leaderboard English tab shows the text embeddings performence of our model.\nMore detailed experimental results can be found in the paper.\nCommunity support\nFine-tuning\nGME models can be fine-tuned by SWIFTÔºö\npip install ms-swift -U\n# MAX_PIXELS settings to reduce memory usage\n# check: https://swift.readthedocs.io/en/latest/BestPractices/Embedding.html\nnproc_per_node=8\nMAX_PIXELS=1003520 \\\nUSE_HF=1 \\\nNPROC_PER_NODE=$nproc_per_node \\\nswift sft \\\n--model Alibaba-NLP/gme-Qwen2-VL-2B-Instruct \\\n--train_type lora \\\n--dataset 'HuggingFaceM4/TextCaps:emb' \\\n--torch_dtype bfloat16 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 2 \\\n--per_device_eval_batch_size 2 \\\n--gradient_accumulation_steps $(expr 64 / $nproc_per_node) \\\n--eval_steps 100 \\\n--save_steps 100 \\\n--eval_strategy steps \\\n--save_total_limit 5 \\\n--logging_steps 5 \\\n--output_dir output \\\n--lazy_tokenize true \\\n--warmup_ratio 0.05 \\\n--learning_rate 5e-6 \\\n--deepspeed zero3 \\\n--dataloader_num_workers 4 \\\n--task_type embedding \\\n--loss_type infonce \\\n--dataloader_drop_last true\nLimitations\nSingle Image Input: In Qwen2-VL, an image could be converted into a very large number of visual tokens. We limit the number of visual tokens to 1024 to obtain a good training efficiency.\nDue to the lack of relevant data, our models and evaluations retain one single image.\nEnglish-only Training: Our models are trained on english data only. Although the Qwen2-VL models are multilingual, the multilingual-multimodal embedding performance are not guaranteed.\nWe will extend to multi-image input, image-text interleaved data as well as multilingual data in the future version.\nRedistribution and Use\nWe encourage and value diverse applications of GME models and continuous enhancements to the models themselves.\nIf you distribute or make GME models (or any derivative works) available, or if you create a product or service (including another AI model) that incorporates them, you must prominently display Built with GME on your website, user interface, blog post, About page, or product documentation.\nIf you utilize GME models or their outputs to develop, train, fine-tune, or improve an AI model that is distributed or made available, you must prefix the name of any such AI model with GME.\nCloud API Services\nIn addition to the open-source GME series models, GME series models are also available as commercial API services on Alibaba Cloud.\nMultiModal Embedding Models: The multimodal-embedding-v1 model service is available.\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\nHiring\nWe have open positions for Research Interns and Full-Time Researchers to join our team at Tongyi Lab.\nWe are seeking passionate individuals with expertise in representation learning, LLM-driven information retrieval, Retrieval-Augmented Generation (RAG), and agent-based systems.\nOur team is located in the vibrant cities of Beijing and Hangzhou, offering a collaborative and dynamic work environment where you can contribute to cutting-edge advancements in artificial intelligence and machine learning.\nIf you are driven by curiosity and eager to make a meaningful impact through your work, we would love to hear from you. Please submit your resume along with a brief introduction to dingkun.ldk@alibaba-inc.com.\nCitation\nIf you find our paper or models helpful, please consider cite:\n@misc{zhang2024gme,\ntitle={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs},\nauthor={Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min},\nyear={2024},\neprint={2412.16855},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={http://arxiv.org/abs/2412.16855},\n}",
    "r3gm/noob-ipa": "No model card",
    "AlirezaF138/PersianLLaMA-13B-Instruct-GGUF": "AlirezaF138/PersianLLaMA-13B-Instruct-Q4_K_M-GGUF\nUse with llama.cpp\nCLI:\nServer:\nAlirezaF138/PersianLLaMA-13B-Instruct-Q4_K_M-GGUF\nThis model was converted to GGUF format from ViraIntelligentDataMining/PersianLLaMA-13B-Instruct using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo AlirezaF138/PersianLLaMA-13B-Instruct-Q4_K_M-GGUF --hf-file persianllama-13b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo AlirezaF138/PersianLLaMA-13B-Instruct-Q4_K_M-GGUF --hf-file persianllama-13b-instruct-q4_k_m.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo AlirezaF138/PersianLLaMA-13B-Instruct-Q4_K_M-GGUF --hf-file persianllama-13b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo AlirezaF138/PersianLLaMA-13B-Instruct-Q4_K_M-GGUF --hf-file persianllama-13b-instruct-q4_k_m.gguf -c 2048",
    "naver/pisco-mistral": "Model Card for PISCO-mistral\nUsage\nModel features\nLicense\nCite\nAcknowledgements\nModel Card for PISCO-mistral\nPISCO is a context compression model to be used for efficient inference when doing Retrieval Augmented Generation (RAG), particularly optimized for question answering.\nPISCO contains two adapters around a backbone LLM:\nAn encoder adapter trained to perform compression of input contexts (the retrieved documents in RAG) into a set of 8 embedding vectors\nA decoder adapter, which can take as input sets of embeddings vectors from documents and a query and provide an answer\nWith a compressed collection of documents to retrieve from, inference becomes about x5 faster. PISCO models have very small loss in accuracy on a wide set of QA benchmarks (0-3%).\nDeveloped by: Naver Labs EuropeLicense: CC BY-NC 4.0.\nModel: Pisco-mistral\nBackbone model: mistralai/Mistral-7B-Instruct-v0.2\nModel size: 7.33 billion parameters\nCompression rate: x16: each document (of size up to 128 tokens) is converted into 8 embedding vectors.\nUsage\nfrom transformers import AutoModel\npisco = AutoModel.from_pretrained('naver/pisco-mistral').to('cuda')\n# Example documents and question:\ndocuments = [\n[\n\"Weldenia is a monotypic genus of flowering plant in the family Commelinaceae, first describ ed in 1829. It has one single species: Weldenia candida, which grows originally in Mexico and Guatemala.\",\n\"Hagsatera is a genus of flowering plants from the orchid family, Orchidaceae. There are two known species, native to Mexico and Guatemala\",\n\"Alsobia is a genus of flowering plants in the family Gesneriaceae, native to Mexico, Guatemala and Costa Rica. The two species are succulent, stoloniferous herbs and were previously included in the genus \\\"Episcia\\\". Recent molecular studies have supported the separation of \\\"Alsobia\\\" from \\\"Episcia\\\"\"\n]\n]\nquestions = [\"Which genus of plant grows originally in Mexico and Guatemala, Phylica or Weldenia?\"]\n# End-to-end usage\nout = pisco.generate_from_text(questions=questions, documents=documents, max_new_tokens=64)\nprint('Generated answer', out)\n# Document compression:\nembeddings = pisco.compress_documents(documents=documents[0])\n# Generation from compressed documents:\nout = pisco.generate_from_compressed_documents_and_questions(questions=questions, compressed_documents=embeddings)\nThe recommended usage is to provide documents cropped to about 128 tokens, which is common practice when doing RAG.\nModel features\nPISCO enables high accuracy responses from the compressed documents\nPISCO is robust to various domains We tested its compression/decoding abilities on various sets of data.\nPISCO enables x5 faster generation when the collection documents to retrieve from is pre-compressed.\nLicense\nThis work is licensed under CC BY-NC 4.0.\nCite\n@article{louis2025pisco,\ntitle={Pisco: Pretty simple compression for retrieval-augmented generation},\nauthor={Louis, Maxime and D{\\'e}jean, Herv{\\'e} and Clinchant, St{\\'e}phane},\njournal={arXiv preprint arXiv:2501.16075},\nyear={2025}\n}\nAcknowledgements\nModel trained at Naver Labs EuropeTeam:\nMaxime LOUIS\nHerv√© Dejean\nSt√©phane Clinchant",
    "mradermacher/Starling-LM-7B-beta-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Nexusflow/Starling-LM-7B-beta\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Starling-LM-7B-beta-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n2.8\nGGUF\nQ3_K_S\n3.3\nGGUF\nQ3_K_M\n3.6\nlower quality\nGGUF\nQ3_K_L\n3.9\nGGUF\nIQ4_XS\n4.0\nGGUF\nQ4_K_S\n4.2\nfast, recommended\nGGUF\nQ4_K_M\n4.5\nfast, recommended\nGGUF\nQ5_K_S\n5.1\nGGUF\nQ5_K_M\n5.2\nGGUF\nQ6_K\n6.0\nvery good quality\nGGUF\nQ8_0\n7.8\nfast, best quality\nGGUF\nf16\n14.6\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "mlx-community/deepseek-vl2-8bit": "mlx-community/deepseek-vl2-8bit\nUse with mlx\nmlx-community/deepseek-vl2-8bit\nThis model was converted to MLX format from prince-canuma/deepseek-vl2 using mlx-vlm version 0.1.9.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/deepseek-vl2-8bit --max-tokens 100 --temp 0.0",
    "mradermacher/Hermes3-BlackSheep-3B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Triangle104/Hermes3-BlackSheep-3B\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Hermes3-BlackSheep-3B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.5\nGGUF\nQ3_K_S\n1.6\nGGUF\nQ3_K_M\n1.8\nlower quality\nGGUF\nQ3_K_L\n1.9\nGGUF\nIQ4_XS\n1.9\nGGUF\nQ4_K_S\n2.0\nfast, recommended\nGGUF\nQ4_K_M\n2.1\nfast, recommended\nGGUF\nQ5_K_S\n2.4\nGGUF\nQ5_K_M\n2.4\nGGUF\nQ6_K\n2.7\nvery good quality\nGGUF\nQ8_0\n3.5\nfast, best quality\nGGUF\nf16\n6.5\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "bytedance-research/Valley-Eagle-7B": "Valley2\nIntroduction\nRelease\nArchitecture\nEnvironment Setup\nLicense Agreement\nRelated Project\nLicense Agreement\nWe are Hiring\nCitation\nValley2\nüéÆÔ∏è Github¬†¬† | ¬†¬† ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope ¬†¬† | ¬†¬† üìë Home Page ¬†¬† | ¬†¬† üìô Paper\nIntroduction\nValley is a cutting-edge multimodal large model designed to handle a variety of tasks involving text, images, and video data, which is developed by ByteDance. Our model\nAchieved the best results in the inhouse e-commerce and short-video benchmarks, much better then other SOTA opensource models.\nDemonstrated comparatively outstanding performance in the OpenCompass Benchmark.\nRelease\n[2025/10/26] üî•üî•üî• Update Valley3, significantly enhance multimodal understanding and reasoning capabilities, achieving 74.4 on OpenCompass Multi-modal Academic Leaderboard!\n[2025/02/15] üî• Update Valley2-DPO, achieve 69.6 on OpenCompass Multi-modal Academic Leaderboard and update AutoModel usage for checkpoints.\n[2025/01/13] üî• Release TechReport. Valley2: Exploring Multimodal Models with Scalable Vision-Language Design\n[2024/12/23] üî• Announcing Valley2 (Valley-Eagle-7B)!\nArchitecture\nThe foundational version of Valley2 is a multimodal large model aligned with Siglip and Qwen2.5, incorporating LargeMLP and ConvAdapter to construct the projector.\nIn the final version, we also referenced Eagle, introducing an additional VisionEncoder that can flexibly adjust the number of tokens and is parallelized with the original visual tokens.\nThis enhancement supplements the model‚Äôs performance in extreme scenarios, and we chose the Qwen2vl VisionEncoder for this purpose.\nThe model structure is shown as follows:\nEnvironment Setup\npip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\npip install -r requirements.txt\nLicense Agreement\nAll of our open-source models are licensed under the Apache-2.0 license.\nRelated Project\nWe list related Project\nValley: Video Assistant with Large Language model Enhanced abilitY\nLLaVA: Large Language and Vision Assistant\nEagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders\nLLaVA-CoT: Let Vision Language Models Reason Step-by-Step\nQwen2.5\nLicense Agreement\nAll of our open-source models are licensed under the Apache-2.0 license.\nWe are Hiring\nThe Data-Ecommerce-Platform Governance-Basic Algorithms Team focuses on the research and development of multi-modal large model algorithms and foundational algorithms, continuously delving deeply into this field. Our mission is to optimize algorithms and collaborate with business teams to comprehensively govern the quality and ecosystem of ByteDance's e-commerce products. Currently, the team has a strong demand for foundational algorithm expertise in NLP, CV, and multimodal technologies. We welcome inquiries and look forward to working on challenging projects with talented individuals like you!\nLocation: Beijing / Shanghai / Singapore\nContact & Resume Submission: wuheng.2024@bytedance.com\nTiktok-ÁîµÂïÜÔºåÂü∫Á°ÄÁÆóÊ≥ïÂõ¢Èòü‰∏ìÊ≥®‰∫éÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁÆóÊ≥ïÂíåÂü∫Á°ÄÁÆóÊ≥ïÁöÑÁ†îÂèëÔºåÂπ∂Âú®Ê≠§ÊñπÂêë‰∏äÊåÅÁª≠Ê∑±ËÄïÔºåÊúüÂæÖÂíå‰ºòÁßÄÁöÑ‰Ω†ÔºàÂÆû‰π†/ÂÖ®ËÅåÔºâÔºå‰∏ÄËµ∑ÂÅöÊúâÊåëÊàòÁöÑ‰∫ãÊÉÖÔºÅ\nÂ≤ó‰ΩçÂüéÂ∏ÇÔºöÂåó‰∫¨/‰∏äÊµ∑/Êñ∞Âä†Âù°\nÂí®ËØ¢&ÁÆÄÂéÜÊäïÈÄíÔºöwuheng.2024@bytedance.com\nCitation\n@article{wu2025valley2,\ntitle={Valley2: Exploring Multimodal Models with Scalable Vision-Language Design},\nauthor={Wu, Ziheng and Chen, Zhenghao and Luo, Ruipu and Zhang, Can and Gao, Yuan and He, Zhentao and Wang, Xian and Lin, Haoran and Qiu, Minghui},\njournal={arXiv preprint arXiv:2501.05901},\nyear={2025}\n}",
    "depth-anything/prompt-depth-anything-vitl-hf": "Prompt-Depth-Anything-Vitl\nIntroduction\nUsage\nCitation\nPrompt-Depth-Anything-Vitl\nIntroduction\nPrompt Depth Anything is a high-resolution and accurate metric depth estimation method, with the following highlights:\nusing prompting to unleash the power of depth foundation models, inspired by success of prompting in VLM and LLM foundation models.\nThe widely available iPhone LiDAR is taken as the prompt, guiding the model to produce up to 4K resolution accurate metric depth.\nA scalable data pipeline is introduced to train the method.\nPrompt Depth Anything benefits downstream applications, including 3D reconstruction and generalized robotic grasping.\nUsage\nThis model is compatible with Hugging Face Transformers (docs).\nimport requests\nfrom PIL import Image\nfrom transformers import PromptDepthAnythingForDepthEstimation, PromptDepthAnythingImageProcessor\nurl = \"https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/image.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_processor = PromptDepthAnythingImageProcessor.from_pretrained(\"depth-anything/prompt-depth-anything-vitl-hf\")\nmodel = PromptDepthAnythingForDepthEstimation.from_pretrained(\"depth-anything/prompt-depth-anything-vitl-hf\")\nprompt_depth_url = \"https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/arkit_depth.png?raw=true\"\nprompt_depth = Image.open(requests.get(prompt_depth_url, stream=True).raw)\ninputs = image_processor(images=image, return_tensors=\"pt\", prompt_depth=prompt_depth)\nwith torch.no_grad():\noutputs = model(**inputs)\npost_processed_output = image_processor.post_process_depth_estimation(\noutputs,\ntarget_sizes=[(image.height, image.width)],\n)\npredicted_depth = post_processed_output[0][\"predicted_depth\"]\nCitation\nIf you find this project useful, please consider citing:\n@inproceedings{lin2024promptda,\ntitle={Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation},\nauthor={Lin, Haotong and Peng, Sida and Chen, Jingxiao and Peng, Songyou and Sun, Jiaming and Liu, Minghuan and Bao, Hujun and Feng, Jiashi and Zhou, Xiaowei and Kang, Bingyi},\njournal={arXiv},\nyear={2024}\n}",
    "mradermacher/Ministral-3b-instruct-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/ministral/Ministral-3b-instruct\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Ministral-3b-instruct-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.4\nGGUF\nQ3_K_S\n1.6\nGGUF\nQ3_K_M\n1.7\nlower quality\nGGUF\nQ3_K_L\n1.9\nGGUF\nIQ4_XS\n1.9\nGGUF\nQ4_K_S\n2.0\nfast, recommended\nGGUF\nQ4_K_M\n2.1\nfast, recommended\nGGUF\nQ5_K_S\n2.4\nGGUF\nQ5_K_M\n2.4\nGGUF\nQ6_K\n2.8\nvery good quality\nGGUF\nQ8_0\n3.6\nfast, best quality\nGGUF\nf16\n6.7\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "Saxo/Linkbricks-Horizon-AI-Japanese-Superb-V4-70B": "Model Card for Model ID\nModel Card for Model ID\nOpen Japanese LLM Leaderboard üèÜ Rank-1 2025/01/12~\nAIÂ∞ÇÈñÄ‰ºÅÊ•≠„Åß„ÅÇ„ÇãLinkbricks Horizon-AI „ÅÆ„Éá„Éº„Çø„Çµ„Ç§„Ç®„É≥„ÉÜ„Ç£„Çπ„Éà„Åß„ÅÇ„Çã„Ç∏„Éª„É¶„É≥„ÇΩ„É≥(Saxo)‰ª£Ë°®„Åå\nSaxo/Linkbricks-Horizon-AI-Japanese-Superb-V3-70B„Éô„Éº„Çπ„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„ÄÅH100-80G 8ÂÄã„ÇíÈÄö„Åò„Å¶Á¥Ñ Êó•Êú¨Ë™û SFT->DPO „Åó„ÅüÊó•Êú¨Ë™ûÂº∑ÂåñË®ÄË™û„É¢„Éá„É´„ÄÇ\n3ÂçÉ‰∏á‰ª∂„ÅÆÊó•Êú¨„Éã„É•„Éº„ÇπÂèä„Å≥„Ç¶„Ç£„Ç≠„Ç≥„Éº„Éë„Çπ„ÇíÂü∫Ê∫ñ„Å´„ÄÅÊßò„ÄÖ„Å™„Çø„Çπ„ÇØÂà•„ÅÆÊó•Êú¨Ë™û„ÉªÈüìÂõΩË™û„Éª‰∏≠ÂõΩË™û„ÉªËã±Ë™û„ÇØ„É≠„ÇπÂ≠¶Áøí„Éá„Éº„Çø„Å®Êï∞Â≠¶Âèä„Å≥Ë´ñÁêÜÂà§Êñ≠„Éá„Éº„Çø„ÇíÈÄö„Åò„Å¶„ÄÅÊó•‰∏≠ÈüìËã±Ë®ÄË™û„ÇØ„É≠„ÇπË£úÂº∑Âá¶ÁêÜ„Å®Ë§áÈõë„Å™Ë´ñÁêÜÂïèÈ°å„Å´„ÇÇÂØæÂøú„Åß„Åç„Çã„Çà„ÅÜ„Å´Ë®ìÁ∑¥„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ\n-„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅØ„ÄÅÂçòË™ûÊã°Âºµ„Å™„Åó„Åß„Éô„Éº„Çπ„É¢„Éá„É´„ÅÆ„Åæ„Åæ‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ\n-„Ç´„Çπ„Çø„Éû„Éº„É¨„Éì„É•„Éº„ÇÑ„ÇΩ„Éº„Ç∑„É£„É´ÊäïÁ®ø„ÅÆÈ´òÊ¨°ÂÖÉÂàÜÊûêÂèä„Å≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Å®„É©„Ç§„ÉÜ„Ç£„É≥„Ç∞„ÄÅÊï∞Â≠¶„ÄÅË´ñÁêÜÂà§Êñ≠„Å™„Å©„ÅåÂº∑Âåñ„Åï„Çå„Åü„É¢„Éá„É´„ÄÇ\n-128k-Context Window\n-Function Call\n-128k-Context Window\n-Deepspeed Stage=3„ÄÅrsloraÂèä„Å≥BAdam Layer Mode„Çí‰ΩøÁî®\n-„Äåtransformers_version„Äç: „Äå4.46.3„Äç\nAI Ï†ÑÎ¨∏ Í∏∞ÏóÖÏù∏ Linkbricks Horizon-AI Ïùò Îç∞Ïù¥ÌÑ∞ÏÇ¨Ïù¥Ïñ∏Ìã∞Ïä§Ìä∏Ïù∏ ÏßÄÏú§ÏÑ±(Saxo) ÎåÄÌëúÍ∞Ä\nSaxo/Linkbricks-Horizon-AI-Japanese-Superb-V3-70B Î≤†Ïù¥Ïä§Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï¥ÏÑú H100-80G 8Í∞úÎ•º ÌÜµÌï¥ ÏùºÎ≥∏Ïñ¥ SFT->DPO  Ìïú ÏùºÎ≥∏Ïñ¥ Í∞ïÌôî Ïñ∏Ïñ¥ Î™®Îç∏\n3Ï≤úÎßåÍ±¥Ïùò ÏùºÎ≥∏ Îâ¥Ïä§ Î∞è ÏúÑÌÇ§ ÏΩîÌçºÏä§Î•º Í∏∞Ï§ÄÏúºÎ°ú Îã§ÏñëÌïú ÌÖåÏä§ÌÅ¨Î≥Ñ ÏùºÎ≥∏Ïñ¥-ÌïúÍµ≠Ïñ¥-Ï§ëÍµ≠Ïñ¥-ÏòÅÏñ¥ ÍµêÏ∞® ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏôÄ ÏàòÌïô Î∞è ÎÖºÎ¶¨ÌåêÎã® Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌïòÏó¨ ÌïúÏ§ëÏùºÏòÅ Ïñ∏Ïñ¥ ÍµêÏ∞® Ï¶ùÍ∞ï Ï≤òÎ¶¨ÏôÄ Î≥µÏû°Ìïú ÎÖºÎ¶¨ Î¨∏Ï†ú Ïó≠Ïãú ÎåÄÏùë Í∞ÄÎä•ÌïòÎèÑÎ°ù ÌõàÎ†®Ìïú Î™®Îç∏Ïù¥Îã§.\n-ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎäî Îã®Ïñ¥ ÌôïÏû• ÏóÜÏù¥ Î≤†Ïù¥Ïä§ Î™®Îç∏ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n-Í≥†Í∞ù Î¶¨Î∑∞ÎÇò ÏÜåÏÖú Ìè¨Ïä§ÌåÖ Í≥†Ï∞®Ïõê Î∂ÑÏÑù Î∞è ÏΩîÎî©Í≥º ÏûëÎ¨∏, ÏàòÌïô, ÎÖºÎ¶¨ÌåêÎã® Îì±Ïù¥ Í∞ïÌôîÎêú Î™®Îç∏\n-128k-Context Window\n-Function Call Î∞è Tool Calling ÏßÄÏõê\n-128k-Context Window\n-Deepspeed Stage=3, rslora Î∞è BAdam Layer Mode ÏÇ¨Ïö©\n-\"transformers_version\": \"4.46.3\"\nFinetuned by Mr. Yunsung Ji (Saxo), a data scientist and CEO at Linkbricks Horiozn-AI, a company specializing in AI and big data analytics\nJapanese SFT->DPO training model based on Saxo/Linkbricks-Horizon-AI-Japanese-Superb-V3-70B through 8 H100-80Gs as a Japanese  boosting language model\nIt is a model that has been trained to handle Japanese-Korean-Chinese-English cross-training data and 30M Japanese news corpus and logic judgment data for various tasks to enable cross-fertilization processing and complex Korean logic & math problems.\n-Tokenizer uses the base model without word expansion\n-Models enhanced with high-dimensional analysis of customer reviews and social posts, as well as coding, writing, math and decision making\n-Function Calling\n-128k-Context Window\n-Deepspeed Stage=3, use rslora and BAdam Layer Mode\nwww.horizonai.ai, www.linkbricks.com, www.linkbricks.vc",
    "keras/phi3_mini_4k_instruct_en": "Links\nInstallation\nPresets\nPrompts\nExample Usage\nExample Usage with Hugging Face URI\nModel Overview\nPhi-3 is a set of large language models published by Microsoft. Models are instruction tuned, and range in size from 3 billion to 14 billion parameters. See the model card below for benchmarks, data sources, and intended use cases.\nWeights are released under the MIT License. Keras model code is released under the Apache 2 License.\nLinks\nPhi-3 Quickstart Notebook\nPhi-3 API Documentation\nPhi-3 Model Card\nKerasHub Beginner Guide\nKerasHub Model Publishing Guide\nInstallation\nKeras and KerasHub can be installed with:\npip install -U -q keras-hub\npip install -U -q keras\nJax, TensorFlow, and Torch come preinstalled in Kaggle Notebooks. For instruction on installing them in another environment see the Keras Getting Started page.\nPresets\nThe following model checkpoints are provided by the Keras team. Full code examples for each are available below.\nPreset name\nParameters\nDescription\nphi3_mini_4k_instruct_en\n3.82B\n3B model with 4K max context\nphi3_mini_128k_instruct_en\n3.82B\n3B model with 128K max context\nPrompts\nPhi-3 models are instruction tuned on turn by turn conversations and should be prompted with examples that precisely match the training data. Specifically, you must alternate user and assistant turns that begin and end with special tokens. New lines do matter. See the following for an example:\nprompt = \"\"\"&lt;|user|&gt;\nHello!&lt;|end|&gt;\n&lt;|assistant|&gt;\nHello! How are you?&lt;|end|&gt;\n&lt;|user|&gt;\nI'm great. Could you help me with a task?&lt;|end|&gt;\n\"\"\"\nExample Usage\npip install -U -q keras-hub\nimport keras\nimport keras_hub\nimport numpy as np\nUse generate() to do text generation.\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\"phi3_mini_4k_instruct_en\")\nphi3_lm.generate(\"&lt;|user|&gt;\\nHow to explain Internet for a medieval knight?&lt;|end|&gt;\\n&lt;|assistant|&gt;\", max_length=500)\n# Generate with batched prompts.\nphi3_lm.generate([\n\"&lt;|user|&gt;\\nWhat is Keras?&lt;|end|&gt;\\n&lt;|assistant|&gt;\",\n\"&lt;|user|&gt;\\nGive me your best brownie recipe.&lt;|end|&gt;\\n&lt;|assistant|&gt;\",\n], max_length=500)\nCompile the generate() function with a custom sampler.\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\"phi3_mini_4k_instruct_en\")\nphi3_lm.compile(sampler=\"greedy\")\nphi3_lm.generate(\"&lt;|user|&gt;\\nWhat is Keras?&lt;|end|&gt;\\n&lt;|assistant|&gt;\", max_length=30)\nphi3_lm.compile(sampler=keras_hub.samplers.BeamSampler(num_beams=2))\nphi3_lm.generate(\"&lt;|user|&gt;\\nWhat is Keras?&lt;|end|&gt;\\n&lt;|assistant|&gt;\", max_length=30)\nUse generate() without preprocessing.\nprompt = {\n\"token_ids\": np.array([[306, 864, 304, 1827, 0, 0, 0, 0, 0, 0]] * 2),\n# Use `\"padding_mask\"` to indicate values that should not be overridden.\n\"padding_mask\": np.array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]] * 2),\n}\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\n\"phi3_mini_4k_instruct_en\",\npreprocessor=None,\ndtype=\"bfloat16\"\n)\nphi3_lm.generate(prompt)\nCall fit() on a single batch.\nfeatures = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\"phi3_mini_4k_instruct_en\")\nphi3_lm.fit(x=features, batch_size=2)\nExample Usage with Hugging Face URI\npip install -U -q keras-hub\nimport keras\nimport keras_hub\nimport numpy as np\nUse generate() to do text generation.\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\"hf://keras/phi3_mini_4k_instruct_en\")\nphi3_lm.generate(\"&lt;|user|&gt;\\nHow to explain Internet for a medieval knight?&lt;|end|&gt;\\n&lt;|assistant|&gt;\", max_length=500)\n# Generate with batched prompts.\nphi3_lm.generate([\n\"&lt;|user|&gt;\\nWhat is Keras?&lt;|end|&gt;\\n&lt;|assistant|&gt;\",\n\"&lt;|user|&gt;\\nGive me your best brownie recipe.&lt;|end|&gt;\\n&lt;|assistant|&gt;\",\n], max_length=500)\nCompile the generate() function with a custom sampler.\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\"hf://keras/phi3_mini_4k_instruct_en\")\nphi3_lm.compile(sampler=\"greedy\")\nphi3_lm.generate(\"&lt;|user|&gt;\\nWhat is Keras?&lt;|end|&gt;\\n&lt;|assistant|&gt;\", max_length=30)\nphi3_lm.compile(sampler=keras_hub.samplers.BeamSampler(num_beams=2))\nphi3_lm.generate(\"&lt;|user|&gt;\\nWhat is Keras?&lt;|end|&gt;\\n&lt;|assistant|&gt;\", max_length=30)\nUse generate() without preprocessing.\nprompt = {\n\"token_ids\": np.array([[306, 864, 304, 1827, 0, 0, 0, 0, 0, 0]] * 2),\n# Use `\"padding_mask\"` to indicate values that should not be overridden.\n\"padding_mask\": np.array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]] * 2),\n}\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\n\"hf://keras/phi3_mini_4k_instruct_en\",\npreprocessor=None,\ndtype=\"bfloat16\"\n)\nphi3_lm.generate(prompt)\nCall fit() on a single batch.\nfeatures = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\nphi3_lm = keras_hub.models.Phi3CausalLM.from_preset(\"hf://keras/phi3_mini_4k_instruct_en\")\nphi3_lm.fit(x=features, batch_size=2)",
    "Graimond/ECBERT-base-mlm": "ECBERT-base-mlm\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nECBERT-base-mlm\nThis model is a pretrained version of answerdotai/ModernBERT-base on 25,581 texts (available here) using MLM but not yet fine-tuned on the monetary policy sentiment analysis task.\nThe best model achieves the following results on an out-of-sample test set (Graimond/ECBERT-idioms-dataset):\nAccuracy: 40.00%\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nTraining data: Graimond/ECBERT-mlm-dataset\nEvaluation data: Graimond/ECBERT-idioms-dataset\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-5\nweight_decay=0.01\nper_device_train_batch_size=16\nseed: 42\nepochs: 20\nTraining results\nEpoch\nTraining Loss\nValidation Loss\n1\n1.905000\n1.903329\n2\n1.689700\n1.764568\n3\n1.600900\nnan\n4\n1.476500\n1.683352\n5\n1.381200\n1.629597\n6\n1.367300\nnan\n7\n1.230300\n1.628195\n8\n1.142700\n1.567721\n9\n1.131800\n1.618517\n10\n1.139700\nnan\n11\n1.086200\nnan\n12\n1.072500\n1.560426\n13\n0.984800\n1.556072\n14\n0.958500\n1.606674\n15\n0.955600\n1.619744\n16\n0.920500\n1.581421\n17\n0.882300\n1.535872\n18\n0.877900\n1.565936\n19\n0.803100\nnan\n20\n0.815700\n1.604986\nFramework versions\nTransformers 4.48.0.dev0\nPytorch 2.5.1+cu121\nDatasets 3.2.0\nTokenizers 0.21.0",
    "Graimond/ECBERT-base-pretrained-finetuned": "ECBERT-base-mlm\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nECBERT-base-mlm\nThis model is a fine-tuned version of Graimond/ECBERT-base-mlm on Gorodnichenko, Y., Pham, T., & Talavera, O. (2023). Data and Code for: The Voice of Monetary Policy (Version v1) [Dataset]. ICPSR - Interuniversity Consortium for Political and Social Research. https://doi.org/10.3886/E178302V1.\nThe best model achieves the following results on the evaluation set:\nLoss: 0.4129\nAccuracy: 85.94%\nThe label_map is as follows: {\"hawkish\": 0, \"neutral\": 1, \"dovish\": 2}\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nGorodnichenko, Y., Pham, T., & Talavera, O. (2023). Data and Code for: The Voice of Monetary Policy (Version v1) [Dataset]. ICPSR - Interuniversity Consortium for Political and Social Research. https://doi.org/10.3886/E178302V1.\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-5\nweight_decay=0.01\nper_device_train_batch_size=16\nseed: 42\nepochs: 20\nTraining results\nEpoch\nTraining Loss\nValidation Loss\n1\nNo log\n0.886533\n2\nNo log\n0.514593\n3\nNo log\n0.437099\n4\n0.683200\n0.420006\n5\n0.683200\n0.453126\n6\n0.683200\n0.412876\n7\n0.262900\n0.621511\n8\n0.262900\n0.527209\n9\n0.262900\n0.673689\n10\n0.191300\n0.711371\n11\n0.191300\n0.578193\n12\n0.191300\n0.854842\n13\n0.141100\n0.809792\n14\n0.141100\n0.847027\n15\n0.141100\n0.847365\n16\n0.085900\n0.846864\n17\n0.085900\n0.880487\n18\n0.085900\n0.870781\n19\n0.085900\n0.868764\n20\n0.076000\n0.871563\nFramework versions\nTransformers 4.48.0.dev0\nPytorch 2.5.1+cu121\nDatasets 3.2.0\nTokenizers 0.21.0",
    "FreedomIntelligence/HuatuoGPT-o1-8B": "Introduction\nModel Info\nUsage\nüìñ Citation\nHuatuoGPT-o1-8B\nGitHub | Paper\nIntroduction\nHuatuoGPT-o1 is a medical LLM designed for advanced medical reasoning.  It generates a complex thought process, reflecting and refining its reasoning, before providing a final response.\nFor more information, visit our GitHub repository:\nhttps://github.com/FreedomIntelligence/HuatuoGPT-o1.\nModel Info\nBackbone\nSupported Languages\nLink\nHuatuoGPT-o1-8B\nLLaMA-3.1-8B\nEnglish\nHF Link\nHuatuoGPT-o1-70B\nLLaMA-3.1-70B\nEnglish\nHF Link\nHuatuoGPT-o1-7B\nQwen2.5-7B\nEnglish & Chinese\nHF Link\nHuatuoGPT-o1-72B\nQwen2.5-72B\nEnglish & Chinese\nHF Link\nUsage\nYou can use HuatuoGPT-o1 in the same way as Llama-3.1-8B-Instruct. You can deploy it with tools like vllm or Sglang,  or perform direct inference:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"FreedomIntelligence/HuatuoGPT-o1-8B\",torch_dtype=\"auto\",device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"FreedomIntelligence/HuatuoGPT-o1-8B\")\ninput_text = \"How to stop a cough?\"\nmessages = [{\"role\": \"user\", \"content\": input_text}]\ninputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True\n), return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=2048)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nHuatuoGPT-o1 adopts a thinks-before-it-answers approach, with outputs formatted as:\n## Thinking\n[Reasoning process]\n## Final Response\n[Output]\nüìñ Citation\n@misc{chen2024huatuogpto1medicalcomplexreasoning,\ntitle={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs},\nauthor={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},\nyear={2024},\neprint={2412.18925},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.18925},\n}",
    "latent-space-dreams/LS_Tiro_XL": "Older versions\nAn anime model based on NoobAI XL and its derivatives.\nvpred v3.0\nRecommended settings:\nCFG 3.0 - 5.0\nDPM++ 2M SDE\nSGM Uniform\n30 steps\nUpdates in vpred v3.0:\nFinetuned from base Noob vpred, with no other models merged into the UNet\nMuch better at flat and sketchy styles\nLess overexposure issues\nBetter at dark compositions - use tags like dark, night\nAlternate aesthetic tag: very aesthetic\nImportant:\nIf the default style is not to your taste, use artist tags\nThe aim of this finetune was to specifically try mitigate the issues present in base Noob vpred; hence the model's knowledge remains similar\nIf you want more compatibility / less bias, use the base variant\nOlder versions\nvpred v2.0\nRecommended settings:\nCFG 3.0\nDPM++ 2M SDE\nSGM Uniform\n30 steps\nUpdates in vpred v2.0:\nBetter white balance\nLess oversaturation issues\nReduced style bias\nNote: If you want even less style bias, grab the base variant\nvpred v1.0\nImportant: vpred version requires much lower CFG than eps models.\nCFG 1.5 is a good starting point for vpred version. CFG 4.0 is a good starting point for eps versions.\nReduce CFG if you find your gens oversaturated.\nDMD2 version integrates DMD2 and needs specific configuration. CFG++ samplers and 16 steps is a good starting point.\nYour choice of sampler is important for the vpred version. Use ddpm for balanced colours, use stronger samplers like the SDE ones for more saturation.\nFollow the same prompting and configuration guidelines as NoobAI XL, especially for vpred version.\nUse artist tags if the default style of the model is not to your taste.\nInherits the same license as NoobAI XL.",
    "MoritzLaurer/ModernBERT-large-zeroshot-v2.0": "ModernBERT-base-zeroshot-v2.0\nModel description\nGeneral takeaways:\nTraining results\nTraining hyperparameters\nFramework versions\nModernBERT-base-zeroshot-v2.0\nModel description\nThis model is answerdotai/ModernBERT-large\nfine-tuned on the same dataset mix as the zeroshot-v2.0 models in the Zeroshot Classifiers Collection.\nGeneral takeaways:\nThe model is very fast and memory efficient. It's multiple times faster and consumes multiple times less memory than DeBERTav3.\nThe memory efficiency enables larger batch sizes. I got a ~2x speed increase by enabling bf16 (instead of fp16).\nIt performs slightly worse then DeBERTav3 on average on the tasks tested below.\nI'm in the process of preparing a newer version trained on better synthetic data to make full use of the 8k context window\nand to update the training mix of the older zeroshot-v2.0 models.\nTraining results\nDatasets\nMean\nMean w/o NLI\nmnli_m\nmnli_mm\nfevernli\nanli_r1\nanli_r2\nanli_r3\nwanli\nlingnli\nwellformedquery\nrottentomatoes\namazonpolarity\nimdb\nyelpreviews\nhatexplain\nmassive\nbanking77\nemotiondair\nemocontext\nempathetic\nagnews\nyahootopics\nbiasframes_sex\nbiasframes_offensive\nbiasframes_intent\nfinancialphrasebank\nappreviews\nhateoffensive\ntrueteacher\nspam\nwikitoxic_toxicaggregated\nwikitoxic_obscene\nwikitoxic_identityhate\nwikitoxic_threat\nwikitoxic_insult\nmanifesto\ncapsotu\nAccuracy\n0.85\n0.851\n0.942\n0.944\n0.894\n0.812\n0.717\n0.716\n0.836\n0.909\n0.815\n0.899\n0.964\n0.951\n0.984\n0.814\n0.8\n0.744\n0.752\n0.802\n0.544\n0.899\n0.735\n0.934\n0.864\n0.877\n0.913\n0.953\n0.921\n0.821\n0.989\n0.901\n0.927\n0.931\n0.959\n0.911\n0.497\n0.73\nF1 macro\n0.834\n0.835\n0.935\n0.938\n0.882\n0.795\n0.688\n0.676\n0.823\n0.898\n0.814\n0.899\n0.964\n0.951\n0.984\n0.77\n0.753\n0.763\n0.69\n0.805\n0.533\n0.899\n0.729\n0.925\n0.864\n0.877\n0.901\n0.953\n0.855\n0.821\n0.983\n0.901\n0.927\n0.931\n0.952\n0.911\n0.362\n0.662\nInference text/sec (A100 40GB GPU, batch=32)\n1116.0\n1104.0\n1039.0\n1241.0\n1138.0\n1102.0\n1124.0\n1133.0\n1251.0\n1240.0\n1263.0\n1231.0\n1054.0\n559.0\n795.0\n1238.0\n1312.0\n1285.0\n1273.0\n1268.0\n992.0\n1222.0\n894.0\n1176.0\n1194.0\n1197.0\n1206.0\n1166.0\n1227.0\n541.0\n1199.0\n1045.0\n1054.0\n1020.0\n1005.0\n1063.0\n1214.0\n1220.0\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 9e-06\ntrain_batch_size: 16\neval_batch_size: 32\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 32\noptimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\nlr_scheduler_type: linear\nlr_scheduler_warmup_ratio: 0.06\nnum_epochs: 2\nFramework versions\nTransformers 4.48.0.dev0\nPytorch 2.5.1+cu124\nDatasets 3.2.0\nTokenizers 0.21.0",
    "mradermacher/sabia-7b-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/maritaca-ai/sabia-7b\nstatic quants are available at https://huggingface.co/mradermacher/sabia-7b-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n1.6\nfor the desperate\nGGUF\ni1-IQ1_M\n1.8\nmostly desperate\nGGUF\ni1-IQ2_XXS\n2.0\nGGUF\ni1-IQ2_XS\n2.1\nGGUF\ni1-IQ2_S\n2.3\nGGUF\ni1-Q2_K_S\n2.4\nvery low quality\nGGUF\ni1-IQ2_M\n2.5\nGGUF\ni1-Q2_K\n2.6\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XXS\n2.7\nlower quality\nGGUF\ni1-IQ3_XS\n2.9\nGGUF\ni1-IQ3_S\n3.0\nbeats Q3_K*\nGGUF\ni1-Q3_K_S\n3.0\nIQ3_XS probably better\nGGUF\ni1-IQ3_M\n3.2\nGGUF\ni1-Q3_K_M\n3.4\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n3.7\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n3.7\nGGUF\ni1-IQ4_NL\n3.9\nprefer IQ4_XS\nGGUF\ni1-Q4_0\n3.9\nfast, low quality\nGGUF\ni1-Q4_K_S\n4.0\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n4.2\nfast, recommended\nGGUF\ni1-Q4_1\n4.3\nGGUF\ni1-Q5_K_S\n4.8\nGGUF\ni1-Q5_K_M\n4.9\nGGUF\ni1-Q6_K\n5.6\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "mradermacher/DOSMo-7B-v0.2-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/maxidl/DOSMo-7B-v0.2\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n2.8\nGGUF\nQ3_K_S\n3.3\nGGUF\nQ3_K_M\n3.6\nlower quality\nGGUF\nQ3_K_L\n3.9\nGGUF\nIQ4_XS\n4.0\nGGUF\nQ4_K_S\n4.2\nfast, recommended\nGGUF\nQ4_K_M\n4.5\nfast, recommended\nGGUF\nQ5_K_S\n5.1\nGGUF\nQ5_K_M\n5.2\nGGUF\nQ6_K\n6.0\nvery good quality\nGGUF\nQ8_0\n7.8\nfast, best quality\nGGUF\nf16\n14.6\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "saheedniyi/YarnGPT": "YarnGPT\nTable of Contents\nModel Summary\nPrompt YarnGPT\nSimple Nigerian Accented-NewsReader\nModel Description\nBias, Risks, and Limitations\nSpeech Samples\nTraining\nFuture Improvements?\nCitation [optional]\nCredits & References\nYarnGPT\nTable of Contents\nModel Summary\nModel Description\nBias, Risks, and Limitations\nRecommendations\nSpeech Samples\nTraining\nFuture Improvements\nCitation\nCredits & References\nModel Summary\nYarnGPT is a text-to-speech (TTS) model designed to synthesize Nigerian-accented English leveraging pure language modelling without external adapters or complex architectures, offering high-quality, natural, and culturally relevant speech synthesis for diverse applications.\nYour browser does not support the video tag.\nHow to use (Colab)\nThe model can generate audio on its own but its better to use a voice to prompt the model, there are about 11 voices supported by default (6 males and 5 females ):\nzainab\njude\ntayo\nremi\nidera (default and best voice)\nregina\nchinenye\numar\nosagie\njoke\nemma (the names do not correlate to any tribe or accent)\nPrompt YarnGPT\n# clone the YarnGPT repo to get access to the `audiotokenizer`\n!git clone https://github.com/saheedniyi02/yarngpt.git\n# install some necessary libraries\n!pip install outetts==0.2.3 uroman\n#import some important packages\nimport os\nimport re\nimport json\nimport torch\nimport inflect\nimport random\nimport uroman as ur\nimport numpy as np\nimport torchaudio\nimport IPython\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outetts.wav_tokenizer.decoder import WavTokenizer\nfrom yarngpt.audiotokenizer import AudioTokenizer\n# download the wavtokenizer weights and config (to encode and decode the audio)\n!wget https://huggingface.co/novateur/WavTokenizer-medium-speech-75token/resolve/main/wavtokenizer_mediumdata_frame75_3s_nq1_code4096_dim512_kmeans200_attn.yaml\n!gdown 1-ASeEkrn4HY49yZWHTASgfGFNXdVnLTt\n# model path and wavtokenizer weight path (the paths are assumed based on Google colab, a different environment might save the weights to a different location).\nhf_path=\"saheedniyi/YarnGPT\"\nwav_tokenizer_config_path=\"/content/wavtokenizer_mediumdata_frame75_3s_nq1_code4096_dim512_kmeans200_attn.yaml\"\nwav_tokenizer_model_path = \"/content/wavtokenizer_large_speech_320_24k.ckpt\"\n# create the AudioTokenizer object\naudio_tokenizer=AudioTokenizer(\nhf_path,wav_tokenizer_model_path,wav_tokenizer_config_path\n)\n#load the model weights\nmodel = AutoModelForCausalLM.from_pretrained(hf_path,torch_dtype=\"auto\").to(audio_tokenizer.device)\n# your input text\ntext=\"Uhm, so, what was the inspiration behind your latest project? Like, was there a specific moment where you were like, 'Yeah, this is it!' Or, you know, did it just kind of, uh, come together naturally over time?\"\n# creating a prompt, when creating a prompt, there is an optional `speaker_name` parameter, the possible speakers are \"idera\",\"emma\",\"jude\",\"osagie\",\"tayo\",\"zainab\",\"joke\",\"regina\",\"remi\",\"umar\",\"chinenye\" if no speaker is selected a speaker is chosen at random\nprompt=audio_tokenizer.create_prompt(text,\"idera\")\n# tokenize the prompt\ninput_ids=audio_tokenizer.tokenize_prompt(prompt)\n# generate output from the model, you can tune the `.generate` parameters as you wish\noutput  = model.generate(\ninput_ids=input_ids,\ntemperature=0.1,\nrepetition_penalty=1.1,\nmax_length=4000,\n)\n# convert the output to \"audio codes\"\ncodes=audio_tokenizer.get_codes(output)\n# converts the codes to audio\naudio=audio_tokenizer.get_audio(codes)\n# play the audio\nIPython.display.Audio(audio,rate=24000)\n# save the audio\ntorchaudio.save(f\"audio.wav\", audio, sample_rate=24000)\nSimple Nigerian Accented-NewsReader\n!git clone https://github.com/saheedniyi02/yarngpt.git\n# install some necessary libraries\n!pip install outetts uroman trafilatura pydub\nimport os\nimport re\nimport json\nimport torch\nimport inflect\nimport random\nimport requests\nimport trafilatura\nimport inflect\nimport uroman as ur\nimport numpy as np\nimport torchaudio\nimport IPython\nfrom pydub import AudioSegment\nfrom pydub.effects import normalize\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outetts.wav_tokenizer.decoder import WavTokenizer\n!wget https://huggingface.co/novateur/WavTokenizer-medium-speech-75token/resolve/main/wavtokenizer_mediumdata_frame75_3s_nq1_code4096_dim512_kmeans200_attn.yaml\n!gdown 1-ASeEkrn4HY49yZWHTASgfGFNXdVnLTt\nfrom yarngpt.audiotokenizer import AudioTokenizer\ntokenizer_path=\"saheedniyi/YarnGPT\"\nwav_tokenizer_config_path=\"/content/wavtokenizer_mediumdata_frame75_3s_nq1_code4096_dim512_kmeans200_attn.yaml\"\nwav_tokenizer_model_path = \"/content/wavtokenizer_large_speech_320_24k.ckpt\"\naudio_tokenizer=AudioTokenizer(\ntokenizer_path,wav_tokenizer_model_path,wav_tokenizer_config_path\n)\nmodel = AutoModelForCausalLM.from_pretrained(tokenizer_path,torch_dtype=\"auto\").to(audio_tokenizer.device)\ndef split_text_into_chunks(text, word_limit=25):\n\"\"\"\nFunction to split a long web page into reasonable chunks\n\"\"\"\nsentences=[sentence.strip() for sentence in text.split('.') if sentence.strip()]\nchunks=[]\nfor sentence in sentences:\nchunks.append(\".\")\nsentence_splitted=sentence.split(\" \")\nnum_words=len(sentence_splitted)\nstart_index=0\nif num_words>word_limit:\nwhile start_index<num_words:\nend_index=min(num_words,start_index+word_limit)\nchunks.append(\" \".join(sentence_splitted[start_index:start_index+word_limit]))\nstart_index=end_index\nelse:\nchunks.append(sentence)\nreturn chunks\n#Extracting the content of a webpage\npage=requests.get(\"https://punchng.com/expensive-feud-how-burna-boy-cubana-chief-priests-fight-led-to-dollar-rain/\")\ncontent=trafilatura.extract(page.text)\nchunks=split_text_into_chunks(content)\n#Looping over the chunks and adding creating a large `all_codes` list\nall_codes=[]\nfor i,chunk in enumerate(chunks):\nprint(i)\nprint(\"\\n\")\nprint(chunk)\nif chunk==\".\":\n#add silence for 0.25 seconds if we encounter a full stop\nall_codes.extend([453]*20)\nelse:\nprompt=audio_tokenizer.create_prompt(chunk,\"chinenye\")\ninput_ids=audio_tokenizer.tokenize_prompt(prompt)\noutput  = model.generate(\ninput_ids=input_ids,\ntemperature=0.1,\nrepetition_penalty=1.1,\nmax_length=4000,\n)\ncodes=audio_tokenizer.get_codes(output)\nall_codes.extend(codes)\n# Converting to audio\naudio=audio_tokenizer.get_audio(all_codes)\nIPython.display.Audio(audio,rate=24000)\ntorchaudio.save(f\"news1.wav\", audio, sample_rate=24000)\nModel Description\nDeveloped by: Saheedniyi\nModel type: Text-to-Speech\nLanguage(s) (NLP): English--> Nigerian Accented English\nFinetuned from: HuggingFaceTB/SmolLM2-360M\nRepository: YarnGPT Github Repository\nPaper: IN PROGRESS.\nDemo: 1) Prompt YarnGPT notebook\n2) Simple news reader\nUses\nGenerate Nigerian-accented English speech for experimental purposes.\nOut-of-Scope Use\nThe model is not suitable for generating speech in languages other than English or other accents.\nBias, Risks, and Limitations\nThe model may not capture the full diversity of Nigerian accents and could exhibit biases based on the training dataset. Also a lot of the text the model was trained on were automatically generated which could impact performance.\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases, and limitations of the model. Feedback and diverse training data contributions are encouraged.\nSpeech Samples\nListen to samples generated by YarnGPT:\nInput\nAudio\nNotes\nHello world! I am Saheed Azeez and I am excited to announce the release of his project, I have been gathering data and learning how to build Audio-based models over the last two months, but thanks to God, I have been able to come up with something\nYour browser does not support the audio element.\n(temperature=0.1, repetition_penalty=1.1), voice: idera\nWizkid, Davido, Burna Boy perform at same event in Lagos. This event has sparked many reactions across social media, with fans and critics alike praising the artistes' performances and the rare opportunity to see the three music giants on the same stage.\nYour browser does not support the audio element.\n(temperature=0.1, repetition_penalty=1.1), voice: jude\nSince Nigeria became a republic in 1963, 14 individuals have served as head of state of Nigeria under different titles. The incumbent president Bola Tinubu is the nation's 16th head of state.\nYour browser does not support the audio element.\n(temperature=0.1, repetition_penalty=1.1), voice: zainab, the model struggled in pronouncing ` in 1963`\nI visited the President, who has shown great concern for the security of Plateau State, especially considering that just a year ago, our state was in mourning. The President‚Äôs commitment to addressing these challenges has been steadfast.\nYour browser does not support the audio element.\n(temperature=0.1, repetition_penalty=1.1), voice: emma\nScientists have discovered a new planet that may be capable of supporting life!\nYour browser does not support the audio element.\n(temperature=0.1, repetition_penalty=1.1)\nTraining\nData\nTrained on a dataset of publicly available Nigerian movies, podcasts ( using the subtitle-audio pairs) and open source Nigerian-related audio data on Huggingface,\nPreprocessing\nAudio files were preprocessed and resampled to 24Khz and tokenized using wavtokenizer.\nTraining Hyperparameters\nNumber of epochs: 5\nbatch_size: 4\nScheduler: linear schedule with warmup for 4 epochs, then linear decay to zero for the last epoch\nOptimizer: AdamW (betas=(0.9, 0.95),weight_decay=0.01)\nLearning rate: 1*10^-3\nHardware\nGPUs: 1 A100 (google colab: 50 hours)\nSoftware\nTraining Framework: Pytorch\nFuture Improvements?\nScaling up model size and human-annotaed/ reviewed training data\nWrap the model around an API endpoint\nAdd support for local Nigerian languages\nVoice cloning.\nPotential expansion into speech-to-speech assistant models\nCitation [optional]\nBibTeX:\n@misc{yarngpt2025,\nauthor = {Saheed Azeez},\ntitle = {YarnGPT: Nigerian-Accented English Text-to-Speech Model},\nyear = {2025},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/SaheedAzeez/yarngpt}\n}\nAPA:\nSaheed Azeez. (2025). YarnGPT: Nigerian-Accented English Text-to-Speech Model. Hugging Face. Available at: https://huggingface.co/saheedniyi/YarnGPT\nCredits & References\nOuteAI/OuteTTS-0.2-500M\nWavTokenizer\nCTC Forced Alignment\nVoicera",
    "geneing/Kokoro": "Usage\nModel Facts\nReleases\nLicenses\nEvaluation\nTraining Details\nLimitations\nAcknowledgements\nModel Card Contact\n‚ù§Ô∏è Kokoro Discord Server: https://discord.gg/QuGxSWBfQy\nüì£ Got Synthetic Data? Want Trained Voicepacks? See https://hf.co/posts/hexgrad/418806998707773\nKokoro is a frontier TTS model for its size of 82 million parameters (text in/audio out).\nOn 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision under an Apache 2.0 license. As of 2 Jan 2025, 10 unique Voicepacks have been released, and a .onnx version of v0.19 is available.\nIn the weeks leading up to its release, Kokoro v0.19 was the #1ü•á ranked model in TTS Spaces Arena. Kokoro had achieved higher Elo in this single-voice Arena setting over other models, using fewer parameters and less data:\nKokoro v0.19: 82M params, Apache, trained on <100 hours of audio\nXTTS v2: 467M, CPML, >10k hours\nEdge TTS: Microsoft, proprietary\nMetaVoice: 1.2B, Apache, 100k hours\nParler Mini: 880M, Apache, 45k hours\nFish Speech: ~500M, CC-BY-NC-SA, 1M hours\nKokoro's ability to top this Elo ladder suggests that the scaling law (Elo vs compute/data/params) for traditional TTS models might have a steeper slope than previously expected.\nYou can find a hosted demo at hf.co/spaces/hexgrad/Kokoro-TTS.\nUsage\nThe following can be run in a single cell on Google Colab.\n# 1Ô∏è‚É£ Install dependencies silently\n!git lfs install\n!git clone https://huggingface.co/hexgrad/Kokoro-82M\n%cd Kokoro-82M\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n!pip install -q phonemizer torch transformers scipy munch\n# 2Ô∏è‚É£ Build the model and load the default voicepack\nfrom models import build_model\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nMODEL = build_model('kokoro-v0_19.pth', device)\nVOICE_NAME = [\n'af', # Default voice is a 50-50 mix of Bella & Sarah\n'af_bella', 'af_sarah', 'am_adam', 'am_michael',\n'bf_emma', 'bf_isabella', 'bm_george', 'bm_lewis',\n'af_nicole', 'af_sky',\n][0]\nVOICEPACK = torch.load(f'voices/{VOICE_NAME}.pt', weights_only=True).to(device)\nprint(f'Loaded voice: {VOICE_NAME}')\n# 3Ô∏è‚É£ Call generate, which returns 24khz audio and the phonemes used\nfrom kokoro import generate\ntext = \"How could I know? It's an unanswerable question. Like asking an unborn child if they'll lead a good life. They haven't even been born.\"\naudio, out_ps = generate(MODEL, text, VOICEPACK, lang=VOICE_NAME[0])\n# Language is determined by the first letter of the VOICE_NAME:\n# üá∫üá∏ 'a' => American English => en-us\n# üá¨üáß 'b' => British English => en-gb\n# 4Ô∏è‚É£ Display the 24khz audio and print the output phonemes\nfrom IPython.display import display, Audio\ndisplay(Audio(data=audio, rate=24000, autoplay=True))\nprint(out_ps)\nIf you have trouble with espeak-ng, see this github issue. Mac users also see this, and Windows users see this.\nFor ONNX usage, see #14.\nModel Facts\nNo affiliation can be assumed between parties on different lines.\nArchitecture:\nStyleTTS 2: https://arxiv.org/abs/2306.07691\nISTFTNet: https://arxiv.org/abs/2203.02395\nDecoder only: no diffusion, no encoder release\nArchitected by: Li et al @ https://github.com/yl4579/StyleTTS2\nTrained by: @rzvzn on Discord\nSupported Languages: American English, British English\nModel SHA256 Hash: 3b0c392f87508da38fad3a2f9d94c359f1b657ebd2ef79f9d56d69503e470b0a\nReleases\n25 Dec 2024: Model v0.19, af_bella, af_sarah\n26 Dec 2024: am_adam, am_michael\n28 Dec 2024: bf_emma, bf_isabella, bm_george, bm_lewis\n30 Dec 2024: af_nicole\n31 Dec 2024: af_sky\n2 Jan 2025: ONNX v0.19 ebef4245\nLicenses\nApache 2.0 weights in this repository\nMIT inference code in spaces/hexgrad/Kokoro-TTS adapted from yl4579/StyleTTS2\nGPLv3 dependency in espeak-ng\nThe inference code was originally MIT licensed by the paper author. Note that this card applies only to this model, Kokoro. Original models published by the paper author can be found at hf.co/yl4579.\nEvaluation\nMetric: Elo rating\nLeaderboard: hf.co/spaces/Pendrokar/TTS-Spaces-Arena\nThe voice ranked in the Arena is a 50-50 mix of Bella and Sarah. For your convenience, this mix is included in this repository as af.pt, but you can trivially reproduce it like this:\nimport torch\nbella = torch.load('voices/af_bella.pt', weights_only=True)\nsarah = torch.load('voices/af_sarah.pt', weights_only=True)\naf = torch.mean(torch.stack([bella, sarah]), dim=0)\nassert torch.equal(af, torch.load('voices/af.pt', weights_only=True))\nTraining Details\nCompute: Kokoro was trained on A100 80GB vRAM instances rented from Vast.ai (referral link). Vast was chosen over other compute providers due to its competitive on-demand hourly rates. The average hourly cost for the A100 80GB vRAM instances used for training was below $1/hr per GPU, which was around half the quoted rates from other providers at the time.\nData: Kokoro was trained exclusively on permissive/non-copyrighted audio data and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\nPublic domain audio\nAudio licensed under Apache, MIT, etc\nSynthetic audio[1] generated by closed[2] TTS models from large providers\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf\n[2] No synthetic audio from open TTS models or \"custom voice clones\"\nEpochs: Less than 20 epochs\nTotal Dataset Size: Less than 100 hours of audio\nLimitations\nKokoro v0.19 is limited in some specific ways, due to its training set and/or architecture:\n[Data] Lacks voice cloning capability, likely due to small <100h training set\n[Arch] Relies on external g2p (espeak-ng), which introduces a class of g2p failure modes\n[Data] Training dataset is mostly long-form reading and narration, not conversation\n[Arch] At 82M params, Kokoro almost certainly falls to a well-trained 1B+ param diffusion transformer, or a many-billion-param MLLM like GPT-4o / Gemini 2.0 Flash\n[Data] Multilingual capability is architecturally feasible, but training data is mostly English\nRefer to the Philosophy discussion to better understand these limitations.\nWill the other voicepacks be released? There is currently no release date scheduled for the other voicepacks, but in the meantime you can try them in the hosted demo at hf.co/spaces/hexgrad/Kokoro-TTS.\nAcknowledgements\n@yl4579 for architecting StyleTTS 2\n@Pendrokar for adding Kokoro as a contender in the TTS Spaces Arena\nModel Card Contact\n@rzvzn on Discord. Server invite: https://discord.gg/QuGxSWBfQy\nhttps://terminator.fandom.com/wiki/Kokoro",
    "calcuis/pony": "gguf quantized legacy models for anime (additional test pack for gguf-node)\nsetup (in general)\nrun it straight (no installation needed way)\nworkflow\nreview\nreference\ngguf quantized legacy models for anime (additional test pack for gguf-node)\nPrompt\nscore_9, score_8_up, score_7_up,   film grain, photo by fuji-proplus-ii film,  raw picture of 20 years old woman in lingerie,  portrait, deep blue sky, cloudy sky, outdoor, high key light, soft shadow, Fiery clouds, colored hair\nNegative Prompt\nscore_6, score_5, score_4, source_pony, (worst quality:1.2), (low quality:1.2), (normal quality:1.2), lowres, bad anatomy, bad hands, signature, watermarks, ugly, imperfect eyes, skewed eyes, unnatural face, unnatural body, error, extra limb, missing limbs, painting by bad-artist\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one; but different model (boleromix)\nPrompt\ndrag it to browser <metadata> same descriptor to the 1st one; but different model (snow)\nsetup (in general)\ndrag gguf file(s) to diffusion_models folder (./ComfyUI/models/diffusion_models)\ndrag clip or encoder(s), i.e., g-clip and l-clip, to text_encoders folder (./ComfyUI/models/text_encoders)\ndrag vae decoder(s), i.e., legacy-vae, to vae folder (./ComfyUI/models/vae)\nrun it straight (no installation needed way)\nget the comfy pack with the new gguf-node here\nrun the .bat file in the main directory\nworkflow\ndrag any workflow json file to the activated browser; or\ndrag any generated output file (i.e., picture, video, etc.; which contains the workflow metadata) to the activated browser\nexample workflow json for the safetensors\nexample workflow json for the gguf\nreview\nuse tag/word(s) as input for more accurate results for those legacy models; not very convenient (compare to the recent models) at the very beginning\ncredits should be given to those contributors from civitai platform\ngood to run on old machines, i.e., 9xx series or before (legacy mode [--disable-cuda-malloc --lowvram] supported); compatible with the new gguf-node\ndisclaimer: some models (original files) are provided by someone else and we might not easily spot out the creator/contributor(s) behind, unless it was specified in the source; rather let it blank instead of anonymous/unnamed/unknown; if it is your work, do let us know; we will address it back properly and probably; thanks for everything\nreference\ncomfyui comfyanonymous\ngguf-node (pypi|repo|pack)"
}