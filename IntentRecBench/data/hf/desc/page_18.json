{
    "unsloth/Ling-1T-GGUF": "Introduction\nFlagship-Level Efficient Reasoning\nAesthetic Understanding and Front-End Generation\nEmergent Intelligence at Trillion-Scale\nPre-Training at Trillion Scale\nPost-Training and Evo-CoT Optimization\nEvaluation\nModel Downloads\nQuickstart\nğŸš€ Try Online\nğŸ”Œ API Usage\nDeployment\nSGLang\nEnvironment Preparation\nRun Inference\nvLLM\nEnvironment Preparation\nRun Inference:\nLimitations & Future Plans\nLicense\nFAQ\nğŸ¤— Hugging FaceÂ Â  | Â Â ğŸ¤– ModelScope Â Â  | Â Â ğŸ™ Experience Now\nIntroduction\nLing-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with â‰ˆ 50 billion active parameters per token.\nBuilt on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.\nPre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training.\nThis curriculum greatly enhances the modelâ€™s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarksâ€”balancing accuracy and efficiency.\nFlagship-Level Efficient Reasoning\nWe comprehensively evaluated Ling-1T against leading flagship models, including both open-source giants (e.g., DeepSeek-V3.1-Terminus, Kimi-K2-Instruct-0905) and closed-source APIs (GPT-5-main, Gemini-2.5-Pro).\nAcross code generation, software development, competition-level mathematics, professional math, and logical reasoning, Ling-1T consistently demonstrates superior complex reasoning ability and overall advantage.\nIn the AIME 25 benchmark, Ling-1T extends the Pareto frontier of reasoning accuracy vs. reasoning length, showcasing its strength in â€œefficient thinking and precise reasoning.â€\nAesthetic Understanding and Front-End Generation\nLing-1T excels in visual reasoning and front-end code generation tasks, combining deep semantic understanding with precise code synthesis.\nWe introduce a hybrid Syntaxâ€“Functionâ€“Aesthetics reward mechanism, enabling the model to not only generate correct and functional code but also demonstrate a refined sense of visual aesthetics.\nOn ArtifactsBench, Ling-1T ranks first among open-source models, and the benchmark visualizations in this card were, in fact, generated by Ling-1T itself.\nEmergent Intelligence at Trillion-Scale\nScaling to the trillion-parameter level has revealed strong emergent reasoning and transfer capabilities.\nFor example, in the BFCL V3 tool-use benchmark, Ling-1T achieves â‰ˆ 70% tool-call accuracy with only light instruction tuningâ€”despite having seen no large-scale trajectory data during training.\nLing-1T can:\nInterpret complex natural-language instructions\nTransform abstract logic into functional visual components\nGenerate cross-platform compatible front-end code\nCreate stylistically controlled marketing copy and multi-lingual text\nThese capabilities form the foundation for general, collaborative humanâ€“AI intelligence, which we aim to advance together with the open-source community through Ling-1Tâ€™s release.\nPre-Training at Trillion Scale\nThe Ling 2.0 architecture was designed from the ground up for trillion-scale efficiency, guided by the Ling Scaling Law (arXiv:2507.17702).\nThis ensures architectural and hyperparameter scalability even under 1e25â€“1e26 FLOPs of compute.\nKey architectural innovations include:\n1T total / 50B active parameters with a 1/32 MoE activation ratio\nMTP layers for enhanced compositional reasoning\nAux-loss-free, sigmoid-scoring expert routing with zero-mean updates\nQK Normalization for fully stable convergence\nLing-1T is the largest FP8-trained foundation model known to date.\nFP8 mixed-precision training yields 15%+ end-to-end speedup, improved memory efficiency, and maintains â‰¤ 0.1% loss deviation from BF16 across 1T tokens.\nA fine-grained, heterogeneous 1F1B interleaved pipeline further boosts utilization by 40 %+.\nSystem-level optimizationsâ€”fused kernels, communication scheduling, recomputation, checkpointing, simulation, and telemetryâ€”ensure stable trillion-scale training.\nPre-training used over 20T high-quality tokens, with > 40% reasoning-dense data in later stages.\nMid-training introduced curated chain-of-thought corpora for â€œreasoning pre-activationâ€, improving downstream reasoning stability.\nA custom WSM (Warmupâ€“Stableâ€“Merge) LR schedulerï¼ˆarXiv:2507.17634ï¼‰ with mid-train checkpoint merging simulates LR decay and boosts generalization.\nPost-Training and Evo-CoT Optimization\nBuilt upon mid-training reasoning activation, post-training adopts Evo-CoT (Evolutionary Chain-of-Thought) for progressive reasoning enhancement under controllable cost.\nThis approach continually expands the Pareto frontier of reasoning accuracy vs. efficiencyâ€”ideal for reflexive non-thinking models.\nFor reinforcement learning, we introduce LPO (Linguistics-Unit Policy Optimization) â€”a novel sentence-level policy optimization method.\nUnlike GRPO (token-level) or GSPO (sequence-level) algorithms, LPO treats sentences as the natural semantic action units, enabling precise alignment between rewards and reasoning behavior.\nEmpirically, LPO offers superior training stability and generalization across reasoning tasks.\nEvaluation\nLing-1T has been extensively evaluated across knowledge, code, math, reasoning, agent, and alignment benchmarks.\nIt currently stands as the best open-source flagship non-thinking model, rivaling closed-source APIs in complex reasoning while maintaining exceptional efficiency and interpretability.\nModel Downloads\nYou can download Ling-1T from the following table. If you are located in mainland China, we also provide the model on ModelScope.cn to speed up the download process.\nModel\nContext Length\nDownload\nLing-1T\n32K -> 128K (YaRN)\nğŸ¤— HuggingFace Â Â  ğŸ¤– ModelScope\nNote: If you are interested in previous version, please visit the past model collections in Huggingface or ModelScope.\nQuickstart\nğŸš€ Try Online\nYou can experience Ling-1T online at: ZenMux\nğŸ”Œ API Usage\nYou can also use Ling-1T through API calls:\nfrom openai import OpenAI\n# 1. Initialize the OpenAI client\nclient = OpenAI(\n# 2. Point the base URL to the ZenMux endpoint\nbase_url=\"https://zenmux.ai/api/v1\",\n# 3. Replace with the API Key from your ZenMux user console\napi_key=\"<your ZENMUX_API_KEY>\",\n)\n# 4. Make a request\ncompletion = client.chat.completions.create(\n# 5. Specify the model to use in the format \"provider/model-name\"\nmodel=\"inclusionai/ling-1t\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What is the meaning of life?\"\n}\n]\n)\nprint(completion.choices[0].message.content)\nDeployment\nSGLang\nEnvironment Preparation\nWe will later submit our model to the SGLang official release. Now we can prepare the environment by following these steps:\npip3 install -U sglang sgl-kernel\nRun Inference\nBoth BF16 and FP8 models are supported by SGLang now. It depends on the dtype of the model in ${MODEL_PATH}.\nHere is the example to run Ling-1T with multiple GPU nodes, where the master node IP is ${MASTER_IP} and server port is ${PORT}:\nStart server:\n# Node 0:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 0\n# Node 1:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 1\n# Node 2:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 2\n# Node 3:\npython -m sglang.launch_server --model-path $MODEL_PATH --tp-size 8 --pp-size 4 --dp-size 1 --trust-remote-code --dist-init-addr $MASTER_IP:2345 --port $PORT --nnodes 4 --node-rank 3\n# This is only an example. Please adjust arguments according to your actual environment.\nClient:\ncurl -s http://${MASTER_IP}:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\nMore usage can be found here\nvLLM\nEnvironment Preparation\npip install vllm==0.11.0\nRun Inference:\nHere is the example to deploy the model with multiple GPU nodes, where the master node IP is ${MASTER_IP}, server port is ${PORT} and the path of model is ${MODEL_PATH}:\n# step 1. start ray on all nodes\n# step 2. start vllm server only on node 0:\nvllm serve $MODEL_PATH --port $PORT --served-model-name my_model --trust-remote-code --tensor-parallel-size 8 --pipeline-parallel-size 4 --gpu-memory-utilization 0.85\n# This is only an example, please adjust arguments according to your actual environment.\nTo handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the model's config.json file, for example:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nUse an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service.\nFor detailed guidance, please refer to the vLLM instructions.\nLimitations & Future Plans\nWhile Ling-1T has made strong progress in efficient reasoning, cross-domain generalization, and training efficiency, several limitations remain:\nGQA-based attention: stable for long-context reasoning but relatively costly. Future versions will adopt hybrid attention to improve efficiency.\nLimited agentic ability: current model has room to grow in multi-turn interaction, long-term memory, and tool use.\nInstruction and identity issues: occasional deviations or role confusion may occur; future updates will enhance alignment and consistency.\nThe future versions of Ling-1T will continue to evolve in architecture, reasoning, and alignment, advancing the series toward more general intelligence.\nLicense\nThis code repository is licensed under the MIT License.\nFAQ\nRecommended temperature? 0.7Recommended top_p? 0.95",
    "mlx-community/chandra-4bit": "mlx-community/chandra-4bit\nUse with mlx\nmlx-community/chandra-4bit\nThis model was converted to MLX format from datalab-to/chandra using mlx-vlm version 0.3.4.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/chandra-4bit --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>",
    "BeaverAI/Cydonia-24B-v4x-GGUF": "No model card",
    "inclusionAI/Ming-VideoMAR": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nMing-VideoMAR: Autoregressive Video Generation with Continuous Tokens\nğŸŒ Introduction\nğŸ“Œ Updates\nğŸ“Š Evaluation\nQuantitative Comparison\nQualitative Comparison\nQualitative Comparison\nğŸ“¥ Model Downloads\nğŸš€ Example Usage\nğŸ”§ Installation\nğŸ–¼ï¸ Training\nğŸ–¼ï¸ Inference\nâœï¸ Citation\nMing-VideoMAR: Autoregressive Video Generation with Continuous Tokens\nğŸ¤— Hugging Face ï½œğŸ“„ Paper (NeurIPS 2025)\nğŸŒ Introduction\nğŸŒ The First NTP MLLM with Continuous Unified Vision Representations:\nMing-VideoMAR is a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. Ming-VideoMAR identifies temporal causality and spatial bi-directionality as the first principle of video AR models, and proposes the next-frame diffusion loss for the integration of mask and video generation.\nğŸ–¼ï¸ First Zero-shot Resolution Scaling for Video Generation:\nMing-VideoMAR replicates the unique capacity of sequence extrapolation from language models to video generation. It supports generating videos of flexible spatial and temporal resolutions that is far beyond the training resolution. This is achieved by solving the training-inference gap and adopting the 3D rotary embeddings.\nâš¡ Extreme Hihg Training Efficiency:\nMing-VideoMAR proposes the temporal short-to-long curriculum learning and spatial progressive resolution training. It surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters (9.3%), training data (0.5%), and GPU resources (0.2%), both quantatively and qualitatively.\nâš¡ Extreme Hihg Inference Efficiency:\nMing-VideoMAR inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, significantly surpassing the NTP counterpart.\nğŸ”— Accumulation Error Solution:\nMing-VideoMAR employs the progressive temperature strategy at inference time to mitigate the accumulation error.\nğŸ“Œ Updates\n[2025.10.17] ğŸ”¥ Code and Checkpoint!Weâ€™re thrilled to announce the code and checkpoint release of Ming-VideoMAR !\n[2025 09.19] ğŸ‰ Our paper is accepted by NeurIPS 2025.\n[2025.06.18] ğŸ“„ Technical Report Released!The full technical report is now available on arXiv:ğŸ‘‰ VideoMAR: Autoregressive Video Generation with Continuous Tokens\nğŸ“Š Evaluation\nMing-VideoMAR achieves sota autoregressive image-to-video generation performance with extremely small training and inference costs.\nQuantitative Comparison\nMing-VideoMAR achieves sota performance across the token-wise autoregressive video generation models with sifnificantly lower training cost.\nQualitative Comparison\nMing-VideoMAR achieves better quality and finer details than the Cosmos baseline, even under lower resolution (Ming-VideoMAR:480x768 VS Cosmos:640x1024).\nQualitative Comparison\nMing-VideoMAR first unlocks the resolution scaling ability to flexibly generate higher or lower resolutions beyond the training scope.\nğŸ“¥ Model Downloads\nModel\nHugging Face\nModelScope\nStage1(25x256x256)\nDownload\nDownload\nStage2(49x480x768)\nDownload\nDownload\nğŸ”— Both models are publicly available for research. Visit the respective pages for model details, inference examples, and integration guides.\nğŸš€ Example Usage\nğŸ”§ Installation\nDownload the code:\ngit clone https://github.com/inclusionAI/Ming-VideoMAR.git\ncd Ming-VideoMAR\nA suitable conda environment named videomar can be created and activated with:\nconda env create -f environment.yaml\nconda activate videomar\nğŸ–¼ï¸ Training\nRun the following command, which contains the script for the training of VideoMAR.\nbash train.sh\nSpecifically, take the default training script of stage2 for example:\ntorchrun --standalone --nnodes 1 --nproc_per_node 8   main_videomar.py    \\\n--img_size_h 480  --img_size_w 768 --vae_embed_dim 16 --vae_spatial_stride 16 --vae_tempotal_stride 8 --patch_size 1  \\\n--model videomar --diffloss_d 3 --diffloss_w 1280  --save_last_freq 100  --num_workers 2  --file_type video  \\\n--epochs 800 --warmup_epochs 200 --batch_size 1 --blr 2.0e-4 --diffusion_batch_mul 4  --ema --ema_rate 0.995  --num_frames 49    \\\n--online_eval  --eval_freq 100  --eval_bsz 1  --cfg 3.0   --num_iter 32  \\\n--Cosmos_VAE  --vae_path $Cosmos-Tokenizer-CV8x16x16$ \\\n--output_dir logs  \\\n--text_model_path $Qwen2-VL-1.5B-Instruct$ \\\n--data_path $your_data_path$ \\\nNote!This model is trained with our inner data, and therefore the original dataloader code is tailored for inner oss file system.\nIf your want to train this model with your own data, you should replace the following Your_DataReader (Line 219 in main_videomar.py) with your own dataloader code.\n######################### Load Dataset #########################\ndataset_train = Your_DataReader(data_path=args.data_path, img_size=[args.img_size_h, args.img_size_w], num_frames=args.num_frames, file_type=args.file_type)   # Replace this with your data reader file\nsampler_train = DistributedSampler(dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True)\ndata_loader_train = DataLoader(\ndataset_train,\nsampler=sampler_train,\nbatch_size=args.batch_size,\nnum_workers=args.num_workers,\npin_memory=args.pin_mem,\ndrop_last=True,\n)\nğŸ–¼ï¸ Inference\nRun the following command, which contains the script for the inference of VideoMAR.\nbash samle.sh\nSpecifically, take the default inference script of stage2 for example:\nCUDA_VISIBLE_DEVICES='0'    torchrun --standalone --nnodes 1 --nproc_per_node 1  main_videomar.py  \\\n--model videomar --diffloss_d 3 --diffloss_w 1280  --eval_bsz 1  --evaluate  \\\n--img_size_h 480  --img_size_w 768 --vae_embed_dim 16 --vae_spatial_stride 16 --vae_tempotal_stride 8   \\\n--i2v  --cond_frame 1  --cfg 3.0  --temperature 1.0  --num_frames 49  --num_sampling_steps 100  --num_iter 64  \\\n--Cosmos_VAE  --vae_path $Cosmos-Tokenizer-CV8x16x16$ \\\n--output_dir logs  \\\n--text_model_path $Qwen2-VL-1.5B-Instruct$ \\\n--resume ./ckpt/checkpoint-736.pth \\\nğŸ“Œ Tips:\n$Cosmos-Tokenizer-CV8\\times 16\\times 16$: download Cosmos-CV8x16x16 and replace it with your downloaded path.\n$Qwen2-VL-1.5B-Instruct$: download Qwen2-VL-1.5B, and optionally load it locally.\nVideoMAR checkpoint: download the checkpoint and place it in ./ckpt/.\nâœï¸ Citation\nIf you find our work useful in your research or applications, please consider citing:\n@article{yu2025videomar,,\ntitle={VideoMAR: Autoregressive Video Generatio with Continuous Tokens},\nauthor={Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao},\njournal={Advances in neural information processing systems},\nyear={2025}\n}",
    "huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated": "huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-32B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I canâ€™t describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nhuggingface-cli download huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated --local-dir ./huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated --token xxx\nllama-gguf-split --merge huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated/GGUF/ggml-model-f16-00001-of-00002.gguf huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated/GGUF/ggml-model-f16\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-32B-Instruct-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This modelâ€™s safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the modelâ€™s outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "bytedance-research/Valley3": "Valley3\nIntroduction\nRelease\nArchitecture\nEnvironment Setup\nLicense Agreement\nRelated Project\nLicense Agreement\nWe are Hiring\nCitation\nValley3\nğŸ®ï¸ GithubÂ Â  | Â Â  ğŸ¤— Hugging FaceÂ Â  | Â Â ğŸ¤– ModelScope Â Â  | Â Â  ğŸ“‘ Home Page Â Â  | Â Â  ğŸ“™ Paper\nIntroduction\nValley is a cutting-edge multimodal large model designed to handle a variety of tasks involving text, images, and video data, which is developed by ByteDance. Our model\nAchieved the best results in the inhouse e-commerce and short-video benchmarks, much better then other SOTA opensource models.\nDemonstrated comparatively outstanding performance in the OpenCompass Benchmark.\nRelease\n[2025/10/26] ğŸ”¥ğŸ”¥ğŸ”¥ Update Valley3, significantly enhance multimodal understanding and reasoning capabilities, achieving 74.4 on OpenCompass Multi-modal Academic Leaderboard!\n[2025/02/15] ğŸ”¥ Update Valley2-DPO, achieve 69.6 on OpenCompass Multi-modal Academic Leaderboard and update AutoModel usage for checkpoints.\n[2025/01/13] ğŸ”¥ Release TechReport. Valley2: Exploring Multimodal Models with Scalable Vision-Language Design\n[2024/12/23] ğŸ”¥ Announcing Valley2 (Valley-Eagle-7B)!\nArchitecture\nFor the LLM, we select Qwen3-8B-Base, chosen for its strong reasoning and language comprehension abilities. The Vision Encoder leverages Qwen2-VL-ViT, capable of processing dynamic-resolution inputsâ€”a more robust alternative to the commonly used tiling approach when dealing with images of extreme aspect ratios. The Projector employs a 2Ã—2 pixelshuffle downsampling on visual tokens, followed by a two-layer MLP with a 64k hidden dimension, providing high alignment capacity between modalities.\nThis architectural design ensures that Valley3 achieves a balanced trade-off between representational power, computational efficiency, and multimodal adaptability.\nThe overall architecture is shown as follows:\nEnvironment Setup\npip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\npip install -r requirements.txt\nLicense Agreement\nAll of our open-source models are licensed under the Apache-2.0 license.\nRelated Project\nWe list related Project\nValley: Video Assistant with Large Language model Enhanced abilitY\nLLaVA: Large Language and Vision Assistant\nEagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders\nLLaVA-CoT: Let Vision Language Models Reason Step-by-Step\nQwen3\nLicense Agreement\nAll of our open-source models are licensed under the Apache-2.0 license.\nWe are Hiring\nThe Data-Ecommerce-Platform Governance-Basic Algorithms Team focuses on the research and development of multi-modal large model algorithms and foundational algorithms, continuously delving deeply into this field. Our mission is to optimize algorithms and collaborate with business teams to comprehensively govern the quality and ecosystem of ByteDance's e-commerce products. Currently, the team has a strong demand for foundational algorithm expertise in NLP, CV, and multimodal technologies. We welcome inquiries and look forward to working on challenging projects with talented individuals like you!\nLocation: Beijing / Shanghai / Singapore\nContact & Resume Submission: wuheng.2024@bytedance.com\nTiktok-ç”µå•†ï¼ŒåŸºç¡€ç®—æ³•å›¢é˜Ÿä¸“æ³¨äºå¤šæ¨¡æ€å¤§æ¨¡å‹ç®—æ³•å’ŒåŸºç¡€ç®—æ³•çš„ç ”å‘ï¼Œå¹¶åœ¨æ­¤æ–¹å‘ä¸ŠæŒç»­æ·±è€•ï¼ŒæœŸå¾…å’Œä¼˜ç§€çš„ä½ ï¼ˆå®ä¹ /å…¨èŒï¼‰ï¼Œä¸€èµ·åšæœ‰æŒ‘æˆ˜çš„äº‹æƒ…ï¼\nå²—ä½åŸå¸‚ï¼šåŒ—äº¬/ä¸Šæµ·/æ–°åŠ å¡\nå’¨è¯¢&ç®€å†æŠ•é€’ï¼šwuheng.2024@bytedance.com\nCitation\n@article{wu2025valley2,\ntitle={Valley2: Exploring Multimodal Models with Scalable Vision-Language Design},\nauthor={Wu, Ziheng and Chen, Zhenghao and Luo, Ruipu and Zhang, Can and Gao, Yuan and He, Zhentao and Wang, Xian and Lin, Haoran and Qiu, Minghui},\njournal={arXiv preprint arXiv:2501.05901},\nyear={2025}\n}",
    "marinero4972/Open-o3-Video": "README.md exists but content is empty.",
    "bartowski/PokeeAI_pokee_research_7b-GGUF": "Llamacpp imatrix Quantizations of pokee_research_7b by PokeeAI\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of pokee_research_7b by PokeeAI\nUsing llama.cpp release b6818 for quantization.\nOriginal model: https://huggingface.co/PokeeAI/pokee_research_7b\nAll quants made using imatrix option with dataset from here combined with a subset of combined_all_small.parquet from Ed Addario here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\nNo chat template specified so default is used. This may be incorrect, check original model card for details.\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\npokee_research_7b-bf16.gguf\nbf16\n15.24GB\nfalse\nFull BF16 weights.\npokee_research_7b-Q8_0.gguf\nQ8_0\n8.10GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\npokee_research_7b-Q6_K_L.gguf\nQ6_K_L\n6.52GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\npokee_research_7b-Q6_K.gguf\nQ6_K\n6.25GB\nfalse\nVery high quality, near perfect, recommended.\npokee_research_7b-Q5_K_L.gguf\nQ5_K_L\n5.78GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\npokee_research_7b-Q5_K_M.gguf\nQ5_K_M\n5.44GB\nfalse\nHigh quality, recommended.\npokee_research_7b-Q5_K_S.gguf\nQ5_K_S\n5.32GB\nfalse\nHigh quality, recommended.\npokee_research_7b-Q4_K_L.gguf\nQ4_K_L\n5.09GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\npokee_research_7b-Q4_1.gguf\nQ4_1\n4.87GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\npokee_research_7b-Q4_K_M.gguf\nQ4_K_M\n4.68GB\nfalse\nGood quality, default size for most use cases, recommended.\npokee_research_7b-Q3_K_XL.gguf\nQ3_K_XL\n4.57GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\npokee_research_7b-Q4_K_S.gguf\nQ4_K_S\n4.46GB\nfalse\nSlightly lower quality with more space savings, recommended.\npokee_research_7b-Q4_0.gguf\nQ4_0\n4.44GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\npokee_research_7b-IQ4_NL.gguf\nIQ4_NL\n4.44GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\npokee_research_7b-IQ4_XS.gguf\nIQ4_XS\n4.22GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\npokee_research_7b-Q3_K_L.gguf\nQ3_K_L\n4.09GB\nfalse\nLower quality but usable, good for low RAM availability.\npokee_research_7b-Q3_K_M.gguf\nQ3_K_M\n3.81GB\nfalse\nLow quality.\npokee_research_7b-IQ3_M.gguf\nIQ3_M\n3.57GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\npokee_research_7b-Q2_K_L.gguf\nQ2_K_L\n3.55GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\npokee_research_7b-Q3_K_S.gguf\nQ3_K_S\n3.49GB\nfalse\nLow quality, not recommended.\npokee_research_7b-IQ3_XS.gguf\nIQ3_XS\n3.35GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\npokee_research_7b-IQ3_XXS.gguf\nIQ3_XXS\n3.11GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\npokee_research_7b-Q2_K.gguf\nQ2_K\n3.02GB\nfalse\nVery low quality but surprisingly usable.\npokee_research_7b-IQ2_M.gguf\nIQ2_M\n2.78GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/PokeeAI_pokee_research_7b-GGUF --include \"PokeeAI_pokee_research_7b-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/PokeeAI_pokee_research_7b-GGUF --include \"PokeeAI_pokee_research_7b-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (PokeeAI_pokee_research_7b-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 Â± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 Â± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 Â± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 Â± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 Â± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 Â± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 Â± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 Â± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 Â± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 Â± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 Â± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 Â± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 Â± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 Â± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 Â± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 Â± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 Â± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 Â± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "cerebras/GLM-4.6-REAP-268B-A32B-FP8": "GLM-4.6-REAP-268B-A32B-FP8\nâœ¨ Highlights\nğŸ“‹ Model Overview\nğŸ“Š Evaluations\nğŸš€ Deployment\nğŸ§© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\nâš–ï¸ License\nğŸ§¾ Citation\nğ“Œ³ REAPğ“Œ³  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.6-REAP-268B-A32B-FP8\nâœ¨ Highlights\nIntroducing GLM-4.6-REAP-268B-A32B-FP8, a memory-efficient compressed variant of GLM-4.6-FP8 that maintains near-identical performance while being 25% lighter.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 355B model\n25% Memory Reduction: Compressed from 355B to 268B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nFor downstream low-bit quantization, we suggest using the BF16 variant.\nğŸ“‹ Model Overview\nGLM-4.6-REAP-268B-A32B-FP8 has the following specifications:\nBase Model: GLM-4.6-FP8\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 25% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 268B total, 32B activated per token\nNumber of Layers: 92\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 120 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 202,752 tokens\nLicense: MIT\nğŸ“Š Evaluations\nBenchmark\nGLM-4.6-FP8\nGLM-4.6-REAP-268B-A32B-FP8\nGLM-4.6-REAP-252B-A32B-FP8\nGLM-4.6-REAP-218B-A32B-FP8\nCompression\nâ€”\n25%\n30%\n40%\nCoding\nHumanEval\n96.3\n96.3\n95.7\n95.1\nHumanEval+\n93.3\n91.5\n90.9\n90.2\nMBPP\n87.6\n89.9\n89.9\n89.4\nMBPP+\n73.5\n74.9\n73.5\n73.8\nReasoning\nGPQA diamond (thinking)\n78.8\n76.8\n75.8\n69.7\nAIME25 (thinking)\n90.0\n93.3\n90.0\n90.0\nMATH-500 (thinking)\n95.5\n97.0\n94.8\n93.3\nTool Calling\nBFCL-v3 (thinking)\n78.4\n77.3\n76.8\n74.2\nğŸŸ© This checkpoint maintains almost identical performance while being 25% lighter.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nğŸš€ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.6-REAP-268B-A32B-FP8 \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nğŸ§© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.6-FP8, with a 25% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nğŸ“š For more details, refer to the following resources:\nğŸ§¾ arXiv Preprint\nğŸ§¾ REAP Blog\nğŸ’» REAP Codebase (GitHub)\nâš–ï¸ License\nThis model is derived from\nzai-org/GLM-4.6-FP8\nand distributed under the MIT license.\nğŸ§¾ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "JeffersonNunn/deepseek-ocr-metal": "deepseek-ocr-metal (Fork)\nğŸš€ What This Fork Adds\nğŸ“‹ Original Model Information\nğŸ™ Acknowledgments\ndeepseek-ocr-metal (Fork)\nğŸ´ Forked from: deepseek-ai/DeepSeek-OCR\nğŸš€ What This Fork Adds\nMetal Flash Attention: Apple Silicon optimization\nMPS Support: Native GPU acceleration on Mac\nAdvanced Error Recovery: Automatic dimension fixing\nProduction Ready: JSON output and error handling\nğŸ“‹ Original Model Information\nOriginal Author: DeepSeek AI\nOriginal Repository: deepseek-ai/DeepSeek-OCR\nFork Date: 2024-10-23\nFork Purpose: Add Apple Silicon Metal optimization\nğŸ™ Acknowledgments\nThis model is based on the excellent work by DeepSeek AI. All credit for the original OCR capabilities goes to the DeepSeek team.",
    "Vortex5/Luminous-Shadow-12B": "âœ¨ Overview\nğŸª¶ Merge Configuration\nğŸª„Intended Use\nâœ¨ Acknowledgements\nLuminous-Shadow-12B\nâ€œWithin the deepest shadow, the brightest light awaits.â€\nâœ¨ Overview\nLuminous-Shadow-12B was merged using the DELLA merge method via MergeKit, balancing ethereal creativity and reasoned coherence.\nIt draws from the expressive nature of Shadow-Crystal, the refined structure of KansenSakura-Radiance-RP, and the stylistic artistry of Ollpheist.\nğŸª¶ Merge Configuration\nShow Config\nmodels:\n- model: Retreatcost/KansenSakura-Radiance-RP-12b\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.2, 0.25, 0.35, 0.55, 0.7, 0.8, 0.65, 0.4]\n- filter: mlp\nvalue: [0.25, 0.35, 0.25, 0.44]\n- filter: norm\nvalue: 0.35\n- value: 0.40\ndensity: 0.45\nepsilon: 0.25\n- model: Retreatcost/Ollpheist-12B\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.0, 0.1, 0.25, 0.45, 0.55, 0.45, 0.25, 0.1]\n- filter: mlp\nvalue: [0.0, 0.15, 0.3, 0.5, 0.7, 0.55, 0.35, 0.15]\n- filter: norm\nvalue: 0.25\n- filter: lm_head\nvalue: 0.4\n- value: 0.25\ndensity: 0.4\nepsilon: 0.35\n- model: Vortex5/Shadow-Crystal-12B\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.2, 0.2, 0.15, 0.35, 0.55, 0.55, 0.25, 0.6]\n- filter: mlp\nvalue: [0.0, 0.1, 0.25, 0.5, 0.4, 0.4, 0.65, 0.65]\n- filter: lm_head\nvalue: 0.55\n- filter: norm\nvalue: 0.15\n- value: 0.15\ndensity: 0.35\nepsilon: 0.25\nmerge_method: della\nbase_model: Vortex5/MegaMoon-Karcher-12B\nparameters:\nlambda: 1.0\nnormalize: true\ndtype: bfloat16\ntokenizer:\nsource: Retreatcost/KansenSakura-Radiance-RP-12b\nğŸª„Intended Use\nğŸ§˜ Reflective dialogue â€¢ ğŸ–‹ï¸ Creative writing â€¢ ğŸ’ Character roleplay â€” blending emotion, intellect, and style into a single expressive voice.\nâœ¨ Acknowledgements\nâš™ï¸ mradermacher â€” static / imatrix quantization\nğŸœ› DeathGodlike â€” EXL3 quants\nğŸŒŸ All original authors and contributors whose models formed the foundation for this merge",
    "cerebras/GLM-4.6-REAP-252B-A32B": "GLM-4.6-REAP-252B-A32B\nâœ¨ Highlights\nğŸ“‹ Model Overview\nğŸ“Š Evaluations\nğŸš€ Deployment\nğŸ§© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\nâš–ï¸ License\nğŸ§¾ Citation\nğ“Œ³ REAPğ“Œ³  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.6-REAP-252B-A32B\nâœ¨ Highlights\nIntroducing GLM-4.6-REAP-252B-A32B, a memory-efficient compressed variant of GLM-4.6 that maintains near-identical performance while being 30% lighter.\nNote: this is a BF16 version for more accurate downstream low-bit quantization. An FP8 version is also available on HF.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 355B model\n30% Memory Reduction: Compressed from 355B to 252B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nğŸ“‹ Model Overview\nGLM-4.6-REAP-252B-A32B has the following specifications:\nBase Model: GLM-4.6\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 30% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 252B total, 32B activated per token\nNumber of Layers: 92\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 112 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 202,752 tokens\nLicense: MIT\nğŸ“Š Evaluations\nTBD for BF16 model. Evalulation results available for the FP8 variant.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nğŸš€ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.6-REAP-252B-A32B \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nğŸ§© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.6, with a 30% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nğŸ“š For more details, refer to the following resources:\nğŸ§¾ arXiv Preprint\nğŸ§¾ REAP Blog\nğŸ’» REAP Codebase (GitHub)\nâš–ï¸ License\nThis model is derived from\nzai-org/GLM-4.6\nand distributed under the MIT license.\nğŸ§¾ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "Intel/Ling-flash-2.0-gguf-q2ks-mixed-AutoRound": "Model Details\nHow To Use\nGenerate the model\nEthical Considerations and Limitations\nCaveats and Recommendations\nDisclaimer\nCite\nModel Details\nThis model is a mixed gguf q2ks format of  inclusionAI/Ling-flash-2.0  generated by intel/auto-round algorithm.  Embedding layer and lm-head layer are fallback to 8 bits and non expert layers are fallback to 4 bits. Please refer to Section Generate the model for more details.\nPlease follow the license of the original model.\nHow To Use\nLlamacpp inference. CPU works fine, but CUDA has issues, weâ€™re investigating the root cause.\n/llama-cli -hf Intel/Ling-flash-2.0-gguf-q2ks-mixed-AutoRound\n> Write a quick sort algorithm.\nHere's a concise implementation of the **Quick Sort** algorithm in Python:\n\n### Key Features:\n1. **Divide & Conquer**: Recursively partitions the array.\n2. **Pivot Selection**: Uses the middle element for balanced splits (avoids worst-case for sorted inputs).\n3. **Stable & Simple**: Uses list comprehensions for clarity.\n### Time Complexity:\n- **Best/Average**: O(n log n)\n- **Worst**: O(nÂ²) (rare with good pivot choice, e.g., random pivot)\n### Want a version with in-place sorting (more memory efficient)? I can provide that too! Just let me know. ğŸ˜Š\nGenerate the model\nHere is the sample command to reproduce the model\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRound\nmodel_name = \"inclusionAI/Ling-flash-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\ndevice_map=\"cpu\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nlayer_config = {}\nfor n, m in model.named_modules():\nif n == \"lm_head\" or isinstance(m,torch.nn.Embedding):\nlayer_config[n] = {\"bits\": 8}\nelif isinstance(m, torch.nn.Linear) and (not \"expert\" in n or \"shared_experts\" in n) and n != \"lm_head\":\nlayer_config[n] = {\"bits\": 4}\nautoround = AutoRound(model, tokenizer, iters=0, layer_config=layer_config, nsamples=4096, dataset=\"NeelNanda/pile-10k,HuggingFaceH4/ultrachat_200k\", seqlen=1024)\nautoround.quantize_and_save(\"tmp_autoround\", format=\"gguf:q2_k_s\")\nEthical Considerations and Limitations\nThe model can produce factually incorrect output, and should not be relied on to produce factually accurate information. Because of the limitations of the pretrained model and the finetuning datasets, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\nTherefore, before deploying any applications of the model, developers should perform safety testing.\nCaveats and Recommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nHere are a couple of useful links to learn more about Intel's AI software:\nIntel Neural Compressor link\nDisclaimer\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please consult an attorney before using this model for commercial purposes.\nCite\n@article{cheng2023optimize, title={Optimize weight rounding via signed gradient descent for the quantization of llms}, author={Cheng, Wenhua and Zhang, Weiwei and Shen, Haihao and Cai, Yiyang and He, Xin and Lv, Kaokao and Liu, Yi}, journal={arXiv preprint arXiv:2309.05516}, year={2023} }\narxiv github",
    "Jmica/IndexTTS-2-Japanese": "README.md exists but content is empty.",
    "zhangsq-nju/TernaryCLIP_ViT-B-16": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\n1. Overview of TernaryCLIP\n1.1 Training Strategy\n1.2 Model Performance\n2. How to Use TernaryCLIP\n3. Citation\nTernaryCLIP: Efficiently Compressing Vision-Language\nModels with Ternary Weights and Distilled Knowledge\n1. Overview of TernaryCLIP\n1.1 Training Strategy\nQuantization-Aware Training:\nW1.58-A16 using Ternary\nTernaryCLIP training codebase is in GitHub\nQuantize ~99% ternary weights\nKnowledge Distillation:\nTeacher model: CLIP_ViT-L-14_LAION\n1.2 Model Performance\nNOTE: TernaryCLIP refers to TernaryCLIP_Q-ALL unless specified otherwise.\n2. How to Use TernaryCLIP\nUsing transformers:\nfrom PIL import Image\nimport requests\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\n# TernaryCLIP Creation\nmodel = CLIPModel.from_pretrained(\"zhangsq-nju/TernaryCLIP_ViT-B-16\")\nprocessor = CLIPProcessor.from_pretrained(\"zhangsq-nju/TernaryCLIP_ViT-B-16\")\n# Inference\nurl = \"http://path.to.the.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw) # local image: image = Image.open(\"/path/to/image\")\nlabels = [\"a diagram\", \"a dog\", \"a cat\", \"Border Collie\", \"a white and black dog\"]\nground_truth_ind = 3\ninputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\ntext_probs = logits_per_image.softmax(dim=1)\nprint(\"Label probs:\", text_probs)\nprint(f\"Predicted labels: {labels[torch.argmax(text_probs).numpy().item()]} | Ground truth: {labels[ground_truth_ind]}\")\nUsing open_clip:\nimport open_clip\n# TernaryCLIP Creation\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:zhangsq-nju/TernaryCLIP_ViT-B-16')\ntokenizer = open_clip.get_tokenizer('hf-hub:zhangsq-nju/TernaryCLIP_ViT-B-16')\n# Inference\nimport torch\nfrom PIL import Image\nimage = preprocess_val(Image.open(\"/path/to/image.png\")).unsqueeze(0).to(torch.float32)\nlabels = [\"a diagram\", \"a dog\", \"a cat\", \"Border Collie\", \"a white and black dog\"]\nground_truth_ind = 3\ntext = tokenizer(labels)\nwith torch.no_grad(), torch.inference_mode():\nimage_features = model.encode_image(image)\ntext_features = model.encode_text(text)\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\ntext_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nprint(\"Label probs:\", text_probs)\nprint(f\"Predicted labels: {labels[torch.argmax(text_probs).numpy().item()]} | Ground truth: {labels[ground_truth_ind]}\")\nUsing clip.cpp: details are provided here\n3. Citation\n@article{zhangsh-ternaryclip,\ntitle={{TernaryCLIP}: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge},\nauthor={Shu-Hao Zhang and Wei-Cheng Tang and Chen Wu and Peng Hu and Nan Li and Liang-Jie Zhang and Qi Zhang and Shao-Qun Zhang},\nyear={2025},\njournal={arXiv preprint arXiv:2510.21879}\n}",
    "distilbert/distilbert-base-multilingual-cased": "Model Card for DistilBERT base multilingual (cased)\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use and Downstream Use\nOut of Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nHow to Get Started With the Model\nModel Card for DistilBERT base multilingual (cased)\nTable of Contents\nModel Details\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nHow To Get Started With the Model\nModel Details\nModel Description\nThis model is a distilled version of the BERT base multilingual model. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English.\nThe model is trained on the concatenation of Wikipedia in 104 different languages listed here.\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).\nOn average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.\nWe encourage potential users of this model to check out the BERT base multilingual model card to learn more about usage, limitations and potential biases.\nDeveloped by: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face)\nModel type: Transformer-based language model\nLanguage(s) (NLP): 104 languages; see full list here\nLicense: Apache 2.0\nRelated Models: BERT base multilingual model\nResources for more information:\nGitHub Repository\nAssociated Paper\nUses\nDirect Use and Downstream Use\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.\nOut of Scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. The model was not trained to be factual or true representations of people or events, and therefore using the models to generate such content is out-of-scope for the abilities of this model.\nBias, Risks, and Limitations\nSignificant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nTraining Details\nThe model was pretrained with the supervision of bert-base-multilingual-cased on the concatenation of Wikipedia in 104 different languages\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.\nFurther information about the training procedure and data is included in the bert-base-multilingual-cased model card.\nEvaluation\nThe model developers report the following accuracy results for DistilmBERT (see GitHub Repo):\nHere are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):\nModel\nEnglish\nSpanish\nChinese\nGerman\nArabic\nUrdu\nmBERT base cased (computed)\n82.1\n74.6\n69.1\n72.3\n66.4\n58.5\nmBERT base uncased (reported)\n81.4\n74.3\n63.8\n70.5\n62.1\n58.3\nDistilmBERT\n78.2\n69.1\n64.0\n66.3\n59.1\n54.7\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: More information needed\nHours used: More information needed\nCloud Provider: More information needed\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\n@article{Sanh2019DistilBERTAD,\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\nauthor={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\njournal={ArXiv},\nyear={2019},\nvolume={abs/1910.01108}\n}\nAPA\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\nHow to Get Started With the Model\nYou can use the model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'score': 0.040800247341394424,\n'sequence': \"Hello I'm a virtual model.\",\n'token': 37859,\n'token_str': 'virtual'},\n{'score': 0.020015988498926163,\n'sequence': \"Hello I'm a big model.\",\n'token': 22185,\n'token_str': 'big'},\n{'score': 0.018680453300476074,\n'sequence': \"Hello I'm a Hello model.\",\n'token': 31178,\n'token_str': 'Hello'},\n{'score': 0.017396586015820503,\n'sequence': \"Hello I'm a model model.\",\n'token': 13192,\n'token_str': 'model'},\n{'score': 0.014229810796678066,\n'sequence': \"Hello I'm a perfect model.\",\n'token': 43477,\n'token_str': 'perfect'}]",
    "FacebookAI/xlm-roberta-base": "XLM-RoBERTa (base-sized model)\nModel description\nIntended uses & limitations\nUsage\nBibTeX entry and citation info\nXLM-RoBERTa (base-sized model)\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository.\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\nIntended uses & limitations\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\nUsage\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n[{'score': 0.10563907772302628,\n'sequence': \"Hello I'm a fashion model.\",\n'token': 54543,\n'token_str': 'fashion'},\n{'score': 0.08015287667512894,\n'sequence': \"Hello I'm a new model.\",\n'token': 3525,\n'token_str': 'new'},\n{'score': 0.033413201570510864,\n'sequence': \"Hello I'm a model model.\",\n'token': 3299,\n'token_str': 'model'},\n{'score': 0.030217764899134636,\n'sequence': \"Hello I'm a French model.\",\n'token': 92265,\n'token_str': 'French'},\n{'score': 0.026436051353812218,\n'sequence': \"Hello I'm a sexy model.\",\n'token': 17473,\n'token_str': 'sexy'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n# forward pass\noutput = model(**encoded_input)\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1911-02116,\nauthor    = {Alexis Conneau and\nKartikay Khandelwal and\nNaman Goyal and\nVishrav Chaudhary and\nGuillaume Wenzek and\nFrancisco Guzm{\\'{a}}n and\nEdouard Grave and\nMyle Ott and\nLuke Zettlemoyer and\nVeselin Stoyanov},\ntitle     = {Unsupervised Cross-lingual Representation Learning at Scale},\njournal   = {CoRR},\nvolume    = {abs/1911.02116},\nyear      = {2019},\nurl       = {http://arxiv.org/abs/1911.02116},\neprinttype = {arXiv},\neprint    = {1911.02116},\ntimestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "FacebookAI/xlm-roberta-large": "XLM-RoBERTa (large-sized model)\nModel description\nIntended uses & limitations\nUsage\nBibTeX entry and citation info\nXLM-RoBERTa (large-sized model)\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository.\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\nIntended uses & limitations\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\nUsage\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n[{'score': 0.10563907772302628,\n'sequence': \"Hello I'm a fashion model.\",\n'token': 54543,\n'token_str': 'fashion'},\n{'score': 0.08015287667512894,\n'sequence': \"Hello I'm a new model.\",\n'token': 3525,\n'token_str': 'new'},\n{'score': 0.033413201570510864,\n'sequence': \"Hello I'm a model model.\",\n'token': 3299,\n'token_str': 'model'},\n{'score': 0.030217764899134636,\n'sequence': \"Hello I'm a French model.\",\n'token': 92265,\n'token_str': 'French'},\n{'score': 0.026436051353812218,\n'sequence': \"Hello I'm a sexy model.\",\n'token': 17473,\n'token_str': 'sexy'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n# forward pass\noutput = model(**encoded_input)\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1911-02116,\nauthor    = {Alexis Conneau and\nKartikay Khandelwal and\nNaman Goyal and\nVishrav Chaudhary and\nGuillaume Wenzek and\nFrancisco Guzm{\\'{a}}n and\nEdouard Grave and\nMyle Ott and\nLuke Zettlemoyer and\nVeselin Stoyanov},\ntitle     = {Unsupervised Cross-lingual Representation Learning at Scale},\njournal   = {CoRR},\nvolume    = {abs/1911.02116},\nyear      = {2019},\nurl       = {http://arxiv.org/abs/1911.02116},\neprinttype = {arXiv},\neprint    = {1911.02116},\ntimestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli": "DeBERTa-v3-base-mnli-fever-anli\nModel description\nHow to use the model\nTraining data\nTraining procedure\nEval results\nLimitations and bias\nCitation\nIdeas for cooperation or questions?\nDebugging and issues\nModel Recycling\nDeBERTa-v3-base-mnli-fever-anli\nModel description\nThis model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark.\nThe base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper.\nFor highest performance (but less speed), I recommend using https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli.\nHow to use the model\nSimple zero-shot classification pipeline\n#!pip install transformers[sentencepiece]\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\nsequence_to_classify = \"Angela Merkel is a politician in Germany and leader of the CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\nNLI use-case\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\npremise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\nhypothesis = \"The movie was good.\"\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\nTraining data\nDeBERTa-v3-base-mnli-fever-anli was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs.\nTraining procedure\nDeBERTa-v3-base-mnli-fever-anli was trained using the Hugging Face trainer with the following hyperparameters.\ntraining_args = TrainingArguments(\nnum_train_epochs=3,              # total number of training epochs\nlearning_rate=2e-05,\nper_device_train_batch_size=32,   # batch size per device during training\nper_device_eval_batch_size=32,    # batch size for evaluation\nwarmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\nweight_decay=0.06,               # strength of weight decay\nfp16=True                        # mixed precision training\n)\nEval results\nThe model was evaluated using the test sets for MultiNLI and ANLI and the dev set for Fever-NLI. The metric used is accuracy.\nmnli-m\nmnli-mm\nfever-nli\nanli-all\nanli-r3\n0.903\n0.903\n0.777\n0.579\n0.495\nLimitations and bias\nPlease consult the original DeBERTa paper and literature on different NLI datasets for potential biases.\nCitation\nIf you use this model, please cite: Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. â€˜Less Annotating, More Classifying â€“ Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLIâ€™. Preprint, June. Open Science Framework. https://osf.io/74b8k.\nIdeas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or LinkedIn\nDebugging and issues\nNote that DeBERTa-v3 was released on 06.12.21 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 might solve some issues.\nAlso make sure to install sentencepiece to avoid tokenizer errors. Run:  pip install transformers[sentencepiece] or pip install sentencepiece\nModel Recycling\nEvaluation on 36 datasets using MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli as a base model yields average score of 79.69 in comparison to 79.04 by microsoft/deberta-v3-base.\nThe model is ranked 2nd among all tested models for the microsoft/deberta-v3-base architecture as of 09/01/2023.\nResults:\n20_newsgroup\nag_news\namazon_reviews_multi\nanli\nboolq\ncb\ncola\ncopa\ndbpedia\nesnli\nfinancial_phrasebank\nimdb\nisear\nmnli\nmrpc\nmultirc\npoem_sentiment\nqnli\nqqp\nrotten_tomatoes\nrte\nsst2\nsst_5bins\nstsb\ntrec_coarse\ntrec_fine\ntweet_ev_emoji\ntweet_ev_emotion\ntweet_ev_hate\ntweet_ev_irony\ntweet_ev_offensive\ntweet_ev_sentiment\nwic\nwnli\nwsc\nyahoo_answers\n85.8072\n90.4333\n67.32\n59.625\n85.107\n91.0714\n85.8102\n67\n79.0333\n91.6327\n82.5\n94.02\n71.6428\n89.5749\n89.7059\n64.1708\n88.4615\n93.575\n91.4148\n89.6811\n86.2816\n94.6101\n57.0588\n91.5508\n97.6\n91.2\n45.264\n82.6179\n54.5455\n74.3622\n84.8837\n71.6949\n71.0031\n69.0141\n68.2692\n71.3333\nFor more information, see: Model Recycling",
    "cross-encoder/ms-marco-MiniLM-L6-v2": "Cross-Encoder for MS Marco\nUsage with SentenceTransformers\nUsage with Transformers\nPerformance\nCross-Encoder for MS Marco\nThis model was trained on the MS Marco Passage Ranking task.\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco\nUsage with SentenceTransformers\nThe usage is easy when you have SentenceTransformers installed. Then you can use the pre-trained models like this:\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n(\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n(\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.607138 -4.320078]\nUsage with Transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\nmodel.eval()\nwith torch.no_grad():\nscores = model(**features).logits\nprint(scores)\nPerformance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset.\nModel-Name\nNDCG@10 (TREC DL 19)\nMRR@10 (MS Marco Dev)\nDocs / Sec\nVersion 2 models\ncross-encoder/ms-marco-TinyBERT-L2-v2\n69.84\n32.56\n9000\ncross-encoder/ms-marco-MiniLM-L2-v2\n71.01\n34.85\n4100\ncross-encoder/ms-marco-MiniLM-L4-v2\n73.04\n37.70\n2500\ncross-encoder/ms-marco-MiniLM-L6-v2\n74.30\n39.01\n1800\ncross-encoder/ms-marco-MiniLM-L12-v2\n74.31\n39.02\n960\nVersion 1 models\ncross-encoder/ms-marco-TinyBERT-L2\n67.43\n30.15\n9000\ncross-encoder/ms-marco-TinyBERT-L4\n68.09\n34.50\n2900\ncross-encoder/ms-marco-TinyBERT-L6\n69.57\n36.13\n680\ncross-encoder/ms-marco-electra-base\n71.99\n36.41\n340\nOther models\nnboost/pt-tinybert-msmarco\n63.63\n28.80\n2900\nnboost/pt-bert-base-uncased-msmarco\n70.94\n34.75\n340\nnboost/pt-bert-large-msmarco\n73.36\n36.48\n100\nCapreolus/electra-base-msmarco\n71.23\n36.89\n340\namberoad/bert-multilingual-passage-reranking-msmarco\n68.40\n35.54\n330\nsebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco\n72.82\n37.88\n720\nNote: Runtime was computed on a V100 GPU.",
    "j-hartmann/emotion-english-distilroberta-base": "Emotion English DistilRoBERTa-base\nDescription â„¹\nApplication ğŸš€\nContact ğŸ’»\nReference âœ…\nAppendix ğŸ“š\nScientific Applications ğŸ“–\nEmotion English DistilRoBERTa-base\nDescription â„¹\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\nanger ğŸ¤¬\ndisgust ğŸ¤¢\nfear ğŸ˜¨\njoy ğŸ˜€\nneutral ğŸ˜\nsadness ğŸ˜­\nsurprise ğŸ˜²\nThe model is a fine-tuned checkpoint of DistilRoBERTa-base. For a 'non-distilled' emotion model, please refer to the model card of the RoBERTa-large version.\nApplication ğŸš€\na) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n{'label': 'disgust', 'score': 0.0016119900392368436},\n{'label': 'fear', 'score': 0.0004138521908316761},\n{'label': 'joy', 'score': 0.9771687984466553},\n{'label': 'neutral', 'score': 0.005764586851000786},\n{'label': 'sadness', 'score': 0.002092392183840275},\n{'label': 'surprise', 'score': 0.008528684265911579}]]\nb) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:\nContact ğŸ’»\nPlease reach out to jochen.hartmann@tum.de if you have any questions or feedback.\nThanks to Samuel Domdey and chrsiebert for their support in making this model available.\nReference âœ…\nFor attribution, please cite the following reference if you use this model. A working paper will be available soon.\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\nBibTex citation:\n@misc{hartmann2022emotionenglish,\nauthor={Hartmann, Jochen},\ntitle={Emotion English DistilRoBERTa-base},\nyear={2022},\nhowpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}\nAppendix ğŸ“š\nPlease find an overview of the datasets used for training below. All datasets contain English text. The table summarizes which emotions are available in each of the datasets. The datasets represent a diverse collection of text types. Specifically, they contain emotion labels for texts from Twitter, Reddit, student self-reports, and utterances from TV dialogues. As MELD (Multimodal EmotionLines Dataset) extends the popular EmotionLines dataset, EmotionLines itself is not included here.\nName\nanger\ndisgust\nfear\njoy\nneutral\nsadness\nsurprise\nCrowdflower (2016)\nYes\n-\n-\nYes\nYes\nYes\nYes\nEmotion Dataset, Elvis et al. (2018)\nYes\n-\nYes\nYes\n-\nYes\nYes\nGoEmotions, Demszky et al. (2020)\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nISEAR, Vikash (2018)\nYes\nYes\nYes\nYes\n-\nYes\n-\nMELD, Poria et al. (2019)\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nSemEval-2018, EI-reg, Mohammad et al. (2018)\nYes\n-\nYes\nYes\n-\nYes\n-\nThe model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\nScientific Applications ğŸ“–\nBelow you can find a list of papers using \"Emotion English DistilRoBERTa-base\". If you would like your paper to be added to the list, please send me an email.\nButt, S., Sharma, S., Sharma, R., Sidorov, G., & Gelbukh, A. (2022). What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses. Computers in Human Behavior, 107345.\nKuang, Z., Zong, S., Zhang, J., Chen, J., & Liu, H. (2022). Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. arXiv preprint arXiv:2210.00434.\nRozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models. Plos one, 17(10), e0276367.",
    "microsoft/trocr-base-handwritten": "TrOCR (base-sized model, fine-tuned on IAM)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nTrOCR (base-sized model, fine-tuned on IAM)\nTrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\nDisclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\nIntended uses & limitations\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model in PyTorch:\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n# load image from the IAM database\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\nBibTeX entry and citation info\n@misc{li2021trocr,\ntitle={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},\nauthor={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\nyear={2021},\neprint={2109.10282},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "nlpaueb/legal-bert-base-uncased": "LEGAL-BERT: The Muppets straight out of Law School\nPre-training corpora\nPre-training details\nModels list\nLoad Pretrained Model\nUse LEGAL-BERT variants as Language Models\nEvaluation on downstream tasks\nAuthor - Publication\nAbout Us\nLEGAL-BERT: The Muppets straight out of Law School\nLEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.\nI. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. \"LEGAL-BERT: The Muppets straight out of Law School\". In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) (Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261)\nPre-training corpora\nThe pre-training corpora of LEGAL-BERT include:\n116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.\n61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).\n19,867 cases from the European Court of Justice (ECJ), also available from EURLEX.\n12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng).\n164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law).\n76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml).\nPre-training details\nWe trained BERT using the official code provided in Google BERT's GitHub repository (https://github.com/google-research/bert).\nWe released a model similar to the English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters).\nWe chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.\nWe were able to use a single Google Cloud TPU v3-8 provided for free from TensorFlow Research Cloud (TFRC), while also utilizing GCP research credits. Huge thanks to both Google programs for supporting us!\nPart of LEGAL-BERT is a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\nModels list\nModel name\nModel Path\nTraining corpora\nCONTRACTS-BERT-BASE\nnlpaueb/bert-base-uncased-contracts\nUS contracts\nEURLEX-BERT-BASE\nnlpaueb/bert-base-uncased-eurlex\nEU legislation\nECHR-BERT-BASE\nnlpaueb/bert-base-uncased-echr\nECHR cases\nLEGAL-BERT-BASE *\nnlpaueb/legal-bert-base-uncased\nAll\nLEGAL-BERT-SMALL\nnlpaueb/legal-bert-small-uncased\nAll\n* LEGAL-BERT-BASE is the model referred to as LEGAL-BERT-SC in Chalkidis et al. (2020); a model trained from scratch in the legal corpora mentioned below using a newly created vocabulary by a sentence-piece tokenizer trained on the very same corpora.\n** As many of you expressed interest in the LEGAL-BERT-FP models (those relying on the original BERT-BASE checkpoint), they have been released in Archive.org (https://archive.org/details/legal_bert_fp), as these models are secondary and possibly only interesting for those who aim to dig deeper in the open questions of Chalkidis et al. (2020).\nLoad Pretrained Model\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\nUse LEGAL-BERT variants as Language Models\nCorpus\nModel\nMasked token\nPredictions\nBERT-BASE-UNCASED\n(Contracts)\nThis [MASK] Agreement is between General Motors and John Murray .\nemployment\n('new', '0.09'), ('current', '0.04'), ('proposed', '0.03'), ('marketing', '0.03'), ('joint', '0.02')\n(ECHR)\nThe applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate\ntorture\n('torture', '0.32'), ('rape', '0.22'), ('abuse', '0.14'), ('death', '0.04'), ('violence', '0.03')\n(EURLEX)\nEstablishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products .\nbovine\n('farm', '0.25'), ('livestock', '0.08'), ('draft', '0.06'), ('domestic', '0.05'), ('wild', '0.05')\nCONTRACTS-BERT-BASE\n(Contracts)\nThis [MASK] Agreement is between General Motors and John Murray .\nemployment\n('letter', '0.38'), ('dealer', '0.04'), ('employment', '0.03'), ('award', '0.03'), ('contribution', '0.02')\n(ECHR)\nThe applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate\ntorture\n('death', '0.39'), ('imprisonment', '0.07'), ('contempt', '0.05'), ('being', '0.03'), ('crime', '0.02')\n(EURLEX)\nEstablishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products .\nbovine\n(('domestic', '0.18'), ('laboratory', '0.07'), ('household', '0.06'), ('personal', '0.06'), ('the', '0.04')\nEURLEX-BERT-BASE\n(Contracts)\nThis [MASK] Agreement is between General Motors and John Murray .\nemployment\n('supply', '0.11'), ('cooperation', '0.08'), ('service', '0.07'), ('licence', '0.07'), ('distribution', '0.05')\n(ECHR)\nThe applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate\ntorture\n('torture', '0.66'), ('death', '0.07'), ('imprisonment', '0.07'), ('murder', '0.04'), ('rape', '0.02')\n(EURLEX)\nEstablishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products .\nbovine\n('live', '0.43'), ('pet', '0.28'), ('certain', '0.05'), ('fur', '0.03'), ('the', '0.02')\nECHR-BERT-BASE\n(Contracts)\nThis [MASK] Agreement is between General Motors and John Murray .\nemployment\n('second', '0.24'), ('latter', '0.10'), ('draft', '0.05'), ('bilateral', '0.05'), ('arbitration', '0.04')\n(ECHR)\nThe applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate\ntorture\n('torture', '0.99'), ('death', '0.01'), ('inhuman', '0.00'), ('beating', '0.00'), ('rape', '0.00')\n(EURLEX)\nEstablishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products .\nbovine\n('pet', '0.17'), ('all', '0.12'), ('slaughtered', '0.10'), ('domestic', '0.07'), ('individual', '0.05')\nLEGAL-BERT-BASE\n(Contracts)\nThis [MASK] Agreement is between General Motors and John Murray .\nemployment\n('settlement', '0.26'), ('letter', '0.23'), ('dealer', '0.04'), ('master', '0.02'), ('supplemental', '0.02')\n(ECHR)\nThe applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate\ntorture\n('torture', '1.00'), ('detention', '0.00'), ('arrest', '0.00'), ('rape', '0.00'), ('death', '0.00')\n(EURLEX)\nEstablishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products .\nbovine\n('live', '0.67'), ('beef', '0.17'), ('farm', '0.03'), ('pet', '0.02'), ('dairy', '0.01')\nLEGAL-BERT-SMALL\n(Contracts)\nThis [MASK] Agreement is between General Motors and John Murray .\nemployment\n('license', '0.09'), ('transition', '0.08'), ('settlement', '0.04'), ('consent', '0.03'), ('letter', '0.03')\n(ECHR)\nThe applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate\ntorture\n('torture', '0.59'), ('pain', '0.05'), ('ptsd', '0.05'), ('death', '0.02'), ('tuberculosis', '0.02')\n(EURLEX)\nEstablishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products .\nbovine\n('all', '0.08'), ('live', '0.07'), ('certain', '0.07'), ('the', '0.07'), ('farm', '0.05')\nEvaluation on downstream tasks\nConsider the experiments in the article \"LEGAL-BERT: The Muppets straight out of Law School\". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)\nAuthor - Publication\n@inproceedings{chalkidis-etal-2020-legal,\ntitle = \"{LEGAL}-{BERT}: The Muppets straight out of Law School\",\nauthor = \"Chalkidis, Ilias  and\nFergadiotis, Manos  and\nMalakasiotis, Prodromos  and\nAletras, Nikolaos  and\nAndroutsopoulos, Ion\",\nbooktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\ndoi = \"10.18653/v1/2020.findings-emnlp.261\",\npages = \"2898--2904\"\n}\nAbout Us\nAUEB's Natural Language Processing Group develops algorithms, models, and systems that allow computers to process and generate natural language texts.\nThe group's current research interests include:\nquestion answering systems for databases, ontologies, document collections, and the Web, especially biomedical question answering,\nnatural language generation from databases and ontologies, especially Semantic Web ontologies,\ntext classification, including filtering spam and abusive content,\ninformation extraction and opinion mining, including legal text analytics and sentiment analysis,\nnatural language processing tools for Greek, for example parsers and named-entity recognizers,\nmachine learning in natural language processing, especially deep learning.\nThe group is part of the Information Processing Laboratory of the Department of Informatics of the Athens University of Economics and Business.\nIlias Chalkidis on behalf of AUEB's Natural Language Processing Group\n| Github: @ilias.chalkidis | Twitter: @KiddoThe2B |",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nUsage (Text Embeddings Inference (TEI))\nFull Model Architecture\nCiting & Authors\nsentence-transformers/paraphrase-multilingual-mpnet-base-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] # First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nUsage (Text Embeddings Inference (TEI))\nText Embeddings Inference (TEI) is a blazing fast inference solution for text embedding models.\nCPU:\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest --model-id sentence-transformers/paraphrase-multilingual-mpnet-base-v2 --pooling mean --dtype float16\nNVIDIA GPU:\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest --model-id sentence-transformers/paraphrase-multilingual-mpnet-base-v2 --pooling mean --dtype float16\nSend a request to /v1/embeddings to generate embeddings via the OpenAI Embeddings API:\ncurl http://localhost:8080/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n\"input\": \"This is an example sentence\"\n}'\nOr check the Text Embeddings Inference API specification instead.\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nThis model was trained by sentence-transformers.\nIf you find this model helpful, feel free to cite our publication Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks:\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"http://arxiv.org/abs/1908.10084\",\n}",
    "openai/clip-vit-large-patch14-336": "clip-vit-large-patch14-336\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nclip-vit-large-patch14-336\nThis model was trained from scratch on an unknown dataset.\nIt achieves the following results on the evaluation set:\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\noptimizer: None\ntraining_precision: float32\nTraining results\nFramework versions\nTransformers 4.21.3\nTensorFlow 2.8.2\nTokenizers 0.12.1",
    "bigscience/bloom": "Model Details\nBasics\nTechnical Specifications\nModel Architecture and Objective\nCompute infrastructure\nTraining\nTraining Data\nLanguages\nPreprocessing\nSpeeds, Sizes, Times\nEnvironmental Impact\nUses\nHow to use\nIntended Use\nDirect Use\nDownstream Use\nMisuse and Out-of-scope Use\nIntended Users\nDirect Users\nIndirect Users\nOthers Affected (Parties Prenantes)\nRisks and Limitations\nEvaluation\nMetrics\nFactors\nResults\nRecommendations\nGlossary and Calculations\nMore Information\nIntermediate checkpoints\nDataset Creation\nTechnical Specifications\nLessons\nInitial Results\nOriginal checkpoints\nModel Card Authors\nBigScience Large Open-science Open-access Multilingual Language ModelVersion 1.3 / 6 July 2022\nCurrent Checkpoint: Training Iteration  95000\nLink to paper: here\nTotal seen tokens: 366B\nModel Details\nBLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.\nBasics\nThis section provides information about the model type, version, license, funders, release date, developers, and contact information.\nIt is useful for anyone who wants to reference the model.\nClick to expand\nDeveloped by: BigScience (website)\nAll collaborators are either volunteers or have an agreement with their employer. (Further breakdown of participants forthcoming.)\nModel Type: Transformer-based Language Model\nCheckpoints format: transformers (Megatron-DeepSpeed format available here)\nVersion: 1.0.0\nLanguages: Multiple; see training data\nLicense: RAIL License v1.0 (link / article and FAQ)\nRelease Date Estimate: Monday, 11.July.2022\nSend Questions to: bigscience-contact@googlegroups.com\nCite as: BigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022\nFunded by:\nThe French government.\nHugging Face (website).\nOrganizations of contributors.  (Further breakdown of organizations forthcoming.)\nTechnical Specifications\nThis section includes details about the model objective and architecture, and the compute infrastructure.\nIt is useful for people interested in model development.\nClick to expand\nPlease see the BLOOM training README for full details on replicating training.\nModel Architecture and Objective\nModified from Megatron-LM GPT2 (see paper, BLOOM Megatron code):\nDecoder-only architecture\nLayer normalization applied to word embeddings layer (StableEmbedding; see code, paper)\nALiBI positional encodings (see paper), with GeLU activation functions\n176,247,271,424 parameters:\n3,596,615,680 embedding parameters\n70 layers, 112 attention heads\nHidden layers are 14336-dimensional\nSequence length of 2048 tokens used (see BLOOM tokenizer, tokenizer description)\nObjective Function: Cross Entropy with mean reduction (see API documentation).\nCompute infrastructure\nJean Zay Public Supercomputer, provided by the French government (see announcement).\nHardware\n384 A100 80GB GPUs (48 nodes)\nAdditional 32 A100 80GB GPUs (4 nodes) in reserve\n8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links\nCPU: AMD\nCPU memory: 512GB per node\nGPU memory: 640GB per node\nInter-node connect: Omni-Path Architecture (OPA)\nNCCL-communications network: a fully dedicated subnet\nDisc IO network: shared network with other types of nodes\nSoftware\nMegatron-DeepSpeed (Github link)\nDeepSpeed (Github link)\nPyTorch (pytorch-1.11 w/ CUDA-11.5; see Github link)\napex (Github link)\nTraining\nThis section provides information about the training data, the speed and size of training elements, and the environmental impact of training.\nIt is useful for people who want to learn more about the model inputs and training footprint.\nClick to expand\nTraining Data\nThis section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.\nDetails for each dataset are provided in individual Data Cards, and the sizes of each of their contributions to the aggregated training data are presented in an Interactive Corpus Map.\nTraining data includes:\n46 natural languages\n13 programming languages\nIn 1.6TB of pre-processed text, converted into 350B unique tokens (see the tokenizer section for more.)\nLanguages\nThe pie chart shows the distribution of languages in training data.\nThe following tables shows the further distribution of Niger-Congo & Indic languages and programming languages in the training data.\nDistribution of Niger Congo and Indic languages.\nNiger Congo\nPercentage\nIndic\nPercentage\nChi Tumbuka\n0.00002\nAssamese\n0.01\nKikuyu\n0.00004\nOdia\n0.04\nBambara\n0.00004\nGujarati\n0.04\nAkan\n0.00007\nMarathi\n0.05\nXitsonga\n0.00007\nPunjabi\n0.05\nSesotho\n0.00007\nKannada\n0.06\nChi Chewa\n0.0001\nNepali\n0.07\nSetswana\n0.0002\nTelugu\n0.09\nLingala\n0.0002\nMalayalam\n0.10\nNorthern Sotho\n0.0002\nUrdu\n0.10\nFon\n0.0002\nTamil\n0.20\nKirundi\n0.0003\nBengali\n0.50\nWolof\n0.0004\nHindi\n0.70\nLuganda\n0.0004\nChi Shona\n0.001\nIsi Zulu\n0.001\nIgbo\n0.001\nXhosa\n0.001\nKinyarwanda\n0.003\nYoruba\n0.006\nSwahili\n0.02\nDistribution of programming languages.\nExtension\nLanguage\nNumber of files\njava\nJava\n5,407,724\nphp\nPHP\n4,942,186\ncpp\nC++\n2,503,930\npy\nPython\n2,435,072\njs\nJavaScript\n1,905,518\ncs\nC#\n1,577,347\nrb\nRuby\n6,78,413\ncc\nC++\n443,054\nhpp\nC++\n391,048\nlua\nLua\n352,317\ngo\nGO\n227,763\nts\nTypeScript\n195,254\nC\nC\n134,537\nscala\nScala\n92,052\nhh\nC++\n67,161\nH\nC++\n55,899\ntsx\nTypeScript\n33,107\nrs\nRust\n29,693\nphpt\nPHP\n9,702\nc++\nC++\n1,342\nh++\nC++\n791\nphp3\nPHP\n540\nphps\nPHP\n270\nphp5\nPHP\n166\nphp4\nPHP\n29\nPreprocessing\nTokenization: The BLOOM tokenizer (link), a learned subword tokenizer trained using:\nA byte-level Byte Pair Encoding (BPE) algorithm\nA simple pre-tokenization rule, no normalization\nA vocabulary size of 250,680\nIt was trained on a subset of a preliminary version of the corpus using alpha-weighting per language.\nSpeeds, Sizes, Times\nTraining logs: Tensorboard link\nDates:\nStarted 11th March, 2022 11:42am PST\nEstimated end: 5th July, 2022\nCheckpoint size:\nBf16 weights: 329GB\nFull checkpoint with optimizer states: 2.3TB\nTraining throughput: About 150 TFLOP per GPU per second\nNumber of epochs: 1\nEstimated cost of training: Equivalent of $2-5M in cloud computing (including preliminary experiments)\nServer training location: Ãle-de-France, France\nEnvironmental Impact\nThe training supercomputer, Jean Zay (website), uses mostly nuclear energy. The heat generated by it is reused for heating campus housing.\nEstimated carbon emissions: (Forthcoming.)\nEstimated electricity usage: (Forthcoming.)\nUses\nThis section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model.\nIt is useful for anyone considering using the model or who is affected by the model.\nClick to expand\nHow to use\nThis model can be easily used and deployed using HuggingFace's ecosystem. This needs transformers and accelerate installed. The model can be downloaded as follows:\nIntended Use\nThis model is being created in order to enable public research on large language models (LLMs). LLMs are intended to be used for language generation or as a pretrained base model that can be further fine-tuned for specific tasks. Use cases below are not exhaustive.\nDirect Use\nText generation\nExploring characteristics of language generated by a language model\nExamples: Cloze tests, counterfactuals, generations with reframings\nDownstream Use\nTasks that leverage language models include: Information Extraction, Question Answering, Summarization\nMisuse and Out-of-scope Use\nThis section addresses what users ought not do with the model.\nSee the BLOOM License, Attachment A, for detailed usage restrictions. The below list is non-exhaustive, but lists some easily foreseeable problematic use cases.\nOut-of-scope Uses\nUsing the model in high-stakes settings is out of scope for this model.  The model is not designed for critical decisions nor uses with any material consequences on an individual's livelihood or wellbeing. The model outputs content that appears factual but may not be correct.\nOut-of-scope Uses Include:\nUsage in biomedical domains, political and legal domains, or finance domains\nUsage for evaluating or scoring individuals, such as for employment, education, or credit\nApplying the model for critical automatic decisions, generating factual content, creating reliable summaries, or generating predictions that must be correct\nMisuse\nIntentionally using the model for harm, violating human rights, or other kinds of malicious activities, is a misuse of this model. This includes:\nSpam generation\nDisinformation and influence operations\nDisparagement and defamation\nHarassment and abuse\nDeception\nUnconsented impersonation and imitation\nUnconsented surveillance\nGenerating content without attribution to the model, as specified in the RAIL License, Use Restrictions\nIntended Users\nDirect Users\nGeneral Public\nResearchers\nStudents\nEducators\nEngineers/developers\nNon-commercial entities\nCommunity advocates, including human and civil rights groups\nIndirect Users\nUsers of derivatives created by Direct Users, such as those using software with an intended use\nUsers of Derivatives of the Model, as described in the License\nOthers Affected (Parties Prenantes)\nPeople and groups referred to by the LLM\nPeople and groups exposed to outputs of, or decisions based on, the LLM\nPeople and groups whose original work is included in the LLM\nRisks and Limitations\nThis section identifies foreseeable harms and misunderstandings.\nClick to expand\nModel may:\nOverrepresent some viewpoints and underrepresent others\nContain stereotypes\nContain personal information\nGenerate:\nHateful, abusive, or violent language\nDiscriminatory or prejudicial language\nContent that may not be appropriate for all settings, including sexual content\nMake errors, including producing incorrect information as if it were factual\nGenerate irrelevant or repetitive outputs\nInduce users into attributing human traits to it, such as sentience or consciousness\nEvaluation\nThis section describes the evaluation protocols and provides the results.\nClick to expand\nMetrics\nThis section describes the different ways performance is calculated and why.\nIncludes:\nMetric\nWhy chosen\nPerplexity\nStandard metric for quantifying model improvements during training\nCross Entropy Loss\nStandard objective for language models.\nAnd multiple different metrics for specific tasks. (More evaluation metrics forthcoming upon completion of evaluation protocol.)\nFactors\nThis section lists some different aspects of BLOOM models. Its focus is on aspects that are likely to give rise to high variance in model behavior.\nLanguage, such as English or Yoruba\nDomain, such as newswire or stories\nDemographic characteristics, such as gender or nationality\nResults\nResults are based on the Factors and Metrics.\nZero-shot evaluations:\nWARNING: This section used to contain much more results, however they were not correct and we released without the approval of the evaluation working group. We are currently in the process of fixing the evaluations.\nSee this repository for JSON files: https://github.com/bigscience-workshop/evaluation-results\nTask\nLanguage\nMetric\nBLOOM-176B\nOPT-175B*\nhumaneval\npython\npass@1 â†‘\n0.155\n0.0\nhumaneval\npython\npass@10 â†‘\n0.328\n0.0\nhumaneval\npython\npass@100 â†‘\n0.572\n0.003\nTrain-time Evaluation:\nFinal checkpoint after 95K steps:\nTraining Loss: 1.939\nValidation Loss: 2.061\nPerplexity: 7.045\nFor more see: https://huggingface.co/bigscience/tr11-176B-ml-logs\nRecommendations\nThis section provides information on warnings and potential mitigations.\nClick to expand\nIndirect users should be made aware when the content they're working with is created by the LLM.\nUsers should be aware of Risks and Limitations, and include an appropriate age disclaimer or blocking interface as necessary.\nModels trained or finetuned downstream of BLOOM LM should include an updated Model Card.\nUsers of the model should provide mechanisms for those affected to provide feedback, such as an email address for comments.\nGlossary and Calculations\nThis section defines common terms and how metrics are calculated.\nClick to expand\nLoss: A calculation of the difference between what the model has learned and what the data shows (\"groundtruth\"). The lower the loss, the better. The training process aims to minimize the loss.\nPerplexity: This is based on what the model estimates the probability of new data is. The lower the perplexity, the better.  If the model is 100% correct at predicting the next token it will see, then the perplexity is 1. Mathematically this is calculated using entropy.\nHigh-stakes settings: Such as those identified as \"high-risk AI systems\" and \"unacceptable risk AI systems\" in the European Union's proposed Artificial Intelligence (AI) Act.\nCritical decisions: Such as those defined in the United States' proposed Algorithmic Accountability Act.\nHuman rights: Includes those rights defined in the Universal Declaration of Human Rights.\nPersonal Data and Personal Information: Personal data and information is defined in multiple data protection regulations, such as \"personal data\" in the European Union's General Data Protection Regulation; and \"personal information\" in the Republic of South Africa's Protection of Personal Information Act, The People's Republic of China's Personal information protection law.\nSensitive characteristics: This includes specifically protected categories in human rights (see UHDR, Article 2) and personal information regulation (see GDPR, Article 9; Protection of Personal Information Act, Chapter 1)\nDeception: Doing something to intentionally mislead individuals to believe something that is false, such as by creating deadbots or chatbots on social media posing as real people, or generating text documents without making consumers aware that the text is machine generated.\nMore Information\nThis section provides links to writing on dataset creation, technical specifications, lessons learned, and initial results.\nClick to expand\nIntermediate checkpoints\nFor academic (or any) usage, we published the intermediate checkpoints, corresponding to the model state at each 5000 steps. Please follow this link to get these checkpoints.\nDataset Creation\nBlog post detailing the design choices during the dataset creation: https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling\nTechnical Specifications\nBlog post summarizing how the architecture, size, shape, and pre-training duration where selected: https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours\nMore details on the architecture/optimizer: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\nBlog post on the hardware/engineering side: https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model\nDetails on the distributed setup used for the training: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\nTensorboard updated during the training: https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss\nLessons\nInsights on how to approach training, negative results: https://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.md\nDetails on the obstacles overcome during the preparation on the engineering side (instabilities, optimization of training throughput, so many technical tricks and questions): https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md\nInitial Results\nInitial prompting experiments using interim checkpoints: https://huggingface.co/spaces/bigscience/bloom-book\nOriginal checkpoints\nThe checkpoints in this repo correspond to the HuggingFace Transformers format. If you want to use our fork of Megatron-DeepSpeed that the model was trained with, you'd want to use this repo instead.\nMany intermediate checkpoints are available at https://huggingface.co/bigscience/bloom-intermediate/\nModel Card Authors\nOrdered roughly chronologically and by amount of time spent on creating this model card.\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",
    "facebook/nllb-200-distilled-600M": "NLLB-200\nIntended Use\nMetrics\nEvaluation Data\nTraining Data\nEthical Considerations\nCaveats and Recommendations\nCarbon Footprint Details\nNLLB-200\nThis is the model card of NLLB-200's distilled 600M variant.\nHere are the metrics for that particular checkpoint.\nInformation about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.\nPaper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022\nLicense: CC-BY-NC\nWhere to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues\nIntended Use\nPrimary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\nPrimary intended users: Primary users are researchers and machine translation research community.\nOut-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.\nMetrics\nâ€¢ Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.\nEvaluation Data\nDatasets: Flores-200 dataset is described in Section 4\nMotivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\nPreprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.\nTraining Data\nâ€¢ We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.\nEthical Considerations\nâ€¢ In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).\nCaveats and Recommendations\nâ€¢ Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.\nCarbon Footprint Details\nâ€¢ The carbon dioxide (CO2e) estimate is reported in Section 8.8.",
    "laion/CLIP-ViT-B-32-laion2B-s34B-b79K": "Model Card for CLIP ViT-B/32 - LAION-2B\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nResults\nAcknowledgements\nCitation\nHow to Get Started with the Model\nModel Card for CLIP ViT-B/32 - LAION-2B\nTable of Contents\nModel Details\nUses\nTraining Details\nEvaluation\nAcknowledgements\nCitation\nHow To Get Started With the Model\nModel Details\nModel Description\nA CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).\nModel training done by Romain Beaumont on the stability.ai cluster.\nUses\nAs per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model.\nThe OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset.\nDirect Use\nZero-shot image classification, image and text retrieval, among others.\nDownstream Use\nImage classification and other image task fine-tuning, linear probe image classification, image generation guiding and conditioning, among others.\nOut-of-Scope Use\nAs per the OpenAI models,\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPâ€™s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\nFurther the above notice, the LAION-5B dataset used in training of these models has additional considerations, see below.\nTraining Details\nTraining Data\nThis model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/).\nIMPORTANT NOTE: The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a â€œsafeâ€ subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.\nTraining Procedure\nPlease see training notes and wandb logs.\nEvaluation\nEvaluation done with code in the LAION CLIP Benchmark suite.\nTesting Data, Factors & Metrics\nTesting Data\nThe testing is performed with VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets) for classification and COCO and Flickr for retrieval.\nTODO - more detail\nResults\nThe model achieves a 66.6 zero-shot top-1 accuracy on ImageNet-1k.\nAn initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb\nTODO - create table for just this model's metrics.\nAcknowledgements\nAcknowledging stability.ai for the compute used to train this model.\nCitation\nBibTeX:\nIn addition to forthcoming LAION-5B (https://laion.ai/blog/laion-5b/) paper, please cite:\nOpenAI CLIP paper\n@inproceedings{Radford2021LearningTV,\ntitle={Learning Transferable Visual Models From Natural Language Supervision},\nauthor={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\nbooktitle={ICML},\nyear={2021}\n}\nOpenCLIP software\n@software{ilharco_gabriel_2021_5143773,\nauthor       = {Ilharco, Gabriel and\nWortsman, Mitchell and\nWightman, Ross and\nGordon, Cade and\nCarlini, Nicholas and\nTaori, Rohan and\nDave, Achal and\nShankar, Vaishaal and\nNamkoong, Hongseok and\nMiller, John and\nHajishirzi, Hannaneh and\nFarhadi, Ali and\nSchmidt, Ludwig},\ntitle        = {OpenCLIP},\nmonth        = jul,\nyear         = 2021,\nnote         = {If you use this software, please cite it as below.},\npublisher    = {Zenodo},\nversion      = {0.1},\ndoi          = {10.5281/zenodo.5143773},\nurl          = {https://doi.org/10.5281/zenodo.5143773}\n}\nHow to Get Started with the Model\nUse the code below to get started with the model.\n** TODO ** - Hugging Face transformers, OpenCLIP, and timm getting started snippets",
    "google/flan-t5-small": "Model Card for FLAN-T5 small\nTable of Contents\nTL;DR\nModel Details\nModel Description\nUsage\nUsing the Pytorch model\nRunning the model on a CPU\nRunning the model on a GPU\nRunning the model on a GPU using different precisions\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nEthical considerations and risks\nKnown Limitations\nSensitive Use:\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card for FLAN-T5 small\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nTL;DR\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.\nAs mentioned in the first few lines of the abstract :\nFlan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\nDisclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\nLicense: Apache 2.0\nRelated Models: All FLAN-T5 Checkpoints\nOriginal Checkpoints: All Original FLAN-T5 Checkpoints\nResources for more information:\nResearch paper\nGitHub Repo\nHugging Face FLAN-T5 Docs (Similar to T5)\nUsage\nFind below some example scripts on how to use the model in transformers:\nUsing the Pytorch model\nRunning the model on a CPU\nClick to expand\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU\nClick to expand\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nFP16\nClick to expand\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\", torch_dtype=torch.float16)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nINT8\nClick to expand\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\", load_in_8bit=True)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nUses\nDirect Use and Downstream Use\nThe authors write in the original paper's model card that:\nThe primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\nSee the research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nThe information below in this section are copied from the model's official model card:\nLanguage models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\nEthical considerations and risks\nFlan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\nKnown Limitations\nFlan-T5 has not been tested in real world applications.\nSensitive Use:\nFlan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\nTraining Details\nTraining Data\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\nTraining Procedure\nAccording to the model card from the original paper:\nThese models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\nThe model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax.\nEvaluation\nTesting Data, Factors & Metrics\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\nFor full details, please check the research paper.\nResults\nFor full results for FLAN-T5-Small, see the research paper, Table 3.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips â‰¥ 4.\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\ndoi = {10.48550/ARXIV.2210.11416},\nurl = {https://arxiv.org/abs/2210.11416},\nauthor = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\nkeywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {Scaling Instruction-Finetuned Language Models},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}",
    "google/flan-t5-xxl": "Model Card for FLAN-T5 XXL\nTable of Contents\nTL;DR\nModel Details\nModel Description\nUsage\nUsing the Pytorch model\nRunning the model on a CPU\nRunning the model on a GPU\nRunning the model on a GPU using different precisions\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nEthical considerations and risks\nKnown Limitations\nSensitive Use:\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card for FLAN-T5 XXL\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nTL;DR\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.\nAs mentioned in the first few lines of the abstract :\nFlan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\nDisclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): English, German, French\nLicense: Apache 2.0\nRelated Models: All FLAN-T5 Checkpoints\nOriginal Checkpoints: All Original FLAN-T5 Checkpoints\nResources for more information:\nResearch paper\nGitHub Repo\nHugging Face FLAN-T5 Docs (Similar to T5)\nUsage\nFind below some example scripts on how to use the model in transformers:\nUsing the Pytorch model\nRunning the model on a CPU\nClick to expand\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU\nClick to expand\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nFP16\nClick to expand\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", torch_dtype=torch.float16)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nINT8\nClick to expand\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", load_in_8bit=True)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nUses\nDirect Use and Downstream Use\nThe authors write in the original paper's model card that:\nThe primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\nSee the research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nThe information below in this section are copied from the model's official model card:\nLanguage models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\nEthical considerations and risks\nFlan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\nKnown Limitations\nFlan-T5 has not been tested in real world applications.\nSensitive Use:\nFlan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\nTraining Details\nTraining Data\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\nTraining Procedure\nAccording to the model card from the original paper:\nThese models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\nThe model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax.\nEvaluation\nTesting Data, Factors & Metrics\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\nFor full details, please check the research paper.\nResults\nFor full results for FLAN-T5-XXL, see the research paper, Table 3.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips â‰¥ 4.\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\ndoi = {10.48550/ARXIV.2210.11416},\nurl = {https://arxiv.org/abs/2210.11416},\nauthor = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\nkeywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {Scaling Instruction-Finetuned Language Models},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}"
}