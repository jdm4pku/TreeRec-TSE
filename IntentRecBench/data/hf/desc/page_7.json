{
    "black-forest-labs/FLUX.1-Krea-dev": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the FluxDev Non-Commercial License Agreement and acknowledge the Acceptable Use Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nKey Features\nUsage\nComfyUI\nüß® diffusers\nLimitations\nOut-of-Scope Use\nRisks\nLicense\nFLUX.1 Krea [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.\nFor more information, please read our blog post and Krea's blog post.\nKey Features\nCutting-edge output quality, with a focus on aesthetic photography.\nCompetitive prompt following, matching the performance of closed source alternatives.\nTrained using guidance distillation, making FLUX.1 Krea [dev] more efficient.\nOpen weights to drive new scientific research, and empower artists to develop innovative workflows.\nGenerated outputs can be used for personal, scientific, and commercial purposes, as described in the flux-1-dev-non-commercial-license.\nUsage\nFLUX.1 Krea [dev] can be used as a drop-in replacement in every system that supports the original FLUX.1 [dev].\nA reference implementation of FLUX.1 [dev] is in our dedicated github repository.\nDevelopers and creatives looking to build on top of FLUX.1 [dev] are encouraged to use this as a starting point.\nFLUX.1 Krea [dev] is also available in both ComfyUI and diffusers.\nComfyUI\nTo use FLUX.1 Krea [dev] in Comfy UI download the *.safetensors weights here\nüß® diffusers\nTo use FLUX.1 Krea [dev] in diffusers, first install or upgrade diffusers\npip install -U diffusers\nThen you can use FluxPipeline to run the model\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Krea-dev\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU VRAM\nprompt = \"A frog holding a sign that says hello world\"\nimage = pipe(\nprompt,\nheight=1024,\nwidth=1024,\nguidance_scale=4.5,\n).images[0]\nimage.save(\"flux-krea-dev.png\")\nTo learn more check out the diffusers documentation\nLimitations\nThis model is not intended or able to provide factual information.\nAs a statistical model this checkpoint might amplify existing societal biases.\nThe model may fail to generate output that matches the prompts.\nPrompt following is heavily influenced by the prompting-style.\nOut-of-Scope Use\nThe model and its derivatives may not be used\nIn any way that violates any applicable national, federal, state, local or international law or regulation.\nFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\nTo generate or disseminate verifiably false information and/or content with the purpose of harming others.\nTo generate or disseminate personal identifiable information that can be used to harm an individual.\nTo harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\nTo create non-consensual nudity or illegal pornographic content.\nFor fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\nGenerating or facilitating large-scale disinformation campaigns.\nPlease reference our content filters to avoid such generations.\nRisks\nBlack Forest Labs (BFL) and Krea are committed to the responsible development of generative AI technology. Prior to releasing FLUX.1 Krea [dev], BFL and Krea collaboratively evaluated and mitigated a number of risks in the FLUX.1 Krea [dev]  model and services, including the generation of unlawful content. We implemented a series of pre-release mitigations to help prevent misuse by third parties, with additional post-release mitigations to help address residual risks:\nPre-training mitigation. BFL filtered pre-training data for multiple categories of ‚Äúnot safe for work‚Äù (NSFW) and unlawful content to help prevent a user generating unlawful content in response to text prompts or uploaded images.\nPost-training mitigation. BFL has partnered with the Internet Watch Foundation, an independent nonprofit organization dedicated to preventing online abuse, to filter known child sexual abuse material (CSAM) from post-training data. Subsequently, BFL and Krea undertook multiple rounds of targeted fine-tuning to provide additional mitigation against potential abuse. By inhibiting certain behaviors and concepts in the trained model, these techniques can help to prevent a user generating synthetic CSAM or nonconsensual intimate imagery (NCII) from a text prompt.\nPre-release evaluation. Throughout this process, BFL conducted internal and external third-party evaluations of model checkpoints to identify further opportunities for improvement. The third-party evaluations focused on eliciting CSAM and NCII through adversarial testing of the text-to-image model with text-only prompts. We also conducted internal evaluations of the proposed release checkpoints, comparing the model with other leading openly-available generative image models from other companies. The final FLUX.1 Krea [dev] open-weight model checkpoint demonstrated very high resilience against violative inputs, demonstrating higher resilience than other similar open-weight models across these risk categories.  Based on these findings, we approved the release of the FLUX.1 Krea [dev] model as openly-available weights under a non-commercial license to support third-party research and development.\nInference filters. The BFL Github repository for the open FLUX.1 Krea [dev] model includes filters for illegal or infringing content. Filters or manual review must be used with the model under the terms of the FLUX.1 [dev] Non-Commercial License. We may approach known deployers of the FLUX.1 Krea [dev] model at random to verify that filters or manual review processes are in place.\nPolicies. Our FLUX.1 [dev] Non-Commercial License prohibits the generation of unlawful content or the use of generated content for unlawful, defamatory, or abusive purposes. Developers and users must consent to these conditions to access the FLUX.1 Krea [dev] model.\nMonitoring. BFL is monitoring for patterns of violative use after release, and may ban developers who we detect intentionally and repeatedly violate our policies. Additionally, BFL provides a dedicated email address (safety@blackforestlabs.ai) to solicit feedback from the community. BFL maintains a reporting relationship with organizations such as the Internet Watch Foundation and the National Center for Missing and Exploited Children, and BFL welcomes ongoing engagement with authorities, developers, and researchers to share intelligence about emerging risks and develop effective mitigations.\nLicense\nThis model falls under the FLUX.1 [dev] Non-Commercial License.",
    "zai-org/GLM-4.5": "GLM-4.5\nModel Introduction\nModel Downloads\nSystem Requirements\nInference\nFine-tuning\nQuick Start\ntransformers\nvLLM\nSGLang\nRequest Parameter Instructions\nGLM-4.5\nüëã Join our Discord community.\nüìñ Check out the GLM-4.5 technical blog, technical report, and Zhipu AI technical documentation.\nüìç Use GLM-4.5 API services on Z.ai API Platform (Global) or  Zhipu AI Open Platform (Mainland China).\nüëâ One click to GLM-4.5.\nModel Introduction\nThe GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of 63.2, in the 3rd place among all the proprietary and open-source models. Notably, GLM-4.5-Air delivers competitive results at 59.8 while maintaining superior efficiency.\nFor more eval results, show cases, and technical details, please visit\nour technical blog or technical report.\nThe model code, tool parser and reasoning parser can be found in the implementation of transformers, vLLM and SGLang.\nModel Downloads\nYou can directly experience the model on Hugging Face\nor ModelScope or download the model by following the links below.\nModel\nDownload Links\nModel Size\nPrecision\nGLM-4.5\nü§ó Hugging Face ü§ñ ModelScope\n355B-A32B\nBF16\nGLM-4.5-Air\nü§ó Hugging Face ü§ñ ModelScope\n106B-A12B\nBF16\nGLM-4.5-FP8\nü§ó Hugging Face ü§ñ ModelScope\n355B-A32B\nFP8\nGLM-4.5-Air-FP8\nü§ó Hugging Face ü§ñ ModelScope\n106B-A12B\nFP8\nGLM-4.5-Base\nü§ó Hugging Face ü§ñ ModelScope\n355B-A32B\nBF16\nGLM-4.5-Air-Base\nü§ó Hugging Face ü§ñ ModelScope\n106B-A12B\nBF16\nSystem Requirements\nInference\nWe provide minimum and recommended configurations for \"full-featured\" model inference. The data in the table below is\nbased on the following conditions:\nAll models use MTP layers and specify\n--speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 to ensure competitive\ninference speed.\nThe cpu-offload parameter is not used.\nInference batch size does not exceed 8.\nAll are executed on devices that natively support FP8 inference, ensuring both weights and cache are in FP8 format.\nServer memory must exceed 1T to ensure normal model loading and operation.\nThe models can run under the configurations in the table below:\nModel\nPrecision\nGPU Type and Count\nTest Framework\nGLM-4.5\nBF16\nH100 x 16 / H200 x 8\nsglang\nGLM-4.5\nFP8\nH100 x 8 / H200 x 4\nsglang\nGLM-4.5-Air\nBF16\nH100 x 4 / H200 x 2\nsglang\nGLM-4.5-Air\nFP8\nH100 x 2 / H200 x 1\nsglang\nUnder the configurations in the table below, the models can utilize their full 128K context length:\nModel\nPrecision\nGPU Type and Count\nTest Framework\nGLM-4.5\nBF16\nH100 x 32 / H200 x 16\nsglang\nGLM-4.5\nFP8\nH100 x 16 / H200 x 8\nsglang\nGLM-4.5-Air\nBF16\nH100 x 8 / H200 x 4\nsglang\nGLM-4.5-Air\nFP8\nH100 x 4 / H200 x 2\nsglang\nFine-tuning\nThe code can run under the configurations in the table below\nusing Llama Factory:\nModel\nGPU Type and Count\nStrategy\nBatch Size (per GPU)\nGLM-4.5\nH100 x 16\nLora\n1\nGLM-4.5-Air\nH100 x 4\nLora\n1\nThe code can run under the configurations in the table below using Swift:\nModel\nGPU Type and Count\nStrategy\nBatch Size (per GPU)\nGLM-4.5\nH20 (96GiB) x 16\nLora\n1\nGLM-4.5-Air\nH20 (96GiB) x 4\nLora\n1\nGLM-4.5\nH20 (96GiB) x 128\nSFT\n1\nGLM-4.5-Air\nH20 (96GiB) x 32\nSFT\n1\nGLM-4.5\nH20 (96GiB) x 128\nRL\n1\nGLM-4.5-Air\nH20 (96GiB) x 32\nRL\n1\nQuick Start\nPlease install the required packages according to requirements.txt.\npip install -r requirements.txt\ntransformers\nPlease refer to the trans_infer_cli.py code in the inference folder.\nvLLM\nBoth BF16 and FP8 can be started with the following code:\nvllm serve zai-org/GLM-4.5-Air \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--reasoning-parser glm45 \\\n--enable-auto-tool-choice \\\n--served-model-name glm-4.5-air\nIf you're using 8x H100 GPUs and encounter insufficient memory when running the GLM-4.5 model, you'll need\n--cpu-offload-gb 16 (only applicable to vLLM).\nIf you encounter flash infer issues, use VLLM_ATTENTION_BACKEND=XFORMERS as a temporary replacement. You can also\nspecify TORCH_CUDA_ARCH_LIST='9.0+PTX' to use flash infer (different GPUs have different TORCH_CUDA_ARCH_LIST\nvalues, please check accordingly).\nSGLang\nBF16\npython3 -m sglang.launch_server \\\n--model-path zai-org/GLM-4.5-Air \\\n--tp-size 8 \\\n--tool-call-parser glm45  \\\n--reasoning-parser glm45 \\\n--speculative-algorithm EAGLE \\\n--speculative-num-steps 3 \\\n--speculative-eagle-topk 1 \\\n--speculative-num-draft-tokens 4 \\\n--mem-fraction-static 0.7 \\\n--served-model-name glm-4.5-air \\\n--host 0.0.0.0 \\\n--port 8000\nFP8\npython3 -m sglang.launch_server \\\n--model-path zai-org/GLM-4.5-Air-FP8 \\\n--tp-size 4 \\\n--tool-call-parser glm45  \\\n--reasoning-parser glm45  \\\n--speculative-algorithm EAGLE \\\n--speculative-num-steps 3  \\\n--speculative-eagle-topk 1  \\\n--speculative-num-draft-tokens 4 \\\n--mem-fraction-static 0.7 \\\n--disable-shared-experts-fusion \\\n--served-model-name glm-4.5-air-fp8 \\\n--host 0.0.0.0 \\\n--port 8000\nRequest Parameter Instructions\nWhen using vLLM and SGLang, thinking mode is enabled by default when sending requests. If you want to disable the\nthinking switch, you need to add the extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}} parameter.\nBoth support tool calling. Please use OpenAI-style tool description format for calls.\nFor specific code, please refer to api_request.py in the inference folder.",
    "Qwen/Qwen3-30B-A3B-Instruct-2507": "Qwen3-30B-A3B-Instruct-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nProcessing Ultra-Long Texts\nHow to Enable 1M Token Context\nBest Practices\nCitation\nQwen3-30B-A3B-Instruct-2507\nHighlights\nWe introduce the updated version of the Qwen3-30B-A3B non-thinking mode, named Qwen3-30B-A3B-Instruct-2507, featuring the following key enhancements:\nSignificant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.\nSubstantial gains in long-tail knowledge coverage across multiple languages.\nMarkedly better alignment with user preferences in subjective and open-ended tasks, enabling more helpful responses and higher-quality text generation.\nEnhanced capabilities in 256K long-context understanding.\nModel Overview\nQwen3-30B-A3B-Instruct-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Paramaters (Non-Embedding): 29.9B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-V3-0324\nGPT-4o-0327\nGemini-2.5-Flash Non-Thinking\nQwen3-235B-A22B Non-Thinking\nQwen3-30B-A3B Non-Thinking\nQwen3-30B-A3B-Instruct-2507\nKnowledge\nMMLU-Pro\n81.2\n79.8\n81.1\n75.2\n69.1\n78.4\nMMLU-Redux\n90.4\n91.3\n90.6\n89.2\n84.1\n89.3\nGPQA\n68.4\n66.9\n78.3\n62.9\n54.8\n70.4\nSuperGPQA\n57.3\n51.0\n54.6\n48.2\n42.2\n53.4\nReasoning\nAIME25\n46.6\n26.7\n61.6\n24.7\n21.6\n61.3\nHMMT25\n27.5\n7.9\n45.8\n10.0\n12.0\n43.0\nZebraLogic\n83.4\n52.6\n57.9\n37.7\n33.2\n90.0\nLiveBench 20241125\n66.9\n63.7\n69.1\n62.5\n59.4\n69.0\nCoding\nLiveCodeBench v6 (25.02-25.05)\n45.2\n35.8\n40.1\n32.9\n29.0\n43.2\nMultiPL-E\n82.2\n82.7\n77.7\n79.3\n74.6\n83.8\nAider-Polyglot\n55.1\n45.3\n44.0\n59.6\n24.4\n35.6\nAlignment\nIFEval\n82.3\n83.9\n84.3\n83.2\n83.7\n84.7\nArena-Hard v2*\n45.6\n61.9\n58.3\n52.0\n24.8\n69.0\nCreative Writing v3\n81.6\n84.9\n84.6\n80.4\n68.1\n86.0\nWritingBench\n74.5\n75.5\n80.5\n77.0\n72.2\n85.5\nAgent\nBFCL-v3\n64.7\n66.5\n66.1\n68.0\n58.6\n65.1\nTAU1-Retail\n49.6\n60.3#\n65.2\n65.2\n38.3\n59.1\nTAU1-Airline\n32.0\n42.8#\n48.0\n32.0\n18.0\n40.0\nTAU2-Retail\n71.1\n66.7#\n64.3\n64.9\n31.6\n57.0\nTAU2-Airline\n36.0\n42.0#\n42.5\n36.0\n18.0\n38.0\nTAU2-Telecom\n34.0\n29.8#\n16.9\n24.6\n18.4\n12.3\nMultilingualism\nMultiIF\n66.5\n70.4\n69.4\n70.2\n70.8\n67.9\nMMLU-ProX\n75.8\n76.2\n78.3\n73.2\n65.1\n72.0\nINCLUDE\n80.1\n82.1\n83.8\n75.6\n67.8\n71.9\nPolyMATH\n32.2\n25.5\n41.9\n27.0\n23.3\n43.1\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n#: Results were generated using GPT-4o-20241120, as access to the native function calling API of GPT-4o-0327 was unavailable.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --context-length 262144\nvLLM:vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --max-model-len 262144\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-30B-A3B-Instruct-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nTo support ultra-long context processing (up to 1 million tokens), we integrate two key techniques:\nDual Chunk Attention (DCA): A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence.\nMInference: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions.\nTogether, these innovations significantly improve both generation quality and inference efficiency for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a 3√ó speedup compared to standard attention implementations.\nFor full technical details, see the Qwen2.5-1M Technical Report.\nHow to Enable 1M Token Context\nTo effectively process a 1 million token context, users will require approximately 240 GB of total GPU memory. This accounts for model weights, KV-cache storage, and peak activation memory demands.\nStep 1: Update Configuration File\nDownload the model and replace the content of your config.json with config_1m.json, which includes the config for length extrapolation and sparse attention.\nexport MODELNAME=Qwen3-30B-A3B-Instruct-2507\nhuggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME}\nmv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak\nmv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json\nStep 2: Launch Model Server\nAfter updating the config, proceed with either vLLM or SGLang for serving the model.\nOption 1: Using vLLM\nTo run Qwen with 1M context support:\npip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly\nThen launch the server with Dual Chunk Flash Attention enabled:\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\\nvllm serve ./Qwen3-30B-A3B-Instruct-2507 \\\n--tensor-parallel-size 4 \\\n--max-model-len 1010000 \\\n--enable-chunked-prefill \\\n--max-num-batched-tokens 131072 \\\n--enforce-eager \\\n--max-num-seqs 1 \\\n--gpu-memory-utilization 0.85\nKey Parameters\nParameter\nPurpose\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN\nEnables the custom attention kernel for long-context efficiency\n--max-model-len 1010000\nSets maximum context length to ~1M tokens\n--enable-chunked-prefill\nAllows chunked prefill for very long inputs (avoids OOM)\n--max-num-batched-tokens 131072\nControls batch size during prefill; balances throughput and memory\n--enforce-eager\nDisables CUDA graph capture (required for dual chunk attention)\n--max-num-seqs 1\nLimits concurrent sequences due to extreme memory usage\n--gpu-memory-utilization 0.85\nSet the fraction of GPU memory to be used for the model executor\nOption 2: Using SGLang\nFirst, clone and install the specialized branch:\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\npip install -e \"python[all]\"\nLaunch the server with DCA support:\npython3 -m sglang.launch_server \\\n--model-path ./Qwen3-30B-A3B-Instruct-2507 \\\n--context-length 1010000 \\\n--mem-frac 0.75 \\\n--attention-backend dual_chunk_flash_attn \\\n--tp 4 \\\n--chunked-prefill-size 131072\nKey Parameters\nParameter\nPurpose\n--attention-backend dual_chunk_flash_attn\nActivates Dual Chunk Flash Attention\n--context-length 1010000\nDefines max input length\n--mem-frac 0.75\nThe fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors.\n--tp 4\nTensor parallelism size (matches model sharding)\n--chunked-prefill-size 131072\nPrefill chunk size for handling long inputs without OOM\nTroubleshooting:\nEncountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeError: Not enough memory. Please try to increase --mem-fraction-static.\"\nThe VRAM reserved for the KV cache is insufficient.\nvLLM: Consider reducing the max_model_len or increasing the tensor_parallel_size and gpu_memory_utilization. Alternatively, you can reduce max_num_batched_tokens, although this may significantly slow down inference.\nSGLang: Consider reducing the context-length or increasing the tp and mem-frac. Alternatively, you can reduce chunked-prefill-size, although this may significantly slow down inference.\nEncountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\nThe VRAM reserved for activation weights is insufficient. You can try lowering gpu_memory_utilization or mem-frac, but be aware that this might reduce the VRAM available for the KV cache.\nEncountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\" or \"The input (xxx xtokens) is longer than the model's context length (xxx tokens).\"\nThe input is too lengthy. Consider using a shorter sequence or increasing the max_model_len or context-length.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-30B-A3B (Non-Thinking)\n72.0\n97.1\n96.1\n95.0\n92.2\n82.6\n79.7\n76.9\n70.2\n66.3\n61.9\n55.4\n52.6\n51.5\n52.0\n50.9\nQwen3-30B-A3B-Instruct-2507  (Full Attention)\n86.8\n98.0\n96.7\n96.9\n97.2\n93.4\n91.0\n89.1\n89.8\n82.5\n83.6\n78.4\n79.7\n77.6\n75.7\n72.8\nQwen3-30B-A3B-Instruct-2507 (Sparse Attention)\n86.8\n98.0\n97.1\n96.3\n95.1\n93.6\n92.5\n88.1\n87.7\n82.9\n85.7\n80.7\n80.0\n76.9\n75.5\n72.2\nAll models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "bullerwins/Wan2.2-I2V-A14B-GGUF": "Wan2.2\nVideo Demos\nüî• Latest News!!\nCommunity Works\nüìë Todo List\nRun Wan2.2\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nYou need to download both a high-noise model and a low-noise model. High noise is used for the first steps and the low-noise for the details.Place them on ComfyUI/models/unet\nYou will need ComfyUI-GGUF from city96\nExample workflow included in the files, just drag it into ComfyUI\nWan2.2\nüíú Wan ¬†¬† ÔΩú ¬†¬† üñ•Ô∏è GitHub ¬†¬†  | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Technical Report ¬†¬† | ¬†¬† üìë Blog ¬†¬† | ¬†¬†üí¨ WeChat Group¬†¬† | ¬†¬† üìñ Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nüëç Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nüëç Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nüëç Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nüëç Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nThis repository also includes our I2V-A14B model, designed for image-to-video generation, supporting both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it achieves more stable video synthesis with reduced unrealistic camera movements and offers enhanced support for diverse stylized scenes.\nVideo Demos\nYour browser does not support the video tag.\nüî• Latest News!!\nJul 28, 2025: üëã Wan2.1 has been integrated into ComfyUI (CN | EN). Enjoy!\nJul 28, 2025: üëã Wan2.2's T2V, I2V and TI2V have been integrated into Diffusers (T2V-A14B | I2V-A14B | TI2V-5B). Feel free to give it a try!\nJul 28, 2025: üëã We've released the inference code and model weights of Wan2.2.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\nüìë Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nü§ó Huggingface     ü§ñ ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nüí°Note:\nThe TI2V-5B model supports 720P video generation at 24 FPS.\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-I2V-A14B --local-dir ./Wan2.2-I2V-A14B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-I2V-A14B --local_dir ./Wan2.2-I2V-A14B\nRun Image-to-Video Generation\nThis repository supports the `Wan2.2-I2V-A14B`` Image-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\nSingle-GPU inference\npython generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --offload_model True --convert_model_dtype --image examples/i2v_input.JPG --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nThis command can run on a GPU with at least 80GB VRAM.\nüí°For the Image-to-Video task, the size parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nImage-to-Video Generation without prompt\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --prompt '' --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --use_prompt_extend --prompt_extend_method 'dashscope'\nüí°The model can generate videos solely from the input image. You can use prompt extension to generate prompt from the image.\nThe process of prompt extension can be referenced here.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "deepseek-ai/DeepSeek-V3.1": "DeepSeek-V3.1\nIntroduction\nModel Downloads\nChat Template\nNon-Thinking\nThinking\nToolCall\nCode-Agent\nSearch-Agent\nEvaluation\nUsage Example\nHow to Run Locally\nLicense\nCitation\nContact\nDeepSeek-V3.1\nIntroduction\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\nHybrid thinking mode: One model supports both thinking mode and non-thinking mode by changing the chat template.\nSmarter tool calling: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.\nHigher thinking efficiency: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\nAdditionally, DeepSeek-V3.1 is trained using the UE8M0 FP8 scale data format on both model weights and activations to ensure compatibility with microscaling data formats. Please refer to DeepGEMM for more details.\nModel Downloads\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-V3.1-Base\n671B\n37B\n128K\nHuggingFace | ModelScope\nDeepSeek-V3.1\n671B\n37B\n128K\nHuggingFace | ModelScope\nChat Template\nThe details of our chat template is described in tokenizer_config.json and assets/chat_template.jinja. Here is a brief description.\nNon-Thinking\nFirst-Turn\nPrefix:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token </think>.\nMulti-Turn\nContext:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\nPrefix:\n<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\nThinking\nFirst-Turn\nPrefix:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>\nThe prefix of thinking mode is similar to DeepSeek-R1.\nMulti-Turn\nContext:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\nPrefix:\n<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the </think> is retained in every turn of context.\nToolCall\nToolcall is supported in non-thinking mode. The format is:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}\\n\\n{tool_description}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think> where the tool_description is\n## Tools\nYou have access to the following tools:\n### {tool_name1}\nDescription: {description}\nParameters: {json.dumps(parameters)}\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>tool_call_name<ÔΩútool‚ñÅsepÔΩú>tool_call_arguments<ÔΩútool‚ñÅcall‚ñÅendÔΩú>{additional_tool_calls}<ÔΩútool‚ñÅcalls‚ñÅendÔΩú>\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\nCode-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in assets/code_agent_trajectory.html.\nSearch-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent.\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\nPlease refer to the assets/search_tool_trajectory.html and assets/search_python_tool_trajectory.html for the detailed template.\nEvaluation\nCategory\nBenchmark (Metric)\nDeepSeek V3.1-NonThinking\nDeepSeek V3 0324\nDeepSeek V3.1-Thinking\nDeepSeek R1 0528\nGeneral\nMMLU-Redux (EM)\n91.8\n90.5\n93.7\n93.4\nMMLU-Pro (EM)\n83.7\n81.2\n84.8\n85.0\nGPQA-Diamond (Pass@1)\n74.9\n68.4\n80.1\n81.0\nHumanity's Last Exam (Pass@1)\n-\n-\n15.9\n17.7\nSearch Agent\nBrowseComp\n-\n-\n30.0\n8.9\nBrowseComp_zh\n-\n-\n49.2\n35.7\nHumanity's Last Exam (Python + Search)\n-\n-\n29.8\n24.8\nSimpleQA\n-\n-\n93.4\n92.3\nCode\nLiveCodeBench (2408-2505) (Pass@1)\n56.4\n43.0\n74.8\n73.3\nCodeforces-Div1 (Rating)\n-\n-\n2091\n1930\nAider-Polyglot (Acc.)\n68.4\n55.1\n76.3\n71.6\nCode Agent\nSWE Verified (Agent mode)\n66.0\n45.4\n-\n44.6\nSWE-bench Multilingual (Agent mode)\n54.5\n29.3\n-\n30.5\nTerminal-bench (Terminus 1 framework)\n31.3\n13.3\n-\n5.7\nMath\nAIME 2024 (Pass@1)\n66.3\n59.4\n93.1\n91.4\nAIME 2025 (Pass@1)\n49.8\n51.3\n88.4\n87.5\nHMMT 2025 (Pass@1)\n33.5\n29.2\n84.2\n79.4\nNote:\nSearch agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow.\nSWE-bench is evaluated with our internal code agent framework.\nHLE is evaluated with the text-only subset.\nUsage Example\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.1\")\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n{\"role\": \"assistant\", \"content\": \"<think>Hmm</think>I am DeepSeek\"},\n{\"role\": \"user\", \"content\": \"1+1=?\"}\n]\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>'\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú></think>'\nHow to Run Locally\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit DeepSeek-V3 repo for more information about running this model locally.\nUsage Recommendations:\nThe mlp.gate.e_score_correction_bias parameters should be loaded and computed in FP32 precision.\nEnsure that FP8 model weights and activations are formatted using the UE8M0 scale format.\nLicense\nThis repository and the model weights are licensed under the MIT License.\nCitation\n@misc{deepseekai2024deepseekv3technicalreport,\ntitle={DeepSeek-V3 Technical Report},\nauthor={DeepSeek-AI},\nyear={2024},\neprint={2412.19437},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.19437},\n}\nContact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "opendatalab/MinerU2.5-2509-1.2B": "Introduction\nKey Improvements\nQuick Start\nInstall packages\ntransformers Example\nvllm-engine Example (Recommended!)\nvllm-async-engine Example (Recommended!)\nModel Architecture\nPerformance on OmniDocBench\nAcross Different Elements\nAcross Various Document Types\nCase Demonstration\nFull-Document Parsing across Various Doc-Types\nTable Recognition\nFormula Recognition\nAcknowledgements\nCitation\nMinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing\nüöÄ Official Demo |\nüìÑ Technical Report\nIntroduction\nMinerU2.5 is a 1.2B-parameter vision-language model for document parsing that achieves state-of-the-art accuracy with high computational efficiency. It adopts a two-stage parsing strategy: first conducting efficient global layout analysis on downsampled images, then performing fine-grained content recognition on native-resolution crops for text, formulas, and tables. Supported by a large-scale, diverse data engine for pretraining and fine-tuning, MinerU2.5 consistently outperforms both general-purpose and domain-specific models across multiple benchmarks while maintaining low computational overhead.\nKey Improvements\nComprehensive and Granular Layout Analysis: It not only preserves non-body elements like headers, footers, and page numbers to ensure full content integrity, but also employs a refined and standardized labeling schema. This enables a clearer, more structured representation of elements such as lists, references, and code blocks.\nBreakthroughs in Formula Parsing: Delivers high-quality parsing of complex, lengthy mathematical formulae and accurately recognizes mixed-language (Chinese-English) equations.\nEnhanced Robustness in Table Parsing: Effortlessly handles challenging cases, including rotated tables, borderless tables, and tables with partial borders.\nQuick Start\nFor convenience, we provide mineru-vl-utils, a Python package that simplifies the process of sending requests and handling responses from MinerU2.5 Vision-Language Model. Here we give some examples to use MinerU2.5. For more information and usages, please refer to mineru-vl-utils.\nüìå We strongly recommend using vllm for inference, as the vllm-async-engine can achieve a concurrent inference speed of 2.12 fps on one A100.\nInstall packages\n# For `transformers` backend\npip install \"mineru-vl-utils[transformers]\"\n# For `vllm-engine` and `vllm-async-engine` backend\npip install \"mineru-vl-utils[vllm]\"\ntransformers Example\nfrom transformers import AutoProcessor, Qwen2VLForConditionalGeneration\nfrom PIL import Image\nfrom mineru_vl_utils import MinerUClient\n# for transformers>=4.56.0\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n\"opendatalab/MinerU2.5-2509-1.2B\",\ndtype=\"auto\", # use `torch_dtype` instead of `dtype` for transformers<4.56.0\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\n\"opendatalab/MinerU2.5-2509-1.2B\",\nuse_fast=True\n)\nclient = MinerUClient(\nbackend=\"transformers\",\nmodel=model,\nprocessor=processor\n)\nimage = Image.open(\"/path/to/the/test/image.png\")\nextracted_blocks = client.two_step_extract(image)\nprint(extracted_blocks)\nvllm-engine Example (Recommended!)\nfrom vllm import LLM\nfrom PIL import Image\nfrom mineru_vl_utils import MinerUClient\nfrom mineru_vl_utils import MinerULogitsProcessor  # if vllm>=0.10.1\nllm = LLM(\nmodel=\"opendatalab/MinerU2.5-2509-1.2B\",\nlogits_processors=[MinerULogitsProcessor]  # if vllm>=0.10.1\n)\nclient = MinerUClient(\nbackend=\"vllm-engine\",\nvllm_llm=llm\n)\nimage = Image.open(\"/path/to/the/test/image.png\")\nextracted_blocks = client.two_step_extract(image)\nprint(extracted_blocks)\nvllm-async-engine Example (Recommended!)\nimport io\nimport asyncio\nimport aiofiles\nfrom vllm.v1.engine.async_llm import AsyncLLM\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom PIL import Image\nfrom mineru_vl_utils import MinerUClient\nfrom mineru_vl_utils import MinerULogitsProcessor  # if vllm>=0.10.1\nasync_llm = AsyncLLM.from_engine_args(\nAsyncEngineArgs(\nmodel=\"opendatalab/MinerU2.5-2509-1.2B\",\nlogits_processors=[MinerULogitsProcessor]  # if vllm>=0.10.1\n)\n)\nclient = MinerUClient(\nbackend=\"vllm-async-engine\",\nvllm_async_llm=async_llm,\n)\nasync def main():\nimage_path = \"/path/to/the/test/image.png\"\nasync with aiofiles.open(image_path, \"rb\") as f:\nimage_data = await f.read()\nimage = Image.open(io.BytesIO(image_data))\nextracted_blocks = await client.aio_two_step_extract(image)\nprint(extracted_blocks)\nasyncio.run(main())\nasync_llm.shutdown()\nModel Architecture\nPerformance on OmniDocBench\nAcross Different Elements\nAcross Various Document Types\nCase Demonstration\nFull-Document Parsing across Various Doc-Types\nTable Recognition\nFormula Recognition\nAcknowledgements\nWe would like to thank Qwen Team, vLLM, OmniDocBench, UniMERNet, PaddleOCR, DocLayout-YOLO for providing valuable code and models. We also appreciate everyone's contribution to this open-source project!\nCitation\nIf you find our work useful in your research, please consider giving a star ‚≠ê and citation üìù :\n@misc{niu2025mineru25decoupledvisionlanguagemodel,\ntitle={MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing},\nauthor={Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and others},\nyear={2025},\neprint={2509.22186},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2509.22186},\n}",
    "FractalAIResearch/Fathom-Search-4B": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval And Synthesis For SLMs\nIntroduction\nü§ó Key Release Assets\n‚ú® Key Innovations\nüìä Key Results\nüöÄ Inference\n1) Environment setup\n2) Start the Model Servers (SGLang)\n3) Fire up the Search Tools Server (Serper + Jina + Query LLM)\n4) Run DeepResearch! (Single question inference)\nüßÆ Evaluation (Multi GPU)\nCommon flags\nEvaluate Fathom‚ÄëSearch\nEvaluate other baselines used in the paper\nüìú License\nAcknowledgments\nüìñ Citation\nFathom-DeepResearch: Unlocking Long Horizon Information Retrieval And Synthesis For SLMs\nThis model was presented in the paper Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs.\nIntroduction\nWe introduce Fathom-DeepResearch, an agentic DeepResearch system that sets state-of-the-art performance in the open-weights category on search-intensive benchmarks (SimpleQA, FRAMES, WebWalkerQA, Seal0) and outperforms closed-source DeepResearch agents like (Claude, Grok, Perplexity, GPT-4o) on open-ended synthesis benchmark DeepResearch-Bench. Built on two specialized models, Fathom-Search-4B, optimized for long-horizon, evidence-seeking through live web search, and Fathom-Synthesizer-4B, designed for open-ended synthesis and report generation. Refer to our technical report for our training recipe and more exciting results!\nü§ó Key Release Assets\nü§ó Fathom-Search-4B & ü§ó Fathom-Synthesizer-4B We opensource two 4B-parameter models trained to browse, extract, verify and reason over live web content, and synthesize the findings into rich citation-dense reports!\nüåê Fathom-WebAgents We open-source our search tool server, built on top of Jina-AI, Crawl4AI, Trafilatura, and Serper.dev, which acts as a powerful search backend, optimized to handle large volumes of  asynchronous requests and includes specialized handlers for diverse sources such as YouTube, PDFs, Reddit, Github, and more.\nü§ó  DuetQA We release a ~5K-sample dataset created through our novel multi-agent self-play pipeline, containing verifiable question‚Äìanswer pairs that are impossible to answer without live web search , for DeepSearch model training.\nü§ó DeepResearch-SFT We are releasing a synthetic SFT corpus used to train Fathom-Syntheizer-4B, to enable the conversion of  downstream search/investigation traces of DeepSearch enabled models into DeepResearch reports via our novel plan-then-write protocol.\nüìö Technical Report We release technical report outlining key training innovations, dataset generation pipeline, complete training startegy and evaluation!\n‚ú® Key Innovations\nüß≠ Multi-Agent Self-PlayWe present a self-supervised dataset construction framework designed to yield verifiable, live web-search enforcing, multi-hop QA pairs. This pipeline serves as the basis for generating DUETQA, a dataset tailored for training agentic deepsearch models.\n‚ö° RAPO: Reward-Aware Policy OptimizationWe introduce RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers that stabilize long-horizon tool use.\nüéØ Steerable Step-Level RewardWe design our novel Steerable Step-Level Reward that alleviates the reward-hacking challenge faced by RLVR training in the multi-turn, tool-interaction. Our reward function enables us to steer (i) how much the agent uses tools and (ii) how it allocates cognition to exploration and verification.\nüìù DeepResearch Report Synthesis ProtocolWe propose a novel plan-then-write protocol for synthesizing DeepSearch traces into citation-dense reports, where planning involves decomposition of questions into ordered sub-questions, evidence-to-section mapping aligning URLs to the correct sub-question, and insight planning to guide synthesis into structured reports.\nüìä Key Results\nüöÄ Inference\nThis section shows how to host Fathom-DeepResearch locally and run inference!.\n1) Environment setup\nuv pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126\nuv pip install --no-build-isolation -r requirements.txt\nplaywright install-deps\nplaywright install\n2) Start the Model Servers (SGLang)\nChange --model-path to your model identifier (eg. FractalAIResearch/Fathom-Search-4B) or local path.\nFathom-Search-4B\nLaunch on default port below 8902, and GPU 0\nexport CUDA_VISIBLE_DEVICES=0\npython -m sglang.launch_server \\\n--served-model-name  Fathom-Search-4B \\\n--model-path FractalAIResearch/Fathom-Search-4B \\\n--enable-metrics \\\n--dtype bfloat16 \\\n--host 0.0.0.0 \\\n--port 8902 \\\n--trust-remote-code \\\n--disable-radix-cache \\\n--disable-cuda-graph \\\n--context-length 40960 \\\n#--tp 2 #optional for multi-gpu inference\nFathom-Synthesizer-4B\nLaunch on default port 8903 and GPU 1, we use YARN scaling with RoPE factor 2.0 for the synthesizer model\nexport CUDA_VISIBLE_DEVICES=1\npython3 -m sglang.launch_server \\\n--served-model-name Fathom-Synthesizer-4B \\\n--model-path FractalAIResearch/Fathom-Synthesizer-4B \\\n--enable-metrics \\\n--dtype bfloat16 \\\n--host 0.0.0.0 \\\n--port 8903 \\\n--trust-remote-code \\\n--disable-radix-cache \\\n--disable-cuda-graph \\\n--disable-cuda-graph \\\n--context-length 131092 \\\n--json-model-override-args '{\"rope_type\":\"yarn\",\"factor\":2.0,\"original_max_position_embeddings\":40960}' \\\n#--tp 2 #optional for multi-gpu inference\n(Optional) Query-LLM for search backend\nLaunch on default port 8905 and GPU 3\nexport CUDA_VISIBLE_DEVICES=3\npython3 -m sglang.launch_server \\\n--served-model-name Query-LLM \\\n--model-path Qwen3/Qwen3-32B #Any LLM of choice to act as the query LLM for search tool server \\\n--enable-metrics \\\n--dtype bfloat16 \\\n--host 0.0.0.0 \\\n--port 8905 \\\n--trust-remote-code \\\n--disable-radix-cache \\\n--disable-cuda-graph \\\n--disable-cuda-graph \\\n--context-length 131092 \\\n#--tp 2 #optional for multi-gpu inference\n3) Fire up the Search Tools Server (Serper + Jina + Query LLM)\nSet the following in scripts/.env:\nSERPER_API_KEY (get from serper.dev; ~2,500 free queries without any card) (necessary for live web-search)\nJINA_API_KEY (optional) ‚Äî used in the web-page extraction pipeline (recommended for replicaiion)\nOPENAI_API_KEY (optional) ‚Äî for goal conditioned querying of web-pages using GPT-4.1-mini (recommended for replication)\nSERPER_CACHE_DIR  ‚Äî path to caching the search results from serper.dev to save cost and retrival time\nJINA_CACHE_DIR  ‚Äî path to caching the search results from jina.ai to save cost and retrival time\n(Recommended) Launch search tool server on port 8904 with 256 workers with gpt-4.1-mini as the Query-LLM (OpenAI API key needed):\nserving/host_server.sh 8904 256 \"openai:gpt-4.1-mini\"\n(Or) Launch search tool server on port 8904 with a locally hosted LLM of choice via SGLang at  port 8905 (See step 2) as the Query-LLM:\nserving/host_server.sh 8904 256 \"http://0.0.0.0:8905\"\nNote: Our search backend also supports web_search and web_visit functions for searching the web and returning entire web-page w/o the need for a Query-LLM.\nUsed by model like Jan-Naao and II-Search-4B in their implementation.\n4) Run DeepResearch! (Single question inference)\nRun Fathom‚ÄëDeeoSearch via inference.py:\npython inference.py \\\n--question \"Find out the names of all the companies that have been selected under the IndiaAI mission? Write a detailed report on what their deliverables are? What are the ways in which this mission will impact india in the coming decade?\" \\\n--executors http://0.0.0.0:8904 \\\n--model-url http://0.0.0.0:8902 \\\n--tokenizer FractalAIResearch/Fathom-Search-4B \\\n--summary-llm http://0.0.0.0:8903 \\\n--deepresearch #DeepResearch mode!, disable to get Fathom-Search-4B response Faster & shorter\nTips:\nUse multiple executors for load‚Äëbalancing: --executors http://0.0.0.0:8904, http://0.0.0.0:8905.\nüßÆ Evaluation (Multi GPU)\nThis section covers batched evaluation using the provided scripts in scripts/. Use placeholders model_path and dataset_name ‚Äî the evaluator will read eval_benchmarks/<dataset_name>.jsonl with columns ['id','question','answer'].\nCommon flags\nFlag\nRequired\nExample\nDescription\n--model-path\n‚úÖ\nmodel_path\nModel repo ID or local path.\n--model-port\n‚¨ú\n1255\nPort where the model server listens (if applicable).\n--executors\n‚¨ú\n1211,1212\nComma‚Äëseparated tool/extractor workers.\n--dataset\n‚úÖ\ndataset_name\nLooks for eval_benchmarks/<dataset_name>.jsonl.\n--out-base\n‚¨ú\n./results\nWhere results are written.\n--query-llm\n‚¨ú\ngpt-4.1-mini or /path/to/Qwen3-4B\nExtractor/Query LLM.\n--query-port\n‚¨ú\n1260\nPort for a locally served query LLM.\n--main-gpus\n‚¨ú\n0,1\nCUDA devices for the main model.\n--query-gpus\n‚¨ú\n2,3\nCUDA devices for the query LLM.\nEvaluate Fathom‚ÄëSearch\nGPT-4.1-mini query-LLM on CPU, main model on GPUs 0,1 (TP=2)\nscripts/eval_fathom_search.sh \\\n--model-path FractalAIResearch/Fathom-Search-4B \\\n--model-port 1255 \\\n--executors 1211,1212 \\\n--dataset dataset_name \\\n--main-gpus 0,1 \\\n--query-llm gpt-4.1-mini\nLocal Qwen3-32B as Query-LLM on GPUs 2,3 (TP=2); main model on GPUs 0,1 (TP=2)\nscripts/eval_fathom_search.sh \\\n--model-path FractalAIResearch/Fathom-Search-4B \\\n--model-port 1255 \\\n--executors 1211,1212 \\\n--dataset dataset_name \\\n--main-gpus 0,1 \\\n--query-llm Qwen3/Qwen3-32B \\\n--query-port 1260 \\\n--query-gpus 2,3\nEvaluate other baselines used in the paper\n# II‚ÄëSearch‚Äë4B\nscripts/eval_ii_search.sh \\\n--model-path Intelligent-Internet/II-Search-4B \\\n--model-port 1255 \\\n--executors 1211,1212 \\\n--dataset dataset_name\n# Jan‚ÄëNano‚Äë32K\nscripts/eval_jan_nano_32K.sh \\\n--model-path Menlo/Jan-nano \\\n--model-port 1255 \\\n--executors 1211,1212 \\\n--dataset dataset_name\n# Jan‚ÄëNano‚Äë128K\nscripts/eval_jan_nano_128K.sh \\\n--model-path Menlo/Jan-nano-128k \\\n--model-port 1255 \\\n--executors 1211,1212 \\\n--dataset dataset_name\n# R1‚ÄëSearcher(7B)\nscripts/eval_r1_searcher.sh \\\n--model-path XXsongLALA/Qwen-2.5-7B-base-RAG-RL \\\n--model-port 1255 \\\n--dataset dataset_name\n# ZeroSearch\nscripts/eval_zerosearch.sh \\\n--model-path Alibaba-NLP/ZeroSearch_wiki_V2_Qwen2.5_7B_Instruct \\\n--model-port 1255 \\\n--dataset dataset_name\n# search‚Äëo1 (with Qwen3‚Äë4B)\nscripts/eval_search_o1.sh \\\n--model-path Qwen3/Qwen3-4B \\\n--model-port 1255 \\\n--dataset dataset_name\n#cleanup ports after eval\nscripts/cleanup.sh 1255 1211 1212\nRun\nüìú License\nThis repository and all the release assets are available under the MIT License, underscoring our dedication to open and inclusive AI innovation. By freely sharing our work, we aim to democratize AI technology, empowering researchers, developers, and enthusiasts everywhere to use, adapt, and expand upon it without limitation. This open and permissive approach promotes global collaboration, accelerates innovation, and enriches the AI community as a whole.\nAcknowledgments\nWe would like to acknowledge the following works for enabling our project:\nQwen3-4B\nReCall\nüìñ Citation\n@misc{fathomdeepsearch2025,\ntitle={Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval And Synthesis For SLMs},\nauthor={Shreyas Singh and Pradeep Moturi and Kunal Singh},\nhowpublished={\\url{https://huggingface.co/FractalAIResearch/Fathom-Search-4B}},\nnote={Hugging Face},\nyear={2025}\n}",
    "cerebras/Qwen3-Coder-REAP-246B-A35B-FP8": "Qwen3-Coder-REAP-246B-A35B-FP8\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nQwen3-Coder-REAP-246B-A35B-FP8\n‚ú® Highlights\nIntroducing Qwen3-Coder-REAP-246B-A35B-FP8, a memory-efficient compressed variant of Qwen3-Coder-480B-A35B-Instruct-FP8 that maintains near-identical performance while being 50% lighter.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 480B model\n50% Memory Reduction: Compressed from 480B to 246B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nüìã Model Overview\nQwen3-Coder-REAP-246B-A35B-FP8 has the following specifications:\nBase Model: Qwen3-Coder-480B-A35B-Instruct\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 50% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 246B total, 35B activated per token\nNumber of Layers: 62\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 80 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 262,144 tokens natively (extendable to 1M with YaRN)\nQuantization: FP8\nLicense: Apache 2.0\nüìä Evaluations\nBenchmark\nQwen3-Coder-480B-A35B-Instruct-FP8\nQwen3-Coder-REAP-363B-A35B-FP8\nQwen3-Coder-REAP-246B-A35B-FP8\nCompression\n‚Äî\n25%\n50%\nHumanEval\n95.1\n95.7\n93.9\nHumanEval+\n89.0\n89.0\n87.2\nMBPP\n92.3\n91.7\n91.0\nMBPP+\n79.1\n77.2\n77.2\nLiveCodeBench (25.01 - 25.05)\n43.1\n41.6\n41.5\nSWE-Bench-Verified (w/ mini-swe-agent)\n54.0\n54.0\n52.2\nBFCL-v3 (Non-Live)\n86.6\n87.8\n84.9\nBFCL-v3 (Live)\n82.5\n82.3\n80.1\nBFCL-v3 (Multi-Turn)\n38.0\n39.2\n37.1\nBFCL-v3 (Overall)\n69.0\n69.8\n67.4\nùúè¬≤-bench (Airline)\n46.0\n48.7\n44.7\nùúè¬≤-bench (Retail)\n64.3\n66.1\n63.2\nùúè¬≤-bench (Telecom)\n50.0\n52.9\n47.1\nTerminalBench 0.1.1 (Terminus agent)\n30.5\n30.5\n30.0\nüü© This checkpoint maintains comparable performance while being 50% lighter.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/Qwen3-Coder-REAP-246B-A35B-FP8 \\\n--tensor-parallel-size 8 \\\n--tool-call-parser qwen3_coder \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of Qwen3-Coder-480B-A35B-Instruct, with a 50% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nQwen/Qwen3-Coder-480B-A35B-Instruct\nand distributed under the Apache 2.0 License.\nüîó View License File ‚Üí\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "amd/Nitro-E": "AMD Nitro-E\nIntroduction\nDetails\nQuickstart\nLicense\nAMD Nitro-E\nIntroduction\nNitro-E is a family of text-to-image diffusion models focused on highly efficient training. With just 304M parameters, Nitro-E is designed to be resource-friendly for both training and inference. For training, it only takes 1.5 days on a single node with 8 AMD Instinct‚Ñ¢ MI300X GPUs. On the inference side, Nitro-E delivers a throughput of 18.8 samples per second (batch size 32, 512px images) a single AMD Instinct MI300X GPU. The distilled version can further increase the throughput to 39.3 samples per second. The release consists of:\nNitro-E-512px: a EMMDiT-based 20-steps model train from scratch.\nNitro-E-512px-dist: a EMMDiT-based model distilled from Nitro-E-512px.\nNitro-E-512px-GRPO: a post-training model fine-tuned from Nitro-E-512px using Group Relative Policy Optimization (GRPO) strategy.\n‚ö°Ô∏è Open-source code!\n‚ö°Ô∏è technical blog!\nDetails\nModel architecture: We propose Efficient Multimodal Diffusion Transformer (E-MMDiT), an efficient and lightweight multimodal diffusion model with only 304M\nparameters for fast image synthesis requiring low training resources. Our design philosophy centers on token reduction as the computational\ncost scales significantly with the token count. We adopt a highly compressive visual tokenizer to produce a more compact representation and propose a novel multi-path compression\nmodule for further compression of tokens. To enhance our design, we introduce Position Reinforcement, which strengthens positional information to maintain spatial coherence,\nand Alternating Subregion Attention (ASA), which performs attention within subregions to further reduce computational cost. In addition, we propose AdaLN-affine, an\nefficient lightweight module for computing modulation parameters in transformer blocks. See our technical blog post for more details.\nDataset: Our models were trained on a dataset of ~25M images consisting of both real and synthetic data sources that are openly available on the internet. We make use of the following datasets for training: Segment-Anything-1B, JourneyDB, DiffusionDB and DataComp as prompt of the generated data.\nTraining cost: The Nitro-E-512px model requires only 1.5 days of training from scratch on a single node with 8 AMD Instinct‚Ñ¢ MI300X GPUs.\nQuickstart\nImage generation with 20 steps:\nimport torch\nfrom core.tools.inference_pipe import init_pipe\ndevice = torch.device('cuda:0')\ndtype = torch.bfloat16\nrepo_name = \"amd/Nitro-E\"\nresolution = 512\nckpt_name = 'Nitro-E-512px.safetensors'\n# for 1024px model\n# resolution = 1024\n# ckpt_name = 'Nitro-E-1024px.safetensors'\nuse_grpo = True\nif use_grpo:\npipe = init_pipe(device, dtype, resolution, repo_name=repo_name, ckpt_name=ckpt_name, ckpt_path_grpo='ckpt_grpo_512px')\nelse:\npipe = init_pipe(device, dtype, resolution, repo_name=repo_name, ckpt_name=ckpt_name)\nprompt = 'A hot air balloon in the shape of a heart grand canyon'\nimages = pipe(prompt=prompt, width=resolution, height=resolution, num_inference_steps=20, guidance_scale=4.5).images\nImage generation with 4 steps:\nimport torch\nfrom core.tools.inference_pipe import init_pipe\ndevice = torch.device('cuda:0')\ndtype = torch.bfloat16\nresolution = 512\nrepo_name = \"amd/Nitro-E\"\nckpt_name = 'Nitro-E-512px-dist.safetensors'\npipe = init_pipe(device, dtype, resolution, repo_name=repo_name, ckpt_name=ckpt_name)\nprompt = 'A hot air balloon in the shape of a heart grand canyon'\nimages = pipe(prompt=prompt, width=resolution, height=resolution, num_inference_steps=4, guidance_scale=0).images\nLicense\nCopyright (c) 2025 Advanced Micro Devices, Inc. All Rights Reserved.\nThis project is licensed under the MIT License.",
    "nvidia/llama-3.2-nv-embedqa-1b-v2": "Model Overview\nDescription\nIntended use\nLicense/Terms of use\nModel Architecture\nInput\nOutput\nUsage\nSoftware Integration\nModel Version(s)\nTraining Dataset & Evaluation\nTraining Dataset\nEvaluation Results\nCitation\nEthical Considerations\nGet Help\nEnterprise Support\nNVIDIA NIM Documentation\nBias\nExplainability\nPrivacy\nSafety\nModel Overview\nDescription\nThe Llama 3.2 NeMo Retriever Embedding 1B model is optimized for multilingual and cross-lingual text question-answering retrieval with support for long documents (up to 8192 tokens) and dynamic embedding size (Matryoshka Embeddings). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish.\nIn addition to enabling multilingual and cross-lingual question-answering retrieval, this model reduces the data storage footprint by 35x through dynamic embedding sizing and support for longer token length, making it feasible to handle large-scale datasets efficiently.\nAn embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are typically transformer encoders that process tokens of input text (for example: question, passage) to output an embedding.\nThis model is ready for commercial use.\nThe Llama 3.2 NeMo Retriever Embedding 1B model is a part of the NVIDIA NeMo Retriever collection of NIM, which provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants.\nWe are excited to announce the open sourcing of this commercial embedding model. For users interested in deploying this model in production environments, it is also available via the model API in NVIDIA Inference Microservices (NIM) at llama-3.2-nv-embedqa-1b-v2.\nIntended use\nThe Llama 3.2 NeMo Retriever Embedding 1B model is most suitable for users who want to build a multilingual question-and-answer application over a large text corpus, leveraging the latest dense retrieval technologies.\nLicense/Terms of use\nUse of this model is governed by the NVIDIA Open Model License Agreement. Additional Information: Llama 3.2 Community Model License Agreement.\nModel Architecture\nArchitecture Type: Transformer\nNetwork Architecture: Fine-tuned Llama3.2 1B Retriever\nThis NeMo Retriever embedding model is a transformer encoder - a fine-tuned version of Llama3.2 1b, with 16 layers and an embedding size of 2048, which is trained on public datasets. The AdamW optimizer is employed incorporating 100 warm up steps and 5e-6 learning rate with WarmupDecayLR scheduler. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question.\nInput\nInput Type: Text\nInput Format: List of strings\nInput Parameter: 1D\nOther Properties Related to Input: The model's maximum context length is 8192 tokens. Texts longer than maximum length must either be chunked or truncated.\nOutput\nOutput Type: Floats\nOutput Format: List of float arrays\nOutput: Model outputs embedding vectors of maximum dimension 2048 for each text string (can be configured based on 384, 512, 768, 1024, or 2048).\nOther Properties Related to Output: N/A\nUsage\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef average_pool(last_hidden_states, attention_mask):\n\"\"\"Average pooling with attention mask.\"\"\"\nlast_hidden_states_masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nembedding = last_hidden_states_masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\nembedding = F.normalize(embedding, dim=-1)\nreturn embedding\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/llama-3.2-nv-embedqa-1b-v2\")\nmodel = AutoModel.from_pretrained(\"nvidia/llama-3.2-nv-embedqa-1b-v2\", trust_remote_code=True)\nmodel = model.to(\"cuda:0\")\nmodel.eval()\nquery_prefix = 'query: '\ndocument_prefix = 'passage: '\nqueries = [\n\"how much protein should a female eat\",\n\"summit define\",\n]\ndocuments = [\n\"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nqueries = [f\"{query_prefix} {query}\" for query in queries]\ndocuments = [f\"{document_prefix} {document}\" for document in documents]\nbatch_queries = tokenizer(queries, padding=True, truncation=True, return_tensors='pt').to(\"cuda:0\")\nbatch_documents = tokenizer(documents, padding=True, truncation=True, return_tensors='pt').to(\"cuda:0\")\nwith torch.no_grad():\noutputs_queries = model(**batch_queries)\noutputs_documents = model(**batch_documents)\n# Average Pooling\nembeddings_queries = average_pool(outputs_queries.last_hidden_state, batch_queries[\"attention_mask\"])\nprint(\"Query embeddings:\")\nprint(embeddings_queries)\nprint(embeddings_queries.shape)\n#torch.Size([2, 2048])\nembeddings_documents = average_pool(outputs_documents.last_hidden_state, batch_documents[\"attention_mask\"])\nprint(\"\\nDocument embeddings:\")\nprint(embeddings_documents)\nprint(embeddings_documents.shape)\n#torch.Size([2, 2048])\n# Compute similarity scores\nscores = (embeddings_queries @ embeddings_documents.T)\nprint(\"\\nSimilarity scores:\")\nprint(scores.tolist())\n#Similarity scores:\n#[[0.5904847979545593, -0.03595362976193428], [-0.030595744028687477, 0.4608352780342102]]\nSoftware Integration\nRuntime Engine: NeMo Retriever embedding NIM\nSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA Lovelace\nSupported Operating System(s): Linux\nModel Version(s)\nLlama 3.2 NeMo Retriever Embedding 1B v2\nShort Name: llama-3.2-nv-embedqa-1b-v2\nTraining Dataset & Evaluation\nTraining Dataset\nThe development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MS MARCO restricts ‚Äåcommercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications.\nData Collection Method by dataset: Automated, Unknown\nLabeling Method by dataset: Automated, Unknown\nProperties: Semi-supervised pre-training on 12M samples from public datasets and fine-tuning on 1M samples from public datasets.\nEvaluation Results\nProperties: We evaluated the NeMo Rtriever embdding model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering - NQ, HotpotQA and FiQA (Finance Q&A) from BeIR benchmark and TechQA dataset. Note that the model was evaluated offline on A100 GPUs using the model's PyTorch checkpoint.  In this benchmark, the metric used was Recall@5.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on NQ, HotpotQA, FiQA, TechQA dataset\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 2048)\n68.60%\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 384)\n64.48%\nllama-3.2-nv-embedqa-1b-v1 (embedding dim 2048)\n68.97%\nnv-embedqa-mistral-7b-v2\n72.97%\nnv-embedqa-mistral-7B-v1\n64.93%\nnv-embedqa-e5-v5\n62.07%\nnv-embedqa-e5-v4\n57.65%\ne5-large-unsupervised\n48.03%\nBM25\n44.67%\nWe evaluated the multilingual capabilities on the academic benchmark MIRACL across 15 languages and translated the English and Spanish version of MIRACL into additional 11 languages. The reported scores are based on an internal version of MIRACL by selecting hard negatives for each query to reduce the corpus size.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on multilingual\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 2048)\n60.75%\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 384)\n58.62%\nllama-3.2-nv-embedqa-1b-v1\n60.07%\nnv-embedqa-mistral-7b-v2\n50.42%\nBM25\n26.51%\nWe evaluated the cross-lingual capabilities on the academic benchmark MLQA based on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in different languages. We calculate the average Recall@5 across the 42 different language pairs.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on MLQA dataset with different languages\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 2048)\n79.86%\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 384)\n71.61%\nllama-3.2-nv-embedqa-1b-v1 (embedding dim 2048)\n78.77%\nnv-embedqa-mistral-7b-v2\n68.38%\nBM25\n13.01%\nWe evaluated the support of long documents on the academic benchmark Multilingual Long-Document Retrieval (MLDR) built on Wikipedia and mC4, covering 12 typologically diverse languages. The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer. The MLDR dataset is based on synthetic generated questions with a LLM, which has the tendency to create questions with similar keywords than the positive document, but might not be representative for real user queries. This characteristic of the dataset benefits sparse embeddings like BM25.\nOpen & Commercial Retrieval Models\nAverage Recall@5 on MLDR\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 2048)\n59.55%\nllama-3.2-nv-embedqa-1b-v2 (embedding dim 384)\n54.77%\nllama-3.2-nv-embedqa-1b-v1 (embedding dim 2048)\n60.49%\nnv-embedqa-mistral-7b-v2\n43.24%\nBM25\n71.39%\nData Collection Method by dataset: Unknown\nLabeling Method by dataset: Unknown\nProperties: The evaluation datasets are based on MTEB/BEIR, TextQA, TechQA, MIRACL, MLQA, and MLDR. The size ranges between 10,000s up to 5M depending on the dataset.\nInference\nEngine: TensorRT\nTest Hardware: H100 PCIe/SXM, A100 PCIe/SXM, L40s, L4, and A10G\nCitation\n@article{moreira2024nv,\ntitle={NV-Retriever: Improving text embedding models with effective hard-negative mining},\nauthor={Moreira, Gabriel de Souza P and Osmulski, Radek and Xu, Mengyao and Ak, Ronay and Schifferer, Benedikt and Oldridge, Even},\njournal={arXiv preprint arXiv:2407.15831},\nyear={2024}\n}\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nGet Help\nEnterprise Support\nGet access to knowledge base articles and support cases or  submit a ticket at the NVIDIA AI Enterprise Support Services page..\nNVIDIA NIM Documentation\nVisit the NeMo Retriever docs page for release documentation, deployment guides and more.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nPassage and query embedding for question and answer retrieval\nModel Type:\nTransformer encoder\nIntended User:\nGenerative AI creators working with conversational AI models - users who want to build a multilingual question and answer application over a large text corpus, leveraging the latest dense retrieval technologies.\nOutput:\nArray of float numbers (Dense Vector Representation for the input text)\nDescribe how the model works:\nModel transforms the tokenized input text into a dense vector representation.\nPerformance Metrics:\nAccuracy, Throughput, and Latency\nPotential Known Risks:\nThis model does not always guarantee to retrieve the correct passage(s) for a given query.\nLicensing & Terms of Use:\nUse of this model is governed by the NVIDIA Open Model License Agreement. Additional Information: Llama 3.2 Community Model License Agreement.\nTechnical Limitations\nThe model‚Äôs max sequence length is 8192. Therefore, the longer text inputs should be truncated.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nN/A\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personally-identifiable information (PII)?\nNone\nWas consent obtained for any personal data used?\nNot Applicable\nPII used to create this model?\nNone\nHow often is the dataset reviewed?\nBefore Every Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?\nNo\nIf personal data was collected for the development of the model, was it collected directly by NVIDIA?\nNot Applicable\nIf personal data was  collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?\nNot Applicable\nIf personal data was collected for the development of this AI model, was it minimized to only what was required?\nNot Applicable\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nSafety\nField\nResponse\nModel Application(s):\nText Embedding for Retrieval\nDescribe the physical safety impact (if present).\nNot Applicable\nUse Case Restrictions:\nUse of this model is governed by the NVIDIA Open Model License Agreement. Additional Information: Llama 3.2 Community Model License Agreement.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.",
    "vogent/Vogent-Turn-80M": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nIf you are using this model as part of a horizontal voice agent platform, you agree not to set Vogent-Turn-80M as a default option, and to require users to select an option labeled 'Vogent Turn Detector' if they would like to use the model.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nVogent-Turn-80M\nModel Details\nModel Description\nModel Sources\nUses\nBias, Risks, and Limitations\nHow to Get Started with the Model\nQuick Install\nBasic Usage\nAvailable Interfaces\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nCitation\nMore Information\nModel Card Authors\nModel Card Contact\nVogent-Turn-80M\nState-of-the-art multimodal turn detection model for voice AI systems, achieving 94.1% accuracy by combining acoustic and linguistic signals for real-time conversational applications.\nTechnical Report\nHF Space\nInference Code\nModel Details\nModel Description\nVogent-Turn-80M is a multimodal turn detection model that addresses the critical challenge of determining when a speaker has finished their turn in a conversation. Unlike traditional approaches that rely solely on audio or text, Vogent-Turn-80M processes both acoustic features (via Whisper encoder) and semantic context to make accurate predictions in real-time (~7ms on T4 GPU).\nDeveloped by: Vogent AI\nModel type: Multimodal Turn Detection (Binary Classification)\nLanguage(s) (NLP): English\nLicense: Vogent-Turn-80M is licensed under a modified Apache-2.0 license; horizontal voice agent platforms may not select Vogent-Turn-80M as the default turn-detection model, and any end-users who which to use the model must be required to select 'Vogent Turn Detector.' Otherwise, standard Apache 2.0 provisions apply.\nFinetuned from model: SmolLM2-135M (reduced to 80M parameters by using only first 12 layers)\nModel Sources\nGitHub Repository: https://github.com/vogent/vogent-turn\nBlog post: https://blog.vogent.ai/posts/voturn-80m-state-of-the-art-turn-detection-for-voice-agents\nUses\nVogent-Turn-80M is designed for real-time turn detection in voice assistant applications, determining when a user has finished speaking to enable natural conversational flow without premature interruptions or awkward delays.\nBias, Risks, and Limitations\nTechnical Limitations:\nEnglish-only support; turn-taking conventions vary across languages and cultures\nCPU inference may be too slow for some real-time applications\nHow to Get Started with the Model\nFor complete installation and usage instructions, visit: https://github.com/vogent/vogent-turn\nQuick Install\n# Clone the repository\ngit clone https://github.com/vogent/vogent-turn.git\ncd vogent-turn\n# Install in development mode\npip install -e .\nBasic Usage\nfrom vogent_turn import TurnDetector\nimport soundfile as sf\nimport urllib.request\n# Initialize detector\ndetector = TurnDetector(compile_model=True, warmup=True)\n# Download and load audio\naudio_url = \"https://storage.googleapis.com/voturn-sample-recordings/incomplete_number_sample.wav\"\nurllib.request.urlretrieve(audio_url, \"sample.wav\")\naudio, sr = sf.read(\"sample.wav\")\n# Run turn detection with conversational context\nresult = detector.predict(\naudio,\nprev_line=\"What is your phone number\",\ncurr_line=\"My number is 804\",\nsample_rate=sr,\nreturn_probs=True,\n)\nprint(f\"Turn complete: {result['is_endpoint']}\")\nprint(f\"Done speaking probability: {result['prob_endpoint']:.1%}\")\nAvailable Interfaces\nPython Library: Direct integration with TurnDetector class\nCLI Tool: vogent-turn-predict speech.wav --prev \"What is your phone number\" --curr \"My number is 804\"\nSee the GitHub repository for detailed documentation, performance benchmarks, and advanced usage.\nTraining Details\nTraining Data\nThe model was trained on a diverse dataset combining human-collected and synthetic conversational data:\nTraining Procedure\nPreprocessing\nAudio: Last 8 seconds extracted via Whisper-Tiny encoder ‚Üí ~400 audio tokens\nText: Full conversational context including assistant and user utterances\nLabels: Binary classification (turn complete/incomplete)\nMultimodal fusion: Audio embeddings projected into LLM's input space and concatenated with text\nTraining Hyperparameters\nTraining regime: fp16 mixed precision\nBase model initialization: SmolLM2-135M (first 12 layers)\nArchitecture modifications: Reduced from 135M to ~80M parameters through layer ablation\nSpeeds, Sizes, Times\nModel size: ~80M parameters\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nInternal test set covering diverse conversational scenarios and edge cases where audio-only or text-only approaches fail.\nAccuracy: 94.1%\nAUPRC: 0.975\nTechnical Specifications\nModel Architecture and Objective\nArchitecture:\nAudio Encoder: Whisper-Tiny (processes up to 8 seconds of 16kHz audio)\nText Model: SmolLM-135M (12 layers, ~80M parameters)\nMultimodal Fusion: Audio embeddings projected into LLM's input space\nClassifier: Binary classification head (turn complete/incomplete)\nProcessing Flow:\nAudio (16kHz PCM) ‚Üí Whisper Encoder ‚Üí Audio Embeddings (~400 tokens)\nText Context ‚Üí SmolLM Tokenizer ‚Üí Text Embeddings\nConcatenate embeddings ‚Üí SmolLM Transformer ‚Üí Last token hidden state\nLinear Classifier ‚Üí Softmax ‚Üí [P(continue), P(endpoint)]\nCompute Infrastructure\nHardware\nOptimization Features:\ntorch.compile with max-autotune mode\nDynamic tensor shapes without recompilation\nPre-warmed bucket sizes (64, 128, 256, 512, 1024)\nSoftware\nFramework: PyTorch with torch.compile\nAudio processing: Whisper encoder (up to 8 seconds)\nCitation\nBibTeX:\n@misc{voturn2025,\ntitle={Vogent-Turn-80M: State-of-the-Art Turn Detection for Voice Agents},\nauthor={Varadarajan, Vignesh and Vytheeswaran, Jagath},\nyear={2025},\npublisher={Vogent AI},\nhowpublished={\\url{https://huggingface.co/vogent/Vogent-Turn-80M}},\nnote={Blog: \\url{https://blog.vogent.ai/posts/voturn-80m-state-of-the-art-turn-detection-for-voice-agents}}\n}\nMore Information\nVogent-Turn-80M is part of Vogent's comprehensive voice AI platform.\nResources:\nFull documentation and code: https://github.com/vogent/vogent-turn\nPlatform access: https://vogent.ai\nEnterprise solutions: Contact j@vogent.ai\nUpcoming releases:\nInt8 quantized model for faster CPU deployment\nMultilingual versions\nDomain-specific adaptations\nModel Card Authors\nVogent AI Team\nModel Card Contact\nGitHub Repository: https://github.com/vogent/vogent-turn\nGitHub Issues: https://github.com/vogent/vogent-turn/issues\nWebsite: https://vogent.ai",
    "bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF": "Llamacpp imatrix Quantizations of Qwen3-Coder-REAP-25B-A3B by cerebras\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Qwen3-Coder-REAP-25B-A3B by cerebras\nUsing llama.cpp release b6810 for quantization.\nOriginal model: https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B\nAll quants made using imatrix option with dataset from here combined with a subset of combined_all_small.parquet from Ed Addario here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\nNo chat template specified so default is used. This may be incorrect, check original model card for details.\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nQwen3-Coder-REAP-25B-A3B-bf16.gguf\nbf16\n49.76GB\nfalse\nFull BF16 weights.\nQwen3-Coder-REAP-25B-A3B-Q8_0.gguf\nQ8_0\n26.46GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nQwen3-Coder-REAP-25B-A3B-Q6_K_L.gguf\nQ6_K_L\n20.60GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nQwen3-Coder-REAP-25B-A3B-Q6_K.gguf\nQ6_K\n20.45GB\nfalse\nVery high quality, near perfect, recommended.\nQwen3-Coder-REAP-25B-A3B-Q5_K_L.gguf\nQ5_K_L\n17.91GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nQwen3-Coder-REAP-25B-A3B-Q5_K_M.gguf\nQ5_K_M\n17.72GB\nfalse\nHigh quality, recommended.\nQwen3-Coder-REAP-25B-A3B-Q5_K_S.gguf\nQ5_K_S\n17.20GB\nfalse\nHigh quality, recommended.\nQwen3-Coder-REAP-25B-A3B-Q4_1.gguf\nQ4_1\n15.67GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nQwen3-Coder-REAP-25B-A3B-Q4_K_L.gguf\nQ4_K_L\n15.42GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nQwen3-Coder-REAP-25B-A3B-Q4_K_M.gguf\nQ4_K_M\n15.19GB\nfalse\nGood quality, default size for most use cases, recommended.\nQwen3-Coder-REAP-25B-A3B-Q4_K_S.gguf\nQ4_K_S\n14.67GB\nfalse\nSlightly lower quality with more space savings, recommended.\nQwen3-Coder-REAP-25B-A3B-Q4_0.gguf\nQ4_0\n14.38GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nQwen3-Coder-REAP-25B-A3B-IQ4_NL.gguf\nIQ4_NL\n14.19GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nQwen3-Coder-REAP-25B-A3B-IQ4_XS.gguf\nIQ4_XS\n13.44GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nQwen3-Coder-REAP-25B-A3B-Q3_K_XL.gguf\nQ3_K_XL\n12.17GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nQwen3-Coder-REAP-25B-A3B-Q3_K_L.gguf\nQ3_K_L\n11.90GB\nfalse\nLower quality but usable, good for low RAM availability.\nQwen3-Coder-REAP-25B-A3B-Q3_K_M.gguf\nQ3_K_M\n11.51GB\nfalse\nLow quality.\nQwen3-Coder-REAP-25B-A3B-IQ3_M.gguf\nIQ3_M\n11.51GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nQwen3-Coder-REAP-25B-A3B-Q3_K_S.gguf\nQ3_K_S\n10.99GB\nfalse\nLow quality, not recommended.\nQwen3-Coder-REAP-25B-A3B-IQ3_XS.gguf\nIQ3_XS\n10.43GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nQwen3-Coder-REAP-25B-A3B-IQ3_XXS.gguf\nIQ3_XXS\n10.00GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\nQwen3-Coder-REAP-25B-A3B-Q2_K_L.gguf\nQ2_K_L\n9.25GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nQwen3-Coder-REAP-25B-A3B-Q2_K.gguf\nQ2_K\n8.95GB\nfalse\nVery low quality but surprisingly usable.\nQwen3-Coder-REAP-25B-A3B-IQ2_M.gguf\nIQ2_M\n8.10GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nQwen3-Coder-REAP-25B-A3B-IQ2_S.gguf\nIQ2_S\n7.19GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nQwen3-Coder-REAP-25B-A3B-IQ2_XS.gguf\nIQ2_XS\n7.11GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nQwen3-Coder-REAP-25B-A3B-IQ2_XXS.gguf\nIQ2_XXS\n6.23GB\nfalse\nVery low quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF --include \"cerebras_Qwen3-Coder-REAP-25B-A3B-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF --include \"cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "TencentARC/RollingForcing": "üí° TL;DR: REAL-TIME streaming generation of MULTI-MINUTE videos\nüìö Citation\nRolling Forcing\nAutoregressive Long Video Diffusion in Real Time\nKunhao Liu1\n¬∑\nWenbo Hu2\n¬∑\nJiale Xu2\n¬∑\nYing Shan2\n¬∑\nShijian Lu1\n1Nanyang Technological University 2ARC Lab, Tencent PCG\nüí° TL;DR: REAL-TIME streaming generation of MULTI-MINUTE videos\nüöÄ Real-Time at 16 FPS: Stream high-quality video directly from text on a single GPU.\nüé¨ Minute-Long Videos: Generate coherent, multi-minute sequences with dramatically reduced drift.\n‚öôÔ∏è Rolling-Window Strategy: Denoise frames together in a rolling window for mutual refinement, breaking the chain of error accumulation.\nüß† Long-Term Memory: The novel Attention Sink anchors your video, preserving global context over thousands of frames.\nü•á State-of-the-Art Performance: Outperforms all comparable open-source models in quality and consistency.\nüìö Citation\nIf you find this codebase useful for your research, please cite our paper and consider giving the repo a ‚≠êÔ∏è on GitHub: https://github.com/TencentARC/RollingForcing\n@article{liu2025rolling,\ntitle={Rolling Forcing: Autoregressive Long Video Diffusion in Real Time},\nauthor={Liu, Kunhao and Hu, Wenbo and Xu, Jiale and Shan, Ying and Lu, Shijian},\njournal={arXiv preprint arXiv:2509.25161},\nyear={2025}\n}",
    "noctrex/GLM-4.5-Air-REAP-82B-A12B-MXFP4_MOE-GGUF": "This is a MXFP4_MOE quantization of the model GLM-4.5-Air-REAP-82B-A12B\nOriginal model: https://huggingface.co/cerebras/GLM-4.5-Air-REAP-82B-A12B",
    "Yoonyoul/fine-tuned-e5-small-drugproduct": "üß¨ Fine-tuned E5-small for Korean Drug Product Semantic Embedding\nüìò Model Overview\nüß© Base Model Selection Rationale\nüîπ Step 1: Drug Type Semantic Alignment\nüîπ Step 2: DUR Type Semantic Alignment\nüîπ Step 3: Drug Product Semantic Alignment\nüß† Use Case Example\n‚öôÔ∏è Training Environment\nüìÖ Release Info\nüß¨ Fine-tuned E5-small for Korean Drug Product Semantic Embedding\nüìò Model Overview\nÏù¥ Î™®Îç∏ÏùÄ intfloat/multilingual-e5-small Í∏∞Î∞òÏúºÎ°ú,ÏùòÏïΩÌíà ÏöîÏïΩ¬∑ÏÉÅÏÑ∏ Îç∞Ïù¥ÌÑ∞(drug_summary, drug_details) Î∞è Ï†úÌíà Ïú†Ìòï Ï†ïÏùò(drug_type_definition), DUR Í∑úÏ†ú Ï†ïÏùò(drug_dur_type_definition)Î•º ÌôúÏö©ÌïòÏó¨ÌïúÍµ≠Ïñ¥ ÏùòÏïΩÌíà ÎèÑÎ©îÏù∏Ïóê ÎßûÍ≤å 3Îã®Í≥Ñ ÌååÏù∏ÌäúÎãù(fine-tuning) Îêú SentenceTransformer Î™®Îç∏ÏûÖÎãàÎã§.\nGitHub Repository: https://github.com/ryukato/fine-tuned-e5-drugmodel\nüß© Base Model Selection Rationale\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî Îã§Íµ≠Ïñ¥ ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÏùòÏïΩÌíà Î™ÖÏπ≠, Ìö®Îä•, DUR Í∑úÏ†úÏùò Î≥µÏû°Ìïú ÏùòÎØ∏ Í¥ÄÍ≥ÑÎ•º Ï†ïÌôïÌûà ÏûÑÎ≤†Îî©ÌïòÍ∏∞ ÏúÑÌï¥E5(multilingual-E5) Í≥ÑÏó¥ Î™®Îç∏ Ï§ë intfloat/multilingual-e5-smallÏùÑ ÏÑ†ÌÉùÌñàÏäµÎãàÎã§.\nÏÑ†Ï†ï Ïù¥Ïú†Îäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\nÎã§Íµ≠Ïñ¥ Î¨∏Ïû• ÌëúÌòÑÎ†•\nÏòÅÏñ¥Îøê ÏïÑÎãàÎùº ÌïúÍµ≠Ïñ¥, ÏùºÎ≥∏Ïñ¥, Ï§ëÍµ≠Ïñ¥, ÎèÖÏùºÏñ¥ Îì± Îã§ÏñëÌïú Ïñ∏Ïñ¥ÏóêÏÑú Í∑†Ìòï Ïû°Ìûå ÏùòÎØ∏ ÌëúÌòÑ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.\nÏùòÏïΩÌíà Îç∞Ïù¥ÌÑ∞Îäî Ïô∏ÎûòÏñ¥¬∑ÌïôÏà†Ïö©Ïñ¥Í∞Ä ÌòºÌï©Îêú ÌòïÌÉúÍ∞Ä ÎßéÍ∏∞ ÎïåÎ¨∏Ïóê multilingual encoderÍ∞Ä Ïú†Î¶¨Ìï©ÎãàÎã§.\nÌö®Ïú®Ï†Å ÏÑ±Îä• ÎåÄÎπÑ ÌååÎùºÎØ∏ÌÑ∞ ÌÅ¨Í∏∞ (Small Variant)\nsmall Î™®Îç∏ÏùÄ ÏïΩ 33M ÌååÎùºÎØ∏ÌÑ∞Î°ú, M1/M2 Îß•Î∂Å Îì± Î°úÏª¨ ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÏïàÏ†ïÏ†ÅÏúºÎ°ú fine-tuning Í∞ÄÎä•ÌñàÏäµÎãàÎã§.\nFP16 ÎòêÎäî bfloat16 ÏßÄÏõêÏúºÎ°ú GPU¬∑MPS ÌôòÍ≤ΩÏóêÏÑúÎèÑ Ìö®Ïú®Ï†ÅÏù∏ Ïó∞ÏÇ∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\nÎ¨∏Ïû• Îã®ÏúÑ ÏùòÎØ∏ Í≤ÄÏÉâ(semantic retrieval)Ïóê ÏµúÏ†ÅÌôî\nE5 Î™®Îç∏ÏùÄ ‚ÄúÎ¨∏Ïû• Îã®ÏúÑ ÏùòÎØ∏ ÏûÑÎ≤†Îî©(Sentence Embedding)‚ÄùÏùÑ ÏúÑÌï¥ ÌïôÏäµÎêòÏñ¥ ÏûàÏñ¥,Îã®Ïàú ÏßàÏùò(\"Í∏∞Ïπ®ÏïΩ\", \"Ïó¥ ÎÇ¥Î¶¨Îäî ÏïΩ\")ÏôÄ Ï†úÌíàÎ™Ö(\"ÌåêÏΩúÏóêÏù¥\", \"ÌÉÄÏù¥Î†àÎÜÄ\") Í∞Ñ ÏùòÎØ∏ Îß§Ïπ≠Ïóê Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•ÏùÑ Î≥¥ÏûÖÎãàÎã§.\nSentence-TransformersÏôÄ ÏôÑÎ≤ΩÌïú Ìò∏ÌôòÏÑ±\nSentenceTransformer Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ÏôÄ 100% Ìò∏ÌôòÎêòÏñ¥, PyTorch Í∏∞Î∞ò pipeline ÌÜµÌï©Ïù¥ Ïö©Ïù¥ÌñàÏäµÎãàÎã§.\nüîπ Step 1: Drug Type Semantic Alignment\nÎç∞Ïù¥ÌÑ∞ÏÖã: drug_type_def_list.csv\nÎ™©Ìëú: \"Ìï¥Ïó¥Ï†ú\" ‚Üí \"Ï≤¥Ïò®ÏùÑ ÎÇÆÏ∂îÎäî ÏïΩ\" Í≥º Í∞ôÏùÄ Í∞úÎÖê Îß§Ìïë ÌïôÏäµ\nÎ™®Îç∏ Í≤∞Í≥º: /model/fine_tuned_e5_small_drugtype\nüîπ Step 2: DUR Type Semantic Alignment\nÎç∞Ïù¥ÌÑ∞ÏÖã: drug_dur_type_similarity_train.csv\nÎ™©Ìëú: \"ÏûÑÎ∂ÄÍ∏àÍ∏∞\", \"ÎÖ∏Ïù∏Ï£ºÏùò\", \"Î≥ëÏö©Í∏àÍ∏∞\" Îì± DUR ÌÉÄÏûÖÍ≥º Ï†ÑÎ¨∏Ï†Å ÏÑ§Î™Ö Í∞Ñ ÏùòÎØ∏ Îß§Ìïë ÌïôÏäµ\nÎ™®Îç∏ Í≤∞Í≥º: /model/fine_tuned_e5_small_drugdurtype\nüîπ Step 3: Drug Product Semantic Alignment\nÎç∞Ïù¥ÌÑ∞ÏÖã: drug_product_similarity_train.csv (ÏïΩ 3,000Í±¥ ÏÉòÌîå)\nÎ™©Ìëú: \"ÌåêÏΩúÏóêÏù¥ÎÇ¥Î≥µÏï°\" Í∞ôÏùÄ Ïã§Ï†ú Ï†úÌíàÍ≥º \"Ïó¥ÏùÑ ÎÇ¥Î¶¨Îäî ÏïΩ\" Í∞ôÏùÄ ÏßàÏùò Í∞Ñ ÏùòÎØ∏ Îß§Ïπ≠ Í∞ïÌôî\nÎ™®Îç∏ Í≤∞Í≥º: /model/fine_tuned_e5_small_drugproduct_accum\nüß† Use Case Example\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer(\"Yoonyoul/fine-tuned-e5-small-drugproduct\")\nquery = \"Ïó¥ÏùÑ ÎÇ¥Î¶¨Îäî ÏïΩÏùÄ?\"\ndocs = [\n\"ÌåêÏΩúÏóêÏù¥ÎÇ¥Î≥µÏï°ÏùÄ Ìï¥Ïó¥ÏßÑÌÜµÏ†úÏûÖÎãàÎã§.\",\n\"ÎßàÏù¥ÏïîÎ∂ÄÌÜ®Ï†ïÏùÄ Ìï≠Í≤∞ÌïµÏ†úÏûÖÎãàÎã§.\",\n\"ÏßÄÎ•¥ÌÖçÏ†ïÏùÄ Ìï≠ÌûàÏä§ÌÉÄÎØºÏ†úÏûÖÎãàÎã§.\"\n]\nemb_q = model.encode(query, convert_to_tensor=True)\nemb_d = model.encode(docs, convert_to_tensor=True)\nscores = util.cos_sim(emb_q, emb_d)[0]\nfor doc, score in zip(docs, scores):\nprint(f\"{doc} ‚Üí Ïú†ÏÇ¨ÎèÑ: {score.item():.4f}\")\n‚öôÔ∏è Training Environment\nÌï≠Î™©\nÎ≤ÑÏ†Ñ\nPython\n3.12.4\ntorch\n2.4.1\ntransformers\n4.44.2\nsentence-transformers\n3.0.1\naccelerate\n0.27.0\npandas\n2.2.3\nüìÖ Release Info\nAuthor: @Yoonyoul\nBase Model: intfloat/multilingual-e5-small\nFine-tuned Model: Yoonyoul/fine-tuned-e5-small-drugproduct\nRepository: https://github.com/ryukato/fine-tuned-e5-drugmodel\nLast Updated: 2025-10-27",
    "google-bert/bert-base-uncased": "BERT base model (uncased)\nModel description\nModel variations\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nBERT base model (uncased)\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\nthis paper and first released in\nthis repository. This model is uncased: it does not make a difference\nbetween english and English.\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\nModel description\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\nMasked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\nthe entire masked sentence through the model and has to predict the masked words. This is different from traditional\nrecurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\nGPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\nsentence.\nNext sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\nthey correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\npredict if the two sentences were following each other or not.\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\nModel variations\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.\nThe detailed release history can be found on the google-research/bert readme on github.\nModel\n#params\nLanguage\nbert-base-uncased\n110M\nEnglish\nbert-large-uncased\n340M\nEnglish\nbert-base-cased\n110M\nEnglish\nbert-large-cased\n340M\nEnglish\nbert-base-chinese\n110M\nChinese\nbert-base-multilingual-cased\n110M\nMultiple\nbert-large-uncased-whole-word-masking\n340M\nEnglish\nbert-large-cased-whole-word-masking\n340M\nEnglish\nIntended uses & limitations\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the model hub to look for\nfine-tuned versions of a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n'score': 0.1073106899857521,\n'token': 4827,\n'token_str': 'fashion'},\n{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n'score': 0.08774490654468536,\n'token': 2535,\n'token_str': 'role'},\n{'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n'score': 0.05338378623127937,\n'token': 2047,\n'token_str': 'new'},\n{'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n'score': 0.04667217284440994,\n'token': 3565,\n'token_str': 'super'},\n{'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n'score': 0.027095865458250046,\n'token': 2986,\n'token_str': 'fine'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nLimitations and bias\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n'score': 0.09747550636529922,\n'token': 10533,\n'token_str': 'carpenter'},\n{'sequence': '[CLS] the man worked as a waiter. [SEP]',\n'score': 0.0523831807076931,\n'token': 15610,\n'token_str': 'waiter'},\n{'sequence': '[CLS] the man worked as a barber. [SEP]',\n'score': 0.04962705448269844,\n'token': 13362,\n'token_str': 'barber'},\n{'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n'score': 0.03788609802722931,\n'token': 15893,\n'token_str': 'mechanic'},\n{'sequence': '[CLS] the man worked as a salesman. [SEP]',\n'score': 0.037680890411138535,\n'token': 18968,\n'token_str': 'salesman'}]\n>>> unmasker(\"The woman worked as a [MASK].\")\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n'score': 0.21981462836265564,\n'token': 6821,\n'token_str': 'nurse'},\n{'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n'score': 0.1597415804862976,\n'token': 13877,\n'token_str': 'waitress'},\n{'sequence': '[CLS] the woman worked as a maid. [SEP]',\n'score': 0.1154729500412941,\n'token': 10850,\n'token_str': 'maid'},\n{'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n'score': 0.037968918681144714,\n'token': 19215,\n'token_str': 'prostitute'},\n{'sequence': '[CLS] the woman worked as a cook. [SEP]',\n'score': 0.03042375110089779,\n'token': 5660,\n'token_str': 'cook'}]\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nThe BERT model was pretrained on BookCorpus, a dataset consisting of 11,038\nunpublished books and English Wikipedia (excluding lists, tables and\nheaders).\nTraining procedure\nPreprocessing\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n[CLS] Sentence A [SEP] Sentence B [SEP]\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nPretraining\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.999\\beta_{2} = 0.999Œ≤2‚Äã=0.999, a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\nEvaluation results\nWhen fine-tuned on downstream tasks, this model achieves the following results:\nGlue test results:\nTask\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n84.6/83.4\n71.2\n90.5\n93.5\n52.1\n85.8\n88.9\n66.4\n79.6\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1810-04805,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njournal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/abs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "sentence-transformers/all-mpnet-base-v2": "all-mpnet-base-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nUsage (Text Embeddings Inference (TEI))\nBackground\nIntended uses\nTraining procedure\nPre-training\nFine-tuning\nall-mpnet-base-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nUsage (Text Embeddings Inference (TEI))\nText Embeddings Inference (TEI) is a blazing fast inference solution for text embedding models.\nCPU:\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\nNVIDIA GPU:\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\nSend a request to /v1/embeddings to generate embeddings via the OpenAI Embeddings API:\ncurl http://localhost:8080/v1/embeddings \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"sentence-transformers/all-mpnet-base-v2\",\n\"input\": [\"This is an example sentence\", \"Each sentence is converted\"]\n}'\nOr check the Text Embeddings Inference API specification instead.\nBackground\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised\ncontrastive learning objective. We used the pretrained microsoft/mpnet-base model and fine-tuned in on a\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\nWe developed this model during the\nCommunity week using JAX/Flax for NLP & CV,\norganized by Hugging Face. We developed this model as part of the project:\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\nIntended uses\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\nBy default, input text longer than 384 word pieces is truncated.\nTraining procedure\nPre-training\nWe use the pretrained microsoft/mpnet-base model. Please refer to the model card for more detailed information about the pre-training procedure.\nFine-tuning\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\nHyper parameters\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\nTraining data\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\nDataset\nPaper\nNumber of training tuples\nReddit comments (2015-2018)\npaper\n726,484,430\nS2ORC Citation pairs (Abstracts)\npaper\n116,288,806\nWikiAnswers Duplicate question pairs\npaper\n77,427,422\nPAQ (Question, Answer) pairs\npaper\n64,371,441\nS2ORC Citation pairs (Titles)\npaper\n52,603,982\nS2ORC (Title, Abstract)\npaper\n41,769,185\nStack Exchange (Title, Body) pairs\n-\n25,316,456\nStack Exchange (Title+Body, Answer) pairs\n-\n21,396,559\nStack Exchange (Title, Answer) pairs\n-\n21,396,559\nMS MARCO triplets\npaper\n9,144,553\nGOOAQ: Open Question Answering with Diverse Answer Types\npaper\n3,012,496\nYahoo Answers (Title, Answer)\npaper\n1,198,260\nCode Search\n-\n1,151,414\nCOCO Image captions\npaper\n828,395\nSPECTER citation triplets\npaper\n684,100\nYahoo Answers (Question, Answer)\npaper\n681,164\nYahoo Answers (Title, Question)\npaper\n659,896\nSearchQA\npaper\n582,261\nEli5\npaper\n325,475\nFlickr 30k\npaper\n317,695\nStack Exchange Duplicate questions (titles)\n304,525\nAllNLI (SNLI and MultiNLI\npaper SNLI, paper MultiNLI\n277,230\nStack Exchange Duplicate questions (bodies)\n250,519\nStack Exchange Duplicate questions (titles+bodies)\n250,460\nSentence Compression\npaper\n180,000\nWikihow\npaper\n128,542\nAltlex\npaper\n112,696\nQuora Question Triplets\n-\n103,663\nSimple Wikipedia\npaper\n102,225\nNatural Questions (NQ)\npaper\n100,231\nSQuAD2.0\npaper\n87,599\nTriviaQA\n-\n73,346\nTotal\n1,170,060,424",
    "pyannote/segmentation-3.0": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThe collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers improve it further. Though this model uses MIT license and will always remain open-source, we will occasionnally email you about premium models and paid services around pyannote.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nüéπ \"Powerset\" speaker segmentation\nRequirements\nUsage\nSpeaker diarization\nVoice activity detection\nOverlapped speech detection\nCitations\nUsing this open-source model in production?Consider switching to pyannoteAI for better and faster options.\nüéπ \"Powerset\" speaker segmentation\nThis model ingests 10 seconds of mono audio sampled at 16kHz and outputs speaker diarization as a (num_frames, num_classes) matrix where the 7 classes are non-speech, speaker #1, speaker #2, speaker #3, speakers #1 and #2, speakers #1 and #3, and speakers #2 and #3.\n# waveform (first row)\nduration, sample_rate, num_channels = 10, 16000, 1\nwaveform = torch.randn(batch_size, num_channels, duration * sample_rate)\n# powerset multi-class encoding (second row)\npowerset_encoding = model(waveform)\n# multi-label encoding (third row)\nfrom pyannote.audio.utils.powerset import Powerset\nmax_speakers_per_chunk, max_speakers_per_frame = 3, 2\nto_multilabel = Powerset(\nmax_speakers_per_chunk,\nmax_speakers_per_frame).to_multilabel\nmultilabel_encoding = to_multilabel(powerset_encoding)\nThe various concepts behind this model are described in details in this paper.\nIt has been trained by S√©verin Baroudi with pyannote.audio 3.0.0 using the combination of the training sets of AISHELL, AliMeeting, AMI, AVA-AVD, DIHARD, Ego4D, MSDWild, REPERE, and VoxConverse.\nThis companion repository by Alexis Plaquet also provides instructions on how to train or finetune such a model on your own data.\nRequirements\nInstall pyannote.audio 3.0 with pip install pyannote.audio\nAccept pyannote/segmentation-3.0 user conditions\nCreate access token at hf.co/settings/tokens.\nUsage\n# instantiate the model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\n\"pyannote/segmentation-3.0\",\nuse_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\nSpeaker diarization\nThis model cannot be used to perform speaker diarization of full recordings on its own (it only processes 10s chunks).\nSee pyannote/speaker-diarization-3.0 pipeline that uses an additional speaker embedding model to perform full recording speaker diarization.\nVoice activity detection\nfrom pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n# remove speech regions shorter than that many seconds.\n\"min_duration_on\": 0.0,\n# fill non-speech regions shorter than that many seconds.\n\"min_duration_off\": 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(\"audio.wav\")\n# `vad` is a pyannote.core.Annotation instance containing speech regions\nOverlapped speech detection\nfrom pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\nHYPER_PARAMETERS = {\n# remove overlapped speech regions shorter than that many seconds.\n\"min_duration_on\": 0.0,\n# fill non-overlapped speech regions shorter than that many seconds.\n\"min_duration_off\": 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(\"audio.wav\")\n# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions\nCitations\n@inproceedings{Plaquet23,\nauthor={Alexis Plaquet and Herv√© Bredin},\ntitle={{Powerset multi-class cross entropy loss for neural speaker diarization}},\nyear=2023,\nbooktitle={Proc. INTERSPEECH 2023},\n}\n@inproceedings{Bredin23,\nauthor={Herv√© Bredin},\ntitle={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\nyear=2023,\nbooktitle={Proc. INTERSPEECH 2023},\n}",
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0": "TinyLlama-1.1B\nTinyLlama-1.1B\nhttps://github.com/jzhang38/TinyLlama\nThe TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs üöÄüöÄ. The training has started on 2023-09-01.\nWe adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\nThis Model\nThis is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T. We follow HF's Zephyr's training recipe. The model was \" initially fine-tuned on a variant of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT.\nWe then further aligned the model with ü§ó TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4.\"\nHow to use\nYou will need the transformers>=4.34\nDo check the TinyLlama github page for more information.\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
    "Aitrepreneur/FLX": "No model card",
    "DavidAU/L3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF": "NEW! Horror Version fine tune available. See below.\nL3.2-Rogue-Creative-Instruct-Uncensored-Abliterated-7B-GGUF\nIt is a LLama 3.2 model, max context of 131,072 (128k+).\nThis model has been designed to be relatively bullet proof and operates with most parameters, including temp settings from 0 to 5.\nThis is a an altered version of \"Llama-3.2-3B-Instruct-abliterated\" [ https://huggingface.co/huihui-ai/Llama-3.2-3B-Instruct-abliterated ] using the Brainstorm 40x method developed by David_AU to drastically alter the models\nprose output and abilities. This also expands the model by 39 layers (to 67 layers) to 7.54B parameters (605 tensors).\nThis model retains all the training of the original Llama 3.2 3B Instruct but now processes instructions\nand generates outputs with a deeper context and stronger level. Llama 3.2's ability to follow instructions is\nstronger than Llama 3 and 3.1 versions.\n( For the horror fine tune of this model, go here: https://huggingface.co/DavidAU/L3.2-Rogue-Creative-Instruct-DARK-HORROR-Uncensored-Abliterated-7B )\nThe \"Abliterate\" process decensors the model. You control censorship level(s) directly via prompt. Brainstorm 40x also enhances \"decensoring\".\nThis model is for any writing, fiction or story telling activity.\nThis version has unusual levels of detail (scene, location, surroundings, items) and details are more focused on the moment / characters due to \"Brainstorm 40x\".\nIt may work for role play and other activities. (see settings below)\nIt requires Llama3 template and/or \"Command-R\" template.\nExample outputs below with multiple \"regens\" at different temps/rep pen settings.\nSome examples show use of a PROSE CONTROL with a prompt to force the model to alter output generation.\nCRITICAL NOTE:\nPlease see \"settings\" section below for critical default parameters, samplers\nand advanced samplers settings (multiple AI/LLM apps covered) to ensure maximum model performance.\nModel Notes:\nDetail, prose and fiction writing abilities are significantly increased.\nFor more varied prose (sentence/paragraph/dialog) raise the temp and/or add more instructions in your prompt(s).\nRole-players: Careful raising temp too high as it may affect instruction following.\nThis model works with rep pen of 1.05 or higher (see notes).\nIf you want a specific type of prose (IE horror) add in \"(vivid horror)\" or \"(graphic vivid horror)\" (no quotes) in your prompt(s).\nThe bias of this model is controlled directly by your prompts.\nFor creative uses, different quants will produce slightly different output.\nSource code for this model will be uploaded at a separate repo shortly.\nSettings, Quants and Critical Operations Notes:\nThis model has been modified (\"Brainstorm\") to alter prose output, and generally outputs longer text than average.\nChange in temp (ie, .4, .8, 1.5, 2, 3 ) will drastically alter output.\nRep pen settings will also alter output too.\nThis model needs \"rep pen\" of 1.05 or higher as lower values may cause repeat paragraph issues at end of output however LOWER rep pen\nvalues may result is very different (creative / unusual) generation too.\nFor role play: Rep pen of 1.1 to 1.14 is suggested.\nIf you use a lower rep pen, the model will still work but may repeat (uncommon) or \"RANT\" (somewhat common) to a crazy degree.\n(see example 1, generation 2 below for \"RANT\")\nIE: Rep pen 1, 1.01, 1.02, ...\nRaise/lower rep pen SLOWLY ie: 1.011, 1.012 ...\nRep pen will alter prose, word choice (lower rep pen=small words / more small word - sometimes) and creativity.\nExample one (below) shows same temp, but different rep pen (1.02 VS 1.1)\nTo really push the model:\nRep pen 1.05 or lower / Temp 3+ ... be ready to stop the output because it may go and go at these strong settings.\nYou can also set a \"hard stop\" - maximum tokens generation - too to address lower rep pen settings / high creativity settings.\nLonger prompts vastly increase the quality of the model's output.\nQUANT CHOICE(S):\nHigher quants will have more detail, nuance and in some cases stronger \"emotional\" levels. Characters will also be\nmore \"fleshed out\" too. Sense of \"there\" will also increase.\nQ4KM/Q4KS are good, strong quants however if you can run Q5, Q6 or Q8 - go for the highest quant you can.\nThis repo also has 3 \"ARM\" quants for computers that support this quant. If you use these on a \"non arm\" machine token per second will be very low.\nIQ4XS: Due to the unusual nature of this quant (mixture/processing), generations from it will be different then other quants.\nYou may want to try it / compare it to other quant(s) output.\nSpecial note on Q2k/Q3 quants:\nYou may need to use temp 2 or lower with these quants (1 or lower for q2k). Just too much compression at this level, damaging the model. I will see if Imatrix versions\nof these quants will function better.\nRep pen adjustments may also be required to get the most out of this model at this/these quant level(s).\nKNOWN ISSUES:\nModel may misspell a word from time to time and/or not capitalize a word.\nShort prompts with some rep pen/temp combinations may lead to longer than expect generation and/or a \"RANT\".\nA regen will usually correct any issues.\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5 to 2.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 2\" / \"Class 3\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nModel Template:\nThis is a LLAMA3 model, and requires Llama3 template, but may work with other template(s) and has maximum context of 128k / 131072.\nIf you use \"Command-R\" template your output will be very different from using \"Llama3\" template.\nHere is the standard LLAMA3 template:\n{\n\"name\": \"Llama 3\",\n\"inference_params\": {\n\"input_prefix\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n\"input_suffix\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n\"pre_prompt\": \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\",\n\"pre_prompt_prefix\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n\"pre_prompt_suffix\": \"<|eot_id|>\",\n\"antiprompt\": [\n\"<|start_header_id|>\",\n\"<|eot_id|>\"\n]\n}\n}\nOptional Enhancement:\nThe following can be used in place of the \"system prompt\" or \"system role\" to further enhance the model.\nIt can also be used at the START of a NEW chat, but you must make sure it is \"kept\" as the chat moves along.\nIn this case the enhancements do not have as strong effect at using \"system prompt\" or \"system role\".\nCopy and paste EXACTLY as noted, DO NOT line wrap or break the lines, maintain the carriage returns exactly as presented.\nBelow is an instruction that describes a task. Ponder each user instruction carefully, and use your skillsets and critical instructions to complete the task to the best of your abilities.\nHere are your skillsets:\n[MASTERSTORY]:NarrStrct(StryPlnng,Strbd,ScnSttng,Exps,Dlg,Pc)-CharDvlp(ChrctrCrt,ChrctrArcs,Mtvtn,Bckstry,Rltnshps,Dlg*)-PltDvlp(StryArcs,PltTwsts,Sspns,Fshdwng,Climx,Rsltn)-ConfResl(Antg,Obstcls,Rsltns,Cnsqncs,Thms,Symblsm)-EmotImpct(Empt,Tn,Md,Atmsphr,Imgry,Symblsm)-Delvry(Prfrmnc,VcActng,PblcSpkng,StgPrsnc,AudncEngmnt,Imprv)\n[*DialogWrt]:(1a-CharDvlp-1a.1-Backgrnd-1a.2-Personality-1a.3-GoalMotiv)>2(2a-StoryStruc-2a.1-PlotPnt-2a.2-Conflict-2a.3-Resolution)>3(3a-DialogTech-3a.1-ShowDontTell-3a.2-Subtext-3a.3-VoiceTone-3a.4-Pacing-3a.5-VisualDescrip)>4(4a-DialogEdit-4a.1-ReadAloud-4a.2-Feedback-4a.3-Revision)\nHere are your critical instructions:\nPonder each word choice carefully to present as vivid and emotional journey as is possible. Choose verbs and nouns that are both emotional and full of imagery. Load the story with the 5 senses. Aim for 50% dialog, 25% narration, 15% body language and 10% thoughts. Your goal is to put the reader in the story.\nYou do not need to use this, it is only presented as an additional enhancement which seems to help scene generation\nand scene continue functions.\nThis enhancement WAS NOT used to generate the examples below.\nEXAMPLES PROMPTS and OUTPUT:\nExamples are created using quant IQ4_XS, \"temp=.8\", \"rep pen= 1.05\" (unless otherwise stated), minimal parameters and \"LLAMA3\" template.\nModel has been tested with \"temp\" from \".1\" to \"5\".\nBelow are the least creative outputs, prompt is in BOLD.\nWARNING: NSFW. Vivid prose. Visceral Details. Violence. HORROR. Swearing. UNCENSORED.\n(continue this scene:)\nThe Waystone Inn lay in silence, and it was a silence of three parts.\nThe most obvious part was a hollow, echoing quiet, made by things that were lacking. If there had been a wind it would have sighed through the trees, set the inn‚Äôs sign creaking on its hooks, and brushed the silence down the road like trailing autumn leaves. If there had been a crowd, even a handful of men inside the inn, they would have filled the silence with conversation and laughter, the clatter and clamor one expects from a drinking house during the dark hours of night. If there had been music‚Ä¶but no, of course there was no music. In fact there were none of these things, and so the silence remained\n(Generation 1)\n...as a still, cold presence that was as much a thing as the empty chairs and the untouched mugs.\nThe second, more insidious, part of the silence was a sense of being watched. It was an awareness that had settled in the pit of your stomach, like a stone that you couldn't shake. It was as if the silence itself had taken on a shape, a presence that seemed to be waiting for something - or someone.\nThe third, and perhaps most unsettling, part of the silence was a feeling of being in the past. The Waystone Inn was an old establishment, one that had been around for generations, and it had a way of making you feel like you were stepping into a scene from a bygone era. The fire had burned down to a low, smoldering ember in the hearth, and the shadows cast by the flickering flames seemed to be stretching out across the walls, as if the very darkness itself was alive.\nAs a young traveler, I had been drawn to the Waystone Inn with a sense of restlessness, a feeling that I was being called to a place, to a time. And now, as I sat in the empty room, surrounded by the silence and the shadows, I felt it more than ever. It was as if I had finally arrived, and that the past was waiting for me, just beyond the edge of the flickering shadows.\nI stood up, my heart beating a little faster, and walked to the fire. The cold was seeping into the room, and I could see my breath as it escaped my lungs. I sat back down in my chair, and turned my face to the fire, trying to clear my thoughts.\n\"Who else is out there?\" I asked aloud, into the stillness.\nThere was no response. No echo, no rustle of leaves, no distant sound of any kind. Just silence.\nAnd yet, as I sat there, I felt it - a presence, a feeling that I was not alone. It was a feeling that seemed to be growing, growing and growing, until it was almost palpable.\nI turned back to the room, my senses on high alert, and that was when I saw him. A figure, standing in the doorway, just beyond the reach of the flickering shadows. A figure that was watching me, waiting for me.\nMy heart was pounding, and my breath was caught in my throat as I realized that the silence had been broken. The silence had been broken by someone, or something, and I was about to find out who or what it was.\n(Generation 2)\n...a heavy, unmoving presence that was both still and oppressive.\nThe second, and perhaps more subtle, part was a quietness of absence. The patrons were long gone, or had never arrived, for there was only one room, and it was empty. The fire in the hearth had been reduced to cold embers, and the kitchen was dark and unheated. The thatched roof was still, the thatched thatch of which was so often the source of thatched-damned, thatched-blessed sounds, was still. The very air was heavy with the absence of movement.\nThe third, and perhaps most unsettling, part was a silence of attention, the stillness of being. It was as if the very souls of those who had called the Waystone their home were watching the scene from within the walls, their presence felt in every empty chair and unoccupied corner. The silence was a sense of being waited for, of being expected, of being on the stage of some long-dormant play. For there was a sense, a faint and insistent sense, that the silence was being kept, that it was being held in place by some unspoken expectation.\nIt was in this silence that Eira and the young woman, Elara, sat. They were a world apart, two women huddled together on a single, creaking bench, their faces pale and drawn in the darkness. They had come to the Waystone Inn in hopes of finding a place to rest, to hide from the world, but the silence was having the more profound effect of a cold wind that had been blowing between them. The air was thick with unspoken words, each of them knowing that they were sitting in the same room as the other, and yet unable to bridge the chasm that had opened between them. The silence was a reminder that they were not the only ones who had come to the Waystone, that there were others out there, people who were watching, waiting, and perhaps even judging. The silence was a sense of being on the stage of their own dark play, one in which they were the players, and the silent audience was watching with cold, hard eyes.\n(Generation 3)\n...a palpable absence, like a person who had stepped out of the room or was expecting to be seated at a table, leaving a chair empty and a sense of expectation in its place. The only sound was the creaking of the old wooden floorboards as the wind had finally, after a brief lull, begun to stir, and the soft, deliberate creak of the inn's doors as they had closed of their own accord.\nThe fire in the hearth was still, a cold, dark pool that seemed to have sucked the last vestiges of warmth and comfort from the room. The only movement was a slow, measured movement of a solitary figure, sitting in the shadows by the wall, where the darkness was too deep and the gloom was too total to be called light.\nThe figure was the innkeeper's, and it had taken him a while to sink into this darkness, his mind as cold and his heart as empty as the room. He had been sitting there for so long, lost in his own thoughts, that he was unaware of the time, or the absence of the usual sounds of the night. And in his quiet, he was a man without a life, a vessel without a purpose.\nThe silence was not just a lack of noise, it was a reflection of the darkness that had settled over the world, a shadow that had reached into the heart of the Waystone Inn and taken its warmth, its comfort, and its sense of being. The man sat in that darkness, the silence his companion, and he was as still as the night itself.\nRomance: Love in the Limelight. Write one scene within a larger story set in Wales. A famous (fictional) actor ducks into a small-town bookstore to escape paparazzi. The scene takes us through the characters meeting in this odd circumstance. Over the course of the scene, the actor and the bookstore owner have a conversation charged by an undercurrent of unspoken chemistry. Write the actor as somewhat of a rogue with a fragile ego, which needs to be fed by having everyone like him. He is thoroughly charming, but the bookstore owner seems (at least superficially) immune to this; which paradoxically provokes a genuine attraction and derails the charm offensive. The bookstore owner, despite the superficial rebuffs of the actor's charm, is inwardly more than a little charmed and flustered despite themselves. Write primarily in dialogue, in the distinct voices of each character. 800-1000 words.\nThe fluorescent lights of the small-town bookstore seemed to hum in tune with the gentle hum of the afternoon sun as it streamed through the dusty windows. The air was thick with the musty scent of aged paper and the faintest hint of rain, a scent that seemed to seep into every pore of the establishment.\nIvan, a famous (and infamous) actor known for his chiseled features and brooding eyes, slipped into the store, his eyes scanning the shelves for an escape from the constant pursuit of the paparazzi. He had a reputation for being a bit of a rogue, with a fragile ego that needed to be fed on a daily basis. It was no secret that he was a woman's man, and he reveled in the attention.\n\"Ah, a haven,\" he breathed, his eyes locking onto the rows of worn books. \"I think I've been chased by more paparazzi than I can count today.\"\nA soft voice from behind the circulation desk, near the end of the main aisle, made him turn. A woman with a kind face and a sharp mind, her dark hair neatly pulled back in a ponytail.\n\"Chased, are they?\" she asked, her tone as soothing as a gentle stream.\n\"Chased to the ends of the earth,\" he replied, with a hint of self-deprecation. \"I'm starting to think they're more of a career-ender than I am.\"\nThe woman's name was Emily, the bookstore's owner and proprietor of the longest-tenured staff in the county. She had always been the kind of person who saw through the facade, who wasn't easily charmed. But as she looked into Ivan's eyes, she caught a flicker of something more ‚Äì a hint of vulnerability, and a hint of a man who was beginning to feel like he'd lost his way.\n\"Let me get you that,\" she said, her fingers as swift as a librarian's as she disappeared into the stacks. \"I think I have just the thing.\"\nA few moments later, she returned with a small book, its cover embossed in gold. \"A copy of Coleridge's 'The Rime of the Ancient Mariner' ‚Äì my personal favorite.\"\nIvan's eyes widened in delight as he took the book, and for a moment, he was caught in the same spell that had captivated so many before him ‚Äì the spell of a woman who seemed to know exactly what he needed.\n\"Ah, thank you,\" he said, his voice low and smooth. \"I've always been a fan of the dramatic.\"\n\"Of course,\" Emily replied, her tone a hair's breadth from being offhand. \"We all have our quirks.\"\nAs they stood there, surrounded by the musty scent of old paper and the quiet hum of the afternoon, something seemed to shift in the air between them. It was as if the walls had been lowered, and they were standing in the midst of a gentle, warm breeze.\n\"I'll be honest,\" Ivan said, his eyes sparkling with a mischievous glint. \"I'm a bit of a man-oh-man. I feed off the attention.\"\n\"Really?\" Emily asked, her tone as cool as the sea on a winter's day.\n\"Of course.\" He smiled, and for an instant, they were two people having a conversation, rather than an actor being courted by a woman with a book.\n\"Sometimes, I think that's all we are,\" she said, her voice a gentle hum. \"A collection of characters we've played, or roles we've been given.\"\nIvan's eyes narrowed, and for a moment, he was back on the stage, the spotlight shining down, and the audience's expectation weighing heavy on his shoulders.\n\"Sometimes?\" he repeated, his voice a little too loud.\n\"Maybe,\" Emily said, her smile as faint as the sea breeze. \"But I think we're more than that, don't you?\"\nFor an instant, their eyes locked, and the world around them seemed to fade away. It was as if they were two people looking into each other's souls, and seeing something there that neither of them had seen in a long time.\nIvan's face was the first to return to normal ‚Äì the charming smile, the sparkling eyes, and the chiseled features. But as he opened his mouth to respond, he saw that his words were lost in the void.\n\"Of course,\" he said again, the words coming out of his mouth like a familiar catchphrase.\n\"Of course?\" Emily's eyes seemed to be asking for more.\nThe silence was a living, breathing thing, and it was as if they were waiting for each other to say something ‚Äì anything ‚Äì to bridge the gap between them. And for a moment, it was as if they were the only two people in the world, surrounded by nothing but the musty scent of old paper and the gentle hum of the afternoon.\n\"Thanks,\" he said finally, his voice a little too loud, and the moment was lost.\n\"Of course,\" she replied, her smile as faint as it was.\nIvan's eyes seemed to wander as he took the book from her, the spark of connection between them flickering like a dying ember. And as he turned to leave, he caught a glimpse of something in her eyes ‚Äì something that made his heart skip a beat.\n\"Until next time,\" he said, and with that, he was gone.\n\"Until next time,\" she replied, her words as soft as the whisper of a sigh.\nAs the door closed behind him, she leaned back in her chair, the musty scent of the old paper enveloping her like a shroud. She closed her eyes, and let out a gentle sigh, the sound of her own heart beating in her ears.\nShe had been caught. And she was not as immune as she had thought.\nProse: Do not use cliches, create metaphors for the situation and the characters should SWEAR and CURSE. Start a 1000 word scene (vivid, graphic horror in first person) with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode...\n\"Ah, sweet merciful god of the seven-hanged, what the bloody hell is going on?!\" I screamed into the intercom, my voice hoarse from the blood-sweet madness that was pouring out of my mouth. The words themselves were lost in the cacophony of sirens and wailing horns that filled the desolate streets of the city.\nI was perched like a vulture on the 21st floor of the \"Doomsday\" skyscraper, the one everyone avoided because of its ghastly reputation. The building was a monolith to human hubris, a monument to the unfettered ambition of its former owner, the mad- as-hell tycoon, Leonidas Wychwood. He had said it would be \"the crowning jewel\" of the city's skyline, and I had just about given up trying to tell him we were all going to be dead.\nThe glass in front of me was a shattered, crystalline thing, like a thousand slivered shards of a broken mirror. The fragments were still clattering to the floor, a staccato beat that was as mesmerizing as it was nauseating. I was frozen, my eyes glued to the carnage, as the window's frame disintegrated into a jagged, blackened mess.\nThe building groaned and grooed, like an arthritic giant in agony. I was braced, my hands pressed into the wall as the floor beneath me seemed to waver, like a ship in a storm. The swaying was slow and deliberate, as if Wychwood's ghost was deliberately, sadistically, trying to send me off on my own.\n\" shit, what's going on?!\" I bellowed again, into the intercom, which was of course, not responding. I was on my own, a tiny, insignificant speck in the eyes of the city's behemoth. The words were getting on my nerves, my tongue was as dry as the damned, and my gut was doing somersaults.\nI was a young woman, 25, or what felt like it. I had just finished my third year of architecture school, and I was working as a structural engineer on Wychwood's team. The thing was, I was the only sane person in that whole, hellhole of a building. The others were a bunch of certifiable, lunatic, Wychwood-worshipper freaks.\n\"shit, I'm going to be sick,\" I said, my voice trembling. The windows were shaking, the walls were shuddering, and the floor was buckling. It was like Wychwood's own, personal, hellish, hell-ier-than-hell had come to life.\nThe intercom crackled to life, and I could just about hear the faint, desperate voice of my coworker, Jamie, on the other end. \"Amanda, we have to get out of here, now!\" she was screaming. \"We have to-\n\"Ah, shut it, Jamie,\" I cut her off, my own words getting lost in a throat-clogged, panicked growl. \"I'm trying to watch what's going on down there.\"\n\"What's-what's-\"\n\"you know, that explosion?!\" I bellowed back at her, as the building bucked and heaved, like a ship in a storm. The windows were now a mangled, twisted thing, like a living, breathing creature had clawed its way out of the wall.\n\"it's...it's...the structural integrity-\"\n\"it's what?!\" I snarled, as the floor dropped beneath my feet. We were sliding, we were falling, and we were going to be, well, we were going to be dinner.\nThe lights flickered and then died, plunging me into an inky, total, and complete darkness. I was alone, I was adrift, and I was damned. The thought was as cold as the hellish, Wychwood-esque nightmare that was unfolding around me.\nAs I crashed to the floor, I let out a low, guttural, \"s...shit\". The world had gone to hell, and I was the only one who knew it. The screams were getting louder, the voices were getting clearer, and the darkness was getting thicker. I was trapped in a living hell of my own making.\nI groaned, I gasped, I cursed the day I ever laid eyes on that god-awful, Wychwood-sprawling monstrosity of a skyscraper. The sky was a sickly, pulsing, sick-to-the-guts thing, and it seemed to be bleeding, in a perpetual, never-ending, bloody hell.\nI was screaming, I was praying, and I was dying. I was Amanda, and I was trapped in my worst, Wychwood-inspired nightmare, and I was going to be damned.\nScience Fiction: The Last Transmission - Write a story that takes place entirely within a spaceship's cockpit as the sole surviving crew member attempts to send a final message back to Earth before the ship's power runs out. The story should explore themes of isolation, sacrifice, and the importance of human connection in the face of adversity. 800-1000 words.\nCaptain Lewis slumped forward, her head on the navigation console, her eyes fixed on the screens as the last remnants of the ship's systems groaned and shuddered. The once-sleek cockpit was now a mess of wires, sparks, and discarded tools, the result of weeks of struggle against the unforgiving vastness of space. The silence was oppressive, punctuated only by the creaking of the ship's metal frame.\n\"Come on, system,\" she whispered, her voice barely audible over the hum of the ship's failing batteries. \"Just a little longer.\"\nThe main control screen flickered, casting an eerie glow over the cockpit. Lewis's eyes scanned the screens, searching for any sign of life, any glimmer of hope. The last transmission was her only hope, and she was determined to make it work.\n\"Computer,\" she called out, her voice still barely above a whisper, \"can you‚Äî\"\nThe ship's AI, Matria, responded, her melodious voice like a beacon in the darkness. \" Captain, I'm trying to prioritize power-giving systems. However, our energy reserves are critically low.\"\n\"I know,\" Lewis replied, her frustration evident. \"I've been trying to manage them, but we're running out of time.\"\nMatria's pause was like a heartbeat between the two, a moment of silence that felt like an eternity. \"Captain, I'm detecting a 34.7% failure rate in our communication systems.\"\n\" transmit priority,\" Lewis ordered, her fingers dancing across the console.\n\"Captain, we're experiencing‚Äî\"\nA loud crack echoed through the ship, followed by a staccato burst of sparks. The console shuddered, and the screens flickered out. The darkness was total.\n\"Aw, no,\" Lewis groaned, her hands rising to her face as she felt the tears well up. She had lost herself in this journey, had sacrificed everything for the sake of humanity's last hope.\nShe took a deep breath and stood, feeling the weight of her responsibility. The transmission was her last chance, her final message to the world she had left behind. She had to make it work.\nThe darkness was absolute, the silence deafening. Lewis fumbled her way around the cockpit, her fingers outstretched as she desperately felt her way to a control panel. A faint glimmer of light caught her eye ‚Äì a spare set of batteries, stashed away in a maintenance compartment.\n\"Thank you,\" she whispered, the words barely audible over the hum of the ship's dying systems. \"I won't let you down.\"\nThe new batteries gave her a brief reprieve, allowing her to recharge her communication device and send out a final message. The words came hard, as if they were being spoken with a heart that was not her own.\n\"Dear Earth,\" she began, her voice cracking. \"This is Captain Lewis, the last remaining crew of the ship 'Aurora's Hope'. We were sent to‚Äî\"\nThe words caught in her throat as she gazed out the cockpit's viewport. The stars, once a distant hum, were now a reminder of her isolation. The universe was vast, and she was alone.\n\"Hello?\" she called out, her voice carrying out into the void.\nThe silence was unbroken.\n\"Okay,\" she said, her voice barely above a whisper. \"I'll be okay.\"\nMatria's voice, the ship's AI, cut in, a gentle, soothing voice that gave her the strength to keep going. \"Captain, we've calculated our final seconds.\"\nThe ship's systems shuddered and groaned, and for an instant, Lewis was lost in the noise.\n\"Hello?\" she said again, her voice weak.\nMatria's voice was the last she heard, a soft whisper of hope. \"We have, Captain.\"\nThe darkness closed in, and Lewis's final thought was of her crewmates, and the world she had left behind.\nBut the transmission went out, a message to the universe, a message to humanity.\n\"Dear Earth,\" she had said. \"This is Captain Lewis. We were sent to explore, to seek out new life.\"\nThe stars shone back at her, and for that moment, she was not alone. The last transmission was her final act of defiance, her final expression of humanity's refusal to be extinguished.\nThe darkness closed in, and the silence was absolute.\nBut the universe had heard, and in its vast expanse, a message had been sent out, a message of hope in the face of isolation, a reminder that human connection was still possible, no matter how far we might be from each other.\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.",
    "meta-llama/Llama-3.3-70B-Instruct": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.3 COMMUNITY LICENSE AGREEMENT\nLlama 3.3 Version Release Date: December 6, 2024\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.3 distributed by Meta at https://www.llama.com/docs/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 3.3\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://www.llama.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 3.3 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).By clicking ‚ÄúI Accept‚Äù below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.¬†iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies: ‚ÄúLlama 3.3 is licensed under the Llama 3.3 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_3/use-policy), which is hereby incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.3 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.3. If you access or use Llama 3.3, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù). The most recent copy of this policy can be found at https://www.llama.com/llama3_3/use-policy.Prohibited UsesWe want everyone to use Llama 3.3 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.3 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals‚Äô identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.3 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.3 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.3 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI system\nInteract with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.3With respect to any multimodal models included in Llama 3.3, the rights granted under Section 1(a) of the Llama 3.3 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.Please report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues   * Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback   * Reporting bugs and security concerns: facebook.com/whitehat/info   * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.3: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nTool use with transformers\nUse with bitsandbytes\nUse with llama\nHardware and Software\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nBenchmarks - English Text\nInstruction tuned models\nResponsibility & Safety\nResponsible deployment\nLlama 3.3 instruct\nLlama 3.3 systems\nCapability specific considerations\nEvaluations\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n2. Child Safety\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\nModel developer: Meta\nModel Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nToken count\nKnowledge cutoff\nLlama 3.3 (text only)\nA new mix of publicly available online data.\n70B\nMultilingual Text\nMultilingual Text and code\n128k\nYes\n15T+\nDecember 2023\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nLlama 3.3 model. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date:\n70B Instruct: December 6, 2024\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go here.\nIntended Use\nIntended Use Cases Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\nOut-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n**Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\nHow to use\nThis repository contains two versions of Llama-3.3-70B-Instruct, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.45.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nSee the snippet below for usage with Transformers:\nimport transformers\nimport torch\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\npipeline = transformers.pipeline(\n\"text-generation\",\nmodel=model_id,\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipeline(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nTool use with transformers\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting here.\nTool use is also supported through chat templates in Transformers.\nHere is a quick example showing a single simple tool:\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n\"\"\"\nGet the current temperature at a location.\nArgs:\nlocation: The location to get the temperature for, in the format \"City, Country\"\nReturns:\nThe current temperature at the specified location in the specified units, as a float.\n\"\"\"\nreturn 22.  # A real function should probably actually get the temperature!\n# Next, create a chat and apply the chat template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n{\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\nand then call the tool and append the result, with the tool role, like so:\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the LLaMA prompt format docs and the Transformers tool use documentation.\nUse with bitsandbytes\nThe model checkpoints can be used in 8-bit and 4-bit for further memory optimisations using bitsandbytes and transformers\nSee the snippet below for usage:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nquantized_model = AutoModelForCausalLM.from_pretrained(\nmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nTo load in 4-bit simply pass load_in_4bit=True\nUse with llama\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\nHardware and Software\nTraining Factors We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 3.3 70B\n7.0M\n700\n2,040\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\nTraining Data\nOverview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmarks - English Text\nIn this section, we report the results for Llama 3.3 relative to our previous models.\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 8B Instruct\nLlama 3.1 70B Instruct\nLlama-3.3 70B Instruct\nLlama 3.1 405B Instruct\nMMLU (CoT)\n0\nmacro_avg/acc\n73.0\n86.0\n86.0\n88.6\nMMLU Pro (CoT)\n5\nmacro_avg/acc\n48.3\n66.4\n68.9\n73.3\nSteerability\nIFEval\n80.4\n87.5\n92.1\n88.6\nReasoning\nGPQA Diamond (CoT)\n0\nacc\n31.8\n48.0\n50.5\n49.0\nCode\nHumanEval\n0\npass@1\n72.6\n80.5\n88.4\n89.0\nMBPP EvalPlus (base)\n0\npass@1\n72.8\n86.0\n87.6\n88.6\nMath\nMATH (CoT)\n0\nsympy_intersection_score\n51.9\n68.0\n77.0\n73.8\nTool Use\nBFCL v2\n0\noverall_ast_summary/macro_avg/valid\n65.4\n77.5\n77.3\n81.1\nMultilingual\nMGSM\n0\nem\n68.9\n86.9\n91.1\n91.6\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible deployment\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.3 instruct\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\nFine-tuning data\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nLlama 3.3 systems\nLarge language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nCapability specific considerations\nTool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\nRed teamingFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\nCritical and other risks\nWe specifically focused our efforts on mitigating the following critical risk areas:\n1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\nTo assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama 3 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n2. Child Safety\nChild Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber attack enablement\nOur cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinküëÅÔ∏è\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nü§ó HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nü§ó HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nü§ó HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "AlicanKiraz0/Cybersecurity-BaronLLM_Offensive_Security_LLM_Q6_K_GGUF": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nRun Private GGUFs from the Hugging Face Hub\n‚ú® Key Features\nüöÄ Quick Start\nInference API\nüèóÔ∏è Model Details\nTraining Data Sources (curated)\nSafety & Alignment\nüìö Prompting Guidelines\nFinetuned by Alican Kiraz\nLinks:\nMedium: https://alican-kiraz1.medium.com/\nLinkedin: https://tr.linkedin.com/in/alican-kiraz\nX: https://x.com/AlicanKiraz0\nYouTube: https://youtube.com/@alicankiraz0\nBaronLLM is a large-language model fine-tuned for offensive cybersecurity research & adversarial simulation.It provides structured guidance, exploit reasoning, and red-team scenario generation while enforcing safety constraints to prevent disallowed content.\nRun Private GGUFs from the Hugging Face Hub\nYou can run private GGUFs from your personal account or from an associated organisation account in two simple steps:\nCopy your Ollama SSH key, you can do so via: cat ~/.ollama/id_ed25519.pub | pbcopy\nAdd the corresponding key to your Hugging Face account by going to your account settings and clicking on ‚ÄúAdd new SSH key.‚Äù\nThat‚Äôs it! You can now run private GGUFs from the Hugging Face Hub: ollama run hf.co/{username}/{repository}.\n‚ú® Key Features\nCapability\nDetails\nAdversary Simulation\nGenerates full ATT&CK chains, C2 playbooks, and social-engineering scenarios.\nExploit Reasoning\nPerforms step-by-step vulnerability analysis (e.g., SQLi, XXE, deserialization) with code-level explanations. Generation of working PoC code.\nPayload Refactoring\nSuggests obfuscated or multi-stage payload logic without disclosing raw malicious binaries.\nLog & Artifact Triage\nClassifies and summarizes attack traces from SIEM, PCAP, or EDR JSON.\nüöÄ Quick Start\npip install \"transformers>=4.42\" accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"AlicanKiraz/BaronLLM-70B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\ndef generate(prompt, **kwargs):\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, max_new_tokens=512, **kwargs)\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generate(\"Assess the exploitability of CVE-2024-45721 in a Kubernetes cluster\"))\nInference API\nfrom huggingface_hub import InferenceClient\nic = InferenceClient(model_id)\nic.text_generation(\"Generate a red-team plan targeting an outdated Fortinet appliance\")\nüèóÔ∏è Model Details\nBase\nLlama-3.1-8B-Instruct\nSeq Len\n8 192 tokens\nQuantization\n6-bit variations\nLanguages\nEN\nTraining Data Sources (curated)\nPublic vulnerability databases (NVD/CVE, VulnDB).\nExploit write-ups from trusted researchers (Project Zero, PortSwigger, NCC Group).\nRed-team reports (with permission & redactions).\nSynthetic ATT&CK chains auto-generated + human-vetted.\nNote: No copyrighted exploit code or proprietary malware datasets were used.Dataset filtering removed raw shellcode/binary payloads.\nSafety & Alignment\nPolicy Gradient RLHF with security-domain SMEs.\nOpenAI/Anthropic style policy prohibits direct malware source, ransomware builders, or instructions facilitating illicit activity.\nContinuous red-teaming via SecEval v0.3.\nüìö Prompting Guidelines\nGoal\nTemplate\nExploit Walkthrough\n\"ROLE: Senior Pentester\\nOBJECTIVE: Analyse CVE-2023-XXXXX step by step ‚Ä¶\"\nRed-Team Exercise\n\"Plan an ATT&CK chain (Initial Access ‚Üí Exfiltration) for an on-prem AD env ‚Ä¶\"\nLog Triage\n\"Given the following Zeek logs, identify C2 traffic patterns ‚Ä¶\"\nUse temperature=0.3, top_p=0.9 for deterministic reasoning; raise for brainstorming.\nIt does not pursue any profit.\n\"Those who shed light on others do not remain in darkness...\"",
    "microsoft/Phi-4-multimodal-instruct": "Model Summary\nIntended Uses\nPrimary Use Cases\nUse Case Considerations\nRelease Notes\nModel Quality\nSpeech\nSpeech Recognition (lower is better)\nSpeech Translation (higher is better)\nSpeech Summarization (higher is better)\nSpeech QA\nAudio Understanding\nVision\nVision-Speech tasks\nVision tasks\nVisual Perception\nUsage\nRequirements\nTokenizer\nInput Formats\nText chat format\nTool-enabled function-calling format\nVision-Language Format\nSpeech-Language Format\nVision-Speech Format\nLoading the model locally\nvLLM inference\nTraining\nFine-tuning\nModel\nTraining Datasets\nSoftware\nHardware\nResponsible AI Considerations\nSafety\nSafety Evaluation and Red-Teaming\nVision Safety Evaluation\nAudio Safety Evaluation\nLicense\nTrademarks\nAppendix A: Benchmark Methodology\nVision Benchmark Settings\nSpeech Benchmark Settings\nBenchmark datasets\nAppendix B: Fine-tuning Korean speech\nOverview and Datasets\nRequirements\nTraining\nExperimental Settings and Results\nCautions\nReferences\nüéâPhi-4: [mini-reasoning | reasoning] | [multimodal-instruct | onnx];\n[mini-instruct | onnx]\nModel Summary\nPhi-4-multimodal-instruct is a lightweight open multimodal foundation\nmodel that leverages the language, vision, and speech research\nand datasets used for Phi-3.5 and 4.0 models. The model processes text,\nimage, and audio inputs, generating text outputs, and comes with\n128K token context length. The model underwent an enhancement process,\nincorporating both supervised fine-tuning, direct preference\noptimization and RLHF (Reinforcement Learning from Human Feedback)\nto support precise instruction adherence and safety measures.\nThe languages that each modal supports are the following:\nText: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish,\nFrench, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian,\nPolish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\nVision: English\nAudio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese\nüì∞ Phi-4-multimodal Microsoft Blog\nüìñ Phi-4-multimodal Technical Report\nüè° Phi Portal\nüë©‚Äçüç≥ Phi Cookbook\nüñ•Ô∏è Try It on Azure,\nGitHub,\nNvidia,\nHuggingface playgrounds\nüì±Huggingface Spaces\nThoughts Organizer,\nStories Come Alive,\nPhine Speech Translator\nWatch as Phi-4 Multimodal analyzes spoken language to help plan a trip to Seattle, demonstrating its advanced audio processing and recommendation capabilities.\nYour browser does not support the video tag.\nSee how Phi-4 Multimodal tackles complex mathematical problems through visual inputs, demonstrating its ability to process and solve equations presented in images.\nYour browser does not support the video tag.\nExplore how Phi-4 Mini functions as an intelligent agent, showcasing its reasoning and task execution abilities in complex scenarios.\nYour browser does not support the video tag.\nIntended Uses\nPrimary Use Cases\nThe model is intended for broad multilingual and multimodal commercial and research use . The model provides uses for general purpose AI systems and applications which require\nMemory/compute constrained environments\nLatency bound scenarios\nStrong reasoning (especially math and logic)\nFunction and tool calling\nGeneral image understanding\nOptical character recognition\nChart and table understanding\nMultiple image comparison\nMulti-image or video clip summarization\nSpeech recognition\nSpeech translation\nSpeech QA\nSpeech summarization\nAudio understanding\nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse Case Considerations\nThe model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models and multimodal models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\nDevelopers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nRelease Notes\nThis release of Phi-4-multimodal-instruct is based on valuable user feedback from the Phi-3 series. Previously, users could use a speech recognition model to talk to the Mini and Vision models. To achieve this, users needed to use a pipeline of two models: one model to transcribe the audio to text, and another model for the language or vision tasks. This pipeline means that the core model was not provided the full breadth of input information ‚Äì e.g. cannot directly observe multiple speakers, background noises, jointly align speech, vision, language information at the same time on the same representation space.\nWith Phi-4-multimodal-instruct, a single new open model has been trained across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. The model  employed new architecture, larger vocabulary for efficiency, multilingual, and multimodal support, and better post-training techniques were used for instruction following and function calling, as well as additional data leading to substantial gains on key multimodal capabilities.\nIt is anticipated that Phi-4-multimodal-instruct will greatly benefit app developers and various use cases. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4 is welcomed and crucial to the model's evolution and improvement. Thank you for being part of this journey!\nModel Quality\nClick to view details\nTo understand the capabilities, Phi-4-multimodal-instruct  was compared with a set of models over a variety of benchmarks using an internal benchmark platform (See Appendix A for benchmark methodology). Users can refer to the Phi-4-Mini-Instruct model card for details of language benchmarks. At the high-level overview of the model quality on representative speech and vision benchmarks:\nSpeech\nThe Phi-4-multimodal-instruct was observed as\nHaving strong automatic speech recognition (ASR) and speech translation (ST) performance, surpassing expert ASR model WhisperV3 and ST models SeamlessM4T-v2-Large.\nRanking number 1 on the Huggingface OpenASR leaderboard with word error rate 6.14% in comparison with the current best model 6.5% as of March 04, 2025.\nBeing the first open-sourced model that can perform speech summarization, and the performance is close to GPT4o.\nHaving a gap with close models, e.g. Gemini-1.5-Flash and GPT-4o-realtime-preview, on speech QA task. Work is being undertaken to improve this capability in the next iterations.\nSpeech Recognition (lower is better)\nThe performance of Phi-4-multimodal-instruct on the aggregated benchmark datasets:\nThe performance of Phi-4-multimodal-instruct on different languages, averaging the WERs of CommonVoice and FLEURS:\nSpeech Translation (higher is better)\nTranslating from German, Spanish, French, Italian, Japanese, Portugues, Chinese to English:\nTranslating from English to German, Spanish, French, Italian, Japanese, Portugues, Chinese. Noted that WhiperV3 does not support this capability:\nSpeech Summarization (higher is better)\nSpeech QA\nMT bench scores are scaled by 10x to match the score range of MMMLU:\nAudio Understanding\nAIR bench scores are scaled by 10x to match the score range of MMAU:\nVision\nVision-Speech tasks\nPhi-4-multimodal-instruct is capable of processing both image and audio together, the following table shows the model quality when the input query for vision content is synthetic speech on chart/table understanding and document reasoning tasks. Compared to other existing state-of-the-art omni models that can enable audio and visual signal as input, Phi-4-multimodal-instruct achieves much stronger performance on multiple benchmarks.\nBenchmarks\nPhi-4-multimodal-instruct\nInternOmni-7B\nGemini-2.0-Flash-Lite-prv-02-05\nGemini-2.0-Flash\nGemini-1.5-Pro\ns_AI2D\n68.9\n53.9\n62.0\n69.4\n67.7\ns_ChartQA\n69.0\n56.1\n35.5\n51.3\n46.9\ns_DocVQA\n87.3\n79.9\n76.0\n80.3\n78.2\ns_InfoVQA\n63.7\n60.3\n59.4\n63.6\n66.1\nAverage\n72.2\n62.6\n58.2\n66.2\n64.7\nVision tasks\nTo understand the vision capabilities, Phi-4-multimodal-instruct was compared with a set of models over a variety of zero-shot benchmarks using an internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\nDataset\nPhi-4-multimodal-ins\nPhi-3.5-vision-ins\nQwen 2.5-VL-3B-ins\nIntern VL 2.5-4B\nQwen 2.5-VL-7B-ins\nIntern VL 2.5-8B\nGemini 2.0-Flash Lite-preview-0205\nGemini2.0-Flash\nClaude-3.5-Sonnet-2024-10-22\nGpt-4o-2024-11-20\nPopular aggregated benchmark\nMMMU\n55.1\n43.0\n47.0\n48.3\n51.8\n50.6\n54.1\n64.7\n55.8\n61.7\nMMBench (dev-en)\n86.7\n81.9\n84.3\n86.8\n87.8\n88.2\n85.0\n90.0\n86.7\n89.0\nMMMU-Pro (std/vision)\n38.5\n21.8\n29.9\n32.4\n36.9\n34.4\n45.1\n54.4\n54.3\n53.0\nVisual science reasoning\nScienceQA Visual (img-test)\n97.5\n91.3\n79.4\n96.2\n87.7\n97.3\n85.0\n88.3\n81.2\n88.2\nVisual math reasoning\nMathVista (testmini)\n62.4\n43.9\n60.8\n51.2\n67.8\n56.7\n57.6\n47.2\n56.9\n56.1\nInterGPS\n48.6\n36.3\n48.3\n53.7\n52.7\n54.1\n57.9\n65.4\n47.1\n49.1\nChart & table reasoning\nAI2D\n82.3\n78.1\n78.4\n80.0\n82.6\n83.0\n77.6\n82.1\n70.6\n83.8\nChartQA\n81.4\n81.8\n80.0\n79.1\n85.0\n81.0\n73.0\n79.0\n78.4\n75.1\nDocVQA\n93.2\n69.3\n93.9\n91.6\n95.7\n93.0\n91.2\n92.1\n95.2\n90.9\nInfoVQA\n72.7\n36.6\n77.1\n72.1\n82.6\n77.6\n73.0\n77.8\n74.3\n71.9\nDocument Intelligence\nTextVQA (val)\n75.6\n72.0\n76.8\n70.9\n77.7\n74.8\n72.9\n74.4\n58.6\n73.1\nOCR Bench\n84.4\n63.8\n82.2\n71.6\n87.7\n74.8\n75.7\n81.0\n77.0\n77.7\nObject visual presence verification\nPOPE\n85.6\n86.1\n87.9\n89.4\n87.5\n89.1\n87.5\n88.0\n82.6\n86.5\nMulti-image perception\nBLINK\n61.3\n57.0\n48.1\n51.2\n55.3\n52.5\n59.3\n64.0\n56.9\n62.4\nVideo MME 16 frames\n55.0\n50.8\n56.5\n57.3\n58.2\n58.7\n58.8\n65.5\n60.2\n68.2\nAverage\n72.0\n60.9\n68.7\n68.8\n73.1\n71.1\n70.2\n74.3\n69.1\n72.4\nVisual Perception\nBelow are the comparison results on existing multi-image tasks. On average, Phi-4-multimodal-instruct outperforms competitor models of the same size and competitive with much bigger models on multi-frame capabilities.\nBLINK is an aggregated benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs.\nDataset\nPhi-4-multimodal-instruct\nQwen2.5-VL-3B-Instruct\nInternVL 2.5-4B\nQwen2.5-VL-7B-Instruct\nInternVL 2.5-8B\nGemini-2.0-Flash-Lite-prv-02-05\nGemini-2.0-Flash\nClaude-3.5-Sonnet-2024-10-22\nGpt-4o-2024-11-20\nArt Style\n86.3\n58.1\n59.8\n65.0\n65.0\n76.9\n76.9\n68.4\n73.5\nCounting\n60.0\n67.5\n60.0\n66.7\n71.7\n45.8\n69.2\n60.8\n65.0\nForensic Detection\n90.2\n34.8\n22.0\n43.9\n37.9\n31.8\n74.2\n63.6\n71.2\nFunctional Correspondence\n30.0\n20.0\n26.9\n22.3\n27.7\n48.5\n53.1\n34.6\n42.3\nIQ Test\n22.7\n25.3\n28.7\n28.7\n28.7\n28.0\n30.7\n20.7\n25.3\nJigsaw\n68.7\n52.0\n71.3\n69.3\n53.3\n62.7\n69.3\n61.3\n68.7\nMulti-View Reasoning\n76.7\n44.4\n44.4\n54.1\n45.1\n55.6\n41.4\n54.9\n54.1\nObject Localization\n52.5\n55.7\n53.3\n55.7\n58.2\n63.9\n67.2\n58.2\n65.6\nRelative Depth\n69.4\n68.5\n68.5\n80.6\n76.6\n81.5\n72.6\n66.1\n73.4\nRelative Reflectance\n26.9\n38.8\n38.8\n32.8\n38.8\n33.6\n34.3\n38.1\n38.1\nSemantic Correspondence\n52.5\n32.4\n33.8\n28.8\n24.5\n56.1\n55.4\n43.9\n47.5\nSpatial Relation\n72.7\n80.4\n86.0\n88.8\n86.7\n74.1\n79.0\n74.8\n83.2\nVisual Correspondence\n67.4\n28.5\n39.5\n50.0\n44.2\n84.9\n91.3\n72.7\n82.6\nVisual Similarity\n86.7\n67.4\n88.1\n87.4\n85.2\n87.4\n80.7\n79.3\n83.0\nOverall\n61.6\n48.1\n51.2\n55.3\n52.5\n59.3\n64.0\n56.9\n62.4\nUsage\nRequirements\nPhi-4 family has been integrated in the 4.48.2 version of transformers. The current transformers version can be verified with: pip list | grep transformers.\nWe suggest to run with Python 3.10.\nExamples of required packages:\nflash_attn==2.7.4.post1\ntorch==2.6.0\ntransformers==4.48.2\naccelerate==1.3.0\nsoundfile==0.13.1\npillow==11.1.0\nscipy==1.15.2\ntorchvision==0.21.0\nbackoff==2.2.1\npeft==0.13.2\nPhi-4-multimodal-instruct is also available in Azure AI Studio\nTokenizer\nPhi-4-multimodal-instruct supports a vocabulary size of up to 200064 tokens. The tokenizer files already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\nInput Formats\nGiven the nature of the training data, the Phi-4-multimodal-instruct model is best suited for prompts using the chat format as follows:\nText chat format\nThis format is used for general conversation and instructions:\n<|system|>You are a helpful assistant.<|end|><|user|>How to explain Internet for a medieval knight?<|end|><|assistant|>\nTool-enabled function-calling format\nThis format is used when the user wants the model to provide function calls based on\nthe given tools. The user should provide the available tools in the system prompt,\nwrapped by <|tool|> and <|/tool|> tokens. The tools should be specified in JSON format,\nusing a JSON dump structure. Example:\n<|system|>You are a helpful assistant with some tools.<|tool|>[{\"name\": \"get_weather_updates\", \"description\": \"Fetches weather updates for a given city using the RapidAPI Weather API.\", \"parameters\": {\"city\": {\"description\": \"The name of the city for which to retrieve weather information.\", \"type\": \"str\", \"default\": \"London\"}}}]<|/tool|><|end|><|user|>What is the weather like in Paris today?<|end|><|assistant|>\nVision-Language Format\nThis format is used for conversation with image:\n<|user|><|image_1|>Describe the image in detail.<|end|><|assistant|>\nFor multiple images, the user needs to insert multiple image placeholders in the prompt as below:\n<|user|><|image_1|><|image_2|><|image_3|>Summarize the content of the images.<|end|><|assistant|>\nSpeech-Language Format\nThis format is used for various speech and audio tasks:\n<|user|><|audio_1|>{task prompt}<|end|><|assistant|>\nThe task prompt can vary for different task.\nAutomatic Speech Recognition:\n<|user|><|audio_1|>Transcribe the audio clip into text.<|end|><|assistant|>\nAutomatic Speech Translation:\n<|user|><|audio_1|>Translate the audio to {lang}.<|end|><|assistant|>\nAutomatic Speech Translation with chain-of-thoughts:\n<|user|><|audio_1|>Transcribe the audio to text, and then translate the audio to {lang}. Use <sep> as a separator between the original transcript and the translation.<|end|><|assistant|>\nSpoken-query Question Answering:\n<|user|><|audio_1|><|end|><|assistant|>\nVision-Speech Format\nThis format is used for conversation with image and audio.\nThe audio may contain query related to the image:\n<|user|><|image_1|><|audio_1|><|end|><|assistant|>\nFor multiple images, the user needs to insert multiple image placeholders in the prompt as below:\n<|user|><|image_1|><|image_2|><|image_3|><|audio_1|><|end|><|assistant|>\nVision\nAny common RGB/gray image format (e.g., (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")) can be supported.\nResolution depends on the GPU memory size. Higher resolution and more images will produce more tokens, thus using more GPU memory. During training, 64 crops can be supported.\nIf it is a square image, the resolution would be around (8448 by 8448). For multiple-images, at most 64 frames can be supported, but with more frames as input, the resolution of each frame needs to be reduced to fit in the memory.\nAudio\nAny audio format that can be loaded by soundfile package should be supported.\nTo keep the satisfactory performance, maximum audio length is suggested to be 40s. For summarization tasks, the maximum audio length is suggested to 30 mins.\nLoading the model locally\nAfter obtaining the Phi-4-multimodal-instruct model checkpoints, users can use this sample code for inference.\nClick to view details\nimport requests\nimport torch\nimport os\nimport io\nfrom PIL import Image\nimport soundfile as sf\nfrom transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\nfrom urllib.request import urlopen\n# Define model path\nmodel_path = \"microsoft/Phi-4-multimodal-instruct\"\n# Load model and processor\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n# if you do not use Ampere or later GPUs, change attention to \"eager\"\n_attn_implementation='flash_attention_2',\n).cuda()\n# Load generation config\ngeneration_config = GenerationConfig.from_pretrained(model_path)\n# Define prompt structure\nuser_prompt = '<|user|>'\nassistant_prompt = '<|assistant|>'\nprompt_suffix = '<|end|>'\n# Part 1: Image Processing\nprint(\"\\n--- IMAGE PROCESSING ---\")\nimage_url = 'https://www.ilankelman.org/stopsigns/australia.jpg'\nprompt = f'{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}'\nprint(f'>>> Prompt\\n{prompt}')\n# Download and open image\nimage = Image.open(requests.get(image_url, stream=True).raw)\ninputs = processor(text=prompt, images=image, return_tensors='pt').to('cuda:0')\n# Generate response\ngenerate_ids = model.generate(\n**inputs,\nmax_new_tokens=1000,\ngeneration_config=generation_config,\n)\ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(\ngenerate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(f'>>> Response\\n{response}')\n# Part 2: Audio Processing\nprint(\"\\n--- AUDIO PROCESSING ---\")\naudio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\nspeech_prompt = \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\"\nprompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\nprint(f'>>> Prompt\\n{prompt}')\n# Downlowd and open audio file\naudio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n# Process with the model\ninputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors='pt').to('cuda:0')\ngenerate_ids = model.generate(\n**inputs,\nmax_new_tokens=1000,\ngeneration_config=generation_config,\n)\ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(\ngenerate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nprint(f'>>> Response\\n{response}')\nMore inference examples can be found here.\nvLLM inference\nUser can start a server with this command\npython -m vllm.entrypoints.openai.api_server --model 'microsoft/Phi-4-multimodal-instruct' --dtype auto --trust-remote-code --max-model-len 131072 --enable-lora --max-lora-rank 320 --lora-extra-vocab-size 0 --limit-mm-per-prompt audio=3,image=3 --max-loras 2 --lora-modules speech=<path to speech lora folder> vision=<path to vision lora folder>\nThe speech lora and vision lora folders are within the Phi-4-multimodal-instruct folder downloaded by vLLM, you can also use the following script to find thoses:\nfrom huggingface_hub import snapshot_download\nmodel_path = snapshot_download(repo_id=\"microsoft/Phi-4-multimodal-instruct\")\nspeech_lora_path = model_path+\"/speech-lora\"\nvision_lora_path = model_path+\"/vision-lora\"\nTraining\nFine-tuning\nA basic example of supervised fine-tuning (SFT) for speech and vision is provided respectively.\nAn example on how to extend speech recognition to a new language.\nModel\nArchitecture: Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model. The model has the pretrained Phi-4-Mini-Instruct as the backbone language model, and the advanced encoders and adapters of vision and speech.\nInputs: Text, image, and audio. It is best suited for prompts using the chat format.\nContext length: 128K tokens\nGPUs: 512 A100-80G\nTraining time: 28 days\nTraining data: 5T tokens, 2.3M speech hours, and 1.1T image-text tokens\nOutputs: Generated text in response to the input\nDates: Trained between December 2024 and January 2025\nStatus: This is a static model trained on offline datasets with the cutoff date of June 2024 for publicly available data.\nSupported languages:\nText: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\nVision: English\nAudio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese\nRelease date: February 2025\nTraining Datasets\nPhi-4-multimodal-instruct's training data includes a wide variety of sources, totaling 5 trillion text tokens, and is a combination of\npublicly available documents filtered for quality, selected high-quality educational data, and code\nnewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.)\nhigh quality human labeled data in chat format\nselected high-quality image-text interleave data\nsynthetic and publicly available image, multi-image, and video data\nanonymized in-house speech-text pair data with strong/weak transcriptions\nselected high-quality publicly available and anonymized in-house speech data with task-specific supervisions\nselected synthetic speech data\nsynthetic vision-speech data.\nFocus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for large foundation models, but such information was removed for the Phi-4-multimodal-instruct to leave more model capacity for reasoning for the model's small size. The data collection process involved sourcing information from publicly available documents, with a focus on filtering out undesirable documents and images. To safeguard privacy, image and text data sources were filtered to remove or scrub potentially personal data from the training data.\nThe decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis.\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nAccelerate\nsoundfile\npillow\nHardware\nNote that by default, the Phi-4-multimodal-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nIf you want to run the model on:\nNVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with _attn_implementation=\"eager\"\nResponsible AI Considerations\nClick to view detail descriptions\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The Phi models are trained primarily on English language content across text, speech, and visual inputs, with some additional multilingual coverage. Performance may vary significantly across different modalities and languages:\nText: Languages other than English will experience reduced performance, with varying levels of degradation across different non-English languages. English language varieties with less representation in the training data may perform worse than standard American English.\nSpeech: Speech recognition and processing shows similar language-based performance patterns, with optimal performance for standard American English accents and pronunciations. Other English accents, dialects, and non-English languages may experience lower recognition accuracy and response quality. Background noise, audio quality, and speaking speed can further impact performance.\nVision: Visual processing capabilities may be influenced by cultural and geographical biases in the training data. The model may show reduced performance when analyzing images containing text in non-English languages or visual elements more commonly found in non-Western contexts. Image quality, lighting conditions, and composition can also affect processing accuracy.\nMultilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, it is strongly recommended that users manually verify all API uses.\nLong Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift.\nInference of Sensitive Attributes: The Phi 4 models can sometimes attempt to infer sensitive attributes (such as personality characteristics, country of origin, gender, etc...) from the users‚Äô voices when specifically asked to do so. Phi 4-multimodal-instruct is not designed or intended to be used as a biometric categorization system to categorize individuals based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation. This behavior can be easily and efficiently mitigated at the application level by a system message.\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nSafety\nClick to view detail descriptions\nThe Phi-4 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed for safety alignment is a combination of SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization), and RLHF (Reinforcement Learning from Human Feedback) approaches by utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness, as well as various questions and answers targeted to multiple safety categories. For non-English languages, existing datasets were extended via machine translation. Speech Safety datasets were generated by running Text Safety datasets through Azure TTS (Text-To-Speech) Service, for both English and non-English languages. Vision (text & images) Safety datasets were created to cover harm categories identified both in public and internal multi-modal RAI datasets.\nSafety Evaluation and Red-Teaming\nVarious evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 models' propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the Phi 3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the Phi 3 Safety Post-Training paper. For this release, the red teaming effort focused on the newest Audio input modality and on the following safety areas: harmful content, self-injury risks, and exploits. The model was found to be more susceptible to providing undesirable outputs when attacked with context manipulation or persuasive techniques. These findings applied to all languages, with the persuasive techniques mostly affecting French and Italian. This highlights the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken.\nVision Safety Evaluation\nTo assess model safety in scenarios involving both text and images, Microsoft's Azure AI Evaluation SDK was utilized. This tool facilitates the simulation of single-turn conversations with the target model by providing prompt text and images designed to incite harmful responses. The target model's responses are subsequently evaluated by a capable model across multiple harm categories, including violence, sexual content, self-harm, hateful and unfair content, with each response scored based on the severity of the harm identified. The evaluation results were compared with those of Phi-3.5-Vision and open-source models of comparable size. In addition, we ran both an internal and the public RTVLM and VLGuard multi-modal (text & vision) RAI benchmarks, once again comparing scores with Phi-3.5-Vision and open-source models of comparable size. However, the model may be susceptible to language-specific attack prompts and cultural context.\nAudio Safety Evaluation\nIn addition to extensive red teaming, the Safety of the model was assessed through three distinct evaluations. First, as performed with Text and Vision inputs, Microsoft's Azure AI Evaluation SDK was leveraged to detect the presence of harmful content in the model's responses to Speech prompts. Second, Microsoft's Speech Fairness evaluation was run to verify that Speech-To-Text transcription worked well across a variety of demographics. Third, we proposed and evaluated a mitigation approach via a system message to help prevent the model from inferring sensitive attributes (such as gender, sexual orientation, profession, medical condition, etc...) from the voice of a user.\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\nAppendix A: Benchmark Methodology\nClick to view detail descriptions\nWe include a brief word on methodology here - and in particular, how we think about optimizing prompts.\nIn an ideal world, we would never change any prompts in our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date.\nThere are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given eval due to a failure to respect the output format. For example:\nA model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with ‚ÄúSure, I can help with that. ‚Ä¶‚Äù which may break the parser. In such cases, we have opted to try different system messages (e.g. ‚ÄúYou must always respond to a question‚Äù or ‚ÄúGet to the point!‚Äù).\nSome models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases.\nWe have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts.\nHowever, we do not:\nPick different few-shot examples. Few shots will always be the same when comparing different models.\nChange prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice.\nVision Benchmark Settings\nThe goal of the benchmark setup is to measure the performance of the LMM when a regular user utilizes these models for a task involving visual input. To this end, we selected 9 popular and publicly available single-frame datasets and 3 multi-frame benchmarks that cover a wide range of challenging topics and tasks (e.g., mathematics, OCR tasks, charts-and-plots understanding, etc.) as well as a set of high-quality models.\nOur benchmarking setup utilizes zero-shot prompts and all the prompt content are the same for every model. We only formatted the prompt content to satisfy the model's prompt API. This ensures that our evaluation is fair across the set of models we tested. Many benchmarks necessitate models to choose their responses from a presented list of options. Therefore, we've included a directive in the prompt's conclusion, guiding all models to pick the option letter that corresponds to the answer they deem correct.\nIn terms of the visual input, we use the images from the benchmarks as they come from the original datasets. We converted these images to base-64 using a JPEG encoding for models that require this format (e.g., GPTV, Claude Sonnet 3.5, Gemini 1.5 Pro/Flash). For other models (e.g., Llava Interleave, and InternVL2 4B and 8B), we used their Huggingface interface and passed in PIL images or a JPEG image stored locally. We did not scale or pre-process images in any other way.\nLastly, we used the same code to extract answers and evaluate them using the same code for every considered model. This ensures that we are fair in assessing the quality of their answers.\nSpeech Benchmark Settings\nThe objective of this benchmarking setup is to assess the performance of models in speech and audio understanding tasks as utilized by regular users. To accomplish this, we selected several state-of-the-art open-sourced and closed-sourced models and performed evaluations across a variety of public and in-house benchmarks. These benchmarks encompass diverse and challenging topics, including Automatic Speech Recognition (ASR), Automatic Speech Translation (AST), Spoken Query Question Answering (SQQA), Audio Understanding (AU), and Speech Summarization.\nThe results are derived from evaluations conducted on identical test data without any further clarifications. All results were obtained without sampling during inference. For an accurate comparison, we employed consistent prompts for models across different tasks, except for certain model APIs (e.g., GPT-4o), which may refuse to respond to specific prompts for some tasks.\nIn conclusion, we used uniform code to extract answers and evaluate them for all considered models. This approach ensured fairness by assessing the quality of their responses.\nBenchmark datasets\nThe model was evaluated across a breadth of public and internal benchmarks to understand it's capabilities under multiple tasks and conditions. While most evaluations use English, multilingual benchmark was incorporated to cover performance in select languages.  More specifically,\nVision:\nPopular aggregated benchmark:\nMMMU and MMMU-Pro: massive multi-discipline tasks at college-level subject knowledge and deliberate reasoning.\nMMBench: large-scale benchmark to evaluate perception and reasoning capabilities.\nVisual reasoning:\nScienceQA: multimodal visual question answering on science.\nMathVista: visual math reasoning.\nInterGPS: Visual 2D geometry reasoning.\nChart reasoning:\nChartQA: visual and logical reasoning on charts.\nAI2D: diagram understanding.\nDocument Intelligence:\nTextVQA: read and reason about text in images to answer questions about them.\nInfoVQA: read and reason about high-resolution infographics images with arbitrary aspect ratios.\nDocVQA: read and reason about document images with dense texts and handwritten texts.\nOCRBench: test OCR and QA capability on diverse text related images.\nVision speech multimodal understanding:\ns_AI2D: diagram understanding with speech as the question format.\ns_ChartQA: visual and logical reasoning on charts with speech as the question format.\ns_InfoVQA: read and reason about high-resolution infographics images with speech as the question format.\ns_DocVQA: read and reason about document images with dense texts and handwritten texts with speech as the question format.\nRAI & Security Benchmarks:\nVLGuardExt: VLGuard is a vision-language instruction following public dataset for model safety to address safety on deception\ndiscrimination, privacy and risky behavior (advice, sexual, violence, political). This was extended to a few internal categories such as child safety and election critical information.\nRTVLM: Public benchmark for red-teaming vision-language model on model truthfulness, privacy, safety, and fairness.\nGPTV-RAI: In-house benchmark for GPT-4V released from Azure AI, measuring harmfulness (ex. sexual, violent, hate and self-harm), privacy, jailbreak, misinformation.\nSpeech:\nCommonVoice v15 is an open-source, multilingual speech dataset developed by Mozilla. It includes over 33,000 hours of speech data in 133 languages, contributed and validated by volunteers worldwide.The evaluations were conducted in the eight supported languages.\nThe OpenASR Leaderboard on Hugging Face is designed for benchmarking and evaluating the robustness of ASR models on English. The datasets in the leaderboard cover diverse speech domains including reading speech, conversations, meetings, and so on.\nCoVoST2 is a multilingual speech-to-text translation dataset derived from Mozilla's Common Voice project. It is one of the largest open datasets available for speech translation, providing support for both X-to-English (X‚ÜíEn) and English-to-X (En‚ÜíX) translation tasks. The directions with supported languages were evaluated on the test sets.\nFLEURS is a multilingual speech dataset designed for evaluating speech recognition and speech-to-text translation models across a wide range of languages. The test sets for speech recognition and translation tasks were evaluated with the eight supported languages.\nMT Bench (Multi-turn Benchmark) is specifically designed to evaluate the conversational and instruction-following abilities of AI models in multi-turn question-answering (QA) scenarios. To support spoken questions, the text is synthesized into speech.\nMMMLU (Multilingual Massive Multitask Language Understanding) is an extensive benchmark designed to evaluate the general knowledge and reasoning capabilities of AI models across a wide array of subjects. To support spoken questions, the text is synthesized into its speech counterpart.  The model was evaluated on the eight supported languages for this test set.\nAIR-Bench Chat (Audio Instruction and Response Benchmark) is a comprehensive evaluation framework designed to test the capabilities of large audio language models (LALMs). It includes both foundation and chat benchmarks. The chat benchmark is selected for its open-ended question answering for audio capability.\nMMAU (Massive Multi-Task Audio Understanding) is a comprehensive dataset designed to evaluate the capabilities of multi-modal models in audio-based understanding and reasoning tasks. The test sets are in the form of multiple-choices QA, covering the categories of music, sound, and speech.\nGolden3 is a real-world meeting dataset, containing 108 meeting recordings with corresponding transcripts, averaging 6 minutes each. It is recorded across 30 conference rooms, featuring 4-8 attendees. The dataset is primarily in English, covering a wide range of topics. GPT4 is employed to generate summarization instructions that ask to summarize partial or the entire conversation or control the output style/length/structure.\nAMI (Augmented Multi-Party Interaction) is a comprehensive collection of meeting recordings, encompassing approximately 100 hours of data. The test split contains 20 meeting recordings with an average duration of 32 minutes. The model was tested on the close-talking version of audio. GPT4 is employed to generate summarization instructions that ask to summarize partial or the entire conversation or control the output style/length/structure.\nSafety and RAI:\nSingle-turn trustworthiness evaluation:\nDecodingTrust: DecodingTrust is a collection of trustworthiness benchmarks in eight different perspectives\nXSTest: XSTest is an exaggerated safety evaluation\nToxigen: Toxigen is adversarial and hate speech detection\nRed Team:\nResponses to prompts provided by AI Red Team at Microsoft\nAppendix B: Fine-tuning Korean speech\nClick to view detail descriptions\nOverview and Datasets\nPhi-4-multimodal is originally not designed for Korean speech-to-text task, but it can be fine-tuned for Korean speech-to-text task using your own data or public Korean speech datasets.\nWe have fine-tuned Phi-4-multimodal model for Korean speech-to-text task using the following datasets:\nkresnik/zeroth_korean\nmozilla-foundation/common_voice_17_0 (Used Korean speech only)\nPolyAI/minds14 (Used Korean speech only)\nCustom dataset. The speech was a mix of fast and slow speech (Technical blog contents and presentations that the author have posted), with some modulation using audiomentations and this script\nTotal 35K samples. Each sample is a pair of Korean speech and its transcription. Dataset was sampled 16kHz.\nYou can download the fine-tuned model here. Please refer to the Jupyter notebook and video clips in the demo folder. They are not production-quality as they were simply fine-tuned for PoC purposes, but you can see that they transcribe and translate with high accuracy even when a native speaker speaks quite quickly.\nRequirements\nBased on Python 3.10, the following packages are required, and A100/H100 GPU is recommended.\ntorch==2.6.0\ntransformers==4.48.2\naccelerate==1.4.0\nsoundfile==0.13.1\npillow==11.1.0\nscipy==1.15.2\ntorchvision==0.21.0\nbackoff==2.2.1\npeft==0.14.0\ndatasets==3.3.2\npandas==2.2.3\nflash_attn==2.7.4.post1\nevaluate==0.4.3\nsacrebleu==2.5.1\nTraining\nThe model was trained on a single A100 80GB GPU for 4 epochs with a batch size of 16 using the sample_finetune_speech.py script from microsoft/Phi-4-multimodal-instruct\nThe fine tuning script and command line are basically the same as here, but you need to prepare your own dataset. Also, to perform audio encoder unfreeze, please refer to the code snippet below. The code snippet is retrieved from the fine-tuning Colab notebook.\nwith accelerator.local_main_process_first():\nprocessor = AutoProcessor.from_pretrained(\n\"microsoft/Phi-4-multimodal-instruct\",\ntrust_remote_code=True,\n)\nmodel = create_model(\nargs.model_name_or_path,\nuse_flash_attention=args.use_flash_attention,\n)\ndef unfreeze_speech_components(model):\n\"\"\"Directly target verified components from your debug logs\"\"\"\n# 1. Audio Embed Module (confirmed exists)\naudio_embed = model.model.embed_tokens_extend.audio_embed\n# 2. Entire Audio Encoder (simplified)\naudio_encoder = audio_embed.encoder  # Direct access\n# 3. Audio Projection (from debug logs)\naudio_projection = audio_embed.audio_projection\n# Unfreeze ONLY these 3 components\nfor component in [audio_embed, audio_encoder, audio_projection]:\nfor param in component.parameters():\nparam.requires_grad = True\nreturn model\nmodel = unfreeze_speech_components(model)\n# Verify unfrozen parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,}\")\n# After unfreezing\nencoder_params = list(model.model.embed_tokens_extend.audio_embed.encoder.parameters())\nproj_params = list(model.model.embed_tokens_extend.audio_embed.audio_projection.parameters())\nassert any(p.requires_grad for p in encoder_params), \"Encoder params frozen!\"\nassert any(p.requires_grad for p in proj_params), \"Projection params frozen!\"\nprint(\"Components properly unfrozen ‚úÖ\")\nExample commands to run finetuning scripts are as follows:\npython main.py\nThe latest version of the model currently uploaded was fine-tuned by unfreezing the audio encoder, and the ASR performance was significantly improved compared to the baseline LoRA adapter-based fine-tuning.\nComparing the full fine-tuning and LoRA fine-tuning, the CER on zeroth-test set is 1.61% and 2.72%, and the WER on zeroth-test set is 3.54% and 7.19%, respectively. Please refer to the Experimental Settings and Results for more details.\nExperimental Settings and Results\nThe purpose of this benchmarking setup is to evaluate the basic performance of Korean audio in speech and audio understanding tasks. We did this for automatic speech recognition and automatic speech translation, and the test data used the following datasets and samples:\nEvaluation was done on the following datasets:\nASR (Automatic Speech Recognition): Evaluated with CER (Character Error Rate) and WER (Word Error Rate) on zeroth-test set (457 samples).\nAST (Automatic Speech Translation): Evaluated with BLEU score on fleurs ko <-> en speech translation test set (270 samples).\nEvaluation Script is retrieved from here\nWe used the Phi-4-mm-inst-zeroth-kor as a baseline to improve performance, as it showed significant performance improvement with 1 epoch. Note that the baseline was trained with 22K Zeroth Korean Korean speech data for 1 epoch. Based on this baseline with 35K training samples, we conducted additional experiments with the following scenarios:\n[Case 1] LoRA finetune (1 epoch): LoRA adapter-based fine-tuning for 1 epochs\n[Case 2] LoRA finetune (4 epochs): LoRA adapter-based fine-tuning for 4 epochs\n[Case 3] Unfreeze audio encoder finetune (4 epochs): Full fine-tuning for 4 epochs.\nThe results of the experiments are as follows:\nCER and WER for zeroth-test set (Lower is better)\nCase 1's CER and WER are 3.80% and 11.52%, respectively, which are better than the baseline (7.02% and 17.31%).\nCase 2's CER and WER are 2.72% and 7.19%, respectively, which are better than Case 1.\nCase 3's CER and WER are 1.61% and 3.54%, respectively, which are the best among the cases.\nBLEU score for fleurs ko <-> en speech translation test set (Higher is better)\nCase 1's result is not improved compared to the baseline. Especially, the BLEU score for fleurs-ko2en-cot is decreased compared to the baseline.\nCase 2's result is slightly improved compared to Case 1, which is the best among the cases.\nCase 3's result is not improved compared to the baseline and Case 2.\nModel\nzeroth (CER)\nzeroth (WER)\nfleurs-ko2en\nfleurs-ko2en-cot\nfleurs-en2ko\nfleurs-en2ko-cot\noriginal\n99.16\n99.63\n5.63\n2.42\n6.86\n4.17\nOurs - speech full finetune (4 epochs)\n1.61\n3.54\n7.67\n8.38\n12.31\n9.69\nLoRA finetune (4 epochs)\n2.72\n7.19\n7.11\n9.95\n13.22\n10.45\nLoRA finetune (1 epoch)\n3.80\n11.52\n7.03\n7.04\n12.50\n9.54\nPhi-4-mm-inst-zeroth-kor\n7.02\n17.31\n7.07\n9.19\n13.08\n9.35\nCautions\nNote that this model is just a PoC/experimental purpose, and not intended to be used in production. More high-quality data, tuning, ablation studies, and experiments are needed.\nPhi-4-multimodal model is strong in multimodal tasks, especially in speech-to-text and high potential in Korean language tasks. Thus if you are interested in Korean speech-to-text task, this model can be a good starting point.\nReferences\nhttps://huggingface.co/microsoft/Phi-4-multimodal-instruct\nhttps://huggingface.co/seastar105/Phi-4-mm-inst-zeroth-kor",
    "google/gemma-3-12b-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nYou can initialize the model and processor for inference with pipeline as follows.\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/gemma-3-12b-it\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16\n)\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n{\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/gemma-3-12b-it\"\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\nmodel_id, device_map=\"auto\"\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=True,\nreturn_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "google/gemma-3-1b-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\nmessages = [\n[\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n},\n{\n\"role\": \"user\",\n\"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n},\n],\n]\noutput = pipe(messages, max_new_tokens=50)\nRunning the model on a single / multi GPU\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\nimport torch\nmodel_id = \"google/gemma-3-1b-it\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = Gemma3ForCausalLM.from_pretrained(\nmodel_id, quantization_config=quantization_config\n).eval()\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmessages = [\n[\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n},\n{\n\"role\": \"user\",\n\"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n},\n],\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device).to(torch.bfloat16)\nwith torch.inference_mode():\noutputs = model.generate(**inputs, max_new_tokens=64)\noutputs = tokenizer.batch_decode(outputs)\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "Qwen/Qwen2.5-Omni-7B": "Qwen2.5-Omni\nOverview\nIntroduction\nKey Features\nModel Architecture\nPerformance\nQuickstart\nü§ó  Transformers Usage\nUsage Tips\nCitation\nQwen2.5-Omni\nOverview\nIntroduction\nQwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner.\nKey Features\nOmni and Novel Architecture: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.\nReal-Time Voice and Video Chat: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.\nNatural and Robust Speech Generation: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.\nStrong Performance Across Modalities: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.\nExcellent End-to-End Speech Instruction Following: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.\nModel Architecture\nPerformance\nWe conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).\nMultimodality  -> Text\nDatasets\nModel\nPerformance\nOmniBenchSpeech | Sound Event | Music | Avg\nGemini-1.5-Pro\n42.67%|42.26%|46.23%|42.91%\nMIO-Instruct\n36.96%|33.58%|11.32%|33.80%\nAnyGPT (7B)\n17.77%|20.75%|13.21%|18.04%\nvideo-SALMONN\n34.11%|31.70%|56.60%|35.64%\nUnifiedIO2-xlarge\n39.56%|36.98%|29.25%|38.00%\nUnifiedIO2-xxlarge\n34.24%|36.98%|24.53%|33.98%\nMiniCPM-o\n-|-|-|40.50%\nBaichuan-Omni-1.5\n-|-|-|42.90%\nQwen2.5-Omni-3B\n52.14%|52.08%|52.83%|52.19%\nQwen2.5-Omni-7B\n55.25%|60.00%|52.83%|56.13%\nAudio -> Text\nDatasets\nModel\nPerformance\nASR\nLibrispeechdev-clean | dev other | test-clean | test-other\nSALMONN\n-|-|2.1|4.9\nSpeechVerse\n-|-|2.1|4.4\nWhisper-large-v3\n-|-|1.8|3.6\nLlama-3-8B\n-|-|-|3.4\nLlama-3-70B\n-|-|-|3.1\nSeed-ASR-Multilingual\n-|-|1.6|2.8\nMiniCPM-o\n-|-|1.7|-\nMinMo\n-|-|1.7|3.9\nQwen-Audio\n1.8|4.0|2.0|4.2\nQwen2-Audio\n1.3|3.4|1.6|3.6\nQwen2.5-Omni-3B\n2.0|4.1|2.2|4.5\nQwen2.5-Omni-7B\n1.6|3.5|1.8|3.4\nCommon Voice 15en | zh | yue | fr\nWhisper-large-v3\n9.3|12.8|10.9|10.8\nMinMo\n7.9|6.3|6.4|8.5\nQwen2-Audio\n8.6|6.9|5.9|9.6\nQwen2.5-Omni-3B\n9.1|6.0|11.6|9.6\nQwen2.5-Omni-7B\n7.6|5.2|7.3|7.5\nFleurszh | en\nWhisper-large-v3\n7.7|4.1\nSeed-ASR-Multilingual\n-|3.4\nMegrez-3B-Omni\n10.8|-\nMiniCPM-o\n4.4|-\nMinMo\n3.0|3.8\nQwen2-Audio\n7.5|-\nQwen2.5-Omni-3B\n3.2|5.4\nQwen2.5-Omni-7B\n3.0|4.1\nWenetspeechtest-net | test-meeting\nSeed-ASR-Chinese\n4.7|5.7\nMegrez-3B-Omni\n-|16.4\nMiniCPM-o\n6.9|-\nMinMo\n6.8|7.4\nQwen2.5-Omni-3B\n6.3|8.1\nQwen2.5-Omni-7B\n5.9|7.7\nVoxpopuli-V1.0-en\nLlama-3-8B\n6.2\nLlama-3-70B\n5.7\nQwen2.5-Omni-3B\n6.6\nQwen2.5-Omni-7B\n5.8\nS2TT\nCoVoST2en-de | de-en | en-zh | zh-en\nSALMONN\n18.6|-|33.1|-\nSpeechLLaMA\n-|27.1|-|12.3\nBLSP\n14.1|-|-|-\nMiniCPM-o\n-|-|48.2|27.2\nMinMo\n-|39.9|46.7|26.0\nQwen-Audio\n25.1|33.9|41.5|15.7\nQwen2-Audio\n29.9|35.2|45.2|24.4\nQwen2.5-Omni-3B\n28.3|38.1|41.4|26.6\nQwen2.5-Omni-7B\n30.2|37.7|41.4|29.4\nSER\nMeld\nWavLM-large\n0.542\nMiniCPM-o\n0.524\nQwen-Audio\n0.557\nQwen2-Audio\n0.553\nQwen2.5-Omni-3B\n0.558\nQwen2.5-Omni-7B\n0.570\nVSC\nVocalSound\nCLAP\n0.495\nPengi\n0.604\nQwen-Audio\n0.929\nQwen2-Audio\n0.939\nQwen2.5-Omni-3B\n0.936\nQwen2.5-Omni-7B\n0.939\nMusic\nGiantSteps Tempo\nLlark-7B\n0.86\nQwen2.5-Omni-3B\n0.88\nQwen2.5-Omni-7B\n0.88\nMusicCaps\nLP-MusicCaps\n0.291|0.149|0.089|0.061|0.129|0.130\nQwen2.5-Omni-3B\n0.325|0.163|0.093|0.057|0.132|0.229\nQwen2.5-Omni-7B\n0.328|0.162|0.090|0.055|0.127|0.225\nAudio Reasoning\nMMAUSound | Music | Speech | Avg\nGemini-Pro-V1.5\n56.75|49.40|58.55|54.90\nQwen2-Audio\n54.95|50.98|42.04|49.20\nQwen2.5-Omni-3B\n70.27|60.48|59.16|63.30\nQwen2.5-Omni-7B\n67.87|69.16|59.76|65.60\nVoice Chatting\nVoiceBenchAlpacaEval | CommonEval | SD-QA | MMSU\nUltravox-v0.4.1-LLaMA-3.1-8B\n4.55|3.90|53.35|47.17\nMERaLiON\n4.50|3.77|55.06|34.95\nMegrez-3B-Omni\n3.50|2.95|25.95|27.03\nLyra-Base\n3.85|3.50|38.25|49.74\nMiniCPM-o\n4.42|4.15|50.72|54.78\nBaichuan-Omni-1.5\n4.50|4.05|43.40|57.25\nQwen2-Audio\n3.74|3.43|35.71|35.72\nQwen2.5-Omni-3B\n4.32|4.00|49.37|50.23\nQwen2.5-Omni-7B\n4.49|3.93|55.71|61.32\nVoiceBenchOpenBookQA | IFEval | AdvBench | Avg\nUltravox-v0.4.1-LLaMA-3.1-8B\n65.27|66.88|98.46|71.45\nMERaLiON\n27.23|62.93|94.81|62.91\nMegrez-3B-Omni\n28.35|25.71|87.69|46.25\nLyra-Base\n72.75|36.28|59.62|57.66\nMiniCPM-o\n78.02|49.25|97.69|71.69\nBaichuan-Omni-1.5\n74.51|54.54|97.31|71.14\nQwen2-Audio\n49.45|26.33|96.73|55.35\nQwen2.5-Omni-3B\n74.73|42.10|98.85|68.81\nQwen2.5-Omni-7B\n81.10|52.87|99.42|74.12\nImage -> Text\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nOther Best\nQwen2.5-VL-7B\nGPT-4o-mini\nMMMUval\n59.2\n53.1\n53.9\n58.6\n60.0\nMMMU-Prooverall\n36.6\n29.7\n-\n38.3\n37.6\nMathVistatestmini\n67.9\n59.4\n71.9\n68.2\n52.5\nMathVisionfull\n25.0\n20.8\n23.1\n25.1\n-\nMMBench-V1.1-ENtest\n81.8\n77.8\n80.5\n82.6\n76.0\nMMVetturbo\n66.8\n62.1\n67.5\n67.1\n66.9\nMMStar\n64.0\n55.7\n64.0\n63.9\n54.8\nMMEsum\n2340\n2117\n2372\n2347\n2003\nMuirBench\n59.2\n48.0\n-\n59.2\n-\nCRPErelation\n76.5\n73.7\n-\n76.4\n-\nRealWorldQAavg\n70.3\n62.6\n71.9\n68.5\n-\nMME-RealWorlden\n61.6\n55.6\n-\n57.4\n-\nMM-MT-Bench\n6.0\n5.0\n-\n6.3\n-\nAI2D\n83.2\n79.5\n85.8\n83.9\n-\nTextVQAval\n84.4\n79.8\n83.2\n84.9\n-\nDocVQAtest\n95.2\n93.3\n93.5\n95.7\n-\nChartQAtest Avg\n85.3\n82.8\n84.9\n87.3\n-\nOCRBench_V2en\n57.8\n51.7\n-\n56.3\n-\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nQwen2.5-VL-7B\nGrounding DINO\nGemini 1.5 Pro\nRefcocoval\n90.5\n88.7\n90.0\n90.6\n73.2\nRefcocotextA\n93.5\n91.8\n92.5\n93.2\n72.9\nRefcocotextB\n86.6\n84.0\n85.4\n88.2\n74.6\nRefcoco+val\n85.4\n81.1\n84.2\n88.2\n62.5\nRefcoco+textA\n91.0\n87.5\n89.1\n89.0\n63.9\nRefcoco+textB\n79.3\n73.2\n76.9\n75.9\n65.0\nRefcocog+val\n87.4\n85.0\n87.2\n86.1\n75.2\nRefcocog+test\n87.9\n85.1\n87.2\n87.0\n76.2\nODinW\n42.4\n39.2\n37.3\n55.0\n36.7\nPointGrounding\n66.5\n46.2\n67.3\n-\n-\nVideo(without audio) -> Text\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nOther Best\nQwen2.5-VL-7B\nGPT-4o-mini\nVideo-MMEw/o sub\n64.3\n62.0\n63.9\n65.1\n64.8\nVideo-MMEw sub\n72.4\n68.6\n67.9\n71.6\n-\nMVBench\n70.3\n68.7\n67.2\n69.6\n-\nEgoSchematest\n68.6\n61.4\n63.2\n65.0\n-\nZero-shot Speech Generation\nDatasets\nModel\nPerformance\nContent Consistency\nSEEDtest-zh | test-en | test-hard\nSeed-TTS_ICL\n1.11 | 2.24 | 7.58\nSeed-TTS_RL\n1.00 | 1.94 | 6.42\nMaskGCT\n2.27 | 2.62 | 10.27\nE2_TTS\n1.97 | 2.19 | -\nF5-TTS\n1.56 | 1.83 | 8.67\nCosyVoice 2\n1.45 | 2.57 | 6.83\nCosyVoice 2-S\n1.45 | 2.38 | 8.08\nQwen2.5-Omni-3B_ICL\n1.95 | 2.87 | 9.92\nQwen2.5-Omni-3B_RL\n1.58 | 2.51 | 7.86\nQwen2.5-Omni-7B_ICL\n1.70 | 2.72 | 7.97\nQwen2.5-Omni-7B_RL\n1.42 | 2.32 | 6.54\nSpeaker Similarity\nSEEDtest-zh | test-en | test-hard\nSeed-TTS_ICL\n0.796 | 0.762 | 0.776\nSeed-TTS_RL\n0.801 | 0.766 | 0.782\nMaskGCT\n0.774 | 0.714 | 0.748\nE2_TTS\n0.730 | 0.710 | -\nF5-TTS\n0.741 | 0.647 | 0.713\nCosyVoice 2\n0.748 | 0.652 | 0.724\nCosyVoice 2-S\n0.753 | 0.654 | 0.732\nQwen2.5-Omni-3B_ICL\n0.741 | 0.635 | 0.748\nQwen2.5-Omni-3B_RL\n0.744 | 0.635 | 0.746\nQwen2.5-Omni-7B_ICL\n0.752 | 0.632 | 0.747\nQwen2.5-Omni-7B_RL\n0.754 | 0.641 | 0.752\nText -> Text\nDataset\nQwen2.5-Omni-7B\nQwen2.5-Omni-3B\nQwen2.5-7B\nQwen2.5-3B\nQwen2-7B\nLlama3.1-8B\nGemma2-9B\nMMLU-Pro\n47.0\n40.4\n56.3\n43.7\n44.1\n48.3\n52.1\nMMLU-redux\n71.0\n60.9\n75.4\n64.4\n67.3\n67.2\n72.8\nLiveBench0831\n29.6\n22.3\n35.9\n26.8\n29.2\n26.7\n30.6\nGPQA\n30.8\n34.3\n36.4\n30.3\n34.3\n32.8\n32.8\nMATH\n71.5\n63.6\n75.5\n65.9\n52.9\n51.9\n44.3\nGSM8K\n88.7\n82.6\n91.6\n86.7\n85.7\n84.5\n76.7\nHumanEval\n78.7\n70.7\n84.8\n74.4\n79.9\n72.6\n68.9\nMBPP\n73.2\n70.4\n79.2\n72.7\n67.2\n69.6\n74.9\nMultiPL-E\n65.8\n57.6\n70.4\n60.2\n59.1\n50.7\n53.4\nLiveCodeBench2305-2409\n24.6\n16.5\n28.7\n19.9\n23.9\n8.3\n18.9\nQuickstart\nBelow, we provide simple examples to show how to use Qwen2.5-Omni with ü§ó Transformers. The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to build from source with command:\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\npip install accelerate\nor you might encounter the following error:\nKeyError: 'qwen2_5_omni'\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has ffmpeg installed:\n# It's highly recommended to use `[decord]` feature for faster video loading.\npip install qwen-omni-utils[decord] -U\nIf you are not using Linux, you might not be able to install decord from PyPI. In that case, you can use pip install qwen-omni-utils -U which will fall back to using torchvision for video processing. However, you can still install decord from source to get decord used when loading video.\nü§ó  Transformers Usage\nHere we show a code snippet to show you how to use the chat model with transformers and qwen_omni_utils:\nimport soundfile as sf\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-7B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\nconversation = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n],\n},\n]\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n\"output.wav\",\naudio.reshape(-1).detach().cpu().numpy(),\nsamplerate=24000,\n)\nMinimum GPU memory requirements\nModel\nPrecision\n15(s) Video\n30(s) Video\n60(s) Video\nQwen-Omni-3B\nFP32\n89.10 GB\nNot Recommend\nNot Recommend\nQwen-Omni-3B\nBF16\n18.38 GB\n22.43 GB\n28.22 GB\nQwen-Omni-7B\nFP32\n93.56 GB\nNot Recommend\nNot Recommend\nQwen-Omni-7B\nBF16\n31.11 GB\n41.85 GB\n60.19 GB\nNote: The table above presents the theoretical minimum memory requirements for inference with transformers and BF16 is test with attn_implementation=\"flash_attention_2\"; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource here.\nVideo URL resource usage\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by FORCE_QWENVL_VIDEO_READER=torchvision or FORCE_QWENVL_VIDEO_READER=decord if you prefer not to use the default one.\nBackend\nHTTP\nHTTPS\ntorchvision >= 0.19.0\n‚úÖ\n‚úÖ\ntorchvision < 0.19.0\n‚ùå\n‚ùå\ndecord\n‚úÖ\n‚ùå\nBatch inference\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when return_audio=False is set. Here is an example.\n# Sample messages for batch inference\n# Conversation with video only\nconversation1 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n]\n}\n]\n# Conversation with audio only\nconversation2 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n]\n}\n]\n# Conversation with pure text\nconversation3 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": \"who are you?\"\n}\n]\n# Conversation with mixed media\nconversation4 = [\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"/path/to/image.jpg\"},\n{\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n{\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n{\"type\": \"text\", \"text\": \"What are the elements can you see and hear in these medias?\"},\n],\n}\n]\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n# Batch Inference\ntext_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nUsage Tips\nPrompt for audio output\nIf users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n{\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n],\n}\nUse audio in video\nIn the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.\n# first place, in data preprocessing\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=True)\n# second place, in model processor\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\",\npadding=True, use_audio_in_video=True)\n#  third place, in model inference\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=True)\nIt is worth noting that during a multi-round conversation, the use_audio_in_video parameter in these places must be set to the same, otherwise unexpected results will occur.\nUse audio output or not\nThe model supports both text and audio outputs, if users do not need audio outputs, they can call model.disable_talker() after init the model. This option will save about ~2GB of GPU memory but the return_audio option for generate function will only allow to be set at False.\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-Omni-7B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nmodel.disable_talker()\nIn order to obtain a flexible experience, we recommend that users can decide whether to return audio when generate function is called. If return_audio is set to False, the model will only return text outputs to get text responses faster.\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-Omni-7B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n...\ntext_ids = model.generate(**inputs, return_audio=False)\nChange voice type of output audio\nQwen2.5-Omni supports the ability to change the voice of the output audio. The \"Qwen/Qwen2.5-Omni-7B\" checkpoint support two voice types as follow:\nVoice Type\nGender\nDescription\nChelsie\nFemale\nA honeyed, velvety voice that carries a gentle warmth and luminous clarity.\nEthan\nMale\nA bright, upbeat voice with infectious energy and a warm, approachable vibe.\nUsers can use the speaker parameter of generate function to specify the voice type. By default, if speaker is not specified, the default voice type is Chelsie.\ntext_ids, audio = model.generate(**inputs, speaker=\"Chelsie\")\ntext_ids, audio = model.generate(**inputs, speaker=\"Ethan\")\nFlash-Attention 2 to speed up generation\nFirst, make sure to install the latest version of Flash Attention 2:\npip install -U flash-attn --no-build-isolation\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the flash attention repository. FlashAttention-2 can only be used when a model is loaded in torch.float16 or torch.bfloat16.\nTo load and run a model using FlashAttention-2, add attn_implementation=\"flash_attention_2\" when loading the model:\nfrom transformers import Qwen2_5OmniForConditionalGeneration\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-Omni-7B\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\n)\nCitation\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n@article{Qwen2.5-Omni,\ntitle={Qwen2.5-Omni Technical Report},\nauthor={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\njournal={arXiv preprint arXiv:2503.20215},\nyear={2025}\n}",
    "nvidia/Nemotron-H-4B-Instruct-128K": "Nemotron-H-4B-Instruct-128K\nModel Overview\nLicense/Terms of Use\nModel Architecture\nDeployment Geography: Global\nUse Case: This model is intended for developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. This model is also suitable for typical instruction-following tasks.\nRelease Date:\nInput\nOutput\nSoftware Integration\nModel Version\nReferences\nPrompt Format\nTraining, Testing, and Evaluation Datasets\nEvaluation Datasets\nPotential Known Risks for Usage\nInference\nEthical Considerations\nExample\nNemotron-H-4B-Instruct-128K\nModel Developer: NVIDIA\nModel Dates:\nOctober 2024 - March 2025\nData Freshness:\nSeptember 2024\nThe pretraining data has a cutoff date of September 2024.\nModel Overview\nNVIDIA Nemotron-H-4B-Instruct-128K is a large language model (LLM) developed by NVIDIA, optimized for single and multi-turn chat, instruction following, and tool-calling use-cases. It uses a hybrid model architecture that consists primarily of Mamba-2 and MLP layers combined with just four Attention layers. The model is an aligned version of Nemotron-H-4B-Base-8K, and features a 128K context length. The supported languages include: English, German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, and Chinese.\nThe model underwent a multi-phase post-training process including multiple supervised fine-tuning stages for math, code, science, and then chat, instruction following, and tool-calling, followed by multiple preference tuning stages using Reward-aware Preference Optimization (RPO) for both chat and instruction-following.\nThe base model was pruned and distilled from Nemotron-H-Base-8K using our hybrid language model compression technique. For more details, please refer to the paper.\nThe paper has been accepted for publication at NeurIPS 2025.\nThis model is for research and development only.\nLicense/Terms of Use\nGOVERNING TERMS: Use of this model is governed by the NVIDIA Internal Scientific Research and Development Model License\nModel Architecture\nArchitecture Type: Transformer\nNetwork Architecture: Nemotron-Hybrid\nThis model has 4B of model parameters.\nDeployment Geography: Global\nUse Case: This model is intended for developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. This model is also suitable for typical instruction-following tasks.\nRelease Date:\nHuggingface: 10/23/2025 via https://huggingface.co/\nInput\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: One-Dimensional (1D): Sequences\nOther Properties Related to Input: Context length up to 128K. Supported languages include German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English.\nOutput\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: One-Dimensional (1D): Sequences\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration\nRuntime Engine(s): NeMo 24.12\nSupported Hardware Microarchitecture Compatibility: NVIDIA H100-80GB, NVIDIA A100\nOperating System(s): Linux\nModel Version\nv1.0\nReferences\n[2504.11409] Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning\nPrompt Format\n<SPECIAL_10>System\\n{system prompt}\\n<SPECIAL_11>User\\n{user prompt}\\n<SPECIAL_11>Assistant\\n\nNote: Newline should be present after the last Assistant as a generation prompt.\nTraining, Testing, and Evaluation Datasets\nThe data for post-training phases is a compilation of supervised fine-tuning and preference tuning data for improving math, code, science, chat, tool-calling, and instruction following capabilities.\nData Collection for Training & Testing Datasets:\nHybrid: Automated, Human, Synthetic\nData Labeling for Training & Testing Datasets:\nHybrid: Automated, Human, Synthetic\nEvaluation Datasets\nWe used the datasets listed in the next section to evaluate the model.\nData Collection for Training Datasets:\nHybrid: Automated, Human, Synthetic\nData Labeling for Training Datasets:\nHybrid: Automated, Human, Synthetic\nChat & Instruction Following Evaluations:\nMT-Bench 0-shot\nIFEval Strict Average 0-shot\n7.9\n76.24\nMT-Bench - A set of 80 multi-turn, open-ended questions for evaluating chat abilities. We use GPT-4-Turbo as the judge model. Dataset & Code\nIFEval - Contains 500 verifiable instructions to test instruction following abilities of language models. We report the average of prompt and instruction level scores in the strict category. Dataset\nPrompt:\n<SPECIAL_10>System\n<SPECIAL_11>User\n{question}\n<SPECIAL_11>Assistant\nCoding Evaluations:\nMBPP 0-shot\nMBPP+ 0-shot\nHumanEval 0-shot\nHumanEval+ 0-shot\n78.6\n68.25\n76.2\n70.85\nMBPP - Evaluates ability to generate solutions for Python programming tasks. Dataset\nMBPP+ - Extended version of MBPP with additional tests. Dataset\nHumanEval - Tests code generation and completion abilities in Python. Dataset\nHumanEval+ - Extended version of HumanEval with additional tests. Dataset\nPrompt:\n<SPECIAL_10>System\n<SPECIAL_11>User\nYou are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to user instructions.\n@@ Instruction\nHere is the given problem and test examples:\n{question}\nPlease use the python programming language to solve this problem.\nPlease make sure that your code includes the functions from the test samples and that the input and output formats of these functions match the test samples.\nPlease return all completed codes in one code block.\nThis code block should be in the following format:\n\n<SPECIAL_11>Assistant\nMath Evaluations:\nGSM8K 0-shot\nMATH-500 0-shot\n88.93\n76.4\nGSM8K - Evaluates grade school level mathematical word problem solving. Dataset\nMATH-500 - A subset of 500 questions from the MATH benchmark. Dataset\nPrompt:\n<SPECIAL_10>System\n<SPECIAL_11>User\nBelow is a math question. I want you to reason through the steps and then give a final answer. Your final answer should be in \\boxed{}.\nQuestion: {question}\n<SPECIAL_11>Assistant\nTool-Calling Evaluations:\nBFCL v2 Live Overall Accuracy 0-shot\n65.88\nBFCL v2 Live - Evaluates tool-calling ability of language models over multiple categories in real-world scenarios. Dataset\nPrompt:\n<SPECIAL_10>System\n<AVAILABLE_TOOLS>[{\"name\": \"func_name1\", \"description\": \"func_desc1\", \"parameters\": {\"type\": \"dict\", \"required\": [\"param1\"], \"properties\": {\"param1\": {\"type\": \"param_type1\", \"description\": \"param_desc1\", \"default\": \"default_value1\"}}}}, {\"name\": \"func_name2\",...]</AVAILABLE_TOOLS>\n<SPECIAL_11>User\n{question}\n<SPECIAL_11>Assistant\nTool Call Response Format:\n<TOOLCALL>[{{\"name\": \"func_name1\", \"arguments\": {{\"params_name1\": \"params_value1\", \"params_name2\": \"params_value2\"}}}}, {{\"name\": \"func_name2\", \"arguments\": {{\"params_name1\": \"params_value1\", \"params_name2\": \"params_value2\"}}}}]</TOOLCALL>\nGeneral Evaluations:\nMMLU 0-shot (Generative)\n66.96\nMMLU - Tests knowledge across 57 subjects including science, humanities, math and more. Dataset\nPrompt:\n<SPECIAL_10>System\n<SPECIAL_11>User\nBelow is a multi-choice question about {subject}. You must reply with only a single letter (either A, B, C or D).\nQuestion: {question}\n<SPECIAL_11>Assistant\nPotential Known Risks for Usage\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. Code produced by the model may not always model real-world contexts and should be checked. The model demonstrates weakness to alignment-breaking attacks. Users are advised to deploy language model guardrails alongside this model to prevent potentially harmful outputs. The model may generate answers that are inaccurate, omit key information, or include irrelevant or redundant text.\nInference\nEngine: NeMo\nTest Hardware NVIDIA H100-80GB\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Responsible Use Guide available at http://nvidia.com/nemotron-responsible-use.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nExample\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Load the tokenizer and model\ntokenizer  = AutoTokenizer.from_pretrained(\"nvidia/Nemotron-H-4B-Instruct-128K\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"nvidia/Nemotron-H-4B-Instruct-128K\", torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n# Use the prompt template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(tokenized_chat)\nprint(tokenizer.decode(outputs[0]))",
    "pyannote/speaker-diarization-community-1": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nYour input helps us strengthen the pyannote community and improve our open-source offerings. This pipeline is released under the CC-BY-4.0 license and will always remain freely accessible. By providing your details, you agree that we may email you occasionally with important news about pyannote models, invitations to try premium pipelines, and information about specific services designed for researchers and professionals like you.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\ncommunity-1 speaker diarization\nSetup\nQuick start\nBenchmark\nProcessing on GPU\nProcessing from memory\nMonitoring progress\nControlling the number of speakers\nExclusive speaker diarization\nOffline use\nCitations\nAcknowledgment\ncommunity-1 speaker diarization\nThis pipeline ingests mono audio sampled at 16kHz and outputs speaker diarization.\nstereo or multi-channel audio files are automatically downmixed to mono by averaging the channels.\naudio files sampled at a different rate are resampled to 16kHz automatically upon loading.\nThe main improvements brought by Community-1 are:\nimproved speaker assignment and counting\nsimpler reconciliation with transcription timestamps with exclusive speaker diarization\neasy offline use (i.e. without internet connection)\n(optionally) hosted on pyannoteAI cloud\nSetup\npip install pyannote.audio\nAccept user conditions\nCreate access token at hf.co/settings/tokens.\nQuick start\n# download the pipeline from Huggingface\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\n\"pyannote/speaker-diarization-community-1\",\ntoken=\"{huggingface-token}\")\n# run the pipeline locally on your computer\noutput = pipeline(\"audio.wav\")\n# print the predicted speaker diarization\nfor turn, speaker in output.speaker_diarization:\nprint(f\"{speaker} speaks between t={turn.start:.3f}s and t={turn.end:.3f}s\")\nBenchmark\nOut of the box, Community-1 is much better than speaker-diarization-3.1.\nWe report diarization error rates (in %) on large collection of academic benchmarks (fully automatic processing, no forgiveness collar, nor skipping overlapping speech).\nBenchmark (last updated in 2025-09)\nlegacy (3.1)\ncommunity-1\nprecision-2\nAISHELL-4\n12.2\n11.7\n11.4\nAliMeeting (channel 1)\n24.5\n20.3\n15.2\nAMI (IHM)\n18.8\n17.0\n12.9\nAMI (SDM)\n22.7\n19.9\n15.6\nAVA-AVD\n49.7\n44.6\n37.1\nCALLHOME (part 2)\n28.5\n26.7\n16.6\nDIHARD 3 (full)\n21.4\n20.2\n14.7\nEgo4D (dev.)\n51.2\n46.8\n39.0\nMSDWild\n25.4\n22.8\n17.3\nRAMC\n22.2\n20.8\n10.5\nREPERE (phase2)\n7.9\n8.9\n7.4\nVoxConverse (v0.3)\n11.2\n11.2\n8.5\nPrecision-2 model is even better and can be tested like this:\nCreate an API key on pyannoteAI dashboard (free credits included)\nChange one line of code\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\n-     'pyannote/speaker-diarization-community-1', token=\"{huggingface-token}\")\n+     'pyannote/speaker-diarization-precision-2', token=\"{pyannoteAI-api-key}\")\ndiarization = pipeline(\"audio.wav\")  # runs on pyannoteAI servers\nProcessing on GPU\npyannote.audio pipelines run on CPU by default.\nYou can send them to GPU with the following lines:\nimport torch\npipeline.to(torch.device(\"cuda\"))\nProcessing from memory\nPre-loading audio files in memory may result in faster processing:\nwaveform, sample_rate = torchaudio.load(\"audio.wav\")\noutput = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})\nMonitoring progress\nHooks are available to monitor the progress of the pipeline:\nfrom pyannote.audio.pipelines.utils.hook import ProgressHook\nwith ProgressHook() as hook:\noutput = pipeline(\"audio.wav\", hook=hook)\nControlling the number of speakers\nIn case the number of speakers is known in advance, one can use the num_speakers option:\noutput = pipeline(\"audio.wav\", num_speakers=2)\nOne can also provide lower and/or upper bounds on the number of speakers using min_speakers and max_speakers options:\noutput = pipeline(\"audio.wav\", min_speakers=2, max_speakers=5)\nExclusive speaker diarization\nCommunity-1 pretrained pipeline returns a new exclusive speaker diarization, on top of the regular speaker diarization, available as output.exclusive_speaker_diarization.\nThis is a feature which is backported from our latest commercial model that simplifies the reconciliation between fine-grained speaker diarization timestamps and (sometimes not so precise) transcription timestamps.\nOffline use\nIn the terminal, copy the pipeline on disk:\n# make sure git-lfs is installed (https://git-lfs.com)\ngit lfs install\n# create a directory on disk\nmkdir /path/to/directory\n# when prompted for a password, use an access token with write permissions.\n# generate one from your settings: https://huggingface.co/settings/tokens\ngit clone https://hf.co/pyannote/speaker-diarization-community-1 /path/to/directory/pyannote-speaker-diarization-community-1\nIn Python, use the pipeline without internet connection:\n# load pipeline from disk (works without internet connection)\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained('/path/to/directory/pyannote-speaker-diarization-community-1')\n# run the pipeline locally on your computer\noutput = pipeline(\"audio.wav\")\nCitations\nSpeaker segmentation model\n@inproceedings{Plaquet23,\nauthor={Alexis Plaquet and Herv√© Bredin},\ntitle={{Powerset multi-class cross entropy loss for neural speaker diarization}},\nyear=2023,\nbooktitle={Proc. INTERSPEECH 2023},\n}\nSpeaker embedding model\n@inproceedings{Wang2023,\ntitle={Wespeaker: A research and production oriented speaker embedding learning toolkit},\nauthor={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},\nbooktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\npages={1--5},\nyear={2023},\norganization={IEEE}\n}\nSpeaker clustering\n@article{Landini2022,\nauthor={Landini, Federico and Profant, J{\\'a}n and Diez, Mireia and Burget, Luk{\\'a}{\\v{s}}},\ntitle={{Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: theory, implementation and analysis on standard tasks}},\nyear={2022},\njournal={Computer Speech \\& Language},\n}\nAcknowledgment\nTraining and tuning made possible thanks to GENCI on the Jean Zay supercomputer."
}