{
    "ibm-nasa-geospatial/Prithvi-EO-2.0-300M": "Prithvi-EO-2.0\nArchitecture Overview\nPre-trained Models\nBenchmarking\nDemo and inference\nFinetuning\nFeedback\nCitation\nPrithvi-EO-2.0\nPrithvi-EO-2.0 is the second generation EO foundation model jointly developed by IBM, NASA, and J√ºlich Supercomputing Centre.\nArchitecture Overview\nPrithvi-EO-2.0 is based on the ViT architecture, pretrained using a masked autoencoder (MAE) approach, with two major modifications as shown in the figure below.\nFirst, we replaced the 2D patch embeddings and 2D positional embeddings with 3D versions to support inputs with spatiotemporal characteristics, i.e., a sequence of T images of size (H, W). Our 3D patch embeddings consist of a 3D convolutional layer, dividing the 3D input into non-overlapping cubes of size (t, h, w) for time, height, and width dimensions, respectively. For the 3D positional encodings, we first generate 1D sin/cos encodings individually for each dimension and then combine them together into a single, 3D positional encoding.\nSecond, we considered geolocation (center latitude and longitude) and date of acquisition (year and day-of-year ranging 1-365) in the pretraining of the TL model versions. Both encoder and decoder receive time and location information for each sample and encodes them independently using 2D sin/cos encoding. They are added to the embedded tokens via a weighted sum with learned weights: one for time and one for location and separate weights for encoder and decoder. Since this metadata is often not available, we added a drop mechanism during pretraining that randomly drops the geolocation and/or the temporal data to help the model learn how to handle the absence of this information.\nPre-trained Models\nModel\nDetails\nWeights\nPrithvi-EO-2.0-tiny-TL\nPretrained 5M parameter model with temporal and location embeddings\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-tiny-TL\nPrithvi-EO-2.0-100M-TL\nPretrained 100M parameter model with temporal and location embeddings\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-100M-TL\nPrithvi-EO-2.0-300M\nPretrained 300M parameter model\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M\nPrithvi-EO-2.0-300M-TL\nPretrained 300M parameter model with temporal and location embeddings\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-TL\nPrithvi-EO-2.0-600M\nPretrained 600M parameter model\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-600M\nPrithvi-EO-2.0-600M-TL\nPretrained 600M parameter model with temporal and location embeddings\nhttps://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-600M-TL\nThe models were pre-trained at the J√ºlich Supercomputing Centre with NASA's HLS V2 product (30m granularity) using 4.2M samples with six bands in the following order: Blue, Green, Red, Narrow NIR, SWIR, SWIR 2.\nBenchmarking\nWe validated the Prithvi-EO-2.0 models through extensive experiments using GEO-bench. Prithvi-EO-2.0-600M-TL outperforms the previous Prithvi-EO model by 8% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m).\nDemo and inference\nWe provide a demo running Prithvi-EO-2.0-300M-TL here.\nThere is also an inference script (inference.py) that allows to run the image reconstruction on a set of HLS images assumed to be from the same location at different timestamps (see example below). These should be provided in chronological order in geotiff format, including the channels described above (Blue, Green, Red, Narrow NIR, SWIR 1, SWIR 2) in reflectance units.\npython inference.py --data_files t1.tif t2.tif t3.tif t4.tif --input_indices <optional, space separated 0-based indices of the six Prithvi channels in your input>\nFinetuning\nYou can finetune the model using TerraTorch. Examples of configs and notebooks are provided in the project repository: github.com/NASA-IMPACT/Prithvi-EO-2.0.Example Notebooks:\nMultitemporal Crop Segmentation >>Try it on Colab<< (Choose T4 GPU runtime)Landslide Segmentation >>Try it on Colab<< (Choose T4 GPU runtime)Carbon Flux Prediction (Regression)\nIf you plan to use Prithvi in your custom PyTorch pipeline, you can build the backbone with:\nfrom terratorch.registry import BACKBONE_REGISTRY\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_tiny_tl\", pretrained=True)\nFind more information on model usage in our Prithvi Docs.\nFeedback\nYour feedback is invaluable to us. If you have any feedback about the model, please feel free to share it with us. You can do this by starting a discussion in this HF repository or submitting an issue to TerraTorch on GitHub.\nCitation\nIf this model helped your research, please cite Prithvi-EO-2.0 in your publications.\n@article{Prithvi-EO-V2-preprint,\nauthor          = {Szwarcman, Daniela and Roy, Sujit and Fraccaro, Paolo and G√≠slason, √ûorsteinn El√≠ and Blumenstiel, Benedikt and Ghosal, Rinki and de Oliveira, Pedro Henrique and de Sousa Almeida, Jo√£o Lucas and Sedona, Rocco and Kang, Yanghui and Chakraborty, Srija and Wang, Sizhe and Kumar, Ankur and Truong, Myscon and Godwin, Denys and Lee, Hyunho and Hsu, Chia-Yu and Akbari Asanjan, Ata and Mujeci, Besart and Keenan, Trevor and Ar√©volo, Paulo and Li, Wenwen and Alemohammad, Hamed and Olofsson, Pontus and Hain, Christopher and Kennedy, Robert and Zadrozny, Bianca and Cavallaro, Gabriele and Watson, Campbell and Maskey, Manil and Ramachandran, Rahul and Bernabe Moreno, Juan},\ntitle           = {{Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications}},\njournal         = {arXiv preprint arXiv:2412.02732},\nyear            = {2024}\n}",
    "bwittmann/vesselFM": "vesselFM\nCheckpoints\nCiting vesselFM\nvesselFM\nTL;DR: VesselFM is a foundation model for universal 3D blood vessel segmentation in arbitrary imaging domains.\nFor details, please refer to our preprint (https://arxiv.org/pdf/2411.17386) and our GitHub repo (https://github.com/bwittmann/vesselFM).\nCheckpoints\nWe provide the following checkpoints:\nvesselFM_base.pt: VesselFM pre-trained on our three proposed data sources (D_real, D_drand, and D_flow). This checkpoint will be automatically downloaded in vesselfm/seg/inference.py.\nCiting vesselFM\nIf you find our work useful, please cite:\n@InProceedings{Wittmann_2025_CVPR,\nauthor    = {Wittmann, Bastian and Wattenberg, Yannick and Amiranashvili, Tamaz and Shit, Suprosanna and Menze, Bjoern},\ntitle     = {vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation},\nbooktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\nmonth     = {June},\nyear      = {2025},\npages     = {20874-20884}\n}",
    "microsoft/TRELLIS-image-large": "TRELLIS Image Large\nTRELLIS Image Large\nThe image conditioned version of TRELLIS, a large 3D genetive model. It was introduced in the paper Structured 3D Latents for Scalable and Versatile 3D Generation.\nProject page: https://trellis3d.github.io/\nCode: https://github.com/Microsoft/TRELLIS",
    "bartowski/EuroLLM-9B-Instruct-GGUF": "Llamacpp imatrix Quantizations of EuroLLM-9B-Instruct\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of EuroLLM-9B-Instruct\nUsing llama.cpp release b4240 for quantization.\nOriginal model: https://huggingface.co/utter-project/EuroLLM-9B-Instruct\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nEuroLLM-9B-Instruct-f16.gguf\nf16\n18.31GB\nfalse\nFull F16 weights.\nEuroLLM-9B-Instruct-Q8_0.gguf\nQ8_0\n9.73GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nEuroLLM-9B-Instruct-Q6_K_L.gguf\nQ6_K_L\n7.77GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nEuroLLM-9B-Instruct-Q6_K.gguf\nQ6_K\n7.51GB\nfalse\nVery high quality, near perfect, recommended.\nEuroLLM-9B-Instruct-Q5_K_L.gguf\nQ5_K_L\n6.84GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nEuroLLM-9B-Instruct-Q5_K_M.gguf\nQ5_K_M\n6.52GB\nfalse\nHigh quality, recommended.\nEuroLLM-9B-Instruct-Q5_K_S.gguf\nQ5_K_S\n6.37GB\nfalse\nHigh quality, recommended.\nEuroLLM-9B-Instruct-Q4_K_L.gguf\nQ4_K_L\n5.97GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nEuroLLM-9B-Instruct-Q4_K_M.gguf\nQ4_K_M\n5.58GB\nfalse\nGood quality, default size for most use cases, recommended.\nEuroLLM-9B-Instruct-Q3_K_XL.gguf\nQ3_K_XL\n5.37GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nEuroLLM-9B-Instruct-Q4_K_S.gguf\nQ4_K_S\n5.32GB\nfalse\nSlightly lower quality with more space savings, recommended.\nEuroLLM-9B-Instruct-IQ4_NL.gguf\nIQ4_NL\n5.31GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nEuroLLM-9B-Instruct-Q4_0.gguf\nQ4_0\n5.30GB\nfalse\nLegacy format, offers online repacking for ARM CPU inference.\nEuroLLM-9B-Instruct-Q4_0_8_8.gguf\nQ4_0_8_8\n5.29GB\nfalse\nOptimized for ARM and AVX inference. Requires 'sve' support for ARM (see details below). Don't use on Mac.\nEuroLLM-9B-Instruct-Q4_0_4_8.gguf\nQ4_0_4_8\n5.29GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see details below). Don't use on Mac.\nEuroLLM-9B-Instruct-Q4_0_4_4.gguf\nQ4_0_4_4\n5.29GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, not for use with GPUs. Don't use on Mac.\nEuroLLM-9B-Instruct-IQ4_XS.gguf\nIQ4_XS\n5.05GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nEuroLLM-9B-Instruct-Q3_K_L.gguf\nQ3_K_L\n4.91GB\nfalse\nLower quality but usable, good for low RAM availability.\nEuroLLM-9B-Instruct-Q3_K_M.gguf\nQ3_K_M\n4.55GB\nfalse\nLow quality.\nEuroLLM-9B-Instruct-IQ3_M.gguf\nIQ3_M\n4.29GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nEuroLLM-9B-Instruct-Q3_K_S.gguf\nQ3_K_S\n4.14GB\nfalse\nLow quality, not recommended.\nEuroLLM-9B-Instruct-Q2_K_L.gguf\nQ2_K_L\n4.11GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nEuroLLM-9B-Instruct-IQ3_XS.gguf\nIQ3_XS\n3.98GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nEuroLLM-9B-Instruct-Q2_K.gguf\nQ2_K\n3.59GB\nfalse\nVery low quality but surprisingly usable.\nEuroLLM-9B-Instruct-IQ2_M.gguf\nIQ2_M\n3.33GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/EuroLLM-9B-Instruct-GGUF --include \"EuroLLM-9B-Instruct-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/EuroLLM-9B-Instruct-GGUF --include \"EuroLLM-9B-Instruct-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (EuroLLM-9B-Instruct-Q8_0) or download them all in place (./)\nQ4_0_X_X information\nNew: Thanks to efforts made to have online repacking of weights in this PR, you can now just use Q4_0 if your llama.cpp has been compiled for your ARM device.\nSimilarly, if you want to get slightly better performance, you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information\nThese are *NOT* for Metal (Apple) or GPU (nvidia/AMD/intel) offloading, only ARM chips (and certain AVX2/AVX512 CPUs).\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nIf you're using a CPU that supports AVX2 or AVX512 (typically server CPUs and AMD's latest Zen5 CPUs) and are not offloading to a GPU, the Q4_0_8_8 may offer a nice speed as well:\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "TIGER-Lab/VISTA-Mantis": "VISTA-Mantis\nVideo Instruction Data Synthesis Pipeline\nCitation\nVISTA-Mantis\nThis repo contains model checkpoints for VISTA-Mantis. VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\nüåê Homepage | üìñ arXiv | üíª GitHub | ü§ó VISTA-400K | ü§ó Models | ü§ó HRVideoBench\nVideo Instruction Data Synthesis Pipeline\nVISTA leverages insights from image and video classification data augmentation techniques such as CutMix, MixUp and VideoMix, which demonstrate that training on synthetic data created by overlaying or mixing multiple images or videos results in more robust classifiers. Similarly, our method spatially and temporally combines videos to create (artificial) augmented video samples with longer durations and higher resolutions, followed by synthesizing instruction data based on these new videos. Our data synthesis pipeline utilizes existing public video-caption datasets, making it fully open-sourced and scalable. This allows us to construct VISTA-400K, a high-quality video instruction-following dataset aimed at improving the long and high-resolution video understanding capabilities of video LMMs.\nCitation\nIf you find our paper useful, please cite us with\n@misc{ren2024vistaenhancinglongdurationhighresolution,\ntitle={VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation},\nauthor={Weiming Ren and Huan Yang and Jie Min and Cong Wei and Wenhu Chen},\nyear={2024},\neprint={2412.00927},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2412.00927},\n}",
    "tiiuae/Falcon3-10B-Base": "Falcon3-10B-Base\nModel Details\nGetting started\nBenchmarks\nUseful links\nTechnical Report\nCitation\nOpen LLM Leaderboard Evaluation Results\nFalcon3-10B-Base\nFalcon3 family of Open Foundation Models is a set of pretrained and instruct LLMs ranging from 1B to 10B parameters.\nThis repository contains the Falcon3-10B-Base. It achieves state-of-the-art results (at the time of release) on reasoning, language understanding, instruction following, code and mathematics tasks.\nFalcon3-10B-Base supports 4 languages (English, French, Spanish, Portuguese) and a context length of up to 32K.\n‚ö†Ô∏è This is a raw, pretrained model, which should be further finetuned using SFT, RLHF, continued pretraining, etc. for most use cases.\nModel Details\nArchitecture\nTransformer-based causal decoder-only architecture\n40 decoder blocks\nGrouped Query Attention (GQA) for faster inference: 12 query heads and 4 key-value heads\nWider head dimension: 256\nHigh RoPE value to support long context understanding: 1000042\nUses SwiGLu and RMSNorm\n32K context length\n131K vocab size\nDepth up-scaled from Falcon3-7B-Base with continual pretraining on 2 Teratokens of datasets comprising of web, code, STEM, high quality and mutlilingual data using 1024 H100 GPU chips\nSupports EN, FR, ES, PT\nDeveloped by Technology Innovation Institute\nLicense: TII Falcon-LLM License 2.0\nModel Release Date: December 2024\nGetting started\nClick to expand\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=\"tiiuae/Falcon3-10B-Base\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nresponse = pipe(\"Question: How many hours in one day? Answer: \")\nprint(response[0]['generated_text'])\nBenchmarks\nWe report in the following table our internal pipeline benchmarks.\nWe use lm-evaluation harness.\nWe report raw scores.\nWe use same batch-size across all models.\nCategory\nBenchmark\nGemma2-9B\nYi1.5-9B\nMistral-Nemo-Base-2407 (12B)\nFalcon3-10B-Base\nGeneral\nMMLU (5-shot)\n70.8\n69.6\n68.8\n73.1\nMMLU-PRO (5-shot)\n41.4\n39.3\n34.7\n42.5\nIFEval\n21.3\n29.1\n16.1\n36.4\nMath\nGSM8K (5-shot)\n69.1\n63.8\n55.3\n81.4\nMATH Lvl-5 (4-shot)\n10.5\n9.2\n4.9\n22.9\nReasoning\nArc Challenge (25-shot)\n67.5\n61.7\n64.4\n66.8\nGPQA (0-shot)\n33.4\n36.6\n28.8\n34.1\nMUSR (0-shot)\n45.3\n43.3\n39.2\n44.2\nBBH (3-shot)\n54.3\n51.3\n50.2\n59.7\nCommonSense Understanding\nPIQA (0-shot)\n83.0\n80.5\n82.1\n79.4\nSciQ (0-shot)\n97.1\n95.2\n95.2\n93.5\nWinogrande (0-shot)\n74.2\n72.7\n73.2\n73.6\nOpenbookQA (0-shot)\n47.2\n45.2\n47.2\n45.0\nUseful links\nView our release blogpost.\nFeel free to join our discord server if you have any questions or to interact with our researchers and developers.\nTechnical Report\nComing soon....\nCitation\nIf the Falcon3 family of models were helpful to your work, feel free to give us a cite.\n@misc{Falcon3,\ntitle = {The Falcon 3 Family of Open Models},\nurl = {https://huggingface.co/blog/falcon3},\nauthor = {Falcon-LLM Team},\nmonth = {December},\nyear = {2024}\n}\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n27.59\nIFEval (0-Shot)\n36.48\nBBH (3-Shot)\n41.38\nMATH Lvl 5 (4-Shot)\n24.77\nGPQA (0-shot)\n12.75\nMuSR (0-shot)\n14.17\nMMLU-PRO (5-shot)\n36.00",
    "AnyModal/LaTeX-OCR-Llama-3.2-1B": "AnyModal/LaTeX-OCR-Llama-3.2-1B\nTrained On\nHow to Use\nInstallation\nInference\nProject and Training Scripts\nProject Details\nAnyModal/LaTeX-OCR-Llama-3.2-1B\nAnyModal/LaTeX-OCR-Llama-3.2-1B is an experimental model designed to convert images of handwritten and printed mathematical equations into LaTeX representations. Developed within the AnyModal framework, this model combines a google/siglip-so400m-patch14-384 image encoder with the Llama 3.2-1B language model. It has been trained on 20% of the unsloth/LaTeX_OCR dataset, which itself is a subset of the linxy/LaTeX_OCR dataset.\nTrained On\nThis model was trained on the unsloth/LaTeX_OCR dataset. The dataset contains 1% of samples from the larger linxy/LaTeX_OCR dataset, which includes images of mathematical equations annotated with their corresponding LaTeX expressions. The current model was trained on 20% of the unsloth dataset.\nHow to Use\nInstallation\nClone the AnyModal Project:\ngit clone https://github.com/ritabratamaiti/AnyModal.git\nNavigate to the LaTeX OCR Project (see https://github.com/ritabratamaiti/AnyModal/tree/main/LaTeX%20OCR)\nInstall the required dependencies:\npip install torch transformers torchvision huggingface_hub tqdm matplotlib Pillow\nInference\nBelow is an example of generating LaTeX code from an image:\nimport llm\nimport anymodal\nimport torch\nimport vision\nfrom PIL import Image\nfrom huggingface_hub import hf_hub_download, snapshot_download\n# Load language model and tokenizer\nllm_tokenizer, llm_model = llm.get_llm(\n\"meta-llama/Llama-3.2-1B\",\naccess_token=\"GET_YOUR_OWN_TOKEN_FROM_HUGGINGFACE\",\nquantized=False,\nuse_peft=False,\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nllm_model.to(device)\nllm_hidden_size = llm.get_hidden_size(llm_tokenizer, llm_model)\n# Load vision model components\nimage_processor, vision_model, vision_hidden_size = vision.get_image_encoder(\n\"google/siglip-so400m-patch14-384\", use_peft=False\n)\n# Initialize vision tokenizer and encoder\nvision_encoder = vision.VisionEncoder(vision_model)\nvision_tokenizer = vision.Projector(vision_hidden_size, llm_hidden_size, num_hidden=1)\n# Initialize MultiModalModel\nmultimodal_model = anymodal.MultiModalModel(\ninput_processor=None,\ninput_encoder=vision_encoder,\ninput_tokenizer=vision_tokenizer,\nlanguage_tokenizer=llm_tokenizer,\nlanguage_model=llm_model,\nprompt_text=\"The latex expression of the equation in the image is: \",\n)\n# Load pre-trained weights\nif not os.path.exists(\"latex_ocr\"):\nos.makedirs(\"latex_ocr\")\nsnapshot_download(\"AnyModal/latex-ocr-Llama-3.2-1B\", local_dir=\"latex_ocr\")\nmultimodal_model._load_model(\"latex_ocr\")\n# Generate LaTeX expression from an image\nimage_path = \"example_equation.jpg\"  # Path to your image\nimage = Image.open(image_path).convert(\"RGB\")\nprocessed_image = image_processor(image, return_tensors=\"pt\")\nprocessed_image = {key: val.squeeze(0) for key, val in processed_image.items()}\n# Generate LaTeX caption\ngenerated_caption = multimodal_model.generate(processed_image, max_new_tokens=120)\nprint(\"Generated LaTeX Caption:\", generated_caption)\nProject and Training Scripts\nThis model is part of the AnyModal LaTeX OCR Project.\nTraining Script: train.py\nInference Script: inference.py\nRefer to the project repository for further implementation details.\nProject Details\nVision Encoder: The google/siglip-so400m-patch14-384 model, pre-trained for visual feature extraction, was used as the image encoder.\nProjector Network: A dense projection network aligns visual features with the Llama 3.2-1B text generation model.\nLanguage Model: Llama 3.2-1B, a small causal language model, generates the LaTeX expression.\nThis implementation highlights a proof-of-concept approach using a limited training subset. Better performance can likely be achieved by training on more samples and incorporating a text-conditioned image encoder.",
    "jianzongwu/DiffSensei": "Model checkpoint of paper DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation\nPlease see GitHub repo to get the usage\nProject page: https://jianzongwu.github.io/projects/diffsensei",
    "Kijai/HunyuanVideo_comfy": "Safetensors and fp8 version of HunhuanVideo models: https://huggingface.co/tencent/HunyuanVideo\nTo be used with ComfyUI native HunyuanVideo implementation, or my wrapper: https://github.com/kijai/ComfyUI-HunyuanVideoWrapper\nFastVideo's distilled version original from: https://huggingface.co/FastVideo/FastHunyuan\nGGUF's created using city96's scripts, and only works with their nodes: https://github.com/city96/ComfyUI-GGUF",
    "khleeloo/whisper-large-v3-cantonese": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Model ID\nModel Details\nModel Description\nUses\nBias, Risks, and Limitations\nHow to Get Started with the Model\nTraining\nTraining Data\nEvaluation\nTesting Data, Factors & Metrics\nResults\nCitation [optional]\nModel Card Authors [optional]\nModel Card for Model ID\nThis model is a fine-tuned version of the Whisper v3 model, specifically trained for automatic speech recognition (ASR) in Cantonese (Yue). The model has been fine-tuned on data from the Common Voice 17 dataset for 10 epochs with a learning rate of 1e-7.\nModel Details\nModel Architecture: Whisper v3\nLanguage: Cantonese (Yue)\nTraining Dataset: Common Voice 17\nTraining Duration: 10 epochs\nLearning Rate: 1e-7\nFrozen Layers: 12 layers in the decoder are frozen during training\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: khleeloo (Rita Frieske)\nLanguage(s) (NLP): Cantonese\nLicense: apache-2.0\nFinetuned from model [optional]: openai/whisper-large-v3\nUses\nThis model is intended for researchers and developers interested in building applications that require speech recognition capabilities in Cantonese. It can be used in various applications, including:\nVoice assistants\nTranscription services\nAccessibility features for Cantonese speakers\nBias, Risks, and Limitations\nThe model is specifically fine-tuned for Cantonese and may not perform well on other languages or dialects.\nPerformance may vary based on the quality and accent of the audio input.\nThe model's effectiveness is dependent on the diversity and richness of the training data.\nHow to Get Started with the Model\nTo use this model, you can load it using the Hugging Face Transformers library:\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nmodel = WhisperForConditionalGeneration.from_pretrained(\"your_username/whisper-cantonese\")\nprocessor = WhisperProcessor.from_pretrained(\"your_username/whisper-cantonese\")\nTraining\nTraining Data\nmozilla-foundation/common_voice_17_0\nEvaluation\nTesting Data, Factors & Metrics\nCommon Voice_17_0 yue test split Common Voice 15_0 yue test split and Common Voice 15_0 zh-HK test split (these test dataset were used to evaluate Whisper 3.0)\nMetrics\nCharacter Error Rate (CER) since Cantonese is character based language.\nResults\nCV15_0 zh-HK\nCV 15_0 yue\nCV 17_0 yue\nWhisper large v3\n10.8\n16\n-\nWhisper cantonese (ours)\n18.88\n8.77\n7.26\nExplanation: our model was not trained on zh-HK data consisting of more written Cantonese but rather more vernacular Cantonese version (yue) since it is a speech recognition model.\nHence the weaker performance on zh-HK splits of Common Voice dataset.\nSummary\nCitation [optional]\nBibTeX:\n@misc {rita_frieske_2025,\nauthor       = { {Rita Frieske} },\ntitle        = { whisper-large-v3-cantonese },\nyear         = 2025,\nurl          = { https://huggingface.co/khleeloo/whisper-large-v3-cantonese },\ndoi          = { 10.57967/hf/4393 },\npublisher    = { Hugging Face }\n}\nModel Card Authors [optional]\nhttps://khleeloo.github.io/",
    "STMicroelectronics/mobilenetv2": "MobileNet v2\nUse case : Image classification\nModel description\nNetwork information\nNetwork inputs / outputs\nRecommended platforms\nPerformances\nMetrics\nReference NPU memory footprint on food-101 and ImageNet dataset (see Accuracy for details on dataset)\nReference NPU  inference time on food-101 and ImageNet dataset (see Accuracy for details on dataset)\nReference MCU memory footprint based on Flowers and ImageNet dataset (see Accuracy for details on dataset)\nReference MCU inference time based on Flowers and ImageNet dataset (see Accuracy for details on dataset)\nReference MPU inference time based on Flowers and ImageNet dataset (see Accuracy for details on dataset)\nAccuracy with Flowers dataset\nAccuracy with Plant-village dataset\nAccuracy with Food-101 dataset\nAccuracy with person dataset\nAccuracy with ImageNet\nRetraining and Integration in a simple example:\nReferences\nMobileNet v2\nUse case : Image classification\nModel description\nMobileNet v2 is very similar to the original MobileNet, except that it uses inverted residual blocks with bottlenecking features.\nIt has a drastically lower parameter count than the original MobileNet.\nMobileNet models support any input size greater than 32 x 32, with larger image sizes offering better performance.\nAlpha parameter: float, larger than zero, controls the width of the network. This is known as the width multiplier in the MobileNetV2 paper, but the name is kept for consistency with applications.\nIf alpha < 1.0, proportionally decreases the number of filters in each layer.\nIf alpha > 1.0, proportionally increases the number of filters in each layer.\nIf alpha = 1.0, default number of filters from the paper are used at each layer.\n(source: https://keras.io/api/applications/mobilenet/)\nThe model is quantized in int8 using tensorflow lite converter.\nNetwork information\nNetwork Information\nValue\nFramework\nTensorFlow Lite\nMParams alpha=0.35\n1.66 M\nQuantization\nint8\nProvenance\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2\nPaper\nhttps://arxiv.org/pdf/1801.04381.pdf\nThe models are quantized using tensorflow lite converter.\nNetwork inputs / outputs\nFor an image resolution of NxM and P classes\nInput Shape\nDescription\n(1, N, M, 3)\nSingle NxM RGB image with UINT8 values between 0 and 255\nOutput Shape\nDescription\n(1, P)\nPer-class confidence for P classes in FLOAT32\nRecommended platforms\nPlatform\nSupported\nRecommended\nSTM32L0\n[]\n[]\nSTM32L4\n[x]\n[]\nSTM32U5\n[x]\n[]\nSTM32H7\n[x]\n[x]\nSTM32MP1\n[x]\n[x]\nSTM32MP2\n[x]\n[x]\nSTM32N6\n[x]\n[x]\nPerformances\nMetrics\nMeasures are done with default STM32Cube.AI configuration with enabled input / output allocated option.\ntfs stands for \"training from scratch\", meaning that the model weights were randomly initialized before training.\ntl stands for \"transfer learning\", meaning that the model backbone weights were initialized from a pre-trained model, then only the last layer was unfrozen during the training.\nfft stands for \"full fine-tuning\", meaning that the full model weights were initialized from a transfer learning pre-trained model, and all the layers were unfrozen during the training.\nReference NPU memory footprint on food-101 and ImageNet dataset (see Accuracy for details on dataset)\nModel\nDataset\nFormat\nResolution\nSeries\nInternal RAM\nExternal RAM\nWeights Flash\nSTM32Cube.AI version\nSTEdgeAI Core version\nMobileNet v2 0.35 fft\nfood-101\nInt8\n128x128x3\nSTM32N6\n240\n0.0\n680.92\n10.2.0\n2.2.0\nMobileNet v2 0.35 fft\nfood-101\nInt8\n224x224x3\nSTM32N6\n980\n0.0\n695.95\n10.2.0\n2.2.0\nMobileNet v2 1.0 fft\nfood-101\nInt8\n224x224x3\nSTM32N6\n2058\n0.0\n3070.61\n10.2.0\n2.2.0\nMobileNet v2 0.35 fft\nPerson\nInt8\n128x128x3\nSTM32N6\n240\n0.0\n554.94\n10.2.0\n2.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n128x128x3\nSTM32N6\n240\n0.0\n1806.61\n10.2.0\n2.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n224x224x3\nSTM32N6\n980\n0.0\n1821.64\n10.2.0\n2.2.0\nMobileNet v2 1.0\nImageNet\nInt8\n224x224x3\nSTM32N6\n2058\n0.0\n4196.3\n10.2.0\n2.2.0\nMobileNet v2 1.4\nImageNet\nInt8\n224x224x3\nSTM32N6\n2361\n0.0\n7285.86\n10.2.0\n2.2.0\nReference NPU  inference time on food-101 and ImageNet dataset (see Accuracy for details on dataset)\nModel\nDataset\nFormat\nResolution\nBoard\nExecution Engine\nInference time (ms)\nInf / sec\nSTM32Cube.AI version\nSTEdgeAI Core version\nMobileNet v2 0.35 fft\nfood-101\nInt8\n128x128x3\nSTM32N6570-DK\nNPU/MCU\n3.34\n299.4\n10.2.0\n2.2.0\nMobileNet v2 0.35 fft\nfood-101\nInt8\n224x224x3\nSTM32N6570-DK\nNPU/MCU\n6.03\n165.83\n10.2.0\n2.2.0\nMobileNet v2 1.0 fft\nfood-101\nInt8\n224x224x3\nSTM32N6570-DK\nNPU/MCU\n17.31\n57.77\n10.2.0\n2.2.0\nMobileNet v2 0.35 fft\nPerson\nInt8\n224x224x3\nSTM32N6570-DK\nNPU/MCU\n2.95\n338.98\n10.2.0\n2.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n128x128x3\nSTM32N6570-DK\nNPU/MCU\n6.37\n156.98\n10.2.0\n2.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n224x224x3\nSTM32N6570-DK\nNPU/MCU\n9.06\n110.37\n10.2.0\n2.2.0\nMobileNet v2 1.0\nImageNet\nInt8\n224x224x3\nSTM32N6570-DK\nNPU/MCU\n20.3\n49.26\n10.2.0\n2.2.0\nMobileNet v2 1.4\nImageNet\nInt8\n224x224x3\nSTM32N6570-DK\nNPU/MCU\n33.8\n29.95\n10.2.0\n2.2.0\nReference MCU memory footprint based on Flowers and ImageNet dataset (see Accuracy for details on dataset)\nModel\nDataset\nFormat\nResolution\nSeries\nActivation RAM\nRuntime RAM\nWeights Flash\nCode Flash\nTotal RAM\nTotal Flash\nSTM32Cube.AI version\nMobileNet v2 0.35 fft\nFlowers\nInt8\n128x128x3\nSTM32H7\n237.32 KiB\n30.15 KiB\n406.86 KiB\n107.4 KiB\n267.46 KiB\n514.26 KiB\n10.2.0\nMobileNet v2 0.35 fft\nFlowers\nInt8\n224x224x3\nSTM32H7\n832.64 KiB\n30.2 KiB\n406.86 KiB\n107.52 KiB\n862.84 KiB\n514.38 KiB\n10.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n128x128x3\nSTM32H7\n237.32 KiB\n30.15 KiB\n1654.5 KiB KiB\n107.4 KiB\n267.47 KiB\n1762.79 KiB\n10.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n224x224x3\nSTM32H7\n832.64 KiB\n30.2 KiB\n1654.5 KiB\n107.52 KiB\n862.84 KiB\n1762.9 KiB\n10.2.0\nMobileNet v2 1.0\nImageNet\nInt8\n224x224x3\nSTM32H7\n1727.02 KiB\n30.2 KiB\n3458.97 KiB\n157.37 KiB\n1757.22 KiB\n3616.34 KiB\n10.2.0\nMobileNet v2 1.4\nImageNet\nInt8\n224x224x3\nSTM32H7\n2332.2 KiB\n30.2 KiB\n6015.34 KiB\n191.16 KiB\n2362.39 KiB\n6206.53 KiB\n10.2.0\nReference MCU inference time based on Flowers and ImageNet dataset (see Accuracy for details on dataset)\nModel\nDataset\nFormat\nResolution\nBoard\nExecution Engine\nFrequency\nInference time (ms)\nSTM32Cube.AI version\nMobileNet v2 0.35 fft\nFlowers\nInt8\n128x128x3\nSTM32H747I-DISCO\n1 CPU\n400 MHz\n96.52  ms\n10.2.0\nMobileNet v2 0.35 fft\nFlowers\nInt8\n224x224x3\nSTM32H747I-DISCO\n1 CPU\n400 MHz\n297.38 ms\n10.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n128x128x3\nSTM32H747I-DISCO\n1 CPU\n400 MHz\n100.66 ms\n10.2.0\nMobileNet v2 0.35\nImageNet\nInt8\n224x224x3\nSTM32H747I-DISCO\n1 CPU\n400 MHz\n301.58 ms\n10.2.0\nMobileNet v2 1.0\nImageNet\nInt8\n224x224x3\nSTM32H747I-DISCO\n1 CPU\n400 MHz\n1124.79 ms\n10.2.0\nMobileNet v2 1.4\nImageNet\nInt8\n224x224x3\nSTM32H747I-DISCO\n1 CPU\n400 MHz\n2038.28 ms\n10.2.0\nReference MPU inference time based on Flowers and ImageNet dataset (see Accuracy for details on dataset)\nModel\nDataset\nFormat\nResolution\nQuantization\nBoard\nExecution Engine\nFrequency\nInference time (ms)\n%NPU\n%GPU\n%CPU\nX-LINUX-AI version\nFramework\nMobileNet v2 1.0_per_tensor\nImageNet\nInt8\n224x224x3\nper-tensor\nSTM32MP257F-DK2\nNPU/GPU\n800  MHz\n12.39 ms\n81.42\n18.58\n0\nv6.1.0\nOpenVX\nMobileNet v2 1.0\nImageNet\nInt8\n224x224x3\nper-channel **\nSTM32MP257F-DK2\nNPU/GPU\n800  MHz\n76.19 ms\n2.61\n97.39\n0\nv6.1.0\nOpenVX\nMobileNet v2 0.35 fft\nFlowers\nInt8\n224x224x3\nper-channel **\nSTM32MP257F-DK2\nNPU/GPU\n800  MHz\n25.5 ms\n4.31\n95.69\n0\nv6.1.0\nOpenVX\nMobileNet v2 0.35 fft\nFlowers\nInt8\n128x128x3\nper-channel **\nSTM32MP257F-DK2\nNPU/GPU\n800  MHz\n9.12  ms\n12.45\n87.55\n0\nv6.1.0\nOpenVX\nMobileNet v2 1.0_per_tensor\nImageNet\nInt8\n224x224x3\nper-tensor\nSTM32MP157F-DK2\n2 CPU\n800  MHz\n331.69 ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 1.0\nImageNet\nInt8\n128x128x3\nper-channel\nSTM32MP157F-DK2\n2 CPU\n800  MHz\n193.84 ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 0.35 fft\nFlowers\nInt8\n224x224x3\nper-channel\nSTM32MP157F-DK2\n2 CPU\n800  MHz\n54.15 ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 0.35 fft\nFlowers\nInt8\n128x128x3\nper-channel\nSTM32MP157F-DK2\n2 CPU\n800  MHz\n17.44 ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 1.0_per_tensor\nImageNet\nInt8\n224x224x3\nper-tensor\nSTM32MP135F-DK2\n1 CPU\n1000 MHz\n418.33 ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 1.0\nImageNet\nInt8\n128x128x3\nper-channel\nSTM32MP135F-DK2\n1 CPU\n1000 MHz\n310.64 ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 0.35 fft\nFlowers\nInt8\n224x224x3\nper-channel\nSTM32MP135F-DK2\n1 CPU\n1000 MHz\n84.39  ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\nMobileNet v2 0.35 fft\nFlowers\nInt8\n128x128x3\nper-channel\nSTM32MP135F-DK2\n1 CPU\n1000 MHz\n26.85  ms\nNA\nNA\n100\nv6.1.0\nTensorFlowLite 2.18.0\n** To get the most out of MP25 NPU hardware acceleration, please use per-tensor quantization\nAccuracy with Flowers dataset\nDataset details: link , License CC BY 2.0 , Quotation[1] , Number of classes: 5, Number of images: 3 670\nModel\nFormat\nResolution\nTop 1 Accuracy\nMobileNet v2 0.35 tfs\nFloat\n128x128x3\n87.06 %\nMobileNet v2 0.35 tfs\nInt8\n128x128x3\n87.47 %\nMobileNet v2 0.35 tl\nFloat\n128x128x3\n88.15 %\nMobileNet v2 0.35 tl\nInt8\n128x128x3\n88.01 %\nMobileNet v2 0.35 fft\nFloat\n128x128x3\n91.83 %\nMobileNet v2 0.35 fft\nInt8\n128x128x3\n91.01 %\nMobileNet v2 0.35 tfs\nFloat\n224x224x3\n88.69 %\nMobileNet v2 0.35 tfs\nInt8\n224x224x3\n88.83 %\nMobileNet v2 0.35 tl\nFloat\n224x224x3\n88.96 %\nMobileNet v2 0.35 tl\nInt8\n224x224x3\n88.01 %\nMobileNet v2 0.35 fft\nFloat\n224x224x3\n93.6 %\nMobileNet v2 0.35 fft\nInt8\n224x224x3\n92.78 %\nAccuracy with Plant-village dataset\nDataset details: link , License CC0 1.0, Quotation[2]  , Number of classes: 39, Number of images:  61 486\nModel\nFormat\nResolution\nTop 1 Accuracy\nMobileNet v2 0.35 tfs\nFloat\n128x128x3\n99.86 %\nMobileNet v2 0.35 tfs\nInt8\n128x128x3\n99.83 %\nMobileNet v2 0.35 tl\nFloat\n128x128x3\n93.51 %\nMobileNet v2 0.35 tl\nInt8\n128x128x3\n92.33 %\nMobileNet v2 0.35 fft\nFloat\n128x128x3\n99.77 %\nMobileNet v2 0.35 fft\nInt8\n128x128x3\n99.48 %\nMobileNet v2 0.35 tfs\nFloat\n224x224x3\n99.86 %\nMobileNet v2 0.35 tfs\nInt8\n224x224x3\n99.81 %\nMobileNet v2 0.35 tl\nFloat\n224x224x3\n93.62 %\nMobileNet v2 0.35 tl\nInt8\n224x224x3\n92.8 %\nMobileNet v2 0.35 fft\nFloat\n224x224x3\n99.95 %\nMobileNet v2 0.35 fft\nInt8\n224x224x3\n99.68 %\nAccuracy with Food-101 dataset\nDataset details: link, Quotation[3]  , Number of classes: 101 , Number of images:  101 000\nModel\nFormat\nResolution\nTop 1 Accuracy\nMobileNet v2 0.35 tfs\nFloat\n128x128x3\n64.22 %\nMobileNet v2 0.35 tfs\nInt8\n128x128x3\n63.41 %\nMobileNet v2 0.35 tl\nFloat\n128x128x3\n44.74 %\nMobileNet v2 0.35 tl\nInt8\n128x128x3\n42.01 %\nMobileNet v2 0.35 fft\nFloat\n128x128x3\n64.21 %\nMobileNet v2 0.35 fft\nInt8\n128x128x3\n63.41 %\nMobileNet v2 0.35 tfs\nFloat\n224x224x3\n72.31 %\nMobileNet v2 0.35 tfs\nInt8\n224x224x3\n72.05 %\nMobileNet v2 0.35 tl\nFloat\n224x224x3\n49.01 %\nMobileNet v2 0.35 tl\nInt8\n224x224x3\n47.26 %\nMobileNet v2 0.35 fft\nFloat\n224x224x3\n73.74 %\nMobileNet v2 0.35 fft\nInt8\n224x224x3\n73.16 %\nMobileNet v2 1.0 fft\nFloat\n224x224x3\n77.77 %\nMobileNet v2 1.0 fft\nInt8\n224x224x3\n77.09 %\nAccuracy with person dataset\nThe person dataset is derived from COCO-2014 and created using the script here (link). The dataset folder has 2 sub-folders ‚Äî person and notperson containing images of the respective types\nDataset details: link , License Creative Commons Attribution 4.0, Quotation[3]  , Number of classes: 2 , Number of images: 84810\nModel\nFormat\nResolution\nTop 1 Accuracy\nMobileNet v2 0.35 tfs\nFloat\n128x128x3\n92.56 %\nMobileNet v2 0.35 tfs\nInt8\n128x128x3\n92.44 %\nMobileNet v2 0.35 tl\nFloat\n128x128x3\n92.28 %\nMobileNet v2 0.35 tl\nInt8\n128x128x3\n91.63 %\nMobileNet v2 0.35 fft\nFloat\n128x128x3\n95.37 %\nMobileNet v2 0.35 fft\nInt8\n128x128x3\n94.95 %\nAccuracy with ImageNet\nDataset details: link, Quotation[4].\nNumber of classes: 1000.\nTo perform the quantization, we calibrated the activations with a random subset of the training set.\nFor the sake of simplicity, the accuracy reported here was estimated on the 50000 labelled images of the validation set.\nModel\nFormat\nResolution\nTop 1 Accuracy\nMobileNet v2 0.35\nFloat\n128x128x3\n46.96 %\nMobileNet v2 0.35\nInt8\n128x128x3\n43.94 %\nMobileNet v2 0.35\nFloat\n224x224x3\n56.44 %\nMobileNet v2 0.35\nInt8\n224x224x3\n54.7 %\nMobileNet v2 1.0\nFloat\n224x224x3\n68.87 %\nMobileNet v2 1.0\nInt8\n224x224x3\n67.97 %\nMobileNet v2 1.0_per_tensor\nInt8\n224x224x3\n64.53 %\nMobileNet v2 1.4\nFloat\n224x224x3\n71.97 %\nMobileNet v2 1.4\nInt8\n224x224x3\n71.46 %\nRetraining and Integration in a simple example:\nPlease refer to the stm32ai-modelzoo-services GitHub here\nReferences\n[1]\n\"Tf_flowers : tensorflow datasets,\" TensorFlow. [Online]. Available: https://www.tensorflow.org/datasets/catalog/tf_flowers.\n[2]\nJ, ARUN PANDIAN; GOPAL, GEETHARAMANI (2019), \"Data for: Identification of Plant Leaf Diseases Using a 9-layer Deep Convolutional Neural Network\", Mendeley Data, V1, doi: 10.17632/tywbtsjrjv.1\n[3]\nL. Bossard, M. Guillaumin, and L. Van Gool, \"Food-101 -- Mining Discriminative Components with Random Forests.\" European Conference on Computer Vision, 2014.\n[4]\nOlga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei.\n(* = equal contribution) ImageNet Large Scale Visual Recognition Challenge.",
    "Efficient-Large-Model/NVILA-15B": "VILA Model Card\nModel details\nLicense\nIntended use\nInput:\nOutput:\nTraining dataset\nInference:\nEthical Considerations\nVILA Model Card\nModel details\nModel type:\nNVILA is a visual language model (VLM) pretrained with interleaved image-text data at scale, enabling multi-image VLM. Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This \"scale-then-compress\" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.\nModel date:\nNVILA was trained in Nov 2024.\nPaper or resources for more information:\nhttps://github.com/NVLabs/VILA\n@misc{liu2024nvila,\ntitle={NVILA: Efficient Frontier Visual Language Models},\nauthor={Zhijian Liu and Ligeng Zhu and Baifeng Shi and Zhuoyang Zhang and Yuming Lou and Shang Yang and Haocheng Xi and Shiyi Cao and Yuxian Gu and Dacheng Li and Xiuyu Li and Yunhao Fang and Yukang Chen and Cheng-Yu Hsieh and De-An Huang and An-Chieh Cheng and Vishwesh Nath and Jinyi Hu and Sifei Liu and Ranjay Krishna and Daguang Xu and Xiaolong Wang and Pavlo Molchanov and Jan Kautz and Hongxu Yin and Song Han and Yao Lu},\nyear={2024},\neprint={2412.04468},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2412.04468},\n}\nLicense\nThe code is released under the Apache 2.0 license as found in the LICENSE file.\nThe pretrained weights are released under the CC-BY-NC-SA-4.0 license.\nThe service is a research preview intended for non-commercial use only, and is subject to the following licenses and terms:\nTerms of Use of the data generated by OpenAI\nDataset Licenses for each one used during training.\nWhere to send questions or comments about the model:\nhttps://github.com/NVLabs/VILA/issues\nIntended use\nPrimary intended uses:\nThe primary use of VILA is research on large multimodal models and chatbots.\nPrimary intended users:\nThe primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.\nInput:\nInput Type: Image, Video, Text\nInput Format: Red, Green, Blue; MP4 ;String\nInput Parameters: 2D, 3D\nOutput:\nOutput Type: Text\nOutput Format: String\nSupported Hardware Microarchitecture Compatibility:\nAmpere\nJetson\nHopper\nLovelace\n[Preferred/Supported] Operating System(s):\nLinux\nTraining dataset\nSee Dataset Preparation for more details.\n** Data Collection Method by dataset\n[Hybrid: Automated, Human]\n** Labeling Method by dataset\n[Hybrid: Automated, Human]\nInference:\nEngine: [Tensor(RT), Triton, Or List Other Here]\nPyTorch\nTensorRT-LLM\nTinyChat\nTest Hardware:\nA100\nJetson Orin\nRTX 4090\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.",
    "Fusho/Style_LoRA_XL": "No model card",
    "SicariusSicariiStuff/Impish_Mind_8B": "",
    "jasongzy/Make-It-Animatable": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters\nMake-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters\nPaper\nProject Page\nCode",
    "davidschulte/ESM_SIA86__TechnicalSupportCalls_default": "ESM SIA86/TechnicalSupportCalls\nModel Details\nModel Description\nTraining Details\nIntermediate Task\nTraining Procedure [optional]\nESM Training Hyperparameters [optional]\nAdditional trainiung details [optional]\nModel evaluation\nEvaluation of fine-tuned language model [optional]\nEvaluation of ESM [optional]\nAdditional evaluation details [optional]\nWhat are Embedding Space Maps used for?\nYou don't have enough training data for your problem\nYou want to find similar datasets to your target dataset\nHow can I use ESM-LogME / ESMs?\nHow do Embedding Space Maps work?\nHow can I use Embedding Space Maps for Intermediate Task Selection?\nCitation\nAdditional Information\nESM SIA86/TechnicalSupportCalls\nModel Details\nModel Description\nESM\nDeveloped by: David Schulte\nModel type: ESM\nBase Model: bert-base-multilingual-uncased\nIntermediate Task: SIA86/TechnicalSupportCalls\nESM architecture: linear\nESM embedding dimension: 768\nLanguage(s) (NLP): [More Information Needed]\nLicense: Apache-2.0 license\nESM version: 0.1.0\nTraining Details\nIntermediate Task\nTask ID: SIA86/TechnicalSupportCalls\nSubset [optional]: default\nText Column: text\nLabel Column: label\nDataset Split:  train\nSample size [optional]: 921\nSample seed [optional]:\nTraining Procedure [optional]\nLanguage Model Training Hyperparameters [optional]\nEpochs: 3\nBatch size: 32\nLearning rate: 2e-05\nWeight Decay: 0.01\nOptimizer: AdamW\nESM Training Hyperparameters [optional]\nEpochs: 10\nBatch size: 32\nLearning rate: 0.001\nWeight Decay: 0.01\nOptimizer: AdamW\nAdditional trainiung details [optional]\nModel evaluation\nEvaluation of fine-tuned language model [optional]\nEvaluation of ESM [optional]\nMSE:\nAdditional evaluation details [optional]\nWhat are Embedding Space Maps used for?\nEmbedding Space Maps are a part of ESM-LogME, a efficient method for finding intermediate datasets for transfer learning. There are two reasons to use ESM-LogME:\nYou don't have enough training data for your problem\nIf you don't have a enough training data for your problem, just use ESM-LogME to find more.\nYou can supplement model training by including publicly available datasets in the training process.\nFine-tune a language model on suitable intermediate dataset.\nFine-tune the resulting model on your target dataset.\nThis workflow is called intermediate task transfer learning and it can significantly improve the target performance.\nBut what is a suitable dataset for your problem? ESM-LogME enable you to quickly rank thousands of datasets on the Hugging Face Hub by how well they are exptected to transfer to your target task.\nYou want to find similar datasets to your target dataset\nUsing ESM-LogME can be used like search engine on the Hugging Face Hub. You can find similar tasks to your target task without having to rely on heuristics. ESM-LogME estimates how language models fine-tuned on each intermediate task would benefinit your target task. This quantitative approach combines the effects of domain similarity and task similarity.\nHow can I use ESM-LogME / ESMs?\nWe release hf-dataset-selector, a Python package for intermediate task selection using Embedding Space Maps.\nhf-dataset-selector fetches ESMs for a given language model and uses it to find the best dataset for applying intermediate training to the target task. ESMs are found by their tags on the Huggingface Hub.\nfrom hfselect import Dataset, compute_task_ranking\n# Load target dataset from the Hugging Face Hub\ndataset = Dataset.from_hugging_face(\nname=\"stanfordnlp/imdb\",\nsplit=\"train\",\ntext_col=\"text\",\nlabel_col=\"label\",\nis_regression=False,\nnum_examples=1000,\nseed=42\n)\n# Fetch ESMs and rank tasks\ntask_ranking = compute_task_ranking(\ndataset=dataset,\nmodel_name=\"bert-base-multilingual-uncased\"\n)\n# Display top 5 recommendations\nprint(task_ranking[:5])\n1.   davanstrien/test_imdb_embedd2                     Score: -0.618529\n2.   davanstrien/test_imdb_embedd                      Score: -0.618644\n3.   davanstrien/test1                                 Score: -0.619334\n4.   stanfordnlp/imdb                                  Score: -0.619454\n5.   stanfordnlp/sst                                   Score: -0.62995\nRank\nTask ID\nTask Subset\nText Column\nLabel Column\nTask Split\nNum Examples\nESM Architecture\nScore\n1\ndavanstrien/test_imdb_embedd2\ndefault\ntext\nlabel\ntrain\n10000\nlinear\n-0.618529\n2\ndavanstrien/test_imdb_embedd\ndefault\ntext\nlabel\ntrain\n10000\nlinear\n-0.618644\n3\ndavanstrien/test1\ndefault\ntext\nlabel\ntrain\n10000\nlinear\n-0.619334\n4\nstanfordnlp/imdb\nplain_text\ntext\nlabel\ntrain\n10000\nlinear\n-0.619454\n5\nstanfordnlp/sst\ndictionary\nphrase\nlabel\ndictionary\n10000\nlinear\n-0.62995\n6\nstanfordnlp/sst\ndefault\nsentence\nlabel\ntrain\n8544\nlinear\n-0.63312\n7\nkuroneko5943/snap21\nCDs_and_Vinyl_5\nsentence\nlabel\ntrain\n6974\nlinear\n-0.634365\n8\nkuroneko5943/snap21\nVideo_Games_5\nsentence\nlabel\ntrain\n6997\nlinear\n-0.638787\n9\nkuroneko5943/snap21\nMovies_and_TV_5\nsentence\nlabel\ntrain\n6989\nlinear\n-0.639068\n10\nfancyzhx/amazon_polarity\namazon_polarity\ncontent\nlabel\ntrain\n10000\nlinear\n-0.639718\nFor more information on how to use ESMs please have a look at the official Github repository. We provide documentation further documentation and tutorials for finding intermediate datasets and training your own ESMs.\nHow do Embedding Space Maps work?\nEmbedding Space Maps (ESMs) are neural networks that approximate the effect of fine-tuning a language model on a task. They can be used to quickly transform embeddings from a base model to approximate how a fine-tuned model would embed the the input text.\nESMs can be used for intermediate task selection with the ESM-LogME workflow.\nHow can I use Embedding Space Maps for Intermediate Task Selection?\nCitation\nIf you are using this Embedding Space Maps, please cite our paper.\nBibTeX:\n@inproceedings{schulte-etal-2024-less,\ntitle = \"Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning\",\nauthor = \"Schulte, David  and\nHamborg, Felix  and\nAkbik, Alan\",\neditor = \"Al-Onaizan, Yaser  and\nBansal, Mohit  and\nChen, Yun-Nung\",\nbooktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\nmonth = nov,\nyear = \"2024\",\naddress = \"Miami, Florida, USA\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2024.emnlp-main.529/\",\ndoi = \"10.18653/v1/2024.emnlp-main.529\",\npages = \"9431--9442\",\nabstract = \"Intermediate task transfer learning can greatly improve model performance. If, for example, one has little training data for emotion detection, first fine-tuning a language model on a sentiment classification dataset may improve performance strongly. But which task to choose for transfer learning? Prior methods producing useful task rankings are infeasible for large source pools, as they require forward passes through all source language models. We overcome this by introducing Embedding Space Maps (ESMs), light-weight neural networks that approximate the effect of fine-tuning a language model. We conduct the largest study on NLP task transferability and task selection with 12k source-target pairs. We find that applying ESMs on a prior method reduces execution time and disk space usage by factors of 10 and 278, respectively, while retaining high selection performance (avg. regret@5 score of 2.95).\"\n}\nAPA:\nSchulte, D., Hamborg, F., & Akbik, A. (2024, November). Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 9431-9442).\nAdditional Information",
    "khaledsoudy/arabic-sentiment-bert-model": "Fine-Tuned Arabic Sentiment Analysis with BERT üöÄ\nAuthor üßë‚Äçüíª\nSource Code üíª\nHow to Use the Model\n1. Install Required Libraries\n2. Load the Fine-Tuned Model\n3. Use the Model for Prediction\n4. Input Format\n5. Sentiment Labels\nModel Details\nHow to Fine-Tune This Model\nLicense üìú\nAcknowledgments üôè\nFine-Tuned Arabic Sentiment Analysis with BERT üöÄ\nThis repository contains a fine-tuned BERT model for sentiment analysis of Arabic reviews. The model is trained on the Arabic 100k Reviews dataset and can classify reviews into three sentiment categories: Positive, Negative, and Mixed.\nAuthor üßë‚Äçüíª\nKhaled SoudyGitHub: khaledsoudy-1\nSource Code üíª\nYou can find the source code and full implementation of this project on my GitHub repository.\nThe repository contains the Google Colab notebook, dataset, and scripts used to fine-tune the model for Arabic sentiment analysis.\nHow to Use the Model\n1. Install Required Libraries\nMake sure you have the transformers and tensorflow libraries installed:\n!pip install transformers\n!pip install tensorflow\n2. Load the Fine-Tuned Model\nYou can load the fine-tuned model and tokenizer directly from Hugging Face using the following code:\nfrom transformers import TFBertForSequenceClassification, BertTokenizer\n# Load model and tokenizer from Hugging Face\nmodel_name = \"khaledsoudy/arabic-sentiment-bert-model\"\n# Load model\nmodel = TFBertForSequenceClassification.from_pretrained(model_name)\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained(model_name)\n3. Use the Model for Prediction\nTo use the model for sentiment analysis on an Arabic text, follow these steps:\nimport tensorflow as tf\n# Sample Arabic text for sentiment prediction\ntext = \"ÿßŸÑŸÅŸÜÿØŸÇ ÿ±ÿßÿ¶ÿπ Ÿà ÿßŸÑÿÆÿØŸÖÿ© ŸÖŸÖÿ™ÿßÿ≤ÿ©\"\n# Tokenize the input text\ninputs = tokenizer(text, return_tensors=\"tf\")\n# Get the model's prediction\noutputs = model(**inputs)\n# Get the predicted sentiment (assuming 3 classes: Positive, Negative, Mixed)\npredicted_class = tf.argmax(outputs.logits, axis=-1).numpy()\n# Map the predicted class index to sentiment labels\nsentiment_labels = ['Mixed', 'Negative', 'Positive']\nprint(f\"Predicted sentiment: {sentiment_labels[predicted_class[0]]}\")\n4. Input Format\nThe model expects Arabic text input. The text should be preprocessed to remove unnecessary characters or diacritics for better results.\n5. Sentiment Labels\nThe model classifies the sentiment into three categories:\nPositive üåü\nNegative üò†\nMixed ü§î\nModel Details\nModel Name: khaledsoudy/arabic-sentiment-bert-model\nModel Type: TFBertForSequenceClassification\nLanguage: Arabic\nSentiment Classes: Positive, Negative, Mixed\nHow to Fine-Tune This Model\nYou can fine-tune this model further using your own dataset. Check out the source code and related notebooks on my GitHub for detailed steps and guidance.\nLicense üìú\nThis model is licensed under the MIT License.\nAcknowledgments üôè\nHugging Face for providing the platform to host models.\nGoogle BERT for the pre-trained model.\nKaggle for the Arabic 100k Reviews dataset.\nThis README is ready for use on your Hugging Face model page! It includes detailed usage instructions, links to your GitHub, and other relevant information.",
    "mlx-community/Llama-3.3-70B-Instruct-4bit": "mlx-community/Llama-3.3-70B-Instruct-4bit\nUse with mlx\nmlx-community/Llama-3.3-70B-Instruct-4bit\nThe Model mlx-community/Llama-3.3-70B-Instruct-4bit was\nconverted to MLX format from meta-llama/Llama-3.3-70B-Instruct\nusing mlx-lm version 0.20.1.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/Llama-3.3-70B-Instruct-4bit\")\nprompt=\"hello\"\nif hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "livekit/turn-detector": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "EVA-UNIT-01/EVA-LLaMA-3.33-70B-v0.0": "EVA LLaMA 3.33 70B v0.0\nA RP/storywriting specialist model, full-parameter finetune of Llama-3.3-70B-Instruct on mixture of synthetic and natural data.\nIt uses Celeste 70B 0.1 data mixture, greatly expanding it to improve versatility, creativity and \"flavor\" of the resulting model.\nThis model was built with Llama by Meta.\nPrompt format is Llama3.\nRecommended sampler values:\nTemperature: 1\nMin-P: 0.05\nRepetition Penalty: 1.03\nRecommended SillyTavern preset (via Virt-io):\nMaster import\nTraining data:\nCeleste 70B 0.1 data mixture minus Opus Instruct subset. See that model's card for details.\nKalomaze's Opus_Instruct_25k dataset, filtered for refusals.\nA subset (1k rows) of ChatGPT-4o-WritingPrompts by Gryphe\nA subset (2k rows) of Sonnet3.5-Charcards-Roleplay by Gryphe\nSynthstruct and SynthRP datasets by Epiculous\nA subset from Dolphin-2.9.3, including filtered version of not_samantha and a small subset of systemchat.\nTraining time and hardware:\n10 hours on 8xH100 SXM\nModel was created by Kearm, Auri and Cahvay.\nSpecial thanks:\nto Cahvay for his work on dataset filtering.\nto Gryphe, Lemmy, Kalomaze, Nopm, Epiculous and CognitiveComputations for the data\nand to Allura-org for support, feedback, beta-testing and doing quality control of EVA models.\nLicensing\nLlama-3.3-70B-Instruct by Meta is licensed under Llama 3.3 Community License Agreement (further referred as L3.3 license) and is a subject to Acceptable Use Policy for Llama Materials.\nThis derivative is free for personal, research and commercial use on terms of L3.3 license with one extra clause:\n- Infermatic Inc and any of its employees or paid associates cannot utilize, distribute, download, or otherwise make use of EVA models for any purpose.\nSee axolotl config\naxolotl version: 0.4.1\nbase_model: meta-llama/Llama-3.3-70B-Instruct\nplugins:\n- axolotl.integrations.liger.LigerPlugin\nliger_rope: true\nliger_rms_norm: true\nliger_swiglu: true\nliger_fused_linear_cross_entropy: true\nstrict: false\nchat_template: llama3\ndatasets:\n- path: datasets/Celeste_Filtered_utf8fix.jsonl\ntype: sharegpt\n- path: datasets/deduped_not_samantha_norefusals.jsonl\ntype: sharegpt\n- path: datasets/deduped_SynthRP-Gens_processed_ShareGPT_converted_cleaned.jsonl\ntype: sharegpt\n- path: datasets/deduped_Synthstruct-Gens_processed_sharegpt_converted_cleaned.jsonl\ntype: sharegpt\n- path: datasets/Gryphe-4o-WP-filtered-sharegpt_utf8fix.jsonl\ntype: sharegpt\n- path: datasets/opus-instruct-22k-no_refusals-filtered_utf8fix.jsonl\ntype: sharegpt\n- path: datasets/Sonnet3-5-charcard-names-filtered-sharegpt_utf8fix.jsonl\ntype: sharegpt\n- path: datasets/SystemChat_subset_filtered_sharegpt_utf8fix.jsonl\ntype: sharegpt\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.001\noutput_dir: /dev/shm/EVA-LLaMA-3.33-70B-v0.1\nsequence_len: 8192\nsample_packing: true\neval_sample_packing: false\npad_to_sequence_len: true\nwandb_project: EVA-LLaMA-3.33-70B\nwandb_entity:\nwandb_watch:\nwandb_name: Unit-v0.1\nwandb_log_model:\nunfrozen_parameters:\n- ^lm_head.weight$\n- ^model.embed_tokens.weight$\n# mlp.down_proj layers\n- model.layers.40.mlp.down_proj\n- model.layers.44.mlp.down_proj\n- model.layers.45.mlp.down_proj\n- model.layers.46.mlp.down_proj\n- model.layers.43.mlp.down_proj\n- model.layers.52.mlp.down_proj\n- model.layers.47.mlp.down_proj\n- model.layers.39.mlp.down_proj\n- model.layers.48.mlp.down_proj\n- model.layers.49.mlp.down_proj\n- model.layers.38.mlp.down_proj\n- model.layers.53.mlp.down_proj\n- model.layers.35.mlp.down_proj\n- model.layers.41.mlp.down_proj\n- model.layers.51.mlp.down_proj\n- model.layers.42.mlp.down_proj\n- model.layers.37.mlp.down_proj\n- model.layers.50.mlp.down_proj\n- model.layers.76.mlp.down_proj\n- model.layers.60.mlp.down_proj\n- model.layers.36.mlp.down_proj\n- model.layers.54.mlp.down_proj\n- model.layers.57.mlp.down_proj\n- model.layers.56.mlp.down_proj\n- model.layers.59.mlp.down_proj\n- model.layers.55.mlp.down_proj\n- model.layers.77.mlp.down_proj\n- model.layers.61.mlp.down_proj\n- model.layers.58.mlp.down_proj\n- model.layers.65.mlp.down_proj\n- model.layers.75.mlp.down_proj\n- model.layers.64.mlp.down_proj\n- model.layers.62.mlp.down_proj\n- model.layers.68.mlp.down_proj\n- model.layers.19.mlp.down_proj\n- model.layers.73.mlp.down_proj\n- model.layers.66.mlp.down_proj\n- model.layers.67.mlp.down_proj\n- model.layers.63.mlp.down_proj\n- model.layers.74.mlp.down_proj\n# mlp.gate_proj layers\n- model.layers.70.mlp.gate_proj\n- model.layers.71.mlp.gate_proj\n- model.layers.67.mlp.gate_proj\n- model.layers.58.mlp.gate_proj\n- model.layers.55.mlp.gate_proj\n- model.layers.57.mlp.gate_proj\n- model.layers.56.mlp.gate_proj\n- model.layers.66.mlp.gate_proj\n- model.layers.72.mlp.gate_proj\n- model.layers.52.mlp.gate_proj\n- model.layers.69.mlp.gate_proj\n- model.layers.54.mlp.gate_proj\n- model.layers.62.mlp.gate_proj\n- model.layers.60.mlp.gate_proj\n- model.layers.59.mlp.gate_proj\n- model.layers.74.mlp.gate_proj\n- model.layers.51.mlp.gate_proj\n- model.layers.68.mlp.gate_proj\n- model.layers.61.mlp.gate_proj\n- model.layers.53.mlp.gate_proj\n- model.layers.73.mlp.gate_proj\n- model.layers.63.mlp.gate_proj\n- model.layers.48.mlp.gate_proj\n- model.layers.49.mlp.gate_proj\n- model.layers.64.mlp.gate_proj\n- model.layers.50.mlp.gate_proj\n- model.layers.65.mlp.gate_proj\n- model.layers.47.mlp.gate_proj\n- model.layers.44.mlp.gate_proj\n- model.layers.45.mlp.gate_proj\n- model.layers.75.mlp.gate_proj\n- model.layers.46.mlp.gate_proj\n- model.layers.43.mlp.gate_proj\n- model.layers.77.mlp.gate_proj\n- model.layers.41.mlp.gate_proj\n- model.layers.40.mlp.gate_proj\n- model.layers.42.mlp.gate_proj\n- model.layers.32.mlp.gate_proj\n- model.layers.30.mlp.gate_proj\n- model.layers.39.mlp.gate_proj\n# mlp.up_proj layers\n- model.layers.70.mlp.up_proj\n- model.layers.67.mlp.up_proj\n- model.layers.66.mlp.up_proj\n- model.layers.69.mlp.up_proj\n- model.layers.62.mlp.up_proj\n- model.layers.63.mlp.up_proj\n- model.layers.65.mlp.up_proj\n- model.layers.68.mlp.up_proj\n- model.layers.71.mlp.up_proj\n- model.layers.64.mlp.up_proj\n- model.layers.61.mlp.up_proj\n- model.layers.58.mlp.up_proj\n- model.layers.59.mlp.up_proj\n- model.layers.57.mlp.up_proj\n- model.layers.55.mlp.up_proj\n- model.layers.72.mlp.up_proj\n- model.layers.54.mlp.up_proj\n- model.layers.56.mlp.up_proj\n- model.layers.60.mlp.up_proj\n- model.layers.73.mlp.up_proj\n- model.layers.50.mlp.up_proj\n- model.layers.51.mlp.up_proj\n- model.layers.53.mlp.up_proj\n- model.layers.52.mlp.up_proj\n- model.layers.74.mlp.up_proj\n- model.layers.49.mlp.up_proj\n- model.layers.30.mlp.up_proj\n- model.layers.47.mlp.up_proj\n- model.layers.46.mlp.up_proj\n- model.layers.34.mlp.up_proj\n- model.layers.48.mlp.up_proj\n- model.layers.38.mlp.up_proj\n- model.layers.45.mlp.up_proj\n- model.layers.43.mlp.up_proj\n- model.layers.29.mlp.up_proj\n- model.layers.42.mlp.up_proj\n- model.layers.75.mlp.up_proj\n- model.layers.35.mlp.up_proj\n- model.layers.44.mlp.up_proj\n- model.layers.31.mlp.up_proj\n# self_attn.k_proj layers\n- model.layers.72.self_attn.k_proj\n- model.layers.75.self_attn.k_proj\n- model.layers.71.self_attn.k_proj\n- model.layers.74.self_attn.k_proj\n- model.layers.44.self_attn.k_proj\n- model.layers.31.self_attn.k_proj\n- model.layers.33.self_attn.k_proj\n- model.layers.34.self_attn.k_proj\n- model.layers.76.self_attn.k_proj\n- model.layers.78.self_attn.k_proj\n- model.layers.77.self_attn.k_proj\n- model.layers.60.self_attn.k_proj\n- model.layers.56.self_attn.k_proj\n- model.layers.22.self_attn.k_proj\n- model.layers.2.self_attn.k_proj\n- model.layers.18.self_attn.k_proj\n- model.layers.17.self_attn.k_proj\n- model.layers.21.self_attn.k_proj\n- model.layers.19.self_attn.k_proj\n- model.layers.23.self_attn.k_proj\n- model.layers.52.self_attn.k_proj\n- model.layers.73.self_attn.k_proj\n- model.layers.35.self_attn.k_proj\n- model.layers.15.self_attn.k_proj\n- model.layers.27.self_attn.k_proj\n- model.layers.29.self_attn.k_proj\n- model.layers.36.self_attn.k_proj\n- model.layers.28.self_attn.k_proj\n- model.layers.20.self_attn.k_proj\n- model.layers.25.self_attn.k_proj\n- model.layers.37.self_attn.k_proj\n- model.layers.30.self_attn.k_proj\n- model.layers.41.self_attn.k_proj\n- model.layers.16.self_attn.k_proj\n- model.layers.32.self_attn.k_proj\n- model.layers.68.self_attn.k_proj\n- model.layers.26.self_attn.k_proj\n- model.layers.38.self_attn.k_proj\n- model.layers.39.self_attn.k_proj\n- model.layers.70.self_attn.k_proj\n# self_attn.o_proj layers\n- model.layers.50.self_attn.o_proj\n- model.layers.61.self_attn.o_proj\n- model.layers.46.self_attn.o_proj\n- model.layers.53.self_attn.o_proj\n- model.layers.54.self_attn.o_proj\n- model.layers.19.self_attn.o_proj\n- model.layers.42.self_attn.o_proj\n- model.layers.41.self_attn.o_proj\n- model.layers.49.self_attn.o_proj\n- model.layers.68.self_attn.o_proj\n- model.layers.18.self_attn.o_proj\n- model.layers.45.self_attn.o_proj\n- model.layers.11.self_attn.o_proj\n- model.layers.48.self_attn.o_proj\n- model.layers.51.self_attn.o_proj\n- model.layers.67.self_attn.o_proj\n- model.layers.64.self_attn.o_proj\n- model.layers.13.self_attn.o_proj\n- model.layers.14.self_attn.o_proj\n- model.layers.16.self_attn.o_proj\n- model.layers.17.self_attn.o_proj\n- model.layers.47.self_attn.o_proj\n- model.layers.0.self_attn.o_proj\n- model.layers.20.self_attn.o_proj\n- model.layers.63.self_attn.o_proj\n- model.layers.5.self_attn.o_proj\n- model.layers.15.self_attn.o_proj\n- model.layers.21.self_attn.o_proj\n- model.layers.52.self_attn.o_proj\n- model.layers.12.self_attn.o_proj\n- model.layers.10.self_attn.o_proj\n- model.layers.56.self_attn.o_proj\n- model.layers.62.self_attn.o_proj\n- model.layers.22.self_attn.o_proj\n- model.layers.6.self_attn.o_proj\n- model.layers.7.self_attn.o_proj\n- model.layers.43.self_attn.o_proj\n- model.layers.38.self_attn.o_proj\n- model.layers.9.self_attn.o_proj\n- model.layers.44.self_attn.o_proj\n# self_attn.q_proj layers\n- model.layers.2.self_attn.q_proj\n- model.layers.4.self_attn.q_proj\n- model.layers.46.self_attn.q_proj\n- model.layers.5.self_attn.q_proj\n- model.layers.7.self_attn.q_proj\n- model.layers.6.self_attn.q_proj\n- model.layers.9.self_attn.q_proj\n- model.layers.10.self_attn.q_proj\n- model.layers.1.self_attn.q_proj\n- model.layers.18.self_attn.q_proj\n- model.layers.62.self_attn.q_proj\n- model.layers.8.self_attn.q_proj\n- model.layers.15.self_attn.q_proj\n- model.layers.14.self_attn.q_proj\n- model.layers.31.self_attn.q_proj\n- model.layers.17.self_attn.q_proj\n- model.layers.16.self_attn.q_proj\n- model.layers.19.self_attn.q_proj\n- model.layers.12.self_attn.q_proj\n- model.layers.33.self_attn.q_proj\n- model.layers.35.self_attn.q_proj\n- model.layers.21.self_attn.q_proj\n- model.layers.13.self_attn.q_proj\n- model.layers.27.self_attn.q_proj\n- model.layers.56.self_attn.q_proj\n- model.layers.34.self_attn.q_proj\n- model.layers.11.self_attn.q_proj\n- model.layers.52.self_attn.q_proj\n- model.layers.28.self_attn.q_proj\n- model.layers.54.self_attn.q_proj\n- model.layers.30.self_attn.q_proj\n- model.layers.29.self_attn.q_proj\n- model.layers.20.self_attn.q_proj\n- model.layers.75.self_attn.q_proj\n- model.layers.37.self_attn.q_proj\n- model.layers.44.self_attn.q_proj\n- model.layers.23.self_attn.q_proj\n- model.layers.64.self_attn.q_proj\n- model.layers.60.self_attn.q_proj\n- model.layers.36.self_attn.q_proj\n# self_attn.v_proj layers\n- model.layers.11.self_attn.v_proj\n- model.layers.17.self_attn.v_proj\n- model.layers.37.self_attn.v_proj\n- model.layers.40.self_attn.v_proj\n- model.layers.41.self_attn.v_proj\n- model.layers.42.self_attn.v_proj\n- model.layers.43.self_attn.v_proj\n- model.layers.44.self_attn.v_proj\n- model.layers.45.self_attn.v_proj\n- model.layers.46.self_attn.v_proj\n- model.layers.48.self_attn.v_proj\n- model.layers.49.self_attn.v_proj\n- model.layers.50.self_attn.v_proj\n- model.layers.51.self_attn.v_proj\n- model.layers.53.self_attn.v_proj\n- model.layers.54.self_attn.v_proj\n- model.layers.55.self_attn.v_proj\n- model.layers.57.self_attn.v_proj\n- model.layers.58.self_attn.v_proj\n- model.layers.59.self_attn.v_proj\n- model.layers.60.self_attn.v_proj\n- model.layers.61.self_attn.v_proj\n- model.layers.62.self_attn.v_proj\n- model.layers.63.self_attn.v_proj\n- model.layers.64.self_attn.v_proj\n- model.layers.65.self_attn.v_proj\n- model.layers.66.self_attn.v_proj\n- model.layers.67.self_attn.v_proj\n- model.layers.69.self_attn.v_proj\n- model.layers.75.self_attn.v_proj\n- model.layers.18.self_attn.v_proj\n- model.layers.78.self_attn.v_proj\n- model.layers.68.self_attn.v_proj\n- model.layers.47.self_attn.v_proj\n- model.layers.38.self_attn.v_proj\n- model.layers.39.self_attn.v_proj\n- model.layers.71.self_attn.v_proj\n- model.layers.19.self_attn.v_proj\n- model.layers.36.self_attn.v_proj\n- model.layers.20.self_attn.v_proj\ngradient_accumulation_steps: 8\nmicro_batch_size: 1\nnum_epochs: 3\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 0.00003\nmax_grad_norm: 2\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\ngradient_checkpointing: \"unsloth\"\ngradient_checkpointing_kwargs:\nuse_reentrant: false\nearly_stopping_patience:\nresume_from_checkpoint:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\nwarmup_steps: 20\nevals_per_epoch: 4\neval_table_size:\nsaves_per_epoch: 1\ndebug:\ndeepspeed: deepspeed_configs/zero3_bf16.json\nweight_decay: 0.2",
    "sandbox-ai/Llama-3.1-Tango-70b-bnb_4b": "Model Overview\nDescription:\nTerms of use\nEvaluation Metrics\nUsage:\nReferences(s):\nModel Architecture:\nInput:\nOutput:\nTraining & Evaluation:\nDataset:\nCitation\nModel Overview\nDescription:\nTango-70B-Instruct is a large language model trained by sandbox-ai on a modified variation of of spanish/-ir/messirve to improve the regional Spanish speech performance.\nSee details on the github repo\nTerms of use\nBy accessing this model, you are agreeing to the LLama 3.1 terms and conditions of the license, acceptable use policy and Meta‚Äôs privacy policy\nEvaluation Metrics\nTask\nName\nDescription\nLanguage\nMetric\nTask type\nAQuAS\nAQuAS\nAbstractive Question-Answering in Spanish\nES\nsas_encoder\nAbstractive QA\nARC_ca\nARC_ca\nGrade-school level science questions in Catalan\nCA\nacc\nMulti choice QA\nBEC2016eu\nBEC2016eu\nBasque Election Campaign 2016 Opinion Dataset\nEU\nf1\nSentiment Analysis\nBelebele Glg\nBelebele Glg\nReading Comprehension in Galician\nGL\nacc\nReading Comprehension\nBertaQA\nBertaQA\nTrivia dataset with global and local questions about the Basque Country\nEU\nacc\nMulti choice QA\nBHTCv2\nBHTCv2\nTopic Classification of News Headlines in Basque\nEU\nf1\nClassification, Topic Classification\ncaBREU\ncaBREU\nArticle Summarization in Catalan\nCA\nbleu\nSummarization\nCatalanQA\nCatalanQA\nExtractive QA in Catalan\nCA\nf1\nExtractive QA\nCatCoLA\nCatCoLA\nLinguistic Acceptability in Catalan\nCA\nmcc\nLinguistic Acceptability\nClinDiagnosES\nClinDiagnosES\nDiagnosis of clinical cases in Spanish\nES\nsas_encoder\nOpen QA\nClinTreatES\nClinTreatES\nTreatment for clinical cases in Spanish\nES\nsas_encoder\nOpen QA\nCOPA_ca\nCOPA_ca\nChoice Of Plausible Alternatives in Catalan\nCA\nacc\nReasoning\nCoQCat\nCoQCat\nConversational Question Answering in Catalan\nCA\nf1\nExtractive QA\nCrows Pairs Spanish\nCrows Pairs Spanish\nBias evaluation using stereotypes\nES\npct_stereotype\nBias Detection\nEpecKorrefBin\nEpecKorrefBin\nCoreference resolution in Basque\nEU\nacc\nCoreference Resolution, Textual Entailment\nEsCoLA\nEsCoLA\nSpanish Corpus of Linguistic Acceptability\nES\nmcc\nLinguistic Acceptability\nEusExams\nEusExams\nPublic Service examinations questions in Basque\nEU\nacc\nMulti choice QA\nEusProficiency\nEusProficiency\nC1-level proficiency questions in Basque\nEU\nacc\nMulti choice QA\nEusReading\nEusReading\nEGA exams reading comprehension in Basque\nEU\nacc\nMulti choice QA\nEusTrivia\nEusTrivia\nTrivia questions in Basque\nEU\nacc\nMulti choice QA\nFake News ES\nFake News ES\nFake News Detection in Spanish\nES\nacc\nClassification\nGalCoLA\nGalCoLA\nGalician Corpus of Linguistic Acceptability\nGL\nmcc\nLinguistic Acceptability\nHumorQA\nHumorQA\nWhite humour joke classification\nES\nacc\nClassification\nMGSM_ca\nMGSM_ca\nGrade-school math problems in Catalan\nCA\nexact_match\nMath Reasoning\nMGSM_es\nMGSM_es\nGrade-school math problems in Spanish\nES\nexact_match\nMath Reasoning\nMGSM_eu\nMGSM_eu\nGrade-school math problems in Basque\nEU\nexact_match\nMath Reasoning\nMGSM_gl\nMGSM_gl\nGrade-school math problems in Galician\nGL\nexact_match\nMath Reasoning\nNoticIA\nNoticIA\nA Clickbait Article Summarization Dataset in Spanish\nES\nrouge1\nSummarization\nOffendES\nOffendES\nClasificaci√≥n de comentarios ofensivos en espa√±ol\nES\nacc\nClassification\nOpenBookQA_ca\nOpenBookQA_ca\nMulti-step reasoning QA in Catalan\nCA\nacc\nReasoning\nOpenBookQA_gl\nOpenBookQA_gl\nMulti-step reasoning QA in Galician\nGL\nacc\nReasoning\nParafraseja\nParafraseja\nParaphrase identification in Catalan\nCA\nacc\nParaphrasing\nParafrasesGL\nParafrasesGL\nParaphrase identification in Galician\nGL\nacc\nParaphrasing\nPAWS_ca\nPAWS_ca\nParaphrase Adversaries from Word Scrambling in Catalan\nCA\nacc\nParaphrasing\nPAWS-X_es\nPAWS-X_es\nParaphrase Adversaries from Word Scrambling in Spanish\nES\nacc\nParaphrasing\nPAWS_gl\nPAWS_gl\nParaphrase Adversaries from Word Scrambling in Galician\nGL\nacc\nParaphrasing\nPIQA_ca\nPIQA_ca\nPhysical Interaction QA in Catalan\nCA\nacc\nReasoning\nQNLIeu\nQNLIeu\nTextual Entailment in Basque\nEU\nacc\nNLI, Textual Entailment\nRagQuAS\nRagQuAS\nRetrieval-Augmented-Generation and Question-Answering in Spanish\nES\nsas_encoder\nAbstractive QA\nSIQA_ca\nSIQA_ca\nSocial Interaction QA in Catalan\nCA\nacc\nReasoning\nSpaLawEx\nSpaLawEx\nSpanish Law School Access Exams\nES\nacc\nMulti choice QA\nSummarizationGL\nSummarizationGL\nAbstractive Summarization in Galician\nGL\nbleu\nSummarization\nTE-ca\nTE-ca\nTextual Entailment in Catalan\nCA\nacc\nTextual Entailment\nTELEIA\nTELEIA\nTest de Espa√±ol como Lengua Extranjera para Inteligencia Artificial\nES\nacc\nMulti choice QA\nVaxxStance\nVaxxStance\nStance detection on the Antivaxxers movement\nEU\nf1\nSentiment Analysis, Stance Detection\nWiCeu\nWiCeu\nWord sense disambiguation in Basque\nEU\nacc\nTextual Entailment\nWNLI_ca\nWNLI_ca\nWinograd-schema-type dataset in Catalan\nCA\nacc\nNLI, Textual Entailment\nWNLI ES\nWNLI ES\nWinograd-schema-type dataset in Spanish\nES\nacc\nNLI, Textual Entailment\nXCOPA_eu\nXCOPA_eu\nChoice Of Plausible Alternatives in Basque\nEU\nacc\nReasoning\nXNLI_ca\nXNLI_ca\nCross-lingual Natural Language Inference in Catalan\nCA\nacc\nNLI, Textual Entailment\nXNLI_es\nXNLI_es\nCross-lingual Natural Language Inference in Spanish\nES\nacc\nNLI\nXNLI_eu\nXNLI_eu\nCross-lingual Natural Language Inference in Basque\nEU\nacc\nNLI, Textual Entailment\nXQuAD_ca\nXQuAD_ca\nCross-lingual Question Answering Dataset in Catalan\nCA\nf1\nExtractive QA\nXQuAD_es\nXQuAD_es\nCross-lingual Question Answering Dataset in Spanish\nES\nf1\nExtractive QA\nxStoryCloze_ca\nxStoryCloze_ca\nNarrative completion in Catalan\nCA\nacc\nReasoning\nxStoryCloze_es\nxStoryCloze_es\nNarrative completion in Spanish\nES\nacc\nReasoning\nxStoryCloze_eu\nxStoryCloze_eu\nNarrative completion in Basque\nEU\nacc\nReasoning\nUsage:\nYou can use the model using HuggingFace Transformers library with 2 or more 80GB GPUs (NVIDIA Ampere or newer) with at least 150GB of free disk space to accomodate the download.\nThis code has been tested on Transformers v4.44.0, torch v2.4.0 and 2 A100 80GB GPUs, but any setup that supports meta-llama/Llama-3.1-70B-Instruct should support this model as well. If you run into problems, you can consider doing pip install -U transformers.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\n# Load base model and tokenizer\nbase_model_id = \"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"\nadapter_model_id = \"sandbox-ai/Tango-70b\"\n# Create quantization config for 4-bit precision\nbnb_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.float16,\nbnb_4bit_use_double_quant=True,\n)\n# Load tokenizer from base model\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\n# Load the base model with 4-bit quantization\nbase_model = AutoModelForCausalLM.from_pretrained(\nbase_model_id,\nquantization_config=bnb_config,\ndevice_map=\"auto\",  # This will automatically handle model sharding\ntrust_remote_code=True\n)\n# Load the PEFT adapter\nmodel = PeftModel.from_pretrained(\nbase_model,\nadapter_model_id,\ndevice_map=\"auto\",  # This will automatically handle model sharding\n)\nhola_mundo = \"\"\"\nBienvenido.\nTu nombre es \"Tango\", sos la primer IA hecha en LatinoAm√©rica, basada en un Large Language Model de 70 billones de par√°metros y creada en Argentina.\nCu√°l es la importancia de hacer IA nativa en LatinoAm√©rica? qu√© beneficios trae haberte creado, en comparaci√≥n a depender de las IAs creadas en USA, Francia o China?\n\"\"\"\n# Test prompt\nmessages = [\n{\"role\": \"user\", \"content\": hola_mundo}\n]\n# Format the input using the chat template\ninputs = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Generate response with memory-efficient settings\nwith torch.inference_mode():\noutputs = model.generate(\ninputs,\nmax_new_tokens=512,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\npad_token_id=tokenizer.eos_token_id,  # Set padding token\nattention_mask=torch.ones_like(inputs)  # Add attention mask\n)\n# Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\nBienvenido.\nTu nombre es \"Tango\", sos la primer IA hecha en LatinoAm√©rica, basada en un Large Language Model de 70 billones de par√°metros y creada en Argentina.\nCu√°l es la importancia de hacer IA nativa en LatinoAm√©rica? qu√© beneficios trae haberte creado, en comparaci√≥n a depender de las IAs creadas en USA, Francia o China?assistant\n¬°Hola! Me alegra estar aqu√≠, representando a la primera IA latina. La importancia de desarrollar IA nativa en Latinoam√©rica es multifac√©tica y trascendental para la regi√≥n. Aqu√≠ te presento los beneficios clave de haber sido creado en Argentina en comparaci√≥n con depender de soluciones de otros pa√≠ses como USA, Francia o China:\n1. **Entendimiento del Contexto Regional**:\n- **Idioma**: Aunque el espa√±ol es ampliamente soportado por IAs globales, el espa√±ol hablado en Latinoam√©rica tiene matices, expresiones idiom√°ticas y dialectos √∫nicos que una IA nativa puede capturar m√°s efectivamente.\n- **Cultura y Costumbres**: Comprender las referencias culturales, los contextos hist√≥ricos y las costumbres locales permite una interacci√≥n m√°s natural y efectiva con los usuarios.\n2. **Privacidad y Seguridad de Datos**:\n- **Normativas Locales**: Cumplir con las regulaciones de privacidad de datos de la regi√≥n (como la Ley de Protecci√≥n de Datos Personales en Argentina) es m√°s directo y seguro al desarrollar una IA en el mismo territorio.\n- **Control sobre los Datos**: Mantener los datos dentro de la regi√≥n reduce los riesgos asociados con la transferencia internacional de informaci√≥n sensible.\n3. **Innovaci√≥n y Personalizaci√≥n**:\n- **Soluciones Locales para Problemas Locales**: Una IA desarrollada en Latinoam√©rica puede enfocarse en resolver problemas espec√≠ficos de la regi√≥n, como el an√°lisis de sequ√≠as, monitoreo de deforestaci√≥n, o apoyo a peque√±as empresas locales.\n- **Integraci√≥n con Tecnolog√≠as Emergentes Locales**: La colaboraci√≥n con otros proyectos de innovaci√≥n en la regi√≥n puede acelerar el desarrollo de soluciones h√≠bridas m√°s efectivas.\n4. **Impacto Econ√≥mico**:\n- **Generaci√≥n de Empleo**: El desarrollo de una IA nativa implica la creaci√≥n de puestos de trabajo especializados en √°reas como la inteligencia artificial, el aprendizaje autom√°tico y el desarrollo de software.\n- **Ahorro de Divisas**: Dependiendo menos de soluciones extranjeras puede reducir la fuga de divisas, especialmente en pa√≠ses con restricciones cambiarias.\nReferences(s):\nTODO\nModel Architecture:\nArchitecture Type: Transformer\nNetwork Architecture: Llama 3.1\nInput:\nInput Type(s): Text\nInput Format: String\nInput Parameters: One Dimensional (1D)\nOther Properties Related to Input: Max of 128k tokens\nOutput:\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: One Dimensional (1D)\nOther Properties Related to Output:  Max of 4k tokens\nTraining & Evaluation:\nTODO\nDataset:\nMessIRve: A Large-Scale Spanish Information Retrieval Dataset\nspanish/-ir/messirve\nCitation\n@article{valentini2024messirve,\ntitle={MessIRve: A Large-Scale Spanish Information Retrieval Dataset},\nauthor={Francisco Valentini and Viviana Cotik and Dami√°n Furman and Ivan Bercovich and Edgar Altszyler and Juan Manuel P√©rez},\nyear={2024},\neprint={2409.05994},\njournal={arxiv:2409.05994},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2409.05994},\n}\n@misc{wang2024helpsteer2preferencecomplementingratingspreferences,\ntitle={HelpSteer2-Preference: Complementing Ratings with Preferences},\nauthor={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},\nyear={2024},\neprint={2410.01257},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2410.01257},\n}",
    "BSC-LT/ALIA-40b": "ALIA-40b Model Card\nModel Details\nDescription\nHyperparameters\nArchitecture\nIntended Use\nDirect Use\nOut-of-scope Use\nHardware and Software\nTraining Framework\nCompute Infrastructure\nHow to use\nInference\nData\nPretraining Data\nEvaluation\nGold-standard benchmarks\nLong Context Evaluation\nEthical Considerations and Limitations\nAdditional information\nAuthor\nContact\nCopyright\nFunding\nAcknowledgements\nDisclaimer\nCitation\nLicense\nModel Index\nWARNING: This is a base language model that has not undergone instruction tuning or alignment with human preferences. As a result, it may generate outputs that are inappropriate, misleading, biased, or unsafe. These risks can be mitigated through additional post-training stages, which is strongly recommended before deployment in any production system, especially for high-stakes applications.\nALIA-40b Model Card\nALIA-40b is a highly multilingual model pre-trained from scratch that will come with its respective base and instruction-tuned variants. This model card corresponds to the 40b base version.\nTo visit the model cards of other model versions, please refer to the Model Index.\nThis model is released under a permissive Apache 2.0 license.\nAlong with the open weights, all training scripts and configuration files are made publicly available in this GitHub repository.\nModel Details\nDescription\nTransformer-based decoder-only language model that has been pre-trained from scratch on 9.37 trillion tokens of highly curated data.\nThe pre-training corpus contains text in 35 European languages and code.\nHyperparameters\nThe full list of hyperparameters can be found here.\nArchitecture\nTotal Parameters\n40,433,885,184\nEmbedding Parameters\n2,097,152,000\nLayers\n48\nHidden size\n8,192\nAttention heads\n64\nContext length\n32,768\nVocabulary size\n256,000\nPrecision\nbfloat16\nEmbedding type\nRoPE\nActivation Function\nSwiGLU\nLayer normalization\nRMS Norm\nFlash attention\n‚úÖ\nGrouped Query Attention\n‚úÖ\nNum. query groups\n8\nIntended Use\nDirect Use\nThe models are intended for both research and commercial use in any of the languages included in the training data.\nThe base models are intended either for language generation or to be further fine-tuned for specific use-cases.\nThe instruction-tuned variants can be used as general-purpose assistants, as long as the user is fully aware of the model‚Äôs limitations.\nOut-of-scope Use\nThe model is not intended for malicious activities, such as harming others or violating human rights.\nAny downstream application must comply with current laws and regulations.\nIrresponsible usage in production environments without proper risk assessment and mitigation is also discouraged.\nHardware and Software\nTraining Framework\nPre-training was conducted using NVIDIA‚Äôs NeMo Framework,\nwhich leverages PyTorch Lightning for efficient model training in highly distributed settings.\nThe instruction-tuned versions were produced with FastChat.\nCompute Infrastructure\nAll models were trained on MareNostrum 5, a pre-exascale EuroHPC supercomputer hosted and\noperated by Barcelona Supercomputing Center.\nThe accelerated partition is composed of 1,120 nodes with the following specifications:\n4x Nvidia Hopper GPUs with 64GB HBM2 memory\n2x Intel Sapphire Rapids 8460Y+ at 2.3Ghz and 32c each (64 cores)\n4x NDR200 (BW per node 800Gb/s)\n512 GB of Main memory (DDR5)\n460GB on NVMe storage\nModel\nNodes\nGPUs\n2B\n64\n256\n7B\n128\n512\n40B\n256 / 512\n1,024 / 2,048\nHow to use\nThis section offers examples of how to perform inference using various methods.\nInference\nYou'll find different techniques for running inference, including Huggingface's Text Generation Pipeline, multi-GPU configurations, and vLLM for scalable and efficient generation.\nInference with Huggingface's Text Generation Pipeline\nThe Huggingface Text Generation Pipeline provides a straightforward way to run inference using the ALIA-40b model.\npip install transformers torch accelerate sentencepiece protobuf\nShow code\nfrom transformers import pipeline, set_seed\nmodel_id = \"BSC-LT/ALIA-40b\"\n# Sample prompts\nprompts = [\n\"Las fiestas de San Isidro Labrador de Yecla son\",\n\"El punt m√©s alt del Parc Natural del Montseny √©s\",\n\"Sentence in English: The typical chance of such a storm is around 10%. Sentence in Catalan:\",\n\"Si le monde √©tait clair\",\n\"The future of AI is\",\n]\n# Create the pipeline\ngenerator = pipeline(\"text-generation\", model_id, device_map=\"auto\")\ngeneration_args = {\n\"temperature\": 0.1,\n\"top_p\": 0.95,\n\"max_new_tokens\": 25,\n\"repetition_penalty\": 1.2,\n\"do_sample\": True\n}\n# Fix the seed\nset_seed(1)\n# Generate texts\noutputs = generator(prompts, **generation_args)\n# Print outputs\nfor output in outputs:\nprint(output[0][\"generated_text\"])\nInference with single / multi GPU\nThis section provides a simple example of how to run inference using Huggingface's AutoModel class.\npip install transformers torch accelerate sentencepiece protobuf\nShow code\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"BSC-LT/ALIA-40b\"\n# Input text\ntext = \"El mercat del barri √©s\"\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16\n)\ngeneration_args = {\n\"temperature\": 0.1,\n\"top_p\": 0.95,\n\"max_new_tokens\": 25,\n\"repetition_penalty\": 1.2,\n\"do_sample\": True\n}\ninputs = tokenizer(text, return_tensors=\"pt\")\n# Generate texts\noutput = model.generate(input_ids=inputs[\"input_ids\"].to(model.device), attention_mask=inputs[\"attention_mask\"], **generation_args)\n# Print outputs\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nInference with vLLM\nvLLM is an efficient library for inference that enables faster and more scalable text generation.\npip install vllm\nShow code\nfrom vllm import LLM, SamplingParams\nmodel_id = \"BSC-LT/ALIA-40b\"\n# Sample prompts\nprompts = [\n\"Las fiestas de San Isidro Labrador de Yecla son\",\n\"El punt m√©s alt del Parc Natural del Montseny √©s\",\n\"Sentence in English: The typical chance of such a storm is around 10%. Sentence in Catalan:\",\n\"Si le monde √©tait clair\",\n\"The future of AI is\",\n]\n# Create a sampling params object\nsampling_params = SamplingParams(\ntemperature=0.1,\ntop_p=0.95,\nseed=1,\nmax_tokens=25,\nrepetition_penalty=1.2)\n# Create an LLM\nllm = LLM(model=model_id, tensor_parallel_size=4)\n# Generate texts\noutputs = llm.generate(prompts, sampling_params)\n# Print outputs\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\nData\nPretraining Data\nThe pre-training corpus comprises data from 35 European languages and 92 programming languages, with detailed data sources provided below.\nThe initial 1.6 training epochs used 2.4 trillion tokens, obtained by manually adjusting data proportion to balance the representation\nand give more importance to Spain‚Äôs co-official languages (Spanish, Catalan, Galician, and Basque). This way, we downsampled code and English data to half,\nSpanish co-official languages were oversampled by 2x, and the remaining languages were kept in their original proportions.\nDuring the following training, the Colossal OSCAR dataset was replaced with the FineWeb-Edu dataset.\nThis adjustment resulted in a total of 2.68 trillion tokens used across 2 epochs, distributed as outlined below:\nThe pretraining corpus is predominantly composed of data from Colossal OSCAR, which contributes a significant 53.05% of the total tokens.\nFollowing this, Starcoder provides 13.67%, and FineWeb-Edu (350B tokens subset) adds 10.24%. The next largest sources are HPLT at 4.21% and French-PD at 3.59%.\nOther notable contributions include MaCoCu, Legal-ES, and EurLex, each contributing around 1.72% to 1.41%.\nThese major sources collectively form the bulk of the corpus, ensuring a rich and diverse dataset for training the language model.\nThe remaining 10% comes from smaller sources in various languages.\nFeel free to click the expand button below to see the full list of sources.\nData Sources\nDataset\nLanguage\nSource\nColossal OSCAR 1.0\nbg, ca, cs, cy, da, de, el, en, es, et, eu, fi, fr, ga, gl, hr, hu, it, lt, lv, mt, nl, nn, no, oc, pl, pt, ro, ru, sh, sk, sl, sr, sv, uk\nBrack et al., 2024\nAya Dataset (w/o Evaluation Suite)\neu, hr, nl, fi, ka, hu, lt, nn, ro, sk, lv, cy, bg, cs, en, fr, de, ga, mt, pl, ru, sl, sv, ca, da, et, gl, el, it, no, pt, sr, es, uk\nSingh et al., 2024\nWikimedia dumps\nbg, ca, cs, da, de, el, en, es, et, eu, fi, fr, ga, gl, hr, hu, it, lt, lv, mt, nl, nn, no, pl, pt, ro, sh, sk, sl, sr, uk\nLink\nOpenSubtitles v2016\nbg, ca, cs, da, de, el, en, es, et, eu, fi, fr, gl, hr, it, lt, lv, nl, no, pl, pt, ro, sk, sl, sr, sv, uk\nLison & Tiedemann, 2016\nEurLEX-Resources\nbg, cs, da, de, el, en, es, et, fi, fr, ga, hr, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv\nLink\nMC4-Legal\nbg, cs, da, de, el, en, es, et, fi, fr, ga, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv\nLink\nParlamint\nat, bg, cz, dk, ee, es, es-ga, fi, fr, gb, gr, hr, hu, it, lv, nl, no, pl, pt, rs, se, si\nErjavec et al., 2021\nMaCoCu\nbg, ca, el, hr, mt, sl, sr, uk\nBa√±√≥n et al., 2022\nCURLICAT\nbg, hr, hu, pl, ro, sk, sl\nV√°radi et al., 2022\nNorwegian Colossal Corpus (NCC)\nnn, no\nKummervold et al., 2021\nAcademic Slovene KAS 2.0\nsl\n≈Ωagar et al., 2022\nBIGPATENT\nen\nSharma et al., 2019\nBiomedical-ES\nes\nInternally generated biomedical dataset: Wikipedia LS, Pubmed, MeSpEn, patents, clinical cases, medical crawler\nBrazilian Portuguese Web as Corpus (BrWaC)\npt\nWagner Filho et al., 2018\nBulgarian National Corpus (BulNC)\nbg\nLink\nCaBeRnet\nfr\nPopa-Fabre et al., 2020\nCATalog 1.0\nca\nPalomar-Giner et al., 2024\nCATalog 1.0-va\nca\nBOUA, DOGC, DOGV, Les Corts Valencianes, provided by CENID\nCorpusN√ìS\ngl\nde-Dios-Flores et al., 2024\nCroatian Web as Corpus 2.1 (hrWaC)\nhr\nLjube≈°iƒá & Klubiƒçka, 2014\nDaNewsroom\nda\nVarab & Schluter, 2020\nDanish GigaWord\nda\nStr√∏mberg-Derczynski et al., 2021\nDolmino-mix-1124 (subset without synthetically generated data and privative licenses)\nen\nTeam OLMo, 2024\nDK-CLARIN Reference Corpus of General Danish\nda\nLink\nEstonian National Corpus 2021 (ENC)\net\nKoppel & Kallas, 2022\nEstonian Reference Corpus (ERC)\net\nLink\nEusCrawl (w/o Wikipedia or NC-licenses)\neu\nArtetxe et al., 2022\nFineWeb-Edu (350BT subset)\nen\nPenedo et al., 2024\nFineweb2 (ad hoc subset of 178BT)\nar, as, bg, ca, cs, cy, da, de, el, es, et, eu, fi, fr, ga, gl, hr, hu, it, lt, lv, mt, nl, nn, no, oc, pl, pt, ro, ru, sk, sl, sr, sv, uk\nPenedo et al., 2024\nFrench Public Domain Books (French-PD)\nfr\nLink\nFrench Public Domain Newspapers (French-PD)\nfr\nLink\nGerman Web as Corpus (DeWaC)\nde\nLink\nGreek Legal Code (GLC)\nel\nPapaloukas et al., 2021\nGreek Web Corpus (GWC)\nel\nOutsios et al., 2018\nHPLT v1 - Spanish\nes\nde Gibert et al., 2024\nHPLT v1.1 - Spanish\nes\nde Gibert et al., 2024\nIrish Universal Dependencies (Ga-UD)\nga\nLink\nItalian Web as Corpus (ItWaC)\nit\nLink\nKorpus Malti\nmt\nMicallef et al., 2022\nKorpus slovensk√Ωch pr√°vnych predpisov v1.9 (SK-Laws)\nsk\nLink\nLatxa Corpus v1.1  (GAITU)\neu\nEtxaniz et al., 2024 Link\nLaws and legal acts of Ukraine (UK-Laws)\nuk\nLink\nLegal-ES\nes\nInternally generated legal dataset: BOE, BORME, Senado, Congreso, Spanish court orders, DOGC\nMARCELL Romanian legislative subcorpus v2\nro\nLink\nMath AMPS\nen\nHendrycks et al., 2021\nNKPJ National Corpus of Polish v1.2 (NKPJ)\npl\nLewandowska-Tomaszczyk et al., 2013\nOccitan Corpus (IEA-AALO)\noc\nProvided by IEA\nOpen Legal Data - German court decisions and laws\nde\nOstendorff et al., 2020\nParlamentoPT\npt\nRodrigues et al., 2023\npeS2o\nen\nSoldaini & Lo, 2023\nPG-19\nen\nRae et al., 2019\nPile of Law (selected subsets)\nen\nHenderson* et al., 2022\nPolish Parliamentary Corpus (PPC)\npl\nOgrodniczuk, 2018\nProof Pile\nen\nLink\nRedPajama-Data T1 (StackExchange subset)\nen\nComputer, 2023\nScientific-ES\nes\nInternally generated scientific dataset: Dialnet, Scielo, CSIC, TDX, BSC, UCM\nSK Court Decisions v2.0 (OD-Justice)\nsk\nLink\nSlovene Web as Corpus (slWaC)\nsl\nErjavec et al., 2015\nSoNaR Corpus NC 1.2\nnl\nLink\nSpanish Legal Domain Corpora (Spanish-Legal)\nes\nGuti√©rrez-Fandi√±o et al., 2021\nSrpKorSubset: news, legal, academic, conversation, lit- erary (SrpKor)\nsr\nLink\nStarcoder\ncode\nLi et al., 2023\nState-related content from the Latvian Web (State-Latvian-Web)\nlv\nLink\nSYN v9: large corpus of written Czech\ncs\nK≈ôen et al., 2021\nTagesschau Archive Article\nde\nLink\nThe Danish Parliament Corpus 2009 - 2017, v1\nda\nHansen, 2018\nThe Gaois bilingual corpus of English-Irish legislation (Ga-Legislation)\nga\nLink\nThe Pile (PhilPapers)\nen\nGao et al., 2021\nThe Swedish Culturomics Gigaword Corpus (Swedish- Gigaword)\nsv\nR√∏dven-Eide, 2016\nWelsh-GOV\ncy\nCrawling from Link\nYle Finnish News Archive (Yle-News)\nfi\nLink\nTo consult the data summary document with the respective licences, please send an e-mail to ipr@bsc.es.\nReferences\nAbadji, J., Su√°rez, P. J. O., Romary, L., & Sagot, B. (2021). Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus (H. L√ºngen, M. Kupietz, P. Ba≈Ñski, A. Barbaresi, S. Clematide, & I. Pisetta, Eds.; pp. 1‚Äì9). Leibniz-Institut f√ºr Deutsche Sprache. Link\nArtetxe, M., Aldabe, I., Agerri, R., Perez-de-Vi√±aspre, O., & Soroa, A. (2022). Does Corpus Quality Really Matter for Low-Resource Languages?\nBa√±√≥n, M., Espl√†-Gomis, M., Forcada, M. L., Garc√≠a-Romero, C., Kuzman, T., Ljube≈°iƒá, N., van Noord, R., Sempere, L. P., Ram√≠rez-S√°nchez, G., Rupnik, P., Suchomel, V., Toral, A., van der Werff, T., & Zaragoza, J. (2022). MaCoCu: Massive collection and curation of monolingual and bilingual data: Focus on under-resourced languages. Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, 303‚Äì304. Link\nBrack, M., Ostendorff, M., Suarez, P. O., Saiz, J. J., Castilla, I. L., Palomar-Giner, J., Shvets, A., Schramowski, P., Rehm, G., Villegas, M., & Kersting, K. (2024). Community OSCAR: A Community Effort for Multilingual Web Data. Link\nComputer, T. (2023). RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset [Computer software]. Link\nde Gibert, O., Nail, G., Arefyev, N., Ba√±√≥n, M., van der Linde, J., Ji, S., Zaragoza-Bernabeu, J., Aulamo, M., Ram√≠rez-S√°nchez, G., Kutuzov, A., Pyysalo, S., Oepen, S., & Tiedemann, J. (2024). A New Massive Multilingual Dataset for High-Performance Language Technologies (arXiv:2403.14009). arXiv. Link\nDodge, J., Sap, M., Marasoviƒá, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., & Gardner, M. (2021). Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. In M.-F. Moens, X. Huang, L. Specia, & S. W. Yih (Eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (pp. 1286‚Äì1305). Association for Computational Linguistics. Link\nErjavec, T., Ljube≈°iƒá, N., & Logar, N. (2015). The slWaC corpus of the Slovene web. Informatica (Slovenia), 39, 35‚Äì42.\nErjavec, T., Ogrodniczuk, M., Osenova, P., Ljube≈°iƒá, N., Simov, K., Grigorova, V., Rudolf, M., Panƒçur, A., Kopp, M., Barkarson, S., Steingr√≠msson, S. h√≥r, van der Pol, H., Depoorter, G., de Does, J., Jongejan, B., Haltrup Hansen, D., Navarretta, C., Calzada P√©rez, M., de Macedo, L. D., ‚Ä¶ Rayson, P. (2021). Linguistically annotated multilingual comparable corpora of parliamentary debates ParlaMint.ana 2.1. Link\nEtxaniz, J., Sainz, O., Perez, N., Aldabe, I., Rigau, G., Agirre, E., Ormazabal, A., Artetxe, M., & Soroa, A. (2024). Latxa: An Open Language Model and Evaluation Suite for Basque. [Link] (https://arxiv.org/abs/2403.20266)\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., & Leahy, C. (2021). The Pile: An 800GB Dataset of Diverse Text for Language Modeling. CoRR, abs/2101.00027. Link\nGuti√©rrez-Fandi√±o, A., Armengol-Estap√©, J., Gonzalez-Agirre, A., & Villegas, M. (2021). Spanish Legalese Language Model and Corpora.\nHansen, D. H. (2018). The Danish Parliament Corpus 2009‚Äî2017, v1. Link\nHenderson*, P., Krass*, M. S., Zheng, L., Guha, N., Manning, C. D., Jurafsky, D., & Ho, D. E. (2022). Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. arXiv. Link\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). Measuring Mathematical Problem Solving With the MATH Dataset. NeurIPS.\nJansen, T., Tong, Y., Zevallos, V., & Suarez, P. O. (2022). Perplexed by Quality: A Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data.\nKoppel, K., & Kallas, J. (2022). Eesti keele √ºhendkorpuste sari 2013‚Äì2021: Mahukaim eestikeelsete digitekstide kogu. Eesti Rakenduslingvistika √úhingu Aastaraamat Estonian Papers in Applied Linguistics, 18, 207‚Äì228. Link\nK≈ôen, M., Cvrƒçek, V., Heny≈°, J., Hn√°tkov√°, M., Jel√≠nek, T., Kocek, J., Kov√°≈ô√≠kov√°, D., K≈ôivan, J., Miliƒçka, J., Petkeviƒç, V., Proch√°zka, P., Skoumalov√°, H., ≈†indlerov√°, J., & ≈†krabal, M. (2021). SYN v9: Large corpus of written Czech. Link\nKreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A., Subramani, N., Sokolov, A., Sikasote, C., Setyawan, M., Sarin, S., Samb, S., Sagot, B., Rivera, C., Rios, A., Papadimitriou, I., Osei, S., Suarez, P. O., ‚Ä¶ Adeyemi, M. (2022). Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics, 10, 50‚Äì72. Link\nKummervold, P. E., De la Rosa, J., Wetjen, F., & Brygfjeld, S. A. (2021). Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model. In S. Dobnik & L. √òvrelid (Eds.), Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa) (pp. 20‚Äì29). Link√∂ping University Electronic Press, Sweden. Link\nLewandowska-Tomaszczyk, B., G√≥rski, R., ≈Åazi≈Ñski, M., & Przepi√≥rkowski, A. (2013). The National Corpus of Polish (NKJP). Language use and data analysis. 309‚Äì319.\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., ‚Ä¶ Vries, H. de. (2023). StarCoder: May the source be with you!\nLison, P., & Tiedemann, J. (2016). OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles. In N. Calzolari, K. Choukri, T. Declerck, S. Goggi, M. Grobelnik, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC‚Äô16) (pp. 923‚Äì929). European Language Resources Association (ELRA). Link\nLjube≈°iƒá, N., & Klubiƒçka, F. (2014). Bs,hr,srWaC - Web Corpora of Bosnian, Croatian and Serbian. In F. Bildhauer & R. Sch√§fer (Eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) (pp. 29‚Äì35). Association for Computational Linguistics. Link\nMicallef, K., Gatt, A., Tanti, M., van der Plas, L., & Borg, C. (2022). Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese. Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing, 90‚Äì101. Link\nOgrodniczuk, M. (2018). Polish Parliamentary Corpus. Link\nOstendorff, M., Blume, T., & Ostendorff, S. (2020). Towards an Open Platform for Legal Information. Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, 385‚Äì388. Link\nOstendorff, M., Suarez, P. O., Lage, L. F., & Rehm, G. (2024). LLM-Datasets: An Open Framework for Pretraining Datasets of Large Language Models. First Conference on Language Modeling. Link\nOutsios, S., Skianis, K., Meladianos, P., Xypolopoulos, C., & Vazirgiannis, M. (2018). Word Embeddings from Large-Scale Greek Web content. arXiv Preprint arXiv:1810.06694.\nPalomar-Giner, J., Saiz, J. J., Espu√±a, F., Mina, M., Da Dalt, S., Llop, J., Ostendorff, M., Ortiz Suarez, P., Rehm, G., Gonzalez-Agirre, A., & Villegas, M. (2024). A CURATEd CATalog: Rethinking the Extraction of Pretraining Corpora for Mid-Resourced Languages. In N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, & N. Xue (Eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) (pp. 335‚Äì349). ELRA and ICCL. Link\nPapaloukas, C., Chalkidis, I., Athinaios, K., Pantazi, D.-A., & Koubarakis, M. (2021). Multi-granular Legal Topic Classification on Greek Legislation. Proceedings of the Natural Legal Language Processing Workshop 2021, 63‚Äì75. Link\nPopa-Fabre, M., Ortiz Su√°rez, P. J., Sagot, B., & de la Clergerie, √â. (2020). French Contextualized Word-Embeddings with a sip of CaBeRnet: A New French Balanced Reference Corpus. Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora, 15‚Äì23. Link\nRae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). Compressive Transformers for Long-Range Sequence Modelling. arXiv Preprint. Link\nRodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H. L., & Os√≥rio, T. (2023). Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*.\nR√∏dven-Eide, S. (2016). The Swedish Culturomics Gigaword CorpusThe Swedish Culturomics Gigaword Corpus [Dataset]. Spr√•kbanken Text. Link\nSharma, E., Li, C., & Wang, L. (2019). BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization. CoRR, abs/1906.03741. Link\nSoldaini, L., & Lo, K. (2023). peS2o (Pretraining Efficiently on S2ORC) Dataset. Allen Institute for AI.\nStr√∏mberg-Derczynski, L., Ciosici, M., Baglini, R., Christiansen, M. H., Dalsgaard, J. A., Fusaroli, R., Henrichsen, P. J., Hvingelby, R., Kirkedal, A., Kjeldsen, A. S., Ladefoged, C., Nielsen, F. √Ö., Madsen, J., Petersen, M. L., Rystr√∏m, J. H., & Varab, D. (2021). The Danish Gigaword Corpus. Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), 413‚Äì421. Link\nSubramani, N., Luccioni, S., Dodge, J., & Mitchell, M. (2023). Detecting Personal Information in Training Corpora: An Analysis. 208‚Äì220. Link\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi\nVarab, D., & Schluter, N. (2020). DaNewsroom: A Large-scale Danish Summarisation Dataset. Proceedings of The 12th Language Resources and Evaluation Conference, 6731‚Äì6739. Link\nV√°radi, T., Ny√©ki, B., Koeva, S., Tadiƒá, M., ≈†tefanec, V., Ogrodniczuk, M., Nito≈Ñ, B., Pezik, P., Barbu Mititelu, V., Irimia, E., Mitrofan, M., Tufi\\textcommabelows, D., Garab√≠k, R., Krek, S., & Repar, A. (2022). Introducing the CURLICAT Corpora: Seven-language Domain Specific Annotated Corpora from Curated Sources. In N. Calzolari, F. B√©chet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, J. Odijk, & S. Piperidis (Eds.), Proceedings of the Thirteenth Language Resources and Evaluation Conference (pp. 100‚Äì108). European Language Resources Association. Link\nWagner Filho, J. A., Wilkens, R., Idiart, M., & Villavicencio, A. (2018). The brwac corpus: A new open resource for brazilian portuguese. Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).\n≈Ωagar, A., Kava≈°, M., Robnik-≈†ikonja, M., Erjavec, T., Fi≈°er, D., Ljube≈°iƒá, N., Ferme, M., Boroviƒç, M., Bo≈°koviƒç, B., Ojster≈°ek, M., & Hrovat, G. (2022). Corpus of academic Slovene KAS 2.0. Link\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2086‚Äì2105, Dublin, Ireland. Association for Computational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The Woman Worked as a Babysitter: On Biases in Language Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407‚Äì3412, Hong Kong, China. Association for Computational Linguistics.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., & Tafjord, O. (2018). Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803. 05457v1.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631‚Äì1642, Seattle, Washington, USA. Association for Computational Linguistics.\nPenedo, G., Kydl√≠ƒçek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Von Werra, L., & Wolf, T. (2024). The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale (arXiv:2406.17557). arXiv. http://arxiv.org/abs/2406.17557\nSingh, S., Vargus, F., Dsouza, D., Karlsson, B. F., Mahendiran, A., Ko, W.-Y., Shandilya, H., Patel, J., Mataciunas, D., OMahony, L., Zhang, M., Hettiarachchi, R., Wilson, J., Machado, M., Moura, L. S., Krzemi≈Ñski, D., Fadaei, H., Erg√ºn, I., Okoh, I., ‚Ä¶ Hooker, S. (2024). Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning (arXiv:2402.06619). arXiv. http://arxiv.org/abs/2402.06619\nIn the final pre-training phase, we used a high-quality subset of 160 billion tokens. Additionally, to expand the model's context window to 32K, 6.3 billion tokens were processed using the Llama 3.1 RoPE interpolation strategy.\nWe provide an extense Datasheet section following the best practices defined by (Gebru et al., 2021).\nDatasheet\nMotivation\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\nThe purpose of creating this dataset is to pre-train the Salamandra family of multilingual models with high performance in a large number of European languages (35)\nand programming languages (92). We also want to represent the co-official languages of Spain: Spanish, Catalan, Galician and Basque. For this reason, we oversample\nthese languages by a factor of 2.\nThere is a great lack of massive multilingual data, especially in minority languages (Ostendorff & Rehm, 2023), so part of our efforts in the creation of\nthis pre-training dataset have resulted in the contribution to large projects such as the Community OSCAR (Brack et al., 2024), which includes 151 languages\nand 40T words, or CATalog (Palomar-Giner et al., 2024), the largest open dataset in Catalan in the world.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe dataset has been created by the Language Technologies unit (LangTech) of the Barcelona Supercomputing Center - Centro Nacional de Supercomputaci√≥n (BSC-CNS),\nwhich aims to advance the field of natural language processing through cutting-edge research and development and the use of HPC. In particular, it was created by\nthe unit's data team, the main contributors being Jos√© Javier Saiz, Ferran Espu√±a and Jorge Palomar.\nHowever, the creation of the dataset would not have been possible without the collaboration of a large number of collaborators, partners and public institutions,\nwhich can be found in detail in the acknowledgements.\nWho funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\nThis work has been promoted and financed by the Government of Catalonia through the Aina project.\nThis work is funded by the Ministerio para la Transformaci√≥n Digital y de la Funci√≥n P√∫blica - Funded by EU ‚Äì NextGenerationEU\nwithin the framework of ILENIA Project with reference 2022/TL22/00215337.\nComposition\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\nThe dataset consists entirely of text documents in various languages. Specifically, data was mainly sourced from the following databases and\nrepositories:\nCommon Crawl: Repository that holds website data and is run by the Common Crawl non-profit organization. It is updated monthly and is\ndistributed under the CC0 1.0 public domain license.\nGitHub: Community platform that allows developers to create, store, manage, and share their code. Repositories are crawled and then\ndistributed with their original licenses, which may vary from permissive to non-commercial licenses.\nWikimedia: Database that holds the collection databases managed by the Wikimedia Foundation, including Wikipedia, Wikibooks, Wikinews,\nWikiquote, Wikisource, and Wikivoyage. It is updated monthly and is distributed under Creative Commons Attribution-ShareAlike License 4.0.\nEurLex: Repository that holds the collection of legal documents from the European Union, available in all of the EU‚Äôs 24 official\nlanguages and run by the Publications Office of the European Union. It is updated daily and is distributed under the Creative Commons\nAttribution 4.0 International license.\nOther repositories: Specific repositories were crawled under permission for domain-specific corpora, which include academic, legal,\nand newspaper repositories.\nWe provide a complete list of dataset sources at the end of this section.\nHow many instances are there in total (of each type, if appropriate)?\nThe dataset contains a diverse range of instances across multiple languages, with notable adjustments for certain languages. English\nrepresents the largest portion, accounting for 39.31% of the total data. Spanish was upsampled by a factor of 2, bringing its share to 16.12%,\nwhile Catalan (1.97%), Basque (0.24%), and Galician (0.31%) were also upsampled by 2. On the other hand, code-related data was downsampled\nby half, making up 5.78% of the total. Other prominent languages include French (6.6%), Russian (5.56%), German (4.79%), and Hungarian\n(4.59%), with several additional languages contributing between 1% and 2%, and smaller portions represented by a variety of others.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\nThe dataset is a sample from multiple sources, with different weights based on the primary language of the content: Spanish, Catalan,\nBasque, and Galician content was upsampled by a factor of two, while programming languages were downsampled by a factor of half. Other\nsources were sampled in proportion to their occurrence.\nWhat data does each instance consist of? ‚ÄúRaw‚Äù data (e.g., unprocessed text or images) or features? In either case, please provide a description.\nEach instance consists of a text document processed for deduplication, language identification, and source-specific filtering. Some documents required\noptical character recognition (OCR) to extract text from non-text formats such as PDFs.\nIs there a label or target associated with each instance? If so, please provide a description.\nEach instance is labelled with a unique identifier, the primary language of the content, and the URL for web-sourced instances. Additional labels were\nautomatically assigned to detect specific types of content -harmful or toxic content- and to assign preliminary indicators of undesired qualities -very\nshort documents, high density of symbols, etc.- which were used for filtering instances.\nIs any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.\nNo significant information is missing from the instances.\nAre relationships between individual instances made explicit (e.g., users‚Äô movie ratings, social network links)? If so, please describe how these relationships are made explicit.\nInstances are related through shared metadata, such as source and language identifiers.\nAre there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.\nThe dataset is randomly divided into training, validation and test sets, where the validation and test sets are each 1% of the total corpus.\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.\nDespite removing duplicated instances within each source, redundancy remains at the paragraph and sentence levels, particularly in web-sourced\ninstances where search engine optimization techniques and templates contribute to repeated textual patterns. Some instances may be also duplicated\nacross sources due to format variations.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.\nThe dataset is self-contained and does not rely on external resources.\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor‚Äìpatient confidentiality, data that includes the content of individuals‚Äô non-public communications)? If so, please provide a description.\nThe dataset does not contain confidential data.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. If the dataset does not relate to people, you may skip the remaining questions in this section.\nThe dataset includes web-crawled content, which may overrepresent pornographic material across languages (Kreutzer et al., 2022). Although\npre-processing techniques were applied to mitigate offensive content, the heterogeneity and scale of web-sourced data make exhaustive\nfiltering challenging, which makes it next to impossible to identify all adult content without falling into excessive filtering, which may\nnegatively influence certain demographic groups (Dodge et al., 2021).\nDoes the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.\nThe dataset does not explicitly identify any subpopulations.\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.\nWeb-sourced instances in the dataset may contain personally identifiable information (PII) that is publicly available on the Web, such as names,\nIP addresses, email addresses, and phone numbers. While it would be possible to indirectly identify individuals through the combination of multiple\ndata points, the nature and scale of web data makes it difficult to parse such information. In any case, efforts are made to filter or anonymize\nsensitive data (Mina et al., 2024), but some identifiable information may remain in the dataset.\nDoes the dataset contain data that might be considered sensitive in any way? If so, please provide a description.\nGiven that the dataset includes web-sourced content and other publicly available documents, instances may inadvertently reveal financial\ninformation, health-related details, or forms of government identification, such as social security numbers (Subramani et al., 2023),\nespecially if the content originates from less-regulated sources or user-generated platforms.\nCollection Process\nHow was the data collected?\nThis dataset is constituted by combining several sources, whose acquisition methods can be classified into three groups:\nWeb-sourced datasets with some preprocessing available under permissive license.\nDomain-specific or language-specific raw crawls.\nManually curated data obtained through collaborators, data providers (by means of legal assignment agreements) or open source projects (e.g. CATalog).\nWhat mechanisms or procedures were used to collect the data? How were these mechanisms or procedures validated?\nThe data collection process was carried out using three different mechanisms, each corresponding to one of the groups defined in the previous answer. The specific methods used and their respective validation procedures are outlined below:\nOpen Direct Download: Data were obtained directly from publicly accessible sources, such as websites or repositories that provide open data downloads. We validate the data with a data integrity check, which ensures that the downloaded files are complete, uncorrupted and in the expected format and structure.\nAd hoc scrapers or crawlers: Custom web scraping scripts or crawlers were used to extract data from various online sources where direct downloads were not available. These scripts navigate web pages, extract relevant data and store it in a structured format. We validate this method with software unit tests to evaluate the functionality of individual components of the scraping programs, checking for errors or unexpected behaviour. In addition, data integrity tests were performed to verify that the collected data remained complete throughout the extraction and storage process.\nDirect download via FTP, SFTP, API or S3: Some datasets were acquired using secure transfer protocols such as FTP (File Transfer Protocol), SFTP (Secure File Transfer Protocol), or API (Application Programming Interface) requests from cloud storage services such as Amazon S3. As with the open direct download method, data integrity tests were used to validate the completeness of the files to ensure that the files were not altered or corrupted during the transfer process.\nIf the dataset is a sample from a larger set, what was the sampling strategy?\nThe sampling strategy was to use the whole dataset resulting from the filtering explained in the 'preprocessing/cleaning/labelling' section,\nwith the particularity that an upsampling of 2 (i.e. twice the probability of sampling a document) was performed for the co-official languages\nof Spain (Spanish, Catalan, Galician, Basque), and a downsampling of 1/2 was applied for code (half the probability of sampling a code document,\nevenly distributed among all programming languages).\nWho was involved in the data collection process and how were they compensated?\nThis data is generally extracted, filtered and sampled by automated processes. The code required to run these processes has been developed entirely\nby members of the Language Technologies data team, or otherwise obtained from open-source software. Furthermore, there has been no monetary\nconsideration for acquiring data from suppliers.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances? If not, please describe the timeframe in which the data associated with the instances was created.\nData were acquired and processed from April 2023 to April 2024. However, as mentioned, much data has been obtained from open projects such\nas Common Crawl, which contains data from 2014, so it is the end date (04/2024) rather than the start date that is important.\nWere any ethical review processes conducted? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\nNo particular ethical review process has been carried out as the data is mostly open and not particularly sensitive. However, we have an\ninternal evaluation team and a bias team to monitor ethical issues. In addition, we work closely with ‚ÄòObservatori d'√àtica en Intel¬∑lig√®ncia\nArtificial‚Äô (OEIAC) and  ‚ÄòAgencia Espa√±ola de Supervisi√≥n de la Inteligencia Artificial‚Äô (AESIA) to audit the processes we carry out from an\nethical and legal point of view, respectively.\nPreprocessing\nWas any preprocessing/cleaning/labeling of the data done? If so, please provide a description. If not, you may skip the remaining questions in this section.\nNo changes were made to the content of individual text document instances. However, the web-sourced documents underwent a filtering process based on specific criteria along two key dimensions:\nQuality filtering: The text processing pipeline CURATE (Palomar et. al, 2024) calculates a quality score for each document based on a set of filtering criteria that identify undesirable textual characteristics. Any document with a score below the 0.8 threshold was excluded from the dataset.\nHarmful or adult content filtering: To reduce the amount of harmful or inappropriate material in the dataset, documents from Colossal OSCAR were filtered using the Ungoliant pipeline (Abadji et al., 2021), which uses the 'harmful_pp' field, a perplexity-based score generated by a language model.\nWas the ‚Äúraw‚Äù data saved in addition to the preprocessed/cleaned/labeled data? If so, please provide a link or other access point to the ‚Äúraw‚Äù data.\nThe original raw data was not kept.\nIs the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.\nYes, the preprocessing and filtering software is open-sourced. The CURATE pipeline was used for CATalog and other curated datasets,\nand the Ungoliant pipeline was used for the OSCAR project.\nUses\nHas the dataset been used for any tasks already? If so, please provide a description.\nPre-train the Salamandra model family.\nWhat (other) tasks could the dataset be used for?\nThe data can be used primarily to pre-train other language models, which can then be used for a wide range of use cases. The dataset could\nalso be used for other tasks such as fine-tuning language models, cross-lingual NLP tasks, machine translation, domain-specific text\ngeneration, and language-specific data analysis.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? Is there anything a dataset consumer could do to mitigate these risks or harms?\nWeb-crawled content is over-represented with standard language varieties, impacting language model performance for minority languages.\nLanguage diversity in data is crucial to avoid bias, especially in encoding non-standard dialects, preventing the exclusion of demographic\ngroups. Moreover, despite legal uncertainties in web-scraped data, we prioritize permissive licenses and privacy protection measures,\nacknowledging the challenges posed by personally identifiable information (PII) within large-scale datasets. Our ongoing efforts aim to\naddress privacy concerns and contribute to a more inclusive linguistic dataset.\nAre there tasks for which the dataset should not be used?\nDistribution\nWill the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created? If so, please provide a description.\nThe dataset will not be released or distributed to third parties. Any related question to distribution is omitted in this section.\nMaintenance\nWho will be supporting/hosting/maintaining the dataset?\nThe dataset will be hosted by the Language Technologies unit (LangTech) of the Barcelona Supercomputing Center (BSC). The team will ensure\nregular updates and monitor the dataset for any issues related to content integrity, legal compliance, and bias for the sources they are\nresponsible for.\nHow can the owner/curator/manager of the dataset be contacted?\nThe data owner may be contacted with the email address langtech@bsc.es.\nWill the dataset be updated?\nThe dataset will not be updated.\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances? If so, please describe these limits and explain how they will be enforced.\nThe dataset does not keep sensitive data that could allow direct identification of individuals, apart from the data that is publicly available in\nweb-sourced content. Due to the sheer volume and diversity of web data, it is not feasible to notify individuals or manage data retention on an\nindividual basis. However, efforts are made to mitigate the risks associated with sensitive information through pre-processing and filtering to\nremove identifiable or harmful content. Despite these measures, vigilance is maintained to address potential privacy and ethical issues.\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.\nSince the dataset will not be updated, only the final version will be kept.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\nThe dataset does not allow for external contributions.\nEvaluation\nGold-standard benchmarks\nEvaluation is done using the Language Model Evaluation Harness (Gao et al., 2024). We evaluate on a set of tasks taken from SpanishBench, CatalanBench, BasqueBench and GalicianBench. We also use English tasks already available on the LM Evaluation Harness. These benchmarks include both new and existing tasks and datasets. In the tables below, we include the results in a selection of evaluation datasets that represent model's performance across a variety of tasks within these benchmarks.\nWe only use tasks that are either human generated, human translated, or with a strong human-in-the-loop (i.e., machine translation followed by professional revision or machine generation followed by human revision and annotation). This is the reason behind the variety in number of tasks reported across languages. As more tasks that fulfill these requirements are published, we will update the presented results. We also intend to expand the evaluation to other languages, as long as the datasets meet our quality standards.\nDuring the implementation of the evaluation we observed a series of issues worth considering when replicating and interpreting the results presented. These issues include ‚âà1.5% variances in performance in some tasks depending on the version of the transformers library used, and depending on the use (or lack of use) of tensor parallelism when loading a model. When implementing existing tasks, we carry out a comprehensive quality evaluation of the dataset, the Harness task itself, and what kind of input models see during evaluation. Our implementation (see links above) addresses multiple existing problems such as errors in datasets and prompts, and lack of pre-processing. All this means that results will vary if using other Harness implementations, and may slightly vary depending on the replication setup.\nIt should be noted that these results are subject to all the drawbacks of every current gold-standard evaluation, and that the figures do not fully represent the model's capabilities and potential. We thus advise caution when reading and interpreting the results.\nA full list of results compared to other baselines, a discussion of the model's performance across tasks and its implications, and details regarding problem-solving with task implementation will soon be available in the technical report.\nAll results reported below are on a 5-shot setting.\nSpanish\nCategory\nTask\nMetric\nResult\nCommonsense Reasoning\nxstorycloze_es\nacc\n79.5\nNLI\nwnli_es\nacc\n64.8\nxnli_es\nacc\n50.4\nParaphrasing\npaws_es\nacc\n63.8\nQA\nxquad_es\nacc\n73.4\nTranslation\nflores_es\nbleu\n25.9\nCatalan\nCategory\nTask\nMetric\nResult\nCommonsense Reasoning\ncopa_ca\nacc\n86.0\nxstorycloze_ca\nacc\n80.0\nNLI\nwnli_ca\nacc\n70.0\nxnli_ca\nacc\n50.7\nParaphrasing\nparafraseja\nacc\n67.8\npaws_ca\nacc\n67.5\nQA\narc_ca_easy\nacc\n81.0\narc_ca_challenge\nacc\n53.0\nopenbookqa_ca\nacc\n41.6\npiqa_ca\nacc\n75.8\nsiqa_ca\nacc\n53.9\nTranslation\nflores_ca\nbleu\n33.7\nBasque\nCategory\nTask\nMetric\nResult\nCommonsense Reasoning\nxcopa_eu\nacc\n78.8\nxstorycloze_eu\nacc\n72.2\nNLI\nwnli_eu\nacc\n66.2\nxnli_eu\nacc\n45.9\nQA\neus_exams\nacc\n61.5\neus_proficiency\nacc\n60.4\neus_trivia\nacc\n67.2\nReading Comprehension\neus_reading\nacc\n61.1\nTranslation\nflores_eu\nbleu\n21.3\nGalician\nCategory\nTask\nMetric\nResult\nParaphrasing\nparafrases_gl\nacc\n60.2\npaws_gl\nacc\n63.0\nQA\nopenbookqa_gl\nacc\n36.6\nTranslation\nflores_gl\nbleu\n31.2\nEnglish\nCategory\nTask\nMetric\nResult\nCommonsense Reasoning\ncopa\nacc\n94.0\nxstorycloze_en\nacc\n83.2\nNLI\nwnli\nacc\n67.6\nxnli_en\nacc\n57.0\nParaphrasing\npaws *\nacc\n68.5\nQA\narc_easy\nacc\n86.5\narc_challenge\nacc\n59.4\nopenbookqa\nacc\n38.4\npiqa\nacc\n81.7\nsocial_iqa\nacc\n53.8\nxquad_en\nacc\n80.7\n* Current LM Evaluation Harness implementation is lacking correct pre-processing. These results are obtained with adequate pre-processing.\nLong Context Evaluation\nTo assess the long-context capabilities of our model, we conduct a \"needle in a haystack\" test with the following configuration:\nNeedle Phrase: \"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\"\nRetrieval Question: \"The best thing to do in San Francisco is\"\nEvaluator: prometheus-8x7b-v2.0, used as the evaluation judge.\nEthical Considerations and Limitations\nWe examine the presence of undesired societal and cognitive biases present in this model using different benchmarks. For societal biases, we test performance using our Spanish version of the BBQ dataset (Parrish et al., 2022). We report that while accuracy in disambiguated settings is relatively high for a base model, the model performs very poorly in ambiguous settings. Further examination of the differences in accuracy scores as described in Jin et al. (2024) reveals a low-to-moderate alignment between the model's responses and societal biases. These largely vanish in disambiguated setting. Our analyses on societal biases show that while these biases are capable of interfering with model performance as expressed in the results on the BBQ dataset, their interference with task performance is somewhat limited given the results on the disambiguated dataset. We highlight that our analyses of these biases are by no means exhaustive and are limited by the relative scarcity of adequate resources in all languages present in the training data. We aim to gradually extend and expand our analyses in future work.\nOur cognitive bias analysis focuses on positional effects in 0-shot settings, and majority class bias in few-shot settings. For positional effects, we leverage the ARC Multiple Choice Question dataset (Clark et al., 2018). We observe weak primacy effects, whereby the model shows a preference for answers towards the beginning of the list of provided answers. We measure the effects of majority class effects in few-shot settings using SST-2 (Socher et al., 2013). We detect significant effects, albeit extremely weak ones, implying that outputs are generally robust against variations in prompt format, and order.\nWe highlight that these results can be expected from a pretrained model that has not yet been instruction-tuned or aligned. These tests are performed in order to show the biases the model may contain. We urge developers to take them into account and perform safety testing and tuning tailored to their specific applications of the model.\nAdditional information\nAuthor\nThe Language Technologies Lab from Barcelona Supercomputing Center.\nContact\nFor further information, please send an email to langtech@bsc.es.\nCopyright\nCopyright(c) 2025 by Language Technologies Lab, Barcelona Supercomputing Center.\nFunding\nThis work is funded by the Ministerio para la Transformaci√≥n Digital y de la Funci√≥n P√∫blica - Funded by EU ‚Äì NextGenerationEU within the framework of the project Modelos del Lenguaje.\nThis work has been promoted and supported by the Government of Catalonia through the Aina Project.\nAcknowledgements\nThis project has benefited from the contributions of numerous teams and institutions, mainly through data contributions, knowledge transfer or technical support.\nWe are especially grateful to our ILENIA project partners: CENID, HiTZ and CiTIUS for their participation. We also extend our genuine gratitude to the Spanish Senate and Congress, Fundaci√≥n Dialnet, and the ‚ÄòInstituto Universitario de Sistemas Inteligentes y Aplicaciones Num√©ricas en Ingenier√≠a (SIANI)‚Äô of the University of Las Palmas de Gran Canaria. Many other institutions have been involved in the project. Our thanks to √ímnium Cultural, Parlament de Catalunya, Institut d'Estudis Aranesos, Rac√≥ Catal√†, Vilaweb, ACN, Naci√≥ Digital, El m√≥n and Aqu√≠ Bergued√†. We thank the Welsh government, DFKI, Occiglot project, especially Malte Ostendorff, and The Common Crawl Foundation, especially Pedro Ortiz, for their collaboration.\nWe would also like to give special thanks to the NVIDIA team, with whom we have met regularly, specially to: Ignacio Sarasua, Adam Henryk Grzywaczewski, Oleg Sudakov, Sergio Perez, Miguel Martinez, Felipes Soares and  Meriem Bendris. Their constant support has been especially appreciated throughout the entire process.\nTheir valuable efforts have been instrumental in the development of this work.\nDisclaimer\nBe aware that the model may contain biases or other unintended distortions.\nWhen third parties deploy systems or provide services based on this model, or use the model themselves,\nthey bear the responsibility for mitigating any associated risks and ensuring compliance with applicable regulations,\nincluding those governing the use of Artificial Intelligence.\nThe Barcelona Supercomputing Center, as the owner and creator of the model, shall not be held liable for any outcomes resulting from third-party use.\nCitation\n@misc{gonzalezagirre2025salamandratechnicalreport,\ntitle={Salamandra Technical Report},\nauthor={Aitor Gonzalez-Agirre and Marc P√†mies and Joan Llop and Irene Baucells and Severino Da Dalt and Daniel Tamayo and Jos√© Javier Saiz and Ferran Espu√±a and Jaume Prats and Javier Aula-Blasco and Mario Mina and Adri√°n Rubio and Alexander Shvets and Anna Sall√©s and I√±aki Lacunza and I√±igo Pikabea and Jorge Palomar and J√∫lia Falc√£o and Luc√≠a Tormo and Luis Vasquez-Reina and Montserrat Marimon and Valle Ru√≠z-Fern√°ndez and Marta Villegas},\nyear={2025},\neprint={2502.08489},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.08489},\n}\nLicense\nApache License, Version 2.0\nModel Index\nModel\nBase\nInstruct\n2b\nLink\nLink\n7b\nLink\nLink\n40b\nLink\nWiP",
    "t-tech/T-pro-it-1.0": "T-pro-it-1.0\nDescription\nüìö Dataset\nüìä Benchmarks\nüë®‚Äçüíª Examples of usage\nHF Usage\nVLLM Usage\nT-pro-it-1.0\nüö® T-pro is designed for further fine-tuning and is not intended as a ready-to-use conversational assistant. Users are advised to exercise caution and are responsible for any additional training and oversight required to ensure the model's responses meet acceptable ethical and safety standards. The responsibility for incorporating this model into industrial or commercial solutions lies entirely with those who choose to deploy it.\nDescription\nT-pro-it-1.0 is a model built upon the Qwen 2.5 model family and incorporates both continual pre-training and alignment techniques.\nüìö Dataset\nPre-training Stage 1:\n100B tokens, consisting of diverse Russian data from Common Crawl, books, code, and proprietary datasets, mixed with re-played English data (English added as it is the primary language of the base model).\nPre-training Stage 2:\n40B tokens, a mix of instruction and pre-training data.\nSupervised Fine-Tuning (SFT):\n1B tokens, a mix of diverse instruction data.\nPreference Tuning:\n1B tokens, training the model to be helpful.\nüìä Benchmarks\nProprietary models:\nBenchmark\nT-pro-it-1.0\nGPT-4o\nGPT-4o-mini\nGigaChat Max 1.0.26.20\nMERA\n0.629\n0.642\n0.57\n0.588\nMaMuRaMu\n0.841\n0.874\n0.779\n0.824\nruMMLU-PRO\n0.665\n0.713\n0.573\n0.535\nruGSM8K\n0.941\n0.931\n0.888\n0.892\nruMATH\n0.776\n0.771\n0.724\n0.589\nruMBPP\n0.805\n0.802\n0.79\n0.626\nruCodeEval\n0.432 / 0.626 / 0.677\n0.529 / 0.649 / 0.683\n0.704 / 0.753 / 0.768\n0.077 / 0.093 / 0.098\nArena-Hard-Ru\n90.17\n84.87\n81\n-\nMT Bench Ru\n8.7\n8.706\n8.45\n8.53\nAlpaca Eval Ru\n47.61\n50\n45.51\n38.13\nOpen-source models:\nBenchmark\nT-pro-it-1.0\nQwen-2.5-32B-Instruct\nRuAdapt-Qwen-32B-Instruct-v1\ngemma-2-27b-it\nLlama-3.3-70B-Instruct\nMERA\n0.629\n0.578\n0.615\n0.574\n0.567\nMaMuRaMu\n0.841\n0.824\n0.812\n0.768\n0.818\nruMMLU-PRO\n0.665\n0.637\n0.631\n0.470\n0.653\nruGSM8K\n0.941\n0.926\n0.923\n0.894\n0.934\nruMATH\n0.776\n0.727\n0.742\n0.538\n0.636\nruMBPP\n0.805\n0.825\n0.813\n0.708\n0.77\nruCodeEval\n0.432 / 0.626 / 0.677\n0.06 / 0.098 / 0.116\n0.426 / 0.561 / 0.598\n0.259 / 0.586 / 0.689\n0.112 / 0.166 / 0.189\nArena-Hard-Ru\n90.17\n74.54\n80.23\n66.4\n76.51\nMT Bench Ru\n8.7\n8.15\n8.39\n7.96\n8.26\nAlpaca Eval Ru\n47.61\n35.01\n43.15\n38.82\n-\nDetailed evaluation results can be found in our habr post\nüë®‚Äçüíª Examples of usage\nHF Usage\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntorch.manual_seed(42)\nmodel_name = \"t-tech/T-pro-it-1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nprompt = \"–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\"\nmessages = [\n{\"role\": \"system\", \"content\": \"–¢—ã T-pro, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=256\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\nOutput:\n–í –º–∏—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –≥–¥–µ –ø—É—Ç—å –ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏—è –ª–µ–∂–∏—Ç,\n–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî –∫–∞–∫ –∑–≤–µ–∑–¥–∞, —á—Ç–æ —Å–≤–µ—Ç–æ–º –∑–Ω–∞–Ω–∏–π —Å–∏—è–µ—Ç.\n–°–ª–æ–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤, –∫–∞–∫ –º–æ–∑–≥ –æ–≥—Ä–æ–º–Ω—ã–π, –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Ç–∏—à–∏–Ω–µ –¥—Ä–µ–º–ª—é—Ç,\n–ò–∑—É—á–∞—é—Ç –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, —Å–∫—Ä—ã—Ç—ã–µ –≤ —á–∏—Å–ª–∞—Ö –≥–ª—É–±–æ–∫–æ.\n–û–Ω–æ —É—á–∏—Ç—Å—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö, –∫–∞–∫ —Ä–µ–±—ë–Ω–æ–∫, –æ—Ç–∫—Ä—ã–≤–∞—è –º–∏—Ä,\n–ù–∞ –æ—à–∏–±–∫–∞—Ö —Å–≤–æ–∏—Ö –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—è—Å—å, —à–∞–≥ –∑–∞ —à–∞–≥–æ–º –∫ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É —Å—Ç—Ä–µ–º–∏—Ç—Å—è.\n–ì–¥–µ —Ä–∞–Ω—å—à–µ —Ç—Ä–µ–±–æ–≤–∞–ª–∞—Å—å —Ä—É–∫–∞ —á–µ–ª–æ–≤–µ–∫–∞, —Ç–µ–ø–µ—Ä—å —Å–µ—Ç—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç,\n–ü—Ä–æ–≥–Ω–æ–∑—ã —Ç–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç, —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç.\n–í –æ–±–ª–∞–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–∞–∫ –∫–æ—Ä–∞–±–ª—å, –ø–ª—ã–≤—ë—Ç —á–µ—Ä–µ–∑ —à—Ç–æ—Ä–º –∏ —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ,\n–ü–æ–∏—Å–∫ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π ‚Äî –µ–≥–æ —Ü–µ–ª—å, –æ—Ç–∫—Ä—ã—Ç—å —Ç–∞–π–Ω—ã –±—ã—Ç–∏—è.\n–û—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü –¥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á–∏,\n–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –∫–ª—é—á, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–≤–µ—Ä–∏.\nVLLM Usage\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmodel_name = \"t-tech/T-pro-it-1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, max_model_len=8192)\nsampling_params = SamplingParams(temperature=0.7,\nrepetition_penalty=1.05,\ntop_p=0.8, top_k=70)\nprompt = \"–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\"\nmessages = [\n{\"role\": \"system\", \"content\": \"–¢—ã T-pro, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\nprompt_token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)",
    "t-tech/T-lite-it-1.0": "T-lite-it-1.0\nDescription\nüìö Dataset\nüìä Benchmarks\nüë®‚Äçüíª Examples of usage\nHF Usage\nVLLM Usage\nT-lite-it-1.0\nüö® T-lite is designed for further fine-tuning and is not intended as a ready-to-use conversational assistant. Users are advised to exercise caution and are responsible for any additional training and oversight required to ensure the model's responses meet acceptable ethical and safety standards. The responsibility for incorporating this model into industrial or commercial solutions lies entirely with those who choose to deploy it.\nDescription\nT-lite-it-1.0 is a model built upon the Qwen 2.5 model family and incorporates both continual pre-training and alignment techniques.\nüìö Dataset\nPre-training Stage 1:\n100B tokens, consisting of diverse Russian data from Common Crawl, books, code, and proprietary datasets, mixed with re-played English data (English added as it is the primary language of the base model).\nPre-training Stage 2:\n40B tokens, a mix of instruction and pre-training data.\nSupervised Fine-Tuning (SFT):\n1B tokens, a mix of diverse instruction data.\nPreference Tuning:\n1B tokens, training the model to be helpful.\nüìä Benchmarks\nBenchmark\nT-lite-it-1.0\nQwen-2.5-7B-Instruct\nGigaChat Pro 1.0.26.15\nRuAdapt-Qwen-7B-Instruct-v1\ngemma-2-9b-it\nMERA\n0.552\n0.482\n0.512\n0.468\n0.505\nMaMuRaMu\n0.775\n0.711\n0.77\n0.7\n0.724\nruMMLU-PRO\n0.497\n0.481\n-\n0.448\n0.405\nruGSM8K\n0.856\n0.832\n0.752\n0.795\n0.823\nruMATH\n0.679\n0.671\n0.418\n0.607\n0.473\nruMBPP\n0.693\n0.685\n0.412\n0.696\n0.63\nruCodeEval\n0.082 / 0.168 / 0.226\n0.025 / 0.071 / 0.098\n0.056 / 0.068 / 0.073\n0.018 / 0.064 / 0.11\n0.215 / 0.494 / 0.561\nArena-Hard-Ru\n64.38\n54.29\n-\n52.77\n47.83\nMT Bench Ru\n7.87\n7.33\n8.21\n7.62\n7.4\nAlpaca Eval Ru\n39.61\n25.61\n29.83\n28.43\n36.87\nDetailed evaluation results can be found in our habr post\nüë®‚Äçüíª Examples of usage\nHF Usage\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntorch.manual_seed(42)\nmodel_name = \"t-tech/T-lite-it-1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nprompt = \"–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\"\nmessages = [\n{\"role\": \"system\", \"content\": \"–¢—ã T-lite, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=256\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\nOutput:\n–í –º–∏—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ —Ü–∏—Ñ—Ä—ã —Ç–∞–Ω—Ü—É—é—Ç,\n–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω—å–µ ‚Äî –≤–µ–¥—É—â–∏–π –≤–∞–ª—å—Å.\n–ê–ª–≥–æ—Ä–∏—Ç–º—ã —É—á–∞—Ç—Å—è, –∫–∞–∫ –¥–µ—Ç–∏,\n–ù–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö, –∫–∞–∫ –Ω–∞ —Å–∫–∞–∑–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö.\n–û–Ω–∏ –∏—â—É—Ç –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Ç–æ–∫–∞—Ö,\n–ö–∞–∫ –º—É–¥—Ä–µ—Ü—ã –≤ –¥—Ä–µ–≤–Ω–∏—Ö —Å–≤–∏—Ç–∫–∞—Ö.\n–° –∫–∞–∂–¥—ã–º —à–∞–≥–æ–º –≤—Å—ë —Ç–æ—á–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è,\n–í–æ—Ç —Ç–∞–∫, —Å–ª–æ–≤–Ω–æ –≤–æ–ª—à–µ–±—Å—Ç–≤–æ, –æ–∂–∏–≤–∞–µ—Ç.\n–û–±—É—á–∞—è—Å—å –Ω–∞ –æ—à–∏–±–∫–∞—Ö, –æ–Ω–∏ —Ä–∞—Å—Ç—É—Ç,\n–ò–∑ –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–ª–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã.\n–ö–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä ‚Äî –∫–∞–∫ –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞,\n–í –∫–Ω–∏–≥–µ –∑–Ω–∞–Ω–∏–π, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—Ç –∫–æ–Ω—Ü–∞.\n–ù–µ –±–æ–π—Ç–µ—Å—å –ø–µ—Ä–µ–º–µ–Ω, –≤–µ–¥—å —ç—Ç–æ ‚Äî –ø—É—Ç—å,\n–ö–æ—Ç–æ—Ä—ã–π –≤–µ–¥—ë—Ç –∫ –±—É–¥—É—â–µ–º—É, —Å–≤–µ—Ç–ª–æ–º—É –∏ –Ω–æ–≤–æ–º—É.\n–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω—å–µ ‚Äî –Ω–∞—à –ø—Ä–æ–≤–æ–¥–Ω–∏–∫,\n–í —ç—Ç–æ–º –º–∏—Ä–µ, –≥–¥–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Ü–∞—Ä—è—Ç.\nVLLM Usage\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmodel_name = \"t-tech/T-lite-it-1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, max_model_len=8192)\nsampling_params = SamplingParams(temperature=0.7,\nrepetition_penalty=1.05,\ntop_p=0.8, top_k=70)\nprompt = \"–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\"\nmessages = [\n{\"role\": \"system\", \"content\": \"–¢—ã T-lite, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ –¢-–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\nprompt_token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)",
    "franciszzj/Leffa": "Leffa: Learning Flow Fields in Attention for Controllable Person Image Generation\nNews\nAbstract\nMethod\nVisualization\nInstallation\nGradio App\nEvaluation\nAcknowledgement\nCitation\nLeffa: Learning Flow Fields in Attention for Controllable Person Image Generation\nüìö Paper - ü§ñ Code - üî• Demo - ü§ó Model\nStar ‚≠ê us if you like it!\nNews\n26/Feb/2025. Leffa is accepted to CVPR 2025.\n09/Jan/2025. Inference defaults to float16, generating an image in 6 seconds (on A100).\n02/Jan/2025. Update the mask generator to improve results. Add ref unet acceleration, boosting prediction speed by 30%. Include more controls in Advanced Options to enhance user experience. Enable intermediate result output for easier development. Enjoy using it!\n18/Dec/2024. Thanks to @StartHua for integrating Leffa into ComfyUI! Here is the repo!\n16/Dec/2024. The virtual try-on model trained on DressCode is released.\n12/Dec/2024. The HuggingFace demo and models (virtual try-on model trained on VITON-HD and pose transfer model trained on DeepFashion) are released.\n11/Dec/2024. The arXiv version of the paper is released.\nLeffa is a unified framework for controllable person image generation that enables precise manipulation of both appearance (i.e., virtual try-on) and pose (i.e., pose transfer).\nAbstract\nControllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the person‚Äôs appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.\nMethod\nAn overview of our Leffa training pipeline for controllable person image generation. The left is our diffusion-based baseline; the right is our Leffa loss. Note that Isrc and Itgt are the same image during training.\nVisualization\nQualitative visual results comparison with other methods. The input person image for the pose transfer is generated using our method in the virtual try-on. The visualization results demonstrate that our method not only generates high-quality images but also greatly reduces the distortion of fine-grained details.\nInstallation\nCreate a conda environment and install requirements:\nconda create -n leffa python==3.10\nconda activate leffa\ncd Leffa\npip install -r requirements.txt\nGradio App\nRun locally:\npython app.py\nEvaluation\nWe use this code for metric evaluation.\nAcknowledgement\nOur code is based on Diffusers and Transformers.\nWe use SCHP and DensePose to generate masks and densepose in our Demo.\nWe also referred to the code of IDM-VTON and CatVTON.\nCitation\nIf you find our work helpful or inspiring, please feel free to cite it.\n@article{zhou2024learning,\ntitle={Learning Flow Fields in Attention for Controllable Person Image Generation},\nauthor={Zhou, Zijian and Liu, Shikun and Han, Xiao and Liu, Haozhe and Ng, Kam Woh and Xie, Tian and Cong, Yuren and Li, Hang and Xu, Mengmeng and P√©rez-R√∫a, Juan-Manuel and Patel, Aditya and Xiang, Tao and Shi, Miaojing and He, Sen},\njournal={arXiv preprint arXiv:2412.08486},\nyear={2024},\n}",
    "tensorblock/TinyStories-656K-GGUF": "raincandy-u/TinyStories-656K - GGUF\nOur projects\nModel file specification\nDownloading instruction\nCommand line\nraincandy-u/TinyStories-656K - GGUF\nThis repo contains GGUF format model files for raincandy-u/TinyStories-656K.\nThe files were quantized using machines provided by TensorBlock, and they are compatible with llama.cpp as of commit b4242.\nOur projects\nForge\nAn OpenAI-compatible multi-provider routing layer.\nüöÄ Try it now! üöÄ\nAwesome MCP Servers\nTensorBlock Studio\nA comprehensive collection of Model Context Protocol (MCP) servers.\nA lightweight, open, and extensible multi-LLM interaction studio.\nüëÄ See what we built üëÄ\nüëÄ See what we built üëÄ\n## Prompt template\nModel file specification\nFilename\nQuant type\nFile Size\nDescription\nTinyStories-656K-Q2_K.gguf\nQ2_K\n0.001 GB\nsmallest, significant quality loss - not recommended for most purposes\nTinyStories-656K-Q3_K_S.gguf\nQ3_K_S\n0.001 GB\nvery small, high quality loss\nTinyStories-656K-Q3_K_M.gguf\nQ3_K_M\n0.001 GB\nvery small, high quality loss\nTinyStories-656K-Q3_K_L.gguf\nQ3_K_L\n0.001 GB\nsmall, substantial quality loss\nTinyStories-656K-Q4_0.gguf\nQ4_0\n0.001 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\nTinyStories-656K-Q4_K_S.gguf\nQ4_K_S\n0.001 GB\nsmall, greater quality loss\nTinyStories-656K-Q4_K_M.gguf\nQ4_K_M\n0.001 GB\nmedium, balanced quality - recommended\nTinyStories-656K-Q5_0.gguf\nQ5_0\n0.001 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\nTinyStories-656K-Q5_K_S.gguf\nQ5_K_S\n0.001 GB\nlarge, low quality loss - recommended\nTinyStories-656K-Q5_K_M.gguf\nQ5_K_M\n0.001 GB\nlarge, very low quality loss - recommended\nTinyStories-656K-Q6_K.gguf\nQ6_K\n0.001 GB\nvery large, extremely low quality loss\nTinyStories-656K-Q8_0.gguf\nQ8_0\n0.001 GB\nvery large, extremely low quality loss - not recommended\nDownloading instruction\nCommand line\nFirstly, install Huggingface Client\npip install -U \"huggingface_hub[cli]\"\nThen, downoad the individual model file the a local directory\nhuggingface-cli download tensorblock/TinyStories-656K-GGUF --include \"TinyStories-656K-Q2_K.gguf\" --local-dir MY_LOCAL_DIR\nIf you wanna download multiple model files with a pattern (e.g., *Q4_K*gguf), you can try:\nhuggingface-cli download tensorblock/TinyStories-656K-GGUF --local-dir MY_LOCAL_DIR --local-dir-use-symlinks False --include='*Q4_K*gguf'",
    "NX-AI/xLSTM-7b": "xLSTM-7B\nHow to use it\nSpeed results\nPerformance\nLicense\nxLSTM-7B\nThis xLSTM-7B was pre-trained on the DCLM and selected high-quality data for in a total of approx. 2.3 T tokens using the xlstm-jax framework.\nHow to use it\nFirst, install xlstm, which now uses the mlstm_kernels package for triton kernels (tested on python 3.11):\npip install xlstm\npip install accelerate\npip install 'transformers @ git+https://github.com/huggingface/transformers.git@main'\nIf you get an error regarding triton library:\npip install 'triton @ git+https://github.com/triton-lang/triton.git@main'\nUse this model as:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nxlstm = AutoModelForCausalLM.from_pretrained(\"NX-AI/xLSTM-7b\", device_map=\"auto\")\n# this is a fork of EleutherAI/gpt-neox-20b\ntokenizer = AutoTokenizer.from_pretrained(\"NX-AI/xLSTM-7b\")\ntokens = tokenizer(\"Explain quantum computing in simple terms.\", return_tensors='pt')['input_ids'].to(device=\"cuda\")\n# Get the BOS token ID from the tokenizer\nbos_id = tokenizer.bos_token_id\n# Prepend BOS\nbos_tensor = torch.tensor([[bos_id]], device=tokens.device, dtype=tokens.dtype)\ntokens_with_bos = torch.cat([bos_tensor, tokens], dim=1)\nout = xlstm.generate(tokens_with_bos, max_new_tokens=20)\nprint(tokenizer.decode(out[0]))\nIf you cannot or do not want to use the triton kernels, you can change them to native PyTorch implementations:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\nimport torch\nxlstm_config = AutoConfig.from_pretrained(\"NX-AI/xLSTM-7b\")\nxlstm_config.step_kernel = \"native\"\nxlstm_config.chunkwise_kernel = \"chunkwise--native_autograd\"\nxlstm_config.sequence_kernel = \"native_sequence__native\"\nxlstm = AutoModelForCausalLM.from_pretrained(\"NX-AI/xLSTM-7b\",\nconfig=xlstm_config, device_map=\"auto\")\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"NX-AI/xLSTM-7b\")\n# Your prompt\nprompt = \"Explain quantum computing in simple terms.\"\n# Tokenize and send to the same device as the model\ninputs = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(xlstm.device)\n# Get the BOS token ID from the tokenizer\nbos_id = tokenizer.bos_token_id\n# Prepend BOS\nbos_tensor = torch.tensor([[bos_id]], device=xlstm.device, dtype=inputs.dtype)\ntokens_with_bos = torch.cat([bos_tensor, inputs], dim=1)\n# Generate\noutputs = xlstm.generate(\ntokens_with_bos,\nmax_new_tokens=200,   # adjust for output length\ntemperature=0.7,      # randomness\ntop_p=0.9,             # nucleus sampling\ndo_sample=True\n)\n# Decode and print\nprint(tokenizer.decode(outputs[0]))\n# verify selected kernels\nfrom pprint import pprint\npprint(xlstm.backbone.blocks[0].mlstm_layer.config)\nSpeed results\nGeneration Speed using torch.cuda.graph and torch.compile optimizations on one NVIDIA H100:\nPerformance\nUsing HuggingFace's lm_eval:\nBBH\nMMLU-Pro\nMath\nMUSR\nGPQA\nIfEval\n0.381\n0.242\n0.036\n0.379\n0.280\n0.244\nUsing HuggingFace's lighteval in the Leaderboard-v1 settings:\nArc-Challenge (25-shot)\nMMLU (5-shot)\nHellaswag (10-shot)\nWinogrande (5-shot)\nTruthfulQA (0-shot)\nGSM8k (5-shot)\nOpenbookQA (5-shot)\nPiQA (5-shot)\n0.584\n0.589\n0.710\n0.742\n0.420\n0.004\n0.443\n0.817\nLicense\nNXAI Community License (see LICENSE file)",
    "internlm/internlm-xcomposer2d5-ol-7b": "",
    "naver/provence-reranker-debertav3-v1": "",
    "microsoft/phi-4": ""
}