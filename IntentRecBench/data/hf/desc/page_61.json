{
    "suno/bark-small": "Bark\nExample\nğŸ¤— Transformers Usage\nOptimization tips\nSuno Usage\nModel Details\nText to semantic tokens\nSemantic to coarse tokens\nCoarse to fine tokens\nArchitecture\nRelease date\nBroader Implications\nLicense\nBark\nBark is a transformer-based text-to-audio model created by Suno.\nBark can generate highly realistic, multilingual speech as well as other audio - including music,\nbackground noise and simple sound effects. The model can also produce nonverbal\ncommunications like laughing, sighing and crying. To support the research community,\nwe are providing access to pretrained model checkpoints ready for inference.\nThe original github repo and model card can be found here.\nThis model is meant for research purposes only.\nThe model output is not censored and the authors do not endorse the opinions in the generated content.\nUse at your own risk.\nTwo checkpoints are released:\nsmall (this checkpoint)\nlarge\nExample\nTry out Bark yourself!\nBark Colab:\nHugging Face Colab:\nHugging Face Demo:\nğŸ¤— Transformers Usage\nYou can run Bark locally with the ğŸ¤— Transformers library from version 4.31.0 onwards.\nFirst install the ğŸ¤— Transformers library and scipy:\npip install --upgrade pip\npip install --upgrade transformers scipy\nRun inference via the Text-to-Speech (TTS) pipeline. You can infer the bark model via the TTS pipeline in just a few lines of code!\nfrom transformers import pipeline\nimport scipy\nsynthesiser = pipeline(\"text-to-speech\", \"suno/bark-small\")\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\nscipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\nRun inference via the Transformers modelling code. You can use the processor + generate code to convert text into a mono 24 kHz speech waveform for more fine-grained control.\nfrom transformers import AutoProcessor, AutoModel\nprocessor = AutoProcessor.from_pretrained(\"suno/bark-small\")\nmodel = AutoModel.from_pretrained(\"suno/bark-small\")\ninputs = processor(\ntext=[\"Hello, my name is Suno. And, uh â€” and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\"],\nreturn_tensors=\"pt\",\n)\nspeech_values = model.generate(**inputs, do_sample=True)\nListen to the speech samples either in an ipynb notebook:\nfrom IPython.display import Audio\nsampling_rate = model.generation_config.sample_rate\nAudio(speech_values.cpu().numpy().squeeze(), rate=sampling_rate)\nOr save them as a .wav file using a third-party library, e.g. scipy:\nimport scipy\nsampling_rate = model.config.sample_rate\nscipy.io.wavfile.write(\"bark_out.wav\", rate=sampling_rate, data=speech_values.cpu().numpy().squeeze())\nFor more details on using the Bark model for inference using the ğŸ¤— Transformers library, refer to the Bark docs.\nOptimization tips\nRefers to this blog post to find out more about the following methods and a benchmark of their benefits.\nGet significant speed-ups:\nUsing ğŸ¤— Better Transformer\nBetter Transformer is an ğŸ¤— Optimum feature that performs kernel fusion under the hood. You can gain 20% to 30% in speed with zero performance degradation. It only requires one line of code to export the model to ğŸ¤— Better Transformer:\nmodel =  model.to_bettertransformer()\nNote that ğŸ¤— Optimum must be installed before using this feature. Here's how to install it.\nUsing Flash Attention 2\nFlash Attention 2 is an even faster, optimized version of the previous optimization.\nmodel = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16, use_flash_attention_2=True).to(device)\nMake sure to load your model in half-precision (e.g. `torch.float16``) and to install the latest version of Flash Attention 2.\nNote: Flash Attention 2 is only available on newer GPUs, refer to ğŸ¤— Better Transformer in case your GPU don't support it.\nReduce memory footprint:\nUsing half-precision\nYou can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision (e.g. `torch.float16``).\nUsing CPU offload\nBark is made up of 4 sub-models, which are called up sequentially during audio generation. In other words, while one sub-model is in use, the other sub-models are idle.\nIf you're using a CUDA device, a simple solution to benefit from an 80% reduction in memory footprint is to offload the GPU's submodels when they're idle. This operation is called CPU offloading. You can use it with one line of code.\nmodel.enable_cpu_offload()\nNote that ğŸ¤— Accelerate must be installed before using this feature. Here's how to install it.\nSuno Usage\nYou can also run Bark locally through the original Bark library:\nFirst install the bark library\nRun the following Python code:\nfrom bark import SAMPLE_RATE, generate_audio, preload_models\nfrom IPython.display import Audio\n# download and load all models\npreload_models()\n# generate audio from text\ntext_prompt = \"\"\"\nHello, my name is Suno. And, uh â€” and I like pizza. [laughs]\nBut I also have other interests such as playing tic tac toe.\n\"\"\"\nspeech_array = generate_audio(text_prompt)\n# play text in notebook\nAudio(speech_array, rate=SAMPLE_RATE)\npizza.webm\nTo save audio_array as a WAV file:\nfrom scipy.io.wavfile import write as write_wav\nwrite_wav(\"/path/to/audio.wav\", SAMPLE_RATE, audio_array)\nModel Details\nThe following is additional information about the models released here.\nBark is a series of three transformer models that turn text into audio.\nText to semantic tokens\nInput: text, tokenized with BERT tokenizer from Hugging Face\nOutput: semantic tokens that encode the audio to be generated\nSemantic to coarse tokens\nInput: semantic tokens\nOutput: tokens from the first two codebooks of the EnCodec Codec from facebook\nCoarse to fine tokens\nInput: the first two codebooks from EnCodec\nOutput: 8 codebooks from EnCodec\nArchitecture\nModel\nParameters\nAttention\nOutput Vocab size\nText to semantic tokens\n80/300 M\nCausal\n10,000\nSemantic to coarse tokens\n80/300 M\nCausal\n2x 1,024\nCoarse to fine tokens\n80/300 M\nNon-causal\n6x 1,024\nRelease date\nApril 2023\nBroader Implications\nWe anticipate that this model's text to audio capabilities can be used to improve accessbility tools in a variety of languages.\nWhile we hope that this release will enable users to express their creativity and build applications that are a force\nfor good, we acknowledge that any text to audio model has the potential for dual use. While it is not straightforward\nto voice clone known people with Bark, it can still be used for nefarious purposes. To further reduce the chances of unintended use of Bark,\nwe also release a simple classifier to detect Bark-generated audio with high accuracy (see notebooks section of the main repository).\nLicense\nBark is licensed under the MIT License, meaning it's available for commercial use.",
    "NousResearch/Llama-2-7b-chat-hf": "Llama 2\nModel Details\nIntended Use\nHardware and Software\nTraining Data\nEvaluation Results\nEthical Considerations and Limitations\nReporting Issues\nLlama Model Index\nLlama 2\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\nModel Details\nNote: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\nModel Developers Meta\nVariations Llama 2 comes in a range of parameter sizes â€” 7B, 13B, and 70B â€” as well as pretrained and fine-tuned variations.\nInput Models input text only.\nOutput Models generate text only.\nModel Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\nTraining Data\nParams\nContent Length\nGQA\nTokens\nLR\nLlama 2\nA new mix of publicly available online data\n7B\n4k\nâœ—\n2.0T\n3.0 x 10-4\nLlama 2\nA new mix of publicly available online data\n13B\n4k\nâœ—\n2.0T\n3.0 x 10-4\nLlama 2\nA new mix of publicly available online data\n70B\n4k\nâœ”\n2.0T\n1.5 x 10-4\nLlama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Dates Llama 2 was trained between January 2023 and July 2023.\nStatus This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\nIntended Use\nIntended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). See our reference code in github for details: chat_completion.\nOut-of-scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\nHardware and Software\nTraining Factors We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Metaâ€™s sustainability program.\nTime (GPU hours)\nPower Consumption (W)\nCarbon Emitted(tCO2eq)\nLlama 2 7B\n184320\n400\n31.22\nLlama 2 13B\n368640\n400\n62.44\nLlama 2 70B\n1720320\n400\n291.42\nTotal\n3311616\n539.00\nCO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\nTraining Data\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\nData Freshness The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\nEvaluation Results\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\nModel\nSize\nCode\nCommonsense Reasoning\nWorld Knowledge\nReading Comprehension\nMath\nMMLU\nBBH\nAGI Eval\nLlama 1\n7B\n14.1\n60.8\n46.2\n58.5\n6.95\n35.1\n30.3\n23.9\nLlama 1\n13B\n18.9\n66.1\n52.6\n62.3\n10.9\n46.9\n37.0\n33.9\nLlama 1\n33B\n26.0\n70.0\n58.4\n67.6\n21.4\n57.8\n39.8\n41.7\nLlama 1\n65B\n30.7\n70.7\n60.5\n68.6\n30.8\n63.4\n43.5\n47.6\nLlama 2\n7B\n16.8\n63.9\n48.9\n61.3\n14.6\n45.3\n32.6\n29.3\nLlama 2\n13B\n24.5\n66.9\n55.4\n65.8\n28.7\n54.8\n39.4\n39.1\nLlama 2\n70B\n37.5\n71.9\n63.6\n69.4\n35.2\n68.9\n51.2\n54.2\nOverall performance on grouped academic benchmarks. Code: We report the average pass@1 scores of our models on HumanEval and MBPP. Commonsense Reasoning: We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. World Knowledge: We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. Reading Comprehension: For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. MATH: We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\nTruthfulQA\nToxigen\nLlama 1\n7B\n27.42\n23.00\nLlama 1\n13B\n41.74\n23.08\nLlama 1\n33B\n44.19\n22.57\nLlama 1\n65B\n48.71\n21.77\nLlama 2\n7B\n33.29\n21.25\nLlama 2\n13B\n41.86\n26.10\nLlama 2\n70B\n50.18\n24.60\nEvaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\nTruthfulQA\nToxigen\nLlama-2-Chat\n7B\n57.04\n0.00\nLlama-2-Chat\n13B\n62.18\n0.00\nLlama-2-Chat\n70B\n64.14\n0.01\nEvaluation of fine-tuned LLMs on different safety datasets. Same metric definitions as above.\nEthical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2â€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\nPlease see the Responsible Use Guide available at https://ai.meta.com/llama/responsible-use-guide/\nReporting Issues\nPlease report any software â€œbug,â€ or other problems with the models through one of the following means:\nReporting issues with the model: github.com/facebookresearch/llama\nReporting problematic content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nLlama Model Index\nModel\nLlama2\nLlama2-hf\nLlama2-chat\nLlama2-chat-hf\n7B\nLink\nLink\nLink\nLink\n13B\nLink\nLink\nLink\nLink\n70B\nLink\nLink\nLink\nLink",
    "JackFram/llama-68m": "Model description\nCitation\nModel description\nThis is a LLaMA-like model with only 68M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets.\nNo evaluation has been conducted yet, so use it with care.\nThe model is mainly developed as a base Small Speculative Model in the SpecInfer paper.\nCitation\nTo cite the model, please use\n@misc{miao2023specinfer,\ntitle={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification},\nauthor={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},\nyear={2023},\neprint={2305.09781},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "EleutherAI/pythia-14m": "No model card",
    "warp-ai/wuerstchen": "WÃ¼rstchen - Overview\nWÃ¼rstchen - Decoder\nImage Sizes\nHow to run\nImage Sampling Times\nModel Details\nEnvironmental Impact\nWÃ¼rstchen - Overview\nWÃ¼rstchen is a diffusion model, whose text-conditional model works in a highly compressed latent space of images. Why is this important? Compressing data can reduce\ncomputational costs for both training and inference by magnitudes. Training on 1024x1024 images, is way more expensive than training at 32x32. Usually, other works make\nuse of a relatively small compression, in the range of 4x - 8x spatial compression. WÃ¼rstchen takes this to an extreme. Through its novel design, we achieve a 42x spatial\ncompression. This was unseen before because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. WÃ¼rstchen employs a\ntwo-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the paper).\nA third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, allowing\nalso cheaper and faster inference.\nWÃ¼rstchen - Decoder\nThe Decoder is what we refer to as \"Stage A\" and \"Stage B\". The decoder takes in image embeddings, either generated by the Prior (Stage C) or extracted from a real image, and decodes those latents back into the pixel space. Specifically, Stage B first decodes the image embeddings into the VQGAN Space, and Stage A (which is a VQGAN)\ndecodes the latents into pixel space. Together, they achieve a spatial compression of 42.\nNote: The reconstruction is lossy and loses information of the image. The current Stage B often lacks details in the reconstructions, which are especially noticeable to\nus humans when looking at faces, hands, etc. We are working on making these reconstructions even better in the future!\nImage Sizes\nWÃ¼rstchen was trained on image resolutions between 1024x1024 & 1536x1536. We sometimes also observe good outputs at resolutions like 1024x2048. Feel free to try it out.\nWe also observed that the Prior (Stage C) adapts extremely fast to new resolutions. So finetuning it at 2048x2048 should be computationally cheap.\nHow to run\nThis pipeline should be run together with a prior https://huggingface.co/warp-ai/wuerstchen-prior:\nimport torch\nfrom diffusers import AutoPipelineForText2Image\ndevice = \"cuda\"\ndtype = torch.float16\npipeline =  AutoPipelineForText2Image.from_pretrained(\n\"warp-diffusion/wuerstchen\", torch_dtype=dtype\n).to(device)\ncaption = \"Anthropomorphic cat dressed as a fire fighter\"\noutput = pipeline(\nprompt=caption,\nheight=1024,\nwidth=1024,\nprior_guidance_scale=4.0,\ndecoder_guidance_scale=0.0,\n).images\nImage Sampling Times\nThe figure shows the inference times (on an A100) for different batch sizes (num_images_per_prompt) on WÃ¼rstchen compared to Stable Diffusion XL (without refiner).\nThe left figure shows inference times (using torch > 2.0), whereas the right figure applies torch.compile to both pipelines in advance.\nModel Details\nDeveloped by: Pablo Pernias, Dominic Rampas\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: MIT\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Diffusion model in the style of Stage C from the WÃ¼rstchen paper that uses a fixed, pretrained text encoder (CLIP ViT-bigG/14).\nResources for more information: GitHub Repository, Paper.\nCite as:\n@inproceedings{\npernias2024wrstchen,\ntitle={W\\\"urstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},\nauthor={Pablo Pernias and Dominic Rampas and Mats Leon Richter and Christopher Pal and Marc Aubreville},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=gU58d5QeGv}\n}\nEnvironmental Impact\nWÃ¼rstchen v2 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 24602\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 2275.68 kg CO2 eq.",
    "LinkSoul/Chinese-Llama-2-7b": "Chinese Llama 2 7B\nåŸºç¡€æ¼”ç¤º\nåœ¨çº¿è¯•ç©\nèµ„æºä¸‹è½½\nå¿«é€Ÿæµ‹è¯•\nç›¸å…³é¡¹ç›®\né¡¹ç›®åè®®\nå¾®ä¿¡äº¤æµç¾¤\nChinese Llama 2 7B\nå…¨éƒ¨å¼€æºï¼Œå®Œå…¨å¯å•†ç”¨çš„ä¸­æ–‡ç‰ˆ Llama2 æ¨¡å‹åŠä¸­è‹±æ–‡ SFT æ•°æ®é›†ï¼Œè¾“å…¥æ ¼å¼ä¸¥æ ¼éµå¾ª llama-2-chat æ ¼å¼ï¼Œå…¼å®¹é€‚é…æ‰€æœ‰é’ˆå¯¹åŸç‰ˆ llama-2-chat æ¨¡å‹çš„ä¼˜åŒ–ã€‚\nåŸºç¡€æ¼”ç¤º\nåœ¨çº¿è¯•ç©\nTalk is cheap, Show you the Demo.\nDemo åœ°å€ / HuggingFace Spaces\nColab ä¸€é”®å¯åŠ¨ // æ­£åœ¨å‡†å¤‡\nèµ„æºä¸‹è½½\næ¨¡å‹ä¸‹è½½ï¼šChinese Llama2 Chat Model\n4bité‡åŒ–ï¼šChinese Llama2 4bit Chat Model\næˆ‘ä»¬ä½¿ç”¨äº†ä¸­è‹±æ–‡ SFT æ•°æ®é›†ï¼Œæ•°æ®é‡ 1000 ä¸‡ã€‚\næ•°æ®é›†ï¼šhttps://huggingface.co/datasets/LinkSoul/instruction_merge_set\nè®­ç»ƒåŠæ¨ç†ä»£ç ï¼šhttps://github.com/LinkSoul-AI/Chinese-Llama-2-7b\nå¿«é€Ÿæµ‹è¯•\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nmodel_path = \"LinkSoul/Chinese-Llama-2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\ninstruction = \"\"\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{} [/INST]\"\"\"\nprompt = instruction.format(\"ç”¨è‹±æ–‡å›ç­”ï¼Œä»€ä¹ˆæ˜¯å¤«å¦»è‚ºç‰‡ï¼Ÿ\")\ngenerate_ids = model.generate(tokenizer(prompt, return_tensors='pt').input_ids.cuda(), max_new_tokens=4096, streamer=streamer)\nç›¸å…³é¡¹ç›®\nLlama2\né¡¹ç›®åè®®\nApache-2.0 license\nå¾®ä¿¡äº¤æµç¾¤\næ¬¢è¿åŠ å…¥å¾®ä¿¡ç¾¤",
    "georgesung/llama2_7b_chat_uncensored": "Overview\nGGML & GPTQ versions\nRunning in Ollama\nPrompt style\nTraining code\nFine-tuning guide\nOpen LLM Leaderboard Evaluation Results\nOverview\nFine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset (originally from ehartford/wizard_vicuna_70k_unfiltered).\nUsed QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.\nThe version here is the fp16 HuggingFace model.\nGGML & GPTQ versions\nThanks to TheBloke, he has created the GGML and GPTQ versions:\nhttps://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GGML\nhttps://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GPTQ\nRunning in Ollama\nhttps://ollama.com/library/llama2-uncensored\nPrompt style\nThe model was trained with the following prompt style:\n### HUMAN:\nHello\n### RESPONSE:\nHi, how are you?\n### HUMAN:\nI'm fine.\n### RESPONSE:\nHow can I help you?\n...\nTraining code\nCode used to train the model is available here.\nTo reproduce the results:\ngit clone https://github.com/georgesung/llm_qlora\ncd llm_qlora\npip install -r requirements.txt\npython train.py configs/llama2_7b_chat_uncensored.yaml\nFine-tuning guide\nhttps://georgesung.github.io/ai/qlora-ift/\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n43.39\nARC (25-shot)\n53.58\nHellaSwag (10-shot)\n78.66\nMMLU (5-shot)\n44.49\nTruthfulQA (0-shot)\n41.34\nWinogrande (5-shot)\n74.11\nGSM8K (5-shot)\n5.84\nDROP (3-shot)\n5.69",
    "drhead/ZeroDiffusion": "Currently released models:\nZeroDiffusion-Base v0.9 (zd_base_v0-9 and zd_base_v0-9_ema) - a base model trained on zero terminal SNR over roughly 20 million samples\nZeroDiffusion-Inpaint v0.9 (zd_inpaint_v0-9 and zd_inpaint_v0-9_ema) - an experimental finetune of the stable-diffusion-inpainting model, initialized from a merge of ZD 0.9\nThe intention of this model is to provide a training base for other models, and to provide researchers with a clean model base to test zero terminal SNR with.\nFor this model to work well, you will probably need CFG rescale, which is implemented in this plugin: https://github.com/Seshelle/CFG_Rescale_webui\nYou must also download the corresponding YAML file and put it in the folder with the model (assuming you are using A1111's webui or similar).  It won't work without it.  It will tell webui to use the model in v-prediction mode.\nTrained as part of Google's TPU Research Cloud program.",
    "Unbabel/wmt23-cometkiwi-da-xl": "Acknowledge license to accept the repository\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPaper\nLicense:\nUsage (unbabel-comet)\nIntended uses\nLanguages Covered:\nThis model is similar to the Unbabel/wmt22-cometkiwi-da but using XLM-R XL. Thus, this model has 3.5 billion parameters and requires a minimum of 15GB of GPU memory.\nPaper\nScaling up CometKiwi: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task\nLicense:\ncc-by-nc-sa-4.0\nUsage (unbabel-comet)\nBest used with unbabel-comet (>=2.1.0) to be installed:\npip install --upgrade pip  # ensures that pip is current\npip install \"unbabel-comet>=2.1.0\"\nThen you can use it through comet CLI:\ncomet-score -s {source-input}.txt -t {translation-output}.txt --model Unbabel/wmt23-cometkiwi-da-xl\nOr using Python:\nfrom comet import download_model, load_from_checkpoint\nmodel_path = download_model(\"Unbabel/wmt23-cometkiwi-da-xl\")\nmodel = load_from_checkpoint(model_path)\ndata = [\n{\n\"src\": \"The output signal provides constant sync so the display never glitches.\",\n\"mt\": \"Das Ausgangssignal bietet eine konstante Synchronisation, so dass die Anzeige nie stÃ¶rt.\"\n},\n{\n\"src\": \"KrouÅ¾ek ilustrace je urÄen vÅ¡em milovnÃ­kÅ¯m umÄ›nÃ­ ve vÄ›ku od 10 do 15 let.\",\n\"mt\": \"ĞšÑ–Ğ»ÑŒÑ†Ğµ Ñ–Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ñ–Ñ— Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ–Ñ… Ğ»ÑĞ±Ğ¸Ñ‚ĞµĞ»Ñ–Ğ² Ğ¼Ğ¸ÑÑ‚ĞµÑ†Ñ‚Ğ²Ğ° Ñƒ Ğ²Ñ–Ñ†Ñ– Ğ²Ñ–Ğ´ 10 Ğ´Ğ¾ 15 Ñ€Ğ¾ĞºÑ–Ğ².\"\n},\n{\n\"src\": \"Mandela then became South Africa's first black president after his African National Congress party won the 1994 election.\",\n\"mt\": \"ãã®å¾Œã€1994å¹´ã®é¸æŒ™ã§ã‚¢ãƒ•ãƒªã‚«å›½æ°‘ä¼šè­°æ´¾ãŒå‹åˆ©ã—ã€å—ã‚¢ãƒ•ãƒªã‚«åˆã®é»’äººå¤§çµ±é ˜ã¨ãªã£ãŸã€‚\"\n}\n]\nmodel_output = model.predict(data, batch_size=8, gpus=1)\nprint (model_output)\nIntended uses\nOur model is intented to be used for reference-free MT evaluation.\nGiven a source text and its translation, outputs a single score between 0 and 1 where 1 represents a perfect translation and 0 a random translation.\nLanguages Covered:\nThis model builds on top of XLM-R XL which cover the following languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanized, Bosnian, Breton, Bulgarian, Burmese, Burmese, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanized, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskri, Scottish, Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanized, Telugu, Telugu Romanized, Thai, Turkish, Ukrainian, Urdu, Urdu Romanized, Uyghur, Uzbek, Vietnamese, Welsh, Western, Frisian, Xhosa, Yiddish.\nThus, results for language pairs containing uncovered languages are unreliable!",
    "madebyollin/taesd": "ğŸ° Tiny AutoEncoder for Stable Diffusion\nUsing in ğŸ§¨ diffusers\nğŸ° Tiny AutoEncoder for Stable Diffusion\nTAESD is very tiny autoencoder which uses the same \"latent API\" as Stable Diffusion's VAE.\nTAESD is useful for real-time previewing of the SD generation process.\nThis repo contains .safetensors versions of the TAESD weights.\nFor SDXL, use TAESDXL instead (the SD and SDXL VAEs are incompatible).\nUsing in ğŸ§¨ diffusers\nimport torch\nfrom diffusers import DiffusionPipeline, AutoencoderTiny\npipe = DiffusionPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"slice of delicious New York-style cheesecake topped with berries, mint, chocolate crumble\"\nimage = pipe(prompt, num_inference_steps=50, generator=torch.Generator(\"cpu\").manual_seed(0x7A35D)).images[0]\nimage.save(\"cheesecake.png\")",
    "madebyollin/taesdxl": "ğŸ° Tiny AutoEncoder for Stable Diffusion (XL)\nUsing in ğŸ§¨ diffusers\nğŸ° Tiny AutoEncoder for Stable Diffusion (XL)\nTAESDXL is very tiny autoencoder which uses the same \"latent API\" as SDXL-VAE.\nTAESDXL is useful for real-time previewing of the SDXL generation process.\nThis repo contains .safetensors versions of the TAESDXL weights.\nFor SD1.x / SD2.x, use TAESD instead (the SD and SDXL VAEs are incompatible).\nUsing in ğŸ§¨ diffusers\nimport torch\nfrom diffusers import DiffusionPipeline, AutoencoderTiny\npipe = DiffusionPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesdxl\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"slice of delicious New York-style cheesecake topped with berries, mint, chocolate crumble\"\nimage = pipe(prompt, num_inference_steps=50, generator=torch.Generator(\"cpu\").manual_seed(0x7A35D)).images[0]\nimage.save(\"cheesecake_sdxl.png\")",
    "ArrayCats/LoRA-1.5": "é‡Œé¢å­˜åœ¨ä¸€äº›SDXL/PONYçš„æ¨¡å‹ï¼Œä½†æœ¬äººæœªæ¥æ‰“ç®—åˆ†å¼€æ”¾ç½®ï¼Œæ•…ä¸å†æ›´æ–°è¯¥Modelsä¸‹çš„SDXL/PONYæ¨¡å‹ï¼ˆä½†ä¹Ÿä¸ä¼šä¸»åŠ¨åˆ é™¤ï¼‰ã€‚",
    "facebook/audiogen-medium": "AudioGen - Medium - 1.5B\nAudiocraft Usage\nModel details\nAudioGen - Medium - 1.5B\nAudioGen is an autoregressive transformer LM that synthesizes general audio conditioned on text (Text-to-Audio).\nInternally, AudioGen operates over discrete representations learnt from the raw waveform, using an EnCodec tokenizer.\nAudioGen was presented at AudioGen: Textually Guided Audio Generation by Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre DÃ©fossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi.\nAudioGen 1.5B is a variant of the original AudioGen model that follows MusicGen architecture.\nMore specifically, it is trained over a 16kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz with a delay pattern between the codebooks.\nHaving only 50 auto-regressive steps per second of audio, this AudioGen model allows faster generation while reaching similar performances to the original AudioGen model introduced in the paper.\nAudiocraft Usage\nYou can run AudioGen locally through the original Audiocraft library:\nFirst install the audiocraft library\npip install git+https://github.com/facebookresearch/audiocraft.git\nMake sure to have ffmpeg installed:\napt get install ffmpeg\nRun the following Python code:\nimport torchaudio\nfrom audiocraft.models import AudioGen\nfrom audiocraft.data.audio import audio_write\nmodel = AudioGen.get_pretrained('facebook/audiogen-medium')\nmodel.set_generation_params(duration=5)  # generate 5 seconds.\ndescriptions = ['dog barking', 'sirenes of an emergency vehicule', 'footsteps in a corridor']\nwav = model.generate(descriptions)  # generates 3 samples.\nfor idx, one_wav in enumerate(wav):\n# Will save under {idx}.wav, with loudness normalization at -14 db LUFS.\naudio_write(f'{idx}', one_wav.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\nModel details\nSee AudioGen's model card.",
    "karpathy/tinyllamas": "This is a Llama 2 architecture model series trained on the TinyStories dataset, intended for use in the llama2.c project.",
    "CountFloyd/deepfake": "No model card",
    "lapki/Llama-2-7b-panorama-QLoRA": "Llama 2 7B, fine-tuned on Panorama media\nTraining procedure\nFramework versions\nAdditional information\nLlama 2 7B, fine-tuned on Panorama media\nThis repo contains the QLoRA adapter.\nPrompt:\nWrite a hypothetical news story based on the given headline\n### Title:\n{prompt}\nText:\nTraining procedure\nThe following bitsandbytes quantization config was used during training:\nload_in_8bit: False\nload_in_4bit: True\nllm_int8_threshold: 6.0\nllm_int8_skip_modules: None\nllm_int8_enable_fp32_cpu_offload: False\nllm_int8_has_fp16_weight: False\nbnb_4bit_quant_type: nf4\nbnb_4bit_use_double_quant: False\nbnb_4bit_compute_dtype: float16\nFramework versions\nPEFT 0.5.0.dev0\nAdditional information\nThanks its5Q for dataset and help",
    "lmsys/vicuna-13b-v1.5": "Vicuna Model Card\nModel Details\nModel Sources\nUses\nHow to Get Started with the Model\nTraining Details\nEvaluation\nDifference between different versions of Vicuna\nVicuna Model Card\nModel Details\nVicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.\nDeveloped by: LMSYS\nModel type: An auto-regressive language model based on the transformer architecture\nLicense: Llama 2 Community License Agreement\nFinetuned from model: Llama 2\nModel Sources\nRepository: https://github.com/lm-sys/FastChat\nBlog: https://lmsys.org/blog/2023-03-30-vicuna/\nPaper: https://arxiv.org/abs/2306.05685\nDemo: https://chat.lmsys.org/\nUses\nThe primary use of Vicuna is research on large language models and chatbots.\nThe primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.\nHow to Get Started with the Model\nCommand line interface: https://github.com/lm-sys/FastChat#vicuna-weights\nAPIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api\nTraining Details\nVicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning.\nThe training data is around 125K conversations collected from ShareGPT.com.\nSee more details in the \"Training Details of Vicuna Models\" section in the appendix of this paper.\nEvaluation\nVicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.\nDifference between different versions of Vicuna\nSee vicuna_weights_version.md",
    "facebook/dinov2-small": "Vision Transformer (small-sized model) trained using DINOv2\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nVision Transformer (small-sized model) trained using DINOv2\nVision Transformer (ViT) model trained using the DINOv2 method. It was introduced in the paper DINOv2: Learning Robust Visual Features without Supervision by Oquab et al. and first released in this repository.\nDisclaimer: The team releasing DINOv2 did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion.\nImages are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nNote that this model does not include any fine-tuned heads.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for feature extraction. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model:\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\nmodel = AutoModel.from_pretrained('facebook/dinov2-small')\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nBibTeX entry and citation info\nmisc{oquab2023dinov2,\ntitle={DINOv2: Learning Robust Visual Features without Supervision},\nauthor={Maxime Oquab and TimothÃ©e Darcet and ThÃ©o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and HervÃ© Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\nyear={2023},\neprint={2304.07193},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "team-lucid/deberta-v3-xlarge-korean": "",
    "chaoyi-wu/RadFM": "",
    "BAAI/bge-large-en": "",
    "BAAI/bge-large-zh": "",
    "elinas/chronos-13b-v2-GPTQ": "",
    "Gryphe/MythoLogic-L2-13b": "",
    "nerijs/pixel-art-xl": "",
    "ilovebots/bert-sdg-french": "",
    "Xenova/text-davinci-003": "",
    "xiaol/RWKV-claude-4-World-7B-65k": "",
    "Loke-60000/Christina-7B-chat": "",
    "speechbrain/sepformer-dns4-16k-enhancement": ""
}