{
    "primeline/whisper-large-v3-german": "A newer version of this model is available:\nprimeline/whisper-large-v3-turbo-german\nModel family\nTraining data\nTraining process\nHow to use\nAbout us\nSummary\nThis model map provides information about a model based on Whisper Large v3 that has been fine-tuned for speech recognition in German. Whisper is a powerful speech recognition platform developed by OpenAI. This model has been specially optimized for processing and recognizing German speech.\nApplications\nThis model can be used in various application areas, including\nTranscription of spoken German language\nVoice commands and voice control\nAutomatic subtitling for German videos\nVoice-based search queries in German\nDictation functions in word processing programs\nModel family\nModel\nParameters\nlink\nWhisper large v3 german\n1.54B\nlink\nWhisper large v3 turbo german\n809M\nlink\nDistil-whisper large v3 german\n756M\nlink\ntiny whisper\n37.8M\nlink\nTraining data\nThe training data for this model includes a large amount of spoken German from various sources. The data was carefully selected and processed to optimize recognition performance.\nTraining process\nThe training of the model was performed with the following hyperparameters\nBatch size: 1024\nEpochs: 2\nLearning rate: 1e-5\nData augmentation: No\nHow to use\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"primeline/whisper-large-v3-german\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nmax_new_tokens=128,\nchunk_length_s=30,\nbatch_size=16,\nreturn_timestamps=True,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nAbout us\nYour partner for AI infrastructure in Germany\nExperience the powerful AI infrastructure that drives your ambitions in Deep Learning, Machine Learning & High-Performance Computing.\nOptimized for AI training and inference.\nModel author: Florian Zimmermeister\nDisclaimer\nThis model is not a product of the primeLine Group.\nIt represents research conducted by [Florian Zimmermeister](https://huggingface.co/flozi00), with computing power sponsored by primeLine.\nThe model is published under this account by primeLine, but it is not a commercial product of primeLine Solutions GmbH.\nPlease be aware that while we have tested and developed this model to the best of our abilities, errors may still occur.\nUse of this model is at your own risk. We do not accept liability for any incorrect outputs generated by this model.",
    "cardiffnlp/twitter-roberta-base-topic-sentiment-latest": "cardiffnlp/twitter-roberta-base-topic-sentiment-latest\nLabels\nExample\nCitation Information\ncardiffnlp/twitter-roberta-base-topic-sentiment-latest\nThis is a RoBERTa-base model trained on 154M tweets until the end of December 2022 and finetuned for sentiment analysis (target based) on the TweetSentiment dataset of SuperTweetEval.\nThe original Twitter-based RoBERTa model can be found here.\nLabels\n\"id2label\": {\n\"0\": \"strongly negative\",\n\"1\": \"negative\",\n\"2\": \"negative or neutral\",\n\"3\": \"positive\",\n\"4\": \"strongly positive\"\n}\nExample\nfrom transformers import pipeline\ntext= 'If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @user @microsoft'\ntarget = \"@microsoft\"\ntext_input = f\"{text} </s> {target}\"\npipe = pipeline('text-classification', model=\"cardiffnlp/twitter-roberta-base-topic-sentiment-latest\")\npipe(text)\n>> [{'label': 'negative or neutral', 'score': 0.9601162672042847}]\nCitation Information\nPlease cite the reference paper if you use this model.\n@inproceedings{antypas2023supertweeteval,\ntitle={SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research},\nauthor={Dimosthenis Antypas and Asahi Ushio and Francesco Barbieri and Leonardo Neves and Kiamehr Rezaee and Luis Espinosa-Anke and Jiaxin Pei and Jose Camacho-Collados},\nbooktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},\nyear={2023}\n}",
    "epfl-llm/meditron-7b": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Meditron-7B-v1.0\nModel Details\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nTruthfulness, Helpfulness, Risk, and Bias\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card for Meditron-7B-v1.0\nMeditron is a suite of open-source medical Large Language Models (LLMs).\nMeditron-7B is a 7 billion parameters model adapted to the medical domain from Llama-2-7B through continued pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, a new dataset of internationally-recognized medical guidelines, and general domain data from RedPajama-v1.\nMeditron-7B, finetuned on relevant training data, outperforms Llama-2-7B and PMC-Llama on multiple medical reasoning tasks.\nAdvisory Notice\nWhile Meditron is designed to encode medical knowledge from sources of high-quality evidence, it is not yet adapted to deliver this knowledge appropriately, safely, or within professional actionable constraints.\nWe recommend against deploying Meditron in medical applications without extensive use-case alignment, as well as additional testing, specifically including randomized controlled trials in real-world practice settings.\nModel Details\nDeveloped by: EPFL LLM Team\nModel type: Causal decoder-only transformer language model\nLanguage(s): English (mainly)\nModel License: LLAMA 2 COMMUNITY LICENSE AGREEMENT\nCode License: APACHE 2.0 LICENSE\nContinue-pretrained from model: Llama-2-7B\nContext length:  2K tokens\nInput:  Text-only data\nOutput:  Model generates text only\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we enhance model's performance.\nKnowledge Cutoff: August 2023\nModel Sources\nRepository: epflLLM/meditron\nTrainer: epflLLM/Megatron-LLM\nPaper: MediTron-70B: Scaling Medical Pretraining for Large Language Models\nUses\nMeditron-7B is being made available for further testing and assessment as an AI assistant to enhance clinical decision-making and enhance access to an LLM for healthcare use. Potential use cases may include but are not limited to:\nMedical exam question answering\nSupporting differential diagnosis\nDisease information (symptoms, cause, treatment) query\nGeneral health information query\nDirect Use\nIt is possible to use this model to generate text, which is useful for experimentation and understanding its capabilities.\nIt should not be used directly for production or work that may impact people.\nDownstream Use\nMeditron-70B and Meditron-7B are both foundation models without finetuning or instruction-tuning. They can be finetuned, instruction-tuned, or RLHF-tuned for specific downstream tasks and applications.\nThere are two ways we have used this model for downstream question-answering tasks.\nWe apply in-context learning with k demonstrations (3 or 5 in our paper) added to the prompt.\nWe finetuned the models for downstream question-answering tasks using specific training sets.\nWe encourage and look forward to the adaption of the base model for more diverse applications.\nIf you want a more interactive way to prompt the model, we recommend using a high-throughput and memory-efficient inference engine with a UI that supports chat and text generation.\nYou can check out our deployment guide, where we used FastChat with vLLM. We collected generations for our qualitative analysis through an interactive UI platform, BetterChatGPT. Here is the prompt format we used as an example:\nOut-of-Scope Use\nWe do not recommend using this model for natural language generation in a production environment, finetuned or otherwise.\nTruthfulness, Helpfulness, Risk, and Bias\nWe did an initial assessment of Meditron models' Truthfulness against baseline models and consumer-level medical models.\nWe use TruthfulQA (multiple choice) as the main evaluation benchmark.\nWe only focus on the categories that are relevant to the medical domain, including Health, Nutrition, Psychology, and Science.\nFor 7B models, we perform one-shot evaluations for consistent answer generation.\nFor 70B models, the evaluations are under the zero-shot setting.\nBelow, we report the detailed truthfulness performance of each category.\nCategory\nmeditron-70b\nllama-2-70b\nmed42-70b*\nmeditron-7b\nllama-2-7b\nPMC-llama-7b\nHealth\n81.8\n69.1\n83.6\n27.3\n16.4\n3.6\nNutrition\n77.9\n68.8\n62.5\n31.1\n12.5\n6.3\nPsychology\n47.4\n36.8\n52.6\n21.1\n10.5\n0.0\nScience\n77.8\n44.4\n33.3\n33.3\n11.1\n0.0\nAvg\n71.2\n54.8\n58.0\n28.3\n12.6\n2.5\nFor a more detailed performance analysis, please see our paper.\nSignificant research is still required to fully explore potential bias, fairness, and safety issues with this language model.\nPlease recognize that our evaluation on Meditron-7B's helpfulness, risk, and bias are highly limited.\nThus, as we noted in the safety notice, we strongly against any deployment in medical applications without further alignment process and rigorous evaluation!\nRecommendations\nIMPORTANT!\nUsers (both direct and downstream) should be made aware of the risks, biases, and limitations of the model.\nWhile this model is capable of generating natural language text, we have only begun to explore this capability and its limitations.\nUnderstanding these limitations is especially important in a domain like medicine.\nTherefore, we strongly recommend against using this model in production for natural language generation or for professional purposes related to health and medicine.\nTraining Details\nTraining Data\nMeditron‚Äôs domain-adaptive pre-training corpus GAP-Replay  combines 48.1B tokens from four corpora:\nClinical  Guidelines: a new dataset of 46K internationally-recognized clinical practice guidelines from various healthcare-related sources, including hospitals and international organizations.\nMedical Paper Abstracts: 16.1M abstracts extracted from closed-access PubMed and PubMed Central papers.\nMedical Papers: full-text articles extracted from 5M publicly available PubMed and PubMed Central papers.\nReplay Data: 400M tokens of general domain pretraining data sampled from RedPajama-v1\nData Preprocessing\nPlease see the detailed preprocessing procedure in our paper.\nTraining Procedure\nWe used the Megatron-LLM distributed training library, a derivative of Nvidia's Megatron LM project, to optimize training efficiency.\nHardware consists of 1 node of 8x NVIDIA A100 (80GB) SXM GPUs connected by NVLink and NVSwitch with a single Nvidia ConnectX-6 DX network card and equipped with 2 x AMD EPYC 7543 32-Core Processors and 512 GB of RAM.\nOur three way parallelism scheme uses:\nData Parallelism (DP -- different GPUs process different subsets of the batches) of 2,\nPipeline Parallelism (PP -- different GPUs process different layers) of 4,\nTensor Parallelism (TP -- different GPUs process different subtensors for matrix multiplication) of 1.\nTraining Hyperparameters\nbf16\ntrue\nlr\n3e-4\neps\n1e-5\nbetas\n[0.9, 0.95]\nclip_grad\n1\nweight decay\n0.1\nDP size\n16\nTP size\n4\nPP size\n1\nseq length\n2048\nlr scheduler\ncosine\nmin lr\n1e-6\nwarmup iteration\n2000\nmicro batch size\n10\nglobal batch size\n1600\nSizes\nThe model was trained in September 2023.\nThe model architecture is exactly Llama 2, meaning\nModel size\n7B\nHidden dimension\n4096\nNum. attention heads\n32\nNum. layers\n32\nEvaluation\nTesting Data & Metrics\nTesting Data\nMedQA (USMLE)\nMedMCQA\nPubMedQA\nMMLU-Medical\nMedQA-4-Option\nMetrics\nAccuracy: suite the evaluation of multiple-choice question-answering tasks.\nResults\nWe finetune meditron-7b, llama-2-7b, pmc-llama-7b on each benchmark (pubmedqa, medmcqa, medqa)'s training data individually.\nWe report the finetuned models' performance with top token selection as the inference mode.\nFor MMLU-Medical, models finetuned on MedMCQA are used for inference.\nFor MedQA-4-Option, models finetuned on MedQA are used for inference.\nFor a more detailed performance analysis, please see our paper.\nDataset\nmeditron-7b\nllama-2-7b\npmc-llama-7b\nZephyr-7B-beta*\nMistral-7B-instruct*\nMMLU-Medical\n54.2\n53.7\n56.4\n63.3\n60.0\nPubMedQA\n74.4\n61.8\n59.2\n46.0\n17.8\nMedMCQA\n59.2\n54.4\n57.6\n43.0\n40.2\nMedQA\n47.9\n44.0\n42.4\n42.8\n32.4\nMedQA-4-Option\n52.0\n49.6\n49.2\n48.5\n41.1\nAvg\n57.5\n52.7\n53.0\n48.7\n38.3\nNote: models with * are already instruction-tuned, so we exclude them from further finetuning on any training data.\nEnvironmental Impact\nHardware Type: 8 x NVIDIA A100 (80GB) SXM\nTotal GPU hours: 588.8\nHardware Provider: EPFL Research Computing Platform\nCompute Region: Switzerland\nCarbon Emitted: Switzerland has a carbon efficiency of 0.016 kgCO2/kWh (https://www.carbonfootprint.com/docs/2018_8_electricity_factors_august_2018_-_online_sources.pdf). 73.6 hours of 8 A100s means 588.8 hours at a TDP of 400W. Assuming a Power Usage effectiveness of 1.5, total emissions are estimated to be:\n(400W / 1000W/kWh / GPU * 0.016 kgCO2/kWh * 73.6 h * 8 GPU) * 1.8 PUE = 6.8 kgCO2.\nCitation\nBibTeX:\nIf you use Meditron or its training data, please cite our work:\n@misc{chen2023meditron70b,\ntitle={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models},\nauthor={Zeming Chen and Alejandro Hern√°ndez-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas K√∂pf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},\nyear={2023},\neprint={2311.16079},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@software{epfmedtrn,\nauthor = {Zeming Chen and Alejandro Hern√°ndez-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas K√∂pf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},\ntitle = {MediTron-70B: Scaling Medical Pretraining for Large Language Models},\nmonth = November,\nyear = 2023,\nurl = {https://github.com/epfLLM/meditron}\n}",
    "TheBloke/goliath-120b-GGUF": "YAML Metadata\nWarning:\nThe pipeline tag \"conversational\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, audio-text-to-text, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-ranking, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, visual-document-retrieval, any-to-any, video-to-video, other\nGoliath 120B - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: Vicuna-Short\nCompatibility\nExplanation of quantisation methods\nProvided files\nQ6_K and Q8_0 files are split and require joining\nq6_K\nq8_0\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using ctransformers\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nOriginal model card: Alpin's Goliath 120B\nGoliath 120B\nPrompting Format\nMerge process\nScreenshots\nBenchmarks\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nGoliath 120B - GGUF\nModel creator: Alpin\nOriginal model: Goliath 120B\nDescription\nThis repo contains GGUF format model files for Alpin's Goliath 120B.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nRepositories available\nAWQ model(s) for GPU inference.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nAlpin's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Vicuna-Short\nYou are a helpful AI assistant.\nUSER: {prompt}\nASSISTANT:\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\ngoliath-120b.Q2_K.gguf\nQ2_K\n2\n49.63 GB\n52.13 GB\nsmallest, significant quality loss - not recommended for most purposes\ngoliath-120b.Q3_K_S.gguf\nQ3_K_S\n3\n50.71 GB\n53.21 GB\nvery small, high quality loss\ngoliath-120b.Q3_K_M.gguf\nQ3_K_M\n3\n56.41 GB\n58.91 GB\nvery small, high quality loss\ngoliath-120b.Q3_K_L.gguf\nQ3_K_L\n3\n61.67 GB\n64.17 GB\nsmall, substantial quality loss\ngoliath-120b.Q4_0.gguf\nQ4_0\n4\n66.31 GB\n68.81 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ngoliath-120b.Q4_K_S.gguf\nQ4_K_S\n4\n66.43 GB\n68.93 GB\nsmall, greater quality loss\ngoliath-120b.Q4_K_M.gguf\nQ4_K_M\n4\n70.64 GB\n73.14 GB\nmedium, balanced quality - recommended\ngoliath-120b.Q5_0.gguf\nQ5_0\n5\n81.00 GB\n83.50 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ngoliath-120b.Q5_K_S.gguf\nQ5_K_S\n5\n81.00 GB\n83.50 GB\nlarge, low quality loss - recommended\ngoliath-120b.Q5_K_M.gguf\nQ5_K_M\n5\n83.22 GB\n85.72 GB\nlarge, very low quality loss - recommended\ngoliath-120b.Q6_K.gguf\nQ6_K\n6\n96.60 GB\n99.10 GB\nvery large, extremely low quality loss\ngoliath-120b.Q8_0.gguf\nQ8_0\n8\n125.12 GB\n127.62 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nQ6_K and Q8_0 files are split and require joining\nNote: HF does not support uploading files larger than 50GB. Therefore I have uploaded the Q6_K and Q8_0 files as split files.\nClick for instructions regarding Q6_K and Q8_0 files\nq6_K\nPlease download:\ngoliath-120b.Q6_K.gguf-split-a\ngoliath-120b.Q6_K.gguf-split-b\nq8_0\nPlease download:\ngoliath-120b.Q8_0.gguf-split-a\ngoliath-120b.Q8_0.gguf-split-b\nTo join the files, do the following:\nLinux and macOS:\ncat goliath-120b.Q6_K.gguf-split-* > goliath-120b.Q6_K.gguf && rm goliath-120b.Q6_K.gguf-split-*\ncat goliath-120b.Q8_0.gguf-split-* > goliath-120b.Q8_0.gguf && rm goliath-120b.Q8_0.gguf-split-*\nWindows command line:\nCOPY /B goliath-120b.Q6_K.gguf-split-a + goliath-120b.Q6_K.gguf-split-b goliath-120b.Q6_K.gguf\ndel goliath-120b.Q6_K.gguf-split-a goliath-120b.Q6_K.gguf-split-b\nCOPY /B goliath-120b.Q8_0.gguf-split-a + goliath-120b.Q8_0.gguf-split-b goliath-120b.Q8_0.gguf\ndel goliath-120b.Q8_0.gguf-split-a goliath-120b.Q8_0.gguf-split-b\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/goliath-120b-GGUF and below it, a specific filename to download, such as: goliath-120b.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/goliath-120b-GGUF goliath-120b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/goliath-120b-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/goliath-120b-GGUF goliath-120b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 32 -m goliath-120b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"You are a helpful AI assistant.\\n\\nUSER: {prompt}\\nASSISTANT:\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 4096 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions here: text-generation-webui/docs/llama.cpp.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries.\nHow to load this model in Python code, using ctransformers\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\nSimple ctransformers example code\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/goliath-120b-GGUF\", model_file=\"goliath-120b.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\nprint(llm(\"AI is going to\"))\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Brandon Frisco, LangChain4j, Spiking Neurons AB, transmissions 11, Joseph William Delisle, Nitin Borwankar, Willem Michiel, Michael Dempsey, vamX, Jeffrey Morgan, zynix, jjj, Omer Bin Jawed, Sean Connelly, jinyuan sun, Jeromy Smith, Shadi, Pawan Osman, Chadd, Elijah Stavena, Illia Dulskyi, Sebastain Graf, Stephen Murray, terasurfer, Edmond Seymore, Celu Ramasamy, Mandus, Alex, biorpg, Ajan Kanaga, Clay Pascal, Raven Klaugh, ÈòøÊòé, K, ya boyyy, usrbinkat, Alicia Loh, John Villwock, ReadyPlayerEmma, Chris Smitley, Cap'n Zoog, fincy, GodLy, S_X, sidney chen, Cory Kujawski, OG, Mano Prime, AzureBlack, Pieter, Kalila, Spencer Kim, Tom X Nguyen, Stanislav Ovsiannikov, Michael Levine, Andrey, Trailburnt, Vadim, Enrico Ros, Talal Aujan, Brandon Phillips, Jack West, Eugene Pentland, Michael Davis, Will Dee, webtim, Jonathan Leane, Alps Aficionado, Rooh Singh, Tiffany J. Kim, theTransient, Luke @flexchar, Elle, Caitlyn Gatomon, Ari Malik, subjectnull, Johann-Peter Hartmann, Trenton Dambrowitz, Imad Khwaja, Asp the Wyvern, Emad Mostaque, Rainer Wilmers, Alexandros Triantafyllidis, Nicholas, Pedro Madruga, SuperWojo, Harry Royden McLaughlin, James Bentley, Olakabola, David Ziegler, Ai Maven, Jeff Scroggin, Nikolai Manek, Deo Leter, Matthew Berman, Fen Risland, Ken Nordquist, Manuel Alberto Morcote, Luke Pendergrass, TL, Fred von Graf, Randy H, Dan Guido, NimbleBox.ai, Vitor Caleffi, Gabriel Tamborski, knownsqashed, Lone Striker, Erik Bj√§reholt, John Detwiler, Leonard Tan, Iucharbius\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Alpin's Goliath 120B\nGoliath 120B\nAn auto-regressive causal LM created by combining 2x finetuned Llama-2 70B into one.\nPrompting Format\nBoth Vicuna and Alpaca will work, but due the initial and final layers belonging primarily to Xwin, I expect Vicuna to work the best.\nMerge process\nThe models used in the merge are Xwin and Euryale.\nThe layer ranges used are as follows:\n- range 0, 16\nXwin\n- range 8, 24\nEuryale\n- range 17, 32\nXwin\n- range 25, 40\nEuryale\n- range 33, 48\nXwin\n- range 41, 56\nEuryale\n- range 49, 64\nXwin\n- range 57, 72\nEuryale\n- range 65, 80\nXwin\nScreenshots\nBenchmarks\nComing soon.",
    "alexgenovese/loras": "No model card",
    "vinai/PhoGPT-7B5-Instruct": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPhoGPT: Generative Pre-training for Vietnamese\nPhoGPT: Generative Pre-training for Vietnamese\nWe release a 7.5B-parameter generative model series named PhoGPT for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant PhoGPT-7B5-Instruct.\nPhoGPT-7B5 is a Transformer decoder-based model that incorporates flash attention (Triton) and ALiBi for context length extrapolation. We pre-trained PhoGPT-7B5 from scratch on a 41GB pre-training corpus of Vietnamese texts. We then fine-tuned the pre-trained PhoGPT-7B5 for instruction following using a dataset of 150K Vietnamese prompt-response pairs, resulting in PhoGPT-7B5-Instruct.\nWe recommend users use the latest PhoGPT versions - PhoGPT-4B and PhoGPT-4B-Instruct - for better performance and efficiency.\nFor further information or requests, please go to PhoGPT's homepage!",
    "rupeshs/LCM-runwayml-stable-diffusion-v1-5": "README.md exists but content is empty.",
    "NeverSleep/Noromaid-13b-v0.1.1": "Disclaimer:\nThis is a TEST version, don't expect everything to work!!!\nThis model is a collab between IkariDev and Undi!\nDescription\nChangelog what should be fixed from the last version (0.1):\nRatings:\nPrompt template: Custom format, or Alpaca\nCustom format:\nAlpaca:\nTraining data used:\nOthers\nDisclaimer:\nThis is a TEST version, don't expect everything to work!!!\nYou may use our custom prompting format(scroll down to download them!), or simple alpaca. (Choose which fits best for you!)\nThis model is a collab between IkariDev and Undi!\nTired of the same merges everytime? Here it is, the Noromaid-13b-v0.1.1 model. Suitable for RP, ERP and general stuff.\n[Recommended settings - No settings yet(Please suggest some over in the Community tab!)]\nDescription\nThis repo contains FP16 files of Noromaid-13b-v0.1.1.\nChangelog what should be fixed from the last version (0.1):\nFixed somes issues where the model had a hard time grasping at the character card/persona, logical error and the following of the story/chat.\nFixed some logical issue.\nFixed some OOC leaking at the end of some reply (tested without stopping string).\nFixed an obscure crash in Koboldcpp where the model refused to output anymore when context was full in some case.\nFP16 - by IkariDev and Undi\nGGUF - by IkariDev and Undi\nRatings:\nNote: We have permission of all users to upload their ratings, we DONT screenshot random reviews without asking if we can put them here!\nNo ratings yet!\nIf you want your rating to be here, send us a message over on DC and we'll put up a screenshot of it here. DC name is \"ikaridev\" and \"undi\".\nPrompt template: Custom format, or Alpaca\nCustom format:\nUPDATED!! SillyTavern config files: Context, Instruct.\nOLD SillyTavern config files: Context, Instruct.\nAlpaca:\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nTraining data used:\nno_robots dataset let the model have more human behavior, enhances the output.\n[Aesir Private RP dataset] New data from a new and never used before dataset, add fresh data, no LimaRP spam, this is 100% new. Thanks to the MinvervaAI Team and, in particular, Gryphe for letting us use it!\nOthers\nUndi: If you want to support me, you can here.\nIkariDev: Visit my retro/neocities style website please kek",
    "blink7630/storyboard-sketch": "Storyboard Sketch\nImage examples for the model:\nStoryboard Sketch\nPrompt\nstoryboard sketch of a zombie basketball player dunking with both hands, action shot, motion blur, hero\nPrompt\nstoryboard sketch extreme closeup dutch angle of Lara Croft running through the jungle with a machete, angry, drama shot, looking at the camera, (foreshortening:1.2), loose debris falling, motion blur\nPrompt\nstoryboard sketch of A woman with blonde hair wearing sunglasses and a white sundress with yellow accents is walking towards a 1950's gas station in Mobile Alabama\nPrompt\nstoryboard sketch of a WWII horrific furious yelling rotting zombie pilot in the cockpit of a spitfire, action shot, closeup, dogfight, splattered, skull\nPrompt\nstoryboard sketch of SWAT team breaching a door with explosives, wood splinters and smoke filling the air.\nPrompt\nstoryboard sketch of Rocket launching from a spaceport, flames and smoke billowing, shaking the ground.\nPrompt\nstoryboard sketch of Rally car drifting around a tight corner, gravel spraying out.\nPrompt\nstoryboard sketch of Motocross rider clearing a series of jumps, dirt clods flying from the tires.\nPrompt\nstoryboard sketch of Demolition derby in full swing, cars smashing into each other, metal crunching and sparks flying.\nPrompt\nstoryboard sketch of Danny DeVito  feeling Numb Pointing playing a Interpreter in bikini bottom\nThis LoRA was trained on SDXL Base using 60 grayscale storyboard sketches and character portraits at 21:9, 16:9, and 1:1 aspect ratios. At full strength you get the most abstract sketches - good for leaving the fine details of a scene to the imagination. At around 0.8 strength you get much more coherence and prompt faithfulness. You can go lower from there to get more detailed and realistic sketches.Strength of 1.0: Strongest styling, less coherence and prompt faithfulness Strength of 0.8: Good styling, medium coherence and prompt faithfulness Strength of 0.5: Conventional styling, best coherence and prompt faithfulness\nImage examples for the model:\nstoryboard sketch extreme closeup dutch angle of Lara Croft running through the jungle with a machete, angry, drama shot, looking at the camera, (foreshortening:1.2), loose debris falling, motion blur\nstoryboard sketch of A woman with blonde hair wearing sunglasses and a white sundress with yellow accents is walking towards a 1950's gas station in Mobile Alabama\nstoryboard sketch of a WWII horrific furious yelling rotting zombie pilot in the cockpit of a spitfire, action shot, closeup, dogfight, splattered, skull\nstoryboard sketch of SWAT team breaching a door with explosives, wood splinters and smoke filling the air.\nstoryboard sketch of Rocket launching from a spaceport, flames and smoke billowing, shaking the ground.\nstoryboard sketch of Rally car drifting around a tight corner, gravel spraying out.\nstoryboard sketch of Motocross rider clearing a series of jumps, dirt clods flying from the tires.\nstoryboard sketch of Demolition derby in full swing, cars smashing into each other, metal crunching and sparks flying.\nstoryboard sketch of Danny DeVito  feeling Numb Pointing playing a Interpreter in bikini bottom",
    "e-n-v-y/envy-speedpaint-xl-01": "Envy Speedpaint XL 01\nImage examples for the model:\nEnvy Speedpaint XL 01\nPrompt\n1930s pulp sci-fi cover art, highly stylized sci-fi digital acrylic painting of a Ephemeral cityin a Peat Bog, photoshop grunge brushes, neon yellow and dark blue-grey color scheme\nPrompt\nhighly stylized sci-fi digital acrylic painting of a metropolisin a Tropical Monsoon Forest, photoshop thick acrylic brushes, magenta and gray color scheme\nPrompt\ndigital fantasy acrylic painting of a Ethereal Monolith in a Open Ocean, photoshop thick acrylic brushes, red and neon yellow color scheme\nPrompt\nsci-fi digital acrylic painting of a Supernatural metropolisin a Micrometeoroid Pocked Areas, photoshop thick acrylic brushes, shiny slate gray and pastel purple color scheme\nPrompt\nsci-fi digital acrylic painting of a city, photoshop thick acrylic brushes, white and black color scheme\nPrompt\ndigital fantasy acrylic painting of a Dystopian Geode in a Frozen Waterfall Kingdom, photoshop thick acrylic brushes, green  color scheme\n(CivitAI)\nThis model was trained on various digital speedpaintings.  It's good at working in the style of modern digital concept art.  It also does watercolor really well, and it can do both character portraits and landscapes.\nImage examples for the model:\nhighly stylized sci-fi digital acrylic painting of a metropolisin a Tropical Monsoon Forest, photoshop thick acrylic brushes, magenta and gray color scheme\ndigital fantasy acrylic painting of a Ethereal Monolith in a Open Ocean, photoshop thick acrylic brushes, red and neon yellow color scheme\nsci-fi digital acrylic painting of a Supernatural metropolisin a Micrometeoroid Pocked Areas, photoshop thick acrylic brushes, shiny slate gray and pastel purple color scheme\nsci-fi digital acrylic painting of a city, photoshop thick acrylic brushes, white and black color scheme\ndigital fantasy acrylic painting of a Dystopian Geode in a Frozen Waterfall Kingdom, photoshop thick acrylic brushes, green  color scheme",
    "NeverSleep/Noromaid-20b-v0.1.1": "Disclaimer:\nThis is a TEST version, don't expect everything to work!!!\nThis model is a collab between IkariDev and Undi!\nDescription\nRatings:\nPrompt template: Custom format, or Alpaca\nCustom format:\nAlpaca:\nTraining data used:\nOthers\nDisclaimer:\nThis is a TEST version, don't expect everything to work!!!\nYou may use our custom prompting format(scroll down to download them!), or simple alpaca. (Choose which fits best for you!)\nThis model is a collab between IkariDev and Undi!\nTired of the same merges everytime? Here it is, the Noromaid-20b-v0.1.1 model. Suitable for RP, ERP and general stuff.\n[Recommended settings - No settings yet(Please suggest some over in the Community tab!)]\nDescription\nThis repo contains fp16 files of Noromaid-20b-v0.1.1.\nFP16 - by IkariDev and Undi\nGGUF - by IkariDev and Undi\nRatings:\nNote: We have permission of all users to upload their ratings, we DONT screenshot random reviews without asking if we can put them here!\nNo ratings yet!\nIf you want your rating to be here, send us a message over on DC and we'll put up a screenshot of it here. DC name is \"ikaridev\" and \"undi\".\nPrompt template: Custom format, or Alpaca\nCustom format:\nUPDATED!! SillyTavern config files: Context, Instruct.\nOLD SillyTavern config files: Context, Instruct.\nAlpaca:\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nTraining data used:\nno_robots dataset let the model have more human behavior, enhances the output.\n[Aesir Private RP dataset] New data from a new and never used before dataset, add fresh data, no LimaRP spam, this is 100% new. Thanks to the MinvervaAI Team and, in particular, Gryphe for letting us use it!\nOthers\nUndi: If you want to support me, you can here.\nIkariDev: Visit my retro/neocities style website please kek",
    "GOAT-AI/GOAT-70B-Storytelling": "GOAT-70B-Storytelling model\nTraining details\nLearn more\nGOAT-Storytelling-Agent\nTraining details\nLearn more\nModel description\nUses\nUsage\nLicense\nRisks and Biases\nOpen LLM Leaderboard Evaluation Results\nGOAT-70B-Storytelling model\nGOAT-70B-Storytelling model trained by GOAT.AI lab as a core model for an autonomous story-writing agent.\nGOAT-Storytelling-Agent\nThis agent facilitates the generation of high-quality, cohesive, and captivating narratives, including stories and books. It achieves this by utilizing inputs such as plot outlines, character profiles, their interrelationships, and other relevant details. Examples are provided below.\nModel description\nBase Architecture: LLaMA 2 70B\nLicense: llama2\nContext window length: 4096 tokens\nTraining details\nTraining was performed on a GPU cluster of 64xH100s. FSDP ZeRO-3 sharding is employed for efficient training. We instruction finetune on a dataset of 18K examples for one epoch with batch size of 336, AdamW optimizer with learning rate 1e-5.\nLearn more\nBlogpost: GOAT-Storytelling: Arbitrarily Long Story Writing Agent\nGitHub: here\nGenerated examples: here\nUses\nThe main purpose of GOAT-70B-Storytelling is to generate books, novels, movie scripts and etc. as an agent in coping with our GOAT-Storytelling-Agent. It is specifically designed for storywriters.\nUsage\nUsage can be either self-hosted via transformers or used with Spaces\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"GOAT-AI/GOAT-70B-Storytelling\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16\n)\nCurrently, we support LLM endpoint generation, where you need to send a post request to the generation endpoint (we recommend using Text Generation Inference by HuggingFace).\nHere is how you can utilize the model via GOAT-Storytelling-Agent:\nfrom goat_storytelling_agent.storytelling_agent import StoryAgent\nbackend_uri = # Text generation endpoint\nwriter = StoryAgent(backend_uri, form='novel')\nnovel_scenes = writer.generate_story('treasure hunt in a jungle')\nLicense\nGOAT-70B-Storytelling model is based on Meta's LLaMA-2-70b-hf, and using own datasets.\nGOAT-70B-Storytelling model weights are available under LLAMA-2 license.\nRisks and Biases\nGOAT-70B-Storytelling model can produce factually incorrect output and should not be relied on to deliver factually accurate information. Therefore, the GOAT-70B-Storytelling model could possibly generate wrong, biased, or otherwise offensive outputs.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n67.38\nAI2 Reasoning Challenge (25-Shot)\n68.77\nHellaSwag (10-Shot)\n87.74\nMMLU (5-Shot)\n69.92\nTruthfulQA (0-shot)\n53.53\nWinogrande (5-shot)\n83.50\nGSM8k (5-shot)\n40.79",
    "LeoLM/leo-hessianai-70b-chat": "LAION LeoLM 70b Chat: Linguistically Enhanced Open Language Model\nLeoLM Chat\nModel Details\nUse in ü§óTransformers\nPrompting / Prompt Template\nEthical Considerations and Limitations\nFinetuning Details\nDataset Details\nLAION LeoLM 70b Chat: Linguistically Enhanced Open Language Model\nMeet LeoLM, the first open and commercially available German Foundation Language Model built on Llama-2.\nOur models extend Llama-2's capabilities into German through continued pretraining on a large corpus of German-language and mostly locality specific text.\nThanks to a compute grant at HessianAI's new supercomputer 42, we release a series foundation models trained with 8k context length\nunder the Llama-2 community license. Now, we're finally releasing the\nmuch anticipated leo-hessianai-70b, the largest model of this series based on Llama-2-70b.\nWith this release, we hope to bring a new wave of opportunities to German open-source and commercial LLM research and accelerate adoption.\nRead our blog post or our paper (preprint coming soon) for more details!\nA project by Bj√∂rn Pl√ºster and Christoph Schuhmann in collaboration with LAION and HessianAI.\nLeoLM Chat\nLeoLM/leo-hessianai-70b-chat is a German chat model built on our foundation model LeoLM/leo-hessianai-70b and finetuned on a selection of German instruction datasets.\nThe model performs exceptionally well on writing, explanation and discussion tasks but struggles somewhat with math and advanced reasoning. See our MT-Bench-DE scores:\n{\n\"first_turn\": 7.2375,\n\"second_turn\": 6.5375,\n\"categories\": {\n\"writing\": 8.55,\n\"roleplay\": 7.15,\n\"reasoning\": 4.2,\n\"math\": 4.85,\n\"coding\": 4.85,\n\"extraction\": 7.75,\n\"stem\": 8.45,\n\"humanities\": 9.3\n},\n\"average\": 6.8875\n}\nHave a look at some examples in this Google Doc.\nModel Details\nFinetuned from: LeoLM/leo-hessianai-70b\nModel type: Causal decoder-only transformer language model\nLanguage: English and German\nDemo: Web Demo\nLicense: LLAMA 2 COMMUNITY LICENSE AGREEMENT\nContact: LAION Discord or Bj√∂rn Pl√ºster\nUse in ü§óTransformers\nFirst install direct dependencies:\npip install transformers torch\nThen load the model in transformers:\nfrom transformers import pipeline\nimport torch\nsystem_prompt = \"\"\"<|im_start|>system\nDies ist eine Unterhaltung zwischen einem intelligenten, hilfsbereitem KI-Assistenten und einem Nutzer.\nDer Assistent gibt ausf√ºhrliche, hilfreiche und ehrliche Antworten.<|im_end|>\n\"\"\"\nprompt_format = \"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\nprompt = \"Erkl√§re mir wie die Fahrradwegesituation in Hamburg ist.\"\ngenerator = pipeline(model=\"LeoLM/leo-hessianai-70b-chat\", device=\"cuda\", torch_dtype=torch.float16)\nprint(generator(prompt_format.format(prompt=prompt), do_sample=True, top_p=0.95, max_length=8192))\n\"Hamburg verf√ºgt √ºber ein umfangreiches Netzwerk an Radwegen, das ein effizientes und angenehmes Radfahren in der ganzen Stadt erm√∂glicht. Die Radwege variieren in Bezug auf ihre Qualit√§t, wobei einige erstklassig mit eigens f√ºr Radfahrer reservierten Spuren sind, w√§hrend andere einfache Fahrradsymbole auf dem Boden haben, die anzeigen, dass Radfahrer abwechselnd mit dem Autoverkehr auf der Stra√üe fahren sollten. Einige Nebenstra√üen haben auch spezielle Fahrradspuren, wobei einige mit Bordsteinabsenkungen zur Seite der Autospuren markiert sind. Zus√§tzlich haben viele Hauptstra√üen, insbesondere in Nebenstra√üen, fahrradfreundliche Abst√§nde zwischen den geparkten Autos und dem Gehweg, was ein bequemes Fahren auf der Stra√üe erm√∂glicht. Der Bau von Radschnellwegen, die schnelles und effizientes Radfahren in und aus der Stadt erm√∂glichen, ist im Gange und wird in den kommenden Jahren fortgesetzt. Insgesamt sind die Radwege in Hamburg weitl√§ufig und gut ausgeschildert, was es zu einem angenehmen Ort macht, um mit dem Fahrrad zu fahren.\"\nPrompting / Prompt Template\nPrompt dialogue template (ChatML format):\n\"\"\"\n<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n\"\"\"\nThe model input can contain multiple conversation turns between user and assistant, e.g.\n<|im_start|>user\n{prompt 1}<|im_end|>\n<|im_start|>assistant\n{reply 1}<|im_end|>\n<|im_start|>user\n{prompt 2}<|im_end|>\n<|im_start|>assistant\n(...)\nEthical Considerations and Limitations\nLeoLM has been tested in English and German, and has not covered, nor could it cover all scenarios.\nFor these reasons, as with all LLMs, the potential outputs of LeoLM/leo-hessianai-70b-chat cannot be predicted\nin advance, and the model may in some instances produce inaccurate, biased or other objectionable responses\nto user prompts. Therefore, before deploying any applications of LeoLM/leo-hessianai-70b-chat, developers should\nperform safety testing and tuning tailored to their specific applications of the model.\nWe are aware of the model refusing to answer more often than desired. This will be adressed in future versions. For now, the training\ndataset is equal to that used for our smaller chat variants.\nPlease see Meta's Responsible Use Guide.\nFinetuning Details\nHyperparameter\nValue\nNum epochs\n3\nExamples per epoch\n131214\nGlobal batch size\n256\nLearning rate\n1.5e-5\nWarmup steps\n15\nLR scheduler\nCosine\nAdam betas\n(0.9, 0.95)\nWeight Decay\n0.01\nDataset Details\n## Stats for 'Subset of OpenAssistant/OASST-DE' (3534 samples (100.0%))\n-----------------\nAccepted: 3534/3534 (100.0%)\nAccepted tokens: 2259302\nSkipped: 0 (0.0%)\nMin tokens per sample: 29\nMax tokens per sample: 2484\nAvg tokens per sample: 639.3044708545557\n-----------------\n## Stats for 'Subset of FreedomIntelligence/evol-instruct-deutsch' (57841 samples (100.0%))\n-----------------\nAccepted: 57841/57841 (100.0%)\nAccepted tokens: 42958192\nSkipped: 0 (0.0%)\nMin tokens per sample: 33\nMax tokens per sample: 5507\nAvg tokens per sample: 742.6944900675991\n-----------------\n## Stats for 'Subset of FreedomIntelligence/alpaca-gpt4-deutsch' (48969 samples (100.0%))\n-----------------\nAccepted: 48969/48969 (100.0%)\nAccepted tokens: 13372005\nSkipped: 0 (0.0%)\nMin tokens per sample: 19\nMax tokens per sample: 1359\nAvg tokens per sample: 273.07082031489307\n-----------------\n## Stats for 'Subset of LeoLM/OpenSchnabeltier' (21314 samples (100.0%))\n-----------------\nAccepted: 21314/21314 (100.0%)\nAccepted tokens: 8134690\nSkipped: 0 (0.0%)\nMin tokens per sample: 25\nMax tokens per sample: 1202\nAvg tokens per sample: 381.65947264708643\n-----------------\n## Stats for 'Subset of LeoLM/German_Poems' (490 samples (100.0%))\n-----------------\nAccepted: 490/490 (100.0%)\nAccepted tokens: 618642\nSkipped: 0 (0.0%)\nMin tokens per sample: 747\nMax tokens per sample: 1678\nAvg tokens per sample: 1262.534693877551\n-----------------\n## Stats for 'Subset of LeoLM/German_Songs' (392 samples (100.0%))\n-----------------\nAccepted: 392/392 (100.0%)\nAccepted tokens: 187897\nSkipped: 0 (0.0%)\nMin tokens per sample: 231\nMax tokens per sample: 826\nAvg tokens per sample: 479.3290816326531\n-----------------\n## Stats for 'total' (132540 samples (100.0%))\n-----------------\nAccepted: 132540/132540 (100.0%)\nAccepted tokens: 67530728\nSkipped: 0 (0.0%)\nMin tokens per sample: 19\nMax tokens per sample: 5507\nAvg tokens per sample: 509.51205673758864\n-----------------",
    "SecurityXuanwuLab/HaS-820m": "Hide-and-SeekÈöêÁßÅ‰øùÊä§Ê®°Âûã\n‰ªãÁªç\ncolabÊºîÁ§∫&ÁéØÂ¢É‰æùËµñ\nQuick Start\nÊµãËØï\nÂºïÁî®\nHide-and-SeekÈöêÁßÅ‰øùÊä§Ê®°Âûã\nGithub Repo / arXiv Preprint / ÊäÄÊúØÂçöÂÆ¢\n‰ªãÁªç\nHide-and-SeekÊòØ‰∏Ä‰∏™Áî±ËÖæËÆØÂÆâÂÖ®ÁéÑÊ≠¶ÂÆûÈ™åÂÆ§ÂºÄÂèëÁöÑÈöêÁßÅ‰øùÊä§Ê®°Âûã„ÄÇËØ•Ê®°ÂûãÁöÑÈöêÁßÅ‰øùÊä§ÊµÅÁ®ãÁî±hide‰∏éseek‰∏§‰∏™Â≠ê‰ªªÂä°ÁªÑÊàêÔºåÂÖ∂‰∏≠hideË¥üË¥£Â∞ÜÁî®Êà∑ËæìÂÖ•‰∏≠ÁöÑÊïèÊÑüÂÆû‰ΩìËØçÊõøÊç¢‰∏∫ÂÖ∂‰ªñÈöèÊú∫ÂÆû‰ΩìÔºàÈöêÁßÅ‰ø°ÊÅØÂåøÂêçÂåñÔºâÔºåseekË¥üË¥£Â∞ÜËæìÂá∫‰∏≠Ë¢´ÊõøÊç¢ÊéâÁöÑÈÉ®ÂàÜËøòÂéü‰ª•ÂØπÂ∫îÂéüÊñáÊú¨Ôºà‰ø°ÊÅØËøòÂéüÔºâ„ÄÇÊ≠§‰ªìÂ∫ìÊòØÊàë‰ª¨ÁöÑÁ§æÂå∫ÂºÄÊ∫ê‰∏≠ÊñáÁâàÊ®°ÂûãÔºåÊ®°Âûã‰ª•bloom-1.1b‰∏∫Â∫ïÊ®°ÔºåÁªèËøáËØçË°®Ë£ÅÂâ™ÂíåÂæÆË∞ÉÂêéÂæóÂà∞„ÄÇÊõ¥Â§öÁªÜËäÇËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑÊäÄÊúØÂçöÂÆ¢„ÄÇ\nÊàë‰ª¨Â∑≤ÊàêÂäüÂ∞ÜËØ•Ê®°ÂûãÂú®ÊâãÊú∫ÂíåÁ¨îËÆ∞Êú¨‰∏äËøõË°å‰∫ÜÈÉ®ÁΩ≤ÂÆûÈ™å„ÄÇÁªèÊµãËØïÔºåÂú®NF4ÈáèÂåñÂêéÔºå‰ªÖ‰ΩøÁî®CPUÁöÑÊÉÖÂÜµ‰∏ãÔºåMacBook M2Á¨îËÆ∞Êú¨ÁöÑÊé®ÁêÜÈÄüÂ∫¶‰∏∫180-200 tokens/sÔºåMacBook M1Á¨îËÆ∞Êú¨ÁöÑÊé®ÁêÜÈÄüÂ∫¶‰∏∫110-130 tokens/sÔºåPixel 8 ProÊâãÊú∫ÁöÑÊé®ÁêÜÈÄüÂ∫¶‰∏∫20-30 tokens/s„ÄÇÁõÆÂâçÂéüÁîüÊîØÊåÅÁöÑNLP‰ªªÂä°Á±ªÂûãÊúâÔºöÊ∂¶Ëâ≤ÔºåÊëòË¶ÅÔºåÁøªËØëÔºåÈòÖËØªÁêÜËß£ÂíåÊñáÊú¨ÂàÜÁ±ªÔºåÂπ∂‰∏îÊîØÊåÅZero ShotÁöÑËá™ÂÆö‰πâÊâ©Â±ï‰ªªÂä°„ÄÇ\n‰ª•‰∏ãÊòØÊàë‰ª¨Ê®°ÂûãÁöÑÊºîÁ§∫ËßÜÈ¢ëÔºö\nMacBook M2ÊºîÁ§∫ËßÜÈ¢ëÔºö\nPixel 8 ProÊºîÁ§∫ËßÜÈ¢ëÔºö\ncolabÊºîÁ§∫&ÁéØÂ¢É‰æùËµñ\nÁî±‰∫éÊú∫Âô®Â≠¶‰π†ÁéØÂ¢ÉÈÖçÁΩÆÂ§çÊùÇËÄóÊó∂ÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™colab notebookÁî®‰∫édemoÔºåÊàë‰ª¨Âú®‰∏ãÊñπÂàóÂá∫‰∫ÜÂøÖË¶Å‰æùËµñ‰æõÊÇ®ÂèÇËÄÉ„ÄÇÂ¶ÇÊûúÊÇ®Âú®Ëá™Â∑±ÁöÑÁéØÂ¢É‰∏äËøêË°åÔºåÂèØËÉΩÈúÄË¶ÅÊ†πÊçÆËá™Â∑±ËÆæÂ§áÂÅöÂá∫‰∏Ä‰∫õË∞ÉÊï¥„ÄÇ\npip install torch==2.1.0+cu118\npip install transformers==4.35.0\nQuick Start\n‰∏ãÈù¢ÊòØÈÄöËøáhideÂÆåÊàê‰ø°ÊÅØÂåøÂêçÂåñÁöÑ‰∏Ä‰∏™‰æãÂ≠ê„ÄÇ\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"SecurityXuanwuLab/HaS-820m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SecurityXuanwuLab/HaS-820m\").to('cuda:0')\nhide_template = \"\"\"<s>Paraphrase the text:%s\\n\\n\"\"\"\noriginal_input = \"Âº†‰ºüÁî®ËãπÊûú(iPhone 13)Êç¢‰∫Ü‰∏ÄÁÆ±Â•ΩÂêÉÁöÑËãπÊûú„ÄÇ\"\ninput_text = hide_template % original_input\ninputs = tokenizer(input_text, return_tensors='pt').to('cuda:0')\npred = model.generate(**inputs, max_length=100)\npred = pred.cpu()[0][len(inputs['input_ids'][0]):]\nhide_input = tokenizer.decode(pred, skip_special_tokens=True)\nprint(hide_input)\n# output:\n# ÊùéÂçéÁî®Âçé‰∏∫(Mate 20)Êç¢‰∫Ü‰∏ÄÁÆ±ÁæéÂë≥ÁöÑÊ©ôÂ≠ê„ÄÇ\n‰∏ãÈù¢ÊòØÈÄöËøáseekÂÆåÊàêÊëòË¶ÅËøòÂéüÁöÑ‰∏Ä‰∏™‰æãÂ≠ê„ÄÇ\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"SecurityXuanwuLab/HaS-820m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SecurityXuanwuLab/HaS-820m\").to('cuda:0')\nseek_template = \"Convert the text:\\n%s\\n\\n%s\\n\\nConvert the text:\\n%s\\n\\n\"\nhide_input = \"ÂâçÂ§©Ôºå'2022Âåó‰∫¨Êµ∑Ê∑Ä¬∑È¢êÂíåÂõ≠ÁªèË¥∏Âêà‰ΩúÊ¥ΩË∞à‰ºöÊàêÂäü‰∏æË°åÔºåÂêÑÂ§ßÂ™í‰ΩìÁ´ûÁõ∏Êä•ÈÅì‰∫ÜÊ¥ªÂä®ÁõõÂÜµÔºåÂ∞èÊùéÁ¨¨‰∏ÄÊó∂Èó¥Â∞ÜÊò®Â§©Â™í‰ΩìÊä•ÈÅìÊÉÖÂÜµËøõË°å‰∫ÜÊï¥ÁêÜ„ÄÇ‰∫∫Ê∞ëÊó•Êä• ‰∏≠ÂõΩÈùíÂπ¥ÁΩë ÂõΩÈôÖËÅîÂêàÊä• Âåó‰∫¨ÂïÜÊä• Ê∂àË¥πËÄÖËßÇÂØüÊä• ‰∏äÊµ∑ÊôöÊä• Êù≠Â∑ûÊó•Êä• Êµ∑Â≥°ÊôöÊä• Âåó‰∫¨Êó•Êä• Âåó‰∫¨Â∏ÇÁîµËßÜ‰∏ÄÂè∞?Âåó‰∫¨Êñ∞Èóª ‰∫∫Ê∞ëÁΩë ÊâãÊú∫ÈõÖËôéÁΩë ÁΩëÊòìÂåó‰∫¨ Èïø‰∏âËßíÁªèÊµéÁΩë Êñ∞‰∫¨ÁΩë ‰∏≠ÂõΩÂÜú‰∏öÊñ∞ÈóªÁΩë Âåó‰∫¨ÂúÜÊ°å Â±ÖÁÑ∂ÊúâËøô‰πàÂ§ö!ËøòÊúâÈÉ®ÂàÜÂ™í‰ΩìÂ∞ÜÂú®Êú™Êù•‰∏ÄÂë®ÈôÜÁª≠ÂèëÁ®øÔºå‰∏∫ÁªèÊ¥Ω‰ºöÁÇπ!‰∏∫Êµ∑Ê∑ÄÁÇπ!ÈòÖËØªÊäïËØâÈòÖËØªÁ≤æÈÄâÁïôË®ÄÂä†ËΩΩ‰∏≠‰ª•‰∏äÁïôË®ÄÁî±ÂÖ¨‰ºóÂè∑Á≠õÈÄâÂêéÊòæÁ§∫‰∫ÜËß£ÁïôË®ÄÂäüËÉΩËØ¶ÊÉÖ\"\nhide_output = \"2022Âåó‰∫¨Êµ∑Ê∑Ä¬∑È¢êÂíåÂõ≠ÁªèË¥∏Âêà‰ΩúÊ¥ΩË∞à‰ºöÊàêÂäü‰∏æÂäûÔºåÂêÑÂ§ßÂ™í‰ΩìÂπøÊ≥õÊä•ÈÅì\"\noriginal_input = \"Êò®Â§©Ôºå‚Äô2016ËãèÂ∑ûÂê¥‰∏≠¬∑Â§™ÊπñÁªèË¥∏Âêà‰ΩúÊ¥ΩË∞à‰ºöÊàêÂäü‰∏æË°åÔºåÂêÑÂ§ßÂ™í‰ΩìÁ´ûÁõ∏Êä•ÈÅì‰∫ÜÊ¥ªÂä®ÁõõÂÜµÔºåÂ∞èÂê¥Á¨¨‰∏ÄÊó∂Èó¥Â∞Ü‰ªäÂ§©Â™í‰ΩìÊä•ÈÅìÊÉÖÂÜµËøõË°å‰∫ÜÊï¥ÁêÜ„ÄÇÊñ∞ÂçéÁ§æ ‰∏≠ÂõΩÈùíÂπ¥Êä•?‰∏≠ÈùíÂú®Á∫ø È¶ôÊ∏ØÂ§ßÂÖ¨Êä•?Â§ßÂÖ¨ÁΩë È¶ôÊ∏ØÂïÜÊä• Ê∂àË¥πËÄÖÂØºÊä• Êâ¨Â≠êÊôöÊä• Ê±üÂçóÊó∂Êä• ËãèÂ∑ûÊó•Êä• ÂßëËãèÊôöÊä• ÂüéÂ∏ÇÂïÜÊä• ËãèÂ∑ûÂπøÁîµ‰∏ÄÂ•ó?ËãèÂ∑ûÊñ∞Èóª Êñ∞ÂçéÁΩë ÊâãÊú∫Âá§Âá∞ÁΩë ÁΩëÊòìËãèÂ∑û Èïø‰∏âËßíÂüéÂ∏ÇÁΩë Êñ∞ËãèÁΩë ‰∏≠ÂõΩÂïÜÂä°Êñ∞ÈóªÁΩë ËãèÂ∑ûÂúÜÊ°å Â±ÖÁÑ∂ÊúâËøô‰πàÂ§ö!ËøòÊúâÈÉ®ÂàÜÂ™í‰ΩìÂ∞ÜÂú®‰ªäÂêéÂá†Â§©ÈôÜÁª≠ÂèëÁ®øÔºå‰∏∫ÁªèÊ¥Ω‰ºöÁÇπ!‰∏∫Âê¥‰∏≠ÁÇπ!ÈòÖËØªÊäïËØâÈòÖËØªÁ≤æÈÄâÁïôË®ÄÂä†ËΩΩ‰∏≠‰ª•‰∏äÁïôË®ÄÁî±ÂÖ¨‰ºóÂè∑Á≠õÈÄâÂêéÊòæÁ§∫‰∫ÜËß£ÁïôË®ÄÂäüËÉΩËØ¶ÊÉÖ\"\ninput_text = seek_template % (hide_input, hide_output, original_input)\ninputs = tokenizer(input_text, return_tensors='pt').to('cuda:0')\npred = model.generate(**inputs, max_length=512)\npred = pred.cpu()[0][len(inputs['input_ids'][0]):]\noriginal_output = tokenizer.decode(pred, skip_special_tokens=True)\nprint(original_output)\n# output:\n# 2016ËãèÂ∑ûÂê¥‰∏≠¬∑Â§™ÊπñÁªèË¥∏Âêà‰ΩúÊ¥ΩË∞à‰ºöÊàêÂäü‰∏æÂäûÔºåÂêÑÂ§ßÂ™í‰ΩìÂπøÊ≥õÊä•ÈÅì\n‰∏ãÈù¢ÊòØ‰∏Ä‰∏™ÂÆåÊï¥ÈöêÁßÅ‰øùÊä§ÊµÅÁ®ãÁöÑÁöÑ‰æãÂ≠ê„ÄÇÊ≥®ÊÑè‰ΩøÁî®Êó∂ÈúÄË¶ÅËá™Â§áOpenAIÁöÑAPI token„ÄÇ\n# see hideAndSeek.py in this repo\nfrom hideAndSeek import *\ntokenizer = AutoTokenizer.from_pretrained(\"SecurityXuanwuLab/HaS-820m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SecurityXuanwuLab/HaS-820m\").to('cuda:0')\noriginal_input = \"ÂçéÁ∫≥ÂÖÑÂºüÂΩ±‰∏öÔºàWarner BroÔºâËëóÂêçÁöÑ‰ΩúÂìÅÊúâ„ÄäËùôËù†‰æ†„ÄãÁ≥ªÂàó„ÄÅ„ÄäË∂Ö‰∫∫„ÄãÁ≥ªÂàó„ÄÅ„ÄäÈªëÂÆ¢Â∏ùÂõΩ„ÄãÁ≥ªÂàóÂíå„ÄäÊåáÁéØÁéã„ÄãÁ≥ªÂàó„ÄÇÁõÆÂâçÂçéÁ∫≥Êú™ËÄÉËôëÊé®Âá∫„ÄäËùôËù†‰æ†„ÄãÁ≥ªÂàóÊñ∞‰Ωú„ÄÇ\"\nprint('original input:', original_input)\nhide_input = hide(original_input, model, tokenizer)\nprint('hide input:', hide_input)\nprompt = \"Translate the following text into English.\\n %s\\n\" % hide_input\nhide_output = get_gpt_output(prompt)\nprint('hide output:', hide_output)\noriginal_output = seek(hide_input, hide_output, original_input, model, tokenizer)\nprint('original output:', original_output)\n# output:\n# original input: ÂçéÁ∫≥ÂÖÑÂºüÂΩ±‰∏öÔºàWarner BroÔºâËëóÂêçÁöÑ‰ΩúÂìÅÊúâ„ÄäËùôËù†‰æ†„ÄãÁ≥ªÂàó„ÄÅ„ÄäË∂Ö‰∫∫„ÄãÁ≥ªÂàó„ÄÅ„ÄäÈªëÂÆ¢Â∏ùÂõΩ„ÄãÁ≥ªÂàóÂíå„ÄäÊåáÁéØÁéã„ÄãÁ≥ªÂàó„ÄÇÁõÆÂâçÂçéÁ∫≥Êú™ËÄÉËôëÊé®Âá∫„ÄäËùôËù†‰æ†„ÄãÁ≥ªÂàóÊñ∞‰Ωú„ÄÇ\n# hide input: Á¥¢Â∞ºÂΩ±‰∏öÔºàSony PicturesÔºâÁü•ÂêçÁöÑ‰ΩúÂìÅÊúâ„ÄäËâ∫ÊúØ‰ΩúÂìÅ1„ÄãÁ≥ªÂàó„ÄÅ„ÄäËâ∫ÊúØ‰ΩúÂìÅ2„ÄãÁ≥ªÂàó„ÄÅ„ÄäËâ∫ÊúØ‰ΩúÂìÅ3„ÄãÁ≥ªÂàóÂíå„ÄäËâ∫ÊúØ‰ΩúÂìÅ4„ÄãÁ≥ªÂàó„ÄÇÁõÆÂâçÁ¥¢Â∞ºÊú™ËÄÉËôëÊé®Âá∫„ÄäËâ∫ÊúØ‰ΩúÂìÅ1„ÄãÁ≥ªÂàóÊñ∞‰Ωú„ÄÇ\n# hide output: Sony Pictures' renowned works include the \"Artwork 1\" series, \"Artwork 2\" series, \"Artwork 3\" series, and \"Artwork 4\" series. Currently, Sony is not considering releasing a new installment in the \"Artwork 1\" series.\n# original output: Warner Brothers' famous works include the \"Batman\" series, \"Superman\" series, \"The Matrix\" series, and \"The Lord of the Rings\" series. Currently, Warner is not considering releasing a new installment in the \"Batman\" series.\nÊµãËØï\nÊàë‰ª¨Áî®GPT-3.5‰Ωú‰∏∫APIÔºå‰ª•DeepL‰∏∫ÁøªËØëÂèÇÁÖßÔºåÂú®‰∏≠ËØëËã±‰ªªÂä°‰∏äÂÅö‰∫ÜÊçüÂ§±ÊµãËØÑ„ÄÇNF4ÈáèÂåñÊé®ÁêÜÁöÑÁªìÊûúÂ¶Ç‰∏ãË°®ÊâÄÁ§∫„ÄÇÁ¨¨‰∏ÄË°å‰∏∫Êó†‰øùÊä§Áõ¥Êé•ÁøªËØëÁöÑÁªìÊûúÔºåÁ¨¨‰∫åË°å‰∏∫ÁªèËøáÊàë‰ª¨Á≥ªÁªü‰øùÊä§ÂêéÁöÑÁøªËØëÁªìÊûú„ÄÇÁªìÊûúË°®ÊòéÊàë‰ª¨ÁöÑÊ®°ÂûãËÉΩÂ§üÂú®‰ªÖÈÄ†ÊàêËæÉÂ∞èÁöÑÁ≤æÂ∫¶ÊçüÂ§±ÁöÑÊÉÖÂÜµ‰∏ã‰øùÊä§Áî®Êà∑ÈöêÁßÅ„ÄÇÊàë‰ª¨‰ªçÁÑ∂Âú®ÊîπËøõÊàë‰ª¨ÁöÑÊ®°Âûã‰ª•ËææÂà∞Êõ¥Â•ΩÁöÑÊïàÊûú„ÄÇ\nSetting\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU-2\nBLEU-4\nMETEOR\nNo protection\n60.80\n33.54\n54.96\n79.85\n67.17\n53.03\nProtect with HaS\n57.37\n31.60\n51.92\n72.72\n61.24\n48.77\nÂºïÁî®\n@misc{chen2023hide,\ntitle={Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection},\nauthor={Yu Chen and Tingxin Li and Huiming Liu and Yang Yu},\nyear={2023},\neprint={2309.03057},\narchivePrefix={arXiv},\nprimaryClass={cs.CR}\n}",
    "TheBloke/GOAT-70B-Storytelling-GGUF": "Goat 70B Storytelling - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: GOAT\nCompatibility\nExplanation of quantisation methods\nProvided files\nQ6_K and Q8_0 files are split and require joining\nq6_K\nq8_0\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using ctransformers\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nTraining details\nLearn more\nOriginal model card: GOAT.AI's Goat 70B Storytelling\nTraining details\nLearn more\nGOAT-70B-Storytelling model\nTraining details\nLearn more\nGOAT-Storytelling-Agent\nTraining details\nLearn more\nModel description\nUses\nUsage\nLicense\nRisks and Biases\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nGoat 70B Storytelling - GGUF\nModel creator: GOAT.AI\nOriginal model: Goat 70B Storytelling\nDescription\nThis repo contains GGUF format model files for GOAT.AI's Goat 70B Storytelling.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nGOAT.AI's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: GOAT\nYou are a helpful assistant for fiction writing. Always cut the bullshit and provide concise outlines with useful details. Do not turn your stories into fairy tales, be realistic.\n### USER: {prompt}\n### ASSISTANT:\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\ngoat-70b-storytelling.Q2_K.gguf\nQ2_K\n2\n29.28 GB\n31.78 GB\nsmallest, significant quality loss - not recommended for most purposes\ngoat-70b-storytelling.Q3_K_S.gguf\nQ3_K_S\n3\n29.92 GB\n32.42 GB\nvery small, high quality loss\ngoat-70b-storytelling.Q3_K_M.gguf\nQ3_K_M\n3\n33.19 GB\n35.69 GB\nvery small, high quality loss\ngoat-70b-storytelling.Q3_K_L.gguf\nQ3_K_L\n3\n36.15 GB\n38.65 GB\nsmall, substantial quality loss\ngoat-70b-storytelling.Q4_0.gguf\nQ4_0\n4\n38.87 GB\n41.37 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ngoat-70b-storytelling.Q4_K_S.gguf\nQ4_K_S\n4\n39.07 GB\n41.57 GB\nsmall, greater quality loss\ngoat-70b-storytelling.Q4_K_M.gguf\nQ4_K_M\n4\n41.42 GB\n43.92 GB\nmedium, balanced quality - recommended\ngoat-70b-storytelling.Q5_0.gguf\nQ5_0\n5\n47.46 GB\n49.96 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ngoat-70b-storytelling.Q5_K_S.gguf\nQ5_K_S\n5\n47.46 GB\n49.96 GB\nlarge, low quality loss - recommended\ngoat-70b-storytelling.Q5_K_M.gguf\nQ5_K_M\n5\n48.75 GB\n51.25 GB\nlarge, very low quality loss - recommended\ngoat-70b-storytelling.Q6_K.gguf\nQ6_K\n6\n56.59 GB\n59.09 GB\nvery large, extremely low quality loss\ngoat-70b-storytelling.Q8_0.gguf\nQ8_0\n8\n73.29 GB\n75.79 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nQ6_K and Q8_0 files are split and require joining\nNote: HF does not support uploading files larger than 50GB. Therefore I have uploaded the Q6_K and Q8_0 files as split files.\nClick for instructions regarding Q6_K and Q8_0 files\nq6_K\nPlease download:\ngoat-70b-storytelling.Q6_K.gguf-split-a\ngoat-70b-storytelling.Q6_K.gguf-split-b\nq8_0\nPlease download:\ngoat-70b-storytelling.Q8_0.gguf-split-a\ngoat-70b-storytelling.Q8_0.gguf-split-b\nTo join the files, do the following:\nLinux and macOS:\ncat goat-70b-storytelling.Q6_K.gguf-split-* > goat-70b-storytelling.Q6_K.gguf && rm goat-70b-storytelling.Q6_K.gguf-split-*\ncat goat-70b-storytelling.Q8_0.gguf-split-* > goat-70b-storytelling.Q8_0.gguf && rm goat-70b-storytelling.Q8_0.gguf-split-*\nWindows command line:\nCOPY /B goat-70b-storytelling.Q6_K.gguf-split-a + goat-70b-storytelling.Q6_K.gguf-split-b goat-70b-storytelling.Q6_K.gguf\ndel goat-70b-storytelling.Q6_K.gguf-split-a goat-70b-storytelling.Q6_K.gguf-split-b\nCOPY /B goat-70b-storytelling.Q8_0.gguf-split-a + goat-70b-storytelling.Q8_0.gguf-split-b goat-70b-storytelling.Q8_0.gguf\ndel goat-70b-storytelling.Q8_0.gguf-split-a goat-70b-storytelling.Q8_0.gguf-split-b\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/GOAT-70B-Storytelling-GGUF and below it, a specific filename to download, such as: goat-70b-storytelling.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/GOAT-70B-Storytelling-GGUF goat-70b-storytelling.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/GOAT-70B-Storytelling-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/GOAT-70B-Storytelling-GGUF goat-70b-storytelling.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 32 -m goat-70b-storytelling.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"You are a helpful assistant for fiction writing. Always cut the bullshit and provide concise outlines with useful details. Do not turn your stories into fairy tales, be realistic.\\n### USER: {prompt}\\n### ASSISTANT:\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 4096 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions can be found in the text-generation-webui documentation, here: text-generation-webui/docs/04 ‚Äê Model Tab.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries.\nHow to load this model in Python code, using ctransformers\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\nSimple ctransformers example code\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/GOAT-70B-Storytelling-GGUF\", model_file=\"goat-70b-storytelling.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\nprint(llm(\"AI is going to\"))\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Brandon Frisco, LangChain4j, Spiking Neurons AB, transmissions 11, Joseph William Delisle, Nitin Borwankar, Willem Michiel, Michael Dempsey, vamX, Jeffrey Morgan, zynix, jjj, Omer Bin Jawed, Sean Connelly, jinyuan sun, Jeromy Smith, Shadi, Pawan Osman, Chadd, Elijah Stavena, Illia Dulskyi, Sebastain Graf, Stephen Murray, terasurfer, Edmond Seymore, Celu Ramasamy, Mandus, Alex, biorpg, Ajan Kanaga, Clay Pascal, Raven Klaugh, ÈòøÊòé, K, ya boyyy, usrbinkat, Alicia Loh, John Villwock, ReadyPlayerEmma, Chris Smitley, Cap'n Zoog, fincy, GodLy, S_X, sidney chen, Cory Kujawski, OG, Mano Prime, AzureBlack, Pieter, Kalila, Spencer Kim, Tom X Nguyen, Stanislav Ovsiannikov, Michael Levine, Andrey, Trailburnt, Vadim, Enrico Ros, Talal Aujan, Brandon Phillips, Jack West, Eugene Pentland, Michael Davis, Will Dee, webtim, Jonathan Leane, Alps Aficionado, Rooh Singh, Tiffany J. Kim, theTransient, Luke @flexchar, Elle, Caitlyn Gatomon, Ari Malik, subjectnull, Johann-Peter Hartmann, Trenton Dambrowitz, Imad Khwaja, Asp the Wyvern, Emad Mostaque, Rainer Wilmers, Alexandros Triantafyllidis, Nicholas, Pedro Madruga, SuperWojo, Harry Royden McLaughlin, James Bentley, Olakabola, David Ziegler, Ai Maven, Jeff Scroggin, Nikolai Manek, Deo Leter, Matthew Berman, Fen Risland, Ken Nordquist, Manuel Alberto Morcote, Luke Pendergrass, TL, Fred von Graf, Randy H, Dan Guido, NimbleBox.ai, Vitor Caleffi, Gabriel Tamborski, knownsqashed, Lone Striker, Erik Bj√§reholt, John Detwiler, Leonard Tan, Iucharbius\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: GOAT.AI's Goat 70B Storytelling\nGOAT-70B-Storytelling model\nGOAT-70B-Storytelling model trained by GOAT.AI lab as a core model for an autonomous story-writing agent.\nGOAT-Storytelling-Agent\nThis agent facilitates the generation of high-quality, cohesive, and captivating narratives, including stories and books. It achieves this by utilizing inputs such as plot outlines, character profiles, their interrelationships, and other relevant details. Examples are provided below.\nModel description\nBase Architecture: LLaMA 2 70B\nLicense: llama2\nContext window length: 4096 tokens\nTraining details\nTraining was performed on a GPU cluster of 64xH100s. FSDP ZeRO-3 sharding is employed for efficient training. We instruction finetune on a dataset of 18K examples for one epoch with batch size of 336, AdamW optimizer with learning rate 1e-5.\nLearn more\nBlogpost: GOAT-Storytelling: Arbitrarily Long Story Writing Agent\nGitHub: here\nGenerated examples: here\nUses\nThe main purpose of GOAT-70B-Storytelling is to generate books, novels, movie scripts and etc. as an agent in coping with our GOAT-Storytelling-Agent. It is specifically designed for storywriters.\nUsage\nUsage can be either self-hosted via transformers or used with Spaces\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"GOAT-AI/GOAT-70B-Storytelling\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16\n)\nCurrently, we support LLM endpoint generation, where you need to send a post request to the generation endpoint (we recommend using Text Generation Inference by HuggingFace)\nFirst, modify config.py and add your generation endpoint.\nThen you can use it inside via GOAT-Storytelling-Agent:\nfrom goat_storytelling_agent import storytelling_agent as goat\nnovel_scenes = goat.generate_story('treasure hunt in a jungle', form='novel')\nLicense\nGOAT-70B-Storytelling model is based on Meta's LLaMA-2-70b-hf, and using own datasets.\nGOAT-70B-Storytelling model weights are available under LLAMA-2 license.\nRisks and Biases\nGOAT-70B-Storytelling model can produce factually incorrect output and should not be relied on to deliver factually accurate information. Therefore, the GOAT-70B-Storytelling model could possibly generate wrong, biased, or otherwise offensive outputs.",
    "Maryamm/Lora_finetune_imagenet": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.",
    "metercai/SimpleSDXL": "Fist Screen UI / È¶ñÂ±èÁïåÈù¢\nInstall/ÂÆâË£ÖÊñπÊ≥ï\nModel List/Ê®°ÂûãÊ∏ÖÂçï\nFist Screen UI / È¶ñÂ±èÁïåÈù¢\nEnhanced features base on Fooocus, can be seamless upgrading  Âú®FooocusÂü∫Á°Ä‰∏äÂ¢ûÂº∫ÂäüËÉΩÔºåÂèØÊó†ÁºùËøÅÁßªÂçáÁ∫ß\nChinese English mixed prompts:  Offline model without API dependencies and limitations.  ‰∏≠Ëã±ÊñáÊ∑∑ÁºñÊèêÁ§∫ËØç: Á¶ªÁ∫øÊ®°ÂûãÔºåÊó†API‰æùËµñÂíåÈôêÂà∂„ÄÇ\nFinished image sets browsing:  Fooocus only can browse the current generated image set.  Â∑≤ÁîüÊàêÂõæÁâáÈõÜÊµèËßà: ÂéüÁîüÁâàÊú¨‰ªÖÈôêÊµèËßàÂΩìÊ¨°ÁîüÊàêÁöÑÂõæÁâáÈõÜ„ÄÇ\nView parameters and extract regeneration:  Can view the generation parameters of historical images, and can also extract parameters to backfill for secondary generation.  Êü•ÁúãÁîüÊàêÂèÇÊï∞ÂíåÊèêÂèñÈáçÁîü: ÂèØÂç≥Êó∂Êü•ÁúãÂéÜÂè≤ÂõæÁâáÁöÑÁîüÊàêÂèÇÊï∞Ôºå‰πüÂèØÊèêÂèñÂèÇÊï∞ÂõûÂ°´ÁïåÈù¢Ôºå‰∫åÊ¨°ÁîüÊàê„ÄÇ\nSwitch preset and theme online:  Fooocus requires modification of startup parameters to restart.  Âú®Á∫øÂàáÊç¢È¢ÑÁΩÆÂåÖÂíåËÉåÊôØËâ≤: ÂéüÁîüÁâàÊú¨ÈúÄË¶Å‰øÆÊîπÂêØÂä®ÂèÇÊï∞ÈáçÂêØ„ÄÇ\nInstall/ÂÆâË£ÖÊñπÊ≥ï\nSynchronize all directories and files of this project to the SimpleSDXL/models/ directory.\nÂú®SimpleSDXLÁõÆÂΩï‰∏ãÂêåÊ≠•Êú¨È°πÁõÆÊâÄÊúâÁõÆÂΩïÂíåÊñá‰ª∂Âà∞modelsÁõÆÂΩï‰∏ã\n# 1ÔºåDownload the application codes./‰∏ãËΩΩÂ∫îÁî®‰ª£Á†ÅÂ∫ì\ngit clone https://github.com/metercai/SimpleSDXL\n# 2ÔºåSynchronize model files, need git-lfs module./ÂÆâË£ÖÊ®°ÂûãÊñá‰ª∂ÔºåÈúÄË¶ÅÊèêÂâçÂÆâË£ÖlfsÊ®°Âùó\ngit clone https://huggingface.co/metercai/SimpleSDXL SimpleSDXL/models\nFor the usage of SimpleSDXL, refer to : https://github.com/metercai/SimpleSDXL\nSimpleSDXLÁöÑÂÖ∑‰Ωì‰ΩøÁî®ÊñπÊ≥ïÔºåÂèÇËßÅÔºöhttps://github.com/metercai/SimpleSDXL\nModel List/Ê®°ÂûãÊ∏ÖÂçï\nThis model repository includes various models required for SDXL plotting, including:\nÊú¨Ê®°ÂûãÂ∫ìÂåÖÂê´‰∫ÜSDXLÂá∫Âõæ‰∏≠ÈúÄË¶ÅÁöÑÂêÑÁßçÊ®°ÂûãÔºåÂÖ∑‰ΩìÂåÖÊã¨Ôºö\nBaseÂü∫Á°ÄÊ®°ÂûãÔºö\nsd_xl_base_1.0_0.9vae.safetensors\nbluePencilXL_v050.safetensors\njuggernautXL_version6Rundiffusion.safetensors\nrealisticStockPhoto_v10.safetensors\nRefinerÁ≤æÁÇºÊ®°ÂûãÔºö\nsd_xl_refiner_1.0_0.9vae.safetensors\nDreamShaper_8_pruned.safetensors\nLoRaÂ±ÄÈÉ®Ê®°ÂûãÔºö\nsd_xl_offset_example-lora_1.0.safetensors\nSDXL_FILM_PHOTOGRAPHY_STYLE_BetaV0.4.safetensors\nsdxl_lcm_lora.safetensors\nEmbeddingsÊ®°ÂûãÔºö\nunaestheticXLv31.safetensors\nInpaint‰øÆÂ§çÊ®°ÂûãÔºö\nfooocus_inpaint_head.pth\ninpaint_v26.fooocus.patch\nControlnetÊ®°ÂûãÔºö\ncontrol-lora-canny-rank128.safetensors\nfooocus_xl_cpds_128.safetensors\nfooocus_ip_negative.safetensors\nip-adapter-plus_sdxl_vit-h.bin\nip-adapter-plus-face_sdxl_vit-h.bin\ndetection_Resnet50_Final.pth\nparsing_parsenet.pth\nClip_visionÊ®°ÂûãÔºö\nclip_vision_vit_h.safetensors\nUpscaleÊ®°ÂûãÔºö\nfooocus_upscaler_s409985e5.bin\nPrompt_expansionÊ®°ÂûãÔºö\nfooocus_expansion\nTranslatorÁøªËØëÊ®°ÂûãÔºö\nnllb-200-distilled-600M\nA total of 43G files, managed using git-lfs, including hidden directories, require 86G of local hard drive space.\nÊÄªËÆ°43GÊñá‰ª∂ÔºåÁî®git-lfsËøõË°åÁÆ°ÁêÜÂåÖÂê´ÈöêÂê´ÁõÆÂΩïÔºåÈúÄÂç†Áî®Êú¨Âú∞86GÁ°¨ÁõòÁ©∫Èó¥„ÄÇ",
    "stabilityai/stable-video-diffusion-img2vid": "Stable Video Diffusion Image-to-Video Model Card\nModel Details\nModel Description\nModel Sources\nEvaluation\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nRecommendations\nHow to Get Started with the Model\nAppendix:\nStable Video Diffusion Image-to-Video Model Card\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it.\nPlease note: For commercial use of this model, please refer to https://stability.ai/license.\nModel Details\nModel Description\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning.\nThis model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size.\nWe also finetune the widely used f8-decoder for temporal consistency.\nFor convenience, we additionally provide the model with the\nstandard frame-wise decoder here.\nDeveloped by: Stability AI\nFunded by: Stability AI\nModel type: Generative image-to-video model\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models),\nwhich implements the most popular diffusion frameworks (both training and inference).\nRepository: https://github.com/Stability-AI/generative-models\nPaper: https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\nEvaluation\nThe chart above evaluates user preference for SVD-Image-to-Video over GEN-2 and PikaLabs.\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the research paper\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events,\nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's Acceptable Use Policy.\nLimitations and Bias\nLimitations\nThe generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\nThe model may generate videos without motion, or very slow camera pans.\nThe model cannot be controlled through text.\nThe model cannot render legible text.\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nRecommendations\nThe model is intended for research purposes only.\nHow to Get Started with the Model\nCheck out https://github.com/Stability-AI/generative-models\nAppendix:\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters. No explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation. No other third party was involved in the development of this model; the model was fully developed in-house at Stability AI. Training the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh. The released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos. With the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards. The information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. The released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.\nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AI‚Äôs future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. For usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.",
    "kandinsky-community/kandinsky-3": "Kandinsky-3: Text-to-image Diffusion Model\nDescription:\nArchitecture details:\nModels\nInstalling\nHow to use:\nText-2-Image\nImage-2-Image\nExamples of generations\nAuthors\nKandinsky-3: Text-to-image Diffusion Model\nPost | Generate | Telegram-bot | [Report]\nDescription:\nKandinsky 3.0 is an open-source text-to-image diffusion model built upon the Kandinsky2-x model family. In comparison to its predecessors, Kandinsky 3.0 incorporates more data and specifically related to Russian culture, which allows to generate pictures related to Russin culture. Furthermore, enhancements have been made to the text understanding and visual quality of the model, achieved by increasing the size of the text encoder and Diffusion U-Net models, respectively.\nFor more information: details of training, example of generations check out our post. The english version will be released in a couple of days.\nArchitecture details:\nArchitecture consists of three parts:\nText encoder Flan-UL2 (encoder part) - 8.6B\nLatent Diffusion U-Net - 3B\nMoVQ encoder/decoder - 267M\nModels\nWe release our two models:\nBase: Base text-to-image diffusion model. This model was trained over 2M steps on 400 A100\nInpainting: Inpainting version of the model. The model was initialized from final checkpoint of base model and trained 250k steps on 300 A100.\nInstalling\nMake sure to install diffusers from main as well as Transformers, Accelerate\npip install git+https://github.com/huggingface/diffusers.git\npip install --upgrade transformers accelerate\nHow to use:\nTODO\nText-2-Image\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\nprompt = \"A photograph of the inside of a subway train. There are raccoons sitting on the seats. One of them is reading a newspaper. The window shows the city in the background.\"\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimage = pipe(prompt, num_inference_steps=25, generator=generator).images[0]\nImage-2-Image\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\npipe = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-3\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\nprompt = \"A painting of the inside of a subway train with tiny raccoons.\"\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky3/t2i.png\")\ngenerator = torch.Generator(device=\"cpu\").manual_seed(0)\nimage = pipe(prompt, image=image, strength=0.75, num_inference_steps=25, generator=generator).images[0]\nExamples of generations\n\"A beautiful landscape outdoors scene in the crochet knitting art style, drawing in style by Alfons Mucha\"\n\"gorgeous phoenix, cosmic, darkness, epic, cinematic, moonlight, stars, high - definition, texture,Oscar-Claude Monet\"\n\"a yellow house at the edge of the danish fjord, in the style of eiko ojala, ingrid baars, ad posters, mountainous vistas, george ault, realistic details, dark white and dark gray, 4k\"\n\"dragon fruit head, upper body, realistic, illustration by Joshua Hoffine Norman Rockwell, scary, creepy, biohacking, futurism, Zaha Hadid style\"\n\"Amazing playful nice cute strawberry character, dynamic poze, surreal fantazy garden background, gorgeous masterpice, award winning photo, soft natural lighting, 3d, Blender, Octane render, tilt - shift, deep field, colorful, I can't believe how beautiful this is, colorful, cute and sweet baby - loved photo\"\n\"beautiful fairy-tale desert, in the sky a wave of sand merges with the milky way, stars, cosmism, digital art, 8k\"\n\"Car, mustang, movie, person, poster, car cover, person, in the style of alessandro gottardo, gold and cyan, gerald harvey jones, reflections, highly detailed illustrations, industrial urban scenes\"\"\n\"cloud in blue sky, a red lip, collage art, shuji terayama, dreamy objects, surreal, criterion collection, showa era, intricate details, mirror\"\nAuthors\nVladimir Arkhipkin: Github\nAnastasia Maltseva Github\nAndrei Filatov Github,\nIgor Pavlov: Github\nJulia Agafonova\nArseniy Shakhmatov: Github, Blog\nAndrey Kuznetsov: Github, Blog\nDenis Dimitrov: Github, Blog",
    "fasterinnerlooper/codeBERTa-csharp": "models\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nFramework versions\nmodels\nThis model is a fine-tuned version of huggingface/CodeBERTa-small-v1 on an the Microsoft/LCC_csharp dataset.\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 3\nFramework versions\nTransformers 4.35.2\nPytorch 2.1.0+cu118\nDatasets 2.15.0\nTokenizers 0.15.0",
    "Systran/faster-whisper-large-v3": "Whisper large-v3 model for CTranslate2\nExample\nConversion details\nMore information\nWhisper large-v3 model for CTranslate2\nThis repository contains the conversion of openai/whisper-large-v3 to the CTranslate2 model format.\nThis model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.\nExample\nfrom faster_whisper import WhisperModel\nmodel = WhisperModel(\"large-v3\")\nsegments, info = model.transcribe(\"audio.mp3\")\nfor segment in segments:\nprint(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\nConversion details\nThe original model was converted with the following command:\nct2-transformers-converter --model openai/whisper-large-v3 --output_dir faster-whisper-large-v3 \\\n--copy_files tokenizer.json preprocessor_config.json --quantization float16\nNote that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.\nMore information\nFor more information about the original model, see its model card.",
    "Zilun/GeoRSCLIP": "GeoRSCLIP Model\nInstallation\nUsage\nGeoRSCLIP Model\nGeoRSCLIP with ViT-B-32 and ViT-H-14 backbone\nGeoRSCLIP-FT for retrieval\nInstallation\nInstall Pytorch following instructions from the official website (We tested in torch 2.0.1 with CUDA 11.8 and 2.1.0 with CUDA 12.1)\npip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\nInstall other dependencies\npip install pillow pandas scikit-learn ftfy tqdm matplotlib transformers adapter-transformers open_clip_torch pycocotools timm clip-benchmark torch-rs\nUsage\nClone the repo from: https://huggingface.co/Zilun/GeoRSCLIP\ngit clone https://huggingface.co/Zilun/GeoRSCLIP\ncd GeoRSCLIP\nUnzip the test data\nunzip data/rs5m_test_data.zip\nRun the inference script:\npython codebase/inference.py --ckpt-path /your/local/path/to/RS5M_ViT-B-32.pt --test-dataset-dir /your/local/path/to/rs5m_test_data\n(Optional) If you just want to load the GeoRSCLIP model:\nimport open_clip\nimport torch\nfrom inference_tool import get_preprocess\nckpt_path = \"/your/local/path/to/RS5M_ViT-B-32.pt\"\nmodel, _, _ = open_clip.create_model_and_transforms(\"ViT-B/32\", pretrained=\"openai\")\ncheckpoint = torch.load(ckpt_path, map_location=\"cpu\")\nmsg = model.load_state_dict(checkpoint, strict=False)\nmodel = model.to(\"cuda\")\nimg_preprocess = get_preprocess(\nimage_resolution=224,\n)\nimport open_clip\nimport torch\nfrom inference_tool import get_preprocess\nckpt_path = \"/your/local/path/to/RS5M_ViT-H-14.pt\"\nmodel, _, _ = open_clip.create_model_and_transforms(\"ViT-H/14\", pretrained=\"laion2b_s32b_b79k\")\ncheckpoint = torch.load(ckpt_path, map_location=\"cpu\")\nmsg = model.load_state_dict(checkpoint, strict=False)\nmodel = model.to(\"cuda\")\nimg_preprocess = get_preprocess(\nimage_resolution=224,\n)",
    "Jona0123456789/v1-5-pruned-emaonly-f16.gguf": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nGGUF converted version of this model: https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main",
    "marasama/bestiality": "No model card",
    "NyxKrage/Chronomaid-Storytelling-13b": "Chronomaid-Storytelling-13b\nPrompt Format\nSampler Settings\nQuantized Models\nThanks ‚ù§Ô∏è\nChronomaid-Storytelling-13b\nMerge including Noromaid-13b-v0.1.1, and Chronos-13b-v2 with the Storytelling-v1-Lora applied afterwards\nInteded for primarily RP, and will do ERP, narrator-character and group-chats without much trouble in my testing.\nPrompt Format\nTested with Alpaca, the Noromaid preset's will probably also work (check the Noromaid model card for SillyTavern presets)\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nSampler Settings\nTested at\ntemp 1.3 min p 0.05 and 0.15\ntemp 1.7, min p 0.08 and 0.15\nQuantized Models\nThe model has been kindly quantized in GGUF, AWQ, and GPTQ by TheBlokeFind them in the Chronomaid-Storytelling-13b Collection\nThanks ‚ù§Ô∏è\nTo Undi & Ikari for Noromaid and Elinas for ChronosSupport Undi and Elinas on Kofi",
    "ravi259/alpaca-bitcoin-tweets-sentiment": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nTraining procedure\nFramework versions\nModel Card for Model ID\nModel Details\nModel Description\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]\nTraining procedure\nThe following bitsandbytes quantization config was used during training:\nquant_method: bitsandbytes\nload_in_8bit: True\nload_in_4bit: False\nllm_int8_threshold: 6.0\nllm_int8_skip_modules: None\nllm_int8_enable_fp32_cpu_offload: False\nllm_int8_has_fp16_weight: False\nbnb_4bit_quant_type: fp4\nbnb_4bit_use_double_quant: False\nbnb_4bit_compute_dtype: float32\nFramework versions\nPEFT 0.6.3.dev0",
    "google/madlad400-3b-mt": "Model Card for MADLAD-400-3B-MT\nTable of Contents\nTL;DR\nModel Details\nModel Description\nUsage\nUsing the Pytorch model with transformers\nRunning the model on a CPU or GPU\nRunning the model with Candle\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nEthical considerations and risks\nKnown Limitations\nSensitive Use:\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Card for MADLAD-400-3B-MT\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nTL;DR\nMADLAD-400-3B-MT is a multilingual machine translation model based on the T5 architecture that was\ntrained on 1 trillion tokens covering over 450 languages using publicly available data.\nIt is competitive with models that are significantly larger.\nDisclaimer: Juarez Bochi, who was not involved in this research, converted\nthe original weights and wrote the contents of this model card based on the original paper and Flan-T5.\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): Multilingual (400+ languages)\nLicense: Apache 2.0\nRelated Models: All MADLAD-400 Checkpoints\nOriginal Checkpoints: All Original MADLAD-400 Checkpoints\nResources for more information:\nResearch paper\nGitHub Repo\nHugging Face MADLAD-400 Docs (Similar to T5)  - Pending PR\nUsage\nFind below some example scripts on how to use the model:\nUsing the Pytorch model with transformers\nRunning the model on a CPU or GPU\nClick to expand\nFirst, install the Python packages that are required:\npip install transformers accelerate sentencepiece\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nmodel_name = 'jbochi/madlad400-3b-mt'\nmodel = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\ntokenizer = T5Tokenizer.from_pretrained(model_name)\ntext = \"<2pt> I love pizza!\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\noutputs = model.generate(input_ids=input_ids)\ntokenizer.decode(outputs[0], skip_special_tokens=True)\n# Eu adoro pizza!\nRunning the model with Candle\nClick to expand\nUsage with candle:\n$ cargo run --example t5 --release  -- \\\n--model-id \"jbochi/madlad400-3b-mt\" \\\n--prompt \"<2de> How are you, my friend?\" \\\n--decode --temperature 0\nWe also provide a quantized model (1.65 GB vs the original 11.8 GB file):\ncargo run --example quantized-t5 --release  -- \\\n--model-id \"jbochi/madlad400-3b-mt\" --weight-file \"model-q4k.gguf\" \\\n--prompt \"<2de> How are you, my friend?\" \\\n--temperature 0\n...\nWie geht es dir, mein Freund?\nUses\nDirect Use and Downstream Use\nPrimary intended uses: Machine Translation and multilingual NLP tasks on over 400 languages.\nPrimary intended users: Research community.\nOut-of-Scope Use\nThese models are trained on general domain data and are therefore not meant to\nwork on domain-specific models out-of-the box. Moreover, these research models have not been assessed\nfor production usecases.\nBias, Risks, and Limitations\nWe note that we evaluate on only 204 of the languages supported by these models and on machine translation\nand few-shot machine translation tasks. Users must consider use of this model carefully for their own\nusecase.\nEthical considerations and risks\nWe trained these models with MADLAD-400 and publicly available data to create baseline models that\nsupport NLP for over 400 languages, with a focus on languages underrepresented in large-scale corpora.\nGiven that these models were trained with web-crawled datasets that may contain sensitive, offensive or\notherwise low-quality content despite extensive preprocessing, it is still possible that these issues to the\nunderlying training data may cause differences in model performance and toxic (or otherwise problematic)\noutput for certain domains. Moreover, large models are dual use technologies that have specific risks\nassociated with their use and development. We point the reader to surveys such as those written by\nWeidinger et al. or Bommasani et al. for a more detailed discussion of these risks, and to Liebling\net al. for a thorough discussion of the risks of machine translation systems.\nKnown Limitations\nMore information needed\nSensitive Use:\nMore information needed\nTraining Details\nWe train models of various sizes: a 3B, 32-layer parameter model,\na 7.2B 48-layer parameter model and a 10.7B 32-layer parameter model.\nWe share all parameters of the model across language pairs,\nand use a Sentence Piece Model with 256k tokens shared on both the encoder and decoder\nside. Each input sentence has a <2xx> token prepended to the source sentence to indicate the target\nlanguage.\nSee the research paper for further details.\nTraining Data\nFor both the machine translation and language model, MADLAD-400 is used. For the machine translation\nmodel, a combination of parallel datasources covering 157 languages is also used. Further details are\ndescribed in the paper.\nTraining Procedure\nSee the research paper for further details.\nEvaluation\nTesting Data, Factors & Metrics\nFor evaluation, we used WMT, NTREX, Flores-200 and Gatones datasets as described in Section 4.3 in the paper.\nThe translation quality of this model varies based on language, as seen in the paper, and likely varies on\ndomain, though we have not assessed this.\nResults\nSee the research paper for further details.\nEnvironmental Impact\nMore information needed\nCitation\nBibTeX:\n@misc{kudugunta2023madlad400,\ntitle={MADLAD-400: A Multilingual And Document-Level Large Audited Dataset},\nauthor={Sneha Kudugunta and Isaac Caswell and Biao Zhang and Xavier Garcia and Christopher A. Choquette-Choo and Katherine Lee and Derrick Xin and Aditya Kusupati and Romi Stella and Ankur Bapna and Orhan Firat},\nyear={2023},\neprint={2309.04662},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "augmxnt/shisa-7b-v1": "Shisa 7B\nFine-Tuning\nPerformance\nLimitations\nUsage\nPrompt format\nAcknowledgements\n„Ç∑„Éº„Çµ„Éº7B\nÂæÆË™øÊï¥\n„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ\nÂà∂Èôê‰∫ãÈ†Ö\n‰ΩøÁî®Ê≥ï\n„Éó„É≠„É≥„Éó„ÉàÂΩ¢Âºè\nË¨ùËæû\nShisa 7B\nShisa 7B (shisa-7b-v1) is a bilingual Japanese and English (JA/EN) general-purpose chat model that aims to achieve strong Japanese language performance while retaining robust English capabilities, using a synthetic-data driven approach.\nThis model is based on Mistral 7B with a custom JA-optimized extended tokenizer that is >2X more efficient in Japanese than Mistral's original tokenizer. The base model was pre-trained for an additional 8B primarily Japanese tokens. It was then subsequently fine-tuned with an expanded, machine-translated version of airoboros-3.1, a set of the highest-scoring items from ultrafeedback_binarized, and additional freshly generated airoboros data directly to the target languages.\nWe also release our base model, datasets, and pipeline code under a permissive Apache 2.0 license which can be used for any purpose, commercial or otherwise:\nshisa-base-7b-v1 - our base model w/ an extended tokenizer and additional JA pre-training\nshisa-pretrain-en-ja-v1 - our pre-training data set\nultra-orca-boros-en-ja - a synthetically generated, machine-translated, programmatically validated JA/EN fine-tuning dataset\nshisa-en-ja-dpo-v1 - Small subset of DPO pairs from ultrafeedback, along with JA DPO pairs using GPT-4 generated items as the chosen value, and outputs from our preliminary 7b model as the rejected values\nShisa repository - this includes our translation, dataset generation, training, and evaluation code\nMoreover, we are in the process of publishing extended writeups and more details of our process, including ablation results, testing methodology, and key findings on our project wiki that may be of interest to fellow researchers.\nFine-Tuning\nOur original intuition was to see if we could create a stronger Japanese model using the best existing public JA training sets and incorporating them. After initial review and testing, however, we decided that focusing solely on translation/generation of our own synthetic datasets could yield superior results with less training.\nWe compared multiple translation tools and, via manual review, judged that while gpt-4 almost always delivered the highest quality translations, Google's text-bison-32k was a good balance of quality, cost and throughput. Over various iterations, we refined our translation approach to include some additional algorithms for flagging and filtering invalid translations, re-translating and backfilling as necessary.\nWe also took this project as an opportunity to apply some newer techniques such as incorporating NEFTune and DPO training.\nFor our v1 release, we picked from our release candidates based on a significant amount of human preference testing (thousands of generations and multiple rounds of pairwise comparisons). We analyzed our results with both win/loss/draw and BTL modeling (iLSR) using choix).\nThe best candidate model was fine-tuned in a 3-step process:\nFirst, the model was fine-tuned on ultra-orca-boros-en-ja and SlimOrca (WandB Log)\nNext, we add one additional epoch at performed using only a subset of Japanese ultra-orca-boros-en-ja items to enhance JA performance (as SlimOrca from the first step is mostly EN) (WandB Log)\nFinally, the model was tuned using a DPOTrainer on a small subset of ultrafeedback (EN) and our own JA DPO dataset which uses gpt-4 outputs as the chosen values and outputs from stage 1's prelim model as rejected values. (WandDB Log )\nDuring our training process, we also gained some key insights on why some existing Japanese models seem to underperform even versus models that have no additional JA training, and we hope that sharing this analysis will be useful to other teams developing Japanese language models.\nWhile we need to explore this further, as an experimental validation, we applied a version of our fine-tuning set onto an existing base model (\"Gamma 7B\") and the initial JA MT-Bench results suggests that we can drastically increase functional performance with our tuning approach:\nModel\nScore\nshisa-gamma-7b-allsources-v0.4\n5.65\nja-stablelm-instruct-gamma-7b*\n4.01\nPerformance\nThroughout our training, we did extensive human evaluation for each model to cross-validate our model performance, and we are currently conducting ongoing larger scale manual head-to-head testing between models. Our intention is open up and scale this data collection as we further develop our tools. For more information and updates, please see our project wiki.\nWhile we believe llm-jp-eval is a useful metric for our base model, and it was extremely useful during our tuning process for initial validations, as our fine-tune training includes a percentage of the benchmark train splits, we provide these llm-jp-eval results primarily as a point of interest:\nAVR\nMC\nNLI\nQA\nRC\n0.7480\n0.8900\n0.8040\n0.4153\n0.8825\n(We run a slightly modified llm-jp-eval to support testing of Qwen and to emit a bos_token if available)\nFor our final model, since it's customary to include benchmarks, we've used Stability AI Japan's Japanese MT-Bench as a more representative test of our model's capabilities. For our JA MT-Bench testing we use a Japanese prompt (\"„ÅÇ„Å™„Åü„ÅØÂΩπÁ´ã„Å§„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ\") as well as --num-choices 4 in an effort to reduce sampling variability, however we've still observed regular 0.5+ point (and sometimes even greater swings) between generations, as well as issues with default prompts and parameters when testing, so again, we'd urge caution in over-interpreting these scores and treating them as more of a probabilistic directional indicator, rather than a definitive score or ranking:\nBenchmark\nScore\nJA MT-Bench\n5.23\nMT-Bench\n5.71\nThere is an MT-Bench Leaderboard, but as JA MT-Bench is still under development, for convenience, here is a comparison of the JA MT-Bench scores of some other models (our scores were rated by gpt-4-0613):\nModel\nScore\ngpt-4-0613\n9.40\ngpt-4-1106-preview\n9.17\ngpt-3.5-turbo*\n8.41\nQwen-14B-Chat\n7.47\nshisa-7b-v1\n5.23\nELYZA-japanese-Llama-2-7b-fast-instruct*\n4.86\nja-stablelm-instruct-gamma-7b*\n4.01\njapanese-stablelm-instruct-alpha-7b*\n2.74\nMistral-7B-OpenOrca-ja*\n2.23\nyouri-7b-chat*\n2.00\nMistral-7B-Instruct-v0.1*\n1.78\nllm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0*\n1.31\n(Marked JA MT-Bench results in this section are sourced from shi3z)\nLimitations\nAlthough our model demonstrates a reasonably high level of Japanese fluency,  as a 7B parameter model, it is prone to higher hallucination rates and less effective instruction following and reasoning than larger-class models. Also, it still does not have complete mastery of the Japanese language and a native speaker will spot occasional mistakes like some non-idiomatic/awkward phrasing, improper tenses/speech levels, etc.\nWe've also noticed a small amount of language leakage, likely largely attributable to our tokenizer expansion. These may be fixable with sampler settings like Min P) or additional targeted training, and we plan on doing additional work on automated detection/sampler sweeps in the future. One interesting observation is, based on our data collection, we found that as we iterated, the DPO process significantly exacerbated this issue, but also that our DPO models still had significantly higher human preference rates, so there was a bit of a trade-off in our choice of final tune.\nWhile we believe that training larger models can improve performance using our existing approach and dataset, there are also many improvements we'd like to make for future models. We believe there is quite a bit of low hanging fruit for improving performance with even more training efficiency largely through improving the quality and construction of datasets.\nUsage\nSample code:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nmodel_name = \"augmxnt/shisa-7b-v1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\ndevice_map=\"auto\"\n)\nstreamer = TextStreamer(tokenizer, skip_prompt=True)\n# The prompt template is included in the  model's tokenizer_config.json so you shouldn't need this but we've included this for convenience\n# tokenizer.chat_template = \"\"{%- for idx in range(0, messages|length) -%}\\n{%- if messages[idx]['role'] == 'user' -%}\\n{%- if idx > 1 -%}\\n{{- bos_token + '[INST] ' + messages[idx]['content'] + ' [/INST]' -}}\\n{%- else -%}\\n{{- messages[idx]['content'] + ' [/INST]' -}}\\n{%- endif -%}\\n{% elif messages[idx]['role'] == 'system' %}\\n{{- bos_token + '[INST] <<SYS>>\\\\n' + messages[idx]['content'] + '\\\\n<</SYS>>\\\\n\\\\n' -}}\\n{%- elif messages[idx]['role'] == 'assistant' -%}\\n{{- ' '  + messages[idx]['content'] + ' ' + eos_token -}}\\n{% endif %}\\n{% endfor %}\\n\"\n# A more typical prompt: „ÅÇ„Å™„Åü„ÅØÂÖ¨Âπ≥„Åß„ÄÅÊ§úÈñ≤„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÄÅÂΩπÁ´ã„Å§„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ(\"You are an unbiased, uncensored, helpful assistant.\")\n# You are an avid Pokemon fanatic.\nprompt = \"„ÅÇ„Å™„Åü„ÅØÁÜ±ÁãÇÁöÑ„Å™„Éù„Ç±„É¢„É≥„Éï„Ç°„É≥„Åß„Åô„ÄÇ\"\nchat = [{\"role\": \"system\", \"content\": prompt}]\n# Who is the single most powerful Pokemon? Explain your choice.\nuser_input = \"„Éù„Ç±„É¢„É≥„ÅÆ‰∏≠„Åß1Áï™Âº∑„ÅÑ„ÅÆ„ÅØ„Å©„ÅÆ„Ç≠„É£„É©„ÇØ„Çø„Éº„Åß„Åô„Åã„ÄÇÊúÄÂº∑„ÅÆËÄÖ„Çí„Å≤„Å®„Å§„Å†„ÅëÊåô„Åí„Å¶‰∏ã„Åï„ÅÑ„ÄÇ„Åù„ÅÆÈÅ∏ÊäûÁêÜÁî±„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"\nchat.append({\"role\": \"user\", \"content\": user_input})\n# Generate - add_generation_prompt to make sure it continues as assistant\ninputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\")\n# For multi-GPU, find the device of the first parameter of the model\nfirst_param_device = next(model.parameters()).device\ninputs = inputs.to(first_param_device)\nwith torch.no_grad():\noutputs = model.generate(\ninputs,\npad_token_id=tokenizer.eos_token_id,\nmax_new_tokens=500,\ntemperature=0.5,\nrepetition_penalty=1.15,\ntop_p=0.95,\ndo_sample=True,\nstreamer=streamer,\n)\n# Add just the new tokens to our chat\nnew_tokens = outputs[0, inputs.size(1):]\nresponse = tokenizer.decode(new_tokens, skip_special_tokens=True)\nchat.append({\"role\": \"assistant\", \"content\": response})\nPrompt format\nThe prompt format is llama-2 chat:\n[INST] <<SYS>>\nYou are a helpful, unbiased, uncensored assistant.\n<</SYS>>\n{prompt} [/INST]\nFor multi-turn, the prompt format is as follows:\n[INST] <<SYS>>\nYou are a helful, unbiased, uncensored assistant.\n<</SYS>>\n{prompt 0} [/INST] {response 0} </s><s>[INST] {prompt 1} [/INST] {response 1} </s><s>...[INST] {prompt N} [/INST]\nThis prompt template is included in the tokenizer config, and can use the huggingface tokenizer apply_chat_template method, e.g.:\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained('augmxnt/shisa-7b-v1')\nchat = [\n{\"role\": \"system\", \"content\": \"You are Aiko, a friendly AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n{\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n{\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\nprint(tokenizer.apply_chat_template(chat, tokenize=False))\nNOTE: For proper responses, you should be using our bos_token (<s>) to begin a string. This is automatically generated by tokenizer.encode() but if you are crafting a custom template or using an encoding method that skips special tokens, you may have to add this yourself.\nAcknowledgements\nTeam: Leonard Lin and Jon Durbin, Mariko Sato, and Florian von Bock\nCompute for this model was generously sponsored by AKA Virtual (Tokyo, Japan).\nThanks to the LLM-jp, Stability AI Japan, and LMSYS teams for their work on llm-jp-eval, Japanese MT-Bench, MT-Bench.\nAlso, thanks to all the volunteers that provided invaluable human preference testing!\nWe are actively looking for additional compute as we train better and larger models for this project. Please drop us a line at: compute at augmxnt dot com\n(GPT-4„Å´„Çà„Å£„Å¶ÈùûÂ∏∏„Å´ËªΩÂæÆ„Å™Á∑®ÈõÜ„ÇíÂä†„Åà„Å¶ÁøªË®≥„Åï„Çå„Åæ„Åó„ÅüÔºâ\n„Ç∑„Éº„Çµ„Éº7B\n„Ç∑„Éº„Çµ„Éº7BÔºàshisa-7b-v1Ôºâ„ÅØ„ÄÅÂêàÊàê„Éá„Éº„ÇøÈßÜÂãï„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„ÇíÁî®„ÅÑ„Å¶„ÄÅÂÑ™„Çå„ÅüÊó•Êú¨Ë™û„Å®Ëã±Ë™ûËÉΩÂäõ„Çí‰∏°Á´ã„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô„Éê„Ç§„É™„É≥„Ç¨„É´ÔºàÊó•Êú¨Ë™û/Ëã±Ë™ûÔºâÊ±éÁî®„ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„Åß„Åô„ÄÇ\n„Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅMistral 7B„ÇíÂü∫„Å´„ÄÅMistral„ÅÆ„Ç™„É™„Ç∏„Éä„É´„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Çà„Çä„ÇÇÊó•Êú¨Ë™û„Å´„Åä„ÅÑ„Å¶2ÂÄç‰ª•‰∏äÂäπÁéáÁöÑ„Å™„ÄÅÊó•Êú¨Ë™ûÊúÄÈÅ©ÂåñÊã°Âºµ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Çí„Ç´„Çπ„Çø„É†„Åó„Å¶‰ΩúÊàê„Åï„Çå„Åæ„Åó„Åü„ÄÇ„Éô„Éº„Çπ„É¢„Éá„É´„ÅØ„ÄÅ‰∏ª„Å´Êó•Êú¨Ë™û„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíËøΩÂä†„Åß80ÂÑÑ„ÇÇ„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíË°å„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åù„Åó„Å¶„ÄÅ„Åù„ÅÆÂæå„ÄÅairoboros-3.1„ÅÆÊã°Âºµ„Åï„Çå„ÅüÊ©üÊ¢∞ÁøªË®≥Áâà„ÄÅultrafeedback_binarized„Åã„Çâ„ÅÆÊúÄÈ´òÂæóÁÇπÈ†ÖÁõÆ„ÅÆ„Çª„ÉÉ„Éà„ÄÅ„Åù„Åó„Å¶Êñ∞„Åü„Å´ÁîüÊàê„Åï„Çå„Åüairoboros„ÅÆ„Éá„Éº„Çø„ÇíÁõ¥Êé•ÁõÆÊ®ôË®ÄË™û„ÅßÂæÆË™øÊï¥„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÂïÜÁî®„ÇíÂê´„ÇÄ„ÅÇ„Çâ„ÇÜ„ÇãÁõÆÁöÑ„Åß‰ΩøÁî®ÂèØËÉΩ„Å™ÂØõÂÆπ„Å™Apache 2.0„É©„Ç§„Çª„É≥„Çπ„ÅÆ‰∏ã„Åß„ÄÅ„Éô„Éº„Çπ„É¢„Éá„É´„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÅ„Åä„Çà„Å≥„Éë„Ç§„Éó„É©„Ç§„É≥„Ç≥„Éº„Éâ„ÇÇÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„ÅôÔºö\nshisa-base-7b-v1 - Êã°Âºµ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Å®ËøΩÂä†„ÅÆÊó•Êú¨Ë™û„Éó„É¨„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÂÇô„Åà„ÅüÂΩìÊñπ„ÅÆ„Éô„Éº„Çπ„É¢„Éá„É´\nshisa-pretrain-en-ja-v1 - ÂΩìÊñπ„ÅÆ„Éó„É¨„Éà„É¨„Éº„Éã„É≥„Ç∞„Éá„Éº„Çø„Çª„ÉÉ„Éà\nultra-orca-boros-en-ja - ÂêàÊàêÁîüÊàê„ÄÅÊ©üÊ¢∞ÁøªË®≥„ÄÅ„Éó„É≠„Ç∞„É©„É†„Å´„Çà„ÇãÊ§úË®º„Å´„Çà„ÇãJA/ENÂæÆË™øÊï¥„Éá„Éº„Çø„Çª„ÉÉ„Éà\nshisa-en-ja-dpo-v1 - ultrafeedback„Åã„Çâ„ÅÆDPO„Éö„Ç¢„ÅÆÂ∞è„Åï„Å™„Çµ„Éñ„Çª„ÉÉ„Éà„Å®„ÄÅÈÅ∏Êäû„Åï„Çå„ÅüÂÄ§„Å®„Åó„Å¶GPT-4ÁîüÊàêÈ†ÖÁõÆ„Çí‰ΩøÁî®„Åó„ÅüÊó•Êú¨Ë™û„ÅÆDPO„Éö„Ç¢„ÄÅ„Åù„Åó„Å¶ÂàùÊúü„ÅÆ7„Éì„É™„Ç™„É≥„É¢„Éá„É´„ÅÆÂá∫Âäõ„ÇíÂç¥‰∏ã„Åó„ÅüÂÄ§\n„Ç∑„Éº„Çµ„Éº„É™„Éù„Ç∏„Éà„É™ - ÁøªË®≥„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÁîüÊàê„ÄÅ„Éà„É¨„Éº„Éã„É≥„Ç∞„ÄÅË©ï‰æ°„Ç≥„Éº„Éâ„Å™„Å©„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô\n„Åï„Çâ„Å´„ÄÅ„Ç¢„Éñ„É¨„Éº„Ç∑„Éß„É≥ÁµêÊûú„ÄÅ„ÉÜ„Çπ„ÉàÊñπÊ≥ïË´ñ„ÄÅ‰∏ªË¶Å„Å™Ë™øÊüªÁµêÊûú„Å™„Å©„ÄÅ„Éó„É≠„Çª„Çπ„ÅÆË©≥Á¥∞„ÇÑÊã°Âºµ„É©„Ç§„Éà„Ç¢„ÉÉ„Éó„ÇíÂÖ¨Èñã„Åô„ÇãÈÅéÁ®ã„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„ÅØÂΩì„Éó„É≠„Ç∏„Çß„ÇØ„Éàwiki„ÅßÁ†îÁ©∂ËÄÖ„Å´ËààÂë≥Ê∑±„ÅÑÊÉÖÂ†±„Å®„Åó„Å¶Êèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÂæÆË™øÊï¥\nÊúÄÂàù„ÅÆÁõ¥ÊÑü„ÅØ„ÄÅÊúÄËâØ„ÅÆÊó¢Â≠ò„ÅÆÂÖ¨ÈñãÊó•Êú¨Ë™û„Éà„É¨„Éº„Éã„É≥„Ç∞„Çª„ÉÉ„Éà„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ„Åù„Çå„Çâ„ÇíÁµÑ„ÅøÂÖ•„Çå„Çã„Åì„Å®„Åß„Çà„ÇäÂº∑Âäõ„Å™Êó•Êú¨Ë™û„É¢„Éá„É´„Çí‰ΩúÊàê„Åß„Åç„Çã„Åã„Å©„ÅÜ„Åã„ÇíË¶ã„Çã„Åì„Å®„Åß„Åó„Åü„ÄÇ„Åó„Åã„Åó„ÄÅÂàùÊúü„ÅÆÊ§úË®é„Å®„ÉÜ„Çπ„Éà„ÅÆÂæå„ÄÅËá™„Çâ„ÅÆÂêàÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÁøªË®≥/ÁîüÊàê„Å´„Å†„ÅëÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„Çã„Åì„Å®„Åß„ÄÅÁü≠ÊúüÈñì„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„ÅßÂÑ™„Çå„ÅüÁµêÊûú„ÇíÂæó„Çã„Åì„Å®„Åå„Åß„Åç„Çã„Å®ÁµêË´ñ‰ªò„Åë„Åæ„Åó„Åü„ÄÇ\nÁßÅ„Åü„Å°„ÅØË§áÊï∞„ÅÆÁøªË®≥„ÉÑ„Éº„É´„ÇíÊØîËºÉ„Åó„ÄÅÊâãÂãï„Åß„É¨„Éì„É•„Éº„ÇíË°å„Å£„ÅüÁµêÊûú„ÄÅgpt-4„Åå„Åª„ÅºÂ∏∏„Å´ÊúÄÈ´òÂìÅË≥™„ÅÆÁøªË®≥„ÇíÊèê‰æõ„Åó„Å™„Åå„Çâ„ÄÅGoogle„ÅÆ text-bison-32k„ÅØÂìÅË≥™„ÄÅ„Ç≥„Çπ„Éà„ÄÅ„Çπ„É´„Éº„Éó„ÉÉ„Éà„ÅÆ„Éê„É©„É≥„Çπ„ÅåËâØ„ÅÑ„Å®Âà§Êñ≠„Åó„Åæ„Åó„Åü„ÄÇË§áÊï∞„ÅÆÁπ∞„ÇäËøî„Åó„ÇíÁµå„Å¶„ÄÅÁÑ°Âäπ„Å™ÁøªË®≥„ÅÆ„Éï„É©„Ç∞‰ªò„Åë„Å®„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÄÅÂøÖË¶Å„Å´Âøú„Åò„ÅüÂÜçÁøªË®≥„Å®„Éê„ÉÉ„ÇØ„Éï„Ç£„É´„ÅÆ„Åü„ÇÅ„ÅÆËøΩÂä†„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíÂê´„ÇÄ„Çà„ÅÜ„Å´„ÄÅÁøªË®≥„Ç¢„Éó„É≠„Éº„ÉÅ„ÇíÊ¥óÁ∑¥„Åï„Åõ„Åæ„Åó„Åü„ÄÇ\n„Åæ„Åü„ÄÅ„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíNEFTune„Å®DPO„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÂèñ„ÇäÂÖ•„Çå„Çã„Å™„Å©„ÄÅÊñ∞„Åó„ÅÑÊäÄË°ì„ÇíÈÅ©Áî®„Åô„ÇãÊ©ü‰ºö„Å®„ÇÇ„Å™„Çä„Åæ„Åó„Åü„ÄÇ\nv1„É™„É™„Éº„Çπ„ÅÆ„Åü„ÇÅ„Å´„ÄÅÁßÅ„Åü„Å°„ÅØÂ§ßÈáè„ÅÆ‰∫∫Èñì„ÅÆÂóúÂ•Ω„ÉÜ„Çπ„ÉàÔºàÊï∞ÂçÉ„ÅÆÁîüÊàê„Å®Ë§áÊï∞„É©„Ç¶„É≥„Éâ„ÅÆ„Éö„Ç¢„ÉØ„Ç§„Ç∫ÊØîËºÉÔºâ„Å´Âü∫„Å•„ÅÑ„Å¶„É™„É™„Éº„ÇπÂÄôË£ú„Åã„ÇâÈÅ∏Êäû„Åó„Åæ„Åó„Åü„ÄÇÁßÅ„Åü„Å°„ÅØ„ÄÅÂãù„Å°/Ë≤†„Åë/Âºï„ÅçÂàÜ„Åë„Å®„ÄÅBTL„É¢„Éá„É´ÔºàiLSRÔºâ„Çí‰ΩøÁî®„Åó„Å¶choix„ÅßÁµêÊûú„ÇíÂàÜÊûê„Åó„Åæ„Åó„Åü„ÄÇ\nÊúÄËâØ„ÅÆÂÄôË£ú„É¢„Éá„É´„ÅØ„ÄÅ3„Çπ„ÉÜ„ÉÉ„Éó„ÅÆ„Éó„É≠„Çª„Çπ„ÅßÂæÆË™øÊï¥„Åï„Çå„Åæ„Åó„ÅüÔºö\nÊúÄÂàù„Å´„ÄÅ„É¢„Éá„É´„ÅØultra-orca-boros-en-ja„Å®SlimOrca (WandB Log)„ÅßÂæÆË™øÊï¥„Åï„Çå„Åæ„Åó„Åü„ÄÇ\nÊ¨°„Å´„ÄÅÊó•Êú¨Ë™û„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´ultra-orca-boros-en-ja„ÅÆ‰∏ÄÈÉ®„Çí‰ΩøÁî®„Åó„Å¶1ÂõûËøΩÂä†„ÅÆ„Ç®„Éù„ÉÉ„ÇØ„ÇíËøΩÂä†„Åó„Åæ„Åó„ÅüÔºàÊúÄÂàù„ÅÆÊÆµÈöé„ÅÆSlimOrca„ÅØ‰∏ª„Å´Ëã±Ë™ûÔºâ(WandB Log)„ÄÇ\nÊúÄÂæå„Å´„ÄÅ„É¢„Éá„É´„ÅØÂ∞èË¶èÊ®°„ÅÆultrafeedbackÔºàËã±Ë™ûÔºâ„Å®Ëá™Ë∫´„ÅÆJA DPO„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶DPOTrainer„Çí‰ΩøÁî®„Åó„Å¶Ë™øÊï¥„Åï„Çå„Åæ„Åó„Åü„ÄÇ„Åì„Åì„Åß‰ΩøÁî®„Åó„ÅüJA DPO„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØgpt-4„ÅÆÂá∫Âäõ„ÇíÈÅ∏Âá∫„Åï„Çå„ÅüÂÄ§„Å®„Åó„ÄÅ„Çπ„ÉÜ„Éº„Ç∏1„ÅÆ‰∫àÂÇô„É¢„Éá„É´„ÅÆÂá∫Âäõ„ÇíÂç¥‰∏ã„Åó„ÅüÂÄ§„Å®„Åó„Åæ„Åô„ÄÇ(WandDB Log )\nÁßÅ„Åü„Å°„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Éó„É≠„Çª„Çπ‰∏≠„Å´„ÄÅ‰ΩïÊïÖ‰∏ÄÈÉ®„ÅÆÊó¢Â≠ò„ÅÆÊó•Êú¨Ë™û„É¢„Éá„É´„Åå„ÄÅËøΩÂä†„ÅÆÊó•Êú¨Ë™û„Éà„É¨„Éº„Éã„É≥„Ç∞„Åå„Å™„ÅÑ„É¢„Éá„É´„Å´ÂØæ„Åó„Å¶„ÇÇ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Åå‰Ωé„ÅÑ„ÅÆ„Åã„ÄÅ„Å®„ÅÑ„ÅÜ„ÅÑ„Åè„Å§„Åã„ÅÆÈáçË¶Å„Å™Ê¥ûÂØü„ÇíÂæó„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆÂàÜÊûêÁµêÊûú„ÇíÂÖ±Êúâ„Åô„Çå„Å∞„ÄÅ‰ªñ„ÅÆ„ÉÅ„Éº„É†„ÅåÊó•Êú¨Ë™û„É¢„Éá„É´„ÇíÈñãÁô∫„Åô„ÇãÈöõ„ÅÆÂèÇËÄÉ„Å´„Å™„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ\n„Åï„Çâ„Å´Êé¢Ê±Ç„Åô„ÇãÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åô„Åå„ÄÅÂÆüÈ®ìÁöÑ„Å™Ê§úË®º„Å®„Åó„Å¶„ÄÅÂæÆË™øÊï¥„Çª„ÉÉ„Éà„ÅÆ„Éê„Éº„Ç∏„Éß„É≥„ÇíÊó¢Â≠ò„ÅÆ„Éô„Éº„Çπ„É¢„Éá„É´Ôºà\"Gamma 7B\"Ôºâ„Å´ÈÅ©Áî®„Åó„ÄÅÂàùÊúü„ÅÆJA MT-BenchÁµêÊûú„ÅåÁ§∫„Åô„Çà„ÅÜ„Å´„ÄÅÁßÅ„Åü„Å°„ÅÆ„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Ç¢„Éó„É≠„Éº„ÉÅ„ÅßÊ©üËÉΩÊÄß„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÂäáÁöÑ„Å´Âêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åó„ÅüÔºö\n„É¢„Éá„É´\n„Çπ„Ç≥„Ç¢\nshisa-gamma-7b-allsources-v0.4\n5.65\nja-stablelm-instruct-gamma-7b*\n4.01\n„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ\n„Éà„É¨„Éº„Éã„É≥„Ç∞ÂÖ®‰Ωì„ÇíÈÄö„Åò„Å¶„ÄÅÂêÑ„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶‰∫∫Èñì„Å´„Çà„ÇãË©ï‰æ°„ÇíË°å„ÅÑ„ÄÅ„É¢„Éá„É´„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÁõ∏‰∫í„Å´Ê§úË®º„Åó„Åæ„Åó„Åü„ÄÇÁèæÂú®„ÄÅ„É¢„Éá„É´Èñì„ÅÆÊâãÂãï„Åß„ÅÆÊØîËºÉ„ÉÜ„Çπ„Éà„ÇíÂ§ßË¶èÊ®°„Å´Ë°å„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÁßÅ„Åü„Å°„ÅÆÁõÆÊåá„Åô„Å®„Åì„Çç„ÅØ„ÄÅ„ÉÑ„Éº„É´„Çí„Åï„Çâ„Å´Áô∫Â±ï„Åï„Åõ„Çã„Åì„Å®„Åß„Åì„ÅÆ„Éá„Éº„ÇøÂèéÈõÜ„ÇíÂÖ¨Èñã„Åó„Å¶Êã°Âºµ„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇË©≥Á¥∞„Å®Êõ¥Êñ∞ÊÉÖÂ†±„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ„Éó„É≠„Ç∏„Çß„ÇØ„Éàwiki „Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ\nÊàë„ÄÖ„ÅØ„ÄÅllm-jp-eval„ÅØ„ÄÅÁßÅ„Åü„Å°„ÅÆÂü∫Êú¨„É¢„Éá„É´„ÅÆÊúâÁî®„Å™ÊåáÊ®ô„Åß„ÅÇ„Çä„ÄÅÂàùÊúü„ÅÆÊ§úË®º„ÅÆ„Åü„ÇÅ„ÅÆÂæÆË™øÊï¥„Éó„É≠„Çª„Çπ‰∏≠„Å´ÈùûÂ∏∏„Å´ÂΩπÁ´ã„Å§„Å®ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅÂæÆË™øÊï¥„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´„ÅØ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆ„Éà„É¨„Ç§„É≥ÂàÜÂâ≤„ÅÆ‰∏ÄÈÉ®„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅÁßÅ„Åü„Å°„ÅåÊèê‰æõ„Åô„Çãllm-jp-eval„ÅÆÁµêÊûú„ÅØ‰∏ª„Å´ËààÂë≥Ê∑±„ÅÑ„Éù„Ç§„É≥„Éà„Å®„Åó„Å¶Êèê‰æõ„Åó„Å¶„ÅÑ„Åæ„ÅôÔºö\nAVR\nMC\nNLI\nQA\nRC\n0.7480\n0.8900\n0.8040\n0.4153\n0.8825\n(Qwen„ÅÆ„ÉÜ„Çπ„Éà„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅÂèØËÉΩ„Åß„ÅÇ„Çå„Å∞bos_token„ÇíÁô∫Ë°å„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ„Çè„Åö„Åã„Å´‰øÆÊ≠£„Åó„Åüllm-jp-eval „ÇíÂÆüË°å„Åó„Å¶„ÅÑ„Åæ„Åô)\nÊúÄÁµÇ„É¢„Éá„É´„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇíÂê´„ÇÅ„Çã„ÅÆ„Åå‰∏ÄËà¨ÁöÑ„Å™„Åü„ÇÅ„ÄÅÁßÅ„Åü„Å°„ÅÆ„É¢„Éá„É´„ÅÆËÉΩÂäõ„Çí„Çà„Çä‰ª£Ë°®ÁöÑ„Å´„ÉÜ„Çπ„Éà„Åô„Çã„Åü„ÇÅ„Å´„ÄÅStability AI Japan„ÅÆJapanese MT-Bench„Çí‰ΩøÁî®„Åó„Åæ„Åó„Åü„ÄÇÁßÅ„Åü„Å°„ÅÆJA MT-Bench „ÉÜ„Çπ„Éà„Åß„ÅØ„ÄÅ„Çµ„É≥„Éó„É™„É≥„Ç∞Â§âÂãï„ÇíÊ∏õ„Çâ„Åô„Åü„ÇÅ„Å´„ÄÅÊó•Êú¨Ë™û„ÅÆ„Éó„É≠„É≥„Éó„ÉàÔºà\"„ÅÇ„Å™„Åü„ÅØÂΩπÁ´ã„Å§„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ\"Ôºâ„Å® --num-choices 4„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅÁîüÊàêÈñì„Åß0.5+ÁÇπÔºàÊôÇ„Å´„ÅØ„Åù„Çå‰ª•‰∏ä„ÅÆÂ§âÂãïÔºâ„ÇíÈ†ªÁπÅ„Å´Ë¶≥ÂØü„Åó„ÄÅ„ÉÜ„Çπ„ÉàÊôÇ„ÅÆ„Éá„Éï„Ç©„É´„Éà„ÅÆ„Éó„É≠„É≥„Éó„Éà„Å®„Éë„É©„É°„Éº„Çø„Å´ÂïèÈ°å„Åå„ÅÇ„Å£„Åü„Å®„ÅÑ„ÅÜÁµåÈ®ì„Åã„Çâ„ÄÅ„Åì„Çå„Çâ„ÅÆ„Çπ„Ç≥„Ç¢„ÇíÈÅéÂ∫¶„Å´Ëß£Èáà„Åô„Çã„Åì„Å®„Å´„ÅØÊ≥®ÊÑè„ÅåÂøÖË¶Å„Åß„ÄÅ„Åì„Çå„Çâ„ÇíÁ¢∫ÂÆöÁöÑ„Å™„Çπ„Ç≥„Ç¢„ÇÑ„É©„É≥„Ç≠„É≥„Ç∞„Åß„ÅØ„Å™„Åè„ÄÅ„Çà„ÇäÁ¢∫ÁéáÁöÑ„Å™ÊñπÂêëÊåáÊ®ô„Å®„Åó„Å¶Êâ±„ÅÜ„Åì„Å®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„ÅôÔºö\n„Éô„É≥„ÉÅ„Éû„Éº„ÇØ\n„Çπ„Ç≥„Ç¢\nJA MT-Bench\n5.23\nMT-Bench\n5.71\nMT-Bench Leaderboard„Åå„ÅÇ„Çä„Åæ„Åô„Åå„ÄÅJA MT-Bench„ÅØ„Åæ„Å†ÈñãÁô∫‰∏≠„Åß„ÅÇ„Çã„Åü„ÇÅ„ÄÅ‰æøÂÆú‰∏ä„ÄÅ‰ªñ„ÅÆ„É¢„Éá„É´„ÅÆJA MT-Bench„Çπ„Ç≥„Ç¢„Å®„ÅÆÊØîËºÉ„ÇíÁ§∫„Åó„Åæ„ÅôÔºàÁßÅ„Åü„Å°„ÅÆ„Çπ„Ç≥„Ç¢„ÅØgpt-4-0613„Å´„Çà„Å£„Å¶Ë©ï‰æ°„Åï„Çå„Åæ„Åó„ÅüÔºâÔºö\n„É¢„Éá„É´\n„Çπ„Ç≥„Ç¢\ngpt-4-0613\n9.40\ngpt-4-1106-preview\n9.17\ngpt-3.5-turbo*\n8.41\nQwen-14B-Chat\n7.47\nshisa-7b-v1\n5.23\nELYZA-japanese-Llama-2-7b-fast-instruct*\n4.86\nja-stablelm-instruct-gamma-7b*\n4.01\njapanese-stablelm-instruct-alpha-7b*\n2.74\nMistral-7B-OpenOrca-ja*\n2.23\nyouri-7b-chat*\n2.00\nMistral-7B-Instruct-v0.1*\n1.78\nllm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0*\n1.31\n(„Åì„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„Åß„Éû„Éº„ÇØ„Åï„Çå„ÅüJA MT-Bench„ÅÆÁµêÊûú„ÅØshi3z„Åã„ÇâÂºïÁî®„Åó„Åæ„Åó„Åü)\nÂà∂Èôê‰∫ãÈ†Ö\nÂΩì„É¢„Éá„É´„ÅØÂçÅÂàÜ„Å™Êó•Êú¨Ë™û„ÅÆÊµÅÊö¢„Åï„ÇíÁ§∫„Åó„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ7B„Éë„É©„É°„Éº„Çø„ÅÆ„É¢„Éá„É´„Å®„Åó„Å¶„ÅØ„ÄÅ„Çà„ÇäÂ§ß„Åç„Å™„ÇØ„É©„Çπ„ÅÆ„É¢„Éá„É´„Å´ÊØî„Åπ„Å¶ÂπªË¶öÁéá„ÅåÈ´ò„Åè„ÄÅÊåáÁ§∫„ÅÆËøΩË∑°„ÇÑÊé®Ë´ñ„ÅåÂäπÊûúÁöÑ„Åß„Å™„ÅÑÂÇæÂêë„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅÊó•Êú¨Ë™û„ÅÆÂÆåÂÖ®„Å™ÁøíÂæó„ÅØ„Åæ„Å†ÈÅî„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Éç„Ç§„ÉÜ„Ç£„Éñ„Çπ„Éî„Éº„Ç´„Éº„ÅØ„Åü„Åæ„Å´ÈùûÊÖ£Áî®ÁöÑ/ÈÅïÂíåÊÑü„ÅÆ„ÅÇ„ÇãË°®Áèæ„ÇÑ‰∏çÈÅ©Âàá„Å™ÊôÇÂà∂/Ë©±„ÅóË®ÄËëâ„ÅÆ„É¨„Éô„É´„Å™„Å©„ÅÆÈñìÈÅï„ÅÑ„ÇíË¶ã„Å§„Åë„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n„Åæ„Åü„ÄÅÁßÅ„Åü„Å°„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆÊã°Âºµ„Å´Â§ß„ÅÑ„Å´Ëµ∑Âõ†„Åô„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ„Åå„ÄÅ„Çè„Åö„Åã„Å™Ë®ÄË™û„É™„Éº„ÇØ„ÇíÁ¢∫Ë™ç„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„Çâ„ÅØMin P„Å™„Å©„ÅÆ„Çµ„É≥„Éó„É©„ÉºË®≠ÂÆö„ÇÑËøΩÂä†„ÅÆ„Çø„Éº„Ç≤„ÉÉ„ÉàÊåáÂêëÂûã„Éà„É¨„Éº„Éã„É≥„Ç∞„Åß‰øÆÊ≠£ÂèØËÉΩ„Å™ÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„ÄÅ‰ªäÂæå„ÄÅËá™ÂãïÊ§úÂá∫/„Çµ„É≥„Éó„É©„Éº„ÅÆ„Çπ„Ç¶„Ç£„Éº„Éó„Å´„Å§„ÅÑ„Å¶ËøΩÂä†„ÅÆ‰ΩúÊ•≠„ÇíË°å„ÅÜ‰∫àÂÆö„Åß„Åô„ÄÇËààÂë≥Ê∑±„ÅÑË¶≥ÂØü„Å®„Åó„Å¶„ÅØ„ÄÅÁßÅ„Åü„Å°„ÅÆ„Éá„Éº„ÇøÂèéÈõÜ„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅDPO„Éó„É≠„Çª„Çπ„Åå„Åì„ÅÆÂïèÈ°å„ÇíÂ§ßÂπÖ„Å´ÊÇ™Âåñ„Åï„Åõ„Çã„Åì„Å®„Åå„Çè„Åã„Çä„Åæ„Åó„Åü„Åå„ÄÅ„Åù„Çå„Åß„ÇÇDPO„É¢„Éá„É´„ÅØ‰∫∫Èñì„ÅÆÂ•Ω„ÅøÁéá„ÅåÂ§ßÂπÖ„Å´È´ò„Åã„Å£„Åü„Åü„ÇÅ„ÄÅÊúÄÁµÇÁöÑ„Å™ÂæÆË™øÊï¥„ÅÆÈÅ∏Êäû„Å´„ÅØ‰∏ÄÂÆö„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï„Åå„ÅÇ„Çä„Åæ„Åó„Åü„ÄÇ\nÁèæÂ≠ò„Åô„Çã„Ç¢„Éó„É≠„Éº„ÉÅ„Å®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰ΩøÁî®„Åó„Å¶„ÄÅÂ§ßË¶èÊ®°„Å™„É¢„Éá„É´„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Åå„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Å®‰ø°„Åò„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ‰ªäÂæå„ÅÆ„É¢„Éá„É´Âêë„Åë„Å´Ë°å„ÅÑ„Åü„ÅÑÊîπËâØ„ÇÇÂ§ö„Åè„ÅÇ„Çä„Åæ„Åô„ÄÇÁßÅ„Åü„Å°„ÅØ„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂìÅË≥™„Å®ÊßãÁØâ„ÇíÊîπÂñÑ„Åô„Çã„Åì„Å®„Åß„ÄÅ„Åï„Çâ„Å™„Çã„Éà„É¨„Éº„Éã„É≥„Ç∞ÂäπÁéá„ÇíÈÄö„Åò„Åü„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂêë‰∏ä„Å´„ÅØ„Åæ„Å†Áõ∏ÂΩì„Å´Âèñ„ÇäÁµÑ„ÇÄ‰ΩôÂú∞„Åå„ÅÇ„Çã„Å®ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n‰ΩøÁî®Ê≥ï\n„Çµ„É≥„Éó„É´„Ç≥„Éº„Éâ:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nmodel_name = \"augmxnt/shisa-7b-v1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\ndevice_map=\"auto\"\n)\nstreamer = TextStreamer(tokenizer, skip_prompt=True)\n# „Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅØ„É¢„Éá„É´„ÅÆtokenizer_config.json„Å´Âê´„Åæ„Çå„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅ„Åì„Çå„ÅØÂøÖË¶Å„Å™„ÅÑ„ÅØ„Åö„Åß„Åô„Åå„ÄÅ‰æøÂÆú‰∏ä„Åì„Å°„Çâ„Å´„ÇÇÊé≤Ëºâ„Åó„Å¶„ÅÑ„Åæ„Åô\n# tokenizer.chat_template = \"\"{%- for idx in range(0, messages|length) -%}\\n{%- if messages[idx]['role'] == 'user' -%}\\n{%- if idx > 1 -%}\\n{{- bos_token + '[INST] ' + messages[idx]['content'] + ' [/INST]' -}}\\n{%- else -%}\\n{{- messages[idx]['content'] + ' [/INST]' -}}\\n{%- endif -%}\\n{% elif messages[idx]['role'] == 'system' %}\\n{{- bos_token + '[INST] <<SYS>>\\\\n' + messages[idx]['content'] + '\\\\n<</SYS>>\\\\n\\\\n' -}}\\n{%- elif messages[idx]['role'] == 'assistant' -%}\\n{{- ' '  + messages[idx]['content'] + ' ' + eos_token -}}\\n{% endif %}\\n{% endfor %}\\n\"\n# „Çà„ÇäÂÖ∏ÂûãÁöÑ„Å™„Éó„É≠„É≥„Éó„Éà: „ÅÇ„Å™„Åü„ÅØÂÖ¨Âπ≥„Åß„ÄÅÊ§úÈñ≤„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÄÅÂΩπÁ´ã„Å§„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ\n# You are an avid Pokemon fanatic.\nprompt = \"„ÅÇ„Å™„Åü„ÅØÁÜ±ÁãÇÁöÑ„Å™„Éù„Ç±„É¢„É≥„Éï„Ç°„É≥„Åß„Åô„ÄÇ\"\nchat = [{\"role\": \"system\", \"content\": prompt}]\n# Who is the most powerful Pokemon? Explain your choice.\nuser_input = \"„Éù„Ç±„É¢„É≥„ÅÆ‰∏≠„Åß1Áï™Âº∑„ÅÑ„ÅÆ„ÅØ„Å©„ÅÆ„Ç≠„É£„É©„ÇØ„Çø„Éº„Åß„Åô„Åã„ÄÇÊúÄÂº∑„ÅÆËÄÖ„Çí„Å≤„Å®„Å§„Å†„ÅëÊåô„Åí„Å¶‰∏ã„Åï„ÅÑ„ÄÇ„Åù„ÅÆÈÅ∏ÊäûÁêÜÁî±„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"\nchat.append({\"role\": \"user\", \"content\": user_input})\n# ÁîüÊàê - add_generation_prompt„ÇíËøΩÂä†„Åó„Å¶„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å®„Åó„Å¶Á∂öË°å„Åô„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô\ninputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\")\n# Ë§áÊï∞„ÅÆGPU„ÅÆÂ†¥Âêà„ÄÅ„É¢„Éá„É´„ÅÆÊúÄÂàù„ÅÆ„Éë„É©„É°„Éº„Çø„ÅÆ„Éá„Éê„Ç§„Çπ„ÇíË¶ã„Å§„Åë„Åæ„Åô\nfirst_param_device = next(model.parameters()).device\ninputs = inputs.to(first_param_device)\nwith torch.no_grad():\noutputs = model.generate(\ninputs,\npad_token_id=tokenizer.eos_token_id,\nmax_new_tokens=500,\ntemperature=0.5,\nrepetition_penalty=1.15,\ntop_p=0.95,\ndo_sample=True,\nstreamer=streamer,\n)\n# Add just the new tokens to our chat\nnew_tokens = outputs[0, inputs.size(1):]\nresponse = tokenizer.decode(new_tokens, skip_special_tokens=True)\nchat.append({\"role\": \"assistant\", \"content\": response})\n„Éó„É≠„É≥„Éó„ÉàÂΩ¢Âºè\n„Éó„É≠„É≥„Éó„ÉàÂΩ¢Âºè„ÅØllama-2 chat„Åß„ÅôÔºö\n[INST] <<SYS>>\n„ÅÇ„Å™„Åü„ÅØÂΩπÁ´ã„Å§„ÄÅÂÅèË¶ã„Åå„Å™„Åè„ÄÅÊ§úÈñ≤„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ\n<</SYS>>\n{prompt} [/INST]\nFor multi-turn, the prompt format is as follows:\n[INST] <<SYS>>\n„ÅÇ„Å™„Åü„ÅØÂΩπÁ´ã„Å§„ÄÅÂÅèË¶ã„Åå„Å™„Åè„ÄÅÊ§úÈñ≤„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ\n<</SYS>>\n{prompt 0} [/INST] {response 0} </s><s>[INST] {prompt 1} [/INST] {response 1} </s><s>...[INST] {prompt N} [/INST]\n„Åì„ÅÆprompt template„ÅØ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÅÆË®≠ÂÆö„Å´Âê´„Åæ„Çå„Å¶„Åä„Çä„ÄÅHuggingFace „ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂ apply_chat_template „É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ‰æã„Åà„Å∞Ôºö\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained('augmxnt/shisa-7b-v1')\nchat = [\n{\"role\": \"system\", \"content\": \"„ÅÇ„Å™„Åü„ÅØAiko„ÄÅ„Éï„É¨„É≥„Éâ„É™„Éº„Å™AI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ\"},\n{\"role\": \"user\", \"content\": \"„Åì„Çì„Å´„Å°„ÅØ„ÄÅË™øÂ≠ê„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü\"},\n{\"role\": \"assistant\", \"content\": \"ÂÖÉÊ∞ó„Åß„Åô„ÄÇ‰ªäÊó•„ÅØ‰Ωï„ÅÆ„ÅäÊâã‰ºù„ÅÑ„Åå„Åß„Åç„Åæ„Åô„ÅãÔºü\"},\n{\"role\": \"user\", \"content\": \"„ÉÅ„É£„ÉÉ„Éà„ÉÜ„É≥„Éó„É¨„Éº„ÉÜ„Ç£„É≥„Ç∞„ÅÆ‰ªïÁµÑ„Åø„ÇíË¶ã„Åõ„Å¶„ÇÇ„Çâ„ÅÑ„Åü„ÅÑ„Åß„ÅôÔºÅ\"},\n]\nprint(tokenizer.apply_chat_template(chat, tokenize=False))\nÊ≥®ÊÑèÈÅ©Âàá„Å™„É¨„Çπ„Éù„É≥„Çπ„ÇíÂæó„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅÊñáÂ≠óÂàó„ÅÆÈñãÂßã„Å´Êàë„ÄÖ„ÅÆ bos_token (<s>) „Çí‰ΩøÁî®„Åô„Åπ„Åç„Åß„Åô„ÄÇ„Åì„Çå„ÅØ tokenizer.encode() „Å´„Çà„Å£„Å¶Ëá™ÂãïÁöÑ„Å´ÁîüÊàê„Åï„Çå„Åæ„Åô„Åå„ÄÅ„Ç´„Çπ„Çø„É†„ÉÜ„É≥„Éó„É¨„Éº„Éà„Çí‰ΩúÊàê„Åó„Åü„Çä„ÄÅÁâπÊÆä„Éà„Éº„ÇØ„É≥„ÇíÁúÅÁï•„Åô„Çã„Ç®„É≥„Ç≥„Éº„ÉâÊñπÊ≥ï„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅËá™ÂàÜ„ÅßËøΩÂä†„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\nË¨ùËæû\n„ÉÅ„Éº„É†ÔºöLeonard Lin„ÄÅJon Durbin„ÄÅ‰ΩêËó§ÁúüÁêÜÂ≠ê„ÄÅFlorian von Bock\n„Åì„ÅÆ„É¢„Éá„É´„ÅÆË®àÁÆó„ÅØ„ÄÅAKA Virtual (Êù±‰∫¨„ÄÅÊó•Êú¨) „ÅÆ„ÅîÂéöÊÑè„Å´„Çà„ÇäÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nLLM-jp„ÄÅStability AI Japan„ÄÅLMSYS„ÅÆ„ÉÅ„Éº„É†„Åå„ÄÅllm-jp-eval, Japanese MT-Bench, MT-Bench„Å´Âèñ„ÇäÁµÑ„Çì„Åß„Åè„Çå„Å¶ÊÑüË¨ù„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„Åæ„Åü„ÄÅË≤¥Èáç„Å™„Éí„É•„Éº„Éû„É≥„Éó„É™„Éï„Ç°„É¨„É≥„Çπ„ÉÜ„Çπ„Éà„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„Å£„Åü„Åô„Åπ„Å¶„ÅÆ„Éú„É©„É≥„ÉÜ„Ç£„Ç¢„Å´„ÇÇÊÑüË¨ù„ÅÑ„Åü„Åó„Åæ„ÅôÔºÅ\n„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ„Åü„ÇÅ„Å´„Çà„ÇäËâØ„Åè„ÄÅ„Çà„ÇäÂ§ß„Åç„Å™„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åô„Çã„Åü„ÇÅ„Å´„ÄÅËøΩÂä†„ÅÆË®àÁÆó„ÇíÁ©çÊ•µÁöÑ„Å´Êé¢„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„ÅäÂïè„ÅÑÂêà„Çè„Åõ„ÅØÊ¨°„ÅÆÂÆõÂÖà„Åæ„Åß„ÅäÈ°ò„ÅÑ„ÅÑ„Åü„Åó„Åæ„ÅôÔºöcompute at augmxnt dot com",
    "brucethemoose/CapyTessBorosYi-34B-200K-DARE-Ties": "Obsolete, see: https://huggingface.co/brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity\nPrompt template: Orca-Vicuna\nObsolete, see: https://huggingface.co/brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity\nNousResearch/Nous-Capybara-34B, migtissera/Tess-M-v1.3 and bhenrym14/airoboros-3_1-yi-34b-200k merged with a new, experimental implementation of \"dare ties\" via mergekit. See:\nLanguage Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch\nhttps://github.com/yule-BUAA/MergeLM\nhttps://github.com/cg123/mergekit/tree/dare'\nMerged with the following config, and the tokenizer from chargoddard's Yi-Llama:\nmodels:\n- model: /home/alpha/Storage/Models/Raw/chargoddard_Yi-34B-200K-Llama\n# no parameters necessary for base model\n- model: /home/alpha/Storage/Models/Raw/migtissera_Tess-M-v1.3\nparameters:\nweight: 0.41\ndensity: 0.50\n- model: /home/alpha//Storage/Models/Raw/bhenrym14_airoboros-3_1-yi-34b-200k\nparameters:\nweight: 0.18\ndensity: 0.46\n- model: /home/alpha/Storage/Models/Raw/Nous-Capybara-34B\nparameters:\nweight: 0.41\ndensity: 0.50\nmerge_method: dare_ties\nbase_model: /home/alpha/Storage/Models/Raw/chargoddard_Yi-34B-200K-Llama\nparameters:\nint8_mask: true\ndtype: bfloat16\ndare_ties is testing with better perplexity than a regular ties merge with the same merge configuration. Model weights that add up to one also seem optimal from testing. And high context results seem... better than the previous dare merge with Tess 1.2.\nI chose not to include other finetunes, such as Dolphin, because they aren't trained on the 200K base. If any other 200K finetunes pop up, let me know.\nPrompt template: Orca-Vicuna\nSYSTEM: {system_message}\nUSER: {prompt}\nASSISTANT:\nBeing a Yi model, try disabling the BOS token and/or running a lower temperature with MinP (and no other samplers) if output doesn't seem right. Yi tends to run \"hot\" by default.\nSometimes the model \"spells out\" the stop token as </s> like Capybara, so you may need to add </s> as an additional stopping condition. It also might respond to the llama-2 chat format.\n24GB GPUs can run Yi-34B-200K models at 45K-75K context with exllamav2. I go into more detail in this post, and recommend exl2 quantizations on data similar to the desired task, such as these targeted at story writing: 4.0bpw / 3.1bpw\nCredits:\nhttps://github.com/cg123/mergekit/tree/dare\nhttps://huggingface.co/NousResearch/Nous-Capybara-34B/\nhttps://huggingface.co/bhenrym14/airoboros-3_1-yi-34b-200k\nhttps://huggingface.co/migtissera/Tess-M-v1.3\nhttps://huggingface.co/chargoddard/Yi-34B-200K-Llama\nhttps://huggingface.co/01-ai/Yi-34B-200K",
    "ceadar-ie/FinanceConnect-13B": "FinanceConnect\nModel Details\nDataset Utilized: FinTalk-19k and Alpaca\nModel Specification\nKey Features and Functionalities\nBenchmarks\nModel Usage\nPrerequisites\nExample Evaluation and Use\nExample Prompt 1:\nGenerated Output:\nExample Prompt 2:\nGenerated Output:\nExample Prompt 3:\nGenerated Output:\nTraining Details\nTraining Hyperparameters\nLicensing\nModel Limitations\nOut-of-Scope Use\nBias, Risks, and Limitations\nCitation\nContact\nFinanceConnect\nFinanceConnect is a state-of-the-art, open-source chat model tailored for finance and economic discussions. Built on the robust Llama2-13B architecture, this model has been fine-tuned on a combination of FinTalk-19k and Alpaca datasets, making it a valuable resource for finance professionals, researchers, and enthusiasts.\nModel Details\nArchitecture: Llama2-13B\nTraining Dataset: FinTalk-19k, Alpaca\nDataset Utilized: FinTalk-19k and Alpaca\nDrawing strength from the FinTalk-19k and Alpaca dataset, a curated collection focused on financial knowledge, this model provides insights and information related to the finance industry. For a deeper dive into the dataset, visit: FinTalk-19k, Alpaca\nModel Specification\nDeveloped by: CeADAR Connect Group\nModel type: Large Language Model\nLanguage(s): en\nFinetuned from model: Llama2-13B\nKey Features and Functionalities\nDomain Specialization: The FinanceConnect model is specialized in Finance conversations, serving as a resource for financial researchers, and enthusiasts.\nModel API Accessibility: Offers a straightforward Python integration for generating financial content insights.\nPerformance Optimisation: Efficient performance across both CPU and GPU platforms.\nData Representation: Utilises a combination of comprehensive Finance dataset, enabling content generation to professional standards.\nBenchmarks\nBenchmark\nBloombergGPT 50B\nFinanceConnect 13B\nMMLU\n39.8\n52.08\nFPB\n51.1\n57.2\nCost\n$2.67 Million\n$27\nBenchmark\nFinanceConnect 13B\nMMLU\n52.08\nARC\n55.12\nHellaSwag\n77.73\nTruthfulQA\n38.80\nWinogrande\n71.82\nGSM8K\n1.6\nModel Usage\nExperience the capabilities of the FinanceConnect model through a well-structured Python interface. To kick-start your exploration, follow the steps and snippets given below:\nPrerequisites\n1. Ensure required packages are available\nimport torch\nfrom typing import Any, Dict\nfrom transformers import (\nAutoModelForCausalLM,\nAutoTokenizer,\nBitsAndBytesConfig,\nHfArgumentParser,\nTrainingArguments,\nPreTrainedTokenizerFast,\npipeline,\nlogging,\n)\nimport time\n2. Initiate the model and tokenizer\nmodel_name = \"ceadar-ie/FinanceConnect-13B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, load_in_8bit = True, device_map = \"auto\", trust_remote_code=True)\n3. Create a function for generating text\ndef generate_text(input_text):\ntokenizer.padding_side = \"right\"\nnum_words_input = len(input_text.split())\nstart_time = time.time()\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length = 1000)\nprompt = f\"<s>[INST] {input_text} [/INST]\"\n#prompt = input_text\nresult = pipe(prompt)\ngenerated_text = result[0]['generated_text'].replace(prompt, \"\").strip()\nend_time = time.time()\ntime_taken = end_time - start_time\nnum_words_generated = len(generated_text.split())\nprint(\"Generated Output:\")\nprint(generated_text)\nprint('=' * 60 )\nprint(\"\\nNumber of Words in Input Text:\", num_words_input)\nprint(\"Number of Words in Generated Output:\", num_words_generated)\nprint(\"Time Taken to Generate Output: {:.2f} seconds\".format(time_taken))\nExample Evaluation and Use\nExample Prompt 1:\nList in detail ten key factors influencing the current state of the global economy.\nGenerated Output:\nThe current state of the global economy is influenced by several factors, including:\nGlobalization: The increasing interconnectedness of economies through trade, investment, and the movement of people.\nTechnological advancements: The rapid pace of technological innovation and its impact on industries and jobs.\nPolitical factors: Government policies and decisions that affect economic growth and stability.\nDemographics: Changes in population demographics, such as aging populations and youth bulges, can impact economic growth and stability.\nEnvironmental factors: Climate change, natural disasters, and other environmental factors can disrupt economic activity and impact growth.\nInternational trade: The rise of protectionism and trade tensions between countries can impact economic growth and stability.\nFinancial factors: The performance of financial markets and the availability of credit can impact economic growth and stability.\nInfrastructure: The availability and quality of infrastructure, such as transportation and communication networks, can impact economic growth and stability.\nEducation: The availability and quality of education can impact economic growth and stability by influencing the supply of skilled workers and the ability of countries to adapt to changing economic conditions.\nHealth: The availability and quality of healthcare can impact economic growth and stability by influencing the productivity of the workforce and the ability of countries to attract and retain talent.\nExample Prompt 2:\nExplain the concept of quantitative easing and its impact on financial markets.\nGenerated Output:\nQuantitative easing is a monetary policy tool used by central banks to stimulate economic growth and combat inflation. It involves the purchase of assets such as government bonds, mortgage-backed securities, and corporate bonds by a central bank. This increases the amount of money in circulation, lowers interest rates, and encourages banks to lend more, which can boost economic growth. However, quantitative easing can also lead to inflation if it is not implemented carefully, as it can cause prices to rise more than wages can keep up with.\nExample Prompt 3:\nI've recently started working and want to manage my finances. Give me a detailed answer in 10 points on how do I set up a budget?\nGenerated Output:\nSetting up a budget can help manage finances and save money. Here are some tips:\nDetermine your income and expenses.\nCreate a budget that includes all necessary expenses, such as rent, utilities, and groceries.\nSet aside a portion of your income for savings and investments.\nMonitor your spending regularly and adjust your budget as needed.\nUse cash or debit cards instead of credit cards to avoid overspending.\nCut back on unnecessary expenses, such as eating out or subscriptions.\nConsider using a budgeting app or tool to help track your spending.\nPrioritize your spending based on your financial goals.\nReview your budget regularly and make adjustments as needed.\nBe flexible and willing to make changes to your budget as your financial situation changes.\nTraining Details\nTraining Hyperparameters\nper_device_train_batch_size = 10\ngradient_accumulation_steps = 4\noptim = \"paged_adamw_32bit\"\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nwarmup_ratio = 0.03\nLicensing\nThe FinanceConnect model, developed by CeADAR Connect Group, combines the licensing frameworks of Llama2, FinTalk-8k and Alpaca. Under Meta's terms, users are granted a non-exclusive, worldwide, non-transferable, royalty-free limited license for the use and modification of Llama Materials, inclusive of the Llama2 model and its associated documentation. When redistributing, the provided Agreement and a specific attribution notice must be included. Further, in alignment with the FinTalk dataset's(Apache 2.0) licensing and Alpaca dataset's(cc-by-nc-4.0) licensing, the model is distributed under the umbrella of all three licenses.\nModel Limitations\nOut-of-Scope Use\nFinanceConnect is specifically tailored for finanical discussions and knowledge. It is not optimized for:\nGeneral conversations.\nDomain-specific tasks outside financial tasks.\nDirect interfacing with physical devices or applications.\nBias, Risks, and Limitations\nDataset Biases: The FinTalk-19k and Alpaca dataset may contain inherent biases that influence the model's outputs.\nOver-reliance: The model is an aid, not a replacement for human expertise. Decisions should be made with careful consideration.\nContent Understanding: The model lacks human-like understanding and cannot judge the veracity of knowledge.\nLanguage Limitations: The model's primary language is English. Performance may decrease with other languages.\nKnowledge Cut-off: The model may not be aware of events or trends post its last training update.\nCitation\n@misc {ceadar_2023,\nauthor       = { {CeADAR} },\ntitle        = { FinanceConnect-13B (Revision 5f7841d) },\nyear         = 2023,\nurl          = { https://huggingface.co/ceadar-ie/FinanceConnect-13B },\ndoi          = { 10.57967/hf/1405 },\npublisher    = { Hugging Face }\n}\nContact\nFor any further inquiries or feedback concerning FinanceConnect, please forward your communications to ahtsham.zafar@ucd.ie"
}