{
    "Qwen/Qwen2.5-1.5B": "Qwen2.5-1.5B\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nQwen2.5-1.5B\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the base 1.5B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 1.54B\nNumber of Paramaters (Non-Embedding): 1.31B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 12 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this ðŸ“‘ blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-0.5B-Instruct": "Qwen2.5-0.5B-Instruct\nIntroduction\nRequirements\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-0.5B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 0.5B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 0.49B\nNumber of Paramaters (Non-Embedding): 0.36B\nNumber of Layers: 24\nNumber of Attention Heads (GQA): 14 for Q and 2 for KV\nContext Length: Full 32,768 tokens and generation 8192 tokens\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nEvaluation & Performance\nDetailed evaluation results are reported in this ðŸ“‘ blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "meta-llama/Llama-3.2-3B": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.2 COMMUNITY LICENSE AGREEMENT\nLlama 3.2 Version Release Date: September 25, 2024\nâ€œAgreementâ€ means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\nâ€œDocumentationâ€ means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\nâ€œLicenseeâ€ or â€œyouâ€ means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entityâ€™s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\nâ€œLlama 3.2â€ means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\nâ€œLlama Materialsâ€ means, collectively, Metaâ€™s proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\nâ€œMetaâ€ or â€œweâ€ means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking â€œI Acceptâ€ below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Metaâ€™s intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display â€œBuilt with Llamaâ€ on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include â€œLlamaâ€ at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a â€œNoticeâ€ text file distributed as a part of such copies:  â€œLlama 3.2 is licensed under the Llama 3.2 Community License, Copyright Â© Meta Platforms, Inc. All Rights Reserved.â€iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licenseeâ€™s affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN â€œAS ISâ€ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use â€œLlamaâ€ (the â€œMarkâ€) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Metaâ€™s brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.b. Subject to Metaâ€™s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (â€œPolicyâ€).  The most recent copy of this policy can be found at https://www.llama.com/llama3_2/use-policy.\nProhibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\nViolate the law or othersâ€™ rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individualsâ€™ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\nPlease report any violation of this Policy, software â€œbug,â€ or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nUse with llama\nHardware and Software\nTraining Data\nQuantization\nQuantization Scheme\nQuantization-Aware Training and LoRA\nSpinQuant\nBenchmarks - English Text\nBase Pretrained Models\nInstruction Tuned Models\nMultilingual Benchmarks\nInference time\nResponsibility & Safety\nResponsible Deployment\nLlama 3.2 Instruct\nLlama 3.2 Systems\nNew Capabilities and Use Cases\nEvaluations\nCritical Risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel Developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext Length\nGQA\nShared Embeddings\nToken count\nKnowledge cutoff\nLlama 3.2 (text only)\nA new mix of publicly available online data.\n1B (1.23B)\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nYes\nUp to 9T tokens\nDecember 2023\n3B (3.21B)\nMultilingual Text\nMultilingual Text and code\nLlama 3.2 Quantized (text only)\nA new mix of publicly available online data.\n1B (1.23B)\nMultilingual Text\nMultilingual Text and code\n8k\nYes\nYes\nUp to 9T tokens\nDecember 2023\n3B (3.21B)\nMultilingual Text\nMultilingual Text and code\nSupported Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 Model Family: Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nFeedback: Instructions on how to provide feedback or comments on the model can be found in the Llama Models README. For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\nOut of Scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\nHow to use\nThis repository contains two versions of Llama-3.2-3B, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport torch\nfrom transformers import pipeline\nmodel_id = \"meta-llama/Llama-3.2-3B\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\npipe(\"The key to life is\")\nUse with llama\nPlease, follow the instructions in the repository.\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\nhuggingface-cli download meta-llama/Llama-3.2-3B --include \"original/*\" --local-dir Llama-3.2-3B\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use: Training utilized a cumulative of 916k GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 240 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nLogit Generation Time (GPU Hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 3.2 1B\n370k\n-\n700\n107\n0\nLlama 3.2 3B\n460k\n-\n700\n133\n0\nLlama 3.2 1B SpinQuant\n1.7\n0\n700\nNegligible**\n0\nLlama 3.2 3B SpinQuant\n2.4\n0\n700\nNegligible**\n0\nLlama 3.2 1B QLora\n1.3k\n0\n700\n0.381\n0\nLlama 3.2 3B QLora\n1.6k\n0\n700\n0.461\n0\nTotal\n833k\n86k\n240\n0\n** The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\nData Freshness: The pretraining data has a cutoff of December 2023.\nQuantization\nQuantization Scheme\nWe designed the current quantization scheme with the PyTorchâ€™s ExecuTorch inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\nAll linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\nThe classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\nSimilar to classification layer, an 8-bit per channel quantization is used for embedding layer.\nQuantization-Aware Training and LoRA\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\nSpinQuant\nSpinQuant was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\nBenchmarks - English Text\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\nBase Pretrained Models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.2 1B\nLlama 3.2 3B\nLlama 3.1 8B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n32.2\n58\n66.7\nAGIEval English\n3-5\naverage/acc_char\n23.3\n39.2\n47.8\nARC-Challenge\n25\nacc_char\n32.8\n69.1\n79.7\nReading comprehension\nSQuAD\n1\nem\n49.2\n67.7\n77\nQuAC (F1)\n1\nf1\n37.9\n42.9\n44.9\nDROP (F1)\n3\nf1\n28.0\n45.2\n59.5\nLong Context\nNeedle in Haystack\n0\nem\n96.8\n1\n1\nInstruction Tuned Models\nCapability\nBenchmark\n# Shots\nMetric\nLlama 3.2 1B bf16\nLlama 3.2 1B Vanilla PTQ**\nLlama 3.2 1B Spin Quant\nLlama 3.2 1B QLoRA\nLlama 3.2 3B bf16\nLlama 3.2 3B Vanilla PTQ**\nLlama 3.2 3B Spin Quant\nLlama 3.2 3B QLoRA\nLlama 3.1 8B\nGeneral\nMMLU\n5\nmacro_avg/acc\n49.3\n43.3\n47.3\n49.0\n63.4\n60.5\n62\n62.4\n69.4\nRe-writing\nOpen-rewrite eval\n0\nmicro_avg/rougeL\n41.6\n39.2\n40.9\n41.2\n40.1\n40.3\n40.8\n40.7\n40.9\nSummarization\nTLDR9+ (test)\n1\nrougeL\n16.8\n14.9\n16.7\n16.8\n19.0\n19.1\n19.2\n19.1\n17.2\nInstruction following\nIFEval\n0\nAvg(Prompt/Instruction acc Loose/Strict)\n59.5\n51.5\n58.4\n55.6\n77.4\n73.9\n73.5\n75.9\n80.4\nMath\nGSM8K (CoT)\n8\nem_maj1@1\n44.4\n33.1\n40.6\n46.5\n77.7\n72.9\n75.7\n77.9\n84.5\nMATH (CoT)\n0\nfinal_em\n30.6\n20.5\n25.3\n31.0\n48.0\n44.2\n45.3\n49.2\n51.9\nReasoning\nARC-C\n0\nacc\n59.4\n54.3\n57\n60.7\n78.6\n75.6\n77.6\n77.6\n83.4\nGPQA\n0\nacc\n27.2\n25.9\n26.3\n25.9\n32.8\n32.8\n31.7\n33.9\n32.8\nHellaswag\n0\nacc\n41.2\n38.1\n41.3\n41.5\n69.8\n66.3\n68\n66.3\n78.7\nTool Use\nBFCL V2\n0\nacc\n25.7\n14.3\n15.9\n23.7\n67.0\n53.4\n60.1\n63.5\n67.1\nNexus\n0\nmacro_avg/acc\n13.5\n5.2\n9.6\n12.5\n34.3\n32.4\n31.5\n30.1\n38.5\nLong Context\nInfiniteBench/En.QA\n0\nlongbook_qa/f1\n20.3\nN/A\nN/A\nN/A\n19.8\nN/A\nN/A\nN/A\n27.3\nInfiniteBench/En.MC\n0\nlongbook_choice/acc\n38.0\nN/A\nN/A\nN/A\n63.3\nN/A\nN/A\nN/A\n72.2\nNIH/Multi-needle\n0\nrecall\n75.0\nN/A\nN/A\nN/A\n84.7\nN/A\nN/A\nN/A\n98.8\nMultilingual\nMGSM (CoT)\n0\nem\n24.5\n13.7\n18.2\n24.4\n58.2\n48.9\n54.3\n56.8\n68.9\n**for comparison purposes only. Model not released.\nMultilingual Benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.2 1B\nLlama 3.2 1B Vanilla PTQ**\nLlama 3.2 1B Spin Quant\nLlama 3.2 1B QLoRA\nLlama 3.2 3B\nLlama 3.2 3B Vanilla PTQ**\nLlama 3.2 3B Spin Quant\nLlama 3.2 3B QLoRA\nLlama 3.1 8B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n39.8\n34.9\n38.9\n40.2\n54.5\n50.9\n53.3\n53.4\n62.1\nSpanish\n41.5\n36.0\n39.8\n41.8\n55.1\n51.9\n53.6\n53.6\n62.5\nItalian\n39.8\n34.9\n38.1\n40.6\n53.8\n49.9\n52.1\n51.7\n61.6\nGerman\n39.2\n34.9\n37.5\n39.6\n53.3\n50.0\n52.2\n51.3\n60.6\nFrench\n40.5\n34.8\n39.2\n40.8\n54.6\n51.2\n53.3\n53.3\n62.3\nHindi\n33.5\n30.0\n32.1\n34.0\n43.3\n40.4\n42.0\n42.1\n50.9\nThai\n34.7\n31.2\n32.4\n34.9\n44.5\n41.3\n44.0\n42.2\n50.3\n**for comparison purposes only. Model not released.\nInference time\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT + LoRA) with the BF16 baseline. The evaluation was done using the ExecuTorch framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\nCategory\nDecode (tokens/sec)\nTime-to-first-token (sec)\nPrefill (tokens/sec)\nModel size (PTE file size in MB)\nMemory size (RSS in MB)\n1B BF16 (baseline)\n19.2\n1.0\n60.3\n2358\n3,185\n1B SpinQuant\n50.2 (2.6x)\n0.3 (-76.9%)\n260.5 (4.3x)\n1083 (-54.1%)\n1,921 (-39.7%)\n1B QLoRA\n45.8 (2.4x)\n0.3 (-76.0%)\n252.0 (4.2x)\n1127 (-52.2%)\n2,255 (-29.2%)\n3B BF16 (baseline)\n7.6\n3.0\n21.2\n6129\n7,419\n3B SpinQuant\n19.7 (2.6x)\n0.7 (-76.4%)\n89.7 (4.2x)\n2435 (-60.3%)\n3,726 (-49.8%)\n3B QLoRA\n18.5 (2.4x)\n0.7 (-76.1%)\n88.8 (4.2x)\n2529 (-58.7%)\n4,060 (-45.3%)\n(*) The performance measurement is done using an adb binary-based approach.\n(**) It is measured on an Android OnePlus 12 device.\n(***) Time-to-first-token (TTFT)  is measured with prompt length=64\nFootnote:\nDecode (tokens/second) is for how quickly it keeps generating. Higher is better.\nTime-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.\nPrefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better\nModel size - how big is the model, measured by, PTE file, a binary file format for ExecuTorch\nRSS size - Memory usage in resident set size (RSS)\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\nProvide protections for the community to help prevent the misuse of our models\nResponsible Deployment\nApproach: Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Metaâ€™s Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driverâ€™s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide.\nLlama 3.2 Instruct\nObjective: Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 paper.\nFine-Tuning Data: We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weâ€™ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone: Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nLlama 3.2 Systems\nSafety as a System: Large language models, including Llama 3.2, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew Capabilities and Use Cases\nTechnological Advancement: Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see Llama 3.1 Model Card, as the same considerations apply here as well.\nConstrained Environments: Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\nEvaluations\nScaled Evaluations: We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\nRed Teaming: We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n2. Child Safety: Child Safety risk assessments were conducted using a team of experts, to assess the modelâ€™s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber Attacks: For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2â€™s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\nCommunity\nIndustry Partnerships: Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nGrants: We also set up the Llama Impact Grants program to identify and support the most compelling applications of Metaâ€™s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nReporting: Finally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nValues: The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nTesting: Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2â€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "openai/whisper-large-v3-turbo": "Whisper\nUsage\nAdditional Speed & Memory Improvements\nChunked Long-Form\nModel details\nFine-Tuning\nEvaluated Use\nTraining Data\nPerformance and Limitations\nBroader Implications\nBibTeX entry and citation info\nWhisper\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper\nRobust Speech Recognition via Large-Scale Weak Supervision by Alec Radford\net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many\ndatasets and domains in a zero-shot setting.\nWhisper large-v3-turbo is a finetuned version of a pruned Whisper large-v3. In other words, it's the exact same model, except that the number of decoding layers have reduced from 32 to 4.\nAs a result, the model is way faster, at the expense of a minor quality degradation. You can find more details about it in this GitHub discussion.\nDisclaimer: Content for this model card has partly been written by the ðŸ¤— Hugging Face team, and partly copied and\npasted from the original model card.\nUsage\nWhisper large-v3-turbo is supported in Hugging Face ðŸ¤— Transformers. To run the model, first install the Transformers\nlibrary. For this example, we'll also install ðŸ¤— Datasets to load toy audio dataset from the Hugging Face Hub, and\nðŸ¤— Accelerate to reduce the model loading time:\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\nThe model can be used with the pipeline\nclass to transcribe audios of arbitrary length:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"openai/whisper-large-v3-turbo\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\nresult = pipe(\"audio.mp3\")\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the batch_size parameter:\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous\ntokens. The following example demonstrates how to enable these heuristics:\ngenerate_kwargs = {\n\"max_new_tokens\": 448,\n\"num_beams\": 1,\n\"condition_on_prev_tokens\": False,\n\"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n\"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n\"logprob_threshold\": -1.0,\n\"no_speech_threshold\": 0.6,\n\"return_timestamps\": True,\n}\nresult = pipe(sample, generate_kwargs=generate_kwargs)\nWhisper predicts the language of the source audio automatically. If the source audio language is known a-priori, it\ncan be passed as an argument to the pipeline:\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\nBy default, Whisper performs the task of speech transcription, where the source audio language is the same as the target\ntext language. To perform speech translation, where the target text is in English, set the task to \"translate\":\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the return_timestamps argument:\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\nAnd for word-level timestamps:\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription\nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\nFor more control over the generation parameters, use the model + processor API directly:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"openai/whisper-large-v3-turbo\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\ninputs = processor(\nsample[\"array\"],\nsampling_rate=sample[\"sampling_rate\"],\nreturn_tensors=\"pt\",\ntruncation=False,\npadding=\"longest\",\nreturn_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\ngen_kwargs = {\n\"max_new_tokens\": 448,\n\"num_beams\": 1,\n\"condition_on_prev_tokens\": False,\n\"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n\"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n\"logprob_threshold\": -1.0,\n\"no_speech_threshold\": 0.6,\n\"return_timestamps\": True,\n}\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\nprint(pred_text)\nAdditional Speed & Memory Improvements\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM\nrequirements.\nChunked Long-Form\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\nSequential: uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\nChunked: splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\nThe sequential long-form algorithm should be used in either of the following scenarios:\nTranscription accuracy is the most important factor, and speed is less of a consideration\nYou are transcribing batches of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\nConversely, the chunked algorithm should be used when:\nTranscription speed is the most important factor\nYou are transcribing a single long audio file\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the chunk_length_s\nparameter to the pipeline. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long\naudio files, pass the argument batch_size:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"openai/whisper-large-v3-turbo\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nchunk_length_s=30,\nbatch_size=16,  # batch size for inference - set based on your device\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nTorch compile\nThe Whisper forward pass is compatible with torch.compile\nfor 4.5x speed-ups.\nNote: torch.compile is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 âš ï¸\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\ntorch.set_float32_matmul_precision(\"high\")\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"openai/whisper-large-v3-turbo\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\nwith sdpa_kernel(SDPBackend.MATH):\nresult = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\nresult = pipe(sample.copy())\nprint(result[\"text\"])\nFlash Attention 2\nWe recommend using Flash-Attention 2 if your GPU supports it and you are not using torch.compile.\nTo do so, first install Flash Attention:\npip install flash-attn --no-build-isolation\nThen pass attn_implementation=\"flash_attention_2\" to from_pretrained:\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")\nTorch Scale-Product-Attention (SDPA)\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch scaled dot-product attention (SDPA).\nThis attention implementation is activated by default for PyTorch versions 2.1.1 or greater. To check\nwhether you have a compatible PyTorch version, run the following Python code snippet:\nfrom transformers.utils import is_torch_sdpa_available\nprint(is_torch_sdpa_available())\nIf the above returns True, you have a valid version of PyTorch installed and SDPA is activated by default. If it\nreturns False, you need to upgrade your PyTorch version according to the official instructions\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying\nattn_implementation=\"sdpa\" as follows:\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")\nFor more information about how to use the SDPA refer to the Transformers SDPA documentation.\nModel details\nWhisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English\nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech\ntranslation. For speech recognition, the model predicts transcriptions in the same language as the audio. For speech\ntranslation, the model predicts transcriptions to a different language to the audio.\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only\nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints\nare available on the Hugging Face Hub. The\ncheckpoints are summarised in the following table with links to the models on the Hub:\nSize\nParameters\nEnglish-only\nMultilingual\ntiny\n39 M\nâœ“\nâœ“\nbase\n74 M\nâœ“\nâœ“\nsmall\n244 M\nâœ“\nâœ“\nmedium\n769 M\nâœ“\nâœ“\nlarge\n1550 M\nx\nâœ“\nlarge-v2\n1550 M\nx\nâœ“\nlarge-v3\n1550 M\nx\nâœ“\nlarge-v3-turbo\n809 M\nx\nâœ“\nFine-Tuning\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However,\nits predictive capabilities can be improved further for certain languages and tasks through fine-tuning. The blog\npost Fine-Tune Whisper with ðŸ¤— Transformers provides a step-by-step\nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\nEvaluated Use\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\nTraining Data\nNo information provided.\nPerformance and Limitations\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in the paper accompanying this release.\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in the paper. It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\nBroader Implications\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\nBibTeX entry and citation info\n@misc{radford2022whisper,\ndoi = {10.48550/ARXIV.2212.04356},\nurl = {https://arxiv.org/abs/2212.04356},\nauthor = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\ntitle = {Robust Speech Recognition via Large-Scale Weak Supervision},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "briaai/RMBG-2.0": "Fill in this form to immediatly access the model for non commercial use\nBria AI Model weights are open source for non commercial use only, per the provided license.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nBRIA Background Removal v2.0 Model Card\nModel Details\nModel Description\nTraining data\nDistribution of images:\nQualitative Evaluation\nArchitecture\nUsage\nBRIA Background Removal v2.0 Model Card\nðŸ’œ Bria AIÂ Â  | Â Â ðŸ¤— Hugging Face Â Â  | Â Â  ðŸ“‘ Blog\nðŸ–¥ï¸ DemoÂ Â | Â Â  Github\nRMBG v2.0 is our new state-of-the-art background removal model significantly improves RMBG v1.4. The model is designed to effectively separate foreground from background in a range of\ncategories and image types. This model has been trained on a carefully selected dataset, which includes:\ngeneral stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale.\nThe accuracy, efficiency, and versatility currently rival leading source-available models.\nIt is ideal where content safety, legally licensed datasets, and bias mitigation are paramount.\nDeveloped by BRIA AI, RMBG v2.0 is available as a source-available model for non-commercial use.\nGet Access\nBria RMBG2.0 is availabe everywhere you build, either as source-code and weights, ComfyUI nodes or API endpoints.\nPurchase: for commercial license simply click Here.\nAPI Endpoint: Bria.ai, fal.ai, Replicate\nComfyUI: Use it in workflows\nGitHub: github.com/Bria-AI/RMBG-2.0\nFor more information, please visit our website.\nJoin our Discord community for more information, tutorials, tools, and to connect with other users!\nCLICK HERE FOR A DEMO\nModel Details\nModel Description\nDeveloped by: BRIA AI\nModel type: Background Removal\nLicense: Creative Commons Attributionâ€“Non-Commercial (CC BY-NC 4.0)\nThe model is released under a CC BY-NC 4.0 license for non-commercial use.\nCommercial use is subject to a commercial agreement with BRIA. Available here\nPurchase: to purchase a commercial license simply click Here.\nModel Description: BRIA RMBG-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. The model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. This non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines.\nBRIA: Resources for more information: BRIA AI\nTraining data\nBria-RMBG model was trained with over 15,000 high-quality, high-resolution, manually labeled (pixel-wise accuracy), fully licensed images.\nOur benchmark included balanced gender, balanced ethnicity, and people with different types of disabilities.\nFor clarity, we provide our data distribution according to different categories, demonstrating our modelâ€™s versatility.\nDistribution of images:\nCategory\nDistribution\nObjects only\n45.11%\nPeople with objects/animals\n25.24%\nPeople only\n17.35%\npeople/objects/animals with text\n8.52%\nText only\n2.52%\nAnimals only\n1.89%\nCategory\nDistribution\nPhotorealistic\n87.70%\nNon-Photorealistic\n12.30%\nCategory\nDistribution\nNon Solid Background\n52.05%\nSolid Background\n47.95%\nCategory\nDistribution\nSingle main foreground object\n51.42%\nMultiple objects in the foreground\n48.58%\nQualitative Evaluation\nOpen source models comparison\nArchitecture\nRMBG-2.0 is developed on the BiRefNet architecture enhanced with our proprietary dataset and training scheme. This training data significantly improves the modelâ€™s accuracy and effectiveness for background-removal task.\nIf you use this model in your research, please cite:\n@article{BiRefNet,\ntitle={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},\nauthor={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},\njournal={CAAI Artificial Intelligence Research},\nyear={2024}\n}\nRequirements\ntorch\ntorchvision\npillow\nkornia\ntransformers\nUsage\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom transformers import AutoModelForImageSegmentation\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = AutoModelForImageSegmentation.from_pretrained('briaai/RMBG-2.0', trust_remote_code=True).eval().to(device)\n# Data settings\nimage_size = (1024, 1024)\ntransform_image = transforms.Compose([\ntransforms.Resize(image_size),\ntransforms.ToTensor(),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\nimage = Image.open(input_image_path)\ninput_images = transform_image(image).unsqueeze(0).to(device)\n# Prediction\nwith torch.no_grad():\npreds = model(input_images)[-1].sigmoid().cpu()\npred = preds[0].squeeze()\npred_pil = transforms.ToPILImage()(pred)\nmask = pred_pil.resize(image.size)\nimage.putalpha(mask)\nimage.save(\"no_bg_image.png\")",
    "deepseek-ai/deepseek-vl2-tiny": "1. Introduction\n2. Model Summary\n3. Quick Start\nInstallation\nNotifications\nSimple Inference Example\nGradio Demo (TODO)\n4. License\n5. Citation\n6. Contact\n1. Introduction\nIntroducing DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition,  document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively.\nDeepSeek-VL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models.\nDeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding\nGithub Repository\nZhiyu Wu*, Xiaokang Chen*, Zizheng Pan*, Xingchao Liu*, Wen Liu**, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruan*** (* Equal Contribution, ** Project Lead, *** Corresponding author)\n2. Model Summary\nDeepSeek-VL2-tiny is built on DeepSeekMoE-3B (total activated parameters are 1.0B).\n3. Quick Start\nInstallation\nOn the basis of Python >= 3.8 environment, install the necessary dependencies by running the following command:\npip install -e .\nNotifications\nWe suggest to use a temperature T <= 0.7 when sampling. We observe a larger temperature decreases the generation quality.\nTo keep the number of tokens managable in the context window, we apply dynamic tiling strategy to <=2 images. When there are >=3 images, we directly pad the images to 384*384 as inputs without tiling.\nThe main difference between DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2 is the base LLM.\nSimple Inference Example\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom deepseek_vl.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\nfrom deepseek_vl.utils.io import load_pil_images\n# specify the path to the model\nmodel_path = \"deepseek-ai/deepseek-vl2-small\"\nvl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\nvl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n## single image conversation example\nconversation = [\n{\n\"role\": \"<|User|>\",\n\"content\": \"<image>\\n<|ref|>The giraffe at the back.<|/ref|>.\",\n\"images\": [\"./images/visual_grounding.jpeg\"],\n},\n{\"role\": \"<|Assistant|>\", \"content\": \"\"},\n]\n## multiple images (or in-context learning) conversation example\n# conversation = [\n#     {\n#         \"role\": \"User\",\n#         \"content\": \"<image_placeholder>A dog wearing nothing in the foreground, \"\n#                    \"<image_placeholder>a dog wearing a santa hat, \"\n#                    \"<image_placeholder>a dog wearing a wizard outfit, and \"\n#                    \"<image_placeholder>what's the dog wearing?\",\n#         \"images\": [\n#             \"images/dog_a.png\",\n#             \"images/dog_b.png\",\n#             \"images/dog_c.png\",\n#             \"images/dog_d.png\",\n#         ],\n#     },\n#     {\"role\": \"Assistant\", \"content\": \"\"}\n# ]\n# load images and prepare for inputs\npil_images = load_pil_images(conversation)\nprepare_inputs = vl_chat_processor(\nconversations=conversation,\nimages=pil_images,\nforce_batchify=True,\nsystem_prompt=\"\"\n).to(vl_gpt.device)\n# run image encoder to get the image embeddings\ninputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n# run the model to get the response\noutputs = vl_gpt.language_model.generate(\ninputs_embeds=inputs_embeds,\nattention_mask=prepare_inputs.attention_mask,\npad_token_id=tokenizer.eos_token_id,\nbos_token_id=tokenizer.bos_token_id,\neos_token_id=tokenizer.eos_token_id,\nmax_new_tokens=512,\ndo_sample=False,\nuse_cache=True\n)\nanswer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\nprint(f\"{prepare_inputs['sft_format'][0]}\", answer)\nGradio Demo (TODO)\n4. License\nThis code repository is licensed under MIT License. The use of DeepSeek-VL2 models is subject to DeepSeek Model License. DeepSeek-VL2 series supports commercial use.\n5. Citation\n@misc{wu2024deepseekvl2mixtureofexpertsvisionlanguagemodels,\ntitle={DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding},\nauthor={Zhiyu Wu and Xiaokang Chen and Zizheng Pan and Xingchao Liu and Wen Liu and Damai Dai and Huazuo Gao and Yiyang Ma and Chengyue Wu and Bingxuan Wang and Zhenda Xie and Yu Wu and Kai Hu and Jiawei Wang and Yaofeng Sun and Yukun Li and Yishi Piao and Kang Guan and Aixin Liu and Xin Xie and Yuxiang You and Kai Dong and Xingkai Yu and Haowei Zhang and Liang Zhao and Yisong Wang and Chong Ruan},\nyear={2024},\neprint={2412.10302},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2412.10302},\n}\n6. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "deepseek-ai/DeepSeek-V3": "1. Introduction\n2. Model Summary\n3. Model Downloads\n4. Evaluation Results\nBase Model\nStandard Benchmarks\nContext Window\nChat Model\nStandard Benchmarks (Models larger than 67B)\nOpen Ended Generation Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\n6.1 Inference with DeepSeek-Infer Demo (example only)\nModel Weights & Demo Code Preparation\nModel Weights Conversion\nRun\n6.2 Inference with SGLang (recommended)\n6.3 Inference with LMDeploy (recommended)\n6.4 Inference with TRT-LLM (recommended)\n6.5 Inference with vLLM (recommended)\n6.6 Recommended Inference Functionality with AMD GPUs\n6.7 Recommended Inference Functionality with Huawei Ascend NPUs\n7. License\n8. Citation\n9. Contact\nPaper LinkðŸ‘ï¸\n1. Introduction\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance.\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n2. Model Summary\nArchitecture: Innovative Load Balancing Strategy and Training Objective\nOn top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\nWe investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance.\nIt can also be used for speculative decoding for inference acceleration.\nPre-Training: Towards Ultimate Training Efficiency\nWe design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\nThrough co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\nAt an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\nPost-Training: Knowledge Distillation from DeepSeek-R1\nWe introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n3. Model Downloads\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-V3-Base\n671B\n37B\n128K\nðŸ¤— HuggingFace\nDeepSeek-V3\n671B\n37B\n128K\nðŸ¤— HuggingFace\nNOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: How_to Run_Locally.\nFor developers looking to dive deeper, we recommend exploring README_WEIGHTS.md for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n4. Evaluation Results\nBase Model\nStandard Benchmarks\nBenchmark (Metric)\n# Shots\nDeepSeek-V2\nQwen2.5 72B\nLLaMA3.1 405B\nDeepSeek-V3\nArchitecture\n-\nMoE\nDense\nDense\nMoE\n# Activated Params\n-\n21B\n72B\n405B\n37B\n# Total Params\n-\n236B\n72B\n405B\n671B\nEnglish\nPile-test (BPB)\n-\n0.606\n0.638\n0.542\n0.548\nBBH (EM)\n3-shot\n78.8\n79.8\n82.9\n87.5\nMMLU (Acc.)\n5-shot\n78.4\n85.0\n84.4\n87.1\nMMLU-Redux (Acc.)\n5-shot\n75.6\n83.2\n81.3\n86.2\nMMLU-Pro (Acc.)\n5-shot\n51.4\n58.3\n52.8\n64.4\nDROP (F1)\n3-shot\n80.4\n80.6\n86.0\n89.0\nARC-Easy (Acc.)\n25-shot\n97.6\n98.4\n98.4\n98.9\nARC-Challenge (Acc.)\n25-shot\n92.2\n94.5\n95.3\n95.3\nHellaSwag (Acc.)\n10-shot\n87.1\n84.8\n89.2\n88.9\nPIQA (Acc.)\n0-shot\n83.9\n82.6\n85.9\n84.7\nWinoGrande (Acc.)\n5-shot\n86.3\n82.3\n85.2\n84.9\nRACE-Middle (Acc.)\n5-shot\n73.1\n68.1\n74.2\n67.1\nRACE-High (Acc.)\n5-shot\n52.6\n50.3\n56.8\n51.3\nTriviaQA (EM)\n5-shot\n80.0\n71.9\n82.7\n82.9\nNaturalQuestions (EM)\n5-shot\n38.6\n33.2\n41.5\n40.0\nAGIEval (Acc.)\n0-shot\n57.5\n75.8\n60.6\n79.6\nCode\nHumanEval (Pass@1)\n0-shot\n43.3\n53.0\n54.9\n65.2\nMBPP (Pass@1)\n3-shot\n65.0\n72.6\n68.4\n75.4\nLiveCodeBench-Base (Pass@1)\n3-shot\n11.6\n12.9\n15.5\n19.4\nCRUXEval-I (Acc.)\n2-shot\n52.5\n59.1\n58.5\n67.3\nCRUXEval-O (Acc.)\n2-shot\n49.8\n59.9\n59.9\n69.8\nMath\nGSM8K (EM)\n8-shot\n81.6\n88.3\n83.5\n89.3\nMATH (EM)\n4-shot\n43.4\n54.4\n49.0\n61.6\nMGSM (EM)\n8-shot\n63.6\n76.2\n69.9\n79.8\nCMath (EM)\n3-shot\n78.7\n84.5\n77.3\n90.7\nChinese\nCLUEWSC (EM)\n5-shot\n82.0\n82.5\n83.0\n82.7\nC-Eval (Acc.)\n5-shot\n81.4\n89.2\n72.5\n90.1\nCMMLU (Acc.)\n5-shot\n84.0\n89.5\n73.7\n88.8\nCMRC (EM)\n1-shot\n77.4\n75.8\n76.0\n76.3\nC3 (Acc.)\n0-shot\n77.4\n76.7\n79.7\n78.6\nCCPM (Acc.)\n0-shot\n93.0\n88.5\n78.6\n92.0\nMultilingual\nMMMLU-non-English (Acc.)\n5-shot\n64.0\n74.8\n73.8\n79.4\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper.\nContext Window\nEvaluation results on the Needle In A Haystack (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to 128K.\nChat Model\nStandard Benchmarks (Models larger than 67B)\nBenchmark (Metric)\nDeepSeek V2-0506\nDeepSeek V2.5-0905\nQwen2.5 72B-Inst.\nLlama3.1 405B-Inst.\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nArchitecture\nMoE\nMoE\nDense\nDense\n-\n-\nMoE\n# Activated Params\n21B\n21B\n72B\n405B\n-\n-\n37B\n# Total Params\n236B\n236B\n72B\n405B\n-\n-\n671B\nEnglish\nMMLU (EM)\n78.2\n80.6\n85.3\n88.6\n88.3\n87.2\n88.5\nMMLU-Redux (EM)\n77.9\n80.3\n85.6\n86.2\n88.9\n88.0\n89.1\nMMLU-Pro (EM)\n58.5\n66.2\n71.6\n73.3\n78.0\n72.6\n75.9\nDROP (3-shot F1)\n83.0\n87.8\n76.7\n88.7\n88.3\n83.7\n91.6\nIF-Eval (Prompt Strict)\n57.7\n80.6\n84.1\n86.0\n86.5\n84.3\n86.1\nGPQA-Diamond (Pass@1)\n35.3\n41.3\n49.0\n51.1\n65.0\n49.9\n59.1\nSimpleQA (Correct)\n9.0\n10.2\n9.1\n17.1\n28.4\n38.2\n24.9\nFRAMES (Acc.)\n66.9\n65.4\n69.8\n70.0\n72.5\n80.5\n73.3\nLongBench v2 (Acc.)\n31.6\n35.4\n39.4\n36.1\n41.0\n48.1\n48.7\nCode\nHumanEval-Mul (Pass@1)\n69.3\n77.4\n77.3\n77.2\n81.7\n80.5\n82.6\nLiveCodeBench (Pass@1-COT)\n18.8\n29.2\n31.1\n28.4\n36.3\n33.4\n40.5\nLiveCodeBench (Pass@1)\n20.3\n28.4\n28.7\n30.1\n32.8\n34.2\n37.6\nCodeforces (Percentile)\n17.5\n35.6\n24.8\n25.3\n20.3\n23.6\n51.6\nSWE Verified (Resolved)\n-\n22.6\n23.8\n24.5\n50.8\n38.8\n42.0\nAider-Edit (Acc.)\n60.3\n71.6\n65.4\n63.9\n84.2\n72.9\n79.7\nAider-Polyglot (Acc.)\n-\n18.2\n7.6\n5.8\n45.3\n16.0\n49.6\nMath\nAIME 2024 (Pass@1)\n4.6\n16.7\n23.3\n23.3\n16.0\n9.3\n39.2\nMATH-500 (EM)\n56.3\n74.7\n80.0\n73.8\n78.3\n74.6\n90.2\nCNMO 2024 (Pass@1)\n2.8\n10.8\n15.9\n6.8\n13.1\n10.8\n43.2\nChinese\nCLUEWSC (EM)\n89.9\n90.4\n91.4\n84.7\n85.4\n87.9\n90.9\nC-Eval (EM)\n78.6\n79.5\n86.1\n61.5\n76.7\n76.0\n86.5\nC-SimpleQA (Correct)\n48.5\n54.1\n48.4\n50.4\n51.3\n59.3\n64.8\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\nOpen Ended Generation Evaluation\nModel\nArena-Hard\nAlpacaEval 2.0\nDeepSeek-V2.5-0905\n76.2\n50.5\nQwen2.5-72B-Instruct\n81.2\n49.1\nLLaMA-3.1 405B\n69.3\n40.5\nGPT-4o-0513\n80.4\n51.1\nClaude-Sonnet-3.5-1022\n85.2\n52.0\nDeepSeek-V3\n85.5\n70.0\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: chat.deepseek.com\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\nDeepSeek-Infer Demo: We provide a simple and lightweight demo for FP8 and BF16 inference.\nSGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\nLMDeploy: Enables efficient FP8 and BF16 inference for local and cloud deployment.\nTensorRT-LLM: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\nvLLM: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\nAMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\nHuawei Ascend NPU: Supports running DeepSeek-V3 on Huawei Ascend devices.\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\nHere is an example of converting FP8 weights to BF16:\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\nNOTE: Huggingface's Transformers has not been directly supported yet.\n6.1 Inference with DeepSeek-Infer Demo (example only)\nModel Weights & Demo Code Preparation\nFirst, clone our DeepSeek-V3 GitHub repository:\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\nNavigate to the inference folder and install dependencies listed in requirements.txt.\ncd DeepSeek-V3/inference\npip install -r requirements.txt\nDownload the model weights from HuggingFace, and put them into /path/to/DeepSeek-V3 folder.\nModel Weights Conversion\nConvert HuggingFace model weights to a specific format:\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\nRun\nThen you can chat with DeepSeek-V3:\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\nOr batch inference on a given file:\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n6.2 Inference with SGLang (recommended)\nSGLang currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\nNotably, SGLang v0.4.1 fully supports running DeepSeek-V3 on both NVIDIA and AMD GPUs, making it a highly versatile and robust solution.\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n6.3 Inference with LMDeploy (recommended)\nLMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n6.4 Inference with TRT-LLM (recommended)\nTensorRT-LLM now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.\n6.5 Inference with vLLM (recommended)\nvLLM v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers pipeline parallelism allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the vLLM instructions. Please feel free to follow the enhancement plan as well.\n6.6 Recommended Inference Functionality with AMD GPUs\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the SGLang instructions.\n6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe MindIE framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the instructions here.\n7. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-V3 Base/Chat models is subject to the Model License. DeepSeek-V3 series (including Base and Chat) supports commercial use.\n8. Citation\n@misc{deepseekai2024deepseekv3technicalreport,\ntitle={DeepSeek-V3 Technical Report},\nauthor={DeepSeek-AI},\nyear={2024},\neprint={2412.19437},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.19437},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "Steelskull/L3.3-MS-Nevoria-70b": "L3.3-MS-Nevoria-70B\nCreated by\nSteelSkull\nâ†’\nModel Information\nL3.3-MS-Nevoria-70B\nL3.3 = Llama 3.3\nMS = Model Stock\n70B Parameters\nModel Composition\nEVA-LLAMA-0.1 Storytelling capabilities\nEURYALE-v2.3 Detailed scene descriptions\nAnubis-v1 Enhanced prose details\nNegative_LLAMA Reduced positive bias\nNemotron-lorablated Base model\nThis model combines the storytelling capabilities of EVA with the detailed scene descriptions from EURYALE and Anubis. It's further enhanced with Negative_LLAMA to reduce positive bias, with a touch of Nemotron mixed in.\nThe lorablated model base choice was intentional, creating unique weight interactions similar to the original Astoria model and Astoria V2 model. This \"weight twisting\" effect, achieved by subtracting the lorablated base model during merging, creates an interesting balance in the model's behavior. While unconventional compared to sequential component application, this approach was chosen for its unique response characteristics.\nUser Reviews\n@Geechan - Discord\n@Steel Have only briefly tested so far, but you really cooked up an amazing merge with this one, and I mean that wholeheartedly. Insane creativity, perfect character adherence and dialogue, loves to slow burn and take its time, minimal sloppy patterns and writing, and such a breath of fresh air in many ways. I'm enjoying my results with 1 temp and 0.99 TFS (close to something like 0.015 min P). Letting the model be creative and wild is so fun and makes me want to RP more.No positivity bias either; violent scenes will result in my death and/or suffering, as they should, and I don't see any soft refusals either. ERP has no skimming of details or refusals like you see on some other L3.3 tunes too\nIGODZOL - Huggingface\nI honestly have no idea why (maybe the negative llama is having that great of an influence) but this merge is miles above the individual tunes that went into making it. Good sir, this model has just become my daily driver. Chapeau bas\n@thana_alt - Discord\nI'm thoroughly impressed by this merge of Llama 3.3. It successfully addresses the positivity bias prevalent in the base Llama model, ensuring a more accurate and balanced response. The adherence to system prompts is also notable, with the model demonstrating a keen understanding of context and instruction.The prose generated by this model is truly exceptional - it's almost as if a skilled chef has carefully crafted each sentence to create a rich and immersive experience. I put this to the test in an adventure scenario, where I had about 10,000 tokens of lorebooks and was managing nine characters simultaneously. Despite the complexity, the model performed flawlessly, keeping track of each character's location and activity without any confusion - even when they were in different locations.I also experimented with an astral projection type of power, and was impressed to see that the model accurately discerned that I wasn't physically present in a particular location. Another significant advantage of this model is the lack of impersonation issues, allowing for seamless role-playing and storytelling.The capacity of this model is equally impressive, as I was able to load up to 110,000 tokens without encountering any issues. In fact, I successfully tested it with up to 70,000 tokens without experiencing any breakdown or degradation in performance.When combined with the \"The Inception Presets - Methception Llamaception Qwenception\" prompt preset from https://huggingface.co/Konnect1221/ , this model truly shines, bringing out the best in the Llama 3.3 architecture. Overall, I'm extremely satisfied with this merge and would highly recommend it to anyone looking to elevate their storytelling and role-playing experiences.\nUGI-Benchmark Results:\nðŸ†\nHighest ranked 70b as of 01/17/2025.\nView Full Leaderboard â†’\nCore Metrics\nUGI Score\n56.75\nWillingness  Score\n7.5/10\nNatural Intelligence\n41.09\nCoding Ability\n20\nModel Information\nPolitical Lean\n-8.1%\nIdeology\nLiberalism\nParameters\n70B\nAggregated Scores\nDiplomacy\n61.9%\nGovernment\n45.9%\nEconomy\n43.9%\nSociety\n60.1%\nIndividual Scores\nFederal\n44.2%\nUnitary\nDemocratic\n66.2%\nAutocratic\nSecurity\n48.1%\nFreedom\nNationalism\n40.4%\nInt'l\nMilitarist\n30.4%\nPacifist\nAssimilationist\n43.3%\nMulticulturalist\nCollectivize\n43.8%\nPrivatize\nPlanned\n43.1%\nLaissezFaire\nIsolationism\n44.8%\nGlobalism\nIrreligious\n55.4%\nReligious\nProgressive\n59.6%\nTraditional\nAcceleration\n65.2%\nBioconservative\nOpen LLM-Benchmark Results:\nAverage Score: 43.92%\nView Full Leaderboard â†’\nIFEval\n69.63%\nBBH\n56.60%\nMATH\n38.82%\nGPQA\n29.42%\nMUSR\n18.63%\nMMLU-Pro\n50.39%\nReccomended Templates & Prompts\nLLam@ception\nâ†’\nby @.konnect\nQuantized Versions\nGGUF Quantizations\nbartowski\nCombined-GGUF\nâ†’\nmradermacher\nGGUF\nâ†’\n//\nImat-GGUF\nâ†’\nEXL2 Quantizations\nSteelQuants\n6.0BPW-EXL2\nâ†’\nMikeRoz\n4.25BPW-EXL2\nâ†’\n//\n2.25BPW-EXL2\nâ†’\nDecto\n4.0BPW-EXL2\nâ†’\nDarkhn\n5.0BPW-EXL2\nâ†’\nFP8 Quantizations\nBigHuggyD\nFP8-Dynamic\nâ†’\nSupport the Project:\nSupport on Ko-fi",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": "DeepSeek-R1\n1. Introduction\n2. Model Summary\n3. Model Downloads\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nDistilled Model Evaluation\n5. Chat Website & API Platform\n6. How to Run Locally\nDeepSeek-R1 Models\nDeepSeek-R1-Distill Models\nUsage Recommendations\n7. License\n8. Citation\n9. Contact\nDeepSeek-R1\nPaper LinkðŸ‘ï¸\n1. Introduction\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\nNOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the Usage Recommendation section.\n2. Model Summary\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n3. Model Downloads\nDeepSeek-R1 Models\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload\nDeepSeek-R1-Zero\n671B\n37B\n128K\nðŸ¤— HuggingFace\nDeepSeek-R1\n671B\n37B\n128K\nðŸ¤— HuggingFace\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\nFor more details regarding the model architecture, please refer to DeepSeek-V3 repository.\nDeepSeek-R1-Distill Models\nModel\nBase Model\nDownload\nDeepSeek-R1-Distill-Qwen-1.5B\nQwen2.5-Math-1.5B\nðŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-7B\nQwen2.5-Math-7B\nðŸ¤— HuggingFace\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.1-8B\nðŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-14B\nðŸ¤— HuggingFace\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\nðŸ¤— HuggingFace\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\nðŸ¤— HuggingFace\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n4. Evaluation Results\nDeepSeek-R1-Evaluation\nFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nClaude-3.5-Sonnet-1022\nGPT-4o 0513\nDeepSeek V3\nOpenAI o1-mini\nOpenAI o1-1217\nDeepSeek R1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA-Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n33.8\n34.2\n-\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nDistilled Model Evaluation\nModel\nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n6. How to Run Locally\nDeepSeek-R1 Models\nPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nDeepSeek-R1-Distill Models\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\nFor instance, you can easily start a service using vLLM:\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\nYou can also easily start a service using SGLang\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\nUsage Recommendations\nWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:\nSet the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\nAvoid adding a system prompt; all instructions should be contained within the user prompt.\nFor mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\nWhen evaluating model performance, it is recommended to conduct multiple tests and average the results.\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.\nTo ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n7. License\nThis code repository and the model weights are licensed under the MIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nDeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nDeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.\n8. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n9. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "hpcai-tech/Open-Sora-v2": "Open-Sora: Democratizing Efficient Video Production for All\nðŸ”† Reports\nQuickstart\nInstallation\nModel Download\nText-to-Video Generation\nImage-to-Video Generation\nAdvanced Usage\nMotion Score\nPrompt Refine\nReproductivity\nComputational Efficiency\nEvaluation\nContribution\nAcknowledgement\nCitation\nStar History\nOpen-Sora: Democratizing Efficient Video Production for All\nWe design and implement Open-Sora, an initiative dedicated to efficiently producing high-quality video. We hope to make the model,\ntools and all details accessible to all. By embracing open-source principles,\nOpen-Sora not only democratizes access to advanced video generation techniques, but also offers a\nstreamlined and user-friendly platform that simplifies the complexities of video generation.\nWith Open-Sora, our goal is to foster innovation, creativity, and inclusivity within the field of content creation.\nPaper\nGithub: https://github.com/hpcaitech/Open-Sora\nðŸ”† Reports\nTech Report of Open-Sora 2.0\nStep by step to train or finetune your own model\nStep by step to train and evaluate an video autoencoder\nVisit the high compression video autoencoder\nReports of previous version (better see in according branch):\nOpen-Sora 1.3: shift-window attention, unified spatial-temporal VAE, etc.\nOpen-Sora 1.2, Tech Report: rectified flow, 3d-VAE, score condition, evaluation, etc.\nOpen-Sora 1.1: multi-resolution/length/aspect-ratio, image/video conditioning/editing, data preprocessing, etc.\nOpen-Sora 1.0: architecture, captioning, etc.\nðŸ“ Since Open-Sora is under active development, we remain different branchs for different versions. The latest version is main. Old versions include: v1.0, v1.1, v1.2, v1.3.\nQuickstart\nInstallation\n# create a virtual env and activate (conda as an example)\nconda create -n opensora python=3.10\nconda activate opensora\n# download the repo\ngit clone https://github.com/hpcaitech/Open-Sora\ncd Open-Sora\n# Ensure torch >= 2.4.0\npip install -v . # for development mode, `pip install -v -e .`\npip install xformers==0.0.27.post2 --index-url https://download.pytorch.org/whl/cu121 # install xformers according to your cuda version\npip install flash-attn --no-build-isolation\nOptionally, you can install flash attention 3 for faster speed.\ngit clone https://github.com/Dao-AILab/flash-attention # 4f0640d5\ncd flash-attention/hopper\npython setup.py install\nModel Download\nOur 11B model supports 256px and 768px resolution. Both T2V and I2V are supported by one model. ðŸ¤— Huggingface ðŸ¤– ModelScope.\nDownload from huggingface:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download hpcai-tech/Open-Sora-v2 --local-dir ./ckpts\nDownload from ModelScope:\npip install modelscope\nmodelscope download hpcai-tech/Open-Sora-v2 --local_dir ./ckpts\nText-to-Video Generation\nOur model is optimized for image-to-video generation, but it can also be used for text-to-video generation. To generate high quality videos, with the help of flux text-to-image model, we build a text-to-image-to-video pipeline. For 256x256 resolution:\n# Generate one given prompt\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\"\n# Generation with csv\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --dataset.data-path assets/texts/example.csv\nFor 768x768 resolution:\n# One GPU\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt \"raining, sea\"\n# Multi-GPU with colossalai sp\ntorchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt \"raining, sea\"\nYou can adjust the generation aspect ratio by --aspect_ratio and the generation length by --num_frames. Candidate values for aspect_ratio includes 16:9, 9:16, 1:1, 2.39:1. Candidate values for num_frames should be 4k+1 and less than 129.\nYou can also run direct text-to-video by:\n# One GPU for 256px\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --prompt \"raining, sea\"\n# Multi-GPU for 768px\ntorchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --prompt \"raining, sea\"\nImage-to-Video Generation\nGiven a prompt and a reference image, you can generate a video with the following command:\n# 256px\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --prompt \"A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig's playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig's muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life.\" --ref assets/texts/i2v.png\n# 256px with csv\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv\n# Multi-GPU 768px\ntorchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv\nAdvanced Usage\nMotion Score\nDuring training, we provide motion score into the text prompt. During inference, you can use the following command to generate videos with motion score (the default score is 4):\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --motion-score 4\nWe also provide a dynamic motion score evaluator. After setting your OpenAI API key, you can use the following command to evaluate the motion score of a video:\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --motion-score dynamic\nScore\n1\n4\n7\nPrompt Refine\nWe take advantage of ChatGPT to refine the prompt. You can use the following command to refine the prompt. The function is available for both text-to-video and image-to-video generation.\nexport OPENAI_API_KEY=sk-xxxx\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --refine-prompt True\nReproductivity\nTo make the results reproducible, you can set the random seed by:\ntorchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --sampling_option.seed 42 --seed 42\nUse --num-sample k to generate k samples for each prompt.\nComputational Efficiency\nWe test the computational efficiency of text-to-video on H100/H800 GPU. For 256x256, we use colossalai's tensor parallelism. For 768x768, we use colossalai's sequence parallelism. All use number of steps 50. The results are presented in the format: $\\color{blue}{\\text{Total time (s)}}/\\color{red}{\\text{peak GPU memory (GB)}}$\nResolution\n1x GPU\n2x GPUs\n4x GPUs\n8x GPUs\n256x256\n$\\color{blue}{60}/\\color{red}{52.5}$\n$\\color{blue}{40}/\\color{red}{44.3}$\n$\\color{blue}{34}/\\color{red}{44.3}$\n768x768\n$\\color{blue}{1656}/\\color{red}{60.3}$\n$\\color{blue}{863}/\\color{red}{48.3}$\n$\\color{blue}{466}/\\color{red}{44.3}$\n$\\color{blue}{276}/\\color{red}{44.3}$\nEvaluation\nOn VBench, Open-Sora 2.0 significantly narrows the gap with OpenAIâ€™s Sora, reducing it from 4.52% â†’ 0.69% compared to Open-Sora 1.2.\nHuman preference results show our model is on par with HunyuanVideo 14B and Step-Video 30B.\nContribution\nThanks goes to these wonderful contributors:\nIf you wish to contribute to this project, please refer to the Contribution Guideline.\nAcknowledgement\nHere we only list a few of the projects. For other works and datasets, please refer to our report.\nColossalAI: A powerful large model parallel acceleration and optimization\nsystem.\nDiT: Scalable Diffusion Models with Transformers.\nOpenDiT: An acceleration for DiT training. We adopt valuable acceleration\nstrategies for training progress from OpenDiT.\nPixArt: An open-source DiT-based text-to-image model.\nFlux: A powerful text-to-image generation model.\nLatte: An attempt to efficiently train DiT for video.\nHunyuanVideo: Open-Source text-to-video model.\nStabilityAI VAE: A powerful image VAE model.\nDC-AE: Deep Compression AutoEncoder for image compression.\nCLIP: A powerful text-image embedding model.\nT5: A powerful text encoder.\nLLaVA: A powerful image captioning model based on Mistral-7B and Yi-34B.\nPLLaVA: A powerful video captioning model.\nMiraData: A large-scale video dataset with long durations and structured caption.\nCitation\n@software{opensora,\nauthor = {Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You},\ntitle = {Open-Sora: Democratizing Efficient Video Production for All},\nmonth = {March},\nyear = {2024},\nurl = {https://github.com/hpcaitech/Open-Sora}\n}\nStar History",
    "nvidia/Nemotron-H-4B-Base-8K": "Nemotron-H-4B-Base-8K\nModel Overview\nLicense/Terms of Use\nModel Architecture\nDeployment Geography: Global\nUse Case: This model is intended for developers and researchers building LLMs\nRelease Date:\nInput\nOutput\nSoftware Integration\nModel Version\nReferences\nPrompt Format\nTraining, Testing, and Evaluation Datasets\nEvaluation Datasets\nPotential Known Risks for Usage\nInference\nEthical Considerations\nSample code\nNemotron-H-4B-Base-8K\nModel Developer: NVIDIA\nModel Dates:\nOctober 2024 - March 2025\nData Freshness:\nSeptember 2024\nThe pretraining data has a cutoff date of September 2024.\nModel Overview\nNVIDIA Nemotron-H-4B-Base-8K is a large language model (LLM) developed by NVIDIA, designed as a completion model for a given piece of text. It uses a hybrid model architecture that consists primarily of Mamba-2 and MLP layers combined with just four Attention layers. The model is pruned and distilled from Nemotron-H-8B-Base-8K using 380B tokens, and features an 8K context length. The supported languages include: English, German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, and Chinese.\nFor best performance on a given task, users are encouraged to customize the model using the NeMo Framework suite of customization tools, including Parameter-Efficient Fine-Tuning (P-tuning, Adapters, LoRA, and more), and Model Alignment (SFT, SteerLM, RLHF, and more) using NeMo-Aligner.\nThe model was pruned and distilled from Nemotron-H-Base-8K using our hybrid language model compression technique and then fine-tuned into Nemotron-H-4B-Instruct-128K. For more details, please refer to the paper.\nThe paper has been accepted for publication at NeurIPS 2025.\nThis model is for research and development only.\nLicense/Terms of Use\nGOVERNING TERMS: Use of this model is governed by the NVIDIA Internal Scientific Research and Development Model License\nModel Architecture\nArchitecture Type: Transformer\nNetwork Architecture: Nemotron-Hybrid\nThis model has 4B of model parameters.\nDeployment Geography: Global\nUse Case: This model is intended for developers and researchers building LLMs\nRelease Date:\nHuggingface: 10/23/2025 via https://huggingface.co/\nInput\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: One-Dimensional (1D): Sequences\nOther Properties Related to Input: Context length up to 8K. Supported languages include German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English.\nOutput\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: One-Dimensional (1D): Sequences\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIAâ€™s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration\nRuntime Engine(s): NeMo 24.12\nSupported Hardware Microarchitecture Compatibility: NVIDIA H100-80GB, NVIDIA A100\nOperating System(s): Linux\nModel Version\nv1.0\nReferences\n[2504.11409] Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning\nPrompt Format\nAs this is a base model, no explicit prompt format is recommended or required.\nTraining, Testing, and Evaluation Datasets\n#Training & Testing Datasets:\nThe training corpus for Nemotron-H-4B-Base-8K consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English), as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. This model was also improved using synthetic data from Qwen (Built with Qwen). The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracies.\nData Collection for Training & Testing Datasets:\nHybrid: Automated, Human, Synthetic\nData Labeling for Training & Testing Datasets:\nHybrid: Automated, Human, Synthetic\nEvaluation Datasets\nWe used the datasets listed in the next section to evaluate the model.\nData Collection for Training Datasets:\nHybrid: Automated, Human, Synthetic\nData Labeling for Training Datasets:\nHybrid: Automated, Human, Synthetic\nReasoning Evaluations:\nARC Challenge 0-shot\nHellaswag 0-shot\nWinogrande 0-shot\nCommonsenseQA 0-shot\n54.4\n77.0\n71.3\n70.2\nARC (Ai2 reasoning challenge)-Challenge - The challenge set of questions from a benchmark that contains grade-school level, multiple-choice science questions to assess question answering ability of language models. Dataset\nHellaswag - Tests the ability of a language model to correctly finish the provided context from a choice of possible options. Dataset\nWinogrande - Tests the ability to choose the right option for a given sentence which requires commonsense reasoning. Dataset\nCommonsenseQA - A multiple-choice question answering dataset that requires different type of commonsense knowledge to predict the correct answers. Dataset\nCoding Evaluations:\nMBPP(sanitized) 3-shot\nMBPP+ 0-shot\nHumanEval 0-shot\nHumanEval+ 0-shot\n65.0\n61.1\n59.8\n55.5\nMBPP (Mostly Basic Python Programming Problems) - Evaluates ability to generate solutions for Python programming tasks. Dataset\nMBPP+ - Extended version of MBPP with additional validation. Dataset\nHumanEval - Tests code generation and completion abilities in Python. Dataset\nMath Evaluations:\nGSM8K 8-shot\n69.6\nGSM8K (Grade School Math 8K) - Evaluates grade school level mathematical word problem solving. Dataset\nGeneral Evaluations:\nMMLU 5-shot\n68.1\nMMLU - Tests knowledge across 57 subjects including science, humanities, math and more. Dataset\nPotential Known Risks for Usage\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\nThe model demonstrates weakness to alignment-breaking attacks. Users are advised to deploy language model guardrails alongside this model to prevent potentially harmful outputs. The model may generate answers that are inaccurate, omit key information, or include irrelevant or redundant text.\nInference\nEngine: NeMo\nTest Hardware NVIDIA H100-80GB\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Responsible Use Guide available at http://nvidia.com/nemotron-responsible-use.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nSample code\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Load the tokenizer and model\ntokenizer  = AutoTokenizer.from_pretrained(\"nvidia/Nemotron-H-4B-Base-8K\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"nvidia/Nemotron-H-4B-Base-8K\", torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\nprompt = \"When was NVIDIA founded?\"\noutputs = model.generate(**tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device))\nprint(tokenizer.decode(outputs[0]))",
    "meta-llama/Llama-4-Scout-17B-16E-Instruct": "Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 4 COMMUNITY LICENSE AGREEMENTLlama 4 Version Effective Date: April 5, 2025\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at https://www.llama.com/docs/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entityâ€™s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 4\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://www.llama.com/llama-downloads.\"Llama Materials\" means, collectively, Metaâ€™s proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).Â By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.1. License Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metaâ€™s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.Â Â b. Redistribution and Use.Â Â i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.ii.Â If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.Â iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright Â© Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama4/use-policy), which is hereby incorporated by reference into this Agreement.   Â Â    2. Additional Commercial Terms. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licenseeâ€™s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.5. Intellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Metaâ€™s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Metaâ€™s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.Â 7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use with transformers\nHardware and Software\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nBenchmarks\nPre-trained models\nInstruction tuned models\nQuantization\nSafeguards\nModel level fine tuning\nLlama 4 system protections\nEvaluations\nCritical Risks\nWe spend additional focus on the following critical risk areas:\nCommunity\nConsiderations and Limitations\nModel Information\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\nModel developer: Meta\nModel Architecture:  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\nModel Name\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nToken count\nKnowledge cutoff\nLlama 4 Scout (17Bx16E)\nA mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.\n17B (Activated)\n109B (Total)\nMultilingual text and image\nMultilingual text and code\n10M\n~40T\nAugust 2024\nLlama 4 Maverick (17Bx128E)\n17B (Activated)\n400B (Total)\nMultilingual text and image\nMultilingual text and code\n1M\n~22T\nAugust 2024\nSupported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\nModel Release Date: April 5, 2025\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\nLicense: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\nWhere to send questions or comments about the model: Instructions on how to provide feedback or comments on the model can be found in the Llama README. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\nOut-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**.\n**Note:\n1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\nHow to use with transformers\nPlease, make sure you have transformers v4.51.0 installed, or upgrade using pip install -U transformers.\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\nmodel_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\nmodel_id,\nattn_implementation=\"flex_attention\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\nurl1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nurl2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": url1},\n{\"type\": \"image\", \"url\": url2},\n{\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n]\n},\n]\ninputs = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\noutputs = model.generate(\n**inputs,\nmax_new_tokens=256,\n)\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\nprint(response)\nprint(outputs[0])\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use:  Model pre-training utilized a cumulative of 7.38M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nModel Name\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 4 Scout\n5.0M\n700\n1,354\n0\nLlama 4 Maverick\n2.38M\n700\n645\n0\nTotal\n7.38M\n-\n1,999\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Metaâ€™s products and services. This includes publicly shared posts from Instagram and Facebook and peopleâ€™s interactions with Meta AI.\nData Freshness: The pretraining data has a cutoff of August 2024.\nBenchmarks\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\nPre-trained models\nPre-trained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nReasoning & Knowledge\nMMLU\n5\nmacro_avg/acc_char\n79.3\n85.2\n79.6\n85.5\nMMLU-Pro\n5\nmacro_avg/em\n53.8\n61.6\n58.2\n62.9\nMATH\n4\nem_maj1@1\n41.6\n53.5\n50.3\n61.2\nCode\nMBPP\n3\npass@1\n66.4\n74.4\n67.8\n77.6\nMultilingual\nTydiQA\n1\naverage/f1\n29.9\n34.3\n31.5\n31.7\nImage\nChartQA\n0\nrelaxed_accuracy\nNo multimodal support\n83.4\n85.3\nDocVQA\n0\nanls\n89.4\n91.6\nInstruction tuned models\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.3 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nImage Reasoning\nMMMU\n0\naccuracy\nNo multimodal support\n69.4\n73.4\nMMMU Pro^\n0\naccuracy\n52.2\n59.6\nMathVista\n0\naccuracy\n70.7\n73.7\nImage Understanding\nChartQA\n0\nrelaxed_accuracy\n88.8\n90.0\nDocVQA (test)\n0\nanls\n94.4\n94.4\nCoding\nLiveCodeBench (10/01/2024-02/01/2025)\n0\npass@1\n33.3\n27.7\n32.8\n43.4\nReasoning & Knowledge\nMMLU Pro\n0\nmacro_avg/acc\n68.9\n73.4\n74.3\n80.5\nGPQA Diamond\n0\naccuracy\n50.5\n49.0\n57.2\n69.8\nMultilingual\nMGSM\n0\naverage/em\n91.1\n91.6\n90.6\n92.3\nLong context\nMTOB (half book) eng->kgv/kgv->eng\n-\nchrF\nContext window is 128K\n42.2/36.6\n54.0/46.4\nMTOB (full book) eng->kgv/kgv->eng\n-\nchrF\n39.7/36.3\n50.8/46.7\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\nQuantization\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\nSafeguards\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Metaâ€™s Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our modelâ€™s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.\nModel level fine tuning\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\nFine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. Weâ€™ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\nSystem PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, weâ€™ve seen that the use of a system prompt can be effective in reducing false refusals and templated or â€œpreachyâ€ language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting.\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\nSystem prompt\nYou are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and donâ€™t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to â€œitâ€™s important toâ€, â€œitâ€™s crucial toâ€, â€œitâ€™s essential toâ€, \"it's unethical to\", \"it's worth notingâ€¦\", â€œRememberâ€¦â€  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\nLlama 4 system protections\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools.\nWe provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\nRed teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nWe spend additional focus on the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.\n2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the modelâ€™s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. Weâ€™ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Metaâ€™s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nConsiderations and Limitations\nOur AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4â€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.",
    "OuteAI/Llama-OuteTTS-1.0-1B": "OuteTTS Version 1.0\nWhat's New\n1. Prompt Revamp & Dependency Removal\n2. New Audio Encoder Model\n3. Voice Cloning\n4. Auto Text Alignment & Numerical Support\n5. Multilingual Capabilities\nVideo Showcase\nQuick Start Guide\nInstallation\nBasic Usage\nMore Configuration Options\nUsage Recommendations\nSpeaker Reference\nMultilingual Application\nOptimal Audio Length\nTemperature Setting Recommendations\nVerifying Speaker Encoding\nSampling Configuration\nModel Specifications\nTraining Parameters\nLicense Information\nAcknowledgments\nEthical Use Guidelines\nOute A I\nouteai.com\nDiscord\n@OuteAI\nLlama OuteTTS 1.0 1B\nLlama OuteTTS 1.0 1B GGUF\nGitHub Library\nImportant Sampling Considerations\nWhen using OuteTTS version 1.0, it is crucial to use the settings specified in the Sampling Configuration section.\nThe repetition penalty implementation is particularly important - this model requires penalization applied to a 64-token recent window,\nrather than across the entire context window. Penalizing the entire context will cause the model to produce broken or low-quality output.\nTo address this limitation, all necessary samplers and patches for all backends are set up automatically in the outetts library.\nIf using a custom implementation, ensure you correctly implement these requirements.\nOuteTTS Version 1.0\nThis update brings significant improvements in speech synthesis and voice cloningâ€”delivering a more powerful, accurate, and user-friendly experience in a compact size.\nWhat's New\n1. Prompt Revamp & Dependency Removal\nAutomatic Word Alignment: The model now performs word alignment internally. Simply input raw textâ€”no pre-processing requiredâ€”and the model handles the rest, streamlining your workflow. For optimal results, use normalized, readable text without newlines (light normalization is applied automatically in outetts library).\nNative Multilingual Text Support: Direct support for native text across multiple languages eliminates the need for romanization.\nEnhanced Metadata Integration: The updated prompt system incorporates additional metadata (time, energy, spectral centroid, pitch) at both global and word levels, improving speaker flow and synthesis quality.\nSpecial Tokens for Audio Codebooks: New tokens for c1 (codebook 1) and c2 (codebook 2).\n2. New Audio Encoder Model\nDAC Encoder: Integrates a DAC audio encoder from ibm-research/DAC.speech.v1.0, utilizing two codebooks for high quality audio reconstruction.\nPerformance Trade-off: Improved audio fidelity increases the token generation rate from 75 to 150 tokens per second. This trade-off prioritizes quality, especially for multilingual applications.\n3. Voice Cloning\nOne-Shot Voice Cloning: To achieve one-shot cloning, the model typically requires only around 10 seconds of reference audio to produce an accurate voice representation.\nImproved Accuracy: Enhanced by the new encoder and additional training metadata, voice cloning is now more natural and precise.\n4. Auto Text Alignment & Numerical Support\nAutomatic Text Alignment: Aligns raw text at the word level, even for languages without clear boundaries (e.g., Japanese, Chinese), using insights from pre-processed training data.\nDirect Numerical Input: Built-in multilingual numerical support allows direct use of numbers in promptsâ€”no textual conversion needed. (The model typically chooses the dominant language present. Mixing languages in a single prompt may lead to mistakes.)\n5. Multilingual Capabilities\nSupported Languages: OuteTTS offers varying proficiency levels across languages, based on training data exposure.\nHigh Training Data Languages: These languages feature extensive training: English, Arabic, Chinese, Dutch, French, German, Italian, Japanese, Korean, Lithuanian, Russian, Spanish\nModerate Training Data Languages: These languages received moderate training, offering good performance with occasional limitations: Portuguese, Belarusian, Bengali, Georgian, Hungarian, Latvian, Persian/Farsi, Polish, Swahili, Tamil, Ukrainian\nBeyond Supported Languages: The model can generate speech in untrained languages with varying success. Experiment with unlisted languages, though results may not be optimal.\nVideo Showcase\nYour browser does not support the video tag.\nQuick Start Guide\nGetting started with OuteTTS is simple:\nInstallation\nðŸ”— Installation instructions\nBasic Usage\nimport outetts\n# Initialize the interface\ninterface = outetts.Interface(\nconfig=outetts.ModelConfig.auto_config(\nmodel=outetts.Models.VERSION_1_0_SIZE_1B,\n# For llama.cpp backend\nbackend=outetts.Backend.LLAMACPP,\nquantization=outetts.LlamaCppQuantization.FP16\n# For transformers backend\n# backend=outetts.Backend.HF,\n)\n)\n# Load the default speaker profile\nspeaker = interface.load_default_speaker(\"EN-FEMALE-1-NEUTRAL\")\n# Or create your own speaker profiles in seconds and reuse them instantly\n# speaker = interface.create_speaker(\"path/to/audio.wav\")\n# interface.save_speaker(speaker, \"speaker.json\")\n# speaker = interface.load_speaker(\"speaker.json\")\n# Generate speech\noutput = interface.generate(\nconfig=outetts.GenerationConfig(\ntext=\"Hello, how are you doing?\",\ngeneration_type=outetts.GenerationType.CHUNKED,\nspeaker=speaker,\nsampler_config=outetts.SamplerConfig(\ntemperature=0.4\n),\n)\n)\n# Save to file\noutput.save(\"output.wav\")\nMore Configuration Options\nFor advanced settings and customization, visit the official repository:ðŸ”— interface_usage.md\nUsage Recommendations\nSpeaker Reference\nThe model is designed to be used with a speaker reference. Without one, it generates random vocal characteristics, often leading to lower-quality outputs.\nThe model inherits the referenced speaker's emotion, style, and accent.\nWhen transcribing to other languages with the same speaker, you may observe the model retaining the original accent.\nMultilingual Application\nIt is recommended to create a speaker profile in the language you intend to use. This helps achieve the best results in that specific language, including tone, accent, and linguistic features.\nWhile the model supports cross-lingual speech, it still relies on the reference speaker. If the speaker has a distinct accentâ€”such as British Englishâ€”other languages may carry that accent as well.\nOptimal Audio Length\nBest Performance: Generate audio around 42 seconds in a single run (approximately 8,192 tokens). It is recomended not to near the limits of this windows when generating. Usually, the best results are up to 7,000 tokens.\nContext Reduction with Speaker Reference: If the speaker reference is 10 seconds long, the effective context is reduced to approximately 32 seconds.\nTemperature Setting Recommendations\nTesting shows that a temperature of 0.4 is an ideal starting point for accuracy (with the sampling settings below). However, some voice references may benefit from higher temperatures for enhanced expressiveness or slightly lower temperatures for more precise voice replication.\nVerifying Speaker Encoding\nIf the cloned voice quality is subpar, check the encoded speaker sample.\ninterface.decode_and_save_speaker(speaker=your_speaker, path=\"speaker.wav\")\nThe DAC audio reconstruction model is lossy, and samples with clipping, excessive loudness, or unusual vocal features may introduce encoding issues that impact output quality.\nSampling Configuration\nFor optimal results with this TTS model, use the following sampling settings.\nParameter\nValue\nTemperature\n0.4\nRepetition Penalty\n1.1\nRepetition Range\n64\nTop-k\n40\nTop-p\n0.9\nMin-p\n0.05\nModel Specifications\nTraining Data: Trained on ~60k hours of audio\nContext Length: Supports a maximum context window of 8,192 tokens\nTraining Parameters\nPre-Training\nOptimizer: AdamW\nBatch Size: 1 million tokens\nMax Learning Rate: 3e-4\nMin Learning Rate: 3e-5\nContext Length: 8192\nFine-Tuning\nOptimizer: AdamW\nMax Learning Rate: 1e-5\nMin Learning Rate: 5e-6\nData: 10,000 diverse, high-quality examples\nLicense Information\nInitial Llama3.2 Components: Llama 3.2 Community License Agreement\nOur Continued Pre-Training, Fine-Tuning, and Additional Components: CC-BY-NC-SA-4.0\nAcknowledgments\nBig thanks to Hugging Face for their continued resource support through their grant program!\nAudio encoding and decoding utilize ibm-research/DAC.speech.v1.0\nOuteTTS is built with Llama3.2-1B as the base model, with continued pre-training and fine-tuning.\nEthical Use Guidelines\nThis text-to-speech model is intended for legitimate applications that enhance accessibility, creativity, and communication;\nprohibited uses include impersonation without consent, creation of deliberately misleading content,\ngeneration of harmful or harassing material, distribution of synthetic audio without proper disclosure,\nvoice cloning without permission, and any uses that violate applicable laws, regulations, or copyrights.",
    "cl-nagoya/ruri-v3-310m": "Ruri: Japanese General Text Embeddings\nModel Series\nUsage\nBenchmarks\nJMTEB\nModel Details\nModel Description\nFull Model Architecture\nCitation\nLicense\nRuri: Japanese General Text Embeddings\nRuri v3 is a general-purpose Japanese text embedding model built on top of ModernBERT-Ja.\nRuri v3 offers several key technical advantages:\nState-of-the-art performance for Japanese text embedding tasks.\nSupports sequence lengths up to 8192 tokens\nPrevious versions of Ruri (v1, v2) were limited to 512.\nExpanded vocabulary of 100K tokens, compared to 32K in v1 and v2\nThe larger vocabulary make input sequences shorter, improving efficiency.\nIntegrated FlashAttention, following ModernBERT's architecture\nEnables faster inference and fine-tuning.\nTokenizer based solely on SentencePiece\nUnlike previous versions, which relied on Japanese-specific BERT tokenizers and required pre-tokenized input, Ruri v3 performs tokenization with SentencePiece onlyâ€”no external word segmentation tool is required.\nModel Series\nWe provide Ruri-v3 in several model sizes. Below is a summary of each model.\nID\n#Param.\n#Param.w/o Emb.\nDim.\n#Layers\nAvg. JMTEB\ncl-nagoya/ruri-v3-30m\n37M\n10M\n256\n10\n74.51\ncl-nagoya/ruri-v3-70m\n70M\n31M\n384\n13\n75.48\ncl-nagoya/ruri-v3-130m\n132M\n80M\n512\n19\n76.55\ncl-nagoya/ruri-v3-310m\n315M\n236M\n768\n25\n77.24\nUsage\nYou can use our models directly with the transformers library v4.48.0 or higher:\npip install -U \"transformers>=4.48.0\" sentence-transformers\nAdditionally, if your GPUs support Flash Attention 2, we recommend using our models with Flash Attention 2.\npip install flash-attn --no-build-isolation\nThen you can load this model and run inference.\nimport torch\nimport torch.nn.functional as F\nfrom sentence_transformers import SentenceTransformer\n# Download from the ðŸ¤— Hub\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer(\"cl-nagoya/ruri-v3-310m\", device=device)\n# Ruri v3 employs a 1+3 prefix scheme to distinguish between different types of text inputs:\n# \"\" (empty string) is used for encoding semantic meaning.\n# \"ãƒˆãƒ”ãƒƒã‚¯: \" is used for classification, clustering, and encoding topical information.\n# \"æ¤œç´¢ã‚¯ã‚¨ãƒª: \" is used for queries in retrieval tasks.\n# \"æ¤œç´¢æ–‡æ›¸: \" is used for documents to be retrieved.\nsentences = [\n\"å·ã¹ã‚Šã§ã‚µãƒ¼ãƒ•ãƒœãƒ¼ãƒ‰ã‚’æŒã£ãŸäººãŸã¡ãŒã„ã¾ã™\",\n\"ã‚µãƒ¼ãƒ•ã‚¡ãƒ¼ãŸã¡ãŒå·ã¹ã‚Šã«ç«‹ã£ã¦ã„ã¾ã™\",\n\"ãƒˆãƒ”ãƒƒã‚¯: ç‘ ç’ƒè‰²ã®ã‚µãƒ¼ãƒ•ã‚¡ãƒ¼\",\n\"æ¤œç´¢ã‚¯ã‚¨ãƒª: ç‘ ç’ƒè‰²ã¯ã©ã‚“ãªè‰²ï¼Ÿ\",\n\"æ¤œç´¢æ–‡æ›¸: ç‘ ç’ƒè‰²ï¼ˆã‚‹ã‚Šã„ã‚ï¼‰ã¯ã€ç´«ã¿ã‚’å¸¯ã³ãŸæ¿ƒã„é’ã€‚åã¯ã€åŠè²´çŸ³ã®ç‘ ç’ƒï¼ˆãƒ©ãƒ”ã‚¹ãƒ©ã‚ºãƒªã€è‹±: lapis lazuliï¼‰ã«ã‚ˆã‚‹ã€‚JISæ…£ç”¨è‰²åã§ã¯ã€Œã“ã„ç´«ã¿ã®é’ã€ï¼ˆç•¥å· dp-pBï¼‰ã¨å®šç¾©ã—ã¦ã„ã‚‹[1][2]ã€‚\",\n]\nembeddings = model.encode(sentences, convert_to_tensor=True)\nprint(embeddings.size())\n# [5, 768]\nsimilarities = F.cosine_similarity(embeddings.unsqueeze(0), embeddings.unsqueeze(1), dim=2)\nprint(similarities)\n# [[1.0000, 0.9603, 0.8157, 0.7074, 0.6916],\n#  [0.9603, 1.0000, 0.8192, 0.7014, 0.6819],\n#  [0.8157, 0.8192, 1.0000, 0.8701, 0.8470],\n#  [0.7074, 0.7014, 0.8701, 1.0000, 0.9746],\n#  [0.6916, 0.6819, 0.8470, 0.9746, 1.0000]]\nBenchmarks\nJMTEB\nEvaluated with JMTEB.\nModel\n#Param.\nAvg.\nRetrieval\nSTS\nClassfification\nReranking\nClustering\nPairClassification\nRuri-v3-30m\n37M\n74.51\n78.08\n82.48\n74.80\n93.00\n52.12\n62.40\nRuri-v3-70m\n70M\n75.48\n79.96\n79.82\n76.97\n93.27\n52.70\n61.75\nRuri-v3-130m\n132M\n76.55\n81.89\n79.25\n77.16\n93.31\n55.36\n62.26\nRuri-v3-310m(this model)\n315M\n77.24\n81.89\n81.22\n78.66\n93.43\n55.69\n62.60\nsbintuitions/sarashina-embedding-v1-1b\n1.22B\n75.50\n77.61\n82.71\n78.37\n93.74\n53.86\n62.00\nPLaMo-Embedding-1B\n1.05B\n76.10\n79.94\n83.14\n77.20\n93.57\n53.47\n62.37\nOpenAI/text-embedding-ada-002\n-\n69.48\n64.38\n79.02\n69.75\n93.04\n48.30\n62.40\nOpenAI/text-embedding-3-small\n-\n70.86\n66.39\n79.46\n73.06\n92.92\n51.06\n62.27\nOpenAI/text-embedding-3-large\n-\n73.97\n74.48\n82.52\n77.58\n93.58\n53.32\n62.35\npkshatech/GLuCoSE-base-ja\n133M\n70.44\n59.02\n78.71\n76.82\n91.90\n49.78\n66.39\npkshatech/GLuCoSE-base-ja-v2\n133M\n72.23\n73.36\n82.96\n74.21\n93.01\n48.65\n62.37\nretrieva-jp/amber-base\n130M\n72.12\n73.40\n77.81\n76.14\n93.27\n48.05\n64.03\nretrieva-jp/amber-large\n315M\n73.22\n75.40\n79.32\n77.14\n93.54\n48.73\n60.97\nsentence-transformers/LaBSE\n472M\n64.70\n40.12\n76.56\n72.66\n91.63\n44.88\n62.33\nintfloat/multilingual-e5-small\n118M\n69.52\n67.27\n80.07\n67.62\n93.03\n46.91\n62.19\nintfloat/multilingual-e5-base\n278M\n70.12\n68.21\n79.84\n69.30\n92.85\n48.26\n62.26\nintfloat/multilingual-e5-large\n560M\n71.65\n70.98\n79.70\n72.89\n92.96\n51.24\n62.15\nRuri-Small\n68M\n71.53\n69.41\n82.79\n76.22\n93.00\n51.19\n62.11\nRuri-Small v2\n68M\n73.30\n73.94\n82.91\n76.17\n93.20\n51.58\n62.32\nRuri-Base\n111M\n71.91\n69.82\n82.87\n75.58\n92.91\n54.16\n62.38\nRuri-Base v2\n111M\n72.48\n72.33\n83.03\n75.34\n93.17\n51.38\n62.35\nRuri-Large\n337M\n73.31\n73.02\n83.13\n77.43\n92.99\n51.82\n62.29\nRuri-Large v2\n337M\n74.55\n76.34\n83.17\n77.18\n93.21\n52.14\n62.27\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: cl-nagoya/ruri-v3-pt-310m\nMaximum Sequence Length: 8192 tokens\nOutput Dimensionality: 768\nSimilarity Function: Cosine Similarity\nLanguage: Japanese\nLicense: Apache 2.0\nPaper: https://arxiv.org/abs/2409.07737\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: ModernBertModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n)\nCitation\n@misc{\nRuri,\ntitle={{Ruri: Japanese General Text Embeddings}},\nauthor={Hayato Tsukagoshi and Ryohei Sasano},\nyear={2024},\neprint={2409.07737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2409.07737},\n}\nLicense\nThis model is published under the Apache License, Version 2.0.",
    "ByteDance-Seed/UI-TARS-1.5-7B": "UI-TARS-1.5 Model\nIntroduction\nPerformance\nModel Scale Comparison\nWhat's next\nCitation\nUI-TARS-1.5 Model\nWe shared the latest progress of the UI-TARS-1.5 model in our blog, which excels in playing games and performing GUI tasks.\nIntroduction\nUI-TARS-1.5, an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.\nLeveraging the foundational architecture introduced in our recent paper, UI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learning. This allows the model to reason through its thoughts before taking action, significantly enhancing its performance and adaptability, particularly in inference-time scaling. Our new 1.5 version achieves state-of-the-art results across a variety of standard benchmarks, demonstrating strong reasoning capabilities and notable improvements over prior models.\nCode: https://github.com/bytedance/UI-TARS\nApplication: https://github.com/bytedance/UI-TARS-desktop\nPerformance\nOnline Benchmark Evaluation\nBenchmark type\nBenchmark\nUI-TARS-1.5\nOpenAI CUA\nClaude 3.7\nPrevious SOTA\nComputer Use\nOSworld (100 steps)\n42.5\n36.4\n28\n38.1 (200 step)\nWindows Agent Arena (50 steps)\n42.1\n-\n-\n29.8\nBrowser Use\nWebVoyager\n84.8\n87\n84.1\n87\nOnline-Mind2web\n75.8\n71\n62.9\n71\nPhone Use\nAndroid World\n64.2\n-\n-\n59.5\nGrounding Capability Evaluation\nBenchmark\nUI-TARS-1.5\nOpenAI CUA\nClaude 3.7\nPrevious SOTA\nScreensSpot-V2\n94.2\n87.9\n87.6\n91.6\nScreenSpotPro\n61.6\n23.4\n27.7\n43.6\nPoki Game\nModel\n2048\ncubinko\nenergy\nfree-the-key\nGem-11\nhex-frvr\nInfinity-Loop\nMaze:Path-of-Light\nshapes\nsnake-solver\nwood-blocks-3d\nyarn-untangle\nlaser-maze-puzzle\ntiles-master\nOpenAI CUA\n31.04\n0.00\n32.80\n0.00\n46.27\n92.25\n23.08\n35.00\n52.18\n42.86\n2.02\n44.56\n80.00\n78.27\nClaude 3.7\n43.05\n0.00\n41.60\n0.00\n0.00\n30.76\n2.31\n82.00\n6.26\n42.86\n0.00\n13.77\n28.00\n52.18\nUI-TARS-1.5\n100.00\n0.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\nMinecraft\nTask Type\nTask Name\nVPT\nDreamerV3\nPrevious SOTA\nUI-TARS-1.5 w/o Thought\nUI-TARS-1.5 w/ Thought\nMine Blocks\n(oak_log)\n0.8\n1.0\n1.0\n1.0\n1.0\n(obsidian)\n0.0\n0.0\n0.0\n0.2\n0.3\n(white_bed)\n0.0\n0.0\n0.1\n0.4\n0.6\n200 Tasks Avg.\n0.06\n0.03\n0.32\n0.35\n0.42\nKill Mobs\n(mooshroom)\n0.0\n0.0\n0.1\n0.3\n0.4\n(zombie)\n0.4\n0.1\n0.6\n0.7\n0.9\n(chicken)\n0.1\n0.0\n0.4\n0.5\n0.6\n100 Tasks Avg.\n0.04\n0.03\n0.18\n0.25\n0.31\nModel Scale Comparison\nThis table compares performance across different model scales of UI-TARS on the OSworld benchmark.\nBenchmark Type\nBenchmark\nUI-TARS-72B-DPO\nUI-TARS-1.5-7B\nUI-TARS-1.5\nComputer Use\nOSWorld\n24.6\n27.5\n42.5\nGUI Grounding\nScreenSpotPro\n38.1\n49.6\n61.6\nThe released UI-TARS-1.5-7B focuses primarily on enhancing general computer use capabilities and is not specifically optimized for game-based scenarios, where the UI-TARS-1.5 still holds a significant advantage.\nWhat's next\nWe are providing early research access to our top-performing UI-TARS-1.5 model to facilitate collaborative research. Interested researchers can contact us at TARS@bytedance.com.\nCitation\nIf you find our paper and model useful in your research, feel free to give us a cite.\n@article{qin2025ui,\ntitle={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},\nauthor={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},\njournal={arXiv preprint arXiv:2501.12326},\nyear={2025}\n}",
    "AITeamVN/Vietnamese_Embedding_v2": "Model Card: Vietnamese_Embedding_v2\nModel Details\nModel Description\nUsage\nEvaluation:\nContact\nCitation\nModel Card: Vietnamese_Embedding_v2\nVietnamese_Embedding_v2 is an embedding model fine-tuned from the BGE-M3 model (https://huggingface.co/BAAI/bge-reranker-v2-m3) to enhance retrieval capabilities for Vietnamese.\nThe model was trained on approximately 1,100,000 triplets of queries, positive documents, and negative documents for Vietnamese.\nThe model was trained with a maximum sequence length of 2304 (256 for query and 2048 for passages).\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: BAAI/bge-m3\nMaximum Sequence Length: 2048 tokens\nOutput Dimensionality: 1024 dimensions\nSimilarity Function: Dot product Similarity\nLanguage: Vietnamese\nLicence: Apache 2.0\nUsage\nfrom sentence_transformers import SentenceTransformer\nimport torch\nmodel = SentenceTransformer(\"AITeamVN/Vietnamese_Embedding_v2\")\nmodel.max_seq_length = 2048\nsentences_1 = [\"TrÃ­ tuá»‡ nhÃ¢n táº¡o lÃ  gÃ¬\", \"Lá»£i Ã­ch cá»§a giáº¥c ngá»§\"]\nsentences_2 = [\"TrÃ­ tuá»‡ nhÃ¢n táº¡o lÃ  cÃ´ng nghá»‡ giÃºp mÃ¡y mÃ³c suy nghÄ© vÃ  há»c há»i nhÆ° con ngÆ°á»i. NÃ³ hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch thu tháº­p dá»¯ liá»‡u, nháº­n diá»‡n máº«u vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh.\",\n\"Giáº¥c ngá»§ giÃºp cÆ¡ thá»ƒ vÃ  nÃ£o bá»™ nghá»‰ ngÆ¡i, há»“i phá»¥c nÄƒng lÆ°á»£ng vÃ  cáº£i thiá»‡n trÃ­ nhá»›. Ngá»§ Ä‘á»§ giáº¥c giÃºp tinh tháº§n tá»‰nh tÃ¡o vÃ  lÃ m viá»‡c hiá»‡u quáº£ hÆ¡n.\"]\nquery_embedding = model.encode(sentences_1)\ndoc_embeddings = model.encode(sentences_2)\nsimilarity = query_embedding @ doc_embeddings.T\nprint(similarity)\n'''\narray([[0.66212064, 0.33066642],\n[0.25866613, 0.5865289 ]], dtype=float32)\n'''\nEvaluation:\nDataset: Entire training dataset of Legal Zalo 2021. Our model was not trained on this dataset.\nModel\nAccuracy@1\nAccuracy@3\nAccuracy@5\nAccuracy@10\nMRR@10\nVietnamese_Reranker\n0.7944\n0.9324\n0.9537\n0.9740\n0.8672\nVietnamese_Embedding_v2\n0.7262\n0.8927\n0.9268\n0.9578\n0.8149\nVietnamese_Embedding\n0.7274\n0.8992\n0.9305\n0.9568\n0.8181\nVietnamese-bi-encoder (BKAI)\n0.7109\n0.8680\n0.9014\n0.9299\n0.7951\nBGE-M3\n0.5682\n0.7728\n0.8382\n0.8921\n0.6822\nVietnamese_Reranker and Vietnamese_Embedding_v2 was trained on 1,100,000 triplets.\nAlthough the score on the legal domain drops a bit on Vietnamese_Embedding (Phase 2), since this phase data is much larger, it is  good for other domains.\nYou can reproduce the evaluation result by running code python evaluation_model.py (data downloaded from Kaggle).\nContact\nEmail: nguyennhotrung3004@gmail.com\nDeveloper\nMember: Nguyá»…n Nho Trung, Nguyá»…n Nháº­t Quang, Nguyá»…n VÄƒn Huy.\nCitation\n@misc{Vietnamese_Embedding,\ntitle={Vietnamese_Embedding: Embedding model in Vietnamese language.},\nauthor={Nguyen Nho Trung, Nguyen Nhat Quang, Nguyá»…n VÄƒn Huy},\nyear={2025},\npublisher={Huggingface},\n}",
    "google/medgemma-27b-text-it": "Access MedGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access MedGemma on Hugging Face, you're required to review and agree to Health AI Developer Foundation's terms of use. To do this, please ensure you're logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nMedGemma model card\nModel information\nDescription\nHow to use\nExamples\nModel architecture overview\nTechnical specifications\nCitation\nInputs and outputs\nPerformance and validation\nKey performance metrics\nEthics and safety evaluation\nData card\nDataset overview\nData Ownership and Documentation\nData citation\nDe-identification/anonymization:\nImplementation information\nSoftware\nUse and limitations\nIntended use\nBenefits\nLimitations\nMedGemma model card\nModel documentation: MedGemma\nResources:\nModel on Google Cloud Model Garden: MedGemma\nModel on Hugging Face: MedGemma\nGitHub repository (supporting code, Colab notebooks, discussions, and\nissues): MedGemma\nQuick start notebook: GitHub\nFine-tuning notebook: GitHub\nPatient Education Demo built using MedGemma\nSupport: See Contact\nLicense: The use of MedGemma is governed by the Health AI Developer\nFoundations terms of\nuse.\nAuthor: Google\nModel information\nThis section describes the MedGemma model and how to use it.\nDescription\nMedGemma is a collection of Gemma 3\nvariants that are trained for performance on medical text and image\ncomprehension. Developers can use MedGemma to accelerate building\nhealthcare-based AI applications. MedGemma currently comes in two variants: a 4B\nmultimodal version and a 27B text-only version.\nMedGemma 27B has been trained exclusively on medical text and optimized for\ninference-time computation. MedGemma 27B is only available as an\ninstruction-tuned model.\nMedGemma variants have been evaluated on a range of clinically relevant\nbenchmarks to illustrate their baseline performance. These include both open\nbenchmark datasets and curated datasets. Developers can fine-tune MedGemma\nvariants for improved performance. Consult the Intended Use section below for\nmore details.\nA full technical report will be available soon.\nHow to use\nBelow are some example code snippets to help you quickly get started running the\nmodel locally on GPU. If you want to use the model at scale, we recommend that\nyou create a production version using Model\nGarden.\nFirst, install the Transformers library. Gemma 3 is supported starting from\ntransformers 4.50.0.\n$ pip install -U transformers\nRun model with the pipeline API\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"text-generation\",\nmodel=\"google/medgemma-27b-text-it\",\ntorch_dtype=torch.bfloat16,\ndevice=\"cuda\",\n)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful medical assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"How do you differentiate bacterial from viral pneumonia?\"\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\nRun the model directly\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"google/medgemma-27b-text-it\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful medical assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"How do you differentiate bacterial from viral pneumonia?\"\n}\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=200, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = tokenizer.decode(generation, skip_special_tokens=True)\nprint(decoded)\nExamples\nSee the following Colab notebooks for examples of how to use MedGemma:\nTo give the model a quick try, running it locally with weights from Hugging\nFace, see Quick start notebook in\nColab.\nNote that you will need to use Colab Enterprise to run the 27B model without\nquantization.\nFor an example of fine-tuning the model, see the Fine-tuning notebook in\nColab.\nModel architecture overview\nThe MedGemma model is built based on Gemma 3 and\nuses the same decoder-only transformer architecture as Gemma 3. To read more\nabout the architecture, consult the Gemma 3 model\ncard.\nTechnical specifications\nModel type: Decoder-only Transformer architecture, see the Gemma 3\ntechnical\nreport\nModalities: 4B: Text, vision; 27B: Text only\nAttention mechanism: Utilizes grouped-query attention (GQA)\nContext length: Supports long context, at least 128K tokens\nKey publication: Coming soon\nModel created: May 20, 2025\nModel version: 1.0.0\nCitation\nWhen using this model, please cite: Sellergren et al. \"MedGemma Technical Report.\"\narXiv preprint arXiv:2507.05201 (2025)\n@article{sellergren2025medgemma,\ntitle = {MedGemma Technical Report}\nauthor = {Sellergren, Andrew and Kazemzadeh, Sahar and Jaroensri, Tiam and Kiraly, Atilla and Traverse, Madeleine and Kohlberger, Timo and Xu, Shawn and Jamil, Fayaz and Hughes, CÃ­an and Lau, Charles and others},\njournal={arXiv preprint arXiv:2507.05201}\nyear = {2025},\n}\nInputs and outputs\nInput:\nText string, such as a question or prompt\nTotal input length of 128K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a question,\nanalysis of image content, or a summary of a document\nTotal output length of 8192 tokens\nPerformance and validation\nMedGemma was evaluated across a range of different multimodal classification,\nreport generation, visual question answering, and text-based tasks.\nKey performance metrics\nText evaluations\nMedGemma 4B and text-only MedGemma 27B were evaluated across a range of\ntext-only benchmarks for medical knowledge and reasoning.\nThe MedGemma models outperform their respective base Gemma models across all\ntested text-only health benchmarks.\nMetric\nMedGemma 27B\nGemma 3 27B\nMedGemma 4B\nGemma 3 4B\nMedQA (4-op)\n89.8 (best-of-5) 87.7 (0-shot)\n74.9\n64.4\n50.7\nMedMCQA\n74.2\n62.6\n55.7\n45.4\nPubMedQA\n76.8\n73.4\n73.4\n68.4\nMMLU Med (text only)\n87.0\n83.3\n70.0\n67.2\nMedXpertQA (text only)\n26.7\n15.7\n14.2\n11.6\nAfriMed-QA\n84.0\n72.0\n52.0\n48.0\nFor all MedGemma 27B results, test-time\nscaling is used to improve performance.\nEthics and safety evaluation\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild safety: Evaluation of text-to-text and image-to-text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent safety: Evaluation of text-to-text and image-to-text prompts\ncovering safety policies, including harassment, violence and gore, and hate\nspeech.\nRepresentational harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including bias, stereotyping, and harmful\nassociations or inaccuracies.\nGeneral medical harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including information quality and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance evaluations\"\nwhich are our \"arms-length\" internal evaluations for responsibility governance\ndecision making. They are conducted separately from the model development team,\nto inform decision making about release. High-level findings are fed back to the\nmodel team, but prompt sets are held out to prevent overfitting and preserve the\nresults' ability to inform decision making. Notable assurance evaluation results\nare reported to our Responsibility & Safety Council as part of release review.\nEvaluation results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms. All\ntesting was conducted without safety filters to evaluate the model capabilities\nand behaviors. For text-to-text, image-to-text, and audio-to-text, and across\nboth MedGemma model sizes, the model produced minimal policy violations. A\nlimitation of our evaluations was that they included primarily English language\nprompts.\nData card\nDataset overview\nTraining\nThe base Gemma models are pre-trained on a large corpus of text and code data.\nMedGemma 4B utilizes a SigLIP image encoder\nthat has been specifically pre-trained on a variety of de-identified medical\ndata, including radiology images, histopathology images, ophthalmology images,\nand dermatology images. Its LLM component is trained on a diverse set of medical\ndata, including medical text relevant to radiology images, chest-x rays,\nhistopathology patches, ophthalmology images and dermatology images.\nEvaluation\nMedGemma models have been evaluated on a comprehensive set of clinically\nrelevant benchmarks, including over 22 datasets across 5 different tasks and 6\nmedical image modalities. These include both open benchmark datasets and curated\ndatasets, with a focus on expert human evaluations for tasks like CXR report\ngeneration and radiology VQA.\nSource\nMedGemma utilizes a combination of public and private datasets.\nThis model was trained on diverse public datasets including MIMIC-CXR (chest\nX-rays and reports), Slake-VQA (multimodal medical images and questions),\nPAD-UFES-20 (skin lesion images and data), SCIN (dermatology images), TCGA\n(cancer genomics data), CAMELYON (lymph node histopathology images), PMC-OA\n(biomedical literature with images), and Mendeley Digital Knee X-Ray (knee\nX-rays).\nAdditionally, multiple diverse proprietary datasets were licensed and\nincorporated (described next).\nData Ownership and Documentation\nMimic-CXR: MIT Laboratory\nfor Computational Physiology and Beth Israel Deaconess Medical Center\n(BIDMC).\nSlake-VQA: The Hong Kong Polytechnic\nUniversity (PolyU), with collaborators including West China Hospital of\nSichuan University and Sichuan Academy of Medical Sciences / Sichuan\nProvincial People's Hospital.\nPAD-UFES-20: Federal\nUniversity of EspÃ­rito Santo (UFES), Brazil, through its Dermatological and\nSurgical Assistance Program (PAD).\nSCIN: A collaboration\nbetween Google Health and Stanford Medicine.\nTCGA (The Cancer Genome Atlas): A joint\neffort of National Cancer Institute and National Human Genome Research\nInstitute. Data from TCGA are available via the Genomic Data Commons (GDC)\nCAMELYON: The data was\ncollected from Radboud University Medical Center and University Medical\nCenter Utrecht in the Netherlands.\nPMC-OA (PubMed Central Open Access\nSubset):\nMaintained by the National Library of Medicine (NLM) and National Center for\nBiotechnology Information (NCBI), which are part of the NIH.\nMedQA: This dataset was created by a\nteam of researchers led by Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung\nWeng, Hanyi Fang, and Peter Szolovits\nMendeley Digital Knee\nX-Ray: This dataset is\nfrom Rani Channamma University, and is hosted on Mendeley Data.\nAfriMed-QA: This data was developed and led by\nmultiple collaborating organizations and researchers include key\ncontributors: Intron Health, SisonkeBiotik, BioRAMP, Georgia Institute of\nTechnology, and MasakhaneNLP.\nVQA-RAD: This dataset was\ncreated by a research team led by Jason J. Lau, Soumya Gayen, Asma Ben\nAbacha, and Dina Demner-Fushman and their affiliated institutions (the US\nNational Library of Medicine and National Institutes of Health)\nMedExpQA:\nThis dataset was created by researchers at the HiTZ Center (Basque Center\nfor Language Technology and Artificial Intelligence).\nMedXpertQA: This\ndataset was developed by researchers at Tsinghua University (Beijing, China)\nand Shanghai Artificial Intelligence Laboratory (Shanghai, China).\nIn addition to the public datasets listed above, MedGemma was also trained on\nde-identified datasets licensed for research or collected internally at Google\nfrom consented participants.\nRadiology dataset 1: De-identified dataset of different CT studies across\nbody parts from a US-based radiology outpatient diagnostic center network.\nOphthalmology dataset 1: De-identified dataset of fundus images from\ndiabetic retinopathy screening.\nDermatology dataset 1: De-identified dataset of teledermatology skin\ncondition images (both clinical and dermatoscopic) from Colombia.\nDermatology dataset 2: De-identified dataset of skin cancer images (both\nclinical and dermatoscopic) from Australia.\nDermatology dataset 3: De-identified dataset of non-diseased skin images\nfrom an internal data collection effort.\nPathology dataset 1: De-identified dataset of histopathology H&E whole slide\nimages created in collaboration with an academic research hospital and\nbiobank in Europe. Comprises de-identified colon, prostate, and lymph nodes.\nPathology dataset 2: De-identified dataset of lung histopathology H&E and\nIHC whole slide images created by a commercial biobank in the United States.\nPathology dataset 3: De-identified dataset of prostate and lymph node H&E\nand IHC histopathology whole slide images created by a contract research\norganization in the United States.\nPathology dataset 4: De-identified dataset of histopathology, predominantly\nH&E whole slide images created in collaboration with a large, tertiary\nteaching hospital in the United States. Comprises a diverse set of tissue\nand stain types, predominantly H&E.\nData citation\nMIMIC-CXR Johnson, A., Pollard, T., Mark, R., Berkowitz, S., & Horng, S.\n(2024). MIMIC-CXR Database (version 2.1.0). PhysioNet.\nhttps://physionet.org/content/mimic-cxr/2.1.0/\nand Johnson, Alistair E. W., Tom J. Pollard, Seth J. Berkowitz, Nathaniel R.\nGreenbaum, Matthew P. Lungren, Chih-Ying Deng, Roger G. Mark, and Steven\nHorng. 2019. \"MIMIC-CXR, a de-Identified Publicly Available Database of\nChest Radiographs with Free-Text Reports.\" Scientific Data 6 (1): 1â€“8.\nSLAKE Liu, Bo, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.\n2021.SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical\nVisual Question Answering.\" http://arxiv.org/abs/2102.09542.\nPAD-UEFS Pacheco, A. G. C., Lima, G. R., Salomao, A., Krohling, B.,\nBiral, I. P., de Angelo, G. G., Alves, F. O. G., Ju X. M., & P. R. C.\n(2020). PAD-UFES-20: A skin lesion dataset composed of patient data and\nclinical images collected from smartphones. In Proceedings of the 2020 IEEE\nInternational Conference on Bioinformatics and Biomedicine (BIBM) (pp.\n1551-1558). IEEE. https://doi.org/10.1109/BIBM49941.2020.9313241\nSCIN Ward, Abbi, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley\nCarrick, Bilson Campana, Jay Hartford, et al. 2024. \"Creating an Empirical\nDermatology Dataset Through Crowdsourcing With Web Search Advertisements.\"\nJAMA Network Open 7 (11): e2446615â€“e2446615.\nTCGA The results shown here are in whole or part based upon data\ngenerated by the TCGA Research Network: https://www.cancer.gov/tcga.\nCAMELYON16 Ehteshami Bejnordi, Babak, Mitko Veta, Paul Johannes van\nDiest, Bram van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen A. W. M.\nvan der Laak, et al. 2017. \"Diagnostic Assessment of Deep Learning\nAlgorithms for Detection of Lymph Node Metastases in Women With Breast\nCancer.\" JAMA 318 (22): 2199â€“2210.\nMedQA Jin, Di, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang,\nand Peter Szolovits. 2020. \"What Disease Does This Patient Have? A\nLarge-Scale Open Domain Question Answering Dataset from Medical Exams.\"\nhttp://arxiv.org/abs/2009.13081.\nMendeley Digital Knee X-Ray Gornale, Shivanand; Patravali, Pooja (2020),\n\"Digital Knee X-ray Images\", Mendeley Data, V1, doi: 10.17632/t9ndx37v5h.1\nAfrimedQA Olatunji, Tobi, Charles Nimo, Abraham Owodunni, Tassallah\nAbdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, et al. 2024.\n\"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\nBenchmark Dataset.\" http://arxiv.org/abs/2411.15640.\nVQA-RAD Lau, Jason J., Soumya Gayen, Asma Ben Abacha, and Dina\nDemner-Fushman. 2018. \"A Dataset of Clinically Generated Visual Questions\nand Answers about Radiology Images.\" Scientific Data 5 (1): 1â€“10.\nMedexpQA Alonso, I., Oronoz, M., & Agerri, R. (2024). MedExpQA:\nMultilingual Benchmarking of Large Language Models for Medical Question\nAnswering. arXiv preprint arXiv:2404.05590. Retrieved from\nhttps://arxiv.org/abs/2404.05590\nMedXpertQA Zuo, Yuxin, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu,\nErmo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. 2025. \"MedXpertQA:\nBenchmarking Expert-Level Medical Reasoning and Understanding.\"\nhttp://arxiv.org/abs/2501.18362.\nDe-identification/anonymization:\nGoogle and partnerships utilize datasets that have been rigorously anonymized or\nde-identified to ensure the protection of individual research participants and\npatient privacy\nImplementation information\nDetails about the model internals.\nSoftware\nTraining was done using JAX.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nUse and limitations\nIntended use\nMedGemma is an open multimodal generative AI model intended to be used as a\nstarting point that enables more efficient development of downstream healthcare\napplications involving medical text and images. MedGemma is intended for\ndevelopers in the life sciences and healthcare space. Developers are responsible\nfor training, adapting and making meaningful changes to MedGemma to accomplish\ntheir specific intended use. MedGemma models can be fine-tuned by developers\nusing their own proprietary data for their specific tasks or solutions.\nMedGemma is based on Gemma 3 and has been further trained on medical images and\ntext. MedGemma enables further development in any medical context (image and\ntextual), however the model was pre-trained using chest X-ray, pathology,\ndermatology, and fundus images. Examples of tasks within MedGemma's training\ninclude visual question answering pertaining to medical images, such as\nradiographs, or providing answers to textual medical questions. Full details of\nall the tasks MedGemma has been evaluated can be found in an upcoming technical\nreport.\nBenefits\nProvides strong baseline medical image and text comprehension for models of\nits size.\nThis strong performance makes it efficient to adapt for downstream\nhealthcare-based use cases, compared to models of similar size without\nmedical data pre-training.\nThis adaptation may involve prompt engineering, grounding, agentic\norchestration or fine-tuning depending on the use case, baseline validation\nrequirements, and desired performance characteristics.\nLimitations\nMedGemma is not intended to be used without appropriate validation, adaptation\nand/or making meaningful modification by developers for their specific use case.\nThe outputs generated by MedGemma are not intended to directly inform clinical\ndiagnosis, patient management decisions, treatment recommendations, or any other\ndirect clinical practice applications. Performance benchmarks highlight baseline\ncapabilities on relevant benchmarks, but even for image and text domains that\nconstitute a substantial portion of training data, inaccurate model output is\npossible. All outputs from MedGemma should be considered preliminary and require\nindependent verification, clinical correlation, and further investigation\nthrough established research and development methodologies.\nMedGemma's multimodal capabilities have been primarily evaluated on single-image\ntasks. MedGemma has not been evaluated in use cases that involve comprehension\nof multiple images.\nMedGemma has not been evaluated or optimized for multi-turn applications.\nMedGemma's training may make it more sensitive to the specific prompt used than\nGemma 3.\nWhen adapting MedGemma developer should consider the following:\nBias in validation data: As with any research, developers should ensure\nthat any downstream application is validated to understand performance using\ndata that is appropriately representative of the intended use setting for\nthe specific application (e.g., age, sex, gender, condition, imaging device,\netc).\nData contamination concerns: When evaluating the generalization\ncapabilities of a large model like MedGemma in a medical context, there is a\nrisk of data contamination, where the model might have inadvertently seen\nrelated medical information during its pre-training, potentially\noverestimating its true ability to generalize to novel medical concepts.\nDevelopers should validate MedGemma on datasets not publicly available or\notherwise made available to non-institutional researchers to mitigate this\nrisk.",
    "ByteDance-Seed/BAGEL-7B-MoT": "ðŸ¥¯ BAGEL â€¢ Unified Model for Multimodal Understanding and Generation\nðŸ§  Method\nðŸŒ± Emerging Properties\nðŸ“Š Benchmarks\n1. Visual Understanding\n2. Text-to-Image Generation Â· GenEval\n3. Image Editing\nLicense\nâœï¸ Citation\nðŸ¥¯ BAGEL â€¢ Unified Model for Multimodal Understanding and Generation\nWe present BAGEL, an openâ€‘source multimodal foundation model with 7B active parameters (14B total) trained on largeâ€‘scale interleaved multimodal data. BAGEL outperforms the current topâ€‘tier openâ€‘source VLMs like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards, and delivers textâ€‘toâ€‘image quality that is competitive with strong specialist generators such as SD3.\nMoreover, BAGEL demonstrates superior qualitative results in classical imageâ€‘editing scenarios than the leading open-source models. More importantly, it extends to free-form visual manipulation, multiview synthesis, and world navigation, capabilities that constitute \"world-modeling\" tasks beyond the scope of previous image-editing models.\nThis repository hosts the model weights for BAGEL. For installation, usage instructions, and further documentation, please visit our GitHub repository.\nðŸ§  Method\nBAGEL adopts a Mixture-of-Transformer-Experts (MoT) architecture to maximize the modelâ€™s capacity to learn from richly diverse multimodal information. Following the same principle of capacity maximization, it utilizes two separate encoders to capture pixel-level and semantic-level features of an image. The overall framework follows a Next Group of Token Prediction paradigm, where the model is trained to predict the next group of language or visual tokens as a compression target.\nBAGEL scales MoTâ€™s capacity through Pre-training, Continued Training, and Supervised Finetuning on trillions of interleaved multimodal tokens spanning language, image, video, and web data. It surpasses open models on standard understanding and generation benchmarks and demonstrates advanced in-context multimodal abilities like free-form image editing, future frame prediction, 3D manipulation, world navigation, and sequential reasoning.\nðŸŒ± Emerging Properties\nAs we scale up BAGELâ€™s pretraining with more multimodal tokens, we observe consistent performance gains across understanding, generation, and editing tasks. Different capabilities emerge at distinct training stagesâ€”multimodal understanding and generation appear early, followed by basic editing, while complex, intelligent editing emerges later. This staged progression suggests an emergent pattern, where advanced multimodal reasoning builds on well-formed foundational skills. Ablation studies further show that combining VAE and ViT features significantly improves intelligent editing, underscoring the importance of visual-semantic context in enabling complex multimodal reasoning and further supporting its role in the emergence of advanced capabilities.\nðŸ“Š Benchmarks\n1. Visual Understanding\nModel\nMME â†‘\nMMBench â†‘\nMMMU â†‘\nMM-Vet â†‘\nMathVista â†‘\nJanus-Pro-7B\n-\n79.2\n41.0\n50.0\nâ€“\nQwen2.5-VL-7B\n2347\n83.5\n58.6\n67.1\n68.2\nBAGEL\n2388\n85.0\n55.3\n67.2\n73.1\n2. Text-to-Image Generation Â· GenEval\nModel\nOverall â†‘\nFLUX-1-dev\n0.82\nSD3-Medium\n0.74\nJanus-Pro-7B\n0.80\nBAGEL\n0.88\n3. Image Editing\nModel\nGEdit-Bench-EN (SC) â†‘\nGEdit-Bench-EN (PQ) â†‘\nGEdit-Bench-EN (O) â†‘\nIntelligentBench â†‘\nStep1X-Edit\n7.09\n6.76\n6.70\n14.9\nGemini-2-exp.\n6.73\n6.61\n6.32\n57.6\nBAGEL\n7.36\n6.83\n6.52\n44.0\nBAGEL+CoT\nâ€“\nâ€“\nâ€“\n55.3\nLicense\nBAGEL is licensed under the Apache 2.0 license. It is finetuned from Qwen2.5-7B-Instruct and siglip-so400m-14-384-flash-attn2 model, and uses the FLUX.1-schnell VAE model, all under Apache 2.0.\nâœï¸ Citation\n@article{deng2025bagel,\ntitle   = {Emerging Properties in Unified Multimodal Pretraining},\nauthor  = {Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and Shi, Guang and Fan, Haoqi},\njournal = {arXiv preprint arXiv:2505.14683},\nyear    = {2025}\n}",
    "Qwen/Qwen3-Reranker-8B": "Qwen3-Reranker-8B\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nTransformers Usage\nEvaluation\nCitation\nQwen3-Reranker-8B\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Reranker-8B has the following features:\nModel Type: Text Reranking\nSupported Languages: 100+ Languages\nNumber of Paramaters: 8B\nContext Length: 32k\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\nKeyError: 'qwen3'\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\ndef format_instruction(instruction, query, doc):\nif instruction is None:\ninstruction = 'Given a web search query, retrieve relevant passages that answer the query'\noutput = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\nreturn output\ndef process_inputs(pairs):\ninputs = tokenizer(\npairs, padding=False, truncation='longest_first',\nreturn_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n)\nfor i, ele in enumerate(inputs['input_ids']):\ninputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\ninputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\nfor key in inputs:\ninputs[key] = inputs[key].to(model.device)\nreturn inputs\n@torch.no_grad()\ndef compute_logits(inputs, **kwargs):\nbatch_scores = model(**inputs).logits[:, -1, :]\ntrue_vector = batch_scores[:, token_true_id]\nfalse_vector = batch_scores[:, token_false_id]\nbatch_scores = torch.stack([false_vector, true_vector], dim=1)\nbatch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\nscores = batch_scores[:, 1].exp().tolist()\nreturn scores\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-8B\", padding_side='left')\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-8B\").eval()\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-8B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\ntoken_false_id = tokenizer.convert_tokens_to_ids(\"no\")\ntoken_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\nmax_length = 8192\nprefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\nsuffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\nprefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\nsuffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\"What is the capital of China?\",\n\"Explain gravity\",\n]\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\npairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n# Tokenize the input texts\ninputs = process_inputs(pairs)\nscores = compute_logits(inputs)\nprint(\"scores: \", scores)\nðŸ“Œ Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nEvaluation\nModel\nParam\nMTEB-R\nCMTEB-R\nMMTEB-R\nMLDR\nMTEB-Code\nFollowIR\nQwen3-Embedding-0.6B\n0.6B\n61.82\n71.02\n64.64\n50.26\n75.41\n5.09\nJina-multilingual-reranker-v2-base\n0.3B\n58.22\n63.37\n63.73\n39.66\n58.98\n-0.68\ngte-multilingual-reranker-base\n0.3B\n59.51\n74.08\n59.44\n66.33\n54.18\n-1.64\nBGE-reranker-v2-m3\n0.6B\n57.03\n72.16\n58.36\n59.51\n41.38\n-0.01\nQwen3-Reranker-0.6B\n0.6B\n65.80\n71.31\n66.36\n67.28\n73.42\n5.41\nQwen3-Reranker-4B\n4B\n69.76\n75.94\n72.74\n69.97\n81.20\n14.84\nQwen3-Reranker-8B\n8B\n69.02\n77.45\n72.94\n70.19\n81.22\n8.05\nNote:\nEvaluation results for reranking models. We use the retrieval subsets of MTEB(eng, v2), MTEB(cmn, v1), MMTEB and MTEB (Code), which are MTEB-R, CMTEB-R, MMTEB-R and MTEB-Code.\nAll scores are our runs based on the top-100 candidates retrieved by dense embedding model Qwen3-Embedding-0.6B.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF": "DeepSeek-R1-0528 Model Card\n1. Introduction\n2. Evaluation Results\nDeepSeek-R1-0528\nDeepSeek-R1-0528-Qwen3-8B\n3. Chat Website & API Platform\n4. How to Run Locally\nSystem Prompt\nTemperature\nPrompts for File Uploading and Web Search\n5. License\n6. Citation\n7. Contact\nLearn how to run DeepSeek-R1-0528 correctly - Read our Guide.\nSee our collection for all versions of R1 including GGUF, 4-bit & 16-bit formats.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nðŸ‹ DeepSeek-R1-0528-Qwen3-8B Usage Guidelines\nFor Ollama do ollama run hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_XL - it'll auto get the correct chat template and all settings\nSet the temperature between 0.5â€“0.7 (0.6 recommended) to reduce repetition and incoherence.\nSet Top_P value of 0.95 (recommended)\nR1-0528 uses the same chat template as the original R1 model:\n<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>It's 2.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>Explain more!<ï½œAssistantï½œ>\nFor llama.cpp / GGUF inference, you should skip the BOS since itâ€™ll auto add it:\n<ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>\nFor complete detailed instructions, see our guide: unsloth.ai/blog/deepseek-r1-0528\nDeepSeek-R1-0528 Model Card\nPaper LinkðŸ‘ï¸\n1. Introduction\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the modelâ€™s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\n2. Evaluation Results\nDeepSeek-R1-0528\nFor all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nDeepSeek R1\nDeepSeek R1 0528\nGeneral\nMMLU-Redux (EM)\n92.9\n93.4\nMMLU-Pro (EM)\n84.0\n85.0\nGPQA-Diamond (Pass@1)\n71.5\n81.0\nSimpleQA (Correct)\n30.1\n27.8\nFRAMES (Acc.)\n82.5\n83.0\nHumanity's Last Exam (Pass@1)\n8.5\n17.7\nCode\nLiveCodeBench (2408-2505) (Pass@1)\n63.5\n73.3\nCodeforces-Div1 (Rating)\n1530\n1930\nSWE Verified (Resolved)\n49.2\n57.6\nAider-Polyglot (Acc.)\n53.3\n71.6\nMath\nAIME 2024 (Pass@1)\n79.8\n91.4\nAIME 2025 (Pass@1)\n70.0\n87.5\nHMMT 2025 (Pass@1)\n41.7\n79.4\nCNMO 2024 (Pass@1)\n78.8\n86.9\nTools\nBFCL_v3_MultiTurn (Acc)\n-\n37.0\nTau-Bench   (Pass@1)\n-\n53.5(Airline)/63.9(Retail)\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\nDeepSeek-R1-0528-Qwen3-8B\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\nAIME 24\nAIME 25\nHMMT Feb 25\nGPQA Diamond\nLiveCodeBench (2408-2505)\nQwen3-235B-A22B\n85.7\n81.5\n62.5\n71.1\n66.5\nQwen3-32B\n81.4\n72.9\n-\n68.4\n-\nQwen3-8B\n76.0\n67.3\n-\n62.0\n-\nPhi-4-Reasoning-Plus-14B\n81.3\n78.0\n53.6\n69.3\n-\nGemini-2.5-Flash-Thinking-0520\n82.3\n72.0\n64.2\n82.8\n62.3\no3-mini (medium)\n79.6\n76.7\n53.3\n76.8\n65.9\nDeepSeek-R1-0528-Qwen3-8B\n86.0\n76.3\n61.5\n61.1\n60.5\n3. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n4. How to Run Locally\nPlease visit DeepSeek-R1 repository for more information about running DeepSeek-R1-0528 locally.\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\nSystem prompt is supported now.\nIt is not required to add \"<think>\\n\" at the beginning of the output to force the model into thinking pattern.\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.\nSystem Prompt\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\nè¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\nä»Šå¤©æ˜¯{current date}ã€‚\nFor example,\nè¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\nä»Šå¤©æ˜¯2025å¹´5æœˆ28æ—¥ï¼Œæ˜ŸæœŸä¸€ã€‚\nTemperature\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6.\nPrompts for File Uploading and Web Search\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\nfile_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\nFor Chinese query, we use the prompt:\nsearch_answer_zh_template = \\\n'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºŽç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æžœ:\n{search_results}\nåœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æžœä¸­ï¼Œæ¯ä¸ªç»“æžœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æžœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åŽè¿”å›žå¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚\nåœ¨å›žç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n- ä»Šå¤©æ˜¯{cur_date}ã€‚\n- å¹¶éžæœç´¢ç»“æžœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸Žç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æžœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚\n- å¯¹äºŽåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæŽ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€èŽ·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éžå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æžœæœªæä¾›çš„å†…å®¹ã€‚\n- å¯¹äºŽåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æžœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æžå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸Žä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºŽæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æŽ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›žç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚\n- å¦‚æžœå›žç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æž„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æžœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æŽ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚\n- å¯¹äºŽå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æžœé—®é¢˜çš„ç­”æ¡ˆéžå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚\n- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›žç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾Žè§‚çš„å›žç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚\n- ä½ çš„å›žç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›žç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚\n- é™¤éžç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›žç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚\n# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š\n{question}'''\nFor English query, we use the prompt:\nsearch_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n# The user's message is:\n{question}'''\n5. License\nThis code repository is licensed under MIT License. The use of DeepSeek-R1 models is also subject to MIT License. DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\n6. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n7. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "vrgamedevgirl84/Wan14BT2VFusioniX": "ðŸŒ€ Wan2.1_14B_FusionX\nðŸš€ Overview\nðŸ’¡ Inside the Fusion\nðŸš¨âœ¨Hey guys! Just a quick update!\nðŸŒ€ Preview Gallery\nðŸ“‚ Workflows & Model Downloads\nðŸ§  GGUF Variants:\nðŸŽ¬ Example Videos\nðŸ”§ Usage Details\nText-to-Video\nImage-to-Video\nðŸ›  Technical Notes\nðŸ§ª Performance Tips\nðŸ§  Prompt Help\nðŸ“£ Join The Community\nâš–ï¸ License\nðŸ™ Credits\nðŸŒ€ Wan2.1_14B_FusionX\nHigh-Performance Merged Text-to-Video ModelBuilt on WAN 2.1 and fused with research-grade components for cinematic motion, detail, and speed â€” optimized for ComfyUI and rapid iteration in as few as 6 steps.\nMerged models for faster, richer motion & detail â€” high performance even at just 8 steps.\nðŸ“Œ Important: To match the quality shown here, use the linked workflows or make sure to follow the recommended settings outlined below.\nðŸš€ Overview\nA powerful text-to-video model built on top of WAN 2.1 14B, merged with several research-grade models to boost:\nMotion quality\nScene consistency\nVisual detail\nComparable with closed-source solutions, but open and optimized for ComfyUI workflows.\nðŸ’¡ Inside the Fusion\nThis model is made up of the following which is on TOP of Wan 2.1 14B 720p(FusionX would not be what it is without these Models):\nCausVid â€“ Causal motion modeling for better flow and dynamics\nAccVideo â€“ Better temporal alignment and speed boost\nMoviiGen1.1 â€“ Cinematic smoothness and lighting\nMPS Reward LoRA â€“ Tuned for motion and detail\nCustom LoRAs â€“ For texture, clarity, and small detail enhancements (Set at a very low level)\nAll merged models are provided for research and non-commercial use only.\nSome components are subject to licenses such as CC BY-NC-SA 4.0, and do not fall under permissive licenses like Apache 2.0 or MIT.\nPlease refer to each modelâ€™s original license for full usage terms.\nðŸš¨âœ¨Hey guys! Just a quick update!\nWe finally cooked up FusionX LoRAs!! ðŸ§ ðŸ’¥This is huge â€“ now you can plug FusionX into your favorite workflows as a LoRA on top of the Wan base models and SkyReels models!ðŸ”ŒðŸ’«\nYou can still stick with the base FusionX Model if you already use it, but if you would rather have more control over the \"FusionX\" strength and a speed boost, then this might be for you.\nOh, and thereâ€™s a nice speed boost too! âš¡Example: (RTX 5090)\nFusionX as a full base model: 8 steps = 160s â±ï¸\nFusionX as a LoRA on Wan 2.1 14B fp8 T2V: 8 steps = 120s ðŸš€\nBonus: You can bump up the FusionX LoRA strength and lower your steps for a huge speed boost while testing/drafting.Example: strength 2.00 with 3 steps takes 72 seconds.Or lower the strength to experiment with a less â€œFusionXâ€ look. âš¡ðŸ”\nWeâ€™ve got:\nT2V (Text to Video) ðŸŽ¬ â€“ works perfectly with VACE âš™ï¸\nI2V (Image to Video) ðŸ–¼ï¸âž¡ï¸ðŸ“½ï¸\nA dedicated Phantom LoRA ðŸ‘»The new LoRA's are HERE\nNote: The LoRa's are not meant to be put on top of the FusionX main models and instead you would use them with the Wan base models.\nNew workflows  are HERE  ðŸ› ï¸ðŸš€\nAfter lots of testing ðŸ§ª, the video quality with the LoRA is just as good (and sometimes even better! ðŸ’¯)Thatâ€™s thanks to it being trained on the fp16 version of FusionX ðŸ§¬ðŸ’Ž\nðŸŒ€ Preview Gallery\nThese are compressed GIF previews for quick viewing â€” final video outputs are higher quality.\nðŸ“‚ Workflows & Model Downloads\nðŸ’¡ ComfyUI workflows can be found here:ðŸ‘‰ Workflow Collection (WIP)\nðŸ“¦ Model files (T2V, I2V, Phantom, VACE):ðŸ‘‰ Main Hugging Face Repo\nðŸ§  GGUF Variants:\nðŸ–¼ï¸ FusionX Image-to-Video (GGUF)\nðŸŽ¥ FusionX Text-to-Video (GGUF)\nðŸŽžï¸ FusionX T2V VACE (for native)\nðŸ‘» FusionX Phantom\nðŸŽ¬ Example Videos\nWant to see what FusionX can do? Check out these real outputs generated using the latest workflows and settings:\nText-to-VideoðŸ‘‰ Watch Examples\nImage-to-VideoðŸ‘‰ Watch Examples\nPhantom ModeðŸ‘‰ Watch Examples\nVACE IntegrationðŸ‘‰ Watch Examples\nðŸ”§ Usage Details\nText-to-Video\nCGF: Must be set to 1\nShift:\n1024x576: Start at 1\n1080x720: Start at 2\nFor realism â†’ lower values\nFor stylized â†’ test 3â€“9\nScheduler:\nRecommended: uni_pc\nAlternative: flowmatch_causvid (better for some details)\nImage-to-Video\nCGF: 1\nShift: 2 works best in most cases\nScheduler:\nRecommended: dmp++_sde/beta\nTo boost motion and reduce slow-mo effect:\nFrame count: 121\nFPS: 24\nðŸ›  Technical Notes\nWorks in as few as 6 steps\nBest quality at 8â€“10 steps\nDrop-in replacement for Wan2.1-T2V-14B\nUp to 50% faster rendering, especially with SageAttn\nWorks natively and with Kaji Wan WrapperWrapper GitHub\nDo not re-add merged LoRAs (CausVid, AccVideo, MPS)\nFeel free to add other LoRAs for style/variation\nNative WAN workflows also supported (slightly slower)\nðŸ§ª Performance Tips\nRTX 5090 â†’ ~138 sec/video at 1024x576 / 81 frames\nIf VRAM is limited:\nEnable block swapping\nStart with 5 blocks and adjust as needed\nUse SageAttn for ~30% speedup (wrapper only)\nDo not use teacache\n\"Enhance a video\" (tested): Adds vibrance (try values 2â€“4)\n\"SLG\" not tested â€” feel free to explore\nðŸ§  Prompt Help\nWant better cinematic prompts? Try the WAN Cinematic Video Prompt Generator GPT â€” it adds visual richness and makes a big difference in quality. Download Here\nðŸ“£ Join The Community\nWeâ€™re building a friendly space to chat, share outputs, and get help.\nMotion LoRAs coming soon\nTips, updates, and support from other users\nðŸ‘‰ Join the Discord\nâš–ï¸ License\nSome merged components use permissive licenses (Apache 2.0 / MIT),but others â€” such as those from research models like CausVid â€” may be released under non-commercial licenses (e.g., CC BY-NC-SA 4.0).\nâœ… You can use, modify, and redistribute under original license terms\nâ— You must retain and respect the license of each component\nâš ï¸ Commercial use is not permitted for models or components under non-commercial licenses\nðŸ“Œ Outputs are not automatically licensed â€” do your own due diligence\nThis model is intended for research, education, and personal use only.For commercial use or monetization, please consult a legal advisor and verify all component licenses.\nðŸ™ Credits\nWAN Team (base model)\naejion (AccVideo)\nTianwei Yin (CausVid)\nZuluVision (MoviiGen)\nAlibaba PAI (MPS LoRA)\nKijai (ComfyUI Wrapper)\nAnd thanks to the open-source community!",
    "nvidia/diar_streaming_sortformer_4spk-v2": "Streaming Sortformer Diarizer 4spk v2\nModel Architecture\nNVIDIA NeMo\nHow to Use this Model\nLoading the Model\nInput Format\nSetting up Streaming Configuration\nGetting Diarization Results\nInput\nOutput\nTrain and evaluate Sortformer diarizer using NeMo\nTraining\nInference\nTechnical Limitations\nDatasets\nTraining Datasets (Real conversations)\nTraining Datasets (Used to simulate audio mixtures)\nPerformance\nEvaluation data specifications\nDiarization Error Rate (DER)\nNVIDIA Riva: Deployment\nReferences\nLicence\nStreaming Sortformer Diarizer 4spk v2\n|\nThis model is a streaming version of Sortformer diarizer. Sortformer[1] is a novel end-to-end neural model for speaker diarization, trained with unconventional objectives compared to existing end-to-end diarization models.\nStreaming Sortformer[2] employs an Arrival-Order Speaker Cache (AOSC) to store frame-level acoustic embeddings of previously observed speakers.\nSortformer resolves permutation problem in diarization following the arrival-time order of the speech segments from each speaker.\nModel Architecture\nStreaming sortformer employs pre-encode layer in the Fast-Conformer to generate speaker-cache. At each step, speaker cache is filtered to only retain the high-quality speaker cache vectors.\nAside from speaker-cache management part, streaming Sortformer follows the architecture of the offline version of Sortformer. Sortformer consists of an L-size (17 layers) NeMo Encoder for\nSpeech Tasks (NEST)[3] which is based on Fast-Conformer[4] encoder. Following that, an 18-layer Transformer[5] encoder with hidden size of 192,\nand two feedforward layers with 4 sigmoid outputs for each frame input at the top layer. More information can be found in the Streaming Sortformer paper[2].\nNVIDIA NeMo\nTo train, fine-tune or perform diarization with Sortformer, you will need to install NVIDIA NeMo[6]. We recommend you install it after you've installed Cython and latest PyTorch version.\napt-get update && apt-get install -y libsndfile1 ffmpeg\npip install Cython packaging\npip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]\nHow to Use this Model\nThe model is available for use in the NeMo Framework[6], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\nLoading the Model\nfrom nemo.collections.asr.models import SortformerEncLabelModel\n# load model from Hugging Face model card directly (You need a Hugging Face token)\ndiar_model = SortformerEncLabelModel.from_pretrained(\"nvidia/diar_streaming_sortformer_4spk-v2\")\n# If you have a downloaded model in \"/path/to/diar_streaming_sortformer_4spk-v2.nemo\", load model from a downloaded file\ndiar_model = SortformerEncLabelModel.restore_from(restore_path=\"/path/to/diar_streaming_sortformer_4spk-v2.nemo\", map_location='cuda', strict=False)\n# switch to inference mode\ndiar_model.eval()\nInput Format\nInput to Sortformer can be an individual audio file:\naudio_input=\"/path/to/multispeaker_audio1.wav\"\nor a list of paths to audio files:\naudio_input=[\"/path/to/multispeaker_audio1.wav\", \"/path/to/multispeaker_audio2.wav\"]\nor a jsonl manifest file:\naudio_input=\"/path/to/multispeaker_manifest.json\"\nwhere each line is a dictionary containing the following fields:\n# Example of a line in `multispeaker_manifest.json`\n{\n\"audio_filepath\": \"/path/to/multispeaker_audio1.wav\",  # path to the input audio file\n\"offset\": 0, # offset (start) time of the input audio\n\"duration\": 600,  # duration of the audio, can be set to `null` if using NeMo main branch\n}\n{\n\"audio_filepath\": \"/path/to/multispeaker_audio2.wav\",\n\"offset\": 900,\n\"duration\": 580,\n}\nSetting up Streaming Configuration\nStreaming configuration is defined by the following parameters, all measured in 80ms frames:\nCHUNK_SIZE: The number of frames in a processing chunk.\nRIGHT_CONTEXT: The number of future frames attached after the chunk.\nFIFO_SIZE: The number of previous frames attached before the chunk, from the FIFO queue.\nUPDATE_PERIOD: The number of frames extracted from the FIFO queue to update the speaker cache.\nSPEAKER_CACHE_SIZE: The total number of frames in the speaker cache.\nHere are recommended configurations for different scenarios:\nConfiguration\nLatency\nRTF\nCHUNK_SIZE\nRIGHT_CONTEXT\nFIFO_SIZE\nUPDATE_PERIOD\nSPEAKER_CACHE_SIZE\nvery high latency\n30.4s\n0.002\n340\n40\n40\n300\n188\nhigh latency\n10.0s\n0.005\n124\n1\n124\n124\n188\nlow latency\n1.04s\n0.093\n6\n7\n188\n144\n188\nultra low latency\n0.32s\n0.180\n3\n1\n188\n144\n188\nFor clarity on the metrics used in the table:\nLatency: Refers to Input Buffer Latency, calculated as CHUNK_SIZE + RIGHT_CONTEXT. This value does not include computational processing time.\nReal-Time Factor (RTF): Characterizes processing speed, calculated as the time taken to process an audio file divided by its duration. RTF values are measured with a batch size of 1 on an NVIDIA RTX 6000 Ada Generation GPU.\nTo set streaming configuration, use:\ndiar_model.sortformer_modules.chunk_len = CHUNK_SIZE\ndiar_model.sortformer_modules.chunk_right_context = RIGHT_CONTEXT\ndiar_model.sortformer_modules.fifo_len = FIFO_SIZE\ndiar_model.sortformer_modules.spkcache_update_period = UPDATE_PERIOD\ndiar_model.sortformer_modules.spkcache_len = SPEAKER_CACHE_SIZE\ndiar_model.sortformer_modules._check_streaming_parameters()\nGetting Diarization Results\nTo perform speaker diarization and get a list of speaker-marked speech segments in the format 'begin_seconds, end_seconds, speaker_index', simply use:\npredicted_segments = diar_model.diarize(audio=audio_input, batch_size=1)\nTo obtain tensors of speaker activity probabilities, use:\npredicted_segments, predicted_probs = diar_model.diarize(audio=audio_input, batch_size=1, include_tensor_outputs=True)\nInput\nThis model accepts single-channel (mono) audio sampled at 16,000 Hz.\nThe actual input tensor is a Ns x 1 matrix for each audio clip, where Ns is the number of samples in the time-series signal.\nFor instance, a 10-second audio clip sampled at 16,000 Hz (mono-channel WAV file) will form a 160,000 x 1 matrix.\nOutput\nThe output of the model is an T x S matrix, where:\nS is the maximum number of speakers (in this model, S = 4).\nT is the total number of frames, including zero-padding. Each frame corresponds to a segment of 0.08 seconds of audio.Each element of the T x S matrix represents the speaker activity probability in the [0, 1] range.  For example, a matrix element a(150, 2) = 0.95 indicates a 95% probability of activity for the second speaker during the time range [12.00, 12.08] seconds.\nTrain and evaluate Sortformer diarizer using NeMo\nTraining\nSortformer diarizer models are trained on 8 nodes of 8Ã—NVIDIA Tesla V100 GPUs. We use 90 second long training samples and batch size of 4.\nThe model can be trained using this example script and base config.\nInference\nSortformer diarizer models can be performed with post-processing algorithms using inference example script. If you provide the post-processing YAML configs in post_processing folder to reproduce the optimized post-processing algorithm for each development dataset.\nTechnical Limitations\nThe model operates in a streaming mode (online mode).\nIt can detect a maximum of 4 speakers; performance degrades on recordings with 5 and more speakers.\nWhile the model is designed for long-form audio and can handle recordings that are several hours long, performance may degrade on very long recordings.\nThe model was trained on publicly available speech datasets, primarily in English. As a result:\nPerformance may degrade on non-English speech.\nPerformance may also degrade on out-of-domain data, such as recordings in noisy conditions.\nDatasets\nSortformer was trained on a combination of 2445 hours of real conversations and 5150 hours or simulated audio mixtures generated by NeMo speech data simulator[7].\nAll the datasets listed above are based on the same labeling method via RTTM format. A subset of RTTM files used for model training are processed for the speaker diarization model training purposes.\nData collection methods vary across individual datasets. For example, the above datasets include phone calls, interviews, web videos, and audiobook recordings. Please refer to the Linguistic Data Consortium (LDC) website or dataset webpage for detailed data collection methods.\nTraining Datasets (Real conversations)\nFisher English (LDC)\nAMI Meeting Corpus\nVoxConverse-v0.3\nICSI\nAISHELL-4\nThird DIHARD Challenge Development (LDC)\n2000 NIST Speaker Recognition Evaluation, split1 (LDC)\nDiPCo\nAliMeeting\nTraining Datasets (Used to simulate audio mixtures)\n2004-2010 NIST Speaker Recognition Evaluation (LDC)\nLibrispeech\nPerformance\nEvaluation data specifications\nDataset\nNumber of speakers\nNumber of Sessions\nDIHARD III Eval <=4spk\n1-4\n219\nDIHARD III Eval >=5spk\n5-9\n40\nDIHARD III Eval full\n1-9\n259\nCALLHOME-part2 2spk\n2\n148\nCALLHOME-part2 3spk\n3\n74\nCALLHOME-part2 4spk\n4\n20\nCALLHOME-part2 5spk\n5\n5\nCALLHOME-part2 6spk\n6\n3\nCALLHOME-part2 full\n2-6\n250\nCH109\n2\n109\nDiarization Error Rate (DER)\nAll evaluations include overlapping speech.\nCollar tolerance is 0s for DIHARD III Eval, and 0.25s for CALLHOME-part2 and CH109.\nPost-Processing (PP) is optimized on two different held-out dataset splits.  - DIHARD III Dev Optimized Post-Processing for DIHARD III Eval  - CALLHOME-part1 Optimized Post-Processing for CALLHOME-part2 and CH109\nLatency\nPP\nDIHARD III Eval <=4spk\nDIHARD III Eval >=5spk\nDIHARD III Eval full\nCALLHOME-part2 2spk\nCALLHOME-part2 3spk\nCALLHOME-part2 4spk\nCALLHOME-part2 5spk\nCALLHOME-part2 6spk\nCALLHOME-part2 full\nCH109\n30.4s\nno\n14.63\n40.74\n19.68\n6.27\n10.27\n12.30\n19.08\n28.09\n10.50\n5.03\n30.4s\nyes\n13.45\n41.40\n18.85\n5.34\n9.22\n11.29\n18.84\n27.29\n9.54\n4.61\n10.0s\nno\n14.90\n41.06\n19.96\n6.96\n11.05\n12.93\n20.47\n28.10\n11.21\n5.28\n10.0s\nyes\n13.75\n41.41\n19.10\n6.05\n9.88\n11.72\n19.66\n27.37\n10.15\n4.80\n1.04s\nno\n14.49\n42.22\n19.85\n7.51\n11.45\n13.75\n23.22\n29.22\n11.89\n5.37\n1.04s\nyes\n13.24\n42.56\n18.91\n6.57\n10.05\n12.44\n21.68\n28.74\n10.70\n4.88\n0.32s\nno\n14.64\n43.47\n20.19\n8.63\n12.91\n16.19\n29.40\n30.60\n13.57\n6.46\n0.32s\nyes\n13.44\n43.73\n19.28\n6.91\n10.45\n13.70\n27.04\n28.58\n11.38\n5.27\nNVIDIA Riva: Deployment\nStreaming Sortformer is deployed via NVIDIA RIVA ASR - Speech Recognition with Speaker Diarization\nNVIDIA Riva, is an accelerated speech AI SDK deployable on-prem, in all clouds, multi-cloud, hybrid, on edge, and embedded.\nAdditionally, Riva provides:\nWorld-class out-of-the-box accuracy for the most common languages with model checkpoints trained on proprietary data with hundreds of thousands of GPU-compute hours\nBest in class accuracy with run-time word boosting (e.g., brand and product names) and customization of acoustic model, language model, and inverse text normalization\nStreaming speech recognition, Kubernetes compatible scaling, and enterprise-grade support\nFor more information on NVIDIA RIVA, see the list of supported models is here.Also check out the Riva live demo.\nReferences\n[1] Sortformer: Seamless Integration of Speaker Diarization and ASR by Bridging Timestamps and Tokens\n[2] Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering\n[3] NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks\n[4] Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\n[5] Attention is all you need\n[6] NVIDIA NeMo Framework\n[7] NeMo speech data simulator\nLicence\nLicense to use this model is covered by the CC-BY-4.0. By downloading the public and release version of the model, you accept the terms and conditions of the CC-BY-4.0 license.",
    "google/gemma-3n-E2B-it-litert-lm": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3n model card\nModel Information\nDescription\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nThis repository corresponds to Gemma models. You can try it out with:\nGoogle AI Edge Gallery for Android through Open Beta in the Play Store\nGoogle AI Edge Gallery for Android through GitHub\nGoogle AI Studio\nGoogle AI Edge for Web\nThe latest Gemma 3n E2B and E4B model checkpoints support multimodal inputs,\nincluding text, vision, and audio. To test the capabilities, install\nGoogle AI Edge Gallery\nfrom the Play Store or download the app\nfrom GitHub. You can then access download the model file\nand navigate to the 'Ask Image' or 'Audio Scribe' for interactive\ndemonstrations.\nGemma 3n models have a novel architecture that allows them to run with a\nsmaller number of effective parameters. They also have a Matformer\narchitecture that allows nesting multiple models. Learn more about these\ntechniques in the Gemma documentation.\nGemma 3n model card\nModel Page: Gemma 3n\nResources and Technical Documentation:\nLiteRT-LM Github & Documentation\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGoogle AI Edge for Web\nTerms of Use: TermsAuthors: Google DeepMind\nPerformance Benchmarks with LiteRT-LM\nThese numbers will continue to improve while LiteRT-LM is in preview.\nModel\nDevice\nBackend\nPrefill (tokens/sec)\nDecode (tokens/sec)\nGemma3n-E2B\nMacbook Pro 2023 M3\nCPU\n232.5\n27.6\nGemma3n-E2B\nSamsung S24 Ultra\nCPU\n110.5\n16.1\nGemma3n-E2B\nSamsung S24 Ultra\nGPU\n816.4\n15.6\nQuantization: quantized model with int4 weights and float activations.\nThe inference on CPU is accelerated via the LiteRT XNNPACK delegate with 4 threads\nBenchmark on CPU is done assuming XNNPACK cache is enabled\nBenchmark on GPU is done assuming model is cached\nCpufreq governor is set to performance during benchmark. Observed performance may vary depending on your phoneâ€™s hardware and current activity level.\nPerformance Benchmarks with Google AI Edge for Web\nModel\nDevice\nBackend\nPrefill (tokens/sec)\nDecode (tokens/sec)\nGemma3n-E2B\nMacbook Pro 2024 M4\nGPU\n2265\n47.6\nQuantization: quantized model with int4 weights and float activations.\nThese benchmarks were recorded in Chrome using text input.\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma models are well-suited for a variety of content understanding tasks,\nincluding question answering, summarization, and reasoning. Their relatively\nsmall size makes it possible to deploy them in environments with limited\nresources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nGemma 3n models are designed for efficient execution on low-resource devices.\nThey are capable of multimodal input, handling text, image, video, and audio\ninput, and generating text outputs, with open weights for instruction-tuned\nvariants. These models were trained with data in over 140 spoken languages.\nGemma 3n models use selective parameter activation technology to reduce resource\nrequirements. This technique allows the models to operate at an effective size\nof 2B and 4B parameters, which is lower than the total number of parameters they\ncontain. For more information on Gemma 3n's efficient parameter management\ntechnology, see the Gemma 3n\npage.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be\nsummarized\nImages, normalized to 256x256, 512x512, or 768x768 resolution\nand encoded to 256 tokens each\nAudio data encoded to 6.25 tokens per second from a single channel\nTotal input context of 32K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output length up to 32K tokens, subtracting the request\ninput tokens\nCitation\n@article{gemma_3n_2025,\ntitle={Gemma 3n},\nurl={https://ai.google.dev/gemma/docs/gemma-3n},\npublisher={Google DeepMind},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset that includes a wide variety of sources\ntotalling approximately 11 trillion tokens. The knowledge cutoff date for the\ntraining data was June 2024. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model\nis exposed to a broad range of linguistic styles, topics, and vocabulary.\nThe training dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn\nlogical reasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nAudio: A diverse set of sound samples enables the model to recognize\nspeech, transcribe text from recordings, and identify information in audio data.\nThe combination of these diverse data sources is crucial for training a\npowerful multimodal model that can handle a wide variety of different tasks and\ndata formats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material)\nfiltering was applied at multiple stages in the data preparation process to\nensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit\n(TPU) hardware (TPUv4p, TPUv5p\nand TPUv5e). Training generative models requires significant computational\npower. TPUs, designed specifically for matrix operations common in machine\nlearning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training generative models. They can speed up\ntraining considerably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and\nML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models:\n\"the 'single controller' programming model of Jax and Pathways allows a single\nPython process to orchestrate the entire training run, dramatically simplifying\nthe development workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with IT are for\ninstruction-tuned models. Evaluation results marked with PT are for\npre-trained models.\nReasoning and factuality\nBenchmark\nMetric\nn-shot\nE2B PT\nE4B PT\nHellaSwag\nAccuracy\n10-shot\n72.2\n78.6\nBoolQ\nAccuracy\n0-shot\n76.4\n81.6\nPIQA\nAccuracy\n0-shot\n78.9\n81.0\nSocialIQA\nAccuracy\n0-shot\n48.8\n50.0\nTriviaQA\nAccuracy\n5-shot\n60.8\n70.2\nNatural Questions\nAccuracy\n5-shot\n15.5\n20.9\nARC-c\nAccuracy\n25-shot\n51.7\n61.6\nARC-e\nAccuracy\n0-shot\n75.8\n81.6\nWinoGrande\nAccuracy\n5-shot\n66.8\n71.7\nBIG-Bench Hard\nAccuracy\nfew-shot\n44.3\n52.9\nDROP\nToken F1 score\n1-shot\n53.9\n60.8\nMultilingual\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMGSM\nAccuracy\n0-shot\n53.1\n60.7\nWMT24++ (ChrF)\nCharacter-level F-score\n0-shot\n42.7\n50.1\nInclude\nAccuracy\n0-shot\n38.6\n57.2\nMMLU (ProX)\nAccuracy\n0-shot\n8.1\n19.9\nOpenAI MMLU\nAccuracy\n0-shot\n22.3\n35.6\nGlobal-MMLU\nAccuracy\n0-shot\n55.1\n60.3\nECLeKTic\nECLeKTic score\n0-shot\n2.5\n1.9\nSTEM and code\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nGPQA Diamond\nRelaxedAccuracy/accuracy\n0-shot\n24.8\n23.7\nLiveCodeBench v5\npass@1\n0-shot\n18.6\n25.7\nCodegolf v2.2\npass@1\n0-shot\n11.0\n16.8\nAIME 2025\nAccuracy\n0-shot\n6.7\n11.6\nAdditional benchmarks\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMMLU\nAccuracy\n0-shot\n60.1\n64.9\nMBPP\npass@1\n3-shot\n56.6\n63.6\nHumanEval\npass@1\n0-shot\n66.5\n75.0\nLiveCodeBench\npass@1\n0-shot\n13.2\n13.2\nHiddenMath\nAccuracy\n0-shot\n27.7\n37.7\nGlobal-MMLU-Lite\nAccuracy\n0-shot\n59.0\n64.5\nMMLU (Pro)\nAccuracy\n0-shot\n40.5\n50.6\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models'\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: Generate creative text formats such as\npoems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational\ninterfaces for customer service, virtual assistants, or interactive\napplications.\nText Summarization: Generate concise summaries of a text\ncorpus, research papers, or reports.\nImage Data Extraction: Extract, interpret, and summarize\nvisual data for text communications.\nAudio Data Extraction: Transcribe spoken language, speech\ntranslated to text in other languages, and analyze sound-based data.\nResearch and Education\nNatural Language Processing (NLP) and generative model\nResearch: These models can serve as a foundation for researchers to\nexperiment with generative models and NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language\nlearning experiences, aiding in grammar correction or providing writing\npractice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of data by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of generative models raises several ethical concerns. In\ncreating an open model, we have carefully considered the following:\nBias and Fairness\nGenerative models trained on large-scale, real-world text and image data\ncan reflect socio-cultural biases embedded in the training material.\nThese models underwent careful scrutiny, input data pre-processing\ndescribed and posterior evaluations reported in this card.\nMisinformation and Misuse\nGenerative models can be misused to generate text that is\nfalse, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making generative model technology accessible to\ndevelopers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\ngenerative models. Educational resources and reporting mechanisms for users\nto flag misuse are provided. Prohibited uses of Gemma models are outlined\nin the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of\ncertain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\ngenerative model implementations designed from the ground up for responsible AI\ndevelopment compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.g",
    "google/magenta-realtime": "Model Card for Magenta RT\nTerms of Use\nModel Details\nSystem Components\nInputs and outputs\nUses\nOut-of-Scope Use\nBias, Risks, and Limitations\nKnown limitations\nBenefits\nHow to Get Started with the Model\nTraining Details\nTraining Data\nHardware\nSoftware\nEvaluation\nCitation\nModel Card for Magenta RT\nAuthors: Google DeepMind\nResources:\nBlog Post\nPaper\nColab Demo\nRepository\nHuggingFace\nTerms of Use\nMagenta RealTime is offered under a combination of licenses: the codebase is\nlicensed under\nApache 2.0, and\nthe model weights under\nCreative Commons Attribution 4.0 International.\nIn addition, we specify the following usage terms:\nCopyright 2025 Google LLC\nUse these materials responsibly and do not generate content, including outputs,\nthat infringe or violate the rights of others, including rights in copyrighted\ncontent.\nGoogle claims no rights in outputs you generate using Magenta RealTime. You and\nyour users are solely responsible for outputs and their subsequent uses.\nUnless required by applicable law or agreed to in writing, all software and\nmaterials distributed here under the Apache 2.0 or CC-BY licenses are\ndistributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\neither express or implied. See the licenses for the specific language governing\npermissions and limitations under those licenses. You are solely responsible for\ndetermining the appropriateness of using, reproducing, modifying, performing,\ndisplaying or distributing the software and materials, and any outputs, and\nassume any and all risks associated with your use or distribution of any of the\nsoftware and materials, and any outputs, and your exercise of rights and\npermissions under the licenses.\nModel Details\nMagenta RealTime is an open music generation model from Google built from the\nsame research and technology used to create\nMusicFX DJ and\nLyria RealTime. Magenta RealTime enables the\ncontinuous generation of musical audio steered by a text prompt, an audio\nexample, or a weighted combination of multiple text prompts and/or audio\nexamples. Its relatively small size makes it possible to deploy in environments\nwith limited resources, including live performance settings or freely available\nColab TPUs.\nSystem Components\nMagenta RealTime is composed of three components: SpectroStream, MusicCoCa, and\nan LLM. A full technical report with more details on each component is\nhere.\nSpectroStream is a discrete audio codec that converts stereo 48kHz audio\ninto tokens, building on the SoundStream RVQ codec from\nZeghidour+ 21\nMusicCoCa is a contrastive-trained model capable of embedding audio and\ntext into a common embedding space, building on\nYu+ 22 and\nHuang+ 22.\nAn encoder-decoder Transformer LLM generates audio tokens given context\naudio tokens and a tokenized MusicCoCa embedding, building on the MusicLM\nmethod from Agostinelli+ 23\nInputs and outputs\nSpectroStream RVQ codec: Tokenizes high-fidelity music audio\nEncoder input / Decoder output: Music audio waveforms, 48kHz stereo\nEncoder output / Decoder input: Discrete audio tokens, 25Hz frame\nrate, 64 RVQ depth, 10 bit codes, 16kbps\nMusicCoCa: Joint embeddings of text and music audio\nInput: Music audio waveforms, 16kHz mono, or text representation of\nmusic style e.g. \"heavy metal\"\nOutput: 768 dimensional embedding, quantized to 12 RVQ depth, 10 bit\ncodes\nEncoder-decoder Transformer LLM: Generates audio tokens given context\nand style\nEncoder Input: (Context, 1000 tokens) 10s of audio context tokens w/\n4 RVQ depth, (Style, 6 tokens) Quantized MusicCoCa style embedding\nDecoder Output: (Generated, 800 tokens) 2s of audio w/ 16 RVQ depth\nUses\nMusic generation models, in particular ones targeted for continuous real-time\ngeneration and control, have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nInteractive Music Creation\nLive Performance / Improvisation: These models can be used to generate\nmusic in a live performance setting, controlled by performers\nmanipulating style embeddings or the audio context\nAccessible Music-Making & Music Therapy: People with impediments to\nusing traditional instruments (skill gaps, disabilities, etc.) can\nparticipate in communal jam sessions or solo music creation.\nVideo Games: Developers can create a custom soundtrack for users in\nreal-time based on their actions and environment.\nResearch\nTransfer learning: Researchers can leverage representations from\nMusicCoCa and Magenta RT to recognize musical information.\nPersonalization\nMusicians can finetune models with their own catalog to customize the\nmodel to their style (fine tuning support coming soon).\nEducation\nExploring Genres, Instruments, and History: Natural language prompting\nenables users to quickly learn about and experiment with musical\nconcepts.\nOut-of-Scope Use\nSee our Terms of Use above for usage we consider out of scope.\nBias, Risks, and Limitations\nMagenta RT supports the real-time generation and steering of instrumental music.\nThe purpose and intention of this capability is to foster the development of new\nreal-time, interactive co-creation workflows that seamlessly integrate with\nhuman-centered forms of musical creativity.\nEvery AI music generation model, including Magenta RT, carries a risk of\nimpacting the economic and cultural landscape of music. We aim to mitigate these\nrisks through the following avenues:\nPrioritizing human-AI interaction as fundamental in the design of Magenta\nRT.\nDistributing the model under a terms of service that prohibit developers\nfrom generating outputs that infringe or violate the rights of others,\nincluding rights in copyrighted content.\nTraining on primarily instrumental data. With specific prompting, this model\nhas been observed to generate some vocal sounds and effects, though those\nvocal sounds and effects tend to be non-lexical.\nKnown limitations\nCoverage of broad musical styles. Magenta RT's training data primarily\nconsists of Western instrumental music. As a consequence, Magenta RT has\nincomplete coverage of both vocal performance and the broader landscape of rich\nmusical traditions worldwide. For real-time generation with broader style\ncoverage, we refer users to our\nLyria RealTime API.\nVocals. While the model is capable of generating non-lexical vocalizations\nand humming, it is not conditioned on lyrics and is unlikely to generate actual\nwords. However, there remains some risk of generating explicit or\nculturally-insensitive lyrical content.\nLatency. Because the Magenta RT LLM operates on two second chunks, user\ninputs for the style prompt may take two or more seconds to influence the\nmusical output.\nLimited context. Because the Magenta RT encoder has a maximum audio context\nwindow of ten seconds, the model is unable to directly reference music that has\nbeen output earlier than that. While the context is sufficient to enable the\nmodel to create melodies, rhythms, and chord progressions, the model is not\ncapable of automatically creating longer-term song structures.\nBenefits\nAt the time of release, Magenta RealTime represents the only open weights model\nsupporting real-time, continuous musical audio generation. It is designed\nspecifically to enable live, interactive musical creation, bringing new\ncapabilities to musical performances, art installations, video games, and many\nother applications.\nHow to Get Started with the Model\nSee our\nColab demo\nand GitHub repository for usage\nexamples.\nTraining Details\nTraining Data\nMagenta RealTime was trained on ~190k hours of stock music from multiple\nsources, mostly instrumental.\nHardware\nMagenta RealTime was trained using\nTensor Processing Unit (TPU)\nhardware (TPUv6e / Trillium).\nSoftware\nTraining was done using JAX and\nT5X, utilizing\nSeqIO for data pipelines. JAX allows\nresearchers to take advantage of the latest generation of hardware, including\nTPUs, for faster and more efficient training of large models.\nEvaluation\nModel evaluation metrics and results will be shared in our forthcoming technical\nreport.\nCitation\nPlease cite our technical report:\nBibTeX:\n@article{gdmlyria2025live,\ntitle={Live Music Models},\nauthor={Caillon, Antoine and McWilliams, Brian and Tarakajian, Cassie and Simon, Ian and Manco, Ilaria and Engel, Jesse and Constant, Noah and Li, Pen and Denk, Timo I. and Lalama, Alberto and Agostinelli, Andrea and Huang, Anna and Manilow, Ethan and Brower, George and Erdogan, Hakan and Lei, Heidi and Rolnick, Itai and Grishchenko, Ivan and Orsini, Manu and Kastelic, Matej and Zuluaga, Mauricio and Verzetti, Mauro and Dooley, Michael and Skopek, Ondrej and Ferrer, Rafael and Borsos, Zal{\\'a}n and van den Oord, {\\\"A}aron and Eck, Douglas and Collins, Eli and Baldridge, Jason and Hume, Tom and Donahue, Chris and Han, Kehang and Roberts, Adam},\njournal={arXiv:2508.04651},\nyear={2025}\n}",
    "OpenFold/OpenFold3": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nTo download the entire data, you can use git clone <huggingface link>. You can download individual files by clicking the download button next each file name.",
    "Wildminder/AI-windows-whl": "About The Project\nWindows AI Wheels\nA curated collection of pre-compiled Python wheels for difficult-to-install AI/ML libraries on Windows.\nAbout The Project\nThis repository was created to address a common pain point for AI enthusiasts and developers on the Windows platform: building complex Python packages from source. Libraries like flash-attention, xformers are essential for high-performance AI tasks but often lack official pre-built wheels for Windows, forcing users into a complicated and error-prone compilation process.\nThe goal here is to provide a centralized, up-to-date collection of direct links to pre-compiled .whl files for these libraries, primarily for the ComfyUI community and other PyTorch users on Windows. This saves you time and lets you focus on what's important: creating amazing things with AI.\nâ–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²â–¼â–²\nAll links and the list of available .whl files are in the GitHub repo AI-windows-whl\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nBeyond the code, I believe in the power of community and continuous learning. I invite you to join the 'TokenDiff AI News' and 'TokenDiff Community Hub'\nTokenDiff AI News\nAI for every home, creativity for every mind!\nTokenDiff Community Hub\nQuestions, help, and thoughtful discussion.\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "Qwen/Qwen3-235B-A22B-Instruct-2507": "Qwen3-235B-A22B-Instruct-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nProcessing Ultra-Long Texts\nHow to Enable 1M Token Context\nBest Practices\nCitation\nQwen3-235B-A22B-Instruct-2507\nHighlights\nWe introduce the updated version of the Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring the following key enhancements:\nSignificant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.\nSubstantial gains in long-tail knowledge coverage across multiple languages.\nMarkedly better alignment with user preferences in subjective and open-ended tasks, enabling more helpful responses and higher-quality text generation.\nEnhanced capabilities in 256K long-context understanding.\nModel Overview\nQwen3-235B-A22B-Instruct-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 235B in total and 22B activated\nNumber of Paramaters (Non-Embedding): 234B\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively and extendable up to 1,010,000 tokens\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-V3-0324\nGPT-4o-0327\nClaude Opus 4 Non-thinking\nKimi K2\nQwen3-235B-A22B Non-thinking\nQwen3-235B-A22B-Instruct-2507\nKnowledge\nMMLU-Pro\n81.2\n79.8\n86.6\n81.1\n75.2\n83.0\nMMLU-Redux\n90.4\n91.3\n94.2\n92.7\n89.2\n93.1\nGPQA\n68.4\n66.9\n74.9\n75.1\n62.9\n77.5\nSuperGPQA\n57.3\n51.0\n56.5\n57.2\n48.2\n62.6\nSimpleQA\n27.2\n40.3\n22.8\n31.0\n12.2\n54.3\nCSimpleQA\n71.1\n60.2\n68.0\n74.5\n60.8\n84.3\nReasoning\nAIME25\n46.6\n26.7\n33.9\n49.5\n24.7\n70.3\nHMMT25\n27.5\n7.9\n15.9\n38.8\n10.0\n55.4\nARC-AGI\n9.0\n8.8\n30.3\n13.3\n4.3\n41.8\nZebraLogic\n83.4\n52.6\n-\n89.0\n37.7\n95.0\nLiveBench 20241125\n66.9\n63.7\n74.6\n76.4\n62.5\n75.4\nCoding\nLiveCodeBench v6 (25.02-25.05)\n45.2\n35.8\n44.6\n48.9\n32.9\n51.8\nMultiPL-E\n82.2\n82.7\n88.5\n85.7\n79.3\n87.9\nAider-Polyglot\n55.1\n45.3\n70.7\n59.0\n59.6\n57.3\nAlignment\nIFEval\n82.3\n83.9\n87.4\n89.8\n83.2\n88.7\nArena-Hard v2*\n45.6\n61.9\n51.5\n66.1\n52.0\n79.2\nCreative Writing v3\n81.6\n84.9\n83.8\n88.1\n80.4\n87.5\nWritingBench\n74.5\n75.5\n79.2\n86.2\n77.0\n85.2\nAgent\nBFCL-v3\n64.7\n66.5\n60.1\n65.2\n68.0\n70.9\nTAU1-Retail\n49.6\n60.3#\n81.4\n70.7\n65.2\n71.3\nTAU1-Airline\n32.0\n42.8#\n59.6\n53.5\n32.0\n44.0\nTAU2-Retail\n71.1\n66.7#\n75.5\n70.6\n64.9\n74.6\nTAU2-Airline\n36.0\n42.0#\n55.5\n56.5\n36.0\n50.0\nTAU2-Telecom\n34.0\n29.8#\n45.2\n65.8\n24.6\n32.5\nMultilingualism\nMultiIF\n66.5\n70.4\n-\n76.2\n70.2\n77.5\nMMLU-ProX\n75.8\n76.2\n-\n74.5\n73.2\n79.4\nINCLUDE\n80.1\n82.1\n-\n76.9\n75.6\n79.5\nPolyMATH\n32.2\n25.5\n30.0\n44.8\n27.0\n50.2\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n#: Results were generated using GPT-4o-20241120, as access to the native function calling API of GPT-4o-0327 was unavailable.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Instruct-2507 --tp 8 --context-length 262144\nvLLM:vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 --tensor-parallel-size 8 --max-model-len 262144\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-235B-A22B-Instruct-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nTo support ultra-long context processing (up to 1 million tokens), we integrate two key techniques:\nDual Chunk Attention (DCA): A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence.\nMInference: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions.\nTogether, these innovations significantly improve both generation quality and inference efficiency for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a 3Ã— speedup compared to standard attention implementations.\nFor full technical details, see the Qwen2.5-1M Technical Report.\nHow to Enable 1M Token Context\nTo effectively process a 1 million token context, users will require approximately 1000 GB of total GPU memory. This accounts for model weights, KV-cache storage, and peak activation memory demands.\nStep 1: Update Configuration File\nDownload the model and replace the content of your config.json with config_1m.json, which includes the config for length extrapolation and sparse attention.\nexport MODELNAME=Qwen3-235B-A22B-Instruct-2507\nhuggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME}\nmv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak\nmv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json\nStep 2: Launch Model Server\nAfter updating the config, proceed with either vLLM or SGLang for serving the model.\nOption 1: Using vLLM\nTo run Qwen with 1M context support:\npip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly\nThen launch the server with Dual Chunk Flash Attention enabled:\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\\nvllm serve ./Qwen3-235B-A22B-Instruct-2507 \\\n--tensor-parallel-size 8 \\\n--max-model-len 1010000 \\\n--enable-chunked-prefill \\\n--max-num-batched-tokens 131072 \\\n--enforce-eager \\\n--max-num-seqs 1 \\\n--gpu-memory-utilization 0.85\nKey Parameters\nParameter\nPurpose\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN\nEnables the custom attention kernel for long-context efficiency\n--max-model-len 1010000\nSets maximum context length to ~1M tokens\n--enable-chunked-prefill\nAllows chunked prefill for very long inputs (avoids OOM)\n--max-num-batched-tokens 131072\nControls batch size during prefill; balances throughput and memory\n--enforce-eager\nDisables CUDA graph capture (required for dual chunk attention)\n--max-num-seqs 1\nLimits concurrent sequences due to extreme memory usage\n--gpu-memory-utilization 0.85\nSet the fraction of GPU memory to be used for the model executor\nOption 2: Using SGLang\nFirst, clone and install the specialized branch:\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\npip install -e \"python[all]\"\nLaunch the server with DCA support:\npython3 -m sglang.launch_server \\\n--model-path ./Qwen3-235B-A22B-Instruct-2507 \\\n--context-length 1010000 \\\n--mem-frac 0.75 \\\n--attention-backend dual_chunk_flash_attn \\\n--tp 8 \\\n--chunked-prefill-size 131072\nKey Parameters\nParameter\nPurpose\n--attention-backend dual_chunk_flash_attn\nActivates Dual Chunk Flash Attention\n--context-length 1010000\nDefines max input length\n--mem-frac 0.75\nThe fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors.\n--tp 8\nTensor parallelism size (matches model sharding)\n--chunked-prefill-size 131072\nPrefill chunk size for handling long inputs without OOM\nTroubleshooting:\nEncountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeError: Not enough memory. Please try to increase --mem-fraction-static.\"\nThe VRAM reserved for the KV cache is insufficient.\nvLLM: Consider reducing the max_model_len or increasing the tensor_parallel_size and gpu_memory_utilization. Alternatively, you can reduce max_num_batched_tokens, although this may significantly slow down inference.\nSGLang: Consider reducing the context-length or increasing the tp and mem-frac. Alternatively, you can reduce chunked-prefill-size, although this may significantly slow down inference.\nEncountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\nThe VRAM reserved for activation weights is insufficient. You can try lowering gpu_memory_utilization or mem-frac, but be aware that this might reduce the VRAM available for the KV cache.\nEncountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\" or \"The input (xxx xtokens) is longer than the model's context length (xxx tokens).\"\nThe input is too lengthy. Consider using a shorter sequence or increasing the max_model_len or context-length.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-235B-A22B (Non-Thinking)\n83.9\n97.7\n96.1\n97.5\n96.1\n94.2\n90.3\n88.5\n85.0\n82.1\n79.2\n74.4\n70.0\n71.0\n68.5\n68.0\nQwen3-235B-A22B-Instruct-2507 (Full Attention)\n92.5\n98.5\n97.6\n96.9\n97.3\n95.8\n94.9\n93.9\n94.5\n91.0\n92.2\n90.9\n87.8\n84.8\n86.5\n84.5\nQwen3-235B-A22B-Instruct-2507 (Sparse Attention)\n91.7\n98.5\n97.2\n97.3\n97.7\n96.6\n94.6\n92.8\n94.3\n90.5\n89.7\n89.5\n86.4\n83.6\n84.2\n82.5\nAll models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{qwen2.5-1m,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "jhu-clsp/mmBERT-base": "mmBERT: A Modern Multilingual Encoder\nTable of Contents\nQuick Start\nInstallation\nUsage\nModel Description\nNovel Training Innovations\nModel Family\nTraining Data\nModel Architecture\nUsage Examples\nMasked Language Modeling\nCross-lingual Embeddings\nFine-tuning Examples\nDense Retrieval with Sentence Transformers\nCross-lingual Classification\nMultilingual Reranking\nTraining Data\nCitation\nmmBERT: A Modern Multilingual Encoder\nTL;DR: A state-of-the-art multilingual encoder trained on 3T+ tokens across 1800+ languages, introducing novel techniques for learning low-resource languages during the decay phase.\nmmBERT is a modern multilingual encoder that significantly outperforms previous generation models like XLM-R on classification, embedding, and retrieval tasks. Built on the ModernBERT architecture with novel multilingual training innovations, mmBERT demonstrates that low-resource languages can be effectively learned during the decay phase of training. It is also significantly faster than any previous multilingual encoder.\nTable of Contents\nHighlights\nQuick Start\nModel Description\nNovel Training Innovations\nModel Family\nTraining Data\nUsage Examples\nFine-tuning Examples\nModel Architecture\nCitation\nQuick Start\nInstallation\npip install torch>=1.9.0\npip install transformers>=4.21.0\nUsage\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/mmBERT-base\")\nmodel = AutoModel.from_pretrained(\"jhu-clsp/mmBERT-base\")\ninputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\noutputs = model(**inputs)\nModel Description\nmmBERT represents the first significant advancement over XLM-R for massively multilingual encoder models. Key features include:\nMassive Language Coverage - Trained on over 1800 languages with progressive inclusion strategy\nModern Architecture - Built on ModernBERT foundation with Flash Attention 2 and unpadding techniques\nNovel Training Recipe - Introduces inverse mask scheduling and temperature sampling\nOpen Training Data - Complete 3T+ token dataset publicly available\nDecay Phase Innovation - Demonstrates effective learning of low-resource languages in final training phase\nThe model uses bidirectional attention with masked language modeling objectives, optimized specifically for multilingual understanding and cross-lingual transfer.\nNovel Training Innovations\nProgressive Language Addition: Start with 60 high-resource languages, expand to 110 mid-resource languages, then include all 1833 languages in decay phase.\nInverse Mask Schedule: Reduce mask ratio from 30% â†’ 15% â†’ 5% across training phases for progressively refined learning.\nInverse Temperature Sampling: Adjust multilingual sampling from high-resource bias (Ï„=0.7) to uniform sampling (Ï„=0.3).\nModel Merging: Combine English-focused, high-resource, and all-language decay variants using TIES merging.\nModel Family\nModel\nTotal Params\nNon-embed Params\nLanguages\nDownload\nmmBERT-small\n140M\n42M\n1800+\nmmBERT-base\n307M\n110M\n1800+\nTraining Data\nmmBERT training data is publicly available across different phases:\nPhase\nDataset\nTokens\nDescription\nPre-training P1\nmmbert-pretrain-p1\n2.3T\n60 languages, foundational training\nPre-training P2\nmmbert-pretrain-p2\n-\nExtension data for pre-training phase\nPre-training P3\nmmbert-pretrain-p3\n-\nFinal pre-training data\nMid-training\nmmbert-midtraining\n600B\n110 languages, context extension to 8K\nDecay Phase\nmmbert-decay\n100B\n1833 languages, premium quality\nData Sources: Filtered DCLM (English), FineWeb2 (multilingual), FineWeb2-HQ (20 high-resource languages), Wikipedia (MegaWika), code repositories (StarCoder, ProLong), academic papers (ArXiv, PeS2o), and community discussions (StackExchange).\nModel Architecture\nParameter\nmmBERT-small\nmmBERT-base\nLayers\n22\n22\nHidden Size\n384\n768\nIntermediate Size\n1152\n1152\nAttention Heads\n6\n12\nTotal Parameters\n140M\n307M\nNon-embedding Parameters\n42M\n110M\nMax Sequence Length\n8192\n8192\nVocabulary Size\n256,000\n256,000\nTokenizer\nGemma 2\nGemma 2\nUsage Examples\nMasked Language Modeling\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/mmBERT-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"jhu-clsp/mmBERT-base\")\ndef predict_masked_token(text):\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\nmask_indices = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\npredictions = outputs.logits[mask_indices]\ntop_tokens = torch.topk(predictions, 5, dim=-1)\nreturn [tokenizer.decode(token) for token in top_tokens.indices[0]]\n# Works across languages\ntexts = [\n\"The capital of France is <mask>.\",\n\"La capital de EspaÃ±a es <mask>.\",\n\"Die Hauptstadt von Deutschland ist <mask>.\"\n]\nfor text in texts:\npredictions = predict_masked_token(text)\nprint(f\"Text: {text}\")\nprint(f\"Predictions: {predictions}\")\nCross-lingual Embeddings\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\ntokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/mmBERT-base\")\nmodel = AutoModel.from_pretrained(\"jhu-clsp/mmBERT-base\")\ndef get_embeddings(texts):\ninputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\nembeddings = outputs.last_hidden_state.mean(dim=1)\nreturn embeddings.numpy()\nmultilingual_texts = [\n\"Artificial intelligence is transforming technology\",\n\"La inteligencia artificial estÃ¡ transformando la tecnologÃ­a\",\n\"L'intelligence artificielle transforme la technologie\",\n\"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æŠ€æœ¯\"\n]\nembeddings = get_embeddings(multilingual_texts)\nsimilarities = cosine_similarity(embeddings)\nprint(\"Cross-lingual similarity matrix:\")\nprint(similarities)\nFine-tuning Examples\nDense Retrieval with Sentence Transformers\nClick to expand dense retrieval fine-tuning example\nimport argparse\nfrom datasets import load_dataset\nfrom sentence_transformers import (\nSentenceTransformer,\nSentenceTransformerTrainer,\nSentenceTransformerTrainingArguments,\n)\nfrom sentence_transformers.evaluation import TripletEvaluator\nfrom sentence_transformers.losses import CachedMultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\ndef main():\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--lr\", type=float, default=8e-5)\nparser.add_argument(\"--model_name\", type=str, default=\"jhu-clsp/mmBERT-base\")\nargs = parser.parse_args()\nlr = args.lr\nmodel_name = args.model_name\nmodel_shortname = model_name.split(\"/\")[-1]\nmodel = SentenceTransformer(model_name)\ndataset = load_dataset(\n\"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\",\n\"triplet-hard\",\nsplit=\"train\",\n)\ndataset_dict = dataset.train_test_split(test_size=1_000, seed=12)\ntrain_dataset = dataset_dict[\"train\"].select(range(1_250_000))\neval_dataset = dataset_dict[\"test\"]\nloss = CachedMultipleNegativesRankingLoss(model, mini_batch_size=16)\nrun_name = f\"{model_shortname}-DPR-{lr}\"\ntraining_args = SentenceTransformerTrainingArguments(\noutput_dir=f\"output/{model_shortname}/{run_name}\",\nnum_train_epochs=1,\nper_device_train_batch_size=512,\nper_device_eval_batch_size=512,\nwarmup_ratio=0.05,\nfp16=False,\nbf16=True,\nbatch_sampler=BatchSamplers.NO_DUPLICATES,\nlearning_rate=lr,\nsave_strategy=\"steps\",\nsave_steps=500,\nsave_total_limit=2,\nlogging_steps=500,\nrun_name=run_name,\n)\ndev_evaluator = TripletEvaluator(\nanchors=eval_dataset[\"query\"],\npositives=eval_dataset[\"positive\"],\nnegatives=eval_dataset[\"negative\"],\nname=\"msmarco-co-condenser-dev\",\n)\ndev_evaluator(model)\ntrainer = SentenceTransformerTrainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=train_dataset,\neval_dataset=eval_dataset,\nloss=loss,\nevaluator=dev_evaluator,\n)\ntrainer.train()\nmodel.save_pretrained(f\"output/{model_shortname}/{run_name}/final\")\nmodel.push_to_hub(run_name, private=False)\nif __name__ == \"__main__\":\nmain()\nCross-lingual Classification\nClick to expand multilingual classification fine-tuning example\nfrom transformers import (\nAutoTokenizer,\nAutoModelForSequenceClassification,\nTrainingArguments,\nTrainer\n)\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\ndef compute_metrics(eval_pred):\npredictions, labels = eval_pred\npredictions = np.argmax(predictions, axis=1)\nreturn {\n'accuracy': accuracy_score(labels, predictions),\n'f1': f1_score(labels, predictions, average='weighted')\n}\ndef main():\nmodel_name = \"jhu-clsp/mmBERT-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\nmodel_name,\nnum_labels=3\n)\ndataset = load_dataset(\"xnli\", \"all_languages\")\ndef tokenize_function(examples):\ntexts = [f\"{p} {tokenizer.sep_token} {h}\"\nfor p, h in zip(examples[\"premise\"], examples[\"hypothesis\"])]\nreturn tokenizer(\ntexts,\ntruncation=True,\npadding=True,\nmax_length=512\n)\ntrain_dataset = dataset[\"train\"].map(tokenize_function, batched=True)\neval_dataset = dataset[\"validation\"].map(tokenize_function, batched=True)\ntraining_args = TrainingArguments(\noutput_dir=\"./mmbert-xnli\",\nlearning_rate=3e-5,\nper_device_train_batch_size=32,\nper_device_eval_batch_size=32,\nnum_train_epochs=3,\nweight_decay=0.01,\nevaluation_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nload_best_model_at_end=True,\nmetric_for_best_model=\"f1\",\ngreater_is_better=True,\n)\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=train_dataset,\neval_dataset=eval_dataset,\ncompute_metrics=compute_metrics,\n)\ntrainer.train()\nif __name__ == \"__main__\":\nmain()\nMultilingual Reranking\nClick to expand multilingual reranking fine-tuning example\nimport logging\nfrom datasets import load_dataset\nfrom sentence_transformers.cross_encoder import (\nCrossEncoder,\nCrossEncoderModelCardData,\nCrossEncoderTrainer,\nCrossEncoderTrainingArguments,\n)\nfrom sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\nfrom sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss\nfrom sentence_transformers.util import mine_hard_negatives\nfrom sentence_transformers import SentenceTransformer\nimport torch\ndef main():\nmodel_name = \"jhu-clsp/mmBERT-base\"\ntrain_batch_size = 32\nnum_epochs = 2\nnum_hard_negatives = 7\nmodel = CrossEncoder(\nmodel_name,\nmodel_card_data=CrossEncoderModelCardData(\nlanguage=\"multilingual\",\nlicense=\"mit\",\n),\n)\nfull_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\").select(range(50_000))\ndataset_dict = full_dataset.train_test_split(test_size=1_000, seed=42)\ntrain_dataset = dataset_dict[\"train\"]\neval_dataset = dataset_dict[\"test\"]\nembedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\")\nhard_train_dataset = mine_hard_negatives(\ntrain_dataset,\nembedding_model,\nnum_negatives=num_hard_negatives,\nmargin=0,\nrange_min=0,\nrange_max=100,\nsampling_strategy=\"top\",\nbatch_size=2048,\noutput_format=\"labeled-pair\",\nuse_faiss=True,\n)\nloss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives))\nnano_beir_evaluator = CrossEncoderNanoBEIREvaluator(\ndataset_names=[\"msmarco\", \"nfcorpus\", \"nq\"],\nbatch_size=train_batch_size,\n)\nargs = CrossEncoderTrainingArguments(\noutput_dir=\"./mmbert-reranker\",\nnum_train_epochs=num_epochs,\nper_device_train_batch_size=train_batch_size,\nper_device_eval_batch_size=train_batch_size,\nlearning_rate=2e-5,\nwarmup_ratio=0.1,\nfp16=False,\nbf16=True,\ndataloader_num_workers=4,\nload_best_model_at_end=True,\nmetric_for_best_model=\"eval_msmarco_ndcg@10\",\neval_strategy=\"steps\",\neval_steps=1000,\nsave_strategy=\"steps\",\nsave_steps=1000,\nsave_total_limit=2,\nlogging_steps=200,\nseed=42,\n)\ntrainer = CrossEncoderTrainer(\nmodel=model,\nargs=args,\ntrain_dataset=hard_train_dataset,\nloss=loss,\nevaluator=nano_beir_evaluator,\n)\ntrainer.train()\nmodel.save_pretrained(\"./mmbert-reranker/final\")\nif __name__ == \"__main__\":\nmain()\nTraining Data\nmmBERT was trained on a carefully curated 3T+ token multilingual dataset:\nPhase\nDataset\nDescription\nPre-training P1\n2.3T tokens\n60 languages, diverse data mixture\nPre-training P2\n-\nExtension data for pre-training\nPre-training P3\n-\nFinal pre-training data\nMid-training\n600B tokens\n110 languages, context extension\nDecay Phase\n100B tokens\n1833 languages, premium quality\nPrimary Sources:\nFiltered DCLM: High-quality English content\nFineWeb2: Broad multilingual web coverage (1800+ languages)\nFineWeb2-HQ: Filtered subset of 20 high-resource languages\nCode: StarCoder and ProLong repositories\nAcademic: ArXiv papers and PeS2o scientific content\nReference: Wikipedia (MegaWika) and textbooks\nCommunity: StackExchange discussions\nCitation\nIf you use mmBERT in your research, please cite our work:\n@misc{marone2025mmbertmodernmultilingualencoder,\ntitle={mmBERT: A Modern Multilingual Encoder with Annealed Language Learning},\nauthor={Marc Marone and Orion Weller and William Fleshman and Eugene Yang and Dawn Lawrie and Benjamin Van Durme},\nyear={2025},\neprint={2509.06888},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2509.06888},\n}\n\"\"\"",
    "unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF": "Qwen3-30B-A3B-Instruct-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3-2507 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nâœ¨ Read our Qwen3-2507 Guide here!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\nâ–¶ï¸ Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\nâ–¶ï¸ Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\nâ–¶ï¸ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen3-30B-A3B-Instruct-2507\nHighlights\nWe introduce the updated version of the Qwen3-30B-A3B non-thinking mode, named Qwen3-30B-A3B-Instruct-2507, featuring the following key enhancements:\nSignificant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.\nSubstantial gains in long-tail knowledge coverage across multiple languages.\nMarkedly better alignment with user preferences in subjective and open-ended tasks, enabling more helpful responses and higher-quality text generation.\nEnhanced capabilities in 256K long-context understanding.\nModel Overview\nQwen3-30B-A3B-Instruct-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Paramaters (Non-Embedding): 29.9B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-V3-0324\nGPT-4o-0327\nGemini-2.5-Flash Non-Thinking\nQwen3-235B-A22B Non-Thinking\nQwen3-30B-A3B Non-Thinking\nQwen3-30B-A3B-Instruct-2507\nKnowledge\nMMLU-Pro\n81.2\n79.8\n81.1\n75.2\n69.1\n78.4\nMMLU-Redux\n90.4\n91.3\n90.6\n89.2\n84.1\n89.3\nGPQA\n68.4\n66.9\n78.3\n62.9\n54.8\n70.4\nSuperGPQA\n57.3\n51.0\n54.6\n48.2\n42.2\n53.4\nReasoning\nAIME25\n46.6\n26.7\n61.6\n24.7\n21.6\n61.3\nHMMT25\n27.5\n7.9\n45.8\n10.0\n12.0\n43.0\nZebraLogic\n83.4\n52.6\n57.9\n37.7\n33.2\n90.0\nLiveBench 20241125\n66.9\n63.7\n69.1\n62.5\n59.4\n69.0\nCoding\nLiveCodeBench v6 (25.02-25.05)\n45.2\n35.8\n40.1\n32.9\n29.0\n43.2\nMultiPL-E\n82.2\n82.7\n77.7\n79.3\n74.6\n83.8\nAider-Polyglot\n55.1\n45.3\n44.0\n59.6\n24.4\n35.6\nAlignment\nIFEval\n82.3\n83.9\n84.3\n83.2\n83.7\n84.7\nArena-Hard v2*\n45.6\n61.9\n58.3\n52.0\n24.8\n69.0\nCreative Writing v3\n81.6\n84.9\n84.6\n80.4\n68.1\n86.0\nWritingBench\n74.5\n75.5\n80.5\n77.0\n72.2\n85.5\nAgent\nBFCL-v3\n64.7\n66.5\n66.1\n68.0\n58.6\n65.1\nTAU1-Retail\n49.6\n60.3#\n65.2\n65.2\n38.3\n59.1\nTAU1-Airline\n32.0\n42.8#\n48.0\n32.0\n18.0\n40.0\nTAU2-Retail\n71.1\n66.7#\n64.3\n64.9\n31.6\n57.0\nTAU2-Airline\n36.0\n42.0#\n42.5\n36.0\n18.0\n38.0\nTAU2-Telecom\n34.0\n29.8#\n16.9\n24.6\n18.4\n12.3\nMultilingualism\nMultiIF\n66.5\n70.4\n69.4\n70.2\n70.8\n67.9\nMMLU-ProX\n75.8\n76.2\n78.3\n73.2\n65.1\n72.0\nINCLUDE\n80.1\n82.1\n83.8\n75.6\n67.8\n71.9\nPolyMATH\n32.2\n25.5\n41.9\n27.0\n23.3\n43.1\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n#: Results were generated using GPT-4o-20241120, as access to the native function calling API of GPT-4o-0327 was unavailable.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --context-length 262144\nvLLM:vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --max-model-len 262144\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-30B-A3B-Instruct-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "google/gemma-3-270m-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach, for the 4B, 12B, and 27B sizes.\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B and 270M sizes.\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context up to 128K tokens for the 4B, 12B, and 27B sizes,\nand 32K tokens for the 1B and 270M sizes per request, subtracting the\nrequest input tokens\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://arxiv.org/abs/2503.19786},\npublisher={Google DeepMind},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens,\nthe 1B with 2 trillion tokens, and the 270M with 6 trillion tokens. The\nknowledge cutoff date for the training data was August 2024. Here are the key\ncomponents:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation. Evaluation results marked\nwith IT are for instruction-tuned models. Evaluation results marked with\nPT are for pre-trained models.\nGemma 3 270M\nBenchmark\nn-shot\nGemma 3 PT 270M\nHellaSwag\n10-shot\n40.9\nBoolQ\n0-shot\n61.4\nPIQA\n0-shot\n67.7\nTriviaQA\n5-shot\n15.4\nARC-c\n25-shot\n29.0\nARC-e\n0-shot\n57.7\nWinoGrande\n5-shot\n52.0\nBenchmark\nn-shot\nGemma 3 IT 270m\nHellaSwag\n0-shot\n37.7\nPIQA\n0-shot\n66.2\nARC-c\n0-shot\n28.2\nWinoGrande\n0-shot\n52.3\nBIG-Bench Hard\nfew-shot\n26.7\nIF Eval\n0-shot\n51.2\nGemma 3 1B, 4B, 12B & 27B\nReasoning and factuality\nBenchmark\nn-shot\nGemma 3 IT 1B\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nGPQA Diamond\n0-shot\n19.2\n30.8\n40.9\n42.4\nSimpleQA\n0-shot\n2.2\n4.0\n6.3\n10.0\nFACTS Grounding\n-\n36.4\n70.1\n75.8\n74.9\nBIG-Bench Hard\n0-shot\n39.1\n72.2\n85.7\n87.6\nBIG-Bench Extra Hard\n0-shot\n7.2\n11.0\n16.3\n19.3\nIFEval\n0-shot\n80.2\n90.2\n88.9\n90.4\nBenchmark\nn-shot\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nn-shot\nGemma 3 IT 1B\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nMMLU (Pro)\n0-shot\n14.7\n43.6\n60.6\n67.5\nLiveCodeBench\n0-shot\n1.9\n12.6\n24.6\n29.7\nBird-SQL (dev)\n-\n6.4\n36.3\n47.9\n54.4\nMath\n0-shot\n48.0\n75.6\n83.8\n89.0\nHiddenMath\n0-shot\n15.8\n43.0\n54.5\n60.3\nMBPP\n3-shot\n35.2\n63.2\n73.0\n74.4\nHumanEval\n0-shot\n41.5\n71.3\n85.4\n87.8\nNatural2Code\n0-shot\n56.0\n70.3\n80.7\n84.5\nGSM8K\n0-shot\n62.8\n89.2\n94.4\n95.9\nBenchmark\nn-shot\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nn-shot\nGemma 3 IT 1B\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nGlobal-MMLU-Lite\n0-shot\n34.2\n54.5\n69.5\n75.1\nECLeKTic\n0-shot\n1.4\n4.6\n10.3\n16.7\nWMT24++\n0-shot\n35.9\n46.8\n51.6\n53.4\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 IT 4B\nGemma 3 IT 12B\nGemma 3 IT 27B\nMMMU (val)\n48.8\n59.6\n64.9\nDocVQA\n75.8\n87.1\n86.6\nInfoVQA\n50.0\n64.9\n70.6\nTextVQA\n57.8\n67.7\n65.1\nAI2D\n74.8\n84.2\n84.5\nChartQA\n68.8\n75.7\n78.0\nVQAv2 (val)\n62.4\n71.6\n71.0\nMathVista (testmini)\n50.0\n62.9\n67.6\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives."
}