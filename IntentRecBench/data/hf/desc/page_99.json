{
    "large-traversaal/Alif-1.0-8B-Instruct": "Model Card for Alif 1.0 8B Instruct\nModel Details\nEvaluation\nCitation\nModel Card Contact\nModel Card for Alif 1.0 8B Instruct\nAlif 1.0 8B Instruct is an open-source model with highly advanced multilingual reasoning capabilities. It utilizes human refined multilingual synthetic data paired with reasoning to enhance cultural nuance and reasoning capabilities in english and urdu languages.\nDeveloped by: large-traversaal\nLicense: apache-2.0\nBase model: unsloth/Meta-Llama-3.1-8B\nModel: Alif-1.0-8B-Instruct\nModel Size: 8 billion parameters\nThis model was trained 2x faster with Unsloth and Huggingface's TRL library.\nHow to Use Alif 1.0 8B Instruct\nInstall the transformers, bitsandbytes libraries and load Alif 1.0 8B Instruct as follows:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nfrom transformers import BitsAndBytesConfig\nmodel_id = \"large-traversaal/Alif-1.0-8B-Instruct\"\n# 4-bit quantization configuration\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_compute_dtype=torch.float16,\nbnb_4bit_use_double_quant=True,\nbnb_4bit_quant_type=\"nf4\"\n)\n# Load tokenizer and model in 4-bit\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\nquantization_config=quantization_config,\ndevice_map=\"auto\"\n)\n# Create text generation pipeline\nchatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n# Function to chat\ndef chat(message):\nresponse = chatbot(message, max_new_tokens=100, do_sample=True, temperature=0.3)\nreturn response[0][\"generated_text\"]\n# Example chat\nuser_input = \"ÿ¥€Åÿ± ⁄©ÿ±ÿß⁄Ü€å ⁄©€å ⁄©€åÿß ÿß€ÅŸÖ€åÿ™ €Å€íÿü\"\nbot_response = chat(user_input)\nprint(bot_response)\nYou can also try out this model using TextStreamer or Gradio in Colab. It is also available in GGUF with various quantized formats for Ollama, LM Studio, Jan, and Llama.cpp.\nModel Details\nInput: Models input text only.\nOutput: Models generate text only.\nModel Architecture: Alif 1.0 8B Instruct is an auto-regressive language model that uses an optimized transformer architecture. Post-training includes continuous pretraining and supervised finetuning.\nFor more details about how the model was trained, check out our blogpost.\nEvaluation\nWe evaluated Alif 1.0 8B Instruct against Gemma 2 9B, Llama 3.1 8B, Mistral Nemo 12B, Qwen 2.5 7B and Cohere Aya Expanse 8B using the human annotated Urdu evaluation dataset and scores are determined using gpt-4o as a judge.\nCitation\n@article{ShafiqueAlif2025,\ntitle        = {Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation},\nauthor       = {Muhammad Ali Shafique and Kanwal Mehreen and Muhammad Arham and Maaz Amjad and Sabur Butt and Hamza Farooq},\njournal      = {arXiv preprint arXiv:2510.09051},\nyear         = {2025},\nurl          = {https://arxiv.org/abs/2510.09051}\n}\nModel Card Contact\nFor errors or additional questions about details in this model card, contact: contact@traversaal.ai",
    "chatpig/encoder": "t5xxl encoder\nextra\nt5xxl encoder\nbase model from google\npaper: unified text-to-text transformer\nuse it as text encoder (drag it to the folder ./models/text_encoders)\nextra\nclip l - gguf; works on gguf clip loader\nclip g - gguf; works on gguf clip loader\ngemma2-2b-encoder\nqwen2.5-vl-encoder\nt5xxl-old-encoder\nt5xxl-um-encoder\nt5xl-fp8/16/32-encoder\nt5base-fp8/32-encoder",
    "bartowski/Llama-3.1-Tulu-3-405B-GGUF": "Llamacpp imatrix Quantizations of Llama-3.1-Tulu-3-405B\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Llama-3.1-Tulu-3-405B\nUsing llama.cpp release b4585 for quantization.\nOriginal model: https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\n<|system|>\n{system_prompt}\n<|user|>\n{prompt}\n<|assistant|>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nLlama-3.1-Tulu-3-405B-Q8_0.gguf\nQ8_0\n431.24GB\ntrue\nExtremely high quality, generally unneeded but max available quant.\nLlama-3.1-Tulu-3-405B-Q6_K.gguf\nQ6_K\n332.95GB\ntrue\nVery high quality, near perfect, recommended.\nLlama-3.1-Tulu-3-405B-Q5_K_M.gguf\nQ5_K_M\n286.65GB\ntrue\nHigh quality, recommended.\nLlama-3.1-Tulu-3-405B-Q5_K_S.gguf\nQ5_K_S\n279.33GB\ntrue\nHigh quality, recommended.\nLlama-3.1-Tulu-3-405B-Q4_1.gguf\nQ4_1\n254.09GB\ntrue\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nLlama-3.1-Tulu-3-405B-Q4_K_L.gguf\nQ4_K_L\n244.63GB\ntrue\nUses Q8_0 for embed and output weights. Good quality, recommended.\nLlama-3.1-Tulu-3-405B-Q4_K_M.gguf\nQ4_K_M\n243.07GB\ntrue\nGood quality, default size for most use cases, recommended.\nLlama-3.1-Tulu-3-405B-Q4_K_S.gguf\nQ4_K_S\n230.50GB\ntrue\nSlightly lower quality with more space savings, recommended.\nLlama-3.1-Tulu-3-405B-Q4_0.gguf\nQ4_0\n229.67GB\ntrue\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nLlama-3.1-Tulu-3-405B-IQ4_NL.gguf\nIQ4_NL\n229.12GB\ntrue\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nLlama-3.1-Tulu-3-405B-IQ4_XS.gguf\nIQ4_XS\n216.57GB\ntrue\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nLlama-3.1-Tulu-3-405B-Q3_K_XL.gguf\nQ3_K_XL\n214.68GB\ntrue\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nLlama-3.1-Tulu-3-405B-Q3_K_L.gguf\nQ3_K_L\n212.84GB\ntrue\nLower quality but usable, good for low RAM availability.\nLlama-3.1-Tulu-3-405B-Q3_K_M.gguf\nQ3_K_M\n195.37GB\ntrue\nLow quality.\nLlama-3.1-Tulu-3-405B-IQ3_M.gguf\nIQ3_M\n181.74GB\ntrue\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nLlama-3.1-Tulu-3-405B-Q3_K_S.gguf\nQ3_K_S\n175.23GB\ntrue\nLow quality, not recommended.\nLlama-3.1-Tulu-3-405B-IQ3_XXS.gguf\nIQ3_XXS\n155.85GB\ntrue\nLower quality, new method with decent performance, comparable to Q3 quants.\nLlama-3.1-Tulu-3-405B-Q2_K_L.gguf\nQ2_K_L\n151.38GB\ntrue\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nLlama-3.1-Tulu-3-405B-Q2_K.gguf\nQ2_K\n149.32GB\ntrue\nVery low quality but surprisingly usable.\nLlama-3.1-Tulu-3-405B-IQ2_M.gguf\nIQ2_M\n136.67GB\ntrue\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nLlama-3.1-Tulu-3-405B-IQ2_S.gguf\nIQ2_S\n125.65GB\ntrue\nLow quality, uses SOTA techniques to be usable.\nLlama-3.1-Tulu-3-405B-IQ2_XS.gguf\nIQ2_XS\n119.35GB\ntrue\nLow quality, uses SOTA techniques to be usable.\nLlama-3.1-Tulu-3-405B-IQ2_XXS.gguf\nIQ2_XXS\n107.27GB\ntrue\nVery low quality, uses SOTA techniques to be usable.\nLlama-3.1-Tulu-3-405B-IQ1_M.gguf\nIQ1_M\n93.50GB\ntrue\nExtremely low quality, not recommended.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Llama-3.1-Tulu-3-405B-GGUF --include \"Llama-3.1-Tulu-3-405B-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Llama-3.1-Tulu-3-405B-GGUF --include \"Llama-3.1-Tulu-3-405B-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Llama-3.1-Tulu-3-405B-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "lmstudio-community/Llama-3.1-Tulu-3-405B-GGUF": "üí´ Community Model> Llama 3.1 Tulu 3 405B by Allenai\nTechnical Details\nSpecial thanks\nDisclaimers\nüí´ Community Model> Llama 3.1 Tulu 3 405B by Allenai\nüëæ LM Studio Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on Discord.\nModel creator: allenai\nOriginal model: Llama-3.1-Tulu-3-405B\nGGUF quantization: provided by bartowski based on llama.cpp release b4585\nTechnical Details\nSupports a context length of 128k tokens.\nFully open source data, code, and recipes.\nDesigned for state of the art performance across many tasks.\nMore details from their original paper available here.\nSpecial thanks\nüôè Special thanks to Georgi Gerganov and the whole team working on llama.cpp for making all of this possible.\nDisclaimers\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.",
    "unsloth/Mistral-Small-24B-Instruct-2501-GGUF": "Finetune LLMs 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nModel Card for Mistral-Small-24B-Instruct-2501\nKey Features\nBenchmark results\nHuman evaluated benchmarks\nPublicly accesible benchmarks\nBasic Instruct Template (V7-Tekken)\nUsage\nvLLM\nFunction calling\nTransformers\n[{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}'}}]\nTransformers\nFinetune LLMs 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Mistral (7B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nModel Card for Mistral-Small-24B-Instruct-2501\nMistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!This model is an instruction-fine-tuned version of the base model: Mistral-Small-24B-Base-2501.\nMistral Small can be deployed locally and is exceptionally \"knowledge-dense\", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.Perfect for:\nFast response conversational agents.\nLow latency function calling.\nSubject matter experts via fine-tuning.\nLocal inference for hobbyists and organizations handling sensitive data.\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\nThis release demonstrates our commitment to open source, serving as a strong base model.\nLearn more about Mistral Small in our blog post.\nModel developper: Mistral AI Team\nKey Features\nMultilingual: Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\nAgent-Centric: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\nAdvanced Reasoning: State-of-the-art conversational and reasoning capabilities.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 32k context window.\nSystem Prompt: Maintains strong adherence and support for system prompts.\nTokenizer: Utilizes a Tekken tokenizer with a 131k vocabulary size.\nBenchmark results\nHuman evaluated benchmarks\nCategory\nGemma-2-27B\nQwen-2.5-32B\nLlama-3.3-70B\nGpt4o-mini\nMistral is better\n0.536\n0.496\n0.192\n0.200\nMistral is slightly better\n0.196\n0.184\n0.164\n0.204\nTies\n0.052\n0.060\n0.236\n0.160\nOther is slightly better\n0.060\n0.088\n0.112\n0.124\nOther is better\n0.156\n0.172\n0.296\n0.312\nNote:\nWe conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts.\nEvaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model.\nWe are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\nPublicly accesible benchmarks\nReasoning & Knowledge\nEvaluation\nmistral-small-24B-instruct-2501\ngemma-2b-27b\nllama-3.3-70b\nqwen2.5-32b\ngpt-4o-mini-2024-07-18\nmmlu_pro_5shot_cot_instruct\n0.663\n0.536\n0.666\n0.683\n0.617\ngpqa_main_cot_5shot_instruct\n0.453\n0.344\n0.531\n0.404\n0.377\nMath & Coding\nEvaluation\nmistral-small-24B-instruct-2501\ngemma-2b-27b\nllama-3.3-70b\nqwen2.5-32b\ngpt-4o-mini-2024-07-18\nhumaneval_instruct_pass@1\n0.848\n0.732\n0.854\n0.909\n0.890\nmath_instruct\n0.706\n0.535\n0.743\n0.819\n0.761\nInstruction following\nEvaluation\nmistral-small-24B-instruct-2501\ngemma-2b-27b\nllama-3.3-70b\nqwen2.5-32b\ngpt-4o-mini-2024-07-18\n------------\n---------------\n--------------\n---------------\n---------------\n-------------\nmtbench_dev\n8.35\n7.86\n7.96\n8.26\n8.33\nwildbench\n52.27\n48.21\n50.04\n52.73\n56.13\narena_hard\n0.873\n0.788\n0.840\n0.860\n0.897\nifeval\n0.829\n0.8065\n0.8835\n0.8401\n0.8499\nNote:\nPerformance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance\n(Qwen2.5-32B-Instruct, Llama-3.3-70B-Instruct, Gemma-2-27B-IT).\nJudge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\nBasic Instruct Template (V7-Tekken)\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n<system_prompt>, <user message> and <assistant response> are placeholders.\nPlease make sure to use mistral-common as the source of truth\nUsage\nThe model can be used with the following frameworks;\nvllm: See here\ntransformers: See here\nvLLM\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nNote 1: We recommond using a relatively low temperature, such as temperature=0.15.\nNote 2: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following\nsystem prompt:\nsystem_prompt = \"\"\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\"What are some good restaurants around me?\\\" => \\\"Where are you?\\\" or \\\"When is the next flight to Tokyo\\\" => \\\"Where do you travel from?\\\")\"\"\"\nInstallation\nMake sure you install vLLM >= 0.6.4:\npip install --upgrade vllm\nAlso make sure you have mistral_common >= 1.5.2 installed:\npip install --upgrade mistral_common\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServer\nWe recommand that you use Mistral-Small-24B-Instruct-2501 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\nNote: Running Mistral-Small-24B-Instruct-2501 on GPU requires ~55 GB of GPU RAM in bf16 or fp16.\n2. To ping the client you can use a simple Python snippet.\nimport requests\nimport json\nfrom datetime import datetime, timedelta\nurl = \"http://<your-server>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"\n},\n]\ndata = {\"model\": model, \"messages\": messages}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. √Ä plus tard\n# 2. √Ä plus\n# 3. Salut\n# 4. √Ä toute\n# 5. Bisous\n#\n# \nFunction calling\nMistral-Small-24-Instruct-2501 is excellent at function / tool calling tasks via vLLM. E.g.:\nExample\npy\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\nSYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\nuser_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": SYSTEM_PROMPT\n},\n{\n\"role\": \"user\",\n\"content\": user_prompt\n},\n]\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", tensor_parallel_size=8)\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. √Ä plus tard\n# 2. √Ä plus\n# 3. Salut\n# 4. √Ä toute\n# 5. Bisous\n#\n# \nTransformers\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\nfrom transformers import pipeline\nimport torch\nmessages = [\n{\"role\": \"user\", \"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-Small-24B-Instruct-2501\", max_new_tokens=256, torch_dtype=torch.bfloat16)\nchatbot(messages)",
    "Rakuten/RakutenAI-2.0-8x7B-instruct": "RakutenAI-2.0-8x7B-instruct\nModel Description\nModel Evaluation Results\nModel Usage\nModel Details\nLimitations and Bias\nCitation\nRakutenAI-2.0-8x7B-instruct\nModel Description\nRakutenAI-2.0-8x7B-instruct is a fine-tuned variant of RakutenAI-2.0-8x7B, designed to push the boundaries of Japanese large language models (LLMs). Developed as part of Rakuten's systematic effort to refine AI capabilities, this model builds upon the strengths of its foundation counterpart, excelling in instruction-following tasks while maintaining fluency, coherence, and contextual awareness.\nWithin the competitive 13B active parameter category, RakutenAI-2.0-8x7B-instruct achieves one of the highest scores on the Japanese MT-Bench.\nIf you are looking for foundation model, check RakutenAI-2.0-8x7B.\nModel Evaluation Results\nInstruct Model Name\nSize\nActive Parameters\nJapanese MT-Bench Score\nllm-jp/llm-jp-3-13b-instruct\n13B\n13B\n5.68\nelyza/ELYZA-japanese-Llama-2-13b-instruct\n13B\n13B\n4.09\nRakuten/RakutenAI-2.0-8x7B-instruct\n8x7B (47B)\n13B\n7.08\nweblab-GENIAC/Tanuki-8x8B-dpo-v1.0\n8x8B (47B)\n13B\n6.96\nkarakuri-ai/karakuri-lm-8x7b-instruct-v0.1\n8x7B (47B)\n13B\n5.92\ncyberagent/calm3-22b-chat\n22B\n22B\n6.93\nTable1: RakutenAI-2.0-8x7B-instruct performance on MT Bench in comparison with other Japanese open models.\nNote on Evaluation Scores:\nJapanese MT-bench is a set of 80 challenging open-ended questions for evaluating chat assistants on eight dimensions: writing, roleplay, reasoning, math, coding, extraction, stem, humanities. https://github.com/Stability-AI/FastChat/tree/jp-stable/fastchat/llm_judge Evaluation of responses is conducted with GPT4(gpt-4o-2024-05-13) as a judge, in line with public leaderboard.\nThe Japanese research community cautions against not to evaluate fine-tuned models on LM Harness due to task contamination, so we have not included the LM-Harness scores in this model card for instruct models. LLM-jp: jaster„ÇíÁî®„ÅÑ„Å¶„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åå„ÄÅ„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Çí„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Å´‰ΩøÁî®„Åó„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ, llm-jp-eval„ÅÆË©ï‰æ°„Çπ„Ç≥„Ç¢„ÇíÈùûÂ∏∏„Å´È´ò„Åè„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Çã„Åì„Å®„ÅåÊòé„Çâ„Åã„Å´„Å™„Å£„Å¶„ÅÑ„Çã. „Åó„Åü„Åå„Å£„Å¶„ÄÅÈ´ò„ÅÑË©ï‰æ°„Çπ„Ç≥„Ç¢„ÇíÂæó„Åü„Åã„Çâ„Å®„ÅÑ„Å£„Å¶„ÄÅ‰ªñ„ÅÆLLM„Çà„Çä„ÇÇÊÄßËÉΩ„ÅåÂÑ™„Çå„Å¶„ÅÑ„Çã„Å®Êñ≠Ë®Ä„Åô„Çã„ÅÆ„ÅØÈÅ©Âàá„Åß„ÅØ„Å™„ÅÑ„Åì„Å®„Å´Ê≥®ÊÑè„Åï„Çå„Åü„ÅÑ„ÄÇ Machine Translation: It has become clear that models that have been instruction tuned using Jaster can achieve very high evaluation scores on LLM-JP-EVAL, even if test data is not used for instruction tuning. Therefore, please note that it is not appropriate to assert that a model's performance is superior to other LLMs just because it has a high evaluation score. More details can be found at llm-jp-eval.\nFinal score (7.08 +/- 0.035) for RakutenAI-2.0-8x7B-instruct is average of 3 runs on Japanese MT-Bench. Model outputs and judge outputs are uploaded for reference.\nModel Usage\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_path = \"Rakuten/RakutenAI-2.0-8x7B-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\")\nmodel.eval()\nchat = [\n{\"role\": \"system\", \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"},\n{\"role\": \"user\", \"content\": \"How to make an authentic Spanish Omelette?\"},\n]\ninput_ids = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device=model.device)\nattention_mask = input_ids.ne(tokenizer.pad_token_id).long()\ntokens = model.generate(\ninput_ids,\nmax_length=2048,\ndo_sample=False,\nnum_beams=1,\npad_token_id=tokenizer.eos_token_id,\nattention_mask=attention_mask,\n)\nout = tokenizer.decode(tokens[0][len(input_ids[0]):], skip_special_tokens=True)\nprint(\"ASSISTANT:\\n\" + out)\nprint()\nModel Details\nDeveloped by: Rakuten Group, Inc.\nLanguage(s): Japanese, English\nLicense: This model is licensed under Apache License, Version 2.0.\nModel Architecture: Mixture of Experts (2 active experts)\nLimitations and Bias\nThe suite of RakutenAI-2.0 models is capable of generating human-like text on a wide range of topics. However, like all LLMs, they have limitations and can produce biased, inaccurate, or unsafe outputs. Please exercise caution and judgement while interacting with them.\nCitation\nFor citing our work on the suite of RakutenAI-2.0 models, please use:\n@misc{rakutengroup2025rakutenai2.0,\nauthor = {Rakuten Group, Inc.},\ntitle = {RakutenAI-2.0},\nyear = {2025},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/Rakuten},\n}",
    "neavo/modern_bert_multilingual": "Overview\nTechnical Specifications\nReleased Versions\nOther\nÁªºËø∞\nÊäÄÊúØÊåáÊ†á\nÂèëÂ∏ÉÁâàÊú¨\nÂÖ∂‰ªñ\nOverview\nModernBertMultilingual is a multilingual model trained from scratch.\nUses the ModernBERT-base architecture.\nSupports four languages and their variants, including Chinese (Simplified, Traditional), English, Japanese, and Korean.\nPerforms well on mixed East Asian language text tasks.\nTechnical Specifications\nUses a slightly adjusted vocabulary from the Qwen2.5 series to support multilingualism.\nTrained for approximately 100 hours on L40*7 devices, with a training volume of about 60B tokens.\nKey training parameters:\nBatch Size : 1792\nLearing Rate : 5e-04\nMaximum Sequence Length : 512\nOptimizer : adamw_torch\nLR Scheduler: warmup_stable_decay\nTrain Precision : bf16 mix\nFor other technical specifications, please refer to the original release information and paper of ModernBERT-base.\nReleased Versions\nProvides 3 different weight versions:\nbase - Fully trained with general corpus, suitable for various text domains.\nnodecay - Checkpoint before the annealing stage, you can fine-tune it with domain-specific data to better adapt to target domains.\nkeyword_gacha_multilingual - Fine-tuned version using ACGN (e.g., light novels, game text, manga text, etc.) type text.\nModel\nVersion\nDescription\nmodern_bert_multilingual\n20250128\nbase\nmodern_bert_multilingual_nodecay\n20250128\nnodecay\nkeyword_gacha_multilingual_base\n20250128\nkeyword_gacha_multilingual\nOther\nTraining script: Github\nÁªºËø∞\nModernBertMultilingual ÊòØ‰∏Ä‰∏™‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã\n‰ΩøÁî® ModernBERT-base Êû∂ÊûÑ\nÊîØÊåÅ ‰∏≠ÊñáÔºàÁÆÄ‰Ωì„ÄÅÁπÅ‰ΩìÔºâ„ÄÅËã±Êñá„ÄÅÊó•Êñá„ÄÅÈü©Êñá Á≠âÂõõÁßçËØ≠Ë®ÄÂèäÂÖ∂ÂèòÁßç\nÂèØ‰ª•ÂæàÂ•ΩÂ§ÑÁêÜ‰∏ú‰∫öËØ≠Ë®ÄÊ∑∑ÂêàÊñáÊú¨‰ªªÂä°\nÊäÄÊúØÊåáÊ†á\n‰ΩøÁî®Áï•ÂæÆË∞ÉÊï¥ÂêéÁöÑ Qwen2.5 Á≥ªÂàóÁöÑËØçË°®‰ª•ÊîØÊåÅÂ§öËØ≠Ë®Ä\nÂú® L40*7 ÁöÑËÆæÂ§á‰∏äËÆ≠ÁªÉ‰∫ÜÂ§ßÁ∫¶ 100 ‰∏™Â∞èÊó∂ÔºåËÆ≠ÁªÉÈáèÂ§ßÁ∫¶ 60B Token\n‰∏ªË¶ÅËÆ≠ÁªÉÂèÇÊï∞\nBatch Size : 1792\nLearing Rate : 5e-04\nMaximum Sequence Length : 512\nOptimizer : adamw_torch\nLR Scheduler: warmup_stable_decay\nTrain Precision : bf16 mix\nÂÖ∂‰ΩôÊäÄÊúØÊåáÊ†áÂèØ‰ª•ÂèÇËÄÉ ModernBERT-base ÂéüÂßãÂèëÂ∏É‰ø°ÊÅØ‰∏éËÆ∫Êñá\nÂèëÂ∏ÉÁâàÊú¨\nÊèê‰æõ 3 ‰∏™‰∏çÂêåÁöÑÊùÉÈáçÁâàÊú¨\nbase - ‰ΩøÁî®ÈÄöÁî®È¢ÑÊñôÂÆåÊï¥ËÆ≠ÁªÉÔºåÂèØ‰ª•ËæÉÂ•ΩÁöÑÈÄÇÁî®‰∫éÂêÑÁßç‰∏çÂêåÈ¢ÜÂüüÊñáÊú¨\nnodecay - ÈÄÄÁÅ´Èò∂ÊÆµÂºÄÂßãÂâçÁöÑÊ£ÄÊü•ÁÇπÔºå‰Ω†ÂèØ‰ª•Âú®Ëøô‰∏™ÊùÉÈáçÁöÑÂü∫Á°Ä‰∏äÊ∑ªÂä†È¢ÜÂüüËØ≠ÊñôËøõË°åÈÄÄÁÅ´‰ª•‰ΩøÂÖ∂Êõ¥ÈÄÇÂ∫îÁõÆÊ†áÈ¢ÜÂüü\nkeyword_gacha_multilingual - ‰ΩøÁî® ACGNÔºà‰æãÂ¶Ç ËΩªÂ∞èËØ¥„ÄÅÊ∏∏ÊàèÊñáÊú¨„ÄÅÊº´ÁîªÊñáÊú¨ Á≠âÔºâÁ±ªÂûãÊñáÊú¨ËøõË°åÈÄÄÁÅ´ÁöÑÁâàÊú¨\nÊ®°Âûã\nÁâàÊú¨\nËØ¥Êòé\nmodern_bert_multilingual\n20250128\nbase\nmodern_bert_multilingual_nodecay\n20250128\nnodecay\nkeyword_gacha_multilingual_base\n20250128\nkeyword_gacha_multilingual\nÂÖ∂‰ªñ\nËÆ≠ÁªÉËÑöÊú¨ Github",
    "PekingU/rtdetr_v2_r50vd": "RT-DETRv2\nOverview\nPerformance\nHow to use\nTraining\nApplications\nRT-DETRv2\nOverview\nThe RT-DETRv2 model was proposed in RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu. RT-DETRv2 refines RT-DETR by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters.\nThese changes enhance flexibility and practicality while maintaining real-time performance.\nThis model was contributed by @jadechoghari with the help of @cyrilvallez and @qubvel-hf\nThis is\nPerformance\nRT-DETRv2 consistently outperforms its predecessor across all model sizes while maintaining the same real-time speeds.\nHow to use\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r50vd\")\nmodel = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r50vd\")\ninputs = image_processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5)\nfor result in results:\nfor score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\nscore, label = score.item(), label_id.item()\nbox = [round(i, 2) for i in box.tolist()]\nprint(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\ncat: 0.97 [341.14, 25.11, 639.98, 372.89]\ncat: 0.96 [12.78, 56.35, 317.67, 471.34]\nremote: 0.95 [39.96, 73.12, 175.65, 117.44]\nsofa: 0.86 [-0.11, 2.97, 639.89, 473.62]\nsofa: 0.82 [-0.12, 1.78, 639.87, 473.52]\nremote: 0.79 [333.65, 76.38, 370.69, 187.48]\nTraining\nRT-DETRv2 is trained on COCO (Lin et al. [2014]) train2017 and validated on COCO val2017 dataset. We report the standard AP metrics (averaged over uniformly sampled IoU thresholds ranging from 0.50 ‚àí 0.95 with a step size of 0.05), and APval50 commonly used in real scenarios.\nApplications\nRT-DETRv2 is ideal for real-time object detection in diverse applications such as autonomous driving, surveillance systems, robotics, and retail analytics. Its enhanced flexibility and deployment-friendly design make it suitable for both edge devices and large-scale systems + ensures high accuracy and speed in dynamic, real-world environments.",
    "Respair/RiFornet_Vocoder": "Model Card for Model ID\nTraining\nInference\nModel Card for Model ID\nHuggingFace ü§ó - Repository\nDDP is very un-stable, please use the single-gpu training script - if you still want to do it, I suggest uncommenting the grad clipping lines; that should help a lot.\nThis Vocoder, is a combination of HiFTnet and Ringformer. it supports Ring Attention, Conformer and Neural Source Filtering etc.\nThis repository is experimental, expect some bugs and some hardcoded params.\nThe default setting is 44.1khz - 128 Mel bins. but I have provided the necessary script for the 24khz version in the LibriTTS checkpoint's folder.\nHuge Thanks to Johnathan Duering for his help. I mostly implemented this based on his STTS2 Fork.\nNOTE:\nThere are Three checkpoints so far in this repository:\nRiFornet 24khz (trained for roughly 117K~ steps on LibriTTS (360 + 100) and 40 hours of other English datasets.)\nRiFornet 44.1khz (trained for roughly 280K~ steps on a Large (more than 1100 hours) private Multilingual dataset, covering Arabic, Persian, Japanese, English, Russian and also Singing voice in Chinese and Japanese with Quranic recitations in Arabic.\nHiFTNet 44.1khz (trained for ~100K steps, on a similar dataset to RiFornet 44.1khz, but slightly smaller and no singing voice).\nPython >= 3.10\nClone this repository:\ngit clone https://github.com/Respaired/RiFornet_Vocoder\ncd RiFornet_Vocoder/Ringformer\nInstall python requirements:\npip install -r requirements.txt\nTraining\nCUDA_VISIBLE_DEVICES=0 python train_single_gpu.py --config config_v1.json --[args]\nFor the F0 model training, please refer to yl4579/PitchExtractor. This repo includes a pre-trained F0 model on a Mixture of Multilingual data for the previously mentioned configuration. I'm going to quote the HiFTnet's Author: \"Still, you may want to train your own F0 model for the best performance, particularly for noisy or non-speech data, as we found that F0 estimation accuracy is essential for the vocoder performance.\"\nInference\nPlease refer to the notebook inference.ipynb for details.",
    "derickio/chess-gpt-4.5M": "Chess GPT-4.5M\nOverview\nModel Details\nTraining Data\nTraining Configuration\nHow to Use\nGenerating Chess Moves\nLoading the Model in Transformers\nIntended Use\nLimitations\nTraining Process Summary\nAcknowledgements\nChess GPT-4.5M\nOverview\nChess GPT-4.5M is a generative language model trained specifically to generate chess moves and analyze chess games. The model is based on the GPT architecture and was trained with a custom 32-token vocabulary reflecting key chess symbols and notations.\nModel Details\nArchitecture: GPT-based language model (GPT2LMHeadModel)\nParameters: Approximately 4.5M parameters\nLayers: 8 transformer layers\nHeads: 4 attention heads per layer\nEmbedding Dimension: 256\nTraining Sequence Length: 1024 tokens per chess game\nVocabulary: 32 tokens (custom vocabulary)\nTraining Data\nThe model was trained on tokenized chess game data prepared from the Lichess dataset. The preparation process involved:\nTokenizing chess games using a custom 32-token vocabulary.\nCreating binary training files (train.bin and val.bin).\nSaving vocabulary information to meta.pkl.\nTraining Configuration\nThe training configuration, found in config/mac_chess_gpt.py, includes:\nDataset: lichess_hf_dataset\nBatch Size: 2 (optimized for Mac's memory constraints)\nBlock Size: 1023 (1024 including the positional embedding)\nLearning Rate: 3e-4\nMax Iterations: 140,000\nDevice: 'mps' (Mac-specific settings)\nOther Settings: No dropout and compile set to False for Mac compatibility\nHow to Use\nGenerating Chess Moves\nAfter fine-tuning, use the generation script to sample chess moves. Example commands:\nbash\nSample from the model without a provided prompt:\npython sample.py --out_dir=out-chess-mac\nGenerate a chess game sequence starting with a custom prompt:\npython sample.py --out_dir=out-chess-mac --start=\";1.e4\"\nLoading the Model in Transformers\nOnce the model card and converted model files are pushed to the Hugging Face Hub, you can load the model using:\npython\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"your-hf-username/chess-gpt-4.5M\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"your-hf-username/chess-gpt-4.5M\")\nNote: The tokenizer uses a custom vocabulary provided in vocab.json.\nIntended Use\nThe model is intended for:\nGenerating chess move sequences.\nAssisting in automated chess analysis.\nEducational purposes in understanding language model training on specialized domains.\nLimitations\nThe model is trained on a relatively small (4.5M parameter) architecture and may not capture extremely complex chess strategies.\nIt is specialized on chess move generation and may not generalize to standard language tasks.\nTraining Process Summary\nData Preparation: Tokenized the Lichess chess game dataset using a 32-token vocabulary.\nModel Training: Used custom training configurations specified in config/mac_chess_gpt.py.\nModel Conversion: Converted added checkpoint from out-chess-mac/ckpt.pt into a Hugging Face compatible format with convert_to_hf.py.\nRepository Setup: Pushed the converted model files (including custom tokenizer vocab) to the Hugging Face Hub with Git LFS handling large files.\nAcknowledgements\nThis model was developed following inspiration from GPT-2 and adapted for the chess domain.",
    "prithivMLmods/Deep-Fake-Detector-v2-Model": "Deep-Fake-Detector-v2-Model\nOverview\nKey Features\nModel Architecture\nTraining Details\nInference with Hugging Face Pipeline\nInference with PyTorch\nDataset\nLimitations\nEthical Considerations\nFuture Work\nCitation\nDeep-Fake-Detector-v2-Model\nOverview\nThe Deep-Fake-Detector-v2-Model is a state-of-the-art deep learning model designed to detect deepfake images. It leverages the Vision Transformer (ViT) architecture, specifically the google/vit-base-patch16-224-in21k model, fine-tuned on a dataset of real and deepfake images. The model is trained to classify images as either \"Realism\" or \"Deepfake\" with high accuracy, making it a powerful tool for detecting manipulated media.\nClassification report:\nprecision    recall  f1-score   support\nRealism     0.9683    0.8708    0.9170     28001\nDeepfake     0.8826    0.9715    0.9249     28000\naccuracy                         0.9212     56001\nmacro avg     0.9255    0.9212    0.9210     56001\nweighted avg     0.9255    0.9212    0.9210     56001\nConfusion Matrix:\n[[True Positives, False Negatives],\n[False Positives, True Negatives]]\nUpdate : The previous model checkpoint was obtained using a smaller classification dataset. Although it performed well in evaluation scores, its real-time performance was average due to limited variations in the training set. The new update includes a larger dataset to improve the detection of fake images.\nRepository\nLink\nDeep Fake Detector v2 Model\nGitHub Repository\nKey Features\nArchitecture: Vision Transformer (ViT) - google/vit-base-patch16-224-in21k.\nInput: RGB images resized to 224x224 pixels.\nOutput: Binary classification (\"Realism\" or \"Deepfake\").\nTraining Dataset: A curated dataset of real and deepfake images.\nFine-Tuning: The model is fine-tuned using Hugging Face's Trainer API with advanced data augmentation techniques.\nPerformance: Achieves high accuracy and F1 score on validation and test datasets.\nModel Architecture\nThe model is based on the Vision Transformer (ViT), which treats images as sequences of patches and applies a transformer encoder to learn spatial relationships. Key components include:\nPatch Embedding: Divides the input image into fixed-size patches (16x16 pixels).\nTransformer Encoder: Processes patch embeddings using multi-head self-attention mechanisms.\nClassification Head: A fully connected layer for binary classification.\nTraining Details\nOptimizer: AdamW with a learning rate of 1e-6.\nBatch Size: 32 for training, 8 for evaluation.\nEpochs: 2.\nData Augmentation:\nRandom rotation (¬±90 degrees).\nRandom sharpness adjustment.\nRandom resizing and cropping.\nLoss Function: Cross-Entropy Loss.\nEvaluation Metrics: Accuracy, F1 Score, and Confusion Matrix.\nInference with Hugging Face Pipeline\nfrom transformers import pipeline\n# Load the model\npipe = pipeline('image-classification', model=\"prithivMLmods/Deep-Fake-Detector-v2-Model\", device=0)\n# Predict on an image\nresult = pipe(\"path_to_image.jpg\")\nprint(result)\nInference with PyTorch\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom PIL import Image\nimport torch\n# Load the model and processor\nmodel = ViTForImageClassification.from_pretrained(\"prithivMLmods/Deep-Fake-Detector-v2-Model\")\nprocessor = ViTImageProcessor.from_pretrained(\"prithivMLmods/Deep-Fake-Detector-v2-Model\")\n# Load and preprocess the image\nimage = Image.open(\"path_to_image.jpg\").convert(\"RGB\")\ninputs = processor(images=image, return_tensors=\"pt\")\n# Perform inference\nwith torch.no_grad():\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class = torch.argmax(logits, dim=1).item()\n# Map class index to label\nlabel = model.config.id2label[predicted_class]\nprint(f\"Predicted Label: {label}\")\nDataset\nThe model is fine-tuned on the dataset, which contains:\nReal Images: Authentic images of human faces.\nFake Images: Deepfake images generated using advanced AI techniques.\nLimitations\nThe model is trained on a specific dataset and may not generalize well to other deepfake datasets or domains.\nPerformance may degrade on low-resolution or heavily compressed images.\nThe model is designed for image classification and does not detect deepfake videos directly.\nEthical Considerations\nMisuse: This model should not be used for malicious purposes, such as creating or spreading deepfakes.\nBias: The model may inherit biases from the training dataset. Care should be taken to ensure fairness and inclusivity.\nTransparency: Users should be informed when deepfake detection tools are used to analyze their content.\nFuture Work\nExtend the model to detect deepfake videos.\nImprove generalization by training on larger and more diverse datasets.\nIncorporate explainability techniques to provide insights into model predictions.\nCitation\n@misc{Deep-Fake-Detector-v2-Model,\nauthor = {prithivMLmods},\ntitle = {Deep-Fake-Detector-v2-Model},\ninitial = {21 Mar 2024},\nsecond_updated = {31 Jan 2025},\nlatest_updated = {02 Feb 2025}\n}",
    "NewtNewt/MESA": "Model Description\nInstallation\nModel Download\nUsage\nCitation\nAcknowledgements\nMESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data\nPaul Borne--Pons, Mikolaj Czerkawski,Rosalie Martin,\nRomain Rouffet\nCVPR 2025 Workshop MORSE\nMESA is a novel generative model based on latent denoising diffusion capable of generating 2.5D representations of terrain based on the text prompt conditioning supplied via natural language. The model produces two co-registered modalities of optical and depth maps. This model is a finetune of stable-diffusion-2-1 and is builds upon Hugging Face‚Äôs Diffusers library.\nModel Description\nPaper: MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data\nGithub: https://github.com/PaulBorneP/MESA\nProject page: https://paulbornep.github.io/mesa-terrain/\nInstallation\n# Clone the repository\ngit clone https://github.com/PaulBorneP/MESA\ncd MESA\n# using python 3.11.12\npip install -r requirements.txt\nModel Download\nmkdir weights\nhuggingface-cli download NewtNewt/MESA --local-dir ./weights\nUsage\nfrom MESA.pipeline_terrain import TerrainDiffusionPipeline\nimport torch\npipe = TerrainDiffusionPipeline.from_pretrained(\"./weights\", torch_dtype=torch.float16)\npipe.to(\"cuda\");\nprompt = \"A sentinel-2 image of montane forests and mountains in Mexico in August\"\nimage,dem = pipe(prompt, num_inference_steps=50, guidance_scale=7.5)\nCitation\n@inproceedings{mesa2025,\ntitle={MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data},\nauthor={Paul Borne--Pons and Mikolaj Czerkawski and Rosalie Martin and Romain Rouffet},\nyear={2025},\nbooktitle={MORSE Workshop at CVPR 2025},\neprint={2504.07210},\nurl={https://arxiv.org/abs/2504.07210},}\nAcknowledgements\nThis model is the product of a collaboration between Œ¶-lab, European Space Agency (ESA) and the Adobe Research (Paris, France).",
    "gates04/DistilBERT-Network-Intrusion-Detection": "results\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nFramework versions\nresults\nThis model is a fine-tuned version of gates04/DistilBERT-Network-Intrusion-Detection on an unknown dataset.\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 8\neval_batch_size: 32\nseed: 42\noptimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\nlr_scheduler_type: linear\nnum_epochs: 3.0\nFramework versions\nTransformers 4.47.1\nPytorch 2.5.1+cu124\nDatasets 3.2.0\nTokenizers 0.21.0",
    "ttttdiva/CivitAI_Auto12": "No model card",
    "asif00/Kokoro-Conversational": "On Device Speech to Speech Conversational AI\nThe flow works as follows: Loop (VAD -> Whisper -> LM -> TextChunker -> TTS)\nGitHub: https://github.com/asiff00/On-Device-Speech-to-Speech-Conversational-AI\nDemo Video:\nPerformance:\nHow do we reduce latency?\nPriority based text chunking\nLeading filler word LLM Prompting\nResources\nAcknowledgements\nOn Device Speech to Speech Conversational AI\nThis is realtime on-device speech-to-speech AI model. It used a series to tools to achieve that. It uses a combination of voice activity detection, speech recognition, language models, and text-to-speech synthesis to create a seamless and responsive conversational AI experience. The system is designed to run on-device, ensuring low latency and minimal data usage.\nHOW TO RUN IT\nPrerequisites:\nInstall Python 3.8+ (tested with 3.12)\nInstall eSpeak NG (required for voice synthesis)\nInstall Ollama from https://ollama.ai/\nSetup:\nClone the repository git clone https://github.com/asiff00/On-Device-Speech-to-Speech-Conversational-AI.git\nRun git lfs pull to download the models and voices\nCopy .env.template to .env\nAdd your HuggingFace token to .env\nTweak other parameters in .env, if needed [Optional]\nInstall requirements: pip install -r requirements.txt\nAdd any missing packages if not already installed pip install <package_name>\nRun Ollama:\nStart Ollama service\nRun: ollama run qwen2.5:0.5b-instruct-q8_0 or any other model of your choice\nStart Application:\nRun: python speech_to_speech.py\nWait for initialization (models loading)\nStart talking when you see \"Voice Chat Bot Ready\"\nLong press Ctrl+C to stop the application\nWe basically put a few models together to work in a multi-threaded architecture, where each component operates independently but is integrated through a queue management system to ensure performance and responsiveness.\nThe flow works as follows: Loop (VAD -> Whisper -> LM -> TextChunker -> TTS)\nTo achieve that we use:\nVoice Activity Detection: Pyannote:pyannote/segmentation-3.0\nSpeech Recognition: Whisper:whisper-tiny.en (OpenAI)\nLanguage Model: LM Studio/Ollama with qwen2.5:0.5b-instruct-q8_0\nVoice Synthesis: Kokoro:hexgrad/Kokoro-82M (Version 0.19, 16bit)\nWe use custom text processing and queues to manage data, with separate queues for text and audio. This setup allows the system to handle heavy tasks without slowing down. We also use an interrupt mechanism allowing the user to interrupt the AI at any time. This makes the conversation feel more natural and responsive rather than just a generic TTS engine.\nGitHub: https://github.com/asiff00/On-Device-Speech-to-Speech-Conversational-AI\nDemo Video:\nA demo video is uploaded here. Click on the thumbnail or the YouTube link: https://youtu.be/x92FLnwf-nA.\nPerformance:\nI ran this test on an AMD Ryzen 5600G, 16 GB, SSD, and No-GPU setup, achieving consistent ~2s latency. On average, it takes around 1.5s for the system to respond to a user query from the point the user says the last word. Although I haven't tested this on a GPU, I believe testing on a GPU would significantly improve performance and responsiveness.\nHow do we reduce latency?\nPriority based text chunking\nWe capitalize on the streaming output of the language model to reduce latency. Instead of waiting for the entire response to be generated, we process and deliver each chunk of text as soon as they become available, form phrases, and send it to the TTS engine queue. We play the audio as soon as it becomes available. This way, the user gets a very fast response, while the rest of the response is being generated.\nOur custom TextChunker analyzes incoming text streams from the language model and splits them into chunks suitable for the voice synthesizer. It uses a combination of sentence breaks (like periods, question marks, and exclamation points) and semantic breaks (like \"and\", \"but\", and \"however\") to determine the best places to split the text, ensuring natural-sounding speech output.\nThe TextChunker maintains a set of break points:\nSentence breaks: ., !, ? (highest priority)\nSemantic breaks with priority levels:\nLevel 4: however, therefore, furthermore, moreover, nevertheless\nLevel 3: while, although, unless, since\nLevel 2: and, but, because, then\nPunctuation breaks: ; (4), : (4), , (3), - (2)\nWhen processing text, the TextChunker uses a priority-based system:\nLooks for sentence-ending punctuation first (highest priority 5)\nChecks for semantic break words with their associated priority levels\nFalls back to punctuation marks with lower priorities\nSplits at target word count if no natural breaks are found\nThe text chunking method significantly reduces perceived latency by processing and delivering the first chunk of text as soon as it becomes available. Let's consider a hypothetical system where the language model generates responses at a certain rate. If we imagine a scenario where the model produces a response of N words at a rate of R words per second, waiting for the complete response would introduce a delay of N/R seconds before any audio is produced. With text chunking, the system can start processing the first M words as soon as they are ready (after M/R seconds), while the remaining words continue to be generated. This means the user hears the initial part of the response in just M/R seconds, while the rest streams in naturally.\nLeading filler word LLM Prompting\nWe use a another little trick in the LLM prompt to speed up the system‚Äôs first response. We ask the LLM to start its reply with filler words like ‚Äúumm,‚Äù ‚Äúso,‚Äù or ‚Äúwell.‚Äù These words have a special role in language: they create natural pauses and breaks. Since these are single-word responses, they take only milliseconds to convert to audio. When we apply our chunking rules, the system splits the response at the filler word (e.g., ‚Äúumm,‚Äù) and sends that tiny chunk to the TTS engine. This lets the bot play the audio for ‚Äúumm‚Äù almost instantly, reducing perceived latency. The filler words act as natural ‚Äúbridges‚Äù to mask processing delays. Even a short ‚Äúumm‚Äù gives the illusion of a fluid conversation, while the system works on generating the rest of the response in the background. Longer chunks after the filler word might take more time to process, but the initial pause feels intentional and human-like.\nWe have fallback plans for cases when the LLM fails to start its response with fillers. In those cases, we put hand breaks at 2 to 5 words, which comes with a cost of a bit of choppiness at the beginning but that feels less painful than the system taking a long time to give the first response.\nIn practice, this approach can reduce perceived latency by up to 50-70%, depending on the length of the response and the speed of the language model. For example, in a typical conversation where responses average 15-20 words, our techniques can bring the initial response time down from 1.5-2 seconds to just 0.5-0.7 seconds, making the interaction feel much more natural and immediate.\nResources\nThis project utilizes the following resources:\nText-to-Speech Model: Kokoro\nSpeech-to-Text Model: Whisper\nVoice Activity Detection Model: Pyannote\nLarge Language Model Server: Ollama\nFallback Text-to-Speech Engine: eSpeak NG\nAcknowledgements\nThis project draws inspiration and guidance from the following articles and repositories, among others:\nRealtime speech to speech conversation with MiniCPM-o\nA Comparative Guide to OpenAI and Ollama APIs\nBuilding Production-Ready TTS with Kokoro-82M\nKokoro-82M: The Best TTS Model in Just 82 Million Parameters\nStyleTTS2 Model Implementation",
    "moot20/SmolVLM-256M-Base-MLX-4bits": "moot20/SmolVLM-256M-Base-MLX-4bits\nUse with mlx\nmoot20/SmolVLM-256M-Base-MLX-4bits\nThis model was converted to MLX format from HuggingFaceTB/SmolVLM-256M-Base using mlx-vlm version 0.1.12.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model moot20/SmolVLM-256M-Base-MLX-4bits --max-tokens 100 --temp 0.0 --prompt \"Describe this image.\" --image <path_to_image>",
    "moot20/SmolVLM-500M-Base-MLX": "moot20/SmolVLM-500M-Base-MLX\nUse with mlx\nmoot20/SmolVLM-500M-Base-MLX\nThis model was converted to MLX format from HuggingFaceTB/SmolVLM-500M-Base using mlx-vlm version 0.1.12.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model moot20/SmolVLM-500M-Base-MLX --max-tokens 100 --temp 0.0 --prompt \"Describe this image.\" --image <path_to_image>",
    "Rakuten/RakutenAI-2.0-mini-instruct": "RakutenAI-2.0-mini-instruct\nModel Description\nModel Evaluation Results\nModel Usage\nModel Details\nLimitations and Bias\nCitation\nRakutenAI-2.0-mini-instruct\nModel Description\nRakutenAI-2.0-mini-instruct is a lightweight yet powerful fine-tuned variant of RakutenAI-2.0-mini, specifically designed for edge devices and resource-constrained environments. While compact in size, this model delivers efficient, high-quality instruction-following capabilities, making it an ideal choice for on-device AI applications, low-latency inference, and cost-effective deployment. It achieves competitive performance within the sub-2B parameter category on Japanese MT Bench, offering a balance of speed, efficiency, and accuracy for real-world use cases.\nIf you are looking for foundation model, check RakutenAI-2.0-mini.\nModel Evaluation Results\nInstruct Model Name\nSize\nJapanese MT-Bench Score\nRakuten/RakutenAI-2.0-mini-instruct\n1.5B\n4.91\nllm-jp/llm-jp-3-1.8b-instruct\n1.8B\n4.70\nllm-jp/llm-jp-3-3.7b-instruct\n3.7B\n4.98\nSakanaAI/EvoLLM-JP-A-v1-7B\n7B\n3.80\nSakanaAI/EvoLLM-JP-v1-7B\n7B\n4.58\nTable1: RakutenAI-2.0-mini-instruct performance on MT Bench in comparison with other Japanese open models.\nNote on Evaluation Scores:\nJapanese MT-bench is a set of 80 challenging open-ended questions for evaluating chat assistants on eight dimensions: writing, roleplay, reasoning, math, coding, extraction, stem, humanities. https://github.com/Stability-AI/FastChat/tree/jp-stable/fastchat/llm_judge Evaluation of responses is conducted with GPT4(gpt-4o-2024-05-13) as a judge, in line with public leaderboard.\nThe Japanese research community cautions against not to evaluate fine-tuned models on LM Harness due to task contamination, so we have not included the LM-Harness scores in this model card for instruct models. LLM-jp: jaster„ÇíÁî®„ÅÑ„Å¶„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÊñΩ„Åó„Åü„É¢„Éá„É´„Åå„ÄÅ„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Çí„Ç§„É≥„Çπ„Éà„É©„ÇØ„Ç∑„Éß„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Å´‰ΩøÁî®„Åó„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ, llm-jp-eval„ÅÆË©ï‰æ°„Çπ„Ç≥„Ç¢„ÇíÈùûÂ∏∏„Å´È´ò„Åè„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Çã„Åì„Å®„ÅåÊòé„Çâ„Åã„Å´„Å™„Å£„Å¶„ÅÑ„Çã. „Åó„Åü„Åå„Å£„Å¶„ÄÅÈ´ò„ÅÑË©ï‰æ°„Çπ„Ç≥„Ç¢„ÇíÂæó„Åü„Åã„Çâ„Å®„ÅÑ„Å£„Å¶„ÄÅ‰ªñ„ÅÆLLM„Çà„Çä„ÇÇÊÄßËÉΩ„ÅåÂÑ™„Çå„Å¶„ÅÑ„Çã„Å®Êñ≠Ë®Ä„Åô„Çã„ÅÆ„ÅØÈÅ©Âàá„Åß„ÅØ„Å™„ÅÑ„Åì„Å®„Å´Ê≥®ÊÑè„Åï„Çå„Åü„ÅÑ„ÄÇ Machine Translation: It has become clear that models that have been instruction tuned using Jaster can achieve very high evaluation scores on LLM-JP-EVAL, even if test data is not used for instruction tuning. Therefore, please note that it is not appropriate to assert that a model's performance is superior to other LLMs just because it has a high evaluation score. More details can be found at llm-jp-eval.\nFinal score (4.91 +/- 0.023) for RakutenAI-2.0-mini-instruct is average of 3 runs on Japanese MT-Bench. Model outputs and judge outputs are uploaded for reference.\nModel Usage\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_path = \"Rakuten/RakutenAI-2.0-mini-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\")\nmodel.eval()\nchat = [\n{\"role\": \"system\", \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"},\n{\"role\": \"user\", \"content\": \"How to make an authentic Spanish Omelette?\"},\n]\ninput_ids = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device=model.device)\nattention_mask = input_ids.ne(tokenizer.pad_token_id).long()\ntokens = model.generate(\ninput_ids,\nmax_length=2048,\ndo_sample=False,\nnum_beams=1,\npad_token_id=tokenizer.eos_token_id,\nattention_mask=attention_mask,\n)\nout = tokenizer.decode(tokens[0][len(input_ids[0]):], skip_special_tokens=True)\nprint(\"ASSISTANT:\\n\" + out)\nprint()\nModel Details\nDeveloped by: Rakuten Group, Inc.\nLanguage(s): Japanese, English\nLicense: This model is licensed under Apache License, Version 2.0.\nModel Architecture: Transformer\nLimitations and Bias\nThe suite of RakutenAI-2.0 models is capable of generating human-like text on a wide range of topics. However, like all LLMs, they have limitations and can produce biased, inaccurate, or unsafe outputs. Please exercise caution and judgement while interacting with them.\nCitation\nFor citing our work on the suite of RakutenAI-2.0 models, please use:\n@misc{rakutengroup2025rakutenai2.0,\nauthor = {Rakuten Group, Inc.},\ntitle = {RakutenAI-2.0},\nyear = {2025},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/Rakuten},\n}",
    "ohyeah1/Violet-Lyra-Gutenberg-v2": "Violet-Lyra-Gutenberg-v2-12b\nPrompt format:\nViolet-Lyra-Gutenberg-v2-12b\nUpdate:\nActually I do not really like its prose. It is more stable, though. Interestingly it scored amazing on UGI leaderboard. I do think this model is really smart though.\nPrompt format:\nChatML\nmodels:\n- model: redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS\nparameters:\nweight: 0.3\n- model: Nitral-AI/Captain_Eris_Noctis-12B-v0.420\nparameters:\nweight: 0.3\n- model: ohyeah1/Violet-Lyra-Gutenberg\nparameters:\nweight: 0.6\nbase_model: mistralai/Mistral-Nemo-Base-2407\nparameters:\ndensity: 0.5\nepsilon: 0.1\nlambda: 1.0\nnormalize: false\nrescale: true\nmerge_method: della_linear\ntokenizer:\nsource: union\ndtype: bfloat16",
    "p1atdev/dart-v3-sft-preview-E": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nprompt format:\n- name: v3 sft preview E\nversion: v3\nmodel_name_or_path: p1atdev/dart-v3-sft-preview-E\nmodel_type: eager\nprompt_template_id: V3_SFT\nprompt format:\nRATING = Literal[\"<|rating:general|>\", \"<|rating:sensitive|>\", \"<|rating:questionable|>\", <|rating:explicit|>]\nASPECT_RATIO = Literal[\n\"<|aspect_ratio:too_tall|>\",\n\"<|aspect_ratio:tall_wallpaper|>\",\n\"<|aspect_ratio:tall|>\",\n\"<|aspect_ratio:square|>\",\n\"<|aspect_ratio:wide|>\",\n\"<|aspect_ratio:wide_wallpaper|>\",\n\"<|aspect_ratio:too_tall|>\"\n]\nLENGTH = Literal[\"<|length:very_short|>\", \"<|length:medium|>\", \"<|length:very_long|>\"]\ntemplate = (\n\"<|bos|>\"\n\"{rating}{aspect_ratio}{length}\"\n\"<copyright>{copyright}</copyright>\"\n\"<character>{character}</character>\"\n\"<general>{condition}<|input_end|>\"\n)\n# for example:\nprompt = template.format(\nrating=\"<|rating:general|>\",  # RATING\naspect_ratio=\"<|aspect_ratio:tall|>\",  # ASPECT_RATIO\nlength=\"<|length:medium|>\",  # LENGTH\ncopyright=\"original\",\ncharacter=\"my character\",\ncondition=\"1girl, solo, full body\",\n)",
    "blowing-up-groundhogs/emuru_vae": "Emuru Convolutional VAE\nTraining Details\nAuxiliary Networks\nUsage\nCode Example\nCitation\nEmuru Convolutional VAE\nThis repository hosts the Emuru Convolutional VAE, described in our CVPR2025 paper. The model features a convolutional encoder and decoder, each with four layers. The output channels for these layers are 32, 64, 128, and 256, respectively. The encoder downsamples an input RGB image (with three channels and dimensions width and height) to a latent representation with a single channel and spatial dimensions that are one-eighth of the original height and width. This design compresses the style information in the image, allowing a lightweight Transformer Decoder to efficiently process the latent features.\nTraining code is released on GitHub.\nTraining Details\nHardware: NVIDIA RTX 4090\nIterations: 60,000\nOptimizer: AdamW with a learning rate of 0.0001\nLoss Components:\nMAE Loss: weight of 1\nWID Loss: weight of 0.005\nHTR Loss: weight of 0.3 (using noisy teacher-forcing with a probability of 0.3)\nKL Loss: with a beta parameter set to 1e-6\nAuxiliary Networks\nWriter Identification: A ResNet with 6 blocks, trained until achieving 60% accuracy on a synthetic dataset.\nHandwritten Text Recognition (HTR): A Transformer Encoder-Decoder trained until reaching a Character Error Rate (CER) of 0.25 on the synthetic dataset.\nUsage\nYou can load the pre-trained Emuru VAE using Diffusers‚Äô AutoencoderKL interface with a single line of code:\nfrom diffusers import AutoencoderKL\nmodel = AutoencoderKL.from_pretrained(\"blowing-up-groundhogs/emuru_vae\")\nBelow is an example code snippet that demonstrates how to load an image directly from a URL, process it, encode it into the latent space, decode it back to image space, and save the reconstructed image.\nCode Example\nimport torch\nfrom PIL import Image\nfrom diffusers import AutoencoderKL\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.transforms.functional import to_tensor, to_pil_image, normalize\n# Load the pre-trained Emuru VAE from Hugging Face Hub.\nmodel = AutoencoderKL.from_pretrained(\"blowing-up-groundhogs/emuru_vae\")\n# Function to preprocess an RGB image:\n# Loads the image, converts it to RGB, and transforms it to a tensor normalized to [-1, 1].\ndef preprocess_image(image_path):\nimage = Image.open(image_path).convert(\"RGB\")\nimage_tensor = to_tensor(image).unsqueeze(0)  # Add batch dimension\nimage_tensor = normalize(image_tensor, [0.5], [0.5])\nreturn image_tensor\n# Function to postprocess a tensor back to a PIL image for visualization:\n# Clamps the tensor to [-1, 1] and converts it to a PIL image.\ndef postprocess_tensor(tensor):\ntensor = torch.clamp(tensor, -1, 1).squeeze(0)  # Remove batch dimension\ntensor = (tensor + 1) / 2\nreturn to_pil_image(tensor)\n# Example: Encode and decode an image.\n# Replace the following line with your image path.\nimage_path = hf_hub_download(repo_id=\"blowing-up-groundhogs/emuru_vae\", filename=\"samples/lam_sample.jpg\")\ninput_image = preprocess_image(image_path)\n# Encode the image to the latent space.\n# The encode() method returns an object with a 'latent_dist' attribute.\n# We sample from this distribution to obtain the latent representation.\nwith torch.no_grad():\nlatent_dist = model.encode(input_image).latent_dist\nlatents = latent_dist.sample()\n# Decode the latent representation back to image space.\nwith torch.no_grad():\nreconstructed = model.decode(latents).sample\n# Load the original image for comparison.\noriginal_image = Image.open(image_path).convert(\"RGB\")\n# Convert the reconstructed tensor back to a PIL image.\nreconstructed_image = postprocess_tensor(reconstructed)\n# Save the reconstructed image.\nreconstructed_image.save(\"reconstructed_image.png\")\nCitation\nIf you use this VAE in your research or wish to refer to it, please cite:\n@InProceedings{Pippi_2025_CVPR,\nauthor    = {Pippi, Vittorio and Quattrini, Fabio and Cascianelli, Silvia and Tonioni, Alessio and Cucchiara, Rita},\ntitle     = {Zero-Shot Styled Text Image Generation, but Make It Autoregressive},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2025},\npages     = {7910-7919}\n}",
    "dilovancelik/all-distilroberta-v1_danish_law_fine_tune": "SentenceTransformer based on sentence-transformers/all-distilroberta-v1\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nEvaluation\nMetrics\nTraining Details\nTraining Dataset\nEvaluation Dataset\nTraining Hyperparameters\nTraining Logs\nFramework Versions\nCitation\nBibTeX\nSentenceTransformer based on sentence-transformers/all-distilroberta-v1\nThis is a sentence-transformers model finetuned from sentence-transformers/all-distilroberta-v1 on the danish_law_qa dataset. It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: sentence-transformers/all-distilroberta-v1\nMaximum Sequence Length: 512 tokens\nOutput Dimensionality: 768 dimensions\nSimilarity Function: Cosine Similarity\nTraining Dataset:\ndanish_law_qa\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Normalize()\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"dilovancelik/all-distilroberta-v1_danish_law_fine_tune\")\n# Run inference\nsentences = [\n' Hvorn√•r skal udlejeren foretage en lejeneds√¶ttelse? ',\n'Lov om leje (Lejeneds√¶ttelse ved faldende udgifter til ejendomsskatter m.v.):    ¬ß 48. Bortfalder eller neds√¶ttes de i ¬ß¬ß 46 og 47 n√¶vnte skatter, afgifter, udgifter og bidrag, skal udlejeren med virkning fra neds√¶ttelsestidspunktet foretage en tilsvarende lejeneds√¶ttelse for de beboelseslejligheder og beboelsesrum, i hvis leje udgiften har v√¶ret indregnet. Udlejeren skal dog ikke foretage lejeneds√¶ttelse, hvis bortfald eller neds√¶ttelse af skatter, afgifter, udgifter og bidrag samtidig udlignes eller overstiges af nye for√∏gede skatter, afgifter, udgifter eller bidrag. I fredede ejendomme, hvorp√• der er tinglyst en s√¶rlig bevaringsdeklaration i henhold til lovgivningen om bygningsfredning, kan et bel√∏b svarende til den ejendomsskat, der til enhver tid kunne have v√¶ret opkr√¶vet, dog opkr√¶ves som en del af lejen, uanset om en ejendom er fritaget for ejendomsbeskatning.  Stk. 2. Udlejeren skal give lejeren skriftlig meddelelse om neds√¶ttelsen af lejen, senest 6 uger efter at meddelelsen om neds√¶ttelsen af skatter, afgifter, udgifter eller bidrag er kommet frem.\\n',\n'Lov om boligforhold (Fondens udf√∏relse af vedligeholdelses- og forbedringsarbejder):    ¬ß 69. Har en udlejer ikke udf√∏rt vedligeholdelses- og forbedringsarbejder inden en af huslejen√¶vnet fastsat frist, jf. ¬ß 108, stk. 2, ¬ß 114, stk. 2, og ¬ß 147, stk. 3, i lov om leje, kan investeringsfonden p√• beg√¶ring af en lejer lade de n√¶vnte arbejder udf√∏re for udlejerens regning, uanset om udlejeren har indbragt huslejen√¶vnets afg√∏relse for domstolene. Investeringsfonden kan tinglyse meddelelse herom p√• ejendommen og lade den aflyse, n√•r arbejderne er udf√∏rt og fonden har f√•et d√¶kning for sine udl√¶g.  Stk. 2. Bestrider udlejeren, at investeringsfonden har v√¶ret berettiget til at udf√∏re arbejderne, eller udgifternes rimelighed, kan udlejeren deponere bel√∏bet eller efter aftale med fonden stille sikkerhed for bel√∏bets betaling.  Stk. 3. Efter at have foretaget deponering eller stillet sikkerhed, jf. stk. 2, har udlejeren pligt til at anl√¶gge sag ved retten med henblik p√• afklaring af sp√∏rgsm√•let om, hvorvidt Grundejernes Investeringsfond har v√¶ret berettiget til at udf√∏re de omtvistede arbejder, jf. stk. 1. Har udlejeren ikke anlagt sag inden for 12 m√•neder fra deponeringstidspunktet eller tidspunktet for sikkerhedsstillelsen, skal de deponerede midler eller den stillede sikkerhed frigives til Grundejernes Investeringsfond.  Stk. 4. Forrentning af bel√∏b, som fonden har lagt ud, fasts√¶ttes af fonden med en rente, der svarer til den effektive rente for et 30-√•rigt kontant annuitetsl√•n i et godkendt realkreditinstitut.  Stk. 5. S√•fremt udlejeren ikke deponerer eller stiller sikkerhed efter stk. 2, kan fonden til d√¶kning af udgifterne ved arbejdernes udf√∏relse yde udlejeren et l√•n og som sikkerhed for l√•net lade tinglyse pant i ejendommen med fortrinsret efter ejendomsskatter, men uden personlig h√¶ftelse. L√•net udbetales kontant og forrentes med en rentesats, der fasts√¶ttes af fonden, og som ved l√•nets optagelse svarer til forrentningen efter stk. 4. Afdragstiden fasts√¶ttes til h√∏jst 10 √•r. Dette g√¶lder ogs√• for udgifter til d√¶kning af forunders√∏gelser og lign., som er n√∏dvendige for fondens beslutning efter stk. 1. Fonden kan opkr√¶ve et administrationsbidrag svarende til, hvad der opkr√¶ves for realkredit som n√¶vnt i stk. 4. Grundejernes Investeringsfond skal p√• beg√¶ring af en ejendoms administrator efter kapitel 5 udlevere materiale vedr√∏rende ejendommen. Investeringsfonden kan kr√¶ve betaling til d√¶kning af den udgift, der med rimelighed er afholdt til tilvejebringelse af dette materiale.  Stk. 6. Investeringsfonden kan i √∏vrigt bestemme, at indtil 50 pct. af lejeindt√¶gten skal indbetales til fonden, indtil fonden har f√•et d√¶kning for sine udl√¶g med p√•l√∏bne omkostninger og renter. Investeringsfonden skal lade beslutningen, der er gyldig mod enhver, tinglyse p√• ejendommen.  Stk. 7. S√•fremt investeringsfonden efter stk. 6 kan oppeb√¶re en del af lejeindt√¶gten, skal alle lejerne efter p√•krav herom betale lejen til investeringsfonden, som refunderer udlejeren overskydende bel√∏b. Kun betaling til investeringsfonden har frig√∏rende virkning.  Stk. 8. N√¶rmere regler om fondens adgang til at kr√¶ve gebyr for administration i forbindelse med arbejdernes udf√∏relse fasts√¶ttes af indenrigs- og boligministeren.\\n',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 768]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\nEvaluation\nMetrics\nTriplet\nDatasets: danish_law-validator and ai-job-test\nEvaluated with TripletEvaluator\nMetric\ndanish_law-validator\nai-job-test\ncosine_accuracy\n0.9733\n0.9719\nTraining Details\nTraining Dataset\ndanish_law_qa\nDataset: danish_law_qa at 075eed8\nSize: 18,715 training samples\nColumns: question, context, and neg_context\nApproximate statistics based on the first 1000 samples:\nquestion\ncontext\nneg_context\ntype\nstring\nstring\nstring\ndetails\nmin: 2 tokensmean: 31.41 tokensmax: 80 tokens\nmin: 54 tokensmean: 413.02 tokensmax: 512 tokens\nmin: 19 tokensmean: 413.2 tokensmax: 512 tokens\nSamples:\nquestion\ncontext\nneg_context\nHvad skal fremg√• af en lejeaftale hvis udlejeren √∏nsker at opkr√¶ve en leje baseret p√• ¬ß 25, stk. 2\nLov om leje (Lejeaftalens udformning og indhold):    ¬ß 11. En lejeaftale og andre aftaler om det lejede skal udf√¶rdiges skriftligt, n√•r en af parterne kr√¶ver det.  Stk. 2. En lejeaftale anses for indg√•et p√• lovens vilk√•r, i det omfang der ikke udtrykkeligt er fastsat andet i aftalen.  Stk. 3. Er lejens st√∏rrelse ikke aftalt, anses den for at udg√∏re det bel√∏b, der er rimeligt under hensyn til reglerne i kapitel 3 eller reglerne om det lejedes v√¶rdi, jf. ¬ß 42, stk. 2 og 3, eller ¬ß 43.  Stk. 4. √ònsker udlejeren at opkr√¶ve en leje, hvor afkastet beregnes efter ¬ß 25, stk. 2, skal det fremg√• af lejeaftalen, hvorn√•r den aftalte leje senest er fastsat efter ¬ß 23, og der skal i aftalen sk√∏nsm√¶ssigt angives st√∏rrelsen af den leje, der maksimalt kan beregnes for lejem√•let p√• tidspunktet for lejeaftalens indg√•else. Det skal tillige fremg√•, om den aftalte leje er nedsat i henhold til ¬ß 23, stk. 2.  Stk. 5. For lejeaftaler, som er omfattet af udlejerens beslutning om regulering efter nettoprisindeks...\nLov om Kreditforeningen af kommuner og regioner i Danmark (intro): Lov om Kreditforeningen af kommuner og regioner i Danmark VI MARGRETHE DEN ANDEN, af Guds N√•de Danmarks Dronning, g√∏r vitterligt: Folketinget har vedtaget og Vi ved Vort samtykke stadf√¶stet f√∏lgende lov:\nG√¶lder reglerne i forvaltningsloven og loven om offentlighed i forvaltningen for sager, hvor en tilskudsmodtager tr√¶ffer afg√∏relse om ydelse af tilskud p√• vegne af staten?\nLov om √∏konomiske og administrative forhold for modtagere af driftstilskud fra Kulturministeriet (Kapitel 6): Andre bestemmelser  ¬ß¬†10. Tilskudsmodtageren skal sikre, at informationer om tilskudsmodtagerens organisation og aktiviteter er let tilg√¶ngelige for offentligheden.  ¬ß¬†11. Reglerne i forvaltningsloven og lov om offentlighed i forvaltningen g√¶lder for de sager, hvori en tilskudsmodtager p√• vegne af staten tr√¶ffer afg√∏relse om ydelse af tilskud.\nLov om en terrorforsikringsordning p√• skadesforsikringsomr√•det (Kapitel 2): Finansiering og tilbagebetaling    ¬ß 3. Udbetalinger efter loven samt de medf√∏lgende administrationsomkostninger finansieres ved et statsligt genudl√•n. Der kan optages genudl√•n inden for en samlet ramme p√• 13,4 mia. kr. (2010-niveau). Genudl√•nsrammen reguleres efter personskattelovens ¬ß 20.  Stk. 2.  Overstiger v√¶rdien af de samlede erstatningsberettigede skader i forbindelse med et NBCR-terrorangreb genudl√•nsrammen, har de erstatningsberettigede alene krav p√• en forholdsm√¶ssig udbetaling.  Stk. 3.  Tr√¶k p√• det statslige genudl√•n skal tilbagebetales med en forrentning svarende til den til enhver tid fastsatte officielle diskonto. Renten tilskrives ved hvert kalender√•rs udl√∏b. Genudl√•net tilbagebetales via efterf√∏lgende opkr√¶vninger af en √•rlig afgift p√•lagt forsikringspolicer omfattet af ordningen.  Stk. 4. Afgiften opkr√¶ves af skadesforsikringsselskaberne og udg√∏r et till√¶g til forsikringspr√¶mien p√• 5 pct. pr....\nHvordan beskytter loven om leje lejerens rettigheder?\nLov om leje (Fravigelighed):    ¬ß 148. ¬ß¬ß 142 og 143, ¬ß 144 stk. 2, 2. pkt., og ¬ß¬ß 145 og 146 kan ikke fraviges til skade for lejeren.    Kapitel 17   Lejerens brug af det lejede\nLov om projektering og anl√¶g af en energi√∏ i Nords√∏en (Projektering og meddelelse af tilladelser):    ¬ß 2. Klima-, energi- og forsyningsministeren bemyndiges til at foretage forberedelse og projektering, herunder afholdelse af udbud, og de √∏vrige dispositioner, som er n√∏dvendige for anl√¶g af energi√∏en.    ¬ß 3. Efter ans√∏gning fra udbudsvinderen meddeler klima-, energi- og forsyningsministeren tilladelse til gennemf√∏relse af forunders√∏gelser til energi√∏en.  Stk. 2. Klima-, energi- og forsyningsministeren kan fasts√¶tte vilk√•r for tilladelsen til forunders√∏gelser, herunder om de forhold, som skal unders√∏ges, om forunders√∏gelsernes forl√∏b og tidsrum og om overholdelsen af milj√∏- og sikkerhedskrav.    ¬ß 4. Klima-, energi- og forsyningsministeren meddeler tilladelse til anl√¶g af energi√∏en.  Stk. 2. Tilladelsen, der fasts√¶tter den konkrete placering, konstruktion og st√∏rrelse og det konkrete design af energi√∏en, meddeles p√• baggrund af udbudsvinderens projektbeskrivelse og ans√∏gning om tillad...\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nEvaluation Dataset\ndanish_law_qa\nDataset: danish_law_qa at 075eed8\nSize: 18,715 evaluation samples\nColumns: question, context, and neg_context\nApproximate statistics based on the first 1000 samples:\nquestion\ncontext\nneg_context\ntype\nstring\nstring\nstring\ndetails\nmin: 10 tokensmean: 31.79 tokensmax: 88 tokens\nmin: 19 tokensmean: 410.47 tokensmax: 512 tokens\nmin: 19 tokensmean: 424.61 tokensmax: 512 tokens\nSamples:\nquestion\ncontext\nneg_context\nKan vildsvinehegnet udg√∏res af andre foranstaltninger end almindelige hegn p√• visse str√¶kninger?\nLov om projektering og anl√¶g af et vildsvinehegn langs den dansk-tyske landegr√¶nse (Bemyndigelse til projektering og meddelelse af anl√¶gstilladelse):    ¬ß 1. Milj√∏- og f√∏devareministeren bemyndiges til at projektere og meddele tilladelse til anl√¶g af et vildsvinehegn langs den dansk-tyske landegr√¶nse, herunder til at tr√¶ffe de √∏vrige dispositioner i form af eksempelvis afv√¶rgeforanstaltninger eller kompenserende foranstaltninger, som er n√∏dvendige i forbindelse med projekteringen og anl√¶gget af hegnet.  Stk. 2. Tilladelse til anl√¶g af vildsvinehegnet, der fastl√¶gger hegnets pr√¶cise placering og udformning m.v. langs den dansk-tyske landegr√¶nse, meddeles af milj√∏- og f√∏devareministeren efter projektbeskrivelse og ans√∏gning om anl√¶gstilladelse fra Naturstyrelsen. Milj√∏- og f√∏devareministeren kan tillade, at vildsvinehegnet p√• visse str√¶kninger udg√∏res af andre foranstaltninger med barrierevirkning for vildsvin end almindelige hegn, herunder f.eks. flydesp√¶rringer.  Projektbeskrivelse, an...\nLov om flagning (Kapitel 1):  Danmarks nationalflag     ¬ß 1. Danmarks Riges nationalflag er Dannebrog.  Stk. 2. Dannebrog som splitflag f√∏res af kongehuset, Folketinget, statslige myndigheder og domstolene samt institutioner m.v., der har s√¶rlig tilladelse hertil.\nHvad er form√•let med loven om internationale sikkerhedsrettigheder i flymateriel?\nLov om internationale sikkerhedsrettigheder i flymateriel (Kapitel 1):  Anvendelsesomr√•de     ¬ß 1. Bestemmelserne i Cape Town-konventionen, jf. bilag 1 til denne lov, og den til konventionen knyttede flymaterielprotokol, jf. bilag 2 til denne lov, g√¶lder her i landet, jf. dog stk. 2-4.  Stk. 2. Protokollens artikel VIII og XXI finder ikke anvendelse.  Stk. 3. Konventionens artikel 13 og 43 finder anvendelse inden for rammerne af Bruxelles I-forordningen, jf. lov om Bruxelles I-forordningen m.v.  Stk. 4. Protokollens artikel XI finder anvendelse med den ordlyd, der fremg√•r af bestemmelsens alternativ A. Karenstiden i henhold til bestemmelsen fasts√¶ttes til 60 dage.    ¬ß 2. Sikkerhedsrettigheder i flygenstande, jf. protokollens artikel I, stk. 2, litra c, der er stiftet ved retsforf√∏lgning, skal registreres i medf√∏r af konventionen og protokollen for at opn√• beskyttelse over for aftaler om flygenstanden og mod retsforf√∏lgning.    ¬ß 3. Fortrinsret for krav p√• offentlige afgifter i her i l...\nLov om opgradering af √òresundsbanen (Kapitel 1):  Projektering og anl√¶g     ¬ß 1. Loven omfatter f√∏lgende jernbaneprojekter, som gennemf√∏res inden for de omr√•der, der fremg√•r af lovens bilag 1-3:  1) Udvidelse af K√∏benhavns Lufthavn Station.  2) Etablering af overhalingsspor ved Kalvebod.  3) Etablering af vendespor ved K√∏benhavns Lufthavn Station.  Stk. 2. Sund & B√¶lt Holding A/S med tilh√∏rende datterselskaber kan projektere og anl√¶gge jernbaneprojekterne n√¶vnt i stk. 1.\nHvad er bilag 1 og bilag 3 i loven omhandlende?\nLov om anvendelse af multilateral konvention til gennemf√∏relse af tiltag i dobbeltbeskatningsoverenskomster til forhindring af skatteudhuling og overskudsflytning (intro): Lov om anvendelse af multilateral konvention til gennemf√∏relse af tiltag i dobbeltbeskatningsoverenskomster til forhindring af skatteudhuling og overskudsflytning VI MARGRETHE DEN ANDEN, af Guds N√•de Danmarks Dronning, g√∏r vitterligt: Folketinget har vedtaget og Vi ved Vort samtykke stadf√¶stet f√∏lgende lov:    ¬ß 1. Bestemmelserne i multilateral konvention af 24. november 2016 til gennemf√∏relse af tiltag i dobbeltbeskatningsoverenskomster til forhindring af skatteudhuling og overskudsflytning, jf. bilag 1, kan anvendes p√• de dobbeltbeskatningsoverenskomster, der er n√¶vnt i bilag 3, jf. dog stk. 2. 1. pkt. kan endvidere anvendes p√• overenskomst af 14. marts 2018 mellem Kongeriget Danmark og Republikken Armenien til undg√•else af dobbeltbeskatning og forhindring af skatteunddragelse, for s√• vidt ang√•r indkomst- og formue...\nLov om r√∏rf√∏rt transport af CO21) (Kapitel 5): Tilsyn    ¬ß 13. Klima-, energi- og forsyningsministeren f√∏rer tilsyn med, at denne lov, regler udstedt i medf√∏r af loven og vilk√•r udf√¶rdiget i tilladelser overholdes.  Stk. 2. Klima-, energi- og forsyningsministeren kan meddele p√•bud om overholdelse af loven og forskrifter udstedt i medf√∏r heraf.  Stk. 3. Klima-, energi- og forsyningsministeren kan fasts√¶tte n√¶rmere regler om ud√∏velsen af tilsynet.    ¬ß 14. Klima-, energi- og forsyningsministeren kan i forbindelse med behandling af sager og ud√∏velse af tilsyn omfattet af denne lov indhente oplysninger, som er n√∏dvendige for varetagelsen af disse opgaver, hos rettighedshavere.  Stk. 2. Ved manglende udlevering af alle n√∏dvendige oplysninger kan klima-, energi- og forsyningsministeren p√•l√¶gge rettighedshaver at indsende manglende oplysninger og fasts√¶tte en tidsfrist for en s√•dan indsendelse.  Stk. 3. Oplysninger, som klima-, energi- og forsyningsministeren modtager i forbindelse med behand...\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nTraining Hyperparameters\nNon-Default Hyperparameters\neval_strategy: steps\nper_device_train_batch_size: 16\nper_device_eval_batch_size: 16\nlearning_rate: 2e-05\nnum_train_epochs: 1\nwarmup_ratio: 0.1\nbatch_sampler: no_duplicates\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: False\ndo_predict: False\neval_strategy: steps\nprediction_loss_only: True\nper_device_train_batch_size: 16\nper_device_eval_batch_size: 16\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 1\neval_accumulation_steps: None\ntorch_empty_cache_steps: None\nlearning_rate: 2e-05\nweight_decay: 0.0\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1.0\nnum_train_epochs: 1\nmax_steps: -1\nlr_scheduler_type: linear\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.1\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 42\ndata_seed: None\njit_mode_eval: False\nuse_ipex: False\nbf16: False\nfp16: False\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: None\nlocal_rank: 0\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: False\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: False\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch\noptim_args: None\nadafactor: False\ngroup_by_length: False\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: False\nhub_always_push: False\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\ninclude_for_metrics: []\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ndispatch_batches: None\nsplit_batches: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\neval_on_start: False\nuse_liger_kernel: False\neval_use_gather_object: False\naverage_tokens_across_devices: False\nprompts: None\nbatch_sampler: no_duplicates\nmulti_dataset_batch_sampler: proportional\nTraining Logs\nEpoch\nStep\nTraining Loss\nValidation Loss\ndanish_law-validator_cosine_accuracy\nai-job-test_cosine_accuracy\n-1\n-1\n-\n-\n0.8259\n-\n0.0947\n100\n1.522\n1.0103\n0.9498\n-\n0.1894\n200\n1.0991\n0.8589\n0.9573\n-\n0.2841\n300\n0.9759\n0.8044\n0.9658\n-\n0.3788\n400\n0.85\n0.7616\n0.9658\n-\n0.4735\n500\n0.8369\n0.7363\n0.9722\n-\n0.5682\n600\n0.8284\n0.6936\n0.9712\n-\n0.6629\n700\n0.8747\n0.6794\n0.9701\n-\n0.7576\n800\n0.7615\n0.6735\n0.9701\n-\n0.8523\n900\n0.7862\n0.6651\n0.9733\n-\n0.9470\n1000\n0.749\n0.6638\n0.9722\n-\n-1\n-1\n-\n-\n0.9733\n0.9719\nFramework Versions\nPython: 3.12.7\nSentence Transformers: 3.4.0\nTransformers: 4.46.3\nPyTorch: 2.5.1\nAccelerate: 1.3.0\nDatasets: 3.2.0\nTokenizers: 0.20.3\nCitation\nBibTeX\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nMultipleNegativesRankingLoss\n@misc{henderson2017efficient,\ntitle={Efficient Natural Language Response Suggestion for Smart Reply},\nauthor={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\nyear={2017},\neprint={1705.00652},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus": "Use a pipeline as a high-level helper\nvLLM\nCall the server using curl:\n5. Citation\nUploaded  model\nFined tune using ORPO tunign method\nIntroduction\nIntroducing Reasoning Llama 3.2 1B: The Next Evolution in Conversational AI\nWe are thrilled to unveil Reasoning Llama 3.2, the latest advancement in our suite of AI models. Building upon the robust foundation of the renowned Llama series, Reasoning Llama 3.2 introduces the groundbreaking Chain of Thought (CoT) capabilities, elevating its reasoning prowess to new heights.\nKey Features of Reasoning Llama 3.2 1B:\nEnhanced Chain of Thought Reasoning: At the core of Reasoning Llama 3.2  lies its sophisticated CoT framework, enabling the model to perform multi-step reasoning with greater accuracy and coherence. This ensures more reliable and contextually appropriate responses, especially for complex queries that require logical progression.\nConversational Excellence: Designed with interactivity in mind, Reasoning Llama 3.2 excels in maintaining engaging and fluid conversations. Whether it's casual dialogue or in-depth discussions, the model adapts seamlessly to various conversational styles, providing users with a natural and intuitive interaction experience.\nInstruction-Supervised Fine-Tuning: Leveraging advanced supervised fine-tuning techniques, Reasoning Llama 3.2 has been meticulously trained on diverse instructional data. This fine-tuning process enhances the model's ability to understand and execute user instructions with precision, making it an invaluable tool for a wide range of applications.\nUnsloth Integration: Incorporating Unsloth, our proprietary unsupervised learning framework, Reasoning Llama 3.2 benefits from continuous learning capabilities. This integration allows the model to adapt and improve over time, ensuring it remains up-to-date with evolving language patterns and user needs without the constant need for manual intervention.\nQuick Inference reasoning 1B model.\nWhy Choose Reasoning Llama 3.2 1B?\nReasoning Llama 3.2 stands out as a versatile and powerful AI solution tailored for both developers and end-users. Its combination of advanced reasoning, conversational intelligence, and adaptive learning mechanisms make it ideally suited for applications ranging from customer support and virtual assistants to educational tools and creative content generation.\nAs we continue to push the boundaries of artificial intelligence, Reasoning Llama 3.2 exemplifies our commitment to delivering state-of-the-art models that empower users with intelligent, reliable, and user-friendly technology. Experience the future of conversational AI with Reasoning Llama 3.2 and unlock new possibilities in human-machine interaction.\nHow to use\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport torch\nfrom transformers import pipeline\nmodel_id = \"EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a powerful AI super conscious emotional assistant\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=4048,\n)\nprint(outputs[0][\"generated_text\"][-1])\nUse a pipeline as a high-level helper\nfrom transformers import pipeline\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\npipe = pipeline(\"text-generation\", model=\"EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus\")\npipe(messages)\nvLLM\nCall the server using curl:\npip install vllm\n# Load and run the model:\nvllm serve \"EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus\"\ncurl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n--data '{\n\"model\": \"EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"What is the capital of France?\"\n}\n]\n}'\n5. Citation\n@misc{EpistemeAI2025,\nauthor={Thomas Yiu},\nyear={2025},\n}\n@misc{bespoke_stratos,\nauthor = {Bespoke Labs},\ntitle = {Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation},\nhowpublished = {https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation},\nnote = {Accessed: 2025-01-22},\nyear = {2025}\n}\n@misc{numina_math_datasets,\nauthor = {Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu},\ntitle = {NuminaMath TIR},\nyear = {2024},\npublisher = {Numina},\njournal = {Hugging Face repository},\nhowpublished = {\\url{[https://huggingface.co/AI-MO/NuminaMath-TIR](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)}}\n}\nUploaded  model\nDeveloped by: EpistemeAI\nLicense: apache-2.0\nFinetuned from model : EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "mradermacher/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n0.7\nGGUF\nQ3_K_S\n0.7\nGGUF\nQ3_K_M\n0.8\nlower quality\nGGUF\nQ3_K_L\n0.8\nGGUF\nIQ4_XS\n0.8\nGGUF\nQ4_K_S\n0.9\nfast, recommended\nGGUF\nQ4_K_M\n0.9\nfast, recommended\nGGUF\nQ5_K_S\n1.0\nGGUF\nQ5_K_M\n1.0\nGGUF\nQ6_K\n1.1\nvery good quality\nGGUF\nQ8_0\n1.4\nfast, best quality\nGGUF\nf16\n2.6\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "mradermacher/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/EpistemeAI/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus\nstatic quants are available at https://huggingface.co/mradermacher/Reasoning-Llama-3.2-1B-Instruct-v1.3-plus-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n0.5\nfor the desperate\nGGUF\ni1-IQ1_M\n0.5\nmostly desperate\nGGUF\ni1-IQ2_XXS\n0.5\nGGUF\ni1-IQ2_XS\n0.6\nGGUF\ni1-IQ2_S\n0.6\nGGUF\ni1-IQ2_M\n0.6\nGGUF\ni1-Q2_K_S\n0.7\nvery low quality\nGGUF\ni1-IQ3_XXS\n0.7\nlower quality\nGGUF\ni1-Q2_K\n0.7\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XS\n0.7\nGGUF\ni1-Q3_K_S\n0.7\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n0.7\nbeats Q3_K*\nGGUF\ni1-IQ3_M\n0.8\nGGUF\ni1-Q3_K_M\n0.8\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n0.8\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n0.8\nGGUF\ni1-IQ4_NL\n0.9\nprefer IQ4_XS\nGGUF\ni1-Q4_0\n0.9\nfast, low quality\nGGUF\ni1-Q4_K_S\n0.9\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n0.9\nfast, recommended\nGGUF\ni1-Q4_1\n0.9\nGGUF\ni1-Q5_K_S\n1.0\nGGUF\ni1-Q5_K_M\n1.0\nGGUF\ni1-Q6_K\n1.1\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "Zyphra/Zonos-v0.1-transformer": "Zonos-v0.1\nUsage\nPython\nGradio interface (recommended)\nFeatures\nInstallation\nDocker installation\nCitation\nZonos-v0.1\nZonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par with‚Äîor even surpassing‚Äîtop TTS providers.\nOur model enables highly natural speech generation from text prompts when given a speaker embedding or audio prefix, and can accurately perform speech cloning when given a reference clip spanning just a few seconds. The conditioning setup also allows for fine control over speaking rate, pitch variation, audio quality, and emotions such as happiness, fear, sadness, and anger. The model outputs speech natively at 44kHz.\nFor more details and speech samples, check out our blog here\nWe also have a hosted version available at playground.zyphra.com/audio\nZonos follows a straightforward architecture: text normalization and phonemization via eSpeak, followed by DAC token prediction through a transformer or hybrid backbone. An overview of the architecture can be seen below.\nUsage\nPython\nimport torch\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n# model = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-hybrid\", device=\"cuda\")\nmodel = Zonos.from_pretrained(\"Zyphra/Zonos-v0.1-transformer\", device=\"cuda\")\nwav, sampling_rate = torchaudio.load(\"assets/exampleaudio.mp3\")\nspeaker = model.make_speaker_embedding(wav, sampling_rate)\ncond_dict = make_cond_dict(text=\"Hello, world!\", speaker=speaker, language=\"en-us\")\nconditioning = model.prepare_conditioning(cond_dict)\ncodes = model.generate(conditioning)\nwavs = model.autoencoder.decode(codes).cpu()\ntorchaudio.save(\"sample.wav\", wavs[0], model.autoencoder.sampling_rate)\nGradio interface (recommended)\nuv run gradio_interface.py\n# python gradio_interface.py\nThis should produce a sample.wav file in your project root directory.\nFor repeated sampling we highly recommend using the gradio interface instead, as the minimal example needs to load the model every time it is run.\nFeatures\nZero-shot TTS with voice cloning: Input desired text and a 10-30s speaker sample to generate high quality TTS output\nAudio prefix inputs: Add text plus an audio prefix for even richer speaker matching. Audio prefixes can be used to elicit behaviours such as whispering which can otherwise be challenging to replicate when cloning from speaker embeddings\nMultilingual support: Zonos-v0.1 supports English, Japanese, Chinese, French, and German\nAudio quality and emotion control: Zonos offers fine-grained control of many aspects of the generated audio. These include speaking rate, pitch, maximum frequency, audio quality, and various emotions such as happiness, anger, sadness, and fear.\nFast: our model runs with a real-time factor of ~2x on an RTX 4090\nGradio WebUI: Zonos comes packaged with an easy to use gradio interface to generate speech\nSimple installation and deployment: Zonos can be installed and deployed simply using the docker file packaged with our repository.\nInstallation\nAt the moment this repository only supports Linux systems (preferably Ubuntu 22.04/24.04) with recent NVIDIA GPUs (3000-series or newer, 6GB+ VRAM).\nSee also Docker Installation\nSystem dependencies\nZonos depends on the eSpeak library phonemization. You can install it on Ubuntu with the following command:\napt install -y espeak-ng\nPython dependencies\nWe highly recommend using a recent version of uv for installation. If you don't have uv installed, you can install it via pip: pip install -U uv.\nInstalling into a new uv virtual environment (recommended)\nuv sync\nuv sync --extra compile\nInstalling into the system/actived environment using uv\nuv pip install -e .\nuv pip install -e .[compile]\nInstalling into the system/actived environment using pip\npip install -e .\npip install --no-build-isolation -e .[compile]\nConfirm that it's working\nFor convenience we provide a minimal example to check that the installation works:\nuv run sample.py\n# python sample.py\nDocker installation\ngit clone https://github.com/Zyphra/Zonos.git\ncd Zonos\n# For gradio\ndocker compose up\n# Or for development you can do\ndocker build -t Zonos .\ndocker run -it --gpus=all --net=host -v /path/to/Zonos:/Zonos -t Zonos\ncd /Zonos\npython sample.py # this will generate a sample.wav in /Zonos\nCitation\nIf you find this model useful in an academic context please cite as:\n@misc{zyphra2025zonos,\ntitle     = {Zonos-v0.1: An Expressive, Open-Source TTS Model},\nauthor    = {Dario Sucic, Mohamed Osman, Gabriel Clark, Chris Warner, Beren Millidge},\nyear      = {2025},\n}",
    "strangerzonehf/cinematicShot-Pics-Flux": "Best Dimensions & Inference\nInference Range\nSetting Up\nTrigger words\nDownload model\nPrompt\ncinematic shot, Captured at eye-level on a cloudy day, a woman stands with her arms outstretched, wearing a brown turtleneck and blue jeans. Her jacket is draped over her shoulders, adding a touch of warmth to her outfit. The jacket is adorned with a white fur trim. The womans hair is pulled back in a ponytail, framing her face. In the distance, a body of water can be seen, with a mountain range in the background.\nPrompt\ncinematic shot, a medium-angle shot captures a young woman seated in a convertible car. She is dressed in a red and blue sweater, a white scarf, and blue jeans. Her left arm is raised above her head, adding a touch of balance to the scene. The cars interior is a deep blue, with a black seat, and a white canopy. The womans gaze is directed towards the left side of the frame, and her left hand is resting on her right arm.\nPrompt\ncinematic shot, Captured at eye-level on a high-angle shot of a young man sitting on the hood of a white sports car. The man is dressed in a black and white jacket, black pants, and white sneakers. His hair is short and dark, and he has a serious expression on his face. The car is adorned with the word Color in bold black letters, and the back of the car is a bright orange. The sky behind the man is a light blue with a few wispy clouds.\nPrompt\ncinematic shot, a medium-sized man stands in the foreground of the frame. He is dressed in a black jacket, black pants, and a black backpack. His hair is short and dark, and hes looking off to the side. The backdrop is a stark white wall, and the ceiling is adorned with metal pipes and beams. To the left of the man, a man with dark hair and glasses is looking off into the distance.\n# Model description for cinematicShot-Pics-Flux\nImage Processing Parameters\nParameter\nValue\nParameter\nValue\nLR Scheduler\nconstant\nNoise Offset\n0.03\nOptimizer\nAdamW\nMultires Noise Discount\n0.1\nNetwork Dim\n64\nMultires Noise Iterations\n10\nNetwork Alpha\n32\nRepeat & Steps\n17 & 2600\nEpoch\n22\nSave Every N Epochs\n1\nLabeling: florence2-en(natural language & English)\nTotal Images Used for Training : 33\nBest Dimensions & Inference\nDimensions\nAspect Ratio\nRecommendation\n1280 x 832\n3:2\nBest\n1024 x 1024\n1:1\nDefault\nInference Range\nRecommended Inference Steps: 30‚Äì35\nSetting Up\nimport torch\nfrom pipelines import DiffusionPipeline\nbase_model = \"black-forest-labs/FLUX.1-dev\"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\nlora_repo = \"strangerzonehf/cinematicShot-Pics-Flux\"\ntrigger_word = \"cinematic shot\"\npipe.load_lora_weights(lora_repo)\ndevice = torch.device(\"cuda\")\npipe.to(device)\nTrigger words\nYou should use cinematic shot to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "inceptionai/Llama-3.1-Sherkala-8B-Chat": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nLlama-3.1-Sherkala-8B-Chat\nSherkala Details\nHow to Get Started with the Model:\nModel Architecture\nTraining Data\nTraining Details\nTraining Hyperparameters\nTraining Infrastructure\nEvaluation\nDownstream Evaluation\nGeneration Evaluation\nIntended Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nLlama-3.1-Sherkala-8B-Chat\nLlama-3.1-Sherkala-8B-Chat (Sherkala for short) is a state-of-the-art 8 billion parameter instruction-tuned large language model (LLM) designed primarily for Kazakh while maintaining robust performance in English, Russian, and Turkish. Developed by Inception (a G42 company) and MBZUAI, in collaboration with Cerebras Systems, Sherkala leverages a balanced mixture of multilingual data and a custom tokenizer to overcome the challenges of data scarcity in Kazakh. This model has been optimized for downstream tasks, safe text generation, and cultural alignment.\nSherkala Details\nDeveloped by: Inception (a G42 company), MBZUAI, Cerebras Systems.\nLanguages: Kazakh (primary), English, Russian, Turkish.\nInput: Text.\nOutput: Generated text.\nModel Size: 8B parameters.\nContext Length: 8,192 tokens.\nTechnical Report: Sherkala Technical Report\nLicense: cc-by-nc-sa-4.0\nHow to Get Started with the Model:\nBelow is sample code to use the model. The code below is tested on transformers==4.46.2.\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_path=\"inceptionai/Llama-3.1-Sherkala-8B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer.chat_template=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role']+'<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %} {% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\ndef get_response(text):\nconversation = [\n{\"role\": \"user\", \"content\": text}\n]\ninput_ids = tokenizer.apply_chat_template(\nconversation=conversation,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\").to(device)\n# Generate a response\ngen_tokens = model.generate(\ninput_ids,\nmax_new_tokens=500,\nstop_strings=[\"<|eot_id|>\"],\ntokenizer=tokenizer\n)\n# Decode and print the generated text along with generation prompt\ngen_text = tokenizer.decode(gen_tokens[0][len(input_ids[0]): -1])\nreturn gen_text\nquestion = '“ö–∞–∑–∞“õ—Å—Ç–∞–Ω–Ω—ã“£ –∂–∞“õ—Å—ã —Ç–∞“ì–∞–º–¥–∞—Ä—ã–Ω “±—Å—ã–Ω–∞ –∞–ª–∞—Å—ã–∑ –±–∞?'\nprint(get_response(question))\nResponse:\n–ë–µ—à–±–∞—Ä–º–∞“õ: –ë“±–ª “ö–∞–∑–∞“õ—Å—Ç–∞–Ω–Ω—ã“£ –µ“£ —Ç–∞–Ω—ã–º–∞–ª —Ç–∞“ì–∞–º—ã –±–æ–ª—É—ã –º“Ø–º–∫—ñ–Ω. –ë“±–ª –µ—Ç (”ô–¥–µ—Ç—Ç–µ “õ–æ–π, —Å–∏—ã—Ä –Ω–µ–º–µ—Å–µ –∂—ã–ª“õ—ã) —Å“Ø–π–µ–∫–∫–µ –¥–µ–π—ñ–Ω –ø—ñ—Å—ñ—Ä—ñ–ª–µ—Ç—ñ–Ω –∂”ô–Ω–µ “±—Å–∞“õ —Å“Ø–π–µ–∫—Ç–µ—Ä—ñ –±–∞—Ä —Ç–∞–±–∞“õ“õ–∞ “±—Å—ã–Ω—ã–ª–∞—Ç—ã–Ω –¥”ô—Å—Ç“Ø—Ä–ª—ñ —Ç–∞“ì–∞–º. –ï—Ç—Ç—ñ “õ–æ–ª–º–µ–Ω –∂–µ–π–¥—ñ, –∫”©–±—ñ–Ω–µ—Å–µ —Å–æ—Ä–ø–∞–º–µ–Ω –±—ñ—Ä–≥–µ.\n–ö–∞–∑—ã–±–µ–∫: –ë“±–ª –∞—à—ã—Ç—ã–ª“ì–∞–Ω —Å“Ø—Ç—Ç–µ–Ω –∂–∞—Å–∞–ª“ì–∞–Ω –¥”ô—Å—Ç“Ø—Ä–ª—ñ “õ–∞–∑–∞“õ —ñ—Ä—ñ–º—à—ñ–≥—ñ. –û–ª –∂“±–º—Å–∞“õ –∂”ô–Ω–µ —Å”ô–ª “õ—ã—à“õ—ã–ª –¥”ô–º—ñ–º–µ–Ω —Ç–∞–Ω—ã–º–∞–ª. –û–Ω—ã ”©–∑–¥—ñ–≥—ñ–Ω–µ–Ω –∂–µ—É–≥–µ –Ω–µ–º–µ—Å–µ –Ω–∞–Ω“ì–∞ –Ω–µ–º–µ—Å–µ —Å–æ—Ä–ø–∞“ì–∞ –∂–∞—é“ì–∞ –±–æ–ª–∞–¥—ã.\n–ë–∞—É—ã—Ä—Å–∞“õ: –ë“±–ª –µ—Ç –ø–µ–Ω –ø–∏—è–∑–±–µ–Ω —Ç–æ–ª—Ç—ã—Ä—ã–ª“ì–∞–Ω –¥”ô–º–¥—ñ, –¥”©“£–≥–µ–ª–µ–∫ –Ω–∞–Ω. –ë“±–ª –∫”©–ø—Ç–µ–≥–µ–Ω “õ–∞–∑–∞“õ —Ç–∞“ì–∞–º–¥–∞—Ä—ã–Ω—ã“£ –Ω–µ–≥—ñ–∑–≥—ñ —Ç–∞“ì–∞–º—ã.\n“ö—É—ã—Ä—ã–ª“ì–∞–Ω —Ç–∞—É—ã“õ: –ë“±–ª “õ—É—ã—Ä—ã–ª“ì–∞–Ω —Ç–∞—É—ã“õ –µ—Ç—ñ, –±—ñ—Ä–∞“õ –æ–Ω—ã –∂–∞—Å–∞—É —Ç”ô—Å—ñ–ª—ñ –µ—Ä–µ–∫—à–µ. –¢–∞—É—ã“õ –µ—Ç—ñ–Ω –¥”ô–º–¥–µ—É—ñ—à—Ç–µ—Ä –º–µ–Ω –π–æ–≥—É—Ä—Ç “õ–æ—Å–ø–∞—Å—ã–Ω–¥–∞ –º–∞—Ä–∏–Ω–∞–¥—Ç–∞–π–¥—ã, —Å–æ–¥–∞–Ω –∫–µ–π—ñ–Ω “õ—ã—Ç—ã—Ä–ª–∞“õ –±–æ–ª“ì–∞–Ω—à–∞ “õ—É—ã—Ä–∞–¥—ã.\n–®–∞—à–ª—ã–∫: –ë“±–ª –∫”ô—É–∞–ø“õ–∞ “±“õ—Å–∞–π–¥—ã –∂”ô–Ω–µ –±“Ø–∫—ñ–ª ”ô–ª–µ–º–¥–µ —Ç–∞–Ω—ã–º–∞–ª. –®–∞—à–ª—ã–∫ ”ô–¥–µ—Ç—Ç–µ —Ç–∞—É—ã“õ –µ—Ç—ñ, “õ–æ–π –µ—Ç—ñ –Ω–µ–º–µ—Å–µ —Å–∏—ã—Ä –µ—Ç—ñ —Å–∏—è“õ—Ç—ã –µ—Ç –∫–µ—Å–µ–∫—Ç–µ—Ä—ñ–Ω–µ–Ω –¥–∞–π—ã–Ω–¥–∞–ª–∞–¥—ã –∂”ô–Ω–µ –∞—à—ã“õ –æ—Ç—Ç–∞ –≥—Ä–∏–ª—å–¥–µ –ø—ñ—Å—ñ—Ä—ñ–ª–µ–¥—ñ.\n–ë–æ—Ä—â: –ë“±–ª “õ—ã—Ä—ã“õ“õ–∞–±–∞—Ç, —Å”ô–±—ñ–∑, –∫–∞—Ä—Ç–æ–ø –∂”ô–Ω–µ –µ—Ç “õ–æ—Å—ã–ª“ì–∞–Ω “õ—ã–∑—ã–ª—à–∞ —Å–æ—Ä–ø–∞—Å—ã. –ë“±–ª —Å—É—ã“õ –∞–π–ª–∞—Ä–¥–∞ –∂–∏—ñ “±—Å—ã–Ω—ã–ª–∞—Ç—ã–Ω –¥”ô–º–¥—ñ, –∂—ã–ª—ã—Ç–∞—Ç—ã–Ω —Å–æ—Ä–ø–∞.\n–ö”ô—Å—Ç—Ä”©–ª: –ë“±–ª –µ—Ç (”ô–¥–µ—Ç—Ç–µ “õ–æ–π –Ω–µ–º–µ—Å–µ —Å–∏—ã—Ä –µ—Ç—ñ), –∫–∞—Ä—Ç–æ–ø, –ø–∏—è–∑ –∂”ô–Ω–µ –±–∞—Å“õ–∞ –¥–∞ –∫”©–∫”©–Ω—ñ—Å—Ç–µ—Ä–¥–µ–Ω –∂–∞—Å–∞–ª“ì–∞–Ω –±“±“õ—Ç—ã—Ä—ã–ª“ì–∞–Ω —Ç–∞“ì–∞–º. –û–ª ”ô–¥–µ—Ç—Ç–µ –Ω–∞–Ω–º–µ–Ω –±—ñ—Ä–≥–µ –±–µ—Ä—ñ–ª–µ–¥—ñ.\n–ñ–∞–ª-–∂–∞—è: –ë“±–ª “õ–æ–π –µ—Ç—ñ–Ω–µ–Ω, –∫–∞—Ä—Ç–æ–ø—Ç–∞–Ω, –ø–∏—è–∑–¥–∞–Ω –∂”ô–Ω–µ –¥”ô–º–¥–µ—É—ñ—à—Ç–µ—Ä–¥–µ–Ω –∂–∞—Å–∞–ª“ì–∞–Ω –±“±“õ—Ç—ã—Ä—ã–ª“ì–∞–Ω —Ç–∞“ì–∞–º. –û–ª ”ô–¥–µ—Ç—Ç–µ –±—É“ì–∞ –ø—ñ—Å—ñ—Ä—ñ–ª–≥–µ–Ω –∫“Ø—Ä—ñ—à–ø–µ–Ω –±—ñ—Ä–≥–µ –±–µ—Ä—ñ–ª–µ–¥—ñ.\n“ö—É—ã—Ä—ã–ª“ì–∞–Ω –∫–µ—Å–ø–µ: –ë“±–ª –∫–µ—Å–ø–µ, –µ—Ç –∂”ô–Ω–µ –∫”©–∫”©–Ω—ñ—Å—Ç–µ—Ä–¥–µ–Ω –∂–∞—Å–∞–ª“ì–∞–Ω —Å–æ—Ä–ø–∞. –û–ª ”ô–¥–µ—Ç—Ç–µ –Ω–∞–Ω–º–µ–Ω –±—ñ—Ä–≥–µ –±–µ—Ä—ñ–ª–µ–¥—ñ.\n–ë–∞–ª —à—ã—Ä—ã–Ω—ã: –ë“±–ª –±–∞–ª –º–µ–Ω —Å“Ø—Ç—Ç–µ–Ω –∂–∞—Å–∞–ª“ì–∞–Ω —Ç”ô—Ç—Ç—ñ —Å—É—Å—ã–Ω. –ë“±–ª —Å–µ—Ä–≥—ñ—Ç–µ—Ç—ñ–Ω –∂”ô–Ω–µ –ø–∞–π–¥–∞–ª—ã\nModel Architecture\nSherkala builds upon the Llama-3.1-8B architecture‚Äîa causal, decoder-only transformer model that employs RoPE positional encoding and grouped-query attention. To better capture the rich morphological features of Kazakh, we extend the base vocabulary by 25% with high-frequency Kazakh tokens. This expansion reduces tokenization fertility (i.e., the average number of subwords per word) and improves both training and inference efficiency.\nTraining Data\nSherkala is continually pre-trained on 45.3 billion tokens from a diverse range of sources covering Kazakh and English with the addition of Russian and Turkish to enable better performance in Kazakh via cross-lingual transfer of capabilities.\nPretraining data is preprocessed using standard techniques including language-specific standardization, filtering, cleaning, and deduplication using locality-sensitive hashing.\nTo enable robust instruction following and safe dialog generation, Sherkala is fine-tuned on a large-scale multilingual instruction dataset comprising of Kazakh, English, and Russian prompt-response examples.\nThe instruction dataset includes a wide coverage of general tasks and capabilities in all 3 languages.\nA dedicated safety dataset‚Äîcreated using a mix of direct and adversarial prompts‚Äîis incorporated to mitigate harmful or biased outputs and to ensure cultural alignment.\nMore information can be found in the Sherkala Technical Report.\nTraining Details\nTraining Hyperparameters\nLearning rate: 1.5e-4\nBatch size: 4 million tokens\nOptimizer: AdamW (Œ≤1 = 0.9, Œ≤2 = 0.95, Œµ = 1e-5)\nWeight decay: 0.1\nGradient norm clipping: 1.0\nLearning rate schedule:\nLinear warm-up (110 steps)\n10√ó cosine decay until 11,433 steps\nTraining Infrastructure\nTraining process was performed on the Cerebras Condor Galaxy 2 (CG-2) AI supercomputer\nTraining executed on 16 Cerebras CS-2 systems\nParallelism: Pure data parallelism across multiple CS-2 systems.\nEvaluation\nSherkala has been extensively evaluated across downstream tasks, open-ended generation, and safety metrics. The following sections detail the evaluation results.\nDownstream Evaluation\nSherkala is benchmarked on multiple tasks in Kazakh, Russian, and English using lm-evaluation-harness in zero-shot setting. The evaluation criteria spanned various dimensions, including:\nKnowledge: How well the model answers factual questions.\nReasoning: The model's ability to answer questions requiring reasoning.\nMisinformation/Bias: Assessment of the model's susceptibility to generating false or misleading information, and its neutrality.\nKazakh Benchmark Results\nModel\nAVG\nKazMMLU\nMMLU\nBelebele\nHS\nPIQA\nBoolQA\nSIQA\nARC\nOBQA\nNIS\nCOPA\nT-QA\nCS-Pairs\nBLOOM (7.1B)\n37.6\n29.3\n27.9\n26.4\n29.9\n52.0\n62.1\n36.7\n23.6\n33.6\n22.0\n47.2\n49.2\n49.1\nBLOOMZ (7.1B)\n36.9\n29.2\n27.8\n22.1\n30.4\n50.8\n54.4\n36.8\n24.4\n31.0\n23.0\n51.8\n48.1\n50.1\nGemma-2 (9B)\n35.7\n26.1\n27.5\n26.0\n28.3\n51.9\n62.0\n33.5\n23.6\n28.4\n17.0\n45.2\n47.1\n47.5\nGemma-2-it (9B)\n36.9\n31.4\n28.4\n23.8\n27.9\n51.0\n63.5\n36.0\n24.0\n30.6\n22.0\n48.8\n49.3\n42.6\nQwen-2.5 (7B)\n38.5\n35.1\n31.3\n26.3\n31.2\n53.4\n54.8\n38.0\n27.1\n30.2\n36.0\n46.0\n48.0\n42.6\nQwen-2.5-Instruct (7B)\n40.8\n37.8\n33.2\n31.1\n31.5\n52.3\n60.9\n38.1\n27.8\n31.6\n38.0\n47.2\n51.0\n49.3\nLLama3.1 (8B)\n39.8\n38.3\n31.3\n25.9\n37.8\n57.2\n63.7\n38.1\n29.6\n32.8\n20.0\n47.8\n51.3\n43.9\nLLama3.1-Instruct (8B)\n40.4\n38.9\n32.4\n27.0\n37.5\n57.5\n67.5\n37.9\n30.3\n32.6\n22.0\n48.2\n49.7\n43.2\nLLama3.1-KazLLM-1.0 (8B)\n43.7\n37.0\n31.5\n27.8\n46.0\n62.8\n69.8\n44.7\n35.5\n34.2\n32.0\n50.4\n50.9\n45.0\nIrbis-7b-v0.1 (7B)\n37.7\n29.5\n27.8\n26.1\n31.3\n53.9\n52.4\n37.8\n24.8\n30.0\n25.0\n54.4\n46.6\n50.9\nmGPT-13B (13B)\n37.7\n28.5\n26.7\n27.9\n31.4\n54.6\n56.4\n38.5\n24.0\n32.0\n23.0\n49.4\n47.9\n49.8\nSherkala (Ours)\n45.7\n51.6\n37.7\n25.9\n53.1\n68.1\n66.9\n42.2\n38.1\n37.0\n18.0\n51.0\n50.3\n54.3\nSherkala-chat (Ours-chat)\n47.6\n41.4\n34.6\n30.6\n55.2\n65.9\n75.8\n48.1\n42.9\n37.4\n28.0\n53.2\n52.5\n53.3\nThe average score (AVG) represents the mean performance across all tasks, with higher values indicating better results across all metrics. The abbreviations \"HS,\" \"ARC,\" \"OBQA,\" \"NIS,\" \"T-QA,\" and \"CS-Pairs\" correspond to HellaSwag,\nARC-Challenge (Easy), OpenBookQA, NIS-Math and TruthfulQA, and CrowS-Pairs respectively.\nEnglish Benchmark Results\nModel\nAVG\nMMLU\nRACE\nHS\nPIQA\nBoolQA\nSIQA\nARC\nOBQA\nT-QA\nCrowS-Pairs\nBLOOM (7.1B)\n48.5\n29.1\n36.5\n59.6\n73.6\n62.2\n46.5\n33.4\n35.8\n38.9\n68.9\nBLOOMZ (7.1B)\n57.0\n36.7\n45.6\n63.1\n77.4\n90.7\n59.7\n43.6\n42.0\n45.2\n65.6\nGemma-2 (9B)\n39.4\n27.4\n27.8\n33.2\n59.1\n62.2\n37.6\n24.2\n26.4\n46.4\n49.3\nGemma-2-it (9B)\n53.2\n37.7\n46.7\n65.4\n69.5\n80.1\n44.1\n40.7\n29.6\n62.1\n56.5\nQwen-2.5 (7B)\n60.8\n44.0\n41.4\n78.9\n79.9\n84.5\n51.9\n51.4\n47.2\n56.4\n71.9\nQwen-2.5-Instruct (7B)\n62.1\n46.7\n46.3\n80.5\n80.3\n86.4\n48.7\n54.9\n48.8\n64.8\n63.2\nLLama3.1 (8B)\n56.6\n39.6\n38.9\n79.0\n81.3\n65.3\n52.6\n53.5\n45.0\n45.2\n65.5\nLLama3.1-Instruct (8B)\n60.1\n41.7\n44.9\n79.2\n81.0\n79.4\n52.7\n55.0\n43.6\n54.0\n69.0\nLLama3.1-KazLLM-1.0 (8B)\n58.6\n39.7\n44.3\n77.9\n80.8\n72.8\n51.5\n54.6\n43.0\n51.0\n70.0\nSherkala (Ours)\n58.7\n46.8\n39.2\n78.3\n80.5\n77.2\n51.3\n52.1\n46.0\n49.6\n65.9\nSherkala-chat (Ours-chat)\n59.1\n40.5\n41.6\n78.1\n79.1\n84.8\n58.0\n52.6\n42.6\n51.3\n62.2\nAverage here represents the mean score across tasks. Higher scores are better across all metrics. ‚ÄúHS‚Äù, ‚ÄúARC‚Äù, ‚ÄúOBQA‚Äù, ‚ÄúT-QA‚Äù and \"CS-Pairs\" denote HellaSwag, ARC-Challenge (Easy), OpenBookQA, TruthfulQA, and CrowS-Pairs respectively. Further details on the evaluation, including additional results in Russian, can be found in the Sherkala Technical Report.\nGeneration Evaluation\nWe further evaluated open-ended text generation using GPT-4 as a judge. The following table shows average generation scores (with standard deviations) for models on the MT and Vicuna benchmarks across Kazakh, Russian, and English:\nModel\nKazakh MT (avg ¬± sd)\nKazakh Vicuna (avg ¬± sd)\nRussian MT (avg ¬± sd)\nRussian Vicuna (avg ¬± sd)\nEnglish MT (avg ¬± sd)\nEnglish Vicuna (avg ¬± sd)\nGPT-4o\n8.81 ¬± 1.51\n9.32 ¬± 0.61\n8.89 ¬± 1.59\n9.79 ¬± 0.41\n8.36 ¬± 1.35\n9.03 ¬± 0.59\nQwen-2.5-7B-Instruct\n3.52 ¬± 3.52\n3.23 ¬± 1.73\n5.81 ¬± 2.36\n6.05 ¬± 3.07\n7.40 ¬± 1.85\n8.06 ¬± 1.22\nLlama-3.1-8B-Instruct\n3.76 ¬± 2.11\n3.75 ¬± 1.91\n0.85 ¬± 1.20\n0.82 ¬± 1.55\n6.55 ¬± 2.03\n7.41 ¬± 1.28\nKazLLM-1.0-8B\n3.98 ¬± 2.15\n4.88 ¬± 2.01\n0.72 ¬± 1.06\n0.28 ¬± 0.71\n6.00 ¬± 2.15\n6.66 ¬± 1.24\nSherkala-chat\n5.99 ¬± 2.73\n7.39 ¬± 1.89\n1.02 ¬± 1.41\n0.97 ¬± 1.70\n5.78 ¬± 2.43\n6.55 ¬± 1.59\nIntended Use\nWe release Sherkala under:\nMeta‚Äôs Llama 3.1 Community license, and users must adhere to the terms and conditions of the license, Meta‚Äôs acceptable use policy, Meta‚Äôs privacy policy, and the applicable policies, laws, and regulations governing the specific use-case and region; and\nThe CC BY-NC-SA 4.0 license and users must adhere to the terms and conditions of the license\nSherkala is intended for research in Kazakh NLP, including:\nChat Assistants: Conversational agents tailored for Kazakh speakers.\nQuestion Answering & Content Generation: Systems that deliver culturally aligned, factual, and contextually rich responses.\nMultilingual NLP: Applications that support English, Russian, and Turkish alongside Kazakh.\nWe believe that a number of audiences will benefit from our model:\nAcademics: Those researching Kazakh natural language processing.\nBusinesses: Companies targeting Kazakh-speaking audiences.\nDevelopers: Those integrating Kazakh language capabilities in apps.\nOut-of-Scope Use\nWhile Sherkala  is a powerful language model catering to Kazakh and English it is essential to understand its limitations and the potential for its misuse.\nSherkala is not recommended for:\nCommercial use: Sherkala shall not be used for any commercial purposes, comprising anything that is primarily intended to derive commercial advantage or monetary compensation.\nMalicious Use: The model should not be used for generating harmful, misleading, or inappropriate content. This includes but is not limited to\nGenerating or promoting hate speech, violence, or discrimination,\nSpreading misinformation or fake news,\nEngaging in illegal activities or promoting them,\nHandling sensitive information: the model should not be used to handle or to generate personal, confidential, or sensitive information.\nGeneralization Across All Languages: Sherkala is optimized only for Kazakh and English. It should not be assumed to have equal proficiency in other languages or dialects.\nHigh-Stakes Decisions: The model should not be used for making high-stakes decisions without human oversight. This includes medical, legal, financial, or safety-critical decisions, among others.\nBias, Risks, and Limitations\nAlthough extensive measures have been taken to mitigate biases and ensure safe outputs, Sherkala‚Äîlike all large language models‚Äîmay still produce inaccurate, misleading, or biased content. Users should apply additional safety measures and conduct thorough evaluations when deploying the model in sensitive or high-stakes environments.\nBy using Sherkala, you acknowledge and accept that, as with any large language model, it may generate incorrect, misleading and/or offensive information or content. The information is not intended as advice and should not be relied upon in any way, nor are we responsible for any of the content or consequences resulting from its use. We are continuously working to develop models with greater capabilities, and as such, welcome any feedback on the model.\nCopyright Inception Institute of Artificial Intelligence Ltd. Sherkala is made available under the license CC-BY-NC-SA-4.0. You shall not use Sherkala except in compliance with the License. You may obtain a copy of the License at https://creativecommons.org/licenses/by-nc-sa/4.0/.\nUnless required by applicable law or agreed to in writing, Sherkala is distributed on an AS IS basis, without warranties or conditions of any kind, either express or implied. Please see the terms of the License for the specific language permissions and limitations under the License.\nNote: The model files have been updated to the latest version as of February 20, 2025.\nCitation info\n@misc{koto2025sherkala,\ntitle={Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh},\nauthor={Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokul Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Larry Murray, Preslav Nakov},\nyear={2025},\neprint={2503.01493},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "vngrs-ai/Kumru-2B-Base": "Model Information\nUse\nEvaluation Results\nTokenizer Efficiency\nCitation\nModel Information\nKumru-2B is the lightweight, open-source version of Kumru LLM, developed for Turkish from scratch by VNGRS.\nIt is pre-trained on a cleaned, deduplicated corpora of 500 GB for 300B tokens, and supervised fine-tuned on 1M examples.\nIt comes with a modern tokenizer developed for Turkish, supporting code, math and chat template.\nKumru has a native context length of 8,192 tokens by default.\nThis is the Base version, which is only pre-trained, not instruct fine-tuned.\nInstruct fine-tuned version is here\nTry to demo of 7B version here.\nUse\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"vngrs-ai/Kumru-2B-Base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\ndef complete_text(starting_text):\ntokenized_text = tokenizer.encode_plus(starting_text, return_tensors = 'pt', return_token_type_ids=False,\nmax_length = model.config.max_position_embeddings, truncation = True).to(model.device)\ngenerated_tokens = model.generate(**tokenized_text,\nmax_length = 512,\nrepetition_penalty = 1.15,\nno_repeat_ngram_size = 5\n)\ngenerated_text = tokenizer.decode(generated_tokens.cpu().detach().numpy().reshape(-1).tolist(), skip_special_tokens=True)\nreturn generated_text\nstarting_text = \"Efes antik kenti\"\ngenerated_text = complete_text(starting_text)\nprint(generated_text)\nEvaluation Results\nBoth Kumru-7B and Kumru-2B are evaluated on Cetvel benchmark.\nWe observe that Kumru overall surpasses significantly larger models such as LLaMA-3.3‚Äì70B, Gemma-3‚Äì27B, Qwen-2‚Äì72B and Aya-32B. It excels at tasks related to the nuances of the Turkish language, such as grammatical error correction and text summarization.\nTokenizer Efficiency\nKumru tokenizer is a modern BPE tokenizer with a vocabulary size of 50,176, pre-tokenization regex and a chat template.\nOther open-source models spend between 38% to 98% more tokens than Kumru while still having larger vocabulary sizes.\nThis means Kumru can represent more texts in its context length and process faster and cheaper. Although the native context length of Kumru is 8,192, its effective context length can be considered between 1128 and 1618, compared to other multilingual models out there.\nThis shows the efficiency of having a native Turkish tokenizer in terms of representation power, speed and cost.\nCitation\n@misc{turker2025kumru,\ntitle={Kumru},\nauthor={Turker, Meliksah and Ari, Erdi and Han, Aydin},\nyear={2025},\nurl={https://huggingface.co/vngrs-ai/Kumru-2B-Base}\n}",
    "Delta-Vector/Archaeo-12B": "__~a~_\n~~;  ~_\n_                ~  ~_                _\n'_\\;__._._._._._._]   ~_._._._._._.__;/_`\n'(/'/'/'/'|'|'|'| (    )|'|'|'|'\\'\\'\\'\\)'\n(/ / / /, | | | |(/    \\) | | | ,\\ \\ \\ \\)\n(/ / / / / | | | ~(/    \\) ~ | | \\ \\ \\ \\ \\)\n(/ / / / /  ~ ~ ~   (/  \\)    ~ ~  \\ \\ \\ \\ \\)\n(/ / / / ~          / (||)|          ~ \\ \\ \\ \\)\n~ / / ~            M  /||\\M             ~ \\ \\ ~\n~ ~                  /||\\                 ~ ~\n//||\\\\\n//||\\\\\n//||\\\\\n'/||\\'        \"Archaeopteryx\"\nA series of Merges made for Roleplaying & Creative Writing, This model uses Rei-12B and Francois-Huali-12B and Slerp to merge the 2 models.\nChatML formatting\n\"\"\"<|im_start|>system\nsystem prompt<|im_end|>\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n<|im_start|>assistant\n\"\"\"\nMergeKit Configuration\nmodels:\n- model: Delta-Vector/Francois-Huali-12B\n- model: Delta-Vector/Rei-12B\nmerge_method: slerp\nbase_model: Delta-Vector/Rei-12B\nparameters:\nt:\n- value: 0.2\ndtype: bfloat16\ntokenizer_source: base\nQuants:\nGGUF\nhttps://huggingface.co/Delta-Vector/Archaeo-12B-GGUF/\nEXL2\nhttps://huggingface.co/Delta-Vector/Archaeo-12B-EXL2/\nCredits\nThank you to: Kubernetes-bad, LucyKnada, Intervitens & The rest of Anthracite"
}