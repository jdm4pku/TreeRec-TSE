{
    "Helsinki-NLP/opus-mt-en-mkh": "Benchmarks\nSystem Info:\neng-mkh\nsource group: English\ntarget group: Mon-Khmer languages\nOPUS readme: eng-mkh\nmodel: transformer\nsource language(s): eng\ntarget language(s): kha khm khm_Latn mnw vie vie_Hani\nmodel: transformer\npre-processing: normalization + SentencePiece (spm32k,spm32k)\na sentence initial language token is required in the form of >>id<< (id = valid target language ID)\ndownload original weights: opus-2020-07-27.zip\ntest set translations: opus-2020-07-27.test.txt\ntest set scores: opus-2020-07-27.eval.txt\nBenchmarks\ntestset\nBLEU\nchr-F\nTatoeba-test.eng-kha.eng.kha\n0.1\n0.015\nTatoeba-test.eng-khm.eng.khm\n0.2\n0.226\nTatoeba-test.eng-mnw.eng.mnw\n0.7\n0.003\nTatoeba-test.eng.multi\n16.5\n0.330\nTatoeba-test.eng-vie.eng.vie\n33.7\n0.513\nSystem Info:\nhf_name: eng-mkh\nsource_languages: eng\ntarget_languages: mkh\nopus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-mkh/README.md\noriginal_repo: Tatoeba-Challenge\ntags: ['translation']\nlanguages: ['en', 'vi', 'km', 'mkh']\nsrc_constituents: {'eng'}\ntgt_constituents: {'vie_Hani', 'mnw', 'vie', 'kha', 'khm_Latn', 'khm'}\nsrc_multilingual: False\ntgt_multilingual: True\nprepro:  normalization + SentencePiece (spm32k,spm32k)\nurl_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-mkh/opus-2020-07-27.zip\nurl_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-mkh/opus-2020-07-27.test.txt\nsrc_alpha3: eng\ntgt_alpha3: mkh\nshort_pair: en-mkh\nchrF2_score: 0.33\nbleu: 16.5\nbrevity_penalty: 1.0\nref_len: 34734.0\nsrc_name: English\ntgt_name: Mon-Khmer languages\ntrain_date: 2020-07-27\nsrc_alpha2: en\ntgt_alpha2: mkh\nprefer_old: False\nlong_pair: eng-mkh\nhelsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\ntransformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\nport_machine: brutasse\nport_time: 2020-08-21-14:41",
    "HooshvareLab/bert-fa-base-uncased": "ParsBERT (v2.0)\nIntroduction\nIntended uses & limitations\nHow to use\nTraining\nGoals\nDerivative models\nBase Config\nEval results\nSentiment Analysis (SA) Task\nText Classification (TC) Task\nNamed Entity Recognition (NER) Task\nBibTeX entry and citation info\nQuestions?\nParsBERT (v2.0)\nA Transformer-based Model for Persian Language Understanding\nWe reconstructed the vocabulary and fine-tuned the ParsBERT v1.1 on the new Persian corpora in order to provide some functionalities for using ParsBERT in other scopes!\nPlease follow the ParsBERT repo for the latest information about previous and current models.\nIntroduction\nParsBERT is a monolingual language model based on Google‚Äôs BERT architecture. This model is pre-trained on large Persian corpora with various writing styles from numerous subjects (e.g., scientific, novels, news) with more than 3.9M documents, 73M sentences, and 1.3B words.\nPaper presenting ParsBERT: arXiv:2005.12515\nIntended uses & limitations\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nTensorFlow 2.0\nfrom transformers import AutoConfig, AutoTokenizer, TFAutoModel\nconfig = AutoConfig.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\ntokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\nmodel = TFAutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\ntext = \"ŸÖÿß ÿØÿ± ŸáŸàÿ¥Ÿàÿßÿ±Ÿá ŸÖÿπÿ™ŸÇÿØ€åŸÖ ÿ®ÿß ÿßŸÜÿ™ŸÇÿßŸÑ ÿµÿ≠€åÿ≠ ÿØÿßŸÜÿ¥ Ÿà ÿ¢⁄ØÿßŸá€åÿå ŸáŸÖŸá ÿßŸÅÿ±ÿßÿØ ŸÖ€åÿ™ŸàÿßŸÜŸÜÿØ ÿßÿ≤ ÿßÿ®ÿ≤ÿßÿ±Ÿáÿß€å ŸáŸàÿ¥ŸÖŸÜÿØ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜŸÜÿØ. ÿ¥ÿπÿßÿ± ŸÖÿß ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ÿ®ÿ±ÿß€å ŸáŸÖŸá ÿßÿ≥ÿ™.\"\ntokenizer.tokenize(text)\n>>> ['ŸÖÿß', 'ÿØÿ±', 'ŸáŸàÿ¥', '##Ÿàÿßÿ±Ÿá', 'ŸÖÿπÿ™ŸÇÿØ€åŸÖ', 'ÿ®ÿß', 'ÿßŸÜÿ™ŸÇÿßŸÑ', 'ÿµÿ≠€åÿ≠', 'ÿØÿßŸÜÿ¥', 'Ÿà', 'ÿß⁄ØÿßŸá€å', 'ÿå', 'ŸáŸÖŸá', 'ÿßŸÅÿ±ÿßÿØ', 'ŸÖ€åÿ™ŸàÿßŸÜŸÜÿØ', 'ÿßÿ≤', 'ÿßÿ®ÿ≤ÿßÿ±Ÿáÿß€å', 'ŸáŸàÿ¥ŸÖŸÜÿØ', 'ÿßÿ≥ÿ™ŸÅÿßÿØŸá', '⁄©ŸÜŸÜÿØ', '.', 'ÿ¥ÿπÿßÿ±', 'ŸÖÿß', 'ŸáŸàÿ¥', 'ŸÖÿµŸÜŸàÿπ€å', 'ÿ®ÿ±ÿß€å', 'ŸáŸÖŸá', 'ÿßÿ≥ÿ™', '.']\nPytorch\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel\nconfig = AutoConfig.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\ntokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\nmodel = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\nTraining\nParsBERT trained on a massive amount of public corpora (Persian Wikidumps, MirasText) and six other manually crawled text data from a various type of websites (BigBang Page scientific, Chetor lifestyle, Eligasht itinerary,  Digikala digital magazine, Ted Talks general conversational, Books novels, storybooks, short stories from old to the contemporary era).\nAs a part of ParsBERT methodology, an extensive pre-processing combining POS tagging and WordPiece segmentation was carried out to bring the corpora into a proper format.\nGoals\nObjective goals during training are as below (after 300k steps).\n***** Eval results *****\nglobal_step = 300000\nloss = 1.4392426\nmasked_lm_accuracy = 0.6865794\nmasked_lm_loss = 1.4469004\nnext_sentence_accuracy = 1.0\nnext_sentence_loss = 6.534152e-05\nDerivative models\nBase Config\nParsBERT v2.0 Model\nHooshvareLab/bert-fa-base-uncased\nParsBERT v2.0 Sentiment Analysis\nHooshvareLab/bert-fa-base-uncased-sentiment-digikala\nHooshvareLab/bert-fa-base-uncased-sentiment-snappfood\nHooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-binary\nHooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-multi\nParsBERT v2.0 Text Classification\nHooshvareLab/bert-fa-base-uncased-clf-digimag\nHooshvareLab/bert-fa-base-uncased-clf-persiannews\nParsBERT v2.0 NER\nHooshvareLab/bert-fa-base-uncased-ner-peyma\nHooshvareLab/bert-fa-base-uncased-ner-arman\nEval results\nParsBERT is evaluated on three NLP downstream tasks: Sentiment Analysis (SA), Text Classification, and Named Entity Recognition (NER). For this matter and due to insufficient resources, two large datasets for SA and two for text classification were manually composed, which are available for public use and benchmarking. ParsBERT outperformed all other language models, including multilingual BERT and other hybrid deep learning models for all tasks, improving the state-of-the-art performance in Persian language modeling.\nSentiment Analysis (SA) Task\nDataset\nParsBERT v2\nParsBERT v1\nmBERT\nDeepSentiPers\nDigikala User Comments\n81.72\n81.74*\n80.74\n-\nSnappFood User Comments\n87.98\n88.12*\n87.87\n-\nSentiPers (Multi Class)\n71.31*\n71.11\n-\n69.33\nSentiPers (Binary Class)\n92.42*\n92.13\n-\n91.98\nText Classification (TC) Task\nDataset\nParsBERT v2\nParsBERT v1\nmBERT\nDigikala Magazine\n93.65*\n93.59\n90.72\nPersian News\n97.44*\n97.19\n95.79\nNamed Entity Recognition (NER) Task\nDataset\nParsBERT v2\nParsBERT v1\nmBERT\nMorphoBERT\nBeheshti-NER\nLSTM-CRF\nRule-Based CRF\nBiLSTM-CRF\nPEYMA\n93.40*\n93.10\n86.64\n-\n90.59\n-\n84.00\n-\nARMAN\n99.84*\n98.79\n95.89\n89.9\n84.03\n86.55\n-\n77.45\nBibTeX entry and citation info\nPlease cite in publications as the following:\n@article{ParsBERT,\ntitle={ParsBERT: Transformer-based Model for Persian Language Understanding},\nauthor={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},\njournal={ArXiv},\nyear={2020},\nvolume={abs/2005.12515}\n}\nQuestions?\nPost a Github issue on the ParsBERT Issues repo.",
    "allenai/longformer-base-4096": "longformer-base-4096\nCiting\nlongformer-base-4096\nLongformer is a transformer model for long documents.\nlongformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096.\nLongformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.\nPlease refer to the examples in modeling_longformer.py and the paper for more details on how to set global attention.\nCiting\nIf you use Longformer in your research, please cite Longformer: The Long-Document Transformer.\n@article{Beltagy2020Longformer,\ntitle={Longformer: The Long-Document Transformer},\nauthor={Iz Beltagy and Matthew E. Peters and Arman Cohan},\njournal={arXiv:2004.05150},\nyear={2020},\n}\nLongformer is an open-source project developed by the Allen Institute for Artificial Intelligence (AI2).\nAI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.",
    "allenai/scibert_scivocab_uncased": "SciBERT\nSciBERT\nThis is the pretrained model presented in SciBERT: A Pretrained Language Model for Scientific Text, which is a BERT model trained on scientific text.\nThe training corpus was papers taken from Semantic Scholar. Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.\nSciBERT has its own wordpiece vocabulary (scivocab) that's built to best match the training corpus. We trained cased and uncased versions.\nAvailable models include:\nscibert_scivocab_cased\nscibert_scivocab_uncased\nThe original repo can be found here.\nIf using these models, please cite the following paper:\n@inproceedings{beltagy-etal-2019-scibert,\ntitle = \"SciBERT: A Pretrained Language Model for Scientific Text\",\nauthor = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\nbooktitle = \"EMNLP\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/D19-1371\"\n}",
    "arpanghoshal/EmoRoBERTa": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nWhat is GoEmotions\nWhat is RoBERTa\nHyperparameters\nResults\nUsage\nBy\nlinkedin.com/in/rohanrkamath\nlinkedin.com/in/arpanghoshal\nWhat is GoEmotions\nDataset labelled 58000 Reddit comments with 28 emotions\nadmiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise + neutral\nWhat is RoBERTa\nRoBERTa builds on BERT‚Äôs language masking strategy and modifies key hyperparameters in BERT, including removing BERT‚Äôs next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT.\nHyperparameters\nParameter\nLearning rate\n5e-5\nEpochs\n10\nMax Seq Length\n50\nBatch size\n16\nWarmup Proportion\n0.1\nEpsilon\n1e-8\nResults\nBest Result of Macro F1 - 49.30%\nUsage\nfrom transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\ntokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\nmodel = TFRobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\nemotion = pipeline('sentiment-analysis',\nmodel='arpanghoshal/EmoRoBERTa')\nemotion_labels = emotion(\"Thanks for using it.\")\nprint(emotion_labels)\nOutput\n[{'label': 'gratitude', 'score': 0.9964383244514465}]",
    "deepset/roberta-base-squad2": "roberta-base for Extractive QA\nOverview\nHyperparameters\nUsage\nIn Haystack\nIn Transformers\nPerformance\nAuthors\nAbout us\nGet in touch and join the Haystack community\nroberta-base for Extractive QA\nThis is the roberta-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering.\nWe have also released a distilled version of this model called deepset/tinyroberta-squad2. It has a comparable prediction quality and runs at twice the speed of deepset/roberta-base-squad2.\nOverview\nLanguage model: roberta-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code:  See an example extractive QA pipeline built with HaystackInfrastructure: 4x Tesla v100\nHyperparameters\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = \"roberta-base\"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\nUsage\nIn Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents.\nTo load and run the model with Haystack:\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\ndocs = [\nDocument(content=\"Python is a popular programming language\"),\nDocument(content=\"python ist eine beliebte Programmiersprache\"),\n]\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the corresponding Haystack tutorial.\nIn Transformers\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nmodel_name = \"deepset/roberta-base-squad2\"\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n'question': 'Why is model conversion important?',\n'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nPerformance\nEvaluated on the SQuAD 2.0 dev set with the official eval script.\n\"exact\": 79.87029394424324,\n\"f1\": 82.91251169582613,\n\"total\": 11873,\n\"HasAns_exact\": 77.93522267206478,\n\"HasAns_f1\": 84.02838248389763,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.79983179142137,\n\"NoAns_f1\": 81.79983179142137,\n\"NoAns_total\": 5945\nAuthors\nBranden Chan: branden.chan@deepset.aiTimo M√∂ller: timo.moeller@deepset.aiMalte Pietsch: malte.pietsch@deepset.aiTanay Soni: tanay.soni@deepset.ai\nAbout us\ndeepset is the company behind the production-ready open-source AI framework Haystack.\nSome of our other work:\nDistilled roberta-base-squad2 (aka \"tinyroberta-squad2\")\nGerman BERT, GermanQuAD and GermanDPR, German embedding model\ndeepset Cloud\ndeepset Studio\nGet in touch and join the Haystack community\nFor more info on Haystack, visit our GitHub repo and Documentation.\nWe also have a Discord community open to everyone!\nTwitter | LinkedIn | Discord | GitHub Discussions | Website | YouTube\nBy the way: we're hiring!",
    "emilyalsentzer/Bio_ClinicalBERT": "ClinicalBERT - Bio + Clinical BERT Model\nPretraining Data\nModel Pretraining\nNote Preprocessing\nPretraining Procedures\nPretraining Hyperparameters\nHow to use the model\nMore Information\nQuestions?\nClinicalBERT - Bio + Clinical BERT Model\nThe Publicly Available Clinical BERT Embeddings paper contains four unique clinicalBERT models: initialized with BERT-Base (cased_L-12_H-768_A-12) or BioBERT (BioBERT-Base v1.0 + PubMed 200K + PMC 270K) & trained on either all MIMIC notes or only discharge summaries.\nThis model card describes the Bio+Clinical BERT model, which was initialized from BioBERT & trained on all MIMIC notes.\nPretraining Data\nThe Bio_ClinicalBERT model was trained on all notes from MIMIC III, a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see here. All notes from the NOTEEVENTS table were included (~880M words).\nModel Pretraining\nNote Preprocessing\nEach note in MIMIC was first split into sections using a rules-based section splitter (e.g. discharge summary notes were split into \"History of Present Illness\", \"Family History\", \"Brief Hospital Course\", etc. sections). Then each section was split into sentences using SciSpacy (en core sci md tokenizer).\nPretraining Procedures\nThe model was trained using code from Google's BERT repository on a GeForce GTX TITAN X 12 GB GPU. Model parameters were initialized with BioBERT (BioBERT-Base v1.0 + PubMed 200K + PMC 270K).\nPretraining Hyperparameters\nWe used a batch size of 32, a maximum sequence length of 128, and a learning rate of 5 ¬∑ 10‚àí5 for pre-training our models. The models trained on all MIMIC notes  were trained for 150,000 steps. The dup factor for duplicating input data with different masks was set to 5. All other default parameters were used (specifically, masked language model probability = 0.15\nand max predictions per sequence = 20).\nHow to use the model\nLoad the model via the transformers library:\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nMore Information\nRefer to the original paper, Publicly Available Clinical BERT Embeddings (NAACL Clinical NLP Workshop 2019) for additional details and performance on NLI and NER tasks.\nQuestions?\nPost a Github issue on the clinicalBERT repo or email ealsentzer@stanford.edu with any questions.",
    "facebook/bart-large-cnn": "BART (large-sized model), fine-tuned on CNN Daily Mail\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nBART (large-sized model), fine-tuned on CNN Daily Mail\nBART model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).\nDisclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nBART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\nBART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\nIntended uses & limitations\nYou can use this model for text summarization.\nHow to use\nHere is how to use this model with the pipeline API:\nfrom transformers import pipeline\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1910-13461,\nauthor    = {Mike Lewis and\nYinhan Liu and\nNaman Goyal and\nMarjan Ghazvininejad and\nAbdelrahman Mohamed and\nOmer Levy and\nVeselin Stoyanov and\nLuke Zettlemoyer},\ntitle     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\nGeneration, Translation, and Comprehension},\njournal   = {CoRR},\nvolume    = {abs/1910.13461},\nyear      = {2019},\nurl       = {http://arxiv.org/abs/1910.13461},\neprinttype = {arXiv},\neprint    = {1910.13461},\ntimestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "facebook/deit-base-distilled-patch16-224": "Distilled Data-efficient Image Transformer (base-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nDistilled Data-efficient Image Transformer (base-sized model)\nDistilled data-efficient Image Transformer (DeiT) model pre-trained and fine-tuned on ImageNet-1k (1 million images, 1,000 classes) at resolution 224x224. It was first introduced in the paper Training data-efficient image transformers & distillation through attention by Touvron et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman.\nDisclaimer: The team releasing DeiT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThis model is a distilled Vision Transformer (ViT). It uses a distillation token, besides the class token, to effectively learn from a teacher (CNN) during both pre-training and fine-tuning. The distillation token is learned through backpropagation, by interacting with the class ([CLS]) and patch tokens through the self-attention layers.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nSince this model is a distilled ViT model, you can plug it into DeiTModel, DeiTForImageClassification or DeiTForImageClassificationWithTeacher. Note that the model expects the data to be prepared using DeiTFeatureExtractor. Here we use AutoFeatureExtractor, which will automatically use the appropriate feature extractor given the model name.\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import AutoFeatureExtractor, DeiTForImageClassificationWithTeacher\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-distilled-patch16-224')\nmodel = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-base-distilled-patch16-224')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n# forward pass\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nCurrently, both the feature extractor and model support PyTorch. Tensorflow and JAX/FLAX are coming soon.\nTraining data\nThis model was pretrained and fine-tuned with distillation on ImageNet-1k, a dataset consisting of 1 million images and 1k classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nAt inference time, images are resized/rescaled to the same resolution (256x256), center-cropped at 224x224 and normalized across the RGB channels with the ImageNet mean and standard deviation.\nPretraining\nThe model was trained on a single 8-GPU node for 3 days. Training resolution is 224. For all hyperparameters (such as batch size and learning rate) we refer to table 9 of the original paper.\nEvaluation results\nModel\nImageNet top-1 accuracy\nImageNet top-5 accuracy\n# params\nURL\nDeiT-tiny\n72.2\n91.1\n5M\nhttps://huggingface.co/facebook/deit-tiny-patch16-224\nDeiT-small\n79.9\n95.0\n22M\nhttps://huggingface.co/facebook/deit-small-patch16-224\nDeiT-base\n81.8\n95.6\n86M\nhttps://huggingface.co/facebook/deit-base-patch16-224\nDeiT-tiny distilled\n74.5\n91.9\n6M\nhttps://huggingface.co/facebook/deit-tiny-distilled-patch16-224\nDeiT-small distilled\n81.2\n95.4\n22M\nhttps://huggingface.co/facebook/deit-small-distilled-patch16-224\nDeiT-base distilled\n83.4\n96.5\n87M\nhttps://huggingface.co/facebook/deit-base-distilled-patch16-224\nDeiT-base 384\n82.9\n96.2\n87M\nhttps://huggingface.co/facebook/deit-base-patch16-384\nDeiT-base distilled 384 (1000 epochs)\n85.2\n97.2\n88M\nhttps://huggingface.co/facebook/deit-base-distilled-patch16-384\nNote that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\n@misc{touvron2021training,\ntitle={Training data-efficient image transformers & distillation through attention},\nauthor={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv√© J√©gou},\nyear={2021},\neprint={2012.12877},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@misc{wu2020visual,\ntitle={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\nauthor={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\nyear={2020},\neprint={2006.03677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "facebook/detr-resnet-50": "DETR (End-to-End Object Detection) model with ResNet-50 backbone\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nTraining\nEvaluation results\nBibTeX entry and citation info\nDETR (End-to-End Object Detection) model with ResNet-50 backbone\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\nDisclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\nIntended uses & limitations\nYou can use the raw model for object detection. See the model hub to look for all available DETR models.\nHow to use\nHere is how to use this model:\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# you can specify the revision tag if you don't want the timm dependency\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score > 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\nbox = [round(i, 2) for i in box.tolist()]\nprint(\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\nf\"{round(score.item(), 3)} at location {box}\"\n)\nThis should output:\nDetected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]\nCurrently, both the feature extractor and model support PyTorch.\nTraining data\nThe DETR model was trained on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\nTraining\nThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).\nEvaluation results\nThis model achieves an AP (average precision) of 42.0 on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2005-12872,\nauthor    = {Nicolas Carion and\nFrancisco Massa and\nGabriel Synnaeve and\nNicolas Usunier and\nAlexander Kirillov and\nSergey Zagoruyko},\ntitle     = {End-to-End Object Detection with Transformers},\njournal   = {CoRR},\nvolume    = {abs/2005.12872},\nyear      = {2020},\nurl       = {https://arxiv.org/abs/2005.12872},\narchivePrefix = {arXiv},\neprint    = {2005.12872},\ntimestamp = {Thu, 28 May 2020 17:38:09 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "google/mt5-base": "Abstract\nGoogle's mT5\nmT5 is pretrained on the mC4 corpus, covering 101 languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.\nNote: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\nPretraining Dataset: mC4\nOther Community Checkpoints: here\nPaper: mT5: A massively multilingual pre-trained text-to-text transformer\nAuthors: Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel\nAbstract\nThe recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available.",
    "google/mt5-small": "Abstract\nGoogle's mT5\nmT5 is pretrained on the mC4 corpus, covering 101 languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.\nNote: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\nPretraining Dataset: mC4\nOther Community Checkpoints: here\nPaper: mT5: A massively multilingual pre-trained text-to-text transformer\nAuthors: Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel\nAbstract\nThe recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available.",
    "google/vit-large-patch16-224": "Vision Transformer (large-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nVision Transformer (large-sized model)\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at the same resolution, 224x224.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nCurrently, both the feature extractor and model  support PyTorch. Tensorflow and JAX/FLAX are coming soon, and the API of ViTFeatureExtractor might change.\nTraining data\nThe ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, and fine-tuned on ImageNet, a dataset consisting of 1 million images and 1k classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\nPretraining\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\n@misc{wu2020visual,\ntitle={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\nauthor={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\nyear={2020},\neprint={2006.03677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "hfl/chinese-macbert-base": "Please use 'Bert' related functions to load this model!\nIntroduction\nCitation\nPlease use 'Bert' related functions to load this model!\nThis repository contains the resources in our paper \"Revisiting Pre-trained Models for Chinese Natural Language Processing\", which will be published in \"Findings of EMNLP\". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.\nRevisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu\nYou may also interested in,\nChinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm\nChinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\nChinese XLNet: https://github.com/ymcui/Chinese-XLNet\nKnowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/TextBrewer\nMore resources by HFL: https://github.com/ymcui/HFL-Anthology\nIntroduction\nMacBERT is an improved BERT with novel MLM as correction pre-training task, which mitigates the discrepancy of pre-training and fine-tuning.\nInstead of masking with [MASK] token, which never appears in the Ô¨Åne-tuning stage, we propose to use similar words for the masking purpose. A similar word is obtained by using Synonyms toolkit (Wang and Hu, 2017), which is based on word2vec (Mikolov et al., 2013) similarity calculations. If an N-gram is selected to mask, we will Ô¨Ånd similar words individually. In rare cases, when there is no similar word, we will degrade to use random word replacement.\nHere is an example of our pre-training task.\nExample\nOriginal Sentence\nwe use a language model to predict the probability of the next word.\nMLM\nwe use a language [M] to [M] ##di ##ct the pro [M] ##bility of the next word .\nWhole word masking\nwe use a language [M] to [M] [M] [M] the [M] [M] [M] of the next word .\nN-gram masking\nwe use a [M] [M] to [M] [M] [M] the [M] [M] [M] [M] [M] next word .\nMLM as correction\nwe use a text system to ca ##lc ##ulate the po ##si ##bility of the next word .\nExcept for the new pre-training task, we also incorporate the following techniques.\nWhole Word Masking (WWM)\nN-gram masking\nSentence-Order Prediction (SOP)\nNote that our MacBERT can be directly replaced with the original BERT as there is no differences in the main neural architecture.\nFor more technical details, please check our paper: Revisiting Pre-trained Models for Chinese Natural Language Processing\nCitation\nIf you find our resource or paper is useful, please consider including the following citation in your paper.\nhttps://arxiv.org/abs/2004.13922\n@inproceedings{cui-etal-2020-revisiting,\ntitle = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\nauthor = \"Cui, Yiming  and\nChe, Wanxiang  and\nLiu, Ting  and\nQin, Bing  and\nWang, Shijin  and\nHu, Guoping\",\nbooktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\npages = \"657--668\",\n}",
    "hfl/chinese-roberta-wwm-ext-large": "Please use 'Bert' related functions to load this model!\nChinese BERT with Whole Word Masking\nCitation\nPlease use 'Bert' related functions to load this model!\nChinese BERT with Whole Word Masking\nFor further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking.\nPre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu\nThis repository is developed based onÔºöhttps://github.com/google-research/bert\nYou may also interested in,\nChinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm\nChinese MacBERT: https://github.com/ymcui/MacBERT\nChinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\nChinese XLNet: https://github.com/ymcui/Chinese-XLNet\nKnowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/TextBrewer\nMore resources by HFL: https://github.com/ymcui/HFL-Anthology\nCitation\nIf you find the technical report or resource is useful, please cite the following technical report in your paper.\nPrimary: https://arxiv.org/abs/2004.13922\n@inproceedings{cui-etal-2020-revisiting,\ntitle = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\nauthor = \"Cui, Yiming  and\nChe, Wanxiang  and\nLiu, Ting  and\nQin, Bing  and\nWang, Shijin  and\nHu, Guoping\",\nbooktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\npages = \"657--668\",\n}\nSecondary: https://arxiv.org/abs/1906.08101\n@article{chinese-bert-wwm,\ntitle={Pre-Training with Whole Word Masking for Chinese BERT},\nauthor={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},\njournal={arXiv preprint arXiv:1906.08101},\nyear={2019}\n}",
    "microsoft/codebert-base": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nCodeBERT-base\nTraining Data\nTraining Objective\nUsage\nReference\nCitation\nCodeBERT-base\nPretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages.\nTraining Data\nThe model is trained on bi-modal data (documents & code) of CodeSearchNet\nTraining Objective\nThis model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper).\nUsage\nPlease see the official repository for scripts that support \"code search\" and \"code-to-document generation\".\nReference\nCodeBERT trained with Masked LM objective (suitable for code completion)\nü§ó Hugging Face's CodeBERTa (small size, 6 layers)\nCitation\n@misc{feng2020codebert,\ntitle={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},\nauthor={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},\nyear={2020},\neprint={2002.08155},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "nreimers/MiniLM-L6-H384-uncased": "MiniLM: 6 Layer Version\nMiniLM: 6 Layer Version\nThis is a 6 layer version of microsoft/MiniLM-L12-H384-uncased by keeping only every second layer.",
    "nvidia/segformer-b5-finetuned-cityscapes-1024-1024": "SegFormer (b5-sized) model fine-tuned on CityScapes\nModel description\nIntended uses & limitations\nHow to use\nLicense\nBibTeX entry and citation info\nSegFormer (b5-sized) model fine-tuned on CityScapes\nSegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\nDisclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nSegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.\nIntended uses & limitations\nYou can use the raw model for semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\nFor more code examples, we refer to the documentation.\nLicense\nThe license for this model can be found here.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2105-15203,\nauthor    = {Enze Xie and\nWenhai Wang and\nZhiding Yu and\nAnima Anandkumar and\nJose M. Alvarez and\nPing Luo},\ntitle     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\nTransformers},\njournal   = {CoRR},\nvolume    = {abs/2105.15203},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2105.15203},\neprinttype = {arXiv},\neprint    = {2105.15203},\ntimestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "oliverguhr/fullstop-punctuation-multilang-large": "Sample Code\nInstall\nRestore Punctuation\nPredict Labels\nResults\nLanguages\nModels\nCommunity Models\nWhere do I find the code and can I train my own model?\nReferences\nThis model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language.\nThis multilanguage model was trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.\nThe model restores the following punctuation markers: \".\" \",\" \"?\" \"-\" \":\"\nSample Code\nWe provide a simple python package that allows you to process text of any length.\nInstall\nTo get started install the package from pypi:\npip install deepmultilingualpunctuation\nRestore Punctuation\nfrom deepmultilingualpunctuation import PunctuationModel\nmodel = PunctuationModel()\ntext = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau M√ºller\"\nresult = model.restore_punctuation(text)\nprint(result)\noutput\nMy name is Clara and I live in Berkeley, California. Ist das eine Frage, Frau M√ºller?\nPredict Labels\nfrom deepmultilingualpunctuation import PunctuationModel\nmodel = PunctuationModel()\ntext = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau M√ºller\"\nclean_text = model.preprocess(text)\nlabled_words = model.predict(clean_text)\nprint(labled_words)\noutput\n[['My', '0', 0.9999887], ['name', '0', 0.99998665], ['is', '0', 0.9998579], ['Clara', '0', 0.6752215], ['and', '0', 0.99990904], ['I', '0', 0.9999877], ['live', '0', 0.9999839], ['in', '0', 0.9999515], ['Berkeley', ',', 0.99800044], ['California', '.', 0.99534047], ['Ist', '0', 0.99998784], ['das', '0', 0.99999154], ['eine', '0', 0.9999918], ['Frage', ',', 0.99622655], ['Frau', '0', 0.9999889], ['M√ºller', '?', 0.99863917]]\nResults\nThe performance differs for the single punctuation markers as hyphens and colons, in many cases, are optional and can be substituted by either a comma or a full stop. The model achieves the following F1 scores for the different languages:\nLabel\nEN\nDE\nFR\nIT\n0\n0.991\n0.997\n0.992\n0.989\n.\n0.948\n0.961\n0.945\n0.942\n?\n0.890\n0.893\n0.871\n0.832\n,\n0.819\n0.945\n0.831\n0.798\n:\n0.575\n0.652\n0.620\n0.588\n-\n0.425\n0.435\n0.431\n0.421\nmacro average\n0.775\n0.814\n0.782\n0.762\nLanguages\nModels\nLanguages\nModel\nEnglish, Italian, French and German\noliverguhr/fullstop-punctuation-multilang-large\nEnglish, Italian, French, German and Dutch\noliverguhr/fullstop-punctuation-multilingual-sonar-base\nDutch\noliverguhr/fullstop-dutch-sonar-punctuation-prediction\nCommunity Models\nLanguages\nModel\nEnglish, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian\nkredor/punctuate-all\nCatalan\nsoftcatala/fullstop-catalan-punctuation-prediction\nWelsh\ntechiaith/fullstop-welsh-punctuation-prediction\nYou can use different models by setting the model parameter:\nmodel = PunctuationModel(model = \"oliverguhr/fullstop-dutch-punctuation-prediction\")\nWhere do I find the code and can I train my own model?\nYes you can! For complete code of the reareach project take a look at this repository.\nThere is also an guide on how to fine tune this model for you data / language.\nReferences\n@article{guhr-EtAl:2021:fullstop,\ntitle={FullStop: Multilingual Deep Models for Punctuation Prediction},\nauthor    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  B√∂hme, Hans Joachim},\nbooktitle      = {Proceedings of the Swiss Text Analytics Conference 2021},\nmonth          = {June},\nyear           = {2021},\naddress        = {Winterthur, Switzerland},\npublisher      = {CEUR Workshop Proceedings},\nurl       = {http://ceur-ws.org/Vol-2957/sepp_paper4.pdf}\n}",
    "papluca/xlm-roberta-base-language-detection": "xlm-roberta-base-language-detection\nModel description\nIntended uses & limitations\nTraining and evaluation data\nBenchmarks\nHow to get started with the model\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nxlm-roberta-base-language-detection\nThis model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.\nModel description\nThis model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output).\nFor additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.\nIntended uses & limitations\nYou can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages:\narabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\nTraining and evaluation data\nThe model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.\nLanguage\nPrecision\nRecall\nF1-score\nsupport\nar\n0.998\n0.996\n0.997\n500\nbg\n0.998\n0.964\n0.981\n500\nde\n0.998\n0.996\n0.997\n500\nel\n0.996\n1.000\n0.998\n500\nen\n1.000\n1.000\n1.000\n500\nes\n0.967\n1.000\n0.983\n500\nfr\n1.000\n1.000\n1.000\n500\nhi\n0.994\n0.992\n0.993\n500\nit\n1.000\n0.992\n0.996\n500\nja\n0.996\n0.996\n0.996\n500\nnl\n1.000\n1.000\n1.000\n500\npl\n1.000\n1.000\n1.000\n500\npt\n0.988\n1.000\n0.994\n500\nru\n1.000\n0.994\n0.997\n500\nsw\n1.000\n1.000\n1.000\n500\nth\n1.000\n0.998\n0.999\n500\ntr\n0.994\n0.992\n0.993\n500\nur\n1.000\n1.000\n1.000\n500\nvi\n0.992\n1.000\n0.996\n500\nzh\n1.000\n1.000\n1.000\n500\nBenchmarks\nAs a baseline to compare xlm-roberta-base-language-detection against, we have used the Python langid library. Since it comes pre-trained on 97 languages, we have used its .set_languages() method to constrain the language set to our 20 languages. The average accuracy of langid on the test set is 98.5%. More details are provided by the table below.\nLanguage\nPrecision\nRecall\nF1-score\nsupport\nar\n0.990\n0.970\n0.980\n500\nbg\n0.998\n0.964\n0.981\n500\nde\n0.992\n0.944\n0.967\n500\nel\n1.000\n0.998\n0.999\n500\nen\n1.000\n1.000\n1.000\n500\nes\n1.000\n0.968\n0.984\n500\nfr\n0.996\n1.000\n0.998\n500\nhi\n0.949\n0.976\n0.963\n500\nit\n0.990\n0.980\n0.985\n500\nja\n0.927\n0.988\n0.956\n500\nnl\n0.980\n1.000\n0.990\n500\npl\n0.986\n0.996\n0.991\n500\npt\n0.950\n0.996\n0.973\n500\nru\n0.996\n0.974\n0.985\n500\nsw\n1.000\n1.000\n1.000\n500\nth\n1.000\n0.996\n0.998\n500\ntr\n0.990\n0.968\n0.979\n500\nur\n0.998\n0.996\n0.997\n500\nvi\n0.971\n0.990\n0.980\n500\nzh\n1.000\n1.000\n1.000\n500\nHow to get started with the model\nThe easiest way to use the model is via the high-level pipeline API:\nfrom transformers import pipeline\ntext = [\n\"Brevity is the soul of wit.\",\n\"Amor, ch'a nullo amato amar perdona.\"\n]\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\nOr one can proceed with the tokenizer and model separately:\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntext = [\n\"Brevity is the soul of wit.\",\n\"Amor, ch'a nullo amato amar perdona.\"\n]\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\npreds = torch.softmax(logits, dim=-1)\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\nTraining procedure\nFine-tuning was done via the Trainer API. Here is the Colab notebook with the training code.\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 64\neval_batch_size: 128\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 2\nmixed_precision_training: Native AMP\nTraining results\nThe validation results on the valid split of the Language Identification dataset are summarised here below.\nTraining Loss\nEpoch\nStep\nValidation Loss\nAccuracy\nF1\n0.2492\n1.0\n1094\n0.0149\n0.9969\n0.9969\n0.0101\n2.0\n2188\n0.0103\n0.9977\n0.9977\nIn short, it achieves the following results on the validation set:\nLoss: 0.0101\nAccuracy: 0.9977\nF1: 0.9977\nFramework versions\nTransformers 4.12.5\nPytorch 1.10.0+cu111\nDatasets 1.15.1\nTokenizers 0.10.3",
    "pritamdeka/S-PubMedBert-MS-MARCO": "pritamdeka/S-PubMedBert-MS-MARCO\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nTraining\nFull Model Architecture\nCiting & Authors\npritamdeka/S-PubMedBert-MS-MARCO\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nThis is the microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext model which has been fine-tuned over the MS-MARCO dataset using sentence-transformers framework. It can be used for the information retrieval task in the medical/health text domain.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('pritamdeka/S-PubMedBert-MS-MARCO')\nmodel = AutoModel.from_pretrained('pritamdeka/S-PubMedBert-MS-MARCO')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nTraining\nThe model was trained with the parameters:\nDataLoader:\ntorch.utils.data.dataloader.DataLoader of length 31434 with parameters:\n{'batch_size': 16, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\nLoss:\nbeir.losses.margin_mse_loss.MarginMSELoss\nParameters of the fit()-Method:\n{\n\"callback\": null,\n\"epochs\": 2,\n\"evaluation_steps\": 10000,\n\"evaluator\": \"sentence_transformers.evaluation.SequentialEvaluator.SequentialEvaluator\",\n\"max_grad_norm\": 1,\n\"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n\"optimizer_params\": {\n\"correct_bias\": false,\n\"eps\": 1e-06,\n\"lr\": 2e-05\n},\n\"scheduler\": \"WarmupLinear\",\n\"steps_per_epoch\": null,\n\"warmup_steps\": 1000,\n\"weight_decay\": 0.01\n}\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 350, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\n@article{deka2022improved,\ntitle={Improved Methods To Aid Unsupervised Evidence-Based Fact Checking For Online Health News},\nauthor={Deka, Pritam and Jurek-Loughrey, Anna and Deepak, P},\njournal={Journal of Data Intelligence},\nvolume={3},\nnumber={4},\npages={474--504},\nyear={2022}\n}",
    "pyannote/speaker-diarization": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThe collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nüéπ Speaker diarization\nTL;DR\nAdvanced usage\nBenchmark\nReal-time factor\nAccuracy\nTechnical report\nCitations\nUsing this open-source model in production?Consider switching to pyannoteAI for better and faster options.\nüéπ Speaker diarization\nRelies on pyannote.audio 2.1.1: see installation instructions.\nTL;DR\n# 1. visit hf.co/pyannote/speaker-diarization and accept user conditions\n# 2. visit hf.co/pyannote/segmentation and accept user conditions\n# 3. visit hf.co/settings/tokens to create an access token\n# 4. instantiate pretrained speaker diarization pipeline\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\nuse_auth_token=\"ACCESS_TOKEN_GOES_HERE\")\n# apply the pipeline to an audio file\ndiarization = pipeline(\"audio.wav\")\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\ndiarization.write_rttm(rttm)\nAdvanced usage\nIn case the number of speakers is known in advance, one can use the num_speakers option:\ndiarization = pipeline(\"audio.wav\", num_speakers=2)\nOne can also provide lower and/or upper bounds on the number of speakers using min_speakers and max_speakers options:\ndiarization = pipeline(\"audio.wav\", min_speakers=2, max_speakers=5)\nBenchmark\nReal-time factor\nReal-time factor is around 2.5% using one Nvidia Tesla V100 SXM2 GPU (for the neural inference part) and one Intel Cascade Lake 6248 CPU (for the clustering part).\nIn other words, it takes approximately 1.5 minutes to process a one hour conversation.\nAccuracy\nThis pipeline is benchmarked on a growing collection of datasets.\nProcessing is fully automatic:\nno manual voice activity detection (as is sometimes the case in the literature)\nno manual number of speakers (though it is possible to provide it to the pipeline)\nno fine-tuning of the internal models nor tuning of the pipeline hyper-parameters to each dataset\n... with the least forgiving diarization error rate (DER) setup (named \"Full\" in this paper):\nno forgiveness collar\nevaluation of overlapped speech\nBenchmark\nDER%\nFA%\nMiss%\nConf%\nExpected output\nFile-level evaluation\nAISHELL-4\n14.09\n5.17\n3.27\n5.65\nRTTM\neval\nAlbayzin (RTVE 2022)\n25.60\n5.58\n6.84\n13.18\nRTTM\neval\nAliMeeting (channel 1)\n27.42\n4.84\n14.00\n8.58\nRTTM\neval\nAMI (headset mix, only_words)\n18.91\n4.48\n9.51\n4.91\nRTTM\neval\nAMI (array1, channel 1, only_words)\n27.12\n4.11\n17.78\n5.23\nRTTM\neval\nCALLHOME (part2)\n32.37\n6.30\n13.72\n12.35\nRTTM\neval\nDIHARD 3 (Full)\n26.94\n10.50\n8.41\n8.03\nRTTM\neval\nEgo4D v1 (validation)\n63.99\n3.91\n44.42\n15.67\nRTTM\neval\nREPERE (phase 2)\n8.17\n2.23\n2.49\n3.45\nRTTM\neval\nThis American Life\n20.82\n2.03\n11.89\n6.90\nRTTM\neval\nVoxConverse (v0.3)\n11.24\n4.42\n2.88\n3.94\nRTTM\neval\nTechnical report\nThis report describes the main principles behind version 2.1 of pyannote.audio speaker diarization pipeline.It also provides recipes explaining how to adapt the pipeline to your own set of annotated data. In particular, those are applied to the above benchmark and consistently leads to significant performance improvement over the above out-of-the-box performance.\nCitations\n@inproceedings{Bredin2021,\nTitle = {{End-to-end speaker segmentation for overlap-aware resegmentation}},\nAuthor = {{Bredin}, Herv{\\'e} and {Laurent}, Antoine},\nBooktitle = {Proc. Interspeech 2021},\nAddress = {Brno, Czech Republic},\nMonth = {August},\nYear = {2021},\n}\n@inproceedings{Bredin2020,\nTitle = {{pyannote.audio: neural building blocks for speaker diarization}},\nAuthor = {{Bredin}, Herv{\\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},\nBooktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},\nAddress = {Barcelona, Spain},\nMonth = {May},\nYear = {2020},\n}",
    "ai-forever/Real-ESRGAN": "Real-ESRGAN\nUsage\nReal-ESRGAN\nPyTorch implementation of a Real-ESRGAN model trained on custom dataset. This model shows better results on faces compared to the original version. It is also easier to integrate this model into your projects.\nReal-ESRGAN is an upgraded ESRGAN trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images.\nPaper\nOriginal implementation\nOur github\nUsage\nCode for using model you can obtain in our repo.\nimport torch\nfrom PIL import Image\nimport numpy as np\nfrom RealESRGAN import RealESRGAN\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = RealESRGAN(device, scale=4)\nmodel.load_weights('weights/RealESRGAN_x4.pth', download=True)\npath_to_image = 'inputs/lr_image.png'\nimage = Image.open(path_to_image).convert('RGB')\nsr_image = model.predict(image)\nsr_image.save('results/sr_image.png')",
    "ctheodoris/Geneformer": "Geneformer\nModel Description\nApplication\nInstallation\nCitations\nGeneformer\nGeneformer is a foundational transformer model pretrained on a large-scale corpus of single cell transcriptomes to enable context-aware predictions in settings with limited data in network biology.\nSee our manuscript for details of the original model trained on ~30 million transcriptomes in June 2021 and the initial report of our in silico perturbation and cell and gene classification strategies.\nSee our manuscript for details of the expanded model, now trained on ~104 million transcriptomes, and our continual learning, multitask learning, and quantization strategies.\nSee geneformer.readthedocs.io for documentation.\nModel Description\nGeneformer is a foundational transformer model pretrained on a large-scale corpus of single cell transcriptomes representing a broad range of human tissues. Geneformer V1 was originally pretrained in June 2021 on Genecorpus-30M, a corpus comprised of ~30 million human single cell transcriptomes. We excluded cells with high mutational burdens (e.g. malignant cells and immortalized cell lines) that could lead to substantial network rewiring without companion genome sequencing to facilitate interpretation. The current updated Geneformer V2 is pretrained on ~104 million human single cell transcriptomes (non-cancer). The cancer continual learning V2 variant was continually pretrained on ~14 million cancer transcriptomes to yield a cancer domain-tuned model.\nEach single cell‚Äôs transcriptome is presented to the model as a rank value encoding where genes are ranked by their expression in that cell scaled by their expression across the entire Genecorpus (~30M for V1, ~104M for V2). The rank value encoding provides a nonparametric representation of that cell‚Äôs transcriptome and takes advantage of the many observations of each gene‚Äôs expression across the pretraining corpus to prioritize genes that distinguish cell state. Specifically, this method will deprioritize ubiquitously highly-expressed housekeeping genes by scaling them to a lower rank. Conversely, genes such as transcription factors that may be lowly expressed when they are expressed but highly distinguish cell state will move to a higher rank within the encoding. Furthermore, this rank-based approach may be more robust against technical artifacts that may systematically bias the absolute transcript counts value while the overall relative ranking of genes within each cell remains more stable.\nThe rank value encoding of each single cell‚Äôs transcriptome then proceeds through N layers of transformer encoder units, where N varies dependent on the model size. Pretraining was accomplished using a masked learning objective where 15% of the genes within each transcriptome were masked and the model was trained to predict which gene should be within each masked position in that specific cell state using the context of the remaining unmasked genes. A major strength of this approach is that it is entirely self-supervised and can be accomplished on completely unlabeled data, which allows the inclusion of large amounts of training data without being restricted to samples with accompanying labels.\nWe detail applications and results in our manuscript.\nDuring pretraining, Geneformer gained a fundamental understanding of network dynamics, encoding network hierarchy in the model‚Äôs attention weights in a completely self-supervised manner. With both zero-shot learning and fine-tuning with limited task-specific data, Geneformer consistently boosted predictive accuracy in a diverse panel of downstream tasks relevant to chromatin and network dynamics. In silico perturbation with zero-shot learning identified a novel transcription factor in cardiomyocytes that we experimentally validated to be critical to their ability to generate contractile force. In silico treatment with limited patient data revealed candidate therapeutic targets for cardiomyopathy that we experimentally validated to significantly improve the ability of cardiomyocytes to generate contractile force in an induced pluripotent stem cell (iPSC) model of the disease. Overall, Geneformer represents a foundational AI model pretrained on a large-scale corpus human single cell transcriptomes to gain a fundamental understanding of gene network dynamics that can now be democratized to a vast array of downstream tasks to accelerate discovery of key network regulators and candidate therapeutic targets.\nThe repository includes the following pretrained models:\nGeneformer-V1-10M: original model trained June 2021 on ~30M human single cell transcriptomes, 10M parameters, input size 2048, vocabulary ~25K protein-coding or non-coding RNA genes\nGeneformer-V2-104M and Geneformer-V2-316M: updated model trained Dec 2024 on ~104M human single cell transcriptomes, 104M or 316M parameters, input size 4096, vocabulary ~20K protein-coding genes\nThe current default model in the main directory of the repository is Geneformer-V2-316M.\nThe repository also contains fined tuned models in the fine_tuned_models directory and the cancer-tuned model following continual learning on ~14 million cancer cells, Geneformer-V2-104M_CLcancer.\nApplication\nThe pretrained Geneformer model can be used directly for zero-shot learning, for example for in silico perturbation analysis, or by fine-tuning towards the relevant downstream task, such as gene or cell state classification.\nExample applications demonstrated in our manuscript include:\nFine-tuning:\ntranscription factor dosage sensitivity\nchromatin dynamics (bivalently marked promoters)\ntranscription factor regulatory range\ngene network centrality\ntranscription factor targets\ncell type annotation\nbatch integration\ncell state classification across differentiation\ndisease classification\nin silico perturbation to determine disease-driving genes\nin silico treatment to determine candidate therapeutic targets\nZero-shot learning:\nbatch integration\ngene context specificity\nin silico reprogramming\nin silico differentiation\nin silico perturbation to determine impact on cell state\nin silico perturbation to determine transcription factor targets\nin silico perturbation to determine transcription factor cooperativity\nInstallation\nIn addition to the pretrained model, contained herein are functions for tokenizing and collating data specific to single cell transcriptomics, pretraining the model, fine-tuning the model, extracting and plotting cell embeddings, and performing in silico pertrubation with either the pretrained or fine-tuned models. To install (~20s):\n# Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/ctheodoris/Geneformer\ncd Geneformer\npip install .\nFor usage, see examples for:\ntokenizing transcriptomes\npretraining\nhyperparameter tuning\nfine-tuning\nextracting and plotting cell embeddings\nin silico perturbation\nPlease also see here for a quickstart tutorial for predicting candidate therapeutic targets with Geneformer.\nComplete documentation is available at https://geneformer.readthedocs.io/en/latest/.\nPlease note that the fine-tuning examples are meant to be generally applicable and the input datasets and labels will vary dependent on the downstream task. Example input files for a few of the downstream tasks demonstrated in the manuscript are located within the example_input_files directory in the dataset repository, but these only represent a few example fine-tuning applications.\nPlease note that GPU resources are required for efficient usage of Geneformer. Additionally, we strongly recommend tuning hyperparameters for each downstream fine-tuning application as this can significantly boost predictive potential in the downstream task (e.g. max learning rate, learning schedule, number of layers to freeze, etc.). Importantly, as usual for deep learning models, there are no uniformly applicable default hyperparameters for Geneformer.\nCitations\nC V Theodoris#, L Xiao, A Chopra, M D Chaffin, Z R Al Sayed, M C Hill, H Mantineo, E Brydon, Z Zeng, X S Liu, P T Ellinor#. Transfer learning enables predictions in network biology. Nature, 31 May 2023. (#co-corresponding authors)\nH Chen*, M S Venkatesh*, J Gomez Ortega, S V Mahesh, T Nandi, R Madduri, K Pelka‚Ä†, C V Theodoris‚Ä†#. Quantized multi-task learning for context-specific representations of gene network dynamics. bioRxiv, 19 Aug 2024. (*co-first authors, ‚Ä†co-senior authors, #corresponding author)",
    "cardiffnlp/twitter-roberta-base-sentiment-latest": "Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\nExample Pipeline\nFull classification example\nReferences\nTwitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark.\nThe original Twitter-based RoBERTa model can be found here and the original reference paper is TweetEval. This model is suitable for English.\nReference Paper: TimeLMs paper.\nGit Repo: TimeLMs official repository.\nLabels:\n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\nThis sentiment analysis model has been integrated into TweetNLP. You can access the demo here.\nExample Pipeline\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n[{'label': 'Negative', 'score': 0.7236}]\nFull classification example\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\nnew_text = []\nfor t in text.split(\" \"):\nt = '@user' if t.startswith('@') and len(t) > 1 else t\nt = 'http' if t.startswith('http') else t\nnew_text.append(t)\nreturn \" \".join(new_text)\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\nl = config.id2label[ranking[i]]\ns = scores[ranking[i]]\nprint(f\"{i+1}) {l} {np.round(float(s), 4)}\")\nOutput:\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\nReferences\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\ntitle = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\nauthor = \"Camacho-collados, Jose  and\nRezaee, Kiamehr  and\nRiahi, Talayeh  and\nUshio, Asahi  and\nLoureiro, Daniel  and\nAntypas, Dimosthenis  and\nBoisson, Joanne  and\nEspinosa Anke, Luis  and\nLiu, Fangyu  and\nMart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\nbooktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\nmonth = dec,\nyear = \"2022\",\naddress = \"Abu Dhabi, UAE\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2022.emnlp-demos.5\",\npages = \"38--49\"\n}\n@inproceedings{loureiro-etal-2022-timelms,\ntitle = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\nauthor = \"Loureiro, Daniel  and\nBarbieri, Francesco  and\nNeves, Leonardo  and\nEspinosa Anke, Luis  and\nCamacho-collados, Jose\",\nbooktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\nmonth = may,\nyear = \"2022\",\naddress = \"Dublin, Ireland\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2022.acl-demo.25\",\ndoi = \"10.18653/v1/2022.acl-demo.25\",\npages = \"251--260\"\n}",
    "microsoft/layoutlmv3-base": "LayoutLMv3\nModel description\nCitation\nLicense\nLayoutLMv3\nMicrosoft Document AI | GitHub\nModel description\nLayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.\nLayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.\nCitation\nIf you find LayoutLM useful in your research, please cite the following paper:\n@inproceedings{huang2022layoutlmv3,\nauthor={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},\ntitle={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},\nbooktitle={Proceedings of the 30th ACM International Conference on Multimedia},\nyear={2022}\n}\nLicense\nThe content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).\nPortions of the source code are based on the transformers project.\nMicrosoft Open Source Code of Conduct",
    "facebook/opt-125m": "OPT : Open Pre-trained Transformer Language Models\nIntro\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nCollection process\nTraining procedure\nPreprocessing\nBibTeX entry and citation info\nOPT : Open Pre-trained Transformer Language Models\nOPT was first introduced in Open Pre-trained Transformer Language Models and first released in metaseq's repository on May 3rd 2022 by Meta AI.\nDisclaimer: The team releasing OPT wrote an official model card, which is available in Appendix D of the paper.\nContent from this model card has been written by the Hugging Face team.\nIntro\nTo quote the first two paragraphs of the official paper\nLarge language models trained on massive text collections have shown surprising emergent\ncapabilities to generate text and perform zero- and few-shot learning. While in some cases the public\ncan interact with these models through paid APIs, full model access is currently limited to only a\nfew highly resourced labs. This restricted access has limited researchers‚Äô ability to study how and\nwhy these large language models work, hindering progress on improving known challenges in areas\nsuch as robustness, bias, and toxicity.\nWe present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M\nto 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match\nthe performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data\ncollection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and\nto bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the\ncollective research community as a whole, which is only possible when models are available for study.\nModel description\nOPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\nOPT belongs to the same family of decoder-only models like GPT-3. As such, it was pretrained using the self-supervised causal language modedling objective.\nFor evaluation, OPT follows GPT-3 by using their prompts and overall experimental setup. For more details, please read\nthe official paper.\nIntended uses & limitations\nThe pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the CLM example. For all other OPT checkpoints, please have a look at the model hub.\nHow to use\nYou can use this model directly with a pipeline for text generation.\n>>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\nBy default, generation is deterministic. In order to use the top-k sampling, please set do_sample to True.\n>>> from transformers import pipeline, set_seed\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\nLimitations and bias\nAs mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased :\nLike other large language models for which the diversity (or lack thereof) of training\ndata induces downstream impact on the quality of our model, OPT-175B has limitations in terms\nof bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\nhallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\nlarge language models.\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nThe Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents:\nBookCorpus, which consists of more than 10K unpublished books,\nCC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\nThe Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.\nPushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\nCCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\nThe final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset‚Äôs size in the pretraining corpus.\nThe dataset might contains offensive content as parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety.\nCollection process\nThe dataset was collected form internet, and went through classic data processing algorithms  and\nre-formatting practices, including removing repetitive/non-informative text like Chapter One or\nThis ebook by Project Gutenberg.\nTraining procedure\nPreprocessing\nThe texts are tokenized using the GPT2 byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50272. The inputs are sequences of 2048 consecutive tokens.\nThe 175B model was trained on 992 80GB A100 GPUs. The training duration was roughly ~33 days of continuous training.\nBibTeX entry and citation info\n@misc{zhang2022opt,\ntitle={OPT: Open Pre-trained Transformer Language Models},\nauthor={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\nyear={2022},\neprint={2205.01068},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "apple/mobilevit-small": "MobileViT (small-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nMobileViT (small-sized model)\nMobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nMobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are \"unflattened\" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nCurrently, both the feature extractor and model support PyTorch.\nTraining data\nThe MobileViT model was pretrained on ImageNet-1k, a dataset consisting of 1 million images and 1,000 classes.\nTraining procedure\nPreprocessing\nTraining requires only basic data augmentation, i.e. random resized cropping and horizontal flipping.\nTo learn multi-scale representations without requiring fine-tuning, a multi-scale sampler was used during training, with image sizes randomly sampled from: (160, 160), (192, 192), (256, 256), (288, 288), (320, 320).\nAt inference time, images are resized/rescaled to the same resolution (288x288), and center-cropped at 256x256.\nPixels are normalized to the range [0, 1]. Images are expected to be in BGR pixel order, not RGB.\nPretraining\nThe MobileViT networks are trained from scratch for 300 epochs on ImageNet-1k on 8 NVIDIA GPUs with an effective batch size of 1024 and learning rate warmup for 3k steps, followed by cosine annealing. Also used were label smoothing cross-entropy loss and L2 weight decay. Training resolution varies from 160x160 to 320x320, using multi-scale sampling.\nEvaluation results\nModel\nImageNet top-1 accuracy\nImageNet top-5 accuracy\n# params\nURL\nMobileViT-XXS\n69.0\n88.9\n1.3 M\nhttps://huggingface.co/apple/mobilevit-xx-small\nMobileViT-XS\n74.8\n92.3\n2.3 M\nhttps://huggingface.co/apple/mobilevit-x-small\nMobileViT-S\n78.4\n94.1\n5.6 M\nhttps://huggingface.co/apple/mobilevit-small\nBibTeX entry and citation info\n@inproceedings{vision-transformer,\ntitle = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor = {Sachin Mehta and Mohammad Rastegari},\nyear = {2022},\nURL = {https://arxiv.org/abs/2110.02178}\n}",
    "yanekyuk/bert-keyword-extractor": "bert-keyword-extractor\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nbert-keyword-extractor\nThis model is a fine-tuned version of bert-base-cased on an unknown dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.1341\nPrecision: 0.8565\nRecall: 0.8874\nAccuracy: 0.9738\nF1: 0.8717\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 16\neval_batch_size: 16\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 8\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nPrecision\nRecall\nAccuracy\nF1\n0.1688\n1.0\n1875\n0.1233\n0.7194\n0.7738\n0.9501\n0.7456\n0.1219\n2.0\n3750\n0.1014\n0.7724\n0.8166\n0.9606\n0.7939\n0.0834\n3.0\n5625\n0.0977\n0.8280\n0.8263\n0.9672\n0.8272\n0.0597\n4.0\n7500\n0.0984\n0.8304\n0.8680\n0.9704\n0.8488\n0.0419\n5.0\n9375\n0.1042\n0.8417\n0.8687\n0.9717\n0.8550\n0.0315\n6.0\n11250\n0.1161\n0.8520\n0.8839\n0.9729\n0.8677\n0.0229\n7.0\n13125\n0.1282\n0.8469\n0.8939\n0.9734\n0.8698\n0.0182\n8.0\n15000\n0.1341\n0.8565\n0.8874\n0.9738\n0.8717\nFramework versions\nTransformers 4.19.2\nPytorch 1.11.0+cu113\nDatasets 2.2.2\nTokenizers 0.12.1",
    "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli": "DeBERTa-v3-large-mnli-fever-anli-ling-wanli\nModel description\nHow to use the model\nTraining data\nTraining procedure\nEval results\nLimitations and bias\nCitation\nIdeas for cooperation or questions?\nDebugging and issues\nDeBERTa-v3-large-mnli-fever-anli-ling-wanli\nModel description\nThis model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.\nThe foundation model is DeBERTa-v3-large from Microsoft. DeBERTa-v3 combines several recent innovations compared to classical Masked Language Models like BERT, RoBERTa etc., see the paper\nHow to use the model\nSimple zero-shot classification pipeline\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\nsequence_to_classify = \"Angela Merkel is a politician in Germany and leader of the CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\nNLI use-case\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\npremise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\nhypothesis = \"The movie was not good.\"\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\nTraining data\nDeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. Note that SNLI was explicitly excluded due to quality issues with the dataset. More data does not necessarily make for better NLI models.\nTraining procedure\nDeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained using the Hugging Face trainer with the following hyperparameters. Note that longer training with more epochs hurt performance in my tests (overfitting).\ntraining_args = TrainingArguments(\nnum_train_epochs=4,              # total number of training epochs\nlearning_rate=5e-06,\nper_device_train_batch_size=16,   # batch size per device during training\ngradient_accumulation_steps=2,    # doubles the effective batch_size to 32, while decreasing memory requirements\nper_device_eval_batch_size=64,    # batch size for evaluation\nwarmup_ratio=0.06,                # number of warmup steps for learning rate scheduler\nweight_decay=0.01,               # strength of weight decay\nfp16=True                        # mixed precision training\n)\nEval results\nThe model was evaluated using the test sets for MultiNLI, ANLI, LingNLI, WANLI and the dev set for Fever-NLI. The metric used is accuracy.\nThe model achieves state-of-the-art performance on each dataset. Surprisingly, it outperforms the previous state-of-the-art on ANLI (ALBERT-XXL) by 8,3%. I assume that this is because ANLI was created to fool masked language models like RoBERTa (or ALBERT), while DeBERTa-v3 uses a better pre-training objective (RTD), disentangled attention and I fine-tuned it on higher quality NLI data.\nDatasets\nmnli_test_m\nmnli_test_mm\nanli_test\nanli_test_r3\nling_test\nwanli_test\nAccuracy\n0.912\n0.908\n0.702\n0.64\n0.87\n0.77\nSpeed (text/sec, A100 GPU)\n696.0\n697.0\n488.0\n425.0\n828.0\n980.0\nLimitations and bias\nPlease consult the original DeBERTa-v3 paper and literature on different NLI datasets for more information on the training data and potential biases. The model will reproduce statistical patterns in the training data.\nCitation\nIf you use this model, please cite: Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. ‚ÄòLess Annotating, More Classifying ‚Äì Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI‚Äô. Preprint, June. Open Science Framework. https://osf.io/74b8k.\nIdeas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or LinkedIn\nDebugging and issues\nNote that DeBERTa-v3 was released on 06.12.21 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 might solve some issues."
}