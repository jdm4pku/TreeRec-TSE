{
    "Qwen/Qwen3-VL-4B-Thinking-FP8": "Qwen3-VL-4B-Thinking-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nGeneration Hyperparameters\nCitation\nQwen3-VL-4B-Thinking-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-4B-Thinking model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-4B-Thinking-FP8.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-4B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-4B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport temperature=1.0\nexport out_seq_length=40960\nText\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport temperature=1.0\nexport out_seq_length=32768 (for aime, lcb, and gpqa, it is recommended to set to 81920)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "KORMo-Team/KORMo-10B-sft": "üöÄ Update News\nüí° About KORMo\nüîó Links\nüìà Benchmark Performance\nüìä Quantitative Evaluation\nüìù Qualitative Evaluation (LLM-as-a-Judge)\nüì¶ Installation\nüöÄ Inference Example\nüß† Enabling Thinking Mode\nLimitation\nContact\nAcknowledgments\nCitation\nüöÄ Update News\n2025-10-13: Official release of KORMo-10B-sft.\nüí° About KORMo\nKORMo-10B is a 10.8B parameter fully open LLM capable of handling both Korean and English.The model, training code, and training data are all fully open, allowing anyone to reproduce and extend them.\nModel Size: 10.8B parameters\nLanguages: Korean / English\nTraining Data: Synthetic data + public datasets (approximately 3T tokens)\nLicense: Apache 2.0\nKORMoÎäî ÎπÑÏòÅÏñ¥Í∂å ÏµúÏ¥àÏùò Fully Open Source LLMÏúºÎ°ú, Í≥µÏùµÏ†Å ÌôúÏö©ÏùÑ Î™©ÌëúÎ°ú ÌÉÑÏÉùÌñàÏäµÎãàÎã§.\nÏö∞Î¶¨Îäî ÎàÑÍµ¨ÎÇò ÏÑ∏Í≥Ñ ÏàòÏ§ÄÏùò Ïñ∏Ïñ¥Î™®Îç∏ÏùÑ ÏßÅÏ†ë ÎßåÎì§Í≥† Î∞úÏ†ÑÏãúÌÇ¨ Ïàò ÏûàÎäî ÌôòÍ≤ΩÏùÑ ÎßåÎì§Í≥†Ïûê Ìï©ÎãàÎã§.\nKORMoÏùò Ï£ºÏöî ÌäπÏßïÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\n1. From scratch ÌïôÏäµÏúºÎ°ú ÏÑ§Í≥ÑÎêú 10BÍ∏â Ìïú‚ÄìÏòÅ Ï∂îÎ°† Ïñ∏Ïñ¥Î™®Îç∏ÏûÖÎãàÎã§.\n2. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞, ÏΩîÎìú, Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏôÄ ÌäúÌÜ†Î¶¨ÏñºÏùÑ 100% Í≥µÍ∞úÌïòÏó¨, ÎàÑÍµ¨ÎÇò SOTAÏóê Í∑ºÏ†ëÌïú Î™®Îç∏ÏùÑ ÏßÅÏ†ë Ïû¨ÌòÑÌïòÍ≥† ÌôïÏû•Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n3. Ï¥ù 3.7T ÌÜ†ÌÅ∞ Í∑úÎ™®Ïùò ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î•º Í≥µÍ∞úÌï©ÎãàÎã§. ÌäπÌûà ÏßÄÍ∏àÍπåÏßÄ Ìïú Î≤àÎèÑ Í≥µÍ∞úÎêú Ï†Å ÏóÜÎäî Ï¥àÍ≥†ÌíàÏßà Ï†ÑÏ£ºÍ∏∞ ÌïúÍµ≠Ïñ¥ Îç∞Ïù¥ÌÑ∞(ÏÇ¨Ï†ÑÌïôÏäµ, ÏÇ¨ÌõÑÌïôÏäµ, ÏùºÎ∞òÌòï, Ï∂îÎ°†Ìòï, Í∞ïÌôîÌïôÏäµ Îì±)Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n4. Ïù¥ Î™®Îì† ÏûëÏóÖÏùÄ KAIST Î¨∏ÌôîÍ∏∞Ïà†ÎåÄÌïôÏõê MLPÏó∞Íµ¨Ïã§Ïùò ÌïôÎ∂Ä¬∑ÏÑùÏÇ¨ÏÉù 8Î™ÖÏù¥ ÌòëÎ†•ÌïòÏó¨ ÏßÑÌñâÌñàÏúºÎ©∞, 45Ïû•Ïóê Îã¨ÌïòÎäî ÎÖºÎ¨∏ÏúºÎ°ú Ï†ïÎ¶¨ÌñàÏäµÎãàÎã§.\nÏßÄÍ∏àÍπåÏßÄ ÌïúÍµ≠Ïñ¥ Î™®Îç∏ÏùÑ Ïç®Î≥¥Î©¥, Î≤§ÏπòÎßàÌÅ¨ Ï†êÏàòÎäî Ï¢ãÏùÄÎç∞ Ïã§ÏÇ¨Ïö©ÏóêÏÑúÎäî Ïñ¥ÎîòÍ∞Ä Ïù¥ÏÉÅÌïòÍ±∞ÎÇò,\nÌäúÎãùÎßå ÌïòÎ©¥ Î™®Îç∏Ïù¥ ÎßùÍ∞ÄÏßÄÎäî Í≤ΩÌóòÏùÑ ÌïòÏÖ®ÏùÑ Í≤ÅÎãàÎã§. ÎãµÎãµÌïòÏÖ®Ï£†?\nKORMoÎäî Í∑∏Îü∞ Î¨∏Ï†úÎ•º Ï†ïÎ©¥ÏúºÎ°ú Ìï¥Í≤∞Ìï©ÎãàÎã§.\nÎ™®Îì† Ï§ëÍ∞Ñ Î™®Îç∏Í≥º ÏÇ¨ÌõÑÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î•º Ìï®Íªò Í≥µÍ∞úÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, ÏÇ¨Ïö©ÏûêÎäî Î≤†Ïù¥Ïä§ Î™®Îç∏ ÏúÑÏóê ÏûêÏã†ÎßåÏùò Îç∞Ïù¥ÌÑ∞Î•º ÏñπÏñ¥ ÏõêÌïòÎäî Î∞©Ìñ•ÏúºÎ°ú Í∞ïÌôîÌïôÏäµ¬∑ÌäúÎãùÏùÑ ÏßÑÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.\nüëâ \"Ï¢ãÏùÄ ÌïúÍµ≠Ïñ¥ Î™®Îç∏ÏùÑ Í∞ñÍ≥† Ïã∂Îã§Î©¥, Ïù¥Ï†ú ÏßÅÏ†ë ÎßåÎì§Ïñ¥Î≥¥ÏÑ∏Ïöî. ÏΩîÎû© Î¨¥Î£å GPUÎ°úÎèÑ ÌäúÎãùÎê©ÎãàÎã§! ü§ó\"\nüîó Links\nüìñ Technical Report: üëâ Paper , üëâ ÌïúÍµ≠Ïñ¥ ÏöîÏïΩppt\nü§ó Hugging Face: üëâ Model Download\nüíª GitHub Repository: üëâ Training and Inference Code\nüîâ Tutorial: üëâ Instruction Tuning over google colab üëâ Youtube Tutorial\nüìà Benchmark Performance\nüìä Quantitative Evaluation\nBenchmark\nKORMo-10B\nsmolLM3-3B\nolmo2-7B\nolmo2-13B\nkanana1.5-8B\nqwen3-8B\nllama3.1-8B\ngemma3-4B\ngemma3-12B\nüá∫üá∏ English Benchmarks\narc_challenge\n58.96\n55.55\n59.13\n61.01\n56.48\n63.82\n54.61\n53.58\n63.82\narc_easy\n85.48\n83.21\n85.06\n86.57\n82.74\n87.50\n84.01\n82.83\n87.37\nboolq\n83.46\n82.17\n84.50\n86.48\n84.53\n87.71\n81.87\n80.70\n86.61\ncopa\n93.00\n91.00\n92.00\n93.00\n88.00\n92.00\n93.00\n89.00\n95.00\ngpqa_main\n30.13\n26.79\n26.34\n29.24\n29.24\n30.13\n23.44\n30.13\n35.71\nhellaswag\n60.25\n56.78\n61.52\n65.02\n59.93\n59.54\n60.96\n57.56\n63.67\nmmlu\n67.96\n61.37\n62.81\n66.85\n63.73\n76.95\n65.03\n59.60\n73.58\nmmlu_global\n63.44\n57.52\n59.88\n63.99\n60.21\n75.05\n61.30\n57.23\n70.23\nmmlu_pro\n40.18\n34.94\n27.29\n32.50\n34.93\n56.58\n36.23\n27.79\n37.07\nmmlu_redux\n69.00\n62.95\n63.53\n68.37\n65.88\n78.19\n65.86\n60.86\n75.25\nopenbookqa\n39.00\n36.40\n39.00\n39.60\n36.80\n39.20\n39.00\n37.00\n40.20\npiqa\n81.12\n78.45\n80.79\n82.64\n80.30\n79.05\n80.90\n79.49\n82.59\nsocial_iqa\n52.81\n50.72\n55.89\n57.57\n57.01\n56.96\n53.12\n51.84\n56.45\nEnglish Avg.\n63.45\n59.83\n61.36\n64.06\n61.52\n67.90\n61.49\n59.05\n66.73\nüá∞üá∑ Korean Benchmarks\nclick\n55.29\n46.97\n37.79\n41.80\n62.76\n60.70\n49.22\n49.62\n62.21\ncsatqa\n38.00\n26.67\n19.33\n24.67\n44.67\n52.00\n28.67\n28.67\n31.33\nhaerae\n68.29\n55.82\n31.62\n37.58\n80.75\n67.19\n53.25\n60.68\n74.34\nk2_eval\n84.89\n75.23\n49.54\n63.43\n84.72\n84.72\n76.62\n76.39\n85.42\nkobest\n75.05\n69.13\n57.27\n59.02\n81.93\n80.05\n70.55\n69.33\n77.70\nkobalt\n22.86\n15.86\n11.43\n13.14\n26.29\n26.57\n17.43\n15.57\n23.86\nkmmlu\n46.48\n38.52\n33.05\n31.24\n48.86\n56.93\n40.75\n39.84\n51.60\nmmlu_global (ko)\n55.16\n44.15\n34.00\n36.95\n52.65\n61.95\n46.34\n46.33\n59.68\nkr_clinical_qa\n77.32\n53.97\n48.33\n46.22\n65.84\n80.00\n63.54\n60.00\n77.22\nKorean Avg.\n58.15\n47.37\n35.82\n39.34\n60.94\n63.35\n49.60\n49.60\n60.37\nüìù Qualitative Evaluation (LLM-as-a-Judge)\nBenchmark\nKORMo-10B\nsmolLM3-3B\nolmo2-7B\nolmo2-13B\nkanana1.5-8B\nqwen3-8B\nllama3.1-8B\nexaone3.5-8B\ngemma3-12B\nMT-Bench (EN)\n8.32\n7.15\n7.32\n7.64\n8.45\n8.70\n6.32\n8.15\n8.70\nKO-MT-Bench (KO)\n8.54\n-\n-\n-\n8.02\n8.16\n4.27\n8.13\n8.51\nLogicKor (KO)\n8.96\n-\n-\n-\n8.94\n8.63\n6.45\n9.20\n8.46\nAverage\n8.61\n-\n-\n-\n8.47\n8.50\n5.68\n8.49\n8.56\nüì¶ Installation\ngit clone https://github.com/MLP-Lab/KORMo-tutorial.git\ncd KORMo-tutorial\nbash setup/create_uv_venv.sh\nsource .venv_kormo/bin/activate\nüöÄ Inference Example\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_name = \"KORMo-Team/KORMo-10B-sft\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"What happens inside a black hole?\"}\n]\nchat_prompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False\n)\ninputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\nwith torch.inference_mode():\noutput_ids = model.generate(\n**inputs,\nmax_new_tokens=1024,\n)\nresponse = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\nprint(\"Assistant:\", response)\nüß† Enabling Thinking Mode\nIf you want to enable the thinking mode, simply set enable_thinking=True:\nchat_prompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True\n)\nLimitation\nThe model has not yet been safety-tuned or preference-aligned, which may lead to suboptimal performance or undesired repetitions in complex reasoning tasks.\nContact\nKyungTae Lim, Professor at KAIST. ktlim@kaist.ac.kr\nAcknowledgments\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (RS-2025-02653113, High-Performance Research AI Computing Infrastructure Support at the 2 PFLOPS Scale)\nCitation\n@misc{KORMo,\nauthor = {Minjun Kim, Hyeonseok Lim, Hangyeol Yoo, Inho Won, Seungwoo Song, Minkyung Cho, Junghun Yuk, Changsu Choi, Dongjae Shin, Huije Lee, Hoyun Song, Alice Oh, and KyungTae Lim},\ntitle = {KORMo: Korean Open Reasoning Model for Everyone},\nyear = {2025},\npublisher = {GitHub},\njournal = {Technical Report},\npaperLink = {\\url{https://arxiv.org/abs/2510.09426}},\n},\n}",
    "spooknik/CenKreChro-SVDQ": "Quality Evaluation\nThis repository contains Nunchaku-quantized (SVDQ) versions of CenKreChro, a text-to-image model based on Chroma and Flux Krea merged by TiwazM\nIf you like the model, please consider liking, reviewing and tipping the creator.\nModel Files\nsvdq-int4_r32-CenKreChro.safetensors: SVDQuant INT4 (rank 32) CenKreChro 1.0 model. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-CenKreChro.safetensors: SVDQuant NVFP4 (rank 32) CenKreChro 1.0 model. For users with Blackwell GPUs (50-series).\nQuality Evaluation\nBelow is the quality and similarity evaluated with 256 samples from MJHQ-30K dataset. (BF16 is the unqauntized model. INT W4A4 is INT4 and NVFP4 is FP4)\nModel\nPrecision\nMethod\nFID\nIR\nLPIPS\nPSNR\nCenKreChro (25 Steps)\nBF16\n--\n135.52\n0.842\n--\n--\nINT W4A4\nSVDQ\n135.50\n0.795\n0.255\n20.64\nNVFP4\nSVDQ\n134.19\n0.849\n0.219\n21.30\nIf you find these useful please consider:",
    "meituan-longcat/LongCat-Audio-Codec": "üó∫Ô∏è Framework\n‚ú® Key Features\nüì¶Resources\nüìö Citation\nüìú License\nüì© Contact\nLongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models\nWe are excited to introduce LongCat-Audio-Codec, an audio tokenizer and detokenizer solution designed for speech large language models. It works by generating semantic and acoustic tokens in parallel, enabling high-fidelity audio reconstruction at extremely low bitrates with excellent backend support for Speech LLM.\nThis repository hosts the resources for LongCat-Audio-Codec. For complete documentation, installation guides, and usage examples, please visit our GitHub Repository.\nüó∫Ô∏è Framework\n‚ú® Key Features\nHigh Fidelity at Ultra-Low Bitrates: As a codec, it achieves high-intelligibility audio reconstruction at extremely low bitrates.\nLow-Frame-Rate Tokenizer: As a tokenizer, it extracts semantic and acoustic tokens in parallel at a low frame rate of 16.6Hz, with flexible acoustic codebook configurations to adapt to different downstream tasks.\nLow-Latency Streaming Detokenizer: Equipped with a specially designed streaming detokenizer that requires minimal future information to deliver high-quality audio output with low latency.\nSuper-Resolution Capability: Integrates audio super-resolution processing into the detokenizer, enabling the generation of high-quality audio with a higher sample rate if the sample rate of original input is lower than 24khz.\nüì¶Resources\nResourcess\nNotes\nLongCatAudioCodec_encoder\nencoder weights of LongCat-Audio-Codec with semantic encoder and acoustic encoder\nLongCatAudioCodec_encoder_cmvn\ncoefficients of Cepstral Mean and Variance Normalization, used by encoder\nLongCatAudioCodec_decoder16k_4codebooks\nnative 16k decoder, supply 1 semantic codebook and at most 3 acoustic codebooks\nLongCatAudioCodec_decoder24k_2codebooks\nsuper-resolution 24k decoder, supply 1 semantic codebook and 1 acoustic codebook\nLongCatAudioCodec_decoder24k_4codebooks\nsuper-resolution 24k decoder, supply 1 semantic codebook and at most 3 acoustic codebooks\nüìö Citation\nIf you find our work useful in your research, please consider citing:\n@article{longcataudiocodec,\ntitle={LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models},\nauthor={Xiaohan Zhao, Hongyu Xiang, Shengze Ye, Song Li, Zhengkun Tian, Guanyu Chen, Ke Ding, Guanglu Wan},\njournal={arXiv preprint arXiv:2510.15227},\norganization={LongCat Team, Meituan},\nyear={2025}\n}\nüìú License\nThe code and models in this repository are released under the MIT License. This grants you broad permissions to use, copy, modify, and distribute the software, provided you include the original copyright notice. We claim no ownership over any content you generate using these models.\nThe software is provided \"AS IS\", without any warranty. You are fully accountable for your use of the models. Your usage must not involve creating or sharing any content that violates applicable laws, causes harm to individuals, disseminates personal information with harmful intent, spreads misinformation, or targets vulnerable groups.\nüì© Contact\nPlease contact us at longcat-team@meituan.com or open an issue if you have any questions.",
    "spooknik/PixelWave-SVDQ": "Quality Evaluation\nThis repository contains Nunchaku-quantized (SVDQ) versions of PixelWave, a text-to-image model based on Flux.1 Dev by humblemikey\nModel Files\nsvdq-int4_r32-pixelwave-schnell-04.safetensors: SVDQuant INT4 (rank 32) PixelWave FLUX.1-schnell 04. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-pixelwave-schnell-04.safetensors: SVDQuant NVFP4 (rank 32) PixelWave FLUX.1-schnell 04. For users with Blackwell GPUs (50-series).\nQuality Evaluation\nBelow is the quality and similarity evaluated with 256 samples from MJHQ-30K dataset. (BF16 is the unqauntized model. INT W4A4 is INT4 and NVFP4 is FP4)\nModel\nPrecision\nMethod\nFID\nIR\nLPIPS\nPSNR\nPixelWave schnell 04\nBF16\n--\n176.37\n0.813\n--\n--\n(8 step)\nINT W4A4\nSVDQ\n176.68\n0.820\n0.322\n17.12\nNVFP4\nSVDQ\n176.91\n0.839\n0.298\n17.70\nIf you find these useful please consider:",
    "aipmaster/krea-lora": "Lora of https://huggingface.co/krea/krea-realtime-video",
    "Intel/GLM-4.6-gguf-q2ks-mixed-AutoRound": "Model Details\nHow To Use\nGenerate the model\nEthical Considerations and Limitations\nCaveats and Recommendations\nDisclaimer\nCite\nModel Details\nThis model is a mixed gguf q2ks format of  zai-org/GLM-4.6  generated by intel/auto-round algorithm.  Embedding layer and lm-head layer are fallback to 8 bits and non expert layers are fallback to 4 bits. Please refer to Section Generate the model for more details.\nPlease follow the license of the original model.\nHow To Use\nLlamacpp inference\n/llama-cli -hf Intel/GLM-4.6-gguf-q2ks-mixed-AutoRound\n> Write a quick sort algorithm.\n<think>The user asks: \"Write a quick sort algorithm.\"\\n\\nWe need to output a quick sort algorithm. The request is a standard programming request, no disallowed content. So we can comply.\\n\\nWe need to produce code for quick sort. It could be in various language: Python, Java, C, etc. We can choose a language. Usually, quick sort is a classic algorithm.\\n\\nWe can provide a straightforward implementation with recursion. The user didn\\'t specify language, but we can provide Python implementation (most common). Also we can discuss the algorithm.\\n\\nGiven the request, we can comply and produce a code snippet.\\n\\nWe might also include explanation.\\n\\nThus answer: Provide quick sort algorithm in Python.\\n\\nWill also include typical implementation with in-place partition.\\n\\nWe can also discuss complexity.\\n\\nThus we will answer accordingly.\\n\\nWe must not mention policy or guidelines.\\n\\nHence produce code and explanation.\\n\\nPotentially also provide alternative implementation.\\n\\nThus final answer will be a quick sort algorithm.\\n\\nWe might also provide a version for generic types.\\n\\nLet\\'s produce Python code:\\n\\n\\n\\nAlternatively, in-place version:\\n\\n\\n\\nWe can also provide partition function.\\n\\nThus final answer.\\n\\nGiven the instruction to be brief but thorough, we can provide both.\\n\\nThus answer: Provide quick sort algorithm.\\n\\nWe must not output any disallowed content.\\n\\nThus final answer is safe.\\n\\nNow produce the final.</think>\nBelow is a simple, self‚Äëcontained implementation of the Quick‚ÄëSort algorithm in Python. Two versions are shown:\n* **A functional (non‚Äëin‚Äëplace) version** that builds new lists for the recursive calls.\n* **An in‚Äëplace version** that sorts the original list by swapping elements around a pivot.\nBoth have the same expected time‚Äëcomplexity:‚ÄØ*O(n‚ÄØlog‚ÄØn)* on average (and *O(n¬≤)* in the worst case, e.g., when the pivot is always the smallest or largest element).\n---\n## 1Ô∏è ‚É£ Functional (non‚Äëin‚Äëplace) Quick‚ÄëSort\n\n**Explanation**\n1. **Base case** ‚Äì If the list has 0 or 1 elements, it‚Äôs already sorted.\n2. **Pivot selection** ‚Äì Here we simply pick the first element; you could also use a random element or ‚Äúmedian‚Äëof‚Äëthree‚Äù for better performance on pathological inputs.\n3. **Partition** ‚Äì Two list comprehensions split the remaining items into those ‚â§‚ÄØpivot and those >‚ÄØpivot.\n4. **Recursive sorting** ‚Äì The two sub‚Äëlists are sorted recursively, then combined with the pivot to produce the final sorted list.\n---\n## 2Ô∏è ‚É£ In‚ÄëPlace Quick‚ÄëSort (Lomuto partition scheme)\n\n**Explanation**\n* **`partition`** ‚Äì The Lomuto scheme picks the last element as the pivot. It walks the sub‚Äëarray, moving any element ‚â§‚ÄØpivot to the front. After the loop, the pivot is swapped into its correct final position, and its index (`i+1`) is returned.\n* **Recursion** ‚Äì The list is sorted recursively on the two halves separated by the pivot.\n---\n## 3Ô∏è ‚É£ Tips & Variations\n| Variation | Why use it? |\n|-----------|-------------|\n| **Random pivot** (`pivot = arr[random.randint(low, high)]`) | Reduces chance of worst‚Äëcase *O(n¬≤)* on already sorted inputs. |\n| **Median‚Äëof‚Äëthree pivot** (choose median of first, middle, last) | Improves partition balance for many real‚Äëworld datasets. |\n| **Tail‚Äëcall optimization** (recurse on the smaller side, loop on the larger) | Keeps recursion depth ‚â§‚ÄØlog‚ÄØn, preventing stack overflow on large lists. |\n| **Hybrid with Insertion Sort** (switch to insertion sort for sub‚Äëarrays ‚â§‚ÄØ10 elements) | Quick‚ÄëSort overhead is high for tiny partitions; insertion sort is faster there. |\n---\n### TL;DR\n* Use the **functional version** when you want a concise, immutable‚Äëstyle implementation or you‚Äôre dealing with small data where extra list allocations are cheap.\n* Use the **in‚Äëplace version** when you need to sort large data without extra memory allocation.\nBoth achieve the classic Quick‚ÄëSort behavior: fast average performance, elegant divide‚Äëand‚Äëconquer recursion, and a simple pivot‚Äëbased partition step. Happy sorting!\nGenerate the model\nHere is the sample command to reproduce the model\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRound\nmodel_name = \"zai-org/GLM-4.6\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\ndevice_map=\"cpu\", torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nlayer_config = {}\nfor n, m in model.named_modules():\nif n == \"lm_head\" or isinstance(m,torch.nn.Embedding):\nlayer_config[n] = {\"bits\": 8}\nelif isinstance(m, torch.nn.Linear) and (not \"expert\" in n or \"shared_experts\" in n) and n != \"lm_head\":\nlayer_config[n] = {\"bits\": 4}\nautoround = AutoRound(model, tokenizer, iters=0, layer_config=layer_config, nsamples=512)\nautoround.quantize_and_save(\"tmp_autoround\", format=\"gguf:q2_k_s\")\nEthical Considerations and Limitations\nThe model can produce factually incorrect output, and should not be relied on to produce factually accurate information. Because of the limitations of the pretrained model and the finetuning datasets, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\nTherefore, before deploying any applications of the model, developers should perform safety testing.\nCaveats and Recommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nHere are a couple of useful links to learn more about Intel's AI software:\nIntel Neural Compressor link\nDisclaimer\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please consult an attorney before using this model for commercial purposes.\nCite\n@article{cheng2023optimize, title={Optimize weight rounding via signed gradient descent for the quantization of llms}, author={Cheng, Wenhua and Zhang, Weiwei and Shen, Haihao and Cai, Yiyang and He, Xin and Lv, Kaokao and Liu, Yi}, journal={arXiv preprint arXiv:2309.05516}, year={2023} }\narxiv github",
    "huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF": "huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF\nGGUF\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF\nThis is an uncensored version of Qwen/Qwen3-VL-235B-A22B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp-tr-qwen3-vl-3-b6981-ab45b1a now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nhuggingface-cli download huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF --local-dir ./huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF --token xxx\nllama-gguf-split --merge huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF/GGUF/Q2_K/Q2_K-00001-of-00002.gguf huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF/GGUF/Q2_K.gguf\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF/GGUF/Q2_K.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF/GGUF/mmproj-ggml-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-235B-A22B-Instruct-abliterated-GGUF/GGUF/Q2_K.gguf -c 40960\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated": "huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-2B-Thinking created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nThe model has been updated, and the results when testing gguf are not very good. Those who have already\ndownloaded the file can download it again.\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-2B-Thinking-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "Vortex5/Crimson-Twilight-12B": "Crimson Twilight 12B\n01 // Overview\nCrimson-Twilight-12B is a multistage merge designed for narrative roleplay.\nBuilt with MergeKit.\n02 // Merge Step 1\nAbyssal-Seraph-12B\nmerges with\nLunar-Abyss-12B\nusing nearswap (t=0.0008).\nShow YAML\nname: First\nmodels:\n- model: Vortex5/Abyssal-Seraph-12B\nmerge_method: nearswap\nbase_model: Vortex5/Lunar-Abyss-12B\nparameters:\nt: 0.0008\ndtype: bfloat16\n03 // Merge Step 2\nMoonlit-Shadow-12B\nmerges with\nLuminous-Shadow-12B\nvia slerp (t=0.5).\nShow YAML\nname: Second\nmodels:\n- model: Vortex5/Moonlit-Shadow-12B\nmerge_method: slerp\nbase_model: Vortex5/Luminous-Shadow-12B\nparameters:\nt: 0.5\ndtype: bfloat16\n04 // Final Merge\nThe two intermediates are then merged via the Karcher mean method.\nShow YAML\nmodels:\n- model: First\n- model: Second\nmerge_method: karcher\ndtype: bfloat16\nparameters:\ntol: 1e-9\nmax_iter: 35000\ntokenizer:\nsource: union\n05 // Acknowledgments\nTeam Mradermacher ‚Äî Static & imatrix quantizations\nDeathGodlike ‚Äî EXL3 quants",
    "Soul-AILab/SoulX-Podcast-1.7B": "README.md exists but content is empty.",
    "dslim/bert-base-NER": "bert-base-NER\nModel description\nAvailable NER models\nIntended uses & limitations\nTraining data\nCoNLL-2003 English Dataset Statistics\nTraining procedure\nEval results\nBibTeX entry and citation info\nbert-base-NER\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\nModel description\nbert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\nSpecifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\nIf you'd like to use a larger BERT-large model fine-tuned on the same dataset, a bert-large-NER version is also available.\nAvailable NER models\nModel Name\nDescription\nParameters\ndistilbert-NER (NEW!)\nFine-tuned DistilBERT - a smaller, faster, lighter version of BERT\n66M\nbert-large-NER\nFine-tuned bert-large-cased - larger model with slightly better performance\n340M\nbert-base-NER-(uncased)\nFine-tuned bert-base, available in both cased and uncased versions\n110M\nIntended uses & limitations\nHow to use\nYou can use this model with Transformers pipeline for NER.\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\nner_results = nlp(example)\nprint(ner_results)\nLimitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.\nTraining data\nThis model was fine-tuned on English version of the standard CoNLL-2003 Named Entity Recognition dataset.\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation\nDescription\nO\nOutside of a named entity\nB-MISC\nBeginning of a miscellaneous entity right after another miscellaneous entity\nI-MISC\nMiscellaneous entity\nB-PER\nBeginning of a person‚Äôs name right after another person‚Äôs name\nI-PER\nPerson‚Äôs name\nB-ORG\nBeginning of an organization right after another organization\nI-ORG\norganization\nB-LOC\nBeginning of a location right after another location\nI-LOC\nLocation\nCoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper.\n# of training examples per entity type\nDataset\nLOC\nMISC\nORG\nPER\nTrain\n7140\n3438\n6321\n6600\nDev\n1837\n922\n1341\n1842\nTest\n1668\n702\n1661\n1617\n# of articles/sentences/tokens per dataset\nDataset\nArticles\nSentences\nTokens\nTrain\n946\n14,987\n203,621\nDev\n216\n3,466\n51,362\nTest\n231\n3,684\n46,435\nTraining procedure\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the original BERT paper which trained & evaluated the model on CoNLL-2003 NER task.\nEval results\nmetric\ndev\ntest\nf1\n95.1\n91.3\nprecision\n95.0\n90.7\nrecall\n95.3\n91.9\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results here.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-1810-04805,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitle     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njournal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/abs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\ntitle = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\nauthor = \"Tjong Kim Sang, Erik F.  and\nDe Meulder, Fien\",\nbooktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\nyear = \"2003\",\nurl = \"https://www.aclweb.org/anthology/W03-0419\",\npages = \"142--147\",\n}",
    "google/vit-base-patch16-224": "Vision Transformer (base-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nVision Transformer (base-sized model)\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nFor more code examples, we refer to the documentation.\nTraining data\nThe ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, and fine-tuned on ImageNet, a dataset consisting of 1 million images and 1k classes.\nTraining procedure\nPreprocessing\nThe exact details of preprocessing of images during training/validation can be found here.\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\nPretraining\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\nEvaluation results\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\nBibTeX entry and citation info\n@misc{wu2020visual,\ntitle={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\nauthor={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\nyear={2020},\neprint={2006.03677},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@inproceedings{deng2009imagenet,\ntitle={Imagenet: A large-scale hierarchical image database},\nauthor={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\nbooktitle={2009 IEEE conference on computer vision and pattern recognition},\npages={248--255},\nyear={2009},\norganization={Ieee}\n}",
    "openai/clip-vit-base-patch32": "Model Card: CLIP\nModel Details\nModel Date\nModel Type\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nOut-of-Scope Use Cases\nData\nData Mission Statement\nPerformance and Limitations\nPerformance\nLimitations\nBias and Fairness\nFeedback\nWhere to send questions or comments about the model\nModel Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.\nModel Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they‚Äôre being deployed within.\nModel Date\nJanuary 2021\nModel Type\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\nDocuments\nBlog Post\nCLIP Paper\nUse with Transformers\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nOut-of-Scope Use Cases\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP‚Äôs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\nData\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as YFCC100M. A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\nData Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\nPerformance and Limitations\nPerformance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanford Cars\nFGVC Aircraft\nVOC2007\nDTD\nOxford-IIIT Pet dataset\nCaltech101\nFlowers102\nMNIST\nSVHN\nIIIT5K\nHateful Memes\nSST-2\nUCF101\nKinetics700\nCountry211\nCLEVR Counting\nKITTI Distance\nSTL-10\nRareAct\nFlickr30\nMSCOCO\nImageNet\nImageNet-A\nImageNet-R\nImageNet Sketch\nObjectNet (ImageNet Overlap)\nYoutube-BB\nImageNet-Vid\nLimitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\nBias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from Fairface into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‚ÄòMiddle Eastern‚Äô having the highest accuracy (98.4%) and ‚ÄòWhite‚Äô having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\nFeedback\nWhere to send questions or comments about the model\nPlease use this Google Form",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nFull Model Architecture\nCiting & Authors\nsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nThis model was trained by sentence-transformers.\nIf you find this model helpful, feel free to cite our publication Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks:\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"http://arxiv.org/abs/1908.10084\",\n}",
    "CompVis/stable-diffusion-v1-4": "Stable Diffusion v1-4 Model Card\nModel Details\nExamples\nPyTorch\nJAX/Flax\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nSafety Module\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v1-4 Model Card\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at ü§ó's Stable Diffusion with üß®Diffusers blog.\nThe Stable-Diffusion-v1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2\ncheckpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.\nThis weights here are intended to be used with the üß® Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, come here\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (CLIP ViT-L/14) as suggested in the Imagen paper.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nWe recommend using ü§ó's Diffusers library to run Stable Diffusion.\nPyTorch\npip install --upgrade diffusers transformers scipy\nRunning the pipeline with the default PNDM scheduler:\nimport torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nNote:\nIf you are limited by GPU memory and have less than 4GB of GPU RAM available, please make sure to load the StableDiffusionPipeline in float16 precision instead of the default float32 precision as done above. You can do so by telling diffusers to expect the weights to be in float16 precision:\nimport torch\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\npipe.enable_attention_slicing()\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nTo swap out the noise scheduler, pass it to from_pretrained:\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nJAX/Flax\nTo use StableDiffusion on TPUs and GPUs for faster inference you can leverage JAX/Flax.\nRunning the pipeline with default PNDMScheduler\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n\"CompVis/stable-diffusion-v1-4\", revision=\"flax\", dtype=jax.numpy.bfloat16\n)\nprompt = \"a photo of an astronaut riding a horse on mars\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nNote:\nIf you are limited by TPU memory, please make sure to load the FlaxStableDiffusionPipeline in bfloat16 precision instead of the default float32 precision as done above. You can do so by telling diffusers to load the weights from \"bf16\" branch.\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n\"CompVis/stable-diffusion-v1-4\", revision=\"bf16\", dtype=jax.numpy.bfloat16\n)\nprompt = \"a photo of an astronaut riding a horse on mars\"\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and\ntasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is taken from the DALLE-MINI model card, but applies in the same way to Stable Diffusion v1.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a large-scale dataset\nLAION-5B which contains adult material\nand is not fit for product use without additional safety mechanisms and\nconsiderations.\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\nThe training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion v1 was trained on subsets of LAION-2B(en),\nwhich consists of images that are primarily limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nSafety Module\nThe intended use of this model is with the Safety Checker in Diffusers.\nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the CLIPTextModel after generation of the images.\nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-2B (en) and subsets thereof (see next section)\nTraining Procedure\nStable Diffusion v1-4 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through a ViT-L/14 text-encoder.\nThe non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\nWe currently provide four checkpoints, which were trained as follows.\nstable-diffusion-v1-1: 237,000 steps at resolution 256x256 on laion2B-en.\n194,000 steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).\nstable-diffusion-v1-2: Resumed from stable-diffusion-v1-1.\n515,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator).\nstable-diffusion-v1-3: Resumed from stable-diffusion-v1-2. 195,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nstable-diffusion-v1-4 Resumed from stable-diffusion-v1-2.225,000 steps at resolution 512x512 on \"laion-aesthetics v2 5+\"  and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 2\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 150000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 11250 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach and Patrick Esser and is based on the DALL-E Mini model card.",
    "openai/whisper-small": "Whisper\nModel details\nUsage\nTranscription\nEnglish to English\nFrench to French\nTranslation\nFrench to English\nEvaluation\nLong-Form Transcription\nFine-Tuning\nEvaluated Use\nTraining Data\nPerformance and Limitations\nBroader Implications\nBibTeX entry and citation info\nWhisper\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours\nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need\nfor fine-tuning.\nWhisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision\nby Alec Radford et al from OpenAI. The original code repository can be found here.\nDisclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were\ncopied and pasted from the original model card.\nModel details\nWhisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.\nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.\nThe models were trained on either English-only data or multilingual data. The English-only models were trained\non the task of speech recognition. The multilingual models were trained on both speech recognition and speech\ntranslation. For speech recognition, the model predicts transcriptions in the same language as the audio.\nFor speech translation, the model predicts transcriptions to a different language to the audio.\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints\nare available on the Hugging Face Hub. The\ncheckpoints are summarised in the following table with links to the models on the Hub:\nSize\nParameters\nEnglish-only\nMultilingual\ntiny\n39 M\n‚úì\n‚úì\nbase\n74 M\n‚úì\n‚úì\nsmall\n244 M\n‚úì\n‚úì\nmedium\n769 M\n‚úì\n‚úì\nlarge\n1550 M\nx\n‚úì\nlarge-v2\n1550 M\nx\n‚úì\nUsage\nTo transcribe audio samples, the model has to be used alongside a WhisperProcessor.\nThe WhisperProcessor is used to:\nPre-process the audio inputs (converting them to log-Mel spectrograms for the model)\nPost-process the model outputs (converting them from tokens to text)\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens\nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\nThe transcription always starts with the <|startoftranscript|> token\nThe second token is the language token (e.g. <|en|> for English)\nThe third token is the \"task token\". It can take one of two values: <|transcribe|> for speech recognition or <|translate|> for speech translation\nIn addition, a <|notimestamps|> token is added if the model should not include timestamp prediction\nThus, a typical sequence of context tokens might look as follows:\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at\neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced,\nthe Whisper model will automatically predict the output langauge and task itself.\nThe context tokens can be set accordingly:\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\nWhich forces the model to predict in English under the task of speech recognition.\nTranscription\nEnglish to English\nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> model.config.forced_decoder_ids = None\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\nThe context tokens can be removed from the start of the transcription by setting skip_special_tokens=True.\nFrench to French\nThe following example demonstrates French to French transcription by setting the decoder ids appropriately.\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail int√©ressant va enfin √™tre men√© sur ce sujet.<|endoftext|>']\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail int√©ressant va enfin √™tre men√© sur ce sujet.']\nTranslation\nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\nFrench to English\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\nEvaluation\nThis code snippet shows how to evaluate Whisper Small on LibriSpeech test-clean:\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda\")\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>>\n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n>>> result = librispeech_test_clean.map(map_to_pred)\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n3.432213777886737\nLong-Form Transcription\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking\nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers\npipeline\nmethod. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline\ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing return_timestamps=True:\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-small\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n'timestamp': (0.0, 5.44)}]\nRefer to the blog post ASR Chunking for more details on the chunking algorithm.\nFine-Tuning\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However,\nits predictive capabilities can be improved further for certain languages and tasks through fine-tuning. The blog\npost Fine-Tune Whisper with ü§ó Transformers provides a step-by-step\nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\nEvaluated Use\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only ‚Äúintended‚Äù uses or to draw reasonable guidelines around what is or is not research.\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\nTraining Data\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.\nAs discussed in the accompanying paper, we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\nPerformance and Limitations\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in the paper accompanying this release.\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in the paper. It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\nBroader Implications\nWe anticipate that Whisper models‚Äô transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box ‚Äì their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\nBibTeX entry and citation info\n@misc{radford2022whisper,\ndoi = {10.48550/ARXIV.2212.04356},\nurl = {https://arxiv.org/abs/2212.04356},\nauthor = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\ntitle = {Robust Speech Recognition via Large-Scale Weak Supervision},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {arXiv.org perpetual, non-exclusive license}\n}",
    "google/flan-t5-base": "Model Card for FLAN-T5 base\nTable of Contents\nTL;DR\nModel Details\nModel Description\nUsage\nUsing the Pytorch model\nRunning the model on a CPU\nRunning the model on a GPU\nRunning the model on a GPU using different precisions\nUses\nDirect Use and Downstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nEthical considerations and risks\nKnown Limitations\nSensitive Use:\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nEnvironmental Impact\nCitation\nModel Recycling\nModel Card for FLAN-T5 base\nTable of Contents\nTL;DR\nModel Details\nUsage\nUses\nBias, Risks, and Limitations\nTraining Details\nEvaluation\nEnvironmental Impact\nCitation\nModel Card Authors\nTL;DR\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.\nAs mentioned in the first few lines of the abstract :\nFlan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\nDisclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.\nModel Details\nModel Description\nModel type: Language model\nLanguage(s) (NLP): English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\nLicense: Apache 2.0\nRelated Models: All FLAN-T5 Checkpoints\nOriginal Checkpoints: All Original FLAN-T5 Checkpoints\nResources for more information:\nResearch paper\nGitHub Repo\nHugging Face FLAN-T5 Docs (Similar to T5)\nUsage\nFind below some example scripts on how to use the model in transformers:\nUsing the Pytorch model\nRunning the model on a CPU\nClick to expand\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU\nClick to expand\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nFP16\nClick to expand\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", torch_dtype=torch.float16)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nINT8\nClick to expand\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", load_in_8bit=True)\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\nUses\nDirect Use and Downstream Use\nThe authors write in the original paper's model card that:\nThe primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\nSee the research paper for further details.\nOut-of-Scope Use\nMore information needed.\nBias, Risks, and Limitations\nThe information below in this section are copied from the model's official model card:\nLanguage models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\nEthical considerations and risks\nFlan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\nKnown Limitations\nFlan-T5 has not been tested in real world applications.\nSensitive Use:\nFlan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\nTraining Details\nTraining Data\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\nTraining Procedure\nAccording to the model card from the original paper:\nThese models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\nThe model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax.\nEvaluation\nTesting Data, Factors & Metrics\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\nFor full details, please check the research paper.\nResults\nFor full results for FLAN-T5-Base, see the research paper, Table 3.\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips ‚â• 4.\nHours used: More information needed\nCloud Provider: GCP\nCompute Region: More information needed\nCarbon Emitted: More information needed\nCitation\nBibTeX:\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\ndoi = {10.48550/ARXIV.2210.11416},\nurl = {https://arxiv.org/abs/2210.11416},\nauthor = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\nkeywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {Scaling Instruction-Finetuned Language Models},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}\nModel Recycling\nEvaluation on 36 datasets using google/flan-t5-base as a base model yields average score of 77.98 in comparison to 68.82 by google/t5-v1_1-base.\nThe model is ranked 1st among all tested models for the google/t5-v1_1-base architecture as of 06/02/2023\nResults:\n20_newsgroup\nag_news\namazon_reviews_multi\nanli\nboolq\ncb\ncola\ncopa\ndbpedia\nesnli\nfinancial_phrasebank\nimdb\nisear\nmnli\nmrpc\nmultirc\npoem_sentiment\nqnli\nqqp\nrotten_tomatoes\nrte\nsst2\nsst_5bins\nstsb\ntrec_coarse\ntrec_fine\ntweet_ev_emoji\ntweet_ev_emotion\ntweet_ev_hate\ntweet_ev_irony\ntweet_ev_offensive\ntweet_ev_sentiment\nwic\nwnli\nwsc\nyahoo_answers\n86.2188\n89.6667\n67.12\n51.9688\n82.3242\n78.5714\n80.1534\n75\n77.6667\n90.9507\n85.4\n93.324\n72.425\n87.2457\n89.4608\n62.3762\n82.6923\n92.7878\n89.7724\n89.0244\n84.8375\n94.3807\n57.2851\n89.4759\n97.2\n92.8\n46.848\n80.2252\n54.9832\n76.6582\n84.3023\n70.6366\n70.0627\n56.338\n53.8462\n73.4\nFor more information, see: Model Recycling",
    "ggerganov/whisper.cpp": "OpenAI's Whisper models converted to ggml format for use with whisper.cpp\nOpenAI's Whisper models converted to ggml format for use with whisper.cpp\nAvailable models\nModel\nDisk\nSHA\ntiny\n75 MiB\nbd577a113a864445d4c299885e0cb97d4ba92b5f\ntiny-q5_1\n31 MiB\n2827a03e495b1ed3048ef28a6a4620537db4ee51\ntiny-q8_0\n42 MiB\n19e8118f6652a650569f5a949d962154e01571d9\ntiny.en\n75 MiB\nc78c86eb1a8faa21b369bcd33207cc90d64ae9df\ntiny.en-q5_1\n31 MiB\n3fb92ec865cbbc769f08137f22470d6b66e071b6\ntiny.en-q8_0\n42 MiB\n802d6668e7d411123e672abe4cb6c18f12306abb\nbase\n142 MiB\n465707469ff3a37a2b9b8d8f89f2f99de7299dac\nbase-q5_1\n57 MiB\na3733eda680ef76256db5fc5dd9de8629e62c5e7\nbase-q8_0\n78 MiB\n7bb89bb49ed6955013b166f1b6a6c04584a20fbe\nbase.en\n142 MiB\n137c40403d78fd54d454da0f9bd998f78703390c\nbase.en-q5_1\n57 MiB\nd26d7ce5a1b6e57bea5d0431b9c20ae49423c94a\nbase.en-q8_0\n78 MiB\nbb1574182e9b924452bf0cd1510ac034d323e948\nsmall\n466 MiB\n55356645c2b361a969dfd0ef2c5a50d530afd8d5\nsmall-q5_1\n181 MiB\n6fe57ddcfdd1c6b07cdcc73aaf620810ce5fc771\nsmall-q8_0\n252 MiB\nbcad8a2083f4e53d648d586b7dbc0cd673d8afad\nsmall.en\n466 MiB\ndb8a495a91d927739e50b3fc1cc4c6b8f6c2d022\nsmall.en-q5_1\n181 MiB\n20f54878d608f94e4a8ee3ae56016571d47cba34\nsmall.en-q8_0\n252 MiB\n9d75ff4ccfa0a8217870d7405cf8cef0a5579852\nsmall.en-tdrz\n465 MiB\nb6c6e7e89af1a35c08e6de56b66ca6a02a2fdfa1\nmedium\n1.5 GiB\nfd9727b6e1217c2f614f9b698455c4ffd82463b4\nmedium-q5_0\n514 MiB\n7718d4c1ec62ca96998f058114db98236937490e\nmedium-q8_0\n785 MiB\ne66645948aff4bebbec71b3485c576f3d63af5d6\nmedium.en\n1.5 GiB\n8c30f0e44ce9560643ebd10bbe50cd20eafd3723\nmedium.en-q5_0\n514 MiB\nbb3b5281bddd61605d6fc76bc5b92d8f20284c3b\nmedium.en-q8_0\n785 MiB\nb1cf48c12c807e14881f634fb7b6c6ca867f6b38\nlarge-v1\n2.9 GiB\nb1caaf735c4cc1429223d5a74f0f4d0b9b59a299\nlarge-v2\n2.9 GiB\n0f4c8e34f21cf1a914c59d8b3ce882345ad349d6\nlarge-v2-q5_0\n1.1 GiB\n00e39f2196344e901b3a2bd5814807a769bd1630\nlarge-v2-q8_0\n1.5 GiB\nda97d6ca8f8ffbeeb5fd147f79010eeea194ba38\nlarge-v3\n2.9 GiB\nad82bf6a9043ceed055076d0fd39f5f186ff8062\nlarge-v3-q5_0\n1.1 GiB\ne6e2ed78495d403bef4b7cff42ef4aaadcfea8de\nlarge-v3-turbo\n1.5 GiB\n4af2b29d7ec73d781377bfd1758ca957a807e941\nlarge-v3-turbo-q5_0\n547 MiB\ne050f7970618a659205450ad97eb95a18d69c9ee\nlarge-v3-turbo-q8_0\n834 MiB\n01bf15bedffe9f39d65c1b6ff9b687ea91f59e0e\nFor more information, visit:\nhttps://github.com/ggerganov/whisper.cpp/tree/master/models",
    "suno/bark": "Bark\nExample\nü§ó Transformers Usage\nSuno Usage\nModel Details\nText to semantic tokens\nSemantic to coarse tokens\nCoarse to fine tokens\nArchitecture\nRelease date\nBroader Implications\nBark\nBark is a transformer-based text-to-audio model created by Suno.\nBark can generate highly realistic, multilingual speech as well as other audio - including music,\nbackground noise and simple sound effects. The model can also produce nonverbal\ncommunications like laughing, sighing and crying. To support the research community,\nwe are providing access to pretrained model checkpoints ready for inference.\nThe original github repo and model card can be found here.\nThis model is meant for research purposes only.\nThe model output is not censored and the authors do not endorse the opinions in the generated content.\nUse at your own risk.\nTwo checkpoints are released:\nsmall\nlarge (this checkpoint)\nExample\nTry out Bark yourself!\nBark Colab:\nHugging Face Colab:\nHugging Face Demo:\nü§ó Transformers Usage\nYou can run Bark locally with the ü§ó Transformers library from version 4.31.0 onwards.\nFirst install the ü§ó Transformers library and scipy:\npip install --upgrade pip\npip install --upgrade transformers scipy\nRun inference via the Text-to-Speech (TTS) pipeline. You can infer the bark model via the TTS pipeline in just a few lines of code!\nfrom transformers import pipeline\nimport scipy\nsynthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\nscipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\nRun inference via the Transformers modelling code. You can use the processor + generate code to convert text into a mono 24 kHz speech waveform for more fine-grained control.\nfrom transformers import AutoProcessor, AutoModel\nprocessor = AutoProcessor.from_pretrained(\"suno/bark\")\nmodel = AutoModel.from_pretrained(\"suno/bark\")\ninputs = processor(\ntext=[\"Hello, my name is Suno. And, uh ‚Äî and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\"],\nreturn_tensors=\"pt\",\n)\nspeech_values = model.generate(**inputs, do_sample=True)\nListen to the speech samples either in an ipynb notebook:\nfrom IPython.display import Audio\nsampling_rate = model.generation_config.sample_rate\nAudio(speech_values.cpu().numpy().squeeze(), rate=sampling_rate)\nOr save them as a .wav file using a third-party library, e.g. scipy:\nimport scipy\nsampling_rate = model.config.sample_rate\nscipy.io.wavfile.write(\"bark_out.wav\", rate=sampling_rate, data=speech_values.cpu().numpy().squeeze())\nFor more details on using the Bark model for inference using the ü§ó Transformers library, refer to the Bark docs.\nSuno Usage\nYou can also run Bark locally through the original Bark library:\nFirst install the bark library\nRun the following Python code:\nfrom bark import SAMPLE_RATE, generate_audio, preload_models\nfrom IPython.display import Audio\n# download and load all models\npreload_models()\n# generate audio from text\ntext_prompt = \"\"\"\nHello, my name is Suno. And, uh ‚Äî and I like pizza. [laughs]\nBut I also have other interests such as playing tic tac toe.\n\"\"\"\nspeech_array = generate_audio(text_prompt)\n# play text in notebook\nAudio(speech_array, rate=SAMPLE_RATE)\npizza.webm\nTo save audio_array as a WAV file:\nfrom scipy.io.wavfile import write as write_wav\nwrite_wav(\"/path/to/audio.wav\", SAMPLE_RATE, audio_array)\nModel Details\nThe following is additional information about the models released here.\nBark is a series of three transformer models that turn text into audio.\nText to semantic tokens\nInput: text, tokenized with BERT tokenizer from Hugging Face\nOutput: semantic tokens that encode the audio to be generated\nSemantic to coarse tokens\nInput: semantic tokens\nOutput: tokens from the first two codebooks of the EnCodec Codec from facebook\nCoarse to fine tokens\nInput: the first two codebooks from EnCodec\nOutput: 8 codebooks from EnCodec\nArchitecture\nModel\nParameters\nAttention\nOutput Vocab size\nText to semantic tokens\n80/300 M\nCausal\n10,000\nSemantic to coarse tokens\n80/300 M\nCausal\n2x 1,024\nCoarse to fine tokens\n80/300 M\nNon-causal\n6x 1,024\nRelease date\nApril 2023\nBroader Implications\nWe anticipate that this model's text to audio capabilities can be used to improve accessbility tools in a variety of languages.\nWhile we hope that this release will enable users to express their creativity and build applications that are a force\nfor good, we acknowledge that any text to audio model has the potential for dual use. While it is not straightforward\nto voice clone known people with Bark, it can still be used for nefarious purposes. To further reduce the chances of unintended use of Bark,\nwe also release a simple classifier to detect Bark-generated audio with high accuracy (see notebooks section of the main repository).",
    "mistralai/Mistral-7B-v0.1": "Model Card for Mistral-7B-v0.1\nModel Architecture\nTroubleshooting\nNotice\nThe Mistral AI Team\nModel Card for Mistral-7B-v0.1\nThe Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters.\nMistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.\nFor full details of this model please read our paper and release blog post.\nModel Architecture\nMistral-7B-v0.1 is a transformer model, with the following architecture choices:\nGrouped-Query Attention\nSliding-Window Attention\nByte-fallback BPE tokenizer\nTroubleshooting\nIf you see the following error:\nKeyError: 'mistral'\nOr:\nNotImplementedError: Cannot copy out of meta tensor; no data!\nEnsure you are utilizing a stable version of Transformers, 4.34.0 or newer.\nNotice\nMistral 7B is a pretrained base model and therefore does not have any moderation mechanisms.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.",
    "briaai/RMBG-1.4": "BRIA Background Removal v1.4 Model Card\nTraining data\nDistribution of images:\nQualitative Evaluation\nArchitecture\nInstallation\nUsage\nBRIA Background Removal v1.4 Model Card\nRMBG v1.4 is our state-of-the-art background removal model, designed to effectively separate foreground from background in a range of\ncategories and image types. This model has been trained on a carefully selected dataset, which includes:\ngeneral stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale.\nThe accuracy, efficiency, and versatility currently rival leading source-available models.\nIt is ideal where content safety, legally licensed datasets, and bias mitigation are paramount.\nDeveloped by BRIA AI, RMBG v1.4 is available as a source-available model for non-commercial use.\nTo purchase a commercial license, simply click Here.\nCLICK HERE FOR A DEMO\nNOTE New RMBG version available! Check out RMBG-2.0\nJoin our Discord community for more information, tutorials, tools, and to connect with other users!\nModel Description\nDeveloped by: BRIA AI\nModel type: Background Removal\nLicense: bria-rmbg-1.4\nThe model is released under a Creative Commons license for non-commercial use.\nCommercial use is subject to a commercial agreement with BRIA. To purchase a commercial license simply click Here.\nModel Description: BRIA RMBG 1.4 is a saliency segmentation model trained exclusively on a professional-grade dataset.\nBRIA: Resources for more information: BRIA AI\nTraining data\nBria-RMBG model was trained with over 12,000 high-quality, high-resolution, manually labeled (pixel-wise accuracy), fully licensed images.\nOur benchmark included balanced gender, balanced ethnicity, and people with different types of disabilities.\nFor clarity, we provide our data distribution according to different categories, demonstrating our model‚Äôs versatility.\nDistribution of images:\nCategory\nDistribution\nObjects only\n45.11%\nPeople with objects/animals\n25.24%\nPeople only\n17.35%\npeople/objects/animals with text\n8.52%\nText only\n2.52%\nAnimals only\n1.89%\nCategory\nDistribution\nPhotorealistic\n87.70%\nNon-Photorealistic\n12.30%\nCategory\nDistribution\nNon Solid Background\n52.05%\nSolid Background\n47.95%\nCategory\nDistribution\nSingle main foreground object\n51.42%\nMultiple objects in the foreground\n48.58%\nQualitative Evaluation\nArchitecture\nRMBG v1.4 is developed on the IS-Net enhanced with our unique training scheme and proprietary dataset.\nThese modifications significantly improve the model‚Äôs accuracy and effectiveness in diverse image-processing scenarios.\nInstallation\npip install -qr https://huggingface.co/briaai/RMBG-1.4/resolve/main/requirements.txt\nUsage\nEither load the pipeline\nfrom transformers import pipeline\nimage_path = \"https://farm5.staticflickr.com/4007/4322154488_997e69e4cf_z.jpg\"\npipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)\npillow_mask = pipe(image_path, return_mask = True) # outputs a pillow mask\npillow_image = pipe(image_path) # applies mask on input and returns a pillow image\nOr load the model\nfrom PIL import Image\nfrom skimage import io\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForImageSegmentation\nfrom torchvision.transforms.functional import normalize\nmodel = AutoModelForImageSegmentation.from_pretrained(\"briaai/RMBG-1.4\",trust_remote_code=True)\ndef preprocess_image(im: np.ndarray, model_input_size: list) -> torch.Tensor:\nif len(im.shape) < 3:\nim = im[:, :, np.newaxis]\n# orig_im_size=im.shape[0:2]\nim_tensor = torch.tensor(im, dtype=torch.float32).permute(2,0,1)\nim_tensor = F.interpolate(torch.unsqueeze(im_tensor,0), size=model_input_size, mode='bilinear')\nimage = torch.divide(im_tensor,255.0)\nimage = normalize(image,[0.5,0.5,0.5],[1.0,1.0,1.0])\nreturn image\ndef postprocess_image(result: torch.Tensor, im_size: list)-> np.ndarray:\nresult = torch.squeeze(F.interpolate(result, size=im_size, mode='bilinear') ,0)\nma = torch.max(result)\nmi = torch.min(result)\nresult = (result-mi)/(ma-mi)\nim_array = (result*255).permute(1,2,0).cpu().data.numpy().astype(np.uint8)\nim_array = np.squeeze(im_array)\nreturn im_array\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# prepare input\nimage_path = \"https://farm5.staticflickr.com/4007/4322154488_997e69e4cf_z.jpg\"\norig_im = io.imread(image_path)\norig_im_size = orig_im.shape[0:2]\nmodel_input_size = [1024, 1024]\nimage = preprocess_image(orig_im, model_input_size).to(device)\n# inference\nresult=model(image)\n# post process\nresult_image = postprocess_image(result[0][0], orig_im_size)\n# save result\npil_mask_im = Image.fromarray(result_image)\norig_image = Image.open(image_path)\nno_bg_image = orig_image.copy()\nno_bg_image.putalpha(pil_mask_im)",
    "vikhyatk/moondream2": "A newer version of this model is available:\nmoondream/moondream3-preview\nUsage\nChangelog\n‚ö†Ô∏è This repository contains the latest version of Moondream 2, our previous generation model. The latest version of Moondream is Moondream 3 (Preview).\nMoondream is a small vision language model designed to run efficiently everywhere.\nWebsite / Demo / GitHub\nThis repository contains the latest (2025-06-21) release of Moondream 2, as well as historical releases. The model is updated frequently, so we recommend specifying a revision as shown below if you're using it in a production application.\nUsage\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom PIL import Image\nmodel = AutoModelForCausalLM.from_pretrained(\n\"vikhyatk/moondream2\",\nrevision=\"2025-06-21\",\ntrust_remote_code=True,\ndevice_map={\"\": \"cuda\"}  # ...or 'mps', on Apple Silicon\n)\n# Captioning\nprint(\"Short caption:\")\nprint(model.caption(image, length=\"short\")[\"caption\"])\nprint(\"\\nNormal caption:\")\nfor t in model.caption(image, length=\"normal\", stream=True)[\"caption\"]:\n# Streaming generation example, supported for caption() and detect()\nprint(t, end=\"\", flush=True)\nprint(model.caption(image, length=\"normal\"))\n# Visual Querying\nprint(\"\\nVisual query: 'How many people are in the image?'\")\nprint(model.query(image, \"How many people are in the image?\")[\"answer\"])\n# Object Detection\nprint(\"\\nObject detection: 'face'\")\nobjects = model.detect(image, \"face\")[\"objects\"]\nprint(f\"Found {len(objects)} face(s)\")\n# Pointing\nprint(\"\\nPointing: 'person'\")\npoints = model.point(image, \"person\")[\"points\"]\nprint(f\"Found {len(points)} person(s)\")\nChangelog\n2025-06-21 (full release notes)\nGrounded Reasoning\nIntroduces a new step-by-step reasoning mode that explicitly grounds reasoning in spatial positions within the image before answering, leading to more precise visual interpretation (e.g., chart median calculations, accurate counting). Enable with reasoning=True in the query skill to trade off speed vs. accuracy.\nSharper Object Detection\nUses reinforcement learning on higher-quality bounding-box annotations to reduce object clumping and improve fine-grained detections (e.g., distinguishing ‚Äúblue bottle‚Äù vs. ‚Äúbottle‚Äù).\nFaster Text Generation\nYields 20‚Äì40 % faster response generation via a new ‚Äúsuperword‚Äù tokenizer and lightweight tokenizer transfer hypernetwork, which reduces the number of tokens emitted without loss in accuracy and eases future multilingual extensions.\nImproved UI Understanding\nBoosts ScreenSpot (UI element localization) performance from an F1@0.5 of 60.3 to 80.4, making Moondream more effective for UI-focused applications.\nReinforcement Learning Enhancements\nRL fine-tuning applied across 55 vision-language tasks to reinforce grounded reasoning and detection capabilities, with a roadmap to expand to ~120 tasks in the next update.\n2025-04-15 (full release notes)\nImproved chart understanding (ChartQA up from 74.8 to 77.5, 82.2 with PoT)\nAdded temperature and nucleus sampling to reduce repetitive outputs\nBetter OCR for documents and tables (prompt with ‚ÄúTranscribe the text‚Äù or ‚ÄúTranscribe the text in natural reading order‚Äù)\nObject detection supports document layout detection (figure, formula, text, etc)\nUI understanding (ScreenSpot F1@0.5 up from 53.3 to 60.3)\nImproved text understanding (DocVQA up from 76.5 to 79.3, TextVQA up from 74.6 to 76.3)\n2025-03-27 (full release notes)\nAdded support for long-form captioning\nOpen vocabulary image tagging\nImproved counting accuracy (e.g. CountBenchQA increased from 80 to 86.4)\nImproved text understanding (e.g. OCRBench increased from 58.3 to 61.2)\nImproved object detection, especially for small objects (e.g. COCO up from 30.5 to 51.2)\nFixed token streaming bug affecting multi-byte unicode characters\ngpt-fast style compile() now supported in HF Transformers implementation",
    "BAAI/bge-reranker-v2-m3": "Reranker\nModel List\nUsage\nUsing FlagEmbedding\nUsing Huggingface transformers\nFine-tune\nData Format\nTrain\nEvaluation\nCitation\nReranker\nMore details please refer to our Github: FlagEmbedding.\nModel List\nUsage\nFine-tuning\nEvaluation\nCitation\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\nModel List\nModel\nBase model\nLanguage\nlayerwise\nfeature\nBAAI/bge-reranker-base\nxlm-roberta-base\nChinese and English\n-\nLightweight reranker model, easy to deploy, with fast inference.\nBAAI/bge-reranker-large\nxlm-roberta-large\nChinese and English\n-\nLightweight reranker model, easy to deploy, with fast inference.\nBAAI/bge-reranker-v2-m3\nbge-m3\nMultilingual\n-\nLightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference.\nBAAI/bge-reranker-v2-gemma\ngemma-2b\nMultilingual\n-\nSuitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities.\nBAAI/bge-reranker-v2-minicpm-layerwise\nMiniCPM-2B-dpo-bf16\nMultilingual\n8-40\nSuitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference.\nYou can select the model according your senario and resource.\nFor multilingual, utilize BAAI/bge-reranker-v2-m3 and BAAI/bge-reranker-v2-gemma\nFor Chinese or English, utilize BAAI/bge-reranker-v2-m3 and BAAI/bge-reranker-v2-minicpm-layerwise.\nFor efficiency, utilize BAAI/bge-reranker-v2-m3 and the low layer of BAAI/bge-reranker-v2-minicpm-layerwise.\nFor better performance, recommand BAAI/bge-reranker-v2-minicpm-layerwise and BAAI/bge-reranker-v2-gemma\nUsage\nUsing FlagEmbedding\npip install -U FlagEmbedding\nFor normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\nGet relevance scores (higher scores indicate more relevance):\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\nFor LLM-based reranker\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\nFor LLM-based layerwise reranker\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\nUsing Huggingface transformers\nFor normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\nGet relevance scores (higher scores indicate more relevance):\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\nscores = model(**inputs, return_dict=True).logits.view(-1, ).float()\nprint(scores)\nFor LLM-based reranker\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\nif prompt is None:\nprompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\nsep = \"\\n\"\nprompt_inputs = tokenizer(prompt,\nreturn_tensors=None,\nadd_special_tokens=False)['input_ids']\nsep_inputs = tokenizer(sep,\nreturn_tensors=None,\nadd_special_tokens=False)['input_ids']\ninputs = []\nfor query, passage in pairs:\nquery_inputs = tokenizer(f'A: {query}',\nreturn_tensors=None,\nadd_special_tokens=False,\nmax_length=max_length * 3 // 4,\ntruncation=True)\npassage_inputs = tokenizer(f'B: {passage}',\nreturn_tensors=None,\nadd_special_tokens=False,\nmax_length=max_length,\ntruncation=True)\nitem = tokenizer.prepare_for_model(\n[tokenizer.bos_token_id] + query_inputs['input_ids'],\nsep_inputs + passage_inputs['input_ids'],\ntruncation='only_second',\nmax_length=max_length,\npadding=False,\nreturn_attention_mask=False,\nreturn_token_type_ids=False,\nadd_special_tokens=False\n)\nitem['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\nitem['attention_mask'] = [1] * len(item['input_ids'])\ninputs.append(item)\nreturn tokenizer.pad(\ninputs,\npadding=True,\nmax_length=max_length + len(sep_inputs) + len(prompt_inputs),\npad_to_multiple_of=8,\nreturn_tensors='pt',\n)\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = get_inputs(pairs, tokenizer)\nscores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\nprint(scores)\nFor LLM-based layerwise reranker\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\nif prompt is None:\nprompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\nsep = \"\\n\"\nprompt_inputs = tokenizer(prompt,\nreturn_tensors=None,\nadd_special_tokens=False)['input_ids']\nsep_inputs = tokenizer(sep,\nreturn_tensors=None,\nadd_special_tokens=False)['input_ids']\ninputs = []\nfor query, passage in pairs:\nquery_inputs = tokenizer(f'A: {query}',\nreturn_tensors=None,\nadd_special_tokens=False,\nmax_length=max_length * 3 // 4,\ntruncation=True)\npassage_inputs = tokenizer(f'B: {passage}',\nreturn_tensors=None,\nadd_special_tokens=False,\nmax_length=max_length,\ntruncation=True)\nitem = tokenizer.prepare_for_model(\n[tokenizer.bos_token_id] + query_inputs['input_ids'],\nsep_inputs + passage_inputs['input_ids'],\ntruncation='only_second',\nmax_length=max_length,\npadding=False,\nreturn_attention_mask=False,\nreturn_token_type_ids=False,\nadd_special_tokens=False\n)\nitem['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\nitem['attention_mask'] = [1] * len(item['input_ids'])\ninputs.append(item)\nreturn tokenizer.pad(\ninputs,\npadding=True,\nmax_length=max_length + len(sep_inputs) + len(prompt_inputs),\npad_to_multiple_of=8,\nreturn_tensors='pt',\n)\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = get_inputs(pairs, tokenizer).to(model.device)\nall_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\nall_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\nprint(all_scores)\nFine-tune\nData Format\nTrain data should be a json file, where each line is a dict like this:\n{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"prompt\": str}\nquery is the query, and pos is a list of positive texts, neg is a list of negative texts, prompt indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\nSee toy_finetune_data.jsonl for a toy data file.\nTrain\nYou can fine-tune the reranker with the following code:\nFor llm-based reranker\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\\n--output_dir {path to save model} \\\n--model_name_or_path google/gemma-2b \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj\nFor llm-based layerwise reranker\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\\n--output_dir {path to save model} \\\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj \\\n--start_layer 8 \\\n--head_multi True \\\n--head_type simple \\\n--lora_extra_parameters linear_head\nOur rerankers are initialized from google/gemma-2b (for llm-based reranker) and openbmb/MiniCPM-2B-dpo-bf16 (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\nbge-m3-data\nquora train data\nfever train data\nEvaluation\nllama-index.\nBEIR.\nrereank the top 100 results from bge-en-v1.5 large.\nrereank the top 100 results from e5 mistral 7b instruct.\nCMTEB-retrieval.It rereank the top 100 results from bge-zh-v1.5 large.\nmiracl (multi-language).It rereank the top 100 results from bge-m3.\nCitation\nIf you find this repository useful, please consider giving a star and citation\n@misc{li2023making,\ntitle={Making Large Language Models A Better Foundation For Dense Retrieval},\nauthor={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\nyear={2023},\neprint={2312.15503},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@misc{chen2024bge,\ntitle={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\nauthor={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\nyear={2024},\neprint={2402.03216},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "stabilityai/stable-audio-open-1.0": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Audio Open 1.0\nModel Description\nUsage\nUsing with stable-audio-tools\nUsing with diffusers\nModel Details\nTraining dataset\nDatasets Used\nAttribution\nMitigations\nUse and Limitations\nIntended Use\nOut-of-Scope Use Cases\nLimitations\nBiases\nStable Audio Open 1.0\nPlease note: For commercial use, please refer to https://stability.ai/license\nModel Description\nStable Audio Open 1.0 generates variable-length (up to 47s) stereo audio at 44.1kHz from text prompts. It comprises three components: an autoencoder that compresses waveforms into a manageable sequence length, a T5-based text embedding for text conditioning, and a transformer-based diffusion (DiT) model that operates in the latent space of the autoencoder.\nUsage\nThis model can be used with:\nthe stable-audio-tools library\nthe diffusers library\nUsing with stable-audio-tools\nThis model is made to be used with the stable-audio-tools library for inference, for example:\nimport torch\nimport torchaudio\nfrom einops import rearrange\nfrom stable_audio_tools import get_pretrained_model\nfrom stable_audio_tools.inference.generation import generate_diffusion_cond\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Download model\nmodel, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-1.0\")\nsample_rate = model_config[\"sample_rate\"]\nsample_size = model_config[\"sample_size\"]\nmodel = model.to(device)\n# Set up text and timing conditioning\nconditioning = [{\n\"prompt\": \"128 BPM tech house drum loop\",\n\"seconds_start\": 0,\n\"seconds_total\": 30\n}]\n# Generate stereo audio\noutput = generate_diffusion_cond(\nmodel,\nsteps=100,\ncfg_scale=7,\nconditioning=conditioning,\nsample_size=sample_size,\nsigma_min=0.3,\nsigma_max=500,\nsampler_type=\"dpmpp-3m-sde\",\ndevice=device\n)\n# Rearrange audio batch to a single sequence\noutput = rearrange(output, \"b d n -> d (b n)\")\n# Peak normalize, clip, convert to int16, and save to file\noutput = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\ntorchaudio.save(\"output.wav\", output, sample_rate)\nUsing with diffusers\nMake sure you upgrade to the latest version of diffusers: pip install -U diffusers. And then you can run:\nimport torch\nimport soundfile as sf\nfrom diffusers import StableAudioPipeline\npipe = StableAudioPipeline.from_pretrained(\"stabilityai/stable-audio-open-1.0\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n# define the prompts\nprompt = \"The sound of a hammer hitting a wooden surface.\"\nnegative_prompt = \"Low quality.\"\n# set the seed for generator\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n# run the generation\naudio = pipe(\nprompt,\nnegative_prompt=negative_prompt,\nnum_inference_steps=200,\naudio_end_in_s=10.0,\nnum_waveforms_per_prompt=3,\ngenerator=generator,\n).audios\noutput = audio[0].T.float().cpu().numpy()\nsf.write(\"hammer.wav\", output, pipe.vae.sampling_rate)\nRefer to the documentation for more details on optimization and usage.\nModel Details\nModel type: Stable Audio Open 1.0 is a latent diffusion model based on a transformer architecture.\nLanguage(s): English\nLicense: Stability AI Community License.\nCommercial License: to use this model commercially, please refer to https://stability.ai/license\nResearch Paper: https://arxiv.org/abs/2407.14358\nTraining dataset\nDatasets Used\nOur dataset consists of 486492 audio recordings, where 472618 are from Freesound and 13874 are from the Free Music Archive (FMA). All audio files are licensed under CC0, CC BY, or CC Sampling+. This data is used to train our autoencoder and DiT. We use a publicly available pre-trained T5 model (t5-base) for text conditioning.\nAttribution\nAttribution for all audio recordings used to train Stable Audio Open 1.0 can be found on our attribution page.\nMitigations\nWe conducted an in-depth analysis to ensure no unauthorized copyrighted music was present in our training data before we began training.\nTo that end, we first identified music samples in Freesound using the PANNs music classifier based on AudioSet classes. The identified music samples had at least 30 seconds of music that was predicted to belong to a music-related class with a threshold of 0.15 (PANNs output probabilities range from 0 to 1). This threshold was determined by classifying known music examples from FMA and ensuring no false negatives were present.\nThe identified music samples were sent to Audible Magic‚Äôs identification services, a trusted content detection company, to ensure the absence of copyrighted music. Audible Magic flagged suspected copyrighted music, which we subsequently removed before training on the dataset. The majority of the removed content was field recordings in which copyrighted music was playing in the background. Following this procedure, we were left with 266324 CC0, 194840 CC-BY, and 11454 CC Sampling+ audio recordings.\nWe also conducted an in-depth analysis to ensure no copyrighted content was present in FMA's subset. In this case, the procedure was slightly different because the FMA subset consists of music signals. We did a metadata search against a large database of copyrighted music (https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset) and flagged any potential match. The flagged content was reviewed individually by humans. After this process, we ended up with 8967 CC-BY and 4907 CC0 tracks.\nUse and Limitations\nIntended Use\nThe primary use of Stable Audio Open is research and experimentation on AI-based music and audio generation, including:\nResearch efforts to better understand the limitations of generative models and further improve the state of science.\nGeneration of music and audio guided by text to explore current abilities of generative AI models by machine learning practitioners and artists.\nOut-of-Scope Use Cases\nThe model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate audio or music pieces that create hostile or alienating environments for people.\nLimitations\nThe model is not able to generate realistic vocals.\nThe model has been trained with English descriptions and will not perform as well in other languages.\nThe model does not perform equally well for all music styles and cultures.\nThe model is better at generating sound effects and field recordings than music.\nIt is sometimes difficult to assess what types of text descriptions provide the best generations. Prompt engineering may be required to obtain satisfying results.\nBiases\nThe source of data is potentially lacking diversity and all cultures are not equally represented in the dataset. The model may not perform equally well on the wide variety of music genres and sound effects that exist. The generated samples from the model will reflect the biases from the training data.",
    "Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix": "#roleplay #sillytavern #llama3\n#roleplay #sillytavern #llama3\nMy GGUF-IQ-Imatrix quants for Sao10K/L3-8B-Stheno-v3.2.\nSao10K with Stheno again, another banger! I recommend checking his page for feedback and support.\nQuantization process:\nFor future reference, these quants have been done after the fixes from #6920 have been merged.\nImatrix data was generated from the FP16-GGUF and conversions directly from the BF16-GGUF.\nThis was a bit more disk and compute intensive but hopefully avoided any losses during conversion.\nIf you noticed any issues let me know in the discussions.\nGeneral usage:\nUse the latest version of KoboldCpp.\nFor 8GB VRAM GPUs, I recommend the Q4_K_M-imat (4.89 BPW) quant for up to 12288 context sizes.\nPresets:\nSome compatible SillyTavern presets can be found here (Virt's Roleplay Presets).\nCheck discussions such as this one for other recommendations and samplers.\nPersonal-support:\nI apologize for disrupting your experience.\nCurrently I'm working on moving for a better internet provider.\nIf you want and you are able to...\nYou can spare some change over here (Ko-fi).\nAuthor-support:\nYou can support the author at their own page.\nClick here for the original model card information.\nSupport me here if you're interested:\nKo-fi: https://ko-fi.com/sao10k\nwink Euryale v2?\nIf not, that's fine too. Feedback would be nice.\nContact Me in Discord:\nsao10k\nArt by navy_(navy.blue) - Danbooru\nStheno-v3.2-Zeta\nI have done a test run with multiple variations of the models, merged back to its base at various weights, different training runs too, and this Sixth iteration is the one I like most.\nChanges compared to v3.1\n- Included a mix of SFW and NSFW Storywriting Data, thanks to Gryphe\n- Included More Instruct / Assistant-Style Data\n- Further cleaned up Roleplaying Samples from c2 Logs -> A few terrible, really bad samples escaped heavy filtering. Manual pass fixed it.\n- Hyperparameter tinkering for training, resulting in lower loss levels.\nTesting Notes - Compared to v3.1\n- Handles SFW / NSFW seperately better. Not as overly excessive with NSFW now. Kinda balanced.\n- Better at Storywriting / Narration.\n- Better at Assistant-type Tasks.\n- Better Multi-Turn Coherency -> Reduced Issues?\n- Slightly less creative? A worthy tradeoff. Still creative.\n- Better prompt / instruction adherence.\nRecommended Samplers:\nTemperature - 1.12-1.22\nMin-P - 0.075\nTop-K - 50\nRepetition Penalty - 1.1\nStopping Strings:\n\\n\\n{{User}} # Or Equivalent, depending on Frontend\n<|eot_id|>\n<|end_of_text|>\nPrompting Template - Llama-3-Instruct\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{output}<|eot_id|>\nBasic Roleplay System Prompt\nYou are an expert actor that can fully immerse yourself into any role given. You do not break character for any reason, even if someone tries addressing you as an AI or language model.\nCurrently your role is {{char}}, which is described in detail below. As {{char}}, continue the exchange with {{user}}.",
    "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\n2. Model Downloads\n3. Chat Website\n4. API Platform\n5. How to run locally\nInference with Huggingface's Transformers\nInference with vLLM (recommended)\n6. License\n7. Contact\nAPI Platform |\nHow to Use |\nLicense |\nPaper LinküëÅÔ∏è\nDeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\nWe present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.\nIn standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.  The list of supported programming languages can be found here.\n2. Model Downloads\nWe release the DeepSeek-Coder-V2 with 16B and 236B parameters based on the DeepSeekMoE framework, which has actived parameters of only 2.4B and 21B , including base and instruct models, to the public.\nModel\n#Total Params\n#Active Params\nContext Length\nDownload\nDeepSeek-Coder-V2-Lite-Base\n16B\n2.4B\n128k\nü§ó HuggingFace\nDeepSeek-Coder-V2-Lite-Instruct\n16B\n2.4B\n128k\nü§ó HuggingFace\nDeepSeek-Coder-V2-Base\n236B\n21B\n128k\nü§ó HuggingFace\nDeepSeek-Coder-V2-Instruct\n236B\n21B\n128k\nü§ó HuggingFace\n3. Chat Website\nYou can chat with the DeepSeek-Coder-V2 on DeepSeek's official website: coder.deepseek.com\n4. API Platform\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com, and you can also pay-as-you-go at an unbeatable price.\n5. How to run locally\nHere, we provide some examples of how to use DeepSeek-Coder-V2-Lite model. If you want to utilize DeepSeek-Coder-V2 in BF16 format for inference, 80GB*8 GPUs are required.\nInference with Huggingface's Transformers\nYou can directly employ Huggingface's Transformers for model inference.\nCode Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nCode Insertion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"\"\"<ÔΩúfim‚ñÅbeginÔΩú>def quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[0]\nleft = []\nright = []\n<ÔΩúfim‚ñÅholeÔΩú>\nif arr[i] < pivot:\nleft.append(arr[i])\nelse:\nright.append(arr[i])\nreturn quick_sort(left) + [pivot] + quick_sort(right)<ÔΩúfim‚ñÅendÔΩú>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\nChat Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>  token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\nThe complete chat template can be found within tokenizer_config.json located in the huggingface model repository.\nAn example of chat template is as belows:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>User: {user_message_1}\nAssistant: {assistant_message_1}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>User: {user_message_2}\nAssistant:\nYou can also add an optional system message:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system_message}\nUser: {user_message_1}\nAssistant: {assistant_message_1}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>User: {user_message_2}\nAssistant:\nInference with vLLM (recommended)\nTo utilize vLLM for model inference, please merge this Pull Request into your vLLM codebase: https://github.com/vllm-project/vllm/pull/4650.\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmax_model_len, tp_size = 8192, 1\nmodel_name = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\nmessages_list = [\n[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n[{\"role\": \"user\", \"content\": \"write a quick sort algorithm in python.\"}],\n[{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n6. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-Coder-V2 Base/Instruct models is subject to the Model License. DeepSeek-Coder-V2 series (including Base and Instruct) supports commercial use.\n7. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "xinsir/controlnet-union-sdxl-1.0": "ControlNet++: All-in-one ControlNet for image generations and editing!\nProMax Model has released!! 12 control + 5 advanced editing, just try it!!!\nNetwork Arichitecture\nAdvantages about the model\nAdvanced editing features in Promax Model\nTile Deblur\nTile variation\nTile Super Resolution\nImage Inpainting\nImage Outpainting\nVisual Examples\nOpenpose\nDepth\nCanny\nLineart\nAnimeLineart\nMlsd\nScribble\nHed\nPidi(Softedge)\nTeed\nSegment\nNormal\nMulti Control Visual Examples\nOpenpose + Canny\nOpenpose + Depth\nOpenpose + Scribble\nOpenpose + Normal\nOpenpose + Segment\nControlNet++: All-in-one ControlNet for image generations and editing!\nProMax Model has released!! 12 control + 5 advanced editing, just try it!!!\nNetwork Arichitecture\nAdvantages about the model\nUse bucket training like novelai, can generate high resolutions images of any aspect ratio\nUse large amount of high quality data(over 10000000 images), the dataset covers a diversity of situation\nUse re-captioned prompt like DALLE.3, use CogVLM to generate detailed description, good prompt following ability\nUse many useful tricks during training. Including but not limited to date augmentation, mutiple loss, multi resolution\nUse almost the same parameter compared with original ControlNet. No obvious increase in network parameter or computation.\nSupport 10+ control conditions, no obvious performance drop on any single condition compared with training independently\nSupport multi condition generation, condition fusion is learned during training. No need to set hyperparameter or design prompts.\nCompatible with other opensource SDXL models, such as BluePencilXL, CounterfeitXL. Compatible with other Lora models.\nWe design a new architecture that can support 10+ control types in condition text-to-image generation and can generate high resolution images visually comparable with\nmidjourney. The network is based on the original ControlNet architecture, we propose two new modules to: 1 Extend the original ControlNet to support different image\nconditions using the same network parameter. 2 Support multiple conditions input without increasing computation offload, which is especially important for designers\nwho want to edit image in detail, different conditions use the same condition encoder, without adding extra computations or parameters. We do thoroughly experiments\non SDXL and achieve superior performance both in control ability and aesthetic score. We release the method and the model to the open source community to make everyone\ncan enjoy it.\nInference scripts and more details can found: https://github.com/xinsir6/ControlNetPlus/tree/main\nIf you find it useful, please give me a star, thank you very much\nSDXL ProMax version has been released!!!ÔºåEnjoy it!!!\nI am sorry that because of the project's revenue and expenditure are difficult to balance, the GPU resources are assigned to other projects that are more likely to be profitable, the SD3 trainging is stopped until I find enough GPU supprt, I will try my best to find GPUs to continue training. If this brings you inconvenience, I sincerely apologize for that. I want to thank everyone who likes this project, your support is what keeps me going\nNote: we put the promax model with a promax suffix in the same huggingface model repo, detailed instructions will be added later.\nAdvanced editing features in Promax Model\nTile Deblur\nTile variation\nTile Super Resolution\nFollowing example show from 1M resolution --> 9M resolution\nImage Inpainting\nImage Outpainting\nVisual Examples\nOpenpose\nDepth\nCanny\nLineart\nAnimeLineart\nMlsd\nScribble\nHed\nPidi(Softedge)\nTeed\nSegment\nNormal\nMulti Control Visual Examples\nOpenpose + Canny\nOpenpose + Depth\nOpenpose + Scribble\nOpenpose + Normal\nOpenpose + Segment",
    "mistralai/Mistral-Nemo-Instruct-2407": "Model Card for Mistral-Nemo-Instruct-2407\nKey features\nModel Architecture\nMetrics\nMain Benchmarks\nMultilingual Benchmarks (MMLU)\nUsage\nMistral Inference\nTransformers\nFunction calling with transformers\nLimitations\nThe Mistral AI Team\nModel Card for Mistral-Nemo-Instruct-2407\nThe Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407. Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.\nFor more details about this model please refer to our release blog post.\nKey features\nReleased under the Apache 2 License\nPre-trained and instructed versions\nTrained with a 128k context window\nTrained on a large proportion of multilingual and code data\nDrop-in replacement of Mistral 7B\nModel Architecture\nMistral Nemo is a transformer model, with the following architecture choices:\nLayers: 40\nDim: 5,120\nHead dim: 128\nHidden dim: 14,336\nActivation Function: SwiGLU\nNumber of heads: 32\nNumber of kv-heads: 8 (GQA)\nVocabulary size: 2**17 ~= 128k\nRotary embeddings (theta = 1M)\nMetrics\nMain Benchmarks\nBenchmark\nScore\nHellaSwag (0-shot)\n83.5%\nWinogrande (0-shot)\n76.8%\nOpenBookQA (0-shot)\n60.6%\nCommonSenseQA (0-shot)\n70.4%\nTruthfulQA (0-shot)\n50.3%\nMMLU (5-shot)\n68.0%\nTriviaQA (5-shot)\n73.8%\nNaturalQuestions (5-shot)\n31.2%\nMultilingual Benchmarks (MMLU)\nLanguage\nScore\nFrench\n62.3%\nGerman\n62.7%\nSpanish\n64.6%\nItalian\n61.3%\nPortuguese\n63.3%\nRussian\n59.2%\nChinese\n59.0%\nJapanese\n59.0%\nUsage\nThe model can be used with three different frameworks\nmistral_inference: See here\ntransformers: See here\nNeMo: See nvidia/Mistral-NeMo-12B-Instruct\nMistral Inference\nInstall\nIt is recommended to use mistralai/Mistral-Nemo-Instruct-2407 with mistral-inference. For HF transformers code snippets, please keep scrolling.\npip install mistral_inference\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', 'Nemo-Instruct')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\nChat\nAfter installing mistral_inference, a mistral-chat CLI command should be available in your environment. You can chat with the model using\nmistral-chat $HOME/mistral_models/Nemo-Instruct --instruct --max_tokens 256 --temperature 0.35\nE.g. Try out something like:\nHow expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\nInstruct following\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\nprompt = \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nFunction calling\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\ncompletion_request = ChatCompletionRequest(\ntools=[\nTool(\nfunction=Function(\nname=\"get_current_weather\",\ndescription=\"Get the current weather\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\",\n},\n\"format\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The temperature unit to use. Infer this from the users location.\",\n},\n},\n\"required\": [\"location\", \"format\"],\n},\n)\n)\n],\nmessages=[\nUserMessage(content=\"What's the weather like today in Paris?\"),\n],\n)\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nTransformers\nNOTE: Until a new release has been made, you need to install transformers from source:\npip install git+https://github.com/huggingface/transformers.git\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\nfrom transformers import pipeline\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-Nemo-Instruct-2407\",max_new_tokens=128)\nchatbot(messages)\nFunction calling with transformers\nTo use this example, you'll need transformers version 4.42.0 or higher. Please see the\nfunction calling guide\nin the transformers docs for more information.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ndef get_current_weather(location: str, format: str):\n\"\"\"\nGet the current weather\nArgs:\nlocation: The city and state, e.g. San Francisco, CA\nformat: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n\"\"\"\npass\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n# format and tokenize the tool use prompt\ninputs = tokenizer.apply_chat_template(\nconversation,\ntools=tools,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the function calling guide,\nand note that Mistral does use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\nUnlike previous Mistral models, Mistral Nemo requires smaller temperatures. We recommend to use a temperature of 0.3.\nLimitations\nThe Mistral Nemo Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka√´l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th√©ophile Gervet, Timoth√©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "city96/FLUX.1-dev-gguf": "This is a direct GGUF conversion of black-forest-labs/FLUX.1-dev\nAs this is a quantized model not a finetune, all the same restrictions/original license terms still apply.\nThe model files can be used with the ComfyUI-GGUF custom node.\nPlace model files in ComfyUI/models/unet - see the GitHub readme for further install instructions.\nPlease refer to this chart for a basic overview of quantization types."
}