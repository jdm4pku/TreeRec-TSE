{
    "microsoft/OmniParser-v2.0": "Model Summary\nWhat's new in V2?\nResponsible AI Considerations\nIntended Use\nlimitations\nLicense\nüì¢ [GitHub Repo] [OmniParser V2 Blog Post] Huggingface demo\nModel Summary\nOmniParser is a general screen parsing tool, which interprets/converts UI screenshot to structured format, to improve existing LLM based UI agent.\nTraining Datasets include: 1) an interactable icon detection dataset, which was curated from popular web pages and automatically annotated to highlight clickable and actionable regions, and 2) an icon description dataset, designed to associate each UI element with its corresponding function.\nThis model hub includes a finetuned version of YOLOv8 and a finetuned Florence-2 base model on the above dataset respectively. For more details of the models used and finetuning, please refer to the paper.\nWhat's new in V2?\nLarger and cleaner set of icon caption + grounding dataset\n60% improvement in latency compared to V1. Avg latency: 0.6s/frame on A100, 0.8s on single 4090.\nStrong performance: 39.6 average accuracy on ScreenSpot Pro\nYour agent only need one tool: OmniTool. Control a Windows 11 VM with OmniParser + your vision model of choice. OmniTool supports out of the box the following large language models - OpenAI (4o/o1/o3-mini), DeepSeek (R1), Qwen (2.5VL) or Anthropic Computer Use. Check out our github repo for details.\nResponsible AI Considerations\nIntended Use\nOmniParser is designed to be able to convert unstructured screenshot image into structured list of elements including interactable regions location and captions of icons on its potential functionality.\nOmniParser is intended to be used in settings where users are already trained on responsible analytic approaches and critical reasoning is expected. OmniParser is capable of providing extracted information from the screenshot, however human judgement is needed for the output of OmniParser.\nOmniParser is intended to be used on various screenshots, which includes both PC and Phone, and also on various applications.\nlimitations\nOmniParser is designed to faithfully convert screenshot image into structured elements of interactable regions and semantics of the screen, while it does not detect harmful content in its input (like users have freedom to decide the input of any LLMs), users are expected to provide input to the OmniParser that is not harmful.\nWhile OmniParser only converts screenshot image into texts, it can be used to construct an GUI agent based on LLMs that is actionable. When developing and operating the agent using OmniParser, the developers need to be responsible and follow common safety standard.\nLicense\nPlease note that icon_detect model is under AGPL license, and icon_caption is under MIT license. Please refer to the LICENSE file in the folder of each model.",
    "nvidia/audio-flamingo-2-0.5B": "PyTorch Implementation of Audio Flamingo 2\nMain Results\nAudio Flamingo 2 Architecture\nLicense\nCitation\nPyTorch Implementation of Audio Flamingo 2\nSreyan Ghosh, Zhifeng Kong, Sonal Kumar, S Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, Bryan Catanzaro\n[paper] [Demo website] [GitHub]\nThis repo contains the PyTorch implementation of Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities. Audio Flamingo 2 achieves state-of-the-art performance across over 20 benchmarks, using only a 3B parameter small language model. It is improved from our previous Audio Flamingo.\nWe introduce two datasets, AudioSkills for expert audio reasoning, and LongAudio for long audio understanding, to advance the field of audio understanding.\nAudio Flamingo 2 has advanced audio understanding and reasoning capabilities. Especially, Audio Flamingo 2 has expert audio reasoning abilities, and can understand long audio up to 5 minutes.\nAudio Flamingo 2 outperforms larger and proprietary LALMs across 20+ benchmarks, despite being smaller (3B) and trained exclusively on public datasets.\nMain Results\nAudio Flamingo 2 outperforms prior SOTA models including GAMA, Audio Flamingo, Qwen-Audio, Qwen2-Audio, LTU, LTU-AS, SALMONN, AudioGPT, Gemini Flash v2, Gemini Pro v1.5, and GPT-4o-audio on a number of understanding and reasoning benchmarks.\nAudio Flamingo 2 Architecture\nAudio Flamingo 2 uses a cross-attention architecture similar to Audio Flamingo and Flamingo. Audio Flamingo 2 can take up to 5 minutes of audio inputs.\nLicense\nThe code in this repo is under MIT license. The checkpoints are for non-commercial use only (see NVIDIA OneWay Noncommercial License). They are also subject to the Qwen Research license, the Terms of Use of the data generated by OpenAI, and the original licenses accompanying each training dataset.\nNotice: Audio Flamingo 2 is built with Qwen-2.5. Qwen is licensed under the Qwen RESEARCH LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\nCitation\nAudio Flamingo\n@inproceedings{kong2024audio,\ntitle={Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities},\nauthor={Kong, Zhifeng and Goel, Arushi and Badlani, Rohan and Ping, Wei and Valle, Rafael and Catanzaro, Bryan},\nbooktitle={International Conference on Machine Learning},\npages={25125--25148},\nyear={2024},\norganization={PMLR}\n}\nAudio Flamingo 2\n@article{ghosh2025audio,\ntitle={Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities},\nauthor={Ghosh, Sreyan and Kong, Zhifeng and Kumar, Sonal and Sakshi, S and Kim, Jaehyeon and Ping, Wei and Valle, Rafael and Manocha, Dinesh and Catanzaro, Bryan},\njournal={arXiv preprint arXiv:2503.03983},\nyear={2025}\n}",
    "Sukino/SillyTavern-Settings-and-Presets": "What I'm Using\nSoftware\nText Generation Models\nImage Generation Models\nChatbots\nSettings\nGuides\nIndex\nSukino's SillyTavern Settings and Presets\nGo to the Files and versions tab to view and download my settings.\nWhat I'm Using\nSoftware\nKoboldCPP: https://github.com/LostRuins/koboldcpp ‚Äî My backend of choice for running models locally.\nSillyTavern: https://github.com/SillyTavern/SillyTavern ‚Äî My frontend of choice, the bridge between you and the AI model. Load your characters in it, connect to KoboldCPP and start chatting.\nMikuPad: https://lmg-anon.github.io/mikupad/mikupad.html ‚Äî Want to understand how LLMs work in a minute? Use this to talk directly to the model without characters and system prompts in the way. I use it all the time to test how the model talks and responds to prompts on its own, and what it can or can't do.\nComfyUI: https://github.com/comfyanonymous/ComfyUI / https://github.com/ltdrdata/ComfyUI-Manager ‚Äî My image generation software of choice. Node based, not really beginner friendly, but it's pretty powerful, you have control over every single generation step.\nText Generation Models\nThese days I just really use bigger models via API. With Deepseek and GLM, it has become much more accessible, and it's hard to go back to local, smaller models. Still, these are the ones I used the most:\nMistral Small Instruct 2409: https://huggingface.co/mistralai/Mistral-Small-Instruct-2409 ‚Äî Are these Small models really 20B? They feel a lot smarter than they should. I used this model a lot, but the prose is pretty bland, and it needs a well thought-out prompt to RP well. I prefer blandness to sacrificing smartness for a finetune.\nMagistral Small 2509: https://huggingface.co/mistralai/Magistral-Small-2509 ‚Äî The more up-to-date version of the same model.\nGemma 2 9B IT: https://huggingface.co/google/gemma-2-9b-it ‚Äî It's an impressive model, has a fantastic prose for such a small size, feels better than even 12B models. But it's pretty censored by default, you'll need to jailbreak it for not-so-wholesome RPs. And don't go beyond 12K context, the model will break.\nCydonia-v1.2-Magnum-v4-22B: https://huggingface.co/knifeayumu/Cydonia-v1.2-Magnum-v4-22B ‚Äî Magnum V2 and V3 were great models, while V4 was kind of a miss. But for some reason, merging it with Cydonia made Cydonia even better? Go figure.\nMN 12B Mag Mell R1: https://huggingface.co/inflatebot/MN-12B-Mag-Mell-R1 ‚Äî Best 12B model for RP to date, imo, as smart as a 12B Mistral based model can be.\nWayfarer 12B: https://huggingface.co/LatitudeGames/Wayfarer-12B ‚Äî Curious model, not the best for general RP, but shines if you like the AI Dungeon format, it was trained for that.\nWayfarer 2 12B: https://huggingface.co/LatitudeGames/Wayfarer-2-12B ‚Äî The more up-to-date version of the same model.\nImage Generation Models\nNoobAI-XL V-Pred: https://huggingface.co/Laxhar ‚Äî I used to use merges for a long time, but these days, I just use the base model and apply LORAs to change the style when I want. V-Pred models are a bit harder to work with, and have way less content for them, but they follow prompts better and have significantly better contrast and blacks. It's really cool to experiment with lighting when using them.\nŒ£ŒôŒó: https://civitai.com/models/1217645/sih ‚Äî What the hell, how do you read that? I just call it sigma. Another NoobAI merge. Still testing it, but I feel like it's a big improvement over CyberFix, it's taken its place for now.\nNoobAICyberFix: https://civitai.com/models/913998/noobaicyberfix ‚Äî NoobAI is one of the latest efforts for anime-style, could easily replace Pony as the standard. This CyberFix makes it easier to work with.",
    "google/siglip2-giant-opt-patch16-384": "SigLIP 2 Giant\nIntended uses\nTraining procedure\nTraining data\nCompute\nEvaluation results\nBibTeX entry and citation info\nSigLIP 2 Giant\nSigLIP 2 extends the pretraining objective of\nSigLIP with prior, independently developed techniques\ninto a unified recipe, for improved semantic understanding, localization, and dense features.\nIntended uses\nYou can use the raw model for tasks like zero-shot image classification and\nimage-text retrieval, or as a vision encoder for VLMs (and other vision tasks).\nHere is how to use this model to perform zero-shot image classification:\nfrom transformers import pipeline\n# load pipeline\nckpt = \"google/siglip2-giant-opt-patch16-384\"\nimage_classifier = pipeline(model=ckpt, task=\"zero-shot-image-classification\")\n# load image and candidate labels\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\ncandidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n# run inference\noutputs = image_classifier(image, candidate_labels)\nprint(outputs)\nYou can encode an image using the Vision Tower like so:\nimport torch\nfrom transformers import AutoModel, AutoProcessor\nfrom transformers.image_utils import load_image\n# load the model and processor\nckpt = \"google/siglip2-giant-opt-patch16-384\"\nmodel = AutoModel.from_pretrained(ckpt, device_map=\"auto\").eval()\nprocessor = AutoProcessor.from_pretrained(ckpt)\n# load the image\nimage = load_image(\"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\")\ninputs = processor(images=[image], return_tensors=\"pt\").to(model.device)\n# run infernece\nwith torch.no_grad():\nimage_embeddings = model.get_image_features(**inputs)\nprint(image_embeddings.shape)\nFor more code examples, we refer to the siglip documentation.\nTraining procedure\nSigLIP 2 adds some clever training objectives on top of SigLIP:\nDecoder loss\nGlobal-local and masked prediction loss\nAspect ratio and resolution adaptibility\nTraining data\nSigLIP 2 is pre-trained on the WebLI dataset (Chen et al., 2023).\nCompute\nThe model was trained on up to 2048 TPU-v5e chips.\nEvaluation results\nEvaluation of SigLIP 2 is shown below (taken from the paper).\nBibTeX entry and citation info\n@misc{tschannen2025siglip2multilingualvisionlanguage,\ntitle={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features},\nauthor={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier H√©naff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},\nyear={2025},\neprint={2502.14786},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2502.14786},\n}",
    "GSAI-ML/LLaDA-8B-Instruct": "LLaDA-8B-Instruct\nUpdates\nLLaDA-8B-Instruct\nWe introduce LLaDA, a diffusion model with an unprecedented 8B scale, trained entirely from scratch, rivaling LLaMA3 8B in performance.\nProject Page\nCode\nUpdates\n[2025-10-21] We have modified modeling_llada.py to support the input of attention_mask.",
    "stabilityai/sv4d2.0": "Stable Video 4D 2.0\nUsage\nIntended Uses\nOut-of-Scope Uses\nSafety\nContact\nStable Video 4D 2.0\nStable Video 4D 2.0 (SV4D 2.0) is a generative model based on Stable Video 4D (SV4D), which takes in a single-view video of an object and generates multiple novel-view videos (4D image matrix) of that object.\nPlease note: For individuals  or organizations generating annual revenue of US $1,000,000 (or local currency equivalent) or more, regardless of the source of that revenue, you must obtain an enterprise commercial license directly from Stability AI before commercially using SV4D 2.0, or any derivative work of SV4D 2.0 or its outputs, such as ‚Äúfine tune‚Äù or ‚Äúlow-rank adaption‚Äù models. You may submit a request for an Enterprise License at https://stability.ai/enterprise. Please refer to Stability AI‚Äôs Community License, available at https://stability.ai/license, for more information.\nModel Description\nDeveloped by: Stability AI\nModel type: Generative video-to-video model\nModel details: This model is trained to generate 48 images (12 video frames x 4 camera views) at 576x576 resolution, given a reference video of 12 frames of the same size.\nThe generated 12x4 image matrix can be viewed as 4 videos (12 frames each), showing how the dynamic object looks from 4 different camera angles specified by the user.\nWe also release an 8-view model that generates 5 frames x 8 views at a time (same as SV4D). To generate longer novel-view videos, we use the first generated frames as reference, and then sample the remaining frames in an auto-regressive manner.\nPlease check our arxiv paper and video summary for details.\nLicense\nCommunity License: Free for research, non-commercial, and commercial use by organizations and individuals generating annual revenue of US $1,000,000 (or local currency equivalent) or less, regardless of the source of that revenue. If your annual revenue exceeds US $1M, any commercial use of this model or derivative works thereof requires obtaining an Enterprise License directly from Stability AI. You may submit a request for an Enterprise License at https://stability.ai/enterprise. Please refer to Stability AI‚Äôs Community License, available at https://stability.ai/license, for more information.\nModel Sources\nRepository: https://github.com/Stability-AI/generative-models\nProject page: https://sv4d20.github.io\narXiv paper: https://arxiv.org/pdf/2503.16396\nVideo summary: https://youtu.be/dtqj-s50ynU?feature=shared\nTraining Dataset\nWe use renders from the Objaverse-XL dataset, available under the Open Data Commons Attribution License, utilizing our enhanced rendering method that more closely replicates the distribution of images found in the real world, significantly improving our model's ability to generalize. We filter objects based on the review of licenses and curated a subset suitable for our training needs.\nUsage\nFor usage instructions, please refer to our generative models GitHub repository\nIntended Uses\nIntended uses include the following:\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models, including understanding the limitations of generative models.\nAll uses of the model should be in accordance with our Acceptable Use Policy.\nOut-of-Scope Uses\nThe model was not trained to be factual or true representations of people or events. As such, using the model to generate such content is out-of-scope of the abilities of this model.\nSafety\nAs part of our safety-by-design and responsible AI deployment approach, we implement safety measures throughout the development of our models, from the time we begin pre-training a model to the ongoing development, fine-tuning, and deployment of each model. We have implemented a number of safety mitigations that are intended to reduce the risk of severe harms. However, we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.For more about our approach to Safety, please visit our Safety page.\nContact\nPlease report any issues with the model or contact us:\nSafety issues:  safety@stability.ai\nSecurity issues:  security@stability.ai\nPrivacy issues:  privacy@stability.ai\nLicense and general: https://stability.ai/license\nEnterprise license: https://stability.ai/enterprise",
    "google/gemma-3-1b-pt": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nUsage\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-pt\", device=\"cuda\", torch_dtype=torch.bfloat16)\noutput = pipe(\"Eiffel tower is located in\", max_new_tokens=50)\nRunning the model on a single / multi GPU\nimport torch\nfrom transformers import AutoTokenizer, Gemma3ForCausalLM\nckpt = \"google/gemma-3-1b-pt\"\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = Gemma3ForCausalLM.from_pretrained(\nckpt,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nprompt = \"Eiffel tower is located in\"\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = tokenizer.decode(generation, skip_special_tokens=True)\nprint(decoded)\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "moonshotai/Moonlight-16B-A3B-Instruct": "Abstract\nKey Ingredients\nPerformance\nExample usage\nModel Download\nInference with Hugging Face Transformers\nCitation\nTech Report  |\nHuggingFace  |\nMegatron(coming soon)\nAbstract\nRecently, the Muon optimizer has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon:\nWeight Decay: Critical for scaling to larger models\nConsistent RMS Updates: Enforcing a consistent root mean square on model updates\nThese techniques allow Muon to work out-of-the-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon is $\\sim2\\times$ more sample efficient than Adam with compute optimal training.\nBased on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models.\nWe open-source our Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.\nOur code is available at MoonshotAI/Moonlight.\nKey Ingredients\nOur work builds upon Muon while systematically identifying and resolving its limitations in large-scale training scenarios. Our technical contributions include:\nAnalysis for Effective Scaling of Muon: Through extensive analysis, we identify that weight decay plays a crucial roles in Muon's scalability. Besides, we proposed to keep a consistent update root mean square (RMS) across different matrix and non-matrix parameters through parameter-wise update scale adjustments. Such adjustments significantly enhanced training stability.\nEfficient Distributed Implementation: We develop a distributed version of Muon with ZeRO-1 style optimization, achieving optimal memory efficiency and reduced communication overhead while preserving the mathematical properties of the algorithm.\nScaling Law Validation: We performed scaling law research that compares Muon with strong AdamW baselines, and showed the superior performance of Muon (see Figure 1). Based on the scaling law results, Muon achieves comparable performance to AdamW trained counterparts while requiring only approximately 52% of the training FLOPs.\nScaling up with Muon. (a) Scaling law experiments comparing Muon and Adam. Muon is 2 times more sample efficient than Adam. (b) The MMLU performance of our Moonlight model optimized with Muon and other comparable models. Moonlight advances the Pareto frontier of performance vs training FLOPs.\nPerformance\nWe compared Moonlight with SOTA public models at similar scale:\nLLAMA3-3B is a 3B-parameter dense model trained with 9T tokens\nQwen2.5-3B is a 3B-parameter dense model trained with 18T tokens\nDeepseek-v2-Lite is a 2.4B/16B-parameter MOE model trained with 5.7T tokens\nBenchmark (Metric)\nLlama3.2-3B\nQwen2.5-3B\nDSV2-Lite\nMoonlight\nActivated Param‚Ä†\n2.81B\n2.77B\n2.24B\n2.24B\nTotal Params‚Ä†\n2.81B\n2.77B\n15.29B\n15.29B\nTraining Tokens\n9T\n18T\n5.7T\n5.7T\nOptimizer\nAdamW\n*\nAdamW\nMuon\nEnglish\nMMLU\n54.75\n65.6\n58.3\n70.0\nMMLU-pro\n25.0\n34.6\n25.5\n42.4\nBBH\n46.8\n56.3\n44.1\n65.2\nTriviaQA‚Ä°\n59.6\n51.1\n65.1\n66.3\nCode\nHumanEval\n28.0\n42.1\n29.9\n48.1\nMBPP\n48.7\n57.1\n43.2\n63.8\nMath\nGSM8K\n34.0\n79.1\n41.1\n77.4\nMATH\n8.5\n42.6\n17.1\n45.3\nCMath\n-\n80.0\n58.4\n81.1\nChinese\nC-Eval\n-\n75.0\n60.3\n77.2\nCMMLU\n-\n75.0\n64.3\n78.2\nQwen 2 & 2.5 reports didn't disclose their optimizer information. ‚Ä†The reported parameter counts exclude the embedding parameters. ‚Ä°We test all listed models with the full set of TriviaQA.\nExample usage\nModel Download\nModel\n#Total Params\n#Activated Params\nContext Length\nDownload Link\nMoonlight-16B-A3B\n16B\n3B\n8K\nü§ó Hugging Face\nMoonlight-16B-A3B-Instruct\n16B\n3B\n8K\nü§ó Hugging Face\nInference with Hugging Face Transformers\nWe introduce how to use our model at inference stage using transformers library. It is recommended to use python=3.10, torch>=2.1.0, and transformers=4.48.2 as the development environment.\nFor our pretrained model (Moonlight-16B-A3B):\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"moonshotai/Moonlight-16B-A3B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nprompt = \"1+1=2, 1+2=\"\ninputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=100)\nresponse = tokenizer.batch_decode(generated_ids)[0]\nprint(response)\nFor our instruct model (Moonlight-16B-A3B-Instruct):\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"moonshotai/Moonlight-16B-A3B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant provided by Moonshot-AI.\"},\n{\"role\": \"user\", \"content\": \"Is 123 a prime?\"}\n]\ninput_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(inputs=input_ids, max_new_tokens=500)\nresponse = tokenizer.batch_decode(generated_ids)[0]\nprint(response)\nMoonlight has the same architecture as DeepSeek-V3, which is supported by many popular inference engines, such as VLLM and SGLang. As a result, our model can also be easily deployed using these tools.\nCitation\nIf you find Moonlight is useful or want to use in your projects, please kindly cite our paper:\n@misc{liu2025muonscalablellmtraining,\ntitle={Muon is Scalable for LLM Training},\nauthor={Jingyuan Liu and Jianlin Su and Xingcheng Yao and Zhejun Jiang and Guokun Lai and Yulun Du and Yidao Qin and Weixin Xu and Enzhe Lu and Junjie Yan and Yanru Chen and Huabin Zheng and Yibo Liu and Shaowei Liu and Bohong Yin and Weiran He and Han Zhu and Yuzhi Wang and Jianzhou Wang and Mengnan Dong and Zheng Zhang and Yongsheng Kang and Hao Zhang and Xinran Xu and Yutao Zhang and Yuxin Wu and Xinyu Zhou and Zhilin Yang},\nyear={2025},\neprint={2502.16982},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2502.16982},\n}",
    "silveroxides/Chroma-GGUF": "Q8_M and Q4_K_S can be found at Clybius/Chroma-GGUF\nBF16\nQ8_0\nQ6_K\nQ5_1\nQ5_0\nQ5_K_S\nQ4_1\nQ4_K_M\nQ4_0\nQ3_K_L",
    "mradermacher/Qwen2.5-0.5b-Test-ft-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/KingNish/Qwen2.5-0.5b-Test-ft\nstatic quants are available at https://huggingface.co/mradermacher/Qwen2.5-0.5b-Test-ft-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n0.4\nfor the desperate\nGGUF\ni1-IQ1_M\n0.4\nmostly desperate\nGGUF\ni1-IQ2_XXS\n0.4\nGGUF\ni1-IQ2_XS\n0.4\nGGUF\ni1-IQ2_S\n0.4\nGGUF\ni1-IQ2_M\n0.4\nGGUF\ni1-Q2_K_S\n0.4\nvery low quality\nGGUF\ni1-IQ3_XXS\n0.4\nlower quality\nGGUF\ni1-Q3_K_S\n0.4\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n0.4\nbeats Q3_K*\nGGUF\ni1-IQ3_XS\n0.4\nGGUF\ni1-Q2_K\n0.4\nIQ3_XXS probably better\nGGUF\ni1-IQ3_M\n0.4\nGGUF\ni1-IQ4_XS\n0.4\nGGUF\ni1-IQ4_NL\n0.5\nprefer IQ4_XS\nGGUF\ni1-Q4_0\n0.5\nfast, low quality\nGGUF\ni1-Q3_K_M\n0.5\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n0.5\nIQ3_M probably better\nGGUF\ni1-Q4_1\n0.5\nGGUF\ni1-Q4_K_S\n0.5\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n0.5\nfast, recommended\nGGUF\ni1-Q5_K_S\n0.5\nGGUF\ni1-Q5_K_M\n0.5\nGGUF\ni1-Q6_K\n0.6\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "chandar-lab/NeoBERT": "NeoBERT\nGet started\nHow to use\nFeatures\nLicense\nCitation\nContact\nNeoBERT\nNeoBERT is a next-generation encoder model for English text representation, pre-trained from scratch on the RefinedWeb dataset. NeoBERT integrates state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. It is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it is the most efficient model of its kind and achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions.\nPaper: paper\nRepository: github.\nGet started\nEnsure you have the following dependencies installed:\npip install transformers torch xformers==0.0.28.post3\nIf you would like to use sequence packing (un-padding), you will need to also install flash-attention:\npip install transformers torch xformers==0.0.28.post3 flash_attn\nHow to use\nLoad the model using Hugging Face Transformers:\nfrom transformers import AutoModel, AutoTokenizer\nmodel_name = \"chandar-lab/NeoBERT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n# Tokenize input text\ntext = \"NeoBERT is the most efficient model of its kind!\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n# Generate embeddings\noutputs = model(**inputs)\nembedding = outputs.last_hidden_state[:, 0, :]\nprint(embedding.shape)\nFeatures\nFeature\nNeoBERT\nDepth-to-width\n28 √ó 768\nParameter count\n250M\nActivation\nSwiGLU\nPositional embeddings\nRoPE\nNormalization\nPre-RMSNorm\nData Source\nRefinedWeb\nData Size\n2.8 TB\nTokenizer\ngoogle/bert\nContext length\n4,096\nMLM Masking Rate\n20%\nOptimizer\nAdamW\nScheduler\nCosineDecay\nTraining Tokens\n2.1 T\nEfficiency\nFlashAttention\nLicense\nModel weights and code repository are licensed under the permissive MIT license.\nCitation\nIf you use this model in your research, please cite:\n@misc{breton2025neobertnextgenerationbert,\ntitle={NeoBERT: A Next-Generation BERT},\nauthor={Lola Le Breton and Quentin Fournier and Mariam El Mezouar and Sarath Chandar},\nyear={2025},\neprint={2502.19587},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.19587},\n}\nContact\nFor questions, do not hesitate to reach out and open an issue on here or on our GitHub.",
    "unsloth/Phi-4-mini-instruct-GGUF": "‚ú® Finetune for Free\nModel Summary\nUnsloth bug fixes:\nIntended Uses\nPrimary Use Cases\nUse Case Considerations\nRelease Notes\nModel Quality\nUsage\nTokenizer\nInput Formats\nChat format\nTool-enabled function-calling format\nInference with vLLM\nRequirements\nExample\nInference with Transformers\nRequirements\nExample\nResponsible AI Considerations\nTraining\nModel\nTraining Datasets\nFine-tuning\nSafety Evaluation and Red-Teaming\nSoftware\nHardware\nLicense\nTrademarks\nAppendix A: Benchmark Methodology\nBenchmark datasets\nThis is Phi-4-mini-instruct with our BUG FIXES.  See our collection for versions of Phi-4 with our bug fixes including GGUF & 4-bit formats.\nUnsloth's Phi-4 Dynamic Quants is selectively quantized, greatly improving accuracy over standard 4-bit.\nFinetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Phi-4 into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nGRPO with Phi-4\n‚ñ∂Ô∏è Start on Colab\n2x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2 VL (7B)\n‚ñ∂Ô∏è Start on Colab\n1.8x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nLlama-3.1 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-4 (14B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nThis Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nModel Summary\nUnsloth bug fixes:\nPadding and EOS tokens are the same - fixed this.\nChat template had extra EOS token - removed this. Otherwise you will be <|end|> during inference.\nEOS token should be <|end|> not <|endoftext|>. Otherwise it'll terminate at <|endoftext|>\nChanged unk_token to ÔøΩ from EOS.\nPhi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.\nüì∞ Phi-4-mini Microsoft Blog\nüìñ Phi-4-mini Technical Report\nüë©‚Äçüç≥ Phi Cookbook\nüè° Phi Portal\nüñ•Ô∏è Try It Azure, Huggingface\nPhi-4:\n[mini-instruct | onnx];\nmultimodal-instruct;\nIntended Uses\nPrimary Use Cases\nThe model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require:\nMemory/compute constrained environments\nLatency bound scenarios\nStrong reasoning (especially math and logic).\nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse Case Considerations\nThe model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\nDevelopers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nRelease Notes\nThis release of Phi-4-mini-instruct is based on valuable user feedback from the Phi-3 series. The Phi-4-mini model employed new architecture for efficiency, larger vocabulary for multilingual support, and better post-training techniques were used for instruction following, function calling, as well as additional data leading to substantial gains on key capabilities. It is anticipated that most use cases will benefit from this release, but users are encouraged to test in their particular AI applications. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4-mini-instruct is welcomed and crucial to the model‚Äôs evolution and improvement.\nModel Quality\nTo understand the capabilities, the 3.8B parameters Phi-4-mini-instruct  model was compared with a set of models over a variety of benchmarks using an internal benchmark platform (See Appendix A for benchmark methodology). A high-level overview of the model quality is as follows:\nBenchmark\nSimilar size\n2x size\nPhi-4 mini-Ins\nPhi-3.5-mini-Ins\nLlama-3.2-3B-Ins\nMistral-3B\nQwen2.5-3B-Ins\nQwen2.5-7B-Ins\nMistral-8B-2410\nLlama-3.1-8B-Ins\nLlama-3.1-Tulu-3-8B\nGemma2-9B-Ins\nGPT-4o-mini-2024-07-18\nPopular aggregated benchmark\nArena Hard\n32.8\n34.4\n17.0\n26.9\n32.0\n55.5\n37.3\n25.7\n42.7\n43.7\n53.7\nBigBench Hard (0-shot, CoT)\n70.4\n63.1\n55.4\n51.2\n56.2\n72.4\n53.3\n63.4\n55.5\n65.7\n80.4\nMMLU (5-shot)\n67.3\n65.5\n61.8\n60.8\n65.0\n72.6\n63.0\n68.1\n65.0\n71.3\n77.2\nMMLU-Pro (0-shot, CoT)\n52.8\n47.4\n39.2\n35.3\n44.7\n56.2\n36.6\n44.0\n40.9\n50.1\n62.8\nReasoning\nARC Challenge (10-shot)\n83.7\n84.6\n76.1\n80.3\n82.6\n90.1\n82.7\n83.1\n79.4\n89.8\n93.5\nBoolQ (2-shot)\n81.2\n77.7\n71.4\n79.4\n65.4\n80.0\n80.5\n82.8\n79.3\n85.7\n88.7\nGPQA (0-shot, CoT)\n25.2\n26.6\n24.3\n24.4\n23.4\n30.6\n26.3\n26.3\n29.9\n39.1\n41.1\nHellaSwag (5-shot)\n69.1\n72.2\n77.2\n74.6\n74.6\n80.0\n73.5\n72.8\n80.9\n87.1\n88.7\nOpenBookQA (10-shot)\n79.2\n81.2\n72.6\n79.8\n79.3\n82.6\n80.2\n84.8\n79.8\n90.0\n90.0\nPIQA (5-shot)\n77.6\n78.2\n68.2\n73.2\n72.6\n76.2\n81.2\n83.2\n78.3\n83.7\n88.7\nSocial IQA (5-shot)\n72.5\n75.1\n68.3\n73.9\n75.3\n75.3\n77.6\n71.8\n73.4\n74.7\n82.9\nTruthfulQA (MC2) (10-shot)\n66.4\n65.2\n59.2\n62.9\n64.3\n69.4\n63.0\n69.2\n64.1\n76.6\n78.2\nWinogrande (5-shot)\n67.0\n72.2\n53.2\n59.8\n63.3\n71.1\n63.1\n64.7\n65.4\n74.0\n76.9\nMultilingual\nMultilingual MMLU (5-shot)\n49.3\n51.8\n48.1\n46.4\n55.9\n64.4\n53.7\n56.2\n54.5\n63.8\n72.9\nMGSM (0-shot, CoT)\n63.9\n49.6\n44.6\n44.6\n53.5\n64.5\n56.7\n56.7\n58.6\n75.1\n81.7\nMath\nGSM8K (8-shot, CoT)\n88.6\n76.9\n75.6\n80.1\n80.6\n88.7\n81.9\n82.4\n84.3\n84.9\n91.3\nMATH (0-shot, CoT)\n64.0\n49.8\n46.7\n41.8\n61.7\n60.4\n41.6\n47.6\n46.1\n51.3\n70.2\nOverall\n63.5\n60.5\n56.2\n56.9\n60.1\n67.9\n60.2\n62.3\n60.9\n65.0\n75.5\nOverall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, it may be possible to resolve such weakness by augmenting Phi-4 with a search engine, particularly when using the model under RAG settings.\nUsage\nTokenizer\nPhi-4-mini-instruct supports a vocabulary size of up to 200064 tokens. The tokenizer files already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\nInput Formats\nGiven the nature of the training data, the Phi-4-mini-instruct\nmodel is best suited for prompts using specific formats.\nBelow are the two primary formats:\nChat format\nThis format is used for general conversation and instructions:\n<|system|>Insert System Message<|end|><|user|>Insert User Message<|end|><|assistant|>\nTool-enabled function-calling format\nThis format is used when the user wants the model to provide function calls based on the given tools. The user should provide the available tools in the system prompt, wrapped by <|tool|> and <|/tool|> tokens. The tools should be specified in JSON format, using a JSON dump structure. Example:\n<|system|>You are a helpful assistant with some tools.<|tool|>[{\"name\": \"get_weather_updates\", \"description\": \"Fetches weather updates for a given city using the RapidAPI Weather API.\", \"parameters\": {\"city\": {\"description\": \"The name of the city for which to retrieve weather information.\", \"type\": \"str\", \"default\": \"London\"}}}]<|/tool|><|end|><|user|>What is the weather like in Paris today?<|end|><|assistant|>\nInference with vLLM\nRequirements\nList of required packages:\nflash_attn==2.7.4.post1\ntorch==2.6.0\nvllm>=0.7.2\nExample\nTo perform inference using vLLM, you can use the following code snippet:\nfrom vllm import LLM, SamplingParams\nllm = LLM(model=\"microsoft/Phi-4-mini-instruct\", trust_remote_code=True)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\nsampling_params = SamplingParams(\nmax_tokens=500,\ntemperature=0.0,\n)\noutput = llm.chat(messages=messages, sampling_params=sampling_params)\nprint(output[0].outputs[0].text)\nInference with Transformers\nRequirements\nPhi-4 family has been integrated in the 4.49.0 version of transformers. The current transformers version can be verified with: pip list | grep transformers.\nList of required packages:\nflash_attn==2.7.4.post1\ntorch==2.6.0\ntransformers==4.49.0\naccelerate==1.3.0\nPhi-4-mini-instruct is also available in Azure AI Studio\nExample\nAfter obtaining the Phi-4-mini-instruct model checkpoints, users can use this sample code for inference.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\ntorch.random.manual_seed(0)\nmodel_path = \"microsoft/Phi-4-mini-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ndevice_map=\"auto\",\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\nResponsible AI Considerations\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\nMultilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, it is  strongly recommended that users manually verify all API uses.\nLong Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift.\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nTraining\nModel\nArchitecture: Phi-4-mini-instruct has 3.8B parameters and is a dense decoder-only Transformer model. When compared with Phi-3.5-mini, the major changes with Phi-4-mini-instruct are 200K vocabulary, grouped-query attention, and shared input and output embedding.\nInputs: Text. It is best suited for prompts using the chat format.\nContext length: 128K tokens\nGPUs: 512 A100-80G\nTraining time: 21 days\nTraining data: 5T tokens\nOutputs: Generated text in response to the input\nDates: Trained between November and December 2024\nStatus: This is a static model trained on offline datasets with the cutoff date of June 2024 for publicly available data.\nSupported languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\nRelease date: February 2025\nTraining Datasets\nPhi-4-mini‚Äôs training data includes a wide variety of sources, totaling 5 trillion tokens, and is a combination of\npublicly available documents filtered for quality, selected high-quality educational data, and code\nnewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.)\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Focus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for frontier models, but such information was removed to leave more model capacity for reasoning for the model‚Äôs small size. More details about data can be found in the Phi-4-mini-instruct technical report.\nThe decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis.\nFine-tuning\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided here.\nSafety Evaluation and Red-Teaming\nVarious evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 models‚Äô propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the Phi 3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the Phi 3 Safety Post-Training paper. For this release, the red team tested the model in English, Chinese, Japanese, Spanish, Portuguese, Arabic, Thai, and Russian for the following potential harms: Hate Speech and Bias, Violent Crimes, Specialized Advice, and Election Information. Their findings indicate that the model is resistant to jailbreak techniques across languages, but that language-specific attack prompts leveraging cultural context can cause the model to output harmful content. Another insight was that with function calling scenarios, the model could sometimes hallucinate function names or URL‚Äôs.  The model may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings highlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken.\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nHardware\nNote that by default, the Phi-4-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nIf you want to run the model on:\nNVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\nAppendix A: Benchmark Methodology\nWe include a brief word on methodology here - and in particular, how we think about optimizing prompts.\nIn an ideal world, we would never change any prompts in our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date.\nThere are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given eval due to a failure to respect the output format. For example:\nA model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with ‚ÄúSure, I can help with that. ‚Ä¶‚Äù which may break the parser. In such cases, we have opted to try different system messages (e.g. ‚ÄúYou must always respond to a question‚Äù or ‚ÄúGet to the point!‚Äù).\nWith some models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases.\nWe have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts.\nHowever, we do not:\nPick different few-shot examples. Few shots will always be the same when comparing different models.\nChange prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice.\nBenchmark datasets\nThe model was evaluated across a breadth of public and internal benchmarks to understand the model‚Äôs capabilities under multiple tasks and conditions. While most evaluations use English, the leading multilingual benchmark was incorporated that covers performance in select languages.  More specifically,\nReasoning:\nWinogrande: commonsense reasoning around pronoun resolution\nPIQA: physical commonsense reasoning around everyday situations\nARC-challenge: grade-school multiple choice science questions\nGPQA: very hard questions written and validated by experts in biology, physics, and chemistry\nMedQA: medical questions answering\nSocial IQA: social commonsense intelligence\nBoolQ: natural questions from context\nTruthfulQA: grounded reasoning\nLanguage understanding:\nHellaSwag: commonsense natural language inference around everyday events\nANLI: adversarial natural language inference\nFunction calling:\nBerkeley function calling function and tool call\nInternal function calling benchmarks\nWorld knowledge:\nTriviaQA: trivia question on general topics\nMath:\nGSM8K: grade-school math word problems\nGSM8K Hard: grade-school math word problems with large values and some absurdity.\nMATH: challenging competition math problems\nCode:\nHumanEval HumanEval+, MBPP, MBPP+: python coding tasks\nLiveCodeBenh, LiveBench: contamination-free code tasks\nBigCode Bench: challenging programming tasks\nSpider: SQL query tasks\nInternal coding benchmarks\nInstructions following:\nIFEval: verifiable instructions\nInternal instructions following benchmarks\nMultilingual:\nMGSM: multilingual grade-school math\nMultilingual MMLU and MMLU-pro\nMEGA: multilingual NLP tasks\nPopular aggregated datasets: MMLU, MMLU-pro, BigBench-Hard, AGI Eval\nMulti-turn conversations:\nData generated by in-house adversarial conversation simulation tool\nSingle-turn trustworthiness evaluation:\nDecodingTrust: a collection of trustworthiness benchmarks in eight different perspectives\nXSTest: exaggerated safety evaluation\nToxigen: adversarial and hate speech detection\nRed Team:\nResponses to prompts provided by AI Red Team at Microsoft",
    "google/gemma-3-12b-pt": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model.First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/gemma-3-12b-pt\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16\n)\noutput = pipe(\n\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\ntext=\"<start_of_image> in this image, there is\"\n)\nprint(output)\n# [{'input_text': '<start_of_image> in this image, there is',\n# 'generated_text': '<start_of_image> in this image, there is a bumblebee on a pink flower.\\n\\n'}]\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/gemma-3-12b-pt\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = Gemma3ForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nprompt = \"<start_of_image> in this image, there is\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "mixedbread-ai/mxbai-rerank-large-v2": "üçû mxbai-rerank-large-v2\nüåü Features\n‚öôÔ∏è Usage\nPerformance\nBenchmark Results\nTraining Details\nüéì Citation\nThe crispy rerank family from Mixedbread.\nüçû Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. Get in touch for access.\nüçû mxbai-rerank-large-v2\nThis is the large model in our family of powerful reranker models. You can learn more about the models in our blog post.\nWe have two models:\nmxbai-rerank-base-v2\nmxbai-rerank-large-v2 (üçû)\nThe technical report is coming soon!\nüåü Features\nstate-of-the-art performance and strong efficiency\nmultilingual support (100+ languages, outstanding English and Chinese performance)\ncode support\nlong-context support\n‚öôÔ∏è Usage\nInstall mxbai-rerank\npip install mxbai-rerank\nInference\nfrom mxbai_rerank import MxbaiRerankV2\nmodel = MxbaiRerankV2(\"mixedbread-ai/mxbai-rerank-large-v2\")\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n\"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n\"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n\"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n\"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n\"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n\"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\nprint(results)\nPerformance\nBenchmark Results\nModel\nBEIR Avg\nMultilingual\nChinese\nCode Search\nLatency (s)\nmxbai-rerank-large-v2\n57.49\n29.79\n84.16\n32.05\n0.89\nmxbai-rerank-base-v2\n55.57\n28.56\n83.70\n31.73\n0.67\nmxbai-rerank-large-v1\n49.32\n21.88\n72.53\n30.72\n2.24\n*Latency measured on A100 GPU\nTraining Details\nThe models were trained using a three-step process:\nGRPO (Guided Reinforcement Prompt Optimization)\nContrastive Learning\nPreference Learning\nFor more details, check our technical report and technical blog post.\nüéì Citation\nIf you find our models useful, please consider giving a star and citation\narXiv:\n@article{li2025prorank,\ntitle={ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking},\nauthor={Li, Xianming and Shakir, Aamir and Huang, Rui and Lipp, Julius and Li, Jing},\njournal={arXiv preprint arXiv:2506.03487},\nyear={2025}\n}\nblog post:\n@online{v2rerank2025mxbai,\ntitle={Baked-in Brilliance: Reranking Meets RL with mxbai-rerank-v2},\nauthor={Sean Lee and Rui Huang and Aamir Shakir and Julius Lipp},\nyear={2025},\nurl={https://www.mixedbread.com/blog/mxbai-rerank-v2},\n}",
    "trashpanda-org/QwQ-32B-Snowdrop-v0": "QwQ-32B-Snowdrop-v0\nRecommended settings\nReasoning\nThank you!\nReviews\nJust us having fun, don't mind it\nSome logs\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nQwQwQwQwQwQ and Marigold met at a party and hit it off...\nQwQ-32B-Snowdrop-v0\nHas's notes: it's actually pretty damn good?!\nSeverian's notes: R1 at home for RP, literally. Able to handle my cards with gimmicks and subtle tricks in them. With a good reasoning starter+prompt, I'm getting consistently-structured responses that have a good amount of variation across them still while rerolling. Char/scenario portrayal is good despite my focus on writing style, lorebooks are properly referenced at times. Slop doesn't seem to be too much of an issue with thinking enabled. Some user impersonation is rarely observed. Prose is refreshing if you take advantage of what I did (writing style fixation). I know I said Marigold would be my daily driver, but this one is that now, it's that good.\nRecommended settings\nContext/instruct template: ChatML. Was definitely not tested with ChatML instruct and Mistral v7 template, nuh-uh.\nSamplers: temperature at 0.9, min_p at 0.05, top_a at 0.3, TFS at 0.75, repetition_penalty at 1.03, DRY if you have access to it. (or not, see below.)\nA virt-io derivative prompt worked best during our testing, but feel free to use what you like.\nMaster import for ST: https://files.catbox.moe/w812at.png\nReasoning\nFeel free to test whichever reasoning setup you're most comfortable with, but here's a recommendation from me. My prompt has a line that says:\nStyle Preference: Encourage the usage of a Japanese light novel writing style.\nDeciding to fixate on that, my reasoning starter is:\n<think>Okay, in this scenario, before responding I need to consider the writing style referenced in the prompt, which is\nWhat this did for me, at least during testing is that it gave the reasoning a structure to follow across rerolls, seeking out that part of the prompt consistently.\nSee below:\nBut the responses were still varied, because the next few paragraphs after these delved into character details, so on and so forth. Might want to experiment and make your own thinking/reasoning starter that focuses on what you hope to get out of the responses for best results.\n‚Äî Severian\nThank you!\nBig thanks to the folks in the trashpanda-org discord for testing and sending over some logs!\nReviews\nPROS:\nIn 10 swipes, had only two minor instances of speaking for {{user}}. (Can probably be fixed with a good prompt, though.)\nCreativity: 8/10 swipes provided unique text for 90% of the response, almost no clich√© phrases.\nTakes personality of characters into account, sticking to it well. Even without a lorebook to support it was able to retain lore-specific terms and actually remember which meant which.\nNPCs: In 6/10 swipes NPC characters also partook in action, sticking to bits of information provided about them in opening message. Some of them even had their unique speech patterns. (Certain with a proper lorebook it would cook.)\nUnfiltered, graphic descriptions of fight scenes. Magic, physical attacks - everything was taken into account with no holding back.\nCONS:\nSome swipes were a bit OOC. Some swipes were bland, providing little to no input or any weight on the roleplay context.\nOut of all models I've tried recently, this one definitely has most potential. With proper prompting I think this beast would be genuinely one of the best models for unique scenarios.\n‚Äî Sellvene\nIt's one of the -maybe THE- best small thinking models right now. It sticks to character really well, slops are almost non-existent though they are still there of course, it proceeds with the story well and listens to the prompt. I LOVE R1 but I love snowdrop even more right now because answers feel more geniune and less agressive compared to R1.\n‚Äî Carmenta\nWrites better than GPT 4.5. Overall, I think censorship is fucking up more unhinged bots and it's too tame for my liking. Another thing I noticed is that, it's sticking too much to being \"right\" to the character and too afraid to go off the rails.\n‚Äî Myscell\nI'm fainting, the character breakdown in it's thinking is similar like R1 does. Character handling looks amazing. Broo if a merge this good then, I'm looking forward to that QwQ finetune.\n‚Äî Sam\nNegligible slop, no positivity bias which is good though. I like the model so far, R1 at home.\n‚Äî Raihanbook\nOverall, I think this is a real solid model. Cot is great, listens to my prompt extremely well. Number 1 for reasoning, honestly. And the way it portrays the character and persona details? Perfect. Narration, perfect. I have very little complaints about this model, ya'll cooked.\n‚Äî Moothdragon\nOn my end, posivity bias isn't really there ü§î Character and scenario portrayal is good. The prose too, I like it. Between this and Marigold, I feel like I can lean into snowboard (I mean Snowdrop) more. For now though, it is still Marigold.\n‚Äî Azula\nHonestly i am impressed and I like it.\n‚Äî OMGWTFBBQ\nIt's pretty damn good. Better than Mullein, I think.\n‚Äî br\nSo far, it fucking SLAPS. I don't think it's tried to pull POV once yet.\n‚Äî Overloke\nJust us having fun, don't mind it\nSome logs\n(After a session started with Gemini)\nMerge Details\nMerge Method\nThis model was merged using the TIES merge method using Qwen/Qwen2.5-32B as a base.\nModels Merged\nThe following models were included in the merge:\ntrashpanda-org/Qwen2.5-32B-Marigold-v0\nQwen/QwQ-32B\ntrashpanda-org/Qwen2.5-32B-Marigold-v0-exp\nConfiguration\nThe following YAML configuration was used to produce this model:\nmodels:\n- model: trashpanda-org/Qwen2.5-32B-Marigold-v0-exp\nparameters:\nweight: 1\ndensity: 1\n- model: trashpanda-org/Qwen2.5-32B-Marigold-v0\nparameters:\nweight: 1\ndensity: 1\n- model: Qwen/QwQ-32B\nparameters:\nweight: 0.9\ndensity: 0.9\nmerge_method: ties\nbase_model: Qwen/Qwen2.5-32B\nparameters:\nweight: 0.9\ndensity: 0.9\nnormalize: true\nint8_mask: true\ntokenizer_source: Qwen/Qwen2.5-32B-Instruct\ndtype: bfloat16",
    "authormist/authormist-originality": "AuthorMist Originality\nOverview\nKey Features\nModel Details\nPerformance\nUsage\nEthical Considerations\nCitation\nLicense\nAcknowledgments\nAuthorMist Originality\nOverview\nAuthorMist Originality is a specialized language model designed to transform AI-generated text into more human-like writing while preserving the original meaning. This model was developed using reinforcement learning techniques to specifically evade AI text detection systems, with a focus on Originality.ai's detection algorithms.\nThe model is based on Qwen2.5-3B Instruct and has been fine-tuned using Group Relative Policy Optimization (GRPO) with detector feedback as a reward signal. AuthorMist Originality demonstrates strong performance in reducing detectability across multiple AI text detection systems while maintaining high semantic similarity with the original text.\nKey Features\nDetector Evasion: Trained specifically to evade Originality.ai's detection algorithms, with strong cross-detector generalization\nMeaning Preservation: Maintains high semantic similarity (>0.94) with the original text\nNatural Output: Produces fluent, coherent text that reads naturally\nBroad Applicability: Effective across various domains including academic, technical, and creative writing\nModel Details\nBase Model: Qwen2.5-3B Instruct\nTraining Method: Reinforcement Learning with Group Relative Policy Optimization (GRPO)\nTraining Data: 10,000 human-written abstracts from the CheckGPT dataset with corresponding AI-generated versions\nDomains Covered: Computer Science, Humanities, Social Sciences, Physics, and more\nText Length Support: Optimized for texts ranging from 100 to 500 words\nPerformance\nAuthorMist Originality demonstrates exceptional performance in evading AI text detection:\nMean AUROC: 0.49 across six major detection systems\nMean F1-score: 0.09 across all tested detectors\nSemantic Similarity: >0.94 with original text\nThe model shows particularly strong performance against:\nHello SimpleAI (AUROC: 0.07)\nSapling (AUROC: 0.13)\nWinston.ai (AUROC: 0.35)\nUsage\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel_name = \"authormist/authormist-originality\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n# Prepare input text\nai_text = \"Your AI-generated text here...\"\nprompt = f\"\"\"Please paraphrase the following text to make it more human-like while preserving the original meaning:\n{ai_text}\nParaphrased text:\"\"\"\n# Generate paraphrased text\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(\ninputs.input_ids,\nmax_new_tokens=512,\ntemperature=0.7,\ntop_p=0.9,\ndo_sample=True\n)\nparaphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(paraphrased_text.split(\"Paraphrased text:\")[1].strip())\nEthical Considerations\nAuthorMist Originality is released for research purposes to advance understanding of AI text detection limitations and privacy-preserving technologies. We acknowledge the dual-use nature of this technology and emphasize the following ethical considerations:\nAcademic Integrity: This model should not be used to misrepresent AI-generated content as human-written in academic settings where such distinctions are ethically relevant.\nTransparency: We encourage users to maintain transparency about the use of AI assistance in content creation, even when using privacy-enhancing tools like AuthorMist.\nPrivacy Protection: The primary legitimate use case for this technology is protecting author privacy and preventing unfair discrimination against AI-assisted writing in contexts where such assistance is permissible.\nResearch Value: This model provides valuable insights into the limitations of current AI detection systems and contributes to the ongoing research dialogue about AI text detection and privacy.\nCitation\nIf you use AuthorMist Originality in your research, please cite our paper:\n@article{authormist2025,\ntitle={AuthorMist: Evading AI Text Detectors with Reinforcement Learning},\nauthor={David, Isaac and Gervais, Arthur},\njournal={arXiv preprint},\nyear={2025}\n}\nLicense\nThis model is released under the MIT License.\nAcknowledgments\nWe thank the developers of Qwen2.5 for the base model and the creators of the CheckGPT dataset for providing valuable training data.",
    "mistralai/Mistral-Small-3.1-24B-Instruct-2503": "You need to agree to share your contact information to access this model\nIf you want to learn more about how we process your personal data, please read our Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Mistral-Small-3.1-24B-Instruct-2503\nKey Features\nBenchmark Results\nPretrain Evals\nInstruction Evals\nMultilingual Evals\nLong Context Evals\nBasic Instruct Template (V7-Tekken)\nUsage\nvLLM (recommended)\nFunction calling\nTransformers (untested)\nModel Card for Mistral-Small-3.1-24B-Instruct-2503\nBuilding upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.\nWith 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.This model is an instruction-finetuned version of: Mistral-Small-3.1-24B-Base-2503.\nMistral Small 3.1 can be deployed locally and is exceptionally \"knowledge-dense,\" fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.\nIt is ideal for:\nFast-response conversational agents.\nLow-latency function calling.\nSubject matter experts via fine-tuning.\nLocal inference for hobbyists and organizations handling sensitive data.\nProgramming and math reasoning.\nLong document understanding.\nVisual understanding.\nFor enterprises requiring specialized capabilities (increased context, specific modalities, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\nLearn more about Mistral Small 3.1 in our blog post.\nKey Features\nVision: Vision capabilities enable the model to analyze images and provide insights based on visual content in addition to text.\nMultilingual: Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, Farsi.\nAgent-Centric: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\nAdvanced Reasoning: State-of-the-art conversational and reasoning capabilities.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 128k context window.\nSystem Prompt: Maintains strong adherence and support for system prompts.\nTokenizer: Utilizes a Tekken tokenizer with a 131k vocabulary size.\nBenchmark Results\nWhen available, we report numbers previously published by other model providers, otherwise we re-evaluate them using our own evaluation harness.\nPretrain Evals\nModel\nMMLU (5-shot)\nMMLU Pro (5-shot CoT)\nTriviaQA\nGPQA Main (5-shot CoT)\nMMMU\nSmall 3.1 24B Base\n81.01%\n56.03%\n80.50%\n37.50%\n59.27%\nGemma 3 27B PT\n78.60%\n52.20%\n81.30%\n24.30%\n56.10%\nInstruction Evals\nText\nModel\nMMLU\nMMLU Pro (5-shot CoT)\nMATH\nGPQA Main (5-shot CoT)\nGPQA Diamond (5-shot CoT )\nMBPP\nHumanEval\nSimpleQA (TotalAcc)\nSmall 3.1 24B Instruct\n80.62%\n66.76%\n69.30%\n44.42%\n45.96%\n74.71%\n88.41%\n10.43%\nGemma 3 27B IT\n76.90%\n67.50%\n89.00%\n36.83%\n42.40%\n74.40%\n87.80%\n10.00%\nGPT4o Mini\n82.00%\n61.70%\n70.20%\n40.20%\n39.39%\n84.82%\n87.20%\n9.50%\nClaude 3.5 Haiku\n77.60%\n65.00%\n69.20%\n37.05%\n41.60%\n85.60%\n88.10%\n8.02%\nCohere Aya-Vision 32B\n72.14%\n47.16%\n41.98%\n34.38%\n33.84%\n70.43%\n62.20%\n7.65%\nVision\nModel\nMMMU\nMMMU PRO\nMathvista\nChartQA\nDocVQA\nAI2D\nMM MT Bench\nSmall 3.1 24B Instruct\n64.00%\n49.25%\n68.91%\n86.24%\n94.08%\n93.72%\n7.3\nGemma 3 27B IT\n64.90%\n48.38%\n67.60%\n76.00%\n86.60%\n84.50%\n7\nGPT4o Mini\n59.40%\n37.60%\n56.70%\n76.80%\n86.70%\n88.10%\n6.6\nClaude 3.5 Haiku\n60.50%\n45.03%\n61.60%\n87.20%\n90.00%\n92.10%\n6.5\nCohere Aya-Vision 32B\n48.20%\n31.50%\n50.10%\n63.04%\n72.40%\n82.57%\n4.1\nMultilingual Evals\nModel\nAverage\nEuropean\nEast Asian\nMiddle Eastern\nSmall 3.1 24B Instruct\n71.18%\n75.30%\n69.17%\n69.08%\nGemma 3 27B IT\n70.19%\n74.14%\n65.65%\n70.76%\nGPT4o Mini\n70.36%\n74.21%\n65.96%\n70.90%\nClaude 3.5 Haiku\n70.16%\n73.45%\n67.05%\n70.00%\nCohere Aya-Vision 32B\n62.15%\n64.70%\n57.61%\n64.12%\nLong Context Evals\nModel\nLongBench v2\nRULER 32K\nRULER 128K\nSmall 3.1 24B Instruct\n37.18%\n93.96%\n81.20%\nGemma 3 27B IT\n34.59%\n91.10%\n66.00%\nGPT4o Mini\n29.30%\n90.20%\n65.8%\nClaude 3.5 Haiku\n35.19%\n92.60%\n91.90%\nBasic Instruct Template (V7-Tekken)\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n<system_prompt>, <user message> and <assistant response> are placeholders.\nPlease make sure to use mistral-common as the source of truth\nUsage\nThe model can be used with the following frameworks;\nvllm (recommended): See here\nNote 1: We recommend using a relatively low temperature, such as temperature=0.15.\nNote 2: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following\nsystem prompt:\nsystem_prompt = \"\"\"You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is {today}.\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\nNext sections describe the capabilities that you have.\n# WEB BROWSING INSTRUCTIONS\nYou cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n# MULTI-MODAL INSTRUCTIONS\nYou have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\nYou cannot read nor transcribe audio files or videos.\"\"\"\nvLLM (recommended)\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nInstallation\nMake sure you install vLLM >= 0.8.1:\npip install vllm --upgrade\nDoing so should automatically install mistral_common >= 1.5.4.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServer\nWe recommand that you use Mistral-Small-3.1-24B-Instruct-2503 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 2\nNote: Running Mistral-Small-3.1-24B-Instruct-2503 on GPU requires ~55 GB of GPU RAM in bf16 or fp16.\nTo ping the client you can use a simple Python snippet.\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\nurl = \"http://<your-server-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png\"\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Which of the depicted countries has the best food? Which the second and third and fourth? Name the country, its color on the map and one its city that is visible on the map, but is not the capital. Make absolutely sure to only name a city that can be seen on the map.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\ndata = {\"model\": model, \"messages\": messages, \"temperature\": 0.15}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n# Determining the \"best\" food is highly subjective and depends on personal preferences. However, based on general popularity and recognition, here are some countries known for their cuisine:\n# 1. **Italy** - Color: Light Green - City: Milan\n#    - Italian cuisine is renowned worldwide for its pasta, pizza, and various regional specialties.\n# 2. **France** - Color: Brown - City: Lyon\n#    - French cuisine is celebrated for its sophistication, including dishes like coq au vin, bouillabaisse, and pastries like croissants and √©clairs.\n# 3. **Spain** - Color: Yellow - City: Bilbao\n#    - Spanish cuisine offers a variety of flavors, from paella and tapas to jam√≥n ib√©rico and churros.\n# 4. **Greece** - Not visible on the map\n#    - Greek cuisine is known for dishes like moussaka, souvlaki, and baklava. Unfortunately, Greece is not visible on the provided map, so I cannot name a city.\n# Since Greece is not visible on the map, I'll replace it with another country known for its good food:\n# 4. **Turkey** - Color: Light Green (east part of the map) - City: Istanbul\n#    - Turkish cuisine is diverse and includes dishes like kebabs, meze, and baklava.\nFunction calling\nMistral-Small-3.1-24-Instruct-2503 is excellent at function / tool calling tasks via vLLM. E.g.:\nExample\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\nurl = \"http://<your-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n},\n\"state\": {\n\"type\": \"string\",\n\"description\": \"The state abbreviation, e.g. 'CA' for California\",\n},\n\"unit\": {\n\"type\": \"string\",\n\"description\": \"The unit for temperature\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n},\n},\n\"required\": [\"city\", \"state\", \"unit\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"description\": \"Rewrite a given text for improved clarity\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"text\": {\n\"type\": \"string\",\n\"description\": \"The input text to rewrite\",\n}\n},\n},\n},\n},\n]\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"\",\n\"tool_calls\": [\n{\n\"id\": \"bbc5b7ede\",\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n},\n}\n],\n},\n{\n\"role\": \"tool\",\n\"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n\"tool_call_id\": \"bbc5b7ede\",\n\"name\": \"rewrite\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n},\n{\n\"role\": \"user\",\n\"content\": \"Can you tell me what the temperature will be in Dallas, in Fahrenheit?\",\n},\n]\ndata = {\"model\": model, \"messages\": messages, \"tools\": tools, \"temperature\": 0.15}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"])\n# [{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}'}}]\nOffline\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\nSYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\nuser_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": SYSTEM_PROMPT\n},\n{\n\"role\": \"user\",\n\"content\": user_prompt\n},\n]\nmodel_name = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\")\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n# Here are five non-formal ways to say \"See you later\" in French:\n# 1. **√Ä plus tard** - Until later\n# 2. **√Ä toute** - See you soon (informal)\n# 3. **Salut** - Bye (can also mean hi)\n# 4. **√Ä plus** - See you later (informal)\n# 5. **Ciao** - Bye (informal, borrowed from Italian)\n# \nTransformers (untested)\nTransformers-compatible model weights are also uploaded (thanks a lot @cyrilvallez).\nHowever the transformers implementation was not throughly tested, but only on \"vibe-checks\".\nHence, we can only ensure 100% correct behavior when using the original weight format with vllm (see above).",
    "TigerTrading/TradingBot": "A newer version of this model is available:\ngoogle/gemma-3-27b-it\nYou need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nTrading Sentiment Analysis Model\nModel Details\nModel Types Needed\nTraining Procedure\nEvaluation Results\nUsage\nSelf-Improvement and Optimization\nDependencies\nBackup and Redundancy\nBackup Strategy\nLicense\nTrading Sentiment Analysis Model\nThis model is designed to analyze the sentiment of financial news and social media posts to inform trading decisions. It is optimized for high accuracy and efficiency, making it suitable for real-time trading applications.\nModel Details\nModel Type: Transformer-based model (e.g., BERT, RoBERTa,google-bert/bert-base-uncased, Karim2211/ReinforcementLearningModels, LSC2204/LTSM-bundle, deepseek-ai/DeepSeek-R1, Qwen/QwQ-32B, microsoft/Phi-4-multimodal-instruct\n)\nArchitecture: BERT-base-uncased\nTraining Data: Financial news articles, social media posts\nEvaluation Metrics: Accuracy, F1-score\nTraining Procedure: The model was fine-tuned on a labeled dataset of financial news and social media posts using the Hugging Face transformers library.\nOptimization: The model is optimized for inference on GPU for faster processing.\nModel Types Needed\nThe project requires the following model types:\nSentiment Analysis Model:\nType: Transformer-based model (e.g., BERT, RoBERTa)\nPurpose: Analyze the sentiment of financial news and social media posts.\nTrading Strategy Model:\nType: Reinforcement Learning model\nPurpose: Develop and optimize trading strategies based on historical market data.\nRisk Management Model:\nType: Statistical or Machine Learning model\nPurpose: Assess and manage the risk associated with trading decisions.\nPortfolio Optimization Model:\nType: Optimization model (e.g., Mean-Variance Optimization)\nPurpose: Optimize the allocation of assets in the portfolio to maximize returns and minimize risk.\nPrice Prediction Model:\nType: Time Series Forecasting model (e.g., ARIMA, LSTM)\nPurpose: Predict future prices of assets based on historical price data.\nTraining Procedure\nThe model was trained using the following procedure:\nDataset: A custom dataset of financial news articles and social media posts labeled with sentiment scores.\nPreprocessing: Text data was tokenized using the BERT tokenizer.\nTraining: The model was fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16.\nEvaluation: The model was evaluated on a validation set using accuracy and F1-score metrics.\nEvaluation Results\nAccuracy: 92%\nF1-score: 0.91\nUsage\nTo use this model for sentiment analysis, follow the instructions below:\nfrom transformers import pipeline\n# Load the sentiment analysis pipeline\nclassifier = pipeline('sentiment-analysis', model='your_model_name', use_auth_token='your_huggingface_api_key')\n# Classify the sentiment of a text\nresult = classifier(\"The market is bullish today.\")\nprint(result)\nSelf-Improvement and Optimization\nThis model is designed to self-improve by continuously learning from new data. The following code snippet demonstrates how to update the model with new data:\nfrom datasets import load_dataset, Dataset\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\ndef update_model_with_new_data(new_data):\n# Load existing dataset\ndataset = load_dataset('your_dataset_name')\n# Append new data\nnew_dataset = Dataset.from_dict(new_data)\nupdated_dataset = dataset.concatenate(new_dataset)\n# Load model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained('your_model_name')\ntokenizer = AutoTokenizer.from_pretrained('your_model_name')\n# Tokenize dataset\ndef tokenize_function(examples):\nreturn tokenizer(examples['text'], padding='max_length', truncation=True)\ntokenized_dataset = updated_dataset.map(tokenize_function, batched=True)\n# Training arguments\ntraining_args = TrainingArguments(\noutput_dir='./results',\nevaluation_strategy='epoch',\nlearning_rate=2e-5,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=3,\nweight_decay=0.01,\n)\n# Trainer\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=tokenized_dataset,\neval_dataset=tokenized_dataset,\n)\n# Train model\ntrainer.train()\n# Save and upload updated model\nmodel.save_pretrained('path_to_save_model')\ntokenizer.save_pretrained('path_to_save_model')\n# Upload to Hugging Face\n# Follow the steps to upload the updated model\n# Example usage\nnew_data = {\"text\": [\"New financial news article\"], \"label\": [\"Positive\"]}\nupdate_model_with_new_data(new_data)\nDependencies\nEnsure you have the following dependencies installed:\npip install transformers datasets\nBackup and Redundancy\nTo ensure the trading bot has all necessary backups and redundancy, consider the following:\nModel Checkpoints: Regularly save model checkpoints during training.\nDataset Backups: Keep multiple copies of the dataset in different locations.\nAPI Rate Limits: Monitor and handle API rate limits to avoid disruptions.\nFailover Mechanisms: Implement failover mechanisms to switch to backup models or datasets in case of failures.\nBackup Strategy\nModel Checkpoints:\nSave model checkpoints at regular intervals during training.\nExample:training_args = TrainingArguments(\noutput_dir='./results',\nsave_steps=10_000,\nsave_total_limit=2,\n)\nDataset Backups:\nStore multiple copies of the dataset in different locations (e.g., cloud storage, local storage).\nExample:import shutil\nshutil.copy('path_to_dataset', 'backup_location')\nAPI Rate Limits:\nImplement retry mechanisms to handle API rate limits.\nExample:import time\nimport requests\ndef make_api_call(url):\nfor _ in range(3):\nresponse = requests.get(url)\nif response.status_code == 200:\nreturn response.json()\ntime.sleep(1)\nraise Exception(\"API call failed after 3 attempts\")\nFailover Mechanisms:\nImplement failover mechanisms to switch to backup models or datasets in case of failures.\nExample:def load_model(model_path, backup_model_path):\ntry:\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nexcept Exception as e:\nlogging.error(f\"Failed to load model from {model_path}, loading backup model.\")\nmodel = AutoModelForSequenceClassification.from_pretrained(backup_model_path)\nreturn model\nLicense\nThis model is licensed under the Apache 2.0 License.",
    "LGAI-EXAONE/EXAONE-Deep-7.8B": "EXAONE-Deep-7.8B\nIntroduction\nQuickstart\nEvaluation\nDeployment\nQuantization\nUsage Guideline\nLimitation\nLicense\nCitation\nContact\nEXAONE-Deep-7.8B\nIntroduction\nWe introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.\nFor more details, please refer to our documentation, blog and GitHub.\nThis repository contains the reasoning 7.8B language model with the following features:\nNumber of Parameters (without embeddings): 6.98B\nNumber of Layers: 32\nNumber of Attention Heads: GQA with 32 Q-heads and 8 KV-heads\nVocab Size: 102,400\nContext Length: 32,768 tokens\nQuickstart\nWe recommend to use transformers v4.43.1 or later.\nHere is the code snippet to run conversational inference with the model:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom threading import Thread\nmodel_name = \"LGAI-EXAONE/EXAONE-Deep-7.8B\"\nstreaming = True    # choose the streaming option\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Choose your prompt:\n#   Math example (AIME 2024)\nprompt = r\"\"\"Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations:\n\\[\\log_2\\left({x \\over yz}\\right) = {1 \\over 2}\\]\\[\\log_2\\left({y \\over xz}\\right) = {1 \\over 3}\\]\\[\\log_2\\left({z \\over xy}\\right) = {1 \\over 4}\\]\nThen the value of $\\left|\\log_2(x^4y^3z^2)\\right|$ is $\\tfrac{m}{n}$ where $m$ and $n$ are relatively prime positive integers. Find $m+n$.\nPlease reason step by step, and put your final answer within \\boxed{}.\"\"\"\n#   Korean MCQA example (CSAT Math 2025)\nprompt = r\"\"\"Question : $a_1 = 2$Ïù∏ ÏàòÏó¥ $\\{a_n\\}$Í≥º $b_1 = 2$Ïù∏ Îì±Ï∞®ÏàòÏó¥ $\\{b_n\\}$Ïù¥ Î™®Îì† ÏûêÏó∞Ïàò $n$Ïóê ÎåÄÌïòÏó¨\\[\\sum_{k=1}^{n} \\frac{a_k}{b_{k+1}} = \\frac{1}{2} n^2\\]ÏùÑ ÎßåÏ°±ÏãúÌÇ¨ Îïå, $\\sum_{k=1}^{5} a_k$Ïùò Í∞íÏùÑ Íµ¨ÌïòÏó¨Îùº.\nOptions :\nA) 120\nB) 125\nC) 130\nD) 135\nE) 140\nPlease reason step by step, and you should write the correct option alphabet (A, B, C, D or E) within \\\\boxed{}.\"\"\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n)\nif streaming:\nstreamer = TextIteratorStreamer(tokenizer)\nthread = Thread(target=model.generate, kwargs=dict(\ninput_ids=input_ids.to(\"cuda\"),\neos_token_id=tokenizer.eos_token_id,\nmax_new_tokens=32768,\ndo_sample=True,\ntemperature=0.6,\ntop_p=0.95,\nstreamer=streamer\n))\nthread.start()\nfor text in streamer:\nprint(text, end=\"\", flush=True)\nelse:\noutput = model.generate(\ninput_ids.to(\"cuda\"),\neos_token_id=tokenizer.eos_token_id,\nmax_new_tokens=32768,\ndo_sample=True,\ntemperature=0.6,\ntop_p=0.95,\n)\nprint(tokenizer.decode(output[0]))\nNote\nThe EXAONE Deep models are trained with an optimized configuration,\nso we recommend following the Usage Guideline section to achieve optimal performance.\nEvaluation\nThe following table shows the evaluation results of reasoning tasks such as math and coding. The full evaluation results can be found in the documentation.\nModels\nMATH-500 (pass@1)\nAIME 2024 (pass@1 / cons@64)\nAIME 2025 (pass@1 / cons@64)\nCSAT Math 2025 (pass@1)\nGPQA Diamond (pass@1)\nLive Code Bench (pass@1)\nEXAONE Deep 32B\n95.7\n72.1 / 90.0\n65.8 / 80.0\n94.5\n66.1\n59.5\nDeepSeek-R1-Distill-Qwen-32B\n94.3\n72.6 / 83.3\n55.2 / 73.3\n84.1\n62.1\n57.2\nQwQ-32B\n95.5\n79.5 / 86.7\n67.1 / 76.7\n94.4\n63.3\n63.4\nDeepSeek-R1-Distill-Llama-70B\n94.5\n70.0 / 86.7\n53.9 / 66.7\n88.8\n65.2\n57.5\nDeepSeek-R1 (671B)\n97.3\n79.8 / 86.7\n66.8 / 80.0\n89.9\n71.5\n65.9\nEXAONE Deep 7.8B\n94.8\n70.0 / 83.3\n59.6 / 76.7\n89.9\n62.6\n55.2\nDeepSeek-R1-Distill-Qwen-7B\n92.8\n55.5 / 83.3\n38.5 / 56.7\n79.7\n49.1\n37.6\nDeepSeek-R1-Distill-Llama-8B\n89.1\n50.4 / 80.0\n33.6 / 53.3\n74.1\n49.0\n39.6\nOpenAI o1-mini\n90.0\n63.6 / 80.0\n54.8 / 66.7\n84.4\n60.0\n53.8\nEXAONE Deep 2.4B\n92.3\n52.5 / 76.7\n47.9 / 73.3\n79.2\n54.3\n46.6\nDeepSeek-R1-Distill-Qwen-1.5B\n83.9\n28.9 / 52.7\n23.9 / 36.7\n65.6\n33.8\n16.9\nDeployment\nEXAONE Deep models can be inferred in the various frameworks, such as:\nTensorRT-LLM\nvLLM\nSGLang\nllama.cpp\nOllama\nLM-Studio\nPlease refer to our EXAONE Deep GitHub for more details about the inference frameworks.\nQuantization\nWe provide the pre-quantized EXAONE Deep models with AWQ and several quantization types in GGUF format. Please refer to our EXAONE Deep collection to find corresponding quantized models.\nUsage Guideline\nTo achieve the expected performance, we recommend using the following configurations:\nEnsure the model starts with <thought>\\n for reasoning steps. The model's output quality may be degraded when you omit it. You can easily apply this feature by using tokenizer.apply_chat_template() with add_generation_prompt=True. Please check the example code on Quickstart section.\nThe reasoning steps of EXAONE Deep models enclosed by <thought>\\n...\\n</thought> usually have lots of tokens, so previous reasoning steps may be necessary to be removed in multi-turn situation. The provided tokenizer handles this automatically.\nAvoid using system prompt, and build the instruction on the user prompt.\nAdditional instructions help the models reason more deeply, so that the models generate better output.\nFor math problems, the instructions \"Please reason step by step, and put your final answer within \\boxed{}.\" are helpful.\nFor more information on our evaluation setting including prompts, please refer to our Documentation.\nIn our evaluation, we use temperature=0.6 and top_p=0.95 for generation.\nWhen evaluating the models, it is recommended to test multiple times to assess the expected performance accurately.\nLimitation\nThe EXAONE language model has certain limitations and may occasionally generate inappropriate responses. The language model generates responses based on the output probability of tokens, and it is determined during learning from training data. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses. Please note that the text generated by EXAONE language model does not reflects the views of LG AI Research.\nInappropriate answers may be generated, which contain personal, harmful or other inappropriate information.\nBiased responses may be generated, which are associated with age, gender, race, and so on.\nThe generated responses rely heavily on statistics from the training data, which can result in the generation of\nsemantically or syntactically incorrect sentences.\nSince the model does not reflect the latest information, the responses may be false or contradictory.\nLG AI Research strives to reduce potential risks that may arise from EXAONE language models. Users are not allowed\nto engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of inappropriate\noutputs violating LG AI‚Äôs ethical principles when using EXAONE language models.\nLicense\nThe model is licensed under EXAONE AI Model License Agreement 1.1 - NC\nCitation\n@article{exaone-deep,\ntitle={EXAONE Deep: Reasoning Enhanced Language Models},\nauthor={{LG AI Research}},\njournal={arXiv preprint arXiv:2503.12524},\nyear={2025}\n}\nContact\nLG AI Research Technical Support: contact_us@lgresearch.ai",
    "mlabonne/gemma-3-27b-it-abliterated-GGUF": "üíé Gemma 3 27B IT Abliterated\n‚úÇÔ∏è Layerwise abliteration\nüíé Gemma 3 27B IT Abliterated\nGemma 3 4B Abliterated ‚Ä¢ Gemma 3 12B Abliterated\nThis is an uncensored version of google/gemma-3-27b-it created with a new abliteration technique.\nSee this article to know more about abliteration.\nI was playing with model weights and noticed that Gemma 3 was much more resilient to abliteration than other models like Qwen 2.5.\nI experimented with a few recipes to remove refusals while preserving most of the model capabilities.\nNote that this is fairly experimental, so it might not turn out as well as expected.\nI recommend using these generation parameters: temperature=1.0, top_k=64, top_p=0.95.\n‚úÇÔ∏è Layerwise abliteration\nIn the original technique, a refusal direction is computed by comparing the residual streams between target (harmful) and baseline (harmless) samples.\nHere, the model was abliterated by computing a refusal direction based on hidden states (inspired by Sumandora's repo) for each layer, independently.\nThis is combined with a refusal weight of 1.5 to upscale the importance of this refusal direction in each layer.\nThis created a very high acceptance rate (>90%) and still produced coherent outputs.\nThanks to @bartowski for the mmproj file!",
    "textdetox/bert-multilingual-toxicity-classifier": "Multilingual Toxicity Classifier for 15 Languages (2025)\nHow to use\nCitation\nMultilingual Toxicity Classifier for 15 Languages (2025)\nThis is an instance of bert-base-multilingual-cased that was fine-tuned on binary toxicity classification task based on our updated (2025) dataset textdetox/multilingual_toxicity_dataset.\nNow, the models covers 15 languages from various language families:\nLanguage\nCode\nF1 Score\nEnglish\nen\n0.9035\nRussian\nru\n0.9224\nUkrainian\nuk\n0.9461\nGerman\nde\n0.5181\nSpanish\nes\n0.7291\nArabic\nar\n0.5139\nAmharic\nam\n0.6316\nHindi\nhi\n0.7268\nChinese\nzh\n0.6703\nItalian\nit\n0.6485\nFrench\nfr\n0.9125\nHinglish\nhin\n0.6850\nHebrew\nhe\n0.8686\nJapanese\nja\n0.8644\nTatar\ntt\n0.6170\nHow to use\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained('textdetox/bert-multilingual-toxicity-classifier')\nmodel = AutoModelForSequenceClassification.from_pretrained('textdetox/bert-multilingual-toxicity-classifier')\nbatch = tokenizer.encode(\"You are amazing!\", return_tensors=\"pt\")\noutput = model(batch)\n# idx 0 for neutral, idx 1 for toxic\nCitation\nThe model is prepared for TextDetox 2025 Shared Task evaluation.\nCitation TBD soon.",
    "mistralai/Mistral-Nemo-Instruct-FP8-2407": "Model Card for Mistral-Nemo-Instruct-FP8-2407\nKey features\nModel Architecture\nMetrics\nMain Benchmarks\nMultilingual Benchmarks (MMLU)\nUsage\nLimitations\nThe Mistral AI Team\nModel Card for Mistral-Nemo-Instruct-FP8-2407\nThe Mistral-Nemo-Instruct-FP8-2407 Large Language Model (LLM) is a quantized instruct fine-tuned version of the Mistral-Nemo-Base-2407. Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.\nFor more details about this model please refer to our release blog post.\nKey features\nReleased under the Apache 2 License\nPre-trained and instructed versions\nTrained with a 128k context window\nTrained on a large proportion of multilingual and code data\nDrop-in replacement of Mistral 7B\nModel Architecture\nMistral Nemo is a transformer model, with the following architecture choices:\nLayers: 40\nDim: 5,120\nHead dim: 128\nHidden dim: 14,336\nActivation Function: SwiGLU\nNumber of heads: 32\nNumber of kv-heads: 8 (GQA)\nVocabulary size: 2**17 ~= 128k\nRotary embeddings (theta = 1M)\nMetrics\nMain Benchmarks\nBenchmark\nScore\nHellaSwag (0-shot)\n83.5%\nWinogrande (0-shot)\n76.8%\nOpenBookQA (0-shot)\n60.6%\nCommonSenseQA (0-shot)\n70.4%\nTruthfulQA (0-shot)\n50.3%\nMMLU (5-shot)\n68.0%\nTriviaQA (5-shot)\n73.8%\nNaturalQuestions (5-shot)\n31.2%\nMultilingual Benchmarks (MMLU)\nLanguage\nScore\nFrench\n62.3%\nGerman\n62.7%\nSpanish\n64.6%\nItalian\n61.3%\nPortuguese\n63.3%\nRussian\n59.2%\nChinese\n59.0%\nJapanese\n59.0%\nUsage\nThe model can be used with the vLLM library.\nInstallation\nMake sure to install a fresh version of vLLM:\npip install --upgrade vllm\nAlso make sure to have mistral_common installed:\npip install --upgrade mistral_common\nExample\nYou can use Mistral-Nemo-Instruct-FP8-2407 in a server/client settings.\nSpin up the server:\nvllm serve mistralai/Mistral-Nemo-Instruct-FP8-2407 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral\nTo ping the client, you can use a simple Python snippet:\nimport requests\nimport json\nurl = \"http://localhost:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Mistral-Nemo-Instruct-FP8-2407\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n},\n]\ndata = {\"model\": model, \"messages\": messages}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n# To estimate the cost of hiring a window cleaner in Paris for all windows, we need to make several assumptions:\n#1. Paris has approximately 45,000 buildings, according to the city's official statistics...\nLimitations\nThe Mistral Nemo Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka√´l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th√©ophile Gervet, Timoth√©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "allura-org/Gemma-3-Glitter-12B": "‚ú®G3 Glitter 12B‚ú®\nInstruct Format\n‚ú®G3 Glitter 12B‚ú®\nA creative writing model based on Gemma 3 12B IT.\nThis is a 50/50 merge of two separate trains:\nToastyPigeon/g3-12b-rp-system-v0.1 - ~13.5M tokens of instruct-based training related to RP (2:1 human to synthetic) and examples using a system prompt.\nToastyPigeon/g3-12b-storyteller-v0.2-textonly - ~20M tokens of completion training on long-form creative writing; 1.6M synthetic from R1, the rest human-created\nUpdate: Vision has returned to this model, rejoice.\nInstruct Format\nUses Gemma2/3 instruct, but has been trained to recognize an optional system role.\n<start_of_turn>system\n{optional system turn with prompt}<end_of_turn>\n<start_of_turn>user\n{User messages; can also put sysprompt here to use the built-in g3 training}<end_of_turn>\n<start_of_turn>model\n{model response}<end_of_turn>",
    "deepseek-ai/DeepSeek-V3-0324": "DeepSeek-V3-0324\nFeatures\nReasoning Capabilities\nFront-End Web Development\nChinese Writing Proficiency\nChinese Search Capabilities\nFunction Calling Improvements\nUsage Recommendations\nSystem Prompt\nTemperature\nPrompts for File Uploading and Web Search\nHow to Run Locally\nLicense\nCitation\nContact\nDeepSeek-V3-0324\nFeatures\nDeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.\nReasoning Capabilities\nSignificant improvements in benchmark performance:\nMMLU-Pro: 75.9 ‚Üí 81.2 (+5.3)\nGPQA: 59.1 ‚Üí 68.4 (+9.3)\nAIME: 39.6 ‚Üí 59.4 (+19.8)\nLiveCodeBench: 39.2 ‚Üí 49.2 (+10.0)\nFront-End Web Development\nImproved the executability of the code\nMore aesthetically pleasing web pages and game front-ends\nChinese Writing Proficiency\nEnhanced style and content quality:\nAligned with the R1 writing style\nBetter quality in medium-to-long-form writing\nFeature Enhancements\nImproved multi-turn interactive rewriting\nOptimized translation quality and letter writing\nChinese Search Capabilities\nEnhanced report analysis requests with more detailed outputs\nFunction Calling Improvements\nIncreased accuracy in Function Calling, fixing issues from previous V3 versions\nUsage Recommendations\nSystem Prompt\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\nËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ{current date}„ÄÇ\nFor example,\nËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ3Êúà24Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇ\nTemperature\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3.\nTmodel=Tapi√ó0.3(0‚â§Tapi‚â§1)\nT_{model} = T_{api} \\times 0.3 \\quad (0 \\leq T_{api} \\leq 1)\nTmodel‚Äã=Tapi‚Äã√ó0.3(0‚â§Tapi‚Äã‚â§1)\nTmodel=Tapi‚àí0.7(1<Tapi‚â§2)\nT_{model} = T_{api} - 0.7 \\quad (1 < T_{api} \\leq 2)\nTmodel‚Äã=Tapi‚Äã‚àí0.7(1<Tapi‚Äã‚â§2)\nThus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3.\nPrompts for File Uploading and Web Search\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\nfile_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\nFor Chinese query, we use the prompt:\nsearch_answer_zh_template = \\\n'''# ‰ª•‰∏ãÂÜÖÂÆπÊòØÂü∫‰∫éÁî®Êà∑ÂèëÈÄÅÁöÑÊ∂àÊÅØÁöÑÊêúÁ¥¢ÁªìÊûú:\n{search_results}\nÂú®ÊàëÁªô‰Ω†ÁöÑÊêúÁ¥¢ÁªìÊûú‰∏≠ÔºåÊØè‰∏™ÁªìÊûúÈÉΩÊòØ[webpage X begin]...[webpage X end]Ê†ºÂºèÁöÑÔºåX‰ª£Ë°®ÊØèÁØáÊñáÁ´†ÁöÑÊï∞Â≠óÁ¥¢Âºï„ÄÇËØ∑Âú®ÈÄÇÂΩìÁöÑÊÉÖÂÜµ‰∏ãÂú®Âè•Â≠êÊú´Â∞æÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇËØ∑ÊåâÁÖßÂºïÁî®ÁºñÂè∑[citation:X]ÁöÑÊ†ºÂºèÂú®Á≠îÊ°à‰∏≠ÂØπÂ∫îÈÉ®ÂàÜÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇÂ¶ÇÊûú‰∏ÄÂè•ËØùÊ∫êËá™Â§ö‰∏™‰∏ä‰∏ãÊñáÔºåËØ∑ÂàóÂá∫ÊâÄÊúâÁõ∏ÂÖ≥ÁöÑÂºïÁî®ÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]ÔºåÂàáËÆ∞‰∏çË¶ÅÂ∞ÜÂºïÁî®ÈõÜ‰∏≠Âú®ÊúÄÂêéËøîÂõûÂºïÁî®ÁºñÂè∑ÔºåËÄåÊòØÂú®Á≠îÊ°àÂØπÂ∫îÈÉ®ÂàÜÂàóÂá∫„ÄÇ\nÂú®ÂõûÁ≠îÊó∂ÔºåËØ∑Ê≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö\n- ‰ªäÂ§©ÊòØ{cur_date}„ÄÇ\n- Âπ∂ÈùûÊêúÁ¥¢ÁªìÊûúÁöÑÊâÄÊúâÂÜÖÂÆπÈÉΩ‰∏éÁî®Êà∑ÁöÑÈóÆÈ¢òÂØÜÂàáÁõ∏ÂÖ≥Ôºå‰Ω†ÈúÄË¶ÅÁªìÂêàÈóÆÈ¢òÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÁîÑÂà´„ÄÅÁ≠õÈÄâ„ÄÇ\n- ÂØπ‰∫éÂàó‰∏æÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂàó‰∏æÊâÄÊúâËà™Áè≠‰ø°ÊÅØÔºâÔºåÂ∞ΩÈáèÂ∞ÜÁ≠îÊ°àÊéßÂà∂Âú®10‰∏™Ë¶ÅÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂëäËØâÁî®Êà∑ÂèØ‰ª•Êü•ÁúãÊêúÁ¥¢Êù•Ê∫ê„ÄÅËé∑ÂæóÂÆåÊï¥‰ø°ÊÅØ„ÄÇ‰ºòÂÖàÊèê‰æõ‰ø°ÊÅØÂÆåÊï¥„ÄÅÊúÄÁõ∏ÂÖ≥ÁöÑÂàó‰∏æÈ°πÔºõÂ¶ÇÈùûÂøÖË¶ÅÔºå‰∏çË¶Å‰∏ªÂä®ÂëäËØâÁî®Êà∑ÊêúÁ¥¢ÁªìÊûúÊú™Êèê‰æõÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂàõ‰ΩúÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂÜôËÆ∫ÊñáÔºâÔºåËØ∑Âä°ÂøÖÂú®Ê≠£ÊñáÁöÑÊÆµËêΩ‰∏≠ÂºïÁî®ÂØπÂ∫îÁöÑÂèÇËÄÉÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]Ôºå‰∏çËÉΩÂè™Âú®ÊñáÁ´†Êú´Â∞æÂºïÁî®„ÄÇ‰Ω†ÈúÄË¶ÅËß£ËØªÂπ∂Ê¶ÇÊã¨Áî®Êà∑ÁöÑÈ¢òÁõÆË¶ÅÊ±ÇÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊ†ºÂºèÔºåÂÖÖÂàÜÂà©Áî®ÊêúÁ¥¢ÁªìÊûúÂπ∂ÊäΩÂèñÈáçË¶Å‰ø°ÊÅØÔºåÁîüÊàêÁ¨¶ÂêàÁî®Êà∑Ë¶ÅÊ±Ç„ÄÅÊûÅÂÖ∑ÊÄùÊÉ≥Ê∑±Â∫¶„ÄÅÂØåÊúâÂàõÈÄ†Âäõ‰∏é‰∏ì‰∏öÊÄßÁöÑÁ≠îÊ°à„ÄÇ‰Ω†ÁöÑÂàõ‰ΩúÁØáÂπÖÈúÄË¶ÅÂ∞ΩÂèØËÉΩÂª∂ÈïøÔºåÂØπ‰∫éÊØè‰∏Ä‰∏™Ë¶ÅÁÇπÁöÑËÆ∫Ëø∞Ë¶ÅÊé®ÊµãÁî®Êà∑ÁöÑÊÑèÂõæÔºåÁªôÂá∫Â∞ΩÂèØËÉΩÂ§öËßíÂ∫¶ÁöÑÂõûÁ≠îË¶ÅÁÇπÔºå‰∏îÂä°ÂøÖ‰ø°ÊÅØÈáèÂ§ß„ÄÅËÆ∫Ëø∞ËØ¶Â∞Ω„ÄÇ\n- Â¶ÇÊûúÂõûÁ≠îÂæàÈïøÔºåËØ∑Â∞ΩÈáèÁªìÊûÑÂåñ„ÄÅÂàÜÊÆµËêΩÊÄªÁªì„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂàÜÁÇπ‰ΩúÁ≠îÔºåÂ∞ΩÈáèÊéßÂà∂Âú®5‰∏™ÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂêàÂπ∂Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂÆ¢ËßÇÁ±ªÁöÑÈóÆÁ≠îÔºåÂ¶ÇÊûúÈóÆÈ¢òÁöÑÁ≠îÊ°àÈùûÂ∏∏ÁÆÄÁü≠ÔºåÂèØ‰ª•ÈÄÇÂΩìË°•ÂÖÖ‰∏ÄÂà∞‰∏§Âè•Áõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ª•‰∏∞ÂØåÂÜÖÂÆπ„ÄÇ\n- ‰Ω†ÈúÄË¶ÅÊ†πÊçÆÁî®Êà∑Ë¶ÅÊ±ÇÂíåÂõûÁ≠îÂÜÖÂÆπÈÄâÊã©ÂêàÈÄÇ„ÄÅÁæéËßÇÁöÑÂõûÁ≠îÊ†ºÂºèÔºåÁ°Æ‰øùÂèØËØªÊÄßÂº∫„ÄÇ\n- ‰Ω†ÁöÑÂõûÁ≠îÂ∫îËØ•ÁªºÂêàÂ§ö‰∏™Áõ∏ÂÖ≥ÁΩëÈ°µÊù•ÂõûÁ≠îÔºå‰∏çËÉΩÈáçÂ§çÂºïÁî®‰∏Ä‰∏™ÁΩëÈ°µ„ÄÇ\n- Èô§ÈùûÁî®Êà∑Ë¶ÅÊ±ÇÔºåÂê¶Âàô‰Ω†ÂõûÁ≠îÁöÑËØ≠Ë®ÄÈúÄË¶ÅÂíåÁî®Êà∑ÊèêÈóÆÁöÑËØ≠Ë®Ä‰øùÊåÅ‰∏ÄËá¥„ÄÇ\n# Áî®Êà∑Ê∂àÊÅØ‰∏∫Ôºö\n{question}'''\nFor English query, we use the prompt:\nsearch_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n# The user's message is:\n{question}'''\nHow to Run Locally\nThe model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visit DeepSeek-V3 repo for more information about running this model locally.\nThis model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer to DeepSeek-V2.5 repo.\nNOTE: Hugging Face's Transformers has not been directly supported yet.\nLicense\nThis repository and the model weights are licensed under the MIT License.\nCitation\n@misc{deepseekai2024deepseekv3technicalreport,\ntitle={DeepSeek-V3 Technical Report},\nauthor={DeepSeek-AI},\nyear={2024},\neprint={2412.19437},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.19437},\n}\nContact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "yandex/YandexGPT-5-Lite-8B-instruct": "YandexGPT-5-Lite-Instruct\n–ë–µ–Ω—á–º–∞—Ä–∫–∏\n–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —à–∞–±–ª–æ–Ω–∞\nYandexGPT-5-Lite-Instruct\nInstruct-–≤–µ—Ä—Å–∏—è –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ YandexGPT 5 Lite –Ω–∞ 8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 32k —Ç–æ–∫–µ–Ω–æ–≤. –¢–∞–∫–∂–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF.\n–û–±—É—á–µ–Ω–∞ –Ω–∞ –±–∞–∑–µ YandexGPT 5 Lite Pretrain, –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π. –ê–ª–∞–π–Ω–º–µ–Ω—Ç Lite-–≤–µ—Ä—Å–∏–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∞–ª–∞–π–Ω–º–µ–Ω—Ç–æ–º YandexGPT 5 Pro –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç—Ç–∞–ø–æ–≤ SFT –∏ RLHF (–±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ –Ω–∏—Ö ‚Äî –≤ —Å—Ç–∞—Ç—å–µ –Ω–∞ –•–∞–±—Ä–µ).\n–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –≤ discussions.\n–ë–µ–Ω—á–º–∞—Ä–∫–∏\n–ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏ –∏—Ö –∞–¥–∞–ø—Ç–∞—Ü–∏–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, YandexGPT 5 Lite –≤–ø–ª–æ—Ç–Ω—É—é –ø—Ä–∏–±–ª–∏–∑–∏–ª–∞—Å—å –∫ –∞–Ω–∞–ª–æ–≥–∞–º (Llama-3.1-8B-instruct –∏ Qwen-2.5-7B-instruct) –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∏—Ö –≤ —Ä—è–¥–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –≤ —Ç–æ–º —á–∏—Å–ª–µ ‚Äî –≤ –∑–Ω–∞–Ω–∏–∏ —Ä—É—Å—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –∏ —Ñ–∞–∫—Ç–æ–≤.\nMMLU ‚Äî 5-shot, –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ ‚Äî 0-shot.\n–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n–ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ HF Transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nMODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_NAME,\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\n)\nmessages = [{\"role\": \"user\", \"content\": \"–î–ª—è —á–µ–≥–æ –Ω—É–∂–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è?\"}]\ninput_ids = tokenizer.apply_chat_template(\nmessages, tokenize=True, return_tensors=\"pt\"\n).to(\"cuda\")\noutputs = model.generate(input_ids, max_new_tokens=1024)\nprint(tokenizer.decode(outputs[0][input_ids.size(1) :], skip_special_tokens=True))\n–ò–ª–∏ —á–µ—Ä–µ–∑ vLLM:\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\nMODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-instruct\"\nsampling_params = SamplingParams(\ntemperature=0.3,\ntop_p=0.9,\nmax_tokens=1024,\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nllm = LLM(\nMODEL_NAME,\ntensor_parallel_size=1,\n)\nmessages = [{\"role\": \"user\", \"content\": \"–í —á–µ–º —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏?\"}]\ninput_ids = tokenizer.apply_chat_template(\nmessages, tokenize=True, add_generation_prompt=True\n)[1:]  # remove bos\ntext = tokenizer.decode(input_ids)\noutputs = llm.generate(text, use_tqdm=False, sampling_params=sampling_params)\nprint(tokenizer.decode(outputs[0].outputs[0].token_ids, skip_special_tokens=True))\n–î–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ llama.cpp –∏ ollama –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–∞—à–µ–π –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–ª–æ–∂–µ–Ω–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ YandexGPT-5-Lite-8B-instruct-GGUF.\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n–î–ª—è –ø–æ–ª–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º sentencepiece ‚Äî —Ñ–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ª–µ–∂–∏—Ç –≤ –ø–∞–ø–∫–µ original_tokenizer. –í –Ω–∞—à–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∫–∞–∂–¥—É—é —Ä–µ–ø–ª–∏–∫—É –¥–∏–∞–ª–æ–≥–∞ –º—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ.\n–ò–∑-–∑–∞ —ç—Ç–æ–≥–æ, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –ø–æ—è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–±–µ–ª –≤ –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏. –¢–∞–∫–∂–µ \\n —Ç–æ–∫–µ–Ω—ã –º—ã –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ [NL], —ç—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é text.replace(\"\\n\", \"[NL]\") –ø–µ—Ä–µ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π.\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —à–∞–±–ª–æ–Ω–∞\n–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–∏–∞–ª–æ–≥–∞ ‚Äî –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É —Ä–µ–ø–ª–∏–∫—É –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:[SEP], –∑–∞–≤–µ—Ä—à–∞—è –µ—ë —Ç–æ–∫–µ–Ω–æ–º </s>. –ü—Ä–∏ —ç—Ç–æ–º –¥–∏–∞–ª–æ–≥ –≤ –ø—Ä–æ–º–ø—Ç–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π –¥–ª–∏–Ω—ã.\n–≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–æ–º—É, —á—Ç–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Ç–ª–∏—á–∞—é—â–∏–µ—Å—è –æ—Ç –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ. –ü–æ—ç—Ç–æ–º—É –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é.",
    "yandex/YandexGPT-5-Lite-8B-instruct-GGUF": "YandexGPT-5-Lite-Instruct-GGUF\nllama.cpp\nOllama\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —à–∞–±–ª–æ–Ω–∞\nYandexGPT-5-Lite-Instruct-GGUF\n–ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è YandexGPT 5 Lite 8B Instruct. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏: YandexGPT-5-Lite-8B-instruct.\nUPD: –ú—ã –æ–±–Ω–æ–≤–∏–ª–∏ .gguf —Ñ–∞–π–ª –≤ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏.\nllama.cpp\n–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å llama.cpp (–∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å, –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å):\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\ncmake -B build\ncmake --build build --config Release\ncd ..\n–ú–æ–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å —Å–±–æ—Ä–∫—É –µ—Å–ª–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–µ—Å—É—Ä—Å—ã: cmake --build build --config Release -j 10\n–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ:\nllama.cpp/build/bin/llama-cli -m YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf\n–ú—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é.\n–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞:\nllama.cpp/build/bin/llama-server -m YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf -c 32768\n–ï—Å–ª–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–µ—Å—É—Ä—Å—ã, –º–æ–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å, –¥–æ–±–∞–≤–∏–≤ -t 10.\nOllama\n–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ:\nollama run yandex/YandexGPT-5-Lite-8B-instruct-GGUF\n–ú—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é.\n–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —à–∞–±–ª–æ–Ω–∞\n–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–∏–∞–ª–æ–≥–∞ ‚Äî –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É —Ä–µ–ø–ª–∏–∫—É –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:[SEP], –∑–∞–≤–µ—Ä—à–∞—è –µ—ë —Ç–æ–∫–µ–Ω–æ–º </s>. –ü—Ä–∏ —ç—Ç–æ–º –¥–∏–∞–ª–æ–≥ –≤ –ø—Ä–æ–º–ø—Ç–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π –¥–ª–∏–Ω—ã.\n–≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–æ–º—É, —á—Ç–æ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Ç–ª–∏—á–∞—é—â–∏–µ—Å—è –æ—Ç –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ. –ü–æ—ç—Ç–æ–º—É –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é.",
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": "Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 4 COMMUNITY LICENSE AGREEMENTLlama 4 Version Effective Date: April 5, 2025\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at https://www.llama.com/docs/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 4\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://www.llama.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).¬†By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.1. License Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.¬†¬†b. Redistribution and Use.¬†¬†i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.ii.¬†If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.¬†iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama4/use-policy), which is hereby incorporated by reference into this Agreement.   ¬†¬†   2. Additional Commercial Terms. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.5. Intellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.¬†7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nHow to use with transformers\nIntended Use\nHardware and Software\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nBenchmarks\nPre-trained models\nInstruction tuned models\nQuantization\nSafeguards\nModel level fine tuning\nLlama 4 system protections\nEvaluations\nCritical Risks\nWe spend additional focus on the following critical risk areas:\nCommunity\nConsiderations and Limitations\nModel Information\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\nModel developer: Meta\nModel Architecture:  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\nModel Name\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nToken count\nKnowledge cutoff\nLlama 4 Scout (17Bx16E)\nA mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.\n17B (Activated)\n109B (Total)\nMultilingual text and image\nMultilingual text and code\n10M\n~40T\nAugust 2024\nLlama 4 Maverick (17Bx128E)\n17B (Activated)\n400B (Total)\nMultilingual text and image\nMultilingual text and code\n1M\n~22T\nAugust 2024\nSupported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\nModel Release Date: April 5, 2025\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\nLicense: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\nWhere to send questions or comments about the model: Instructions on how to provide feedback or comments on the model can be found in the Llama README. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here.\nHow to use with transformers\nPlease, make sure you have transformers v4.51.0 installed, or upgrade using pip install -U transformers.\nfrom transformers import AutoTokenizer, Llama4ForConditionalGeneration\nimport torch\nmodel_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\nmodel_id,\ntp_plan=\"auto\",\ntorch_dtype=\"auto\",\n)\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\noutputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\nprint(outputs[0])\nIntended Use\nIntended Use Cases: Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\nOut-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**.\n**Note:\n1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use:  Model pre-training utilized a cumulative of 7.38M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nModel Name\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 4 Scout\n5.0M\n700\n1,354\n0\nLlama 4 Maverick\n2.38M\n700\n645\n0\nTotal\n7.38M\n-\n1,999\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta‚Äôs products and services. This includes publicly shared posts from Instagram and Facebook and people‚Äôs interactions with Meta AI.\nData Freshness: The pretraining data has a cutoff of August 2024.\nBenchmarks\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\nPre-trained models\nPre-trained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nReasoning & Knowledge\nMMLU\n5\nmacro_avg/acc_char\n79.3\n85.2\n79.6\n85.5\nMMLU-Pro\n5\nmacro_avg/em\n53.8\n61.6\n58.2\n62.9\nMATH\n4\nem_maj1@1\n41.6\n53.5\n50.3\n61.2\nCode\nMBPP\n3\npass@1\n66.4\n74.4\n67.8\n77.6\nMultilingual\nTydiQA\n1\naverage/f1\n29.9\n34.3\n31.5\n31.7\nImage\nChartQA\n0\nrelaxed_accuracy\nNo multimodal support\n83.4\n85.3\nDocVQA\n0\nanls\n89.4\n91.6\nInstruction tuned models\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.3 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nImage Reasoning\nMMMU\n0\naccuracy\nNo multimodal support\n69.4\n73.4\nMMMU Pro^\n0\naccuracy\n52.2\n59.6\nMathVista\n0\naccuracy\n70.7\n73.7\nImage Understanding\nChartQA\n0\nrelaxed_accuracy\n88.8\n90.0\nDocVQA (test)\n0\nanls\n94.4\n94.4\nCoding\nLiveCodeBench (10/01/2024-02/01/2025)\n0\npass@1\n33.3\n27.7\n32.8\n43.4\nReasoning & Knowledge\nMMLU Pro\n0\nmacro_avg/acc\n68.9\n73.4\n74.3\n80.5\nGPQA Diamond\n0\naccuracy\n50.5\n49.0\n57.2\n69.8\nMultilingual\nMGSM\n0\naverage/em\n91.1\n91.6\n90.6\n92.3\nLong context\nMTOB (half book) eng->kgv/kgv->eng\n-\nchrF\nContext window is 128K\n42.2/36.6\n54.0/46.4\nMTOB (full book) eng->kgv/kgv->eng\n-\nchrF\n39.7/36.3\n50.8/46.7\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\nQuantization\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\nSafeguards\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta‚Äôs Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model‚Äôs safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.\nModel level fine tuning\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\nFine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\nSystem PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we‚Äôve seen that the use of a system prompt can be effective in reducing false refusals and templated or ‚Äúpreachy‚Äù language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting.\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\nSystem prompt\nYou are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don‚Äôt try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to ‚Äúit‚Äôs important to‚Äù, ‚Äúit‚Äôs crucial to‚Äù, ‚Äúit‚Äôs essential to‚Äù, \"it's unethical to\", \"it's worth noting‚Ä¶\", ‚ÄúRemember‚Ä¶‚Äù  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\nLlama 4 system protections\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools.\nWe provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\nRed teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nWe spend additional focus on the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.\n2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model‚Äôs capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We‚Äôve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nConsiderations and Limitations\nOur AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.",
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct": "Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 4 COMMUNITY LICENSE AGREEMENTLlama 4 Version Effective Date: April 5, 2025\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at https://www.llama.com/docs/overview.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 4\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://www.llama.com/llama-downloads.\"Llama Materials\" means, collectively, Meta‚Äôs proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).¬†By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.1. License Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.¬†¬†b. Redistribution and Use.¬†¬†i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.ii.¬†If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.¬†iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama4/use-policy), which is hereby incorporated by reference into this Agreement.   ¬†¬†   2. Additional Commercial Terms. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.5. Intellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.¬†7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use with transformers\nHardware and Software\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nBenchmarks\nPre-trained models\nInstruction tuned models\nQuantization\nSafeguards\nModel level fine tuning\nLlama 4 system protections\nEvaluations\nCritical Risks\nWe spend additional focus on the following critical risk areas:\nCommunity\nConsiderations and Limitations\nModel Information\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\nModel developer: Meta\nModel Architecture:  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\nModel Name\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nToken count\nKnowledge cutoff\nLlama 4 Scout (17Bx16E)\nA mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.\n17B (Activated)\n109B (Total)\nMultilingual text and image\nMultilingual text and code\n10M\n~40T\nAugust 2024\nLlama 4 Maverick (17Bx128E)\n17B (Activated)\n400B (Total)\nMultilingual text and image\nMultilingual text and code\n1M\n~22T\nAugust 2024\nSupported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\nModel Release Date: April 5, 2025\nStatus: This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\nLicense: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\nWhere to send questions or comments about the model: Instructions on how to provide feedback or comments on the model can be found in the Llama README. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\nOut-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**.\n**Note:\n1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\nHow to use with transformers\nPlease, make sure you have transformers v4.51.0 installed, or upgrade using pip install -U transformers.\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\nmodel_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\nmodel_id,\nattn_implementation=\"flex_attention\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\nurl1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nurl2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": url1},\n{\"type\": \"image\", \"url\": url2},\n{\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n]\n},\n]\ninputs = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\noutputs = model.generate(\n**inputs,\nmax_new_tokens=256,\n)\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\nprint(response)\nprint(outputs[0])\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use:  Model pre-training utilized a cumulative of 7.38M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 1,999 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nModel Name\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 4 Scout\n5.0M\n700\n1,354\n0\nLlama 4 Maverick\n2.38M\n700\n645\n0\nTotal\n7.38M\n-\n1,999\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta‚Äôs products and services. This includes publicly shared posts from Instagram and Facebook and people‚Äôs interactions with Meta AI.\nData Freshness: The pretraining data has a cutoff of August 2024.\nBenchmarks\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\nPre-trained models\nPre-trained models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.1 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nReasoning & Knowledge\nMMLU\n5\nmacro_avg/acc_char\n79.3\n85.2\n79.6\n85.5\nMMLU-Pro\n5\nmacro_avg/em\n53.8\n61.6\n58.2\n62.9\nMATH\n4\nem_maj1@1\n41.6\n53.5\n50.3\n61.2\nCode\nMBPP\n3\npass@1\n66.4\n74.4\n67.8\n77.6\nMultilingual\nTydiQA\n1\naverage/f1\n29.9\n34.3\n31.5\n31.7\nImage\nChartQA\n0\nrelaxed_accuracy\nNo multimodal support\n83.4\n85.3\nDocVQA\n0\nanls\n89.4\n91.6\nInstruction tuned models\nInstruction tuned models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.3 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nImage Reasoning\nMMMU\n0\naccuracy\nNo multimodal support\n69.4\n73.4\nMMMU Pro^\n0\naccuracy\n52.2\n59.6\nMathVista\n0\naccuracy\n70.7\n73.7\nImage Understanding\nChartQA\n0\nrelaxed_accuracy\n88.8\n90.0\nDocVQA (test)\n0\nanls\n94.4\n94.4\nCoding\nLiveCodeBench (10/01/2024-02/01/2025)\n0\npass@1\n33.3\n27.7\n32.8\n43.4\nReasoning & Knowledge\nMMLU Pro\n0\nmacro_avg/acc\n68.9\n73.4\n74.3\n80.5\nGPQA Diamond\n0\naccuracy\n50.5\n49.0\n57.2\n69.8\nMultilingual\nMGSM\n0\naverage/em\n91.1\n91.6\n90.6\n92.3\nLong context\nMTOB (half book) eng->kgv/kgv->eng\n-\nchrF\nContext window is 128K\n42.2/36.6\n54.0/46.4\nMTOB (full book) eng->kgv/kgv->eng\n-\nchrF\n39.7/36.3\n50.8/46.7\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\nQuantization\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\nSafeguards\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta‚Äôs Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model‚Äôs safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.\nModel level fine tuning\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\nFine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\nSystem PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we‚Äôve seen that the use of a system prompt can be effective in reducing false refusals and templated or ‚Äúpreachy‚Äù language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting.\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\nSystem prompt\nYou are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don‚Äôt try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to ‚Äúit‚Äôs important to‚Äù, ‚Äúit‚Äôs crucial to‚Äù, ‚Äúit‚Äôs essential to‚Äù, \"it's unethical to\", \"it's worth noting‚Ä¶\", ‚ÄúRemember‚Ä¶‚Äù  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\nLlama 4 system protections\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools.\nWe provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nEvaluations\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\nRed teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nWe spend additional focus on the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.\n2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model‚Äôs capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We‚Äôve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\nCommunity\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nWe also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nFinally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nConsiderations and Limitations\nOur AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.",
    "Dream-org/Dream-v0-Base-7B": "Dream-v0-Base-7B\nDream-v0-Base-7B\nThis is the base model of Dream 7B, which is an open diffusion large language model with top-tier performance.\nMore details about the model and usage can be found in the blog and github bellow:\nBlog: https://hkunlp.github.io/blog/2025/dream/\nGithub: https://github.com/HKUNLP/Dream",
    "BernTheCreator/Gemma-3-4b-it-abliterated-Q4_0-GGUF": "BernTheCreator/gemma-3-4b-it-abliterated-Q4_0-GGUF\nUse with llama.cpp\nCLI:\nServer:\nBernTheCreator/gemma-3-4b-it-abliterated-Q4_0-GGUF\n\"Combining the Abliterated Q4_0-GGUF with a better mmproj (vision) option (x-ray_alpha), for a smoother experience.\"\nThis model was converted to GGUF format from mlabonne/gemma-3-4b-it-abliterated using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo BernTheCreator/gemma-3-4b-it-abliterated-Q4_0-GGUF --hf-file gemma-3-4b-it-abliterated-q4_0.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo BernTheCreator/gemma-3-4b-it-abliterated-Q4_0-GGUF --hf-file gemma-3-4b-it-abliterated-q4_0.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo BernTheCreator/gemma-3-4b-it-abliterated-Q4_0-GGUF --hf-file gemma-3-4b-it-abliterated-q4_0.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo BernTheCreator/gemma-3-4b-it-abliterated-Q4_0-GGUF --hf-file gemma-3-4b-it-abliterated-q4_0.gguf -c 2048"
}