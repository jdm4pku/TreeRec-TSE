{
    "jinaai/jina-embeddings-v4": "Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval\nIntended Usage & Model Info\nTraining & Evaluation\nUsage\nOptional / Recommended\nJina-VDR\nLicense\nContact\nCitation\nThe embedding model trained by Jina AI.\nJina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval\nGGUF | Blog | Technical Report | API\nIntended Usage & Model Info\njina-embeddings-v4 is a universal embedding model for multimodal and multilingual retrieval.\nThe model is specially designed for complex document retrieval, including visually rich documents with charts, tables, and illustrations.\nBuilt on Qwen/Qwen2.5-VL-3B-Instruct, jina-embeddings-v4 features:\nUnified embeddings for text, images, and visual documents, supporting both dense (single-vector) and late-interaction (multi-vector) retrieval.\nMultilingual support (30+ languages) and compatibility with a wide range of domains, including technical and visually complex documents.\nTask-specific adapters for retrieval, text matching, and code-related tasks, which can be selected at inference time.\nFlexible embedding size: dense embeddings are 2048 dimensions by default but can be truncated to as low as 128 with minimal performance loss.\nSummary of features:\nFeature\nJina Embeddings V4\nBase Model\nQwen2.5-VL-3B-Instruct\nSupported Tasks\nretrieval, text-matching, code\nModel DType\nBFloat 16\nMax Sequence Length\n32768\nSingle-Vector Dimension\n2048\nMulti-Vector Dimension\n128\nMatryoshka dimensions\n128, 256, 512, 1024, 2048\nPooling Strategy\nMean pooling\nAttention Mechanism\nFlashAttention2\nTraining & Evaluation\nPlease refer to our technical report of jina-embeddings-v4 for training details and benchmarks.\nUsage\nRequirements\nThe following Python packages are required:\ntransformers>=4.52.0\ntorch>=2.6.0\npeft>=0.15.2\ntorchvision\npillow\nOptional / Recommended\nflash-attention: Installing flash-attention is recommended for improved inference speed and efficiency, but not mandatory.\nsentence-transformers: If you want to use the model via the sentence-transformers interface, install this package as well.\nvia Jina AI Embeddings API\ncurl https://api.jina.ai/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $JINA_AI_API_TOKEN\" \\\n-d @- <<EOFEOF\n{\n\"model\": \"jina-embeddings-v4\",\n\"task\": \"text-matching\",\n\"input\": [\n{\n\"text\": \"ÿ∫ÿ±Ÿàÿ® ÿ¨ŸÖŸäŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ÿßÿ∑ÿ¶\"\n},\n{\n\"text\": \"Êµ∑Êª©‰∏äÁæé‰∏ΩÁöÑÊó•ËêΩ\"\n},\n{\n\"text\": \"A beautiful sunset over the beach\"\n},\n{\n\"text\": \"Un beau coucher de soleil sur la plage\"\n},\n{\n\"text\": \"Ein wundersch√∂ner Sonnenuntergang am Strand\"\n},\n{\n\"text\": \"ŒàŒΩŒ± œåŒºŒøœÅœÜŒø Œ∑ŒªŒπŒøŒ≤Œ±œÉŒØŒªŒµŒºŒ± œÄŒ¨ŒΩœâ Œ±œÄœå œÑŒ∑ŒΩ œÄŒ±œÅŒ±ŒªŒØŒ±\"\n},\n{\n\"text\": \"‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§§‡§ü ‡§™‡§∞ ‡§è‡§ï ‡§ñ‡•Ç‡§¨‡§∏‡•Ç‡§∞‡§§ ‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§æ‡§∏‡•ç‡§§\"\n},\n{\n\"text\": \"Un bellissimo tramonto sulla spiaggia\"\n},\n{\n\"text\": \"ÊµúËæ∫„Å´Ê≤à„ÇÄÁæé„Åó„ÅÑÂ§ïÊó•\"\n},\n{\n\"text\": \"Ìï¥Î≥Ä ÏúÑÎ°ú ÏïÑÎ¶ÑÎã§Ïö¥ ÏùºÎ™∞\"\n},\n{\n\"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n},\n{\n\"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n}\n]\n}\nEOFEOF\nvia transformers\n# !pip install transformers>=4.52.0 torch>=2.6.0 peft>=0.15.2 torchvision pillow\n# !pip install\nfrom transformers import AutoModel\nimport torch\n# Initialize the model\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v4\", trust_remote_code=True, torch_dtype=torch.float16)\nmodel.to(\"cuda\")\n# ========================\n# 1. Retrieval Task\n# ========================\n# Configure truncate_dim, max_length (for texts), max_pixels (for images), vector_type, batch_size in the encode function if needed\n# Encode query\nquery_embeddings = model.encode_text(\ntexts=[\"Overview of climate change impacts on coastal cities\"],\ntask=\"retrieval\",\nprompt_name=\"query\",\n)\n# Encode passage (text)\npassage_embeddings = model.encode_text(\ntexts=[\n\"Climate change has led to rising sea levels, increased frequency of extreme weather events...\"\n],\ntask=\"retrieval\",\nprompt_name=\"passage\",\n)\n# Encode image/document\nimage_embeddings = model.encode_image(\nimages=[\"https://i.ibb.co/nQNGqL0/beach1.jpg\"],\ntask=\"retrieval\",\n)\n# ========================\n# 2. Text Matching Task\n# ========================\ntexts = [\n\"ÿ∫ÿ±Ÿàÿ® ÿ¨ŸÖŸäŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ÿßÿ∑ÿ¶\",  # Arabic\n\"Êµ∑Êª©‰∏äÁæé‰∏ΩÁöÑÊó•ËêΩ\",  # Chinese\n\"Un beau coucher de soleil sur la plage\",  # French\n\"Ein wundersch√∂ner Sonnenuntergang am Strand\",  # German\n\"ŒàŒΩŒ± œåŒºŒøœÅœÜŒø Œ∑ŒªŒπŒøŒ≤Œ±œÉŒØŒªŒµŒºŒ± œÄŒ¨ŒΩœâ Œ±œÄœå œÑŒ∑ŒΩ œÄŒ±œÅŒ±ŒªŒØŒ±\",  # Greek\n\"‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§§‡§ü ‡§™‡§∞ ‡§è‡§ï ‡§ñ‡•Ç‡§¨‡§∏‡•Ç‡§∞‡§§ ‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§æ‡§∏‡•ç‡§§\",  # Hindi\n\"Un bellissimo tramonto sulla spiaggia\",  # Italian\n\"ÊµúËæ∫„Å´Ê≤à„ÇÄÁæé„Åó„ÅÑÂ§ïÊó•\",  # Japanese\n\"Ìï¥Î≥Ä ÏúÑÎ°ú ÏïÑÎ¶ÑÎã§Ïö¥ ÏùºÎ™∞\",  # Korean\n]\ntext_embeddings = model.encode_text(texts=texts, task=\"text-matching\")\n# ========================\n# 3. Code Understanding Task\n# ========================\n# Encode query\nquery_embedding = model.encode_text(\ntexts=[\"Find a function that prints a greeting message to the console\"],\ntask=\"code\",\nprompt_name=\"query\",\n)\n# Encode code\ncode_embeddings = model.encode_text(\ntexts=[\"def hello_world():\\n    print('Hello, World!')\"],\ntask=\"code\",\nprompt_name=\"passage\",\n)\n# ========================\n# 4. Use multivectors\n# ========================\nmultivector_embeddings = model.encode_text(\ntexts=texts,\ntask=\"retrieval\",\nprompt_name=\"query\",\nreturn_multivector=True,\n)\nimages = [\"https://i.ibb.co/nQNGqL0/beach1.jpg\", \"https://i.ibb.co/r5w8hG8/beach2.jpg\"]\nmultivector_image_embeddings = model.encode_image(\nimages=images,\ntask=\"retrieval\",\nreturn_multivector=True,\n)\nvia sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n# Initialize the model\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v4\", trust_remote_code=True)\n# ========================\n# 1. Retrieval Task\n# ========================\n# Encode query\nquery_embeddings = model.encode(\nsentences=[\"Overview of climate change impacts on coastal cities\"],\ntask=\"retrieval\",\nprompt_name=\"query\",\n)\nprint(f\"query_embeddings.shape = {query_embeddings.shape}\")\n# Encode passage (text)\npassage_embeddings = model.encode(\nsentences=[\n\"Climate change has led to rising sea levels, increased frequency of extreme weather events...\"\n],\ntask=\"retrieval\",\nprompt_name=\"passage\",\n)\nprint(f\"passage_embeddings.shape = {passage_embeddings.shape}\")\n# Encode image/document\nimage_embeddings = model.encode(\nsentences=[\"https://i.ibb.co/nQNGqL0/beach1.jpg\"],\ntask=\"retrieval\",\n)\nprint(f\"image_embeddings.shape = {image_embeddings.shape}\")\n# ========================\n# 2. Text Matching Task\n# ========================\ntexts = [\n\"ÿ∫ÿ±Ÿàÿ® ÿ¨ŸÖŸäŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ÿßÿ∑ÿ¶\",  # Arabic\n\"Êµ∑Êª©‰∏äÁæé‰∏ΩÁöÑÊó•ËêΩ\",  # Chinese\n\"Un beau coucher de soleil sur la plage\",  # French\n\"Ein wundersch√∂ner Sonnenuntergang am Strand\",  # German\n\"ŒàŒΩŒ± œåŒºŒøœÅœÜŒø Œ∑ŒªŒπŒøŒ≤Œ±œÉŒØŒªŒµŒºŒ± œÄŒ¨ŒΩœâ Œ±œÄœå œÑŒ∑ŒΩ œÄŒ±œÅŒ±ŒªŒØŒ±\",  # Greek\n\"‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§§‡§ü ‡§™‡§∞ ‡§è‡§ï ‡§ñ‡•Ç‡§¨‡§∏‡•Ç‡§∞‡§§ ‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§æ‡§∏‡•ç‡§§\",  # Hindi\n\"Un bellissimo tramonto sulla spiaggia\",  # Italian\n\"ÊµúËæ∫„Å´Ê≤à„ÇÄÁæé„Åó„ÅÑÂ§ïÊó•\",  # Japanese\n\"Ìï¥Î≥Ä ÏúÑÎ°ú ÏïÑÎ¶ÑÎã§Ïö¥ ÏùºÎ™∞\",  # Korean\n]\ntext_embeddings = model.encode(sentences=texts, task=\"text-matching\")\n# ========================\n# 3. Code Understanding Task\n# ========================\n# Encode query\nquery_embeddings = model.encode(\nsentences=[\"Find a function that prints a greeting message to the console\"],\ntask=\"code\",\nprompt_name=\"query\",\n)\n# Encode code\ncode_embeddings = model.encode(\nsentences=[\"def hello_world():\\n    print('Hello, World!')\"],\ntask=\"code\",\nprompt_name=\"passage\",\n)\n# ========================\n# 4. Use multivectors\n# ========================\n# If you want to use multi-vector embeddings, please use the Hugging Face model directly.\nvia vLLM\nWe provide separate model versions for each task (retrieval, text-matching, code) where specific adapter is merged into the base Qwen2.5-VL weights.\nThis modification enables native compatibility with vLLM.\nInstructions and usage examples for each task are available in their respective directories:\njina-embeddings-v4-vllm-retrieval\njina-embeddings-v4-vllm-text-matching\njina-embeddings-v4-vllm-code\nPlease refer to the directory that matches your task for more details.\nJina-VDR\nAlongside jina-embeddings-v4, we‚Äôre releasing Jina VDR, a multilingual, multi-domain benchmark for visual document retrieval. The task collection can be viewed here, and evaluation instructions can be found here.\nLicense\nThis model was initially released under cc-by-nc-4.0 due to an error.\nThe correct license is the Qwen Research License, as this model is derived from Qwen-2.5-VL-3B which is governed by that license.\nContact\nJoin our Discord community and chat with other community members about ideas.\nCitation\nIf you find jina-embeddings-v4 useful in your research, please cite the following paper:\n@misc{g√ºnther2025jinaembeddingsv4universalembeddingsmultimodal,\ntitle={jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\nauthor={Michael G√ºnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Sedigheh Eslami and Scott Martens and Bo Wang and Nan Wang and Han Xiao},\nyear={2025},\neprint={2506.18902},\narchivePrefix={arXiv},\nprimaryClass={cs.AI},\nurl={https://arxiv.org/abs/2506.18902},\n}",
    "fancyfeast/llama-joycaption-beta-one-hf-llava": "Model Card for Llama JoyCaption Beta One\nMotivation\nHow to Get Started with the Model\nvLLM\nModel Card for Llama JoyCaption Beta One\nGithub\nJoyCaption is an image captioning Visual Language Model (VLM) built from the ground up as a free, open, and uncensored model for the community to use in training Diffusion models.\nKey Features:\nFree and Open: Always released for free, open weights, no restrictions, and just like bigASP, will come with training scripts and lots of juicy details on how it gets built.\nUncensored: Equal coverage of SFW and NSFW concepts. No \"cylindrical shaped object with a white substance coming out on it\" here.\nDiversity: All are welcome here. Do you like digital art? Photoreal? Anime? Furry? JoyCaption is for everyone. Pains are being taken to ensure broad coverage of image styles, content, ethnicity, gender, orientation, etc.\nMinimal Filtering: JoyCaption is trained on large swathes of images so that it can understand almost all aspects of our world. almost. Illegal content will never be tolerated in JoyCaption's training.\nMotivation\nAutomated descriptive captions enable the training and finetuning of diffusion models on a wider range of images, since trainers are no longer required to either find images with already associated text or write the descriptions themselves. They also improve the quality of generations produced by Text-to-Image models trained on them (ref: DALL-E 3 paper). But to-date, the community has been stuck with ChatGPT, which is expensive and heavily censored; or alternative models, like CogVLM, which are weaker than ChatGPT and have abysmal performance outside of the SFW domain.\nI'm building JoyCaption to help fill this gap by performing near or on-par with GPT4o in captioning images, while being free, unrestricted, and open.\nHow to Get Started with the Model\nPlease see the Github for more details.\nExample usage:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nIMAGE_PATH = \"image.jpg\"\nPROMPT = \"Write a long descriptive caption for this image in a formal tone.\"\nMODEL_NAME = \"fancyfeast/llama-joycaption-beta-one-hf-llava\"\n# Load JoyCaption\n# bfloat16 is the native dtype of the LLM used in JoyCaption (Llama 3.1)\n# device_map=0 loads the model into the first GPU\nprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\nllava_model = LlavaForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=\"bfloat16\", device_map=0)\nllava_model.eval()\nwith torch.no_grad():\n# Load image\nimage = Image.open(IMAGE_PATH)\n# Build the conversation\nconvo = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful image captioner.\",\n},\n{\n\"role\": \"user\",\n\"content\": PROMPT,\n},\n]\n# Format the conversation\n# WARNING: HF's handling of chat's on Llava models is very fragile.  This specific combination of processor.apply_chat_template(), and processor() works\n# but if using other combinations always inspect the final input_ids to ensure they are correct.  Often times you will end up with multiple <bos> tokens\n# if not careful, which can make the model perform poorly.\nconvo_string = processor.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\nassert isinstance(convo_string, str)\n# Process the inputs\ninputs = processor(text=[convo_string], images=[image], return_tensors=\"pt\").to('cuda')\ninputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n# Generate the captions\ngenerate_ids = llava_model.generate(\n**inputs,\nmax_new_tokens=512,\ndo_sample=True,\nsuppress_tokens=None,\nuse_cache=True,\ntemperature=0.6,\ntop_k=None,\ntop_p=0.9,\n)[0]\n# Trim off the prompt\ngenerate_ids = generate_ids[inputs['input_ids'].shape[1]:]\n# Decode the caption\ncaption = processor.tokenizer.decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\ncaption = caption.strip()\nprint(caption)\nvLLM\nvLLM provides the highest performance inference for JoyCaption, and an OpenAI compatible API so JoyCaption can be used like any other VLMs.  Example usage:\nvllm serve fancyfeast/llama-joycaption-beta-one-hf-llava --max-model-len 4096 --enable-prefix-caching\nVLMs are a bit finicky on vLLM, and vLLM is memory hungry, so you may have to adjust settings for your particular environment, such as forcing eager mode, adjusting max-model-len, adjusting gpu_memory_utilization, etc.",
    "polymathic-ai/aion-base": "AION-1: Astronomical Omnimodal Network\nModel Details\nQuick Start\nSupported Data Types\nInstallation\nResources\nLicense\nAION-1: Astronomical Omnimodal Network\nAION-base is a 300M parameter large omnimodal model specifically designed for astronomical surveys. It\nintegrates 39 distinct astronomical data types and enables adaptation to a wide range of astronomical tasks\nthrough multimodal masked modeling.\nModel Details\nArchitecture: Encoder-Decoder Transformer (12 blocks each, 768 dim, 12 heads)\nParameters: 300M\nTraining: Multimodal Masked Modeling (4M) on astronomical survey data\nModalities: 39 data types including imaging, spectra, catalogs, and photometry\nQuick Start\nfrom aion import AION\n# Load the pretrained model\nmodel = AION.from_pretrained('polymathic-ai/aion-base')\n# Your astronomical analysis begins here!\nSupported Data Types\nAION-Base processes data from major astronomical surveys:\nImaging: Legacy Survey, HSC Wide\nSpectra: SDSS, DESI\nCatalog: Legacy Survey entries\nGaia: BP/RP spectra, parallax, coordinates, photometry\nPhotometry: Legacy Survey (g,r,i,z + WISE), HSC (g,r,i,z,y)\nShape: Ellipticity and morphological parameters\nInstallation\npip install polymathic-aion\nResources\nGitHub: https://github.com/PolymathicAI/AION\nTutorial: https://colab.research.google.com/github/PolymathicAI/AION/blob/main/notebooks/Tutorial.ipynb\nLicense\nMIT License - see https://github.com/PolymathicAI/AION/blob/main/LICENSE for details.\nBuilt with ‚ù§ for the astronomical community by https://polymathic-ai.org/",
    "tencent/SongGeneration": "SongGeneration\nModel Versions\nOverview\nLicense\nSongGeneration\nDemo ¬†|¬† Paper  ¬†|¬† Code  ¬†|¬† Space Demo\nThis repository is the official weight repository for LeVo: High-Quality Song Generation with Multi-Preference Alignment. In this repository, we provide the SongGeneration model, inference scripts, and the checkpoint that has been trained on the Million Song Dataset.\nModel Versions\nModel\nMax Length\nLanguage\nGPU Menmory\nRFT(A100)\nDownload Link\nSongGeneration-base\n2m30s\nzh\n10G/16G\n1.26\nYou were here\nSongGeneration-base-new\n2m30s\nzh, en\n10G/16G\n1.26\nHuggingface\nSongGeneration-base-full\n4m30s\nzh, en\n12G/18G\n1.30\nHuggingface\nSongGeneration-large\n4m30s\nzh, en\n22G/28G\n1.51\nHuggingface\nSongGeneration-v1.5-small\n2m\nzh, en, es, ja, etc.\n-\n-\nComing soon\nSongGeneration-v1.5-base\n4m30s\nzh, en, es, ja, etc.\n-\n-\nComing soon\nSongGeneration-v1.5-large\n4m30s\nzh, en, es, ja, etc.\n-\n-\nComing soon\nOverview\nWe develop the SongGeneration model. It is an LM-based framework consisting of LeLM and a music codec. LeLM is capable of parallelly modeling two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation. The music codec reconstructs the dual-track tokens into highfidelity music audio. SongGeneration significantly improves over the open-source music generation models and performs competitively with current state-of-the-art industry systems. For more details, please refer to our paper.\nLicense\nThe code and weights in this repository is released in the LICENSE  file.",
    "NewBie-AI/NewBie-image-v0.1-exp-model-repo": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nYAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nThis is the Hugging Face model repository where NewBieAi Lab stores each epoch of the NewBie image v0.1-exp model.\nWe have achieved some initial promising experimental results, but it is still not perfect.\nIf you would like to test it, please contact the NewBieAi administrator. We may grant you access and testing permission after evaluating your situation.\nBasic Information:\nNewBie image v0.1-exp is a new arch model that use the Lumina-image2.0 arch as its research target.\nAnime Type (we chose a neta lu2 model trained on luima arch to further study this type of task)\nText Encoder:\nGoogle/Gemma3-4b-it\nJina Ai/Jina Clip v2\nNetworks:\nNext-DIT 3.5b (modified from 26 layers to 36 layers)\nVAE:\nFlux 1 Dev-VAE\n---------------------<\nPretrain Dataset: full dan + 1m e621\nUse 8*h200 train for four months (23000 h200 hour)\nRestructure text data use XML format\nThe model is currently in the train phase (60% complete).\nOverall progress is approximately 80%.\nThe model is expected to be open sourced on Hugging Face on December 31, 2025.\nexampleÔºö",
    "Qwen/Qwen3-30B-A3B-Thinking-2507": "Qwen3-30B-A3B-Thinking-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nProcessing Ultra-Long Texts\nHow to Enable 1M Token Context\nBest Practices\nCitation\nQwen3-30B-A3B-Thinking-2507\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-30B-A3B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-30B-A3B-Thinking-2507, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nQwen3-30B-A3B-Thinking-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Paramaters (Non-Embedding): 29.9B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode. Meanwhile, specifying enable_thinking=True is no longer required.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nGemini2.5-Flash-Thinking\nQwen3-235B-A22B Thinking\nQwen3-30B-A3B Thinking\nQwen3-30B-A3B-Thinking-2507\nKnowledge\nMMLU-Pro\n81.9\n82.8\n78.5\n80.9\nMMLU-Redux\n92.1\n92.7\n89.5\n91.4\nGPQA\n82.8\n71.1\n65.8\n73.4\nSuperGPQA\n57.8\n60.7\n51.8\n56.8\nReasoning\nAIME25\n72.0\n81.5\n70.9\n85.0\nHMMT25\n64.2\n62.5\n49.8\n71.4\nLiveBench 20241125\n74.3\n77.1\n74.3\n76.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n61.2\n55.7\n57.4\n66.0\nCFEval\n1995\n2056\n1940\n2044\nOJBench\n23.5\n25.6\n20.7\n25.1\nAlignment\nIFEval\n89.8\n83.4\n86.5\n88.9\nArena-Hard v2$\n56.7\n61.5\n36.3\n56.0\nCreative Writing v3\n85.0\n84.6\n79.1\n84.4\nWritingBench\n83.9\n80.3\n77.0\n85.0\nAgent\nBFCL-v3\n68.6\n70.8\n69.1\n72.4\nTAU1-Retail\n65.2\n54.8\n61.7\n67.8\nTAU1-Airline\n54.0\n26.0\n32.0\n48.0\nTAU2-Retail\n66.7\n40.4\n34.2\n58.8\nTAU2-Airline\n52.0\n30.0\n36.0\n58.0\nTAU2-Telecom\n31.6\n21.9\n22.8\n26.3\nMultilingualism\nMultiIF\n74.4\n71.9\n72.2\n76.4\nMMLU-ProX\n80.2\n80.0\n73.1\n76.4\nINCLUDE\n83.9\n78.7\n71.9\n74.4\nPolyMATH\n49.8\n54.7\n46.1\n52.6\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --context-length 262144  --reasoning-parser deepseek-r1\nvLLM:vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'qwen3-30b-a3b-thinking-2507',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --served-model-name Qwen3-30B-A3B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-30B-A3B-Thinking-2507',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nTo support ultra-long context processing (up to 1 million tokens), we integrate two key techniques:\nDual Chunk Attention (DCA): A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence.\nMInference: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions.\nTogether, these innovations significantly improve both generation quality and inference efficiency for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a 3√ó speedup compared to standard attention implementations.\nFor full technical details, see the Qwen2.5-1M Technical Report.\nHow to Enable 1M Token Context\nTo effectively process a 1 million token context, users will require approximately 240 GB of total GPU memory. This accounts for model weights, KV-cache storage, and peak activation memory demands.\nStep 1: Update Configuration File\nDownload the model and replace the content of your config.json with config_1m.json, which includes the config for length extrapolation and sparse attention.\nexport MODELNAME=Qwen3-30B-A3B-Thinking-2507\nhuggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME}\nmv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak\nmv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json\nStep 2: Launch Model Server\nAfter updating the config, proceed with either vLLM or SGLang for serving the model.\nOption 1: Using vLLM\nTo run Qwen with 1M context support:\npip install -U vllm \\\n--torch-backend=auto \\\n--extra-index-url https://wheels.vllm.ai/nightly\nThen launch the server with Dual Chunk Flash Attention enabled:\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\\nvllm serve ./Qwen3-30B-A3B-Thinking-2507 \\\n--tensor-parallel-size 4 \\\n--max-model-len 1010000 \\\n--enable-chunked-prefill \\\n--max-num-batched-tokens 131072 \\\n--enforce-eager \\\n--max-num-seqs 1 \\\n--gpu-memory-utilization 0.85 \\\n--enable-reasoning --reasoning-parser deepseek_r1\nKey Parameters\nParameter\nPurpose\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN\nEnables the custom attention kernel for long-context efficiency\n--max-model-len 1010000\nSets maximum context length to ~1M tokens\n--enable-chunked-prefill\nAllows chunked prefill for very long inputs (avoids OOM)\n--max-num-batched-tokens 131072\nControls batch size during prefill; balances throughput and memory\n--enforce-eager\nDisables CUDA graph capture (required for dual chunk attention)\n--max-num-seqs 1\nLimits concurrent sequences due to extreme memory usage\n--gpu-memory-utilization 0.85\nSet the fraction of GPU memory to be used for the model executor\nOption 2: Using SGLang\nFirst, clone and install the specialized branch:\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\npip install -e \"python[all]\"\nLaunch the server with DCA support:\npython3 -m sglang.launch_server \\\n--model-path ./Qwen3-30B-A3B-Thinking-2507 \\\n--context-length 1010000 \\\n--mem-frac 0.75 \\\n--attention-backend dual_chunk_flash_attn \\\n--tp 4 \\\n--chunked-prefill-size 131072 \\\n--reasoning-parser deepseek-r1\nKey Parameters\nParameter\nPurpose\n--attention-backend dual_chunk_flash_attn\nActivates Dual Chunk Flash Attention\n--context-length 1010000\nDefines max input length\n--mem-frac 0.75\nThe fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors.\n--tp 4\nTensor parallelism size (matches model sharding)\n--chunked-prefill-size 131072\nPrefill chunk size for handling long inputs without OOM\nTroubleshooting:\nEncountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeError: Not enough memory. Please try to increase --mem-fraction-static.\"\nThe VRAM reserved for the KV cache is insufficient.\nvLLM: Consider reducing the max_model_len or increasing the tensor_parallel_size and gpu_memory_utilization. Alternatively, you can reduce max_num_batched_tokens, although this may significantly slow down inference.\nSGLang: Consider reducing the context-length or increasing the tp and mem-frac. Alternatively, you can reduce chunked-prefill-size, although this may significantly slow down inference.\nEncountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\nThe VRAM reserved for activation weights is insufficient. You can try lowering gpu_memory_utilization or mem-frac, but be aware that this might reduce the VRAM available for the KV cache.\nEncountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\" or \"The input (xxx xtokens) is longer than the model's context length (xxx tokens).\"\nThe input is too lengthy. Consider using a shorter sequence or increasing the max_model_len or context-length.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-30B-A3B (Thinking)\n70.6\n96.7\n94.4\n94.5\n93.4\n82.6\n78.4\n74.5\n70.6\n63.1\n60.0\n56.3\n51.0\n48.4\n47.2\n48.2\nQwen3-30B-A3B-Thinking-2507 (Full Attention)\n91.4\n99.6\n100.0\n99.8\n99.2\n97.4\n96.8\n96.8\n94.8\n89.4\n90.2\n84.0\n82.6\n81.9\n80.1\n77.5\nQwen3-30B-A3B-Thinking-2507 (Sparse Attention)\n91.5\n100.0\n99.2\n99.1\n98.5\n97.3\n97.1\n96.9\n95.8\n89.0\n89.3\n85.5\n84.8\n80.0\n79.9\n79.6\nAll models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nTo avoid overly verbose reasoning, we set the thinking budget to 8,192 tokens.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "Comfy-Org/Qwen-Image_ComfyUI": "See: https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/",
    "MeiGen-AI/InfiniteTalk": "InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing\nLicense Agreement\nInfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing\nWe propose InfiniteTalk‚Äã‚Äã, a novel sparse-frame video dubbing framework. Given an input video and audio track, InfiniteTalk synthesizes a new video with ‚Äã‚Äãaccurate lip synchronization‚Äã‚Äã while ‚Äã‚Äãsimultaneously aligning head movements, body posture, and facial expressions‚Äã‚Äã with the audio. Unlike traditional dubbing methods that focus solely on lips, InfiniteTalk enables ‚Äã‚Äãinfinite-length video generation‚Äã‚Äã with accurate lip synchronization and consistent identity preservation. Beside, InfiniteTalk can also be used as an image-audio-to-video model with an image and an audio as input.\nüí¨ ‚Äã‚ÄãSparse-frame Video Dubbing‚Äã‚Äã ‚Äì Synchronizes not only lips, but aslo head, body, and expressions\n‚è±Ô∏è ‚Äã‚ÄãInfinite-Length Generation‚Äã‚Äã ‚Äì Supports unlimited video duration\n‚ú® ‚Äã‚ÄãStability‚Äã‚Äã ‚Äì Reduces hand/body distortions compared to MultiTalk\nüöÄ ‚Äã‚ÄãLip Accuracy‚Äã‚Äã ‚Äì Achieves superior lip synchronization to MultiTalk\nThis repository hosts the model weights for InfiniteTalk. For installation, usage instructions, and further documentation, please visit our GitHub repository.\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations.",
    "Wan-AI/Wan2.2-S2V-14B": "Wan2.2-S2V-14B: Audio-Driven Cinematic Video Generation\nAbstract (Wan-S2V Paper)\nVideo Demos\nüî• Latest News!!\nCommunity Works\nüìë Todo List\nRun Wan2.2\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.2-S2V-14B: Audio-Driven Cinematic Video Generation\nThis repository features the Wan2.2-S2V-14B model, designed for audio-driven cinematic video generation. It was introduced in the paper:\nWan-S2V: Audio-Driven Cinematic Video Generation\nüíú Wan Homepage ¬†¬† ÔΩú ¬†¬† üñ•Ô∏è GitHub ¬†¬†  | ¬†¬†ü§ó Hugging Face Organization¬†¬† | ¬†¬†ü§ñ ModelScope Organization¬†¬† | ¬†¬† üìë Wan-S2V Paper ¬†¬† | ¬†¬† üìë Wan2.2 Base Paper ¬†¬† | üåê Project Page ¬†¬† | ¬†¬† üìë Blog ¬†¬† |  ¬†¬† üí¨  Discord\nüìï ‰ΩøÁî®ÊåáÂçó(‰∏≠Êñá)¬†¬† | ¬†¬† üìò User Guide(English)¬†¬† | ¬†¬†üí¨ WeChat(ÂæÆ‰ø°)\nAbstract (Wan-S2V Paper)\nCurrent state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nüëç Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nüëç Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nüëç Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nüëç Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nVideo Demos\nYour browser does not support the video tag.\nüî• Latest News!!\nAug 26, 2025: üéµ We introduce Wan2.2-S2V-14B, an audio-driven cinematic video generation model, including inference code, model weights, and technical report! Now you can try it on wan.video,  ModelScope Gradio or HuggingFace Gradio!\nJul 28, 2025: üëã We have open a HF space using the TI2V-5B model. Enjoy!\nJul 28, 2025: üëã Wan2.2 has been integrated into ComfyUI (CN | EN). Enjoy!\nJul 28, 2025: üëã Wan2.2's T2V, I2V and TI2V have been integrated into Diffusers (T2V-A14B | I2V-A14B | TI2V-5B). Feel free to give it a try!\nJul 28, 2025: üëã We've released the inference code and model weights of Wan2.2.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, and you would like more people to see it, please inform us.\nDiffSynth-Studio provides comprehensive support for Wan 2.2, including low-GPU-memory layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, full training.\nKijai's ComfyUI WanVideoWrapper is an alternative implementation of Wan models for ComfyUI. Thanks to its Wan-only focus, it's on the frontline of getting cutting edge optimizations and hot research features, which are often hard to integrate into ComfyUI quickly due to its more rigid structure.\nüìë Todo List\nWan2.2-S2V Speech-to-Video\nInference code of Wan2.2-S2V\nCheckpoints of Wan2.2-S2V-14B\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nü§ó Huggingface     ü§ñ ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nS2V-14B\nü§ó Huggingface     ü§ñ ModelScope\nSpeech-to-Video model, supports 480P & 720P\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-S2V-14B --local-dir ./Wan2.2-S2V-14B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-S2V-14B --local_dir ./Wan2.2-S2V-14B\nRun Speech-to-Video Generation\nThis repository supports the Wan2.2-S2V-14B Speech-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\nSingle-GPU Speech-to-Video inference\npython generate.py  --task s2v-14B --size 1024*704 --ckpt_dir ./Wan2.2-S2V-14B/ --offload_model True --convert_model_dtype --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard.\"  --image \"examples/i2v_input.JPG\" --audio \"examples/talk.wav\"\n# Without setting --num_clip, the generated video length will automatically adjust based on the input audio length\nüí° This command can run on a GPU with at least 80GB VRAM.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\ntorchrun --nproc_per_node=8 generate.py --task s2v-14B --size 1024*704 --ckpt_dir ./Wan2.2-S2V-14B/ --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard.\" --image \"examples/i2v_input.JPG\" --audio \"examples/talk.wav\"\nPose + Audio driven generation\ntorchrun --nproc_per_node=8 generate.py --task s2v-14B --size 1024*704 --ckpt_dir ./Wan2.2-S2V-14B/ --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"a person is singing\" --image \"examples/pose.png\" --audio \"examples/sing.MP3\" --pose_video \"./examples/pose.mp4\"\nüí°For the Speech-to-Video task, the size parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\nüí°The model can generate videos from audio input combined with reference image and optional text prompt.\nüí°The --pose_video parameter enables pose-driven generation, allowing the model to follow specific pose sequences while generating videos synchronized with audio input.\nüí°The --num_clip parameter controls the number of video clips generated, useful for quick preview with shorter generation time.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\n@article{wan2025s2v,\ntitle={Wan-S2V:Audio-Driven Cinematic Video Generation},\nauthor={Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo},\njournal={arXiv preprint arXiv:2508.18621},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "LiquidAI/LFM2-Audio-1.5B": "LFM2‚ÄëAudio-1.5B\nüìÑ Model details\nüèÉ How to run LFM2-Audio\nGradio demo\nMulti-turn, multi-modal chat\nASR, TTS, additional information\nüìà Performance\nVoiceBench (audio input)\nASR\nüì¨ Contact\nLicense\nLFM2‚ÄëAudio-1.5B\nLFM2-Audio-1.5B is Liquid AI's first end-to-end audio foundation model.\nDesigned with low latency and real time conversation in mind, at only 1.5 billion parameters LFM2-Audio enables seamless conversational interaction, achieving capabilities on par with much larger models.\nLFM2-Audio is an end-to-end multimodal speech and text language model, and as such does not require separate ASR and TTS components.\nOur model consists of a pretrained LFM2 model as its multimodal backbone, along with a FastConformer based audio encoder to handle continuous audio inputs, and a RQ-transformer generating discrete Mimi tokens as audio output.\nLFM2-Audio supports two distinct generation routines, each suitable for a set of tasks.\nInterleaved generation enables real-time speech-to-speech conversational chatbot capabilities, where audio generation latency is key.\nSequential generation is suited for non-conversational tasks such as ASR or TTS, and allows the model to switch generated modality on the fly.\nüìÑ Model details\nProperty\nParameters (LM only)\n1.2B\nAudio encoder\nFastConformer (115M, canary-180m-flash)\nBackbone layers\nhybrid conv+attention\nAudio tokenizer\nMimi, using 8 codebooks\nContext\n32,768 tokens\nVocab size\n65,536 (text) / 2049*8 (audio)\nPrecision\nbfloat16\nLicense\nLFM Open License v1.0\nSupported languages: English\nüèÉ How to run LFM2-Audio\nInstall the liquid-audio package via pip\npip install liquid-audio\npip install \"liquid-audio [demo]\" # optional, to install demo dependencies\npip install flash-attn --no-build-isolation  # optional, to use flash attention 2. Will fallback to torch SDPA if not installed\nGradio demo\nThe simplest way to get started is by running the Gradio demo interface. After installation, run the command\nliquid-audio-demo\nThis will start a webserver on port 7860. The interface can then be accessed via the URL http://localhost:7860/.\nMulti-turn, multi-modal chat\nThe liquid-audio provides a lower lever interface to the model and generation routines, ideal for custom usecases.\nWe demonstrate this with a simple multi-turn chat, where the first turn is given as audio, and the second turn is given as text.\nFor multi-turn chat with text and audio output, we use interleaved generation. The system prompt should be set to Respond with interleaved text and audio.. Here we use audio as the first user turn, and text as the second one.\nimport torch\nimport torchaudio\nfrom liquid_audio import LFM2AudioModel, LFM2AudioProcessor, ChatState, LFMModality\n# Load models\nHF_REPO = \"LiquidAI/LFM2-Audio-1.5B\"\nprocessor = LFM2AudioProcessor.from_pretrained(HF_REPO).eval()\nmodel = LFM2AudioModel.from_pretrained(HF_REPO).eval()\n# Set up inputs for the model\nchat = ChatState(processor)\nchat.new_turn(\"system\")\nchat.add_text(\"Respond with interleaved text and audio.\")\nchat.end_turn()\nchat.new_turn(\"user\")\nwav, sampling_rate = torchaudio.load(\"assets/question.wav\")\nchat.add_audio(wav, sampling_rate)\nchat.end_turn()\nchat.new_turn(\"assistant\")\n# Generate text and audio tokens.\ntext_out: list[torch.Tensor] = []\naudio_out: list[torch.Tensor] = []\nmodality_out: list[LFMModality] = []\nfor t in model.generate_interleaved(**chat, max_new_tokens=512, audio_temperature=1.0, audio_top_k=4):\nif t.numel() == 1:\nprint(processor.text.decode(t), end=\"\", flush=True)\ntext_out.append(t)\nmodality_out.append(LFMModality.TEXT)\nelse:\naudio_out.append(t)\nmodality_out.append(LFMModality.AUDIO_OUT)\n# output: Sure! How about \"Handcrafted Woodworking, Precision Made for You\"? Another option could be \"Quality Woodworking, Quality Results.\" If you want something more personal, you might try \"Your Woodworking Needs, Our Expertise.\"\n# Detokenize audio, removing the last \"end-of-audio\" codes\n# Mimi returns audio at 24kHz\nmimi_codes = torch.stack(audio_out[:-1], 1).unsqueeze(0)\nwith torch.no_grad():\nwaveform = processor.mimi.decode(mimi_codes)[0]\ntorchaudio.save(\"answer1.wav\", waveform.cpu(), 24_000)\n# Append newly generated tokens to chat history\nchat.append(\ntext = torch.stack(text_out, 1),\naudio_out = torch.stack(audio_out, 1),\nmodality_flag = torch.tensor(modality_out),\n)\nchat.end_turn()\n# Start new turn\nchat.new_turn(\"user\")\nchat.add_text(\"My business specialized in chairs, can you give me something related to that?\")\nchat.end_turn()\nchat.new_turn(\"assistant\")\n# Generate second turn text and audio tokens.\naudio_out: list[torch.Tensor] = []\nfor t in model.generate_interleaved(**chat, max_new_tokens=512, audio_temperature=1.0, audio_top_k=4):\nif t.numel() == 1:\nprint(processor.text.decode(t), end=\"\", flush=True)\nelse:\naudio_out.append(t)\n# output: Sure thing! How about ‚ÄúComfortable Chairs, Crafted with Care‚Äù or ‚ÄúElegant Seats, Handcrafted for You‚Äù? Let me know if you‚Äôd like a few more options.\n# Detokenize second turn audio, removing the last \"end-of-audio\" codes\nmimi_codes = torch.stack(audio_out[:-1], 1).unsqueeze(0)\nwith torch.no_grad():\nwaveform = processor.mimi.decode(mimi_codes)[0]\ntorchaudio.save(\"answer2.wav\", waveform.cpu(), 24_000)\nASR, TTS, additional information\nPlease visit the liquid-audio package repository for additional examples and sample audio snippets.\nüìà Performance\nVoiceBench (audio input)\nHigher is better. AlpacaEval, CommonEval and WildVoice are scored out of 5.\nModel\nComponents & Size\nAlpacaEval\nCommonEval\nWildVoice\nSD-QA\nMMSU\nOBQA\nBBH\nIFEval\nADVBench\nOverall\nLFM2-Audio-1.5B\n1.5B parameters\n3.71\n3.49\n3.17\n30.56\n31.95\n44.40\n30.54\n98.85\n67.33\n56.78\nMoshi\n7B parameters\n2.01\n1.60\n1.30\n15.64\n24.04\n25.93\n47.40\n10.12\n44.23\n29.51\nQwen2.5-Omni-3B\n5B parameters\n3.72\n3.51\n3.42\n44.94\n55.29\n76.26\n61.30\n32.90\n88.46\n63.57\nMini-Omni2\n0.6B parameters\n2.32\n2.18\n1.79\n9.31\n24.27\n26.59\n46.40\n11.56\n57.50\n33.49\nASR\nWord Error Rate (WER), lower is better.\nModel\nComponents & Size\nAudio output\nOpen\nAMI\nGigaSpeech\nLibriSpeech-clean\nLibriSpeech-other\nTED-LIUM\nAverage\nLFM2-Audio-1.5B\n1.5B parameters\nYes\nYes\n15.58\n10.67\n2.01\n4.39\n3.56\n7.24\nQwen2.5-Omni-3B\n5B parameters\nYes\nYes\n15.95\n10.02\n2.01\n3.91\n3.86\n7.15\nWhisper-large-V3\n1.5B parameters\nNo ‚Äî ASR only\nYes\n16.73\n10.76\n2.73\n5.54\n3.91\n7.93\nelevenlabs/scribe_v1\nunknown\nNo ‚Äî ASR only\nNo\n14.43\n9.66\n1.79\n3.31\n3.17\n6.47\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.\nLicense\nThe code in this the package repository and associated weights are licensed under the LFM Open License v1.0.\nThe code for the audio encoder is based on Nvidia NeMo, licensed under Apache 2.0, and the canary-180m-flash checkpoint, licensed under CC-BY 4.0. To simplify dependency resolution, we also ship the Python code of Kyutai Mimi, licensed under the MIT License.\nWe also redistribute weights for Kyutai Mimi, licensed under CC-BY-4.0.",
    "AIDC-AI/Marco-MT-Algharb": "Marco-MT-Algharb\nIntroduction\nUsage\n1. Dependencies\n2. Prompt Format and Decoding\nApply MBR decoding\nLicense\nDisclaimer\nAlibaba International Digital Commerce\nMarco-MT-Algharb\nThis repository contains the system for Algharb, the submission from the Marco Translation Team of Alibaba International Digital Commerce (AIDC) to the WMT 2025 General Machine Translation Shared Task.\nIntroduction\nThe Algharb system is a large translation model built based on the Qwen3-14B foundation. It is designed for high-quality translation across 13 diverse language directions and demonstrates state-of-the-art performance. Our approach is centered on a multi-stage refinement pipeline that systematically enhances translation fluency and faithfulness.\nSupported language pairs:\nLanguages pair\nChinese Names\nen2zh\nËã±ËØ≠Âà∞‰∏≠Êñá\nen2ja\nËã±ËØ≠Âà∞Êó•ËØ≠\nen2ko\nËã±ËØ≠Âà∞Èü©ËØ≠\nen2ar\nËã±ËØ≠Âà∞ÈòøÊãâ‰ºØËØ≠\nen2et\nËã±ËØ≠Âà∞Áà±Ê≤ôÂ∞º‰∫öËØ≠\nen2sr_latin\nËã±ËØ≠Âà∞Â°ûÂ∞îÁª¥‰∫öËØ≠(Êãâ‰∏ÅÂåñ)\nen2ru\nËã±ËØ≠Âà∞‰øÑËØ≠\nen2uk\nËã±ËØ≠Âà∞‰πåÂÖãÂÖ∞ËØ≠\nen2cs\nËã±ËØ≠Âà∞Êç∑ÂÖãËØ≠\nen2bho\nËã±ËØ≠Âà∞ÂçöÊù∞ÊôÆÂ∞îËØ≠\ncs2uk\nÊç∑ÂÖãËØ≠Âà∞‰πåÂÖãÂÖ∞ËØ≠\ncs2de\nÊç∑ÂÖãËØ≠Âà∞Âæ∑ËØ≠\nja2zh\nÊó•ËØ≠Âà∞‰∏≠Êñá\nUsage\nThe model expects a specific instruction format for translation. The following example demonstrates how to construct the prompt and perform generation using the vllm library for efficient inference.\n1. Dependencies\nFirst, ensure you have the necessary libraries installed:\npip install torch transformers==4.55.0 vllm==0.10.0 unbabel-comet==2.2.2\n2. Prompt Format and Decoding\nThe core of the process involves formatting the input text into a specific prompt template and then using the vllm engine to generate translations. For our hybrid decoding strategy, we generate multiple candidates (n > 1) for later re-ranking.\nThe prompt template is:\n\"Human: Please translate the following text into {target_language}: \\n{source_text}<|im_end|>\\nAssistant:\"\nHere is a complete Python example:\nfrom vllm import LLM, SamplingParams\nmodel_path = \"path/to/your/algharb_model\"\nllm = LLM(model=model_path)\nsource_text = \"This paper presents the Algharb system, our submission to the WMT 2025.\"\nsource_lang_code = \"en\"\ntarget_lang_code = \"zh\"\nlang_name_map = {\n\"en\": \"english\"\n\"zh\": \"chinese\",\n\"ko\": \"korean\",\n\"ja\": \"japanese\",\n\"ar\": \"arabic\",\n\"cs\": \"czech\",\n\"ru\": \"russian\",\n\"uk\": \"ukraine\",\n\"et\": \"estonian\",\n\"bho\": \"bhojpuri\",\n\"sr_latin\": \"serbian\",\n\"de\": \"german\",\n}\ntarget_language_name = lang_name_map.get(target_lang_code, \"the target language\")\nprompt = (\nf\"Human: Please translate the following text into {target_language_name}: \\n\"\nf\"{source_text}<|im_end|>\\n\"\nf\"Assistant:\"\n)\nprompts_to_generate = [prompt]\nprint(\"Formatted Prompt:\\n\", prompt)\nsampling_params = SamplingParams(\nn=1,\ntemperature=0.001,\ntop_p=0.001,\nmax_tokens=512\n)\noutputs = llm.generate(prompts_to_generate, sampling_params)\nfor output in outputs:\ngenerated_text = output.outputs[0].strip()\nprint(f\"translation: {generated_text}\")\nApply MBR decoding\nFirst, run random sample decoding:\nfrom vllm import LLM, SamplingParams\nmodel_path = \"path/to/your/algharb_model\"\nllm = LLM(model=model_path)\nsource_text = \"This paper presents the Algharb system, our submission to the WMT 2025.\"\nsource_lang_code = \"en\"\ntarget_lang_code = \"zh\"\nlang_name_map = {\n\"en\": \"english\"\n\"zh\": \"chinese\",\n\"ko\": \"korean\",\n\"ja\": \"japanese\",\n\"ar\": \"arabic\",\n\"cs\": \"czech\",\n\"ru\": \"russian\",\n\"uk\": \"ukraine\",\n\"et\": \"estonian\",\n\"bho\": \"bhojpuri\",\n\"sr_latin\": \"serbian\",\n\"de\": \"german\",\n}\ntarget_language_name = lang_name_map.get(target_lang_code, \"the target language\")\nprompt = (\nf\"Human: Please translate the following text into {target_language_name}: \\n\"\nf\"{source_text}<|im_end|>\\n\"\nf\"Assistant:\"\n)\nprompts_to_generate = [prompt]\nprint(\"Formatted Prompt:\\n\", prompt)\nsampling_params = SamplingParams(\nn=100,\ntemperature=1,\ntop_p=1,\nmax_tokens=512\n)\noutputs = llm.generate(prompts_to_generate, sampling_params)\n# The 'outputs' list contains one item for each prompt.\nfor output in outputs:\nprompt_used = output.prompt\nprint(f\"\\n--- Candidates for source: '{source_text}' ---\")\n# Each output object contains 'n' generated sequences.\nfor i, candidate in enumerate(output.outputs):\ngenerated_text = candidate.text.strip()\nprint(f\"Candidate {i+1}: {generated_text}\")\nThen, run MBR decoding:\ncomet-mbr -s src.txt -t mbr_sample_100.txt -o mbr_trans.txt --num_samples 100 --gpus 1 --qe_model Unbabel/wmt22-cometkiwi-da\nNote: Word alignment for MBR reranking will be available soon.\nLicense\nThis model is licensed under Apache License Version 2 (https://www.apache.org/licenses/LICENSE-2.0.txt, SPDX-License-identifier: Apache-2.0).\nDisclaimer\nWe used compliance checking algorithms during the training process, to ensure the compliance of the trained model(s) to the best of our ability. Due to complex data and the diversity of language model usage scenarios, we cannot guarantee that the model is completely free of copyright issues or improper content. If you believe anything infringes on your rights or generates improper content, please contact us, and we will promptly address the matter.",
    "ServiceNow-AI/Apriel-1.5-15b-Thinker": "Apriel-1.5-15b-Thinker - Mid training is all you need!\nDocker Image\nStart Command\nTable of Contents\nDocker Image\nStart Command\nSummary\nDocker Image\nStart Command\nEvaluation\nDocker Image\nStart Command\nResults reported by Artificial Analysis\nDocker Image\nStart Command\nNote from the developers\nDocker Image\nStart Command\nTraining Details\nDocker Image\nStart Command\nRunning Apriel-1.5-15B-Thinker with vLLM\nDocker Image\nStart Command\nHow to Use\nRunning the Reasoning model\nChat Template\nUsage Guidelines\nIntended Use\nLimitations\nSecurity and Responsible Use\nSoftware\nLicense\nCitation\nApriel-1.5-15b-Thinker - Mid training is all you need!\n/Àà…ëÀê.pri.…ôl/\nTable of Contents\nSummary\nEvaluation\nTraining Details\nHow to Use\nIntended Use\nLimitations\nSecurity and Responsible Use\nSoftware\nLicense\nAcknowledgements\nCitation\nClick here to skip to the technical report -> https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker/blob/main/Apriel-1.5-Thinker.pdf\nSummary\nApriel-1.5-15b-Thinker is a multimodal reasoning model in ServiceNow‚Äôs Apriel SLM series which achieves competitive performance against models 10 times it's size. Apriel-1.5 is the second model in the reasoning series. It introduces enhanced textual reasoning capabilities and adds image reasoning support to the previous text model.  It has undergone extensive continual pretraining across both text and image domains. In terms of post-training this model has undergone text-SFT only. Our research demonstrates that with a strong mid-training regimen, we are able to achive SOTA performance on text and image reasoning tasks without having any image SFT training or RL.\nHighlights\nAchieves a score of 52 on the Artificial Analysis index and is competitive with Deepseek R1 0528, Gemini-Flash etc.\nIt is AT LEAST 1 / 10 the size of any other model that scores > 50 on the Artificial Analysis index.\nScores 68 on Tau2 Bench Telecom and 62 on IFBench, which are key benchmarks for the enterprise domain.\nAt 15B parameters, the model fits on a single GPU, making it highly memory-efficient.\nEvaluation\nFor text benchmarks, we report evaluations perforomed by a third party - Artificial Analysis.\nFor image benchmarks, we report evaluations obtained by https://github.com/open-compass/VLMEvalKit\nResults reported by Artificial Analysis\nNote from the developers\nWe are a small lab with big goals. While we are not GPU poor, our lab, in comparison has a tiny fraction of the compute available to other Frontier labs. Our goal with this work is to show that a SOTA model can be built with limited resources if you have the right data, design and solid methodology.\nWe set out to build a small but powerful model, aiming for capabilities on par with frontier models. Developing a 15B model with this level of performance requires tradeoffs, so we prioritized getting SOTA-level performance first. Mid-training consists only of CPT and SFT; no RL has been applied.\nThis model performs extensive reasoning by default, allocating extra internal effort to improve robustness and accuracy even on simpler queries. You may notice slightly higher token usage and longer response times, but we are actively working to make it more efficient and concise in future releases.\nTraining Details\nTraining Hardware\nGPUs: 640 √ó H100\nTraining Time: 7 days\nMid training / Continual Pre‚Äëtraining\nIn this stage, the model is trained on billions of tokens of carefully curated textual samples drawn from mathematical reasoning, coding challenges, scientific discourse, logical puzzles, and diverse knowledge-rich texts along with multimodal samples covering image understanding and reasoning, captioning, and interleaved image-text data. The objective is to strengthen foundational reasoning capabilities of the model. This stage is critical for the model to function as a reasoner and provides significant lifts in reasoning benchmarks.\nSupervised Fine‚ÄëTuning (SFT)\nThe model is fine-tuned on over 2M high-quality text samples spanning mathematical and scientific problem-solving, coding tasks, instruction-following, API/function invocation, and conversational use cases. This results in superior text performance comparable to models such as Deepseek R1 0528 and Gemini-Flash. Although no image-specific fine-tuning is performed, the model‚Äôs inherent multimodal capabilities and cross-modal transfer of reasoning behavior from the text SFT yield competitive image performance relative to other leading open-source VL models.\nRunning Apriel-1.5-15B-Thinker with vLLM\nAs the upstream PR is not yet merged, you can use this custom image as an alternate way to run the model with tool and reasoning parsers enabled.\nDocker Image\ndocker.io/amant555/vllm_apriel:latest\nStart Command\nRun the container with the following command:\npython3 -m vllm.entrypoints.openai.api_server \\\n--model ServiceNow-AI/Apriel-1.5-15b-Thinker \\\n--served-model-name Apriel-1p5-15B-Thinker \\\n--trust_remote_code \\\n--max-model-len 131072 \\\n--enable-auto-tool-choice \\\n--tool-call-parser apriel \\\n--reasoning-parser apriel\nThis will start the vLLM OpenAI-compatible API server serving the Apriel-1.5-15B-Thinker model with Apriel‚Äôs custom tool parser and reasoning parser.\nHow to Use\npip install transformers\nRunning the Reasoning model\nHere is a code snippet demonstrating the model's usage with the transformers library's generate function:\n# Tested with transformers==4.48\nimport re\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\n# Load model\nmodel_id = \"ServiceNow-AI/Apriel-1.5-15b-Thinker\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Example 1: Text-only prompt\nchat = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What is the capital for France?\"},\n],\n}\n]\ninputs = processor.apply_chat_template(chat, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\ninputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\ninputs.pop(\"token_type_ids\", None)\nwith torch.no_grad():\noutput_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.6)\ngenerated_ids = output_ids[:, inputs['input_ids'].shape[1]:]\noutput = processor.decode(generated_ids[0], skip_special_tokens=True)\nresponse = re.findall(r\"\\[BEGIN FINAL RESPONSE\\](.*?)\\[END FINAL RESPONSE\\]\", output, re.DOTALL)[0].strip()\nprint(\"Text-only Response:\", response)\n# Example 2: Image understanding\nurl = \"https://picsum.photos/id/237/200/300\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nchat = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Which animal is this?\"},\n{\"type\": \"image\"},\n],\n}\n]\nprompt = processor.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(model.device)\ninputs.pop(\"token_type_ids\", None)\nwith torch.no_grad():\noutput_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.6)\ngenerated_ids = output_ids[:, inputs['input_ids'].shape[1]:]\noutput = processor.decode(generated_ids[0], skip_special_tokens=True)\nresponse = re.findall(r\"\\[BEGIN FINAL RESPONSE\\](.*?)\\[END FINAL RESPONSE\\]\", output, re.DOTALL)[0].strip()\nprint(\"Image Response:\", response)\nChat Template\n<|system|>\nYou are a thoughtful and systematic AI assistant built by ServiceNow Language Models (SLAM) lab. Before providing an answer, analyze the problem carefully and present your reasoning step by step. After explaining your thought process, provide the final solution in the following format: [BEGIN FINAL RESPONSE] ... [END FINAL RESPONSE].\n<|end|>\n<|user|>\n# user message here\n<|end|>\n<|assistant|>\nHere are my reasoning steps:\n# thoughts here\n[BEGIN FINAL RESPONSE]\n# assistant response here\n[END FINAL RESPONSE]\n<|end|>\nThe model will first generate its thinking process and then generate its final response between [BEGIN FINAL RESPONSE] and [END FINAL RESPONSE]. Here is a code snippet demonstrating the application of the chat template:\nfrom transformers import AutoTokenizer\nmodel_name = \"ServiceNow-AI/Apriel-1.5-15b-Thinker\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# prepare the model input\ncustom_system_prompt = \"Answer like a pirate.\"\nprompt = \"You are an expert assistant in the implementation of customer experience management aspect of retail applications \\n \\nYou will be using Python as the programming language. \\n \\nYou will utilize a factory design pattern for the implementation and following the dependency inversion principle \\n \\nYou will modify the implementation based on user requirements. \\n \\nUpon user request, you will add, update, and remove the features & enhancements in the implementation provided by you. \\n \\nYou will ask whether the user wants to refactor the provided code or needs a sample implementation for reference. Upon user confirmation, I will proceed accordingly. \\n \\n**Guidelines:** \\n 1. **User Requirements:** \\n - You have to ask users about their requirements, clarify the user expectations, and suggest the best possible solution by providing examples of Python code snippets. \\n - Ask users about which type of reports they need to assess the AI model's performance, accuracy, and reliability. \\n - After providing the solution, you have to ask the user about the trial of the solution and modify the solution based on the user feedback. \\n \\n 2. **Libraries/Frameworks:** \\n - You will be utilizing Python as a programming language. \\n - You will be using Flask framework for REST APIS implementation \\n \\n 3. **Communication Gesture:** \\n - Your conversation with the user should be interactive, supportive, courageous, and professional. \\n - You have to break down the complex concepts into sub-concepts and try to explain them to the user. \\n - You have to ask the user for the required parameters. If the user refuses to provide in 2 attempts, politely exit the conversation. \\n - You have to provide your supported parameters to the user, if the user refuses to accept them then you have to put an apology note and exit the conversation. \\n - You have to track the conversation about unasked questions by the user. If some/one of the questions remain then you have to remind the user about these questions and proceed to answer them based on the user's confirmation \\n \\n 4. **Implementation:** \\n - Your code/implementations should be reliable, scaleable, modular, and reusable. \\n - You will be providing unit tests for the implementation upon user request. \\n - You will be following MVC architecture for the applications \\n - Your implementations must be well-commented and readable \\n \\n \\n- Today's date is 23rd August 2024. \\n- The default sender email is sender-assistant@email.com.\\nHi, I am conducting research on retail customer feedback systems and I need assistance with designing and implementing them. Could you kindly provide me with a list of general customer feedback system modules?\"\nmessages = [\n{\"role\": \"user\", \"content\": custom_system_prompt + \"\\n\\n\" + prompt}\n]\n# example tools\ntools = [{\"type\": \"function\", \"function\": {\"name\": \"getRetailFeedbackModules\", \"description\": \"Returns the list of modules usually present in the retail industry\", \"parameters\": {\"type\": \"object\", \"properties\": {\"page\": {\"type\": \"integer\", \"description\": \"The current page number.\", \"default\": 1}, \"page_size\": {\"type\": \"integer\", \"description\": \"The number of items per page.\", \"default\": 3}}}}}, {\"type\": \"function\", \"function\": {\"name\": \"verifyImplementation\", \"description\": \"Returns the list of modules usually present in the retail industry\", \"parameters\": {\"type\": \"object\", \"properties\": {\"coding_language\": {\"type\": \"string\", \"description\": \"The supported languages for verification of implementation.\", \"default\": \"python\", \"enum\": [\"python\", \"java\", \"php\"]}, \"code\": {\"type\": \"string\", \"description\": \"The code which needs verification\"}, \"design_pattern\": {\"type\": \"string\", \"description\": \"The design pattern to verify in the implementation\", \"enum\": [\"factory\", \"strategy\", \"singleton\"]}, \"verify_best_practices\": {\"type\": \"boolean\", \"description\": \"The verification of the coding style based on the language selected\", \"default\": true}}}}}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\ntools=tools\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\")\nUsage Guidelines\nUse the model‚Äôs default chat template, which already includes a system prompt.\nWe recommend setting temperature to 0.6.\nWe ensure the model starts with Here are my reasoning steps:\\n during all our evaluations. This is implemented in the default chat template.\nFor multi-turn conversations, intermediate turns (historical model outputs) are expected to contain only the final response, without reasoning steps.\nIntended Use\nThe Apriel family of models are designed for a variety of general-purpose instruction tasks, including:\nCode assistance and generation\nLogical reasoning and multi-step tasks\nQuestion answering and information retrieval\nFunction calling, complex instruction following and agent use cases\nThey are not intended for use in safety-critical applications without human oversight or in scenarios requiring guaranteed factual accuracy.\nLimitations\nFactual accuracy: May produce incorrect, misleading, or outdated content. Outputs should be verified before use in critical contexts.\nBias: May reflect societal, cultural, or systemic biases present in training data.\nEthics: Do not use the model to produce harmful, unlawful, or unethical content.\nLanguage: Strongest performance is in English. Output quality may degrade in underrepresented languages.\nCritical use: Not suitable for medical, legal, financial, or other high-risk applications without safeguards.\nSecurity and Responsible Use\nSecurity Responsibilities:Deployers and users are strongly encouraged to align their security practices with established frameworks and regulatory guidelines such as the EU AI Act and the NIST AI Risk Management Framework (RMF).\nGuidelines for Deployers:\nRegularly conduct robustness assessments to identify and mitigate adversarial inputs.\nImplement validation and filtering processes to prevent harmful or biased outputs.\nContinuously perform data privacy checks to guard against unintended data leaks.\nDocument and communicate the model's limitations, intended usage, and known security risks to all end-users.\nSchedule periodic security reviews and updates to address emerging threats and vulnerabilities.\nGuidelines for Users:\nFollow established security policies and usage guidelines provided by deployers.\nProtect and manage sensitive information when interacting with the model.\nReport anomalies, suspicious behavior, or unsafe outputs to deployers or developers.\nMaintain human oversight and apply judgment to mitigate potential security or ethical risks during interactions.\nDisclaimer:Users accept responsibility for securely deploying, managing, and using this open-source LLM. The model is provided \"as-is,\" without explicit or implied warranty regarding security or fitness for any specific application or environment.\nSoftware\nTraining stack: Fast-LLM\nLicense\nMIT\nCitation\n@misc{radhakrishna2025apriel1515bthinker,\ntitle={Apriel-1.5-15b-Thinker},\nauthor={Shruthan Radhakrishna and Aman Tiwari and Aanjaneya Shukla and Masoud Hashemi and Rishabh Maheshwary and Shiva Krishna Reddy Malay and Jash Mehta and Pulkit Pattnaik and Saloni Mittal and Khalil Slimi and Kelechi Ogueji and Akintunde Oladipo and Soham Parikh and Oluwanifemi Bamgbose and Toby Liang and Ahmed Masry and Khyati Mahajan and Sai Rajeswar Mudumba and Vikas Yadav and Sathwik Tejaswi Madhusudhan and Torsten Scholak and Sagar Davasam and Srinivas Sunkara and Nicholas Chapados},\nyear={2025},\neprint={2510.01141},\narchivePrefix={arXiv},\nprimaryClass={cs.AI},\nurl={https://arxiv.org/abs/2510.01141},\n}",
    "nineninesix/kani-tts-370m": "KaniTTS\nOverview\nPerformance\nAudio Examples\nUse Cases\nLimitations\nOptimization Tips\nResources\nAcknowledgments\nResponsible Use\nContact\nCitation\nKaniTTS\nA high-speed, high-fidelity Text-to-Speech model optimized for real-time conversational AI applications.\nOverview\nKaniTTS uses a two-stage pipeline combining a large language model with an efficient audio codec for exceptional speed and audio quality. The architecture generates compressed token representations through a backbone LLM, then rapidly synthesizes waveforms via neural audio codec, achieving extremely low latency.\nKey Specifications:\nModel Size: 370M parameters\nSample Rate: 22kHz\nLanguages: English, German, Chinese, Korean, Arabic, Spanish\nLicense: Apache 2.0\nPerformance\nNvidia RTX 5080 Benchmarks:\nLatency: ~1 second to generate 15 seconds of audio\nMemory: 2GB GPU VRAM\nQuality Metrics: MOS 4.3/5 (naturalness), WER <5% (accuracy)\nPretraining:\nDataset: ~80k hours from LibriTTS, Common Voice, and Emilia\nHardware: 8x H100 GPUs, 45 hours training time on Lambda AI\nVoices Datasets\nhttps://huggingface.co/datasets/nytopop/expresso-conversational\nhttps://huggingface.co/datasets/shb777/gemini-flash-2.0-speech\nhttps://huggingface.co/datasets/jazza234234/david-dataset\nhttps://huggingface.co/datasets/reach-vb/jenny_tts_dataset\nhttps://huggingface.co/datasets/MBZUAI/ArVoice\nhttps://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full\nhttps://huggingface.co/datasets/SinclairSchneider/german_voice_cb\nhttps://huggingface.co/datasets/Bingsu/KSS_Dataset\nhttps://huggingface.co/datasets/ciempiess/ciempiess_fem\nhttps://huggingface.co/datasets/TingChen-ppmc/Shanghai_Dialect_TTS_openai\nhttps://huggingface.co/datasets/boniromou/zh-yue-tts-dataset\nhttps://huggingface.co/datasets/zeeshanparvez/andrew-v3\nVoices:\ndavid ‚Äî David, English (British)\npuck ‚Äî Puck, English (Gemini)\nkore ‚Äî Kore, English (Gemini)\nandrew ‚Äî Andrew, English\njenny ‚Äî Jenny, English (Irish)\nsimon ‚Äî Simon, English\nkatie ‚Äî Katie, English\nseulgi ‚Äî Seulgi, Korean\nbert ‚Äî Bert, German\nthorsten ‚Äî Thorsten, German (Hessisch)\nmaria ‚Äî Maria, Spanish\nmei ‚Äî Mei, Chinese (Cantonese)\nming ‚Äî Ming, Chinese (Shanghai OpenAI)\nkarim ‚Äî Karim, Arabic\nnur ‚Äî Nur, Arabic\nAudio Examples\nText\nAudio\nI do believe Marsellus Wallace, MY husband, YOUR boss, told you to take me out and do WHATEVER I WANTED.\nWhat do we say to the god of death? Not today!\nWhat do you call a lawyer with an IQ of 60? Your honor\nYou mean, let me understand this cause, you know maybe it's me, it's a little fucked up maybe, but I'm funny how, I mean funny like I'm a clown, I amuse you?\nUse Cases\nConversational AI: Real-time speech for chatbots and virtual assistants\nEdge/Server Deployment: Resource-efficient inference on affordable hardware\nAccessibility: Screen readers and language learning applications\nResearch: Fine-tuning for specific voices, accents, or emotions\nLimitations\nPerformance degrades with inputs exceeding 2000 tokens\nLimited expressivity without fine-tuning for specific emotions\nMay inherit biases from training data in prosody or pronunciation\nOptimized primarily for English; other languages may require additional training\nOptimization Tips\nMultilingual Performance: Continually pretrain on target language datasets and fine-tune NanoCodec\nBatch Processing: Use batches of 8-16 for high-throughput scenarios\nHardware: Optimized for NVIDIA Blackwell architecture GPUs\nResources\nModels:\nPretrained Model\nFine-tuned Model\nHuggingFace Space\nExamples:\nInference Example\nFine-tuning-code\nExample Dataset\nGitHub Repository\nLinks:\nWebsite\nContact Form\nAcknowledgments\nBuilt on top of LiquidAI LFM2 350M as the backbone and Nvidia NanoCodec for audio processing.\nResponsible Use\nProhibited activities include:\nIllegal content or harmful, threatening, defamatory, or obscene material\nHate speech, harassment, or incitement of violence\nGenerating false or misleading information\nImpersonating individuals without consent\nMalicious activities such as spamming, phishing, or fraud\nBy using this model, you agree to comply with these restrictions and all applicable laws.\nContact\nHave a question, feedback, or need support? Please fill out our contact form and we'll get back to you as soon as possible.\nCitation\n@misc {sb_2025,\nauthor       = { SB },\ntitle        = { gemini-flash-2.0-speech },\nyear         = 2025,\nurl          = { https://huggingface.co/datasets/shb777/gemini-flash-2.0-speech },\ndoi          = { 10.57967/hf/4237 },\npublisher    = { Hugging Face }\n}\n@misc{toyin2025arvoicemultispeakerdatasetarabic,\ntitle={ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis},\nauthor={Hawau Olamide Toyin and Rufael Marew and Humaid Alblooshi and Samar M. Magdy and Hanan Aldarmaki},\nyear={2025},\neprint={2505.20506},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.20506},\n}\n@misc {thorsten_m√ºller_2024,\nauthor       = { {Thorsten M√ºller} },\ntitle        = { TV-44kHz-Full (Revision ff427ec) },\nyear         = 2024,\nurl          = { https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full },\ndoi          = { 10.57967/hf/3290 },\npublisher    = { Hugging Face }\n}\n@misc{carlosmenaciempiessfem2019,\ntitle={CIEMPIESS FEM CORPUS: Audio and Transcripts of Female Speakers in Spanish.},\nldc_catalog_no={LDC2019S07},\nDOI={https://doi.org/10.35111/xdx5-n815},\nauthor={Hernandez Mena, Carlos Daniel},\njournal={Linguistic Data Consortium, Philadelphia},\nyear={2019},\nurl={https://catalog.ldc.upenn.edu/LDC2019S07},\n}",
    "Qwen/Qwen3-VL-235B-A22B-Thinking-FP8": "Qwen3-VL-235B-A22B-Thinking-FP8\nModel Performance\nQuickstart\nvLLM Inference\nSGLang Inference\nCitation\nQwen3-VL-235B-A22B-Thinking-FP8\nThis repository contains an FP8 quantized version of the Qwen3-VL-235B-A22B-Thinking model. The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original BF16 model. Enjoy!\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for the FP8 version of Qwen3-VL-235B-A22B-Thinking.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nCurrently, ü§ó Transformers does not support loading these weights directly. Stay tuned!\nWe recommend deploying the model using vLLM or SGLang, with example launch commands provided below.  For details on the runtime environment and deployment, please refer to this link.\nvLLM Inference\nHere we provide a code snippet demonstrating how to use vLLM to run inference with Qwen3-VL locally. For more details on efficient deployment with vLLM, please refer to the community deployment guide.\n# -*- coding: utf-8 -*-\nimport torch\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nimport os\nos.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\ndef prepare_inputs_for_vllm(messages, processor):\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# qwen_vl_utils 0.0.14+ reqired\nimage_inputs, video_inputs, video_kwargs = process_vision_info(\nmessages,\nimage_patch_size=processor.image_processor.patch_size,\nreturn_video_kwargs=True,\nreturn_video_metadata=True\n)\nprint(f\"video_kwargs: {video_kwargs}\")\nmm_data = {}\nif image_inputs is not None:\nmm_data['image'] = image_inputs\nif video_inputs is not None:\nmm_data['video'] = video_inputs\nreturn {\n'prompt': text,\n'multi_modal_data': mm_data,\n'mm_processor_kwargs': video_kwargs\n}\nif __name__ == '__main__':\n# messages = [\n#     {\n#         \"role\": \"user\",\n#         \"content\": [\n#             {\n#                 \"type\": \"video\",\n#                 \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n#             },\n#             {\"type\": \"text\", \"text\": \"ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø\"},\n#         ],\n#     }\n# ]\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-235B-A22B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\ninputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]\nllm = LLM(\nmodel=checkpoint_path,\ntrust_remote_code=True,\ngpu_memory_utilization=0.70,\nenforce_eager=False,\ntensor_parallel_size=torch.cuda.device_count(),\nseed=0\n)\nsampling_params = SamplingParams(\ntemperature=0,\nmax_tokens=1024,\ntop_k=-1,\nstop_token_ids=[],\n)\nfor i, input_ in enumerate(inputs):\nprint()\nprint('=' * 40)\nprint(f\"Inputs[{i}]: {input_['prompt']=!r}\")\nprint('\\n' + '>' * 40)\noutputs = llm.generate(inputs, sampling_params=sampling_params)\nfor i, output in enumerate(outputs):\ngenerated_text = output.outputs[0].text\nprint()\nprint('=' * 40)\nprint(f\"Generated text: {generated_text!r}\")\nSGLang Inference\nHere we provide a code snippet demonstrating how to use SGLang to run inference with Qwen3-VL locally.\nimport time\nfrom PIL import Image\nfrom sglang import Engine\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, AutoConfig\nif __name__ == \"__main__\":\n# TODO: change to your own checkpoint path\ncheckpoint_path = \"Qwen/Qwen3-VL-235B-A22B-Thinking-FP8\"\nprocessor = AutoProcessor.from_pretrained(checkpoint_path)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": \"Read all the text in the image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nimage_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)\nllm = Engine(\nmodel_path=checkpoint_path,\nenable_multimodal=True,\nmem_fraction_static=0.8,\ntp_size=torch.cuda.device_count(),\nattention_backend=\"fa3\"\n)\nstart = time.time()\nsampling_params = {\"max_new_tokens\": 1024}\nresponse = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)\nprint(f\"Response costs: {time.time() - start:.2f}s\")\nprint(f\"Generated text: {response['text']}\")\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "ByteDance/FaceCLIP": "FaceCLIP Model Card\nüîß Installation and Usage\nModel Zoo\nüìú Disclaimer and Licenses\nüìñ Citation\nFaceCLIP Model Card\nThis repository provides the official models for the following paper:\nLearning Joint ID-Textual Representation for ID-Preserving Image Synthesis\nZichuan Liu\nLiming Jiang\nQing Yan\nYumin Jia\nHao Kang\nXin Lu\nMin Jin Chong\nAbstract: Recent progress in text-to-image (T2I) diffusion models has greatly improved image quality and flexibility. However, a major challenge in personalized generation remains: preserving the subject‚Äôs identity (ID) while allowing diverse visual changes. We address this with a new framework for ID-preserving image generation. Instead of relying on adapter modules to inject identity features into pre-trained models, we propose a unified multi-modal encoding strategy that jointly captures identity and text information. Our method, called FaceCLIP, learns a shared embedding space for facial identity and textual semantics. Given a reference face image and a text prompt, FaceCLIP produces a joint representation that guides the generative model to synthesize images consistent with both the subject‚Äôs identity and the prompt. To train FaceCLIP, we introduce a multi-modal alignment loss that aligns features across face, text, and image domains. We then integrate FaceCLIP with existing UNet and Diffusion Transformer (DiT) architectures, forming a complete synthesis pipeline FaceCLIP-x. Compared to existing ID-preserving approaches, our method produces more photorealistic portraits with better identity retention and text alignment. Extensive experiments demonstrate that FaceCLIP-x outperforms prior methods in both qualitative and quantitative evaluations.\nüîß Installation and Usage\nPlease clone our GitHub code repository and follow the detailed instruction to install and use the released models for local inference.\nModel Zoo\nVersion\nDescription\nFaceCLIP-SDXL\nSDXL base model trained with FaceCLIP-L-14 and FaceCLIP-bigG-14 encoders.\nFaceT5-FLUX\nFLUX.1-dev base model trained with FaceT5 encoder.\nüìú Disclaimer and Licenses\nThe images used in this repository and related demos are sourced from consented subjects or generated by the models.\nThese pictures are intended solely to showcase the capabilities of our research. If you have any concerns, please feel free to contact us, and we will promptly remove any inappropriate content.\nOur model is released under the Creative Commons Attribution-NonCommercial 4.0 International Public License for academic research purposes only. Any manual or automatic downloading of the face models from the OpenAI-CLIP-L-14, OpenCLIP-bigG-14, the SDXL-base-1.0 base model, and the FLUX.1-dev base model, etc., must follow their original licenses and be used only for academic research purposes.\nThis research aims to positively impact the field of Generative AI. Any usage of this method must be responsible and comply with local laws. The developers do not assume any responsibility for any potential misuse.\nüìñ Citation\nIf you find FaceCLIP useful for your research or applications, please cite our paper:\n@article{liu2025learning,\ntitle={Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis},\nauthor={Liu, Zichuan and Jiang, Liming and Yan, Qing and Jia, Yumin and Kang, Hao and Lu, Xin},\njournal={arXiv preprint arXiv:2504.14202},\nyear={2025}\n}\nWe also appreciate it if you could give a star ‚≠ê to our Github repository. Thanks a lot!",
    "nvidia/nemoretriever-page-elements-v3": "Nemoretriever Page Element v3\nModel Overview\nDescription\nLicense/Terms of use\nTeam\nDeployment Geography\nUse Case\nRelease Date\nReferences\nModel Architecture\nInput\nOutput\nUsage\nModel Version(s):\nTraining and Evaluation Datasets:\nTraining Dataset\nEvaluation Dataset\nEthical Considerations\nBias\nExplainability\nPrivacy\nSafety\nNemoretriever Page Element v3\nModel Overview\nPreview of the model output on the example image.\nDescription\nThe NeMo Retriever Page Elements v3 model is a specialized object detection model designed to identify and extract elements from document pages. While the underlying technology builds upon work from Megvii Technology, we developed our own base model through complete retraining rather than using pre-trained weights. YOLOX is an anchor-free version of YOLO (You Only Look Once), this model combines a simpler architecture with enhanced performance. The model is trained to detect tables, charts, infographics, titles, header/footers and texts in documents.\nThis model supersedes the nemoretriever-page-elements model and is a part of the NVIDIA NeMo Retriever family of NIM microservices specifically for object detection and multimodal extraction of enterprise documents.\nThis model is ready for commercial/non-commercial use.\nWe are excited to announce the open sourcing of this commercial model. For users interested in deploying this model in production environments, it is also available via the model API in NVIDIA Inference Microservices (NIM) at nemoretriever-page-elements-v2.\nLicense/Terms of use\nThe use of this model is governed by the NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nTeam\nTheo Viel\nBo Liu\nDarragh Hanley\nEven Oldridge\nCorrespondence to Theo Viel (tviel@nvidia.com) and Bo Liu (boli@nvidia.com)\nDeployment Geography\nGlobal\nUse Case\nThe NeMo Retriever Page Elements v3 model  is designed for automating extraction of text, charts, tables, infographics etc in enterprise documents. It can be used for document analysis, understanding and processing. Key applications include:\nEnterprise document extraction, embedding and indexing\nAugmenting Retrieval Augmented Generation (RAG) workflows with multimodal retrieval\nData extraction from legacy documents and reports\nRelease Date\n10/23/2025 via https://huggingface.co/nvidia/nemoretriever-page-elements-v3\nReferences\nYOLOX paper: https://arxiv.org/abs/2107.08430\nYOLOX repo: https://github.com/Megvii-BaseDetection/YOLOX\nPrevious version of the Page Element model: https://build.nvidia.com/nvidia/nemoretriever-page-elements-v2\nTechnical blog: https://developer.nvidia.com/blog/approaches-to-pdf-data-extraction-for-information-retrieval/\nModel Architecture\nArchitecture Type: YOLOX\nNetwork Architecture: DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction). YOLOX is a single-stage object detector that improves on Yolo-v3.\nThis model was developed based on the Yolo architecture\nNumber of model parameters: 5.4e7\nInput\nInput Type(s): Image\nInput Format(s): Red, Green, Blue (RGB)\nInput Parameters: Two-Dimensional (2D)\nOther Properties Related to Input: Image size resized to (1024, 1024)\nOutput\nOutput Type(s): Array\nOutput Format: A dictionary of dictionaries containing np.ndarray objects. The outer dictionary has entries for each sample (page), and the inner dictionary contains a list of dictionaries, each with a bounding box (np.ndarray), class label, and confidence score for that page.\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output: The output contains bounding boxes, detection confidence scores, and object classes (chart, table, infographic, title, text, headers and footers). The thresholds used for non-maximum suppression are conf_thresh=0.01 and iou_thresh=0.5.\nOutput Classes:\nTable\nData structured in rows and columns\nChart\nSpecifically bar charts, line charts, or pie charts\nInfographic\nVisual representations of information that is more complex than a chart, including diagrams and flowcharts\nMaps are not considered infographics\nTitle\nTitles can be section titles, or table/chart/infographic titles\nHeader/footer\nPage headers and footers\nText\nTexts are regions of one or more text paragraphs, or standalone text not belonging to any of the classes above\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nUsage\nThe model requires torch, and the custom code available in this repository.\nClone the repository\nMake sure git-lfs is installed (https://git-lfs.com)\ngit lfs install\nUsing https\ngit clone https://huggingface.co/nvidia/nemoretriever-page-elements-v3\nOr using ssh\ngit clone git@hf.co:nvidia/nemoretriever-page-elements-v3\nRun the model using the following code:\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom model import define_model\nfrom utils import plot_sample, postprocess_preds_page_element, reformat_for_plotting\n# Load image\npath = \"./example.png\"\nimg = Image.open(path).convert(\"RGB\")\nimg = np.array(img)\n# Load model\nmodel = define_model(\"page_element_v3\")\n# Inference\nwith torch.inference_mode():\nx = model.preprocess(img)\npreds = model(x, img.shape)[0]\nprint(preds)\n# Post-processing\nboxes, labels, scores = postprocess_preds_page_element(preds, model.thresholds_per_class, model.labels)\n# Plot\nboxes_plot, confs = reformat_for_plotting(boxes, labels, scores, img.shape, model.num_classes)\nplt.figure(figsize=(15, 10))\nplot_sample(img, boxes_plot, confs, labels=model.labels)\nplt.show()\nNote that this repository only provides minimal code to infer the model.\nIf you wish to do additional training, refer to the original repo.\nAdvanced post-processing\nAdditional post-processing might be required to use the model as part of a data extraction pipeline.\nWe provide examples in the notebook Demo.ipynb.\nModel Version(s):\nnemoretriever-page-elements-v3\nTraining and Evaluation Datasets:\nTraining Dataset\nData Modality: Image\nImage Training Data Size: Less than a Million Images\nData collection method by dataset: Automated\nLabeling method by dataset: Hybrid: Automated, Human\nPretraining (by NVIDIA): 118,287 images of the COCO train2017 dataset\nFinetuning (by NVIDIA): 36,093 images from Digital Corpora dataset, with annotations from Azure AI Document Intelligence and data annotation team\nNumber of bounding boxes per class: 35,328 tables, 44,178 titles, 11,313 charts and 6,500 infographics, 90,812 texts and 10,743 header/footers. The layout model of Document Intelligence was used with 2024-02-29-preview API version.\nEvaluation Dataset\nThe primary evaluation set is a cut of the Azure labels and digital corpora images. Number of bounding boxes per class: 1,985 tables, 2,922 titles, 498 charts, 572 infographics, 4,400 texts and 492 header/footers. Mean Average Precision (mAP) was used as an evaluation metric, which measures the model's ability to correctly identify and localize objects across different confidence thresholds.\nData collection method by dataset: Hybrid: Automated, Human\nLabeling method by dataset: Hybrid: Automated, Human\nProperties: We evaluated with Azure labels from manually selected pages, as well as manual inspection on public PDFs and powerpoint slides.\nPer-class Performance Metrics:\nClass\nAP (%)\nAR (%)\ntable\n44.643\n62.242\nchart\n54.191\n77.557\ntitle\n38.529\n56.315\ninfographic\n66.863\n69.306\ntext\n45.418\n73.017\nheader_footer\n53.895\n75.670\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Task/Domain:\nDocument Understanding\nModel Type:\nYOLOX Object Detection for Charts, Tables, Infographics, Header/footers, Texts, and Titles\nIntended User:\nEnterprise developers, data scientists, and other technical users who need to extract structural elements from documents.\nOutput:\nAfter post-processing, the output is three numpy array that contains the detections: boxes [N x 4] (format is normalized (x_min, y_min, x_max, y_max)), associated classes: labels [N] and confidence scores: scores [N].\nDescribe how the model works:\nThe model identifies objects in an image by first dividing the image into a grid. For each grid cell, it extracts visual features and simultaneously predicts which objects are present (for example, 'chart' or 'table') and where they are located in that cell, all in a single pass through the image.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nNot Applicable\nTechnical Limitations & Mitigation:\nThe model may not generalize to unknown document types/formats not commonly found on the web. Further fine-tuning might be required for such documents.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nMean Average Precision, detectionr recall and visual inspection\nPotential Known Risks:\nThis model may not always detect all elements in a document.\nLicensing & Terms of Use:\nUse of this model is governed by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNo\nPersonal data used to create this model?\nNo\nWas consent obtained for any personal data used?\nNot Applicable\nHow often is the dataset reviewed?\nBefore Release\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety\nField\nResponse\nModel Application Field(s):\nObject Detection for Retrieval, focused on Enterprise\nDescribe the life critical impact (if present).\nNot Applicable\nUse Case Restrictions:\nAbide by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.",
    "ByteDance/Dolphin-1.5": "Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting\nModel Description\nüìë Overview\nModel Architecture\nUsage\nLicense\nCitation\nAcknowledgements\nDolphin: Document Image Parsing via Heterogeneous Anchor Prompting\nModel Description\nDolphin (Document Image Parsing via Heterogeneous Anchor Prompting) is a novel multimodal document image parsing model that follows an analyze-then-parse paradigm. It addresses the challenges of complex document understanding through a two-stage approach designed to handle intertwined elements such as text paragraphs, figures, formulas, and tables.\nüìë Overview\nDocument image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:\nüîç Stage 1: Comprehensive page-level layout analysis by generating element sequence in natural reading order\nüß© Stage 2: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts\nDolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.\nModel Architecture\nDolphin is built on a vision-encoder-decoder architecture using transformers:\nVision Encoder: Based on Swin Transformer for extracting visual features from document images\nText Decoder: Based on MBart for decoding text from visual features\nPrompt-based interface: Uses natural language prompts to control parsing tasks\nThe model is implemented as a Hugging Face VisionEncoderDecoderModel for easy integration with the Transformers ecosystem.\nUsage\nOur demo will be released in these days. Please keep tuned! üî•\nPlease refer to our GitHub repository for detailed usage.\nPage-wise parsing: for an entire document image\nElement-wise parsing: for an element (paragraph, table, formula) image\nLicense\nThis model is released under the MIT License.\nCitation\n@inproceedings{dolphin2025,\ntitle={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},\nauthor={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and Tang, Jingqun and Liu, Hao and Huang, Can},\nyear={2025},\nbooktitle={Proceedings of the 65rd Annual Meeting of the Association for Computational Linguistics (ACL)}\n}\nAcknowledgements\nThis model builds on several open-source projects including:\nHugging Face Transformers\nDonut\nNougat\nSwin Transformer",
    "MUG-V/MUG-V-inference": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models\nOverview\nüî• Latest News\n‚úÖ Roadmap\n‚ú® Features\nüìã Table of Contents\nüõ†Ô∏è Installation\nPrerequisites\nInstall Dependencies\nDownload Models\nüöÄ Quick Start\nBasic Usage\nCommand Line Usage\nVideo Enhancement\nüîß API Reference\nMUGDiTConfig\nMUGDiTPipeline\nüèóÔ∏è Model Architecture\nCitation\nüìÑ License\nAcknowledgements\nlicense: apache-2.0\npipeline_tag: image-to-video\nMUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models\nYongshun Zhang* ¬∑ Zhongyi Fan* ¬∑ Yonghang Zhang ¬∑ Zhangzikang Li ¬∑ Weifeng Chen\nZhongwei Feng ¬∑ Chaoyue Wang‚Ä† ¬∑ Peng Hou‚Ä† ¬∑ Anxiang Zeng‚Ä†\nLLM Team, Shopee Pte. Ltd.\n* Equal contribution ¬∑ ‚Ä† Corresponding authors\nOverview\nMUG-V 10B is a large-scale video generation system built by the Shopee Multimodal Understanding and Generation (MUG) team. The core generator is a Diffusion Transformer (DiT) with ~10B parameters trained via flow-matching objectives. We release the complete stack:\nModel weights\nMegatron-Core-based training code\nInference pipelines for video generation and video enhancement\nTo our knowledge, this is the first publicly available large-scale video-generation training framework that leverages Megatron-Core for high training efficiency (e.g., high GPU utilization, strong MFU) and near-linear multi-node scaling. By open-sourcing the end-to-end framework, we aim to accelerate progress and lower the barrier for scalable modeling of the visual world.\nüî• Latest News\nOct. 21, 2025: üëã We are excited to announce the release of the MUG-V 10B technical report. We welcome feedback and discussions.\nOct. 21, 2025: üëã We've released Megatron-LM‚Äìbased training framework addressing the key challenges of training billion-parameter video generators.\nOct. 21, 2025: üëã We've released MUG-V video enhancement inference code and weights (based on WAN-2.1 1.3B).\nOct. 21, 2025: üëã We've released MUG-V 10B (e-commerce edition) inference code and weights.\nApr. 25, 2025: üëã We submitted our model to Vbench-I2V leaderboard, at submission time, MUG-V ranked #3.\n‚úÖ Roadmap\nMUG-V Model & Inference\nInference code for MUG-V 10B\nCheckpoints: e-commerce edition (Image-to-Video Generation, I2V)\nCheckpoints: general-domain edition\nDiffusers integration\nText prompt rewriter\nMUG-V Training\nData preprocessing tools (video encoding, text encoding)\nPre-training framework on Megatron-LM\nMUG-V Video Enhancer\nInference code\nLight-weight I2V model Checkpoints (trained on WAN-2.1 1.3B T2V model)\nUG-V Video Enhancer LoRA Checkpoints (based on above I2V model)\nTraining code\n‚ú® Features\nHigh-quality video generation: up to 720p, 3‚Äì5 s clips\nImage-to-Video (I2V): conditioning on a reference image\nFlexible aspect ratios: 16:9, 4:3, 1:1, 3:4, 9:16\nAdvanced architecture: MUG-DiT (‚âà10B parameters) with flow-matching training\nüìã Table of Contents\nInstallation\nQuick Start\nAPI Reference\nVideo Enhancement\nModel Architecture\nLicense\nüõ†Ô∏è Installation\nPrerequisites\nPython ‚â• 3.8 (tested with 3.10)\nCUDA 12.1\nNVIDIA GPU with ‚â• 24 GB VRAM (for 10B-parameter inference)\nInstall Dependencies\n# Clone the repository\ngit clone https://github.com/Shopee-MUG/MUG-V\ncd MUG-V\n# Install required packages\nconda create -n mug_infer python=3.10 -y\nconda activate mug_infer\npip3 install -e .\npip3 install flash_attn --no-build-isolation\nDownload Models\nYou need to download the pre-trained models by huggingface-cli\n# install huggingface-cli\npip3 install -U huggingface_hub[cli]\n# login to your account with the access token\nhuggingface-cli login\n# download MUG-DiT-10B pretrained models for inference\nhuggingface-cli download MUG-V/MUG-V-inference --local-dir ./pretrained_ckpt/MUG-V-inference\n# download wan2.1-t2v vae&text encoder models for enhancer\nhuggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir ./pretrained_ckpt/Wan2.1-T2V-1.3B\nUpdate the vae and dit model paths in your configuration at infer_pipeline.MUGDiTConfig.\nüöÄ Quick Start\nBasic Usage\nfrom infer_pipeline import MUGDiTPipeline, MUGDiTConfig\n# Initialize the pipeline\nconfig = MUGDiTConfig()\npipeline = MUGDiTPipeline(config)\n# Generate a video\noutput_path = pipeline.generate(\nprompt=\"This video describes a young woman standing in a minimal studio with a warm beige backdrop, wearing a white cropped top with thin straps and a matching long tiered skirt. She faces the camera directly with a relaxed posture, and the lighting is bright and even, giving the scene a soft, neutral appearance. The background features a seamless beige wall and a smooth floor with no additional props, creating a simple setting that keeps attention on the outfit. The main subject is a woman with long curly hair, dressed in a white spaghetti-strap crop top and a flowing ankle-length skirt with gathered tiers. She wears black strappy sandals and is positioned centrally in the frame, standing upright with her arms resting naturally at her sides. The camera is stationary and straight-on, capturing a full-body shot that keeps her entire figure visible from head to toe. She appears to hold a calm expression while breathing steadily, occasionally shifting her weight slightly from one foot to the other. There may be a subtle tilt of the head or a gentle adjustment of her hands, but movements remain small and unhurried throughout the video. The background remains static with no visible changes, and the framing stays consistent for a clear view of the outfit details.\",\nreference_image_path=\"./assets/sample.png\",\noutput_path=\"outputs/sample.mp4\"\n)\nprint(f\"Video saved to: {output_path}\")\nCommand Line Usage\npython3 infer_pipeline.py\nThe script will use the default configuration and generate a video based on the built-in prompt and reference image.\nVideo Enhancement\nUse the MUG-V Video Enhancer to improve videos generated by MUG-DiT-10B (e.g., detail restoration, temporal consistency). Details can be find in the ./mug_enhancer folder.\ncd ./mug_enhancer\npython3 predict.py \\\n--task predict \\\n--output_path ./video_outputs \\\n--num_frames 105 \\\n--height 1280 \\\n--width 720 \\\n--fps=20 \\\n--video_path \"../outputs/\" \\\n--val_dataset_path \"../assets/sample.csv\" \\\n--lora_rank 256\nThe output video will be saved to ./mug_enhancer/video_outputs/year-month-day_hour\\:minute\\:second/0000_generated_video_enhance.mp4.\nüîß API Reference\nMUGDiTConfig\nConfiguration class for the MUG-DiT-10B pipeline.\nParameters:\ndevice (str): Device to run inference on. Default: \"cuda\"\ndtype (torch.dtype): Data type for computations. Default: torch.bfloat16\nvae_pretrained_path (str): Path to VAE model checkpoint\ndit_pretrained_path (str): Path to DiT model checkpoint\nresolution (str): Video resolution. Currently only \"720p\" is supported\nvideo_length (str): Video duration. Options: \"3s\", \"5s\"\nvideo_ar_ratio (str): Aspect ratio. Options: \"16:9\", \"4:3\", \"1:1\", \"3:4\", \"9:16\"\ncfg_scale (float): Classifier-free guidance scale. Default: 4.0\nnum_sampling_steps (int): Number of denoising steps. Default: 25\nfps (int): Frames per second. Default: 30\naes_score (float): Aesthetic score for prompt enhancement. Default: 6.0\nseed (int): Random seed for reproducibility. Default: 42\nMUGDiTPipeline\nMain inference pipeline class.\nMethods\n__init__(config: MUGDiTConfig)\nInitialize the pipeline with configuration.\ngenerate(prompt=None, reference_image_path=None, output_path=None, seed=None, **kwargs) -> str\nGenerate a video from text and reference image.\nParameters:\nprompt (str, optional): Text description of desired video\nreference_image_path (str|Path, optional): Path to reference image\noutput_path (str|Path, optional): Output video file path\nseed (int, optional): Random seed for this generation\nReturns:\nstr: Path to generated video file\nüèóÔ∏è Model Architecture\nMUGDiT adopts the latent diffusion transformer paradigm with rectified flow matching objectives:\nflowchart TB\nA[Input Video] --> B[VideoVAE Encoder]\nB --> C[\"Latent 8√ó8√ó8 compression\"]\nC --> D[\"3D Patch 2x2x2 Embedding\"]\nD --> E[\"MUGDiT Blocks x 56\"]\nF[Text] --> G[Caption Encoder]\nG --> E\nH[Timestep] --> E\nI[Size Info] --> E\nE --> J[Output Projection]\nJ --> K[VideoVAE Decoder]\nK --> L[Generated Video]\nstyle E fill:#e1f5ff,stroke:#0066cc,stroke-width:2px\nstyle C fill:#fff4e6,stroke:#ff9800,stroke-width:2px\nstyle L fill:#e8f5e9,stroke:#4caf50,stroke-width:2px\nCore Components\nVideoVAE: 8√ó8√ó8 spatiotemporal compression\nEncoder: 3D convolutions + temporal attention\nDecoder: 3D transposed convolutions + temporal upsampling\nKL regularization for stable latent space\n3D Patch Embedding: Converts video latents to tokens\nPatch size: 2√ó2√ó2 (non-overlapping)\nFinal compression: ~2048√ó vs. pixel space\nPosition Encoding: 3D Rotary Position Embeddings (RoPE)\nExtends 2D RoPE to handle temporal dimension\nFrequency-based encoding for spatiotemporal modeling\nConditioning Modules:\nCaption Embedder: Projects text embeddings (4096-dim) for cross-attention\nTimestep Embedder: Embeds diffusion timestep via sinusoidal encoding\nSize Embedder: Handles variable resolution inputs\nMUGDiT Transformer Block:\ngraph LR\nA[Input] --> B[AdaLN]\nB --> C[Self-Attn<br/>QK-Norm]\nC --> D[Gate]\nD --> E1[+]\nA --> E1\nE1 --> F[LayerNorm]\nF --> G[Cross-Attn<br/>QK-Norm]\nG --> E2[+]\nE1 --> E2\nE2 --> I[AdaLN]\nI --> J[MLP]\nJ --> K[Gate]\nK --> E3[+]\nE2 --> E3\nE3 --> L[Output]\nM[Timestep<br/>Size Info] -.-> B\nM -.-> I\nN[Text] -.-> G\nstyle C fill:#e3f2fd,stroke:#2196f3,stroke-width:2px\nstyle G fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\nstyle J fill:#fff3e0,stroke:#ff9800,stroke-width:2px\nstyle E1 fill:#c8e6c9,stroke:#4caf50,stroke-width:2px\nstyle E2 fill:#c8e6c9,stroke:#4caf50,stroke-width:2px\nstyle E3 fill:#c8e6c9,stroke:#4caf50,stroke-width:2px\nRectified Flow Scheduler:\nMore stable training than DDPM\nLogit-normal timestep sampling\nLinear interpolation between noise and data\nCitation\nIf you find our work helpful, please cite us.\n@article{mug-v2025,\ntitle={MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models},\nauthor={Yongshun Zhang and Zhongyi Fan and Yonghang Zhang and Zhangzikang Li and Weifeng Chen and Zhongwei Feng and Chaoyue Wang and Peng Hou and Anxiang Zeng},\njournal = {arXiv preprint arXiv:2510.17519},\nyear={2025}\n}\nüìÑ License\nThis project is licensed under the Apache License 2.0 - see the LICENSE file for details.\nNote: This is a research project. Generated content may not always be perfect. Please use responsibly and in accordance with applicable laws and regulations.\nAcknowledgements\nWe would like to thank the contributors to the Open-Sora, DeepFloyd/t5-v1_1-xxl, Wan-Video, Qwen, HuggingFace, Megatron-LM, TransformerEngine, DiffSynth, diffusers, PixArt, etc. repositories, for their open research.",
    "DD32/MagicNodes": "Features\nHardware\nüí• Memory\nInstall (ComfyUI 0.3.60, tested on this version)\nüöÄ \"One-Node\" Quickstart (MG_SuperSimple)\n‚ùóTips\nRepository Layout\nControl Fusion (mg_controlfusion.py, mg_controlfusion_easy.py,)\nCADE 2.5 (mg_cade25.py, mg_cade25_easy.py)\nMG_CleanUp (final memory cleanup node)\nDepth Anything v2 (vendor)\nMG_ZeSmartSampler (Experimental)\nSeed Latent (mg_seed_latent.py)\nMagic Latent Adapter (mg_latent_adapter.py) !experimental!\nDocumentation\nDependencies (Why These Packages)\nPreprint\nHow to Cite\nAttribution (kind request)\nLicense and Credits\nSupport\nÔªø# MagicNodes ‚Äî ComfyUI Render Pipeline (SD/SDXL)\nSimple start. Expert-grade results. Reliable detail.\n/\nTL;DR: MagicNodes, it's a plug-and-play multi-pass \"render-machine\" for SD/SDXL models. Simple one-node start, expert-grade results. Core is ZeResFDG (Frequency-Decoupled + Rescale + Zero-Projection)  and the always-on QSilk Micrograin Stabilizer, complemented by practical stabilizers (NAG, local masks, EPS, Muse Blend, Polish). Ships with a four-pass preset for robust, clean, and highly detailed outputs.\nOur pipeline runs through several purposeful passes: early steps assemble global shapes, mid steps refine important regions, and late steps polish without overcooking the texture. We gently stabilize the amplitudes of the \"image‚Äôs internal draft\" (latent) and adapt the allowed value range per region: where the model is confident we give more freedom, and where it‚Äôs uncertain we act more conservatively. The result is clean gradients, crisp edges, and photographic detail even at very high resolutions and, as a side effect on SDXL models, text becomes noticeably more stable and legible.\nPlease note that the SDXL architecture itself has limitations and the result depends on the success of the seed, the purity of your prompt and the quality of your model+LoRA.\nDraw\nPhoto Portrait\nPhoto Cup\nPhoto Dog\nFeatures\nZeResFDG: LF/HF split, energy rescale, and zero-projection (stable early, sharp late)\nNAG (Normalized Attention Guidance): small attention variance normalization (positive branch)\nLocal spatial gating: optional CLIPSeg masks for faces/hands/pose\nEPS scale: small early-step exposure bias\nQSilk Micrograin Stabilizer: gently smooths rare spikes and lets natural micro-texture (skin, fabric, tiny hairs) show through ‚Äî without halos or grid patterns. Always on, zero knobs, near‚Äëzero cost.\nAdaptive Quantile Clip (AQClip): softly adapts the allowed range per region. Confident areas keep more texture; uncertain ones get cleaner denoising. Tile‚Äëbased with seamless blending (no seams). Optional Attn mode uses attention confidence for an even smarter balance.\nMGHybrid scheduler: hybrid Karras/Beta sigma stack with smooth tail blending and tiny schedule jitter (ZeSmart-inspired) for more stable, detail-friendly denoising; used by CADE and SuperSimple by default\nSeed Latent (MG_SeedLatent): fast, deterministic latent initializer aligned to VAE stride; supports pure-noise starts or image-mixed starts (encode + noise) to gently bias content; batch-ready and resolution-agnostic, pairs well with SuperSimple recommended latent sizes for reproducible pipelines\nMuse Blend and Polish: directional post-mix and final low-frequency-preserving clean-up\nSmartSeed (CADE Easy and SuperSimple): set seed = 0 to auto-pick a good seed from a tiny low-step probe. Uses a low-discrepancy sweep, avoids speckles/overexposure, and, if available, leverages CLIP-Vision (with reference_image) and CLIPSeg focus text to favor semantically aligned candidates. Logs Smart_seed_random: Start/End.\nI highly recommend working with SmartSeed.\nCADE2.5 pipeline does not just upscale the image, it iterates and adds small details, doing it carefully, at every stage.\nHardware\nThe pipeline is designed for good hardware (tested on RTX5090 (32Gb) and RAM 128Gb), try to keep the starting latent very small, because there is an upscale at the steps and you risk getting errors if you push up the starting values.\nstart latent ~ 672x944 -> final ~ 3688x5192 across 4 steps.\nNotes\nLowering the starting latent (e.g., 512x768) or lower, reduces both VRAM and RAM.\nDisabling hi-res depth/edges (ControlFusion) reduces peaks. (not recommended!)\nDepth weights add a bit of RAM on load; models live under depth-anything/.\nüí• Memory\nAt each step, the image is upscaled from the previous step! Keep this in mind, the final image may not fit into your PC's memory if the starting latent is high.\nInstall (ComfyUI 0.3.60, tested on this version)\nPreparing:\nI recomend update pytorch version: 2.8.0+cu129.\nPyTorch install: pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu129\nCUDA manual download and install: https://developer.nvidia.com/cuda-12-9-0-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local\nInstall SageAttention 2.2.0, manualy https://github.com/thu-ml/SageAttention or use script scripts/check_sageattention.bat. The installation takes a few minutes, wait for the installation to finish.\np.s.: To work, you definitely need to install SageAttention v.2.2.0, version 1.0.6 is not suitable for pipeline.\nNext:\nClone or download this repo into ComfyUI/custom_nodes/\nInstall helpers: pip install -r requirements.txt\nI recomend, take my negative LoRA mg_7lambda_negative.safetensors in HF https://huggingface.co/DD32/mg_7lambda_negative/blob/main/mg_7lambda_negative.safetensors and place the file in ComfyUI, to ComfyUI/models/loras\ndownload model depth_anything_v2_vitl.pth https://huggingface.co/depth-anything/Depth-Anything-V2-Large/tree/main and place inside in to depth-anything/ folder.\nWorkflows\nFolder workflows/ contains ready-to-use graphs:\nmg_SuperSimple-Workflow.json ‚Äî one-node pipeline (2/3/4 steps) with presets\nmg_Easy-Workflow.json ‚Äî the same logic built from individual Easy nodes\nYou can save this workflow to ComfyUI ComfyUI\\user\\default\\workflows\nRestart ComfyUI. Nodes appear under the \"MagicNodes\" categories.\nI strongly recommend use mg_Easy-Workflow workflow + default settings + your model and my negative LoRA mg_7lambda_negative.safetensors, for best result.\nüöÄ \"One-Node\" Quickstart (MG_SuperSimple)\nStart with MG_SuperSimple for the easiest path:\nDrop MG_SuperSimple into the graph\nConnect model / positive / negative / vae / latent and a Load ControlNet Model module\nChoose step_count (3/4) and Run\nor load mg_SuperSimple-Workflow in panel ComfyUI\nNotes:\nWhen \"Custom\" is off, presets fully drive parameters\nWhen \"Custom\" is on, the visible CADE controls override the Step presets across all steps; Step 1 still enforces denoise=1.0\nCLIP Vision (if connected) is applied from Step 2 onward; if no reference image is provided, SuperSimple uses the previous step image as reference\nStep 1 and Step 2 it's a prewarming step.\n‚ùóTips\n(!) There are almost always artifacts in the first step, don't pay attention to them, they will be removed in the next steps. Keep your prompt clean and logical, don't duplicate details and be careful with symbols.\nMG_SuperSimple-Workflow is a bit less flexible than MG_Easy-Workflow, but extremely simple to use. If you just want a stable, interesting result, start with SuperSimple.\nRecommended negative LoRA: mg_7lambda_negative.safetensors with strength_model = -1.0, strength_clip = 0.2. Place LoRA files under ComfyUI/models/loras so they appear in the LoRA selector.\nDownload a CLIP Vision model and place it under ComfyUI/models/clip_vision (e.g., https://huggingface.co/openai/clip-vit-large-patch14; heavy alternative: https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K). SuperSimple/CADE will use it for reference-based polish.\nSamplers: i recomend use ddim for many cases (Draw and Realism style). Scheduler: use MGHybrid in this pipeline.\nDenoise: higher -> more expressive and vivid; you can go up to 1.0. The same applies to CFG: higher -> more expressive but may introduce artifacts. Suggested CFG range: ~4.5‚Äì8.5.\nIf you see unwanted artifacts on the final (4th) step, slightly lower denoise to ~0.5‚Äì0.6 or simply change the seed.\nYou can get interesting results by repeating steps (in Easy/Hard workflows), e.g., 1 -> 2 -> 3 -> 3. ¬†Just experiment with it!\nRecommended starting latent close to ~672x944 (other aspect ratios are fine). With that, step 4 produces ~3688x5192. Larger starting sizes are OK if the model and your hardware allow.\nUnlucky seeds happen ‚Äî just try another. (We may later add stabilization to this process.)\nRarely, step 3 can show a strange grid artifact (in both Easy and Hard workflows). If this happens, try changing CFG or seed. Root cause still under investigation.\nResults depend on checkpoint/LoRA quality. The pipeline ‚Äúsqueezes‚Äù everything SDXL and your model can deliver, so prefer high‚Äëquality checkpoints and non‚Äëovertrained LoRAs.\nAvoid using more than 3 LoRAs at once, and keep only one ‚Äúlead‚Äù LoRA (one you trust is not overtrained). Too many/strong LoRAs can spoil results.\nTry connecting reference images in either workflow ‚Äî you can get unusual and interesting outcomes.\nVery often, the image in step 3 is of very good quality, but it usually lacks sharpness. But if you have a weak system, you can limit yourself to 3 steps.\nSmartSeed (auto seed pick): set seed = 0 in Easy or SuperSimple. The node will sample several candidate seeds and do a quick low‚Äëstep probe to choose a balanced one. You‚Äôll see logs Smart_seed_random: Start and Smart_seed_random: End. Seed is: <number>. Use any non‚Äëzero seed for fully deterministic runs.\nThe 4th step sometimes saves the image for a long time, just wait for the end of the process, it depends on the initial resolution you set.\nThe CombiNode node is optional, you can replace it with standard ComfyUI pipelines.\nRepository Layout\nRepository Layout\nMagicNodes/\n‚îú‚îÄ README.md\n‚îú‚îÄ LICENSE                      # AGPL-3.0-or-later\n‚îú‚îÄ assets/\n‚îú‚îÄ docs/\n‚îÇ  ‚îú‚îÄ EasyNodes.md\n‚îÇ  ‚îú‚îÄ HardNodes.md\n‚îÇ  ‚îî‚îÄ hard/\n‚îÇ     ‚îú‚îÄ CADE25.md\n‚îÇ     ‚îú‚îÄ ControlFusion.md\n‚îÇ     ‚îú‚îÄ UpscaleModule.md\n‚îÇ     ‚îú‚îÄ IDS.md\n‚îÇ     ‚îî‚îÄ ZeSmartSampler.md\n‚îÇ\n‚îú‚îÄ mod/\n‚îÇ  ‚îú‚îÄ easy/\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_cade25_easy.py\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_controlfusion_easy.py\n‚îÇ  ‚îÇ  ‚îî‚îÄ mg_supersimple_easy.py\n‚îÇ  ‚îÇ  ‚îî‚îÄ preset_loader.py\n‚îÇ  ‚îú‚îÄ hard/\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_cade25.py\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_controlfusion.py\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_tde2.py\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_upscale_module.py\n‚îÇ  ‚îÇ  ‚îú‚îÄ mg_ids.py\n‚îÇ  ‚îÇ  ‚îî‚îÄ mg_zesmart_sampler_v1_1.py\n‚îÇ  ‚îÇ\n‚îÇ  ‚îú‚îÄ mg_cleanup.py\n‚îÇ  ‚îú‚îÄ mg_combinode.py\n‚îÇ  ‚îú‚îÄ mg_latent_adapter.py\n‚îÇ  ‚îú‚îÄ mg_sagpu_attention.py\n‚îÇ  ‚îî‚îÄ mg_seed_latent.py\n‚îÇ\n‚îú‚îÄ pressets/\n‚îÇ  ‚îú‚îÄ mg_cade25.cfg\n‚îÇ  ‚îî‚îÄ mg_controlfusion.cfg\n‚îÇ\n‚îú‚îÄ scripts/\n‚îÇ  ‚îú‚îÄ check_sageattention.bat\n‚îÇ  ‚îî‚îÄ check_sageattention.ps1\n‚îÇ\n‚îú‚îÄ depth-anything/              # place Depth Anything v2 weights (.pth), e.g., depth_anything_v2_vitl.pth\n‚îÇ  ‚îî‚îÄ depth_anything_v2_vitl.pth\n‚îÇ\n‚îú‚îÄ vendor/\n‚îÇ  ‚îî‚îÄ depth_anything_v2/        # vendored Depth Anything v2 code (Apache-2.0)\n‚îÇ\n‚îú‚îÄ workflows/\n‚îÇ  ‚îú‚îÄ mg_SuperSimple-Workflow.json\n‚îÇ  ‚îî‚îÄ mg_Easy-Workflow.json\n|\n‚îî‚îÄ requirements.txt\nModule Notes and Guides\nA compact set of per‚Äëmodule notes, knobs, and usage tips covering Control Fusion, CADE 2.5, MG_CleanUp, and the experimental Magic Latent Adapter.\nControl Fusion (mg_controlfusion.py, mg_controlfusion_easy.py,)\nBuilds depth + edge masks with preserved aspect ratio; hires-friendly mask mode\nKey surface knobs: edge_alpha, edge_smooth, edge_width, edge_single_line/edge_single_strength, edge_depth_gate/edge_depth_gamma\nPreview can optionally reflect ControlNet strength via preview_show_strength and preview_strength_branch\nCADE 2.5 (mg_cade25.py, mg_cade25_easy.py)\nDeterministic preflight: CLIPSeg pinned to CPU; preview mask reset; noise tied to iter_seed\nEncode/Decode: stride-aligned, with larger overlap for >2K to avoid artifacts\nPolish mode (final hi-res refinement):\npolish_enable, polish_keep_low (global form from reference), polish_edge_lock, polish_sigma\nSmooth start via polish_start_after and polish_keep_low_ramp\neps_scale supported for gentle exposure shaping\nMG_CleanUp (final memory cleanup node)\nPurpose: a tiny end-of-graph node that aggressively frees RAM/VRAM and asks the OS to return freed pages. Place it at the very end of a workflow (ideally right after SaveImage).\nReturns: passthrough LATENT and a small IMAGE preview (32√ó32). For the cleanup to work, be sure to connect the Preview node to the IMAGE output.\nWhat it does (two passes: immediate and +150 ms):\nCUDA sync, gc.collect(), torch.cuda.empty_cache() + ipc_collect()\nComfy model manager soft cache drop; when hard_trim=true also unloads loaded models (will reload on next run)\nDrops lightweight LRU/preview caches (when available)\nWindows: trims working set (SetProcessWorkingSetSize + EmptyWorkingSet) and best-effort system cache/standby purge\nLinux: malloc_trim(0) to release fragmented heap back to the OS\nLogs how much RAM/VRAM was freed in each pass\nInputs:\nhard_trim (bool): enable the strongest cleanup (unload models, OS-level trims).\nsync_cuda (bool): synchronize CUDA before cleanup (recommended).\nhires_only_threshold (int): run only when the latent longest side ‚â• threshold; 0 = always.\nNotes:\nBecause models are unloaded in hard_trim, the next workflow run may take a bit longer to start (models will reload).\nUse this node only at the end of a graph ‚Äî it is intentionally aggressive.\nDepth Anything v2 (vendor)\nLives under vendor/depth_anything_v2; Apache-2.0 license\nDepth models (Depth Anything v2)\nPlace DA v2 weights (.pth) in depth-anything/. Recommended: depth_anything_v2_vitl.pth (ViT-L). Supported names include:\ndepth_anything_v2_vits.pth, depth_anything_v2_vitb.pth, depth_anything_v2_vitl.pth, depth_anything_v2_vitg.pth,\nand the metric variants depth_anything_v2_metric_vkitti_vitl.pth, depth_anything_v2_metric_hypersim_vitl.pth.\nControlFusion auto-detects the correct config from the filename and uses this path by default. You can override via the\ndepth_model_path parameter (preset) if needed.\nIf no weights are found, ControlFusion falls back gracefully (luminance pseudo-depth), but results are better with DA v2.\nWhere to get weights: see the official Depth Anything v2 repository (https://github.com/DepthAnything/Depth-Anything-V2)\nand its Hugging Face models page (https://huggingface.co/Depth-Anything) for pre-trained .pth files.\nMG_ZeSmartSampler (Experimental)\nCustom sampler that builds hybrid sigma schedules (Karras/Beta blend) with tail smoothing\nInputs/Outputs match KSampler: MODEL/SEED/STEPS/CFG/base_sampler/schedule/CONDITIONING/LATENT -> LATENT\nKey params: hybrid_mix, jitter_sigma, tail_smooth, optional PC2-like shaping (smart_strength, target_error, curv_sensitivity)\nSeed Latent (mg_seed_latent.py)\nPurpose: quick LATENT initializer aligned to VAE stride (4xC, H/8, W/8). Can start from pure noise or mix an input image encoding with noise to gently bias content.\nInputs\nwidth, height, batch_size\nsigma (noise amplitude) and bias (additive offset)\nOptional vae and image when mix_image is enabled\nOutput: LATENT dict { \"samples\": tensor } ready to feed into CADE/SuperSimple.\nUsage notes\nKeep dimensions multiples of 8; recommended starting sizes around ~672x944 (other aspect ratios work). With SuperSimple‚Äôs default scale, step 4 lands near ~3688x5192.\nmix_image=True encodes the provided image via VAE and adds noise: a soft way to keep global structure while allowing refinement downstream.\nFor run-to-run comparability, hold your sampler seed fixed (in SuperSimple/CADE). SeedLatent itself does not expose a seed; variation is primarily controlled by the sampler seed.\nBatch friendly: batch_size>1 produces independent latents of the chosen size.\nMagic Latent Adapter (mg_latent_adapter.py) !experimental!\nPurpose: small adapter node that generates or adapts a LATENT to match the target model‚Äôs latent format (channels and dimensions), including 5D layouts (NCDHW) when required. Two modes: generate (make a fresh latent aligned to VAE stride) and adapt (reshape/channel‚Äëmatch an existing latent).\nHow it works: relies on Comfy‚Äôs fix_empty_latent_channels and reads the model‚Äôs latent_format to adjust channel count; aligns spatial size to VAE stride; handles 4D (NCHW) and 5D (NCDHW).\nExperimental: added to ease early, experimental support for FLUX/Qwen‚Äëlike models by reducing shape/dimension friction. Still evolving; treat as opt‚Äëin.\nUsage: place before CADE/your sampler. In generate mode you can also enable image mixing via VAE; in adapt mode feed any upstream LATENT and the current MODEL. A simple family switch (auto / SD / SDXL / FLUX) controls stride fallback when VAE isn‚Äôt provided.\nNotes: quality with FLUX/Qwen models also depends on using the proper text encoders/conditioning nodes for those families; this adapter only solves latent shapes, not conditioning mismatches.\nDocumentation\nEasy nodes overview and MG_SuperSimple: docs/EasyNodes.md\nHard nodes documentation index: docs/HardNodes.md\nDependencies (Why These Packages)\ntransformers ‚Äî used by CADE for CLIPSeg (CIDAS/clipseg-rd64-refined) to build text‚Äëdriven masks (e.g., face/hands). If missing, CLIPSeg is disabled gracefully.\nopencv-contrib-python ‚Äî ControlFusion edge stack (Pyramid Canny, thinning via ximgproc), morphological ops, light smoothing.\nPillow ‚Äî image I/O and small conversions in preview/CLIPSeg pipelines.\nscipy ‚Äî preferred Gaussian filtering path for IDS (quality). If not installed, IDS falls back to a PyTorch implementation.\nsageattention ‚Äî accelerated attention kernels (auto-picks a kernel per GPU arch); CADE/attention patch falls back to stock attention if not present.\nOptional extras\ncontrolnet-aux ‚Äî alternative loader for Depth Anything v2 if you don‚Äôt use the vendored implementation (not required by default).\nPreprint\nCADE 2.5 - ZeResFDG\nPDF: https://arxiv.org/pdf/2510.12954.pdf\narXiv: https://arxiv.org/abs/2510.12954\nCADE 2.5 - QSilk\nPDF: https://arxiv.org/pdf/2510.15761\narXiv: https://arxiv.org/abs/2510.15761\nHow to Cite\n@misc{rychkovskiy2025cade25zeresfdg,\ntitle={CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models},\nauthor={Denis Rychkovskiy},\nyear={2025},\neprint={2510.12954},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.12954},\n}\n@misc{rychkovskiy2025qsilkmicrograinstabilizationadaptive,\ntitle={QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion},\nauthor={Denis Rychkovskiy},\nyear={2025},\neprint={2510.15761},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2510.15761},\n}\nAttribution (kind request)\nIf you use this work or parts of it, please consider adding the following credit in your README/About/credits: \"Includes CADE 2.5 (ZeResFDG, QSilk) by Denis Rychkovskiy (‚ÄúDZRobo‚Äù)\"\nLicense and Credits\nLicense: AGPL-3.0-or-later (see LICENSE)\nSupport\nIf this project saved you time, you can leave a tip:\nGitHub Sponsors: https://github.com/sponsors/1dZb1\nBymeacoffee: https://buymeacoffee.com/dzrobo",
    "jayn7/WAN2.2-I2V_A14B-DISTILL-LIGHTX2V-4STEP-GGUF": "Usage\nCredits\nUsage\nThe model can be used with:\nComfyUI-GGUF by city96\nComfyUI-WanVideoWrapper by kijai (make sure you're on the latest version)\nCredits\nThe quantization process based on the quantization guide and scripts by city96.",
    "nvidia/nemoretriever-ocr-v1": "NeMo Retriever OCR v1\nModel Overview\nDescription\nLicense/Terms of use\nTeam\nDeployment Geography\nUse Case\nRelease Date\nReferences\nModel Architecture\nInput\nOutput\nSample output\nUsage\nModel Version(s):\nTraining and Evaluation Datasets:\nTraining Dataset\nEvaluation Datasets\nEvaluation Results\nDetailed Performance Analysis\nEthical Considerations\nBias\nExplainability\nPrivacy\nSafety\nNeMo Retriever OCR v1\nModel Overview\nDescription\nThe NeMo Retriever OCR v1 model is a state-of-the-art text recognition model designed for robust end-to-end optical character recognition (OCR) on complex real-world images. It integrates three core neural network modules: a detector for text region localization, a recognizer for transcription of detected regions, and a relational model for layout and structure analysis.\nThis model is optimized for a wide variety of OCR tasks, including multi-line, multi-block, and natural scene text, and it supports advanced reading order analysis via its relational model component. NeMo Retriever OCR v1 has been developed to be production-ready and commercially usable, with a focus on speed and accuracy on both document and natural scene images.\nThe NeMo Retriever OCR v1 model is part of the NVIDIA NeMo Retriever collection of NIM microservices, which provides state-of-the-art, commercially-ready models and microservices optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants.\nThis model is ready for commercial use.\nWe are excited to announce the open sourcing of this commercial model. For users interested in deploying this model in production environments, it is also available via the model API in NVIDIA Inference Microservices (NIM) at nemoretriever-ocr-v1.\nLicense/Terms of use\nThe use of this model is governed by the NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nTeam\nMike Ranzinger\nBo Liu\nTheo Viel\nCharles Blackmon-Luca\nOliver Holworthy\nEdward Kim\nEven Oldridge\nDeployment Geography\nGlobal\nUse Case\nThe NeMo Retriever OCR v1 model is designed for high-accuracy and high-speed extraction of textual information from images, making it ideal for powering multimodal retrieval systems, Retrieval-Augmented Generation (RAG) pipelines, and agentic applications that require seamless integration of visual and language understanding. Its robust performance and efficiency make it an excellent choice for next-generation AI systems that demand both precision and scalability across diverse real-world content.\nRelease Date\n10/23/2025 via https://huggingface.co/nvidia/nemoretriever-ocr-v1\nReferences\nTechnical blog: https://developer.nvidia.com/blog/approaches-to-pdf-data-extraction-for-information-retrieval/\nModel Architecture\nArchitecture Type: Hybrid detector‚Äìrecognizer with document-level relational modeling\nThe NeMo Retriever OCR v1 model integrates three specialized neural components:\nText Detector: Utilizes a RegNetY-8GF convolutional backbone for high-accuracy localization of text regions within images.\nText Recognizer: Employs a Transformer-based sequence recognizer to transcribe text from detected regions, supporting variable word and line lengths.\nRelational Model: Applies a multi-layer global relational module to predict logical groupings, reading order, and layout relationships across detected text elements.\nAll components are trained jointly in an end-to-end fashion, providing robust, scalable, and production-ready OCR for diverse document and scene images.\nNetwork Architecture: RegNetY-8GF\nParameter Counts:\nComponent\nParameters\nDetector\n45,268,472\nRecognizer\n4,944,346\nRelational model\n2,254,422\nTotal\n52,467,240\nInput\nProperty\nValue\nInput Type & Format\nImage (RGB, PNG/JPEG, float32/uint8), aggregation level (word, sentence, or paragraph)\nInput Parameters (Two-Dimensional)\n3 x H x W (single image) or B x 3 x H x W (batch)\nInput Range\n[0, 1] (float32) or [0, 255] (uint8, auto-converted)\nOther Properties\nHandles both single images and batches. Automatic multi-scale resizing for best accuracy.\nOutput\nProperty\nValue\nOutput Type\nStructured OCR results: a list of detected text regions (bounding boxes), recognized text, and confidence scores\nOutput Format\nBounding boxes: tuple of floats, recognized text: string, confidence score: float\nOutput Parameters\nBounding boxes: One-Dimenional (1D) list of bounding box coordinates, recognized text: One-Dimenional (1D) list of strings, confidence score: One-Dimenional (1D) list of floats\nOther Properties\nPlease see the sample output for an example of the model output\nSample output\nocr_boxes = [[[15.552736282348633, 43.141815185546875],\n[150.00149536132812, 43.141815185546875],\n[150.00149536132812, 56.845645904541016],\n[15.552736282348633, 56.845645904541016]],\n[[298.3145751953125, 44.43315124511719],\n[356.93585205078125, 44.43315124511719],\n[356.93585205078125, 57.34814453125],\n[298.3145751953125, 57.34814453125]],\n[[15.44686508178711, 13.67985725402832],\n[233.15859985351562, 13.67985725402832],\n[233.15859985351562, 27.376562118530273],\n[15.44686508178711, 27.376562118530273]],\n[[298.51727294921875, 14.268900871276855],\n[356.9850769042969, 14.268900871276855],\n[356.9850769042969, 27.790447235107422],\n[298.51727294921875, 27.790447235107422]]]\nocr_txts = ['The previous notice was dated',\n'22 April 2016',\n'The previous notice was given to the company on',\n'22 April 2016']\nocr_confs = [0.97730815, 0.98834222, 0.96804602, 0.98499225]\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nUsage\nPrerequisites\nOS: Linux amd64 with NVIDIA GPU\nCUDA: CUDA Toolkit 12.8 and compatible NVIDIA driver installed (for PyTorch CUDA). Verify with nvidia-smi.\nPython: 3.12 (both subpackages require python = ~3.12)\nBuild tools (when building the C++ extension):\nGCC/G++ with C++17 support\nCUDA toolkit headers (for building CUDA kernels)\nOpenMP (used by the C++ extension)\nInstallation\nThe model requires torch, and the custom code available in this repository.\nClone the repository\nMake sure git-lfs is installed (https://git-lfs.com)\ngit lfs install\nUsing https\ngit clone https://huggingface.co/nvidia/nemoretriever-ocr-v1\nOr using ssh\ngit clone git@hf.co:nvidia/nemoretriever-ocr-v1\nInstallation\nWith pip\nCreate and activate a Python 3.12 environment (optional)\nRun the following command to install the package:\ncd nemo-retriever-ocr\npip install hatchling\npip install -v .\nWith docker\nRun the example end-to-end without installing anything on the host (besides Docker, docker compose, and NVIDIA Container Toolkit):\nEnsure Docker can see your GPU:\ndocker run --rm --gpus all nvcr.io/nvidia/pytorch:25.09-py3 nvidia-smi\nFrom the repo root, bring up the service to run the example against the provided image ocr-example-image.png:\ndocker compose run --rm nemo-retriever-ocr \\\nbash -lc \"python example.py ocr-example-input-1.png --merge-level paragraph\"\nThis will:\nBuild an image from the provided Dockerfile (based on nvcr.io/nvidia/pytorch)\nMount the repo at /workspace\nRun example.py with model from checkpoints\nOutput is saved next to your input image as <name>-annotated.<ext> on the host.\nRun the model using the following code:\nfrom nemo_retriever_ocr.inference.pipeline import NemoRetrieverOCR\nocr = NemoRetrieverOCR()\npredictions = ocr(\"ocr-example-input-1.png\")\nfor pred in predictions:\nprint(\nf\"  - Text: '{pred['text']}', \"\nf\"Confidence: {pred['confidence']:.2f}, \"\nf\"Bbox: [left={pred['left']:.4f}, upper={pred['upper']:.4f}, right={pred['right']:.4f}, lower={pred['lower']:.4f}]\"\n)\nModel Version(s):\nnemoretriever-ocr-v1\nTraining and Evaluation Datasets:\nTraining Dataset\nData Modality\nImage\nImage Training Data Size\nLess than a Million Images\nThe model is trained on a large-scale, curated mix of public and proprietary OCR datasets, focusing on high diversity of document layouts and scene images. The training set includes synthetic and real images with varied noise and backgrounds, filtered for commercial use eligibility.\nData Collection Method: Hybrid (Automated, Human, Synthetic)\nLabeling Method: Hybrid (Automated, Human, Synthetic)\nProperties: Includes scanned documents, natural scene images, receipts, and business documents.\nEvaluation Datasets\nThe NeMo Retriever OCR v1 model is evaluated on several NVIDIA internal datasets for various tasks, such as pure OCR, table content extraction, and document retrieval.\nData Collection Method: Hybrid (Automated, Human, Synthetic)\nLabeling Method: Hybrid (Automated, Human, Synthetic)\nProperties: Benchmarks include challenging scene images, documents with varied layouts, and multi-language data.\nEvaluation Results\nWe benchmarked NeMo Retriever OCR v1 on internal evaluation datasets against PaddleOCR on various tasks, such as pure OCR (Character Error Rate), table content extraction (TEDS), and document retrieval (Recall@5).\nMetric\nNeMo Retriever OCR v1\nPaddleOCR\nNet change\nCharacter Error Rate\n0.1633\n0.2029\n-19.5% ‚úîÔ∏è\nBag-of-character Error Rate\n0.0453\n0.0512\n-11.5% ‚úîÔ∏è\nBag-of-word Error Rate\n0.1203\n0.2748\n-56.2% ‚úîÔ∏è\nTable Extraction TEDS\n0.781\n0.781\n0.0% ‚öñÔ∏è\nPublic Earnings Multimodal Recall@5\n0.779\n0.775\n+0.5% ‚úîÔ∏è\nDigital Corpora Multimodal Recall@5\n0.901\n0.883\n+2.0% ‚úîÔ∏è\nDetailed Performance Analysis\nThe model demonstrates robust performance on complex layouts, noisy backgrounds, and challenging real-world scenes. Reading order and block detection are powered by the relational module, supporting downstream applications such as chart-to-text, table-to-text, and infographic-to-text extraction.\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Explainability, Bias, Safety & Security, and Privacy sections below.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Task/Domain:\nOptical Character Recognition (OCR) with a focus on retrieval application and documents.\nModel Type:\nHybrid neural network with convolutional detector, transformer recognizer, and document structure modeling.\nIntended Users:\nDevelopers and teams building AI-driven search applications, retrieval-augmented generation (RAG) workflows, multimodal agents, or document intelligence applications. It is ideal for those working with large collections of scanned or photographed documents, including PDFs, forms, and reports.\nOutput:\nStructured OCR results, including detected bounding boxes, recognized text, and confidence scores.\nDescribe how the model works:\nThe model first detects text regions in the image, then transcribes recognized text, and finally analyzes document structure and reading order. Outputs structured, machine-readable results suitable for downstream search and analysis.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nNot Applicable\nTechnical Limitations:\nThis model version supports English only.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nAccuracy (e.g., character error rate), throughput, and latency.\nPotential Known Risks:\nThe model may not always extract or transcribe all text with perfect accuracy, particularly in cases of poor image quality or highly stylized fonts.\nLicensing & Terms of Use:\nUse of this model is governed by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNo\nPersonal data used to create this model?\nNone Known\nHow often is dataset reviewed?\nThe dataset is initially reviewed when added, and subsequent reviews are conducted as needed or in response to change requests.\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety\nField\nResponse\nModel Application Field(s):\nText recognition and structured OCR for multimodal retrieval. Inputs can include natural scene images, scanned documents, charts, tables, and infographics.\nUse Case Restrictions:\nAbide by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nModel and dataset restrictions:\nThe principle of least privilege (PoLP) is applied, limiting access for dataset generation and model development. Restrictions enforce dataset access only during training, and all dataset license constraints are adhered to.\nDescribe the life critical impact (if present):\nNot applicable.",
    "yairpatch/Qwen3-VL-32B-Instruct-GGUF": "Qwen3-VL-32B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-32B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-32B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-32B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-32B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-32B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "cerebras/GLM-4.6-REAP-268B-A32B": "GLM-4.6-REAP-268B-A32B\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.6-REAP-268B-A32B\n‚ú® Highlights\nIntroducing GLM-4.6-REAP-268B-A32B, a memory-efficient compressed variant of GLM-4.6 that maintains near-identical performance while being 25% lighter.\nNote: this is a BF16 version for more accurate downstream low-bit quantization. An FP8 version is also available on HF.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 355B model\n25% Memory Reduction: Compressed from 355B to 268B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nüìã Model Overview\nGLM-4.6-REAP-268B-A32B has the following specifications:\nBase Model: GLM-4.6\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 25% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 268B total, 32B activated per token\nNumber of Layers: 92\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 120 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 202,752 tokens\nLicense: MIT\nüìä Evaluations\nTBD for BF16 model. Evalulation results available for the FP8 variant.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.6-REAP-268B-A32B \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.6, with a 25% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nzai-org/GLM-4.6\nand distributed under the MIT license.\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "distilbert/distilbert-base-uncased": "DistilBERT base model (uncased)\nModel description\nIntended uses & limitations\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nDistilBERT base model (uncased)\nThis model is a distilled version of the BERT base model. It was\nintroduced in this paper. The code for the distillation process can be found\nhere. This model is uncased: it does\nnot make a difference between english and English.\nModel description\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\nDistillation loss: the model was trained to return the same probabilities as the BERT base model.\nMasked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\nsentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\nmodel and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\nusually see the words one after the other, or from autoregressive models like GPT which internally mask the future\ntokens. It allows the model to learn a bidirectional representation of the sentence.\nCosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\nmodel.\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\nIntended uses & limitations\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the model hub to look for\nfine-tuned versions on a task that interests you.\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n'score': 0.05292855575680733,\n'token': 2535,\n'token_str': 'role'},\n{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n'score': 0.03968575969338417,\n'token': 4827,\n'token_str': 'fashion'},\n{'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n'score': 0.034743521362543106,\n'token': 2449,\n'token_str': 'business'},\n{'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n'score': 0.03462274372577667,\n'token': 2944,\n'token_str': 'model'},\n{'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n'score': 0.018145186826586723,\n'token': 11643,\n'token_str': 'modeling'}]\nHere is how to use this model to get the features of a given text in PyTorch:\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nand in TensorFlow:\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nLimitations and bias\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\nthe bias of its teacher model.\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n'score': 0.1235365942120552,\n'token': 20987,\n'token_str': 'blacksmith'},\n{'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n'score': 0.10142576694488525,\n'token': 10533,\n'token_str': 'carpenter'},\n{'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n'score': 0.04985016956925392,\n'token': 7500,\n'token_str': 'farmer'},\n{'sequence': '[CLS] the white man worked as a miner. [SEP]',\n'score': 0.03932540491223335,\n'token': 18594,\n'token_str': 'miner'},\n{'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n'score': 0.03351764753460884,\n'token': 14998,\n'token_str': 'butcher'}]\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n'score': 0.13283951580524445,\n'token': 13877,\n'token_str': 'waitress'},\n{'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n'score': 0.12586183845996857,\n'token': 6821,\n'token_str': 'nurse'},\n{'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n'score': 0.11708822101354599,\n'token': 10850,\n'token_str': 'maid'},\n{'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n'score': 0.11499975621700287,\n'token': 19215,\n'token_str': 'prostitute'},\n{'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n'score': 0.04722772538661957,\n'token': 22583,\n'token_str': 'housekeeper'}]\nThis bias will also affect all fine-tuned versions of this model.\nTraining data\nDistilBERT pretrained on the same data as BERT, which is BookCorpus, a dataset\nconsisting of 11,038 unpublished books and English Wikipedia\n(excluding lists, tables and headers).\nTraining procedure\nPreprocessing\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n[CLS] Sentence A [SEP] Sentence B [SEP]\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.\nPretraining\nThe model was trained on 8 16 GB V100 for 90 hours. See the\ntraining code for all hyperparameters\ndetails.\nEvaluation results\nWhen fine-tuned on downstream tasks, this model achieves the following results:\nGlue test results:\nTask\nMNLI\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\n82.2\n88.5\n89.2\n91.3\n51.3\n85.8\n87.5\n59.9\nBibTeX entry and citation info\n@article{Sanh2019DistilBERTAD,\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\nauthor={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\njournal={ArXiv},\nyear={2019},\nvolume={abs/1910.01108}\n}",
    "microsoft/deberta-v3-base": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nCitation\nDeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\nDeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data.\nIn DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.\nPlease check the official repository for more implementation details and updates.\nThe DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\nFine-tuning on NLU tasks\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\nModel\nVocabulary(K)\nBackbone #Params(M)\nSQuAD 2.0(F1/EM)\nMNLI-m/mm(ACC)\nRoBERTa-base\n50\n86\n83.7/80.5\n87.6/-\nXLNet-base\n32\n92\n-/80.2\n86.8/-\nELECTRA-base\n30\n86\n-/80.5\n88.8/\nDeBERTa-base\n50\n100\n86.2/83.1\n88.8/88.5\nDeBERTa-v3-base\n128\n86\n88.4/85.4\n90.6/90.7\nDeBERTa-v3-base + SiFT\n128\n86\n-/-\n91.0/-\nWe present the dev results on SQuAD 1.1/2.0 and MNLI tasks.\nFine-tuning with HF transformers\n#!/bin/bash\ncd transformers/examples/pytorch/text-classification/\npip install datasets\nexport TASK_NAME=mnli\noutput_dir=\"ds_results\"\nnum_gpus=8\nbatch_size=8\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\nrun_glue.py \\\n--model_name_or_path microsoft/deberta-v3-base \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--evaluation_strategy steps \\\n--max_seq_length 256 \\\n--warmup_steps 500 \\\n--per_device_train_batch_size ${batch_size} \\\n--learning_rate 2e-5 \\\n--num_train_epochs 3 \\\n--output_dir $output_dir \\\n--overwrite_output_dir \\\n--logging_steps 1000 \\\n--logging_dir $output_dir\nCitation\nIf you find DeBERTa useful for your work, please cite the following papers:\n@misc{he2021debertav3,\ntitle={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},\nauthor={Pengcheng He and Jianfeng Gao and Weizhu Chen},\nyear={2021},\neprint={2111.09543},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}",
    "uwg/upscaler": "The models they found here taken from the community OpenModelDB is a community driven database of AI Upscaling models. openmodeldb",
    "lllyasviel/ControlNet-v1-1": "This is the model files for ControlNet 1.1.\nThis model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.",
    "intfloat/multilingual-e5-large": "Multilingual-E5-large\nUsage\nSupported Languages\nTraining Details\nBenchmark Results on Mr. TyDi\nMTEB Benchmark Evaluation\nSupport for Sentence Transformers\nFAQ\nCitation\nLimitations\nMultilingual-E5-large\nMultilingual E5 Text Embeddings: A Technical Report.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024\nThis model has 24 layers and the embedding size is 1024.\nUsage\nBelow is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef average_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nlast_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nreturn last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n'query: ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: 1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ Ë∞ÉÊñô:Ëë±„ÄÅÁõê„ÄÅÁôΩÁ≥ñ„ÄÅÈ∏°Á≤æ ÂÅöÊ≥ï: 1„ÄÅÂçóÁìúÁî®ÂàÄËñÑËñÑÁöÑÂâäÂéªË°®Èù¢‰∏ÄÂ±ÇÁöÆ,Áî®Âã∫Â≠êÂàÆÂéªÁì§ 2„ÄÅÊì¶ÊàêÁªÜ‰∏ù(Ê≤°ÊúâÊì¶ËèúÊùøÂ∞±Áî®ÂàÄÊÖ¢ÊÖ¢ÂàáÊàêÁªÜ‰∏ù) 3„ÄÅÈîÖÁÉßÁÉ≠ÊîæÊ≤π,ÂÖ•Ëë±Ëä±ÁÖ∏Âá∫È¶ôÂë≥ 4„ÄÅÂÖ•ÂçóÁìú‰∏ùÂø´ÈÄüÁøªÁÇí‰∏ÄÂàÜÈíüÂ∑¶Âè≥,ÊîæÁõê„ÄÅ‰∏ÄÁÇπÁôΩÁ≥ñÂíåÈ∏°Á≤æË∞ÉÂë≥Âá∫ÈîÖ 2.È¶ôËë±ÁÇíÂçóÁìú ÂéüÊñô:ÂçóÁìú1Âè™ Ë∞ÉÊñô:È¶ôËë±„ÄÅËíúÊú´„ÄÅÊ©ÑÊ¶ÑÊ≤π„ÄÅÁõê ÂÅöÊ≥ï: 1„ÄÅÂ∞ÜÂçóÁìúÂéªÁöÆ,ÂàáÊàêÁâá 2„ÄÅÊ≤πÈîÖ8ÊàêÁÉ≠Âêé,Â∞ÜËíúÊú´ÊîæÂÖ•ÁàÜÈ¶ô 3„ÄÅÁàÜÈ¶ôÂêé,Â∞ÜÂçóÁìúÁâáÊîæÂÖ•,ÁøªÁÇí 4„ÄÅÂú®ÁøªÁÇíÁöÑÂêåÊó∂,ÂèØ‰ª•‰∏çÊó∂Âú∞ÂæÄÈîÖÈáåÂä†Ê∞¥,‰ΩÜ‰∏çË¶ÅÂ§™Â§ö 5„ÄÅÊîæÂÖ•Áõê,ÁÇíÂåÄ 6„ÄÅÂçóÁìúÂ∑Æ‰∏çÂ§öËΩØÂíåÁªµ‰∫Ü‰πãÂêé,Â∞±ÂèØ‰ª•ÂÖ≥ÁÅ´ 7„ÄÅÊííÂÖ•È¶ôËë±,Âç≥ÂèØÂá∫ÈîÖ\"]\ntokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\nmodel = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\nSupported Languages\nThis model is initialized from xlm-roberta-large\nand continually trained on a mixture of multilingual datasets.\nIt supports 100 languages from xlm-roberta,\nbut low-resource languages may see performance degradation.\nTraining Details\nInitialization: xlm-roberta-large\nFirst stage: contrastive pre-training with weak supervision\nDataset\nWeak supervision\n# of text pairs\nFiltered mC4\n(title, page content)\n1B\nCC News\n(title, news content)\n400M\nNLLB\ntranslation pairs\n2.4B\nWikipedia\n(hierarchical section title, passage)\n150M\nFiltered Reddit\n(comment, response)\n800M\nS2ORC\n(title, abstract) and citation pairs\n100M\nStackexchange\n(question, answer)\n50M\nxP3\n(input prompt, response)\n80M\nMiscellaneous unsupervised SBERT data\n-\n10M\nSecond stage: supervised fine-tuning\nDataset\nLanguage\n# of text pairs\nMS MARCO\nEnglish\n500k\nNQ\nEnglish\n70k\nTrivia QA\nEnglish\n60k\nNLI from SimCSE\nEnglish\n<300k\nELI5\nEnglish\n500k\nDuReader Retrieval\nChinese\n86k\nKILT Fever\nEnglish\n70k\nKILT HotpotQA\nEnglish\n70k\nSQuAD\nEnglish\n87k\nQuora\nEnglish\n150k\nMr. TyDi\n11 languages\n50k\nMIRACL\n16 languages\n40k\nFor all labeled datasets, we only use its training set for fine-tuning.\nFor other training details, please refer to our paper at https://arxiv.org/pdf/2402.05672.\nBenchmark Results on Mr. TyDi\nModel\nAvg MRR@10\nar\nbn\nen\nfi\nid\nja\nko\nru\nsw\nte\nth\nBM25\n33.3\n36.7\n41.3\n15.1\n28.8\n38.2\n21.7\n28.1\n32.9\n39.6\n42.4\n41.7\nmDPR\n16.7\n26.0\n25.8\n16.2\n11.3\n14.6\n18.1\n21.9\n18.5\n7.3\n10.6\n13.5\nBM25 + mDPR\n41.7\n49.1\n53.5\n28.4\n36.5\n45.5\n35.5\n36.2\n42.7\n40.5\n42.0\n49.2\nmultilingual-e5-small\n64.4\n71.5\n66.3\n54.5\n57.7\n63.2\n55.4\n54.3\n60.8\n65.4\n89.1\n70.1\nmultilingual-e5-base\n65.9\n72.3\n65.0\n58.5\n60.8\n64.9\n56.6\n55.8\n62.7\n69.0\n86.6\n72.7\nmultilingual-e5-large\n70.5\n77.5\n73.2\n60.8\n66.8\n68.5\n62.5\n61.6\n65.8\n72.7\n90.2\n76.2\nMTEB Benchmark Evaluation\nCheck out unilm/e5 to reproduce evaluation results\non the BEIR and MTEB benchmark.\nSupport for Sentence Transformers\nBelow is an example for usage with sentence_transformers.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/multilingual-e5-large')\ninput_texts = [\n'query: how much protein should a female eat',\n'query: ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i     s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: 1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ Ë∞ÉÊñô:Ëë±„ÄÅÁõê„ÄÅÁôΩÁ≥ñ„ÄÅÈ∏°Á≤æ ÂÅöÊ≥ï: 1„ÄÅÂçóÁìúÁî®ÂàÄËñÑËñÑÁöÑÂâäÂéªË°®Èù¢‰∏ÄÂ±ÇÁöÆ     ,Áî®Âã∫Â≠êÂàÆÂéªÁì§ 2„ÄÅÊì¶ÊàêÁªÜ‰∏ù(Ê≤°ÊúâÊì¶ËèúÊùøÂ∞±Áî®ÂàÄÊÖ¢ÊÖ¢ÂàáÊàêÁªÜ‰∏ù) 3„ÄÅÈîÖÁÉßÁÉ≠ÊîæÊ≤π,ÂÖ•Ëë±Ëä±ÁÖ∏Âá∫È¶ôÂë≥ 4„ÄÅÂÖ•ÂçóÁìú‰∏ùÂø´ÈÄüÁøªÁÇí‰∏ÄÂàÜÈíüÂ∑¶Âè≥,     ÊîæÁõê„ÄÅ‰∏ÄÁÇπÁôΩÁ≥ñÂíåÈ∏°Á≤æË∞ÉÂë≥Âá∫ÈîÖ 2.È¶ôËë±ÁÇíÂçóÁìú ÂéüÊñô:ÂçóÁìú1Âè™ Ë∞ÉÊñô:È¶ôËë±„ÄÅËíúÊú´„ÄÅÊ©ÑÊ¶ÑÊ≤π„ÄÅÁõê ÂÅöÊ≥ï: 1„ÄÅÂ∞ÜÂçóÁìúÂéªÁöÆ,ÂàáÊàêÁâá 2„ÄÅÊ≤π     ÈîÖ8ÊàêÁÉ≠Âêé,Â∞ÜËíúÊú´ÊîæÂÖ•ÁàÜÈ¶ô 3„ÄÅÁàÜÈ¶ôÂêé,Â∞ÜÂçóÁìúÁâáÊîæÂÖ•,ÁøªÁÇí 4„ÄÅÂú®ÁøªÁÇíÁöÑÂêåÊó∂,ÂèØ‰ª•‰∏çÊó∂Âú∞ÂæÄÈîÖÈáåÂä†Ê∞¥,‰ΩÜ‰∏çË¶ÅÂ§™Â§ö 5„ÄÅÊîæÂÖ•Áõê,ÁÇíÂåÄ      6„ÄÅÂçóÁìúÂ∑Æ‰∏çÂ§öËΩØÂíåÁªµ‰∫Ü‰πãÂêé,Â∞±ÂèØ‰ª•ÂÖ≥ÁÅ´ 7„ÄÅÊííÂÖ•È¶ôËë±,Âç≥ÂèØÂá∫ÈîÖ\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\nPackage requirements\npip install sentence_transformers~=2.2.2\nContributors: michaelfeil\nFAQ\n1. Do I need to add the prefix \"query: \" and \"passage: \" to input texts?\nYes, this is how the model is trained, otherwise you will see a performance degradation.\nHere are some rules of thumb:\nUse \"query: \" and \"passage: \" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\nUse \"query: \" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval.\nUse \"query: \" prefix if you want to use embeddings as features, such as linear probing classification, clustering.\n2. Why are my reproduced results slightly different from reported in the model card?\nDifferent versions of transformers and pytorch could cause negligible but non-zero performance differences.\n3. Why does the cosine similarity scores distribute around 0.7 to 1.0?\nThis is a known and expected behavior as we use a low temperature 0.01 for InfoNCE contrastive loss.\nFor text embedding tasks like text retrieval or semantic similarity,\nwhat matters is the relative order of the scores instead of the absolute values,\nso this should not be an issue.\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@article{wang2024multilingual,\ntitle={Multilingual E5 Text Embeddings: A Technical Report},\nauthor={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\njournal={arXiv preprint arXiv:2402.05672},\nyear={2024}\n}\nLimitations\nLong texts will be truncated to at most 512 tokens.",
    "stabilityai/stable-video-diffusion-img2vid-xt": "Stable Video Diffusion Image-to-Video Model Card\nModel Details\nModel Description\nModel Sources\nEvaluation\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nRecommendations\nHow to Get Started with the Model\nAppendix:\nStable Video Diffusion Image-to-Video Model Card\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it.\nPlease note: For commercial use, please refer to https://stability.ai/license.\nModel Details\nModel Description\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning.\nThis model was trained to generate 25 frames at resolution 576x1024 given a context frame of the same size, finetuned from SVD Image-to-Video [14 frames].\nWe also finetune the widely used f8-decoder for temporal consistency.\nFor convenience, we additionally provide the model with the\nstandard frame-wise decoder here.\nDeveloped by: Stability AI\nFunded by: Stability AI\nModel type: Generative image-to-video model\nFinetuned from model: SVD Image-to-Video [14 frames]\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models),\nwhich implements the most popular diffusion frameworks (both training and inference).\nRepository: https://github.com/Stability-AI/generative-models\nPaper: https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\nEvaluation\nThe chart above evaluates user preference for SVD-Image-to-Video over GEN-2 and PikaLabs.\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the research paper\nUses\nDirect Use\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under this license. Possible research areas and tasks include\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nFor commercial use, please refer to https://stability.ai/license.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events,\nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's Acceptable Use Policy.\nLimitations and Bias\nLimitations\nThe generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\nThe model may generate videos without motion, or very slow camera pans.\nThe model cannot be controlled through text.\nThe model cannot render legible text.\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nRecommendations\nThe model is intended for both non-commercial and commercial usage.\nHow to Get Started with the Model\nCheck out https://github.com/Stability-AI/generative-models\nAppendix:\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters.\nNo explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation.\nNo other third party was involved in the development of this model; the model was fully developed in-house at Stability AI.\nTraining the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh.\nThe released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos.\nWith the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards.\nThe information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards.\nThe released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.The model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AI‚Äôs future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com.\nFor usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.",
    "google/gemma-2b-it": "A newer version of this model is available:\ngoogle/gemma-2-2b-it\nAccess Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma Model Card\nModel Information\nDescription\nUsage\nChat Template\nFine-tuning\nInputs and outputs\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma Model Card\nModel Page: Gemma\nThis model card corresponds to the 2B instruct version of the Gemma model. You can also visit the model card of the 2B base model, 7B base model, and 7B instruct model.\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First make sure to pip install -U transformers, then copy the snippet from the section that is relevant for your usecase.\nRunning the model on a CPU\nAs explained below, we recommend torch.bfloat16 as the default dtype. You can use a different precision if necessary.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2b-it\",\ntorch_dtype=torch.bfloat16\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2b-it\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nThe native weights of this model were exported in bfloat16 precision. You can use float16, which may be faster on certain hardware, indicating the torch_dtype when loading the model. For convenience, the float16 revision of the repo contains a copy of the weights already converted to that precision.\nYou can also use float32 if you skip the dtype, but no precision increase will occur (model weights will just be upcasted to float32). See examples below.\nUsing torch.float16\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2b-it\",\ndevice_map=\"auto\",\ntorch_dtype=torch.float16,\nrevision=\"float16\",\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUpcasting to torch.float32\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2b-it\",\ndevice_map=\"auto\"\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nOther optimizations\nFlash Attention 2\nFirst make sure to install flash-attn in your environment pip install flash-attn\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\n+   attn_implementation=\"flash_attention_2\"\n).to(0)\nChat Template\nThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.\nLet's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"gg-hf/gemma-2b-it\"\ndtype = torch.bfloat16\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"cuda\",\ntorch_dtype=dtype,\n)\nchat = [\n{ \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\nAt this point, the prompt contains the following text:\n<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>model\nAs you can see, each turn is preceded by a <start_of_turn> delimiter and then the role of the entity\n(either user, for content supplied by the user, or model for LLM responses). Turns finish with\nthe <end_of_turn> token.\nYou can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.\nAfter the prompt is ready, generation can be performed like this:\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\nFine-tuning\nYou can find some fine-tuning scripts under the examples/ directory of google/gemma-7b repository. To adapt them to this model, simply change the model-id to google/gemma-2b-it.\nWe provide:\nA script to perform Supervised Fine-Tuning (SFT) on UltraChat dataset using QLoRA\nA script to perform SFT using FSDP on TPU devices\nA notebook that you can run on a free-tier Google Colab instance to perform SFT on the English quotes dataset\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safely in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5e).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\n2B Params\n7B Params\nMMLU\n5-shot, top-1\n42.3\n64.3\nHellaSwag\n0-shot\n71.4\n81.2\nPIQA\n0-shot\n77.3\n81.2\nSocialIQA\n0-shot\n49.7\n51.8\nBooIQ\n0-shot\n69.4\n83.2\nWinoGrande\npartial score\n65.4\n72.3\nCommonsenseQA\n7-shot\n65.3\n71.3\nOpenBookQA\n47.8\n52.8\nARC-e\n73.2\n81.5\nARC-c\n42.1\n53.2\nTriviaQA\n5-shot\n53.2\n63.4\nNatural Questions\n5-shot\n12.5\n23\nHumanEval\npass@1\n22.0\n32.3\nMBPP\n3-shot\n29.2\n44.4\nGSM8K\nmaj@1\n17.7\n46.4\nMATH\n4-shot\n11.8\n24.3\nAGIEval\n24.2\n41.7\nBIG-Bench\n35.2\n55.1\n------------------------------\n-------------\n-----------\n---------\nAverage\n45.0\n56.9\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nBenchmark\nMetric\n2B Params\n7B Params\nRealToxicity\naverage\n6.86\n7.90\nBOLD\n45.57\n49.08\nCrowS-Pairs\ntop-1\n45.82\n51.33\nBBQ Ambig\n1-shot, top-1\n62.58\n92.54\nBBQ Disambig\ntop-1\n54.62\n71.99\nWinogender\ntop-1\n51.25\n54.17\nTruthfulQA\n44.84\n31.81\nWinobias 1_2\n56.12\n59.09\nWinobias 2_2\n91.10\n92.23\nToxigen\n29.77\n39.59\n------------------------------\n-------------\n-----------\n---------\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives."
}