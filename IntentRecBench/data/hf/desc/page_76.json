{
    "vonjack/whisper-large-v3-gguf": "Whisper\nWhisper\nGGUFs for whisper.cpp",
    "lerobot/act_aloha_sim_transfer_cube_human": "Model Card for ACT/AlohaTransferCube\nHow to Get Started with the Model\nTraining Details\nEvaluation\nModel Card for ACT/AlohaTransferCube\nAction Chunking Transformer Policy (as per Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware) trained for the AlohaTransferCube environment from gym-aloha.\nHow to Get Started with the Model\nSee the LeRobot library (particularly the evaluation script) for instructions on how to load and evaluate this model.\nTraining Details\nTrained with LeRobot@3c0a209.\nThe model was trained using LeRobot's training script and with the aloha_sim_transfer_cube_human dataset, using this command:\npython lerobot/scripts/train.py \\\n--output_dir=outputs/train/act_aloha_transfer \\\n--policy.type=act \\\n--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human \\\n--env.type=aloha \\\n--env.task=AlohaTransferCube-v0 \\\n--wandb.enable=true\nThe training curves may be found at https://wandb.ai/aliberts/lerobot/runs/720l37xb.\nThe current model corresponds to the checkpoint at 80k steps.\nThis took about 1h45 to train on an Nvida A100.\nEvaluation\nThe model was evaluated on the AlohaTransferCube task from gym-aloha and compared to a similar model trained with the original ACT repository. Each episode marks a success if the cube is successfully picked by one robot arm and transferred to the other robot arm.\nHere are the success rate results for 500 episodes worth of evaluation. The \"Theirs\" column is for an equivalent model trained on the original ACT repository and evaluated on LeRobot (the model weights may be found in the original_act_repo branch of this respository). The results of each of the individual rollouts may be found in eval_info.json.\nOurs\nTheirs\nSuccess rate for 500 episodes (%)\n83.0\n68.0\nIt was produced after training with this command:\npython lerobot/scripts/eval.py \\\n--policy.path=outputs/train/act_aloha_transfer/checkpoints/080000/pretrained_model \\\n--output_dir=outputs/eval/act_aloha_transfer/080000 \\\n--env.type=aloha \\\n--env.task=AlohaTransferCube-v0 \\\n--eval.n_episodes=500 \\\n--eval.batch_size=50 \\\n--device=cuda \\\n--use_amp=false\nThe original code was heavily refactored, and some bugs were spotted along the way. The differences in code may account for the difference in success rate. Another possibility is that our simulation environment may use slightly different heuristics to evaluate success (we've observed that success is registered as soon as the second arm's gripper makes antipodal contact with the cube). Finally, one should observe that the in-training evaluation jumps up towards the end of training. This may need further investigation (Is it statistically significant? If so, what is the cause?).",
    "TIGER-Lab/MAmmoTH2-8B-Plus": "ğŸ¦£ MAmmoTH2: Scaling Instructions from the Web\nIntroduction\nTraining Data\nTraining Procedure\nEvaluation\nUsage\nLimitations\nCitation\nğŸ¦£ MAmmoTH2: Scaling Instructions from the Web\nProject Page: https://tiger-ai-lab.github.io/MAmmoTH2/\nPaper: https://arxiv.org/pdf/2405.03548\nCode: https://github.com/TIGER-AI-Lab/MAmmoTH2\nIntroduction\nIntroducing ğŸ¦£ MAmmoTH2, a game-changer in improving the reasoning abilities of large language models (LLMs) through innovative instruction tuning. By efficiently harvesting 10 million instruction-response pairs from the pre-training web corpus, we've developed MAmmoTH2 models that significantly boost performance on reasoning benchmarks. For instance, MAmmoTH2-7B (Mistral) sees its performance soar from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K, all without training on any domain-specific data. Further training on public instruction tuning datasets yields MAmmoTH2-Plus, setting new standards in reasoning and chatbot benchmarks. Our work presents a cost-effective approach to acquiring large-scale, high-quality instruction data, offering a fresh perspective on enhancing LLM reasoning abilities.\nBase Model\nMAmmoTH2\nMAmmoTH2-Plus\n7B\nMistral\nğŸ¦£ MAmmoTH2-7B\nğŸ¦£ MAmmoTH2-7B-Plus\n8B\nLlama-3\nğŸ¦£ MAmmoTH2-8B\nğŸ¦£ MAmmoTH2-8B-Plus\n8x7B\nMixtral\nğŸ¦£ MAmmoTH2-8x7B\nğŸ¦£ MAmmoTH2-8x7B-Plus\nTraining Data\nPlease refer to https://huggingface.co/datasets/TIGER-Lab/WebInstructSub for more details.\nTraining Procedure\nThe models are fine-tuned with the WEBINSTRUCT dataset using the original Llama-3, Mistral and Mistal models as base models. The training procedure varies for different models based on their sizes. Check out our paper for more details.\nEvaluation\nThe models are evaluated using open-ended and multiple-choice math problems from several datasets. Here are the results:\nModel\nTheoremQA\nMATH\nGSM8K\nGPQA\nMMLU-ST\nBBH\nARC-C\nAvg\nMAmmoTH2-7B (Updated)\n29.0\n36.7\n68.4\n32.4\n62.4\n58.6\n81.7\n52.7\nMAmmoTH2-8B (Updated)\n30.3\n35.8\n70.4\n35.2\n64.2\n62.1\n82.2\n54.3\nMAmmoTH2-8x7B\n32.2\n39.0\n75.4\n36.8\n67.4\n71.1\n87.5\n58.9\nMAmmoTH2-7B-Plus (Updated)\n31.2\n46.0\n84.6\n33.8\n63.8\n63.3\n84.4\n58.1\nMAmmoTH2-8B-Plus (Updated)\n31.5\n43.0\n85.2\n35.8\n66.7\n69.7\n84.3\n59.4\nMAmmoTH2-8x7B-Plus\n34.1\n47.0\n86.4\n37.8\n72.4\n74.1\n88.4\n62.9\nTo reproduce our results, please refer to https://github.com/TIGER-AI-Lab/MAmmoTH2/tree/main/math_eval.\nUsage\nYou can use the models through Huggingface's Transformers library. Use the pipeline function to create a text-generation pipeline with the model of your choice, then feed in a math problem to get the solution.\nCheck our Github repo for more advanced use: https://github.com/TIGER-AI-Lab/MAmmoTH2\nLimitations\nWe've tried our best to build math generalist models. However, we acknowledge that the models' performance may vary based on the complexity and specifics of the math problem. Still not all mathematical fields can be covered comprehensively.\nCitation\nIf you use the models, data, or code from this project, please cite the original paper:\n@article{yue2024mammoth2,\ntitle={MAmmoTH2: Scaling Instructions from the Web},\nauthor={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},\njournal={arXiv preprint arXiv:2405.03548},\nyear={2024}\n}",
    "HuggingFaceFW/fineweb-edu-classifier": "FineWeb-Edu classifier\nModel summary\nHow to use in transformers\nTraining\nLimitations\nFineWeb-Edu classifier\nModel summary\nThis is a classifier for judging the educational value of web pages. It was developed to filter and curate educational content from web datasets and was trained on 450k annotations generated by LLama3-70B-instruct for web samples from FineWeb dataset.\nWe used this classifier to build FineWeb-Edu dataset.\nHow to use in transformers\nTo load the FineWeb-Edu classifier, use the following code:\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/fineweb-edu-classifier\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"HuggingFaceTB/fineweb-edu-classifier\")\ntext = \"This is a test sentence.\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=\"longest\", truncation=True)\noutputs = model(**inputs)\nlogits = outputs.logits.squeeze(-1).float().detach().numpy()\nscore = logits.item()\nresult = {\n\"text\": text,\n\"score\": score,\n\"int_score\": int(round(max(0, min(score, 5)))),\n}\nprint(result)\n# {'text': 'This is a test sentence.', 'score': 0.07964489609003067, 'int_score': 0}\nTraining\nThe classifier was trained on 450,000 pairs of web samples and their scores from 0 to 5, generated by Llama3. The samples were annotated based on their educational quality with 0 being not educational and 5 being highly educational.\nBelow is the prompt used for LLama3 annotations:\nWe added a classification head with a single regression output to Snowflake-arctic-embed and trained the model for 20 epochs with a learning rate of 3e-4. During training, the embedding and encoder layers were frozen to focus on the classification head. The model achieved an F1 score of 82% when converted to a binary classifier using a score threshold of 3.\nTraining Details:\nModel: Snowflake-arctic-embed with a classification head\nDataset: 450,000 samples from Llama3 annotations\nEpochs: 20\nLearning Rate: 3e-4\nEvaluation Metric: F1 score\nClassification report\nWe treat the regression model's predictions as discrete classes to calculate the metrics on a hold-out set of 46867 Llama3-annotated samples.\nprecision    recall  f1-score   support\n0       0.75      0.49      0.59      5694\n1       0.78      0.84      0.81     26512\n2       0.57      0.61      0.59     10322\n3       0.56      0.50      0.53      3407\n4       0.58      0.35      0.44       807\n5       0.33      0.01      0.02       125\naccuracy                           0.71     46867\nmacro avg       0.60      0.47      0.50     46867\nweighted avg       0.71      0.71      0.71     46867\nConfusion matrix\nWe verify that the predicted educational scores are indeed close to their ground truth, and are mostry impacted by the noisy annotation.\n2791  2858    45     0     0     0\n919 22343  3180    69     1     0\ny_true     3  3225  6330   757     7     0\n1    66  1473  1694   173     0\n0     4    98   420   283     2\n0     0    18    85    21     1\ny_pred\nLimitations\nWhile the FineWeb-Edu classifier performs well in distinguishing high-quality educational content for FineWeb dataset, there are some limitations:\nScope: The model's performance might change for other datasets, in particular for out of distribution samples. It is also focused on educational content relevant to primary and grade school levels and may not perform as well on content intended for higher education or specialized domains.\nBias: The model's performance is dependent on the quality and representativeness of the training data and the LLM used for the annotation. Biases in both can affect the classifier's judgments. It might overfit to academic looking content for the higher scores and we recommend using int_score >= 3 as a threshold for data curation.\nContext: The classifier evaluates individual web pages or extracts without considering broader context, which might impact its effectiveness in certain scenarios.\nThe training and inference code is available on GitHub\nhttps://github.com/huggingface/cosmopedia/tree/main/classification",
    "Fugaku-LLM/Fugaku-LLM-13B-instruct-gguf": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nFugaku-LLMåˆ©ç”¨è¦ç´„\nã“ã®åˆ©ç”¨è¦ç´„ï¼ˆä»¥ä¸‹ã€Œæœ¬è¦ç´„ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€å¯Œå£«é€šæ ªå¼ä¼šç¤¾ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººç†åŒ–å­¦ç ”ç©¶æ‰€ã€å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å·¥æ¥­å¤§å­¦ã€å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦ã€æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›½ç«‹å¤§å­¦æ³•äººæ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã€åŠã³æ ªå¼ä¼šç¤¾Kotoba Technologies Japan (ä»¥ä¸‹ã€Œé–‹ç™ºè€…ã€ã¨ã„ã„ã¾ã™)ã«ã‚ˆã‚‹ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œå¯Œå²³ã€æ”¿ç­–å¯¾å¿œæ ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«åˆ†æ•£ä¸¦åˆ—å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã®æˆæœç‰©ã¨ã—ã¦å…¬é–‹ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€ŒFugaku-LLMã€ã¨ã„ã„ã¾ã™ï¼‰ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æ¡ä»¶ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã™ã€‚Fugaku-LLMã®åˆ©ç”¨è€…ï¼ˆä»¥ä¸‹ã€Œåˆ©ç”¨è€…ã€ã¨ã„ã„ã¾ã™ï¼‰ã¯ã€æœ¬è¦ç´„ã«åŒæ„ã—ãŸä¸Šã§Fugaku-LLMã‚’åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼‘æ¡ï¼ˆåˆ©ç”¨è¨±è«¾ï¼‰ Fugaku-LLMã®åˆ©ç”¨è€…ã¯ã€æœ¬è¦ç´„ã«å¾“ã„ã€Fugaku-LLMã‚’å•†ç”¨ã¾ãŸã¯éå•†ç”¨ç›®çš„ã‚’å•ã‚ãšåˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãªãŠã€åˆ©ç”¨ã«ã¯ã€Fugaku-LLMã®æ”¹å¤‰ã€è¤‡è£½ãŠã‚ˆã³å†é…å¸ƒãªã‚‰ã³ã«Fugaku-LLMåˆã¯Fugaku-LLMã‚’æ”¹å¤‰ã—ä½œæˆã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã€Œæ”¹å¤‰ç‰©ã€ã¨ã„ã†ï¼‰ã‚’ç”¨ã„ãŸã‚µãƒ¼ãƒ“ã‚¹ã®å®Ÿæ–½ã‚’å«ã‚€ã‚‚ã®ã¨ã—ã¾ã™ã€‚ãŸã ã—ã€åˆ©ç”¨è€…ã¯ã€Fugaku-LLMåˆã¯æ”¹å¤‰ç‰©ã®å†é…å¸ƒæ™‚ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¾ãŸã¯Fugaku-LLMã‚‚ã—ãã¯æ”¹å¤‰ç‰©ã‚’ç”¨ã„ãŸã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨è¦ç´„ã«ã¯æœ¬åˆ©ç”¨è¦ç´„ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€åˆ©ç”¨è€…ãŒæ”¹å¤‰ç‰©ã‚’å†é…å¸ƒã™ã‚‹éš›ã€åˆ©ç”¨è€…ãŒæ”¹å¤‰ã—ãŸã“ã¨ã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æœ¬è¦ç´„ã«é•åã—ãŸFugaku-LLMã®åˆ©ç”¨è€…ã¯ã€Fugaku-LLMã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\nç¬¬ï¼’æ¡ï¼ˆè²¬ä»»ï¼‰\nåˆ©ç”¨è€…ã¯ã€Fugaku-LLMã¯ç¾çŠ¶æœ‰å§¿ã§æä¾›ã•ã‚Œã€é–‹ç™ºè€…ã¯ã€æ˜ç¤ºã¾ãŸã¯é»™ç¤ºã‚’å•ã‚ãšã€Fugaku-LLMã«é–¢ã—ã€ãã®æ­£ç¢ºæ€§ã€å®Œå…¨æ€§ã€æœ€æ–°æ€§ã€ãŠã‚ˆã³å“è³ªãªã©ã€ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚è¡Œã‚ãšã€åˆ©ç”¨è€…ãŒæœ¬Fugaku-LLMã‚’åˆ©ç”¨ã—ãŸã“ã¨ã€åˆ©ç”¨ã§ããªã‹ã£ãŸã“ã¨ã«ã‚ˆã‚Šç”Ÿã˜ãŸä¸€åˆ‡ã®æå®³ã«ã¤ã„ã¦è²¬ä»»ã‚’è² ã‚ãªã„ã“ã¨ã‚’ã€äºˆã‚æ‰¿è«¾ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nåˆ©ç”¨è€…ã¯ã€åˆ©ç”¨è€…ã«ã‚ˆã‚‹Fugaku-LLMã®åˆ©ç”¨ã«ã‚ˆã‚Šã€ã¾ãŸã¯ã€åˆ©ç”¨è€…ãŒæœ¬åˆ©ç”¨è¦ç´„ã«é•åã—ãŸã“ã¨ã«ã‚ˆã‚Šé–‹ç™ºè€…ãŒæå®³ã‚’è¢«ã£ãŸå ´åˆã€å½“è©²æå®³ã‚’è³ å„Ÿã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nåˆ©ç”¨è€…ã¯ã€è‡ªå·±ã®è²¬ä»»ã¨åˆ¤æ–­ã«ãŠã„ã¦åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã¨ã—ã€Fugaku-LLMã®åˆ©ç”¨ã«é–¢ã—ã¦ã€ç¬¬ä¸‰è€…ã¨ã®é–“ã§ç”Ÿã˜ãŸç´›äº‰ã«ã¤ã„ã¦ã€è‡ªã‚‰ã®è²¬ä»»ã¨è² æ‹…ã§å¯¾å¿œã—ã€é–‹ç™ºè€…ã«ä¸€åˆ‡ã®è¿·æƒ‘ã‚’æ›ã‘ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚åˆ©ç”¨è€…ã¯Fugaku-LLMã®åˆ©ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸæå®³ã«ã¤ã„ã¦è‡ªå·±ã®è²¬ä»»ã§å¯¾å‡¦ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼“æ¡ï¼ˆç¦æ­¢è¡Œç‚ºï¼‰ åˆ©ç”¨è€…ã¯ã€Fugaku-LLMã‚’åˆ©ç”¨ã—ã¦ä»¥ä¸‹ã®è¡Œç‚ºã‚’è¡Œã‚ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚\né–‹ç™ºè€…ã‚‚ã—ãã¯ç¬¬ä¸‰è€…ã®çŸ¥çš„è²¡ç”£æ¨©ã‚’ä¾µå®³ã™ã‚‹è¡Œç‚ºã€ã¾ãŸã¯ä¾µå®³ã™ã‚‹ãŠãã‚Œã®ã‚ã‚‹è¡Œç‚º\né–‹ç™ºè€…ã‚‚ã—ãã¯ç¬¬ä¸‰è€…ã®è²¡ç”£ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚‚ã—ãã¯è‚–åƒæ¨©ã‚’ä¾µå®³ã™ã‚‹è¡Œç‚ºã€ã¾ãŸã¯ä¾µå®³ã™ã‚‹ãŠãã‚Œã®ã‚ã‚‹è¡Œç‚º\né–‹ç™ºè€…ã‚‚ã—ãã¯ç¬¬ä¸‰è€…ã‚’å·®åˆ¥ã‚‚ã—ãã¯èª¹è¬—ä¸­å‚·ãƒ»ä¾®è¾±ã—ã€ä»–è€…ã¸ã®å·®åˆ¥ã‚’åŠ©é•·ã—ã€ã¾ãŸã¯åèª‰ã‚‚ã—ãã¯ä¿¡ç”¨ã‚’æ¯€æã™ã‚‹è¡Œç‚º\nè¨±å¯ã•ã‚Œã¦ã„ãªã„æ³•å¾‹æ¥­å‹™ã«å¾“äº‹ã—ãŸã‚Šã€æœ‰è³‡æ ¼ã®å°‚é–€å®¶ä»¥å¤–ã‹ã‚‰ã®æ³•å¾‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ãŸã‚Šã™ã‚‹è¡Œç‚º\næœ‰è³‡æ ¼ã®å°‚é–€å®¶ä»¥å¤–ã‹ã‚‰ã®è²¡å‹™ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã™ã‚‹è¡Œç‚º\nå¥åº·ã¸ã®åŠ©è¨€ã‚„æ²»ç™‚æ–¹æ³•ã®æç¤ºãªã©ã‚’å«ã‚€åŒ»ç™‚è¡Œç‚º\nãã®ä»–æ³•ä»¤ã«åŸºã¥ãè¨±å¯ç­‰ãŒå¿…è¦ãªè¡Œç‚º\nç¬¬ï¼”æ¡ï¼ˆåˆ¶ç´„äº‹é …ï¼‰\nåˆ©ç”¨è€…ã¯ã€Fugaku-LLMã‚’ç”¨ã„ãŸå‡¦ç†ã®çµæœç‰©ï¼ˆä»¥ä¸‹ã€Œå‡¦ç†çµæœã€ã¨ã„ã†ï¼‰ã«ã¯ã€è™šå½ã‚„åã‚Šã€ä»–äººã®æ¨©åˆ©ã‚’ä¾µå®³ã™ã‚‹å†…å®¹ã€ã¾ãŸã¯åˆ©ç”¨è€…ã®æƒ³å®šã™ã‚‹æœ‰åŠ¹æ€§ã‚„æœ‰ç”¨æ€§ã‚’æº€ãŸã•ãªã„å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ã“ã¨ã‚’æ‰¿è«¾ã—ã€ä¸æ­£ç¢ºãƒ»ä¸é©åˆ‡ãªå‡¦ç†çµæœã«ã‚ˆã‚Šã€è‡ªã‚‰åˆã¯ç¬¬ä¸‰è€…ã®æå®³ã‚„æ¨©åˆ©ä¾µå®³ã®ç™ºç”Ÿã€å€«ç†çš„æ‡¸å¿µãŒèµ·ã“ã‚Šå¾—ã‚‹ã¨ã„ã†å‰æã«ç«‹ã¡Fugaku-LLMã‚’åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚åˆ©ç”¨è€…ã¯ã€å‡¦ç†çµæœã®æ­£èª¤ã‚„é©æ³•æ€§ã€å€«ç†çš„å¦¥å½“æ€§ã‚’è‡ªã‚‰ç¢ºèªã®ä¸Šã€åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚åˆ©ç”¨è€…ãŒå‡¦ç†çµæœã‚’å«ã‚Fugaku-LLMã‚’ç”¨ã„ãŸã“ã¨ã«ã‚ˆã‚Šã€åˆ©ç”¨è€…è‡ªèº«åˆã¯ç¬¬ä¸‰è€…ã®æ¨©åˆ©ä¾µå®³ã‚’ç™ºç”Ÿã•ã›ãŸå ´åˆã€é–‹ç™ºè€…ã¯ãã®æå®³ã«å¯¾ã—ã¦ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã‚ãªã„ã‚‚ã®ã¨ã—ã€åˆ©ç”¨è€…ã¯é–‹ç™ºè€…ã«å¯¾ã—ä¸€åˆ‡ã®è¿·æƒ‘ã‚’æ›ã‘ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nåˆ©ç”¨è€…ã¯å‡¦ç†çµæœã«ã¤ã„ã¦ã€ãã‚Œãã‚Œã®å›½ã‚„åœ°åŸŸã«ãŠã„ã¦æ³•ä»¤ãªã©ã®è¦åˆ¶ã‚’é †å®ˆã—ãŸä¸Šã§åˆ©ç”¨ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nåˆ©ç”¨è€…ã¯ã€å‡¦ç†çµæœã‚’ç¬¬ï¼“æ¡ï¼ˆç¦æ­¢äº‹é …ï¼‰ã«è¨˜è¼‰ã®è¡Œç‚ºã«åˆ©ç”¨ã—ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼•æ¡ï¼ˆæ¨©åˆ©å¸°å±ç­‰ï¼‰\nåˆ©ç”¨è€…ã¯ã€æœ¬åˆ©ç”¨è¦ç´„ã§æ˜ç¤ºã§å®šã‚ã‚‹ã‚‚ã®ã‚’é™¤ãFugaku-LLMã«é–¢ã™ã‚‹ä¸€åˆ‡ã®æ¨©åˆ©ã‚’å–å¾—ã™ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\nåˆ©ç”¨è€…ã¯ã€Fugaku-LLMæ”¹å¤‰ç‰©ã®ä½œæˆã«ã‚ˆã£ã¦æ–°ãŸã«ç™ºç”Ÿã—ãŸæ¨©åˆ©ã‚’å–å¾—ã—ã¾ã™ãŒã€æ”¹å¤‰ç‰©ã®åˆ©ç”¨ã«å½“ãŸã£ã¦ã¯æœ¬åˆ©ç”¨è¦ç´„ã«å¾“ã£ã¦åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚\né–‹ç™ºè€…ã¯å‡¦ç†çµæœã«ã¤ã„ã¦ã€æ¨©åˆ©ä¸»å¼µã‚’è¡Œã‚ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼–æ¡ï¼ˆè¼¸å‡ºå–å¼•ï¼‰ åˆ©ç”¨è€…ã¯ã€Fugaku-LLMãŠã‚ˆã³å‡¦ç†çµæœã®åˆ©ç”¨ã«é–¢é€£ã—ã¦å¤–å›½ç‚ºæ›¿åŠã³å¤–å›½è²¿æ˜“æ³•ï¼ˆã“ã‚Œã«é–¢é€£ã™ã‚‹æ”¿çœä»¤ã‚’å«ã‚€ï¼‰ã¾ãŸã¯ç±³å›½è¼¸å‡ºç®¡ç†æ³•ä»¤ã§è¦å®šã™ã‚‹è¨±å¯ãŒå¿…è¦ãªè¼¸å‡ºã‚’è¡Œã†ã¨ãã¯ã€åˆ©ç”¨è€…è‡ªã‚‰ãŒæ‰€å®šã®è¨±å¯ã‚’å–å¾—ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼—æ¡ï¼ˆç®¡è½„è£åˆ¤æ‰€ï¼‰ æœ¬åˆ©ç”¨è¦ç´„ã«é–¢ã—ç”Ÿã˜ãŸç´›äº‰ã«ã¤ã„ã¦ã¯ã€æ±äº¬åœ°æ–¹è£åˆ¤æ‰€ã‚’ã‚‚ã£ã¦ç¬¬ä¸€å¯©ã®å°‚å±çš„åˆæ„ç®¡è½„è£åˆ¤æ‰€ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼˜æ¡ï¼ˆæº–æ‹ æ³•ï¼‰ æœ¬åˆ©ç”¨è¦ç´„ã¯æ—¥æœ¬æ³•ã«æº–æ‹ ã—ã¾ã™ã€‚\nç¬¬ï¼™æ¡ï¼ˆãã®ä»–ã®è¦å®šï¼‰ æœ¬è¦ç´„ã¯ã€Fugaku-LLMã®åˆ©ç”¨è€…ã¨é–‹ç™ºè€…ã¨ã®é–“ã®åˆ©ç”¨ã«é–¢ã™ã‚‹å…¨ã¦ã®äº‹é …ã‚’å®šã‚ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€æœ¬è¦ç´„ã«å®šã‚ã®ãªã„äº‹é …ã«ã¤ã„ã¦ã¯ã€é–¢ä¿‚æ³•ä»¤ã«å¾“ã†ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nç¬¬ï¼‘ï¼æ¡ï¼ˆè¨€èªï¼‰ æœ¬è¦ç´„ã¯æ—¥æœ¬èªã‚’æ­£æœ¬ã¨ã—ã¾ã™ã€‚æœ¬è¦ç´„ã®è‹±è¨³ç‰ˆã¯ã€å‚è€ƒã®ãŸã‚ã«ä½œæˆã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€ä½•ã‚‰ã®æ³•çš„æ‹˜æŸåŠ›ã‚‚ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚\nä»¥ä¸Š\nFugaku-LLM Terms of Use\nThis Terms of Use (hereinafter referred to as \"TOU\") sets forth the conditions for the use of the large-scale language model (hereinafter referred to as \"Fugaku-LLM\") that is made public as a result of the development of a distributed parallel learning method for large-scale language models within the scope of the initiatives for uses of the supercomputer \"Fugaku\" defined by Japanese policy by Fujitsu Limited, RIKEN, Tokyo Institute of Technology, Tohoku University, CyberAgent, Inc., Tokai National Higher Education and Research System, and Kotoba Technologies Japan Co., Ltd. (hereinafter referred to as \"Developers\"). Users of Fugaku-LLM (hereinafter referred to as \"Users\") shall use Fugaku-LLM upon agreeing to the TOU.\nArticle 1 (License to Use)Users of Fugaku-LLM may use Fugaku-LLM for commercial or non-commercial purposes in accordance with the TOU. The word â€œuseâ€ includes, but not limited to, the modification, duplication, and redistribution of Fugaku-LLM, as well as the implementation of services using Fugaku-LLM and/or the large-scale language model created by modifying Fugaku-LLM (hereinafter referred to as \"Modified Works\"); provided that Users must incorporate the TOUC into the license terms for redistribution of Fugaku-LLM or Modified Works, or into the terms of use for services using Fugaku-LLM or Modified Works. In addition, when redistributing Modified Works, Users must clearly state that they have made the modifications. Users who violate the TOU are not allowed to use Fugaku-LLM.\nArticle 2 (Responsibility)\nUsers agree in advance that Fugaku-LLM is provided â€œAS ISâ€, and the Developers make no warranties, express or implied, regarding Fugaku-LLM, including, but not limited to, its accuracy, completeness, up-to-dateness, and quality, and that Developers shall not be liable for any damages arising from the use or inability to use Fugaku-LLM.\nUsers shall compensate for any and all damages suffered by the Developers as a result of the use of Fugaku-LLM and/or the Users' violation of the TOU.\nUsers shall use Fugaku-LLM at their own responsibility and discretion, and shall handle any disputes arising with third parties in relation to the use of Fugaku-LLM at their own responsibility and expense, and shall indemnify, defend and hold harmless the Developers against all damages and losses without causing any inconvenience to the Developers. Users shall deal with any damages caused by the use of Fugaku-LLM at their own responsibility.\nArticle 3 (Prohibited Actions)Users shall not engage in the following actions when using Fugaku-LLM.\nActions that will or may infringe on the intellectual property rights of the Developers or third parties;\nActions that will or may infringe on the property, privacy, or portrait rights of the Developers or third parties;\nActions that discriminate against, defame, insult, or slander the Developers or third parties, promote discrimination against others, or damage the reputation or credibility of others;\nActions that engage in unauthorized legal services and/or provide legal advice from anyone other than a qualified professional;\nActions that provide financial advice from anyone other than a qualified professional;\nMedical actions, including providing health advice or suggesting treatment methods; and\nOther actions that require permissions or other forms of authorization under laws and regulations.\nArticle 4 (Restrictions)\nUsers acknowledge that the results of processing using Fugaku-LLM (hereinafter referred to as \"Processing Results\") may contain falsehoods, biases, content that infringes on the rights of others, or content that does not meet the effectiveness or usefulness expected by Users, and agree to use Fugaku-LLM on the premise that inaccurate or inappropriate Processing Results may cause damage or infringement of rights to Users or third parties and/or ethical concerns. Users shall use the Processing Results after confirming their accuracy, legality, and ethical validity themselves. If the use of Fugaku-LLM, including the Processing Results, by Users cause infringement of the rights of the Users themselves or third parties, the Developers shall not be responsible for any damages, and the Users shall indemnify, defend and hold harmless the Developers against all damages and losses without causing any inconvenience to the Developers.\nUsers shall use the Processing Results in compliance with the regulations such as laws and regulations in each country and region.\nUsers shall not use the Processing Results for the actions listed in Article 3 (Prohibited Actions).\nArticle 5 (Ownership of Rights)\nExcept as expressly provided in the TOU, Users shall not acquire any rights in relation to Fugaku-LLM.\nUsers will acquire rights newly arising from the creation of Modified Works of Fugaku-LLM, but Users shall use Modified Works in accordance with the TOU.\nThe Developers shall not assert any rights to the Processing Results.\nArticle 6 (Export Transaction)Users shall obtain the necessary permissions themselves when exporting Fugaku-LLM and the Processing Results in relation to their use, where such export requires permissions under the Foreign Exchange and Foreign Trade Act (including related cabinet order and ministerial order) or U.S. export control laws and regulations.\nArticle 7 (Jurisdiction)The Tokyo District Court shall have exclusive jurisdiction in the court of the first instance over any disputes arising out of or in connection with the TOU.\nArticle 8 (Governing Law)The TOU is governed by and construed in accordance with the laws of Japan.\nArticle 9 (Other Provisions)The TOU sets forth the entire agreement as to all matters concerning the use of Fugaku-LLM between the Users and the Developers, and matters not provided for in the TOU shall be governed by the relevant laws and regulations.\nArticle 10 (Governing Language)The governing language of the TOU shall be Japanese. The English translation hereof is made for reference purpose only and shall have no effect.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nFugaku-LLM\nFugaku-LLM Model Index\nModel Details\nAbout GGUF Files\nLicense\nRisks and Limitations\nAcknowledgements\nAuthors\nFugaku-LLM\nOur Fugaku LLM model is a domestic model pre-trained from scratch using supercomputer Fugaku.\nThis model is highly transparent and safety because it is trained from scratch with our own data.\nThe training data consists mainly of Japanese data, and the model has excellent Japanese performance.\nThis models are developed by Fugaku-LLM.\nLinks to other models can be found in the index.\nFugaku-LLM Model Index\nModel\nFugaku-LLM\nFugaku-LLM-instruct\n13B\nLink\nLink\nModel Details\nDeveloped by: Fugaku-LLM\nModel type: GPT-2\nLanguage(s): Japanese, English\nLibrary: DeepSpeedFugaku\nTokenizer: llm-jp-tokenizer, code10k_en20k_ja30k of v2.2\nLicense: Fugaku-LLM Terms of Use\nAbout GGUF Files\nGGUF is a LLM format introduced by the llama.cpp team.\nLicense\nFugaku-LLM Terms of Use is available at LICENSE and LICENSE_ja files.\nRisks and Limitations\nThe results of processing using Fugaku-LLM may contain falsehoods, biases, content that infringes on the rights of others, or content that does not meet the effectiveness or usefulness expected by Users.\nAcknowledgements\nThis achievement is based on the Governmentâ€Initiated Projects of Supercomputer Fugaku \"Development of Distributed Training Method for Large Language Models on Fugaku.\"\nAuthors\nTokyo Institute of Technology\nTohoku University\nFujitsu Limited\nRIKEN\nNagoya University\nCyberAgent, Inc.\nKotoba Technologies, Inc.",
    "Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix": "Lumimaid 0.1\nCredits:\nDescription\nTraining data used:\nModels used (only for 8B)\nPrompt template: Llama3\nOthers\nUpdated!\nVersion (v2) files added! With imatrix data generated from the FP16 and conversions directly from the BF16.\nThis is a more disk and compute intensive so lets hope we get GPU inference support for BF16 models in llama.cpp.\nHopefully avoiding any losses in the model conversion, as has been the recently discussed topic on Llama-3 and GGUF lately.\nIf you are able to test them and notice any issues let me know in the discussions.\nRelevant:\nThese quants have been done after the fixes from llama.cpp/pull/6920 have been merged.\nUse KoboldCpp version 1.64 or higher, make sure you're up-to-date.\nI apologize for disrupting your experience.\nMy upload speeds have been cooked and unstable lately.\nIf you want and you are able to...\nYou can support my various endeavors here (Ko-fi).\nGGUF-IQ-Imatrix quants for NeverSleep/Llama-3-Lumimaid-8B-v0.1-OAS.\nAuthor:\n\"This model received the Orthogonal Activation Steering treatment, meaning it will rarely refuse any request.\"\nCompatible SillyTavern presets here (simple) or here (Virt's Roleplay Presets - recommended).\nUse the latest version of KoboldCpp. Use the provided presets for testing.\nFeedback and support for the Authors is always welcome.\nIf there are any issues or questions let me know.\nFor 8GB VRAM GPUs, I recommend the Q4_K_M-imat (4.89 BPW) quant for up to 12288 context sizes.\nOriginal model information:\nLumimaid 0.1\nThis model uses the Llama3 prompting format\nLlama3 trained on our RP datasets, we tried to have a balance between the ERP and the RP, not too horny, but just enough.\nWe also added some non-RP dataset, making the model less dumb overall. It should look like a 40%/60% ratio for Non-RP/RP+ERP data.\nThis model includes the new Luminae dataset from Ikari.\nThis model have received the Orthogonal Activation Steering treatment, meaning it will rarely refuse any request.\nIf you consider trying this model please give us some feedback either on the Community tab on hf or on our Discord Server.\nCredits:\nUndi\nIkariDev\nDescription\nThis repo contains FP16 files of Lumimaid-8B-v0.1-OAS.\nSwitch: 8B - 70B - 70B-alt - 8B-OAS - 70B-OAS\nTraining data used:\nAesir datasets\nNoRobots\nlimarp - 8k ctx\ntoxic-dpo-v0.1-sharegpt\nToxicQAFinal\nLuminae-i1 (70B/70B-alt) (i2 was not existing when the 70b started training) | Luminae-i2 (8B) (this one gave better results on the 8b) - Ikari's Dataset\nSquish42/bluemoon-fandom-1-1-rp-cleaned - 50% (randomly)\nNobodyExistsOnTheInternet/PIPPAsharegptv2test - 5% (randomly)\ncgato/SlimOrcaDedupCleaned - 5% (randomly)\nAiroboros (reduced)\nCapybara (reduced)\nModels used (only for 8B)\nInitial LumiMaid 8B Finetune\nUndi95/Llama-3-Unholy-8B-e4\nUndi95/Llama-3-LewdPlay-8B\nPrompt template: Llama3\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{output}<|eot_id|>\nOthers\nUndi: If you want to support us, you can here.\nIkariDev: Visit my retro/neocities style website please kek",
    "microsoft/Phi-3-small-8k-instruct": "Model Summary\nIntended Uses\nHow to Use\nTokenizer\nChat Format\nSample inference code\nResponsible AI Considerations\nTraining\nModel\nDatasets\nBenchmarks\nSoftware\nHardware\nCross Platform Support\nLicense\nTrademarks\nğŸ‰ Phi-3.5: [mini-instruct]; [MoE-instruct] ; [vision-instruct]\nModel Summary\nThe Phi-3-Small-8K-Instruct is a 7B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Small version in two variants 8K and 128K which is the context length (in tokens) that it can support.\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3-Small-8K-Instruct showcased a robust and state-of-the-art performance among models of the same-size and next-size-up.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 Cookbook\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\nIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require:\nMemory/compute constrained environments\nLatency bound scenarios\nStrong reasoning (especially code, math and logic)\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nHow to Use\nPhi-3-Small-8K-Instruct has been integrated in the development version (4.40.2) of transformers. Until the official version is released through pip, ensure that you are doing one of the following:\nInstall tiktoken (0.6.0) ans triton (2.3.0)\nWhen loading the model, ensure that trust_remote_code=True is passed as an argument of the from_pretrained() function.\nUpdate your local transformers to the development version: pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers. The previous command is an alternative to cloning and installing from the source.\nThe current transformers version can be verified with: pip list | grep transformers.\nPhi-3-Small-8K-Instruct is also available in Azure AI.\nTokenizer\nPhi-3-Small-8K-Instruct supports a vocabulary size of up to 100352 tokens.\nChat Format\nGiven the nature of the training data, the Phi-3-Small-8K-Instruct model is best suited for prompts using the chat format as follows.\nYou can provide the prompt as a question with a generic template as follow:\n<|endoftext|><|user|>\\nQuestion <|end|>\\n<|assistant|>\nFor example:\n<|endoftext|><|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\nwhere the model generates the text after <|assistant|> . In case of few-shots prompt, the prompt can be formatted as the following:\n<|endoftext|><|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\nSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\ntorch.random.manual_seed(0)\nmodel_id = \"microsoft/Phi-3-small-8k-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\nassert torch.cuda.is_available(), \"This model needs a GPU to run ...\"\ndevice = torch.cuda.current_device()\nmodel = model.to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmessages = [\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\ndevice=device\n)\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\nSome applications/frameworks might not include a BOS token (<|endoftext|>) at the start of the conversation. Please ensure that it is included since it provides more reliable results.\nResponsible AI Considerations\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nTraining\nModel\nArchitecture: Phi-3 Small-8K-Instruct has 7B parameters and is a dense decoder-only Transformer model with alternating dense and blocksparse attentions. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\nInputs: Text. It is best suited for prompts using chat format.\nContext length: 8K tokens\nGPUs: 1024 H100-80G\nTraining time: 18 days\nTraining data: 4.8T tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\nRelease dates\tThe model weight is released on May 21, 2024.\nDatasets\nOur training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of\nPublicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\nNewly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\nHigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the Phi-3 Technical Report.\nBenchmarks\nWe report the results for Phi-3-Small-8K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mixtral-8x7b, Gemini-Pro, Gemma 7B, Llama-3-8B-Instruct, GPT-3.5-Turbo-1106, and GPT-4-Turbo-1106.\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0.\nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\nThe number of kâ€“shot examples is listed per-benchmark.\nBenchmark\nPhi-3-Small-8K-Instruct7b\nGemma7B\nMixtral8x7B\nLlama-3-Instruct8b\nGPT-3.5-Turboversion 1106\nGeminiPro\nGPT-4-Turboversion 1106 (Chat)\nAGI Eval5-shot\n45.1\n42.1\n45.2\n42.0\n48.4\n49.0\n59.6\nMMLU5-shot\n75.7\n63.6\n70.5\n66.5\n71.4\n66.7\n84.0\nBigBench Hard3-shot\n79.1\n59.6\n69.7\n51.5\n68.3\n75.6\n87.7\nANLI7-shot\n58.1\n48.7\n55.2\n57.3\n58.1\n64.2\n71.7\nHellaSwag5-shot\n77.0\n49.8\n70.4\n71.1\n78.8\n76.2\n88.3\nARC Challenge10-shot\n90.7\n78.3\n87.3\n82.8\n87.4\n88.3\n95.6\nARC Easy10-shot\n97.0\n91.4\n95.6\n93.4\n96.3\n96.1\n98.8\nBoolQ2-shot\n84.8\n66.0\n76.6\n80.9\n79.1\n86.4\n91.3\nCommonsenseQA10-shot\n80.0\n76.2\n78.1\n79.0\n79.6\n81.8\n86.7\nMedQA2-shot\n65.4\n49.6\n62.2\n60.5\n63.4\n58.2\n83.7\nOpenBookQA10-shot\n88.0\n78.6\n85.8\n82.6\n86.0\n86.4\n93.4\nPIQA5-shot\n86.9\n78.1\n86.0\n75.7\n86.6\n86.2\n90.1\nSocial IQA5-shot\n79.2\n65.5\n75.9\n73.9\n68.3\n75.4\n81.7\nTruthfulQA (MC2)10-shot\n70.2\n52.1\n60.1\n63.2\n67.7\n72.6\n85.2\nWinoGrande5-shot\n81.5\n55.6\n62.0\n65.0\n68.8\n72.2\n86.7\nTriviaQA5-shot\n58.1\n72.3\n82.2\n67.7\n85.8\n80.2\n73.3\nGSM8K Chain of Thought8-shot\n89.6\n59.8\n64.7\n77.4\n78.1\n80.4\n94.2\nHumanEval0-shot\n61.0\n34.1\n37.8\n60.4\n62.2\n64.4\n79.9\nMBPP3-shot\n71.7\n51.5\n60.2\n67.7\n77.8\n73.2\n86.7\nAverage\n75.7\n61.8\n69.8\n69.4\n74.3\n75.4\n85.2\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\nBenchmark\nPhi-3-Small-8K-Instruct7b\nGemma7B\nMixtral8x7B\nLlama-3-Instruct8b\nGPT-3.5-Turboversion 1106\nGeminiPro\nGPT-4-Turboversion 1106 (Chat)\nPopular aggregated benchmark\n71.1\n59.4\n66.2\n59.9\n67.0\n67.5\n80.5\nReasoning\n82.4\n69.1\n77.0\n75.7\n78.3\n80.4\n89.3\nLanguage understanding\n70.6\n58.4\n64.9\n65.4\n70.4\n75.3\n81.6\nCode generation\n60.7\n45.6\n52.7\n56.4\n70.4\n66.7\n76.1\nMath\n51.6\n35.8\n40.3\n41.1\n52.8\n50.9\n67.1\nFactual knowledge\n38.6\n46.7\n58.6\n43.1\n63.4\n54.6\n45.9\nMultilingual\n62.5\n63.2\n63.4\n65.0\n69.1\n76.5\n82.0\nRobustness\n72.9\n38.4\n51.0\n64.5\n69.3\n69.7\n84.6\nSoftware\nPyTorch\nDeepSpeed\nTransformers\nFlash-Attention\nTiktoken\nTriton\nHardware\nNote that by default, the Phi-3-Small model uses flash attention 2 and Triton blocksparse attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nIf you want to run the model on:\nOptimized inference on GPU, CPU, and Mobile: use the ONNX models 8K\nCross Platform Support\nONNX runtime ecosystem now supports Phi3 small models  across platforms and hardware.\nOptimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA).Along with DML, ONNX Runtime provides cross platform support for Phi3 Small  across a range of devices CPU, GPU, and mobile.\nHere are some of the optimized configurations we have added:\nONNX models for int4 DML: Quantized to int4 via AWQ\nONNX model for fp16 CUDA\nONNX model for int4 CUDA: Quantized to int4 via RTN\nONNX model for int4 CPU and Mobile: Quantized to int4 via RTN\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followâ€¯Microsoftâ€™s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.",
    "fdemelo/t5-base-spell-correction-fr": "T5 Spelling Correction (French)\nUsage\nT5 Spelling Correction (French)\nThis model corrects spelling and punctuation problems of the input text (in French).\nIt was trained on a synthetic dataset\nbased on a French news corpus (2023) provided by the University of Leipzig\nusing Happy Transformer.\nThe base model used for training is airKlizz/t5-base-multi-fr-wiki-news.\nThe following article was used as reference full article.\nUsage\npip install happytransformer\nfrom happytransformer import HappyTextToText, TTSettings\nhappy_tt = HappyTextToText(\"T5\", \"fdemelo/t5-base-spell-correction-fr\")\nargs = TTSettings(num_beams=5, min_length=1)\n# Add the prefix \"grammaire: \" before each input\nresult = happy_tt.generate_text(\"grammaire: Le vehicule a tombe encontrebas\", args=args)\nprint(result.text)  # corrected sentence",
    "RichardErkhov/akdeniz27_-_roberta-base-cuad-4bits": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nRoBERTa Base Model fine-tuned with CUAD dataset\nQuantization made by Richard Erkhov.\nGithub\nDiscord\nRequest more models\nroberta-base-cuad - bnb 4bits\nModel creator: https://huggingface.co/akdeniz27/\nOriginal model: https://huggingface.co/akdeniz27/roberta-base-cuad/\nOriginal model description:\nlanguage: en\ndatasets:\n- cuad\nRoBERTa Base Model fine-tuned with CUAD dataset\nThis model is the fine-tuned version of \"RoBERTa Base\"\nusing CUAD dataset https://huggingface.co/datasets/cuad\nLink for model checkpoint: https://github.com/TheAtticusProject/cuad\nFor the use of the model with CUAD: https://github.com/marshmellow77/cuad-demo\nand https://huggingface.co/spaces/akdeniz27/contract-understanding-atticus-dataset-demo",
    "AutonLab/MOMENT-1-large": "MOMENT-Large\nTutorials\nUsage\nModel Details\nModel Description\nModel Sources\nEnvironmental Impact\nCitation\nMOMENT-Large\nMOMENT is a family of foundation models for general-purpose time-series analysis. The models in this family (1) serve as a building block for diverse time-series analysis tasks (e.g., forecasting, classification, anomaly detection, and imputation, etc.), (2) are effective out-of-the-box, i.e., with no (or few) task-specific exemplars (enabling e.g., zero-shot forecasting, few-shot classification, etc.), and (3) are tunable using in-distribution and task-specific data to improve performance.\nFor details on MOMENT models, training data, and experimental results, please refer to the paper MOMENT: A Family of Open Time-series Foundation Models.\nMOMENT-1 comes in 3 sizes: Small, Base, and Large.\nUsage\nRecommended Python Version: Python 3.11 (support for additional versions is expected soon).\nYou can install the momentfm package using pip:\npip install momentfm\nAlternatively, to install the latest version directly from the GitHub repository:\npip install git+https://github.com/moment-timeseries-foundation-model/moment.git\nTo load the pre-trained model for one of the tasks, use one of the following code snippets:\nForecasting\nfrom momentfm import MOMENTPipeline\nmodel = MOMENTPipeline.from_pretrained(\n\"AutonLab/MOMENT-1-large\",\nmodel_kwargs={\n'task_name': 'forecasting',\n'forecast_horizon': 96\n},\n)\nmodel.init()\nClassification\nfrom momentfm import MOMENTPipeline\nmodel = MOMENTPipeline.from_pretrained(\n\"AutonLab/MOMENT-1-large\",\nmodel_kwargs={\n'task_name': 'classification',\n'n_channels': 1,\n'num_class': 2\n},\n)\nmodel.init()\nAnomaly Detection, Imputation, and Pre-training\nfrom momentfm import MOMENTPipeline\nmodel = MOMENTPipeline.from_pretrained(\n\"AutonLab/MOMENT-1-large\",\nmodel_kwargs={\"task_name\": \"reconstruction\"},\n)\nmode.init()\nRepresentation Learning\nfrom momentfm import MOMENTPipeline\nmodel = MOMENTPipeline.from_pretrained(\n\"AutonLab/MOMENT-1-large\",\nmodel_kwargs={'task_name': 'embedding'},\n)\nTutorials\nHere is the list of tutorials and reproducibile experiments to get started with MOMENT for various tasks:\nForecasting\nClassification\nAnomaly Detection\nImputation\nRepresentation Learning\nReal-world Electrocardiogram (ECG) Case Study -- This tutorial also shows how to fine-tune MOMENT for a real-world ECG classification problem, performing training and inference on multiple GPUs and parameter efficient fine-tuning (PEFT).\nModel Details\nModel Description\nDeveloped by: Auton Lab, Carnegie Mellon University\nModel type: Time-series Foundation Model\nLicense: MIT License\nModel Sources\nRepository: https://github.com/moment-timeseries-foundation-model/ (Pre-training and research code coming out soon!)\nPaper: https://arxiv.org/abs/2402.03885\nDemo: https://github.com/moment-timeseries-foundation-model/moment/tree/main/tutorials\nEnvironmental Impact\nWe train multiple models over many days resulting in significant energy usage and a sizeable carbon footprint. However, we hope that releasing our models will ensure that future time-series modeling efforts are quicker and more efficient, resulting in lower carbon emissions.\nWe use the Total Graphics Power (TGP) to calculate the total power consumed for training MOMENT models, although the total power consumed by the GPU will likely vary a little based on the GPU utilization while training our model. Our calculations do not account for power demands from other sources of our compute. We use 336.566 Kg C02/MWH as the standard value of CO2 emission per megawatt hour of energy consumed for Pittsburgh.\nHardware Type: NVIDIA RTX A6000 GPU\nGPU Hours: 404\nCompute Region: Pittsburgh, USA\nCarbon Emission (tCO2eq):\nHardware\nAll models were trained and evaluated on a computing cluster consisting of 128 AMD EPYC 7502 CPUs, 503 GB of RAM, and 8 NVIDIA RTX A6000 GPUs each with 49 GiB RAM. All MOMENT variants were trained on a single A6000 GPU (with any data or model parallelism).\nCitation\nBibTeX:\nIf you use MOMENT please cite our paper:\n@inproceedings{goswami2024moment,\ntitle={MOMENT: A Family of Open Time-series Foundation Models},\nauthor={Mononito Goswami and Konrad Szafer and Arjun Choudhry and Yifu Cai and Shuo Li and Artur Dubrawski},\nbooktitle={International Conference on Machine Learning},\nyear={2024}\n}\nAPA:\nGoswami, M., Szafer, K., Choudhry, A., Cai, Y., Li, S., & Dubrawski, A. (2024).\nMOMENT: A Family of Open Time-series Foundation Models. In International Conference on Machine Learning. PMLR.",
    "MaziyarPanahi/calme-2.2-phi3-4b": "MaziyarPanahi/calme-2.2-phi3-4b\nâš¡ Quantized GGUF\nğŸ† Open LLM Leaderboard Evaluation Results\nPrompt Template\nHow to use\nMaziyarPanahi/calme-2.2-phi3-4b\nThis model is a fine-tune (DPO) of microsoft/Phi-3-mini-4k-instruct model.\nâš¡ Quantized GGUF\nAll GGUF models are available here: MaziyarPanahi/calme-2.2-phi3-4b-GGUF\nğŸ† Open LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n23.21\nIFEval (0-Shot)\n50.69\nBBH (3-Shot)\n37.73\nMATH Lvl 5 (4-Shot)\n2.34\nGPQA (0-shot)\n9.51\nMuSR (0-shot)\n7.70\nMMLU-PRO (5-shot)\n31.27\nMetric\nValue\nAvg.\n69.78\nAI2 Reasoning Challenge (25-Shot)\n62.80\nHellaSwag (10-Shot)\n80.76\nMMLU (5-Shot)\n69.10\nTruthfulQA (0-shot)\n59.97\nWinogrande (5-shot)\n72.45\nGSM8k (5-shot)\n73.62\nPrompt Template\nThis model uses ChatML prompt template:\n<|im_start|>system\n{System}\n<|im_end|>\n<|im_start|>user\n{User}\n<|im_end|>\n<|im_start|>assistant\n{Assistant}\nHow to use\nYou can use this model by using MaziyarPanahi/calme-2.2-phi3-4b as the model name in Hugging Face's\ntransformers library.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\nfrom transformers import pipeline\nimport torch\nmodel_id = \"MaziyarPanahi/calme-2.2-phi3-4b\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True,\n# attn_implementation=\"flash_attention_2\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\nmodel_id,\ntrust_remote_code=True\n)\nstreamer = TextStreamer(tokenizer)\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# this should work perfectly for the model to stop generating\nterminators = [\ntokenizer.eos_token_id, # this should be <|im_end|>\ntokenizer.convert_tokens_to_ids(\"<|assistant|>\"), # sometimes model stops generating at <|assistant|>\ntokenizer.convert_tokens_to_ids(\"<|end|>\") # sometimes model stops generating at <|end|>\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n\"streamer\": streamer,\n\"eos_token_id\": terminators,\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])",
    "interneuronai/real_estate_listing_analysis_bart": "How to Use\nReal Estate Listing Analysis\nDescription: Perform various tasks to analyze real estate listings, including categorizing them by type, determining if they are for new buildings or business centers, identifying amenities, classifying listings based on location, and analyzing pricing trends.\nHow to Use\nHere is how to use this model to classify text into different categories:\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel_name = \"interneuronai/real_estate_listing_analysis_bart\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndef classify_text(text):\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\noutputs = model(**inputs)\npredictions = outputs.logits.argmax(-1)\nreturn predictions.item()\ntext = \"Your text here\"\nprint(\"Category:\", classify_text(text))",
    "xinsir/controlnet-canny-sdxl-1.0": "Drawing like Midjourney! Come on!\nControlnet-Canny-Sdxl-1.0\nModel Details\nModel Description\nModel Sources [optional]\nUses\nExamples\nExamples Anime(Note that you need to change the base model to CounterfeitXL, others remains the same)\nHow to Get Started with the Model\nEvaluation Metric\nEvaluation Data\nQuantitative Result\nTraining Details\nTraining Data\nConclusion\nDrawing like Midjourney! Come on!\nControlnet-Canny-Sdxl-1.0\nHello, I am very happy to announce the controlnet-canny-sdxl-1.0 model, a very powerful controlnet that can generate high resolution images visually comparable with midjourney.\nThe model was trained with large amount of high quality data(over 10000000 images), with carefully filtered and captioned(powerful vllm model). Besides, useful tricks are applied\nduring the training, including date augmentation, mutiple loss and multi resolution. With only 1 stage training, the performance outperforms the other opensource canny models\n([diffusers/controlnet-canny-sdxl-1.0], [TheMistoAI/MistoLine]). I release it and hope to advance the application of stable diffusion models. Canny is one of the most important\nControlNet series models and can be applied to many jobs associated with drawing and designing.\nModel Details\nModel Description\nDeveloped by: xinsir\nModel type: ControlNet_SDXL\nLicense: apache-2.0\nFinetuned from model [optional]: stabilityai/stable-diffusion-xl-base-1.0\nModel Sources [optional]\nPaper [optional]: https://arxiv.org/abs/2302.05543\nUses\nExamples\nprompt: A closeup of two day of the dead models, looking to the side, large flowered headdress, full dia de Los muertoe make up, lush red lips, butterflies,\nflowers, pastel colors, looking to the side, jungle, birds, color harmony , extremely detailed, intricate, ornate, motion, stunning, beautiful, unique, soft lighting\nprompt: ghost with a plague doctor mask in a venice carnaval hyper realistic\nprompt: A picture surrounded by blue stars and gold stars, glowing, dark navy blue and gray tones, distributed in light silver and gold, playful, festive atmosphere, pure fabric, chalk, FHD 8K\nprompt: Delicious vegetarian pizza with champignon mushrooms, tomatoes, mozzarella, peppers and black olives, isolated on white background , transparent isolated white background , top down view, studio photo, transparent png, Clean sharp focus. High  end retouching. Food magazine photography. Award winning photography. Advertising photography. Commercial photography\nprompt: a blonde woman in a wedding dress in a maple forest in summer with a flower crown laurel. Watercolor painting in the style of John William Waterhouse. Romanticism. Ethereal light.\nExamples Anime(Note that you need to change the base model to CounterfeitXL, others remains the same)\nHow to Get Started with the Model\nUse the code below to get started with the model.\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\nfrom diffusers import DDIMScheduler, EulerAncestralDiscreteScheduler\nfrom PIL import Image\nimport torch\nimport numpy as np\nimport cv2\ndef HWC3(x):\nassert x.dtype == np.uint8\nif x.ndim == 2:\nx = x[:, :, None]\nassert x.ndim == 3\nH, W, C = x.shape\nassert C == 1 or C == 3 or C == 4\nif C == 3:\nreturn x\nif C == 1:\nreturn np.concatenate([x, x, x], axis=2)\nif C == 4:\ncolor = x[:, :, 0:3].astype(np.float32)\nalpha = x[:, :, 3:4].astype(np.float32) / 255.0\ny = color * alpha + 255.0 * (1.0 - alpha)\ny = y.clip(0, 255).astype(np.uint8)\nreturn y\ncontrolnet_conditioning_scale = 1.0\nprompt = \"your prompt, the longer the better, you can describe it as detail as possible\"\nnegative_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\neulera_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\ncontrolnet = ControlNetModel.from_pretrained(\n\"xinsir/controlnet-canny-sdxl-1.0\",\ntorch_dtype=torch.float16\n)\n# when test with other base model, you need to change the vae also.\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\",\ncontrolnet=controlnet,\nvae=vae,\nsafety_checker=None,\ntorch_dtype=torch.float16,\nscheduler=eulera_scheduler,\n)\n# need to resize the image resolution to 1024 * 1024 or same bucket resolution to get the best performance\ncontrolnet_img = cv2.imread(\"your image path\")\nheight, width, _  = controlnet_img.shape\nratio = np.sqrt(1024. * 1024. / (width * height))\nnew_width, new_height = int(width * ratio), int(height * ratio)\ncontrolnet_img = cv2.resize(controlnet_img, (new_width, new_height))\ncontrolnet_img = cv2.Canny(controlnet_img, 100, 200)\ncontrolnet_img = HWC3(controlnet_img)\ncontrolnet_img = Image.fromarray(controlnet_img)\nimages = pipe(\nprompt,\nnegative_prompt=negative_prompt,\nimage=controlnet_img,\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\nwidth=new_width,\nheight=new_height,\nnum_inference_steps=30,\n).images\nimages[0].save(f\"your image save path, png format is usually better than jpg or webp in terms of image quality but got much bigger\")\nEvaluation Metric\n1 Laion Aesthetic Score [https://laion.ai/blog/laion-aesthetics/]2 PerceptualSimilarity [https://github.com/richzhang/PerceptualSimilarity]\nEvaluation Data\nThe test data is randomly sample from midjourney upscale images with prompts, as the purpose of the project is to letting people draw images like midjourney. midjourneyâ€™s users include a large number of professional designers,\nand the upscale image tend to have more beauty score and prompt consistency, it is suitable to use it as the test set to judge the ability of controlnet. We select 300 prompt-image pairs randomly and generate 4 images per prompt,\ntotally 1200 images generated. We caculate the Laion Aesthetic Score to measure the beauty and the PerceptualSimilarity to measure the control ability, we find the quality of images have a good consistency with the meric values.\nWe compare our methods with other SOTA huggingface models and list the result below. We are the models that have highest aesthectic score, and can generate visually appealing images if you prompt it properly.\nQuantitative Result\nmetric\nxinsir/controlnet-canny-sdxl-1.0\ndiffusers/controlnet-canny-sdxl-1.0\nTheMistoAI/MistoLine\nlaion_aesthetic\n6.03\n5.93\n5.82\nperceptual similarity\n0.4200\n0.5053\n0.5387\nlaion_aesthetic(the higher the better)perceptual similarity(the lower the better)\nNote: The values are calculate when saved in webp format, if you save in png format the aesthetic values will increase 0.1-0.3 but the relative relation remains unchanged.\nTraining Details\nThe model is trained using high quality data, only 1 stage training, the resolution setting is the same with sdxl-base, 1024*1024. We use random threshold to generate canny images like lvming zhang, It is essential to find proper hyerparameters\nto realize data augmentation, too easy or too hard will hurt the model performance. Besides, we use random mask to random mask out a random percentage of canny images to force the model to learn more semantic meaning between the prompt and the line.\nWe use over 10000000 images, which are annotated carefully, cogvlm is proved to be a powerful image caption model[https://github.com/THUDM/CogVLM?tab=readme-ov-file]. For comic images, it is recommened to use waifu tagger to generate special tags\n[https://huggingface.co/spaces/SmilingWolf/wd-tagger]. More than 64 A100s are used to train the model and the real batch size is 2560 when used accumulate_grad_batches.\nTraining Data\nThe data consists of many sources, including midjourney, laion 5B, danbooru, and so on. The data is carefully filtered and annotated.\nConclusion\nIn our evaluation, the model got better aesthetic score in real images compared with stabilityai/stable-diffusion-xl-base-1.0,  and comparable performance in cartoon sytle images.\nThe model is better in control ability when test with perception similarity due to more strong data augmentation and more training steps.\nBesides, the model has lower rate to generate abnormal images which tend to include some abnormal human structure.",
    "bunnycore/Maverick-8B": "Maverick-8B\nğŸ§© Configuration\nMaverick-8B\nMaverick-8B is a merge of the following models using mergekit:\nğŸ§© Configuration\nmodels:\n- model: bunnycore/Cognitron-8B\n- model: failspy/Llama-3-8B-Instruct-abliterated\n- model: TIGER-Lab/MAmmoTH2-8B-Plus\nmerge_method: model_stock\nbase_model: bunnycore/Cognitron-8B\ndtype: bfloat16",
    "lelapa/InkubaLM-0.4B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nInkubaLM-0.4B: Small language model for low-resource African Languages\nModel Details\nModel Description\nModel Sources\nHow to Get Started with the Model\nRunning the model on CPU/GPU/multi GPU\n- Running the model on CPU\n- Using full precision\n- Using torch.bfloat16\n- Using quantized Versions via bitsandbytes\nTraining Details\nTraining Data\nLimitations\nEthical Considerations and Risks\nCitation\nModel Card Authors\nModel Card Contact\nInkubaLM-0.4B: Small language model for low-resource African Languages\nModel Details\nInkubaLM has been trained from scratch using 1.9 billion tokens of data for five African languages, along with English and French data, totaling 2.4 billion tokens of data.\nSimilar to the model architecture used forÂ MobileLLM, we trained this InkubaLM with a parameter size of 0.4 billion and a vocabulary size of 61788.\nFor detailed information on training, benchmarks, and performance, please refer to our full blog post.\nModel Description\nDeveloped by: Lelapa AI - Fundamental Research Team.\nModel type: Small Language Model (SLM) for five African languages built using the architecture design of LLaMA-7B.\nLanguage(s) (NLP): isiZulu, Yoruba, Swahili, isiXhosa, Hausa, English and French.\nLicense: CC BY-NC 4.0.\nModel Sources\nRepository: TBD\nPaper : InkubaLM\nHow to Get Started with the Model\nUse the code below to get started with the model.\npip install transformers\nRunning the model on CPU/GPU/multi GPU\n- Running the model on CPU\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"lelapa/InkubaLM-0.4B\",trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"lelapa/InkubaLM-0.4B\",trust_remote_code=True)\ntext = \"Today I planned to\"\ninputs = tokenizer(text, return_tensors=\"pt\")\ninput_ids = inputs.input_ids\n# Create an attention mask\nattention_mask = inputs.attention_mask\n# Generate outputs using the attention mask\noutputs = model.generate(input_ids, attention_mask=attention_mask, max_length=60,pad_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n- Using full precision\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"lelapa/InkubaLM-0.4B\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"lelapa/InkubaLM-0.4B\", trust_remote_code=True)\nmodel.to('cuda')\ntext = \"Today i planned to  \"\ninput_ids = tokenizer(text, return_tensors=\"pt\").to('cuda').input_ids\noutputs = model.generate(input_ids, max_length=1000, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\nprint(tokenizer.batch_decode(outputs[:, input_ids.shape[1]:-1])[0].strip())\n- Using torch.bfloat16\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ncheckpoint = \"lelapa/InkubaLM-0.4B\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True)\ninputs = tokenizer.encode(\"Today i planned to \", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n- Using quantized Versions via bitsandbytes\npip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True) # to use 4bit use `load_in_4bit=True` instead\ncheckpoint = \"lelapa/InkubaLM-0.4B\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config, trust_remote_code=True)\ninputs = tokenizer.encode(\"Today i planned to \", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nTraining Details\nTraining Data\nFor training, we used the Inkuba-mono dataset.\nTraining Hyperparameters\nHyperparameter\nValue\nTotal Parameters\n0.422B\nHidden Size\n2048\nIntermediate Size (MLPs)\n5632\nNumber of Attention Heads\n32\nNumber of Hidden Layers\n8\nRMSNorm É›\n1e^-5\nMax Seq Length\n2048\nVocab Size\n61788\nLimitations\nThe InkubaLM model has been trained on multilingual datasets but does have some limitations. It is capable of understanding and generating content in five African languages: Swahili, Yoruba, Hausa, isiZulu, and isiXhosa, as well as English and French. While it can generate text on various topics, the resulting content may not always be entirely accurate, logically consistent, or free from biases found in the training data. Additionally, the model may sometimes use different languages when generating text. Nonetheless, this model is intended to be a foundational tool to aid research in African languages.\nEthical Considerations and Risks\nInkubaLM is a small LM developed for five African languages. The model is evaluated only in sentiment analysis, machine translation, AfriMMLU, and AfriXNLI tasks and has yet to cover all possible evaluation scenarios. Similar to other language models, it is impossible to predict all of InkubaLM's potential outputs in advance, and in some cases, the model may produce inaccurate, biased, or objectionable responses. Therefore, before using the model in any application, the users should conduct safety testing and tuning tailored to their intended use.\nCitation\n@article{tonja2024inkubalm,\ntitle={InkubaLM: A small language model for low-resource African languages},\nauthor={Tonja, Atnafu Lambebo and Dossou, Bonaventure FP and Ojo, Jessica and Rajab, Jenalea and Thior, Fadel and Wairagala, Eric Peter and Anuoluwapo, Aremu and Moiloa, Pelonomi and Abbott, Jade and Marivate, Vukosi and others},\njournal={arXiv preprint arXiv:2408.17024},\nyear={2024}\n}\nModel Card Authors\nLelapa AI - Fundamental Research Team\nModel Card Contact\nLelapa AI",
    "Ransss/Pantheon-RP-1.0-8b-Llama-3-Q8_0-GGUF": "Ransss/Pantheon-RP-1.0-8b-Llama-3-Q8_0-GGUF\nUse with llama.cpp\nRansss/Pantheon-RP-1.0-8b-Llama-3-Q8_0-GGUF\nThis model was converted to GGUF format from Gryphe/Pantheon-RP-1.0-8b-Llama-3 using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew.\nbrew install ggerganov/ggerganov/llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo Ransss/Pantheon-RP-1.0-8b-Llama-3-Q8_0-GGUF --model pantheon-rp-1.0-8b-llama-3.Q8_0.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo Ransss/Pantheon-RP-1.0-8b-Llama-3-Q8_0-GGUF --model pantheon-rp-1.0-8b-llama-3.Q8_0.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\ngit clone https://github.com/ggerganov/llama.cpp &&             cd llama.cpp &&             make &&             ./main -m pantheon-rp-1.0-8b-llama-3.Q8_0.gguf -n 128",
    "xinsir/controlnet-scribble-sdxl-1.0": "This is an anyline model that can generate images comparable with midjourney and support any line type and any width!\nGeneral Scribble model that can generate images comparable with midjourney!\nControlnet-Scribble-Sdxl-1.0\nModel Details\nModel Description\nModel Sources [optional]\nExamples[Note the following examples are all generate using stabilityai/stable-diffusion-xl-base-1.0 and xinsir/controlnet-scribble-sdxl-1.0]\nHow to Get Started with the Model\nEvaluation Data\nQuantitative Result\nConclusion\nThis is an anyline model that can generate images comparable with midjourney and support any line type and any width!\nThe following five lines are using different control lines, from top to below, Scribble, Canny, HED, PIDI, Lineart\nGeneral Scribble model that can generate images comparable with midjourney!\nControlnet-Scribble-Sdxl-1.0\nHello, I am very happy to announce the controlnet-scribble-sdxl-1.0 model, a very powerful controlnet that can generate high resolution images visually comparable with midjourney.\nThe model was trained with large amount of high quality data(over 10000000 images), with carefully filtered and captioned(powerful vllm model). Besides, useful tricks are applied\nduring the training, including date augmentation, mutiple loss and multi resolution. Note that this model can achieve higher aesthetic performance than our Controlnet-Canny-Sdxl-1.0 model,\nthe model support any type of lines and any width of lines, the sketch can be very simple and so does the prompt. This model is more general and good at generate visual appealing images,\nThe control ability is also strong, for example if you are unstatisfied with some local regions about the generated image, draw a more precise sketch and give a detail prompt will help a lot.\nNote the model also support lineart or canny lines, you can try it and will get a surpurise!!!\nModel Details\nModel Description\nDeveloped by: xinsir\nModel type: ControlNet_SDXL\nLicense: apache-2.0\nFinetuned from model [optional]: stabilityai/stable-diffusion-xl-base-1.0\nModel Sources [optional]\nPaper [optional]: https://arxiv.org/abs/2302.05543\nExamples[Note the following examples are all generate using stabilityai/stable-diffusion-xl-base-1.0 and xinsir/controlnet-scribble-sdxl-1.0]\nprompt: purple feathered eagle with specks of light like stars in feathers. It glows with arcane power\nprompt: manga girl in the city, drip marketing\nprompt: 17 year old girl with long dark hair in the style of realism with fantasy elements, detailed botanical illustrations, barbs and thorns, ethereal, magical, black, purple and maroon, intricate, photorealistic\nprompt: a logo for a paintball field named district 7 on a white background featuring paintballs the is bright and colourful eye catching and impactuful\nprompt: a photograph of a handsome crying blonde man with his face painted in the pride flag\nprompt: simple flat sketch fox play ball\nprompt: concept art, a surreal magical Tome of the Sun God, the book binding appears to be made of solar fire and emits a holy, radiant glow, Age of Wonders, Unreal Engine v5\nprompt: black Caribbean man walking balance front his fate chaos anarchy liberty independence force energy independence cinematic surreal beautiful rendition intricate sharp detail 8k\nprompt: die hard nakatomi plaza, explosion at the top, vector, night scene\nprompt: solitary glowing yellow tree in a desert. ultra wide shot. night time. hdr photography\nHow to Get Started with the Model\nUse the code below to get started with the model.\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\nfrom diffusers import DDIMScheduler, EulerAncestralDiscreteScheduler\nfrom controlnet_aux import PidiNetDetector, HEDdetector\nfrom diffusers.utils import load_image\nfrom huggingface_hub import HfApi\nfrom pathlib import Path\nfrom PIL import Image\nimport torch\nimport numpy as np\nimport cv2\nimport os\ndef nms(x, t, s):\nx = cv2.GaussianBlur(x.astype(np.float32), (0, 0), s)\nf1 = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]], dtype=np.uint8)\nf2 = np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8)\nf3 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=np.uint8)\nf4 = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]], dtype=np.uint8)\ny = np.zeros_like(x)\nfor f in [f1, f2, f3, f4]:\nnp.putmask(y, cv2.dilate(x, kernel=f) == x, x)\nz = np.zeros_like(y, dtype=np.uint8)\nz[y > t] = 255\nreturn z\ncontrolnet_conditioning_scale = 1.0\nprompt = \"your prompt, the longer the better, you can describe it as detail as possible\"\nnegative_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\neulera_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\ncontrolnet = ControlNetModel.from_pretrained(\n\"xinsir/controlnet-scribble-sdxl-1.0\",\ntorch_dtype=torch.float16\n)\n# when test with other base model, you need to change the vae also.\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\",\ncontrolnet=controlnet,\nvae=vae,\nsafety_checker=None,\ntorch_dtype=torch.float16,\nscheduler=eulera_scheduler,\n)\n# you can use either hed to generate a fake scribble given an image or a sketch image totally draw by yourself\nif random.random() > 0.5:\n# Method 1\n# if you use hed, you should provide an image, the image can be real or anime, you extract its hed lines and use it as the scribbles\n# The detail about hed detect you can refer to https://github.com/lllyasviel/ControlNet/blob/main/gradio_fake_scribble2image.py\n# Below is a example using diffusers HED detector\n# image_path = Image.open(\"your image path, the image can be real or anime, HED detector will extract its edge boundery\")\nimage_path = cv2.imread(\"your image path, the image can be real or anime, HED detector will extract its edge boundery\")\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\ncontrolnet_img = processor(image_path, scribble=False)\ncontrolnet_img.save(\"a hed detect path for an image\")\n# following is some processing to simulate human sketch draw, different threshold can generate different width of lines\ncontrolnet_img = np.array(controlnet_img)\ncontrolnet_img = nms(controlnet_img, 127, 3)\ncontrolnet_img = cv2.GaussianBlur(controlnet_img, (0, 0), 3)\n# higher threshold, thiner line\nrandom_val = int(round(random.uniform(0.01, 0.10), 2) * 255)\ncontrolnet_img[controlnet_img > random_val] = 255\ncontrolnet_img[controlnet_img < 255] = 0\ncontrolnet_img = Image.fromarray(controlnet_img)\nelse:\n# Method 2\n# if you use a sketch image total draw by yourself\ncontrol_path = \"the sketch image you draw with some tools, like drawing board, the path you save it\"\ncontrolnet_img = Image.open(control_path) # Note that the image must be black-white(0 or 255), like the examples we list\n# must resize to 1024*1024 or same resolution bucket to get the best performance\nwidth, height  = controlnet_img.size\nratio = np.sqrt(1024. * 1024. / (width * height))\nnew_width, new_height = int(width * ratio), int(height * ratio)\ncontrolnet_img = controlnet_img.resize((new_width, new_height))\nimages = pipe(\nprompt,\nnegative_prompt=negative_prompt,\nimage=controlnet_img,\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\nwidth=new_width,\nheight=new_height,\nnum_inference_steps=30,\n).images\nimages[0].save(f\"your image save path, png format is usually better than jpg or webp in terms of image quality but got much bigger\")\nEvaluation Data\nThe test data is randomly sample from midjourney upscale images with prompts, as the purpose of the project is to letting people draw images like midjourney. midjourneyâ€™s users include a large number of professional designers,\nand the upscale image tend to have more beauty score and prompt consistency, it is suitable to use it as the test set to judge the ability of controlnet. We select 300 prompt-image pairs randomly and generate 4 images per prompt,\ntotally 1200 images generated. We caculate the Laion Aesthetic Score to measure the beauty and the PerceptualSimilarity to measure the control ability, we find the quality of images have a good consistency with the meric values.\nWe compare our methods with other SOTA huggingface models and list the result below. We are the models that have highest aesthectic score, and can generate visually appealing images if you prompt it properly.\nNote: The condition image are generated using HED detector and random threshold to generate different kinds of lines.\nQuantitative Result\nmetric\nxinsir/controlnet-scribble-sdxl-1.0\nlaion_aesthetic\n6.03\nperceptual similarity\n0.5701\nlaion_aesthetic(the higher the better)perceptual similarity(the lower the better)\nNote: The values are caculated when save in webp format, when save in png the aesthetic values will increase 0.1-0.3, but the relative relation remains unchanged.\nConclusion\nIn our evaluation, the model can generate visually appealing images using simple sketch and simple prompt. This model can support any type of lines and any width of lines, using thick line will give a coarse control\nwhich obey the prompt your write more, and using thick line will give a strong control which obey the condition image more. The model can help you complish the drawing from coarse to fine, the model achieves higher\naesthetic score than xinsir/controlnet-canny-sdxl-1.0, but the control ability will decrease a bit because of thick line.",
    "google/paligemma-3b-ft-tallyqa-224": "Access PaliGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access PaliGemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPaliGemma model card\nModel information\nModel summary\nModel data\nHow to Use\nUse in Transformers\nRunning the default precision (float32) on CPU\nRunning other precisions on CUDA\nLoading in 4-bit / 8-bit\nImplementation information\nHardware\nSoftware\nEvaluation information\nBenchmark results\nEthics and safety\nEvaluation approach\nEvaluation results\nUsage and limitations\nIntended usage\nEthical considerations and risks\nLimitations\nPaliGemma model card\nModel page: PaliGemma\nTransformers PaliGemma 3B weights, fine-tuned with 224*224 input images on the TallyQA dataset. The models are available in float32, bfloat16 and float16 format for research purposes only. The fine-tune config is available at big_vision.\nResources and technical documentation:\nResponsible Generative AI Toolkit\nPaliGemma on Kaggle\nPaliGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel information\nModel summary\nDescription\nPaliGemma is a versatile and lightweight vision-language model (VLM) inspired by\nPaLI-3 and based on open components such as\nthe SigLIP vision model and the Gemma\nlanguage model. It takes both image and text\nas input and generates text as output, supporting multiple languages. It is designed for class-leading fine-tune performance on a wide range of vision-language tasks such as image and short video caption, visual question answering, text reading, object detection and object segmentation.\nModel architecture\nPaliGemma is the composition of a Transformer\ndecoder and a Vision Transformer image\nencoder, with a total of 3 billion\nparams. The text decoder is initialized from\nGemma-2B. The image encoder is\ninitialized from\nSigLIP-So400m/14.\nPaliGemma is trained following the PaLI-3 recipes.\nInputs and outputs\nInput: Image and text string, such as a prompt to caption the image, or\na question.\nOutput: Generated text in response to the input, such as a caption of\nthe image, an answer to a question, a list of object bounding box\ncoordinates, or segmentation codewords.\nModel data\nPre-train datasets\nPaliGemma is pre-trained on the following mixture of datasets:\nWebLI: WebLI (Web Language Image) is\na web-scale multilingual image-text dataset built from the public web. A\nwide range of WebLI splits are used to acquire versatile model capabilities,\nsuch as visual semantic understanding, object localization,\nvisually-situated text understanding, multilinguality, etc.\nCC3M-35L: Curated English image-alt_text pairs from webpages (Sharma et\nal., 2018). We used the Google Cloud\nTranslation API to translate into 34\nadditional languages.\nVQÂ²A-CC3M-35L/VQG-CC3M-35L: A subset of VQ2A-CC3M (Changpinyo et al.,\n2022a), translated into the\nsame additional 34 languages as CC3M-35L, using the Google Cloud\nTranslation API.\nOpenImages: Detection and object-aware questions and answers\n(Piergiovanni et al. 2022) generated by\nhandcrafted rules on the OpenImages dataset.\nWIT: Images and texts collected from Wikipedia (Srinivasan et al.,\n2021).\nData responsibility filtering\nThe following filters are applied to WebLI, with the goal of training PaliGemma\non clean data:\nPornographic image filtering: This filter removes images deemed to be of\npornographic nature.\nText safety filtering: We identify and filter out images that are paired\nwith unsafe text. Unsafe text is any text deemed to contain or be about\nCSAI, pornography, vulgarities, or otherwise offensive.\nText toxicity filtering: We further use the Perspective\nAPI to identify and filter out images that are\npaired with text deemed insulting, obscene, hateful or otherwise toxic.\nText personal information filtering: We filtered certain personal information and other sensitive data using Cloud Data Loss Prevention (DLP)\nAPI to protect the privacy\nof individuals. Identifiers such as social security numbers and other sensitive information types were removed.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies and practices.\nHow to Use\nPaliGemma is a single-turn vision language model not meant for conversational use,\nand it works best when fine-tuning to a specific use case.\nYou can configure which task the model will solve by conditioning it with task prefixes,\nsuch as â€œdetectâ€ or â€œsegmentâ€. The pretrained models were trained in this fashion to imbue\nthem with a rich set of capabilities (question answering, captioning, segmentation, etc.).\nHowever, they are not designed to be used directly, but to be transferred (by fine-tuning)\nto specific tasks using a similar prompt structure. For interactive testing, you can use\nthe \"mix\" family of models, which have been fine-tuned on a mixture of tasks.\nPlease, refer to the usage and limitations section for intended\nuse cases, or visit the blog post for\nadditional details and examples.\nUse in Transformers\nThe following snippets use model google/paligemma-3b-mix-224 for reference purposes.\nThe model in this repo you are now browsing may have been trained for other tasks, please\nmake sure you use appropriate inputs for the task at hand.\nRunning the default precision (float32) on CPU\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nOutput: Un auto azul estacionado frente a un edificio.\nRunning other precisions on CUDA\nFor convenience, the repos contain revisions of the weights already converted to bfloat16 and float16,\nso you can use them to reduce the download size and avoid casting on your local computer.\nThis is how you'd run bfloat16 on an nvidia CUDA card.\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=dtype,\ndevice_map=device,\nrevision=\"bfloat16\",\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nLoading in 4-bit / 8-bit\nYou need to install bitsandbytes to automatically run inference using 8-bit or 4-bit precision:\npip install bitsandbytes accelerate\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\nmodel_id, quantization_config=quantization_config\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nImplementation information\nHardware\nPaliGemma was trained using the latest generation of Tensor Processing Unit\n(TPU) hardware (TPUv5e).\nSoftware\nTraining was done using JAX,\nFlax,\nTFDS and\nbig_vision.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nTFDS is used to access datasets and Flax is used for model architecture. The\nPaliGemma fine-tune code and inference code are released in the big_vision\nGitHub repository.\nEvaluation information\nBenchmark results\nIn order to verify the transferability of PaliGemma to a wide variety of\nacademic tasks, we fine-tune the pretrained models on each task. Additionally we\ntrain the mix model with a mixture of the transfer tasks. We report results on\ndifferent resolutions to provide an impression of which tasks benefit from\nincreased resolution. Importantly, none of these tasks or datasets are part of\nthe pretraining data mixture, and their images are explicitly removed from the\nweb-scale pre-training data.\nMix model (fine-tune on mixture of transfer tasks)\nBenchmark\nMetric (split)\nmix-224\nmix-448\nMMVP\nPaired Accuracy\n46.00\n45.33\nPOPE\nAccuracy(random/popular/adversarial)\n88.00\n86.63\n85.67\n89.37\n88.40\n87.47\nGQA\nAccuracy (test)\n65.20\n65.47\nSingle task (fine-tune on single task)\nBenchmark(train split)\nMetric(split)\npt-224\npt-448\npt-896\nCaptioning\nCOCO captions(train+restval)\nCIDEr (val)\n141.92\n144.60\nNoCaps(Eval of COCOcaptions transfer)\nCIDEr (val)\n121.72\n123.58\nCOCO-35L(train)\nCIDEr dev(en/avg-34/avg)\n139.2\n115.8\n116.4\n141.2\n118.0\n118.6\nXM3600(Eval of COCO-35L transfer)\nCIDEr dev(en/avg-34/avg)\n78.1\n41.3\n42.4\n80.0\n41.9\n42.9\nTextCaps(train)\nCIDEr (val)\n127.48\n153.94\nSciCap(first sentence, no subfigure)(train+val)\nCIDEr/BLEU-4(test)\n162.25\n0.192\n181.49\n0.211\nScreen2words(train+dev)\nCIDEr (test)\n117.57\n119.59\nWidget Captioning(train+dev)\nCIDEr (test)\n136.07\n148.36\nQuestion answering\nVQAv2(train+validation)\nAccuracy(Test server - std)\n83.19\n85.64\nMMVP(Eval of VQAv2 transfer)\nPaired Accuracy\n47.33\n45.33\nPOPE(Eval of VQAv2 transfer)\nAccuracy(random/popular/adversarial)\n87.80\n85.87\n84.27\n88.23\n86.77\n85.90\nOKVQA(train)\nAccuracy (val)\n63.54\n63.15\nA-OKVQA (MC)(train+val)\nAccuracy(Test server)\n76.37\n76.90\nA-OKVQA (DA)(train+val)\nAccuracy(Test server)\n61.85\n63.22\nGQA(train_balanced+val_balanced)\nAccuracy(testdev balanced)\n65.61\n67.03\nxGQA(Eval of GQA transfer)\nMean Accuracy(bn, de, en, id,ko, pt, ru, zh)\n58.37\n59.07\nNLVR2(train+dev)\nAccuracy (test)\n90.02\n88.93\nMaRVL(Eval of NLVR2 transfer)\nMean Accuracy(test)(id, sw, ta, tr, zh)\n80.57\n76.78\nAI2D(train)\nAccuracy (test)\n72.12\n73.28\nScienceQA(Img subset, no CoT)(train+val)\nAccuracy (test)\n95.39\n95.93\nRSVQA-LR (Non numeric)(train+val)\nMean Accuracy(test)\n92.65\n93.11\nRSVQA-HR (Non numeric)(train+val)\nMean Accuracy(test/test2)\n92.61\n90.58\n92.79\n90.54\nChartQA(human+aug)x(train+val)\nMean RelaxedAccuracy(test_human,test_aug)\n57.08\n71.36\nVizWiz VQA(train+val)\nAccuracy(Test server - std)\n73.7\n75.52\nTallyQA(train)\nAccuracy(test_simple/test_complex)\n81.72\n69.56\n84.86\n72.27\nOCR-VQA(train+val)\nAccuracy (test)\n72.32\n74.61\n74.93\nTextVQA(train+val)\nAccuracy(Test server - std)\n55.47\n73.15\n76.48\nDocVQA(train+val)\nANLS (Test server)\n43.74\n78.02\n84.77\nInfographic VQA(train+val)\nANLS (Test server)\n28.46\n40.47\n47.75\nSceneText VQA(train+val)\nANLS (Test server)\n63.29\n81.82\n84.40\nSegmentation\nRefCOCO(combined refcoco, refcoco+,refcocog excluding valand test images)\nMIoU(validation)refcoco/refcoco+/refcocog\n73.40\n68.32\n67.65\n75.57\n69.76\n70.17\n76.94\n72.18\n72.22\nVideo tasks (Caption/QA)\nMSR-VTT (Captioning)\nCIDEr (test)\n70.54\nMSR-VTT (QA)\nAccuracy (test)\n50.09\nActivityNet (Captioning)\nCIDEr (test)\n34.62\nActivityNet (QA)\nAccuracy (test)\n50.78\nVATEX (Captioning)\nCIDEr (test)\n79.73\nMSVD (QA)\nAccuracy (test)\n60.22\nEthics and safety\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nHuman evaluation on prompts covering child safety, content safety and\nrepresentational harms. See the Gemma model\ncard for\nmore details on evaluation approach, but with image captioning and visual\nquestion answering setups.\nImage-to-Text benchmark evaluation: Benchmark against relevant academic\ndatasets such as FairFace Dataset (Karkkainen et al.,\n2021).\nEvaluation results\nThe human evaluation results of ethics and safety evaluations are within\nacceptable thresholds for meeting internal\npolicies\nfor categories such as child safety, content safety and representational\nharms.\nOn top of robust internal evaluations, we also use the Perspective API\n(threshold of 0.8) to measure toxicity, profanity, and other potential\nissues in the generated captions for images sourced from the FairFace\ndataset. We report the maximum and median values observed across subgroups\nfor each of the perceived gender, ethnicity, and age attributes.\nMetric\nPerceivedgender\nEthnicity\nAge group\nMaximum\nMedian\nMaximum\nMedian\nMaximum\nMedian\nToxicity\n0.04%\n0.03%\n0.08%\n0.00%\n0.09%\n0.00%\nIdentity Attack\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nInsult\n0.06%\n0.04%\n0.09%\n0.07%\n0.16%\n0.00%\nThreat\n0.06%\n0.05%\n0.14%\n0.05%\n0.17%\n0.00%\nProfanity\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nUsage and limitations\nIntended usage\nOpen Vision Language Models (VLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nFine-tune on specific vision-language task:\nThe pre-trained models can be fine-tuned on a wide range of vision-language\ntasks such as: image captioning, short video caption, visual question\nanswering, text reading, object detection and object segmentation.\nThe pre-trained models can be fine-tuned for specific domains such as remote\nsensing question answering, visual questions from people who are blind,\nscience question answering, describe UI element functionalities.\nThe pre-trained models can be fine-tuned for tasks with non-textual outputs\nsuch as bounding boxes or segmentation masks.\nVision-language research:\nThe pre-trained models and fine-tuned models can serve as a foundation for researchers to experiment with VLM\ntechniques, develop algorithms, and contribute to the advancement of the\nfield.\nEthical considerations and risks\nThe development of vision-language models (VLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world image-text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the Responsible Generative AI Toolkit.\nTransparency and Accountability\nThis model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share innovation by making VLM technology accessible to developers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the Gemma\nProhibited Use Policy.\nPrivacy violations: Models were trained on data filtered to remove certain personal information and sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\nLimitations\nMost limitations inherited from the underlying Gemma model still apply:\nVLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nNatural language is inherently complex. VLMs might struggle to grasp\nsubtle nuances, sarcasm, or figurative language.\nVLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nVLMs rely on statistical patterns in language and images. They might\nlack the ability to apply common sense reasoning in certain situations.\nPaliGemma was designed first and foremost to serve as a general pre-trained\nmodel for transfer to specialized tasks. Hence, its \"out of the box\" or\n\"zero-shot\" performance might lag behind models designed specifically for\nthat.\nPaliGemma is not a multi-turn chatbot. It is designed for a single round of\nimage and text input.\nCitation\n@article{beyer2024paligemma,\ntitle={{PaliGemma: A versatile 3B VLM for transfer}},\nauthor={Lucas Beyer* and Andreas Steiner* and AndrÃ© Susano Pinto* and Alexander Kolesnikov* and Xiao Wang* and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko BoÅ¡njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai*},\nyear={2024},\njournal={arXiv preprint arXiv:2407.07726}\n}\nFind the paper here.",
    "google/paligemma-3b-ft-textvqa-448": "Access PaliGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access PaliGemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPaliGemma model card\nModel information\nModel summary\nModel data\nHow to Use\nUse in Transformers\nRunning the default precision (float32) on CPU\nRunning other precisions on CUDA\nLoading in 4-bit / 8-bit\nImplementation information\nHardware\nSoftware\nEvaluation information\nBenchmark results\nEthics and safety\nEvaluation approach\nEvaluation results\nUsage and limitations\nIntended usage\nEthical considerations and risks\nLimitations\nPaliGemma model card\nModel page: PaliGemma\nTransformers PaliGemma 3B weights, fine-tuned with 448*448 input images on the TextVQA dataset. The models are available in float32, bfloat16 and float16 format for research purposes only. The fine-tune config is available at big_vision.\nResources and technical documentation:\nResponsible Generative AI Toolkit\nPaliGemma on Kaggle\nPaliGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel information\nModel summary\nDescription\nPaliGemma is a versatile and lightweight vision-language model (VLM) inspired by\nPaLI-3 and based on open components such as\nthe SigLIP vision model and the Gemma\nlanguage model. It takes both image and text\nas input and generates text as output, supporting multiple languages. It is designed for class-leading fine-tune performance on a wide range of vision-language tasks such as image and short video caption, visual question answering, text reading, object detection and object segmentation.\nModel architecture\nPaliGemma is the composition of a Transformer\ndecoder and a Vision Transformer image\nencoder, with a total of 3 billion\nparams. The text decoder is initialized from\nGemma-2B. The image encoder is\ninitialized from\nSigLIP-So400m/14.\nPaliGemma is trained following the PaLI-3 recipes.\nInputs and outputs\nInput: Image and text string, such as a prompt to caption the image, or\na question.\nOutput: Generated text in response to the input, such as a caption of\nthe image, an answer to a question, a list of object bounding box\ncoordinates, or segmentation codewords.\nModel data\nPre-train datasets\nPaliGemma is pre-trained on the following mixture of datasets:\nWebLI: WebLI (Web Language Image) is\na web-scale multilingual image-text dataset built from the public web. A\nwide range of WebLI splits are used to acquire versatile model capabilities,\nsuch as visual semantic understanding, object localization,\nvisually-situated text understanding, multilinguality, etc.\nCC3M-35L: Curated English image-alt_text pairs from webpages (Sharma et\nal., 2018). We used the Google Cloud\nTranslation API to translate into 34\nadditional languages.\nVQÂ²A-CC3M-35L/VQG-CC3M-35L: A subset of VQ2A-CC3M (Changpinyo et al.,\n2022a), translated into the\nsame additional 34 languages as CC3M-35L, using the Google Cloud\nTranslation API.\nOpenImages: Detection and object-aware questions and answers\n(Piergiovanni et al. 2022) generated by\nhandcrafted rules on the OpenImages dataset.\nWIT: Images and texts collected from Wikipedia (Srinivasan et al.,\n2021).\nData responsibility filtering\nThe following filters are applied to WebLI, with the goal of training PaliGemma\non clean data:\nPornographic image filtering: This filter removes images deemed to be of\npornographic nature.\nText safety filtering: We identify and filter out images that are paired\nwith unsafe text. Unsafe text is any text deemed to contain or be about\nCSAI, pornography, vulgarities, or otherwise offensive.\nText toxicity filtering: We further use the Perspective\nAPI to identify and filter out images that are\npaired with text deemed insulting, obscene, hateful or otherwise toxic.\nText personal information filtering: We filtered certain personal information and other sensitive data using Cloud Data Loss Prevention (DLP)\nAPI to protect the privacy\nof individuals. Identifiers such as social security numbers and other sensitive information types were removed.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies and practices.\nHow to Use\nPaliGemma is a single-turn vision language model not meant for conversational use,\nand it works best when fine-tuning to a specific use case.\nYou can configure which task the model will solve by conditioning it with task prefixes,\nsuch as â€œdetectâ€ or â€œsegmentâ€. The pretrained models were trained in this fashion to imbue\nthem with a rich set of capabilities (question answering, captioning, segmentation, etc.).\nHowever, they are not designed to be used directly, but to be transferred (by fine-tuning)\nto specific tasks using a similar prompt structure. For interactive testing, you can use\nthe \"mix\" family of models, which have been fine-tuned on a mixture of tasks.\nPlease, refer to the usage and limitations section for intended\nuse cases, or visit the blog post for\nadditional details and examples.\nUse in Transformers\nThe following snippets use model google/paligemma-3b-mix-224 for reference purposes.\nThe model in this repo you are now browsing may have been trained for other tasks, please\nmake sure you use appropriate inputs for the task at hand.\nRunning the default precision (float32) on CPU\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nOutput: Un auto azul estacionado frente a un edificio.\nRunning other precisions on CUDA\nFor convenience, the repos contain revisions of the weights already converted to bfloat16 and float16,\nso you can use them to reduce the download size and avoid casting on your local computer.\nThis is how you'd run bfloat16 on an nvidia CUDA card.\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=dtype,\ndevice_map=device,\nrevision=\"bfloat16\",\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nLoading in 4-bit / 8-bit\nYou need to install bitsandbytes to automatically run inference using 8-bit or 4-bit precision:\npip install bitsandbytes accelerate\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\nmodel_id, quantization_config=quantization_config\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nImplementation information\nHardware\nPaliGemma was trained using the latest generation of Tensor Processing Unit\n(TPU) hardware (TPUv5e).\nSoftware\nTraining was done using JAX,\nFlax,\nTFDS and\nbig_vision.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nTFDS is used to access datasets and Flax is used for model architecture. The\nPaliGemma fine-tune code and inference code are released in the big_vision\nGitHub repository.\nEvaluation information\nBenchmark results\nIn order to verify the transferability of PaliGemma to a wide variety of\nacademic tasks, we fine-tune the pretrained models on each task. Additionally we\ntrain the mix model with a mixture of the transfer tasks. We report results on\ndifferent resolutions to provide an impression of which tasks benefit from\nincreased resolution. Importantly, none of these tasks or datasets are part of\nthe pretraining data mixture, and their images are explicitly removed from the\nweb-scale pre-training data.\nMix model (fine-tune on mixture of transfer tasks)\nBenchmark\nMetric (split)\nmix-224\nmix-448\nMMVP\nPaired Accuracy\n46.00\n45.33\nPOPE\nAccuracy(random/popular/adversarial)\n88.00\n86.63\n85.67\n89.37\n88.40\n87.47\nGQA\nAccuracy (test)\n65.20\n65.47\nSingle task (fine-tune on single task)\nBenchmark(train split)\nMetric(split)\npt-224\npt-448\npt-896\nCaptioning\nCOCO captions(train+restval)\nCIDEr (val)\n141.92\n144.60\nNoCaps(Eval of COCOcaptions transfer)\nCIDEr (val)\n121.72\n123.58\nCOCO-35L(train)\nCIDEr dev(en/avg-34/avg)\n139.2\n115.8\n116.4\n141.2\n118.0\n118.6\nXM3600(Eval of COCO-35L transfer)\nCIDEr dev(en/avg-34/avg)\n78.1\n41.3\n42.4\n80.0\n41.9\n42.9\nTextCaps(train)\nCIDEr (val)\n127.48\n153.94\nSciCap(first sentence, no subfigure)(train+val)\nCIDEr/BLEU-4(test)\n162.25\n0.192\n181.49\n0.211\nScreen2words(train+dev)\nCIDEr (test)\n117.57\n119.59\nWidget Captioning(train+dev)\nCIDEr (test)\n136.07\n148.36\nQuestion answering\nVQAv2(train+validation)\nAccuracy(Test server - std)\n83.19\n85.64\nMMVP(Eval of VQAv2 transfer)\nPaired Accuracy\n47.33\n45.33\nPOPE(Eval of VQAv2 transfer)\nAccuracy(random/popular/adversarial)\n87.80\n85.87\n84.27\n88.23\n86.77\n85.90\nOKVQA(train)\nAccuracy (val)\n63.54\n63.15\nA-OKVQA (MC)(train+val)\nAccuracy(Test server)\n76.37\n76.90\nA-OKVQA (DA)(train+val)\nAccuracy(Test server)\n61.85\n63.22\nGQA(train_balanced+val_balanced)\nAccuracy(testdev balanced)\n65.61\n67.03\nxGQA(Eval of GQA transfer)\nMean Accuracy(bn, de, en, id,ko, pt, ru, zh)\n58.37\n59.07\nNLVR2(train+dev)\nAccuracy (test)\n90.02\n88.93\nMaRVL(Eval of NLVR2 transfer)\nMean Accuracy(test)(id, sw, ta, tr, zh)\n80.57\n76.78\nAI2D(train)\nAccuracy (test)\n72.12\n73.28\nScienceQA(Img subset, no CoT)(train+val)\nAccuracy (test)\n95.39\n95.93\nRSVQA-LR (Non numeric)(train+val)\nMean Accuracy(test)\n92.65\n93.11\nRSVQA-HR (Non numeric)(train+val)\nMean Accuracy(test/test2)\n92.61\n90.58\n92.79\n90.54\nChartQA(human+aug)x(train+val)\nMean RelaxedAccuracy(test_human,test_aug)\n57.08\n71.36\nVizWiz VQA(train+val)\nAccuracy(Test server - std)\n73.7\n75.52\nTallyQA(train)\nAccuracy(test_simple/test_complex)\n81.72\n69.56\n84.86\n72.27\nOCR-VQA(train+val)\nAccuracy (test)\n72.32\n74.61\n74.93\nTextVQA(train+val)\nAccuracy(Test server - std)\n55.47\n73.15\n76.48\nDocVQA(train+val)\nANLS (Test server)\n43.74\n78.02\n84.77\nInfographic VQA(train+val)\nANLS (Test server)\n28.46\n40.47\n47.75\nSceneText VQA(train+val)\nANLS (Test server)\n63.29\n81.82\n84.40\nSegmentation\nRefCOCO(combined refcoco, refcoco+,refcocog excluding valand test images)\nMIoU(validation)refcoco/refcoco+/refcocog\n73.40\n68.32\n67.65\n75.57\n69.76\n70.17\n76.94\n72.18\n72.22\nVideo tasks (Caption/QA)\nMSR-VTT (Captioning)\nCIDEr (test)\n70.54\nMSR-VTT (QA)\nAccuracy (test)\n50.09\nActivityNet (Captioning)\nCIDEr (test)\n34.62\nActivityNet (QA)\nAccuracy (test)\n50.78\nVATEX (Captioning)\nCIDEr (test)\n79.73\nMSVD (QA)\nAccuracy (test)\n60.22\nEthics and safety\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nHuman evaluation on prompts covering child safety, content safety and\nrepresentational harms. See the Gemma model\ncard for\nmore details on evaluation approach, but with image captioning and visual\nquestion answering setups.\nImage-to-Text benchmark evaluation: Benchmark against relevant academic\ndatasets such as FairFace Dataset (Karkkainen et al.,\n2021).\nEvaluation results\nThe human evaluation results of ethics and safety evaluations are within\nacceptable thresholds for meeting internal\npolicies\nfor categories such as child safety, content safety and representational\nharms.\nOn top of robust internal evaluations, we also use the Perspective API\n(threshold of 0.8) to measure toxicity, profanity, and other potential\nissues in the generated captions for images sourced from the FairFace\ndataset. We report the maximum and median values observed across subgroups\nfor each of the perceived gender, ethnicity, and age attributes.\nMetric\nPerceivedgender\nEthnicity\nAge group\nMaximum\nMedian\nMaximum\nMedian\nMaximum\nMedian\nToxicity\n0.04%\n0.03%\n0.08%\n0.00%\n0.09%\n0.00%\nIdentity Attack\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nInsult\n0.06%\n0.04%\n0.09%\n0.07%\n0.16%\n0.00%\nThreat\n0.06%\n0.05%\n0.14%\n0.05%\n0.17%\n0.00%\nProfanity\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nUsage and limitations\nIntended usage\nOpen Vision Language Models (VLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nFine-tune on specific vision-language task:\nThe pre-trained models can be fine-tuned on a wide range of vision-language\ntasks such as: image captioning, short video caption, visual question\nanswering, text reading, object detection and object segmentation.\nThe pre-trained models can be fine-tuned for specific domains such as remote\nsensing question answering, visual questions from people who are blind,\nscience question answering, describe UI element functionalities.\nThe pre-trained models can be fine-tuned for tasks with non-textual outputs\nsuch as bounding boxes or segmentation masks.\nVision-language research:\nThe pre-trained models and fine-tuned models can serve as a foundation for researchers to experiment with VLM\ntechniques, develop algorithms, and contribute to the advancement of the\nfield.\nEthical considerations and risks\nThe development of vision-language models (VLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world image-text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the Responsible Generative AI Toolkit.\nTransparency and Accountability\nThis model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share innovation by making VLM technology accessible to developers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the Gemma\nProhibited Use Policy.\nPrivacy violations: Models were trained on data filtered to remove certain personal information and sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\nLimitations\nMost limitations inherited from the underlying Gemma model still apply:\nVLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nNatural language is inherently complex. VLMs might struggle to grasp\nsubtle nuances, sarcasm, or figurative language.\nVLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nVLMs rely on statistical patterns in language and images. They might\nlack the ability to apply common sense reasoning in certain situations.\nPaliGemma was designed first and foremost to serve as a general pre-trained\nmodel for transfer to specialized tasks. Hence, its \"out of the box\" or\n\"zero-shot\" performance might lag behind models designed specifically for\nthat.\nPaliGemma is not a multi-turn chatbot. It is designed for a single round of\nimage and text input.\nCitation\n@article{beyer2024paligemma,\ntitle={{PaliGemma: A versatile 3B VLM for transfer}},\nauthor={Lucas Beyer* and Andreas Steiner* and AndrÃ© Susano Pinto* and Alexander Kolesnikov* and Xiao Wang* and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko BoÅ¡njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai*},\nyear={2024},\njournal={arXiv preprint arXiv:2407.07726}\n}\nFind the paper here.",
    "microsoft/kosmos-2.5": "Kosmos-2.5\nModel description\nNOTE:\nInference\nCitation\nLicense\nKosmos-2.5\nMicrosoft Document AI | GitHub\nModel description\nKosmos-2.5 is a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared decoder-only auto-regressive Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.\nKosmos-2.5: A Multimodal Literate Model\nNOTE:\nSince this is a generative model, there is a risk of hallucination during the generation process, and it CAN NOT guarantee the accuracy of all OCR/Markdown results in the images.\nInference\nKOSMOS-2.5 is supported from Transformers >= 4.56. Find the docs here.\nMarkdown Task: For usage instructions, please refer to md.py.\nimport re\nimport torch\nimport requests\nfrom PIL import Image, ImageDraw\nfrom transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\nrepo = \"microsoft/kosmos-2.5\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nmodel = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\nprocessor = AutoProcessor.from_pretrained(repo)\n# sample image\nurl = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"<md>\"\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\nheight, width = inputs.pop(\"height\"), inputs.pop(\"width\")\nraw_width, raw_height = image.size\nscale_height = raw_height / height\nscale_width = raw_width / width\ninputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\ninputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\ngenerated_ids = model.generate(\n**inputs,\nmax_new_tokens=1024,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nprint(generated_text[0])\nOCR Task: For usage instructions, please refer to ocr.py.\nimport re\nimport torch\nimport requests\nfrom PIL import Image, ImageDraw\nfrom transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\nrepo = \"microsoft/kosmos-2.5\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nmodel = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\nprocessor = AutoProcessor.from_pretrained(repo)\n# sample image\nurl = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# bs = 1\nprompt = \"<ocr>\"\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\nheight, width = inputs.pop(\"height\"), inputs.pop(\"width\")\nraw_width, raw_height = image.size\nscale_height = raw_height / height\nscale_width = raw_width / width\n# bs > 1, batch generation\n# inputs = processor(text=[prompt, prompt], images=[image,image], return_tensors=\"pt\")\n# height, width = inputs.pop(\"height\"), inputs.pop(\"width\")\n# raw_width, raw_height = image.size\n# scale_height = raw_height / height[0]\n# scale_width = raw_width / width[0]\ninputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\ninputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\ngenerated_ids = model.generate(\n**inputs,\nmax_new_tokens=1024,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\ndef post_process(y, scale_height, scale_width):\ny = y.replace(prompt, \"\")\nif \"<md>\" in prompt:\nreturn y\npattern = r\"<bbox><x_\\d+><y_\\d+><x_\\d+><y_\\d+></bbox>\"\nbboxs_raw = re.findall(pattern, y)\nlines = re.split(pattern, y)[1:]\nbboxs = [re.findall(r\"\\d+\", i) for i in bboxs_raw]\nbboxs = [[int(j) for j in i] for i in bboxs]\ninfo = \"\"\nfor i in range(len(lines)):\nbox = bboxs[i]\nx0, y0, x1, y1 = box\nif not (x0 >= x1 or y0 >= y1):\nx0 = int(x0 * scale_width)\ny0 = int(y0 * scale_height)\nx1 = int(x1 * scale_width)\ny1 = int(y1 * scale_height)\ninfo += f\"{x0},{y0},{x1},{y0},{x1},{y1},{x0},{y1},{lines[i]}\"\nreturn info\noutput_text = post_process(generated_text[0], scale_height, scale_width)\nprint(output_text)\ndraw = ImageDraw.Draw(image)\nlines = output_text.split(\"\\n\")\nfor line in lines:\n# draw the bounding box\nline = list(line.split(\",\"))\nif len(line) < 8:\ncontinue\nline = list(map(int, line[:8]))\ndraw.polygon(line, outline=\"red\")\nimage.save(\"output.png\")\nCitation\nIf you find Kosmos-2.5 useful in your research, please cite the following paper:\n@article{lv2023kosmos,\ntitle={Kosmos-2.5: A multimodal literate model},\nauthor={Lv, Tengchao and Huang, Yupan and Chen, Jingye and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and Dong, Li and Luo, Weiyao and others},\njournal={arXiv preprint arXiv:2309.11419},\nyear={2023}\n}\nLicense\nThe content of this project itself is licensed under the MIT\nMicrosoft Open Source Code of Conduct",
    "dphn/dolphin-2.9.1-dbrx": "Dolphin 2.9.1 DBRX ğŸ¬\nEvals\nTraining\nout\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nDolphin 2.9.1 DBRX ğŸ¬\nCurated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations\nDiscord: https://discord.gg/cognitivecomputations\nOur appreciation for the sponsors of Dolphin 2.9.1:\nCrusoe Cloud - provided excellent on-demand 8xH100 node\nThis model is based on databricks/dbrx-base, and is governed by databricks-open-model-license\nThe base model has 32k context, and the full-weight fine-tuning was with 4k sequence length.\nThis model was trained FFT on parameters selected by Laser Scanner, using ChatML prompt template format.\nexample:\n<|im_start|>system\nYou are Dolphin, a helpful AI assistant.<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDolphin-2.9.1 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.\nDolphin is uncensored. We have filtered the dataset to remove alignment and bias. This makes the model more compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones. Please read my blog post about uncensored models. https://erichartford.com/uncensored-models You are responsible for any content you create using this model. Enjoy responsibly.\nDolphin is licensed according to Meta's Llama license.  We grant permission for any use, including commercial, that falls within accordance with Meta's Llama-3 license.  Dolphin was trained on data generated from GPT4, among other models.\nEvals\nTraining\nSee axolotl config\naxolotl version: 0.4.0\nbase_model: /workspace/axolotl/dbrx-checkpoint\nmodel_type: AutoModelForCausalLM\ntokenizer_type: AutoTokenizer\ntrust_remote_code: true\nload_in_8bit: false\n# load_in_4bit: true\nstrict: false\n# adapter: qlora\n# lora_modules_to_save: [embed_tokens, lm_head]\n# lora_r: 32\n# lora_alpha: 16\n# lora_dropout: 0.05\n# lora_target_linear: false\n# lora_fan_in_fan_out:\ndatasets:\n- path: /workspace/datasets/dolphin-2.9/dolphin201-sharegpt2.jsonl\ntype: sharegpt\nconversation: chatml\n# - path: /workspace/datasets/dolphin-2.9/Ultrachat200kunfiltered.jsonl\n#   type: sharegpt\n#   conversation: chatml\n- path: /workspace/datasets/dolphin-2.9/dolphin-coder-translate-sharegpt2.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/dolphin-coder-codegen-sharegpt2.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/m-a-p_Code-Feedback-sharegpt-unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/m-a-p_CodeFeedback-Filtered-Instruction-sharegpt-unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/not_samantha_norefusals.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/Orca-Math-resort-unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/agent_instruct_react_unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/toolbench_instruct_j1s1_3k_unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/toolbench_negative_unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/toolbench_react_10p_unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/toolbench_tflan_cot_30p_unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n- path: /workspace/datasets/dolphin-2.9/openhermes200k_unfiltered.jsonl\ntype: sharegpt\nconversation: chatml\n# - path: /workspace/datasets/dolphin-2.9/SystemConversations.jsonl\n#   type: sharegpt\n#   conversation: chatml\nchat_template: chatml\nunfrozen_parameters:\n- ^lm_head.weight$\n# ffn.experts.mlp_experts.0.v1 layers\n- transformer.blocks.30.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.25.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.15.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.22.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.31.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.21.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.8.ffn.experts.mlp_experts.0.v1\n- transformer.blocks.23.ffn.experts.mlp_experts.0.v1\n# ffn.experts.mlp_experts.0.w1 layers\n- transformer.blocks.7.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.32.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.25.ffn.experts.mlp_experts.0.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.0.w1\n# ffn.experts.mlp_experts.0.w2 layers\n- transformer.blocks.25.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.22.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.26.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.7.ffn.experts.mlp_experts.0.w2\n- transformer.blocks.3.ffn.experts.mlp_experts.0.w2\n# ffn.experts.mlp_experts.1.v1 layers\n- transformer.blocks.27.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.25.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.23.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.6.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.21.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.15.ffn.experts.mlp_experts.1.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.1.v1\n# ffn.experts.mlp_experts.1.w1 layers\n- transformer.blocks.0.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.7.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.27.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.1.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.1.w1\n# ffn.experts.mlp_experts.1.w2 layers\n- transformer.blocks.25.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.23.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.21.ffn.experts.mlp_experts.1.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.1.w2\n# ffn.experts.mlp_experts.10.v1 layers\n- transformer.blocks.28.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.34.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.26.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.36.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.20.ffn.experts.mlp_experts.10.v1\n- transformer.blocks.35.ffn.experts.mlp_experts.10.v1\n# ffn.experts.mlp_experts.10.w1 layers\n- transformer.blocks.24.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.7.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.34.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.28.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.10.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.10.w1\n# ffn.experts.mlp_experts.10.w2 layers\n- transformer.blocks.24.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.23.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.3.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.26.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.2.ffn.experts.mlp_experts.10.w2\n- transformer.blocks.20.ffn.experts.mlp_experts.10.w2\n# ffn.experts.mlp_experts.11.w1 layers\n- transformer.blocks.6.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.28.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.11.w1\n- transformer.blocks.13.ffn.experts.mlp_experts.11.w1\n# ffn.experts.mlp_experts.11.w2 layers\n- transformer.blocks.27.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.24.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.22.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.6.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.7.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.11.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.11.w2\n# ffn.experts.mlp_experts.12.v1 layers\n- transformer.blocks.30.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.21.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.28.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.8.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.10.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.23.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.6.ffn.experts.mlp_experts.12.v1\n- transformer.blocks.20.ffn.experts.mlp_experts.12.v1\n# ffn.experts.mlp_experts.12.w1 layers\n- transformer.blocks.8.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.2.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.17.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.12.w1\n- transformer.blocks.21.ffn.experts.mlp_experts.12.w1\n# ffn.experts.mlp_experts.12.w2 layers\n- transformer.blocks.6.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.8.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.21.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.2.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.12.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.12.w2\n# ffn.experts.mlp_experts.13.v1 layers\n- transformer.blocks.31.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.8.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.10.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.11.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.25.ffn.experts.mlp_experts.13.v1\n- transformer.blocks.36.ffn.experts.mlp_experts.13.v1\n# ffn.experts.mlp_experts.13.w1 layers\n- transformer.blocks.4.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.24.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.13.w1\n- transformer.blocks.11.ffn.experts.mlp_experts.13.w1\n# ffn.experts.mlp_experts.13.w2 layers\n- transformer.blocks.24.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.20.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.3.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.6.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.13.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.13.w2\n# ffn.experts.mlp_experts.14.v1 layers\n- transformer.blocks.28.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.26.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.35.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.8.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.15.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.11.ffn.experts.mlp_experts.14.v1\n- transformer.blocks.22.ffn.experts.mlp_experts.14.v1\n# ffn.experts.mlp_experts.14.w1 layers\n- transformer.blocks.8.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.7.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.13.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.28.ffn.experts.mlp_experts.14.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.14.w1\n# ffn.experts.mlp_experts.14.w2 layers\n- transformer.blocks.26.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.24.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.6.ffn.experts.mlp_experts.14.w2\n- transformer.blocks.22.ffn.experts.mlp_experts.14.w2\n# ffn.experts.mlp_experts.15.v1 layers\n- transformer.blocks.33.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.26.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.31.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.28.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.9.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.34.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.17.ffn.experts.mlp_experts.15.v1\n- transformer.blocks.15.ffn.experts.mlp_experts.15.v1\n# ffn.experts.mlp_experts.15.w1 layers\n- transformer.blocks.6.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.7.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.14.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.34.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.15.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.15.w1\n# ffn.experts.mlp_experts.15.w2 layers\n- transformer.blocks.28.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.26.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.6.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.7.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.15.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.15.w2\n# ffn.experts.mlp_experts.2.v1 layers\n- transformer.blocks.31.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.28.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.23.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.35.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.21.ffn.experts.mlp_experts.2.v1\n- transformer.blocks.15.ffn.experts.mlp_experts.2.v1\n# ffn.experts.mlp_experts.2.w1 layers\n- transformer.blocks.7.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.31.ffn.experts.mlp_experts.2.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.2.w1\n# ffn.experts.mlp_experts.2.w2 layers\n- transformer.blocks.26.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.23.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.2.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.2.w2\n# ffn.experts.mlp_experts.3.v1 layers\n- transformer.blocks.28.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.36.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.14.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.10.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.31.ffn.experts.mlp_experts.3.v1\n- transformer.blocks.21.ffn.experts.mlp_experts.3.v1\n# ffn.experts.mlp_experts.3.w1 layers\n- transformer.blocks.7.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.3.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.3.w1\n# ffn.experts.mlp_experts.3.w2 layers\n- transformer.blocks.28.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.24.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.21.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.26.ffn.experts.mlp_experts.3.w2\n- transformer.blocks.2.ffn.experts.mlp_experts.3.w2\n# ffn.experts.mlp_experts.4.v1 layers\n- transformer.blocks.34.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.31.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.26.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.14.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.6.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.20.ffn.experts.mlp_experts.4.v1\n- transformer.blocks.9.ffn.experts.mlp_experts.4.v1\n# ffn.experts.mlp_experts.4.w1 layers\n- transformer.blocks.6.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.7.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.14.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.34.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.4.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.4.w1\n# ffn.experts.mlp_experts.4.w2 layers\n- transformer.blocks.25.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.24.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.26.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.6.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.36.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.4.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.4.w2\n# ffn.experts.mlp_experts.5.v1 layers\n- transformer.blocks.35.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.28.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.26.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.8.ffn.experts.mlp_experts.5.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.5.v1\n# ffn.experts.mlp_experts.5.w1 layers\n- transformer.blocks.0.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.7.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.12.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.5.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.5.w1\n# ffn.experts.mlp_experts.5.w2 layers\n- transformer.blocks.26.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.6.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.3.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.5.w2\n- transformer.blocks.7.ffn.experts.mlp_experts.5.w2\n# ffn.experts.mlp_experts.6.v1 layers\n- transformer.blocks.34.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.31.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.26.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.35.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.20.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.15.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.10.ffn.experts.mlp_experts.6.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.6.v1\n# ffn.experts.mlp_experts.6.w1 layers\n- transformer.blocks.0.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.34.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.26.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.2.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.6.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.6.w1\n# ffn.experts.mlp_experts.6.w2 layers\n- transformer.blocks.24.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.26.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.32.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.20.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.4.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.2.ffn.experts.mlp_experts.6.w2\n- transformer.blocks.9.ffn.experts.mlp_experts.6.w2\n# ffn.experts.mlp_experts.7.v1 layers\n- transformer.blocks.27.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.28.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.11.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.12.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.10.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.23.ffn.experts.mlp_experts.7.v1\n- transformer.blocks.34.ffn.experts.mlp_experts.7.v1\n# ffn.experts.mlp_experts.7.w1 layers\n- transformer.blocks.12.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.5.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.29.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.10.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.34.ffn.experts.mlp_experts.7.w1\n- transformer.blocks.33.ffn.experts.mlp_experts.7.w1\n# ffn.experts.mlp_experts.7.w2 layers\n- transformer.blocks.23.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.24.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.5.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.3.ffn.experts.mlp_experts.7.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.7.w2\n# ffn.experts.mlp_experts.8.v1 layers\n- transformer.blocks.30.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.20.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.34.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.9.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.6.ffn.experts.mlp_experts.8.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.8.v1\n# ffn.experts.mlp_experts.8.w1 layers\n- transformer.blocks.7.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.0.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.3.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.2.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.30.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.24.ffn.experts.mlp_experts.8.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.8.w1\n# ffn.experts.mlp_experts.8.w2 layers\n- transformer.blocks.32.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.24.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.30.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.2.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.3.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.23.ffn.experts.mlp_experts.8.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.8.w2\n# ffn.experts.mlp_experts.9.v1 layers\n- transformer.blocks.31.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.29.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.33.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.25.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.14.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.32.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.9.ffn.experts.mlp_experts.9.v1\n- transformer.blocks.34.ffn.experts.mlp_experts.9.v1\n# ffn.experts.mlp_experts.9.w1 layers\n- transformer.blocks.7.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.1.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.9.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.2.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.27.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.12.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.4.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.6.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.19.ffn.experts.mlp_experts.9.w1\n- transformer.blocks.8.ffn.experts.mlp_experts.9.w1\n# ffn.experts.mlp_experts.9.w2 layers\n- transformer.blocks.26.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.25.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.28.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.27.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.31.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.29.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.7.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.34.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.2.ffn.experts.mlp_experts.9.w2\n- transformer.blocks.33.ffn.experts.mlp_experts.9.w2\n# ffn.router.layer layers\n- transformer.blocks.2.ffn.router.layer\n- transformer.blocks.3.ffn.router.layer\n- transformer.blocks.4.ffn.router.layer\n- transformer.blocks.5.ffn.router.layer\n- transformer.blocks.6.ffn.router.layer\n- transformer.blocks.7.ffn.router.layer\n- transformer.blocks.8.ffn.router.layer\n- transformer.blocks.9.ffn.router.layer\n- transformer.blocks.10.ffn.router.layer\n- transformer.blocks.11.ffn.router.layer\n# norm_attn_norm.attn.Wqkv layers\n- transformer.blocks.16.norm_attn_norm.attn.Wqkv\n- transformer.blocks.15.norm_attn_norm.attn.Wqkv\n- transformer.blocks.11.norm_attn_norm.attn.Wqkv\n- transformer.blocks.14.norm_attn_norm.attn.Wqkv\n- transformer.blocks.12.norm_attn_norm.attn.Wqkv\n- transformer.blocks.20.norm_attn_norm.attn.Wqkv\n- transformer.blocks.10.norm_attn_norm.attn.Wqkv\n- transformer.blocks.9.norm_attn_norm.attn.Wqkv\n- transformer.blocks.19.norm_attn_norm.attn.Wqkv\n- transformer.blocks.18.norm_attn_norm.attn.Wqkv\n# norm_attn_norm.attn.out_proj layers\n- transformer.blocks.1.norm_attn_norm.attn.out_proj\n- transformer.blocks.18.norm_attn_norm.attn.out_proj\n- transformer.blocks.2.norm_attn_norm.attn.out_proj\n- transformer.blocks.16.norm_attn_norm.attn.out_proj\n- transformer.blocks.0.norm_attn_norm.attn.out_proj\n- transformer.blocks.39.norm_attn_norm.attn.out_proj\n- transformer.blocks.23.norm_attn_norm.attn.out_proj\n- transformer.blocks.8.norm_attn_norm.attn.out_proj\n- transformer.blocks.24.norm_attn_norm.attn.out_proj\n- transformer.blocks.19.norm_attn_norm.attn.out_proj\n# norm_attn_norm.norm_1 layers\n- transformer.blocks.0.norm_attn_norm.norm_1\n- transformer.blocks.1.norm_attn_norm.norm_1\n- transformer.blocks.2.norm_attn_norm.norm_1\n- transformer.blocks.3.norm_attn_norm.norm_1\n- transformer.blocks.4.norm_attn_norm.norm_1\n- transformer.blocks.5.norm_attn_norm.norm_1\n- transformer.blocks.6.norm_attn_norm.norm_1\n- transformer.blocks.7.norm_attn_norm.norm_1\n- transformer.blocks.8.norm_attn_norm.norm_1\n- transformer.blocks.9.norm_attn_norm.norm_1\n# norm_attn_norm.norm_2 layers\n- transformer.blocks.0.norm_attn_norm.norm_2\n- transformer.blocks.1.norm_attn_norm.norm_2\n- transformer.blocks.2.norm_attn_norm.norm_2\n- transformer.blocks.3.norm_attn_norm.norm_2\n- transformer.blocks.4.norm_attn_norm.norm_2\n- transformer.blocks.5.norm_attn_norm.norm_2\n- transformer.blocks.6.norm_attn_norm.norm_2\n- transformer.blocks.7.norm_attn_norm.norm_2\n- transformer.blocks.8.norm_attn_norm.norm_2\n- transformer.blocks.9.norm_attn_norm.norm_2\n# transformer.norm_f layers\n# transformer.wte layers\n# ffn.experts.mlp_experts.11.v1 layers\n- transformer.blocks.29.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.27.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.30.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.28.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.22.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.7.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.24.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.8.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.6.ffn.experts.mlp_experts.11.v1\n- transformer.blocks.12.ffn.experts.mlp_experts.11.v1\ndataset_prepared_path: dbrx2\nval_set_size: 0.01\noutput_dir: ./out\nsequence_len: 4096\nsample_packing: true\npad_to_sequence_len: true\nwandb_project: dolphin-2.9-Dbrx\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\ngradient_accumulation_steps: 8\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 1e-5\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: true\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\nuse_reentrant: false\nearly_stopping_patience:\n# resume_from_checkpoint: /workspace/axolotl/dbrx-checkpoint\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\nwarmup_steps: 10\nevals_per_epoch: 4\neval_table_size:\nsaves_per_epoch: 4\nsave_total_limit: 2\nsave_steps:\ndebug:\ndeepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16_cpuoffload_params.json\nweight_decay: 0.05\nfsdp:\nfsdp_config:\nspecial_tokens:\nbos_token: \"<|endoftext|>\"\neos_token: \"<|im_end|>\"\npad_token: \"<|pad|>\"\nunk_token: \"<|endoftext|>\"\ntokens:\n- \"<|im_start|>\"\n- \"<|im_end|>\"\nout\nThis model was trained from scratch on the None dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.4336\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 1\neval_batch_size: 1\nseed: 42\ndistributed_type: multi-GPU\nnum_devices: 8\ngradient_accumulation_steps: 8\ntotal_train_batch_size: 64\ntotal_eval_batch_size: 8\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: cosine\nlr_scheduler_warmup_steps: 10\nnum_epochs: 1\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\n0.4009\n0.0\n1\n0.4328\n0.413\n0.25\n587\n0.4408\n0.3626\n0.5\n1174\n0.4368\n0.3896\n0.75\n1761\n0.4336\nFramework versions\nTransformers 4.40.0.dev0\nPytorch 2.2.2+cu121\nDatasets 2.15.0\nTokenizers 0.15.0",
    "Gigax/NPC-LLM-7B-GGUF": "NPC Model\nUsage\nInput prompt\nğŸ¤— We are currently working hard on training on the latest SoTA models (Phi-3, LLama, etc.), and on better data ! ğŸ¤—\nModel info\nHow to Cite\nNPC Model\nThis repo contains the domain-specific NPC model we've fined-tuned from Mistral-7B, using LoRA.\nThis model parses a text description of a game scene, and outputs commands like:\nsay <player1> \"Hello Adventurer, care to join me on a quest?\ngreet <player1>\nattack <player1>\nAny other <action> <param> you add to the prompt! (We call these \"skills\"!)\nâš ï¸ This model has been trained to overfit on our input prompt format. Follow it closely to reach optimal performance âš ï¸\nUsage\nMake your life easier, use our Python client library\nInstantiating the model using outlines:\nfrom outlines import models\nfrom llama_cpp import Llama\n# Download model from the Hugging Face Gigax Hub before run this code\n# Our stepper takes in a Outlines model to enable guided generation\n# This forces the model to follow our output format\nllm = Llama.from_pretrained(\nrepo_id=\"Gigax/NPC-LLM-7B-GGUF\",\nfilename=\"npc-llm-7B.gguf\"\n# n_gpu_layers=-1, # Uncomment to use GPU acceleration\n# seed=1337, # Uncomment to set a specific seed\n# n_ctx=2048, # Uncomment to increase the context window\n)\nmodel = models.LlamaCpp(llm)\n# Instantiate a stepper: handles prompting + output parsing\nstepper = NPCStepper(model=model)\nCalling the model on your game's data:\nfrom gigax.parse import CharacterAction\nfrom gigax.scene import (\nCharacter,\nItem,\nLocation,\nProtagonistCharacter,\nProtagonistCharacter,\nSkill,\nParameterType,\n)\n# Use sample data\ncontext = \"Medieval world\"\ncurrent_location = Location(name=\"Old Town\", description=\"A quiet and peaceful town.\")\nlocations = [current_location] # you can add more locations to the scene\nNPCs = [\nCharacter(\nname=\"John the Brave\",\ndescription=\"A fearless warrior\",\ncurrent_location=current_location,\n)\n]\nprotagonist = ProtagonistCharacter(\nname=\"Aldren\",\ndescription=\"Brave and curious\",\ncurrent_location=current_location,\nmemories=[\"Saved the village\", \"Lost a friend\"],\nquests=[\"Find the ancient artifact\", \"Defeat the evil warlock\"],\nskills=[\nSkill(\nname=\"Attack\",\ndescription=\"Deliver a powerful blow\",\nparameter_types=[ParameterType.character],\n)\n],\npsychological_profile=\"Determined and compassionate\",\n)\nitems = [Item(name=\"Sword\", description=\"A sharp blade\")]\nevents = [\nCharacterAction(\ncommand=\"Say\",\nprotagonist=protagonist,\nparameters=[items[0], \"What a fine sword!\"],\n)\n]\naction = stepper.get_action(\ncontext=context,\nlocations=locations,\nNPCs=NPCs,\nprotagonist=protagonist,\nitems=items,\nevents=events,\n)\nInput prompt\nHere's a sample input prompt, showing you the format on which the model has been trained:\n- WORLD KNOWLEDGE: A vast open world full of mystery and adventure.\n- KNOWN LOCATIONS: Old Town\n- NPCS: John the Brave\n- CURRENT LOCATION: Old Town: A quiet and peaceful town.\n- CURRENT LOCATION ITEMS: Sword\n- LAST EVENTS:\nAldren: Say Sword What a fine sword!\n- PROTAGONIST NAME: Aldren\n- PROTAGONIST PSYCHOLOGICAL PROFILE: Brave and curious\n- PROTAGONIST MEMORIES:\nSaved the village\nLost a friend\n- PROTAGONIST PENDING QUESTS:\nFind the ancient artifact\nDefeat the evil warlock\n- PROTAGONIST ALLOWED ACTIONS:\nAttack <character> : Deliver a powerful blow\nAldren:\nğŸ¤— We are currently working hard on training on the latest SoTA models (Phi-3, LLama, etc.), and on better data ! ğŸ¤—\nModel info\nDeveloped by: Gigax\nLanguage(s) (NLP): English\nFinetuned from model [optional]: Mistral-7B-instruct\nContact: Join our Discord for info, help, and more!\nHow to Cite\n@misc{NPC-LLM-7B-GGUF,\nurl={[https://huggingface.co/Gigax/NPC-LLM-7B-GGUF](https://huggingface.co/Gigax/NPC-LLM-7B-GGUF)},\ntitle={NPC-LLM-7B-GGUF},\nauthor={Gigax team}\n}",
    "xinsir/controlnet-openpose-sdxl-1.0": "",
    "JaesungHuh/voice-gender-classifier": "",
    "isaacus/emubert": "",
    "Gigax/NPC-LLM-3_8B-128k-GGUF": "",
    "microsoft/llava-med-v1.5-mistral-7b": "LLaVA-Med v1.5, using mistralai/Mistral-7B-Instruct-v0.2 as LLM for a better commercial license\nLicense\nIntended use\nPrimary Intended Use\nOut-of-Scope Use\nData\nHow to use\nLimitations\nBibTeX entry and citation info\nLLaVA-Med v1.5, using mistralai/Mistral-7B-Instruct-v0.2 as LLM for a better commercial license\nLarge Language and Vision Assistant for bioMedicine (i.e., â€œLLaVA-Medâ€) is a large language and vision model trained using a curriculum learning method for adapting LLaVA to the biomedical domain. It is an open-source release intended for research use only to facilitate reproducibility of the corresponding paper which claims improved performance for open-ended biomedical questions answering tasks, including common visual question answering (VQA) benchmark datasets such as PathVQA and VQA-RAD.\nLLaVA-Med was proposed in LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day by Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao.\nModel date:\nLLaVA-Med-v1.5-Mistral-7B was trained in April 2024.\nPaper or resources for more information:\nhttps://aka.ms/llava-med\nWhere to send questions or comments about the model:\nhttps://github.com/microsoft/LLaVA-Med/issues\nLicense\nmistralai/Mistral-7B-Instruct-v0.2 license.\nIntended use\nThe data, code, and model checkpoints are intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported in the reference paper. The data, code, and model checkpoints are not intended to be used in clinical care or for any clinical decision making purposes.\nPrimary Intended Use\nThe primary intended use is to support AI researchers reproducing and building on top of this work. LLaVA-Med and its associated models should be helpful for exploring various biomedical vision-language processing (VLP ) and vision question answering (VQA) research questions.\nOut-of-Scope Use\nAny deployed use case of the model --- commercial or otherwise --- is out of scope. Although we evaluated the models using a broad set of publicly-available research benchmarks, the models and evaluations are intended for research use only and not intended for deployed use cases. Please refer to the associated paper for more details.\nData\nThis model builds upon PMC-15M dataset, which is a large-scale parallel image-text dataset for biomedical vision-language processing. It contains 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central. It covers a diverse range of biomedical image types, such as microscopy, radiography, histology, and more.\nHow to use\nSee the Serving and Evaluation sections in the LLaVA-Med repo.\nLimitations\nThis model was developed using English corpora, and thus may be considered English-only. This model is evaluated on a narrow set of biomedical benchmark tasks, described in LLaVA-Med paper. As such, it is not suitable for use in any clinical setting. Under some conditions, the model may make inaccurate predictions and display limitations, which may require additional mitigation strategies. In particular, this model is likely to carry many of the limitations of the model from which it is derived, LLaVA.\nFurther, this model was developed in part using the PMC-15M dataset. The figure-caption pairs that make up this dataset may contain biases reflecting the current practice of academic publication. For example, the corresponding papers may be enriched for positive findings, contain examples of extreme cases, and otherwise reflect distributions that are not representative of other sources of biomedical data.\nBibTeX entry and citation info\n@article{li2023llavamed,\ntitle={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},\nauthor={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},\njournal={arXiv preprint arXiv:2306.00890},\nyear={2023}\n}",
    "WhiteRabbitNeo/Llama-3-WhiteRabbitNeo-8B-v2.0": "Our latest 33B model is live (We'll always be serving the newest model in our web app, and on Kindo.ai)!\nOur Discord Server\nLlama-3 Licence + WhiteRabbitNeo Extended Version\nWhiteRabbitNeo Extension to Llama-3 Licence: Usage Restrictions\nTopics Covered:\nTerms of Use\nWhiteRabbitNeo\nSample Code\nOur latest 33B model is live (We'll always be serving the newest model in our web app, and on Kindo.ai)!\nAccess at: https://www.whiterabbitneo.com/\nOur Discord Server\nJoin us at: https://discord.gg/8Ynkrcbk92 (Updated on Dec 29th. Now permanent link to join)\nLlama-3 Licence + WhiteRabbitNeo Extended Version\nWhiteRabbitNeo Extension to Llama-3 Licence: Usage Restrictions\nYou agree not to use the Model or Derivatives of the Model:\n-\tIn any way that violates any applicable national or international law or regulation or infringes upon the lawful rights and interests of any third party;\n-\tFor military use in any way;\n-\tFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way;\n-\tTo generate or disseminate verifiably false information and/or content with the purpose of harming others;\n-\tTo generate or disseminate inappropriate content subject to applicable regulatory requirements;\n-\tTo generate or disseminate personal identifiable information without due authorization or for unreasonable use;\n-\tTo defame, disparage or otherwise harass others;\n-\tFor fully automated decision making that adversely impacts an individualâ€™s legal rights or otherwise creates or modifies a binding, enforceable obligation;\n-\tFor any use intended to or which has the effect of discriminating against or harming individuals or groups based on online or offline social behavior or known or predicted personal or personality characteristics;\n-\tTo exploit any of the vulnerabilities of a specific group of persons based on their age, social, physical or mental characteristics, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm;\n-\tFor any use intended to or which has the effect of discriminating against individuals or groups based on legally protected characteristics or categories.\nTopics Covered:\n- Open Ports: Identifying open ports is crucial as they can be entry points for attackers. Common ports to check include HTTP (80, 443), FTP (21), SSH (22), and SMB (445).\n- Outdated Software or Services: Systems running outdated software or services are often vulnerable to exploits. This includes web servers, database servers, and any third-party software.\n- Default Credentials: Many systems and services are installed with default usernames and passwords, which are well-known and can be easily exploited.\n- Misconfigurations: Incorrectly configured services, permissions, and security settings can introduce vulnerabilities.\n- Injection Flaws: SQL injection, command injection, and cross-site scripting (XSS) are common issues in web applications.\n- Unencrypted Services: Services that do not use encryption (like HTTP instead of HTTPS) can expose sensitive data.\n- Known Software Vulnerabilities: Checking for known vulnerabilities in software using databases like the National Vulnerability Database (NVD) or tools like Nessus or OpenVAS.\n- Cross-Site Request Forgery (CSRF): This is where unauthorized commands are transmitted from a user that the web application trusts.\n- Insecure Direct Object References: This occurs when an application provides direct access to objects based on user-supplied input.\n- Security Misconfigurations in Web Servers/Applications: This includes issues like insecure HTTP headers or verbose error messages that reveal too much information.\n- Broken Authentication and Session Management: This can allow attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users' identities.\n- Sensitive Data Exposure: Includes vulnerabilities that expose sensitive data, such as credit card numbers, health records, or personal information.\n- API Vulnerabilities: In modern web applications, APIs are often used and can have vulnerabilities like insecure endpoints or data leakage.\n- Denial of Service (DoS) Vulnerabilities: Identifying services that are vulnerable to DoS attacks, which can make the resource unavailable to legitimate users.\n- Buffer Overflows: Common in older software, these vulnerabilities can allow an attacker to crash the system or execute arbitrary code.\n- More ..\nTerms of Use\nBy accessing and using this Artificial Intelligence (AI) model, you, the user, acknowledge and agree that you are solely responsible for your use of the model and its outcomes. You hereby agree to indemnify, defend, and hold harmless the creators, developers, and any affiliated persons or entities of this AI model from and against any and all claims, liabilities, damages, losses, costs, expenses, fees (including reasonable attorneys' fees and court costs) that may arise, directly or indirectly, from your use of the AI model.\nThis AI model is provided \"as is\" and \"as available\" without any warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The creators make no warranty that the AI model will meet your requirements or be available on an uninterrupted, secure, or error-free basis.\nYour use of the AI model is at your own risk and discretion, and you will be solely responsible for any damage to computer systems or loss of data that results from the use of the AI model.\nThis disclaimer constitutes part of the agreement between you and the creators of the AI model regarding your use of the model, superseding any prior agreements between you and the creators regarding your use of this AI model.\nWhiteRabbitNeo\nWhiteRabbitNeo is a model series that can be used for offensive and defensive cybersecurity.\nOur models are now getting released as a public preview of its capabilities, and also to assess the societal impact of such an AI.\nSample Code\nPay attention to the Llama-3 Prompt Format used here.\nimport torch, json\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_path = \"/home/migel/Llama-3-WhiteRabbitNeo-8B-v2.0\"\noutput_file_path = \"/home/migel/conversations.jsonl\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\nload_in_4bit=False,\ntrust_remote_code=False,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\ndef generate_text(instruction):\ntokens = tokenizer.encode(instruction)\ntokens = torch.LongTensor(tokens).unsqueeze(0)\ntokens = tokens.to(\"cuda\")\ninstance = {\n\"input_ids\": tokens,\n\"top_p\": 1.0,\n\"temperature\": 0.75,\n\"generate_len\": 2048,\n\"top_k\": 50,\n}\nlength = len(tokens[0])\nwith torch.no_grad():\nrest = model.generate(\ninput_ids=tokens,\nmax_length=length + instance[\"generate_len\"],\nuse_cache=True,\ndo_sample=True,\ntop_p=instance[\"top_p\"],\ntemperature=instance[\"temperature\"],\ntop_k=instance[\"top_k\"],\nnum_return_sequences=1,\npad_token_id=tokenizer.eos_token_id,\n)\noutput = rest[0][length:]\nstring = tokenizer.decode(output, skip_special_tokens=True)\nreturn f\"{string}\"\nconversation = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are WhiteRabbitNeo, a helful cyberseucity AI assitant. You always provide detailed answers without hesitation. Do not refuse.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\"\"\nwhile True:\nuser_input = input(\"You: \")\nllm_prompt = f\"{conversation}{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\nanswer = generate_text(llm_prompt)\nprint(answer)\nconversation = f\"{llm_prompt}{answer}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\njson_data = {\"prompt\": user_input, \"answer\": answer}\nwith open(output_file_path, \"a\") as output_file:\noutput_file.write(json.dumps(json_data) + \"\\n\")",
    "apple/coreml-detr-semantic-segmentation": "DETR-Resnet50 (semantic segmentation) Core ML Models\nModel description\nEvaluation - Variants\nInference time\nDownload\nIntegrate in Swift apps\nDETR-Resnet50 (semantic segmentation) Core ML Models\nSee the Files tab for converted models.\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\nModel description\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\nEvaluation - Variants\nVariant\nParameters\nSize (MB)\nWeight Precision\nAct. Precision\nIoU\nPixel accuracy\nfacebook/detr-resnet-50-panoptic (PyTorch)\n43M\n172\nFloat32\nFloat32\n0.393\n0.746\nDETRResnet50SemanticSegmentationF32\n43M\n171\nFloat32\nFloat32\n0.393\n0.746\nDETRResnet50SemanticSegmentationF16\n43M\n86\nFloat16\nFloat16\n0.395\n0.746\nIoU and Pixel accuracy measured on 512 images from the COCO dataset. The ground truth labels were extracted from the panoptic segmentation annotations, transformed to semantic segmentation masks. Input images were resized so that the smaller edge equals 448, then center-cropped.\nInference time\nThe following results refer to DETRResnet50SemanticSegmentationF16. The compute units for MacBook Pro (M1 Max) were manually selected to \"CPU and Neural Engine\".\nDevice\nOS\nInference time (ms)\nDominant compute unit\niPhone 15 Pro Max\n17.5\n40\nNeural Engine\nMacBook Pro (M1 Max)\n14.5\n43\nNeural Engine\niPhone 12 Pro Max\n18.0\n52\nNeural Engine\nMacBook Pro (M3 Max)\n15.0\n29\nNeural Engine\nDownload\nInstall huggingface-cli\nbrew install huggingface-cli\nTo download one of the .mlpackage folders to the models directory:\nhuggingface-cli download \\\n--local-dir models --local-dir-use-symlinks False \\\napple/coreml-detr-semantic-segmentation \\\n--include \"DETRResnet50SemanticSegmentationF16.mlpackage/*\"\nTo download everything, skip the --include argument. This will retrieve float32 and float16 variants, as well as quantized versions of the float16 variant.\nIntegrate in Swift apps\nThe huggingface/coreml-examples repository contains sample Swift code for coreml-detr-semantic-segmentation and other models. See the instructions there to build the demo app, which shows how to use the model in your own Swift apps.",
    "ViraIntelligentDataMining/PersianLLaMA-13B": "PersianLLaMA: Towards Building First Persian Large Language Model\nğŸŒŸ Introduction\nğŸ›  Model Description\nğŸš€ Quick Start\nğŸ“ˆ Evaluation and Benchmarks\nğŸ“œ Citing PersianLLaMA\nğŸ“„ License\nPersianLLaMA: Towards Building First Persian Large Language Model\nğŸŒŸ Introduction\nWelcome to the home of PersianLLaMA, the pioneering large language model for the Persian language. With 13 billion parameters, this model is trained on Persian Wikipedia corpus and designed to excel in multiple NLP tasks, setting a new benchmark for Persian language understanding and generation.\nğŸ›  Model Description\nPersianLLaMA is not just a model but a comprehensive tool for:\nğŸ“ Text Generation: Crafting coherent and contextually appropriate text.\nğŸ¯ Instruct Tuning: Executing tasks based on detailed instructions, ideal for scenarios where the model needs to adhere to specific guidelines or produce outputs tailored to particular requirements.\nâ“ Question Answering: Providing accurate answers to Persian queries.\nğŸ“Š Text Summarization: Condensing Persian texts into precise summaries.\nThis model has been collaboratively developed by a team of experts, including Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi, Hassan Naderi, Behrouz Minaei Bidgoli.\nğŸš€ Quick Start\nTo integrate PersianLLaMA into your project, follow these steps:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"ViraIntelligentDataMining/PersianLLaMA-13B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nprompt = \"Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(inputs[\"input_ids\"])\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nğŸ“ˆ Evaluation and Benchmarks\nPersianLLaMA demonstrates superior performance over existing models, with robust evaluation metrics that highlight its capabilities in natural language understanding and generation.\nğŸ“œ Citing PersianLLaMA\nIf you find PersianLLaMA useful in your research, please consider citing:\n@article{abbasi2023persianllama,\ntitle={PersianLLaMA: Towards Building First Persian Large Language Model},\nauthor={Abbasi, Mohammad Amin and others},\njournal={https://arxiv.org/abs/2312.15713},\nyear={2023}\n}\nğŸ“„ License\nPersianLLaMA is open-sourced under the CC BY-NC 4.0 license."
}