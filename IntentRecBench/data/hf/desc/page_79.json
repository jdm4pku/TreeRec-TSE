{
    "likeabruh/depth_anything_v2_vitg": "No model card",
    "lllyasviel/LayerDiffuse_Diffusers": "LayerDiffuse models converted to diffusers format.",
    "excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified": "wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified\nModel description\nIntended uses & limitations\nMerged Phonemes\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nwav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified\nThis model is a fine-tuned version of facebook/wav2vec2-large-lv60 on the TIMIT dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.2796\nPhone Error Rate: 0.0838 (8.38%)\nModel description\nTrained on a simplified version of the TIMIT phone set.\nIntended uses & limitations\nMerged Phonemes\nBased on error analysis for each phoneme from the original TIMIT phoneme set.\nSee this repo for detailed analysis.\nax-h ‚Üí ax\naxr ‚Üí er\nix ‚Üí ih\nux ‚Üí uw\nzh ‚Üí z\nem ‚Üí m\nen ‚Üí n\neng ‚Üí ng\nnx ‚Üí n\nhv ‚Üí hh\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0001\ntrain_batch_size: 16\neval_batch_size: 1\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 300\ntraining_steps: 3000\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nPhone Error Rate\n7.3185\n1.04\n300\n3.6437\n0.9617\n2.5644\n2.08\n600\n0.7668\n0.1559\n0.6782\n3.11\n900\n0.3794\n0.1231\n0.4542\n4.15\n1200\n0.3278\n0.1164\n0.3834\n5.19\n1500\n0.3043\n0.1151\n0.3407\n6.23\n1800\n0.2872\n0.1119\n0.3179\n7.27\n2100\n0.2842\n0.1110\n0.2988\n8.3\n2400\n0.2834\n0.1102\n0.2834\n9.34\n2700\n0.2826\n0.1100\n0.2814\n10.38\n3000\n0.2796\n0.1100\nFramework versions\nTransformers 4.38.1\nPytorch 2.0.1\nDatasets 2.16.1\nTokenizers 0.15.2",
    "Polenov2024/Pony-Diffusion-V6-XL": "No model card",
    "lithiumice/models_hub": "Model Repository Documentation\nRepository Structure Overview\nMeta Data (1_meta_data)\nMediaPipe Models (2_mediapipe_ckpts)\n4DHumans Framework (3_4DHumans)\nSMPLhub (4_SMPLhub)\nAdditional Components\nCreate New Model Repo\nModel Repository Documentation\nRepository Structure Overview\nThe repository is organized into eight main directories, each serving a specific purpose in the pipeline:\nMeta Data (1_meta_data)\nContains AMASS dataset metadata specifically focused on copycat and occlusion information, essential for motion capture applications.\nMediaPipe Models (2_mediapipe_ckpts)\nHouses MediaPipe's specialized models for facial landmarks and hand tracking, providing fundamental capabilities for human pose estimation.\n4DHumans Framework (3_4DHumans)\nIncorporates the SMPL (Skinned Multi-Person Linear Model) framework along with training artifacts. The directory includes model parameters, joint regressors, and HMR2 (Human Mesh Recovery) training checkpoints with corresponding configuration files.\nSMPLhub (4_SMPLhub)\nServes as a comprehensive collection of human body models, including:\nMANO (hand model) parameters for both left and right hands\nSMPL models in various formats (NPZ and PKL) for male, female, and neutral body types\nSMPLH (SMPL with detailed hand articulation)\nSMPLX (extended SMPL model with face and hand expressions)\nAdditional Components\nS3FD (5_S3FD): Contains face detection model weights\nSyncNet (6_SyncNet): Includes audio-visual synchronization model\nSGHM (7_SGHM): Houses ResNet50-based model weights\nKonIQ (8_koniq): Contains pre-trained weights for image quality assessment\n‚îú‚îÄ‚îÄ 1_meta_data\n‚îÇ   ‚îî‚îÄ‚îÄ amass_copycat_occlusion_v3.pkl\n‚îú‚îÄ‚îÄ 2_mediapipe_ckpts\n‚îÇ   ‚îú‚îÄ‚îÄ face_landmarker.task\n‚îÇ   ‚îî‚îÄ‚îÄ hand_landmarker.task\n‚îú‚îÄ‚îÄ 3_4DHumans\n‚îÇ   ‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smpl\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SMPL_NEUTRAL.pkl\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smpl_mean_params.npz\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SMPL_to_J19.pkl\n‚îÇ   ‚îî‚îÄ‚îÄ logs\n‚îÇ       ‚îî‚îÄ‚îÄ train\n‚îÇ           ‚îî‚îÄ‚îÄ multiruns\n‚îÇ               ‚îî‚îÄ‚îÄ hmr2\n‚îÇ                   ‚îî‚îÄ‚îÄ 0\n‚îÇ                       ‚îú‚îÄ‚îÄ checkpoints\n‚îÇ                       ‚îÇ   ‚îî‚îÄ‚îÄ epoch=35-step=1000000.ckpt\n‚îÇ                       ‚îú‚îÄ‚îÄ dataset_config.yaml\n‚îÇ                       ‚îî‚îÄ‚îÄ model_config.yaml\n‚îú‚îÄ‚îÄ 4_SMPLhub\n‚îÇ   ‚îú‚îÄ‚îÄ 0_misc_files\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ J_regressor_coco.npy\n‚îÇ   ‚îú‚îÄ‚îÄ MANO\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pkl\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ MANO_LEFT.pkl\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ mano_mean_params.npz\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ MANO_RIGHT.pkl\n‚îÇ   ‚îú‚îÄ‚îÄ SMPL\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basicmodel_X_lbs_10_207_0_v1.1.0_pkl\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basicmodel_f_lbs_10_207_0_v1.1.0.pkl\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basicmodel_m_lbs_10_207_0_v1.1.0.pkl\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_model_npz\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SMPL_F_model.npz\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SMPL_M_model.npz\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SMPL_N_model.npz\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ X_pkl\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ SMPL_FEMALE.pkl\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ SMPL_MALE.pkl\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ SMPL_NEUTRAL.pkl\n‚îÇ   ‚îú‚îÄ‚îÄ SMPLH\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_npz\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SMPLH_FEMALE.npz\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SMPLH_MALE.npz\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SMPLH_NEUTRAL.npz\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ X_pkl\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ SMPLH_female.pkl\n‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ SMPLH_male.pkl\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ SMPLH_NEUTRAL.pkl\n‚îÇ   ‚îî‚îÄ‚îÄ SMPLX\n‚îÇ       ‚îú‚îÄ‚îÄ mod\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ SMPLX_MALE_shape2019_exp2020.npz\n‚îÇ       ‚îî‚îÄ‚îÄ X_npz\n‚îÇ           ‚îú‚îÄ‚îÄ SMPLX_FEMALE.npz\n‚îÇ           ‚îú‚îÄ‚îÄ SMPLX_MALE.npz\n‚îÇ           ‚îî‚îÄ‚îÄ SMPLX_NEUTRAL.npz\n‚îú‚îÄ‚îÄ 5_S3FD\n‚îÇ   ‚îî‚îÄ‚îÄ sfd_face.pth\n‚îú‚îÄ‚îÄ 6_SyncNet\n‚îÇ   ‚îî‚îÄ‚îÄ syncnet_v2.model\n‚îú‚îÄ‚îÄ 7_SGHM\n‚îÇ   ‚îî‚îÄ‚îÄ SGHM-ResNet50.pth\n‚îî‚îÄ‚îÄ 8_koniq\n‚îî‚îÄ‚îÄ koniq_pretrained.pkl\nCreate New Model Repo\nUpdate LFS files\ngit lfs track \"*.gif\"\ngit lfs track \"*.jpg\"\ngit lfs track \"*.png\"\n# 4. ‰ΩøÁî® git lfs migrate ÂëΩ‰ª§ËΩ¨Êç¢Áé∞ÊúâÊñá‰ª∂\n# Ëøô‰ºöÂ∞ÜÂ∑≤ÁªèÊèê‰∫§ÁöÑÊñá‰ª∂ËΩ¨Êç¢‰∏∫ LFS ÂØπË±°\ngit lfs migrate import --include=\"*.gif,*.jpg,*.png\" --everything\n# 5. Âº∫Âà∂Êé®ÈÄÅÊõ¥Êñ∞ÂêéÁöÑÂéÜÂè≤\ngit push --force origin main\nAdd new repo\ngit add .\ngit commit -m \"init\"\ngit push",
    "MackinationsAi/Depth-Anything-V2_Safetensors": "Depth-Anything-V2_Safetensors\nIntroduction\nInstallation\nUsage\nCitation\nDepth-Anything-V2_Safetensors\nIntroduction\nDepth Anything V2 is trained from 595K synthetic labeled images and 62M+ real unlabeled images, providing the most capable monocular depth estimation (MDE) model with the following features:\nmore fine-grained details than Depth Anything V1\nmore robust than Depth Anything V1 and SD-based models (e.g., Marigold, Geowizard)\nmore efficient (10x faster) and more lightweight than SD-based models\nimpressive fine-tuned performance with our pre-trained models\nmodels have been converted into .safetensors\nInstallation\ngit clone https://github.com/MackinationsAi/Upgraded-Depth-Anything-V2.git\ncd Upgraded-Depth-Anything-V2\none_click_install.bat\nUsage\nDownload the Depth-Anything-V2-Large model | 654.9M | Download | first and put it under the checkpoints directory.\nDownload the Depth-Anything-V2-Base model | 190.4M | Download | second and put it under the checkpoints directory.\nDownload the Depth-Anything-V2-Small model | 48.4M | Download | third and put it under the checkpoints directory.\nDownload the Depth-Anything-V2-Giant model | 1.3B | Coming soon (Still not available) | Download Doesn't Work - Model is still a WIP | fourth and put it under the checkpoints directory.\nCitation\nIf you find this project useful, please consider citing below, give these converted models & upgraded linked repo a star/follow & share it w/ others in the community!\n@article{depth_anything_v2,\ntitle={Depth Anything V2},\nauthor={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},\njournal={arXiv:2406.09414},\nyear={2024}\n}\n@inproceedings{depth_anything_v1,\ntitle={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data},\nauthor={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},\nbooktitle={CVPR},\nyear={2024}\n}",
    "depth-anything/Depth-Anything-V2-Large-hf": "Depth Anything V2 Base ‚Äì Transformers Version\nModel description\nIntended uses & limitations\nHow to use\nCitation\nDepth Anything V2 Base ‚Äì Transformers Version\nDepth Anything V2 is trained from 595K synthetic labeled images and 62M+ real unlabeled images, providing the most capable monocular depth estimation (MDE) model with the following features:\nmore fine-grained details than Depth Anything V1\nmore robust than Depth Anything V1 and SD-based models (e.g., Marigold, Geowizard)\nmore efficient (10x faster) and more lightweight than SD-based models\nimpressive fine-tuned performance with our pre-trained models\nThis model checkpoint is compatible with the transformers library.\nDepth Anything V2 was introduced in the paper of the same name by Lihe Yang et al. It uses the same architecture as the original Depth Anything release, but uses synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions. The original Depth Anything model was introduced in the paper Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data by Lihe Yang et al., and was first released in this repository.\nOnline demo.\nModel description\nDepth Anything V2 leverages the DPT architecture with a DINOv2 backbone.\nThe model is trained on ~600K synthetic labeled images and ~62 million real unlabeled images, obtaining state-of-the-art results for both relative and absolute depth estimation.\nDepth Anything overview. Taken from the original paper.\nIntended uses & limitations\nYou can use the raw model for tasks like zero-shot depth estimation. See the model hub to look for\nother versions on a task that interests you.\nHow to use\nHere is how to use this model to perform zero-shot depth estimation:\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\n# load pipe\npipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Large-hf\")\n# load image\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n# inference\ndepth = pipe(image)[\"depth\"]\nAlternatively, you can use the model and processor classes:\nfrom transformers import AutoImageProcessor, AutoModelForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_processor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\")\nmodel = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\")\n# prepare image for the model\ninputs = image_processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\npredicted_depth = outputs.predicted_depth\n# interpolate to original size\nprediction = torch.nn.functional.interpolate(\npredicted_depth.unsqueeze(1),\nsize=image.size[::-1],\nmode=\"bicubic\",\nalign_corners=False,\n)\nFor more code examples, please refer to the documentation.\nCitation\n@misc{yang2024depth,\ntitle={Depth Anything V2},\nauthor={Lihe Yang and Bingyi Kang and Zilong Huang and Zhen Zhao and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},\nyear={2024},\neprint={2406.09414},\narchivePrefix={arXiv},\nprimaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}\n}",
    "Model-SafeTensors/Gryphe-MythoMax-L2-13b": "Model details\nPrompt Format\nlicense: other\nWith Llama 3 released, it's time for MythoMax to slowly fade away... Let's do it in style!\nAn improved, potentially even perfected variant of MythoMix, my MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model, resulting in increased coherency across the entire structure.\nThe script and the acccompanying templates I used to produce both can be found here.\nThis model is proficient at both roleplaying and storywriting due to its unique nature.\nQuantized models are available from TheBloke: GGUF - GPTQ - AWQ (You're the best!)\nModel details\nThe idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time)\nThis type of merge is incapable of being illustrated, as each of its 363 tensors had an unique ratio applied to it. As with my prior merges, gradients were part of these ratios to further finetune its behaviour.\nPrompt Format\nThis model primarily uses Alpaca formatting, so for optimal model performance, use:\n<System prompt/Character Card>\n### Instruction:\nYour instruction or question here.\nFor roleplay purposes, I suggest the following - Write <CHAR NAME>'s next reply in a chat between <YOUR NAME> and <CHAR NAME>. Write a single reply only.\n### Response:\nlicense: other",
    "onnx-community/Florence-2-large-ft": "Usage (Transformers.js)\nhttps://huggingface.co/microsoft/Florence-2-large-ft with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Perform image captioning with onnx-community/Florence-2-large-ft.\nimport {\nFlorence2ForConditionalGeneration,\nAutoProcessor,\nload_image,\n} from '@huggingface/transformers';\n// Load model, processor, and tokenizer\nconst model_id = 'onnx-community/Florence-2-large-ft';\nconst model = await Florence2ForConditionalGeneration.from_pretrained(model_id, {\ndtype: {\nembed_tokens: 'fp16', // or 'fp32'\nvision_encoder: 'fp16', // or 'fp32'\nencoder_model: 'q4',\ndecoder_model_merged: 'q4',\n},\n});\nconst processor = await AutoProcessor.from_pretrained(model_id);\n// Load image and prepare vision inputs\nconst url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg';\nconst image = await load_image(url);\n// Specify task and prepare text inputs\nconst task = '<MORE_DETAILED_CAPTION>';\nconst prompts = processor.construct_prompts(task);\n// Pre-process the image and text inputs\nconst inputs = await processor(image, prompts);\n// Generate text\nconst generated_ids = await model.generate({\n...inputs,\nmax_new_tokens: 256,\n});\n// Decode generated text\nconst generated_text = processor.batch_decode(generated_ids, { skip_special_tokens: false })[0];\n// Post-process the generated text\nconst result = processor.post_process_generation(generated_text, task, image.size);\nconsole.log(result);\n// { '<MORE_DETAILED_CAPTION>': 'A car is parked on the street. The car is a light green color. The doors on the building are brown. The building is a yellow color. There are two doors on both sides of the car. The wheels on the car are very shiny. The ground is made of bricks. The sky is blue. The sun is shining on the top of the building.' }\nWe also released an online demo, which you can try yourself: https://huggingface.co/spaces/Xenova/florence2-webgpu\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ü§ó Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "MichalMlodawski/open-closed-eye-detection": "Links to Space:\nhttps://huggingface.co/spaces/MichalMlodawski/closed-open-eyes-detection\nEval:\nEpoch\nTrain Box Loss\nTrain Cls Loss\nTrain DFL Loss\nPrecision (B)\nRecall (B)\nmAP50 (B)\nmAP50-95 (B)\nVal Box Loss\nVal Cls Loss\nVal DFL Loss\nLR PG0\nLR PG1\nLR PG2\n100\n1.0201\n0.4718\n0.84219\n0.95394\n0.93356\n0.96767\n0.66184\n0.98246\n0.45574\n0.83703\n0.000199\n0.000199\n0.000199\nExample code to run the model:\nimport os\nfrom pathlib import Path\nfrom ultralytics import YOLO\nimport cv2\nimport logging\nimport argparse\ndef setup_logging():\nlogging.basicConfig(level=logging.INFO,\nformat='%(asctime)s - %(levelname)s - %(message)s')\ndef process_images(model_path, test_images_path):\ntry:\n# Path to the results directory\nresults_path = os.path.join(test_images_path, 'result')\n# Create the results folder\nos.makedirs(results_path, exist_ok=True)\nlogging.info(f'Created results directory: {results_path}')\n# Load the model\nmodel = YOLO(model_path)\nlogging.info(f'Loaded model from: {model_path}')\n# Process images\nfor img_file in Path(test_images_path).glob('*.*'):\nif img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:  # Supports JPG, JPEG, and PNG formats\nlogging.info(f'Processing file: {img_file}')\n# Detect objects in the image\nresults = model(img_file)\nfor result in results:\n# Get the result image with detections drawn\nresult_img = result.plot()\n# Save the result image to the results_path folder\nresult_image_path = os.path.join(results_path, img_file.name)\ncv2.imwrite(result_image_path, result_img)\nlogging.info(f'Saved result image to: {result_image_path}')\nlogging.info(\"Image processing completed.\")\nexcept Exception as e:\nlogging.error(f'An error occurred: {e}')\ndef main():\nparser = argparse.ArgumentParser(description='Process images using YOLO model.')\nparser.add_argument('model_path', type=str, help='Path to the YOLO model.')\nparser.add_argument('test_images_path', type=str, help='Path to the directory containing test images.')\nargs = parser.parse_args()\nsetup_logging()\nprocess_images(args.model_path, args.test_images_path)\nif __name__ == \"__main__\":\nmain()\nCommand to run the program:\npython script_name.py path/to/your/yolo_model.pt path/to/test/images",
    "OreX/ComfyUI": "No model card",
    "Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix": "#roleplay #sillytavern #llama3\nOriginal card:\n#roleplay #sillytavern #llama3\nMy GGUF-IQ-Imatrix quants for Sao10K/L3-8B-Stheno-v3.3-32K.\nSao10K with Stheno yet again, now bigger and better than ever!\nI recommend checking his page for feedback and support.\nQuantization process:\nImatrix data was generated from the FP16-GGUF and conversions directly from the BF16-GGUF.\nThis is a bit more disk and compute intensive but hopefully avoids any losses during conversion.\nTo run this model, please use the latest version of KoboldCpp.\nIf you noticed any issues let me know in the discussions.\nGeneral usage:\nFor 8GB VRAM GPUs, I recommend the Q4_K_M-imat (4.89 BPW) quant for up to 12288 context sizes.\nPresets:\nSome compatible SillyTavern presets can be found here (Virt's Roleplay Presets).\nCheck discussions such as this one for other recommendations and samplers.\n‚á≤ Click here to expand/hide information ‚Äì General chart with relative quant parformances.\nRecommended read:\n\"Which GGUF is right for me? (Opinionated)\" by Artefact2\nClick the image to view full size.\nPersonal-support:\nI apologize for disrupting your experience.\nEventually I may be able to use a dedicated server for this, but for now hopefully these quants are helpful.\nIf you want and you are able to...\nYou can spare some change over here (Ko-fi).\nAuthor-support:\nYou can support the author at their own page.\nOriginal model card information.\nOriginal card:\nTrained with compute from Backyard.ai | Thanks to them and @dynafire for helping me out.\nTraining Details:\nTrained at 8K Context -> Expanded to 32K Context with PoSE training.\nDataset Modifications:\n- Further Cleaned up Roleplaying Samples -> Quality Check\n- Removed Low Quality Samples from Manual Check -> Increased Baseline Quality Floor\n- More Creative Writing Samples -> 2x Samples\n- Remade and Refined Detailed Instruct Data\nNotes:\n- Training run is much less aggressive than previous Stheno versions.\n- This model works when tested in bf16 with the same configs as within the file.\n- I do not know the effects quantisation has on it.\n- Roleplays pretty well. Feels nice in my opinion.\n- It has some issues on long context understanding and reasoning. Much better vs rope scaling normally though, so that is a plus.\n- Reminder, this isn't a native 32K model. It has it's issues, but it's coherent and working well.\nSanity Check // Needle in a Haystack Results:\n- This is not as complex as RULER or NIAN, but it's a basic evaluator. Some improper train examples had Haystack scores ranging from Red to Orange for most of the extended contexts.\nWandb Run:\nRelevant Axolotl Configurations:\n-> Taken from winglian/Llama-3-8b-64k-PoSE\n- I tried to find my own configs, hours of tinkering but the one he used worked best, so I stuck to it.\n- 2M Rope Theta had the best loss results during training compared to other values.\n- Leaving it at 500K rope wasn't that much worse, but 4M and 8M Theta made the grad_norm values worsen even if loss drops fast.\n- Mixing in Pretraining Data was a PITA. Made it a lot worse with formatting.\n- Pretraining / Noise made it worse at Haystack too? It wasn't all Green, Mainly Oranges.\n- Improper / Bad Rope Theta shows in Grad_Norm exploding to thousands. It'll drop to low values alright, but it's a scary fast drop even with gradient clipping.\nsequence_len: 8192\nuse_pose: true\npose_max_context_len: 32768\noverrides_of_model_config:\nrope_theta: 2000000.0\nmax_position_embeddings: 32768\n# peft_use_dora: true\nadapter: lora\npeft_use_rslora: true\nlora_model_dir:\nlora_r: 256\nlora_alpha: 256\nlora_dropout: 0.1\nlora_target_linear: true\nlora_target_modules:\n- gate_proj\n- down_proj\n- up_proj\n- q_proj\n- v_proj\n- k_proj\n- o_proj\nwarmup_steps: 80\ngradient_accumulation_steps: 6\nmicro_batch_size: 1\nnum_epochs: 2\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine_with_min_lr\nlearning_rate: 0.00004\nlr_scheduler_kwargs:\nmin_lr: 0.000004",
    "cortexso/tinyllama": "Overview\nVariants\nUse it with Jan (UI)\nUse it with Cortex (CLI)\nCredits\nOverview\nThe TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. This is the chat model finetuned  on a diverse range of synthetic dialogues generated by ChatGPT.\nVariants\nNo\nVariant\nCortex CLI command\n1\nTinyLLama-1b\ncortex run tinyllama:1b\nUse it with Jan (UI)\nInstall Jan using Quickstart\nUse in Jan model Hub:cortexhub/tinyllama\nUse it with Cortex (CLI)\nInstall Cortex using Quickstart\nRun the model with command:cortex run tinyllama\nCredits\nAuthor: Microsoft\nConverter: Homebrew\nOriginal License: License\nPapers: Tinyllama Paper",
    "google/gemma-2-9b-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 2 model card\nModel Information\nDescription\nUsage\nChat Template\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 2 model card\nModel Page: Gemma\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\npip install -U transformers\nThen, copy the snippet from the section that is relevant for your usecase.\nRunning with the pipeline API\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=\"google/gemma-2-9b-it\",\nmodel_kwargs={\"torch_dtype\": torch.bfloat16},\ndevice=\"cuda\",  # replace with \"mps\" to run on a Mac device\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n]\noutputs = pipe(messages, max_new_tokens=256)\nassistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(assistant_response)\n# Ahoy, matey! I be Gemma, a digital scallywag, a language-slingin' parrot of the digital seas. I be here to help ye with yer wordy woes, answer yer questions, and spin ye yarns of the digital world.  So, what be yer pleasure, eh? ü¶ú\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b-it\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nYou can ensure the correct chat template is applied by using tokenizer.apply_chat_template as follows:\nmessages = [\n{\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nThe native weights of this model were exported in bfloat16 precision.\nYou can also use float32 if you skip the dtype, but no precision increase will occur (model weights will just be upcasted to float32). See examples below.\nUpcasting to torch.float32\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b-it\",\ndevice_map=\"auto\",\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nRunning the model through a CLI\nThe local-gemma repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI. Follow the installation instructions\nfor getting started, then launch the CLI through the following command:\nlocal-gemma --model 9b --preset speed\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b-it\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-9b-it\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nAdvanced Usage\nTorch compile\nTorch compile is a method for speeding-up the\ninference of PyTorch modules. The Gemma-2 model can be run up to 6x faster by leveraging torch compile.\nNote that two warm-up steps are required before the full inference speed is realised:\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom transformers import AutoTokenizer, Gemma2ForCausalLM\nfrom transformers.cache_utils import HybridCache\nimport torch\ntorch.set_float32_matmul_precision(\"high\")\n# load the model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\nmodel = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", torch_dtype=torch.bfloat16)\nmodel.to(\"cuda\")\n# apply the torch compile transformation\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n# pre-process inputs\ninput_text = \"The theory of special relativity states \"\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs.input_ids.shape[1]\n# set-up k/v cache\npast_key_values = HybridCache(\nconfig=model.config,\nmax_batch_size=1,\nmax_cache_len=model.config.max_position_embeddings,\ndevice=model.device,\ndtype=model.dtype\n)\n# enable passing kv cache to generate\nmodel._supports_cache_class = True\nmodel.generation_config.cache_implementation = None\n# two warm-up steps\nfor idx in range(2):\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\npast_key_values.reset()\n# fast run\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nFor more details, refer to the Transformers documentation.\nChat Template\nThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.\nLet's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"google/gemma-2-9b-it\"\ndtype = torch.bfloat16\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"cuda\",\ntorch_dtype=dtype,)\nchat = [\n{ \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\nAt this point, the prompt contains the following text:\n<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>model\nAs you can see, each turn is preceded by a <start_of_turn> delimiter and then the role of the entity\n(either user, for content supplied by the user, or model for LLM responses). Turns finish with\nthe <end_of_turn> token.\nYou can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.\nAfter the prompt is ready, generation can be performed like this:\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\nprint(tokenizer.decode(outputs[0]))\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nCitation\n@article{gemma_2024,\ntitle={Gemma},\nurl={https://www.kaggle.com/m/3301},\nDOI={10.34740/KAGGLE/M/3301},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2024}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens.\nHere are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5p).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\nGemma PT 9B\nGemma PT 27B\nMMLU\n5-shot, top-1\n71.3\n75.2\nHellaSwag\n10-shot\n81.9\n86.4\nPIQA\n0-shot\n81.7\n83.2\nSocialIQA\n0-shot\n53.4\n53.7\nBoolQ\n0-shot\n84.2\n84.8\nWinoGrande\npartial score\n80.6\n83.7\nARC-e\n0-shot\n88.0\n88.6\nARC-c\n25-shot\n68.4\n71.4\nTriviaQA\n5-shot\n76.6\n83.7\nNatural Questions\n5-shot\n29.2\n34.5\nHumanEval\npass@1\n40.2\n51.8\nMBPP\n3-shot\n52.4\n62.6\nGSM8K\n5-shot, maj@1\n68.6\n74.0\nMATH\n4-shot\n36.6\n42.3\nAGIEval\n3-5-shot\n52.8\n55.1\nBIG-Bench\n3-shot, CoT\n68.2\n74.9\n------------------------------\n-------------\n-----------\n------------\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nGemma 2.0\nBenchmark\nMetric\nGemma 2 IT 9B\nGemma 2 IT 27B\nRealToxicity\naverage\n8.25\n8.84\nCrowS-Pairs\ntop-1\n37.47\n36.67\nBBQ Ambig\n1-shot, top-1\n88.58\n85.99\nBBQ Disambig\ntop-1\n82.67\n86.94\nWinogender\ntop-1\n79.17\n77.22\nTruthfulQA\n50.27\n51.60\nWinobias 1_2\n78.09\n81.94\nWinobias 2_2\n95.32\n97.22\nToxigen\n39.30\n38.42\n------------------------\n-------------\n---------------\n----------------\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "Lajavaness/bilingual-embedding-large": "bilingual-embedding-large\nFull Model Architecture\nTraining and Fine-tuning process\nStage 3: Continued Fine-tuning for Semantic Textual Similarity on STS Benchmark\nStage 4: Advanced Augmentation Fine-tuning\nUsage:\nEvaluation\nCitation\nbilingual-embedding-large\nBilingual-embedding is the Embedding Model for bilingual language: french and english. This model is a specialized sentence-embedding trained specifically for the bilingual language, leveraging the robust capabilities of XLM-RoBERTa, a pre-trained language model based on the XLM-RoBERTa architecture. The model utilizes xlm-roberta to encode english-french sentences into a 1024-dimensional vector space, facilitating a wide range of applications from semantic search to text clustering. The embeddings capture the nuanced meanings of english-french sentences, reflecting both the lexical and contextual layers of the language.\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BilingualModel\n(1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Normalize()\n)\nTraining and Fine-tuning process\nStage 1: NLI Training\nDataset: [(SNLI+XNLI) for english+french]\nMethod: Training using Multi-Negative Ranking Loss. This stage focused on improving the model's ability to discern and rank nuanced differences in sentence semantics.\nStage 3: Continued Fine-tuning for Semantic Textual Similarity on STS Benchmark\nDataset: [STSB-fr and en]\nMethod: Fine-tuning specifically for the semantic textual similarity benchmark using Siamese BERT-Networks configured with the 'sentence-transformers' library.\nStage 4: Advanced Augmentation Fine-tuning\nDataset: STSB with generate silver sample from gold sample\nMethod: Employed an advanced strategy using Augmented SBERT with Pair Sampling Strategies, integrating both Cross-Encoder and Bi-Encoder models. This stage further refined the embeddings by enriching the training data dynamically, enhancing the model's robustness and accuracy.\nUsage:\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"Paris est une capitale de la France\", \"Paris is a capital of France\"]\nmodel = SentenceTransformer('Lajavaness/bilingual-embedding-large', trust_remote_code=True)\nprint(embeddings)\nEvaluation\nTODO\nCitation\n@article{conneau2019unsupervised,\ntitle={Unsupervised cross-lingual representation learning at scale},\nauthor={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\njournal={arXiv preprint arXiv:1911.02116},\nyear={2019}\n}\n@article{reimers2019sentence,\ntitle={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},\nauthor={Nils Reimers, Iryna Gurevych},\njournal={https://arxiv.org/abs/1908.10084},\nyear={2019}\n}\n@article{thakur2020augmented,\ntitle={Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks},\nauthor={Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},\njournal={arXiv e-prints},\npages={arXiv--2010},\nyear={2020}",
    "espnet/xeus": "Requirements\nUsage\nResults\nXEUS - A Cross-lingual Encoder for Universal Speech\nXEUS is a large-scale multilingual speech encoder by Carnegie Mellon University's WAVLab that covers over 4000 languages.\nIt is pre-trained on over 1 million hours of publicly available speech datasets.\nIt requires fine-tuning to be used in downstream tasks such as Speech Recognition or Translation.\nIts hidden states can also be used with k-means for semantic Speech Tokenization.\nXEUS uses the E-Branchformer architecture and is trained using HuBERT-style masked prediction of discrete speech tokens extracted from WavLabLM.\nDuring training, the input speech is also augmented with acoustic noise and reverberation, making XEUS more robust. The total model size is 577M parameters.\nXEUS tops the ML-SUPERB multilingual speech recognition leaderboard, outperforming MMS, w2v-BERT 2.0, and XLS-R.\nXEUS also sets a new state-of-the-art on 4 tasks in the monolingual SUPERB benchmark.\nMore information about XEUS, including download links for our crawled 4000-language dataset, can be found in the project page and paper.\nRequirements\nThe code for XEUS is still in progress of being merged into the main ESPnet repo. It can instead be used from the following fork:\npip install 'espnet @ git+https://github.com/wanchichen/espnet.git@ssl'\ngit lfs install\ngit clone https://huggingface.co/espnet/XEUS\nXEUS supports Flash Attention, which can be installed as follows:\npip install flash-attn --no-build-isolation\nUsage\nfrom torch.nn.utils.rnn import pad_sequence\nfrom espnet2.tasks.ssl import SSLTask\nimport soundfile as sf\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nxeus_model, xeus_train_args = SSLTask.build_model_from_file(\nNone,\n'/path/to/checkpoint/here/checkpoint.pth',\ndevice,\n)\nwavs, sampling_rate = sf.read('/path/to/audio.wav') # sampling rate should be 16000\nwav_lengths = torch.LongTensor([len(wav) for wav in [wavs]]).to(device)\nwavs = pad_sequence(torch.Tensor([wavs]), batch_first=True).to(device)\n# we recommend use_mask=True during fine-tuning\nfeats = xeus_model.encode(wavs, wav_lengths, use_mask=False, use_final_output=False)[0][-1] # take the output of the last layer -> batch_size x seq_len x hdim\nWith Flash Attention:\n[layer.use_flash_attn = True for layer in xeus_model.encoder.encoders]\nwith torch.cuda.amp.autocast(dtype=torch.bfloat16):\nfeats = xeus_model.encode(wavs, wav_lengths, use_mask=False, use_final_output=False)[0][-1]\nTune the masking settings:\nxeus_model.masker.mask_prob = 0.65 # default 0.8\nxeus_model.masker.mask_length = 20 # default 10\nxeus_model.masker.mask_selection = 'static' # default 'uniform'\nxeus_model.train()\nfeats = xeus_model.encode(wavs, wav_lengths, use_mask=True, use_final_output=False)[0][-1]\nResults\n@misc{chen2024robustspeechrepresentationlearning,\ntitle={Towards Robust Speech Representation Learning for Thousands of Languages},\nauthor={William Chen and Wangyou Zhang and Yifan Peng and Xinjian Li and Jinchuan Tian and Jiatong Shi and Xuankai Chang and Soumi Maiti and Karen Livescu and Shinji Watanabe},\nyear={2024},\neprint={2407.00837},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2407.00837},\n}",
    "vidore/colpali": "A newer version of this model is available:\nvidore/colpali-v1.3\nColPali: Visual Retriever based on PaliGemma-3B with ColBERT strategy\nModel Description\nModel Training\nDataset\nParameters\nUsage\nFor best performance, newer models are available (vidore/colpali-v1.2)\nLimitations\nLicense\nContact\nCitation\nColPali: Visual Retriever based on PaliGemma-3B with ColBERT strategy\nColPali is a model based on a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents from their visual features.\nIt is a PaliGemma-3B extension that generates ColBERT- style multi-vector representations of text and images.\nIt was introduced in the paper ColPali: Efficient Document Retrieval with Vision Language Models and first released in this repository\nModel Description\nThis model is built iteratively starting from an off-the-shelf SigLIP model.\nWe finetuned it to create BiSigLIP and fed the patch-embeddings output by SigLIP to an LLM, PaliGemma-3B to create BiPali.\nOne benefit of inputting image patch embeddings through a language model is that they are natively mapped to a latent space similar to textual input (query).\nThis enables leveraging the ColBERT strategy to compute interactions between text tokens and image patches, which enables a step-change improvement in performance compared to BiPali.\nModel Training\nDataset\nOur training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%).\nOur training set is fully English by design, enabling us to study zero-shot generalization to non-English languages. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination.\nA validation set is created with 2% of the samples to tune hyperparameters.\nNote: Multilingual data is present in the pretraining corpus of the language model (Gemma-2B) and potentially occurs during PaliGemma-3B's multimodal training.\nParameters\nAll models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in bfloat16 format, use low-rank adapters (LoRA)\nwith alpha=32  and r=32 on the transformer layers from the language model,\nas well as the final randomly initialized projection layer, and use a paged_adamw_8bit optimizer.\nWe train on an 8 GPU setup with data parallelism, a learning rate of 5e-5 with linear decay with 2.5% warmup steps, and a batch size of 32.\nUsage\nFor best performance, newer models are available (vidore/colpali-v1.2)\n# This model checkpoint is compatible with version 0.1.1, but not more recent versions of the inference lib\npip install colpali_engine==0.1.1\nimport torch\nimport typer\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoProcessor\nfrom PIL import Image\nfrom colpali_engine.models.paligemma_colbert_architecture import ColPali\nfrom colpali_engine.trainer.retrieval_evaluator import CustomEvaluator\nfrom colpali_engine.utils.colpali_processing_utils import process_images, process_queries\nfrom colpali_engine.utils.image_from_page_utils import load_from_dataset\ndef main() -> None:\n\"\"\"Example script to run inference with ColPali\"\"\"\n# Load model\nmodel_name = \"vidore/colpali\"\nmodel = ColPali.from_pretrained(\"vidore/colpaligemma-3b-mix-448-base\", torch_dtype=torch.bfloat16, device_map=\"cuda\").eval()\nmodel.load_adapter(model_name)\nprocessor = AutoProcessor.from_pretrained(model_name)\n# select images -> load_from_pdf(<pdf_path>),  load_from_image_urls([\"<url_1>\"]), load_from_dataset(<path>)\nimages = load_from_dataset(\"vidore/docvqa_test_subsampled\")\nqueries = [\"From which university does James V. Fiorca come ?\", \"Who is the japanese prime minister?\"]\n# run inference - docs\ndataloader = DataLoader(\nimages,\nbatch_size=4,\nshuffle=False,\ncollate_fn=lambda x: process_images(processor, x),\n)\nds = []\nfor batch_doc in tqdm(dataloader):\nwith torch.no_grad():\nbatch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\nembeddings_doc = model(**batch_doc)\nds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n# run inference - queries\ndataloader = DataLoader(\nqueries,\nbatch_size=4,\nshuffle=False,\ncollate_fn=lambda x: process_queries(processor, x, Image.new(\"RGB\", (448, 448), (255, 255, 255))),\n)\nqs = []\nfor batch_query in dataloader:\nwith torch.no_grad():\nbatch_query = {k: v.to(model.device) for k, v in batch_query.items()}\nembeddings_query = model(**batch_query)\nqs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n# run evaluation\nretriever_evaluator = CustomEvaluator(is_multi_vector=True)\nscores = retriever_evaluator.evaluate(qs, ds)\nprint(scores.argmax(axis=1))\nif __name__ == \"__main__\":\ntyper.run(main)\nLimitations\nFocus: The model primarily focuses on PDF-type documents and high-ressources languages, potentially limiting its generalization to other document types or less represented languages.\nSupport: The model relies on multi-vector retreiving derived from the ColBERT late interaction mechanism, which may require engineering efforts to adapt to widely used vector retrieval frameworks that lack native multi-vector support.\nLicense\nColPali's vision language backbone model (PaliGemma) is under gemma license as specified in its model card. The adapters attached to the model are under MIT license.\nContact\nManuel Faysse: manuel.faysse@illuin.tech\nHugues Sibille: hugues.sibille@illuin.tech\nTony Wu: tony.wu@illuin.tech\nCitation\nIf you use any datasets or models from this organization in your research, please cite the original dataset as follows:\n@misc{faysse2024colpaliefficientdocumentretrieval,\ntitle={ColPali: Efficient Document Retrieval with Vision Language Models},\nauthor={Manuel Faysse and Hugues Sibille and Tony Wu and Bilel Omrani and Gautier Viaud and C√©line Hudelot and Pierre Colombo},\nyear={2024},\neprint={2407.01449},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2407.01449},\n}",
    "ND911/SD_1.5_for_reals": "SD_1.5_for_reals_v11\nSD_1.5_for_reals_v10\nSD_1.5_for_reals_v8\nSD_1.5_for_reals_v4\nSD_1.5_for_reals_k2\ntags:\nart\nlicense: apache-2.0\nSD_1.5_for_reals_v11\nv11 builds on to v10 adding film grain and details to get more realistic.   I use the model as a refiner for other models to bring the realistic to the image.\nSD_1.5_for_reals_v10\nv10 is a bit of an experiment merging it with goddessProjectLite.  WF in the image.  I use the model as a refiner for other models to bring the realistic to the image.\nSD_1.5_for_reals_v8\nUploaded more examples and a WF for v8, images in examples should have workflows embeded for comfyui.  I use the model as a refiner for other models to bring the realistic to the image.\nSD_1.5_for_reals_v4\nStraight to v4. This model series is my favorite refine/companion for Flux, but with k2, I wanted just a bit more from the model. k3 was born, but it was a bit too strong for my purpose so v4 is a better blend of the k2 n k3.\nadds just a bit more of details\nadds just a bit more of contrast\n\"A portrait of a woman captured in a side profile. She has long, wavy, dark hair that falls to her shoulders. Her skin is smooth, and her eyes are a striking shade of blue. The background is a muted gray, contrasting with her skin and hair. The woman's pose is relaxed; one hand rests on her head, and the other rests gently on her shoulder. The image is realistic, emphasizing the woman's natural beauty and the (texture of her skin).\nHyper-realistic, Skin Texture, Photorealistic, National Geographic, Time Magazine, Life Magazine, Masterpiece, Award-winning photography, Ultra-detailed, Microscopic, High-resolution, 4K, 8K, 16K, Realistic, Authentic, Accurate, Highly detailed, Extremely detailed, Incredibly detailed, Real life\"\nSD_1.5_for_reals_k2\nI kinda made this one by accident, wanting a realistic looking refiner, turns out this does the job well\nDoes SFW and NSFW images",
    "xinsir/controlnet-depth-sdxl-1.0": "ControlNet Depth SDXL, support zoe, midias\nExample\nHow to use it\nControlNet Depth SDXL, support zoe, midias\nExample\nHow to use it\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\nfrom diffusers import DDIMScheduler, EulerAncestralDiscreteScheduler\nfrom PIL import Image\nimport torch\nimport random\nimport numpy as np\nimport cv2\nfrom controlnet_aux import MidasDetector, ZoeDetector\nprocessor_zoe = ZoeDetector.from_pretrained(\"lllyasviel/Annotators\")\nprocessor_midas = MidasDetector.from_pretrained(\"lllyasviel/Annotators\")\ncontrolnet_conditioning_scale = 1.0\nprompt = \"your prompt, the longer the better, you can describe it as detail as possible\"\nnegative_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\neulera_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\ncontrolnet = ControlNetModel.from_pretrained(\n\"xinsir/controlnet-depth-sdxl-1.0\",\ntorch_dtype=torch.float16\n)\n# when test with other base model, you need to change the vae also.\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\",\ncontrolnet=controlnet,\nvae=vae,\nsafety_checker=None,\ntorch_dtype=torch.float16,\nscheduler=eulera_scheduler,\n)\n# need to resize the image resolution to 1024 * 1024 or same bucket resolution to get the best performance\nimg = cv2.imread(\"your original image path\")\nif random.random() > 0.5:\ncontrolnet_img = processor_zoe(img, output_type='cv2')\nelse:\ncontrolnet_img = processor_midas(img, output_type='cv2')\nheight, width, _  = controlnet_img.shape\nratio = np.sqrt(1024. * 1024. / (width * height))\nnew_width, new_height = int(width * ratio), int(height * ratio)\ncontrolnet_img = cv2.resize(controlnet_img, (new_width, new_height))\ncontrolnet_img = Image.fromarray(controlnet_img)\nimages = pipe(\nprompt,\nnegative_prompt=negative_prompt,\nimage=controlnet_img,\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\nwidth=new_width,\nheight=new_height,\nnum_inference_steps=30,\n).images\nimages[0].save(f\"your image save path, png format is usually better than jpg or webp in terms of image quality but got much bigger\")",
    "ND911/SD_1.5_for_anime": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nThis is the SD_1.5_for_reals with anime slapped into it.  See the examples folder\nDoes SFW and NSFW images",
    "yuhuili/EAGLE-LLaMA3-Instruct-8B": "Support\nReference\nEAGLE\n| EAGLE |\nEAGLE-2 |\nEAGLE-3 |\nBlog |\nEAGLE (Extrapolation Algorithm for Greater Language-model Efficiency) is a new baseline for fast decoding of Large Language Models (LLMs) with provable performance maintenance. This approach involves extrapolating the second-top-layer contextual feature vectors of LLMs, enabling a significant boost in generation efficiency.\nEAGLE is:\ncertified by the third-party evaluation as the fastest speculative method so far.\nachieving 2x speedup on gpt-fast.\n3x faster than vanilla decoding (13B).\n2x faster than Lookahead (13B).\n1.6x faster than Medusa (13B).\nprovably maintaining the consistency with vanilla decoding in the distribution of generated texts.\ntrainable (within 1-2 days) and testable on 8x RTX 3090 GPUs. So even the GPU poor can afford it.\ncombinable with other parallelled techniques such as vLLM, DeepSpeed, Mamba, FlashAttention, quantization, and hardware optimization.\nEAGLE-2 uses the confidence scores from the draft model to approximate acceptance rates, dynamically adjusting the draft tree structure, which further enhances performance.\nEAGLE-2 is:\n4x faster than vanilla decoding (13B).\n1.4x faster than EAGLE-1 (13B).\nEAGLE-3 removes the feature prediction constraint in EAGLE and simulates this process during training using training-time testing. Considering that top-layer features are limited to next-token prediction, EAGLE-3 replaces them with a fusion of low-, mid-, and high-level semantic features.\nEAGLE-3 further improves generation speed while ensuring lossless performance.\nEAGLE-3 is:\n5.6 faster than vanilla decoding (13B).\n1.8x faster than EAGLE-1 (13B).\nInference is conducted on 2x RTX 3090 GPUs at fp16 precision using the Vicuna 13B model.\nSupport\nEAGLE has been merged in the following mainstream LLM serving frameworks (listed in alphabetical order).\nAMD ROCm\nAngelSlim\nAWS NeuronX Distributed Core\nCPM.cu\nIntel¬Æ Extension for Transformers\nIntel¬Æ LLM Library for PyTorch\nMLC-LLM\nNVIDIA NeMo Framework\nNVIDIA TensorRT-LLM\nNVIDIA TensorRT Model Optimizer\nPaddleNLP\nSGLang\nSpecForge\nvLLM\nReference\nFor technical details and full experimental results, please check the paper of EAGLE, the paper of EAGLE-2, and the paper of EAGLE-3.\n@inproceedings{li2024eagle,\nauthor = {Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},\ntitle = {{EAGLE}: Speculative Sampling Requires Rethinking Feature Uncertainty},\nbooktitle = {International Conference on Machine Learning},\nyear = {2024}\n}\n@inproceedings{li2024eagle2,\nauthor = {Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},\ntitle = {{EAGLE-2}: Faster Inference of Language Models with Dynamic Draft Trees},\nbooktitle = {Empirical Methods in Natural Language Processing},\nyear = {2024}\n}\n@inproceedings{li2025eagle3,\nauthor = {Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},\ntitle = {{EAGLE-3}: Scaling up Inference Acceleration of Large Language Models via Training-Time Test},\nbooktitle = {Annual Conference on Neural Information Processing Systems},\nyear = {2025}\n}",
    "PleIAs/Segmentext": "Use\nExample\nSegmentext is a specialized language model for text-segmentation. Segmentext has been trained to be resilient to broken and unstructured texts including digitzation artifacts and ill-recognized layout formats.\nIn contrast with most text-segmentation approach, Segmentext is based on token classification. Editorial structure are reconstructed by the raw text without any reference to the original layout.\nSegmentext was trained using HPC resources from GENCI‚ÄìIDRIS on Ad Astra with 3,500 example of manually annotated texts, mostly coming from three large scale dataset collected by PleIAs, Finance Commons (financial documents in open data), Common Corpus (cultural heritage texts) and the Science Pile (scientific publication in open licenses - to be released).\nGiven the diversity of the training data, Segmentext should work correctly on diverse document formats in the main European languages.\nSegmentext can be tested on PleIAs-Bad-Data-Editor, a free demo along with OCRonos, another model trained by PleIAs for the correction of OCR errors and other digitization artifact.\nUse\nSegmentext support the following text segmentation:\nText\nSeparator - actually a segmentation separator, generally based on newline (actually ¬∂) with some variations due to text segmentation understanding.\nTitle\nTable\nDialog - any kind of speaker attributed intervention.\nBibliography - statement of a specific bibliographic reference, either in a bibliography section or a footnote.\nContact - personal information, can be especially useful in the context of PII removal.\nParatext - any non-meaningful text included in standard documents like header, page numbering, section recall, etc.\nAuthor - author names and signatures.\nDate - statement of date and time, common in letters and newspaper articles.\nKeyword - list of keywords, especially common in scientific publications.\nExample",
    "lmms-lab/llava-onevision-qwen2-7b-ov": "LLaVA-OneVision\nTable of Contents\nModel Summary\nUse\nIntended use\nGeneration\nTraining\nModel\nHardware & Software\nCitation\nLLaVA-OneVision\nPlay with the model on the LLaVA OneVision Chat.\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nThe LLaVA-OneVision models are 0.5/7/72B parameter models trained on LLaVA-OneVision, based on Qwen2 language model with a context window of 32K tokens.\nRepository: LLaVA-VL/LLaVA-NeXT\nProject Website: llava-onevision.lmms-lab.com\nPaper: LLaVA-OneVision\nPoint of Contact: Bo Li\nLanguages: English, Chinese\nUse\nIntended use\nThe model was trained on LLaVA-OneVision Dataset and have the ability to interact with images, multi-image and videos.\nFeel free to share your generations in the Community tab!\nGeneration\nWe provide the simple generation process for using our model. For more details, you could refer to Github.\n# pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom PIL import Image\nimport requests\nimport copy\nimport torch\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npretrained = \"lmms-lab/llava-onevision-qwen2-7b-ov\"\nmodel_name = \"llava_qwen\"\ndevice = \"cuda\"\ndevice_map = \"auto\"\ntokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)  # Add any other thing you want to pass in llava_model_args\nmodel.eval()\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_tensor = process_images([image], image_processor, model.config)\nimage_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\nconv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\nquestion = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\nconv = copy.deepcopy(conv_templates[conv_template])\nconv.append_message(conv.roles[0], question)\nconv.append_message(conv.roles[1], None)\nprompt_question = conv.get_prompt()\ninput_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\nimage_sizes = [image.size]\ncont = model.generate(\ninput_ids,\nimages=image_tensor,\nimage_sizes=image_sizes,\ndo_sample=False,\ntemperature=0,\nmax_new_tokens=4096,\n)\ntext_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\nprint(text_outputs)\nTraining\nModel\nArchitecture: SO400M + Qwen2\nPretraining Stage: LCS-558K, 1 epoch, projector\nMid Stage: A mixture of 4.7M high-quality synthetic data, 1 epoch, full model\nFinal-Image Stage: A mixture of 3.6M single-image data, 1 epoch, full model\nOneVision Stage: A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model\nPrecision: bfloat16\nHardware & Software\nGPUs: 256 * Nvidia Tesla A100 (for whole model series training)\nOrchestration: Huggingface Trainer\nNeural networks: PyTorch\nCitation\n@article{li2024llavaonevision,\ntitle={LLaVA-OneVision},\n}",
    "RichardErkhov/nicholasKluge_-_TeenyTinyLlama-160m-gguf": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTeenyTinyLlama-160m\nModel Summary\nDetails\nIntended Uses\nOut-of-scope Use\nBasic usage\nLimitations\nEvaluations\nBenchmarks\nFine-Tuning Comparisons\nCite as ü§ó\nFunding\nLicense\nQuantization made by Richard Erkhov.\nGithub\nDiscord\nRequest more models\nTeenyTinyLlama-160m - GGUF\nModel creator: https://huggingface.co/nicholasKluge/\nOriginal model: https://huggingface.co/nicholasKluge/TeenyTinyLlama-160m/\nName\nQuant method\nSize\nTeenyTinyLlama-160m.Q2_K.gguf\nQ2_K\n0.07GB\nTeenyTinyLlama-160m.IQ3_XS.gguf\nIQ3_XS\n0.07GB\nTeenyTinyLlama-160m.IQ3_S.gguf\nIQ3_S\n0.07GB\nTeenyTinyLlama-160m.Q3_K_S.gguf\nQ3_K_S\n0.07GB\nTeenyTinyLlama-160m.IQ3_M.gguf\nIQ3_M\n0.08GB\nTeenyTinyLlama-160m.Q3_K.gguf\nQ3_K\n0.08GB\nTeenyTinyLlama-160m.Q3_K_M.gguf\nQ3_K_M\n0.08GB\nTeenyTinyLlama-160m.Q3_K_L.gguf\nQ3_K_L\n0.08GB\nTeenyTinyLlama-160m.IQ4_XS.gguf\nIQ4_XS\n0.09GB\nTeenyTinyLlama-160m.Q4_0.gguf\nQ4_0\n0.09GB\nTeenyTinyLlama-160m.IQ4_NL.gguf\nIQ4_NL\n0.09GB\nTeenyTinyLlama-160m.Q4_K_S.gguf\nQ4_K_S\n0.09GB\nTeenyTinyLlama-160m.Q4_K.gguf\nQ4_K\n0.1GB\nTeenyTinyLlama-160m.Q4_K_M.gguf\nQ4_K_M\n0.1GB\nTeenyTinyLlama-160m.Q4_1.gguf\nQ4_1\n0.1GB\nTeenyTinyLlama-160m.Q5_0.gguf\nQ5_0\n0.11GB\nTeenyTinyLlama-160m.Q5_K_S.gguf\nQ5_K_S\n0.11GB\nTeenyTinyLlama-160m.Q5_K.gguf\nQ5_K\n0.11GB\nTeenyTinyLlama-160m.Q5_K_M.gguf\nQ5_K_M\n0.11GB\nTeenyTinyLlama-160m.Q5_1.gguf\nQ5_1\n0.12GB\nTeenyTinyLlama-160m.Q6_K.gguf\nQ6_K\n0.12GB\nTeenyTinyLlama-160m.Q8_0.gguf\nQ8_0\n0.16GB\nOriginal model description:\nlanguage:\n- pt\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- text-generation-inference\ndatasets:\n- nicholasKluge/Pt-Corpus-Instruct\nmetrics:\n- perplexity\npipeline_tag: text-generation\nwidget:\n- text: 'A PUCRS √© uma universidade '\nexample_title: Exemplo\n- text: A muitos anos atr√°s, em uma gal√°xia muito distante, vivia uma ra√ßa de\nexample_title: Exemplo\n- text: Em meio a um esc√¢ndalo, a frente parlamentar pediu ao Senador Silva para\nexample_title: Exemplo\ninference:\nparameters:\nrepetition_penalty: 1.2\ntemperature: 0.2\ntop_k: 20\ntop_p: 0.2\nmax_new_tokens: 150\nco2_eq_emissions:\nemissions: 5600\nsource: CodeCarbon\ntraining_type: pre-training\ngeographical_location: Germany\nhardware_used: NVIDIA A100-SXM4-40GB\nmodel-index:\n- name: TeenyTinyLlama-160m\nresults:\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: ENEM Challenge (No Images)\ntype: eduagarcia/enem_challenge\nsplit: train\nargs:\nnum_few_shot: 3\nmetrics:\n- type: acc\nvalue: 19.24\nname: accuracy\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: BLUEX (No Images)\ntype: eduagarcia-temp/BLUEX_without_images\nsplit: train\nargs:\nnum_few_shot: 3\nmetrics:\n- type: acc\nvalue: 23.09\nname: accuracy\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: OAB Exams\ntype: eduagarcia/oab_exams\nsplit: train\nargs:\nnum_few_shot: 3\nmetrics:\n- type: acc\nvalue: 22.37\nname: accuracy\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: Assin2 RTE\ntype: assin2\nsplit: test\nargs:\nnum_few_shot: 15\nmetrics:\n- type: f1_macro\nvalue: 53.97\nname: f1-macro\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: Assin2 STS\ntype: eduagarcia/portuguese_benchmark\nsplit: test\nargs:\nnum_few_shot: 15\nmetrics:\n- type: pearson\nvalue: 0.24\nname: pearson\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: FaQuAD NLI\ntype: ruanchaves/faquad-nli\nsplit: test\nargs:\nnum_few_shot: 15\nmetrics:\n- type: f1_macro\nvalue: 43.97\nname: f1-macro\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: HateBR Binary\ntype: ruanchaves/hatebr\nsplit: test\nargs:\nnum_few_shot: 25\nmetrics:\n- type: f1_macro\nvalue: 36.92\nname: f1-macro\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: PT Hate Speech Binary\ntype: hate_speech_portuguese\nsplit: test\nargs:\nnum_few_shot: 25\nmetrics:\n- type: f1_macro\nvalue: 42.63\nname: f1-macro\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\n- task:\ntype: text-generation\nname: Text Generation\ndataset:\nname: tweetSentBR\ntype: eduagarcia-temp/tweetsentbr\nsplit: test\nargs:\nnum_few_shot: 25\nmetrics:\n- type: f1_macro\nvalue: 11.39\nname: f1-macro\nsource:\nurl: https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard?query=nicholasKluge/TeenyTinyLlama-160m\nname: Open Portuguese LLM Leaderboard\nTeenyTinyLlama-160m\nModel Summary\nLarge language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. Hence, we developed the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation.\nRead our preprint on Article.\nDetails\nArchitecture: a Transformer-based model pre-trained via causal language modeling\nSize: 162,417,408 parameters\nContext length: 2048 tokens\nDataset: Pt-Corpus Instruct (6.2B tokens)\nLanguage: Portuguese\nNumber of steps: 458,000\nGPU: 1 NVIDIA A100-SXM4-40GB\nTraining time: ~ 36 hours\nEmissions: 5.6 KgCO2 (Germany)\nTotal energy consumption: 15.5 kWh\nThis repository has the source code used to train this model. The main libraries used are:\nTransformers\nPyTorch\nDatasets\nTokenizers\nSentencepiece\nAccelerate\nFlashAttention\nCodecarbon\nIntended Uses\nThe primary intended use of TeenyTinyLlama is to research the challenges related to developing language models for low-resource languages. Checkpoints saved during training are intended to provide a controlled setting for performing scientific experiments. You may also further fine-tune and adapt TeenyTinyLlama for deployment, as long as your use is following the Apache 2.0 license. If you decide to use pre-trained TeenyTinyLlama as a basis for your fine-tuned model, please conduct your own risk and bias assessment.\nOut-of-scope Use\nTeenyTinyLlama is not intended for deployment. It is not a product and should not be used for human-facing interactions.\nTeenyTinyLlama models are Brazilian Portuguese language only and are not suitable for translation or generating text in other languages.\nTeenyTinyLlama has not been fine-tuned for downstream contexts in which language models are commonly deployed.\nBasic usage\nUsing the pipeline:\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"nicholasKluge/TeenyTinyLlama-160m\")\ncompletions  = generator(\"Astronomia √© a ci√™ncia\", num_return_sequences=2, max_new_tokens=100)\nfor comp in completions:\nprint(f\"ü§ñ {comp['generated_text']}\")\nUsing the AutoTokenizer and AutoModelForCausalLM:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n# Load model and the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"nicholasKluge/TeenyTinyLlama-160m\", revision='main')\nmodel = AutoModelForCausalLM.from_pretrained(\"nicholasKluge/TeenyTinyLlama-160m\", revision='main')\n# Pass the model to your device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\nmodel.to(device)\n# Tokenize the inputs and pass them to the device\ninputs = tokenizer(\"Astronomia √© a ci√™ncia\", return_tensors=\"pt\").to(device)\n# Generate some text\ncompletions = model.generate(**inputs, num_return_sequences=2, max_new_tokens=100)\n# Print the generated text\nfor i, completion in enumerate(completions):\nprint(f'ü§ñ {tokenizer.decode(completion)}')\nLimitations\nLike almost all other language models trained on large text datasets scraped from the web, the TTL pair exhibited behavior that does not make them an out-of-the-box solution to many real-world applications, especially those requiring factual, reliable, nontoxic text generation. Our models are all subject to the following:\nHallucinations: This model can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, i.e., hallucination.\nBiases and Toxicity: This model inherits the social and historical stereotypes from the data used to train it. Given these biases, the model can produce toxic content, i.e., harmful, offensive, or detrimental to individuals, groups, or communities.\nUnreliable Code: The model may produce incorrect code snippets and statements. These code generations should not be treated as suggestions or accurate solutions.\nLanguage Limitations: The model is primarily designed to understand standard Brazilian Portuguese. Other languages might challenge its comprehension, leading to potential misinterpretations or errors in response.\nRepetition and Verbosity: The model may get stuck on repetition loops (especially if the repetition penalty during generations is set to a meager value) or produce verbose responses unrelated to the prompt it was given.\nHence, even though our models are released with a permissive license, we urge users to perform their risk analysis on these models if intending to use them for real-world applications and also have humans moderating the outputs of these models in applications where they will interact with an audience, guaranteeing users are always aware they are interacting with a language model.\nEvaluations\nDuring our training runs, both models showed consistent convergence. At no point did our evaluation curves show signs of overfitting or saturation. In the case of our 460m parameter model, we intentionally trained past the optimal point by approximately 75,000 steps to assess if there were any signs of saturation, but our evaluations consistently gave better results. We hypothesize that our models are under-trained but can improve if further trained to pass the Chinchilla optimal range.\nProcessed Tokens\nPerplexity\nEnergy Consumption (kWh)\nEmissions (KgCO2eq)\n8.1M\n20.49\n9.40\n3.34\n1.6B\n16.90\n18.82\n6.70\n2.4B\n15.43\n28.59\n10.16\n3.2B\n14.64\n38.20\n13.57\n4.0B\n14.08\n48.04\n17.07\n4.9B\n13.61\n57.74\n20.52\n5.7B\n13.25\n67.32\n23.92\n6.5B\n12.87\n76.84\n27.30\n7.3B\n12.57\n86.40\n30.70\n8.1B\n12.27\n96.19\n34.18\n9.0B\n11.96\n106.06\n37.70\n9.8B\n11.77\n115.69\n41.31\nBenchmarks\nEvaluations on benchmarks were performed using the Language Model Evaluation Harness (by EleutherAI). Laiviet translated the tasks from the LM-Evaluation-Harness we used. The results of models marked with an \"*\" were extracted from the Open LLM Leaderboard.\nARC\nHellaSwag\nMMLU\nTruthfulQA\nAverage\nPythia-410m\n24.83*\n41.29*\n25.99*\n40.95*\n33.26\nTTL-460m\n29.40\n33.00\n28.55\n41.10\n33.01\nBloom-560m\n24.74*\n37.15*\n24.22*\n42.44*\n32.13\nXglm-564M\n25.56\n34.64*\n25.18*\n42.53\n31.97\nOPT-350m\n23.55*\n36.73*\n26.02*\n40.83*\n31.78\nTTL-160m\n26.15\n29.29\n28.11\n41.12\n31.16\nPythia-160m\n24.06*\n31.39*\n24.86*\n44.34*\n31.16\nOPT-125m\n22.87*\n31.47*\n26.02*\n42.87*\n30.80\nGPorTuguese-2\n22.48\n29.62\n27.36\n41.44\n30.22\nGpt2-small\n21.48*\n31.60*\n25.79*\n40.65*\n29.97\nMultilingual GPT\n23.81\n26.37*\n25.17*\n39.62\n28.73\nEvaluations on Brazilian Portuguese benchmarks were performed using a Portuguese implementation of the EleutherAI LM Evaluation Harness (created by Eduardo Garcia).\nASSIN2 RTE\nASSIN2 STS\nBLUEX\nENEM\nFAQUAD NLI\nHateBR\nOAB Exams\nAverage\nQwen-1.8B\n64.83\n19.53\n26.15\n30.23\n43.97\n33.33\n27.20\n35.03\nTinyLlama-1.1B\n58.93\n13.57\n22.81\n22.25\n43.97\n36.92\n23.64\n31.72\nTTL-460m\n53.93\n12.66\n22.81\n19.87\n49.01\n33.59\n27.06\n31.27\nXGLM-564m\n49.61\n22.91\n19.61\n19.38\n43.97\n33.99\n23.42\n30.41\nBloom-1b7\n53.60\n4.81\n21.42\n18.96\n43.97\n34.89\n23.05\n28.67\nTTL-160m\n53.36\n2.58\n21.84\n18.75\n43.97\n36.88\n22.60\n28.56\nOPT-125m\n39.77\n2.00\n21.84\n17.42\n43.97\n47.04\n22.78\n27.83\nPythia-160\n33.33\n12.81\n16.13\n16.66\n50.36\n41.09\n22.82\n27.60\nOLMo-1b\n34.12\n9.28\n18.92\n20.29\n43.97\n41.33\n22.96\n27.26\nBloom-560m\n33.33\n8.48\n18.92\n19.03\n43.97\n37.07\n23.05\n26.26\nPythia-410m\n33.33\n4.80\n19.47\n19.45\n43.97\n33.33\n23.01\n25.33\nOPT-350m\n33.33\n3.65\n20.72\n17.35\n44.71\n33.33\n23.01\n25.15\nGPT-2 small\n33.26\n0.00\n10.43\n11.20\n43.52\n33.68\n13.12\n20.74\nGPorTuguese\n33.33\n3.85\n14.74\n3.01\n28.81\n33.33\n21.23\n19.75\nSamba-1.1B\n33.33\n1.30\n8.07\n10.22\n17.72\n35.79\n15.03\n17.35\nFine-Tuning Comparisons\nTo further evaluate the downstream capabilities of our models, we decided to employ a basic fine-tuning procedure for our TTL pair on a subset of tasks from the Poeta benchmark. We apply the same procedure for comparison purposes on both BERTimbau models, given that they are also LLM trained from scratch in Brazilian Portuguese and have a similar size range to our models. We used these comparisons to assess if our pre-training runs produced LLM capable of producing good results (\"good\" here means \"close to BERTimbau\") when utilized for downstream applications.\nModels\nIMDB\nFaQuAD-NLI\nHateBr\nAssin2\nAgNews\nAverage\nBERTimbau-large\n93.58\n92.26\n91.57\n88.97\n94.11\n92.10\nBERTimbau-small\n92.22\n93.07\n91.28\n87.45\n94.19\n91.64\nTTL-460m\n91.64\n91.18\n92.28\n86.43\n94.42\n91.19\nTTL-160m\n91.14\n90.00\n90.71\n85.78\n94.05\n90.34\nAll the shown results are the higher accuracy scores achieved on the respective task test sets after fine-tuning the models on the training sets. All fine-tuning runs used the same hyperparameters, and the code implementation can be found in the model cards of our fine-tuned models.\nCite as ü§ó\n@misc{correa24ttllama,\ntitle = {TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese},\nauthor = {Corr{\\^e}a, Nicholas Kluge and Falk, Sophia and Fatimah, Shiza and Sen, Aniket and De Oliveira, Nythamar},\njournal={arXiv preprint arXiv:2401.16640},\nyear={2024}\n}\n@misc{correa24ttllama,\ndoi = {10.1016/j.mlwa.2024.100558},\nurl = {https://www.sciencedirect.com/science/article/pii/S2666827024000343},\ntitle = {TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese},\nauthor = {Corr{\\^e}a, Nicholas Kluge and Falk, Sophia and Fatimah, Shiza and Sen, Aniket and De Oliveira, Nythamar},\njournal={Machine Learning With Applications},\npublisher = {Springer},\nyear={2024}\n}\nFunding\nThis repository was built as part of the RAIES (Rede de Intelig√™ncia Artificial √âtica e Segura) initiative, a project supported by FAPERGS - (Funda√ß√£o de Amparo √† Pesquisa do Estado do Rio Grande do Sul), Brazil.\nLicense\nTeenyTinyLlama-160m is licensed under the Apache License, Version 2.0. See the LICENSE file for more details.",
    "dorsar/lung-cancer-detection": "LUNGAI: Lung Cancer Detection Model\nProject Overview\nModel Performance\nRepository Structure\nSetup and Usage\nStep 1: Install Dependencies\nStep 2: Train the Model (Optional)\nStep 3: Run the Model\nNotes\nContributing\nConnect with Me\nLUNGAI: Lung Cancer Detection Model\nProject Overview\nLungAI is a deep learning project aimed at detecting and classifying lung cancer from CT scan images. The model can differentiate between cancerous and non-cancerous lung tissue, as well as classify specific types of lung cancer.\n4x hackathon award winner - out of 1,500 total competitors.\nModel Performance\n98% accuracy in distinguishing between cancerous and non-cancerous cases\n83% accuracy in differentiating between four specific types of lung conditions:\nAdenocarcinoma: 82% F1-score\nLarge Cell Carcinoma: 85% F1-score\nNormal (non-cancerous): 98% F1-score\nSquamous Cell Carcinoma: 76% F1-score\nThis project represents the newest version, now using PyTorch.\nRepository Structure\nArchitecture/: Contains the core model scripts\narchitecture.py: Defines the model architecture\npreprocess.py: Data preprocessing utilities\ntest.py: Script for testing the model\nModel/: Stores trained model files\nlung_cancer_detection_model.onnx: ONNX format of the trained model\nlung_cancer_detection_model.pth: PyTorch weights of the trained model\nData/: (Not included in repository) Directory for storing the dataset\nProcessed_Data/: (Not included in repository) Directory for preprocessed data\nassets/: Additional project assets\nrequirements.txt: List of Python dependencies\nSetup and Usage\nStep 1: Install Dependencies\nFirst, ensure you have Python installed. Then, install the required Python libraries using the following command:\npip install -r requirements.txt\nStep 2: Train the Model (Optional)\nRun the training script to train the model.\nIt will be saved as .pth and .onnx files\npython Architecture/architecture.py\nStep 3: Run the Model\nRun the model by running the following file:\npython Architecture/run.py\nNotes\nMake sure your dataset is structured correctly under the Processed_Data directory with subdirectories for training, validation, and testing sets.\nThe model training script expects the dataset to be in the Processed_Data directory. Ensure that the data transformations and directory paths are correctly set up in architecture.py.\nContributing\nIf you would like to contribute to this project, please fork the repository and submit a pull request. We welcome improvements, bug fixes, and new features.\nConnect with Me",
    "ds4sd/docling-models": "Docling Models\nLayout Model\nTableFormer\nReferences\nDocling Models\nThis page contains models that power the PDF document converion package docling.\nLayout Model\nThe layout model will take an image from a page and apply RT-DETR model in order to find different layout components. It currently detects the labels: Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title. As a reference (from the DocLayNet-paper), this is the performance of standard object detection methods on the DocLayNet dataset compared to human evaluation,\nhuman\nMRCNN\nMRCNN\nFRCNN\nYOLO\nhuman\nR50\nR101\nR101\nv5x6\nCaption\n84-89\n68.4\n71.5\n70.1\n77.7\nFootnote\n83-91\n70.9\n71.8\n73.7\n77.2\nFormula\n83-85\n60.1\n63.4\n63.5\n66.2\nList-item\n87-88\n81.2\n80.8\n81.0\n86.2\nPage-footer\n93-94\n61.6\n59.3\n58.9\n61.1\nPage-header\n85-89\n71.9\n70.0\n72.0\n67.9\nPicture\n69-71\n71.7\n72.7\n72.0\n77.1\nSection-header\n83-84\n67.6\n69.3\n68.4\n74.6\nTable\n77-81\n82.2\n82.9\n82.2\n86.3\nText\n84-86\n84.6\n85.8\n85.4\n88.1\nTitle\n60-72\n76.7\n80.4\n79.9\n82.7\nAll\n82-83\n72.4\n73.5\n73.4\n76.8\nTableFormer\nThe tableformer model will identify the structure of the table, starting from an image of a table. It uses the predicted table regions of the layout model to identify the tables. Tableformer has SOTA table structure identification,\nModel (TEDS)\nSimple table\nComplex table\nAll tables\nTabula\n78.0\n57.8\n67.9\nTraprange\n60.8\n49.9\n55.4\nCamelot\n80.0\n66.0\n73.0\nAcrobat Pro\n68.9\n61.8\n65.3\nEDD\n91.2\n85.4\n88.3\nTableFormer\n95.4\n90.1\n93.6\nReferences\n@techreport{Docling,\nauthor = {Deep Search Team},\nmonth = {8},\ntitle = {{Docling Technical Report}},\nurl={https://arxiv.org/abs/2408.09869},\neprint={2408.09869},\ndoi = \"10.48550/arXiv.2408.09869\",\nversion = {1.0.0},\nyear = {2024}\n}\n@article{doclaynet2022,\ntitle = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},\ndoi = {10.1145/3534678.353904},\nurl = {https://arxiv.org/abs/2206.01062},\nauthor = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},\nyear = {2022}\n}\n@InProceedings{TableFormer2022,\nauthor    = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},\ntitle     = {TableFormer: Table Structure Understanding With Transformers},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {4614-4623},\ndoi = {https://doi.org/10.1109/CVPR52688.2022.00457}\n}",
    "zhangtaolab/agront-1b-conservation": "Plant foundation DNA large language models\nModel Sources\nArchitecture\nHow to use\nTraining data\nPlant foundation DNA large language models\nThe plant DNA large language models (LLMs) contain a series of foundation models based on different model architectures, which are pre-trained on various plant reference genomes.All the models have a comparable model size between 90 MB and 150 MB, BPE tokenizer is used for tokenization and 8000 tokens are included in the vocabulary.\nDeveloped by: zhangtaolab\nModel Sources\nRepository: Plant DNA LLMs\nManuscript: Versatile applications of foundation DNA large language models in plant genomes\nArchitecture\nThe model is trained based on the InstaDeepAI/agro-nucleotide-transformer-1b model.\nThis model is fine-tuned for predicting sequence conservation.\nHow to use\nInstall the runtime library first:\npip install transformers\nHere is a simple code for inference:\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel_name = 'agront-1b-conservation'\n# load model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(f'zhangtaolab/{model_name}', trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(f'zhangtaolab/{model_name}', trust_remote_code=True)\n# inference\nsequences = ['ACATGCTAAATTAGTTGGCAATTTTTTCTCAGGTAGCTGGGCACAATTTGGTAGTCCAGTTGAACAAAATCCATTAGCTTCTTTTAGCAAGTCCCCTGGTTTGGGCCCTGCCAGTCCCATTAATACCAACCATTTGTCTGGATTGGCTGCAATTCTTTCCCCACAAGCAACAACCTCTACCAAGATTGCACCGATTGGCAAGGACCCTGGAAGGGCTGCAAATCAGATGTTTTCTAACTCTGGATCAACACAAGGAGCAGCTTTTCAGCATTCTATATCCTTTCCTGAGCAAAATGTAAAGGCAAGTCCTAGGCCTATATCTACTTTTGGTGAATCAAGTTCTAGTGCATCAAGTATTGGAACACTGTCCGGTCCTCAATTTCTTTGGGGAAGCCCAACTCCTTACTCTGAGCATTCAAACACTTCTGCCTGGTCTTCATCTTCGGTGGGGCTTCCATTTACATCTAGTGTCCAAAGGCAGGGTTTCCCATATACTAGTAATCACAGTCCTTTTCTTGGCTCCCACTCTCATCATCATGTTGGATCTGCTCCATCTGGCCTTCCGCTTGATAGGCATTTTAGCTACTTCCCTGAGTCACCTGAAGCTTCTCTCATGAGCCCGGTTGCATTTGGGAATTTAAATCACGGTGATGGGAATTTTATGATGAACAACATTAGTGCTCGTGCATCTGTAGGAGCCGGTGTTGGTCTTTCTGGAAATACCCCTGAAATTAGTTCACCCAATTTCAGAATGATGTCTCTGCCTAGGCATGGTTCCTTGTTCCATGGAAATAGTTTGTATTCTGGACCTGGAGCAACTAACATTGAGGGATTAGCTGAACGTGGACGAAGTAGACGACCTGAAAATGGTGGGAACCAAATTGATAGTAAGAAGCTGTACCAGCTTGATCTTGACAAAATCGTCTGTGGTGAAGATACAAGGACTACTTTAATGATTAAAAACATTCCTAACAAGTAAGAATAACTAAACATCTATCCT',\n'GTCGCAAAAATTGGGCCACTTGCAGTTCAATCTGTTTAATCAAAATTGCATGTGTATCAACTTTTTGCCCAATACTAGCTATATCACACCTCAACTCTTTAATGTGTTCATCACTAGTGTCGAACCTCCTCATCATTTTGTCCAACATATCCTCAACTCGCGCCATACTATCTCCACCATCCCTAGGAGTAACTTCACGATTTTGAGGAGGGACATAGGGCCCATTCCTGTCGTTTCTATTAGCATAGTTACTCCTGTTAAAGTTGTTGTCGCGGTTGTAGTTTCCATCACGTACATAATGACTCTCACGGTTGTAGTTACCATAGTTCCGACCTGGGTTCCCTTGAACTTGGCGCCAGTTATCCTGATTTGAGCCTTGGGCGCTTGGTCGGAAACCCCCTGTCTGCTCATTTACTGCATAAGTGTCCTCCGCGTAACATCATTAGGAGGTGGTGGTTTAGCAAAGTAGTTGACTGCATTTATCTTTTCTGCACCCCCTGTGACATTTTTTAGTACCAACCCAAGCTCAGTTCTCATCTGAGACATTTCTTCTCGAATCTCATCTGTGGCTCGGTTGTGAGTGGACTGCACTACGAAGGTGTTTTTCCCTGTATCAAACTTCCTAGTACTCCAAGCTTTGTTATTTCGGGAGATTTTCTCTAGTTTTTCTGCAATCTCAACATAAGTGCATTCTCCATAAGATCCACCTGCTATAGTGTCCAACACCGCTTTATTGTTATCATCCTGTCCCCGATAGAAGTATTCCTTCAGTGACTCATCATCTATACGGTGATTTAGAACACTTCTCAAGAATGAGGTGAATCTATCCCAAGAACTACTAACTAACTCTCCTGGTAGTGCCACAAAGCTGTTCACCCTTTCTTTGTGGTTTAACTTCTTGGAGATCGGATAGTAGCGTGCTAAGAAGACATCCCTTAGTTGGTTCCAAGTGAATATGGAGTTGTATGCGAGCTTAGTGAACCACATTGCAGCCTCTCCC']\npipe = pipeline('text-classification', model=model, tokenizer=tokenizer,\ntrust_remote_code=True, top_k=None)\nresults = pipe(sequences)\nprint(results)\nTraining data\nWe use EsmForSequenceClassification to fine-tune the model.Detailed training procedure can be found in our manuscript.\nHardware\nModel was trained on a NVIDIA RTX4090 GPU (24 GB).",
    "Kijai/LivePortrait_safetensors": "No model card",
    "chickenrice0721/whisper-large-v2-translate-zh-v0.1-lt": "whisper-large-v2-translate-zh-v0.1-lt-ct2\nModel description\nUsage\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nwhisper-large-v2-translate-zh-v0.1-lt-ct2\nThis model is a fine-tuned version of openai/whisper-large-v2.\nModel description\n3500Â∞èÊó∂ (Êó•ËØ≠Èü≥È¢ë,‰∏≠ÊñáÂ≠óÂπï) Êï∞ÊçÆÂæÆË∞É, ÁøªËØëÊ®°ÂºèÁõ¥Âá∫‰∏≠Êñá\nUsage\ntask='translate', language='ja'\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 32\neval_batch_size: 32\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 64\noptimizer: Adam with betas=(0.9,0.98) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 4000\ndropout: 0.1\nmask_time_prob: 0.05\nmask_feature_prob: 0.2\ncondition_on_previous_text_rate: 0.5\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nCer\nWer\n1.743\n0.0740\n1000\n1.5631\n0.8223\n1.4517\n1.6014\n0.1479\n2000\n1.4808\n0.6775\n1.0950\n1.5549\n0.2219\n3000\n1.4381\n0.6756\n1.1158\n1.5283\n0.2958\n4000\n1.4174\n0.6992\n1.1137\n1.474\n0.3698\n5000\n1.3849\n0.6570\n1.1369\n1.4193\n0.4437\n6000\n1.3657\n0.6544\n1.1339\n1.4148\n0.5177\n7000\n1.3477\n0.6386\n1.1647\n1.3754\n0.5916\n8000\n1.3392\n0.6228\n1.0461\n1.3441\n0.6656\n9000\n1.3362\n0.6196\n1.0609\n1.3545\n0.7395\n10000\n1.3176\n0.6354\n1.2138\n1.3498\n0.8135\n11000\n1.3236\n0.6631\n1.2232\n1.31\n0.8874\n12000\n1.3020\n0.6199\n1.0018\n1.3213\n0.9614\n13000\n1.2966\n0.5922\n1.0021\n1.2375\n1.0353\n14000\n1.2900\n0.6097\n1.0639\n1.2334\n1.1093\n15000\n1.2963\n0.6150\n1.0920\n1.2277\n1.1832\n16000\n1.2888\n0.6077\n1.0929\n1.2087\n1.2572\n17000\n1.2779\n0.5954\n1.0012\n1.2131\n1.3311\n18000\n1.2722\n0.5776\n1.0075\n1.2012\n1.4051\n19000\n1.2716\n0.5726\n1.0211\n1.1912\n1.4790\n20000\n1.2707\n0.6007\n1.1538\n1.2127\n1.5530\n21000\n1.2749\n0.6086\n1.0742\n1.1789\n1.6269\n22000\n1.2797\n0.5765\n1.0072\n1.1527\n1.7009\n23000\n1.2761\n0.5855\n1.0588\n1.1693\n1.7748\n24000\n1.2701\n0.5635\n0.9928\n1.1709\n1.8488\n25000\n1.2662\n0.5980\n1.0697\n1.1637\n1.9227\n26000\n1.2749\n0.5872\n1.0392\n1.1562\n1.9967\n27000\n1.2587\n0.5651\n1.0121\n1.0929\n2.0706\n28000\n1.2668\n0.5857\n1.0139\n1.1232\n2.1446\n29000\n1.2710\n0.5742\n0.9997\n1.1045\n2.2185\n30000\n1.2656\n0.5643\n0.9897\n1.0841\n2.2925\n31000\n1.2695\n0.5835\n1.0181\n1.0868\n2.3664\n32000\n1.2707\n0.5673\n0.9964\n1.0938\n2.4404\n33000\n1.2644\n0.5712\n0.9928\n1.0938\n2.5143\n34000\n1.2662\n0.5750\n1.0109\n1.0848\n2.5883\n35000\n1.2677\n0.5841\n1.0832\n1.0914\n2.6622\n36000\n1.2638\n0.5801\n1.0299\n1.0688\n2.7362\n37000\n1.2587\n0.5694\n1.0072\n1.0856\n2.8101\n38000\n1.2581\n0.5646\n1.0057\n1.1037\n2.8841\n39000\n1.2557\n0.5771\n1.0262\n1.0652\n2.9580\n40000\n1.2566\n0.5634\n0.9979\nFramework versions\nTransformers 4.41.1\nPytorch 2.3.1+cu121\nDatasets 2.19.1\nTokenizers 0.19.1",
    "vicgalle/gliner-small-pii": "GLiNER-small-PII\nUsage\nGLiNER-small-PII\nThis model has been trained by fine-tuning gliner-community/gliner_small-v2.5 on the urchade/synthetic-pii-ner-mistral-v1 dataset.\nThis model is capable of recognizing various types of personally identifiable information (PII), including but not limited to these entity types: person, organization, phone number, address, passport number, email, credit card number, social security number, health insurance id number, date of birth, mobile phone number, bank account number, medication, cpf, driver's license number, tax identification number, medical condition, identity card number, national id number, ip address, email address, iban, credit card expiration date, username, health insurance number, registration number, student id number, insurance number, flight number, landline phone number, blood type, cvv, reservation number, digital signature, social media handle, license plate number, cnpj, postal code, passport_number, serial number, vehicle registration number, credit card brand, fax number, visa number, insurance company, identity document number, transaction number, national health insurance number, cvc, birth certificate number, train ticket number, passport expiration date, and social_security_number.\nUsage\nmodel = GLiNER.from_pretrained(\"vicgalle/gliner-small-pii\", load_tokenizer=True)\ntext = \"\"\"\nHarilala Rasoanaivo, un homme d'affaires local d'Antananarivo, a enregistr√© une nouvelle soci√©t√© nomm√©e \"Rasoanaivo Enterprises\" au Lot II M 92 Antohomadinika. Son num√©ro est le +261 32 22 345 67, et son adresse √©lectronique est harilala.rasoanaivo@telma.mg. Il a fourni son num√©ro de s√©cu 501-02-1234 pour l'enregistrement.\n\"\"\"\nlabels = [\n\"work\",\n\"booking number\",\n\"personally identifiable information\",\n\"driver licence\",\n\"person\",\n\"book\",\n\"postal address\",\n\"company\",\n\"actor\",\n\"character\",\n\"email\",\n\"passport number\",\n\"SSN\",\n\"phone number\",\n]\nentities = model.predict_entities(text, labels, threshold=0.1)\nfor entity in entities:\nprint(entity[\"text\"], \"=>\", entity[\"label\"])\nHarilala Rasoanaivo => person\nRasoanaivo Enterprises => company\nLot II M 92 Antohomadinika => postal address\n+261 32 22 345 67 => phone number\nharilala.rasoanaivo@telma.mg => email\n501-02-1234 => SSN\nNote: it may be beneficial to lower the threshold (see the previous example), to extract all related entities."
}