{
    "kredor/punctuate-all": "This is based on Oliver Guhr's work. The difference is that it is a finetuned xlm-roberta-base instead of an xlm-roberta-large and on twelve languages instead of four. The languages are: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.\n----- report -----\nprecision    recall  f1-score   support\n0       0.99      0.99      0.99  73317475\n.       0.94      0.95      0.95   4484845\n,       0.86      0.86      0.86   6100650\n?       0.88      0.85      0.86    136479\n-       0.60      0.29      0.39    233630\n:       0.71      0.49      0.58    152424\naccuracy                           0.98  84425503\nmacro avg       0.83      0.74      0.77  84425503\nweighted avg       0.98      0.98      0.98  84425503\n----- confusion matrix -----\nt/p      0     .     ,     ?     -     :\n0   1.0   0.0   0.0   0.0   0.0   0.0\n.   0.0   1.0   0.0   0.0   0.0   0.0\n,   0.1   0.0   0.9   0.0   0.0   0.0\n?   0.0   0.1   0.0   0.8   0.0   0.0\n-   0.1   0.1   0.5   0.0   0.3   0.0\n:   0.0   0.3   0.1   0.0   0.0   0.5",
    "Salesforce/codegen-350M-multi": "CodeGen (CodeGen-Multi 350M)\nModel description\nTraining data\nTraining procedure\nEvaluation results\nIntended Use and Limitations\nHow to use\nEthical Considerations\nBibTeX entry and citation info\nCodeGen (CodeGen-Multi 350M)\nModel description\nCodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).\nThe checkpoint included in this repository is denoted as CodeGen-Multi 350M in the paper, where \"Multi\" means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and \"350M\" refers to the number of trainable parameters.\nTraining data\nThis checkpoint (CodeGen-Multi 350M) was firstly initialized with CodeGen-NL 350M, and then pre-trained on BigQuery, a large-scale dataset of multiple programming languages from GitHub repositories. The data consists of 119.2B tokens and includes C, C++, Go, Java, JavaScript, and Python.\nTraining procedure\nCodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.\nThe family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.\nSee Section 2.3 of the paper for more details.\nEvaluation results\nWe evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details.\nIntended Use and Limitations\nAs an autoregressive language model, CodeGen is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them.\nHowever, the model is intended for and best at program synthesis, that is, generating executable code given English prompts, where the prompts should be in the form of a comment string. The model can complete partially-generated code as well.\nHow to use\nThis model can be easily loaded using the AutoModelForCausalLM functionality:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-multi\")\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nBibTeX entry and citation info\n@article{Nijkamp2022ACP,\ntitle={A Conversational Paradigm for Program Synthesis},\nauthor={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},\njournal={arXiv preprint},\nyear={2022}\n}",
    "Voicelab/sbert-large-cased-pl": "SHerbert large - Polish SentenceBERT\nCorpus\nTokenizer\nUsage\nResults\nLicense\nCitation\nAuthors\nSHerbert large - Polish SentenceBERT\nSentenceBERT is a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. Training was based on the original paper Siamese BERT models for the task of semantic textual similarity (STS) with a slight modification of how the training data was used. The goal of the model is to generate different embeddings based on the semantic and topic similarity of the given text.\nSemantic textual similarity analyzes how similar two pieces of texts are.\nRead more about how the model was prepared in our blog post.\nThe base trained model is a Polish HerBERT. HerBERT is a BERT-based Language Model. For more details, please refer to: \"HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish\".\nCorpus\nTe model was trained solely on Wikipedia.\nTokenizer\nAs in the original HerBERT implementation, the training dataset was tokenized into subwords using a character level byte-pair encoding (CharBPETokenizer) with a vocabulary size of 50k tokens. The tokenizer itself was trained with a tokenizers library.\nWe kindly encourage you to use the Fast version of the tokenizer, namely HerbertTokenizerFast.\nUsage\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics import pairwise\nsbert = AutoModel.from_pretrained(\"Voicelab/sbert-large-cased-pl\")\ntokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-large-cased-pl\")\ns0 = \"Uczenie maszynowe jest konsekwencją rozwoju idei sztucznej inteligencji i metod jej wdrażania praktycznego.\"\ns1 = \"Głębokie uczenie maszynowe jest sktukiem wdrażania praktycznego metod sztucznej inteligencji oraz jej rozwoju.\"\ns2 = \"Kasparow zarzucił firmie IBM oszustwo, kiedy odmówiła mu dostępu do historii wcześniejszych gier Deep Blue. \"\ntokens = tokenizer([s0, s1, s2],\npadding=True,\ntruncation=True,\nreturn_tensors='pt')\nx = sbert(tokens[\"input_ids\"],\ntokens[\"attention_mask\"]).pooler_output\n# similarity between sentences s0 and s1\nprint(pairwise.cosine_similarity(x[0], x[1])) # Result: 0.8011128\n# similarity between sentences s0 and s2\nprint(pairwise.cosine_similarity(x[0], x[2])) # Result: 0.58822715\nResults\nModel\nAccuracy\nSource\nSBERT-WikiSec-base (EN)\n80.42%\nhttps://arxiv.org/abs/1908.10084\nSBERT-WikiSec-large (EN)\n80.78%\nhttps://arxiv.org/abs/1908.10084\nsbert-base-cased-pl\n82.31%\nhttps://huggingface.co/Voicelab/sbert-base-cased-pl\nsbert-large-cased-pl\n84.42%\nhttps://huggingface.co/Voicelab/sbert-large-cased-pl\nLicense\nCC BY 4.0\nCitation\nIf you use this model, please cite the following paper:\nAuthors\nThe model was trained by NLP Research Team at Voicelab.ai.\nYou can contact us here.",
    "Helsinki-NLP/opus-mt-tc-big-it-en": "opus-mt-tc-big-it-en\nModel info\nUsage\nBenchmarks\nAcknowledgements\nModel conversion info\nopus-mt-tc-big-it-en\nNeural machine translation model for translating from Italian (it) to English (en).\nThis model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of Marian NMT, an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from OPUS and training pipelines use the procedures of OPUS-MT-train.\nPublications: OPUS-MT – Building open translation services for the World and The Tatoeba Translation Challenge – Realistic Data Sets for Low Resource and Multilingual MT (Please, cite if you use this model.)\n@inproceedings{tiedemann-thottingal-2020-opus,\ntitle = \"{OPUS}-{MT} {--} Building open translation services for the World\",\nauthor = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\nbooktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Lisboa, Portugal\",\npublisher = \"European Association for Machine Translation\",\nurl = \"https://aclanthology.org/2020.eamt-1.61\",\npages = \"479--480\",\n}\n@inproceedings{tiedemann-2020-tatoeba,\ntitle = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\nauthor = {Tiedemann, J{\\\"o}rg},\nbooktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2020.wmt-1.139\",\npages = \"1174--1182\",\n}\nModel info\nRelease: 2022-02-25\nsource language(s): ita\ntarget language(s): eng\nmodel: transformer-big\ndata: opusTCv20210807+bt (source)\ntokenization: SentencePiece (spm32k,spm32k)\noriginal model: opusTCv20210807+bt_transformer-big_2022-02-25.zip\nmore information released models: OPUS-MT ita-eng README\nUsage\nA short example code:\nfrom transformers import MarianMTModel, MarianTokenizer\nsrc_text = [\n\"So chi è il mio nemico.\",\n\"Tom è illetterato; non capisce assolutamente nulla.\"\n]\nmodel_name = \"pytorch-models/opus-mt-tc-big-it-en\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\nfor t in translated:\nprint( tokenizer.decode(t, skip_special_tokens=True) )\n# expected output:\n#     I know who my enemy is.\n#     Tom is illiterate; he understands absolutely nothing.\nYou can also use OPUS-MT models with the transformers pipelines, for example:\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-it-en\")\nprint(pipe(\"So chi è il mio nemico.\"))\n# expected output: I know who my enemy is.\nBenchmarks\ntest set translations: opusTCv20210807+bt_transformer-big_2022-02-25.test.txt\ntest set scores: opusTCv20210807+bt_transformer-big_2022-02-25.eval.txt\nbenchmark results: benchmark_results.txt\nbenchmark output: benchmark_translations.zip\nlangpair\ntestset\nchr-F\nBLEU\n#sent\n#words\nita-eng\ntatoeba-test-v2021-08-07\n0.82288\n72.1\n17320\n119214\nita-eng\nflores101-devtest\n0.62115\n32.8\n1012\n24721\nita-eng\nnewssyscomb2009\n0.59822\n34.4\n502\n11818\nita-eng\nnewstest2009\n0.59646\n34.3\n2525\n65399\nAcknowledgements\nThe work is supported by the European Language Grid as pilot project 2866, by the FoTran project, funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 771113), and the MeMAD project, funded by the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by CSC -- IT Center for Science, Finland.\nModel conversion info\ntransformers version: 4.16.2\nOPUS-MT git hash: 3405783\nport time: Wed Apr 13 19:40:08 EEST 2022\nport machine: LM0-400-22516.local",
    "hustvl/yolos-small": "YOLOS (small-sized) model\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining\nEvaluation results\nBibTeX entry and citation info\nYOLOS (small-sized) model\nYOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository.\nDisclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nYOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\nIntended uses & limitations\nYou can use the raw model for object detection. See the model hub to look for all available YOLOS models.\nHow to use\nHere is how to use this model:\nfrom transformers import YolosFeatureExtractor, YolosForObjectDetection\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\nCurrently, both the feature extractor and model support PyTorch.\nTraining data\nThe YOLOS model was pre-trained on ImageNet-1k and fine-tuned on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\nTraining\nThe model was pre-trained for 200 epochs on ImageNet-1k and fine-tuned for 150 epochs on COCO.\nEvaluation results\nThis model achieves an AP (average precision) of 36.1 on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2106-00666,\nauthor    = {Yuxin Fang and\nBencheng Liao and\nXinggang Wang and\nJiemin Fang and\nJiyang Qi and\nRui Wu and\nJianwei Niu and\nWenyu Liu},\ntitle     = {You Only Look at One Sequence: Rethinking Transformer in Vision through\nObject Detection},\njournal   = {CoRR},\nvolume    = {abs/2106.00666},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2106.00666},\neprinttype = {arXiv},\neprint    = {2106.00666},\ntimestamp = {Fri, 29 Apr 2022 19:49:16 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2106-00666.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "staka/fugumt-en-ja": "FuguMT\nHow to use\nEval results\nFuguMT\nThis is a translation model using Marian-NMT.\nFor more details, please see my repository.\nsource language: en\ntarget language: ja\nHow to use\nThis model uses transformers and sentencepiece.\n!pip install transformers sentencepiece\nYou can use this model directly with a pipeline:\nfrom transformers import pipeline\nfugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\nfugu_translator('This is a cat.')\nIf you want to translate multiple sentences, we recommend using pySBD.\n!pip install transformers sentencepiece pysbd\nimport pysbd\nseg_en = pysbd.Segmenter(language=\"en\", clean=False)\nfrom transformers import pipeline\nfugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\ntxt = 'This is a cat. It is very cute.'\nprint(fugu_translator(seg_en.segment(txt)))\nEval results\nThe results of the evaluation using tatoeba(randomly selected 500 sentences) are as follows:\nsource\ntarget\nBLEU(*1)\nen\nja\n32.7\n(*1) sacrebleu --tokenize ja-mecab",
    "spital/gpt2-small-czech-cs": "GPT2-small-czech-cs: a Language Model for Czech text generation (and more NLP tasks ...)\nIntroduction\nModel description\nHow to use GPT2-small-czech-cs with HuggingFace (PyTorch)\nLoad the model and its sub-word tokenizer (Byte-level BPE)\nGenerate one word\nGenerate few full sequences\nLimitations and bias\nAuthor\nCitation\nGPT2-small-czech-cs: a Language Model for Czech text generation (and more NLP tasks ...)\nIntroduction\nGPT2-small-czech-cs is a first experimental model for Czech language based on the GPT-2 small model.\nIt was trained on Czech Wikipedia using Transfer Learning and Fine-tuning techniques in about over a weekend on one GPU NVIDIA GTX 1080ti and with about 1GB of training data (cswiki). A training server with couple GPUs for experiments and one RTX 3080 ti was generously provided by ONYX engineering, spol. s r.o..\nThis experiment is a proof-of-concept that it is possible to get a state-of-the-art language model in any language with low resources.\nIt was fine-tuned from the English pre-trained GPT-2 small using the Hugging Face libraries (Transformers and Tokenizers) wrapped into the fastai2 Deep Learning framework. All the fine-tuning fastai v2 techniques were used. This work was inspired by Faster than training from scratch — Fine-tuning the English GPT-2 in any language with Hugging Face and fastai v2 (practical case with Portuguese), citation below.\nIt is now available on Hugging Face under gpt2-small-czech-cs. We release it under CC BY SA 4.0 license (i.e. allowing commercial use).  For further information or requests, please post a Github issue at Github - gpt2-small-czech-cs.\nModel description\nNote: information copied/pasted from Model: gpt2 >> Model description\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\nHow to use GPT2-small-czech-cs with HuggingFace (PyTorch)\nLoad the model and its sub-word tokenizer (Byte-level BPE)\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\npretrained = 'spital/gpt2-small-czech-cs'  # a local directory or huggingface model name\ntokenizer = GPT2Tokenizer.from_pretrained(pretrained)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained, pad_token_id=tokenizer.eos_token_id)\n# Sequence length max is 1024\ntokenizer.model_max_length = 1024\n# disable dropout (or leave in train mode to finetune)\nmodel.eval()\nGenerate one word\nimport torch\n# input sequence\ntext = \"Umělá inteligence pomůže lidstvu překonat budoucí\"\ninp_tokens = tokenizer(text, return_tensors=\"pt\")\n# model output\noutputs = model(**inp_tokens, labels=inp_tokens[\"input_ids\"])\nloss, logits = outputs[:2]\npredicted_index = torch.argmax(logits[0, -1, :]).item()\npredicted_text = tokenizer.decode([predicted_index])\n# results\nprint('input text:', text)\nprint('predicted text:', predicted_text)\n# predicted text:  problémy\nGenerate few full sequences\ntext = \"Umělá inteligence pomůže lidstvu překonat budoucí\"\nencoded = tokenizer.encode(text, return_tensors='pt')\n# torch.random.manual_seed(0)  # if you need reproducibility\nsample_outputs = model.generate(encoded, do_sample=True,\nmax_length=encoded.size()[1]+20,\nno_repeat_ngram_size=2, top_p=0.95, top_k=50,\ntemperature=0.65, num_return_sequences=3)\nfor i, sample_output in enumerate(sample_outputs): print(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\nLimitations and bias\nThe training data used for this model come from Czech Wikipedia dump. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\nBecause large-scale language models like GPT-2 do not distinguish fact from fiction, we don't support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\nAuthor\nCzech GPT-2 small was trained and evaluated by Jiri Spitalsky thanks to the computing power of the GPUs and other hardware generously provided by ONYX engineering, spol. s r.o..\nCitation\nMy special thanks go to Pierre Guillou for his work GPorTuguese-2 (Portuguese GPT-2 small): a Language Model for Portuguese text generation (and more NLP tasks...), my work would not be possible without it.",
    "speechbrain/tts-tacotron2-ljspeech": "Text-to-Speech (TTS) with Tacotron2 trained on LJSpeech\nInstall SpeechBrain\nPerform Text-to-Speech (TTS)\nInference on GPU\nTraining\nLimitations\nAbout SpeechBrain\nCiting SpeechBrain\nText-to-Speech (TTS) with Tacotron2 trained on LJSpeech\nThis repository provides all the necessary tools for Text-to-Speech (TTS)  with SpeechBrain using a Tacotron2 pretrained on LJSpeech.\nThe pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.\nInstall SpeechBrain\npip install speechbrain\nPlease notice that we encourage you to read our tutorials and learn more about\nSpeechBrain.\nPerform Text-to-Speech (TTS)\nimport torchaudio\nfrom speechbrain.inference.TTS import Tacotron2\nfrom speechbrain.inference.vocoders import HIFIGAN\n# Intialize TTS (tacotron2) and Vocoder (HiFIGAN)\ntacotron2 = Tacotron2.from_hparams(source=\"speechbrain/tts-tacotron2-ljspeech\", savedir=\"tmpdir_tts\")\nhifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"tmpdir_vocoder\")\n# Running the TTS\nmel_output, mel_length, alignment = tacotron2.encode_text(\"Mary had a little lamb\")\n# Running Vocoder (spectrogram-to-waveform)\nwaveforms = hifi_gan.decode_batch(mel_output)\n# Save the waverform\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\nIf you want to generate multiple sentences in one-shot, you can do in this way:\nfrom speechbrain.pretrained import Tacotron2\ntacotron2 = Tacotron2.from_hparams(source=\"speechbrain/TTS_Tacotron2\", savedir=\"tmpdir\")\nitems = [\n\"A quick brown fox jumped over the lazy dog\",\n\"How much wood would a woodchuck chuck?\",\n\"Never odd or even\"\n]\nmel_outputs, mel_lengths, alignments = tacotron2.encode_batch(items)\nInference on GPU\nTo perform inference on the GPU, add  run_opts={\"device\":\"cuda\"}  when calling the from_hparams method.\nTraining\nThe model was trained with SpeechBrain.\nTo train it from scratch follow these steps:\nClone SpeechBrain:\ngit clone https://github.com/speechbrain/speechbrain/\nInstall it:\ncd speechbrain\npip install -r requirements.txt\npip install -e .\nRun Training:\ncd recipes/LJSpeech/TTS/tacotron2/\npython train.py --device=cuda:0 --max_grad_norm=1.0 --data_folder=/your_folder/LJSpeech-1.1 hparams/train.yaml\nYou can find our training results (models, logs, etc) here.\nLimitations\nThe SpeechBrain team does not provide any warranty on the performance achieved by this model when used on other datasets.\nAbout SpeechBrain\nWebsite: https://speechbrain.github.io/\nCode: https://github.com/speechbrain/speechbrain/\nHuggingFace: https://huggingface.co/speechbrain/\nCiting SpeechBrain\nPlease, cite SpeechBrain if you use it for your research or business.\n@misc{speechbrain,\ntitle={{SpeechBrain}: A General-Purpose Speech Toolkit},\nauthor={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\nyear={2021},\neprint={2106.04624},\narchivePrefix={arXiv},\nprimaryClass={eess.AS},\nnote={arXiv:2106.04624}\n}",
    "apple/mobilevit-xx-small": "MobileViT (extra extra small-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nMobileViT (extra extra small-sized model)\nMobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nMobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are \"unflattened\" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-xx-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\")\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nCurrently, both the feature extractor and model support PyTorch.\nTraining data\nThe MobileViT model was pretrained on ImageNet-1k, a dataset consisting of 1 million images and 1,000 classes.\nTraining procedure\nPreprocessing\nTraining requires only basic data augmentation, i.e. random resized cropping and horizontal flipping.\nTo learn multi-scale representations without requiring fine-tuning, a multi-scale sampler was used during training, with image sizes randomly sampled from: (160, 160), (192, 192), (256, 256), (288, 288), (320, 320).\nAt inference time, images are resized/rescaled to the same resolution (288x288), and center-cropped at 256x256.\nPixels are normalized to the range [0, 1]. Images are expected to be in BGR pixel order, not RGB.\nPretraining\nThe MobileViT networks are trained from scratch for 300 epochs on ImageNet-1k on 8 NVIDIA GPUs with an effective batch size of 1024 and learning rate warmup for 3k steps, followed by cosine annealing. Also used were label smoothing cross-entropy loss and L2 weight decay. Training resolution varies from 160x160 to 320x320, using multi-scale sampling.\nEvaluation results\nModel\nImageNet top-1 accuracy\nImageNet top-5 accuracy\n# params\nURL\nMobileViT-XXS\n69.0\n88.9\n1.3 M\nhttps://huggingface.co/apple/mobilevit-xx-small\nMobileViT-XS\n74.8\n92.3\n2.3 M\nhttps://huggingface.co/apple/mobilevit-x-small\nMobileViT-S\n78.4\n94.1\n5.6 M\nhttps://huggingface.co/apple/mobilevit-small\nBibTeX entry and citation info\n@inproceedings{vision-transformer,\ntitle = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor = {Sachin Mehta and Mohammad Rastegari},\nyear = {2022},\nURL = {https://arxiv.org/abs/2110.02178}\n}",
    "AAUBS/PatentSBERTa_V2": "PatentSBERTa_V2\nCiting & Authors\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nEvaluation Results\nTraining\nFull Model Architecture\nPatentSBERTa_V2\nPatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT\nAalborg University Business School, AI: Growth-Lab\nhttps://www.sciencedirect.com/science/article/abs/pii/S0040162524003329\nhttps://github.com/AI-Growth-Lab/PatentSBERTa\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nCiting & Authors\nBekamiri, H., Hain, D. S., & Jurowetzki, R. (2024). PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT. Technological Forecasting and Social Change, 206, 123536.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nEvaluation Results\nFor an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net\nTraining\nThe model was trained with the parameters:\nDataLoader:\ntorch.utils.data.dataloader.DataLoader of length 1658 with parameters:\n{'batch_size': 16, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\nLoss:\nsentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss\nParameters of the fit()-Method:\n{\n\"epochs\": 4,\n\"evaluation_steps\": 1000,\n\"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n\"max_grad_norm\": 1,\n\"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n\"optimizer_params\": {\n\"lr\": 2e-05\n},\n\"scheduler\": \"WarmupLinear\",\n\"steps_per_epoch\": null,\n\"warmup_steps\": 664,\n\"weight_decay\": 0.01\n}\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)",
    "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1": "Cross-Encoder for multilingual MS Marco\nUsage with SentenceTransformers\nUsage with Transformers\nCross-Encoder for multilingual MS Marco\nThis model was trained on the MMARCO dataset. It is a machine translated version of MS MARCO using Google Translate. It was translated to 14 languages. In our experiments, we observed that it performs also well for other languages.\nAs a base model, we used the multilingual MiniLMv2 model.\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco\nUsage with SentenceTransformers\nThe usage becomes easy when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name')\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\nUsage with Transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\nmodel.eval()\nwith torch.no_grad():\nscores = model(**features).logits\nprint(scores)",
    "projecte-aina/roberta-base-ca-v2": "Catalan BERTa-v2 (roberta-base-ca-v2) base model\nTable of Contents\nModel description\nIntended uses and limitations\nHow to use\nLimitations and bias\nTraining\nTraining data\nTraining procedure\nEvaluation\nCLUB benchmark\nEvaluation results\nAdditional information\nAuthor\nContact information\nCopyright\nLicensing information\nFunding\nCitation information\nDisclaimer\nCatalan BERTa-v2 (roberta-base-ca-v2) base model\nTable of Contents\nClick to expand\nModel description\nIntended uses and limitations\nHow to use\nLimitations and bias\nTraining\nTraining data\nTraining procedure\nEvaluation\nCLUB benchmark\nEvaluation results\nLicensing Information\nAdditional information\nAuthor\nContact information\nCopyright\nLicensing information\nFunding\nCiting information\nDisclaimer\nModel description\nThe roberta-base-ca-v2 is a transformer-based masked language model for the Catalan language.\nIt is based on the RoBERTA base model\nand has been trained on a medium-size corpus collected from publicly available corpora and crawlers.\nIntended uses and limitations\nroberta-base-ca-v2 model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).\nHowever, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.\nHow to use\nHere is how to use this model:\nfrom transformers import AutoModelForMaskedLM\nfrom transformers import AutoTokenizer, FillMaskPipeline\nfrom pprint import pprint\ntokenizer_hf = AutoTokenizer.from_pretrained('projecte-aina/roberta-base-ca-v2')\nmodel = AutoModelForMaskedLM.from_pretrained('projecte-aina/roberta-base-ca-v2')\nmodel.eval()\npipeline = FillMaskPipeline(model, tokenizer_hf)\ntext = f\"Em dic <mask>.\"\nres_hf = pipeline(text)\npprint([r['token_str'] for r in res_hf])\nLimitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\nTraining\nTraining data\nThe training corpus consists of several corpora gathered from web crawling and public corpora.\nCorpus\nSize in GB\nCatalan Crawling\n13.00\nWikipedia\n1.10\nDOGC\n0.78\nCatalan Open Subtitles\n0.02\nCatalan Oscar\n4.00\nCaWaC\n3.60\nCat. General Crawling\n2.50\nCat. Goverment Crawling\n0.24\nACN\n0.42\nPadicat\n0.63\nRacoCatalá\n8.10\nNació Digital\n0.42\nVilaweb\n0.06\nTweets\n0.02\nTraining procedure\nThe training corpus has been tokenized using a byte version of Byte-Pair Encoding (BPE)\nused in the original RoBERTA model with a vocabulary size of 50,262 tokens.\nThe RoBERTa-ca-v2 pretraining consists of a masked language model training that follows the approach employed for the RoBERTa base model\nwith the same hyperparameters as in the original work. The training lasted a total of 96 hours with 16 NVIDIA V100 GPUs of 16GB DDRAM.\nEvaluation\nCLUB benchmark\nThe BERTa model has been fine-tuned on the downstream tasks of the Catalan Language Understanding Evaluation benchmark (CLUB),\nthat has been created along with the model.\nIt contains the following tasks and their related datasets:\nNamed Entity Recognition (NER)\nNER (AnCora): extracted named entities from the original Ancora version,\nfiltering out some unconventional ones, like book titles, and transcribed them into a standard CONLL-IOB format\nPart-of-Speech Tagging (POS)\nPOS (AnCora): from the Universal Dependencies treebank of the well-known Ancora corpus.\nText Classification (TC)\nTeCla: consisting of 137k news pieces from the Catalan News Agency (ACN) corpus, with 30 labels.\nTextual Entailment (TE)\nTE-ca: consisting of 21,163 pairs of premises and hypotheses, annotated according to the inference relation they have (implication, contradiction, or neutral), extracted from the Catalan Textual Corpus.\nSemantic Textual Similarity (STS)\nSTS-ca: consisting of more than 3000 sentence pairs, annotated with the semantic similarity between them, scraped from the Catalan Textual Corpus.\nQuestion Answering (QA):\nVilaQuAD: contains 6,282 pairs of questions and answers, outsourced from 2095 Catalan language articles from VilaWeb newswire text.\nViquiQuAD: consisting of more than 15,000 questions outsourced from Catalan Wikipedia randomly chosen from a set of 596 articles that were originally written in Catalan.\nCatalanQA: an aggregation of 2 previous datasets (VilaQuAD and ViquiQuAD), 21,427 pairs of Q/A balanced by type of question, containing one question and one answer per context, although the contexts can repeat multiple times.\nXQuAD-ca: the Catalan translation of XQuAD, a multilingual collection of manual translations of 1,190 question-answer pairs from English Wikipedia used only as a test set.\nHere are the train/dev/test splits of the datasets:\nTask (Dataset)\nTotal\nTrain\nDev\nTest\nNER (Ancora)\n13,581\n10,628\n1,427\n1,526\nPOS (Ancora)\n16,678\n13,123\n1,709\n1,846\nSTS (STS-ca)\n3,073\n2,073\n500\n500\nTC (TeCla)\n137,775\n110,203\n13,786\n13,786\nTE (TE-ca)\n21,163\n16,930\n2,116\n2,117\nQA (VilaQuAD)\n6,282\n3,882\n1,200\n1,200\nQA (ViquiQuAD)\n14,239\n11,255\n1,492\n1,429\nQA (CatalanQA)\n21,427\n17,135\n2,157\n2,135\nEvaluation results\nTask\nNER (F1)\nPOS (F1)\nSTS-ca (Comb)\nTeCla (Acc.)\nTEca (Acc.)\nVilaQuAD (F1/EM)\nViquiQuAD (F1/EM)\nCatalanQA (F1/EM)\nXQuAD-ca 1 (F1/EM)\nRoBERTa-large-ca-v2\n89.82\n99.02\n83.41\n75.46\n83.61\n89.34/75.50\n89.20/75.77\n90.72/79.06\n73.79/55.34\nRoBERTa-base-ca-v2\n89.29\n98.96\n79.07\n74.26\n83.14\n87.74/72.58\n88.72/75.91\n89.50/76.63\n73.64/55.42\nBERTa\n89.76\n98.96\n80.19\n73.65\n79.26\n85.93/70.58\n87.12/73.11\n89.17/77.14\n69.20/51.47\nmBERT\n86.87\n98.83\n74.26\n69.90\n74.63\n82.78/67.33\n86.89/73.53\n86.90/74.19\n68.79/50.80\nXLM-RoBERTa\n86.31\n98.89\n61.61\n70.14\n33.30\n86.29/71.83\n86.88/73.11\n88.17/75.93\n72.55/54.16\n1 : Trained on CatalanQA, tested on XQuAD-ca.\nAdditional information\nAuthor\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\nContact information\nFor further information, send an email to aina@bsc.es\nCopyright\nCopyright (c) 2022 Text Mining Unit at Barcelona Supercomputing Center\nLicensing information\nApache License, Version 2.0\nFunding\nThis work was funded by the Departament de la Vicepresidència i de Polítiques Digitals i Territori de la Generalitat de Catalunya within the framework of Projecte AINA.\nCitation information\nIf you use any of these resources (datasets or models) in your work, please cite our latest paper:\n@inproceedings{armengol-estape-etal-2021-multilingual,\ntitle = \"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan\",\nauthor = \"Armengol-Estap{\\'e}, Jordi  and\nCarrino, Casimiro Pio  and\nRodriguez-Penagos, Carlos  and\nde Gibert Bonet, Ona  and\nArmentano-Oller, Carme  and\nGonzalez-Agirre, Aitor  and\nMelero, Maite  and\nVillegas, Marta\",\nbooktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\nmonth = aug,\nyear = \"2021\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2021.findings-acl.437\",\ndoi = \"10.18653/v1/2021.findings-acl.437\",\npages = \"4933--4946\",\n}\nDisclaimer\nClick to expand\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\nIn no event shall the owner and creator of the models (BSC – Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.",
    "keras-io/timeseries-anomaly-detection": "Timeseries anomaly detection using an Autoencoder\nBackground and Datasets\nTraining hyperparameters\nTraining Metrics\nModel Plot\nTimeseries anomaly detection using an Autoencoder\nThis repo contains the model and the notebook to this Keras example on Timeseries anomaly detection using an Autoencoder.\nFull credits to: Pavithra Vijay\nBackground and Datasets\nThis script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.\nTraining hyperparameters\nThe following hyperparameters were used during training:\noptimizer: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\ntraining_precision: float32\nTraining Metrics\nEpochs\nTrain Loss\nValidation Loss\n1\n0.011\n0.014\n2\n0.011\n0.015\n3\n0.01\n0.012\n4\n0.01\n0.013\n5\n0.01\n0.012\n6\n0.009\n0.014\n7\n0.009\n0.013\n8\n0.009\n0.012\n9\n0.009\n0.012\n10\n0.009\n0.011\n11\n0.008\n0.01\n12\n0.008\n0.011\n13\n0.008\n0.009\n14\n0.008\n0.011\n15\n0.008\n0.009\n16\n0.008\n0.009\n17\n0.008\n0.009\n18\n0.007\n0.01\n19\n0.007\n0.009\n20\n0.007\n0.008\n21\n0.007\n0.009\n22\n0.007\n0.008\n23\n0.007\n0.008\n24\n0.007\n0.007\n25\n0.007\n0.008\n26\n0.006\n0.009\n27\n0.006\n0.008\n28\n0.006\n0.009\n29\n0.006\n0.008\nModel Plot\nView Model Plot",
    "facebook/genre-kilt": "GENRE\nBibTeX entry and citation info\nUsage\nGENRE\nThe GENRE (Generative ENtity REtrieval) system as presented in Autoregressive Entity Retrieval implemented in pytorch.\nIn a nutshell, GENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned BART architecture. GENRE performs retrieval generating the unique entity name conditioned on the input text using constrained beam search to only generate valid identifiers. The model was first released in the facebookresearch/GENRE repository using fairseq (the transformers models are obtained with a conversion script similar to this.\nThis model was trained on the full training set of KILT (i.e., 11 datasets for fact-checking, entity-linking, slot filling, dialogue, open-domain extractive and abstractive QA).\nBibTeX entry and citation info\nPlease consider citing our works if you use code from this repository.\n@inproceedings{decao2020autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola {De Cao} and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nurl={https://openreview.net/forum?id=5k8F6UU39V},\nyear={2021}\n}\nUsage\nHere is an example of generation for Wikipedia page retrieval for open-domain fact-checking:\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# OPTIONAL: load the prefix tree (trie), you need to additionally download\n# https://huggingface.co/facebook/genre-kilt/blob/main/trie.py and\n# https://huggingface.co/facebook/genre-kilt/blob/main/kilt_titles_trie_dict.pkl\n# import pickle\n# from trie import Trie\n# with open(\"kilt_titles_trie_dict.pkl\", \"rb\") as f:\n#     trie = Trie.load_from_dict(pickle.load(f))\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-kilt\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-kilt\").eval()\nsentences = [\"Einstein was a German physicist.\"]\noutputs = model.generate(\n**tokenizer(sentences, return_tensors=\"pt\"),\nnum_beams=5,\nnum_return_sequences=5,\n# OPTIONAL: use constrained beam search\n# prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\nwhich outputs the following top-5 predictions (using constrained beam search)\n['Albert Einstein',\n'Erwin Schrödinger',\n'Werner Bruschke',\n'Werner von Habsburg',\n'Werner von Moltke']",
    "facebook/genre-linking-blink": "GENRE\nBibTeX entry and citation info\nUsage\nGENRE\nThe GENRE (Generative ENtity REtrieval) system as presented in Autoregressive Entity Retrieval implemented in pytorch.\nIn a nutshell, GENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned BART architecture. GENRE performs retrieval generating the unique entity name conditioned on the input text using constrained beam search to only generate valid identifiers. The model was first released in the facebookresearch/GENRE repository using fairseq (the transformers models are obtained with a conversion script similar to this.\nThis model was trained on the full training set of BLINK (i.e., 9M datapoints for entity-disambiguation grounded on Wikipedia).\nBibTeX entry and citation info\nPlease consider citing our works if you use code from this repository.\n@inproceedings{decao2020autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola {De Cao} and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nurl={https://openreview.net/forum?id=5k8F6UU39V},\nyear={2021}\n}\nUsage\nHere is an example of generation for Wikipedia page disambiguation:\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# OPTIONAL: load the prefix tree (trie), you need to additionally download\n# https://huggingface.co/facebook/genre-linking-blink/blob/main/trie.py and\n# https://huggingface.co/facebook/genre-linking-blink/blob/main/kilt_titles_trie_dict.pkl\n# import pickle\n# from trie import Trie\n# with open(\"kilt_titles_trie_dict.pkl\", \"rb\") as f:\n#     trie = Trie.load_from_dict(pickle.load(f))\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-blink\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-blink\").eval()\nsentences = [\"Einstein was a [START_ENT] German [END_ENT] physicist.\"]\noutputs = model.generate(\n**tokenizer(sentences, return_tensors=\"pt\"),\nnum_beams=5,\nnum_return_sequences=5,\n# OPTIONAL: use constrained beam search\n# prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n)\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\nwhich outputs the following top-5 predictions (using constrained beam search)\n['Germans',\n'Germany',\n'German Empire',\n'Weimar Republic',\n'Greeks']",
    "IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese": "Wenzhong2.0-GPT2-3.5B-chinese\n简介 Brief Introduction\n模型分类 Model Taxonomy\n模型信息 Model Information\n使用 Usage\n加载模型 Loading Models\n使用示例 Usage Examples\n引用 Citation\nWenzhong2.0-GPT2-3.5B-chinese\nMain Page:Fengshenbang\nGithub: Fengshenbang-LM\n简介 Brief Introduction\n基于悟道数据集预训练，善于处理NLG任务，目前最大的，中文版的GPT2。\nPretraining on Wudao Corpus, focused on handling NLG tasks, the current largest, Chinese GPT2.\n模型分类 Model Taxonomy\n需求 Demand\n任务 Task\n系列 Series\n模型 Model\n参数 Parameter\n额外 Extra\n通用 General\n自然语言生成 NLG\n闻仲 Wenzhong\nGPT2\n3.5B\n中文 Chinese\n模型信息 Model Information\n为了可以获得一个强大的单向语言模型，我们采用GPT模型结构，并且应用于中文语料上。类似于Wenzhong-GPT2-3.5B，这个模型拥有30层解码器和35亿参数，这比原本的GPT2-XL还要大。不同的是，我们把这个模型在悟道（300G版本）语料上进行预训练。据我们所知，它是目前最大的中文的GPT模型。\nTo obtain a powerful unidirectional language model, we adopt the GPT model structure and apply it to the Chinese corpus. Similar to Wenzhong-GPT2-3.5B, this model has 30 decoder layers and 3.5 billion parameters, which is larger than the original GPT2-XL. The difference is that we pre-trained this model on the Wudao (300G version) corpus. To the best of our knowledge, it is the largest Chinese GPT model currently available.\n使用 Usage\n加载模型 Loading Models\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\ntokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese')\nmodel = GPT2LMHeadModel.from_pretrained('IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n使用示例 Usage Examples\nfrom transformers import pipeline, set_seed\nset_seed(55)\ngenerator = pipeline('text-generation', model='IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese')\ngenerator(\"北京位于\", max_length=30, num_return_sequences=1)\n引用 Citation\n如果您在您的工作中使用了我们的模型，可以引用我们的论文：\nIf you are using the resource for your work, please cite the our paper:\n@article{fengshenbang,\nauthor    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\ntitle     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\njournal   = {CoRR},\nvolume    = {abs/2209.02970},\nyear      = {2022}\n}\n也可以引用我们的网站:\nYou can also cite our website:\n@misc{Fengshenbang-LM,\ntitle={Fengshenbang-LM},\nauthor={IDEA-CCNL},\nyear={2021},\nhowpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}",
    "google/ddpm-cifar10-32": "",
    "facebook/nllb-200-3.3B": "",
    "facebook/nllb-200-distilled-1.3B": "NLLB-200\nIntended Use\nMetrics\nEvaluation Data\nTraining Data\nEthical Considerations\nCaveats and Recommendations\nCarbon Footprint Details\nNLLB-200\nThis is the model card of NLLB-200's distilled 1.3B variant.\nHere are the metrics for that particular checkpoint.\nInformation about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.\nPaper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022\nLicense: CC-BY-NC\nWhere to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues\nIntended Use\nPrimary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\nPrimary intended users: Primary users are researchers and machine translation research community.\nOut-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.\nMetrics\n• Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.\nEvaluation Data\nDatasets: Flores-200 dataset is described in Section 4\nMotivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\nPreprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.\nTraining Data\n• We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.\nEthical Considerations\n• In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).\nCaveats and Recommendations\n• Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.\nCarbon Footprint Details\n• The carbon dioxide (CO2e) estimate is reported in Section 8.8.",
    "dlicari/Italian-Legal-BERT": "ITALIAN-LEGAL-BERT:A pre-trained Transformer Language Model for Italian Law\nITALIAN-LEGAL-BERT is based on bert-base-italian-xxl-cased with additional pre-training of the Italian BERT model on Italian civil law corpora.\nIt achieves better results than the ‘general-purpose’ Italian BERT in different domain-specific tasks.\nITALIAN-LEGAL-BERT variants [NEW!!!]\nFROM SCRATCH, It is the ITALIAN-LEGAL-BERT variant pre-trained from scratch on Italian legal documents (ITA-LEGAL-BERT-SC) based on the CamemBERT architecture\nDISTILLED, a distilled version of ITALIAN-LEGAL-BERT ( DISTIL-ITA-LEGAL-BERT)\nFor long documents\nLSG ITA LEGAL BERT, Local-Sparse-Global version of ITALIAN-LEGAL-BERT (FURTHER PRETRAINED)\nLSG ITA LEGAL BERT-SC, Local-Sparse-Global version of ITALIAN-LEGAL-BERT-SC (FROM SCRATCH)\nNote: We are working on the extended version of the paper with more details and the results of these new models. We will update you soon\nTraining procedure\nWe initialized ITALIAN-LEGAL-BERT with ITALIAN XXL BERT\nand pretrained for an additional 4 epochs on 3.7 GB of preprocessed text from the National Jurisprudential\nArchive using the Huggingface PyTorch-Transformers library. We used BERT architecture\nwith a language modeling head on top, AdamW Optimizer, initial learning rate 5e-5 (with\nlinear learning rate decay, ends at 2.525e-9), sequence length 512, batch size 10 (imposed\nby GPU capacity), 8.4 million training steps, device 1*GPU V100 16GB\nUsage\nITALIAN-LEGAL-BERT model can be loaded like:\nfrom transformers import AutoModel, AutoTokenizer\nmodel_name = \"dlicari/Italian-Legal-BERT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nYou can use the Transformers library fill-mask pipeline to do inference with ITALIAN-LEGAL-BERT.\nfrom transformers import pipeline\nmodel_name = \"dlicari/Italian-Legal-BERT\"\nfill_mask = pipeline(\"fill-mask\", model_name)\nfill_mask(\"Il [MASK] ha chiesto revocarsi l'obbligo di pagamento\")\n#[{'sequence': \"Il ricorrente ha chiesto revocarsi l'obbligo di pagamento\",'score': 0.7264330387115479},\n# {'sequence': \"Il convenuto ha chiesto revocarsi l'obbligo di pagamento\",'score': 0.09641049802303314},\n# {'sequence': \"Il resistente ha chiesto revocarsi l'obbligo di pagamento\",'score': 0.039877112954854965},\n# {'sequence': \"Il lavoratore ha chiesto revocarsi l'obbligo di pagamento\",'score': 0.028993653133511543},\n# {'sequence': \"Il Ministero ha chiesto revocarsi l'obbligo di pagamento\", 'score': 0.025297977030277252}]\nIn this  COLAB: ITALIAN-LEGAL-BERT: Minimal Start for Italian Legal Downstream Tasks\nhow to use it for sentence similarity, sentence classification, and named entity recognition\nhttps://colab.research.google.com/drive/1ZOWaWnLaagT_PX6MmXMP2m3MAOVXkyRK?usp=sharing\nCitation\nIf you find our resource or paper is useful, please consider including the following citation in your paper.\n@inproceedings{licari_italian-legal-bert_2022,\naddress = {Bozen-Bolzano, Italy},\nseries = {{CEUR} {Workshop} {Proceedings}},\ntitle = {{ITALIAN}-{LEGAL}-{BERT}: {A} {Pre}-trained {Transformer} {Language} {Model} for {Italian} {Law}},\nvolume = {3256},\nshorttitle = {{ITALIAN}-{LEGAL}-{BERT}},\nurl = {https://ceur-ws.org/Vol-3256/#km4law3},\nlanguage = {en},\nurldate = {2022-11-19},\nbooktitle = {Companion {Proceedings} of the 23rd {International} {Conference} on {Knowledge} {Engineering} and {Knowledge} {Management}},\npublisher = {CEUR},\nauthor = {Licari, Daniele and Comandè, Giovanni},\neditor = {Symeonidou, Danai and Yu, Ran and Ceolin, Davide and Poveda-Villalón, María and Audrito, Davide and Caro, Luigi Di and Grasso, Francesca and Nai, Roberto and Sulis, Emilio and Ekaputra, Fajar J. and Kutz, Oliver and Troquard, Nicolas},\nmonth = sep,\nyear = {2022},\nnote = {ISSN: 1613-0073},\nfile = {Full Text PDF:https://ceur-ws.org/Vol-3256/km4law3.pdf},\n}",
    "google/ddpm-celebahq-256": "Denoising Diffusion Probabilistic Models (DDPM)\nInference\nTraining\nSamples\nDenoising Diffusion Probabilistic Models (DDPM)\nPaper: Denoising Diffusion Probabilistic Models\nAuthors: Jonathan Ho, Ajay Jain, Pieter Abbeel\nAbstract:\nWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\nInference\nDDPM models can use discrete noise schedulers such as:\nscheduling_ddpm\nscheduling_ddim\nscheduling_pndm\nfor inference. Note that while the ddpm scheduler yields the highest quality, it also takes the longest.\nFor a good trade-off between quality and inference speed you might want to consider the ddim or pndm schedulers instead.\nSee the following code:\n# !pip install diffusers\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\nmodel_id = \"google/ddpm-celebahq-256\"\n# load model and scheduler\nddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference\n# run pipeline in inference (sample random noise and denoise)\nimage = ddpm()[\"sample\"]\n# save image\nimage[0].save(\"ddpm_generated_image.png\")\nFor more in-detail information, please have a look at the official inference example\nTraining\nIf you want to train your own model, please have a look at the official training example\nSamples",
    "MCG-NJU/videomae-large": "VideoMAE (large-sized model, pre-trained only)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nEvaluation results\nBibTeX entry and citation info\nVideoMAE (large-sized model, pre-trained only)\nVideoMAE model pre-trained on Kinetics-400 for 1600 epochs in a self-supervised way. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\nDisclaimer: The team releasing VideoMAE did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nVideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\nVideos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder.\nBy pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\nIntended uses & limitations\nYou can use the raw model for predicting pixel values for masked patches of a video, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to predict pixel values for randomly masked patches:\nfrom transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\nimport numpy as np\nimport torch\nnum_frames = 16\nvideo = list(np.random.randn(16, 3, 224, 224))\nprocessor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-large\")\nmodel = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-large\")\npixel_values = processor(video, return_tensors=\"pt\").pixel_values\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\nloss = outputs.loss\nFor more code examples, we refer to the documentation.\nTraining data\n(to do, feel free to open a PR)\nTraining procedure\nPreprocessing\n(to do, feel free to open a PR)\nPretraining\n(to do, feel free to open a PR)\nEvaluation results\n(to do, feel free to open a PR)\nBibTeX entry and citation info\nmisc{https://doi.org/10.48550/arxiv.2203.12602,\ndoi = {10.48550/ARXIV.2203.12602},\nurl = {https://arxiv.org/abs/2203.12602},\nauthor = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},\nkeywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}",
    "imjeffhi/syllabizer": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nAbout\nCalling the Model\nUsing pipelines to syllabize sentences\nAbout\nThis model takes in a word as an input and splits it into syllables. I did this by pre-training a T5 model from a syllables dataset I scraped from the internet. I'm using a custom tokenizer that is effectively character-based. It seems to work okay in my limited tests, but the output may be unpredictable when inputting multiple words, numbers, or non-English characters. It can, however, handle things such as trailing punctuation.\nCalling the Model\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\nmodel = T5ForConditionalGeneration.from_pretrained('imjeffhi/syllabizer')\ntokenizer = AutoTokenizer.from_pretrained('imjeffhi/syllabizer')\ndef generate_output(word):\ntokens = tokenizer(word, return_tensors='pt')\noutput = model.generate(**tokens, do_sample=False, max_length=30, early_stopping=True)[0]\nreturn tokenizer.decode(output, skip_special_tokens=True)\nsyllables = generate_output('syllabizer')\nThe model returns syllables in spaced format. See output below.\nsyl la biz er\nUsing pipelines to syllabize sentences\nYou can easily syllabize an entire sentence/paragraph and/or convert the output into a list of syllables with the following code:\nfrom transformers import pipeline\nsyllabizer_pipe = pipeline('text2text-generation', model = 'imjeffhi/syllabizer', tokenizer='imjeffhi/syllabizer')\nsentence = \"A unit of spoken language consisting of a single uninterrupted sound formed by a vowel, diphthong, or syllabic consonant alone, or by any of these sounds preceded, followed, or surrounded by one or more consonants.\"\nwords = sentence.split(\" \")\noutput = syllabizer_pipe(words, batch_size=len(words),do_sample=False, max_length=30, early_stopping=True)\n[{words[i]: gen_text['generated_text'].split(\" \")} for i, gen_text in enumerate(output)]\nThis outputs the following:\n[{'A': ['a']},\n{'unit': ['u', 'nit']},\n{'of': ['of']},\n{'spoken': ['spok', 'en']},\n{'language': ['lan', 'guage']},\n{'consisting': ['con', 'sis', 'ting']},\n{'of': ['of']},\n{'a': ['a']},\n{'single': ['sing', 'le']},\n{'uninterrupted': ['un', 'in', 'ter', 'rupt', 'ed']},\n{'sound': ['sound']},\n{'formed': ['formed']},\n{'by': ['by']},\n{'a': ['a']},\n{'vowel,': ['vow', 'el']},\n{'diphthong,': ['diph', 'thong']},\n{'or': ['or']},\n{'syllabic': ['syl', 'la', 'bic']},\n{'consonant': ['con', 'so', 'nant']},\n{'alone,': ['a', 'lone']},\n{'or': ['or']},\n{'by': ['by']},\n{'any': ['an', 'y']},\n{'of': ['of']},\n{'these': ['these']},\n{'sounds': ['sounds']},\n{'preceded,': ['pre', 'ced', 'ed']},\n{'followed,': ['fol', 'lowed']},\n{'or': ['or']},\n{'surrounded': ['sur', 'round', 'ed']},\n{'by': ['by']},\n{'one': ['one']},\n{'or': ['or']},\n{'more': ['more']},\n{'consonants.': ['con', 'so', 'nants']}]",
    "CompVis/stable-diffusion-v-1-2-original": "Stable Diffusion v1 Model Card\nModel Details\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v1 Model Card\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nThe Stable-Diffusion-v-1-2 checkpoint was initialized with the weights of the Stable-Diffusion-v-1-1\ncheckpoint and subsequently fine-tuned on 515,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5.\nFor more information, please refer to Training.\nDownload the weights\nsd-v1-2.ckpt\nsd-v1-2-full-ema.ckpt\nThis weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the D🧨iffusers library, come here.\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (CLIP ViT-L/14) as suggested in the Imagen paper.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and\ntasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is taken from the DALLE-MINI model card, but applies in the same way to Stable Diffusion v1.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a large-scale dataset\nLAION-5B which contains adult material\nand is not fit for product use without additional safety mechanisms and\nconsiderations.\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\nThe training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion v1 was trained on subsets of LAION-2B(en),\nwhich consists of images that are primarily limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-2B (en) and subsets thereof (see next section)\nTraining Procedure\nStable Diffusion v1 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through a ViT-L/14 text-encoder.\nThe non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\nWe currently provide three checkpoints, sd-v1-1.ckpt, sd-v1-2.ckpt and sd-v1-3.ckpt,\nwhich were trained as follows,\nsd-v1-1.ckpt: 237k steps at resolution 256x256 on laion2B-en.\n194k steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).\nsd-v1-2.ckpt: Resumed from sd-v1-1.ckpt.\n515k steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator).\nsd-v1-3.ckpt: Resumed from sd-v1-2.ckpt. 195k steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 2\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 150000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 11250 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach and Patrick Esser and is based on the DALL-E Mini model card.",
    "CompVis/stable-diffusion": "Stable Diffusion\nStable Diffusion Version 1\nModel Access\nDemo\nLicense\nCitation\nStable Diffusion\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nThis model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access.\nStable Diffusion Version 1\nFor the first version 4 model checkpoints are released.\nHigher versions have been trained for longer and are thus usually better in terms of image generation quality then lower versions. More specifically:\nstable-diffusion-v1-1: The checkpoint is randomly initialized and has been trained on 237,000 steps at resolution 256x256 on laion2B-en.\n194,000 steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).\nstable-diffusion-v1-2: The checkpoint resumed training from stable-diffusion-v1-1.\n515,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator).\nstable-diffusion-v1-3: The checkpoint resumed training from stable-diffusion-v1-2. 195,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling\nstable-diffusion-v1-4: The checkpoint resumed training from stable-diffusion-v1-2. 195,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nstable-diffusion-v1-4 Resumed from stable-diffusion-v1-2.225,000 steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nModel Access\nEach checkpoint can be used both with Hugging Face's  🧨 Diffusers library or the original Stable Diffusion GitHub repository. Note that you have to \"click-request\" them on each respective model repository.\n🤗's 🧨 Diffusers library\nStable Diffusion GitHub repository\nstable-diffusion-v1-1\nstable-diffusion-v-1-1-original\nstable-diffusion-v1-2\nstable-diffusion-v-1-2-original\nstable-diffusion-v1-3\nstable-diffusion-v-1-3-original\nstable-diffusion-v1-4\nstable-diffusion-v-1-4-original\nDemo\nTo quickly try out the model, you can try out the Stable Diffusion Space.\nLicense\nThe CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach and Patrick Esser and is based on the DALL-E Mini model card.",
    "CompVis/stable-diffusion-v1-1": "Stable Diffusion v1-1 Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nTraining Data\nTraining Procedure\nTraining details\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v1-1 Model Card\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at 🤗's Stable Diffusion with D🧨iffusers blog.\nThe Stable-Diffusion-v1-1 was trained on 237,000 steps at resolution 256x256 on laion2B-en, followed by\n194,000 steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024). For more information, please refer to Training.\nThis weights here are intended to be used with the D🧨iffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, come here\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (CLIP ViT-L/14) as suggested in the Imagen paper.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nWe recommend using 🤗's Diffusers library to run Stable Diffusion.\npip install --upgrade diffusers transformers scipy\nRunning the pipeline with the default PNDM scheduler:\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\nmodel_id = \"CompVis/stable-diffusion-v1-1\"\ndevice = \"cuda\"\npipe = StableDiffusionPipeline.from_pretrained(model_id)\npipe = pipe.to(device)\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\nimage = pipe(prompt)[\"sample\"][0]\nimage.save(\"astronaut_rides_horse.png\")\nNote:\nIf you are limited by GPU memory and have less than 10GB of GPU RAM available, please make sure to load the StableDiffusionPipeline in float16 precision instead of the default float32 precision as done above. You can do so by telling diffusers to expect the weights to be in float16 precision:\nimport torch\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\nimage = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]\nimage.save(\"astronaut_rides_horse.png\")\nTo swap out the noise scheduler, pass it to from_pretrained:\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\nmodel_id = \"CompVis/stable-diffusion-v1-1\"\n# Use the K-LMS scheduler here instead\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, use_auth_token=True)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nwith autocast(\"cuda\"):\nimage = pipe(prompt, guidance_scale=7.5)[\"sample\"][0]\nimage.save(\"astronaut_rides_horse.png\")\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and\ntasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is taken from the DALLE-MINI model card, but applies in the same way to Stable Diffusion v1.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a large-scale dataset\nLAION-5B which contains adult material\nand is not fit for product use without additional safety mechanisms and\nconsiderations.\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\nThe training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion v1 was trained on subsets of LAION-2B(en),\nwhich consists of images that are primarily limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-2B (en) and subsets thereof (see next section)\nTraining Procedure\nStable Diffusion v1-4 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through a ViT-L/14 text-encoder.\nThe non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\nWe currently provide four checkpoints, which were trained as follows.\nstable-diffusion-v1-1: 237,000 steps at resolution 256x256 on laion2B-en.\n194,000 steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).\nstable-diffusion-v1-2: Resumed from stable-diffusion-v1-1.\n515,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator).\nstable-diffusion-v1-3: Resumed from stable-diffusion-v1-2. 195,000 steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nstable-diffusion-v1-4 Resumed from stable-diffusion-v1-2.225,000 steps at resolution 512x512 on \"laion-aesthetics v2 5+\"  and 10 % dropping of the text-conditioning to improve classifier-free guidance sampling.\nTraining details\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 2\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 150000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 11250 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach and Patrick Esser and is based on the DALL-E Mini model card.",
    "nghuyong/ernie-3.0-base-zh": "ERNIE-3.0-base-zh\nIntroduction\nReleased Model Info\nHow to use\nCitation\nERNIE-3.0-base-zh\nIntroduction\nERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\nMore detail: https://arxiv.org/abs/2107.02137\nReleased Model Info\nThis released pytorch model is converted from the officially released PaddlePaddle ERNIE model and\na series of experiments have been conducted to check the accuracy of the conversion.\nOfficial PaddlePaddle ERNIE repo:https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/ERNIE/contents.html\nPytorch Conversion repo:  https://github.com/nghuyong/ERNIE-Pytorch\nHow to use\nThen you can load ERNIE-3.0 model as before:\nfrom transformers import BertTokenizer, ErnieForMaskedLM\ntokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-base-zh\")\nmodel = ErnieForMaskedLM.from_pretrained(\"nghuyong/ernie-3.0-base-zh\")\nCitation\n@article{sun2021ernie,\ntitle={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},\nauthor={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},\njournal={arXiv preprint arXiv:2107.02137},\nyear={2021}\n}",
    "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7": "Model card for mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\nModel description\nHow to use the model\nTraining data\nTraining procedure\nEval results\nLimitations and bias\nCitation\nIdeas for cooperation or questions?\nDebugging and issues\nModel card for mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\nModel description\nThis multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people.\nAs of December 2021, mDeBERTa-v3-base is the best performing multilingual base-sized transformer model introduced by Microsoft in this paper.\nHow to use the model\nSimple zero-shot classification pipeline\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\nsequence_to_classify = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\nNLI use-case\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\npremise = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\nhypothesis = \"Emmanuel Macron is the President of France\"\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\nTraining data\nThis model was trained on the multilingual-nli-26lang-2mil7 dataset and the XNLI validation dataset.\nThe multilingual-nli-26lang-2mil7 dataset contains 2 730 000 NLI hypothesis-premise pairs in 26 languages spoken by more than 4 billion people. The dataset contains 105 000 text pairs per language. It is based on the English datasets MultiNLI, Fever-NLI, ANLI, LingNLI and WANLI and was created using the latest open-source machine translation models. The languages in the dataset are: ['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh'] (see ISO language codes. For more details, see the datasheet. In addition, a sample of 105 000 text pairs was also added for English following the same sampling method as the other languages, leading to 27 languages.\nMoreover, for each language a random set of 10% of the hypothesis-premise pairs was added where an English hypothesis was paired with the premise in the other language (and the same for English premises and other language hypotheses). This mix of languages in the text pairs should enable users to formulate a hypothesis in English for a target text in another language.\nThe XNLI validation set consists of 2490 professionally translated texts from English to 14 other languages (37350 texts in total) (see this paper). Note that XNLI also contains a training set of 14 machine translated versions of the MultiNLI dataset for 14 languages, but this data was excluded due to quality issues with the machine translations from 2018.\nNote that for evaluation purposes, three languages were excluded from the XNLI training data and only included in the test data: [\"bg\",\"el\",\"th\"]. This was done in order to test the performance of the model on languages it has not seen during NLI fine-tuning on 27 languages, but only during pre-training on 100 languages - see evaluation metrics below.\nThe total training dataset had a size of 3 287 280 hypothesis-premise pairs.\nTraining procedure\nmDeBERTa-v3-base-mnli-xnli was trained using the Hugging Face trainer with the following hyperparameters.\ntraining_args = TrainingArguments(\nnum_train_epochs=3,              # total number of training epochs\nlearning_rate=2e-05,\nper_device_train_batch_size=32,   # batch size per device during training\ngradient_accumulation_steps=2,   # to double the effective batch size for\nwarmup_ratio=0.06,                # number of warmup steps for learning rate scheduler\nweight_decay=0.01,               # strength of weight decay\nfp16=False\n)\nEval results\nThe model was evaluated on the XNLI test set in 15 languages (5010 texts per language, 75150 in total) and the English test sets of MultiNLI, Fever-NLI, ANLI, LingNLI and WANLI . Note that multilingual NLI models are capable of classifying NLI texts without receiving NLI training data in the specific language (cross-lingual transfer). This means that the model is also able to do NLI on the other 73 languages mDeBERTa was pre-trained on, but performance is most likely lower than for those languages seen during NLI fine-tuning. The performance on the languages [\"bg\",\"el\",\"th\"] in the table below is a good indicated of this cross-lingual transfer, as these languages were not included in the training data.\nXNLI subsets\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\nth\ntr\nur\nvi\nzh\nAccuracy\n0.794\n0.822\n0.824\n0.809\n0.871\n0.832\n0.823\n0.769\n0.803\n0.746\n0.786\n0.792\n0.744\n0.793\n0.803\nSpeed (text/sec, A100-GPU)\n1344.0\n1355.0\n1472.0\n1149.0\n1697.0\n1446.0\n1278.0\n1115.0\n1380.0\n1463.0\n1713.0\n1594.0\n1189.0\n877.0\n1887.0\nEnglish Datasets\nmnli_test_m\nmnli_test_mm\nanli_test\nanli_test_r3\nfever_test\nling_test\nwanli_test\nAccuracy\n0.857\n0.856\n0.537\n0.497\n0.761\n0.788\n0.732\nSpeed (text/sec, A100-GPU)\n1000.0\n1009.0\n794.0\n672.0\n374.0\n1177.0\n1468.0\nAlso note that if other multilingual models on the model hub claim performance of around 90% on languages other than English, the authors have most likely made a mistake during testing since non of the latest papers shows a multilingual average performance of more than a few points above 80% on XNLI (see here or here).\nLimitations and bias\nPlease consult the original DeBERTa-V3 paper and literature on different NLI datasets for potential biases. Moreover, note that the multilingual-nli-26lang-2mil7 dataset was created using machine translation, which reduces the quality of the data for a complex task like NLI. You can inspect the data via the Hugging Face dataset viewer for languages you are interested in. Note that grammatical errors introduced by machine translation are less of an issue for zero-shot classification, for which grammar is less important.\nCitation\nIf the dataset is useful for you, please cite the following article:\n@article{laurer_less_2022,\ntitle = {Less {Annotating}, {More} {Classifying} – {Addressing} the {Data} {Scarcity} {Issue} of {Supervised} {Machine} {Learning} with {Deep} {Transfer} {Learning} and {BERT} - {NLI}},\nurl = {https://osf.io/74b8k},\nlanguage = {en-us},\nurldate = {2022-07-28},\njournal = {Preprint},\nauthor = {Laurer, Moritz and Atteveldt, Wouter van and Casas, Andreu Salleras and Welbers, Kasper},\nmonth = jun,\nyear = {2022},\nnote = {Publisher: Open Science Framework},\n}\nIdeas for cooperation or questions?\nFor updates on new models and datasets, follow me on Twitter.\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or on LinkedIn\nDebugging and issues\nNote that DeBERTa-v3 was released in late 2021 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers==4.13 or higher might solve some issues. Note that mDeBERTa currently does not support FP16, see here: https://github.com/microsoft/DeBERTa/issues/77",
    "microsoft/xclip-base-patch32": "X-CLIP (base-sized model)\nModel description\nIntended uses & limitations\nHow to use\nTraining data\nPreprocessing\nEvaluation results\nX-CLIP (base-sized model)\nX-CLIP model (base-sized, patch resolution of 32) trained fully-supervised on Kinetics-400. It was introduced in the paper Expanding Language-Image Pretrained Models for General Video Recognition by Ni et al. and first released in this repository.\nThis model was trained using 8 frames per video, at a resolution of 224x224.\nDisclaimer: The team releasing X-CLIP did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nX-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs.\nThis allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\nIntended uses & limitations\nYou can use the raw model for determining how well text goes with a given video. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nFor code examples, we refer to the documentation.\nTraining data\nThis model was trained on Kinetics-400.\nPreprocessing\nThe exact details of preprocessing during training can be found here.\nThe exact details of preprocessing during validation can be found here.\nDuring validation, one resizes the shorter edge of each frame, after which center cropping is performed to a fixed-size resolution (like 224x224). Next, frames are normalized across the RGB channels with the ImageNet mean and standard deviation.\nEvaluation results\nThis model achieves a top-1 accuracy of 80.4% and a top-5 accuracy of 95.0%.",
    "busecarik/berturk-sunlp-ner-turkish": "berturk-sunlp-ner-turkish\nIntroduction\nTraining data\nHow to use berturk-sunlp-ner-turkish with HuggingFace\nModel performances on SUNLP-NER-Twitter test set (metric: seqeval)\nberturk-sunlp-ner-turkish\nIntroduction\n[berturk-sunlp-ner-turkish] is a NER model that was fine-tuned from the BERTurk-cased model on the SUNLP-NER-Twitter dataset.\nTraining data\nThe model was trained on the SUNLP-NER-Twitter dataset (5000 tweets). The dataset can be found at https://github.com/SU-NLP/SUNLP-Twitter-NER-Dataset\nNamed entity types are as follows:\nPerson, Location, Organization, Time, Money, Product, TV-Show\nHow to use berturk-sunlp-ner-turkish with HuggingFace\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(\"busecarik/berturk-sunlp-ner-turkish\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"busecarik/berturk-sunlp-ner-turkish\")\nModel performances on SUNLP-NER-Twitter test set (metric: seqeval)\nPrecision\nRecall\nF1\n82.96\n82.42\n82.69\nClassification Report\nEntity\nPrecision\nRecall\nF1\nLOCATION\n0.70\n0.80\n0.74\nMONEY\n0.80\n0.71\n0.75\nORGANIZATION\n0.78\n0.86\n0.78\nPERSON\n0.90\n0.91\n0.91\nPRODUCT\n0.44\n0.47\n0.45\nTIME\n0.94\n0.85\n0.89\nTVSHOW\n0.61\n0.35\n0.45\nYou can cite the following paper, if you use this model:\n@InProceedings{ark-yeniterzi:2022:LREC,\nauthor    = {\\c{C}ar\\i k, Buse  and  Yeniterzi, Reyyan},\ntitle     = {A Twitter Corpus for Named Entity Recognition in Turkish},\nbooktitle      = {Proceedings of the Language Resources and Evaluation Conference},\nmonth          = {June},\nyear           = {2022},\naddress        = {Marseille, France},\npublisher      = {European Language Resources Association},\npages     = {4546--4551},\nurl       = {https://aclanthology.org/2022.lrec-1.484}\n}"
}