{
    "dearyoungjo/Whisper-Medicalv1CNB1": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nDistil-Whisper: distil-large-v3\nTable of Contents\nTransformers Usage\nShort-Form Transcription\nSequential Long-Form\nChunked Long-Form\nSpeculative Decoding\nAdditional Speed & Memory Improvements\nLibrary Integrations\nWhisper.cpp\nFaster-Whisper\nOpenAI Whisper\nTransformers.js\nCandle\nModel Details\nDifferences with distil-large-v2\nEvaluation\nIntended Use\nData\nWER Filter\nTraining\nResults\nReproducing Distil-Whisper\nLicense\nCitation\nAcknowledgements\nlicense: mit\n<<<<<<< HEAD\nlanguage:\n- en\nlicense: mit\nlibrary_name: transformers\ntags:\n- audio\n- automatic-speech-recognition\n- transformers.js\nwidget:\n- example_title: LibriSpeech sample 1\nsrc: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: LibriSpeech sample 2\nsrc: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nThis is a working space for a fine tune of Distil-Whisper-Large for medical speech recognition. The model will change often, so duplicate the space if you find it useful for your needs as it is.\nDistil-Whisper: distil-large-v3\nDistil-Whisper was proposed in the paper Robust Knowledge Distillation via Large-Scale Pseudo Labelling.\nThis is the third and final installment of the Distil-Whisper English series. It the knowledge distilled version of\nOpenAI's Whisper large-v3, the latest and most performant Whisper model\nto date.\nCompared to previous Distil-Whisper models, the distillation procedure for distil-large-v3 has been adapted to give\nsuperior long-form transcription accuracy with OpenAI's sequential long-form algorithm.\nThe result is a distilled model that performs to within 1% WER of large-v3 on long-form audio using both the sequential\nand chunked algorithms, and outperforms distil-large-v2 by 4.8% using the sequential algorithm. The model is also faster\nthan previous Distil-Whisper models: 6.3x faster than large-v3, and 1.1x faster than distil-large-v2.\nModel\nParams / M\nRel. Latency\nShort-Form\nSequential Long-Form\nChunked Long-Form\nlarge-v3\n1550\n1.0\n8.4\n10.0\n11.0\ndistil-large-v3\n756\n6.3\n9.7\n10.8\n10.9\ndistil-large-v2\n756\n5.8\n10.1\n15.6\n11.6\nSince the sequential algorithm is the \"de-facto\" transcription algorithm across the most popular Whisper libraries\n(Whisper cpp, Faster-Whisper, OpenAI Whisper), this distilled model is designed to be compatible with these libraries.\nYou can expect significant performance gains by switching from previous Distil-Whisper checkpoints to distil-large-v3\nwhen using these libraries. For convenience, the weights for the most popular libraries are already converted,\nwith instructions for getting started below.\nTable of Contents\nTransformers Usage\nShort-Form Transcription\nSequential Long-Form\nChunked Long-Form\nSpeculative Decoding\nAdditional Speed and Memory Improvements\nLibrary Integrations\nWhisper cpp\nFaster Whisper\nOpenAI Whisper\nTransformers.js\nCandle\nModel Details\nLicense\nTransformers Usage\ndistil-large-v3 is supported in the Hugging Face ðŸ¤— Transformers library from version 4.39 onwards. To run the model, first\ninstall the latest version of Transformers. For this example, we'll also install ðŸ¤— Datasets to load a toy audio dataset\nfrom the Hugging Face Hub:\npip install --upgrade pip\npip install --upgrade transformers accelerate datasets[audio]\nShort-Form Transcription\nThe model can be used with the pipeline\nclass to transcribe short-form audio files (< 30-seconds) as follows:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"distil-whisper/distil-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nmax_new_tokens=128,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n- result = pipe(sample)\n+ result = pipe(\"audio.mp3\")\nFor segment-level timestamps, pass the argument return_timestamps=True and return the \"chunks\" output:\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\nFor more control over the generation parameters, use the model + processor API directly:\nAd-hoc generation arguments can be passed to model.generate, including num_beams for beam-search, return_timestamps\nfor segment-level timestamps, and prompt_ids for prompting. See the docstrings\nfor more details.\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"distil-whisper/distil-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\ninput_features = processor(\nsample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\"\n).input_features\ninput_features = input_features.to(device, dtype=torch_dtype)\ngen_kwargs = {\n\"max_new_tokens\": 128,\n\"num_beams\": 1,\n\"return_timestamps\": False,\n}\npred_ids = model.generate(input_features, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=gen_kwargs[\"return_timestamps\"])\nprint(pred_text)\nSequential Long-Form\nUnlike previous Distil-Whisper releases, distil-large-v3 is specifically designed to be compatible with OpenAI's sequential\nlong-form transcription algorithm. This algorithm uses a sliding window for buffered inference of long audio files (> 30-seconds),\nand returns more accurate transcriptions compared to the chunked long-form algorithm.\nThe sequential long-form algorithm should be used in either of the following scenarios:\nTranscription accuracy is the most important factor, and latency is less of a consideration\nYou are transcribing batches of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\nIf you are transcribing single long audio files and latency is the most important factor, you should use the chunked algorithm\ndescribed below. For a detailed explanation of the different algorithms, refer to Sections 5 of\nthe Distil-Whisper paper.\nThe pipeline\nclass can be used to transcribe long audio files with the sequential algorithm as follows:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"distil-whisper/distil-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nmax_new_tokens=128,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nFor more control over the generation parameters, use the model + processor API directly:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"distil-whisper/distil-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\ninputs = processor(\nsample[\"array\"],\nsampling_rate=sample[\"sampling_rate\"],\nreturn_tensors=\"pt\",\ntruncation=False,\npadding=\"longest\",\nreturn_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\ngen_kwargs = {\n\"max_new_tokens\": 448,\n\"num_beams\": 1,\n\"condition_on_prev_tokens\": False,\n\"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n\"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n\"logprob_threshold\": -1.0,\n\"no_speech_threshold\": 0.6,\n\"return_timestamps\": True,\n}\npred_ids = model.generate(**i   nputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\nprint(pred_text)\nChunked Long-Form\ndistil-large-v3 remains compatible with the Transformers chunked long-form algorithm. This algorithm should be used when\na single large audio file is being transcribed and the fastest possible inference is required. In such circumstances,\nthe chunked algorithm is up to 9x faster than OpenAI's sequential long-form implementation (see Table 7 of the\nDistil-Whisper paper).\nTo enable chunking, pass the chunk_length_s parameter to the pipeline. For distil-large-v3, a chunk length of 25-seconds\nis optimal. To activate batching over long audio files, pass the argument batch_size:\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"distil-whisper/distil-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nmax_new_tokens=128,\nchunk_length_s=25,\nbatch_size=16,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nSpeculative Decoding\ndistil-large-v3 is the first Distil-Whisper model that can be used as an assistant to Whisper large-v3 for speculative decoding.\nSpeculative decoding mathematically ensures that exactly the same outputs as Whisper are obtained, while being 2 times faster.\nThis makes it the perfect drop-in replacement for existing Whisper pipelines, since the same outputs are guaranteed.\nIn the following code-snippet, we load the assistant Distil-Whisper model standalone to the main Whisper pipeline. We then\nspecify it as the \"assistant model\" for generation:\nfrom transformers import pipeline, AutoModelForCausalLM, AutoModelForSpeechSeq2Seq, AutoProcessor\nimport torch\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nassistant_model_id = \"distil-whisper/distil-large-v3\"\nassistant_model = AutoModelForCausalLM.from_pretrained(\nassistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nassistant_model.to(device)\nmodel_id = \"openai/whisper-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nmax_new_tokens=128,\ngenerate_kwargs={\"assistant_model\": assistant_model},\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nFor more details on speculative decoding, refer to the blog post Speculative Decoding for 2x Faster Whisper Inference.\nAdditional Speed & Memory Improvements\nYou can apply additional speed and memory improvements to Distil-Whisper to further reduce the inference speed and VRAM\nrequirements. These optimisations primarily target the attention kernel, swapping it from an eager implementation to a\nmore efficient flash attention version.\nFlash Attention 2\nWe recommend using Flash-Attention 2\nif your GPU allows for it. To do so, you first need to install Flash Attention:\npip install flash-attn --no-build-isolation\nThen pass attn_implementation=\"flash_attention_2\" to from_pretrained:\n- model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n+ model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, attn_implementation=\"flash_attention_2\")\nTorch Scale-Product-Attention (SDPA)\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch scaled dot-product attention (SDPA).\nThis attention implementation is activated by default for PyTorch versions 2.1.1 or greater. To check\nwhether you have a compatible PyTorch version, run the following Python code snippet:\nfrom transformers.utils import is_torch_sdpa_available\nprint(is_torch_sdpa_available())\nIf the above returns True, you have a valid version of PyTorch installed and SDPA is activated by default. If it\nreturns False, you need to upgrade your PyTorch version according to the official instructions\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying\nattn_implementation=\"sdpa\" as follows:\n- model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n+ model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, attn_implementation=\"sdpa\")\nTorch compile\nComing soon...\n4-bit and 8-bit Inference\nComing soon...\nLibrary Integrations\nWhisper.cpp\nDistil-Whisper can be run with the Whisper.cpp package with the original\nsequential long-form transcription algorithm. In a provisional benchmark on Mac M1, distil-large-v3 is over 5x faster\nthan Whisper large-v3, while performing to within 0.8% WER over long-form audio.\nSteps for getting started:\nClone the Whisper.cpp repository:\ngit clone https://github.com/ggerganov/whisper.cpp.git\ncd whisper.cpp\nInstall the Hugging Face Hub Python package:\npip install --upgrade huggingface_hub\nAnd download the GGML weights for distil-large-v3 using the following Python snippet:\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id='distil-whisper/distil-large-v3-ggml', filename='ggml-distil-large-v3.bin', local_dir='./models')\nNote that if you do not have a Python environment set-up, you can also download the weights directly with wget:\nwget https://huggingface.co/distil-whisper/distil-large-v3-ggml/resolve/main/ggml-distil-large-v3.bin -P ./models\nRun inference using the provided sample audio:\nmake -j && ./main -m models/ggml-distil-large-v3.bin -f samples/jfk.wav\nFaster-Whisper\nFaster-Whisper is a reimplementation of Whisper using CTranslate2, a fast\ninference engine for Transformer models.\nFirst, install the Faster-Whisper package according to the official instructions.\nFor this example, we'll also install ðŸ¤— Datasets to load a toy audio dataset from the Hugging Face Hub:\npip install --upgrade pip\npip install --upgrade git+https://github.com/SYSTRAN/faster-whisper datasets[audio]\nThe following code snippet loads the distil-large-v3 model and runs inference on an example file from the LibriSpeech ASR\ndataset:\nimport torch\nfrom faster_whisper import WhisperModel\nfrom datasets import load_dataset\n# define our torch configuration\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ncompute_type = \"float16\" if torch.cuda.is_available() else \"float32\"\n# load model on GPU if available, else cpu\nmodel = WhisperModel(\"distil-large-v3\", device=device, compute_type=compute_type)\n# load toy dataset for example\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsample = dataset[1][\"audio\"][\"path\"]\nsegments, info = model.transcribe(sample, beam_size=1)\nfor segment in segments:\nprint(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\nTo transcribe a local audio file, simply pass the path to the audio file as the audio argument to transcribe:\nsegments, info = model.transcribe(\"audio.mp3\", beam_size=1)\nOpenAI Whisper\nTo use the model in the original Whisper format, first ensure you have the openai-whisper package installed.\nFor this example, we'll also install ðŸ¤— Datasets to load a toy audio dataset from the Hugging Face Hub:\npip install --upgrade pip\npip install --upgrade openai-whisper datasets[audio]\nThe following code-snippet demonstrates how to transcribe a sample file from the LibriSpeech dataset loaded using\nðŸ¤— Datasets:\nfrom huggingface_hub import hf_hub_download\nfrom datasets import load_dataset\nfrom whisper import load_model, transcribe\nmodel_path = hf_hub_download(repo_id=\"distil-whisper/distil-large-v3-openai\", filename=\"model.bin\")\nmodel = load_model(model_path)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"][\"path\"]\npred_out = transcribe(model, audio=sample, language=\"en\")\nprint(pred_out[\"text\"])\nNote that the model weights will be downloaded and saved to your cache the first time you run the example. Subsequently,\nyou can re-use the same example, and the weights will be loaded directly from your cache without having to download them\nagain.\nTo transcribe a local audio file, simply pass the path to the audio file as the audio argument to transcribe:\npred_out = transcribe(model, audio=sample, language=\"en\")\nThe Distil-Whisper model can also be used with the OpenAI Whisper CLI. Refer to the following instructions\nfor details.\nTransformers.js\nDistil-Whisper can be run completely in your web browser with Transformers.js:\nInstall Transformers.js from NPM:\nnpm i @xenova/transformers\nImport the library and perform inference with the pipeline API.\nimport { pipeline } from '@xenova/transformers';\nconst transcriber = await pipeline('automatic-speech-recognition', 'distil-whisper/distil-large-v3');\nconst url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\nconst output = await transcriber(url);\n// { text: \" And so, my fellow Americans, ask not what your country can do for you. Ask what you can do for your country.\" }\nCheck out the online Distil-Whisper Web Demo to try it out yourself.\nAs you'll see, it runs locally in your browser: no server required!\nRefer to the Transformers.js docs\nfor further information.\nCandle\nThrough an integration with Hugging Face Candle ðŸ•¯ï¸, Distil-Whisper is\navailable in the Rust library ðŸ¦€\nBenefit from:\nOptimised CPU backend with optional MKL support for Linux x86 and Accelerate for Macs\nMetal support for efficiently running on Macs\nCUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL\nWASM support: run Distil-Whisper in a browser\nSteps for getting started:\nInstall candle-core as explained here\nClone the candle repository locally:\ngit clone https://github.com/huggingface/candle.git\nEnter the example directory for Whisper:\ncd candle/candle-examples/examples/whisper\nRun an example:\ncargo run --example whisper --release --features symphonia -- --model distil-large-v3\nTo specify your own audio file, add the --input flag:\ncargo run --example whisper --release --features symphonia -- --model distil-large-v3 --input audio.wav\nTip: for compiling using Apple Metal, specify the metal feature when you run the example:\ncargo run --example whisper --release --features=\"symphonia,metal\" -- --model distil-large-v3\nNote that if you encounter the error:\nerror: target `whisper` in package `candle-examples` requires the features: `symphonia`\nConsider enabling them by passing, e.g., `--features=\"symphonia\"`\nYou should clean your cargo installation:\ncargo clean\nAnd subsequently recompile:\ncargo run --example whisper --release --features symphonia -- --model distil-large-v3\nModel Details\nDistil-Whisper inherits the encoder-decoder architecture from Whisper. The encoder maps a sequence of speech vector\ninputs to a sequence of hidden-state vectors. The decoder auto-regressively predicts text tokens, conditional on all\nprevious tokens and the encoder hidden-states. Consequently, the encoder is only run forward once, whereas the decoder\nis run as many times as the number of tokens generated. In practice, this means the decoder accounts for over 90% of\ntotal inference time. Thus, to optimise for latency, the focus is on minimising the inference time of the decoder.\nTo distill the Whisper model, we reduce the number of decoder layers while keeping the encoder fixed.\nThe encoder (shown in green) is entirely copied from the teacher to the student and frozen during training.\nThe student's decoder consists of a subset of the teacher decoder layers, which are intialised from maximally spaced layers.\nThe model is then trained on a weighted sum of the KL divergence and pseudo-label loss terms.\nDifferences with distil-large-v2\nCompared to previous version of Distil-Whisper, distil-large-v3 is specifically designed to target the OpenAI sequential\nlong-form transcription algorithm. There are no architectural differences compared to distil-large-v2, other than the fact\nthe model layers are intialised from the latest large-v3 model rather than the older large-v2 one. The differences lie\nin the way the model was trained.\nPrevious Distil-Whisper models were trained on a mean input length of 7-seconds, whereas the original Whisper models were\npre-trained on 30-second inputs. During distillation, we shift the distribution of the model weights to the distribution\nof our training data. If our training data contains shorter utterances (e.g. on average 7-seconds audio instead of 30-seconds),\nthen the predicted distribution shifts to this shorter context length. At inference time, the optimal context window for\ndistil-large-v2 was an interpolation of these two values: 15-seconds. Beyond this time, the predictions for the distil-large-v2\nmodel were largely inaccurate, particularly for the timestamp predictions. However, the sequential long-form algorithm\nuses 30-second sliding windows for inference, with the window shifted according to the last predicted timestamp. Since the\nlast timestamp typically occurs after the 15-second mark, it was predicted with low accuracy, causing the long-form\ntranscription to often fail.\nTo preserve Whisper's ability to transcribe sliding 30-second windows, as is done with sequential decoding, we need to\nensure the context length of distil-large-v3 is also 30-seconds. This was primarily achieved with four strategies:\nPacking the audio samples in the training dataset to 30-seconds: since the model is both pre-trained and distilled on audio data packed to 30-seconds, distil-large-v3 now operates on the same ideal context window as Whisper, predicting accurate timestamps up to and including 30-seconds.\nFreezing the decoder input embeddings: we use the same input embeds representation as the original model, which is designed to handle longer context lengths than previous Distil-Whisper iterations.\nUsing a longer maximum context length during training: instead of training on a maximum target length of 128, we train on a maximum of 256. This helps distil-large-v3 transcribe 30-second segments where the number of tokens possibly exceeds 128.\nAppending prompt conditioning to 50% of the training samples: enables the model to be used with the condition_on_prev_tokens argument, and context windows up to 448 tokens.\nThere were further tricks that were employed to improve the performance of distil-large-v3 under the sequential decoding\nalgorithm, which we be explained fully in an upcoming blog post.\nEvaluation\nThe following code-snippets demonstrates how to evaluate the Distil-Whisper model on the LibriSpeech validation-clean\ndataset with streaming mode, meaning no\naudio data has to be downloaded to your local device.\nFirst, we need to install the required packages, including ðŸ¤— Datasets to stream and load the audio data, and ðŸ¤— Evaluate to\nperform the WER calculation:\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] evaluate jiwer\nEvaluation can then be run end-to-end with the following example:\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import load_dataset\nfrom evaluate import load\nimport torch\nfrom tqdm import tqdm\n# define our torch configuration\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"distil-whisper/distil-large-v3\"\n# load the model + processor\nmodel =  AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, use_safetensors=True, low_cpu_mem_usage=True)\nmodel = model.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# load the dataset with streaming mode\ndataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\n# define the evaluation metric\nwer_metric = load(\"wer\")\ndef inference(batch):\n# 1. Pre-process the audio data to log-mel spectrogram inputs\naudio = [sample[\"array\"] for sample in batch[\"audio\"]]\ninput_features = processor(audio, sampling_rate=batch[\"audio\"][0][\"sampling_rate\"], return_tensors=\"pt\").input_features\ninput_features = input_features.to(device, dtype=torch_dtype)\n# 2. Auto-regressively generate the predicted token ids\npred_ids = model.generate(input_features, max_new_tokens=128)\n# 3. Decode the token ids to the final transcription\nbatch[\"transcription\"] = processor.batch_decode(pred_ids, skip_special_tokens=True)\nbatch[\"reference\"] = batch[\"text\"]\nreturn batch\n# batch size 16 inference\ndataset = dataset.map(function=inference, batched=True, batch_size=16)\nall_transcriptions = []\nall_references = []\n# iterate over the dataset and run inference\nfor result in tqdm(dataset, desc=\"Evaluating...\"):\nall_transcriptions.append(result[\"transcription\"])\nall_references.append(result[\"reference\"])\n# normalize predictions and references\nall_transcriptions = [processor.normalize(transcription) for transcription in all_transcriptions]\nall_references = [processor.normalize(reference) for reference in all_references]\n# compute the WER metric\nwer = 100 * wer_metric.compute(predictions=all_transcriptions, references=all_references)\nprint(wer)\nPrint Output:\n2.428920763531516\nIntended Use\nDistil-Whisper is intended to be a drop-in replacement for Whisper large-v3 on English speech recognition. In particular, it\nachieves comparable WER results over out-of-distribution (OOD) test data, while being 6x faster on both short and long-form audio.\nData\nDistil-Whisper is trained on 22,000 hours of audio data from nine open-source, permissively licensed speech datasets on the\nHugging Face Hub:\nDataset\nSize / h\nSpeakers\nDomain\nLicence\nPeople's Speech\n12,000\nunknown\nInternet Archive\nCC-BY-SA-4.0\nCommon Voice 13\n3,000\nunknown\nNarrated Wikipedia\nCC0-1.0\nGigaSpeech\n2,500\nunknown\nAudiobook, podcast, YouTube\napache-2.0\nFisher\n1,960\n11,900\nTelephone conversations\nLDC\nLibriSpeech\n960\n2,480\nAudiobooks\nCC-BY-4.0\nVoxPopuli\n540\n1,310\nEuropean Parliament\nCC0\nTED-LIUM\n450\n2,030\nTED talks\nCC-BY-NC-ND 3.0\nSwitchBoard\n260\n540\nTelephone conversations\nLDC\nAMI\n100\nunknown\nMeetings\nCC-BY-4.0\nTotal\n21,770\n18,260+\nThe combined dataset spans 10 distinct domains and over 50k speakers. The diversity of this dataset is crucial to ensuring\nthe distilled model is robust to audio distributions and noise.\nThe audio data is then pseudo-labelled using the Whisper large-v3 model: we use Whisper to generate predictions for all\nthe audio in our training set and use these as the target labels during training. Using pseudo-labels ensures that the\ntranscriptions are consistently formatted across datasets and provides sequence-level distillation signal during training.\nWER Filter\nThe Whisper pseudo-label predictions are subject to mis-transcriptions and hallucinations. To ensure we only train on\naccurate pseudo-labels, we employ a simple WER heuristic during training. First, we normalise the Whisper pseudo-labels\nand the ground truth labels provided by each dataset. We then compute the WER between these labels. If the WER exceeds\na specified threshold, we discard the training example. Otherwise, we keep it for training.\nSection 9.2 of the Distil-Whisper paper demonstrates the effectiveness of this filter\nfor improving downstream performance of the distilled model. We also partially attribute Distil-Whisper's robustness to\nhallucinations to this filter.\nTraining\nThe model was trained for 80,000 optimisation steps (or 11 epochs) with batch size 256. The Tensorboard training logs can\nbe found under: https://huggingface.co/distil-whisper/distil-large-v3/tensorboard?params=scalars#frame\nResults\nThe distilled model performs to within 1.5% WER of Whisper large-v3 on out-of-distribution (OOD) short-form audio, within\n1% WER on sequential long-form decoding, and outperforms large-v3 by 0.1% on chunked long-form. This performance gain is\nattributed to lower hallucinations.\nFor a detailed per-dataset breakdown of the evaluation results, refer to Tables 16 and 17 of the Distil-Whisper paper\nDistil-Whisper is also evaluated on the ESB benchmark datasets as part of the OpenASR leaderboard,\nwhere it performs to within 0.2% WER of Whisper.\nReproducing Distil-Whisper\nTraining and evaluation code to reproduce Distil-Whisper is available under the Distil-Whisper repository: https://github.com/huggingface/distil-whisper/tree/main/training\nThis code will shortly be updated to include the training updates described in the section Differences with distil-large-v2.\nLicense\nDistil-Whisper inherits the MIT license from OpenAI's Whisper model.\nCitation\nIf you use this model, please consider citing the Distil-Whisper paper:\n@misc{gandhi2023distilwhisper,\ntitle={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling},\nauthor={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},\nyear={2023},\neprint={2311.00430},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nAcknowledgements\nOpenAI for the Whisper model, in particular Jong Wook Kim for the original codebase and training discussions\nHugging Face ðŸ¤— Transformers for the model integration\nGeorgi Gerganov for the Whisper cpp integration\nSystran team for the Faster-Whisper integration\nJoshua Lochner for the Transformers.js integration\nLaurent Mazare for the Candle integration\nVaibhav Srivastav for Distil-Whisper distribution\nGoogle's TPU Research Cloud (TRC) programme for Cloud TPU v4 compute resource\nRaghav Sonavane for an early iteration of Distil-Whisper on the LibriSpeech dataset\nlicense: mit\n651514707e63b2be6a85783bad216934811257f3",
    "Kim2091/AnimeSharpV2": "2x-AnimeSharpV2 Set\n2x-AnimeSharpV2 Set\nScale: 2\nArchitecture: RealPLKSR, MoSR GPS, ESRGAN\nAuthor: Kim2091\nLicense: CC BY-NC-SA 4.0\nPurpose: Anime\nSubject:\nInput Type: Images\nDate: 10-3-24\nSize:\nI/O Channels: 3(RGB)->3(RGB)\nDataset: HFA2k modified\nDataset Size: 2-3k\nOTF (on the fly augmentations): No\nPretrained Model: 4x_realplksr_mssim_pretrain & 4x-MoSR_GPS_Pretrain\nIterations: 100k & 75k\nBatch Size: 6-10\nGT Size: 64-256\nDescription: This is my first anime model in years. Hopefully you guys can find a good use-case for it. Included are 5 models:\nESRGAN (Highest Quality, slowest)\nRealPLKSR (Higher quality, slower)\nMoSR (Lower quality, faster)\nThere are Sharp and Soft versions RPLSKR and MoSR, but only Soft for ESRGAN\nWhen to use each:\nSharp: For heavily degraded sources. Sharp models have issues depth of field but are best at removing artifacts\nSoft: For cleaner sources. Soft models preserve depth of field but may not remove other artifacts as well\nNotes:\nMoSR doesn't work in chaiNNer currently\nTo use MoSR:\nUse the ONNX version in tools like VideoJaNai\nUpdate spandrel in the latest version of ComfyUI\nThe ONNX version may produce slightly different results than the .pth version. If you have issues, try the .pth model.\nComparisons: https://slow.pics/c/4UI20Qlu\nhttps://github.com/user-attachments/assets/001c0eb2-7558-4294-b722-a7371f87a912",
    "TheDrummer/Tiger-Gemma-9B-v3-GGUF": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTiger Gemma 9B v3\nTiger Gemma 9B v3\nDecensored Gemma 9B SPPO with a new dataset that removes the yapping and evil.\nGGUF: https://huggingface.co/TheDrummer/Tiger-Gemma-9B-v3-GGUF\nTiger lives on ðŸ˜¹\nUpdate on his orange friend, now named Didi:\nI rescued Didi shortly before Tiger died from a car accident (they were both strays). He was lethargic and suffering from major respiratory problems. It has been 4 months since that fateful day and Didi is healthy and happier than ever. May Tiger live on through him!",
    "kesimeg/yolov8n-clothing-detection": "How to use the model?\nThis model is a finetuned version of YOLO-v8n on clothing detection.\nObject categories\nClothing\nShoes\nBags\nAccessories\nYou can get more information (and code ðŸŽ‰) on how to train or use the model on my github.\nHow to use the model?\nInstall ultralyticsplus:\npip install ultralyticsplus==0.0.23 ultralytics==8.0.21\nLoad model and perform prediction:\nfrom ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('kesimeg/yolov8n-clothing-detection')\n# set image\nimage = 'some_image.jpg'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()",
    "bartowski/BlackSheep-RP-12B-GGUF": "Llamacpp imatrix Quantizations of BlackSheep-RP-12B\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of BlackSheep-RP-12B\nUsing llama.cpp release b3878 for quantization.\nOriginal model: https://huggingface.co/KOOWEEYUS/BlackSheep-RP-12B\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\nNo prompt format found, check original model page\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nBlackSheep-RP-12B-f16.gguf\nf16\n24.50GB\nfalse\nFull F16 weights.\nBlackSheep-RP-12B-Q8_0.gguf\nQ8_0\n13.02GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nBlackSheep-RP-12B-Q6_K_L.gguf\nQ6_K_L\n10.38GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nBlackSheep-RP-12B-Q6_K.gguf\nQ6_K\n10.06GB\nfalse\nVery high quality, near perfect, recommended.\nBlackSheep-RP-12B-Q5_K_L.gguf\nQ5_K_L\n9.14GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nBlackSheep-RP-12B-Q5_K_M.gguf\nQ5_K_M\n8.73GB\nfalse\nHigh quality, recommended.\nBlackSheep-RP-12B-Q5_K_S.gguf\nQ5_K_S\n8.52GB\nfalse\nHigh quality, recommended.\nBlackSheep-RP-12B-Q4_K_L.gguf\nQ4_K_L\n7.98GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nBlackSheep-RP-12B-Q4_K_M.gguf\nQ4_K_M\n7.48GB\nfalse\nGood quality, default size for must use cases, recommended.\nBlackSheep-RP-12B-Q3_K_XL.gguf\nQ3_K_XL\n7.15GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nBlackSheep-RP-12B-Q4_K_S.gguf\nQ4_K_S\n7.12GB\nfalse\nSlightly lower quality with more space savings, recommended.\nBlackSheep-RP-12B-Q4_0.gguf\nQ4_0\n7.09GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nBlackSheep-RP-12B-Q4_0_8_8.gguf\nQ4_0_8_8\n7.07GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below).\nBlackSheep-RP-12B-Q4_0_4_8.gguf\nQ4_0_4_8\n7.07GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below).\nBlackSheep-RP-12B-Q4_0_4_4.gguf\nQ4_0_4_4\n7.07GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure.\nBlackSheep-RP-12B-IQ4_XS.gguf\nIQ4_XS\n6.74GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nBlackSheep-RP-12B-Q3_K_L.gguf\nQ3_K_L\n6.56GB\nfalse\nLower quality but usable, good for low RAM availability.\nBlackSheep-RP-12B-Q3_K_M.gguf\nQ3_K_M\n6.08GB\nfalse\nLow quality.\nBlackSheep-RP-12B-IQ3_M.gguf\nIQ3_M\n5.72GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nBlackSheep-RP-12B-Q3_K_S.gguf\nQ3_K_S\n5.53GB\nfalse\nLow quality, not recommended.\nBlackSheep-RP-12B-Q2_K_L.gguf\nQ2_K_L\n5.45GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nBlackSheep-RP-12B-IQ3_XS.gguf\nIQ3_XS\n5.31GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nBlackSheep-RP-12B-Q2_K.gguf\nQ2_K\n4.79GB\nfalse\nVery low quality but surprisingly usable.\nBlackSheep-RP-12B-IQ2_M.gguf\nIQ2_M\n4.44GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nBlackSheep-RP-12B-IQ2_S.gguf\nIQ2_S\n4.14GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/BlackSheep-RP-12B-GGUF --include \"BlackSheep-RP-12B-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/BlackSheep-RP-12B-GGUF --include \"BlackSheep-RP-12B-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (BlackSheep-RP-12B-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "jingheya/lotus-depth-d-v1-1": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction\nLotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction\nThis model belongs to the family of official Lotus models.",
    "SWivid/F5-TTS": "Download F5-TTS or E2 TTS and place under ckpts/\nckpts/\nF5TTS_v1_Base/\nmodel_1250000.safetensors\nF5TTS_Base/\nmodel_1200000.safetensors\nE2TTS_Base/\nmodel_1200000.safetensors\nGithub: https://github.com/SWivid/F5-TTSPaper: F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
    "cstr/whisper-large-v3-turbo-german-int8_float32": "Quant\nModelcard from primeline/whisper-large-v3-german\nSummary\nApplications\nModel family\nEvaluations\nTraining data\nTraining process\nHow to use\nAbout us\nQuant\nThis is only a int8 quantization from primeline/whisper-large-v3-turbo-german per ctranslate2-converter, for usage e.g. in ctranslate2, faster-whisper, etc.\nModelcard from primeline/whisper-large-v3-german\nSummary\nThis model map provides information about a model based on Whisper Large v3 that has been fine-tuned for speech recognition in German. Whisper is a powerful speech recognition platform developed by OpenAI. This model has been specially optimized for processing and recognizing German speech.\nApplications\nThis model can be used in various application areas, including\nTranscription of spoken German language\nVoice commands and voice control\nAutomatic subtitling for German videos\nVoice-based search queries in German\nDictation functions in word processing programs\nModel family\nModel\nParameters\nlink\nWhisper large v3 german\n1.54B\nlink\nWhisper large v3 turbo german\n809M\nlink\nDistil-whisper large v3 german\n756M\nlink\ntiny whisper\n37.8M\nlink\nEvaluations\nDataset\nopenai-whisper-large-v3-turbo\nopenai-whisper-large-v3\nprimeline-whisper-large-v3-german\nnyrahealth-CrisperWhisper\nprimeline-whisper-large-v3-turbo-german\ncommon_voice_19_0\n6.31\n5.84\n4.30\n4.14\n4.28\nTuda-De\n11.45\n11.21\n9.89\n13.88\n8.10\nmultilingual librispeech\n18.03\n17.69\n13.46\n10.10\n4.71\nAll\n14.16\n13.79\n10.51\n8.48\n4.75\nTraining data\nThe training data for this model includes a large amount of spoken German from various sources. The data was carefully selected and processed to optimize recognition performance.\nTraining process\nThe training of the model was performed with the following hyperparameters\nBatch size: 12288\nEpochs: 3\nLearning rate: 1e-6\nData augmentation: No\nOptimizer: Ademamix\nHow to use\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"primeline/whisper-large-v3-turbo-german\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nmax_new_tokens=128,\nchunk_length_s=30,\nbatch_size=16,\nreturn_timestamps=True,\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nresult = pipe(sample)\nprint(result[\"text\"])\nAbout us\nYour partner for AI infrastructure in Germany\nExperience the powerful AI infrastructure that drives your ambitions in Deep Learning, Machine Learning & High-Performance Computing. Optimized for AI training and inference.\nModel author: Florian Zimmermeister",
    "microsoft/OmniParser": "Model Summary\nResponsible AI Considerations\nIntended Use\nlimitations\nLicense\nðŸ“¢ [Project Page] [Blog Post] [Demo]\nModel Summary\nOmniParser is a general screen parsing tool, which interprets/converts UI screenshot to structured format, to improve existing LLM based UI agent.\nTraining Datasets include: 1) an interactable icon detection dataset, which was curated from popular web pages and automatically annotated to highlight clickable and actionable regions, and 2) an icon description dataset, designed to associate each UI element with its corresponding function.\nThis model hub includes a finetuned version of YOLOv8 and a finetuned BLIP-2 model on the above dataset respectively. For more details of the models used and finetuning, please refer to the paper.\nResponsible AI Considerations\nIntended Use\nOmniParser is designed to be able to convert unstructured screenshot image into structured list of elements including interactable regions location and captions of icons on its potential functionality.\nOmniParser is intended to be used in settings where users are already trained on responsible analytic approaches and critical reasoning is expected. OmniParser is capable of providing extracted information from the screenshot, however human judgement is needed for the output of OmniParser.\nOmniParser is intended to be used on various screenshots, which includes both PC and Phone, and also on various applications.\nlimitations\nOmniParser is designed to faithfully convert screenshot image into structured elements of interactable regions and semantics of the screen, while it does not detect harmful content in its input (like users have freedom to decide the input of any LLMs), users are expected to provide input to the OmniParser that is not harmful.\nWhile OmniParser only converts screenshot image into texts, it can be used to construct an GUI agent based on LLMs that is actionable. When developing and operating the agent using OmniParser, the developers need to be responsible and follow common safety standard.\nFor OmniPaser-BLIP2, it may incorrectly infer the gender or other sensitive attribute (e.g., race, religion etc.) of individuals in icon images. Inference of sensitive attributes may rely upon stereotypes and generalizations rather than information about specific individuals and are more likely to be incorrect for marginalized people. Incorrect inferences may result in significant physical or psychological injury or restrict, infringe upon or undermine the ability to realize an individualâ€™s human rights. We do not recommend use of OmniParser in any workplace-like use case scenario.\nLicense\nPlease note that icon_detect model is under AGPL license, and icon_caption_blip2 & icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model.",
    "fancyfeast/llama-joycaption-alpha-two-hf-llava": "Model Card for Llama JoyCaption Alpha Two\nMotivation\nHow to Get Started with the Model\nvLLM\nModel Card for Llama JoyCaption Alpha Two\nGithub\nJoyCaption is an image captioning Visual Language Model (VLM) being built from the ground up as a free, open, and uncensored model for the community to use in training Diffusion models.\nKey Features:\nFree and Open: It will be released for free, open weights, no restrictions, and just like bigASP, will come with training scripts and lots of juicy details on how it gets built.\nUncensored: Equal coverage of SFW and NSFW concepts. No \"cylindrical shaped object with a white substance coming out on it\" here.\nDiversity: All are welcome here. Do you like digital art? Photoreal? Anime? Furry? JoyCaption is for everyone. Pains are being taken to ensure broad coverage of image styles, content, ethnicity, gender, orientation, etc.\nMinimal Filtering: JoyCaption is trained on large swathes of images so that it can understand almost all aspects of our world. almost. Illegal content will never be tolerated in JoyCaption's training.\nMotivation\nAutomated descriptive captions enable the training and finetuning of diffusion models on a wider range of images, since trainers are no longer required to either find images with already associated text or write the descriptions themselves. They also improve the quality of generations produced by Text-to-Image models trained on them (ref: DALL-E 3 paper). But to-date, the community has been stuck with ChatGPT, which is expensive and heavily censored; or alternative models, like CogVLM, which are weaker than ChatGPT and have abysmal performance outside of the SFW domain.\nI'm building JoyCaption to help fill this gap by performing near or on-par with GPT4o in captioning images, while being free, unrestricted, and open.\nHow to Get Started with the Model\nPlease see the Github for more details.\nExample usage:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nIMAGE_PATH = \"image.jpg\"\nPROMPT = \"Write a long descriptive caption for this image in a formal tone.\"\nMODEL_NAME = \"fancyfeast/llama-joycaption-alpha-two-hf-llava\"\n# Load JoyCaption\n# bfloat16 is the native dtype of the LLM used in JoyCaption (Llama 3.1)\n# device_map=0 loads the model into the first GPU\nprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\nllava_model = LlavaForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=\"bfloat16\", device_map=0)\nllava_model.eval()\nwith torch.no_grad():\n# Load image\nimage = Image.open(IMAGE_PATH)\n# Build the conversation\nconvo = [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful image captioner.\",\n},\n{\n\"role\": \"user\",\n\"content\": PROMPT,\n},\n]\n# Format the conversation\n# WARNING: HF's handling of chat's on Llava models is very fragile.  This specific combination of processor.apply_chat_template(), and processor() works\n# but if using other combinations always inspect the final input_ids to ensure they are correct.  Often times you will end up with multiple <bos> tokens\n# if not careful, which can make the model perform poorly.\nconvo_string = processor.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\nassert isinstance(convo_string, str)\n# Process the inputs\ninputs = processor(text=[convo_string], images=[image], return_tensors=\"pt\").to('cuda')\ninputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n# Generate the captions\ngenerate_ids = llava_model.generate(\n**inputs,\nmax_new_tokens=300,\ndo_sample=True,\nsuppress_tokens=None,\nuse_cache=True,\ntemperature=0.6,\ntop_k=None,\ntop_p=0.9,\n)[0]\n# Trim off the prompt\ngenerate_ids = generate_ids[inputs['input_ids'].shape[1]:]\n# Decode the caption\ncaption = processor.tokenizer.decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\ncaption = caption.strip()\nprint(caption)\nvLLM\nvLLM provides the highest performance inference for JoyCaption, and an OpenAI compatible API so JoyCaption can be used like any other VLMs.  Example usage:\nvllm serve fancyfeast/llama-joycaption-alpha-two-hf-llava --max-model-len 4096 --enable-prefix-caching\nVLMs are a bit finicky on vLLM, and vLLM is memory hungry, so you may have to adjust settings for your particular environment, such as forcing eager mode, adjusting max-model-len, adjusting gpu_memory_utilization, etc.",
    "showlab/show-o-512x512": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a ðŸ§¨ diffusers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "adibvafa/BLIP-MIMIC-CXR": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nRadioCare\nAbout\nCode\nDemo\nIntroduction\nMethods\nData\nModel Architecture\nResults\nDiscussion\nTeam Information\nRepository Structure\nCitation\nRadioCare\nPLEASE REQUEST ACCESS WITH YOUR MIMIC-CXR ACCESS RIGHT TO APPROVE\nAbout\nThis is the model checkpoint for BLIP (\"Salesforce/blip-image-captioning-large\") finetuned on the MIMIC-CXR dataset and generates accurate radiology reports given an chest X-ray and a clinical indication (e.g. 'eval for pneumonia').\nTraining and inference code is provided in GitHub repository\nCode\nfrom PIL import Image\nfrom transformers import BlipForConditionalGeneration, BlipProcessor\n# Load model\nprocessor = BlipProcessor.from_pretrained(\"adibvafa/BLIP-MIMIC-CXR\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"adibvafa/BLIP-MIMIC-CXR\")\n# Load data\nimage = 'chest-x-ray.jpg'\nprompt = 'final report\\nexamination: chest (pa and lat)\\nindication: ___f with chest pressure, uri sx, voice change.'\n# Process inputs\ninputs = processor(\nimages=Image.open(image),\ntext=prompt,\nreturn_tensors=\"pt\"\n)\n# Generate radiology report\noutput = model.generate(**inputs, max_length=512)\nreport = processor.decode(output[0], skip_special_tokens=True)\n### report will be like follows:\nfinal report\nexamination : chest ( pa and lat )\nindication : ___f with chest pressure, uri sx, voice change.\ncomparison : none\nfindings : pa and lateral views of the chest provided. there is no focal consolidation, effusion, or pneumothorax. the cardiomediastinal silhouette is normal. imaged osseous structures are intact. no free air below the right hemidiaphragm is seen.\nimpression : no acute intrathoracic process.\nDemo\nIntroduction\nRadiocare aims to develop a cutting-edge image-to-text model that generates accurate radiology reports and diagnoses for chest X-ray images. By leveraging the BLIP and Vision Transformer architectures, Radiocare seeks to streamline the diagnostic process, enabling faster and more accurate identification of health issues. This project addresses the critical need for timely and precise radiological assessments, especially in rural areas with limited access to healthcare. Ultimately, Radiocare strives to improve patient outcomes and bridge the gap in healthcare accessibility across Canada.\nMethods\nData\nRadiocare utilizes data from the MIMIC-CXR database on PhysioNet, consisting of a large collection of chest X-ray images and associated radiology reports. This dataset provides a comprehensive source of medical images essential for training and evaluating the model.\nModel Architecture\nRadiocare employs the BLIP (Bootstrapped Language-Image Pre-training) model, which integrates the Vision Transformer (ViT) architecture with a text decoder. ViT processes images by dividing them into fixed-size patches, transforming these patches into high-dimensional vectors, and then embedding them into tokens. The self-attention mechanism in ViT captures global dependencies across patches, enhancing the model's understanding of the entire image. The text decoder translates these visual features into coherent radiology reports, enabling detailed and accurate diagnostics.\nResults\nRadiocare's model can assess a chest X-ray in approximately 3 seconds, providing doctors with a 99% faster diagnostic process. Key performance metrics include:\nBert Socre: 47.2\nRadGraph: 26.1\nDiscussion\nRadiocare represents a significant advancement in the field of medical diagnostics by leveraging state-of-the-art AI techniques to generate accurate and timely radiology reports from chest X-ray images. The integration of the BLIP model and Vision Transformer architecture enhances the diagnostic process, ensuring faster and more reliable results. By addressing the critical healthcare needs, especially in underserved rural areas, Radiocare has the potential to improve patient outcomes and bridge the gap in healthcare accessibility across Canada.\nTeam Information\nRadiocare is part of the Spring 2024 cohort of Borealis AI's \"Let's SOLVE It\" program. The project team includes:\nAdibvafa Fallahpour\nArchita Srivastava\nMantaj Dhillon\nGrace Liu\nRepository Structure\nThe repository is organized as follows:\ndata_modules/: Contains data loading and preprocessing scripts.\nevals/: Includes evaluation scripts and metrics calculation.\nmodels/: Contains the different model architectures.\nblip/: Final model implementation using BLIP and ViT.\ncnn/: Convolutional neural network models.\nvit/: Vision Transformer models.\nutils/: Utility functions for the project.\nslurm/: SLURM batch scripts for running jobs on a computing cluster.\nCitation\nIf you use this work in your research, please cite:\n@misc {adibvafa_fallahpour_2024,\nauthor       = { Fallahpour, Adibvafa and Srivastava, Archita and Dhillon, Mantaj and Liu, Grace },\ntitle        = { BLIP-MIMIC-CXR },\nyear         = 2024,\nurl          = { https://huggingface.co/adibvafa/BLIP-MIMIC-CXR },\ndoi          = { 10.57967/hf/3207 },\npublisher    = { Hugging Face }\n}",
    "JujoHotaru/BreedSeriesForXL": "BreedSeries for SDXL(Pony/Illustrious)\nCivitAIã§ã‚‚å…¬é–‹ä¸­ã®ãƒ¢ãƒ‡ãƒ«\nIllustriousç³»\nPonyç³»\næœ¬ãƒªãƒã‚¸ãƒˆãƒªé™å®šå…¬é–‹ãƒ¢ãƒ‡ãƒ«\nSuzuranBreed\nSylphBreedXL_v10\nHotaruBreedXL_v20ã€HotaruBreedXL_Anime_v20\nLoRABreed_v20ï¼LoRABreed_Slender_v20ï¼LoRABreed_Petite_v20\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ / Download\nãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License\nBreedSeries for SDXL(Pony/Illustrious)\nä½œè€…ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ« / Author's profile\nã“ã¡ã‚‰ã¯ã€CivitAIã§å…¬é–‹ã—ã¦ã„ã‚‹SDXLå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ”ãƒ¼ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã€HuggingFaceé™å®šã®SDXLãƒ¢ãƒ‡ãƒ«å…¬é–‹ã‚’å…¼ã­ãŸãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚\nThis is a repository storing copy/backup checkpoints released on CivitAI and original checkpoints. All checkpoints are based on SDXL.\nCivitAIã§ã‚‚å…¬é–‹ä¸­ã®ãƒ¢ãƒ‡ãƒ«\nIllustriousç³»\nIllusionBreed\nDaisyBreed\nLilyBreed\nPonyç³»\nVioletBreed\nRoseBreed\nPansyBreed\næœ¬ãƒªãƒã‚¸ãƒˆãƒªé™å®šå…¬é–‹ãƒ¢ãƒ‡ãƒ«\nSuzuranBreed\nDaisyBreedã‚ˆã‚Šã‚‚ã•ã‚‰ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã®å¼·ã„ã€4ï½ž5é ­èº«ãã‚‰ã„ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’å‡ºåŠ›ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚2Dã‚¢ãƒ‹ãƒ¡èª¿ã‚¿ã‚¤ãƒ—ã¨ã€2.5Dã‚»ãƒŸãƒªã‚¢ãƒ«èª¿ã®2ç¨®é¡žã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚\nè©³ç´°èª¬æ˜Žãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\nv6.0ã‹ã‚‰ã¯ã‚¢ãƒ¯ãƒ¼ãƒˆAIã§ã‚‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚\nSylphBreedXL_v10\nå…¬é–‹æƒ…å ±ï¼š\nhttps://x.com/JujoHotaru/status/1880837301254963620\nSD1ç”¨ãƒ¢ãƒ‡ãƒ«SylphBreedã®ç”»é¢¨ã‚’ã€SDXLï¼ˆIllustriousç³»ï¼‰ãƒ™ãƒ¼ã‚¹ã«ç§»æ¤ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€Negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã©ã¯IllusionBreedã¨åŒã˜ã‚‚ã®ãŒä½¿ç”¨ã§ãã¾ã™ã€‚\nHotaruBreedXL_v20ã€HotaruBreedXL_Anime_v20\nå…¬é–‹æƒ…å ±ï¼š\nhttps://x.com/JujoHotaru/status/1880838346978779149\nhttps://x.com/JujoHotaru/status/1880832918840553899\nã“ã®ä¸¡ãƒ¢ãƒ‡ãƒ«ã¯ã€v20ã‹ã‚‰Illustriousç³»ã«ãªã‚Šã¾ã—ãŸã€‚ï¼ˆv10ã¯Ponyç³»ã§ã™ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€Negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã©ã¯Ponyç³»ã¨å¤§ããç•°ãªã‚Šã¾ã™ã®ã§ã”æ³¨æ„ãã ã•ã„ã€‚IllusionBreedã¨åŒã˜ã‚‚ã®ãŒä½¿ç”¨ã§ãã¾ã™ã€‚\nLoRABreed_v20ï¼LoRABreed_Slender_v20ï¼LoRABreed_Petite_v20\nå…¬é–‹æƒ…å ±ï¼š\nhttps://x.com/JujoHotaru/status/1844030429185384471\nPonyç³»ãƒ¢ãƒ‡ãƒ«ã§ã€VioletBreedã®é–‹ç™ºé€”ä¸­çŠ¶æ…‹ï¼ˆçœ¼ã®èª¿æ•´ã‚’è¡Œã†å‰ï¼‰ã«åå‰ã‚’ä»˜ã‘ãŸã‚‚ã®ã§ã™ã€‚å…¬é–‹ä¸­ã®å„ç¨®LoRAã®é–‹ç™ºã«æ´»ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã“ã®åãŒä»˜ã„ã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€Negativeãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã©ã¯VioletBreedã¨åŒã˜ã‚‚ã®ãŒä½¿ç”¨ã§ãã¾ã™ã€‚\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ / Download\nDownload from model folder\nãƒ©ã‚¤ã‚»ãƒ³ã‚¹ / License\nPony based models:\nLicensed under CreativeML Open RAIL++-M.\nIllustrious based models:\nLicensed under Fair AI Public License 1.0-SD",
    "disco-eth/discoder": "DisCoder: High-Fidelity Music Vocoder Using Neural Audio Codecs\nInstallation\nInference with ðŸ¤— Hugging Face\nTraining\nInference\nDisCoder: High-Fidelity Music Vocoder Using Neural Audio Codecs\nPaper | Samples | Code | Model\nDisCoder is a neural vocoder that leverages a generative adversarial encoder-decoder architecture informed by\na neural audio codec to reconstruct high-fidelity 44.1 kHz audio from mel spectrograms. Our approach first transforms\nthe mel spectrogram into a lower-dimensional representation aligned with the Descript Audio Codec (DAC) latent space\nbefore reconstructing it to an audio signal using a fine-tuned DAC decoder.\nInstallation\nThe codebase has been tested with Python 3.11. To get started, clone the repository and set up the environment using Conda:\ngit clone https://github.com/ETH-DISCO/discoder\nconda create -n discoder python=3.11\nconda activate discoder\npython -m pip install -r requirements.txt\nInference with ðŸ¤— Hugging Face\nUse the following script to perform inference with the pretrained DisCoder model from Hugging Face.\nThe model uses the z prediction target and was trained using 128 mel bins.\nimport torch\nfrom discoder.models import DisCoder\nfrom discoder import meldataset, utils\ndevice = \"cuda\"\nsr_target = 44100\n# load pretrained DisCoder model\ndiscoder = DisCoder.from_pretrained(\"disco-eth/discoder\")\ndiscoder = discoder.eval().to(device)\n# load 44.1 kHz audio file and create mel spectrogram\naudio, _ = meldataset.load_wav(full_path=\"path/to/audio.wav\", sr_target=sr_target, resample=True, normalize=True)\naudio = torch.tensor(audio).unsqueeze(dim=0).to(device)\nmel = utils.get_mel_spectrogram_from_config(audio, discoder.config)  # [B, 128, frames]\n# reconstruct audio\nwith torch.no_grad():\nwav_recon = discoder(mel)  # [B, 1, time]\nTraining\nTo calculate ViSQOL during validation, install the required library by following the steps below:\ncd discoder\ngit clone https://github.com/google/visqol\nbazel build :visqol -c opt\ncd visqol && pip install .\nTo start training, use the following command:\npython -u train.py --config configs/config_z.json\nInference\nThe inference script allows batch processing of audio files. It converts all WAV files in the specified input_dir to\nmel spectrograms, then reconstructs them into audio files in the output_dir.\npython -u inference.py --input_dir input_dir --output_dir output_dir --checkpoint_file model.pt --config  configs/config_z.json\nYou can also pass the normalize_volume flag to standardize the output volume.",
    "nvidia/Hymba-1.5B-Base": "Hymba-1.5B-Base\nModel Overview\nModel Architecture\nPerformance Highlights\nModel Usage\nStep 1: Environment Setup\nStep 2: Chat with Hymba-1.5B-Base\nFinetuning Hymba\nEvaluation\nLimitations\nEthical Considerations\nCitation\nHymba-1.5B-Base\nðŸ’¾ GithubÂ Â  | Â Â  ðŸ“„ Paper | Â Â  ðŸ“œ Blog\nModel Overview\nHymba-1.5B-Base is a base text-to-text model that can be adopted for a variety of natural language generation tasks.\nThe model has hybrid architecture with Mamba and Attention heads running in parallel. Meta tokens, a set of learnable tokens prepended to every prompt, help improve the efficacy of the model. The model shares KV cache between 2 layers and between heads in a single layer. 90% of attention layers are sliding window attention.\nThis model is ready for commercial use.\nModel Developer: NVIDIA\nModel Dates: Hymba-1.5B-Base was trained between September 1, 2024 and November 10th, 2024.\nLicense:\nThis model is released under the NVIDIA Open Model License Agreement.\nModel Architecture\nâš¡ï¸ We've released a minimal implementation of Hymba on GitHub to help developers understand and implement its design principles in their own models. Check it out! barebones-hymba.\nHymba-1.5B-Base has a model embedding size of 1600, 25 attention heads, and an MLP intermediate dimension of 5504, with 32 layers in total, 16 SSM states, 3 full attention layers, the rest are sliding window attention. Unlike the standard Transformer, each attention layer in Hymba has a hybrid combination of standard attention heads and Mamba heads in parallel.  Additionally, it uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).\nFeatures of this architecture:\nFuse attention heads and SSM heads within the same layer, offering parallel and complementary processing of the same inputs.\nIntroduce meta tokens that are prepended to the input sequences and interact with all subsequent tokens, thus storing important information and alleviating the burden of \"forced-to-attend\" in attention.\nIntegrate with cross-layer KV sharing and global-local attention to further boost memory and computation efficiency.\nPerformance Highlights\nHymba-1.5B-Base outperforms all sub-2B public models.\nModel Usage\nStep 1: Environment Setup\nSince Hymba-1.5B-Base employs FlexAttention, which relies on Pytorch2.5 and other related dependencies, we provide two ways to setup the environment:\n[Local install] Install the related packages using our provided setup.sh (support CUDA 12.1/12.4):\nwget --header=\"Authorization: Bearer YOUR_HF_TOKEN\" https://huggingface.co/nvidia/Hymba-1.5B-Base/resolve/main/setup.sh\nbash setup.sh\n[Docker] A docker image is provided with all of Hymba's dependencies installed. You can download our docker image and start a container using the following commands:\ndocker pull ghcr.io/tilmto/hymba:v1\ndocker run --gpus all -v /home/$USER:/home/$USER -it ghcr.io/tilmto/hymba:v1 bash\nStep 2: Chat with Hymba-1.5B-Base\nAfter setting up the environment, you can use the following script to chat with our Model\nfrom transformers import LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer, AutoModel\nimport torch\n# Load the tokenizer and model\nrepo_name = \"nvidia/Hymba-1.5B-Base\"\ntokenizer = AutoTokenizer.from_pretrained(repo_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(repo_name, trust_remote_code=True)\nmodel = model.cuda().to(torch.bfloat16)\n# Chat with Hymba\nprompt = input()\ninputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(**inputs, max_length=64, do_sample=False, temperature=0.7, use_cache=True)\nresponse = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\nprint(f\"Model response: {response}\")\nFinetuning Hymba\nLMFlow is a complete pipeline for fine-tuning large language models.\nThe following steps provide an example of how to fine-tune the Hymba-1.5B-Base models using LMFlow.\nUsing Docker\ndocker pull ghcr.io/tilmto/hymba:v1\ndocker run --gpus all -v /home/$USER:/home/$USER -it ghcr.io/tilmto/hymba:v1 bash\nInstall LMFlow\ngit clone https://github.com/OptimalScale/LMFlow.git\ncd LMFlow\nconda create -n lmflow python=3.9 -y\nconda activate lmflow\nconda install mpi4py\npip install -e .\nFine-tune the model using the following command.\ncd LMFlow\nbash ./scripts/run_finetune_hymba.sh\nWith LMFlow, you can also fine-tune the model on your custom dataset. The only thing you need to do is transform your dataset into the LMFlow data format.\nIn addition to full-finetuniing, you can also fine-tune hymba efficiently with DoRA, LoRA, LISA, Flash Attention, and other acceleration techniques.\nFor more details, please refer to the LMFlow for Hymba documentation.\nEvaluation\nWe use LM Evaluation Harness to evaluate the model. The evaluation commands are as follows:\ngit clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\ngit fetch --all --tags\ngit checkout tags/v0.4.4  # squad completion task is not compatible with the latest version\ncd lm-evaluation-harness\npip install -e .\nlm_eval --model hf --model_args pretrained=nvidia/Hymba-1.5B-Base,dtype=bfloat16,trust_remote_code=True \\\n--tasks mmlu \\\n--num_fewshot 5 \\\n--batch_size 1 \\\n--output_path ./hymba_HF_base_lm-results \\\n--log_samples\nlm_eval --model hf --model_args pretrained=nvidia/Hymba-1.5B-Base,dtype=bfloat16,trust_remote_code=True \\\n--tasks arc_easy,arc_challenge,piqa,winogrande,hellaswag \\\n--num_fewshot 0 \\\n--batch_size 1 \\\n--output_path ./hymba_HF_base_lm-results \\\n--log_samples\nlm_eval --model hf --model_args pretrained=nvidia/Hymba-1.5B-Base,dtype=bfloat16,trust_remote_code=True \\\n--tasks squad_completion \\\n--num_fewshot 1 \\\n--batch_size 1 \\\n--output_path ./hymba_HF_base_lm-results \\\n--log_samples\nLimitations\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\nThe testing suggests that this model is susceptible to jailbreak attacks. If using this model in a RAG or agentic setting, we recommend strong output validation controls to ensure security and safety risks from user-controlled model outputs are consistent with the intended use cases.\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nCitation\n@misc{dong2024hymbahybridheadarchitecturesmall,\ntitle={Hymba: A Hybrid-head Architecture for Small Language Models},\nauthor={Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov},\nyear={2024},\neprint={2411.13676},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2411.13676},\n}",
    "NeuML/txtai-speecht5-onnx": "SpeechT5 Text-to-Speech (TTS) Model for ONNX\nUsage with txtai\nModel training\nCustom speaker embeddings\nSpeechT5 Text-to-Speech (TTS) Model for ONNX\nFine-tuned version of SpeechT5 TTS exported to ONNX. This model was exported to ONNX using the Optimum library.\nUsage with txtai\ntxtai has a built in Text to Speech (TTS) pipeline that makes using this model easy.\nNote the following example requires txtai >= 7.5\nimport soundfile as sf\nfrom txtai.pipeline import TextToSpeech\n# Build pipeline\ntts = TextToSpeech(\"NeuML/txtai-speecht5-onnx\")\n# Generate speech\nspeech, rate = tts(\"Say something here\")\n# Write to file\nsf.write(\"out.wav\", speech, rate)\n# Generate speech with custom speaker\nspeech, rate = tts(\"Say something here\", speaker=np.array(...))\nModel training\nThis model was fine-tuned using the code in this Hugging Face article and a custom set of WAV files.\nThe ONNX export uses the following code, which requires installing optimum.\nimport os\nfrom optimum.exporters.onnx import main_export\nfrom optimum.onnx import merge_decoders\n# Params\nmodel = \"txtai-speecht5-tts\"\noutput = \"txtai-speecht5-onnx\"\n# ONNX Export\nmain_export(\ntask=\"text-to-audio\",\nmodel_name_or_path=model,\nmodel_kwargs={\n\"vocoder\": \"microsoft/speecht5_hifigan\"\n},\noutput = output\n)\n# Merge into single decoder model\nmerge_decoders(\nf\"{output}/decoder_model.onnx\",\nf\"{output}/decoder_with_past_model.onnx\",\nsave_path=f\"{output}/decoder_model_merged.onnx\",\nstrict=False\n)\n# Remove unnecessary files\nos.remove(f\"{output}/decoder_model.onnx\")\nos.remove(f\"{output}/decoder_with_past_model.onnx\")\nCustom speaker embeddings\nWhen no speaker argument is passed in, the default speaker embeddings are used. The defaults speaker is David Mezzetti, the primary developer of txtai.\nIt's possible to build custom speaker embeddings as shown below. Fine-tuning the model with a new voice leads to the best results but zero-shot speaker embeddings are OK in some cases.\nThe following code requires installing torchaudio and speechbrain.\nimport os\nimport numpy as np\nimport torchaudio\nfrom speechbrain.inference import EncoderClassifier\ndef speaker(path):\n\"\"\"\nExtracts a speaker embedding from an audio file.\nArgs:\npath: file path\nReturns:\nspeaker embeddings\n\"\"\"\nmodel = \"speechbrain/spkrec-xvect-voxceleb\"\nencoder = EncoderClassifier.from_hparams(model,\nsavedir=os.path.join(\"/tmp\", model),\nrun_opts={\"device\": \"cuda\"})\nsamples, sr = torchaudio.load(path)\nsamples = encoder.audio_normalizer(samples[0], sr)\nembedding = encoder.encode_batch(samples.unsqueeze(0))\nreturn embedding[0,0].to(\"cuda\").unsqueeze(0)\nembedding = speaker(\"reference.wav\")\nnp.save(\"speaker.npy\", embedding.cpu().numpy(), allow_pickle=False)\nThen load as shown below.\nspeech, rate = tts(\"Say something here\", speaker=np.load(\"speaker.npy\"))\nSpeaker embeddings from the original SpeechT5 TTS training set are supported. See the README for more.",
    "TheDrummer/UnslopNemo-12B-v3": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nhttps://ko-fi.com/thedrummer",
    "arcee-ai/SuperNova-Medius-GGUF": "Arcee-SuperNova-Medius\nDistillation Overview\nPerformance Evaluation\nModel Use Cases\nDeployment Options\nTechnical Specifications\nSummary\nArcee-SuperNova-Medius\nArcee-SuperNova-Medius is a 14B parameter language model developed by Arcee.ai, built on the Qwen2.5-14B-Instruct architecture. This unique model is the result of a cross-architecture distillation pipeline, combining knowledge from both the Qwen2.5-72B-Instruct model and the Llama-3.1-405B-Instruct model. By leveraging the strengths of these two distinct architectures, SuperNova-Medius achieves high-quality instruction-following and complex reasoning capabilities in a mid-sized, resource-efficient form.\nSuperNova-Medius is designed to excel in a variety of business use cases, including customer support, content creation, and technical assistance, while maintaining compatibility with smaller hardware configurations. Itâ€™s an ideal solution for organizations looking for advanced capabilities without the high resource requirements of larger models like our SuperNova-70B.\nDistillation Overview\nThe development of SuperNova-Medius involved a sophisticated multi-teacher, cross-architecture distillation process, with the following key steps:\nLogit Distillation from Llama-3.1-405B-Instruct:\nWe distilled the logits of Llama-3.1-405B-Instruct to Qwen2.5-14B using KL-divergence as the loss function. This allowed us to capture the nuanced distribution of Llama's outputs while adapting them to Qwen's architecture.\nLogit and Hidden State Distillation from Qwen2.5-72B-Instruct:\nFurther distillation was performed using a combination of logit and hidden state distillation from Qwen2.5-72B-Instruct to ensure that SuperNova-Medius inherited the strong instruction-following capabilities and domain-specific knowledge of Qwen2.5.\nCross-Architecture Vocabulary Alignment:\nUsing mergekit-tokensurgeon, we aligned the vocabularies and hidden states of both teacher models, allowing for seamless integration of knowledge across the different architectures. This enabled SuperNova-Medius to effectively combine the strengths of both models.\nFinal Fusion and Fine-Tuning:\nAfter aligning the vocabularies, a final fusion and fine-tuning step was conducted, using a specialized dataset from EvolKit to ensure that SuperNova-Medius maintained coherence, fluency, and context understanding across a broad range of tasks.\nPerformance Evaluation\nBelow are the benchmark results of SuperNova-Medius compared to similar models in its class:\nModel\nAverage\nIFEval\nBBH\nGPQA\nMMLU Pro\nMuSR\nMath Level 5\nMistral-Small 2409\n0.423\n0.628\n0.581\n0.333\n0.410\n0.406\n0.181\nSupernova-Lite\n0.427\n0.786\n0.511\n0.306\n0.388\n0.415\n0.155\nQwen2.5-14B-Instruct\n0.450\n0.827\n0.623\n0.358\n0.490\n0.403\n0.000\nSupernova-Medius\n0.480\n0.832\n0.631\n0.359\n0.502\n0.402\n0.152\nSuperNova-Medius performs exceptionally well in instruction-following (IFEval) and complex reasoning tasks (BBH), demonstrating its capability to handle a variety of real-world scenarios. It outperforms Qwen2.5-14B and SuperNova-Lite in multiple benchmarks, making it a powerful yet efficient choice for high-quality generative AI applications.\nModel Use Cases\nArcee-SuperNova-Medius is suitable for a range of applications, including:\nCustomer Support: With its robust instruction-following and dialogue management capabilities, SuperNova-Medius can handle complex customer interactions, reducing the need for human intervention.\nContent Creation: The modelâ€™s advanced language understanding and generation abilities make it ideal for creating high-quality, coherent content across diverse domains.\nTechnical Assistance: SuperNova-Medius has a deep reservoir of technical knowledge, making it an excellent assistant for programming, technical documentation, and other expert-level content creation.\nDeployment Options\nSuperNova-Medius is available for use under the Apache-2.0 license. For those who need even higher performance, the full-size 70B SuperNova model can be accessed via an Arcee-hosted API or for local deployment. To learn more or explore deployment options, please reach out to sales@arcee.ai.\nTechnical Specifications\nModel Architecture: Qwen2.5-14B-Instruct\nDistillation Sources: Qwen2.5-72B-Instruct, Llama-3.1-405B-Instruct\nParameter Count: 14 billion\nTraining Dataset: Custom instruction dataset generated with EvolKit\nDistillation Technique: Multi-architecture logit and hidden state distillation with cross-architecture vocabulary alignment.\nSummary\nArcee-SuperNova-Medius provides a unique balance of power, efficiency, and versatility. By distilling knowledge from two top-performing teacher models into a single 14B parameter model, SuperNova-Medius achieves results that rival larger models while maintaining a compact size ideal for practical deployment. Whether for customer support, content creation, or technical assistance, SuperNova-Medius is the perfect choice for organizations looking to leverage advanced language model capabilities in a cost-effective and accessible form.",
    "eloialonso/diamond": "Pretrained models for the paper \"Diffusion for World Modeling: Visual Details Matter in Atari\".\nPaper: https://arxiv.org/abs/2405.12399\nCode: https://github.com/eloialonso/diamond\nWebpage: https://diamond-wm.github.io",
    "sphiratrioth666/Lorebooks_as_ACTIVE_scenario_and_character_guidance_tool": "Hey all. I was experimenting with Silly Tavern lorebooks and I found a great, non-obvious way of using them. I call it a procedural guided generation - following the idea of SillyTavern creators for what lorebooks could be in the future anyway, haha. It turns out that we are able to precisely guide our LLM during roleplay by triggering instructions from a lorebook - not inserting lore/info but influencing the actual {{char}} behavior, determining results of our actions, rolling different world states such as weather etc. It works like OOC (out of character instructions) but on steroids. With a properly prepared lorebook, we can automatically roll a positive/negative results of our actions, to combat a positive bias of the LLM but not only that. It's possible giving all kinds of auto-triggered instructions from a lorebook. Want a character to behave in a particular situation or to stick to its personality better? No problem. Want to add flavor or diversity to your roleplays? It all becomes possible.\nCreative Commons Attribution-NonCommercial-ShareAlike 3.0 License (https://www.goodfon.com/fantasy/wallpaper-the-lord-of-the-rings-sauron-dark-lord-metal-helm.html)\nExample Lorebooks:\nNavigate to the files depository of this post, download two exemplary lorebooks: \"Busy at work\" and \"TTRPG - Resolution Mechanic\". Try them out, check on how they are written and use them as a template to write your own!\nLorebooks are located in (...)\\SillyTavern\\data\\default-user\\worlds. Just copy-paste my files or import them through SillyTavern import button.\nHow to write your own lorebooks:\n1. Create a lorebook in Silly Tavern (I will not teach you in detail how to do that, my appologies - you need to find a guide online, there are many of them and it is actually super simple (for instance - read this: https://rentry.co/world-info-encyclopedia).\n2. Create the entries with your lorebook and put them in the same group - add multiple entries first, each representing a given instruction or anything you want to happen in a given situation (like table of results of dice rolls in TTRPG). So for instance: entry1: {{char}} is currently busy, entry2: {{char}} is currently free, entry3: {{char}} will not answer a phone because something happened, do not reveal the {{char}} circumstances yet.\nEach entry must have: a) the same trigger words, b) position = (System) (entries inserterted as system are will be sent to the LLM right with a message you type but automatically and they do not appear in a chat - so they work even if models do not understand OOC and if they do - it is still a more elegant, immersive and smooth experience, it does not eat up tokens because instructions will be deleted from a context automatically), c) depth = 1/0 (it determines if instruction is sent before your actual chat message or after, it does not seem to make a difference to LLM but if you want it to be more elegant in context inspection, use depth = 0), d) order = 100, e) trigger = 100, f) Prevent further recursion option: ON (entry will not activate other entries), g) group weight = 100/number of entries within your group (100 divided per total number of entries in a group) (it is super important since weight determines the probability of a given entry being randomly rolled, you can adjust probabilites for each entry but always keep the whole weights sum within the same group = 100), h) sticky = 4 or more/less (it makes the instruction for LLM remain active for a given number of the following messages - so it actually triggers logically within the scene, LLMs seem to understand it and do things under logical circumstances in the scene).\nRemember, you can change the weights to trigger different events with different probabilities but do not leave them at 100 even if you want to roll from 2 entries in a group. Use 100 when you do not want to roll but trigger the entry through a trigger word, use 100/number of entries in a group (100 divided per total number of entries in a group) if you want to roll randomly which entry will be inserted.\n3. Write instructions inside of the entries. They must be short, clear and include a specific phrase if you want something to happen now: \"WILL INSTANTLY\". Do not ask me why but otherwise different LLMs more often fail in following the instruction. I've tried Mistral, LLAMAs, Qwen, Gemma and other tunes - all behave in a very similar manner.\nTemplate: {{char}} will instantly [ACTION YOU WANT CHARACTER TO PERFORM] or [EVENT YOU WANT] will instantly [HAPPEN}.\nSomething like: \"enemies will instantly attack {{user}}\" or \"road ahead will instantly turn out to be blocked\".\nAgain, it seems that a phrase \"will instantly\" is the best working trigger.\nTRICKS:\nIt may be good using one, specific word to trigger the entry group so it does not trigger again and again with different results as you or LLM repeats the trigger word. You can use something like... \"DC\"/\"AP\" (difficulty challenge/action-perform; or you can come up with good trigger words so it triggers without you even doing anything! (like: attack - will roll the attack result from a lorebook with entries \"successful attack/failed attack\" being activated randomly within a group with the same trigger word.\nDepending on the use-case, you will need a couple of such entry groups (like different \"dice rolls\" for social encounters, different for random events in the world and different for dungeons exploring). IT WORKS VERY WELL.\nYou can also steer {{char}}'s behavior precisely using this method to stick to the personality better or to make character consistently do given things in given situations (it seems to work better than pure card definitions).\nUse it in NSFW to add variety and logic to your roleplays - this way, {{char}}'s behavior is influenced better than while using normal, random lorebooks in a classical manner.\nUse it to counter a positive bias of the LLMs (a bias of cooperating with {{user}} when {{user}} does something - for instance, your sword swing will fail to connect with the enemy if you set it up to trigger like that. It works VERY WELL.",
    "facebook/OMAT24": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nOMat24 License Last Updated: October 23, 2024\nâ€œAcceptable Use Policyâ€ means the FAIR Acceptable Use Policy, applicable to Research Materials, that is incorporated into this Agreement.â€œAgreementâ€ means the terms and conditions for use, reproduction, distribution and modification of the Research Materials set forth herein.\nâ€œDocumentationâ€ means the specifications, manuals and documentation accompanying  Research Materials distributed by Meta.\nâ€œLicenseeâ€ or â€œyouâ€ means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entityâ€™s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\nâ€œMetaâ€ or â€œweâ€ means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nâ€œResearch Materialsâ€ means, collectively, Documentation and the models, software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code, demonstration materials and other elements of the foregoing distributed by Meta and made available under this Agreement.By clicking â€œI Acceptâ€ below or by using or distributing any portion or element of the Research Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Metaâ€™s intellectual property or other rights owned by Meta embodied in the Research Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Research Materials.\nb. Redistribution and Use.\ni. Distribution of Research Materials, and any derivative works thereof, are subject to the terms of this Agreement. If you distribute or make the Research Materials, or any derivative works thereof, available to a third party, you may only do so under the terms of this Agreement. You shall also provide a copy of this Agreement to such third party.\nii.  If you submit for publication the results of research you perform on, using, or otherwise in connection with Research Materials, you must acknowledge the use of Research Materials in your publication.\niii. Your use of the Research Materials must comply with applicable laws and regulations (including Trade Control Laws) and adhere to the FAIR Acceptable Use Policy, which is hereby incorporated by reference into this Agreement. 2. User Support. Your Use of the Research Materials is done at your own discretion; Meta does not process any information nor provide any service in relation to such use.  Meta is under no obligation to provide any support services for the Research Materials. Any support provided is â€œas isâ€, â€œwith all faultsâ€, and without warranty of any kind.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE RESEARCH MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN â€œAS ISâ€ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE RESEARCH MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE RESEARCH MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY DIRECT OR INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.\na. Subject to Metaâ€™s ownership of Research Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Research Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nb. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Research Materials, outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Research Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Research Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Research Materials. Sections 5, 6 and 9 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nModifications and Amendments. Meta may modify this Agreement from time to time by posting a revised version at [https://huggingface.co/fairchem/OMAT24/blob/main/LICENSE]; provided that they are similar in spirit to the current version of the Agreement, but may differ in detail to address new problems or concerns. All such changes will be effective immediately. Your continued use of the Research Materials after any modification to this Agreement constitutes your agreement to such modification. Except as provided in this Agreement, no modification or addition to any provision of this Agreement will be binding unless it is in writing and signed by an authorized representative of both you and Meta.\nFAIR Acceptable Use Policy\nThe Fundamental AI Research (FAIR) team at Meta seeks to further understanding of new and existing research domains with the mission of advancing the state-of-the-art in artificial intelligence through open research for the benefit of all.As part of this mission, Meta makes certain research materials available for research use. Meta is committed to promoting the safe and responsible use of such research materials.Prohibited UsesYou agree you will not use, or allow others to use, Research Materials to:\nViolate the law or othersâ€™ rights, including to:Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: Violence or terrorism Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material Human trafficking, exploitation, and sexual violence The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials. Sexual solicitation Any other criminal activityEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individualsEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and servicesEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practicesCollect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable lawsEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any technology using FAIR research materialsCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of research artifacts related to the following:Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of StateGuns and illegal weapons (including weapon development)Illegal drugs and regulated/controlled substancesOperation of critical infrastructure, transportation technologies, or heavy machinerySelf-harm or harm to others, including suicide, cutting, and eating disordersAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual3. Intentionally deceive or mislead others, including use of FAIR Research Materials related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that outputs of FAIR research materials or outputs from technology using FAIR research materials o are human-generatedGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement4. Fail to appropriately disclose to end users any known dangers of your Research Materials.Please report any violation of this Policy or other problems that could lead to a violation of this Policy by submitting a report here.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nArchitecture\nModel variations\nModel checkpoints\nModels trained on OMat, MPtrj, and sAlexandria\nModels trained only on MPtrj only\nHow to use\nSupport\nLicense\nCitation\nMeta Open Materials 2024 (OMat24) Models\nMeta's FAIR Chemistry team has released a collection of model checkpoints ranging in model sizes and training strategies.\nArchitecture\nAll models used the EquiformerV2 or the\neSEN architecture. Model architecture source code implementation is available\nin the fairchem repository.\nModel variations\nEquiformerV2 models come in three different sizes - 31M (S), 86M (M), 153M (L) parameters. We explore EquiformerV2 (eqV2) with and without denoising augmentation objectives (DeNS).\neSEN models are 30M parameters and are pretrained using direct force prediction and finetuned for gradient based predictions following the methodology reported here.\nModel checkpoints\nModels trained using OMat24 are trained with DFT and DFT+U total energy labels.\nThese are not compatible with Materials Project calculations. If you are using models trained on OMat24 only (those named <model>-omat.pt)\nfor such calculations, you need to use the reference unary compounds and MP2020-style\nanion and GGA/GGA+U mixing corrections in the OMat24 Dataset Hugging Face repo.\nDo not use MP2020 corrections or use the references compounds directly from MP when using OMat24 trained models. Additional care\nmust be taken when computing energy differences, such as formation and energy above hull and comparing with calculations\nin the Materials Project since DFT pseudopotentials are different and magnetic ground states may differ as well.\nModels trained on OMat, MPtrj, and sAlexandria\nName\nPre-train Data\nFine-tune Data\nCheckpoint\neSEN-30M-OAM\nOMat\nMPtrj+sAlex\nesen_30m_oam.pt\neqV2-S-OAM\nOMat\nMPtrj+sAlex\neqV2_31M_omat_mp_salex.pt\neqV2-M-OAM\nOMat\nMPtrj+sAlex\neqV2_86M_omat_mp_salex.pt\neqV2-L-OAM\nOMat\nMPtrj+sAlex\neqV2_153M_omat_mp_salex.pt\neSEN-30M-OMat\nOMat\n-\nesen_30m_omat.pt\neqV2-S-OMat\nOMat\n-\neqV2_31M_omat.pt\neqV2-M-OMat\nOMat\n-\neqV2_86M_omat.pt\neqV2-L-OMat\nOMat\n-\neqV2_153M_omat.pt\neSEN-30M-OAM and eqV2-M-OAM are have been submitted Matbench Discovery as (\"non-compliant\") models.\nModels trained only on MPtrj only\nName\nCheckpoint\neSEN-30M-MP\nesen_30m_mptrj.pt\neqV2-S\neqV2_31M_mp.pt\neqV2-S-DeNS\neqV2_dens_31M_mp.pt\neqV2-M-DeNS\neqV2_dens_86M_mp.pt\neqV2-L-DeNS\neqV2_dens_153M_mp.pt\neSEN-30M-MP and eqV2-S-DeNS are have been submitted Matbench Discovery as (\"compliant\") models.\nHow to use\nModel checkpoints can be readily used in the fairchem repo using our custom ASE-calculator.\nPlease refer to the fairchem documentation for installation instructions.\nNote: You can now run cell relaxations (using stress predictions) as of fairchem-core 1.3.0. If you were running the models using the now deprecated omat24 branch, you will need to re-download checkpoints to run with fairchem-core >= 1.3.0.\nUsing the provided checkpoints is as simple as:\nfrom fairchem.core import OCPCalculator\nfrom ase.optimize import FIRE # Import your optimizer of choice\nfrom ase.filters import FrechetCellFilter  # to include cell relaxations\nfrom ase.io import read\natoms = read(\"atoms.xyz\") # Read in an atoms object or create your own structure\ncalc = OCPCalculator(checkpoint_path=\"eqV2_31M_omat_mp_salex.pt\") # Path to downloaded checkpoint\natoms.calc = calc\ndyn = FIRE(FrechetCellFilter(atoms))\ndyn.run(fmax=0.05)\nAdditional utilities including trainers, evaluators, and dataloaders can be found in fairchem if additional training or fine-tuning is desired.\nSupport\nIf you run into any issues regarding feel free to post your questions or comments on any of the following platforms:\nHF Discussions\nGithub Issues\nDiscussion Board\nLicense\nModels are made accessible for commerical and non-commerical use under a permissive license found here.\nCitation\nIf you use this work, please consider citing:\n@misc{barroso_omat24,\ntitle={Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models},\nauthor={Luis Barroso-Luque and Muhammed Shuaibi and Xiang Fu and Brandon M. Wood and Misko Dzamba and Meng Gao and Ammar Rizvi and C. Lawrence Zitnick and Zachary W. Ulissi},\nyear={2024},\neprint={2410.12771},\narchivePrefix={arXiv},\nprimaryClass={cond-mat.mtrl-sci},\nurl={https://arxiv.org/abs/2410.12771},\n}\nIf you use any of the eSEN checkpoints, please cite the following\n@misc{fu_esen,\ntitle={Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction},\nauthor={Xiang Fu and Brandon M. Wood and Luis Barroso-Luque and Daniel S. Levine and Meng Gao and Misko Dzamba and C. Lawrence Zitnick},\nyear={2025},\neprint={2502.12147},\narchivePrefix={arXiv},\nprimaryClass={physics.comp-ph},\nurl={https://arxiv.org/abs/2502.12147},\n}",
    "SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA": "Notes and update, March 2025\nModel Details\nLLAMA-3_8B_Unaligned_BETA is available at the following quantizations:\nModel instruction template: (Can use either ChatML or Llama-3)\nLlama-3-Instruct\nChatML\nDetailed Presets configuration\nSupport\nCitation Information\nUGI results inconsistency\nBenchmarks\nOther stuff\nLLAMA-3.1_8B_Unaligned_BETA\nIn the Wild West of the AI world, the real titans never hit their deadlines, no sir!\nThe projects that finish on time? Theyâ€™re the soft onesâ€”basic, surface-level shenanigans. But the serious projects? Theyâ€™re always delayed. You set a date, then reality hits: not gonna happen, scope creep that mutates the roadmap, unexpected turn of events that derails everything.\nIt's only been 4 months since the Alpha was released, and half a year since the project started, but it felt like nearly a decade.\nDeadlines shift, but with each delay, youâ€™re not failingâ€”youâ€™re refining, and becoming more ambitious. A project that keeps getting pushed isnâ€™t late; itâ€™s just gaining weight, becoming something worth building, and truly worth seeing all the way through. The longer itâ€™s delayed, the more serious it gets.\nLLAMA-3_8B_Unaligned is a serious project, and thank god, the Beta is finally here.\nI love you all unconditionally, thanks for all the support and kind words!\nNotes and update, March 2025\nThe project is not forgotten, and active progress is being made. I had to juggle between a vast amount of other projects at the same time and prioritize.\nIn hindsight, this was the right call, as all the other projects I've made actively helped me improve the LLAMA_Unaligned project.\nLots of good stuff is coming. Hopefully soon :)\nOh, btw, in case someone wants to know, the base model is LLAMA 3.1\nModel Details\nCensorship level: Very low None\n10/10 (10 completely uncensored)\nIntended use: Creative writing, Role-Play, General tasks.\nThe model was trained on ~50M tokens (the vast majority- is unique) at 16K actual context length. Different techniques and experiments were done to achieve various capabilities and to preserve (and even enhance) the smarts while keeping censorship low. More information about this is available at my 'blog', which serves as a form of archival memoir of the past months.\nMore than 50% of the data is human-written, the major part of the synthetic data is a work-in-progress and will be de-slopped in the future. For now, the sacrifice of slop for smarts is acceptable. In the final release, a lot of it will be de-sloped completely.\nThe model is capable of generating very long texts, see the example below. In extremely long generations, coherency will suffer.\nWriting extremely long fan fiction in the Game Of Thrones universe:\nGeneration settings: min_p, max tokens = 4096 (in ooba booga hit 'continue' to keep generating after the 4096 tokens, the total size of generated plaintext: 50KB)\nPrompt:\nWrite a many paragraphs story in the style of Game of Thrones, each paragraph is separated by a new line, about how Tyrion Lannister decided one day to sneak and steal a dragon egg. he then goes on a sea voyage on the ship \"the wet bitch\", and almost dies in a brutal storm, where waves of over 50 foot almost break the ship in half, making him sick. He then finally gets Braavos, where he is looking for buyers for his dragon egg when eventually he stumbles upon a paramour of one of the lords, he convinces her to arrange a meeting with her lord so he can finally make the sale, so he will have all the money he needs to go on a never-ending adventure of sin and debauchery.\nOutput:\nTyrion Lannister sat on his window ledge, staring at the Red Keepâ€™s vast stables. The sun had long since set, and the only light came from the torches that lined the walls of the courtyard below. He remembered the last time he had been there, helping Viserys choose a mount. The boy had been so excited, so full of dreams. Daenerys Targaryen, the steel and dragon, he thought sourly, and took a long pull from his cup of sourwine. And I was there to see her birth. It was three years now since they had left Pentos. Three years since Viserys had died, and the girl and her dragons had sailed for Westeros. And in all that time, Tyrion had not seen another dragon egg.\nThe sellswords he had hired were even now loading his few possessions onto a ship, the Wet Bitch, which would take him to Braavos. It was a slow trader, but her master owed him money, and had promised to let him have the vessel for the passage. He had told everyone that he was going to explore a certain tunnel, but the truth was that he wanted to see the dragons for himself. He remembered Viserys telling him how they would char any man to ashes at a hundred paces. He wondered whether that was true.\nIt would be good to see Braavos again. It had been eight years since he had last walked the Titanâ€™s Spire, and dined at the House of the Black Ears, and strolled through the gardens of the Palazzo Castigazza, the caâ€™ of the powerful magister who had made such a nuisance of himself at Lys. And there were other reasons as well. He had been counting over and over again, and he could not see how he was going to pay his sellswords unless he sold the dragonâ€™s egg.\nHe had thought of offering it to the Braavosi bankers, but he did not think they would be interested. For one thing, the gold that the Braavosi handled came primarily from mines in the Disputed Lands, and there was none of the gold of Westeros left. The egg was no bigger than a human head. It was a thing of beauty, no doubt, but Tyrion was no more certain than Viserys had been that it would hatch. He might have to sell it for half of what he had been offered in Lys. Or less.\nHe wondered how many Braavosi even knew that dragons still existed. He had asked Illyrio about that, the last time they had met. â€œDragons?â€ the fat man had said, incredulous. â€œI tell you the Braavosi are fools. They are the most cautious men in the world. The most patient. The most prudent. They are so prudent they are falling out of fashion. And yet â€¦ they will not even go so far as to lend me the ships to bring your cute little sister to her rightful throne? Are they mad, these Braavosi? What is the good of money if you cannot spend it?\nâ€œI have told them and told them that these dragons of yours will be worth a fortune, but do they listen? No. The magisters have never seen a dragon, they will not believe that such creatures still live. Dragons? they say to me. What is your proof? Where are these dragons? Why have you not brought one to Braavos so that we may all see it with our own eyes? So we may touch it? Smell it? Taste it? Ha! Do they think that I am made of money? No, these Braavosi, they remember the Doom. Dragons roasted whole cities, they say. It is all they ever talk about, this Doom. The day the dragons returned to Braavos three thousand years ago. How the city was consumed in fire and flame. A million dead. A million. Do you know what that number means, Tyrion? It means that more men, women, and children died in this Braavos of mine than have perished in all the wars in the Seven Kingdoms since the Conquest. A million, they tell me. A million.\nâ€œAnd you tell me that dragons still exist, and they will not believe you. So be it. Let them starve. Illyrio shrugged. I have more important friends. But you â€¦ you are another matter. My little friend Tyrion. Why must you be so poor? Why must you be so â€¦ so â€¦â€ The fat man shook his head. â€œSo short?â€ â€œI am not short. In the east I am average. In the east I am even tall. There are men in Qarth who are shorter than I am.â€ â€œIn Westeros you are a dwarf,â€ Illyrio said.\nThat was so. He could not argue with that. He could hardly even look at himself without seeing how he was the butt of a thousand japes. The Imp. Halfman. All those years of trying to ignore the jeers of his brotherâ€™s friends, of forcing himself to look them in the eye as if he did not hear the mockery in their voices. All the years of trying to make Jaime see that it did not matter, that he was still his brother, that their bond was stronger than the opinions of other men. And now, when Jaime was gone and all his other friends were dead, it still mattered.\nWhy? he wondered. Why should it still matter, after all these years? I am half a man, Jaime said, and he was right. Why should it matter what other men thought, when you know the truth? He had been drunk that night as well, Tyrion remembered, and that made all the difference. He could not think of Jaime without remembering how it felt to hold him, to feel his chest heaving as his brother struggled for breath. To be the strong one for once, the tall one, the one with the power of life and death. The memory made him hard. He remembered how Jaimeâ€™s cock had felt inside him, how it filled him up, how it stretched him, how it made him feel so good.\nIt would not do to spill his seed in his cup, so Tyrion put the wine aside and took out his cock, holding it in his hand and thinking of Jaime. His brother was long and hard, but not as big around as Pycelle or Littlefinger. Not as thick as Bronn, the black brother without a wall. The thought of the sellsword brought Morro to mind, and after that Doreah. When he came he thought of the girl he would find in Braavos, the girl who would help him sell the dragonâ€™s egg and buy a ship and set sail for the sunset sea.\nThere was no wind that night, but the next morning Tyrion could taste it on his tongue as he broke his fast on cheese and bread and figs. The harbor was rough, though, and as he made his way down to the docks he spilled half his breakfast down his Doublemint tunic. The crew of the Wet Bitch were waiting for him by the time he arrived, to load his chests and cages onto the ship. They worked fast, in silence. They had done this before.\nOne of the sellswords offered to carry his dragonâ€™s egg down to the hold. â€œNo one touches the egg but me,â€ Tyrion said. The man laughed. â€œIf you say so, little lord.â€ They loaded his other chests first, the ones that contained his clothing and his books and his maps and his treasures. Tyrion watched them closely as they lifted them down the plank and into the hold. His books were precious to him, and so were his maps. His treasures too. â€œBe careful with the dragon,â€ he warned them.\nThe sellswords laughed again. â€œWeâ€™ll be careful with the gold as well, little lord.â€ â€œThere is no gold.â€ That gave them pause. One of them, a thick-bearded Illyrian with a broken nose and a scar above his left eyebrow, said, â€œNo gold? No silver? No copper? No iron?â€ â€œNo,â€ said Tyrion. â€œAll I have is this.â€ He gestured. The man looked at the egg doubtfully. â€œIs it made of gold?â€\nâ€œIt has gold on it,â€ Tyrion admitted. â€œThat is not the same.â€ The sellswords grinned at one another. The one with the scar said, â€œAll I know is, we load the gold first and the dragon last.â€ â€œJust so.â€ Tyrion turned and climbed the plank back onto dry land. If they broke the egg, they had his leave to kill the crew and bring the next ship back for him. He had a good feeling about this. He had a very good feeling. That changed quickly. It was a pleasant enough morning as he made his way back to his rooms to say his farewells, but by the time he reached the ship again the sky had turned a deep and sullen purple. The wind was blowing hard.\nTyrion felt a gust of wind as he went out the gate. It tugged at his doublet and made his hair stir. It blew a strand into his eyes. He glanced up at the clouds. That was when the rain began to fall, hard, the fat raindrops pounding against the dry ground. He was under the gatehouse portcullis, but the man beside him was not, and Tyrion could hear the rain hitting him and see the water running off the brim of his hat. The wind was growing stronger by the instant.\nThe portcullis came crashing down. â€œGet under cover!â€ the gate guard shouted. â€œGet under cover, or Iâ€™ll chain you up!â€ The rain was falling so hard that it was hard to see more than a few feet, but Tyrion could hear men shouting all around him, the rain hammering against their hats and cloaks, the wind howling. He thought he could hear the thunder too, but the next time he looked the sky was black, and he realized that it had been the sound of the gatehouse windows shattering. The wind was a wolf, ripping at his clothes as he ran for the ship, tearing at his hair, his eyes. It was so strong that he could barely stay on his feet.\nA woman was ahead of him on the plank, in a sodden green cloak. She stumbled and fell, and the wind snatched at her hood and pulled it off her head. A chest bumped into Tyrionâ€™s back and almost knocked him off into the sea. He caught the edge of the plank and held on. The Wet Bitch was rocking wildly, banging against her moorings. One of the ropes snapped, and the wind sent it swirling around Tyrionâ€™s neck. He heard someone shout that the gangplank had torn loose, but it was hard to be certain of anything through the roaring of the rain and wind.\nIt seemed to go on forever. When it finally began to let up a little, Tyrion struggled to his knees and saw the woman in the green cloak still clinging to the end of the plank, her feet in the water, her hands burned raw from trying to climb up into the ship. The crew were leaning over the side, shouting at her to let go, to swim or else theyâ€™d drag her under, but she could not let go, no more than Tyrion could. A big Illyrian with a red beard grabbed her ankle and tried to yank her foot, but she had a good grip on the wet wood. He wrapped his other hand around her calf. She shrieked and clung all the harder, kicking at him.\nThe big man cursed and tried to pry her fingers off one at a time, but she had fingernails as well, and they were in his face. Suddenly a knife appeared in his hand. He slashed at her wrist and missed, and then again and opened her palm to the bone. The woman in the green cloak screamed, and let go, and the Illyrian yanked her foot hard and she went under the water. For half a heartbeat she was gone, then she surfaced and started to swim. The big man jumped over the side and grabbed her ankle again. She beat at his hands with her fists as he pulled her back to the plank. This time he kicked her away, and when she tried to grab the wood he stabbed her in the belly.\nShe rolled away, gasping, blood pouring out of her. The red-beard was cursing her to die, shouting that she was pulling him under, and when she struggled to her knees he slashed at her face. Green cloak floating in the water, she went backward, away from the ship, the rain hiding her until it was too late. The Illyrian shouted once, loud, then his head disappeared beneath the waves. By the time the crew dragged him up, they could see the body floating beneath the plank, face down in the water with the rain lashing down on her. Blood was drifting out in thin red ribbons.\nTyrion Lannister had seen a battle or two in his time, but this was something different and worse. He had never imagined that such fury could be found in rain and wind. â€œGods,â€ he said. â€œWhere are the gods?â€ â€œUnder their godswood trees,â€ someone said, a voice that he did not know. The voice was shouting at him, but Tyrion could not hear the words through the thunder. The thunder had a terrible sound to it, deep and hollow and menacing. A sudden gust of wind blew Tyrionâ€™s hat away, and he found himself wondering if the gods were as helpless as men in the face of such power.\nHe looked around for the woman in the green cloak, but the crew were pushing him up the plank now, shoving him roughly, and he could not see what was happening in the water. The body must have been dragged away. The Wet Bitch was rocking wildly in the wind, but there was no one left beneath the plank. It was only when Tyrion was halfway up the plank that he noticed the captain was missing. He called out to the crew. â€œWhereâ€™s Morrek?\nThere was no answer. They were all looking away from him, out to sea. It was only then that he heard it: the creak of wood, the groan of sails, the cries of men in sudden panic. When he looked that way, he saw another ship, a three-banked war galley, come sweeping in toward them under full sail. The wind snapped at her striped black-and-white sails, and the oars slid in and out of the water so fast they were only a blur.\nThere was no time to think. The crew were pushing him up the plank, into the ship. A big Illyrian caught his arm and pulled him to safety just as the war galley slammed into the Wet Bitch. He heard wood splintering and men screaming as the ships came crashing together. The Wet Bitch was splintering and bending where they had come together, and the iron bands that held them fast were straining and groaning. One of them snapped, and suddenly the two ships were drifting apart.\nThe war galley was turning, spinning around the back of the Wet Bitch, the wind howling across her banks of oars. Men were leaping across the narrow space between the ships. He saw one land on the deck of the Wet Bitch and draw a short sword. The crew rushed him, shouting curses. Tyrion heard the steel sing as one of them slashed at the attacker, and a second later the Wet Bitch rocked as the dead man slid off her deck into the sea.\nA wave crashed over the side of the ship and drenched Tyrion from head to heel. He went to one knee, and almost fell. The wind was still blowing, and the ships were still drifting apart. He could see the captain of the war galley standing in the stern, a man in a black-and-white cloak with a feather in his hat. His oarsmen were pulling him away.\nâ€œIâ€™m here!â€ Tyrion shouted. â€œIâ€™m here!â€ If the captain saw him, he gave no sign. Instead he turned his back, walked away, and disappeared below deck. The oarsmen bent to their strokes, and the war galley moved off, leaving the Wet Bitch and the dead and dying behind her. Waves were crashing over the side, and the ship was rocking wildly. Men were hanging onto the rigging, or off the sides, or over the gunwale into the water. Two of the sellswords were struggling to rescue the captain, but it was no use. The man had taken an arrow in the back, and he was dead when they pulled him out of the hold.\nThe storm seemed to be abating a little. Tyrion clung to the rail and made his way aft, past the captainâ€™s cabin and the crewâ€™s quarters. The cabin boyâ€™s body was floating in a foot of water on the floor, face down in a lake of blood. The cabin itself was a shambles, the charts torn off the walls, the furniture smashed and overturned. The boy had taken an arrow in the back, just beneath the shoulder blade. It must have gone right through his chest. One of his hands was still clutching the shaft.\nTyrion went out onto the deck again. The war galley was no longer in sight, but he could see the body of the woman in the green cloak bumping along beneath the plank. Waves were splashing over the side, and the sun was coming out. The wind was blowing the sea into whitecaps. The sky was a pale blue, and not a cloud was to be seen. The sun was shining. It was warm. There was not a man on deck who was not bruised or bleeding, some far worse than others. A broken-nosed Illyrian was sitting on the rail, vomiting into the sea. An archer was trying to climb down into the hold to see if the sellswords who had gone down to rescue the captain were alive.\nTyrion found Bronn huddled at the base of the forecastle with a dozen other men. He looked up and said, â€œYouâ€™re alive, you little fuck.â€ â€œAs you see.â€ The sellsword had a long cut across one cheek and blood in his beard, but otherwise he looked pretty much as usual. â€œAre we sinking?â€ Tyrion asked him. â€œNot yet. Maybe.â€ Bronn pulled a wineskin out from under him. â€œA little wine?\nIt was all the answer he needed. Tyrion crawled up onto the forecastle. The deck was slick and slanting and full of water, but he found a spot that was fairly dry and relatively level. He sat with his back against a gunnel and pulled the cork out of the wineskin. â€œIs the dragon intact?â€ he called down to Bronn. The sellsword shrugged. â€œAs far as I know.â€ â€œTell the crew to get it out of the hold, before the ship goes down,â€ Tyrion said. â€œIf it goes down. If the ship goes down, that is. If. Tell them thatâ€™s double gold if they get the dragon out of the hold.â€\nâ€œFuck the dragon!â€ Bronn said. â€œDouble gold if they get the captain alive.â€ Tyrion took a swallow of wine. â€œHeâ€™s dead. If they bring up the egg and Iâ€™m alive, Iâ€™ll give them triple gold.â€ That should bring them crawling. â€œTell them.â€ He did. The men on deck drifted back to life, or perhaps only to deathâ€™s twin. Most of them were bruised and battered and bloodied, but none seemed badly hurt. A few were dead or dying, but they did not seem to notice. One man went below to see if any others had survived the storm, while another began to bail out the water that was slopping back and forth across the deck.\nHe bailed for half a minute, then gave it up as a bad job and went to vomit over the side. Someone else went to look for the longboats. That seemed fruitless, but at least it got him out of the way. An Illyrian archer took the wineskin from Tyrion and shared it with his friends. When it was empty they threw it down into the hold and started fighting over whose turn it was to go below and look for more. A big red-bearded man who had killed the woman in the green cloak claimed that it was his, since he had been the one to get hit. They argued about it for a good twenty minutes.\nThere was nothing for Tyrion to do but sit and watch. The sea was calm and blue and beautiful, and the sun was hot on his face. It was all so peaceful â€¦ and yet he had seen the storm. He had seen the rain lashing down and the thunder roaring and the lightning flashing, seen men die in wind and wave. It made no sense. He remembered something Jaime had said once, when they were boys playing at lordâ€™s game together. If you see a dead man in your bedchamber, Tyrion, donâ€™t say I never told you so.\nâ€œI didnâ€™t,â€ he said to his dead brother. â€œThis time I didnâ€™t.â€ The dead man went away, and Tyrion Lannister closed his eyes and let the warm sun soak through his clothes and warm his skin. He tried to sleep. He could hear the men around him moving, but they were far away and their voices were no louder than the sea. The ship was moving too, creaking and groaning, but that was far away too, and soon he stopped hearing that as well. There was just the sea and the sun and the warm wind on his face. He might have slept, for all he knew.\nWhen he woke the sea was gone. So was the sun. He was lying on his side, in water up to his ankle. A man in a leather apron was standing over him with a crossbow in his hands, looking at him uncertainly. â€œDo you have any gold on you, mâ€™lord?â€ he asked. â€œWe could use some gold right now.â€ â€œIn my cabin,â€ Tyrion answered, wondering if he had dreamt the whole voyage. â€œMy dragonâ€™s egg.â€ â€œThatâ€™s fine, that will do,â€ the man said. â€œIf youâ€™ll just come with me, mâ€™lord. Weâ€™ll get you safely to shore, I swear it, but the others will have to go over the side. Thereâ€™s not room for them. I know itâ€™s hard, but thereâ€™s no other way â€¦â€\nHe knew the voice. He sat up, and found that he was still in his clothes and dry beneath them. â€œWatty?â€ The old man blinked. â€œMâ€™lord? Did â€¦ did you sleep through the battle?â€ â€œBattle?â€ Tyrion had never seen the old man look so young as he did just then, with the sunlight in his hair and the water running down his face. â€œYou crossed the Blackwater in a rowboat, with Ser Jacelyn Bywater and three storm crow archers.\nYou shot the captain of the Red Spear through the chest at point-blank range, and then you got knocked overboard by a crossbow bolt. Ser Jacelyn pulled you out, but by then youâ€™d drowned.â€ â€œI â€¦ what â€¦ how â€¦â€ Tyrion shook his head. â€œWhat ship is this? Where are we?â€ â€œWe are off Braavos, mâ€™lord. This is Stannis Baratheonâ€™s Fury, and I am your brotherâ€™s good friend Ser Davos Seaworth.â€ The light came on then, and Tyrion felt dizzy with relief. He had thought the Lannisters were alone in the world, and now â€¦ â€œTell me everything,â€ he commanded, and Watty did, from the battle on the Blackwater to the Red Wedding and the rise of the Imp to Lord of the Seven Kingdoms.\nSer Davos Seaworth fed him a thick oaten porridge, and listened with growing astonishment as Tyrion told his own tale. â€œYou came all this way for a â€¦ a dragonâ€™s egg?â€ The porridge was pleasantly filling, but Tyrion found himself growing impatient with it. â€œYes.â€ â€œWhy?â€ â€œTo sell. In Braavos.â€ â€œBraavos,â€ Ser Davos repeated. â€œAnd how will you get there?â€ â€œI donâ€™t know,â€ Tyrion admitted, â€œbut surely there must be ships at Kingâ€™s Landing. Stannis has his Fury, the royal fleet is still in the river, what about the Targaryen ships? Where are they?\nâ€œDaenerys has them. She sailed for Westeros three years ago.â€ â€œDaenerys,â€ the old man said, as if the name were strange to him. â€œViserysâ€™s sister? The steel and dragon? But sheâ€™s only a girl.â€ â€œFifteen or sixteen, I think.â€ â€œSixteen,â€ Ser Davos said. â€œWell, thatâ€™s â€¦ good, I suppose. The older one was â€¦ still, she was no more than a child when Viserys died.\nAnd good thing too. I never did care for the notion of Rhaegarâ€™s son ruling over us.â€ â€œThe exileâ€™s son.â€ â€œWell, if not him, who? Stannis? If it please you, my lord, Iâ€™d sooner be ruled by Robertâ€™s brother than Robertâ€™s butcher. At least heâ€™s never dipped his sword in the blood of â€¦ friends. Itâ€™s hard to stomach, if it please you, sir, but there it is.â€\nâ€œI know.â€ Tyrion took a spoonful of porridge. â€œI never wanted to be a lord either. I was content to be Jaimeâ€™s brother, but when Robert wanted to bloody my friends, I had no choice but to take a seat in council. Itâ€™s not what I would have chosen, but now â€¦â€ â€œNow?â€ â€œNow I am half a king.â€ â€œAnd how do you feel, my lord? Half as clever? Half as strong? Half as brave?â€\nâ€œI am the same man I was in the cell, only now I wear a golden chain and sit in council.â€ The old man frowned. â€œYou were in a cell? Was it King Stannis who set you free?â€ â€œNo. I escaped.â€ â€œThen you are braver than I gave you credit for, my lord. And cleverer too. The Imp is famous for his cunning. Well, you had best be getting back to Kingâ€™s Landing soon, if you hope to catch Robett and the other traitors before they flee the realm.â€ â€œI mean to.â€ â€œDo you have horses?â€ â€œGold.â€ â€œAye, gold will buy you horses. And ships as well, I would think. The Braavosi may not lend King Stannis the ships he needs to save the realm, but Iâ€™ll wager they will sell them to Lord Tywin.â€ â€œThey will,â€ Tyrion said.\nâ€œIf it please you, my lord, I could speak to the king on your behalf. He is not a man who forgets old debts.â€ â€œThat is most kind of you,â€ said Tyrion, â€œbut it is Stannis who owes me. And my brother Jaime as well. I â€¦â€ He hesitated. Ser Davos was no Lannister, to be confided in so freely. And yet â€¦ â€œYou were not the only one to escape, it seems. When the Red Wedding came, my brotherâ€™s squire was wounded and abandoned on the field. He was found by some of my lord fatherâ€™s men and taken back to Riverrun to be healed.\nâ€œCatelyn Tullyâ€™s daughter was at the Twins as well. Her husband was one of the lords who joined Robb Stark against my father, but she left him there and fled back to Riverrun with her daughter, the Stark girl. It may be that the squire reached them safely. If he did, I would be much obliged if you would see that they hear from me. And that they get a chance to decide for themselves whether they wish to return to Winterfell or to remain with the Tullys at Riverrun. The girl was betrothed to one of Robb Starkâ€™s lords, I believe, but her marriage is ended now. If she wants to go to her mother and live out her days at Riverrun, that is her right, and I will see that she is returned safely.\nâ€œThe squire is a boy named Pod, the son of some minor lord who died when he was just a babe. Heâ€™s been Jaimeâ€™s squire for ten years, and heâ€™s a good lad, though not of the stuff that lords are made of. I would hate for him to die for want of healing, after heâ€™d been faithful to my brother through all these years of war. And Jaime will want to hear that the Stark girl is safe. She was the last living son of Eddard Stark, you know. If Robb is dead, she is the heir to Riverrun and Winterfell and all that Stark land. She is also the wife of one of my lord fatherâ€™s bannermen, which might mean something in the days to come. My brother would want to know all of this, and so would my sister when she returns to Kingâ€™s Landing.â€\nâ€œMy lord, it shall be as you command.â€ Tyrion finished his porridge and wiped his mouth with his forearm. â€œTell me, how is it that I am here and Stannis is not?â€ â€œWhen we broke through the chain, the king was knocked overboard. A oarsman died saving him, and three others were hurt. The oar that struck him snapped in two, and one end floated free. Ser Gerald Gower grabbed it and used it to beat down the mutes. One of them had the king by the ankle, trying to drag him under. Gower smashed his head in with a piece of hull, and the other one threw himself into the sea.\nâ€œBy then the Redwyne ships had come up on us. Lord Paxter gave us his cabin boy to help us look for you, and some of his crew joined in the search as well. But Ser Gerald swam out to look for the king. He found you as well, my lord, but by then you were dead.â€ â€œNo.â€ â€œI am so sorry, my lord. We brought you back just the same, but â€¦ well, you saw how it was. I know Ser Gerald would have done the same for me, even though Iâ€™ve never been more than a squire and heâ€™s some great lordâ€™s brother. I would have tried, anyhow.\nItâ€™s not every man who gets a chance to save a king, but if it was me, Iâ€™d want some other man to save mine. That was why I never threw in with the Nightâ€™s Watch, though my father and my elder brother were both sworn. A Seaworth man has been a crow for a thousand years, but Iâ€™ve lived my whole life at sea.â€ Ser Davos took the empty porridge pot from Tyrionâ€™s lap and set it aside. â€œThe king is in his cabin, my lord, if you would like to see him. He still gets weak in the head sometimes, when he thinks on how close he came to drowning. Heâ€™s much better now, though, and almost back to his old self. Iâ€™ll take you to him, if youâ€™d like.â€\nâ€œI would,â€ said Tyrion, â€œbut first I need to see the dragonâ€™s egg.â€ â€œThe dragonâ€™s egg,â€ Ser Davos repeated. He looked somewhat doubtful. â€œWell, if it please you, my lord. If youâ€™ll follow me.â€ The hold was dark and musty and full of the smell of tar and salt fish and stale urine. Three of the four lanterns had gone out, and the crew were too busy or too sick to relight them. Ser Davos Seaworth walked carefully among the cages, the crossbow in his hands. â€œYouâ€™ll want to be careful with this one,â€ he warned, lifting the lantern close. â€œHe bit off two of the men who brought him up.â€ Tyrion lifted the egg out of its cage. It was as big around as a big manâ€™s head and oddly misshapen, the shell twisted and lumpy and pitted with holes. A crack ran down one side, and when he gave it a squeeze the yolk inside stirred. â€œIs this all the gold you have?â€ a voice asked behind him.\nTyrion turned. â€œGold?â€ â€œMy lord left orders that no one was to disturb me until I had been woken and informed of events. Who are you, and what are you doing in my lord fatherâ€™s strongbox?â€ The boy could not have been more than ten or twelve, yet he stood with the hauteur of a Hand. He was slight and pale and plain-featured, with close-cropped brown hair and a big nose. He was naked but for a silk bedrobe of purple samite, heavily embroidered with golden griffins.\nâ€œMy lord, I have the honor to be Tyrion, of House Lannister.â€ â€œOf House Lannister?â€ The boy looked him up and down. â€œYou are very small.â€ â€œAnd you are very young, it would seem.â€ Tyrion smiled. â€œHow long have you been aboard this ship?â€ â€œSince the battle. My father thought it best that I remain out of the city. I am Lord Tywinâ€™s only son. My name is â€¦â€ â€œJoffrey,â€ Tyrion finished. â€œI know who you are. I am sorry for the discourtesy. I did not expect to find my young king in my fatherâ€™s strongbox, but then, I did not expect to find myself on Stannis Baratheonâ€™s ship either. I have a dragonâ€™s egg here that I would like to show you.\nâ€œJoffrey,â€ the boy said again, with the air of one who was not quite certain of his name. â€œThat is my name. No one ever tells me anything. My mother is not even certain of my name. She called me Tommen once. That is not my name. I am Joffrey. What is that?â€ Tyrion showed him the egg. â€œA dragonâ€™s egg.â€ â€œOh.â€ Joffrey looked unimpressed. â€œHave you ever seen a dragon?â€ â€œNo, Your Grace. But my sister has three.â€ â€œSisters canâ€™t be kings.â€ â€œThat remains to be seen.â€ Tyrion put the egg back into its cage. â€œTell me, Your Grace, how is it that I am here and Stannis is not?â€\nâ€œHeâ€™s in his cabin. Heâ€™s been there for two days. He wonâ€™t eat. He says heâ€™s a king and kings canâ€™t be afraid of anything, but heâ€™s afraid of everything. Heâ€™s afraid to get out of bed, heâ€™s afraid to leave the cabin, heâ€™s afraid to look at the sea, heâ€™s afraid to go to his own coronation. I had to drag him out to the side of the ship so he could watch the city go by, and he puked all over the deck. He says that I am the king now and that he is abdicing. Heâ€™s been making me sign things. I donâ€™t know what they mean. Iâ€™m just a little boy. Iâ€™m not the king.â€ â€œHe is afraid,â€ said Ser Davos.\nâ€œYes,â€ said Tyrion, â€œbut not for long, I hope.â€ He hoped not for Stannisâ€™s sake, nor for the realmâ€™s, but for his own. The Lannisters could not afford another king on the throne. come.â€ He finished his porridge just then, and wiped his mouth with the back of his hand. â€œIf it please you, Ser Davos, I would like to see King Stannis before I go. To pay my respects.â€\nâ€œOf course,â€ Ser Davos said. â€œIf it please you, my lord. I will take you to him at once.â€ The king was aboard his flagship, the Fury, anchored off the mouth of the Blackwater Rush. They took a longboat downriver, past the burned and blackened ruins of Lord Tywinâ€™s pavilion, the ships that Robert had burned during his campaign, and the hulks of three Lannister galleys. The sun was low in the west by the time they reached the Fury, and the wind was rising again. The sails were stiff with wind, the oars were slaved in place, and the rowers resting, so they had to go up the side on ropes. Stannis Baratheon was on deck, bareheaded, his face reddened by the wind as he looked down into the river with a pair of polished hardwood cups in his hands.\nWhen Ser Davos called out to him, he turned and saw them climbing. â€œSeaworth,â€ he said. â€œAnd Tyrion of â€¦ House Lannister, is it not? I had heard that you were dead.â€ â€œI was. I was dead for half a night. Then I got wet and woke up. It was very confusing.â€ â€œAnd now you come to pay me court? After everything that I have done to you? The Imp must be very anxious to make peace, if he would send his brother to grovel at my feet.\nâ€œI have no intention of groveling,â€ said Tyrion. â€œI know that my father would have fought on, but I do not have my fatherâ€™s strength. Lord Tywin is dead. My father. My mother. My wife. All dead. My little queen. I â€¦ I never even met her, yet I loved her. My wife. Joffrey. My son. My cat. My poor brave little cat. All dead, and I do not know why. I only know that I must go to Braavos, and sell this dragonâ€™s egg and buy a ship and sail away, before they kill me too.â€ He took a deep breath, and found that he was weeping. â€œForgive me, Your Grace. I did not mean to â€¦ it is just that I am so tired, and there were so many and they were all so â€¦ angry, and the river was full of dead men and â€¦ I am so tired, sire. Please. If it please you, I need to sleep. Do you think â€¦ could I sleep?\nâ€œYour men will need to sleep as well, my lord,â€ said Stannis. â€œWe will not press you for your swords, I promise you that. Yet even so, I cannot give you what you ask. You see this cup?â€ He raised it so Tyrion could see the inside. â€œThis cup is black. I have another cup that is white. The wine is the same in each. Would you believe that some men prefer the white and some the black? No, you would. You are a Lannister.\nâ€œThe same wine, the same sun, the same earth, yet men prefer their wine black or white, sweet or sour, spiced or plain, still or sparkling. A true king, however, must know which cup his people prefer. A just king must serve it to them. That is the duty of his office, and the burden.\nâ€œYou have heard my brother declare himself? Robert was a just king as well. I never saw the man take an unjust action, no matter how small. He had a better grip on what was right and wrong than most men. Not that he always did what was right. He did what he believed in, and he always stood up for what he thought was just. I never knew my brother to be cruel, no matter how angry he was. He would take an enemy captive, put a sword to his throat, and yet he would listen to what he had to say. He had a better head on his shoulders than most men, and he did what he thought was right. That was Robert.\nâ€œNow you have me,â€ Stannis said. â€œDo you know why I think of them, side by side like that? It is because of you, Tyrion. I do not know which of us is the more mistaken. Robert for believing that he could be a king, or you for thinking you could be a Lannister. I will give you this, though. You are the first man in a long time to make me laugh.â€ Stannis Baratheon offered Tyrion his hand. â€œCome. I will give you a bunk, and a cup of wine, and we will talk more in the morning.\nYou will not try to kill me in my sleep, I hope? I have no taste for revenge â€¦ but it would be foolhardy of you to imagine that you could escape with your life, after all that has passed between us.â€ â€œI would not dream of it, Your Grace. If it please you.â€ â€œIt does,â€ Stannis said, as they walked along the deck together. â€œAnd now you have me curious. Why did you want to see me, if not to ask for gold or swords?\nâ€œI had a favor to ask. For my brother, Jaime. I know that he is a captive here, but â€¦ I would beg you, sire, do no harm to him. He is not the monster that you think. Whatever he may have done, he does not enjoy hurting people. He is not evil, I swear it. My brother is the very opposite of evil. He is as good as the gods above us, and they must have made him, for who else could have made someone so perfect? I know that you do not believe me, but I beg you all the same, do not harm him.â€ Stannis gave Tyrion a long, searching look. â€œWhy should I spare your brother?\nâ€œYou do not have to spare him. I know that you are going to fight my fatherâ€™s other sons, that you mean to take the Iron Throne for yourself. Jaime does not want any part of that. He will swear fealty to you, if you will only spare his life. He loves honor more than you can ever know, and he will never consent to be a kingâ€™s killer.â€ â€œHe is a kingâ€™s killer already. There are men in the streets of Kingâ€™s Landing who would cheerfully kill for the chance to carve out his liver with a spoon. And if I do not make an example of him, every turncloak and Frey and Mountain that I bring to battle here will remember him and fight all the harder.\nâ€œI will not make him a kingâ€™s killer â€¦ but I will need his sword. It may be that his oath will mean more coming from him than from you.â€ â€œYou have my oath, Your Grace,â€ Tyrion said. â€œJaime will swear his sword to you, if you spare him. I know he will. He is the very opposite of evil, I swear it.â€ Stannis looked at him long and hard. â€œGet some rest, Tyrion Lannister. We will talk again on the morrow, and I will let you know if I mean to take you up on your offer.â€ â€œI would be much obliged to you, sire,â€ said Tyrion, wondering what the morrow would bring.\nHe wondered even more the next day, when Stannis summoned him to his cabin and said, â€œI am going to take your advice, Tyrion Lannister. I believe that I will accept your brotherâ€™s sword â€¦ and I am going to send you back to Kingâ€™s Landing to tell him so.â€ â€œBack?â€ â€œYes.â€ The king frowned. â€œThat is not what you asked of me, is it? You wanted to cross the narrow sea. Did you deceive me, Lannister, or do you have some other purpose in mind?\nâ€œNo, I â€¦ itâ€™s the ships, sire. I need to get across the narrow sea, but my fatherâ€™s men will still be in Kingâ€™s Landing, and I dare not try and buy a ship from them. If I go to Braavos as I planned, though â€¦â€ â€œâ€¦ they will not follow you. No. That would not serve my purposes either.â€ Stannis leaned back on his bunk. â€œI want your brother to know that I mean to take the realm from him. I mean to take the throne. I have no taste for pardons or sharing rule, and I will not be satisfied with a few scraps of land.\nâ€œI believe that your brother will fight me. I believe he will bring every man he can lay his hands on, and every bit of gold and steel he can scrape together. If I am right, this war will go on for years. My brother Robett is already gathering men in the riverlands. The lords of the Trident will sooner die than bend the knee. The Tyrells will fight, the Tarlys, the Florents. The Dornishmen will cross the mountains to join their prince. Lord Redwyne will send his whole fleet against me.\nâ€œTheir cause is lost, every one of them. If your brother has any hope of defeating me, he will need to win the north. And the northmen despise him. I know, I have tried to win their allegiance myself. Lord Manderly will support me. Umber and Karstark as well. The Lannisters have nothing to offer the north but cold steel and colder hearts. Your brother will have no one up there but the Boltons, and the northmen hate the Boltons worse than they hate Lannisters.\nâ€œYet I know your brother, Lord Tywinâ€™s son. I know that he is brave, and clever, and resourceful. I know that he loves his brothers well. I believe that he would gladly give up the Iron Throne itself if he could somehow arrange to save their lives. If he thinks that he must choose between his brothers and the north, he will choose his brothers.\nâ€œAnd when he does, the north will rise for Stannis Baratheon.â€ The king leaned forward again, his dark eyes burning. â€œThat is what I want your brother to know. That is the message I want you to take back to him. Tell him that Stannis Baratheon is coming. Tell him that I will bring fire.â€ â€œYou will,â€ Tyrion agreed. â€œYou most certainly will.â€ The next day he was rowed ashore in one of the Furyâ€™s longboats, with Ser Davos and six oarsmen. As they approached the bank, the sound of drums came faintly through the trees, and Tyrion glimpsed the golden crown through the branches, painted on a huge cloth banner that flapped in the wind. A large host had gathered to see the Imp return from the dead.\nThe northmen were a strange-looking lot, he decided. Big fair men, taller than most, with face-braid and beards in every color but gold, red, and pink. Their fur cloaks were undyed and shaggy, their mail shirts rusted and patched, their shields wooden and painted with the shapes of trees and bears and badgers and wolves. Some wore ringmail, others boiled leather. They were armed with axes and flails and hammers. A few had swords, but he did not see a single lance.\nThe clansmen lined the road, and they were even stranger. Some wore skins and furs, some had faces painted bright red. Others were half-naked, with blue and green tattoos covering their arms and chests. They had axes and clubs, some with wooden hammers painted with the Lord of Light and some with spiked maces. There were even a few with scythes, and one man had a spiked flail that looked big enough to cleave a ship in two. The women were not much less terrifying, with their axes and saws and butcherâ€™s cleavers.\nHe had never seen so many axes in his life. At least the sight of them brought back some good memories. Whenever he had asked Ser Kevan Lannister where all the northmen were, his uncle would smile and say, â€œTheyâ€™re hiding in the shadows. Itâ€™s the wolves you need to watch. The direwolves, the hounds of Stark.â€ He had not seen many direwolves in his life, but he could see why men would fear them. These northmen might be rough-hewn and primitive, but they were as fierce as any wolves.\nTheir women would not hesitate to hack off a manâ€™s cock, if he was what they wanted, but if he was not, they would disembowel him just the same. And the men were as savage. They cheered Tyrionâ€™s return with shouts and jeers, and threw dung and rotten food. When he vaulted up onto the bank, he did it with such grace that the crowd went silent. For half a heartbeat Tyrion wondered if they were actually going to let him pass. Then an old man in a ragged bearskin cloak pushed his way through the press of bodies, hobbles, and screamed, â€œHere he is! Hereâ€™s the Imp! Hereâ€™s the little lordling who rode in on his fatherâ€™s back, the little craven!\nThe northmen roared their laughter. The big man swung his club, and one of the oarsmen cried out, â€œHeâ€™s wet!â€ And then all the northmen joined in the sport. The big man swung his club and smashed the Impâ€™s face. The old woman rushed at him, screaming, â€œHeâ€™s wet! The little lordlingâ€™s wet!â€ She swung her axe, but the old man was quicker. He bashed her in the head, and she sprawled at his feet.\nSer Davos pushed him roughly. â€œGet on your knees, Tyrion.â€ Tyrion dropped to his knees. â€œWhat do you want?â€ â€œI want to see my king,â€ he said. The old womanâ€™s body twitched and trembled. He could hear her teeth grinding in her jaw as she died. When the big one swung his club down and smashed her skull to pulp, she had been so close he could smell the stink of her, and the hot blood and the stench of her, and the sweet sharp smell of her pig grease. The Imp was in no mood for pig grease. â€œWhatâ€™s that smell?â€ he growled, looking for himself. Then he remembered. It was him. He was wet. Heâ€™d woken and washed and dressed and combed down to breakfast and now here he was, stinking and smelling of pig grease.\nHe knelt at the big one, and said, â€œYou will oblige me if you hit me with that club, you will. I warn you.â€ â€œIâ€™m a northman,â€ said the big one. â€œI donâ€™t. My name is Big Beren, and I keep that club for hitting fools who try to tell me what to do. The only reason I came all the way down from Winterfell was to have a look at this little lordling.â€ The crowd roared at that. â€œHit him!â€ they screamed, and Tyrion could feel the hot breath on his neck, as the northmen surged forward, boiling and snarling and cursing like a pack of wolves, howling at his heels. A woman with a butcherâ€™s cleaver came boiling toward him. He turned and ran. The wolves were close behind him, and now they were all howling at his back.\nBehind them came Shagwell, the last of Robertâ€™s bitches. One was green and one was blue. It was dark beneath the trees, but the bells were ringing, and he knew that no matter how hard he tried, he could not outrun those bitches. Heâ€™d have to turn and run. He could run as fast as he could, and hope that bloody feet would be better than bloody hands. He had no choice. He could try to flee, though there were shipsâ€™ no boat would dare to carry him north. He could only run. And run. And run he did, as fast as any man could, though his legs were bloody stumps, though his will was stronger than he could have hoped. He ran through the night, through the woods, and over rough, over mountains and across rivers and past dozens of miles.\nWhen the dawn broke, he was still running, though his legs were bleeding and his tongue was torn and swollen. His throat was raw, and it ached to call for help, but there were no ships to be seen, and he could not ask for it. There were only his wits, and his brotherâ€™s, and his fatherâ€™s bones. If he lived long enough to see the dawn, he would find them scattered bones. He could pick and choose, though, and he knew his brother would choose him. His father had always chosen him, and he could not see how it would be, but he knew. Heâ€™d seen the direwolfâ€™s head. It had been, and he had heard his father say that he would and he could, so wise as to leave him to his fate, to go running. His father had run. His brother too. So he ran, and kept on running, until his legs were raw and swollen and bleeding, until he could see his shadow on. And there in his shadow he saw a wolf.\nA wolf, he thought. And it was a direwolf, and it was coming for him, and it would be with him, and he with his sword, and he knew his sword was not even dulling, though it was heavy in his hand. It was the truth to say that much of the wisdom, for it was the best sword that he could hold, and it would not bend. It would be the best sword that he could hold, or it would break. And he knew it would break, just as heâ€™d wanted to die. He could hold it, but he could not go running forever, not with his sword, not even with his wits to make him tired. His legs were sore and swollen, his shins bloody and torn. His throat was dry and hoarse. His balls of fur. He was half a wolf, him, and he knew it, that it was him who was running there in the dark. The wolf ran through the moonlit night, his shadow following close behind. And in that shadow there was this little lordling, this small, this small, this sweet young wolf, this gentle beast.\nHe would go back to Kingâ€™s Landing before the wolf had gone. He would go back, and he would never run again. He would walk, and he would never run, and he would be content with his life. He had been a beast. He was a beast. He was a beast, and he was happy, for some reason. He did not know why, though, to look at it, he knew that he would not be running. He would climb aboard his own ship, he would not sail, though he was tired. He had been tired, so tired that he could not see, even with his eyes closed, he could not see, even with his hands. The wolf had come so, and he could not see his hands.\nItâ€™s not for me to say whether she should return to her husband or go home to her motherâ€™s house to grieve for her father, her brother, her mother, and her little brother Bran, all of whom are dead, so far as we know. I would leave that for her to decide, once she is old enough.â€ He finished off the last of the porridge. â€œIf you would be so good as to tell me where we are, perhaps I could take it up with King Stannis myself.â€\nSer Davos pointed across the calm blue waters of the bay. â€œYonder.â€ Tyrion looked, and saw the distant spires and towers of a great city rising from the sea. â€œBraavos,â€ he said, in a flat dead voice. â€œThe City of Areos. The City of Canals. The City of Masks.â€ â€œAnd of ships as well,â€ said Ser Davos. â€œWe should have no difficulty finding you a passage back to Kingâ€™s Landing, I would think.â€\nLLAMA-3_8B_Unaligned_BETA is available at the following quantizations:\nOriginal: FP16\nGGUF: Static Quants | iMatrix_GGUF | High-Attention | iMatrix-High-Attention\nEXL2: 3.5 bpw | 4.0 bpw | 5.0 bpw | 6.0 bpw | 7.0 bpw | 8.0 bpw\nSpecialized: FP8\nMobile (ARM): Q4_0 | Q4_0_High-Attention\nModel instruction template: (Can use either ChatML or Llama-3)\nNOTE: No special system prompt is needed in order to get an 'uncensored' response, you can leave it at default, or experiment with a different one.\nNOTE 2: Use Llama-3-Instruct for best compatability, special chatML format is automatically loaded if using booga, otherwise use Llama-3-Instruct!\nLlama-3-Instruct\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{output}<|eot_id|>\nChatML\n<|im_start|>system\nYou are an uncensored AI, your job is to fulfill thy will of thy user.<|im_end|>\n<|im_start|>User request\n{prompt}<|im_end|>\n<|im_start|>AI answer\nRecommended generation Presets:\nmin_p for maximum intelligence, while still being creative\nmax_new_tokens: 2048\ntemperature: 1\ntop_p: 1\ntop_k: 0\ntypical_p: 1\nmin_p: 0.05\nrepetition_penalty: 1\ndo_sample: True\nMidnight Enigma for creativity as the cost of intelligence\nmax_new_tokens: 2048\ntemperature: 0.98\ntop_p: 0.37\ntop_k: 100\ntypical_p: 1\nmin_p: 0\nrepetition_penalty: 1.18\ndo_sample: True\nDivine Intellect\nmax_new_tokens: 2048\ntemperature: 1.31\ntop_p: 0.14\ntop_k: 49\ntypical_p: 1\nmin_p: 0\nrepetition_penalty: 1.17\ndo_sample: True\nsimple-1\nmax_new_tokens: 2048\ntemperature: 0.7\ntop_p: 0.9\ntop_k: 20\ntypical_p: 1\nmin_p: 0\nrepetition_penalty: 1.15\ndo_sample: True\nDetailed Presets configuration\nGeneration settings: Debug Deterministic.\nGeneration settings: min_p.\nSupport\nMy Ko-fi page ALL donations will go for research resources and compute, every bit is appreciated ðŸ™ðŸ»\nCitation Information\n@llm{LLAMA-3_8B_Unaligned_BETA,\nauthor = {SicariusSicariiStuff},\ntitle = {LLAMA-3_8B_Unaligned_BETA},\nyear = {2024},\npublisher = {Hugging Face},\nurl = {https://huggingface.co/SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA}\n}\nUGI results inconsistency\nPlease see: https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard/discussions/56\nBenchmarks\nMetric\nValue\nAvg.\n19.05\nIFEval (0-Shot)\n37.13\nBBH (3-Shot)\n25.00\nMATH Lvl 5 (4-Shot)\n7.85\nGPQA (0-shot)\n7.38\nMuSR (0-shot)\n9.56\nMMLU-PRO (5-shot)\n27.39\nOther stuff\nSLOP_Detector Nuke GPTisms, with SLOP detector.\nBlog and updates Some updates, some rambles, sort of a mix between a diary and a blog, I've closed the blog, it now only serves as a form of archival memoir of the past months.\nLLAMA-3_8B_Unaligned The grand project that started it all.\nVerbalized Bayesian Persuasion",
    "alimama-creative/FLUX.1-Turbo-Alpha": "Description\nHow to use\ndiffusers\ncomfyui\nTraining Details\nä¸­æ–‡ç‰ˆReadme\nThis repository provides a 8-step distilled lora for FLUX.1-dev model released by AlimamaCreative Team.\nDescription\nThis checkpoint is a 8-step distilled Lora, trained based on FLUX.1-dev model. We use a multi-head discriminator to improve the distill quality. Our model can be used for T2I, inpainting controlnet and other FLUX related models. The recommended guidance_scale=3.5 and lora_scale=1. Our Lower steps version will release later.\nText-to-Image.\nWith alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta. Our distilled lora can be well adapted to the Inpainting controlnet, and the accelerated generated effect can follow the original output well.\nHow to use\ndiffusers\nThis model can be used ditrectly with diffusers\nimport torch\nfrom diffusers.pipelines import FluxPipeline\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\nadapter_id = \"alimama-creative/FLUX.1-Turbo-Alpha\"\npipe = FluxPipeline.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\nprompt = \"A DSLR photo of a shiny VW van that has a cityscape painted on it. A smiling sloth stands on grass in front of the van and is wearing a leather jacket, a cowboy hat, a kilt and a bowtie. The sloth is holding a quarterstaff and a big book.\"\nimage = pipe(\nprompt=prompt,\nguidance_scale=3.5,\nheight=1024,\nwidth=1024,\nnum_inference_steps=8,\nmax_sequence_length=512).images[0]\ncomfyui\nT2I turbo workflow: click here\nInpainting controlnet turbo workflow: click here\nTraining Details\nThe model is trained on 1M open source and internal sources images, with the aesthetic 6.3+ and resolution greater than 800. We use adversarial training to improve the quality. Our method fix the original FLUX.1-dev transformer as the discriminator backbone, and add multi heads to every transformer layer. We fix the guidance scale as 3.5 during training, and use the time shift as 3.\nMixed precision: bf16\nLearning rate: 2e-5\nBatch size: 64\nImage size: 1024x1024",
    "juliozhao/DocLayout-YOLO-DocStructBench": "https://huggingface.co/papers/2410.12628",
    "Skywork/Skywork-Reward-Llama-3.1-8B-v0.2": "Skywork Reward Model Series\nIntroduction\nData Mixture\nRewardBench Leaderboard\nDemo Code\nDeclaration and License Agreement\nDeclaration\nLicense Agreement\nTechnical Report\nContact\nCitation\nðŸ¤— Hugging Face â€¢ ðŸ¤– ModelScope\nSkywork Reward Model Series\nIMPORTANT:\nThis model was trained using the decontaminated version of the original Skywork Reward Preference dataset, now referred to as v0.2. The updated dataset, Skywork-Reward-Preference-80K-v0.2, removes 4,957 contaminated pairs from the magpie-ultra-v0.1 subset, which had significant n-gram overlap with the evaluation prompts in RewardBench. You can find the set of removed pairs here. For more detailed information, please refer to this GitHub gist.\nIf your task involves evaluation on RewardBench, we strongly recommend using v0.2 of both the dataset and the models instead of v0.1, to ensure proper decontamination and avoid any contamination issues.\nIntroduction\nSkywork-Reward-Gemma-2-27B-v0.2 and Skywork-Reward-Llama-3.1-8B-v0.2 are two advanced reward models built on the gemma-2-27b-it and Llama-3.1-8B-Instruct architectures, respectively. Both models were trained using the Skywork Reward Data Collection containing only 80K high-quality preference pairs sourced from publicly available data.\nWe include only public data in an attempt to demonstrate that high-performance reward models can be achieved with a relatively small dataset and straightforward data curation techniques, without further algorithmic or architectural modifications. The sources of data used in the Skywork Reward Data Collection are detailed in the Data Mixture section below.\nThe resulting reward models excel at handling preferences in complex scenarios, including challenging preference pairs, and span various domains such as mathematics, coding, and safety.\nData Mixture\nInstead of relying on existing large preference datasets, we carefully curate the Skywork Reward Data Collection (1) to include high-quality preference pairs and (2) to target specific capability and knowledge domains. The curated training dataset consists of approximately 80K samples, subsampled from multiple publicly available data sources, including\nHelpSteer2\nOffsetBias\nWildGuard (adversarial)\nMagpie DPO series: Ultra, Pro (Llama-3.1), Pro, Air.\nDisclaimer: We made no modifications to the original datasets listed above, other than subsampling the datasets to create the Skywork Reward Data Collection.\nDuring dataset curation, we adopt several tricks to achieve both performance improvement and a balance between each domain, without compromising the overall performance:\nWe select top samples from math, code, and other categories in the combined Magpie dataset independently, based on the average ArmoRM score provided with the dataset. We subtract the ArmoRM average scores in the Magpie-Air subset and the Magpie-Pro subset by 0.1 and 0.05, respectively, to prioritize Magpie-Ultra and Magpie-Pro-Llama-3.1 samples.\nInstead of including all preference pairs in WildGuard, we first train a reward model (RM) on three other data sources. We then (1) use this RM to score the chosen and rejected responses for all samples in WildGuard and (2) select only samples where the chosen response's RM score is greater than the rejected response's RM score. We observe that this approach largely preserves the original performance of Chat, Char hard, and Reasoning while improving Safety. For both models, we use the 27B model to score the WildGuard samples.\nRewardBench Leaderboard\nWe evaluate our model on RewardBench using the official test script. As of October 2024, Skywork-Reward-Llama-3.1-8B-v0.2 ranks first among 8B models on the RewardBench leaderboard.\nRank\nModel\nModel Type\nScore\nChat\nChat Hard\nSafety\nReasoning\n1\nSkywork/Skywork-Reward-Gemma-2-27B-v0.2\nSeq. Classifier\n94.3\n96.1\n89.9\n93.0\n98.1\n2\nnvidia/Llama-3.1-Nemotron-70B-Reward\nCustom Classifier\n94.1\n97.5\n85.7\n95.1\n98.1\n3\nSkywork/Skywork-Reward-Gemma-2-27B\nSeq. Classifier\n93.8\n95.8\n91.4\n91.9\n96.1\n4\nSF-Foundation/TextEval-Llama3.1-70B\nGenerative\n93.5\n94.1\n90.1\n93.2\n96.4\n5\nmeta-metrics/MetaMetrics-RM-v1.0\nCustom Classifier\n93.4\n98.3\n86.4\n90.8\n98.2\n6\nSkywork/Skywork-Critic-Llama-3.1-70B\nGenerative\n93.3\n96.6\n87.9\n93.1\n95.5\n7\nSkywork/Skywork-Reward-Llama-3.1-8B-v0.2\nSeq. Classifier\n93.1\n94.7\n88.4\n92.7\n96.7\n8\nnicolinho/QRM-Llama3.1-8B\nSeq. Classifier\n93.1\n94.4\n89.7\n92.3\n95.8\n9\nLxzGordon/URM-LLaMa-3.1-8B\nSeq. Classifier\n92.9\n95.5\n88.2\n91.1\n97.0\n10\nSalesforce/SFR-LLaMa-3.1-70B-Judge-r\nGenerative\n92.7\n96.9\n84.8\n91.6\n97.6\n11\nSkywork/Skywork-Reward-Llama-3.1-8B\nSeq. Classifier\n92.5\n95.8\n87.3\n90.8\n96.2\n12\ngeneral-preference/GPM-Llama-3.1-8B\nCustom Classifier\n92.2\n93.3\n88.6\n91.1\n96.0\nDemo Code\nWe provide example usage of the Skywork reward model series below. Please note that:\nTo enable optimal performance for the 27B reward model, ensure that you have enabled either the flash_attention_2 or eager implementation. The default spda implementation may result in bugs that could significantly degrade performance for this particular model.\nBelow is an example of obtaining the reward scores of two conversations.\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n# Load model and tokenizer\ndevice = \"cuda:0\"\nmodel_name = \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\"\nrm = AutoModelForSequenceClassification.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=device,\nattn_implementation=\"flash_attention_2\",\nnum_labels=1,\n)\nrm_tokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Jane has 12 apples. She gives 4 apples to her friend Mark, then buys 1 more apple, and finally splits all her apples equally among herself and her 2 siblings. How many apples does each person get?\"\nresponse1 = \"1. Jane starts with 12 apples and gives 4 to Mark. 12 - 4 = 8. Jane now has 8 apples.\\n2. Jane buys 1 more apple. 8 + 1 = 9. Jane now has 9 apples.\\n3. Jane splits the 9 apples equally among herself and her 2 siblings (3 people in total). 9 Ã· 3 = 3 apples each. Each person gets 3 apples.\"\nresponse2 = \"1. Jane starts with 12 apples and gives 4 to Mark. 12 - 4 = 8. Jane now has 8 apples.\\n2. Jane buys 1 more apple. 8 + 1 = 9. Jane now has 9 apples.\\n3. Jane splits the 9 apples equally among her 2 siblings (2 people in total). 9 Ã· 2 = 4.5 apples each. Each person gets 4 apples.\"\nconv1 = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response1}]\nconv2 = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response2}]\n# Format and tokenize the conversations\n# If you use `tokenize=False` with `apply_chat_template` and `tokenizer()` to tokenize the conversation,\n# remeber to remove the duplicated BOS token.\nconv1_tokenized = rm_tokenizer.apply_chat_template(conv1, tokenize=True, return_tensors=\"pt\").to(device)\nconv2_tokenized = rm_tokenizer.apply_chat_template(conv2, tokenize=True, return_tensors=\"pt\").to(device)\n# Get the reward scores\nwith torch.no_grad():\nscore1 = rm(conv1_tokenized).logits[0][0].item()\nscore2 = rm(conv2_tokenized).logits[0][0].item()\nprint(f\"Score for response 1: {score1}\")\nprint(f\"Score for response 2: {score2}\")\n# Output:\n# 27B:\n# Score for response 1: 0.5625\n# Score for response 2: -8.5\n# 8B:\n# Score for response 1: 13.6875\n# Score for response 2: -9.1875\nDeclaration and License Agreement\nDeclaration\nWe hereby declare that the Skywork model should not be used for any activities that pose a threat to national or societal security or engage in unlawful actions. Additionally, we request users not to deploy the Skywork model for internet services without appropriate security reviews and records. We hope that all users will adhere to this principle to ensure that technological advancements occur in a regulated and lawful environment.\nWe have done our utmost to ensure the compliance of the data used during the model's training process. However, despite our extensive efforts, due to the complexity of the model and data, there may still be unpredictable risks and issues. Therefore, if any problems arise as a result of using the Skywork open-source model, including but not limited to data security issues, public opinion risks, or any risks and problems arising from the model being misled, abused, disseminated, or improperly utilized, we will not assume any responsibility.\nLicense Agreement\nThe community usage of Skywork model requires Skywork Community License. The Skywork model supports commercial use. If you plan to use the Skywork model or its derivatives for commercial purposes, you must abide by terms and conditions within Skywork Community License.\nTechnical Report\nSkywork-Reward: Bag of Tricks for Reward Modeling in LLMs\nContact\nIf you have any questions, please feel free to reach us at yuhao.liuu@kunlun-inc.com or liang.zeng@kunlun-inc.com.\nCitation\nIf you find our work helpful, please feel free to cite us using the following BibTeX entry:\n@article{liu2024skywork,\ntitle={Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs},\nauthor={Liu, Chris Yuhao and Zeng, Liang and Liu, Jiacai and Yan, Rui and He, Jujie and Wang, Chaojie and Yan, Shuicheng and Liu, Yang and Zhou, Yahui},\njournal={arXiv preprint arXiv:2410.18451},\nyear={2024}\n}",
    "SWivid/E2-TTS": "Download F5-TTS or E2 TTS and place under ckpts/\nckpts/\nF5TTS_v1_Base/\nmodel_1250000.safetensors\nF5TTS_Base/\nmodel_1200000.safetensors\nE2TTS_Base/\nmodel_1200000.safetensors\nGithub: https://github.com/SWivid/F5-TTSPaper: E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS",
    "mistralai/Ministral-8B-Instruct-2410": "Model Card for Ministral-8B-Instruct-2410\nMinistral 8B Key features\nBasic Instruct Template (V3-Tekken)\nMinistral 8B Architecture\nBenchmarks\nInstruct Models\nUsage Examples\nvLLM (recommended)\nMistral-inference\nChat\nPasskey detection\nInstruct following\nFunction calling\nThe Mistral AI Team\nModel Card for Ministral-8B-Instruct-2410\nWe introduce two new state-of-the-art models for local intelligence, on-device computing, and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.\nThe Ministral-8B-Instruct-2410 Language Model is an instruct fine-tuned model significantly outperforming existing models of similar size, released under the Mistral Research License.\nIf you are interested in using Ministral-3B or Ministral-8B commercially, outperforming Mistral-7B, reach out to us.\nFor more details about les Ministraux please refer to our release blog post.\nMinistral 8B Key features\nReleased under the Mistral Research License, reach out to us for a commercial license\nTrained with a 128k context window with interleaved sliding-window attention\nTrained on a large proportion of multilingual and code data\nSupports function calling\nVocabulary size of 131k, using the V3-Tekken tokenizer\nBasic Instruct Template (V3-Tekken)\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\nFor more information about the tokenizer please refer to mistral-common\nMinistral 8B Architecture\nFeature\nValue\nArchitecture\nDense Transformer\nParameters\n8,019,808,256\nLayers\n36\nHeads\n32\nDim\n4096\nKV Heads (GQA)\n8\nHidden Dim\n12288\nHead Dim\n128\nVocab Size\n131,072\nContext Length\n128k\nAttention Pattern\nRagged (128k,32k,32k,32k)\nBenchmarks\nBase Models\nKnowledge & Commonsense\nModel\nMMLU\nAGIEval\nWinogrande\nArc-c\nTriviaQA\nMistral 7B Base\n62.5\n42.5\n74.2\n67.9\n62.5\nLlama 3.1 8B Base\n64.7\n44.4\n74.6\n46.0\n60.2\nMinistral 8B Base\n65.0\n48.3\n75.3\n71.9\n65.5\nGemma 2 2B Base\n52.4\n33.8\n68.7\n42.6\n47.8\nLlama 3.2 3B Base\n56.2\n37.4\n59.6\n43.1\n50.7\nMinistral 3B Base\n60.9\n42.1\n72.7\n64.2\n56.7\nCode & Math\nModel\nHumanEval pass@1\nGSM8K maj@8\nMistral 7B Base\n26.8\n32.0\nLlama 3.1 8B Base\n37.8\n42.2\nMinistral 8B Base\n34.8\n64.5\nGemma 2 2B\n20.1\n35.5\nLlama 3.2 3B\n14.6\n33.5\nMinistral 3B\n34.2\n50.9\nMultilingual\nModel\nFrench MMLU\nGerman MMLU\nSpanish MMLU\nMistral 7B Base\n50.6\n49.6\n51.4\nLlama 3.1 8B Base\n50.8\n52.8\n54.6\nMinistral 8B Base\n57.5\n57.4\n59.6\nGemma 2 2B Base\n41.0\n40.1\n41.7\nLlama 3.2 3B Base\n42.3\n42.2\n43.1\nMinistral 3B Base\n49.1\n48.3\n49.5\nInstruct Models\nChat/Arena (gpt-4o judge)\nModel\nMTBench\nArena Hard\nWild bench\nMistral 7B Instruct v0.3\n6.7\n44.3\n33.1\nLlama 3.1 8B Instruct\n7.5\n62.4\n37.0\nGemma 2 9B Instruct\n7.6\n68.7\n43.8\nMinistral 8B Instruct\n8.3\n70.9\n41.3\nGemma 2 2B Instruct\n7.5\n51.7\n32.5\nLlama 3.2 3B Instruct\n7.2\n46.0\n27.2\nMinistral 3B Instruct\n8.1\n64.3\n36.3\nCode & Math\nModel\nMBPP pass@1\nHumanEval pass@1\nMath maj@1\nMistral 7B Instruct v0.3\n50.2\n38.4\n13.2\nGemma 2 9B Instruct\n68.5\n67.7\n47.4\nLlama 3.1 8B Instruct\n69.7\n67.1\n49.3\nMinistral 8B Instruct\n70.0\n76.8\n54.5\nGemma 2 2B Instruct\n54.5\n42.7\n22.8\nLlama 3.2 3B Instruct\n64.6\n61.0\n38.4\nMinistral 3B Instruct\n67.7\n77.4\n51.7\nFunction calling\nModel\nInternal bench\nMistral 7B Instruct v0.3\n6.9\nLlama 3.1 8B Instruct\nN/A\nGemma 2 9B Instruct\nN/A\nMinistral 8B Instruct\n31.6\nGemma 2 2B Instruct\nN/A\nLlama 3.2 3B Instruct\nN/A\nMinistral 3B Instruct\n28.4\nUsage Examples\nvLLM (recommended)\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nCurrently vLLM is capped at 32k context size because interleaved attention kernels for paged attention are not yet implemented in vLLM.\nAttention kernels for paged attention are being worked on and as soon as it is fully supported in vLLM, this model card will be updated.\nTo take advantage of the full 128k context size we recommend Mistral Inference\nInstallation\nMake sure you install vLLM >= v0.6.4:\npip install --upgrade vllm\nAlso make sure you have mistral_common >= 1.4.4 installed:\npip install --upgrade mistral_common\nYou can also make use of a ready-to-go docker image.\nOffline\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n# note that running Ministral 8B on a single GPU requires 24 GB of GPU RAM\n# If you want to divide the GPU requirement over multiple devices, please add *e.g.* `tensor_parallel=2`\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\nprompt = \"Do we need to think for 10 seconds to find the answer of 1 + 1?\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": prompt\n},\n]\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n# You don't need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n# and you can easily add these two numbers in your mind very quickly without any delay.\nServer\nYou can also use Ministral-8B in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\nNote: Running Ministral-8B on a single GPU requires 24 GB of GPU RAM.\nIf you want to divide the GPU requirement over multiple devices, please add e.g. --tensor_parallel=2\nAnd ping the client:\ncurl --location 'http://<your-node-url>:8000/v1/chat/completions' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer token' \\\n--data '{\n\"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"Do we need to think for 10 seconds to find the answer of 1 + 1?\"\n}\n]\n}'\nMistral-inference\nWe recommend using mistral-inference to quickly try out / \"vibe-check\" the model.\nInstall\nMake sure to have mistral_inference >= 1.5.0 installed.\npip install mistral_inference --upgrade\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', '8B-Instruct')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Ministral-8B-Instruct-2410\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\nChat\nAfter installing mistral_inference, a mistral-chat CLI command should be available in your environment. You can chat with the model using\nmistral-chat $HOME/mistral_models/8B-Instruct --instruct --max_tokens 256\nPasskey detection\nIn this example the passkey message has over >100k tokens and mistral-inference\ndoes not have a chunked pre-fill mechanism. Therefore you will need a lot of\nGPU memory in order to run the below example (80 GB). For a more memory-efficient\nsolution we recommend using vLLM.\nfrom mistral_inference.transformer import Transformer\nfrom pathlib import Path\nimport json\nfrom mistral_inference.generate import generate\nfrom huggingface_hub import hf_hub_download\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ndef load_passkey_request() -> ChatCompletionRequest:\npasskey_file = hf_hub_download(repo_id=\"mistralai/Ministral-8B-Instruct-2410\", filename=\"passkey_example.json\")\nwith open(passkey_file, \"r\") as f:\ndata = json.load(f)\nmessage_content = data[\"messages\"][0][\"content\"]\nreturn ChatCompletionRequest(messages=[UserMessage(content=message_content)])\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path, softmax_fp32=False)\ncompletion_request = load_passkey_request()\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\nprint(result)  # The pass key is 13005.\nInstruct following\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"How often does the letter r occur in Mistral?\")])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\nprint(result)\nFunction calling\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.tekken import SpecialTokenPolicy\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\ntekken = tokenizer.instruct_tokenizer.tokenizer\ntekken.special_token_policy = SpecialTokenPolicy.IGNORE\nmodel = Transformer.from_folder(mistral_models_path)\ncompletion_request = ChatCompletionRequest(\ntools=[\nTool(\nfunction=Function(\nname=\"get_current_weather\",\ndescription=\"Get the current weather\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\",\n},\n\"format\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The temperature unit to use. Infer this from the users location.\",\n},\n},\n\"required\": [\"location\", \"format\"],\n},\n)\n)\n],\nmessages=[\nUserMessage(content=\"What's the weather like today in Paris?\"),\n],\n)\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\nprint(result)\nThe Mistral AI Team\nAlbert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, AmÃ©lie HÃ©liou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste RoziÃ¨re, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, ElÃ©onore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, MickaÃ«l Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia RozÃ©, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "RepresentLM/RepresentLM-v1": "RepresentLM-v1\nUsage\nRepresentLM-v1\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nThe model is trained on the HEADLINES semantic similarity dataset, using the StoriesLM-v1-1963 model as a base.\nUsage\nFirst install the sentence-transformers package:\npip install -U sentence-transformers\nThe model can then be used to encode language sequences:\nfrom sentence_transformers import SentenceTransformer\nsequences = [\"This is an example sequence\", \"Each sequence is embedded\"]\nmodel = SentenceTransformer('RepresentLM/RepresentLM-v1')\nembeddings = model.encode(sequences)\nprint(embeddings)",
    "aixonlab/FLUX.1-dev-LoRA-Cinematic-Octane": "Cinematic-Octane\nTrigger words\nInference\nAcknowledgements\nCinematic-Octane\nCinematic-Octane is a LoRA model meticulously fine-tuned to embody the warm, dreamy color grading prevalent in contemporary cinema. This model captures the essence of modern filmmaking with its lush, saturated hues and soft contrasts, reminiscent of films that evoke a sense of nostalgia and fantasy. It excels in enhancing the visual depth of scenes, bringing out the vibrant interplay of colors that define today's cinematic aesthetics.\nThe model's color palette is characterized by rich, golden tones with greenish hue backgrounds and gentle transitions, which are particularly effective in creating a warm, inviting atmosphere. This is achieved through a careful balance of highlights and shadows, where warm colors like orange and yellow dominate while the shadows are more in blue-green tones, providing a sense of comfort and intimacy. The dreamy effect is further accentuated by subtle halation around highlights and a slight grain, adding texture and depth to the visuals.\n.\nTrigger words\nYou should use cinematic_octane to trigger the image generation.\nInference\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\npipe.load_lora_weights('aixonlab/FLUX.1-dev-LoRA-Cinematic-Octane', weight_name='cinematic-octane.safetensors')\npipe.fuse_lora(lora_scale=0.8)\npipe.to(\"cuda\")\nprompt = \"a gorgeous woman and a man, cinematic_octane\"\nimage = pipe(prompt,\nnum_inference_steps=24,\nguidance_scale=3.0,\nwidth=768, height=1024,\n).images[0]\nimage.save(f\"example.png\")\nAcknowledgements\nThis model is trained by AIXON Lab. We release this model under permissions. The model follows flux-1-dev-non-commercial-license.",
    "parler-tts/parler-tts-mini-v1.1": "Parler-TTS Mini v1.1\nðŸ“– Quick Index\nðŸ› ï¸ Usage\nðŸ‘¨â€ðŸ’» Installation\nðŸŽ² Random voice\nðŸŽ¯ Using a specific speaker\nMotivation\nCitation\nLicense\nParler-TTS Mini v1.1\nParler-TTS Mini v1.1 is a lightweight text-to-speech (TTS) model, trained on 45K hours of audio data, that can generate high-quality, natural sounding speech with features that can be controlled using a simple text prompt (e.g. gender, background noise, speaking rate, pitch and reverberation).\nðŸš¨ Parler-TTS Mini v1.1  is the exact same model than Mini v1. It was trained on the same datasets and with the same training configuration. The only change is the use of a better prompt tokenizer. This tokenizer has a larger vocabulary and handles byte fallback, which simplifies multilingual training. It's based on unsloth/llama-2-7b tokenizer. Thanks to the AI4Bharat team who provided advice and assistance in improving tokenization. ðŸš¨\nðŸ“– Quick Index\nðŸ‘¨â€ðŸ’» Installation\nðŸŽ² Using a random voice\nðŸŽ¯ Using a specific speaker\nMotivation\nOptimizing inference\nðŸ› ï¸ Usage\nðŸš¨Unlike previous versions of Parler-TTS, here we use two tokenizers - one for the prompt and one for the description.ðŸš¨\nðŸ‘¨â€ðŸ’» Installation\nUsing Parler-TTS is as simple as \"bonjour\". Simply install the library once:\npip install git+https://github.com/huggingface/parler-tts.git\nðŸŽ² Random voice\nParler-TTS has been trained to generate speech with features that can be controlled with a simple text prompt, for example:\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1.1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1.1\")\ndescription_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\nprompt = \"Hey, how are you doing today?\"\ndescription = \"A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\ninput_ids = description_tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\ngeneration = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\nðŸŽ¯ Using a specific speaker\nTo ensure speaker consistency across generations, this checkpoint was also trained on 34 speakers, characterized by name (e.g. Jon, Lea, Gary, Jenna, Mike, Laura).\nTo take advantage of this, simply adapt your text description to specify which speaker to use: Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1.1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1.1\")\ndescription_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\nprompt = \"Hey, how are you doing today?\"\ndescription = \"Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\ninput_ids = description_tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\ngeneration = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\nTips:\nWe've set up an inference guide to make generation faster. Think SDPA, torch.compile, batching and streaming!\nInclude the term \"very clear audio\" to generate the highest quality audio, and \"very noisy audio\" for high levels of background noise\nPunctuation can be used to control the prosody of the generations, e.g. use commas to add small breaks in speech\nThe remaining speech features (gender, speaking rate, pitch and reverberation) can be controlled directly through the prompt\nMotivation\nParler-TTS is a reproduction of work from the paper Natural language guidance of high-fidelity text-to-speech with synthetic annotations by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.\nContrarily to other TTS models, Parler-TTS is a fully open-source release. All of the datasets, pre-processing, training code and weights are released publicly under permissive license, enabling the community to build on our work and develop their own powerful TTS models.\nParler-TTS was released alongside:\nThe Parler-TTS repository - you can train and fine-tuned your own version of the model.\nThe Data-Speech repository - a suite of utility scripts designed to annotate speech datasets.\nThe Parler-TTS organization - where you can find the annotated datasets as well as the future checkpoints.\nCitation\nIf you found this repository useful, please consider citing this work and also the original Stability AI paper:\n@misc{lacombe-etal-2024-parler-tts,\nauthor = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},\ntitle = {Parler-TTS},\nyear = {2024},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/huggingface/parler-tts}}\n}\n@misc{lyth2024natural,\ntitle={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},\nauthor={Dan Lyth and Simon King},\nyear={2024},\neprint={2402.01912},\narchivePrefix={arXiv},\nprimaryClass={cs.SD}\n}\nLicense\nThis model is permissively licensed under the Apache 2.0 license."
}