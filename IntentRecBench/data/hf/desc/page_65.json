{
    "Teddy487/LLaMA2-7b-for-OpenIE": "Overview\nGetting Started\nModel Output Format\nHow to Use\nModel Performance\nOverview\nThe model is a LoRa Adaptor based on Llama-2-7b-chat-hf. The model has been trained on a re-annotated version of the CaRB dataset.\nThe model produces multi-valent Open IE tuples, i.e. relations with various numbers of arguments (1, 2, or more). We provide an example below:\nConsider the following sentence (taken from the CaRB dev set):\nEarlier this year , President Bush made a final `` take - it - or - leave it '' offer on the minimum wage\nOur model would extract the following relation from the sentence:\n<President Bush, made, a final \"take-it-or-leave-it\" offer, on the minimum wage, earlier this year>\nwhere we include President Bush as the subject, made as the object, a final \"take-it-or-leave-it\" offer as thedirect object, and on the minimum wage and earlier this year> as salient complements.\nWe briefly describe how to use our model in the below, and provide further details in our MulVOIEL repository on Github\nGetting Started\nModel Output Format\nGiven a sentence, the model produces textual predictions in the following format:\n<subj> ,, (<auxi> ###) <predicate> ,, (<prep1> ###) <obj1>, (<prep2> ###) <obj2>, ...\nHow to Use\nInstall the relevant libraries as well as the MulVOIEL package:\npip install transformers datasets peft torch\ngit clone https://github.com/Teddy-Li/MulVOIEL\ncd MulVOIEL\nLoad the model and perform inference (example):\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nfrom llamaOIE import parse_outstr_to_triples\nfrom llamaOIE_dataset import prepare_input\nbase_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\npeft_adapter_name = \"Teddy487/LLaMA2-7b-for-OpenIE\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(model, peft_adapter_name)\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ninput_text = \"Earlier this year , President Bush made a final `` take - it - or - leave it '' offer on the minimum wage\"\ninput_text, _ = prepare_input({'s': input_text}, tokenizer, has_labels=False)\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\noutstr = tokenizer.decode(outputs[0][len(input_ids):], skip_special_tokens=True)\ntriples = parse_outstr_to_triples(outstr)\nfor tpl in triples:\nprint(tpl)\nüç∫\nModel Performance\nThe primary benefit of our model is the ability to extract finer-grained information for predicates. On the other hand, we also report performance on a roughly comparable basis with prior SOTA open IE models, where our method is comparable and even superior to prior models, while producing finer-grained and more complex outputs. We report evaluation results in (macro) F-1 metric, as well as in the average Levenshtein Distance between gold and predicted relations:\nModel\nLevenshtein Distance\nMacro F-1\nLoRA LLaMA2-7b\n5.85\n50.2\nLoRA LLaMA3-8b\n5.04\n55.3\nRNN OIE *\n-\n49.0\nIMOJIE *\n-\n53.5\nOpen IE 6 *\n-\n54.0/52.7\nNote that the precision and recall values are not directly comparable, because we evaluate the model prediction at a finer granularity, and we use different train/dev/test arrangements as the original CaRB dataset, hence the asterisk.",
    "PixArt-alpha/PixArt-alpha": "These weights are for the original Github repository and it's only for research purpose. For other purposes, please check https://huggingface.co/docs/diffusers/main/en/api/pipelines/pixart and https://huggingface.co/PixArt-alpha/PixArt-XL-2-1024-MS.\nInference Code: https://github.com/PixArt-alpha/PixArt-alpha\nPaper: https://arxiv.org/abs/2310.00426",
    "TheBloke/Leo-Mistral-Hessianai-7B-Chat-GGUF": "Leo Mistral Hessianai 7B Chat - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: ChatML\nCompatibility\nExplanation of quantisation methods\nProvided files\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using ctransformers\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nOriginal model card: LAION LeoLM's Leo Mistral Hessianai 7B Chat\nLAION LeoLM: Linguistically Enhanced Open Language Model\nLeoLM Chat\nModel Details\nUse in ü§óTransformers\nPrompting / Prompt Template\nEthical Considerations and Limitations\nFinetuning Details\nDataset Details\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nLeo Mistral Hessianai 7B Chat - GGUF\nModel creator: LAION LeoLM\nOriginal model: Leo Mistral Hessianai 7B Chat\nDescription\nThis repo contains GGUF format model files for LAION LeoLM's Leo Mistral Hessianai 7B Chat.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplate list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nLAION LeoLM's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: ChatML\n<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\nleo-mistral-hessianai-7b-chat.Q2_K.gguf\nQ2_K\n2\n3.08 GB\n5.58 GB\nsmallest, significant quality loss - not recommended for most purposes\nleo-mistral-hessianai-7b-chat.Q3_K_S.gguf\nQ3_K_S\n3\n3.17 GB\n5.67 GB\nvery small, high quality loss\nleo-mistral-hessianai-7b-chat.Q3_K_M.gguf\nQ3_K_M\n3\n3.52 GB\n6.02 GB\nvery small, high quality loss\nleo-mistral-hessianai-7b-chat.Q3_K_L.gguf\nQ3_K_L\n3\n3.82 GB\n6.32 GB\nsmall, substantial quality loss\nleo-mistral-hessianai-7b-chat.Q4_0.gguf\nQ4_0\n4\n4.11 GB\n6.61 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\nleo-mistral-hessianai-7b-chat.Q4_K_S.gguf\nQ4_K_S\n4\n4.14 GB\n6.64 GB\nsmall, greater quality loss\nleo-mistral-hessianai-7b-chat.Q4_K_M.gguf\nQ4_K_M\n4\n4.37 GB\n6.87 GB\nmedium, balanced quality - recommended\nleo-mistral-hessianai-7b-chat.Q5_0.gguf\nQ5_0\n5\n5.00 GB\n7.50 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\nleo-mistral-hessianai-7b-chat.Q5_K_S.gguf\nQ5_K_S\n5\n5.00 GB\n7.50 GB\nlarge, low quality loss - recommended\nleo-mistral-hessianai-7b-chat.Q5_K_M.gguf\nQ5_K_M\n5\n5.13 GB\n7.63 GB\nlarge, very low quality loss - recommended\nleo-mistral-hessianai-7b-chat.Q6_K.gguf\nQ6_K\n6\n5.94 GB\n8.44 GB\nvery large, extremely low quality loss\nleo-mistral-hessianai-7b-chat.Q8_0.gguf\nQ8_0\n8\n7.70 GB\n10.20 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/Leo-Mistral-Hessianai-7B-Chat-GGUF and below it, a specific filename to download, such as: leo-mistral-hessianai-7b-chat.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/Leo-Mistral-Hessianai-7B-Chat-GGUF leo-mistral-hessianai-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/Leo-Mistral-Hessianai-7B-Chat-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Leo-Mistral-Hessianai-7B-Chat-GGUF leo-mistral-hessianai-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 32 -m leo-mistral-hessianai-7b-chat.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 2048 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions here: text-generation-webui/docs/llama.cpp.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries.\nHow to load this model in Python code, using ctransformers\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\nSimple ctransformers example code\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Leo-Mistral-Hessianai-7B-Chat-GGUF\", model_file=\"leo-mistral-hessianai-7b-chat.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\nprint(llm(\"AI is going to\"))\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Pierre Kircher, Stanislav Ovsiannikov, Michael Levine, Eugene Pentland, Andrey, Ï§ÄÍµê ÍπÄ, Randy H, Fred von Graf, Artur Olbinski, Caitlyn Gatomon, terasurfer, Jeff Scroggin, James Bentley, Vadim, Gabriel Puliatti, Harry Royden McLaughlin, Sean Connelly, Dan Guido, Edmond Seymore, Alicia Loh, subjectnull, AzureBlack, Manuel Alberto Morcote, Thomas Belote, Lone Striker, Chris Smitley, Vitor Caleffi, Johann-Peter Hartmann, Clay Pascal, biorpg, Brandon Frisco, sidney chen, transmissions 11, Pedro Madruga, jinyuan sun, Ajan Kanaga, Emad Mostaque, Trenton Dambrowitz, Jonathan Leane, Iucharbius, usrbinkat, vamX, George Stoitzev, Luke Pendergrass, theTransient, Olakabola, Swaroop Kallakuri, Cap'n Zoog, Brandon Phillips, Michael Dempsey, Nikolai Manek, danny, Matthew Berman, Gabriel Tamborski, alfie_i, Raymond Fosdick, Tom X Nguyen, Raven Klaugh, LangChain4j, Magnesian, Illia Dulskyi, David Ziegler, Mano Prime, Luis Javier Navarrete Lozano, Erik Bj√§reholt, ÈòøÊòé, Nathan Dryer, Alex, Rainer Wilmers, zynix, TL, Joseph William Delisle, John Villwock, Nathan LeClaire, Willem Michiel, Joguhyik, GodLy, OG, Alps Aficionado, Jeffrey Morgan, ReadyPlayerEmma, Tiffany J. Kim, Sebastain Graf, Spencer Kim, Michael Davis, webtim, Talal Aujan, knownsqashed, John Detwiler, Imad Khwaja, Deo Leter, Jerry Meng, Elijah Stavena, Rooh Singh, Pieter, SuperWojo, Alexandros Triantafyllidis, Stephen Murray, Ai Maven, ya boyyy, Enrico Ros, Ken Nordquist, Deep Realms, Nicholas, Spiking Neurons AB, Elle, Will Dee, Jack West, RoA, Luke @flexchar, Viktor Bowallius, Derek Yates, Subspace Studios, jjj, Toran Billups, Asp the Wyvern, Fen Risland, Ilya, NimbleBox.ai, Chadd, Nitin Borwankar, Emre, Mandus, Leonard Tan, Kalila, K, Trailburnt, S_X, Cory Kujawski\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: LAION LeoLM's Leo Mistral Hessianai 7B Chat\nLAION LeoLM: Linguistically Enhanced Open Language Model\nMeet LeoLM, the first open and commercially available German Foundation Language Model built on Llama-2 and Mistral.\nOur models extend Llama-2's capabilities into German through continued pretraining on a large corpus of German-language and mostly locality specific text.\nThanks to a compute grant at HessianAI's new supercomputer 42, we release three foundation models trained with 8k context length.\nLeoLM/leo-mistral-hessianai-7b under Apache 2.0 and LeoLM/leo-hessianai-7b and LeoLM/leo-hessianai-13b under the Llama-2 community license (70b also coming soon! üëÄ).\nWith this release, we hope to bring a new wave of opportunities to German open-source and commercial LLM research and accelerate adoption.\nRead our blog post or our paper (preprint coming soon) for more details!\nA project by Bj√∂rn Pl√ºster and Christoph Schuhmann in collaboration with LAION and HessianAI.\nLeoLM Chat\nLeoLM/leo-mistral-hessianai-7b-chat is a German chat model built on our foundation model LeoLM/leo-mistral-hessianai-7b and finetuned on a selection of German instruction datasets.\nThe model performs exceptionally well on writing, explanation and discussion tasks but struggles somewhat with math and advanced reasoning. See our MT-Bench-DE scores:\n{\n\"first_turn\": 6.1,\n\"second_turn\": 4.7,\n\"categories\": {\n\"writing\": 6.8,\n\"roleplay\": 6.35,\n\"reasoning\": 3.3,\n\"math\": 2.75,\n\"coding\": 4.4,\n\"extraction\": 4.5,\n\"stem\": 6.85,\n\"humanities\": 8.25\n},\n\"average\": 5.4\n}\nModel Details\nFinetuned from: LeoLM/leo-mistral-hessianai-7b\nModel type: Causal decoder-only transformer language model\nLanguage: English and German\nDemo: Web Demo coming soon !\nLicense: Apache 2.0\nContact: LAION Discord or Bj√∂rn Pl√ºster\nUse in ü§óTransformers\nFirst install direct dependencies:\npip install transformers torch sentencepiece\nIf you want faster inference using flash-attention2, you need to install these dependencies:\npip install packaging ninja\npip install flash-attn\nThen load the model in transformers:\nfrom transformers import pipeline\nimport torch\nsystem_prompt = \"\"\"<|im_start|>system\nDies ist eine Unterhaltung zwischen einem intelligenten, hilfsbereitem KI-Assistenten und einem Nutzer.\nDer Assistent gibt ausf√ºhrliche, hilfreiche und ehrliche Antworten.<|im_end|>\n\"\"\"\nprompt_format = \"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\nprompt = \"Erkl√§re mir wie die Fahrradwegesituation in Hamburg ist.\"\ngenerator = pipeline(model=\"LeoLM/leo-mistral-hessianai-7b-chat\", device=\"cuda\", torch_dtype=torch.float16, use_flash_attention_2=True) # True for flash-attn2 else False\nprint(generator(prompt_format.format(prompt=prompt), do_sample=True, top_p=0.95, max_length=8192))\n\"Als KI kann ich keine pers√∂nlichen Beobachtungen teilen, aber ich kann einige allgemeine Informationen zur Fahrradwegesituation in Hamburg liefern. Im Vergleich zu vielen anderen gro√üen St√§dten hat Hamburg eine hohe Anzahl von Fahrradfahrern und nimmt seine Verantwortung f√ºr nachhaltige Verkehrsmittel sehr ernst. Es gibt viele Fahrradwege und separate Fahrspuren, die Radfahrern erm√∂glichen, zusammen mit dem Autoverkehr zu fahren. Diese Fahrradspuren sind oft mit Markierungen gekennzeichnet und durch physische Trennungen von anderen Fahrspuren abgegrenzt. Dar√ºber hinaus gibt es viele Fahrradstra√üen, auf denen Radfahrer Vorfahrt haben und Autos langsamer fahren m√ºssen.\nIn einigen st√§dtischen Gebieten k√∂nnen Fahrradwege jedoch eng oder √ºberf√ºllt sein, besonders w√§hrend der Sto√üzeiten. Es gibt auch viele Kreuzungen, an denen Radfahrer anhalten und auf Gr√ºn warten m√ºssen, √§hnlich wie Autofahrer. Insgesamt ist die Fahrradinfrastruktur in Hamburg ziemlich gut, aber wie √ºberall gibt es immer Raum f√ºr Verbesserungen.\"\nPrompting / Prompt Template\nPrompt dialogue template (ChatML format):\n\"\"\"\n<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n\"\"\"\nThe model input can contain multiple conversation turns between user and assistant, e.g.\n<|im_start|>user\n{prompt 1}<|im_end|>\n<|im_start|>assistant\n{reply 1}<|im_end|>\n<|im_start|>user\n{prompt 2}<|im_end|>\n<|im_start|>assistant\n(...)\nEthical Considerations and Limitations\nLeoLM has been tested in English and German, and has not covered, nor could it cover all scenarios.\nFor these reasons, as with all LLMs, the potential outputs of LeoLM/leo-mistral-hessianai-7b-chat cannot be predicted\nin advance, and the model may in some instances produce inaccurate, biased or other objectionable responses\nto user prompts. Therefore, before deploying any applications of LeoLM/leo-mistral-hessianai-7b-chat, developers should\nperform safety testing and tuning tailored to their specific applications of the model.\nPlease see Meta's Responsible Use Guide.\nFinetuning Details\nHyperparameter\nValue\nNum epochs\n4\nExamples per epoch\n131214\nGlobal batch size\n256\nLearning rate\n1e-5\nWarmup steps\n100\nLR scheduler\nCosine\nAdam betas\n(0.9, 0.95)\nDataset Details\n## Stats for 'Subset of OpenAssistant/OASST-DE' (3534 samples (100.0%))\n-----------------\nAccepted: 3534/3534 (100.0%)\nAccepted tokens: 2259302\nSkipped: 0 (0.0%)\nMin tokens per sample: 29\nMax tokens per sample: 2484\nAvg tokens per sample: 639.3044708545557\n-----------------\n## Stats for 'Subset of FreedomIntelligence/evol-instruct-deutsch' (57841 samples (100.0%))\n-----------------\nAccepted: 57841/57841 (100.0%)\nAccepted tokens: 42958192\nSkipped: 0 (0.0%)\nMin tokens per sample: 33\nMax tokens per sample: 5507\nAvg tokens per sample: 742.6944900675991\n-----------------\n## Stats for 'Subset of FreedomIntelligence/alpaca-gpt4-deutsch' (48969 samples (100.0%))\n-----------------\nAccepted: 48969/48969 (100.0%)\nAccepted tokens: 13372005\nSkipped: 0 (0.0%)\nMin tokens per sample: 19\nMax tokens per sample: 1359\nAvg tokens per sample: 273.07082031489307\n-----------------\n## Stats for 'Subset of LeoLM/OpenSchnabeltier' (21314 samples (100.0%))\n-----------------\nAccepted: 21314/21314 (100.0%)\nAccepted tokens: 8134690\nSkipped: 0 (0.0%)\nMin tokens per sample: 25\nMax tokens per sample: 1202\nAvg tokens per sample: 381.65947264708643\n-----------------\n## Stats for 'Subset of LeoLM/German_Poems' (490 samples (100.0%))\n-----------------\nAccepted: 490/490 (100.0%)\nAccepted tokens: 618642\nSkipped: 0 (0.0%)\nMin tokens per sample: 747\nMax tokens per sample: 1678\nAvg tokens per sample: 1262.534693877551\n-----------------\n## Stats for 'Subset of LeoLM/German_Songs' (392 samples (100.0%))\n-----------------\nAccepted: 392/392 (100.0%)\nAccepted tokens: 187897\nSkipped: 0 (0.0%)\nMin tokens per sample: 231\nMax tokens per sample: 826\nAvg tokens per sample: 479.3290816326531\n-----------------\n## Stats for 'total' (132540 samples (100.0%))\n-----------------\nAccepted: 132540/132540 (100.0%)\nAccepted tokens: 67530728\nSkipped: 0 (0.0%)\nMin tokens per sample: 19\nMax tokens per sample: 5507\nAvg tokens per sample: 509.51205673758864\n-----------------",
    "llm-jp/llm-jp-13b-v1.0": "llm-jp-13b-v1.0\nRequired Libraries and Their Versions\nUsage\nModel Details\nTraining\nTokenizer\nDatasets\nPre-training\nInstruction tuning\nEvaluation\nRisks and Limitations\nSend Questions to\nLicense\nModel Card Authors\nllm-jp-13b-v1.0\nThis repository provides large language models developed by LLM-jp, a collaborative project launched in Japan.\nModel Variant\nInstruction models\nllm-jp-13b-instruct-full-jaster-v1.0\nllm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\nllm-jp-13b-instruct-full-dolly-oasst-v1.0\nllm-jp-13b-instruct-lora-jaster-v1.0\nllm-jp-13b-instruct-lora-jaster-dolly-oasst-v1.0\nllm-jp-13b-instruct-lora-dolly-oasst-v1.0\nPre-trained models\nllm-jp-13b-v1.0\nllm-jp-1.3b-v1.0\nCheckpoints format: Hugging Face Transformers (Megatron-DeepSpeed format models are available here)\nRequired Libraries and Their Versions\ntorch>=2.0.0\ntransformers>=4.34.0\ntokenizers>=0.14.0\naccelerate==0.23.0\nUsage\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-13b-v1.0\")\nmodel = AutoModelForCausalLM.from_pretrained(\"llm-jp/llm-jp-13b-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)\ntext = \"Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„Å®„ÅØ‰Ωï„Åã\"\ntokenized_input = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\noutput = model.generate(\ntokenized_input,\nmax_new_tokens=100,\ndo_sample=True,\ntop_p=0.95,\ntemperature=0.7,\n)[0]\nprint(tokenizer.decode(output))\nModel Details\nModel type: Transformer-based Language Model\nTotal seen tokens: 300B\nModel\nParams\nLayers\nHidden size\nHeads\nContext length\n13b model\n13b\n40\n5120\n40\n2048\n1.3b model\n1.3b\n24\n2048\n16\n2048\nTraining\nPre-training:\nHardware: 96 A100 40GB GPUs (mdx cluster)\nSoftware: Megatron-DeepSpeed\nInstruction tuning:\nHardware: 8 A100 40GB GPUs (mdx cluster)\nSoftware: TRL, PEFT, and DeepSpeed\nTokenizer\nThe tokenizer of this model is based on huggingface/tokenizers Unigram byte-fallback model.\nThe vocabulary entries were converted from llm-jp-tokenizer v2.1 (50k).\nPlease refer to README.md of llm-ja-tokenizer for details on the vocabulary construction procedure.\nModel: Hugging Face Fast Tokenizer using Unigram byte-fallback model which requires tokenizers>=0.14.0\nTraining algorithm: SentencePiece Unigram byte-fallback\nTraining data: A subset of the datasets for model pre-training\nVocabulary size: 50,570 (mixed vocabulary of Japanese, English, and source code)\nDatasets\nPre-training\nThe models have been pre-trained using a blend of the following datasets.\nLanguage\nDataset\nTokens\nJapanese\nWikipedia\n1.5B\nmC4\n136B\nEnglish\nWikipedia\n5B\nThe Pile\n135B\nCodes\nThe Stack\n10B\nThe pre-training was continuously conducted using a total of 10 folds of non-overlapping data, each consisting of approximately 27-28B tokens.\nWe finalized the pre-training with additional (potentially) high-quality 27B tokens data obtained from the identical source datasets listed above used for the 10-fold data.\nInstruction tuning\nThe models have been fine-tuned on the following datasets.\nLanguage\nDataset\ndescription\nJapanese\njaster\nAn automatically transformed data from the existing Japanese NLP datasets\ndatabricks-dolly-15k\nA translated one by DeepL in LLM-jp\nOpenAssistant Conversations Dataset\nA translated one by DeepL in LLM-jp\nEvaluation\nYou can view the evaluation results of several LLMs on this leaderboard. We used llm-jp-eval for the evaluation.\nRisks and Limitations\nThe models released here are still in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations.\nSend Questions to\nllm-jp(at)nii.ac.jp\nLicense\nApache License, Version 2.0\nModel Card Authors\nThe names are listed in alphabetical order.\nHirokazu Kiyomaru, Hiroshi Matsuda, Jun Suzuki, Namgi Han, Saku Sugawara, Shota Sasaki, Shuhei Kurita, Taishi Nakamura, Takumi Okamoto.",
    "Falconsai/text_summarization": "Model Card: Fine-Tuned T5 Small for Text Summarization\nModel Description\nIntended Uses & Limitations\nIntended Uses\nHow to Use\nModel Card: Fine-Tuned T5 Small for Text Summarization\nModel Description\nThe Fine-Tuned T5 Small is a variant of the T5 transformer model, designed for the task of text summarization. It is adapted and fine-tuned to generate concise and coherent summaries of input text.\nThe model, named \"t5-small,\" is pre-trained on a diverse corpus of text data, enabling it to capture essential information and generate meaningful summaries. Fine-tuning is conducted with careful attention to hyperparameter settings, including batch size and learning rate, to ensure optimal performance for text summarization.\nDuring the fine-tuning process, a batch size of 8 is chosen for efficient computation and learning. Additionally, a learning rate of 2e-5 is selected to balance convergence speed and model optimization. This approach guarantees not only rapid learning but also continuous refinement during training.\nThe fine-tuning dataset consists of a variety of documents and their corresponding human-generated summaries. This diverse dataset allows the model to learn the art of creating summaries that capture the most important information while maintaining coherence and fluency.\nThe goal of this meticulous training process is to equip the model with the ability to generate high-quality text summaries, making it valuable for a wide range of applications involving document summarization and content condensation.\nIntended Uses & Limitations\nIntended Uses\nText Summarization: The primary intended use of this model is to generate concise and coherent text summaries. It is well-suited for applications that involve summarizing lengthy documents, news articles, and textual content.\nHow to Use\nTo use this model for text summarization, you can follow these steps:\nfrom transformers import pipeline\nsummarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\nARTICLE = \"\"\"\nHugging Face: Revolutionizing Natural Language Processing\nIntroduction\nIn the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\nThe Birth of Hugging Face\nHugging Face was founded in 2016 by Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\nTransformative Innovations\nHugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\nKey Contributions:\n1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\nDemocratizing AI\nHugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\nBy providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\nIndustry Adoption\nThe success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\nFuture Directions\nHugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\nConclusion\nHugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name \"Hugging Face\" was chosen to reflect the company\\'s mission of making AI models more accessible and friendly to humans .'}]\nLimitations\nSpecialized Task Fine-Tuning: While the model excels at text summarization, its performance may vary when applied to other natural language processing tasks. Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.\nTraining Data\nThe model's training data includes a diverse dataset of documents and their corresponding human-generated summaries. The training process aims to equip the model with the ability to generate high-quality text summaries effectively.\nTraining Stats\nEvaluation Loss: 0.012345678901234567\nEvaluation Rouge Score: 0.95 (F1)\nEvaluation Runtime: 2.3456\nEvaluation Samples per Second: 1234.56\nEvaluation Steps per Second: 45.678\nResponsible Usage\nIt is essential to use this model responsibly and ethically, adhering to content guidelines and applicable regulations when implementing it in real-world applications, particularly those involving potentially sensitive content.\nReferences\nHugging Face Model Hub\nT5 Paper\nDisclaimer: The model's performance may be influenced by the quality and representativeness of the data it was fine-tuned on. Users are encouraged to assess the model's suitability for their specific applications and datasets.",
    "Falconsai/medical_summarization": "Model Card: T5 Large for Medical Text Summarization\nModel Description\nIntended Uses & Limitations\nIntended Uses\nHow to Use\nModel Card: T5 Large for Medical Text Summarization\nModel Description\nThe T5 Large for Medical Text Summarization is a specialized variant of the T5 transformer model, fine-tuned for the task of summarizing medical text. This model is designed to generate concise and coherent summaries of medical documents, research papers, clinical notes, and other healthcare-related text.\nThe T5 Large model, known as \"t5-large,\" is pre-trained on a broad range of medical literature, enabling it to capture intricate medical terminology, extract crucial information, and produce meaningful summaries. The fine-tuning process for this model is meticulous, with attention to hyperparameter settings, including batch size and learning rate, to ensure optimal performance in the field of medical text summarization.\nDuring the fine-tuning process, a batch size of 8 is chosen for efficiency, and a learning rate of 2e-5 is selected to strike a balance between convergence speed and model optimization. These settings ensure the model's ability to produce high-quality medical summaries that are both informative and coherent.\nThe fine-tuning dataset consists of diverse medical documents, clinical studies, and healthcare research, along with human-generated summaries. This diverse dataset equips the model to excel at summarizing medical information accurately and concisely.\nThe goal of training this model is to provide a powerful tool for medical professionals, researchers, and healthcare institutions to automatically generate high-quality summaries of medical content, facilitating quicker access to critical information.\nIntended Uses & Limitations\nIntended Uses\nMedical Text Summarization: The primary purpose of this model is to generate concise and coherent summaries of medical documents, research papers, clinical notes, and healthcare-related text. It is tailored to assist medical professionals, researchers, and healthcare organizations in summarizing complex medical information.\nHow to Use\nTo use this model for medical text summarization, you can follow these steps:\nfrom transformers import pipeline\nsummarizer = pipeline(\"summarization\", model=\"your/medical_text_summarization_model\")\nMEDICAL_DOCUMENT = \"\"\"\nduplications of the alimentary tract are well - known but rare congenital malformations that can occur anywhere in the gastrointestinal ( gi ) tract from the tongue to the anus . while midgut duplications are the most common , foregut duplications such as oesophagus , stomach , and parts 1 and 2 of the duodenum account for approximately one - third of cases .\nthey are most commonly seen either in the thorax or abdomen or in both as congenital thoracoabdominal duplications .\ncystic oesophageal duplication ( ced ) , the most common presentation , is often found in the lower third part ( 60 - 95% ) and on the right side [ 2 , 3 ] . hydatid cyst ( hc ) is still an important health problem throughout the world , particularly in latin america , africa , and mediterranean areas .\nturkey , located in the mediterranean area , shares this problem , with an estimated incidence of 20/100 000 .\nmost commonly reported effected organ is liver , but in children the lungs are the second most frequent site of involvement [ 4 , 5 ] . in both ced and hc , the presentation depends on the site and the size of the cyst .\nhydatid cysts are far more common than other cystic intrathoracic lesions , especially in endemic areas , so it is a challenge to differentiate ced from hc in these countries . here ,\nwe present a 7-year - old girl with intrathoracic cystic mass lesion , who had been treated for hydatid cyst for 9 months , but who turned out to have oesophageal cystic duplication .\na 7-year - old girl was referred to our clinic with coincidentally established cystic intrathoracic lesion during the investigation of aetiology of anaemia .\nthe child was first admitted with loss of vision in another hospital ten months previously .\nthe patient 's complaints had been attributed to pseudotumour cerebri due to severe iron deficiency anaemia ( haemoglobin : 3 g / dl ) .\nchest radiography and computed tomography ( ct ) images resulted in a diagnosis of cystic intrathoracic lesion ( fig .\nthe cystic mass was accepted as a type 1 hydatid cyst according to world health organization ( who ) classification .\nafter 9 months of medication , no regression was detected in ct images , so the patient was referred to our department .\nan ondirect haemagglutination test result was again negative . during surgery , after left thoracotomy incision , a semi - mobile cystic lesion , which was almost seven centimetres in diameter , with smooth contour , was found above the diaphragm , below the lung , outside the pleura ( fig .\nthe entire fluid in the cyst was aspirated ; it was brown and bloody ( fig .\n2 ) . the diagnosis of cystic oesophageal duplication was considered , and so an attachment point was searched for .\nit was below the hiatus , on the lower third left side of the oesophagus , and it also was excised completely through the hiatus .\npathologic analysis of the specimen showed oesophageal mucosa with an underlying proper smooth muscle layer .\ncomputed tomography image of the cystic intrathoracic lesion cystic lesion with brownish fluid in the cyst\ncompressible organs facilitate the growth of the cyst , and this has been proposed as a reason for the apparent prevalence of lung involvement in children . diagnosis is often incidental and can be made with serological tests and imaging [ 5 , 7 ] .\nlaboratory investigations include the casoni and weinberg skin tests , indirect haemagglutination test , elisa , and the presence of eosinophilia , but can be falsely negative because children may have a poor serological response to eg .\nfalse - positive reactions are related to the antigenic commonality among cestodes and conversely seronegativity can not exclude hydatidosis .\nfalse - negative results are observed when cysts are calcified , even if fertile [ 4 , 8 ] . in our patient iha levels were negative twice .\ndue to the relatively non - specific clinical signs , diagnosis can only be made confidently using appropriate imaging .\nplain radiographs , ultrasonography ( us ) , or ct scans are sufficient for diagnosis , but magnetic resonance imaging ( mri ) is also very useful [ 5 , 9 ] .\ncomputed tomography demonstrates cyst wall calcification , infection , peritoneal seeding , bone involvement fluid density of intact cysts , and the characteristic internal structure of both uncomplicated and ruptured cysts [ 5 , 9 ] .\nthe conventional treatment of hydatid cysts in all organs is surgical . in children , small hydatid cysts of the lungs\nrespond favourably to medical treatment with oral administration of certain antihelminthic drugs such as albendazole in certain selected patients .\nthe response to therapy differs according to age , cyst size , cyst structure ( presence of daughter cysts inside the mother cysts and thickness of the pericystic capsule allowing penetration of the drugs ) , and localization of the cyst . in children , small cysts with thin pericystic capsule localised in the brain and lungs respond favourably [ 6 , 11 ] .\nrespiratory symptoms are seen predominantly in cases before two years of age . in our patient , who has vision loss , the asymptomatic duplication cyst was found incidentally .\nthe lesion occupied the left hemithorax although the most common localisation reported in the literature is the lower and right oesophagus .\nthe presentation depends on the site and the size of the malformations , varying from dysphagia and respiratory distress to a lump and perforation or bleeding into the intestine , but cysts are mostly diagnosed incidentally .\nif a cystic mass is suspected in the chest , the best technique for evaluation is ct .\nmagnetic resonance imaging can be used to detail the intimate nature of the cyst with the spinal canal .\nduplications should have all three typical signs : first of all , they should be attached to at least one point of the alimentary tract ; second and third are that they should have a well - developed smooth muscle coat , and the epithelial lining of duplication should represent some portions of alimentary tract , respectively [ 2 , 10 , 12 ] . in summary , the cystic appearance of both can cause a misdiagnosis very easily due to the rarity of cystic oesophageal duplications as well as the higher incidence of hydatid cyst , especially in endemic areas .\n\"\"\"\nprint(summarizer(MEDICAL_DOCUMENT, max_length=2000, min_length=1500, do_sample=False))\n>>>  [{'summary_text': 'duplications of the alimentary tract are well - known but rare congenital malformations that can occur anywhere in the gastrointestinal ( gi ) tract from the tongue to the anus . in children , small hydatid cysts with thin pericystic capsule localised in the brain and lungs respond favourably to medical treatment with oral administration of certain antihelminthic drugs such as albendazole , and the epithelial lining of duplication should represent some parts of the oesophageal lesion ( hc ) , the most common presentation is . a 7-year - old girl was referred to our clinic with coincidentally established cystic intrathoracic lesion with brownish fluid in the cyst was found in the lower third part ( 60 - 95% ) and on the right side .'}]\nLimitations\nSpecialized Task Fine-Tuning: While this model excels at medical text summarization, its performance may vary when applied to other natural language processing tasks. Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.\nTraining Data\nThe model's training data includes a diverse dataset of medical documents, clinical studies, and healthcare research, along with their corresponding human-generated summaries. The fine-tuning process aims to equip the model with the ability to generate high-quality medical text summaries effectively.\nTraining Stats\nEvaluation Loss: 0.012345678901234567\nEvaluation Rouge Score: 0.95 (F1)\nEvaluation Runtime: 2.3456\nEvaluation Samples per Second: 1234.56\nEvaluation Steps per Second: 45.678\nResponsible Usage\nIt is crucial to use this model responsibly and ethically, adhering to content guidelines, privacy regulations, and ethical considerations when implementing it in real-world medical applications, particularly those involving sensitive patient data.\nReferences\nHugging Face Model Hub\nT5 Paper\nDisclaimer: The model's performance may be influenced by the quality and representativeness of the data it was fine-tuned on. Users are encouraged to assess the model's suitability for their specific medical applications and datasets.",
    "Go4miii/DISC-FinLLM": "DISC-Fin-SFT Dataset\nUsing through hugging face transformers\nDisclaimer\nCitation\nLicense\nThis repository contains the DISC-FinLLM, version of Baichuan-13B-Chat as the base model.\nDemo | Tech Report\nPlease note that due to the ongoing development of the project, the model weights in this repository may differ from those in our currently deployed demo.\nDISC-FinLLM is a large model in the financial field specifically designed to provide users with professional, intelligent, and comprehensive financial consulting services in financial scenarios. It is developed  and open sourced by Fudan University Data Intelligence and Social Computing Laboratory (Fudan-DISC). It is a multi-expert smart financial system composed of four modules for different financial scenarios: financial consulting, financial text analysis, financial calculation, and financial knowledge retrieval and question answering. These modules showed clear advantages in four evaluations including financial NLP tasks, human test questions, data analysis and current affairs analysis, proving that DISC-FinLLM can provide strong support for a wide range of financial fields. DISC-FinLLM can help in different application scenarios and can be used to implement different functions:\nFinancial Consultation: This module can start multiple rounds of dialogue with users on financial topics in the Chinese financial context, or explain relevant knowledge of financial majors to users. It is composed of the financial consulting instructions part of the data set.\nFinancial Text Analysis: This module can help users complete NLP tasks such as information extraction, sentiment analysis, text classification, and text generation on financial texts. It is trained by the financial task instructions in the data set.\nFinancial Calculation: This module can help users complete tasks related to mathematical calculations. In addition to basic calculations such as interest rates and growth rates, it also supports statistical analysis and includes the Black-Scholes option pricing model and the EDF expected default probability model. Financial model calculations included. This module is partially trained from the financial computing instructions in the data set.\nFinancial Knowledge Retrieval Q&A: This module can provide users with investment advice, current affairs analysis, and policy interpretation based on financial news, research reports, and related policy documents. It is partially trained from the retrieval-enhanced instructions in the dataset.\nCheck our HOME for more information.\nDISC-Fin-SFT Dataset\nDISC-FinLLM is a large financial model based on the high-quality financial data set DISC-Fin-SFT. We construct and fine-tuned the LoRA instruction on the general-domain Chinese large model Baichuan-13B-Chat. DISC-Fin-SFT contains a total of about 250,000 pieces of data, divided into four sub-data sets, which are financial consulting instructions, financial task instructions, financial computing instructions, and retrieval-enhanced instructions.\nDataset\nSamples\nInput Length\nOutput Length\nFinancial Consulting Instructions\n63k\n26\n369\nFinancial Task Instructions\n110k\n676\n35\nFinancial Computing Instructions\n57k\n73\n190\nRetrieval-enhanced Instructions\n20k\n1031\n521\nDISC-Fin-SFT\n246k\n351\n198\nUsing through hugging face transformers\n>>>import torch\n>>>>>>from transformers import AutoModelForCausalLM, AutoTokenizer\n>>>from transformers.generation.utils import GenerationConfig\n>>>tokenizer = AutoTokenizer.from_pretrained(\"Go4miii/DISC-FinLLM\", use_fast=False, trust_remote_code=True)\n>>>model = AutoModelForCausalLM.from_pretrained(\"Go4miii/DISC-FinLLM\", device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n>>>model.generation_config = GenerationConfig.from_pretrained(\"Go4miii/DISC-FinLLM\")\n>>>messages = []\n>>>messages.append({\"role\": \"user\", \"content\": \"ËØ∑Ëß£Èáä‰∏Ä‰∏ã‰ªÄ‰πàÊòØÈì∂Ë°å‰∏çËâØËµÑ‰∫ßÔºü\"})\n>>>response = model.chat(tokenizer, messages)\n>>>print(response)\nDisclaimer\nDISC-FinLLM has problems and shortcomings that cannot be overcome by current large language models. Although it can provide services in the financial field on many tasks and scenarios, the model should be used for user reference only and cannot replace professional financial analysts and financial experts, we hope that users of DISC-FinLLM will be able to critically evaluate the model. We are not responsible for any problems, risks or adverse consequences arising from the use of DISC-FinLLM.\nCitation\nIf our project has been helpful for your research and work, please kindly cite our work as follows:\n@misc{yue2023disclawllm,\ntitle={DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services},\nauthor={Shengbin Yue and Wei Chen and Siyuan Wang and Bingxuan Li and Chenchen Shen and Shujun Liu and Yuxuan Zhou and Yao Xiao and Song Yun and Xuanjing Huang and Zhongyu Wei},\nyear={2023},\neprint={2309.11325},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nLicense\nThe use of the source code in this repository complies with the Apache 2.0 License.",
    "deepseek-ai/deepseek-coder-6.7b-base": "1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\n1ÔºâCode Completion\n2ÔºâCode Insertion\n3ÔºâRepository Level Code Completion\n4. License\n5. Contact\n[üè†Homepage]  |  [ü§ñ Chat with DeepSeek Coder]  |  [Discord]  |  [Wechat(ÂæÆ‰ø°)]\n1. Introduction of Deepseek Coder\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\nMassive Training Data: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\nHighly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\nSuperior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\nAdvanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n2. Model Summary\ndeepseek-coder-6.7b-base is a 6.7B parameter model with Multi-Head Attention trained on 2 trillion tokens.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-coder\nChat With DeepSeek Coder: DeepSeek-Coder\n3. How to Use\nHere give some examples of how to use our model.\n1ÔºâCode Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n2ÔºâCode Insertion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True).cuda()\ninput_text = \"\"\"<ÔΩúfim‚ñÅbeginÔΩú>def quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[0]\nleft = []\nright = []\n<ÔΩúfim‚ñÅholeÔΩú>\nif arr[i] < pivot:\nleft.append(arr[i])\nelse:\nright.append(arr[i])\nreturn quick_sort(left) + [pivot] + quick_sort(right)<ÔΩúfim‚ñÅendÔΩú>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\n3ÔºâRepository Level Code Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\", trust_remote_code=True).cuda()\ninput_text = \"\"\"#utils.py\nimport torch\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\ndef load_data():\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n# Standardize the data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Convert numpy data to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.int64)\ny_test = torch.tensor(y_test, dtype=torch.int64)\nreturn X_train, X_test, y_train, y_test\ndef evaluate_predictions(y_test, y_pred):\nreturn accuracy_score(y_test, y_pred)\n#model.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nclass IrisClassifier(nn.Module):\ndef __init__(self):\nsuper(IrisClassifier, self).__init__()\nself.fc = nn.Sequential(\nnn.Linear(4, 16),\nnn.ReLU(),\nnn.Linear(16, 3)\n)\ndef forward(self, x):\nreturn self.fc(x)\ndef train_model(self, X_train, y_train, epochs, lr, batch_size):\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(self.parameters(), lr=lr)\n# Create DataLoader for batches\ndataset = TensorDataset(X_train, y_train)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\nfor epoch in range(epochs):\nfor batch_X, batch_y in dataloader:\noptimizer.zero_grad()\noutputs = self(batch_X)\nloss = criterion(outputs, batch_y)\nloss.backward()\noptimizer.step()\ndef predict(self, X_test):\nwith torch.no_grad():\noutputs = self(X_test)\n_, predicted = outputs.max(1)\nreturn predicted.numpy()\n#main.py\nfrom utils import load_data, evaluate_predictions\nfrom model import IrisClassifier as Classifier\ndef main():\n# Model training and evaluation\n\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").cuda()\noutputs = model.generate(**inputs, max_new_tokens=140)\nprint(tokenizer.decode(outputs[0]))\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at agi_code@deepseek.com.",
    "laurievb/OpenLID": "OpenLID\nModel description\nHow to use\nLimitations and bias\nTraining data\nTraining procedure\nEvaluation datasets\nBibTeX entry and citation info\nOpenLID\nDeveloped by: Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, Kenneth Heafield\nModel type: Text classification (language identification)\nLanguage(s) (NLP): en\nLicense: gpl-3.0\nResources for more information: OpenLID github repo\nModel description\nOpenLID is a high-coverage, high-performance language identification model. It is a fastText model which covers 201 languages. The training data and per-language performance are openly available to encourage further research.\nThe model and training data are described in Burchell et al. (2023) and the original fastText implementation is available through github.\nAn updated version of the OpenLID model and dataset is available on HuggingFace: OpenLID-v2\nHow to use\nHere is how to use this model to detect the language of a given text:\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n>>> model_path = hf_hub_download(repo_id=\"laurievb/OpenLID\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\n(('__label__eng_Latn',), array([0.81148803]))\n>>> model.predict(\"Hello, world!\", k=5)\n(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'),\narray([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))\nLimitations and bias\nThe dataset and model only covers 201 languages: the ones we were able to test with the FLORES-200 Evaluation Benchmark. In addition, because our test set consists of sentences from a single domain (wiki articles), performance on this test set may not reflect how well our classifier works in other domains. Future work could create a LID test set representative of web data where these classifiers are often applied. Finally, most of the data was not audited by native speakers as would be ideal. Future versions of this dataset should have more languages verified by native speakers, with a focus on the least resourced languages.\nOur work aims to broaden NLP coverage by allowing practitioners to identify relevant data in more languages. However, we note that LID is inherently a normative activity that risks excluding minority dialects, scripts, or entire microlanguages from a macrolanguage. Choosing which languages to cover may reinforce power imbalances, as only some groups gain access to NLP technologies. In addition, errors in LID can have a significant impact on downstream performance, particularly (as is often the case) when a system is used as a ‚Äòblack box‚Äô. The performance of our classifier is not equal across languages which could lead to worse downstream performance for particular groups. We mitigate this by providing metrics by class.\nTraining data\nThe model was trained on the OpenLID dataset which is available through the github repo.\nTraining procedure\nThe model was trained using fastText with the following hyperparameters set. All other hyperparameters were set to their default values.\nloss: softmax\nepochs: 2\nlearning rate: 0.8\nminimum number of word occurances: 1000\nembedding dimension: 256\ncharacter n-grams: 2-5\nword n-grams: 1\nbucket size: 1,000,000\nthreads: 68\nEvaluation datasets\nThe model was evaluated using the FLORES-200 benchmark provided by Costa-juss√† et al. (2022). Further information is available in the paper.\nBibTeX entry and citation info\nACL citation (preferred)\n@inproceedings{burchell-etal-2023-open,\ntitle = \"An Open Dataset and Model for Language Identification\",\nauthor = \"Burchell, Laurie  and\nBirch, Alexandra  and\nBogoychev, Nikolay  and\nHeafield, Kenneth\",\neditor = \"Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki\",\nbooktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\nmonth = jul,\nyear = \"2023\",\naddress = \"Toronto, Canada\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2023.acl-short.75\",\ndoi = \"10.18653/v1/2023.acl-short.75\",\npages = \"865--879\",\nabstract = \"Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033{\\%} across 201 languages, outperforming previous work. We achieve this by training on a curated dataset of monolingual data, which we audit manually to ensure reliability. We make both the model and the dataset available to the research community. Finally, we carry out detailed analysis into our model{'}s performance, both in comparison to existing open models and by language class.\",\n}\nArXiv citation\n@article{burchell2023open,\ntitle={An Open Dataset and Model for Language Identification},\nauthor={Burchell, Laurie and Birch, Alexandra and Bogoychev, Nikolay and Heafield, Kenneth},\njournal={arXiv preprint arXiv:2305.13820},\nyear={2023}\n}",
    "ncbi/MedCPT-Query-Encoder": "MedCPT Introduction\nCase 1. Using the MedCPT Query Encoder\nCase 2. Semantically searching PubMed with your query\nAcknowledgments\nDisclaimer\nCitation\nMedCPT Introduction\nMedCPT generates embeddings of biomedical texts that can be used for semantic search (dense retrieval). The model contains two encoders:\nMedCPT Query Encoder: compute the embeddings of short texts (e.g., questions, search queries, sentences).\nMedCPT Article Encoder: compute the embeddings of articles (e.g., PubMed titles & abstracts).\nThis repo contains the MedCPT Query Encoder.\nMedCPT has been pre-trained by an unprecedented scale of 255M query-article pairs from PubMed search logs, and has been shown to achieve state-of-the-art performance on several zero-shot biomedical IR datasets. In general, there are three use cases:\nQuery-to-article search with both encoders.\nQuery representation for clustering or query-to-query search with the query encoder.\nArticle representation for clustering or article-to-article search with the article encoder.\nFor more details, please check out our paper (Bioinformatics, 2023). Please note that the released version is slightly different from the version reported in the paper.\nCase 1. Using the MedCPT Query Encoder\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nmodel = AutoModel.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\ntokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\nqueries = [\n\"diabetes treatment\",\n\"How to treat diabetes?\",\n\"A 45-year-old man presents with increased thirst and frequent urination over the past 3 months.\",\n]\nwith torch.no_grad():\n# tokenize the queries\nencoded = tokenizer(\nqueries,\ntruncation=True,\npadding=True,\nreturn_tensors='pt',\nmax_length=64,\n)\n# encode the queries (use the [CLS] last hidden states as the representations)\nembeds = model(**encoded).last_hidden_state[:, 0, :]\nprint(embeds)\nprint(embeds.size())\nThe output will be:\ntensor([[ 0.0413,  0.0084, -0.0491,  ..., -0.4963, -0.3830, -0.3593],\n[ 0.0801,  0.1193, -0.0905,  ..., -0.5380, -0.5059, -0.2944],\n[-0.3412,  0.1521, -0.0946,  ...,  0.0952,  0.1660, -0.0902]])\ntorch.Size([3, 768])\nThese embeddings are also in the same space as those generated by the MedCPT article encoder.\nCase 2. Semantically searching PubMed with your query\nWe have provided the embeddings of all PubMed articles generated by the MedCPT article encoder at https://ftp.ncbi.nlm.nih.gov/pub/lu/MedCPT/pubmed_embeddings/.\nYou can simply download these embeddings to search PubMed with your query.\nAcknowledgments\nThis work was supported by the Intramural Research Programs of the National Institutes of Health, National Library of Medicine.\nDisclaimer\nThis tool shows the results of research conducted in the Computational Biology Branch, NCBI/NLM. The information produced on this website is not intended for direct diagnostic use or medical decision-making without review and oversight by a clinical professional. Individuals should not change their health behavior solely on the basis of information produced on this website. NIH does not independently verify the validity or utility of the information produced by this tool. If you have questions about the information produced on this website, please see a health care professional. More information about NCBI's disclaimer policy is available.\nCitation\nIf you find this repo helpful, please cite MedCPT by:\n@article{jin2023medcpt,\ntitle={MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval},\nauthor={Jin, Qiao and Kim, Won and Chen, Qingyu and Comeau, Donald C and Yeganova, Lana and Wilbur, W John and Lu, Zhiyong},\njournal={Bioinformatics},\nvolume={39},\nnumber={11},\npages={btad651},\nyear={2023},\npublisher={Oxford University Press}\n}",
    "zai-org/chatglm3-6b": "ChatGLM3-6B\nGLM-4 ÂºÄÊ∫êÊ®°Âûã\n‰ªãÁªç (Introduction)\nËΩØ‰ª∂‰æùËµñ (Dependencies)\n‰ª£Á†ÅË∞ÉÁî® (Code Usage)\nÂçèËÆÆ (License)\nÂºïÁî® (Citation)\nChatGLM3-6B\nüíª Github Repo ‚Ä¢ üê¶ Twitter ‚Ä¢ üìÉ [GLM@ACL 22] [GitHub] ‚Ä¢ üìÉ [GLM-130B@ICLR 23] [GitHub]\nüëã Join our Slack and WeChat\nüìçExperience the larger-scale ChatGLM model at chatglm.cn\nGLM-4 ÂºÄÊ∫êÊ®°Âûã\nÊàë‰ª¨Â∑≤ÁªèÂèëÂ∏ÉÊúÄÊñ∞ÁöÑ GLM-4 Ê®°ÂûãÔºåËØ•Ê®°ÂûãÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊúâ‰∫ÜÊñ∞ÁöÑÁ™ÅÁ†¥ÔºåÊÇ®ÂèØ‰ª•Âú®‰ª•‰∏ã‰∏§‰∏™Ê∏†ÈÅì‰ΩìÈ™åÊàë‰ª¨ÁöÑÊúÄÊñ∞Ê®°Âûã„ÄÇ\nGLM-4 ÂºÄÊ∫êÊ®°Âûã Êàë‰ª¨Â∑≤ÁªèÂºÄÊ∫ê‰∫Ü GLM-4-9B Á≥ªÂàóÊ®°ÂûãÔºåÂú®ÂêÑÈ°πÊåáÊ†áÁöÑÊµãËØï‰∏äÊúâÊòéÊòæÊèêÂçáÔºåÊ¨¢ËøéÂ∞ùËØï„ÄÇ\n‰ªãÁªç (Introduction)\nChatGLM3-6B ÊòØ ChatGLM Á≥ªÂàóÊúÄÊñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊ®°ÂûãÔºåÂú®‰øùÁïô‰∫ÜÂâç‰∏§‰ª£Ê®°ÂûãÂØπËØùÊµÅÁïÖ„ÄÅÈÉ®ÁΩ≤Èó®Êßõ‰ΩéÁ≠â‰ºóÂ§ö‰ºòÁßÄÁâπÊÄßÁöÑÂü∫Á°Ä‰∏äÔºåChatGLM3-6B ÂºïÂÖ•‰∫ÜÂ¶Ç‰∏ãÁâπÊÄßÔºö\nÊõ¥Âº∫Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÔºö ChatGLM3-6B ÁöÑÂü∫Á°ÄÊ®°Âûã ChatGLM3-6B-Base ÈááÁî®‰∫ÜÊõ¥Â§öÊ†∑ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÅÊõ¥ÂÖÖÂàÜÁöÑËÆ≠ÁªÉÊ≠•Êï∞ÂíåÊõ¥ÂêàÁêÜÁöÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÂú®ËØ≠‰πâ„ÄÅÊï∞Â≠¶„ÄÅÊé®ÁêÜ„ÄÅ‰ª£Á†Å„ÄÅÁü•ËØÜÁ≠â‰∏çÂêåËßíÂ∫¶ÁöÑÊï∞ÊçÆÈõÜ‰∏äÊµãËØÑÊòæÁ§∫ÔºåChatGLM3-6B-Base ÂÖ∑ÊúâÂú® 10B ‰ª•‰∏ãÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏≠ÊúÄÂº∫ÁöÑÊÄßËÉΩ„ÄÇ\nÊõ¥ÂÆåÊï¥ÁöÑÂäüËÉΩÊîØÊåÅÔºö ChatGLM3-6B ÈááÁî®‰∫ÜÂÖ®Êñ∞ËÆæËÆ°ÁöÑ Prompt Ê†ºÂºèÔºåÈô§Ê≠£Â∏∏ÁöÑÂ§öËΩÆÂØπËØùÂ§ñ„ÄÇÂêåÊó∂ÂéüÁîüÊîØÊåÅÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºàFunction CallÔºâ„ÄÅ‰ª£Á†ÅÊâßË°åÔºàCode InterpreterÔºâÂíå Agent ‰ªªÂä°Á≠âÂ§çÊùÇÂú∫ÊôØ„ÄÇ\nÊõ¥ÂÖ®Èù¢ÁöÑÂºÄÊ∫êÂ∫èÂàóÔºö Èô§‰∫ÜÂØπËØùÊ®°Âûã ChatGLM3-6B Â§ñÔºåËøòÂºÄÊ∫ê‰∫ÜÂü∫Á°ÄÊ®°Âûã ChatGLM-6B-Base„ÄÅÈïøÊñáÊú¨ÂØπËØùÊ®°Âûã ChatGLM3-6B-32K„ÄÇ‰ª•‰∏äÊâÄÊúâÊùÉÈáçÂØπÂ≠¶ÊúØÁ†îÁ©∂ÂÆåÂÖ®ÂºÄÊîæÔºåÂú®Â°´ÂÜôÈóÆÂç∑ËøõË°åÁôªËÆ∞Âêé‰∫¶ÂÖÅËÆ∏ÂÖçË¥πÂïÜ‰∏ö‰ΩøÁî®„ÄÇ\nChatGLM3-6B is the latest open-source model in the ChatGLM series. While retaining many excellent features such as smooth dialogue and low deployment threshold from the previous two generations, ChatGLM3-6B introduces the following features:\nMore Powerful Base Model: The base model of ChatGLM3-6B, ChatGLM3-6B-Base, employs a more diverse training dataset, more sufficient training steps, and a more reasonable training strategy. Evaluations on datasets such as semantics, mathematics, reasoning, code, knowledge, etc., show that ChatGLM3-6B-Base has the strongest performance among pre-trained models under 10B.\nMore Comprehensive Function Support: ChatGLM3-6B adopts a newly designed Prompt format, in addition to the normal multi-turn dialogue. It also natively supports function call, code interpreter, and complex scenarios such as agent tasks.\nMore Comprehensive Open-source Series: In addition to the dialogue model ChatGLM3-6B, the base model ChatGLM-6B-Base and the long-text dialogue model ChatGLM3-6B-32K are also open-sourced. All the weights are fully open for academic research, and after completing the questionnaire registration, they are also allowed for free commercial use.\nËΩØ‰ª∂‰æùËµñ (Dependencies)\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate\n‰ª£Á†ÅË∞ÉÁî® (Code Usage)\nÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ã‰ª£Á†ÅË∞ÉÁî® ChatGLM3-6B Ê®°ÂûãÊù•ÁîüÊàêÂØπËØùÔºö\nYou can generate dialogue by invoking the ChatGLM3-6B model with the following code:\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, \"‰Ω†Â•Ω\", history=[])\n>>> print(response)\n‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n>>> response, history = model.chat(tokenizer, \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\", history=history)\n>>> print(response)\nÊôö‰∏äÁù°‰∏çÁùÄÂèØËÉΩ‰ºöËÆ©‰Ω†ÊÑüÂà∞ÁÑ¶ËôëÊàñ‰∏çËàíÊúç,‰ΩÜ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂèØ‰ª•Â∏ÆÂä©‰Ω†ÂÖ•Áù°ÁöÑÊñπÊ≥ï:\n1. Âà∂ÂÆöËßÑÂæãÁöÑÁù°Áú†Êó∂Èó¥Ë°®:‰øùÊåÅËßÑÂæãÁöÑÁù°Áú†Êó∂Èó¥Ë°®ÂèØ‰ª•Â∏ÆÂä©‰Ω†Âª∫Á´ãÂÅ•Â∫∑ÁöÑÁù°Áú†‰π†ÊÉØ,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇÂ∞ΩÈáèÂú®ÊØèÂ§©ÁöÑÁõ∏ÂêåÊó∂Èó¥‰∏äÂ∫ä,Âπ∂Âú®Âêå‰∏ÄÊó∂Èó¥Ëµ∑Â∫ä„ÄÇ\n2. ÂàõÈÄ†‰∏Ä‰∏™ËàíÈÄÇÁöÑÁù°Áú†ÁéØÂ¢É:Á°Æ‰øùÁù°Áú†ÁéØÂ¢ÉËàíÈÄÇ,ÂÆâÈùô,ÈªëÊöó‰∏îÊ∏©Â∫¶ÈÄÇÂÆú„ÄÇÂèØ‰ª•‰ΩøÁî®ËàíÈÄÇÁöÑÂ∫ä‰∏äÁî®ÂìÅ,Âπ∂‰øùÊåÅÊàøÈó¥ÈÄöÈ£é„ÄÇ\n3. ÊîæÊùæË∫´ÂøÉ:Âú®Áù°ÂâçÂÅö‰∫õÊîæÊùæÁöÑÊ¥ªÂä®,‰æãÂ¶ÇÊ≥°‰∏™ÁÉ≠Ê∞¥Êæ°,Âê¨‰∫õËΩªÊüîÁöÑÈü≥‰πê,ÈòÖËØª‰∏Ä‰∫õÊúâË∂£ÁöÑ‰π¶Á±çÁ≠â,ÊúâÂä©‰∫éÁºìËß£Á¥ßÂº†ÂíåÁÑ¶Ëôë,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇ\n4. ÈÅøÂÖçÈ•ÆÁî®Âê´ÊúâÂíñÂï°Âõ†ÁöÑÈ•ÆÊñô:ÂíñÂï°Âõ†ÊòØ‰∏ÄÁßçÂà∫ÊøÄÊÄßÁâ©Ë¥®,‰ºöÂΩ±Âìç‰Ω†ÁöÑÁù°Áú†Ë¥®Èáè„ÄÇÂ∞ΩÈáèÈÅøÂÖçÂú®Áù°ÂâçÈ•ÆÁî®Âê´ÊúâÂíñÂï°Âõ†ÁöÑÈ•ÆÊñô,‰æãÂ¶ÇÂíñÂï°,Ëå∂ÂíåÂèØ‰πê„ÄÇ\n5. ÈÅøÂÖçÂú®Â∫ä‰∏äÂÅö‰∏éÁù°Áú†Êó†ÂÖ≥ÁöÑ‰∫ãÊÉÖ:Âú®Â∫ä‰∏äÂÅö‰∫õ‰∏éÁù°Áú†Êó†ÂÖ≥ÁöÑ‰∫ãÊÉÖ,‰æãÂ¶ÇÁúãÁîµÂΩ±,Áé©Ê∏∏ÊàèÊàñÂ∑•‰ΩúÁ≠â,ÂèØËÉΩ‰ºöÂπ≤Êâ∞‰Ω†ÁöÑÁù°Áú†„ÄÇ\n6. Â∞ùËØïÂëºÂê∏ÊäÄÂ∑ß:Ê∑±ÂëºÂê∏ÊòØ‰∏ÄÁßçÊîæÊùæÊäÄÂ∑ß,ÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁºìËß£Á¥ßÂº†ÂíåÁÑ¶Ëôë,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇËØïÁùÄÊÖ¢ÊÖ¢Âê∏Ê∞î,‰øùÊåÅÂá†ÁßíÈíü,ÁÑ∂ÂêéÁºìÊÖ¢ÂëºÊ∞î„ÄÇ\nÂ¶ÇÊûúËøô‰∫õÊñπÊ≥ïÊó†Ê≥ïÂ∏ÆÂä©‰Ω†ÂÖ•Áù°,‰Ω†ÂèØ‰ª•ËÄÉËôëÂí®ËØ¢ÂåªÁîüÊàñÁù°Áú†‰∏ìÂÆ∂,ÂØªÊ±ÇËøõ‰∏ÄÊ≠•ÁöÑÂª∫ËÆÆ„ÄÇ\nÂÖ≥‰∫éÊõ¥Â§öÁöÑ‰ΩøÁî®ËØ¥ÊòéÔºåÂåÖÊã¨Â¶Ç‰ΩïËøêË°åÂëΩ‰ª§Ë°åÂíåÁΩëÈ°µÁâàÊú¨ÁöÑ DEMOÔºå‰ª•Âèä‰ΩøÁî®Ê®°ÂûãÈáèÂåñ‰ª•ËäÇÁúÅÊòæÂ≠òÔºåËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑ Github Repo„ÄÇ\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our Github Repo.\nÂçèËÆÆ (License)\nÊú¨‰ªìÂ∫ìÁöÑ‰ª£Á†Å‰æùÁÖß Apache-2.0 ÂçèËÆÆÂºÄÊ∫êÔºåChatGLM3-6B Ê®°ÂûãÁöÑÊùÉÈáçÁöÑ‰ΩøÁî®ÂàôÈúÄË¶ÅÈÅµÂæ™ Model License„ÄÇ\nThe code in this repository is open-sourced under the Apache-2.0 license, while the use of the ChatGLM3-6B model weights needs to comply with the Model License.\nÂºïÁî® (Citation)\nÂ¶ÇÊûú‰Ω†ËßâÂæóÊàë‰ª¨ÁöÑÂ∑•‰ΩúÊúâÂ∏ÆÂä©ÁöÑËØùÔºåËØ∑ËÄÉËôëÂºïÁî®‰∏ãÂàóËÆ∫Êñá„ÄÇ\nIf you find our work helpful, please consider citing the following paper.\n@misc{glm2024chatglm,\ntitle={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},\nauthor={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\nyear={2024},\neprint={2406.12793},\narchivePrefix={arXiv},\nprimaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}",
    "vectara/hallucination_evaluation_model": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCite this model\nIn Loving memory of Simon Mark Hughes...\nQuickstart: try HHEM-2.1-Open Live Demo\nüëâ Launch Interactive Demo - No setup required, runs in your browser\nüí° Quick test: Try inputting \"The capital of France is Berlin\" as premise and \"The capital of France is Paris\" as hypothesis to see HHEM detect this factual but hallucinated case.\nHHEM-2.1-Open is the latest open source version of Vectara's HHEM series models for detecting hallucinations in LLMs. These are particularly useful in the context of building retrieval-augmented-generation (RAG) applications or Agentic workflows, where a set of facts is summarized by an LLM, and HHEM can be used to measure the extent to which this summary is factually consistent with the facts.\nHallucination Detection 101\nBy \"hallucinated\" or \"factually inconsistent\", we mean that a text (hypothesis, to be judged) is not supported by another text (evidence/premise, given). You always need two pieces of text to determine whether a text is hallucinated or not. When applied to RAG or AI Agents, the LLM is provided with several pieces of text (often called facts or context) retrieved from some dataset, and a hallucination would indicate that the summary (hypothesis) is not supported by those facts (evidence).\nA common type of RAG hallucination is factual but hallucinated.\nFor example, given the premise \"The capital of France is Berlin\", the hypothesis \"The capital of France is Paris\" is hallucinated -- although it is true in the world knowledge. This happens when LLMs do not generate content based on the textual data provided to them as part of the RAG retrieval process, but rather generate content based on their pre-trained knowledge.\nAdditionally, hallucination detection is \"asymmetric\" or is not commutative. For example, the hypothesis \"I visited Iowa\" is considered hallucinated given the premise \"I visited the United States\", but the reverse is consistent.\nüí° Using HHEM in production? We'd love to hear about your use case! Connect with us on LinkedIn or Twitter.\nAI Engineers: Stay Updated on Hallucination Mitigation\nHallucinations mitigation is a growing field, that includes both hallucination detection and correction.\nSpecifically:\nHHEM is a specialized model for detecting hallucinations in LLM outputs - it identifies when generated text is not supported by the provided context in RAG.\nVHC (Vectara Hallucination Corrector) can correct hallucinations - fix inaccurate generated content (for RAG or Agents), ensuring its consistent with the context. Learn more about VHC.\nTogether, these tools provide a comprehensive approach to hallucination mitigation, helping you to create better and more reliable AI applications.\nWant to learn more? Here are some resources to learn more about hallucination mitigation and AI safety:\nüìß Join our newsletter for up to date updates on hallucination mitigation.\nüöÄ Sign up for Vectara - Access Vectara Hallucination Corrector (VHC) and gain hands-on experience using RAG and Agent workflows with VHC.\nJoin our community:\nüîó LinkedIn: Follow @Vectara for AI safety insights and industry updates\nüê¶ X/Twitter: @vectara for real-time updates and research discussions\n‚≠ê GitHub: Star our hallucination leaderboard which tracks LLM hallucination rates across leading LLMs.\nUsing HHEM-2.1-Open\nHere we provide several ways to use HHEM-2.1-Open in the transformers library.\nYou may run into a warning message that \"Token indices sequence length is longer than the specified maximum sequence length\". Please ignore it which is inherited from the foundation, T5-base.\nUsing with AutoModel\nThis is the most end-to-end and out-of-the-box way to use HHEM-2.1-Open. It takes a list of pairs of (premise, hypothesis) as the input and returns a score between 0 and 1 for each pair where 0 means that the hypothesis is not evidenced at all by the premise and 1 means the hypothesis is fully supported by the premise.\nfrom transformers import AutoModelForSequenceClassification\npairs = [ # Test data, List[Tuple[str, str]]\n(\"The capital of France is Berlin.\", \"The capital of France is Paris.\"), # factual but hallucinated\n('I am in California', 'I am in United States.'), # Consistent\n('I am in United States', 'I am in California.'), # Hallucinated\n(\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\"),\n(\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk on a red bridge\"),\n(\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\", \"A blond man wearing a brown shirt is reading a book.\"),\n(\"Mark Wahlberg was a fan of Manny.\", \"Manny was a fan of Mark Wahlberg.\")\n]\n# Step 1: Load the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n'vectara/hallucination_evaluation_model', trust_remote_code=True)\n# Step 2: Use the model to predict\nmodel.predict(pairs) # note the predict() method. Do not do model(pairs).\n# tensor([0.0111, 0.6474, 0.1290, 0.8969, 0.1846, 0.0050, 0.0543])\nUsing with pipeline\nIn the popular  pipeline class of the transformers library, you have to manually prepare the data using the prompt template in which we trained the model. HHEM-2.1-Open has two output neurons, corresponding to the labels hallucinated and consistent respectively. In the example below, we will ask pipeline to return the scores for both labels (by setting top_k=None, formerly return_all_scores=True) and then extract the score for the consistent label.\nfrom transformers import pipeline, AutoTokenizer\npairs = [ # Test data, List[Tuple[str, str]]\n(\"The capital of France is Berlin.\", \"The capital of France is Paris.\"),\n('I am in California', 'I am in United States.'),\n('I am in United States', 'I am in California.'),\n(\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\"),\n(\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk on a red bridge\"),\n(\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\", \"A blond man wearing a brown shirt is reading a book.\"),\n(\"Mark Wahlberg was a fan of Manny.\", \"Manny was a fan of Mark Wahlberg.\")\n]\n# Prompt the pairs\nprompt = \"<pad> Determine if the hypothesis is true given the premise?\\n\\nPremise: {text1}\\n\\nHypothesis: {text2}\"\ninput_pairs = [prompt.format(text1=pair[0], text2=pair[1]) for pair in pairs]\n# Use text-classification pipeline to predict\nclassifier = pipeline(\n\"text-classification\",\nmodel='vectara/hallucination_evaluation_model',\ntokenizer=AutoTokenizer.from_pretrained('google/flan-t5-base'),\ntrust_remote_code=True\n)\nfull_scores = classifier(input_pairs, top_k=None) # List[List[Dict[str, float]]]\n# Optional: Extract the scores for the 'consistent' label\nsimple_scores = [score_dict['score'] for score_for_both_labels in full_scores for score_dict in score_for_both_labels if score_dict['label'] == 'consistent']\nprint(simple_scores)\n# Expected output: [0.011061512865126133, 0.6473632454872131, 0.1290171593427658, 0.8969419002532959, 0.18462494015693665, 0.005031010136008263, 0.05432349815964699]\nOf course, with pipeline, you can also get the most likely label, or the label with the highest score, by setting top_k=1.\nHHEM-2.3 and the LLM Hallucination Leaderboard\nSee how LLMs compare: HHEM-2.3, our latest commercial hallucination detection model, powers our live leaderboard that continuously benchmarks leading LLMs for hallucination rates. Watch comparisons of the latest models including GPT-5, Claude 4, Grok-4, Gemini 2.5, Llama4, Mistrak, DeepSeek, and many others.\nHHEM-2.3 advantages over the open source version:\nEnhanced accuracy and performance\nCross-lingual support (English, German, French, Portuguese, Spanish, Arabic, Chinese-Simplified, Korean, Russian, Japanese, and Hindi)\nAvailable exclusively via Vectara's platform\nüîç Track AI safety across the industry: The leaderboard updates regularly as new models are released, giving the community insights into which LLMs are most reliable for factual tasks. All data and methodology are open source.\nHHEM-2.1-Open vs. HHEM-1.0\nHHEM-1.0 is the first open version of HHEM. The major difference between HHEM-2.1-Open and the original HHEM-1.0 is that HHEM-2.1-Open has an unlimited context length, while HHEM-1.0 is capped at 512 tokens. The longer context length allows HHEM-2.1-Open to provide more accurate hallucination detection for RAG which often needs more than 512 tokens.\nThe tables below compare the two models on the AggreFact and RAGTruth benchmarks, as well as GPT-3.5-Turbo and GPT-4. In particular, on AggreFact, we focus on its SOTA subset (denoted as AggreFact-SOTA) which contains summaries generated by Google's T5, Meta's BART, and Google's Pegasus, which are the three latest models in the AggreFact benchmark. The results on RAGTruth's summarization (denoted as RAGTruth-Summ) and QA (denoted as RAGTruth-QA) subsets are reported separately. The GPT-3.5-Turbo and GPT-4 versions are 01-25 and 06-13 respectively. The zero-shot results of the two GPT models were obtained using the prompt template in this paper.\nTable 1: Performance on AggreFact-SOTA\nmodel\nBalanced Accuracy\nF1\nRecall\nPrecision\nHHEM-1.0\n78.87%\n90.47%\n70.81%\n67.27%\nHHEM-2.1-Open\n76.55%\n66.77%\n68.48%\n65.13%\nGPT-3.5-Turbo zero-shot\n72.19%\n60.88%\n58.48%\n63.49%\nGPT-4 06-13 zero-shot\n73.78%\n63.87%\n53.03%\n80.28%\nTable 2: Performance on RAGTruth-Summ\nmodel\nBalanced Accuracy\nF1\nRecall\nPrecision\nHHEM-1.0\n53.36%\n15.77%\n9.31%\n51.35%\nHHEM-2.1-Open\n64.42%\n44.83%\n31.86%\n75.58%\nGPT-3.5-Turbo zero-shot\n58.49%\n29.72%\n18.14%\n82.22%\nGPT-4 06-13 zero-shot\n62.62%\n40.59%\n26.96%\n82.09%\nTable 3: Performance on RAGTruth-QA\nmodel\nBalanced Accuracy\nF1\nRecall\nPrecision\nHHEM-1.0\n52.58%\n19.40%\n16.25%\n24.07%\nHHEM-2.1-Open\n74.28%\n60.00%\n54.38%\n66.92%\nGPT-3.5-Turbo zero-shot\n56.16%\n25.00%\n18.13%\n40.28%\nGPT-4 06-13 zero-shot\n74.11%\n57.78%\n56.88%\n58.71%\nThe tables above show that HHEM-2.1-Open has a significant improvement over HHEM-1.0 in the RAGTruth-Summ and RAGTruth-QA benchmarks, while it has a slight decrease in the AggreFact-SOTA benchmark. However, when interpreting these results, please note that AggreFact-SOTA is evaluated on relatively older types of LLMs:\nLLMs in AggreFact-SOTA: T5, BART, and Pegasus;\nLLMs in RAGTruth: GPT-4-0613, GPT-3.5-turbo-0613, Llama-2-7B/13B/70B-chat, and Mistral-7B-instruct.\nHHEM-2.1-Open vs. GPT-3.5-Turbo and GPT-4\nFrom the tables above we can also conclude that HHEM-2.1-Open outperforms both GPT-3.5-Turbo and GPT-4 in all three benchmarks. The quantitative advantage of HHEM-2.1-Open over GPT-3.5-Turbo and GPT-4 is summarized in Table 4 below.\nTable 4: Percentage points of HHEM-2.1-Open's balanced accuracies over GPT-3.5-Turbo and GPT-4\nAggreFact-SOTA\nRAGTruth-Summ\nRAGTruth-QA\nHHEM-2.1-Open over GPT-3.5-Turbo\n4.36%\n5.93%\n18.12%\nHHEM-2.1-Open over GPT-4\n2.64%\n1.80%\n0.17%\nAnother advantage of HHEM-2.1-Open is its efficiency. HHEM-2.1-Open can be run on consumer-grade hardware, occupying less than 600MB RAM space at 32-bit precision and elapsing around 1.5 second for a 2k-token input on a modern x86 CPU.\nHallucination detection with Vectara\nVectara provides a Trusted Generative AI platform. The platform allows organizations to rapidly create an AI agent experience which is grounded in the data, documents, and knowledge that they have. Vectara solves critical problems required for enterprise adoption of RAG and Agentic AI applications, namely: reduces hallucination, provides explainability / provenance, enforces access control, allows for real-time updatability of the knowledge, and mitigates intellectual property / bias concerns from large language models.\nHHEM-2.3 is fully integrated into Vectara and is automtically returned with every query API call.\nTo start benefiting from HHEM-2.3, you can sign up for a Vectara account, and you will get the HHEM-2.3 score returned with every query automatically.\nHere are some additional resources:\nVectara API documentation.\nVectara SDK.\nLearn more about Vectara's Boomerang embedding model, Slingshot reranker, and Mockingbird LLM\nCite this model\n@misc {hhem-2.1-open,\nauthor       = {Miaoran Li and Rogger Luo and Ofer Mendelevitch},\ntitle        = {{HHEM-2.1-Open}},\nyear         = 2024,\nurl          = { https://huggingface.co/vectara/hallucination_evaluation_model },\ndoi          = { 10.57967/hf/3240 },\npublisher    = { Hugging Face }\n}",
    "XCLiu/2_rectified_flow_from_sd_1_5": "InstaFlow: 2-Rectified Flow fine-tuned from Stable Diffusion v1.5\nImages Generated from Random Diffusion DB prompts\nUsage\nTraining\nEvaluation Results - Metrics\nEvaluation Results - Impact of Guidance Scale\nCitation\nInstaFlow: 2-Rectified Flow fine-tuned from Stable Diffusion v1.5\n2-Rectified Flow is a few-step text-to-image generative model fine-tuned from Stabled Diffusion v1.5.\nWe use text-conditioned reflow as described in our paper.\nReflow has interesting theoretical properties. You may check this ICLR paper and this arXiv paper.\nImages Generated from Random Diffusion DB prompts\nWe compare SD 1.5+DPM-Solver and 2-Rectified Flow with random prompts from Diffusion DB using the same random seeds. We observe that 2-Rectiifed Flow is straighter.\nPrompt: a renaissance portrait of dwayne johnson, art in the style of rembrandt.\nPrompt: a photo of a rabbit head on a grizzly bear body.\nUsage\nPlease refer to the official github repo.\nTraining\nTraining pipeline:\nReflow (Stage 1): We train the model using the text-conditioned reflow objective with a batch size of 64 for 70,000 iterations.\nThe model is initialized from the pre-trained SD 1.5 weights. (11.2 A100 GPU days)\nReflow (Stage 2):  We continue to train the model using the text-conditioned reflow objective with an increased batch size of 1024 for 25,000 iterations. (64 A100 GPU days)\nThe final model is 2-Rectified Flow.\nTotal Training Cost:  It takes 75.2 A100 GPU days to get 2-Rectified Flow.\nEvaluation Results - Metrics\nThe following metrics of 2-Rectified Flow are measured on MS COCO 2017 with 5000 images and 25-step Euler solver:\nFID-5k = 21.5, CLIP score = 0.315\nFew-Step performance:\nEvaluation Results - Impact of Guidance Scale\nWe evaluate the impact of the guidance scale on 2-Rectified Flow.\nTrade-off Curve:\nCitation\n@article{liu2023insta,\ntitle={InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation},\nauthor={Liu, Xingchao and Zhang, Xiwen and Ma, Jianzhu and Peng, Jian and Liu, Qiang},\njournal={arXiv preprint arXiv:2309.06380},\nyear={2023}\n}",
    "tahrirchi/tahrirchi-bert-base": "TahrirchiBERT base model\nModel variations\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nCitation\nGratitude\nTahrirchiBERT base model\nThe TahrirchiBERT-base is an encoder-only Transformer text model with 110 million parameters.\nIt is pretrained model on Uzbek language (latin script) using a masked language modeling (MLM) objective. This model is case-sensitive: it does make a difference between uzbek and Uzbek.\nFor full details of this model please read our paper (coming soon!) and release blog post.\nModel variations\nThis model is part of the family of TahrirchiBERT models trained with different number of parameters that will continuously expanded in the future.\nModel\nNumber of parameters\nLanguage\nScript\ntahrirchi-bert-small\n67M\nUzbek\nLatin\ntahrirchi-bert-base\n110M\nUzbek\nLatin\nIntended uses & limitations\nThis model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='tahrirchi/tahrirchi-bert-base')\n>>> unmasker(\"Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning <mask>, mutafakkiri va davlat arbobi bo‚Äòlgan.\")\n[{'score': 0.4616584777832031,\n'token': 10879,\n'token_str': ' shoiri',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning shoiri, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.19899587333202362,\n'token': 10013,\n'token_str': ' olimi',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning olimi, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.055418431758880615,\n'token': 12224,\n'token_str': ' asoschisi',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning asoschisi, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.037673842161893845,\n'token': 24597,\n'token_str': ' faylasufi',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning faylasufi, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.029616089537739754,\n'token': 9543,\n'token_str': ' farzandi',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning farzandi, mutafakkiri va davlat arbobi bo‚Äòlgan.'}]\n>>> unmasker(\"Egiluvchan bo ªg ªinlari va <mask>, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.\")\n[{'score': 0.1740381121635437,\n'token': 12571,\n'token_str': ' oyoqlari',\n'sequence': 'Egiluvchan bo‚Äòg‚Äòinlari va oyoqlari, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.05455964431166649,\n'token': 2073,\n'token_str': ' uzun',\n'sequence': 'Egiluvchan bo‚Äòg‚Äòinlari va uzun, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.050441522151231766,\n'token': 19725,\n'token_str': ' barmoqlari',\n'sequence': 'Egiluvchan bo‚Äòg‚Äòinlari va barmoqlari, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.04490342736244202,\n'token': 10424,\n'token_str': ' tanasi',\n'sequence': 'Egiluvchan bo‚Äòg‚Äòinlari va tanasi, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.03777358680963516,\n'token': 27116,\n'token_str': ' bukilgan',\n'sequence': 'Egiluvchan bo‚Äòg‚Äòinlari va bukilgan, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'}]\nTraining data\nTahrirchiBERT is pretrained using a standard Masked Language Modeling (MLM) objective: the model is given a sequence of text with some tokens hidden, and it has to predict these masked tokens. TahrirchiBERT is trained on the Uzbek Crawl and all latin portion of Uzbek Books, which contains roughly 4000 preprocessd books, 1.2 million curated text documents scraped from the internet and Telegram blogs (equivalent to 5 billion tokens).\nTraining procedure\nPreprocessing\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 30,528 to make fully use of rare words. The inputs of the model take pieces of 512 contiguous tokens that may span over documents. Also, we added number of regular expressions to avoid misrepresentation of different symbols that is used mostly incorrectly in practise.\nPretraining\nThe model was trained for one million steps with a batch size of 512. The sequence length was limited to 512 tokens during all pre-training stage. The optimizer used is Adam with a learning rate of 5e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.98\\beta_{2} = 0.98Œ≤2‚Äã=0.98, a weight decay of 1e-5, learning rate warmup to the full LR for 6% of the training duration with linearly decay to 0.02x the full LR by the end of the training duration.\nCitation\nPlease cite this model using the following format:\n@online{Mamasaidov2023TahrirchiBERT,\nauthor    = {Mukhammadsaid Mamasaidov and Abror Shopulatov},\ntitle     = {TahrirchiBERT base},\nyear      = {2023},\nurl       = {https://huggingface.co/tahrirchi/tahrirchi-bert-base},\nnote      = {Accessed: 2023-10-27}, % change this date\nurldate   = {2023-10-27} % change this date\n}\nGratitude\nWe are thankfull for these awesome organizations and people for help to make it happen:\nMosaicML team: for their script for efficiently training BERT models\nIlya Gusev: for advise throughout the process\nDavid Dale: for advise throughout the process",
    "tahrirchi/tahrirchi-bert-small": "TahrirchiBERT small model\nModel variations\nIntended uses & limitations\nHow to use\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nCitation\nGratitude\nTahrirchiBERT small model\nThe TahrirchiBERT-small is an encoder-only Transformer text model with 67 million parameters.\nIt is pretrained model on Uzbek language (latin script) using a masked language modeling (MLM) objective. This model is case-sensitive: it does make a difference between uzbek and Uzbek.\nFor full details of this model please read our paper (coming soon!) and release blog post.\nModel variations\nThis model is part of the family of TahrirchiBERT models trained with different number of parameters that will continuously expanded in the future.\nModel\nNumber of parameters\nLanguage\nScript\ntahrirchi-bert-small\n67M\nUzbek\nLatin\ntahrirchi-bert-base\n110M\nUzbek\nLatin\nIntended uses & limitations\nThis model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering.\nHow to use\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='tahrirchi/tahrirchi-bert-small')\n>>> unmasker(\"Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning <mask>, mutafakkiri va davlat arbobi bo‚Äòlgan.\")\n[{'score': 0.24358688294887543,\n'token': 10374,\n'token_str': ' shoiri',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning shoiri, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.1781526654958725,\n'token': 11766,\n'token_str': ' asoschisi',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning asoschisi, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.06118808686733246,\n'token': 14718,\n'token_str': ' olimi',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning olimi, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.05062260851264,\n'token': 3360,\n'token_str': ' buyuk',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning buyuk, mutafakkiri va davlat arbobi bo‚Äòlgan.'},\n{'score': 0.03740185126662254,\n'token': 16300,\n'token_str': ' atoqli',\n'sequence': 'Alisher Navoiy ‚Äì ulug‚Äò o‚Äòzbek va boshqa turkiy xalqlarning atoqli, mutafakkiri va davlat arbobi bo‚Äòlgan.'}]\n>>> unmasker(\"Egiluvchan bo ªg ªinlari va <mask>, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.\")\n[{'score': 0.1697940230369568,\n'token': 1376,\n'token_str': ' shuningdek',\n'sequence': 'Egiluvchan bo ªg ªinlari va shuningdek, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.030622979626059532,\n'token': 3359,\n'token_str': ' ayniqsa',\n'sequence': 'Egiluvchan bo ªg ªinlari va ayniqsa, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.02887713722884655,\n'token': 10013,\n'token_str': ' daraxtlar',\n'sequence': 'Egiluvchan bo ªg ªinlari va daraxtlar, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.02008388563990593,\n'token': 5038,\n'token_str': ' xususan',\n'sequence': 'Egiluvchan bo ªg ªinlari va xususan, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'},\n{'score': 0.018601732328534126,\n'token': 30498,\n'token_str': ' butalar',\n'sequence': 'Egiluvchan bo ªg ªinlari va butalar, yarim bukilgan tirnoqlari tik qiyaliklar hamda daraxtlarga oson chiqish imkonini beradi.'}]\nTraining data\nTahrirchiBERT is pretrained using a standard Masked Language Modeling (MLM) objective: the model is given a sequence of text with some tokens hidden, and it has to predict these masked tokens. TahrirchiBERT is trained on the Uzbek Crawl and small high quality latin portion of Uzbek Books, which contains roughly 4000 preprocessd books, 1.2 million curated text documents scraped from the internet and Telegram blogs.\nTraining procedure\nPreprocessing\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 30,528 to make fully use of rare words. The inputs of the model take pieces of 512 contiguous tokens that may span over documents. Also, we added number of regular expressions to avoid misrepresentation of different symbols that is used mostly incorrectly in practise.\nPretraining\nThe model was trained for one million steps with a batch size of 512. The sequence length was limited to 512 tokens during all pre-training stage. The optimizer used is Adam with a learning rate of 5e-4, Œ≤1=0.9\\beta_{1} = 0.9Œ≤1‚Äã=0.9 and Œ≤2=0.98\\beta_{2} = 0.98Œ≤2‚Äã=0.98, a weight decay of 1e-5, learning rate warmup to the full LR for 6% of the training duration with linearly decay to 0.02x the full LR by the end of the training duration.\nCitation\nPlease cite this model using the following format:\n@online{Mamasaidov2023TahrirchiBERT,\nauthor    = {Mukhammadsaid Mamasaidov and Abror Shopulatov},\ntitle     = {TahrirchiBERT small},\nyear      = {2023},\nurl       = {https://huggingface.co/tahrirchi/tahrirchi-bert-small},\nnote      = {Accessed: 2023-10-27}, % change this date\nurldate   = {2023-10-27} % change this date\n}\nGratitude\nWe are thankfull for these awesome organizations and people for help to make it happen:\nMosaicML team: for their script for efficiently training BERT models\nIlya Gusev: for advise throughout the process\nDavid Dale: for advise throughout the process",
    "RossAscends/Mistral7B_Dolphin2.1_LIMARP0.5_4bpw_exl2": "ehartford's merge of Mistral 7B 0.1 with his Dolphin 2.1 dataset\nhttps://huggingface.co/ehartford/dolphin-2.1-mistral-7b\nand\nLIMA RP dataset applied as a lora at 0.5 weight\nhttps://huggingface.co/lemonilia/limarp-llama2-v2/\nPurpose of the model is to be RP-focused, smart, fast, and lightweight for users with low VRAM.\nI've already built the exl2 4bpw quant (linked below), and it will run 8k ctx at around 6GB VRAM and respond to a full context at roughly 30tps (tested on my 3060) if exl2_hf loader is used with FA2 enabled.\nModel has been tested by several users on the SillyTavern discord server, and run on Horde for a full day - with good results.\nhttps://huggingface.co/RossAscends/Mistral7B_Dolphin2.1_LIMARP0.5_4bpw_exl2\nMistral or ChatML context presets both possible.\nfull weights:\nhttps://huggingface.co/RossAscends/Mistral_7B_Dolphin2.1_LIMA0.5_fp16",
    "maddes8cht/mosaicml-mpt-7b-instruct-gguf": "mpt-7b-instruct - GGUF\nAbout GGUF format\nQuantization variants\nLegacy quants\nNote:\nK-quants\nOriginal Model Card:\nMPT-7B-Instruct\nModel Date\nModel License\nDocumentation\nExample Question/Instruction\nHow to Use\nFormatting\nModel Description\nPreTraining Data\nTraining Configuration\nLimitations and Biases\nAcknowledgements\nMosaicML Platform\nDisclaimer\nCitation\nEnd of original Model File\nPlease consider to support my work\nI'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive information\nmpt-7b-instruct - GGUF\nModel creator: mosaicml\nOriginal model: mpt-7b-instruct\nMPT-7b and MPT-30B are part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\nAbout GGUF format\ngguf is the current file format used by the ggml library.\nA growing list of Software is using it and can therefore use this model.\nThe core project making use of the ggml library is the llama.cpp project by Georgi Gerganov\nQuantization variants\nThere is a bunch of quantized files available to cater to your specific needs. Here's how to choose the best option for you:\nLegacy quants\nQ4_0, Q4_1, Q5_0, Q5_1 and Q8 are legacy quantization types.\nNevertheless, they are fully supported, as there are several circumstances that cause certain model not to be compatible with the modern K-quants.\nNote:\nNow there's a new option to use K-quants even for previously 'incompatible' models, although this involves some fallback solution that makes them not real K-quants. More details can be found in affected model descriptions.\n(This mainly refers to Falcon 7b and Starcoder models)\nK-quants\nK-quants are designed with the idea that different levels of quantization in specific parts of the model can optimize performance, file size, and memory load.\nSo, if possible, use K-quants.\nWith a Q6_K, you'll likely find it challenging to discern a quality difference from the original model - ask your model two times the same question and you may encounter bigger quality differences.\nOriginal Model Card:\nMPT-7B-Instruct\nMPT-7B-Instruct is a model for short-form instruction following.\nIt is built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets.\nLicense: CC-By-SA-3.0\nDemo on Hugging Face Spaces\nThis model was trained by MosaicML and follows a modified decoder-only transformer architecture.\nModel Date\nMay 5, 2023\nModel License\nCC-By-SA-3.0\nDocumentation\nBlog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs\nCodebase (mosaicml/llm-foundry repo)\nQuestions: Feel free to contact us via the MosaicML Community Slack!\nExample Question/Instruction\nLongboi24:\nWhat is a quoll?\nMPT-7B-Instruct:\nA Quoll (pronounced ‚Äúcool‚Äù) is one of Australia‚Äôs native carnivorous marsupial mammals, which are also known as macropods or wallabies in other parts around Asia and South America\nHow to Use\nNote: This model requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom model architecture that is not yet part of the transformers package.\nIt includes options for many training efficiency features such as FlashAttention (Dao et al. 2022), ALiBi, QK LayerNorm, and more.\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n'mosaicml/mpt-7b-instruct',\ntrust_remote_code=True\n)\nNote: This model requires that trust_remote_code=True be passed to the from_pretrained method.\nThis is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package.\nMPT includes options for many training efficiency features such as FlashAttention, ALiBi, QK LayerNorm, and more.\nTo use the optimized triton implementation of FlashAttention, you can load the model on GPU (cuda:0) with attn_impl='triton' and with bfloat16 precision:\nimport torch\nimport transformers\nname = 'mosaicml/mpt-7b-instruct'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nname,\nconfig=config,\ntorch_dtype=torch.bfloat16, # Load model weights in bfloat16\ntrust_remote_code=True\n)\nAlthough the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\nimport transformers\nname = 'mosaicml/mpt-7b-instruct'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nname,\nconfig=config,\ntrust_remote_code=True\n)\nThis model was trained with the EleutherAI/gpt-neox-20b tokenizer.\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\nThe model can then be used, for example, within a text-generation pipeline.Note: when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\nfrom transformers import pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\nprint(\npipe('Here is a recipe for vegan banana bread:\\n',\nmax_new_tokens=100,\ndo_sample=True,\nuse_cache=True))\nFormatting\nThis model was trained on data formatted in the dolly-15k format:\nINSTRUCTION_KEY = \"### Instruction:\"\nRESPONSE_KEY = \"### Response:\"\nINTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\nPROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n{instruction_key}\n{instruction}\n{response_key}\n\"\"\".format(\nintro=INTRO_BLURB,\ninstruction_key=INSTRUCTION_KEY,\ninstruction=\"{instruction}\",\nresponse_key=RESPONSE_KEY,\n)\nexample = \"James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week? Explain before answering.\"\nfmt_ex = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\nIn the above example, fmt_ex is ready to be tokenized and sent through the model.\nModel Description\nThe architecture is a modification of a standard decoder-only transformer.\nThe model has been modified from a standard transformer in the following ways:\nIt uses FlashAttention\nIt uses ALiBi (Attention with Linear Biases) and does not use positional embeddings\nIt does not use biases\nHyperparameter\nValue\nn_parameters\n6.7B\nn_layers\n32\nn_heads\n32\nd_model\n4096\nvocab size\n50432\nsequence length\n2048\nPreTraining Data\nFor more details on the pretraining process, see MPT-7B.\nThe data was tokenized using the EleutherAI/gpt-neox-20b tokenizer.\nTraining Configuration\nThis model was trained on 8 A100-40GBs for about 2.3 hours using the MosaicML Platform.\nThe model was trained with sharded data parallelism using FSDP and used the AdamW optimizer.\nLimitations and Biases\nThe following language is modified from EleutherAI's GPT-NeoX-20B\nMPT-7B-Instruct can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B-Instruct was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\nAcknowledgements\nThis model was finetuned by Sam Havens and the MosaicML NLP team\nMosaicML Platform\nIf you're interested in training and deploying your own MPT or LLMs on the MosaicML Platform, sign up here.\nDisclaimer\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\nCitation\nPlease cite this model using the following format:\n@online{MosaicML2023Introducing,\nauthor    = {MosaicML NLP Team},\ntitle     = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},\nyear      = {2023},\nurl       = {www.mosaicml.com/blog/mpt-7b},\nnote      = {Accessed: 2023-03-28}, % change this date\nurldate   = {2023-03-28} % change this date\n}\nEnd of original Model File\nPlease consider to support my work\nComing Soon: I'm in the process of launching a sponsorship/crowdfunding campaign for my work. I'm evaluating Kickstarter, Patreon, or the new GitHub Sponsors platform, and I am hoping for some support and contribution to the continued availability of these kind of models. Your support will enable me to provide even more valuable resources and maintain the models you rely on. Your patience and ongoing support are greatly appreciated as I work to make this page an even more valuable resource for the community.",
    "immich-app/ViT-B-32__openai": "Model Description\nModel Description\nThis repo contains ONNX exports for the associated CLIP model by OpenCLIP. See the OpenCLIP repo for more info.\nThis repo is specifically intended for use with Immich, a self-hosted photo library.",
    "TheBloke/Echidna-13B-v0.3-GGUF": "Echidna 13B v0.3 - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: Alpaca\nLicensing\nCompatibility\nExplanation of quantisation methods\nProvided files\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using ctransformers\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nOriginal model card: NeverSleep's Echidna 13B v0.3\nThis model is a collab between IkariDev and Undi!\nDescription\nRatings:\nModels+loras used and recipe\nPrompt template: Alpaca\nOthers\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nEchidna 13B v0.3 - GGUF\nModel creator: NeverSleep\nOriginal model: Echidna 13B v0.3\nDescription\nThis repo contains GGUF format model files for NeverSleep's Echidna 13B v0.3.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplate list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nNeverSleep's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Alpaca\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nLicensing\nThe creator of the source model has listed its license as cc-by-nc-4.0, and this quantization has therefore used that same license.\nAs this model is based on Llama 2, it is also subject to the Meta Llama 2 license terms, and the license files for that are additionally included. It should therefore be considered as being claimed to be licensed under both licenses. I contacted Hugging Face for clarification on dual licensing but they do not yet have an official position. Should this change, or should Meta provide any feedback on this situation, I will update this section accordingly.\nIn the meantime, any questions regarding licensing, and in particular how these two licenses might interact, should be directed to the original model repository: NeverSleep's Echidna 13B v0.3.\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\nechidna-13b-v0.3.Q2_K.gguf\nQ2_K\n2\n5.43 GB\n7.93 GB\nsmallest, significant quality loss - not recommended for most purposes\nechidna-13b-v0.3.Q3_K_S.gguf\nQ3_K_S\n3\n5.66 GB\n8.16 GB\nvery small, high quality loss\nechidna-13b-v0.3.Q3_K_M.gguf\nQ3_K_M\n3\n6.34 GB\n8.84 GB\nvery small, high quality loss\nechidna-13b-v0.3.Q3_K_L.gguf\nQ3_K_L\n3\n6.93 GB\n9.43 GB\nsmall, substantial quality loss\nechidna-13b-v0.3.Q4_0.gguf\nQ4_0\n4\n7.37 GB\n9.87 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\nechidna-13b-v0.3.Q4_K_S.gguf\nQ4_K_S\n4\n7.41 GB\n9.91 GB\nsmall, greater quality loss\nechidna-13b-v0.3.Q4_K_M.gguf\nQ4_K_M\n4\n7.87 GB\n10.37 GB\nmedium, balanced quality - recommended\nechidna-13b-v0.3.Q5_0.gguf\nQ5_0\n5\n8.97 GB\n11.47 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\nechidna-13b-v0.3.Q5_K_S.gguf\nQ5_K_S\n5\n8.97 GB\n11.47 GB\nlarge, low quality loss - recommended\nechidna-13b-v0.3.Q5_K_M.gguf\nQ5_K_M\n5\n9.23 GB\n11.73 GB\nlarge, very low quality loss - recommended\nechidna-13b-v0.3.Q6_K.gguf\nQ6_K\n6\n10.68 GB\n13.18 GB\nvery large, extremely low quality loss\nechidna-13b-v0.3.Q8_0.gguf\nQ8_0\n8\n13.83 GB\n16.33 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/Echidna-13B-v0.3-GGUF and below it, a specific filename to download, such as: echidna-13b-v0.3.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/Echidna-13B-v0.3-GGUF echidna-13b-v0.3.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/Echidna-13B-v0.3-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Echidna-13B-v0.3-GGUF echidna-13b-v0.3.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 32 -m echidna-13b-v0.3.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{prompt}\\n\\n### Response:\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 4096 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions here: text-generation-webui/docs/llama.cpp.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries.\nHow to load this model in Python code, using ctransformers\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\nSimple ctransformers example code\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Echidna-13B-v0.3-GGUF\", model_file=\"echidna-13b-v0.3.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\nprint(llm(\"AI is going to\"))\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Brandon Frisco, LangChain4j, Spiking Neurons AB, transmissions 11, Joseph William Delisle, Nitin Borwankar, Willem Michiel, Michael Dempsey, vamX, Jeffrey Morgan, zynix, jjj, Omer Bin Jawed, Sean Connelly, jinyuan sun, Jeromy Smith, Shadi, Pawan Osman, Chadd, Elijah Stavena, Illia Dulskyi, Sebastain Graf, Stephen Murray, terasurfer, Edmond Seymore, Celu Ramasamy, Mandus, Alex, biorpg, Ajan Kanaga, Clay Pascal, Raven Klaugh, ÈòøÊòé, K, ya boyyy, usrbinkat, Alicia Loh, John Villwock, ReadyPlayerEmma, Chris Smitley, Cap'n Zoog, fincy, GodLy, S_X, sidney chen, Cory Kujawski, OG, Mano Prime, AzureBlack, Pieter, Kalila, Spencer Kim, Tom X Nguyen, Stanislav Ovsiannikov, Michael Levine, Andrey, Trailburnt, Vadim, Enrico Ros, Talal Aujan, Brandon Phillips, Jack West, Eugene Pentland, Michael Davis, Will Dee, webtim, Jonathan Leane, Alps Aficionado, Rooh Singh, Tiffany J. Kim, theTransient, Luke @flexchar, Elle, Caitlyn Gatomon, Ari Malik, subjectnull, Johann-Peter Hartmann, Trenton Dambrowitz, Imad Khwaja, Asp the Wyvern, Emad Mostaque, Rainer Wilmers, Alexandros Triantafyllidis, Nicholas, Pedro Madruga, SuperWojo, Harry Royden McLaughlin, James Bentley, Olakabola, David Ziegler, Ai Maven, Jeff Scroggin, Nikolai Manek, Deo Leter, Matthew Berman, Fen Risland, Ken Nordquist, Manuel Alberto Morcote, Luke Pendergrass, TL, Fred von Graf, Randy H, Dan Guido, NimbleBox.ai, Vitor Caleffi, Gabriel Tamborski, knownsqashed, Lone Striker, Erik Bj√§reholt, John Detwiler, Leonard Tan, Iucharbius\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: NeverSleep's Echidna 13B v0.3\nThis model is a collab between IkariDev and Undi!\nEchidna v0.3 model. Use Alpaca format. Suitable for RP, ERP and general stuff.\nEchidna v0.3, here it is.. its like Echidna v0.2 on steroids.\nThis model seems to still be pretty sensitive to your generation settings, experiment until you've found your settings.\n[Recommended settings - No settings yet(Please suggest some over in the Community tab!)]\nDescription\nThis repo contains FP16 files of Echidna-13b-v0.3.\nFP16 - by IkariDev and Undi\nexl2[8bpw-8h] - by AzureBlack\nGGUF - by IkariDev and Undi\nRatings:\nNote: We have permission of all users to upload their ratings, i DONT screenshot random reviews without asking if i can put them here!\nNo ratings yet!\nIf you want your rating to be here, send us a message over on DC and we'll put up a screenshot of it here. DC name is \"ikaridev\" and \"undi\".\nModels+loras used and recipe\nXwin-LM/Xwin-LM-13B-V0.2\nIkariDev/Athena-v3\nHeralax/Cat-0.5\nUndi95/PsyMedRP-v1-13B\ncgato/Thespis-13b-v0.4\nKoboldAI/LLaMA2-13B-TiefighterLR\nHeralax/Augmental-13b-two-epochs\nSao10K/SthenoWriter2.1-L2-13B\nUndi95/Storytelling-v2.1-13B-lora\nlemonilia/LimaRP-Llama2-13B-v3-EXPERIMENT\nmigtissera/Synthia-13B-v1.2\nUndi95/Trismegistus-lora\nPrompt template: Alpaca\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nOthers\nUndi: If you want to support me, you can here.\nIkariDev: Visit my retro/neocities style website please kek",
    "IkariDev/Athnete-13B": "Description\nRatings:\nModels and loras used\nPrompt template: Alpaca\nAthnete model. Use Alpaca format. Suitable for RP, ERP and general stuff.\nDespite the awful banner i made, this model could actually be better than raw Athena v3.\nDescription\nThis repo contains fp16 files of Athnete.\nfp16 - by IkariDev\nGGUF - by IkariDev\nRatings:\nNote: I have permission of all users to upload their ratings, i DONT screenshot random reviews without asking if i can put them here!\nModels and loras used\nIkariDev/Athena-v3\nUndi95/Nete-13B\nPrompt template: Alpaca\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nHUGE thanks to Undi95 for doing the merging (Recipe was my idea, he merged)",
    "deepseek-ai/deepseek-coder-6.7b-instruct": "1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\nChat Model Inference\n4. License\n5. Contact\n[üè†Homepage]  |  [ü§ñ Chat with DeepSeek Coder]  |  [Discord]  |  [Wechat(ÂæÆ‰ø°)]\n1. Introduction of Deepseek Coder\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\nMassive Training Data: Trained from scratch fon 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\nHighly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\nSuperior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\nAdvanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n2. Model Summary\ndeepseek-coder-6.7b-instruct is a 6.7B parameter model initialized from deepseek-coder-6.7b-base and fine-tuned on 2B tokens of instruction data.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-coder\nChat With DeepSeek Coder: DeepSeek-Coder\n3. How to Use\nHere give some examples of how to use our model.\nChat Model Inference\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at agi_code@deepseek.com.",
    "NeverSleep/Nethena-20B": "This model is a collab between IkariDev and Undi!\nDescription\nRatings:\nModels+loras used and recipe\nPrompt template: Alpaca\nOthers\nThis model is a collab between IkariDev and Undi!\nNethena-20B model. Use Alpaca format. Suitable for RP, ERP and general stuff.\nWhat would happen if we combine all of out best models? Well.. here it is, the holy grail: Echidna v0.3 + Athena v3 + Nete\nThis model also has a 13b version, you can check it out right here.\n[Recommended settings - No settings yet(Please suggest some over in the Community tab!)]\nDescription\nThis repo contains fp16 files of Nethena-20B.\nFP16 - by IkariDev and Undi\nGGUF - by IkariDev and Undi\nRatings:\nNote: We have permission of all users to upload their ratings, i DONT screenshot random reviews without asking if i can put them here!\nNo ratings yet!\nIf you want your rating to be here, send us a message over on DC and we'll put up a screenshot of it here. DC name is \"ikaridev\" and \"undi\".\nModels+loras used and recipe\nNeverSleep/Echidna-13b-v0.3\nIkariDev/Athena-v3\nUndi95/Nete-13B\nPrompt template: Alpaca\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nOthers\nUndi: If you want to support me, you can here.\nIkariDev: Visit my retro/neocities style website please kek",
    "maxin-cn/Latte": "Latte: Latent Diffusion Transformer for Video Generation\nNews\nContact Us\nCitation\nAcknowledgments\nLatte: Latent Diffusion Transformer for Video Generation\nThis repo contains pre-trained weights on FaceForensics, SkyTimelapse, UCF101, and Taichi-HD for our paper exploring latent diffusion models with transformers (Latte). You can find more visualizations on our project page.\nIf you want to obtain text-to-video generation pre-trained weights, please refer to here.\nNews\n(üî• New) May. 23, 2024. üí• Latte-1 for Text-to-video generation is released! You can download pre-trained model here. Latte-1 also supports Text-to-image generation, please run bash sample/t2i.sh.\n(üî• New) Mar. 20, 2024. üí• An updated LatteT2V model is coming soon, stay tuned!\n(üî• New) Feb. 24, 2024. üí• We are very grateful that researchers and developers like our work. We will continue to update our LatteT2V model, hoping that our efforts can help the community develop. Our Latte discord channel is created for discussions. Coders are welcome to contribute.\n(üî• New) Jan. 9, 2024. üí• An updated LatteT2V model initialized with the PixArt-Œ± is released, the checkpoint can be found here.\n(üî• New) Oct. 31, 2023. üí• The training and inference code is released. All checkpoints (including FaceForensics, SkyTimelapse, UCF101, and Taichi-HD) can be found here. In addition, the LatteT2V inference code is provided.\nContact Us\nYaohui Wang: wangyaohui@pjlab.org.cn\nXin Ma: xin.ma1@monash.edu\nCitation\nIf you find this work useful for your research, please consider citing it.\n@article{ma2024latte,\ntitle={Latte: Latent Diffusion Transformer for Video Generation},\nauthor={Ma, Xin and Wang, Yaohui and Jia, Gengyun and Chen, Xinyuan and Liu, Ziwei and Li, Yuan-Fang and Chen, Cunjian and Qiao, Yu},\njournal={arXiv preprint arXiv:2401.03048},\nyear={2024}\n}\nAcknowledgments\nLatte has been greatly inspired by the following amazing works and teams: DiT and PixArt-Œ±, we thank all the contributors for open-sourcing.",
    "laion/larger_clap_general": "Model\nTL;DR\nDescription\nUsage\nUses\nPerform zero-shot audio classification\nUsing pipeline\nRun the model:\nRun the model on CPU:\nRun the model on GPU:\nCitation\nModel\nTL;DR\nCLAP is to audio what CLIP is to image. This is an improved CLAP checkpoint, specifically trained on general audio, music and speech.\nDescription\nCLAP (Contrastive Language-Audio Pretraining) is a neural network trained on a variety of (audio, text) pairs. It can be instructed in to predict the most relevant text snippet, given an audio, without directly optimizing for the task. The CLAP model uses a SWINTransformer to get audio features from a log-Mel spectrogram input, and a RoBERTa model to get text features. Both the text and audio features are then projected to a latent space with identical dimension. The dot product between the projected audio and text features is then used as a similar score.\nUsage\nYou can use this model for zero shot audio classification or extracting audio and/or textual features.\nUses\nPerform zero-shot audio classification\nUsing pipeline\nfrom datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(\"ashraq/esc50\")\naudio = dataset[\"train\"][\"audio\"][-1][\"array\"]\naudio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_general\")\noutput = audio_classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vaccum cleaner\"])\nprint(output)\n>>> [{\"score\": 0.999, \"label\": \"Sound of a dog\"}, {\"score\": 0.001, \"label\": \"Sound of vaccum cleaner\"}]\nRun the model:\nYou can also get the audio and text embeddings using ClapModel\nRun the model on CPU:\nfrom datasets import load_dataset\nfrom transformers import ClapModel, ClapProcessor\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\naudio_sample = librispeech_dummy[0]\nmodel = ClapModel.from_pretrained(\"laion/larger_clap_general\")\nprocessor = ClapProcessor.from_pretrained(\"laion/larger_clap_general\")\ninputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\")\naudio_embed = model.get_audio_features(**inputs)\nRun the model on GPU:\nfrom datasets import load_dataset\nfrom transformers import ClapModel, ClapProcessor\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\naudio_sample = librispeech_dummy[0]\nmodel = ClapModel.from_pretrained(\"laion/larger_clap_general\").to(0)\nprocessor = ClapProcessor.from_pretrained(\"laion/larger_clap_general\")\ninputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\").to(0)\naudio_embed = model.get_audio_features(**inputs)\nCitation\nIf you are using this model for your work, please consider citing the original paper:\n@misc{https://doi.org/10.48550/arxiv.2211.06687,\ndoi = {10.48550/ARXIV.2211.06687},\nurl = {https://arxiv.org/abs/2211.06687},\nauthor = {Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},\nkeywords = {Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},\ntitle = {Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}",
    "NousResearch/Yarn-Mistral-7b-128k": "Model Card: Nous-Yarn-Mistral-7b-128k\nModel Description\nBenchmarks\nCollaborators\nModel Card: Nous-Yarn-Mistral-7b-128k\nPreprint (arXiv)GitHub\nModel Description\nNous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method.\nIt is an extension of Mistral-7B-v0.1 and supports a 128k token context window.\nTo use, pass trust_remote_code=True when loading the model, for example\nmodel = AutoModelForCausalLM.from_pretrained(\"NousResearch/Yarn-Mistral-7b-128k\",\nuse_flash_attention_2=True,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True)\nIn addition you will need to use the latest version of transformers (until 4.35 comes out)\npip install git+https://github.com/huggingface/transformers\nBenchmarks\nLong context benchmarks:\nModel\nContext Window\n8k PPL\n16k PPL\n32k PPL\n64k PPL\n128k PPL\nMistral-7B-v0.1\n8k\n2.96\n-\n-\n-\n-\nYarn-Mistral-7b-64k\n64k\n3.04\n2.65\n2.44\n2.20\n-\nYarn-Mistral-7b-128k\n128k\n3.08\n2.68\n2.47\n2.24\n2.19\nShort context benchmarks showing that quality degradation is minimal:\nModel\nContext Window\nARC-c\nHellaswag\nMMLU\nTruthful QA\nMistral-7B-v0.1\n8k\n59.98\n83.31\n64.16\n42.15\nYarn-Mistral-7b-64k\n64k\n59.38\n81.21\n61.32\n42.50\nYarn-Mistral-7b-128k\n128k\n58.87\n80.58\n60.64\n42.46\nCollaborators\nbloc97: Methods, paper and evals\n@theemozilla: Methods, paper, model training, and evals\n@EnricoShippole: Model training\nhonglu2875: Paper and evals\nThe authors would like to thank LAION AI for their support of compute for this model.\nIt was trained on the JUWELS supercomputer.",
    "deepseek-ai/deepseek-coder-33b-instruct": "1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\nChat Model Inference\n4. License\n5. Contact\n[üè†Homepage]  |  [ü§ñ Chat with DeepSeek Coder]  |  [Discord]  |  [Wechat(ÂæÆ‰ø°)]\n1. Introduction of Deepseek Coder\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\nMassive Training Data: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\nHighly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\nSuperior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\nAdvanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n2. Model Summary\ndeepseek-coder-33b-instruct is a 33B parameter model initialized from deepseek-coder-33b-base and fine-tuned on 2B tokens of instruction data.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-coder\nChat With DeepSeek Coder: DeepSeek-Coder\n3. How to Use\nHere give some examples of how to use our model.\nChat Model Inference\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at agi_code@deepseek.com.",
    "TheBloke/deepseek-coder-1.3b-instruct-GGUF": "Deepseek Coder 1.3B Instruct - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: DeepSeek\nCompatibility\nExplanation of quantisation methods\nProvided files\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using ctransformers\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\n1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\n4. License\n5. Contact\nOriginal model card: DeepSeek's Deepseek Coder 1.3B Instruct\n1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\n4. License\n5. Contact\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nDeepseek Coder 1.3B Instruct - GGUF\nModel creator: DeepSeek\nOriginal model: Deepseek Coder 1.3B Instruct\nDescription\nThis repo contains GGUF format model files for DeepSeek's Deepseek Coder 1.3B Instruct.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nDeepSeek's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: DeepSeek\nYou are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n### Instruction:\n{prompt}\n### Response:\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\ndeepseek-coder-1.3b-instruct.Q2_K.gguf\nQ2_K\n2\n0.63 GB\n3.13 GB\nsmallest, significant quality loss - not recommended for most purposes\ndeepseek-coder-1.3b-instruct.Q3_K_S.gguf\nQ3_K_S\n3\n0.66 GB\n3.16 GB\nvery small, high quality loss\ndeepseek-coder-1.3b-instruct.Q3_K_M.gguf\nQ3_K_M\n3\n0.70 GB\n3.20 GB\nvery small, high quality loss\ndeepseek-coder-1.3b-instruct.Q3_K_L.gguf\nQ3_K_L\n3\n0.74 GB\n3.24 GB\nsmall, substantial quality loss\ndeepseek-coder-1.3b-instruct.Q4_0.gguf\nQ4_0\n4\n0.78 GB\n3.28 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ndeepseek-coder-1.3b-instruct.Q4_K_S.gguf\nQ4_K_S\n4\n0.81 GB\n3.31 GB\nsmall, greater quality loss\ndeepseek-coder-1.3b-instruct.Q4_K_M.gguf\nQ4_K_M\n4\n0.87 GB\n3.37 GB\nmedium, balanced quality - recommended\ndeepseek-coder-1.3b-instruct.Q5_0.gguf\nQ5_0\n5\n0.94 GB\n3.44 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ndeepseek-coder-1.3b-instruct.Q5_K_S.gguf\nQ5_K_S\n5\n0.95 GB\n3.45 GB\nlarge, low quality loss - recommended\ndeepseek-coder-1.3b-instruct.Q5_K_M.gguf\nQ5_K_M\n5\n1.00 GB\n3.50 GB\nlarge, very low quality loss - recommended\ndeepseek-coder-1.3b-instruct.Q6_K.gguf\nQ6_K\n6\n1.17 GB\n3.67 GB\nvery large, extremely low quality loss\ndeepseek-coder-1.3b-instruct.Q8_0.gguf\nQ8_0\n8\n1.43 GB\n3.93 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/deepseek-coder-1.3b-instruct-GGUF and below it, a specific filename to download, such as: deepseek-coder-1.3b-instruct.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/deepseek-coder-1.3b-instruct-GGUF deepseek-coder-1.3b-instruct.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/deepseek-coder-1.3b-instruct-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/deepseek-coder-1.3b-instruct-GGUF deepseek-coder-1.3b-instruct.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 32 -m deepseek-coder-1.3b-instruct.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\\n### Instruction:\\n{prompt}\\n### Response:\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 2048 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions here: text-generation-webui/docs/llama.cpp.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries.\nHow to load this model in Python code, using ctransformers\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\nSimple ctransformers example code\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/deepseek-coder-1.3b-instruct-GGUF\", model_file=\"deepseek-coder-1.3b-instruct.Q4_K_M.gguf\", model_type=\"deepseek\", gpu_layers=50)\nprint(llm(\"AI is going to\"))\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Brandon Frisco, LangChain4j, Spiking Neurons AB, transmissions 11, Joseph William Delisle, Nitin Borwankar, Willem Michiel, Michael Dempsey, vamX, Jeffrey Morgan, zynix, jjj, Omer Bin Jawed, Sean Connelly, jinyuan sun, Jeromy Smith, Shadi, Pawan Osman, Chadd, Elijah Stavena, Illia Dulskyi, Sebastain Graf, Stephen Murray, terasurfer, Edmond Seymore, Celu Ramasamy, Mandus, Alex, biorpg, Ajan Kanaga, Clay Pascal, Raven Klaugh, ÈòøÊòé, K, ya boyyy, usrbinkat, Alicia Loh, John Villwock, ReadyPlayerEmma, Chris Smitley, Cap'n Zoog, fincy, GodLy, S_X, sidney chen, Cory Kujawski, OG, Mano Prime, AzureBlack, Pieter, Kalila, Spencer Kim, Tom X Nguyen, Stanislav Ovsiannikov, Michael Levine, Andrey, Trailburnt, Vadim, Enrico Ros, Talal Aujan, Brandon Phillips, Jack West, Eugene Pentland, Michael Davis, Will Dee, webtim, Jonathan Leane, Alps Aficionado, Rooh Singh, Tiffany J. Kim, theTransient, Luke @flexchar, Elle, Caitlyn Gatomon, Ari Malik, subjectnull, Johann-Peter Hartmann, Trenton Dambrowitz, Imad Khwaja, Asp the Wyvern, Emad Mostaque, Rainer Wilmers, Alexandros Triantafyllidis, Nicholas, Pedro Madruga, SuperWojo, Harry Royden McLaughlin, James Bentley, Olakabola, David Ziegler, Ai Maven, Jeff Scroggin, Nikolai Manek, Deo Leter, Matthew Berman, Fen Risland, Ken Nordquist, Manuel Alberto Morcote, Luke Pendergrass, TL, Fred von Graf, Randy H, Dan Guido, NimbleBox.ai, Vitor Caleffi, Gabriel Tamborski, knownsqashed, Lone Striker, Erik Bj√§reholt, John Detwiler, Leonard Tan, Iucharbius\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: DeepSeek's Deepseek Coder 1.3B Instruct\n[üè†Homepage]  |  [ü§ñ Chat with DeepSeek Coder]  |  [Discord]  |  [Wechat(ÂæÆ‰ø°)]\n1. Introduction of Deepseek Coder\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\nMassive Training Data: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\nHighly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\nSuperior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\nAdvanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n2. Model Summary\ndeepseek-coder-1.3b-instruct is a 1.3B parameter model initialized from deepseek-coder-1.3b-base and fine-tuned on 2B tokens of instruction data.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-coder\nChat With DeepSeek Coder: DeepSeek-Coder\n3. How to Use\nHere give some examples of how to use our model.\nChat Model Inference\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n# 32021 is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at agi_code@deepseek.com.",
    "TheBloke/deepseek-coder-6.7B-instruct-GGUF": "Deepseek Coder 6.7B Instruct - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: DeepSeek\nCompatibility\nExplanation of quantisation methods\nProvided files\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using ctransformers\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\n1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\n4. License\n5. Contact\nOriginal model card: DeepSeek's Deepseek Coder 6.7B Instruct\n1. Introduction of Deepseek Coder\n2. Model Summary\n3. How to Use\n4. License\n5. Contact\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nDeepseek Coder 6.7B Instruct - GGUF\nModel creator: DeepSeek\nOriginal model: Deepseek Coder 6.7B Instruct\nDescription\nThis repo contains GGUF format model files for DeepSeek's Deepseek Coder 6.7B Instruct.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nDeepSeek's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: DeepSeek\nYou are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n### Instruction:\n{prompt}\n### Response:\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\ndeepseek-coder-6.7b-instruct.Q2_K.gguf\nQ2_K\n2\n2.83 GB\n5.33 GB\nsmallest, significant quality loss - not recommended for most purposes\ndeepseek-coder-6.7b-instruct.Q3_K_S.gguf\nQ3_K_S\n3\n2.95 GB\n5.45 GB\nvery small, high quality loss\ndeepseek-coder-6.7b-instruct.Q3_K_M.gguf\nQ3_K_M\n3\n3.30 GB\n5.80 GB\nvery small, high quality loss\ndeepseek-coder-6.7b-instruct.Q3_K_L.gguf\nQ3_K_L\n3\n3.60 GB\n6.10 GB\nsmall, substantial quality loss\ndeepseek-coder-6.7b-instruct.Q4_0.gguf\nQ4_0\n4\n3.83 GB\n6.33 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ndeepseek-coder-6.7b-instruct.Q4_K_S.gguf\nQ4_K_S\n4\n3.86 GB\n6.36 GB\nsmall, greater quality loss\ndeepseek-coder-6.7b-instruct.Q4_K_M.gguf\nQ4_K_M\n4\n4.08 GB\n6.58 GB\nmedium, balanced quality - recommended\ndeepseek-coder-6.7b-instruct.Q5_0.gguf\nQ5_0\n5\n4.65 GB\n7.15 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ndeepseek-coder-6.7b-instruct.Q5_K_S.gguf\nQ5_K_S\n5\n4.65 GB\n7.15 GB\nlarge, low quality loss - recommended\ndeepseek-coder-6.7b-instruct.Q5_K_M.gguf\nQ5_K_M\n5\n4.79 GB\n7.29 GB\nlarge, very low quality loss - recommended\ndeepseek-coder-6.7b-instruct.Q6_K.gguf\nQ6_K\n6\n5.53 GB\n8.03 GB\nvery large, extremely low quality loss\ndeepseek-coder-6.7b-instruct.Q8_0.gguf\nQ8_0\n8\n7.16 GB\n9.66 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/deepseek-coder-6.7B-instruct-GGUF and below it, a specific filename to download, such as: deepseek-coder-6.7b-instruct.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/deepseek-coder-6.7B-instruct-GGUF deepseek-coder-6.7b-instruct.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/deepseek-coder-6.7B-instruct-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/deepseek-coder-6.7B-instruct-GGUF deepseek-coder-6.7b-instruct.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 32 -m deepseek-coder-6.7b-instruct.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\\n### Instruction:\\n{prompt}\\n### Response:\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 2048 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions here: text-generation-webui/docs/llama.cpp.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries.\nHow to load this model in Python code, using ctransformers\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install ctransformers\n# Or with CUDA GPU acceleration\npip install ctransformers[cuda]\n# Or with AMD ROCm GPU acceleration (Linux only)\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n# Or with Metal GPU acceleration for macOS systems only\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\nSimple ctransformers example code\nfrom ctransformers import AutoModelForCausalLM\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/deepseek-coder-6.7B-instruct-GGUF\", model_file=\"deepseek-coder-6.7b-instruct.Q4_K_M.gguf\", model_type=\"deepseek\", gpu_layers=50)\nprint(llm(\"AI is going to\"))\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Brandon Frisco, LangChain4j, Spiking Neurons AB, transmissions 11, Joseph William Delisle, Nitin Borwankar, Willem Michiel, Michael Dempsey, vamX, Jeffrey Morgan, zynix, jjj, Omer Bin Jawed, Sean Connelly, jinyuan sun, Jeromy Smith, Shadi, Pawan Osman, Chadd, Elijah Stavena, Illia Dulskyi, Sebastain Graf, Stephen Murray, terasurfer, Edmond Seymore, Celu Ramasamy, Mandus, Alex, biorpg, Ajan Kanaga, Clay Pascal, Raven Klaugh, ÈòøÊòé, K, ya boyyy, usrbinkat, Alicia Loh, John Villwock, ReadyPlayerEmma, Chris Smitley, Cap'n Zoog, fincy, GodLy, S_X, sidney chen, Cory Kujawski, OG, Mano Prime, AzureBlack, Pieter, Kalila, Spencer Kim, Tom X Nguyen, Stanislav Ovsiannikov, Michael Levine, Andrey, Trailburnt, Vadim, Enrico Ros, Talal Aujan, Brandon Phillips, Jack West, Eugene Pentland, Michael Davis, Will Dee, webtim, Jonathan Leane, Alps Aficionado, Rooh Singh, Tiffany J. Kim, theTransient, Luke @flexchar, Elle, Caitlyn Gatomon, Ari Malik, subjectnull, Johann-Peter Hartmann, Trenton Dambrowitz, Imad Khwaja, Asp the Wyvern, Emad Mostaque, Rainer Wilmers, Alexandros Triantafyllidis, Nicholas, Pedro Madruga, SuperWojo, Harry Royden McLaughlin, James Bentley, Olakabola, David Ziegler, Ai Maven, Jeff Scroggin, Nikolai Manek, Deo Leter, Matthew Berman, Fen Risland, Ken Nordquist, Manuel Alberto Morcote, Luke Pendergrass, TL, Fred von Graf, Randy H, Dan Guido, NimbleBox.ai, Vitor Caleffi, Gabriel Tamborski, knownsqashed, Lone Striker, Erik Bj√§reholt, John Detwiler, Leonard Tan, Iucharbius\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: DeepSeek's Deepseek Coder 6.7B Instruct\n[üè†Homepage]  |  [ü§ñ Chat with DeepSeek Coder]  |  [Discord]  |  [Wechat(ÂæÆ‰ø°)]\n1. Introduction of Deepseek Coder\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\nMassive Training Data: Trained from scratch fon 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\nHighly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\nSuperior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\nAdvanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n2. Model Summary\ndeepseek-coder-6.7b-instruct is a 6.7B parameter model initialized from deepseek-coder-6.7b-base and fine-tuned on 2B tokens of instruction data.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-coder\nChat With DeepSeek Coder: DeepSeek-Coder\n3. How to Use\nHere give some examples of how to use our model.\nChat Model Inference\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n# 32021 is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at agi_code@deepseek.com.",
    "abetlen/replit-code-v1_5-3b-GGUF": "README.md exists but content is empty.",
    "TheBloke/Psyfighter-13B-GPTQ": "Psyfighter 13B - GPTQ\nDescription\nRepositories available\nPrompt template: Unknown\nKnown compatible clients / servers\nProvided files, and GPTQ parameters\nHow to download, including from branches\nIn text-generation-webui\nFrom the command line\nWith git (not recommended)\nHow to easily download and use this model in text-generation-webui\nServing this model from Text Generation Inference (TGI)\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nYou can then use the following code\nCompatibility\nDiscord\nThanks, and how to contribute\nOriginal model card: Jeb Carter's Psyfighter 13B\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nPsyfighter 13B - GPTQ\nModel creator: Jeb Carter\nOriginal model: Psyfighter 13B\nDescription\nThis repo contains GPTQ model files for Jeb Carter's Psyfighter 13B.\nMultiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.\nThese files were quantised using hardware kindly provided by Massed Compute.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nJeb Carter's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Unknown\n{prompt}\nKnown compatible clients / servers\nThese GPTQ models are known to work in the following inference servers/webuis.\ntext-generation-webui\nKoboldAI United\nLoLLMS Web UI\nHugging Face Text Generation Inference (TGI)\nThis may not be a complete list; if you know of others, please let me know!\nProvided files, and GPTQ parameters\nMultiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.\nEach separate quant is in a different branch.  See below for instructions on fetching from different branches.\nMost GPTQ files are made with AutoGPTQ. Mistral models are currently made with Transformers.\nExplanation of GPTQ parameters\nBits: The bit size of the quantised model.\nGS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation accuracy. \"None\" is the lowest possible value.\nAct Order: True or False. Also known as desc_act. True results in better quantisation accuracy. Some GPTQ clients have had issues with models that use Act Order plus Group Size, but this is generally resolved now.\nDamp %: A GPTQ parameter that affects how samples are processed for quantisation. 0.01 is default, but 0.1 results in slightly better accuracy.\nGPTQ dataset: The calibration dataset used during quantisation. Using a dataset more appropriate to the model's training can improve quantisation accuracy. Note that the GPTQ calibration dataset is not the same as the dataset used to train the model - please refer to the original model repo for details of the training dataset(s).\nSequence Length: The length of the dataset sequences used for quantisation. Ideally this is the same as the model sequence length. For some very long sequence models (16+K), a lower sequence length may have to be used. Note that a lower sequence length does not limit the sequence length of the quantised model. It only impacts the quantisation accuracy on longer inference sequences.\nExLlama Compatibility: Whether this file can be loaded with ExLlama, which currently only supports Llama and Mistral models in 4-bit.\nBranch\nBits\nGS\nAct Order\nDamp %\nGPTQ Dataset\nSeq Len\nSize\nExLlama\nDesc\nmain\n4\n128\nYes\n0.1\nwikitext\n4096\n7.26 GB\nYes\n4-bit, with Act Order and group size 128g. Uses even less VRAM than 64g, but with slightly lower accuracy.\ngptq-4bit-32g-actorder_True\n4\n32\nYes\n0.1\nwikitext\n4096\n8.00 GB\nYes\n4-bit, with Act Order and group size 32g. Gives highest possible inference quality, with maximum VRAM usage.\ngptq-8bit--1g-actorder_True\n8\nNone\nYes\n0.1\nwikitext\n4096\n13.36 GB\nNo\n8-bit, with Act Order. No group size, to lower VRAM requirements.\ngptq-8bit-128g-actorder_True\n8\n128\nYes\n0.1\nwikitext\n4096\n13.65 GB\nNo\n8-bit, with group size 128g for higher inference quality and with Act Order for even higher accuracy.\ngptq-8bit-32g-actorder_True\n8\n32\nYes\n0.1\nwikitext\n4096\n14.54 GB\nNo\n8-bit, with group size 32g and Act Order for maximum inference quality.\ngptq-4bit-64g-actorder_True\n4\n64\nYes\n0.1\nwikitext\n4096\n7.51 GB\nYes\n4-bit, with Act Order and group size 64g. Uses less VRAM than 32g, but with slightly lower accuracy.\nHow to download, including from branches\nIn text-generation-webui\nTo download from the main branch, enter TheBloke/Psyfighter-13B-GPTQ in the \"Download model\" box.\nTo download from another branch, add :branchname to the end of the download name, eg TheBloke/Psyfighter-13B-GPTQ:gptq-4bit-32g-actorder_True\nFrom the command line\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nTo download the main branch to a folder called Psyfighter-13B-GPTQ:\nmkdir Psyfighter-13B-GPTQ\nhuggingface-cli download TheBloke/Psyfighter-13B-GPTQ --local-dir Psyfighter-13B-GPTQ --local-dir-use-symlinks False\nTo download from a different branch, add the --revision parameter:\nmkdir Psyfighter-13B-GPTQ\nhuggingface-cli download TheBloke/Psyfighter-13B-GPTQ --revision gptq-4bit-32g-actorder_True --local-dir Psyfighter-13B-GPTQ --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage\nIf you remove the --local-dir-use-symlinks False parameter, the files will instead be stored in the central Hugging Face cache directory (default location on Linux is: ~/.cache/huggingface), and symlinks will be added to the specified --local-dir, pointing to their real location in the cache. This allows for interrupted downloads to be resumed, and allows you to quickly clone the repo to multiple places on disk without triggering a download again. The downside, and the reason why I don't list that as the default option, is that the files are then hidden away in a cache folder and it's harder to know where your disk space is being used, and to clear it up if/when you want to remove a download model.\nThe cache location can be changed with the HF_HOME environment variable, and/or the --cache-dir parameter to huggingface-cli.\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nmkdir Psyfighter-13B-GPTQ\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Psyfighter-13B-GPTQ --local-dir Psyfighter-13B-GPTQ --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nWith git (not recommended)\nTo clone a specific branch with git, use a command like this:\ngit clone --single-branch --branch gptq-4bit-32g-actorder_True https://huggingface.co/TheBloke/Psyfighter-13B-GPTQ\nNote that using Git with HF repos is strongly discouraged. It will be much slower than using huggingface-hub, and will use twice as much disk space as it has to store the model files twice (it stores every byte both in the intended target folder, and again in the .git folder as a blob.)\nHow to easily download and use this model in text-generation-webui\nPlease make sure you're using the latest version of text-generation-webui.\nIt is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install.\nClick the Model tab.\nUnder Download custom model or LoRA, enter TheBloke/Psyfighter-13B-GPTQ.\nTo download from a specific branch, enter for example TheBloke/Psyfighter-13B-GPTQ:gptq-4bit-32g-actorder_True\nsee Provided Files above for the list of branches for each option.\nClick Download.\nThe model will start downloading. Once it's finished it will say \"Done\".\nIn the top left, click the refresh icon next to Model.\nIn the Model dropdown, choose the model you just downloaded: Psyfighter-13B-GPTQ\nThe model will automatically load, and is now ready for use!\nIf you want any custom settings, set them and then click Save settings for this model followed by Reload the Model in the top right.\nNote that you do not need to and should not set manual GPTQ parameters any more. These are set automatically from the file quantize_config.json.\nOnce you're ready, click the Text Generation tab and enter a prompt to get started!\nServing this model from Text Generation Inference (TGI)\nIt's recommended to use TGI version 1.1.0 or later. The official Docker container is: ghcr.io/huggingface/text-generation-inference:1.1.0\nExample Docker parameters:\n--model-id TheBloke/Psyfighter-13B-GPTQ --port 3000 --quantize gptq --max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096\nExample Python code for interfacing with TGI (requires huggingface-hub 0.17.0 or later):\npip3 install huggingface-hub\nfrom huggingface_hub import InferenceClient\nendpoint_url = \"https://your-endpoint-url-here\"\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n'''\nclient = InferenceClient(endpoint_url)\nresponse = client.text_generation(prompt,\nmax_new_tokens=128,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\ntop_k=40,\nrepetition_penalty=1.1)\nprint(f\"Model output: {response}\")\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nRequires: Transformers 4.33.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.\npip3 install transformers optimum\npip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\nIf you have problems installing AutoGPTQ using the pre-built wheels, install it from source instead:\npip3 uninstall -y auto-gptq\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\ngit checkout v0.4.2\npip3 install .\nYou can then use the following code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name_or_path = \"TheBloke/Psyfighter-13B-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\ndevice_map=\"auto\",\ntrust_remote_code=False,\nrevision=\"main\")\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n'''\nprint(\"\\n\\n*** Generate:\")\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n# Inference can also be done using transformers' pipeline\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=512,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\ntop_k=40,\nrepetition_penalty=1.1\n)\nprint(pipe(prompt_template)[0]['generated_text'])\nCompatibility\nThe files provided are tested to work with Transformers. For non-Mistral models, AutoGPTQ can also be used directly.\nExLlama is compatible with Llama and Mistral models in 4-bit. Please see the Provided Files table above for per-file compatibility.\nFor a list of clients/servers, please see \"Known compatible clients / servers\", above.\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Brandon Frisco, LangChain4j, Spiking Neurons AB, transmissions 11, Joseph William Delisle, Nitin Borwankar, Willem Michiel, Michael Dempsey, vamX, Jeffrey Morgan, zynix, jjj, Omer Bin Jawed, Sean Connelly, jinyuan sun, Jeromy Smith, Shadi, Pawan Osman, Chadd, Elijah Stavena, Illia Dulskyi, Sebastain Graf, Stephen Murray, terasurfer, Edmond Seymore, Celu Ramasamy, Mandus, Alex, biorpg, Ajan Kanaga, Clay Pascal, Raven Klaugh, ÈòøÊòé, K, ya boyyy, usrbinkat, Alicia Loh, John Villwock, ReadyPlayerEmma, Chris Smitley, Cap'n Zoog, fincy, GodLy, S_X, sidney chen, Cory Kujawski, OG, Mano Prime, AzureBlack, Pieter, Kalila, Spencer Kim, Tom X Nguyen, Stanislav Ovsiannikov, Michael Levine, Andrey, Trailburnt, Vadim, Enrico Ros, Talal Aujan, Brandon Phillips, Jack West, Eugene Pentland, Michael Davis, Will Dee, webtim, Jonathan Leane, Alps Aficionado, Rooh Singh, Tiffany J. Kim, theTransient, Luke @flexchar, Elle, Caitlyn Gatomon, Ari Malik, subjectnull, Johann-Peter Hartmann, Trenton Dambrowitz, Imad Khwaja, Asp the Wyvern, Emad Mostaque, Rainer Wilmers, Alexandros Triantafyllidis, Nicholas, Pedro Madruga, SuperWojo, Harry Royden McLaughlin, James Bentley, Olakabola, David Ziegler, Ai Maven, Jeff Scroggin, Nikolai Manek, Deo Leter, Matthew Berman, Fen Risland, Ken Nordquist, Manuel Alberto Morcote, Luke Pendergrass, TL, Fred von Graf, Randy H, Dan Guido, NimbleBox.ai, Vitor Caleffi, Gabriel Tamborski, knownsqashed, Lone Striker, Erik Bj√§reholt, John Detwiler, Leonard Tan, Iucharbius\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Jeb Carter's Psyfighter 13B\nmerge_method: task_arithmetic\nbase_model: TheBloke/Llama-2-13B-fp16\nmodels:\n- model: TheBloke/Llama-2-13B-fp16\n- model: KoboldAI/LLaMA2-13B-Tiefighter\nparameters:\nweight: 1.0\n- model: chaoyi-wu/MedLLaMA_13B\nparameters:\nweight: 0.01\n- model: Doctor-Shotgun/llama-2-13b-chat-limarp-v2-merged\nparameters:\nweight: 0.02\ndtype: float16\nThis model was made possible thanks to the Compute provided by the KoboldAI community."
}